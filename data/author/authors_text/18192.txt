Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2161?2171, Dublin, Ireland, August 23-29 2014.
Employing Event Inference to Improve Semi-Supervised Chinese 
Event Extraction 
 
 
Peifeng Li, Qiaoming Zhu, Guodong Zhou 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, qmzhu, gdzhou}@suda.edu.cn 
 
 
 
Abstract 
Although semi-supervised model can extract the event mentions matching frequent event patterns, it suf-
fers much from those event mentions, which match infrequent patterns or have no matching pattern. To 
solve this issue, this paper introduces various kinds of linguistic knowledge-driven event inference 
mechanisms to semi-supervised Chinese event extraction. These event inference mechanisms can capture 
linguistic knowledge from four aspects, i.e. semantics of argument role, compositional semantics of trig-
ger, consistency on coreference events and relevant events, to further recover missing event mentions 
from unlabeled texts. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mech-
anisms significantly outperform the refined state-of-the-art semi-supervised Chinese event extraction 
system in F1-score by 8.5%. 
1 Introduction 
An event is a specific occurrence involving arguments (participants and attributes) of the specific roles. 
In an event, trigger is the main word which most clearly expresses its occurrence, so recognizing an 
event can be recast as identifying a corresponding trigger. An event may have several arguments, 
which are entity mentions (e.g., person name, time, location, etc.) and must fulfill the corresponding 
roles. Take the following sentence as an example: 
S1: On the 25th Dec. (A1: Artifact), peacekeepers (A2: Artifact) returned (E1: Transport) to Am-
man (A3: Place) by flight (A4: Vehicle). 
For this example, an event extraction system should identify one event mention E1, which is trig-
gered by verb ?returned? whose event type is Transport, with four arguments, ?peacekeepers?, ?25th 
Dec.?, ?flight?, and ?Amman?, fulfilling the roles of Artifact, Time, Vehicle, and Place, respectively. 
Automatically extracting events from free texts is a higher-level Information Extraction (IE) task, 
which is still a challenge due to the complexity of natural language and the domain-specific nature, 
especially in Chinese for its specific characteristics. In particular, most of previous studies have fo-
cused on English event extraction, while only a few concern Chinese. 
Currently, supervised learning models have dominated event extraction. To reduce the labeled data 
required, a few semi-supervised models have been applied to English event extraction (e.g., Riloff 
1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Huang and Riloff, 2012). Since classi-
fier-based model needs dozens of annotated documents to train model, most of previous semi-
supervised models focused on pattern-based approach, which only needed a few seed (event) patterns. 
In those pattern-based approaches, frequent event patterns, which occur in many documents, were 
chosen as relevant patterns to match event mentions in unlabeled texts. However, the order of words in 
a Chinese sentence is rather agile for its open and flexible structure, and different orders might express 
the same meaning due to the semantics-driven nature of the Chinese language. This results in the di-
versity of Chinese event patterns and numerous infrequent patterns, even some event mentions having 
no matching patterns. Hence, it is an issue to extract the event mentions with infrequent patterns. 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 
2161
In this paper, we first implement a pattern-based semi-supervised model for Chinese event extrac-
tion as a baseline, following the state-of-the-art system as described in (Liao and Grishman, 2010a) 
and then refine this model to suit Chinese event extraction. Moreover, we propose various kinds of 
novel linguistic knowledge-driven event inference mechanisms to address the above issue and recover 
missing event mentions. These event inference mechanisms can capture the linguistic knowledge from 
semantics of argument role, compositional semantics of trigger, consistency on coreference events and 
relevant events. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mecha-
nisms dramatically outperform the baseline. 
The rest of this paper is organized as follows. Section 2 overviews related work. Section 3 presents 
the refined semi-supervised model for Chinese event extraction. Section 4 proposes several linguistic 
knowledge-driven event inference mechanisms. Section 5 reports and analyzes the experimental re-
sults. Finally, we conclude our work in Section 6. 
2 Related Work 
Almost all previous semi-supervised models focus on English event extraction, which can be subdi-
vided into pattern-based models (e.g., Riloff, 1996; Yangrber et al., 2000; Liao and Grishman, 2010a; 
Chambers and Jurafsky, 2011; Balasubramanian et al., 2013) and classifier-based models (e.g., Chieu 
et al., 2003; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009; Liu and Strzalkowski, 2012; 
Wang et al., 2013). Classifier-based models normally require a small set of annotated data (e.g., 100 
annotated documents), while pattern-based models need dozens of high quality seed patterns. 
Riloff (1996) first divided unlabeled documents into irrelevant and relevant documents, and the lat-
ter was much likely to contain further relevant patterns. Then event patterns from relevant documents 
were generated by using an annotated data and a set of heuristic rules. Yangarber et al. (2000) pro-
posed a document-centric view to boost a semi-supervised event extraction system, which assumes 
relevant documents always contain some shared patterns. Yangarber (2003) further introduced multi-
ple learners into the bootstrapping procedure to make the final decision on the combination of multiple 
learners on distinct event types. Huang and Riloff (2012) employed role-identifying nouns, which pro-
posed by Phillips and Riloff (2007), as seed terms to extract patterns from relevant documents and 
then generated the labeled instances to train three classifiers in their event extraction system. 
As an alternative, Stevenson and Greenwood (2005) proposed a pattern similarity-centric view and 
selected relevant patterns on similarity scores. Normally, bootstrapping on the document-centric view 
tends to accept the irrelevant patterns with a high occurrence frequency in relevant documents. To ad-
dress this problem, Liao and Grishman (2010a) introduced a pattern similarity metric into the docu-
ment-centric view as a filter to eliminate those irrelevant patterns. Liao and Grishman (2011) further 
applied an information retrieval mechanism to detect relevant documents and proposed a self-training 
strategy for bootstrapping. 
In addition, several studies focused on the event pattern representation, such as pairwise (e.g., Sub-
ject-Verb, Verb-Object) (Chambers and Jurafsky, 2008, 2009), SVO (Subject-Verb-Object) 
(Yangarber, 2000; Balasubramanian et al., 2013), chain (Sudo et al., 2001), subtree (Sudo et al., 2003) 
and complex pattern (Liu and Strzalkowski, 2012). 
In the literature, only one paper concerns semi-supervised Chinese event extraction. Chen and Ji 
(2009a) applied various kinds of cross-lingual features in the bootstrapping procedure to extract Chi-
nese event. With the help of over 500 annotated seed event mentions in 100 documents, they only 
achieved 35% in F1-score. This indicates the critical challenge in semi-supervised Chinese event ex-
traction. 
Only a few studies concern event inference mechanisms. Ji and Grishman (2008) employed a rule-
based approach to propagate consistent triggers and arguments across topic-related documents. Liao 
and Grishman (2010b) employed cross-event consistent information to improve sentence-level event 
extraction. Hong et al. (2011) regarded entity type consistency as a key feature to predict event men-
tions and adopted an information retrieval mechanism to promote event extraction. Li et al. (2013) 
proposed a global argument inference model on Chinese argument extraction to explore specific rela-
tionships among relevant event mentions to recover those inter-sentence arguments in the sentence, 
discourse and document layers. Li et al. (2014) also introduced Markov Logic Network (MLN) to cap-
ture the discourse-level consistency between Chinese trigger mentions to further recover those poor-
2162
context event mentions. In a word, all of above mechanisms focus on supervised event extraction and 
no literature involves in the event inference of semi-supervised event extraction. 
3 Semi-supervised Model for Chinese Event Extraction 
In this section, we refine a semi-supervised model for Chinese event extraction as a baseline, which 
includes two views, the document-centric view and pattern similarity-centric view. 
3.1 Semi-supervised Model 
Liao and Grishman (2010a) proposed a state-of-the-art semi-supervised event extraction system, 
which was a pattern-based approach and adopted bootstrapping mechanism to extract relevant patterns. 
Besides, two distinct views, the document-centric view and the pattern similarity-centric view as de-
scribed in Subsection 3.2 and 3.3, are incorporated in the bootstrapping procedure to rank event pat-
terns on different metrics. In each iteration, the candidate patterns, which extracted from unlabeled 
texts as the candidates of relevant patterns, are ranked following the document-centric view, then the 
candidate patterns with pattern similarity scores below a similarity threshold (0.9 in (Liao and Grish-
man, 2010a)) will be removed; only top 3 candidate patterns in the ranking scores of the document-
centric view will be accepted as relevant patterns. In addition, if no pattern is found in the current iter-
ation, the threshold will be reduced by 0.1 until new relevant patterns are extracted. 
As we mentioned earlier, the open and flexible structure of Chinese sentences results in the diversi-
ty of Chinese event patterns. Moreover, the syntax or semantic path is often used to represent event 
patterns, but the performance in Chinese syntactic parsers and Semantic Role Labeling (SRL) tools is 
lower than that in English. Therefore, we refine this semi-supervised model to suit Chinese event ex-
traction in three aspects as follows, due to the above characteristics of Chinese language. 
Firstly, we construct a refined event pattern representation of Chinese events. Liao and Grishman 
(2010a) used semantic roles to represent the relationship between the trigger and its arguments. Due to 
the wide spread of ellipsis (especially entities) and the relatively low performance of Chinese SRL, 
pairwise (trigger-entity) representation and dependency path are introduced to represent Chinese event 
pattern in our refined model. Hence, the event pattern in this paper is a triple-style template as follows. 
<trigger, entity type, their dependency path > 
A pattern is formed by a trigger, the entity type of its argument1 and the dependency path from the 
trigger to the argument. For example, trigger ?returned? and its argument ?peacekeepers? (entity type: 
PER) in sentence S1 can be described as a pattern <returned, PER, nsubj>. 
Secondly, we introduce a novel mechanism to extract candidate patterns. Since verb and noun dom-
inate in triggering an event in Chinese and they are chosen as candidate triggers to create candidate 
patterns. Besides, since different event types may have different roles and different roles are fulfilled 
by entities with different types, the entities whose types can fulfil the core roles of a specific event are 
chosen as candidate entities. For example, Attacker and Target are the core roles of event Attack and 
entity types PER/ORG/GPE2 can fulfil above two roles, so we only accept those entities, whose types 
belong to PER/ORG/GPE, to form candidate patterns. For each sentence in the unlabeled data, all can-
didate trigger-entity pairs and their dependency path are enumerated as candidate patterns. 
Finally, we present a new mechanism to generate seed patterns based on seed triggers. Considering 
the relatively large number of Chinese triggers and the flexibility of Chinese sentences, an instance-
based approach is adopted by enumerating a few high-quality seed triggers with explicit meaning and 
high probability to trigger a specific event. Instead of dozens of predefined patterns required in previ-
ous studies, only one seed trigger is given to each event type or subtype without any predefined pat-
terns. Hence, all patterns consisting of a seed trigger in the candidate patterns are accepted as seed pat-
terns for their high probability to trigger a specific event. 
3.2 Document-centric View 
The document-centric view regards those documents containing the patterns always identified as rele-
vant to a specific event as relevant documents and concludes that they are likely to contain additional 
                                                 
1 All event arguments must be entity mentions following the ACE 2005 annotation guidelines of events. 
2 PER/ORG/GPE refers to person, organization and geo-political entity respectively, which are annotated in the ACE 2005 
corpus. These helpful information can be seen as ontological classes. 
2163
relevant patterns. Hence, those candidate patterns occurring in the relevant documents frequently will 
be extracted as relevant ones. Following Yangarber et al. (2000) and Liao and Grishman (2010a), we 
also employ the disjunctive voting scheme to calculate the ranking scores Rscore(p) of pattern p as fol-
lows. 
 
?
?
?
?
)p(Ld
)p(Ld
Score )d(lRelog*)p(L
)d(lRe
)p(R =
                                                   (1) 
 
where L(p) is the set of documents, which contain candidate pattern p, and Rel(d) is the relevance 
score of document d as follows. 
 
?
?
?
?--
Pp
)p(Ld
'
)
)p(L
)d(lRe
1(1)d(lRe =                                                          (2) 
 
where Rel?(d) is the relevance score of document d in the previous iteration. Initially the relevance 
score of document d is set to n if document d has n relevant patterns in the set of extracted patterns P. 
3.3 Pattern Similarity-centric View 
The similarity-centric view tries to find the candidate patterns who are similar to those seed patterns. 
The similarity scores derive from two aspects, lexical similarity and syntactic similarity, while the 
former is based on the trigger and entity type in a pattern and the latter is based on the relation be-
tween the trigger and the entity. Especially, we realize the pattern similarity view following the lexical 
and syntactic similarity, and refine the similarity ranking score Iscore(p) of candidate pattern p as fol-
lows: 
 
)d,d(DSim)e,e(ESim)t,t(WSim(Max)p(I spspspPsscore ??= ?
                      (3) 
 
where t, e and d represent the trigger, entity type and dependency path in candidate pattern p(tp, ep, dp) 
or seed pattern s(ts, es, ds) in the set of extracted patterns P, respectively; ESim identifies whether two 
entities have the same type, and assigned 1 if two entities have the same entity type and otherwise a 
small number 0.1; DSim calculates the similarity between two dependency paths in edit distance. Fi-
nally, WSim is to obtain the trigger similarity in lexical semantics, using Hownet (Dong and Dong, 
2006) following Liu and Li (2002): 
 
?
?
?? ),(),( spsp ttDisttWSim
                                                                (4) 
 
where Dis(tp,ts) is the distance between the sememes of triggers tp and ts, in HowNet?s sememe hierar-
chical architecture, with parameter ? assigned 0.75 following Liu and Li (2002). 
4 Event Inference 
The pattern-based semi-supervised model cannot extract those event mentions matching infrequent 
patterns or without matching patterns. The knowledge from linguistic aspect (e.g., definition of events, 
compositional semantics of Chinese words, coreference events and relevant events, etc.) is helpful to 
further recover missing event mentions or filter pseudo event mentions. In this section, various kinds 
of event inference mechanisms based on linguistic knowledge are proposed to improve the perfor-
mance of semi-supervised Chinese event extraction. 
We unify the semi-supervised model and the event inference mechanisms into one model as follows: 
In each iteration, after the top 3 patterns have been chosen following the document-centric view and 
event mentions in the unlabeled data have been extracted by pattern matching, all event inference 
2164
mechanisms are applied to recover missing event mentions,. Due to our inference mechanisms are 
trigger-based and each inferred event mention may have more than one pattern while most of them are 
noisy, we do not add those patterns in the set of relevant patterns for bootstrapping. 
4.1 Event Inference on Role Semantics 
The core of an event can be expressed as ?Who do What to Whom? in which ?Who? and ?Whom? are 
the core roles3 to participate in an event, while ?What? often refers to event trigger. The relationship 
between the verbal trigger and its core roles are the key clues to express event semantics. Since the 
subject or object always play the core roles in an event mention, SVO (Sbject-Verb-Object) is a better 
representation of event pattern. However, ellipsis is a widespread phenomenon in Chinese language 
and many sentences do not have an overt subject or object, so lots of event mentions cannot be repre-
sented as SVO pattern. In this paper, we only use the trigger-entity pair to represent event pattern and 
one of the disadvantages of this representation is its loose constraint on events, which will extract lots 
of pseudo event mentions. 
In most cases in Chinese, the object is often the most important core role to identify a specific event 
and it is more helpful than the subject to distinguish true event mentions from pseudo ones. Take fol-
lowing two sentences as examples: 
S2: ??(PER) ?(hit)? ????(PER)?(The teacher hit this student.) 
S3: ??(PER) ?(call)? ?? ? ????(PER)?(The teacher made a phone call to this stu-
dent.) 
The relation between verb ? (hit) and object???? (this student) is clear to indicate sentence 
S2 is an Attack event mention since the object is a person, while object ?? (phone) in sentence S3 is 
not a person and it indicates this sentence is not an Attack event mention following the sense of verb 
? (call). Therefore, the object is an effective evidence to indicate event mentions and it is incorpo-
rated in our model to remove pseudo event mentions as follows. 
Role Semantics: If the object of a candidate verbal trigger mention is not an entity or its entity type 
cannot fulfil the object roles (e.g., Victim in events Injure and Die) in a specific event, this candidate 
trigger mention4 will be inferred as pseudo one. 
For example, core role Target of event Attack often acts as the object of a verbal trigger and entity 
types PER, ORG and GPE can fulfill this role according to be definition of event Attack in the ACE 
2005 corpus. Hence, a candidate trigger mention of event Attack will be regarded as pseudo one when 
this mention has an object which is not an entity or whose entity type is not PER, ORG or GPE. 
4.2 Event Inference on Compositional Semantics 
In Chinese language, a word is composed of one or more characters. Almost all Chinese characters 
have their own meanings and are morpheme (or single-morpheme word), the minimal meaningful unit. 
If a Chinese word contains more than one character, its meaning can often be derived from its compo-
site morphemes. This more fine-grained semantics is compositional semantics of Chinese words. Ac-
tually, it is also a normal way for a native Chinese speaker to understand a new Chinese word. 
Two-morpheme words are used widely in Chinese language and almost all Chinese triggers contain 
one or two morphemes. The compositional semantics of a two-morpheme word comes from both its 
morphemes and morphological structure. Besides morphological structure Coordination, all other 
morphological structures (e.g., Modifier-Head, Predicate-Object, Predicate-Complement (Li and zhou, 
2012)) always have one head morpheme, the morpheme as the governing semantic element, to express 
the meaning of a word. Commonly, there are two head morphemes in a two-morpheme word of Coor-
dination structure. In particular, a two-morpheme word triggers an event if its two head morphemes 
are homogeneous (e.g., ?(attack)?(attack), ?(die)?(die)). Otherwise, it may refer to more than one 
event and this means that two triggers are within a word whose morphological structure is Coordina-
tion. Take the following sentence as an example: 
                                                 
3 We select core roles following the ACE Chinese annotation guidelines of events. Agent/Victim are the core roles of events 
Die/Injure while Attacker/Target are the core roles of event Attack. 
4 Recognizing a trigger mention can be recast as identifying a corresponding event mention, since trigger is the main word 
which most clearly expresses the occurrence of an event. 
2165
S4: ?????(E2: Attack)?(E3: Die)?????(A younger stabbed (E2: Attack) a woman to 
death (E3: Die).) 
In S4, two-morpheme word ?? (stab a person to death) is a trigger with the Coordination struc-
ture. There are two event mentions in sentence S4, one Attack (E2) and one Die (E3), while morpheme
? (stab) triggers an Attack event and ? (die) refers to a Die one.  
Almost all event extraction systems assigned only one event type to a trigger and this will lead to 
that the other event type does not have any patterns to match and then cannot be identified. To address 
this issue, we first identify those triggers who refers to two distinct events as follows: for each two-
morpheme candidate trigger in the candidate patterns whose morphemes are m1 and m2, it will be iden-
tified as candidate trigger with two event types and split into two single-morpheme word to generate 
two candidate trigger mentions when the following three conditions are satisfied: 
1) )m(POSverb)m(POSverb 21 ???  
2) 
)s(Etype)s(Etype))s,m(Wsim())s,m(Wsim( MaxMax seedssseedss 212211 11 21 ??? ?? ??
 
3) Morph(m1 m2)= Coordination 
where POS(m) returns all possible parts of speech of morpheme m in Hownet and Etype(s) is to obtain 
the event type of seed trigger s; WSim(m,s) is defined in Subsection 3.3 and returns 1 when one word 
m is the synonym of the other word s; Morph(w) is to obtain the morphological structure of word w 
following Li and Zhou (2012). 
Since there is a strong trigger consistency in those two-morpheme words of Coordination structure 
which refers to two distinct events, we propose an event inference mechanism as follows. 
Compositional semantics: For each two-morpheme word identified by the above three conditions, 
if one of its morphemes has been extracted as an trigger mention of a specific event type, the other 
morpheme in the same word will refer to an a relevant event type. 
4.3 Event Inference on Coreference Events 
To mine more event mentions, we use the simple trigger-entity pair to represent event pattern in this 
paper. However, lots of event mentions still cannot be extracted due to the ellipsis of arguments. Take 
following sentences as examples: 
S5: ?????????????(E4: Meeting)?(The US and DPRK finished talking (E4: 
Meeting) in Kuala Lumpur.) 
S6: ??(E5: Meeting)??????(The talks (E5: Meeting) are serious.) 
Obviously, more than one pattern of event mention E4 can be generated from sentence S5, since it 
contains more than one entity. On the contrary, no pattern can be extracted from S6 and this leads to 
event mention E5 cannot be extracted in our pattern-based semi-supervised model. 
Within a document, almost all event mentions are around a topic and there is a strong trigger con-
sistency: if one mention of a word triggers a specific event, its other mentions in the same document 
will refer to the same event type. Besides, similar words (e.g., ? (bomb), ?? (bomb), ?? (bomb)), 
which contains the same head morpheme, always express the same or similar meaning following the 
principle of compositional semantics. Similarly, there is a strong trigger consistency on those similar 
words: If one mention of a word refers to a specific event, the mentions of its similar words in the 
same document will trigger events of the same type. 
Since the mentions of the same word or similar words are often coreference ones and always refer 
to the same event type, we propose an event inference mechanism on coreference events to recover 
missing event mentions based on head morpheme as follows. In particular, head morphemes are also 
identified following Li and Zhou (2012). 
Coreference events: 1) if a mention of a candidate trigger refers to a specific event, all its other 
mentions in the same document will trigger the same type event; 2) if one mention of a candidate trig-
ger refers to a specific event, all the mentions of its similar words in the same document will trigger 
the same type event too. 
4.4 Event Inference on Relevant Events 
The bootstrapping procedure of the document-centric view selects frequent patterns in relevant docu-
2166
ments and ignores those infrequent patterns both in relevant or irrelevant documents. However, the 
number of infrequent patterns in Chinese is larger than that in English, due to its open and flexible 
sentence structure, as mentioned in Subsection 3.1. 
Besides the pattern-based semi-supervised model, we propose a trigger-based mechanism as a sup-
plement to recover those missing event mentions concerning infrequent patterns following this as-
sumption: if a trigger mention refers a specific event in a document, there is a high probability that its 
relevant events occur in the same document. Take the following sentence as an example: 
S7: ???(E6: Attack)??? 1???????(E7: Die)?(An Arabian was dying (E7: Die) in 
this conflict (E6: Attack).) 
In sentence S7, there is an extracted Die event mention E7 triggered by ?? (die) and ?? (con-
flict) is a candidate trigger mention. If there is an evidence that ?? (conflict) triggers an Attack event 
in the other documents, it is possible to identify ?? (conflict) as a trigger mention of Attack event in 
S7 for the high probability that events Die and Attack occur in the same document. We propose an in-
ference mechanism on relevant events as follows. 
Relevant Events: If a trigger mention is identified in a document, each candidate trigger mention in 
the same document will be recognized as true ones when it satisfies the following condition: this can-
didate trigger occurs in the other documents as an event trigger and refers to the relevant events of this 
identified trigger mentions.  
Since the seed triggers have a high probability to trigger a specific event, to further explore those 
missing event mentions, we expand this inference mechanism following compositional semantics in 
Chinese and expand the condition as follows: This candidate trigger occurs in the other documents as 
an event trigger or contains one of the seed triggers, which refers to the relevant events of this identi-
fied trigger mentions. 
5 Experimentation 
In this section, we systematically evaluate our event inference mechanisms on the ACE 2005 Chinese 
corpus and provide the analysis. 
5.1 Experimental Setting 
The ACE 2005 Chinese corpus is the only available corpus in Chinese event extraction and it is used 
in all our experiments. This corpus contains 633 documents annotated with 33 predefined types. Due 
to evaluation on all 33 types is a hard work for the time-consuming bootstrapping procedure and the 
diversity of distinct event types, most of previous works selected part of event types for evaluation. In 
this paper, 3 event types (i.e. Die, Injure and Attack) are selected for evaluation, because they reflect 
the relevance of different event types and occur at different frequencies in the corpus. While events 
Die and Injure are easy to define, event Attack is rather complicated and can be divided into several 
subtypes. In the ACE 2005 Chinese corpus, almost one third of the annotated event mentions belong to 
the above three event types. Moreover, we report the experimental results on all 33 event types to fur-
ther verify the effectiveness of our inference mechanisms in Subsection 5.2. 
Unlike MUC shared task, which only distinguishes whether a sentence contains a specific event 
mention or not, we follow previous studies on the ACE 2005 corpus and report the performance of 
trigger-based event extraction: a trigger is correctly identified if its position and event type match a 
reference trigger. As for evaluation, we use the ground truth entities, time and values annotated in the 
ACE 2005 Chinese corpus, and report the micro-average Precision (P), Recall (R) and F1-score (F1). 
Table 1 shows the seed triggers for the three event types. For example, only one seed trigger is pro-
vided for either the Die or Injure event, while three seed triggers are given for event Attack. Since the 
Attack event contains several distinct event subtypes, we assign one seed trigger to each of its major 
subtypes. Thus, all patterns whose triggers belong to the set of seed triggers are accepted as seed pat-
terns automatically. 
 
Type Die Injure Attack 
Seed triggers ?(die) ?(injure) ??(attack), ??(conflict), ?(hit) 
Table 1. Seed triggers of Die, Injure and Attack event types 
2167
Besides, all the sentences in the corpus are divided into words using a Chinese word segmentation 
tool (ICTCLAS) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford 
Parser to create the constituent and dependency parse trees. 
5.2 Experimental Results 
To verify the performance of our event inference mechanisms, it is compared with the refined baseline, 
a supervised model for Chinese event extraction. Table 2 shows the results of our event inference 
mechanisms with peak recall, precision and F1-score, following Liao and Grishman (2010a). Com-
pared with the baseline, Table 2 shows that our event inference mechanisms improve the F1-score of 
Chinese event extraction by 8.5%, largely due to the improvement of 11.8% in recall. These results 
confirm the effectiveness of our event inference mechanisms in recovering missing event mentions. 
The disadvantage of our event inference mechanisms is the fact that it will also introduce some pseudo 
event mentions into our model and harm the precision. Additionally, there is still a big performance 
gap between our model and the supervised model and this leaves much room for future research. 
 
Approach Attack Injure Die All (micro-average) 
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 
Baseline 71.4 36.6 48.4 93.2 41.7 57.6 90.1 44.0 59.3 79.7 39.4 52.7 
+Event inference 70.9 47.5 56.9 83.2 54.6 65.9 80.8 57.2 67.0 75.5 51.2 61.2 
Supervised model 70.4 72.5 71.4 85.3 78.4 81.7 83.9 92.9 88.1 77.2 78.4 77.8 
Table 2. Performance of event inference mechanisms in Chinese event extraction (Attack/Injure/Die). 
 
Table 2 also indicates the performance difference of our inference mechanisms for distinct event 
types. Among all event types, event Attack achieves the highest improvement (8.5%) in F1-score, with 
a dramatic improvement of 10.9% in recall and a less loss of 0.5% in precision. Event Die and Injure 
also gain a significant improvement of 7.7% and 8.3% in F1-score respectively, largely due to the in-
crease in recall, while their precisions reduce rapidly due to those pseudo event mentions inferred by 
our inference mechanisms. However, the loss of precision of event Attack is much less than these of 
events Die and Injure. The reason is that the inference on role semantics mainly impacts on Attack 
events to remove pseudo event mentions. 
To well evaluate different approaches, it is better to compare them on different corpora. Since the 
ACE 2005 Chinese corpus is the only available corpus in Chinese event extraction, we divide it into 
three sub-corpora according to data sources, i.e. Broadcast News, Newswire and WebLog, which are 
much different in various aspects, such as quality, length and style. Figure 1 compares the perfor-
mance of different models on different sub-corpora. It indicates that our event inference mechanisms 
perfect better than the baseline in all three sub-corpora and that results confirm the huge influence of 
the event inference mechanisms. It also shows that the WebLog sub-corpus reports the worst F1-score 
due to the low document quality and the low percentage of relevant documents, and that the Newswire 
sub-corpus reports significantly better performance than the Broadcast News sub-corpus due to its 
spoken nature. 
 
Figure 1. Performance comparison (F1-score) on different data sources. 
To further verify the effectiveness of our event inference mechanisms, we evaluate them on all 33 
event types. Due to event extraction is a domain-specific task, distinct event types have the different 
seed triggers and different pro-process procedures. In this paper, we just report the final results for the 
50.9 57.8 
36.9 
59.6 65.7 
44.8 
0
50
100
Broadcast news Newwise WebLog
Baseline Baseline+Event Inference
2168
sake of brevity. Table 3 shows the experimental results on all 33 event types and it ensures that our 
mechanisms are effective on extracting all event types. Compared with the baseline, our approach im-
proves the F1-score by 7.6%, which is less than that reported in Table 2. Among all 33 event types, the 
performances of almost all event types associated with justice are higher than other event types for 
their unambiguous definitions and high coverage of seed triggers while event Transport achieves the 
lowest performance for its complexity and low coverage of seed triggers. Besides, the performance on 
all event types is lower than that on 3 event types and this result comes from the low performance of 
the Transport event which occupies almost 20% of all annotated event mentions in the ACE 2005 
Chinese corpus. 
Approach P(%) R(%) F1 
Baseline 70.7 34.2 46.1 
+Event inference 65.2 45.7 53.7 
Table 3. Performance of event inference mechanisms in Chinese event extraction (All 33 event types). 
5.3 Analysis on Event Inference Mechanisms 
Table 4 shows the contributions of the different event inference mechanisms. It is worthy to mention 
that an event mentions may be identified by both the semi-supervised model and the event inference 
mechanisms. In this paper, we attribute those extracted event mentions to the former and the contribu-
tion of our inference mechanisms is greater than those in Table 4. 
 
Inference P(%) R(%) F1 
Baseline 79.7 39.4 52.7 
+Inference on role semantics (RS) 87.5(+7.8) 39.1(-0.3) 54.1(+1.4) 
+Inference on compositional semantic (CS) 85.7(+6.0) 43.7(+4.3) 57.8(+3.7) 
+Inference on coreference events (CE) 83.0(+3.3) 45.8(+6.4) 59.0(+1.2) 
+Inference on relevant events (RE) 75.7(-4.0) 51.3(+11.9) 61.2(+2.2) 
Table 4. The contribution of event inference on Chinese event extraction. 
 
Actually, inference mechanism RS is a filter to remove those pseudo event mentions and it can im-
prove the precision (+7.8%), with a less lost (-0.3%) in recall. Moreover, it can also help the seed pat-
tern generation to generate high quality seed patterns. Table 5 shows the contribution of RS on seed 
pattern generation and we report the result of Chinese event extraction which only uses the seed pat-
terns5. It improves the accuracy from 75.8% to 82.5%, largely due to the decline (-30) in the set of 
pseudo event mentions. These results indicate that the object is a key clue to identify event mentions. 
 
Method #True event mentions #Pseudo event mentions 
w/o RS 273 87 
w/ RS 269 57 
Table 5. The contribution of RS on seed pattern generation. 
 
Chen and Ji (2009b) have reported that almost 13% of Chinese triggers are in-word or cross-words 
and this figure ensures it is an important issue. Inference mechanism CS gains the highest improve-
ment (+3.7%) in F1-score and this result indicates that compositional semantics is an effective way to 
solve such issue. The accuracy of this inference mechanism is very high (~92%) and most of the ex-
ceptions need the help of deep semantics since these instances are also hard to be distinguished by 
humans without the context. 
Inference mechanisms CE and RE improve the F1-scores by 1.2% and 2.2% respectively. CE as-
sumes all mentions of a word in a document only have one sense and it will introduce lots of pseudo 
event mentions to reduce precision. The experimental results also show that RE is an effective sup-
plement of the document-centric view to mine event mentions. Although they derive from the similar 
                                                 
5 Since sometimes a pattern can infer both true event mentions and pseudo event mentions, it is hard to identify whether a 
pattern is relevant or irrelevant without the test data. Hence, we compare their extracted event mentions in this paper. 
2169
principle of occurrence of relevant events, they focus on different perspectives where RE is trigger-
based and the document-centric view is pattern-based. RE ignores the difference on patterns and iden-
tifies event mentions on the occurrence of their relevant event mentions. In addition, sense shifting of 
Chinese words in different contexts is the main factor to extract lots of pseudo event mentions and 
then reduce the precision rapidly. 
It?s obvious that these inference mechanisms interact with others. In particular, almost 20% event 
mentions can be inferred by both CE and RE for the transitivity of event inference on coreference and 
relevant events. Besides, RS is not only beneficial to the semi-supervised model, but also helpful to 
the other inference mechanisms to further remove pseudo event mentions. 
6 Conclusion 
This paper proposes various kinds of novel linguistic knowledge-driven event inference mechanisms 
as a supplement of the semi-supervised Chinese event extraction to recover missing event mentions. 
The experimental results verify their effectiveness to extract the event mentions with infrequent pat-
terns or without matching pattern. Although this paper focuses on Chinese language, most of the event 
inference mechanisms are language-independent and can be applied to other languages. Our future 
work will focus on how to apply our event inference mechanisms to other languages and introduce 
more effective inference mechanisms to further improve the performance of semi-supervised event 
extraction. 
Acknowledgments 
The authors would like to thank three anonymous reviewers for their comments on this paper. This 
research was supported by the National Natural Science Foundation of China under Grant No. 
61331011 and No. 61272260, the National 863 Project of China under Grant No. 2012AA011102. 
Reference 
Niranjan Balasubramanian, Stephen Soderland, Mausam and Oren Etzioni. 2013. Generating Coherent Event 
Schemas at Scale. In Proc. EMNLP 2013, pages 1721-1731, Seatle, WA. 
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In Proc. ACL-
HLT 2008, pages 787-797, Hawaii. 
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised Learning of Narrative Schemas and Their Partici-
pants. In Proc. ACL 2009, pages 602-610, Columbus, OH. 
Nathanael Chambers and Dan Jurafsky. 2011. Template-Based Information Extraction without the Templates. In 
Proc. ACL 2011, pages 976-986, Portland, OR. 
Hai Leong Chieu, Hwee Tou Ng and Yoong Keok Lee. 2003. Closing the Gap: Learning-based Information 
Extraction Rivaling Knowledge-Engineering Methods. In Proc. ACL 2003, pages 216-230, Sapporo, Japan. 
Zheng Chen and Heng Ji. 2009a. Can One Language Bootstrap the Other: A Case Study on Event Extraction. In 
Proc. NAACL-HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing, pages 
66-74, Boulder, CO. 
Zheng Chen and Heng Ji. 2009b. Language Specific Issue and Feature Exploration in Chinese Event Extraction. 
In Proc. NAACL-HLT 2009, pages 209-212, Boulder, CO. 
Zhengdong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific Pub Co. 
Inc. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou and Qiaoming Zhu. 2011. Using Cross-Entity 
Inference to Improve Event Extraction. In Proc. ACL 2011, pages 1127-1136, Portland, OR. 
Ruihong Huang and Ellen Riloff. 2012. Bootstrpped Training of Event Extraction Classifiers. In Proc. EACL 
2012, pages 286-295, Avignon, France. 
Heng Ji and Ralph Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proc. 
ACL-HLT 2008, pages 254-262, Columbus, OH. 
2170
Peifeng Li and Guodong Zhou. 2012. Employing Morphological Structures and Sememes for Chinese Event Ex-
traction. In Proc. COLING 2012, pages 1619-1634, Mumbai, India. 
Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013. Argument Inference from Relevant Event Mentions in 
Chinese Argument Extraction. In Proc. ACL 2013, pages 1477-1487, Sofia, Bugaria. 
Peifeng Li, Qiaoming Zhu, Guodong Zhou. 2014. Using Compositional Semantics and Discourse Consistency to 
Improve Chinese Trigger Identification. Information Processing and Management, 50: 399?415.  
Shasha Liao and Ralph Grishman. 2010a. Filtered Ranking for Bootstrapping in Event Extraction. In Proc. COL-
ING 2010, pages 680-688, Beijing, China. 
Shasha Liao and Ralph Grishman. 2010b. Using Document Level Cross-Event Inference to Improve Event Ex-
traction. In Proc. ACL 2010, pages 789-797, Uppsala, Sweden. 
Shasha Liao and Ralph Grishman. 2011. Can Document Selection Help Semi-supervised Learning? A Case 
Study On Event Extraction. In Proc. ACL 2011, pages 260-265, Portland, OR. 
Qun Liu and Sujian Li. 2002. Word Similarity Computing Based on How-net. In Proc. 3th Chinese Lexical Se-
mantic Workshop, Taibei, Taiwan. 
Ting Liu and Tomek Strzalkowski. 2012. Bootstrapping Events and Relations from Text. In Proc. EACL 2012, 
pages 296-305, Avignon, France. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A Multi-resolution Framework for Information Extraction from 
Free Text. In Proc. ACL 2007, pages 592-599, Prague, Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. EMNLP 2009, pages 151-160, Singapore. 
William Phillips and Ellen Riloff. 2007. Exploiting Role-Identifying Nouns and Expressions for Information Ex-
traction. In Proc. RANLP 2007, pages 468-473, Borovets, Bulgaria. 
Ellen Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proc. AAAI 1996, 
pages 1044-1049, Portland, OR. 
Mark Stevenson and Mark Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proc. ACL 2005, 
pages 379-386, Ann Arbor, MI. 
Kiyoshi Sudo, Satoshi Sekine, Ralph Grishman. 2001. Automatic Pattern Acquisition for Japanese Information 
Extraction. In Proc. HLT 2001, pages 1-7, San Diego, CA.  
Kiyoshi Sudo, Satoshi Sekine, Ralph Grishman. 2003. An Improved Extraction Pattern Representation Model 
for Automatic IE Pattern Acquisition. In Proc. ACL 2003, pages 224-231, Tokyo, Japan. 
Roman Yangarber, Ralph Grishman, Pasi Tapanainen and Silja Huttunen. 2000. Automatic Acquisition of Do-
main Knowledge for Information Extraction. In Proc. COLING 2000, pages 940-946, Hong Kong. 
Roman Yangarber. 2003. Counter-Training in Discovery of Semantic Patterns. In Proc. ACL 2003, pages 343-
350, Sapporo, Japan. 
Jian Wang, Qian Xu, Hongfei Lin, Zhihao Yang, Yanpeng Li. 2013. Semi-supervised Method for Biomedical 
Event Extraction. Proteome Science, 11(Suppl 1): S17. 
2171
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1006?1016, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Employing Compositional Semantics and Discourse Consistency in 
Chinese Event Extraction 
Peifeng Li, Guodong Zhou, Qiaoming Zhu, Libin Hou 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, gdzhou, qmzhu, 20094227021}@suda.edu.cn 
 
Abstract 
Current Chinese event extraction systems suffer 
much from two problems in trigger 
identification: unknown triggers and word 
segmentation errors to known triggers. To 
resolve these problems, this paper proposes two 
novel inference mechanisms to explore special 
characteristics in Chinese via compositional 
semantics inside Chinese triggers and discourse 
consistency between Chinese trigger mentions. 
Evaluation on the ACE 2005 Chinese corpus 
justifies the effectiveness of our approach over 
a strong baseline. 
1 Introduction 
Event extraction, a classic information extraction 
task, is to identify instances of a predefined event 
type and can be typically divided into four subtasks: 
trigger identification, trigger type determination, 
argument identification and argument role 
determination. In the literature, most studies focus 
on English event extraction and have achieved 
certain success (e.g. Grishman et al 2005; Ahn, 
2006; Hardy et al 2006; Maslennikov and Chua, 
2007; Finkel et al 2005; Ji and Grishman, 2008; 
Patwardhan and Riloff, 2009, 2011; Liao and 
Grishman 2010; Hong et al 2011).  
In comparison, there are few successful stories 
regarding Chinese event extraction due to special 
characteristics in Chinese trigger identification. In 
particular, there are two major reasons for the low 
performance: unknown triggers 1  and word 
segmentation errors to known triggers. Table 1 
gives the statistics of unknown triggers and word 
segmentation errors to known triggers in both the 
                                                          
1 In this paper, a trigger word/phrase occurring in the training 
data is called a known trigger and otherwise, an unknown 
trigger.  
ACE 2005 Chinese and English corpora2 using 10-
fold cross-validation. In each validation, we leave 
10% trigger mentions as the test set and the 
remaining ones as the training set. If a mention in 
the test set doesn?t occurred in the training set, we 
regard it as an unknown trigger. It shows that these 
two cases cover almost 30% of Chinese trigger 
mentions while this figure reduces to only about 
9% in English. It also shows that given the same 
number of event mentions, there are 30% more 
different triggers in Chinese than that in English. 
This justifies the low performance (specifically, 
the recall) of a Chinese event extraction system, 
which normally extracts those known triggers 
occurring in the training data as candidate 
instances and uses a classifier to distinguish correct 
triggers from wrong ones. 
 
Language Chinese English 
%unknown triggers 33.7% 18.5% 
%unknown trigger mentions 20.9% 8.9% 
%word segmentation errors 
to known trigger mentions 
8.7% 0% 
#triggers 763 586 
Table 1. Statistics: a comparison between Chinese and 
English event extraction with regard to unknown 
triggers and word segmentation errors to known triggers. 
Note that word segmentation only applies to Chinese. 
 
In this paper, we propose two novel inference 
mechanisms to Chinese trigger identification by 
employing compositional semantics inside Chinese 
triggers and discourse consistency between 
Chinese trigger mentions.  
The first mechanism is motivated by the 
compositional nature of Chinese words, whose 
semantics can be often determined by the 
component characters. Hence, it is natural to infer 
                                                          
2  The whole Chinese ACE corpus has about 3300 event 
mentions. For the sake of fair comparison, we choose the same 
number of event mentions from the English corpus as the 
cross-validation data. 
1006
unknown triggers by employing compositional 
semantics inside Chinese triggers.  
The second mechanism is enlightened by the 
wide use of discourse consistency in natural 
languages, particularly for Chinese, due to its 
discourse-driven nature (Zhu, 1980). Very often, 
distinguishing true trigger mentions from pseudo 
ones is only possible with contextual information.  
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
introduces a state-of-the-art baseline system for 
Chinese event extraction. Sections 4 and 5 describe 
two novel inference mechanisms to Chinese trigger 
identification by employing compositional 
semantics inside Chinese triggers and discourse 
consistency between Chinese trigger mentions. 
Section 6 presents the experimental results. Section 
7 concludes the paper and points out future work. 
2 Related Work 
Almost all the existing studies on event extraction 
concern English. While earlier studies focus on 
sentence-level extraction (Grishman et al 2005; 
Ahn, 2006; Hardy et al 2006), later ones turn to 
employ high-level information, such as document 
(Maslennikov and Chua, 2007; Finkel et al 2005; 
Patwardhan and Riloff, 2009), cross-document (Ji 
and Grishman, 2008), cross-event (Liao and 
Grishman, 2010; Gupta and Ji, 2009) and cross-
entity (Hong et al 2011) information. 
2.1 Chinese Event Extraction  
Compared with tremendous efforts in English 
event extraction, there are only a few studies on 
Chinese event extraction.  
Tan et al(2008) modeled event extraction as a 
pipeline of classification tasks. Specially, they used 
a local feature selection approach to ensure the 
performance of trigger classification (trigger 
identification + trigger type determination) and 
applied multiple levels of patterns to improve the 
coverage of patterns in argument classification 
(argument identification + argument role 
determination). Chen and Ji (2009a) proposed a 
bootstrapping framework, which exploited extra 
information captured by an English event 
extraction system. Chen and Ji (2009b) applied 
various kinds of lexical, syntactic and semantic 
features to address the specific issues in Chinese. 
They also constructed a global errata table to 
record the inconsistency in the training set and 
used it to correct the inconsistency in the test set. Ji 
(2009) extracted cross-lingual predicate clusters 
using bilingual parallel corpora and a cross-lingual 
information extraction system, and then used the 
derived clusters to improve the performance of 
Chinese event extraction. 
2.2 Compositional Semantics 
Almost all the related studies on compositional 
semantics focus on how to combine words together 
to convey complex meanings, such as semantic 
parser (Zettlemoyer and Collins, 2007; Wong and 
Mooney, 2007; Liang et al 2011). However, the 
compositional semantics mentioned in this paper is 
more fined-grained and focuses on how to 
construct Chinese characters into a word and mine 
the semantics of words from the word structures, 
especially of verbs as event triggers.  
To our knowledge, there is only one paper 
associated with compositional semantics inside 
Chinese words. Li (2011) discussed the internal 
structures inside Chinese nouns and used it in word 
segmentation.  
2.3 Discourse Consistency 
Discourse consistency is an important hypothesis 
in natural languages and has been applied to many 
natural language processing applications, such as 
named entity recognition and coreference 
resolution. Specially, several studies have 
successfully incorporated trigger or entity 
consistency constraint into event extraction.  
Yarowsky (1995) and Yangarber et al
(Yangarber and Jokipii, 2005; Yangarber et al 
2007) applied cross-document inference to refine 
local extraction results for disease name, location 
and start/end time. Mann (2007) proposed some 
specific inference rules to improve extraction of 
personal information. Ji and Grishman (2008) 
employed a rule-based approach to propagate 
consistent triggers and arguments across topic-
related documents. Gupta and Ji (2009) used a 
similar approach to recover implicit time 
information for events. Liao and Grishman (2011) 
also used a similar approach and a self-training 
strategy to extract events. Liao and Grishman 
(2010) employed cross-event consistency 
information to improve sentence-level event 
extraction. Hong et al(2011) regarded entity type 
1007
consistency as a key feature to predict event 
mentions and adopted this inference method to 
improve the traditional event extraction system.  
3 Baseline 
As a baseline, we re-implement a state-of-the-art 
system, which consists of four typical components 
(trigger identification, trigger type determination, 
argument identification and argument role 
determination), in a pipeline way and employ the 
same set of features as described in Chen and Ji 
(2009b). 
Besides, the Maximum-Entropy (ME) model is 
employed to train individual component classifiers 
for the above four components. During testing, 
each word in the test set is first scanned for 
instances of known triggers from the training set. 
When an instance is found, the trigger identifier is 
applied to distinguish true trigger mentions from 
pseudo ones. If true, the trigger type determiner is 
then applied to recognize its event type. For any 
entity mentions in the sentence, the argument 
identifier is employed to assign possible arguments 
to them afterwards. Finally, the argument role 
determiner is introduced to assign a role to each 
argument.  
One problem with Chen and Ji?s system is its 
ignoring effective long-distance features. In order 
to resolve this problem and provide a stronger 
baseline, we introduce more refined and 
dependency features in four components:  
? Trigger Identification and Trigger Type 
Determination: 1) syntactic features: path to 
the root of the governing clause, 2) nearest 
entity information: entity type of left 
syntactically/physically nearest entity to the 
trigger + entity, entity type of right 
syntactically/physically nearest entity to the 
trigger mention in the sentence + entity; 3) 
dependency features: the subject and the object 
of the trigger when they are entities. 
? Argument Identification and Argument Role 
Determination: 1) basic features: POS of 
trigger; 2) neighboring words: left neighboring 
word of the entity + its POS, right neighbor 
word of the entity + its POS, left neighbor word 
of the trigger + its POS, right neighbor word of 
the trigger + its POS; 3) dependency feature: 
dependency path from the entity to the trigger; 
4) semantic role features: Arg0 and Arg1 which 
tagged by semantic role labeling tool (Li, et al 
2010). 
3.1 Experimental Setting 
The ACE 2005 Chinese corpus (only the training 
data is available) is used in all our experiments. 
The corpus contains 633 Chinese documents 
annotated with 8 predefined event types and 33 
predefined subtypes. Similar to previous studies, 
we treat these subtypes simply as 33 separate event 
types and do not consider the hierarchical structure 
among them. 
Following Chen and Ji (2009b), we randomly 
select 567 documents as the training set and the 
remaining 66 documents as the test set. Besides, 
we reserve 33 documents in the training set as the 
development set, and follow the setting of ACE 
diagnostic tasks and use the ground truth entities, 
times and values for our training and testing. 
For evaluation, we follow the standards as 
defined in Ji (2009):  
? A trigger is correctly identified if its position in 
the document matches a reference trigger; 
? A trigger type is correctly determined if its 
event type and position in the document match 
a reference trigger; 
? An argument is correctly identified if its 
involved event type and position in the 
document match any of the reference argument 
mentions; 
? An argument role is correctly determined if its 
involved event type, position in the document, 
and role match any of the reference argument 
mentions. 
Finally, all sentences in the corpus are divided 
into words using a word segmentation tool 
ICTCLAS3 with all entities annotated in the corpus 
kept. Besides, we use Stanford Parser (Levy and 
Manning, 2003, Chang, et al 2009) to create the 
constituent and dependency parse trees and employ 
the ME model to train individual component 
classifiers. 
3.2 Experimental Results 
Table 2 and 3 show the Precision (P), Recall (R) 
and F1-Measure (F) on the held-out test set. It 
shows that our baseline system outperforms Chen 
and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1-
measure on trigger identification, trigger type 
                                                          
3 http://ictclas.org/ 
1008
determination, argument identification and 
argument role determination, respectively, with 
both gains in precision and recall. This is simply 
due to contribution of the newly-added refined and 
dependency features. 
 
Performance 
 
System 
Trigger 
Identification 
Trigger Type 
Determination 
P(%) R(%) F P(%) R(%) F 
Chen and Ji 
(2009b) 
71.5 51.2 59.7 66.5 47.7 55.6 
Our Baseline 75.2 52.0 61.5 70.3 49.0 57.8 
Table 2. Performance of trigger identification and 
trigger type determination  
 
Performance 
 
System 
Argument 
Identification 
Argument Role 
Determination 
P(%) R(%) F P(%) R(%) F 
Chen and Ji 
(2009b) 
56.1 38.2 45.4 53.1 36.2 43.1 
Our Baseline 58.4 42.7 49.3 55.2 38.6 45.4 
Table 3. Performance of argument identification and 
argument role determination 
 
For our baseline system, given the small 
performance gaps between trigger identification 
and trigger type determination (3.7 in F1-measure: 
61.5 vs. 57.8) and between argument identification 
and argument role determination (3.9 in F1-
measure: 49.3 vs. 45.4), the performance 
bottlenecks of our baseline system mainly exist in 
trigger identification and argument identification, 
particularly for the former one. While argument 
identification has the performance gap of 8.5 in 
F1-measure compared to trigger type 
determination (49.3 vs. 57.8), the former one, 
trigger identification, can only achieve the 
performance of 61.5 in F1-measure (in particular 
the recall with only 52.0). In this paper, we will 
focus on trigger identification to improve its 
performance, particularly for the recall, via 
compositional semantics inside Chinese triggers 
and discourse consistency between Chinese trigger 
mentions.  
4 Employing Compositional Semantics 
inside Chinese Triggers  
Language is perhaps the only communicative 
system in nature, which compositionally builds 
structured meanings from smaller pieces, and this 
compositionality is the cognitive mechanism that 
allows for what Humboldt called language?s 
?infinite use of finite means.? As usual, the lexical 
semantics is the smallest piece in most Chinese 
language processing applications. In this section, 
we introduce a more fine-grained semantics - the 
compositional semantics in Chinese verb structure 
- and unveil its effect and usage in Chinese 
language processing by employing it into Chinese 
event extraction. 
4.1 Compositional Semantics inside Chinese 
Triggers 
In English, a component character is just the basic 
unit to form a word instead of a semantics unit. In 
comparison, almost all Chinese characters have 
their own meanings and can be formed as SCWs 
(Single Character Words) themselves. If a Chinese 
word contains more than one character, its 
meaning can be often inferred from the meanings 
of its component characters (Yuan, 1998). Actually, 
it is the normal way of understanding a new 
Chinese word in everyday life of a Chinese native 
speaker. A general method to this problem is to 
systematically explore the morphological 
structures in Chinese words. In this paper, 
compositional semantics provides a simple but 
effective compromise to the general method and 
we leave the general method in the future work. 
Table 4 shows samples of such compositional 
semantics in Chinese words. For example, ???? 
is composed of two characters: ??? and ??? 
which have their own semantics and the semantics 
of ??? ? comes from that of its component 
characters ??? and ???.  
 
Words Characters 
?? (interview4) ? (meet) ?(meet) 
?? (shoot and kill) ?(shoot) ? (kill) 
??(come)  
?? (private letter) 
? (come) ? (to) 
?(private) ?(letter) 
Table 4. Examples of compositional semantics in 
Chinese words 
 
Therefore, it is natural to infer unknown triggers 
by employing compositional semantics inside 
Chinese triggers. Take following two sentences as 
examples: 
(1) 4?????????(Known trigger) 
                                                          
4  Most Chinese words have more than one sense. Here, we 
just give the one when it acts as a trigger. 
1009
(Four students were scratched by the glass.) 
(2)  1???????(Unknown trigger) 
(A passenger was stabbed.) 
where ???? is a known trigger and ???? is an 
unknown one.  
In above examples, the semantics of ???? 
(injure by scratching) can be largely determined 
from those of its component characters ?? ? 
(scratch) and ??? (injure) while the semantics of 
??? ? (injure by stabbing) from those of its 
component characters ??? (stab) and ??? (injure). 
Since these two triggers have similar internal 
structures, we can easily infer that ???? is a 
trigger of injure event if ???? is known as a 
trigger of injure event. Similarly, we can infer 
more triggers for injure event, such as ???? 
(injure by burning), ???? (injure by hitting), ??
? ? (injure by pressing), all with component 
character ??? (injure) as the head and the other 
component character as the way of causing injury.  
Since most triggers in Chinese event extraction 
are verbs 5 , we focus on the compositional 
semantics in the verb structure. Statistics on the 
training set shows that 3.3% triggers (e.g. ???
?? (open letter), ???? (event), ???? (patient's 
condition), etc.) don?t contain a BV and all of them 
are nouns. Normally, almost all verbs contain one 
or more single-character verbs as the basic element 
to construct a verb (we call it basic verb, shorted as 
BV) and the semantics of such a verb thus can be 
inferred from its BV. There are some studies on the 
Chinese verb structure in linguistics. However, 
their structures are much more complex and there 
are no annotated corpora available. We define 
following six main structures from our empirical 
observations: 
(1) BV (e.g. ??? (see), ??? (kill)) 
(2) BV + verb (e.g. ???? (meet)) 
(3) verb + BV (e.g. ???? (fire) ) 
(4) BV + complementation (e.g. ???? (kill) ) 
(5) BV + noun/adj. (e.g. ???? (go to home)) 
(6) noun/adj. +BV (e.g. ???? (shoot using 
gun)). 
                                                          
5 Actually, in the ACE 2005 Chinese (training) corpus, more 
than 90% of triggers are either verbs al or verbal nouns (those 
verbs which act as nouns). For simplicity, we don?t 
differentiate these two types in this paper. 
From above structures, a BV plays an important 
role in the verb structure and most of semantics of 
a verb can be interred from its contained BV and 
two words normally have very similar semantics if 
they have the same BV (e.g. ???? (meet) and 
???? (meet)). Actually, sometime the verb can 
be shortened to its contained BV (e.g. ?????
? ? and ??????? ? have the same 
semantics.).  
4.2 Inferring via Compositional Semantics 
inside Chinese Triggers 
Here a simple rule is employed to infer triggers via 
compositional semantics inside Chinese triggers: a 
verb is a trigger if it contains a BV which occurs 
as a known trigger or is contained in a known 
trigger. Table 5 shows the distribution of the set of 
triggers (contains the same BV 6 ) classified by 
number of triggers.  
From Table 5, we can find out that 85.3% of 
BVs occur in more than one trigger and 56.2% of 
them in more than 4 triggers. As for trigger 
mentions, these percentages become 89.1% and 
65.2% respectively. A extreme example is that 
85.2% (75/88) of triggers of Trial-Hearing event 
mentions contain ??? (trial) and 85.4% (117/138) 
of triggers of injure event mentions contains ??? 
(injure).  
 
Number  Distribution over 
Triggers 
Distribution over 
Trigger Mentions 
1 14.7% 10.9% 
2~4 29.1% 23.9% 
5~9 28.1% 32.9% 
>=10 28.1% 32.3% 
Table 5. Distribution of BVs in the number of 
triggers/trigger mentions  
 
In this paper, the inference is done as follows: 
? Add all single-character triggers into the BV set 
if it?s a verb; 
? Split all other triggers in the training set into a 
set of single characters and include all single 
characters into the BV set if it?s a verb; 
? For each word in the test set, it is identified as a 
trigger if it contains a BV. 
It is worthwhile to note that such inference 
works for unknown triggers and word 
                                                          
6 We didn?t tag BVs in the training set and regards all single-
character verbs contained in triggers as BVs. 
1010
segmentation errors to known triggers since in both 
cases, their BVs will always exist as either a SCW 
or a component of a word. 
4.3 Noise Filtering  
One problem with above inference is that while it 
is able to recover some true triggers and increase 
the recall, it may introduce many pseudo ones and 
harm the precision. To filter out those pseudo 
triggers, we propose following rules according to 
our intuition and statistics over the training set. 
Non-trigger Filtering 
A Chinese word will not be a trigger if it 
appears in the training set but never trigger an 
event. Statistics on the training set shows that this 
rule applies at 99.7% of cases. 
POS filtering 
A Chinese word will not be a trigger if it has a 
different POS from that of the same known 
trigger or similar known triggers 7  in the 
training set. In Chinese, a single-character verb 
has very high probability of composing words (e.g. 
??? (come), ??? (act as), ??? (combine), etc) 
with different POS from the single-character verb 
itself, such as preposition (e.g. ??? ? (for)), 
conjunction (e.g. ???? (and)), etc. Statistics on 
the training set shows that this rule applies at 
97.3% of cases.  
Verb structure filtering 
A Chinese word will not be a trigger if its verb 
structure is different from that of the same 
known trigger or similar known triggers in the 
training set. Figure 1 shows different distributions 
of three BVs over six verb structures as described 
in subsection 4.1. For example, we can find that all 
triggers including ??? (unbind) (e.g. ???? (fire), 
???? (fire), ???? (disband)) just have one verb 
structure (BV + verb) and those of ??? (kill) have 
4 structures. Obviously, we can use such 
distribution information to filter out pseudo 
triggers. For example, although both word ???? 
(console) and ???? (decompose) are constructed 
form verb ???, their verb structure (verb + BV) 
does not appear in the training set. Therefore, they 
will be filtered our via verb structure filtering. 
                                                          
7 Similar triggers are those ones which have the same BV and 
verb structure. 
Statistics on the training set shows that this rule 
applies at 95.5% of cases. 
 
0
0.2
0.4
0.6
0.8
1
BV verb+BV
BV+Verb
N/Adj+BV
BV+Comp
BV+N/Adj
?
?
?
Figure 1. Distribution of three BVs (??? (unbind), ??? 
(trial) and ??? (kill)) over six verb structures in 
constructing triggers 
5 Employing Discourse Consistency 
between Chinese Trigger Mentions  
Chinese event extraction may suffer much from the 
errors propagated from upstream processing such 
as part-of-speech tagging and parsing, especially 
word segmentation. To alleviate word 
segmentation errors to known triggers, Chen and Ji 
(2009b) constructed a global errata table to record 
the inconsistency in the training set and proved its 
effectiveness. In this paper, a merge and split 
method is applied to recover those known triggers. 
In this way, word segmentation errors can be 
alleviated to certain extent.  
For unknown triggers, we can merge two or 
more neighboring short words or single characters 
as a trigger candidate. In this paper, for each 
single-character verb in a document after word 
segmentation, this single-character verb can be 
merged with either previous SCW or next SCW to 
form a trigger candidate if this single-character 
verb has occurred in the training set with the same 
verb structure. 
Given above recovered triggers for both known 
and unknown triggers, the key issue here is how to 
distinguish true triggers from pseudo ones. In this 
paper, we employ discourse consistency between 
Chinese trigger mentions for Chinese event 
extraction. Previous studies on English event 
extraction have proved the effectiveness of both 
cross-entity and cross-document consistency.  
5.1 Discourse Consistency between Chinese 
Trigger Mentions  
As a discourse-driven language, the syntax of 
1011
Chinese is not as strict as English and sometime 
we must infer from the discourse-level information 
to understand the meaning of a sentence. Kim 
(2000) compared the use of overt subjects in 
English and Chinese and he found that overt 
subjects occupy over 96% in English, while this 
percentage drops to only 64% in Chinese. 
Similarly, argument missing is another issue in 
Chinese event extraction and almost 55% of 
arguments are missing in the ACE 2005 Chinese 
corpus. Normally, using a feature-based approach 
to distinguish true triggers from pseudo ones is 
very difficult from the sentence level if some of 
related arguments are missing from the trigger-
occurring sentence. Take following two contingent 
sentences as examples: 
(3) ????? 3????????????  
(The United States and the Democratic 
People's Republic of Korea finished missile 
talks in Kuala Lumpur.) 
(4) ???????? 
(The talks are serious.) 
While it is relatively easy to determine that 
mention ???? in sentence (3) indicates a meet 
event from the contained information in itself 
(there are many entities, such as agents, time and 
place in the sentence) and difficult to determine 
that mention ???? in sentence (4) is a meet event 
from the contained information in itself, we can 
easily infer from sentence (3) that sentence (4) also 
indicates a meet event, using discourse consistency: 
if one instance of a word is a trigger mention, other 
instances in the same discourse will be a trigger 
mention with high probability.  
 
Language Discourse-based Instance-based 
English 70.2% 87.5% 
Chinese 90.5% 95.4% 
Table 6. Comparison of discourse consistency between 
Chinese and English trigger mentions 
 
Table 6 compares the probabilities of discourse 
consistency between Chinese and English trigger 
mentions in the ACE 2005 Chinese and English 
corpora. A trigger may appear many times in a 
discourse. It?s considered discourse-consistent 
when all the appearances of a trigger have the 
same event type while instance-based consistency 
refers to pair-wired cases. It shows that within the 
discourse, there is a strong consistency in both 
Chinese and English between trigger mentions: if 
one instance of a word is a trigger, other instances 
in the same discourse will be a trigger of the same 
event type with very high probability. 
0.85
0.9
0.95
1
?
?
? ?
?
? ?
?
?
?
?
?
?
?
?
?
?
 
Figure 2. Probabilities of discourse-level consistency of 
top 10 frequent triggers 
It also shows that discourse consistency in 
Chinese triggers holds much more likely than the 
English counterpart. Figure 2 give the probabilities 
of discourse-level consistency of top 10 frequent 
triggers, which occupy 18% of event mentions in 
the ACE 2005 Chinese corpus. 
5.2 Inference via Discourse Consistency 
between Chinese Trigger Mentions  
Given a discourse and different mentions of a 
trigger returned by the trigger identifier, we can 
simply accept those mentions with high probability 
as true mentions of the trigger and discard those 
with low probability8. However, for those mentions 
in-between, an additional discourse-level trigger 
identifier is further employed to determine whether 
a trigger mention is true or not from the discourse 
level by augmenting the normal trigger identifier 
with several features to explore the consistency 
information between trigger mentions in the 
discourse (first three features) and the related 
information returned from the trigger type 
identifier (last two features).  
? Probability of the discourse consistency of the 
candidate trigger mention in the training set. If 
it doesn?t exist in the training set, we infer its 
probability from that of all of its similar triggers 
? Number of candidate trigger mentions being a 
trigger in the same discourse via trigger 
identification 
? Number of candidate trigger mentions being a 
non-trigger in the same discourse via trigger 
identification 
                                                          
8 The high and low probability thresholds are fine-tuned to 
95% and 5% respectively, using the development set. 
1012
? Event type of candidate trigger mention via 
trigger type determination 
? Confidence of trigger type determination 
6 Experiments 
In this section, we evaluate our two inference 
mechanisms in Chinese trigger identification and 
its application to overall Chinese event extraction, 
using the same experimental settings as described 
in Subsection 3.1. 
6.1 Chinese Trigger Identification 
Table 7 shows the impact of compositional 
semantics in trigger identification. Here, the 
baseline just extracts those triggers occurring in the 
training data. It justifies the effectiveness of our 
compositional semantics-based inference 
mechanism in recovering true triggers and its three 
filtering rules in removing pseudo triggers.  
 
                    Numbers 
Approaches 
Triggers Non-triggers 
Baseline 266 629 
+Compositional semantics 
without filtering 
334 1885 
+ Non-trigger filtering 328 1062 
+ POS filtering 325 974 
+ Verb structure filtering 302 444 
Gold 367 - 
Table 7. Impact of compositional semantics in trigger 
identification 
 
To reduce those pseudo triggers after above 
inference process, three rules are introduced.  
The first rule, the non-trigger filtering rule, 
filters out those pseudo ones in the test set which 
do not frequently occur as trigger mentions in the 
training set. In particular, to keep true triggers in 
our candidate set as many as possible, we just filter 
out those candidates which occur as non-triggers 
more than 5 times in the training set according to 
our validation on the development set. Table 7 
shows that 43.7% (823) of pseudo triggers are 
filtered out while only 1.8% (6) of true ones is 
wrongly filtered out.  
The second rule, the POS filtering rule, just 
filters out 8.3% (88) of pseudo triggers, due to 
POS errors in word segmentation and constituent 
parsing (e.g. 9.4% of candidate triggers have 
wrong POS tags in the development set.). Manual 
inspection shows that if we correct those wrong 
POS tags, that percentage will be increased to 
14.5%. 
The third rule, the verb structure filtering rule, is 
deployed in following steps: 1) keeping all 
candidates if they act as a trigger in the training set; 
2) if the candidate is a SCW, removing it when it 
does not occur as a BV in any triggers in the 
training set; 3) if the candidate is not a SCW, 
calculating the condition probability of its similar 
trigger words as triggers in the training set9 and 
then deleting all candidates whose conditional 
probabilities are less than a threshold ? , which is 
fine-tuned to 0.5. Figure 3 shows the effect on 
precision, recall and F1-measure of varying the 
threshold?  on the development set. 
0.5
0.6
0.7
0.8
0 0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
6
0
.
7
P
R
F
 
Figure 3. Effect of threshold ?  on the development 
set 
 
                Performance 
System 
Trigger Identification 
P(%) R(%) F 
Baseline 75.2 52.0 61.5 
+Compositional semantics 
without filtering 
34.8 66.8 45.8 
+ Non-trigger filtering 49.4 66.5 56.7 
+ POS filtering 50.2 65.9 57.0 
+ Verb structure filtering 73.5 62.1 67.4 
+Discourse consistency 79.3 63.5 70.5 
Table 8. Contribution to Chinese triggers identification 
(incremental) 
 
Table 8 shows the contribution of employing 
compositional semantics and discourse consistency 
to trigger identification on the held-out test set. We 
can find out that our approach dramatically 
enhances F1-measure by 9.0 units, largely due to a 
dramatic increase of 11.5% in recall, benefiting 
from both compositional semantics and discourse 
consistency mechanisms. We expect that the 
precision will also increase since our filtering 
approach successfully filters out almost 30% more 
                                                          
9 If there are more than one BV in a candidate, we calculate 
the average one. 
1013
non-triggers and the number of non-trigger 
mentions is less than that of the baseline. 
Unfortunately, the resulting set of 444 non-trigger 
mentions (after all filtering) is not a subset of 
original 629 non-trigger ones. Our observation 
shows that our compositional semantics inference 
adds almost 10% new non-triggers into candidates 
which are very hard to distinguish.  
Table 8 also justifies the impact of the discourse 
consistency between trigger mentions in trigger 
identification and the effect of the additional 
discourse-level trigger identifier, with a big gain of 
5.8% in precision and a small gain of 1.4% in 
recall. 
6.2 Chinese Event Extraction 
Table 9 shows the contribution of trigger 
identification with compositional semantics and 
discourse consistency to overall event extraction 
on the held-out test set. In addition, we also report 
the performance of two human annotators (The 
human annotator 1 is a first year postgraduate 
student with no background to Chinese event 
extraction while the human annotator 2 is a third 
year postgraduate student working on Chinese 
event extraction) on 33 texts (a subset of the held-
out test set). From the results presented in Table 9, 
we can find that our approach can improve the F1-
measure for trigger identification by 9.0 units, 
trigger type determination by 9.1 units, argument 
identification by 6.0 units and argument role 
determination (i.e. overall event extraction) by 5.4 
units, largely due to the dramatic increase in recall 
of 11.5%, 11.2%, 7.5% and 7.2%.  
 
                        Performance 
 
System/Human 
Trigger 
Identification 
Trigger Type 
Determination 
Argument 
Identification 
Argument Role 
Determination 
P(%) R(%) F P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Our Baseline 75.2 52.0 61.5 70.3 49.0 57.8 58.4 42.7 49.3 55.2 38.6 45.4 
+Compositional semantics 73.5 62.1 67.4 70.2 59.1 64.2 58.0 48.9 53.0 54.7 44.5 49.1 
+Discourse consistency 79.3 63.5 70.5 75.2 60.2 66.9 61.6 50.2 55.3 56.9 45.8 50.8 
Human annotator1(blind) 63.3 62.9 63.1 61.7 59.5 60.6 64.6 54.1 58.9 60.9 48.2 53.8 
Human annotator2(familiar) 72.6 74.3 73.4 69.1 70.2 69.6 71.5 65.9 68.6 66.4 54.6 59.9 
Inter-Annotator Agreement 45.8 42.9 44.3 45.3 42.5 43.8 60.4 49.7 54.5 55.1 45.9 50.1 
Table 9: Overall contribution to Chinese event extraction  
 
In addition, the results of two annotators show 
that Chinese event extraction is really challenging 
even for a well-educated human being. As shown 
in Table 9, the inter-annotator agreement on trigger 
identification and trigger type determination is 
even less than 45%. Although this figure is very 
low, it is not surprising: the results on the English 
ACE 2005 corpus show that the inter-annotator 
agreement on trigger identification is only about 
40% (Ji and Grishman, 2008). Detailed analysis 
shows that a human annotator tends to make more 
mistakes in trigger identification for two reasons. 
The first reason is that a human annotator always 
misses some event mentions when a sentence 
contains more than one event mention. The second 
reason is that it is hard to identify an event mention 
due to the failure of following specified annotation 
guidelines, as mentioned in Ji and Grishman 
(2008). Table 9 also shows the performance gaps 
of human annotators between trigger identification 
and trigger type determination is very small (2.5% 
and 3.8% in F1-measure). It ensures that trigger 
identification is the most important step in Chinese 
event extraction for a human being. For human 
annotators, it?s much easier to determine the event 
type of a trigger, identify its arguments and 
determine the role of each argument, all with more 
than 90% in accuracy, once a trigger is identified 
correctly.  
6.3 Discussion 
Compared with English, the word structures in 
Chinese are much more complex and diverse, 
causing a lot of troubles in Chinese language 
processing. We ensure that compositional 
semantics in Chinese words is very useful for 
many Chinese language processing applications, 
such as machine translation, semantic parser, etc. 
For example, many actions (e.g. ??? (hack), ??? 
(bite), ??? (kick), etc) can combine with ??? 
(injure) to form words and most of those words 
have similar semantics. The results in table 8 show 
its contribution in Chinese event extraction. 
Although our approach is simple, the result is 
1014
promising enough for further efforts in this 
direction.  
This paper shows that the compositional 
semantics in the verb structure provides an ideal 
way to expand the coverage of triggers. As a 
discourse-driven language, ellipsis is very common 
in Chinese, causing inference from the discourse-
level information is a fundamental requirement to 
understand the meaning of a clause, sentence or 
discourse. 
7 Conclusion 
In this paper we propose two novel inference 
mechanisms to Chinese trigger identification. In 
particular, compositional semantics inside Chinese 
triggers and discourse consistency between 
Chinese trigger mentions are used to resolve two 
critical issues in Chinese trigger identification: 
unknown triggers and word segmentation errors to 
known triggers. We give good reasons why this 
should be done, and present effective methods how 
this could be done. It shows that such novel 
inference mechanisms for Chinese event extraction 
are linguistically justified and pragmatically 
beneficial to real world applications.  
In future work, we will focus on how to 
introduce the discourse information into the 
individual classifiers to capture those long-distance 
features and joint learning of subtasks in Chinese 
event extraction. 
Acknowledgments 
The authors would like to thank three anonymous 
reviewers for their comments on this paper. This 
research was supported by the National Natural 
Science Foundation of China under Grant No. 
61070123 and No. 90920004, the National 863 
Project of China under Grant No. 2012AA011102. 
References 
David Ahn. 2006. The Stages of Event Extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events. Pages 1-8, 
Sydney, Australia. 
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and 
Christopher Manning. 2009. Discriminative 
Reordering with Chinese Grammatical Relations 
Features. In Proc. Third Workshop on Syntax and 
Structure in Statistical Translation, pages 51-59. 
Zheng Chen and Heng Ji. 2009a. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In Proc. NAACL HLT Workshop on 
Semi-supervised Learning for Natural Language 
Processing, pages 66-74, Boulder, Colorado. 
Zheng Chen and Heng Ji. 2009b. Language Specific 
Issue and Feature Exploration in Chinese Event 
Extraction. In Proc. NAACL HLT 2009, pages 209-
212, Boulder, CO. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local 
Information into Information Extraction Systems by 
Gibbs Sampling. In Proc. ACL 2005, pages 363-370, 
Ann Arbor, MI. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009, pages 369-272, Suntec, 
Singapore. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek 
Strzalkowski. 2006. Automatic Event Classification 
Using Surface Text Features. In Proc. AAAI 2006 
Workshop on Event Extraction and Synthesis, pages 
36-41, Boston, MA. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, 
Guodong Zhou and Qiaoming Zhu. 2011. Using 
Cross-Entity Inference to Improve Event Extraction. 
In Proc. ACL 2011, pages 1127-1136, Portland, OR. 
Heng Ji. 2009. Cross-lingual Predicate Cluster 
Acquisition to Improve Bilingual Event Extraction 
by Inductive Learning. In Proc. NAACL HLT 
Workshop on Unsupervised and Minimally 
Supervised Learning of Lexical Semantics, pages 
27-35, Boulder, CO. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254-262, Columbus, OH. 
Young-Joo Kim. 2000. Subject/object drop in the 
acquisition of Korean: A Cross-linguistic 
Comparison. Journal of East Asian Linguistics, 9(4): 
325-351. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? In 
Proc. ACL 2003, pages 439-446, Sapporo, Japan.  
Shasha Liao and Ralph Grishman. 2010. Using 
Document Level Cross-Event Inference to Improve 
Event Extraction. In Proc. ACL 2010, pages 789-
797, Uppsala, Sweden. 
Zhongguo Li. 2011. Parsing the Internal Structure of 
Words: A New Paradigm for Chinese Word 
Segmentation. In Proc. ACL 2011, pages 1405-1414, 
Portland, OR. 
Percy Liang, Michael I. Joedan and Dan Klein. 2011. 
Learning Dependency-Based Compositional 
1015
Semantics. In Proc. ACL 2011, pages 590-599, 
Portland, OR. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. In 
Proc. HLT/NAACL 2007, pages 332-229, Rochester, 
NY. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi Resolution Framework for Information 
Extraction from Free Text. In Proc. ACL 2007, 
pages 592-599, Prague, Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity 
Patterns and Relevant Regions. In Proc. 
EMNLP/CoNLL 2007, pages 717-727, Prague, 
Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for 
Information Extraction. In Proc. EMNLP 2009, 
pages 151-160, Singapore. 
Hongye Tan, Tiejun Zhao, Jiaheng Zheng. 2008. 
Identification of Chinese Event and Their Argument 
Roles. Proc. of the 2008 IEEE 8th International 
Conference on Computer and Information 
Technology Workshops, pages 14-19, Sydney, 
Australia. 
Yuk Wah Wong and Raymond J. Mooney. 2007. 
Learning Synchronous Grammars for Semantic 
Parsing with Lambda Calculus. In Proc. ACL 2007, 
pages 960-967, Prague, Czech Republic. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proc. RANLP 2007 
workshop on Multi-source, Multilingual Information 
Extraction and Summarization. Borovets, pages 41-
48, Borovets, Bulgaria.  
Roman Yangarber and Lauri Jokipii. 2005. 
Redundancy-based Correction of Automatically 
Extracted Facts. In Proc. EMNLP 2005, pages 57-64, 
Vancouver, Canada.  
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. ACL 1995, pages 189-196, Cambridge, MA. 
Minglin Yuan. 1998. Studies on Valency in Modern 
Chinese. Chinese Commerce and Trade Press, 
Beijing, China. 
Luke S. Zettlemoyer and Michael Collins. 2007. Online 
Learning of Relaxed CCG Grammars for Parsing to 
Logical Form. In EMNLP/CoNLL 2007, pages 678-
687, Prague, Czech Republic. 
Dexi Zhu. 1980. Research on Chinese Modern 
Grammars. Chinese Commerce and Trade Press, 
Beijing, China.  
1016
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1477?1487,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Argument Inference from Relevant Event Mentions in Chinese 
Argument Extraction 
 
 
Peifeng Li, Qiaoming Zhu, Guodong Zhou* 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, qmzhu, gdzhou}@suda.edu.cn 
 
 
 
Abstract 
As a paratactic language, sentence-level 
argument extraction in Chinese suffers 
much from the frequent occurrence of 
ellipsis with regard to inter-sentence 
arguments. To resolve such problem, this 
paper proposes a novel global argument 
inference model to explore specific 
relationships, such as Coreference, 
Sequence and Parallel, among relevant 
event mentions to recover those inter-
sentence arguments in the sentence, 
discourse and document layers which 
represent the cohesion of an event or a 
topic. Evaluation on the ACE 2005 
Chinese corpus justifies the effectiveness 
of our global argument inference model 
over a state-of-the-art baseline. 
1 Introduction 
The task of event extraction is to recognize event 
mentions of a predefined event type and their 
arguments (participants and attributes). 
Generally, it can be divided into two subtasks: 
trigger extraction, which aims to identify 
trigger/event mentions and determine their event 
type, and argument extraction, which aims to 
extract various arguments of a specific event and 
assign the roles to them. In this paper, we focus 
on argument extraction in Chinese event 
extraction. While most of previous studies in 
Chinese event extraction deal with Chinese 
trigger extraction (e.g., Chen and Ji, 2009a; Qin 
et al, 2010; Li et al, 2012a, 2012b), there are 
only a few on Chinese argument extraction (e.g., 
Tan et al, 2008; Chen and Ji, 2009b). Following 
previous studies, we divide argument extraction 
into two components, argument identification 
and role determination, where the former 
recognizes the arguments in a specific event 
mention and the latter classifies these arguments 
by roles.  
With regard to methodology, most of previous 
studies on argument extraction recast it as a 
Semantic Role Labeling (SRL) task and focus on 
intra-sentence information to identify the 
arguments and their roles. However, argument 
extraction is much different from SRL in the 
sense that, while the relationship between a 
predicate and its arguments in SRL can be 
mainly decided from the syntactic structure, the 
relationship between an event trigger and its 
arguments are more semantics-based, especially 
in Chinese, as a paratactic (e.g., discourse-driven 
and pro-drop) language with the wide spread of 
ellipsis and the open flexible sentence structure. 
Therefore, some arguments of a specific event 
mention are far away from the trigger and how to 
recover those inter-sentence arguments becomes 
a challenging issue in Chinese argument 
extraction. Consider the following discourse 
(from ACE 2005 Chinese corpus) as a sample: 
D1: ??????????????? 20 ?
????????????(E1)?????
(E2)?????????????(E3)???
???? (The Palestinian National Authority 
denied any involvement in the bomb attack (E2) 
occurred in the Gaza Strip on the morning of the 
20th, which killed (E1) two Israelites. ? They 
claimed that they will be investigating this 
attack (E3).) - From CBS20001120.1000.0823 
In above discourse, there are three event 
mentions, one kill (E1) and two Attack (E2, E3). 
While it is relatively easy to identify 20??? 
(morning of 20th), ???? (Gaza Strip) and ?
?  (bomb) as the Time, Place and Instrument 
roles in E2 by a sentence-based argument 
1477
extractor, it is really challenging to recognize 
these entities as the arguments of its corefered 
mention E3 since to reduce redundancy in a 
Chinese discourse, the later Chinese sentences 
omit many of these entities already mentioned in 
previous sentences. Similarly, it is hard to 
recognize ?????? (two Israelites) as the 
Target role for event mention E2 and identify?
?  (bomb) as the Instrument role for event 
mention E1. An alternative way is to employ 
various relationships among relevant event 
mentions in a discourse to infer those inter-
sentence arguments. 
The contributions of this paper are: 
1) We propose a novel global argument 
inference model, in which various kinds of 
event relations are involved to infer more 
arguments on their semantic relations. 
2) Different from Liao and Grishman (2010) 
and Hong et al (2011), which only consider 
document-level consistency, we propose a 
more fine-gained consistency model to 
enforce the consistency in the sentence, 
discourse and document layers. 
3) We incorporate argument semantics into our 
global argument inference model to unify the 
semantics of the event and its arguments. 
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
describes a state-of-the-art Chinese argument 
extraction system as the baseline. Section 4 
introduces our global model in inferring those 
inter-sentence arguments. Section 5 reports 
experimental results and gives deep analysis. 
Finally, we conclude our work in Section 6. 
2 Related Work 
Almost all the existing studies on argument 
extraction concern English. While some apply 
pattern-based approaches (e.g., Riloff, 1996; 
Califf and Mooney, 2003; Patwardhan and Riloff, 
2007; Chambers and Jurafsky, 2011), the others 
use machine learning-based approaches (e.g., 
Grishman et al, 2005; Ahn, 2006; Patwardhan 
and Riloff, 2009; Lu and Roth, 2012), most of 
which rely on various kinds of features in the 
context of a sentence. In comparison, there are 
only a few studies exploring inter-sentence 
information or argument semantics (e.g., Liao 
and Grishman, 2010; Hong et al, 2011; Huang 
and Riloff, 2011, 2012). 
Compared with the tremendous work on 
English event extraction, there are only a few 
studies (e.g., Tan et al, 2008; Chen and Ji, 2009b; 
Fu et al, 2010; Qin et al, 2010; Li et al, 2012) 
on Chinese event extraction with focus on either 
feature engineering or trigger expansion, under 
the same framework as English trigger 
identification. In additional, there are only very 
few of them focusing on Chinese argument 
extraction and almost all aim to feature 
engineering and are based on sentence-level 
information and recast this task as an SRL-style 
task. Tan et al (2008) introduce multiple levels 
of patterns to improve the coverage in Chinese 
argument classification. Chen and Ji (2009b) 
apply various kinds of lexical, syntactic and 
semantic features to address the special issues in 
Chinese argument extraction. Fu et al (2010) use 
a feature weighting scheme to re-weight various 
features for Chinese argument extraction. Li et al 
(2012b) introduce more refined features to the 
system of Chen and Ji (2009b) as their baseline. 
Specially, several studies have successfully 
incorporated cross-document or document-level 
information and argument semantics into event 
extraction, most of them focused on English.  
Yangarber et al (2007) apply a cross-
document inference mechanism to refine local 
extraction results for the disease name, location 
and start/end time. Mann (2007) proposes some 
constraints on relationship rescoring to impose 
the discourse consistency on the CEO?s personal 
information. Chambers and Jurafsky (2008) 
propose a narrative event chain which are 
partially ordered sets of event mentions centered 
around a common protagonist and this chain can 
represent the relationship among the relevant 
event mentions in a document. 
Ji and Grishman (2008) employ a rule-based 
approach to propagate consistent triggers and 
arguments across topic-related documents. Liao 
and Grishman (2010) mainly focus on employing 
the cross-event consistency information to 
improve sentence-level trigger extraction and 
they also propose an inference method to infer 
the arguments following role consistency in a 
document. Hong et al (2011) employ the 
background information to divide an entity type 
into more cohesive subtypes to create the bridge 
between two entities and then infer arguments 
and their roles using cross-entity inference on the 
subtypes of entities. Huang and Rillof (2012) 
propose a sequentially structured sentence 
classifier which uses lexical associations and 
discourse relations across sentences to identify 
event-related document contexts and then apply 
it to recognize arguments and their roles on the 
relation among triggers and arguments. 
1478
3 Baseline 
In the task of event extraction as defined in ACE 
evaluations, an event is defined as a specific 
occurrence involving participants (e.g., Person, 
Attacker, Agent, Defendant) and attributes (e.g., 
Place, Time). Commonly, an event mention is 
triggered via a word (trigger) in a phrase or 
sentence which clearly expresses the occurrence 
of a specific event. The arguments are the entity 
mentions involved in an event mention with a 
specific role, the relation of an argument to an 
event where it participates. Hence, extracting an 
event consists of four basic steps, identifying an 
event trigger, determining its event type, 
identifying involved arguments (participants and 
attributes) and determining their roles. 
As the baseline, we choose a state-of-the-art 
Chinese event extraction system, as described in 
Li et al (2012b), which consists of four typical 
components: trigger identification, event type 
determination, argument identification and role 
determination. In their system, the former two 
components, trigger identification and event type 
determination, are processed in a joint model, 
where the latter two components are run in a 
pipeline way. Besides, the Maximum-Entropy 
(ME) model is employed to train individual 
component classifiers for above four components. 
This paper focuses on argument identification 
and role determination. In order to provide a 
stronger baseline, we introduce more refined 
features in such two components, besides those 
adopted in Li et al (2012b). Following is a list of 
features adopted in our baseline. 
1) Basic features: trigger, POS (Part Of Speech) 
of the trigger, event type, head word of the 
entity, entity type, entity subtype; 
2) Neighbouring features: left neighbouring 
word of the entity + its POS, right neighbour 
word of the entity + its POS, left neighbour 
word of the trigger + its POS, right neighbour 
word of the trigger + its POS;  
3) Dependency features: dependency path from 
the entity to the trigger, depth of the 
dependency path; 
4) Syntactic features: path from the trigger to the 
entity, difference of the depths of the trigger 
and entity, place of the entity (before trigger 
or after trigger), depth of the path from the  
trigger to the entity, siblings of the entity; 
5) Semantic features: semantic role of the entity 
tagged by an SRL tool (e.g., ARG0, ARG1) 
(Li et al, 2010), sememe of trigger in Hownet 
(Dong and Dong, 2006). 
4 Inferring Inter-Sentence Arguments 
on Relevant Event Mentions 
In this paper, a global argument inference model 
is proposed to infer those inter-sentence 
arguments and their roles, incorporating with 
semantic relations between relevant event 
mention pairs and argument semantics. 
4.1 Motivation 
It?s well-known that Chinese is a paratactic 
language, with an open flexible sentence 
structure and often omits the subject or the object, 
while English is a hypotactic language with a 
strict sentence structure and emphasizes on 
cohesion between clauses. Hence, there are two 
issues in Chinese argument extraction, associated 
with its nature of the paratactic language. 
The first is that many arguments of an event 
mention are out of the event mention scope since 
ellipsis is a common phenomenon in Chinese. 
We call them inter-sentence arguments in this 
paper. Table 1 gives the statistics of intra-
sentence and inter-sentence arguments in the 
ACE 2005 Chinese corpus and it shows that 
20.8% of the arguments are inter-sentence ones 
while this figure is less than 1% of the ACE 2005 
English corpus. The main reason of that 
difference is that some Chinese arguments are 
omitted in the same sentence of the trigger since 
Chinese is a paratactic language with the wide 
spread of ellipsis. Besides, a Chinese sentence 
does not always end with a full stop. In particular, 
a comma is used frequently as the stop sign of a 
sentence in Chinese. We detect sentence 
boundaries, relying on both full stop and comma 
signs, since in a Chinese document, comma can 
be also used to sign the end of a sentence. In 
particular, we detect sentence boundaries on full 
stop, exclamatory mark and question mark firstly. 
Then, we identify the sentence boundaries on 
comma, using a binary classifier with a set of 
lexical and constituent-based syntactic features, 
similar to Xue and Yang (2010). 
 
Category Number 
#Arguments 8032 
#Inter-sentence 1673(20.8%) 
#Intra-sentence 6359(79.2%) 
Table 1. Statistics: Chinese argument extraction 
with regard to intra- sentence and inter-sentence 
arguments. 
 
The second issue is that the Chinese word 
order in a sentence is rather agile for the open 
1479
flexible sentence structure. Hence, different word 
orders can often express the same semantics. For 
example, a Die event mention ?Three person 
died in this accident.? can be expressed in many 
different orders in Chinese, such as ??????
?????, ??????????, ??????
?????, etc. 
In a word, above two issues indicate that 
syntactic feature-based approaches are limited in 
identifying Chinese arguments and it will lead to 
low recall in argument identification. Therefore, 
employing those high level information to 
capture the semantic relation, not only the 
syntactic structure, between the trigger and its 
long distance arguments is the key to improve 
the performance of the Chinese argument 
identification. Unfortunately, it is really hard to 
find their direct relations since they always 
appear in different clauses or sentences. An 
alternative way is to link the different event 
mentions with their predicates (triggers) and use 
the trigger as a bridge to connect the arguments 
to the trigger in another event mention indirectly. 
Hence, the semantic relations among event 
mentions are helpful to be a bridge to identify 
those inter-sentence arguments. 
4.2 Relations of Event Mention Pairs 
In a discourse, most event mentions are 
surrounding a specific topic. It?s obvious that 
those mentions have the intrinsic relationships to 
reveal the essential structure of a discourse. 
Those relevant semantics-based relations are 
helpful to infer the arguments for a specific 
trigger mention when the syntactic relations in 
Chinese argument extraction are not as effective 
as that in English. In this paper, we divide the 
relations among relevant event mentions into 
three categories: Coreference, Sequence and 
Parallel. 
An event may have more than one mention in 
a document and coreference event mentions refer 
to the same event, as same as the definition in the 
ACE evaluations. Those coreference event 
mentions always have the same arguments and 
roles. Therefore, employing this relation can 
infer the arguments of an event mention from 
their Coreference ones. For example, we can 
recover the Time, Place and Instrument for E3 
via its Coreference mention E2 in discourse D1, 
mentioned in Section 1. 
Li et al (2012a) find out that sometimes two 
trigger mentions are within a Chinese word 
whose morphological structure is Coordination. 
Take the following sentence as a sample: 
D2: ?? 17 ????????????(E4)
? (E5)????? (A 12-year-old younger 
hijacked a bus and then stabbed (E4) a woman 
to death (E5).) - From ZBN20001218.0400.0005 
In D2, ??  (stab a person to death) is a 
trigger with the Coordination structure and can 
be divided into two single-morpheme words ? 
(stab) and ? (die) while the former triggers an 
Attack event and the latter refers to a Die one. 
It?s interesting that they share all arguments in 
this sentence. The relation between those event 
mentions whose triggers merge a Chinese word 
or share the subject and the object are Parallel. 
For the errors in the syntactic parsing, the second 
single-morpheme trigger is often assigned a 
wrong tag (e.g., NN, JJ) and this leads to the 
errors in the argument extraction. Therefore, 
inferring the arguments of the second single-
morpheme trigger from that of the first one based 
on Parallel relation is also an available way to 
recover arguments. 
Like that the topic is an axis in a discourse, the 
relations among those relevant event mentions 
with the different types is the bone to link them 
into a narration. There are a few studies on using 
the event relations in NLP (e.g., summarization 
(Li et al, 2006), learning narrative event chains 
(Chambers and Jurafsky, 2007)) to ensure its 
effectiveness. In this paper, we define two types 
of Sequence relations of relevant event mentions: 
Cause and Temporal for their high probabilities 
of sharing arguments.  
The Cause relation between the event 
mentions are similar to that in the Penn 
Discourse TreeBank 2.0 (Prasad et al, 2008). 
For example, an Attack event often is the cause 
of an Die or Injure event. Our Temporal relation 
is limited to those mentions with the same or 
relevant event types (e.g., Transport and Arrest) 
for the high probabilities of sharing arguments. 
Take the following discourse as a sample: 
D3: ??????(E6)??????????
????(E7)?????????????
(These prisoners left (E6) Tindouf, a western 
city of Algeria, and went (E7) to Agadir, a 
southwestern city of Morocco.) - From 
Xin20001215.2000.0158 
In D3, there are two Transport mentions and it 
is natural to infer ????  (Agadir) as the 
Destination role of E6 and??? (Tindouf) as 
the Origin role of E7 via their Sequence relation. 
1480
4.3 Identifying Relations of Event Mention 
Pairs 
Currently, there are only few studies focusing on 
such area (e.g., Ahn, 2006; Chamber and 
Jurafsky, 2007; Huang and Rillof, 2012; Do et al, 
2012) and their approaches cannot be introduced 
to our system directly for the language nature 
and the different goal. We try to achieve a higher 
accuracy in this stage so that our argument 
inference can recover more true arguments.  
Inspired by Li and Zhou (2012), we also use 
the morphological structure to identify the 
Parallel relation. Two parallel event mentions 
with the adjacent trigger mentions w1 and w2 must 
satisfy follows two conditions: 
1) Morph(w1,w2) is Coordination 
2) jiTwHMTwHM ji ??? )(,)( 21   
where Morph(w1,w2) is a function to recognize 
the morphological structure of joint word w1w2, 
HM(wi) is to identify the head morpheme 1  in 
word wi and Ti is the set of the head morphemes 
with ith event type. These constraints are 
enlightened by the fact that only Chinese words 
with Coordination structure can be divided into 
two new words and each word can trigger an 
event with the different event type 2 . The 
implementation of Morph(w1,w2) and HM(w) are 
described in Li and Zhou (2012). 
The Coreference relation is divided into two 
types: Noun-based Coreference (NC) and Event-
based Coreference (EC) while the former always 
uses a verbal noun to refer to an event mentioned 
in current or previous sentence and the latter is 
that an event is mentioned twice or more actually. 
For example, the relation between E2 and E3 in 
D1 is NC while the trigger of E3 is only a verbal 
noun without any direct arguments and it refers 
to E2. 
We adopt a simple rule to recognize those NC 
relations: for each event mention whose trigger is 
a noun and doesn?t act as the subject/object, we 
regard their relation as NC if there is another 
event mention with the same trigger in current or 
previous sentence. 
Inspired by Ahn (2006), we use the following 
conditions to infer the EC relations between two 
event mentions with the same event type: 
1) Their trigger mentions refer to the same 
trigger; 
2) They have at least one same or similar 
                                                          
1 It acts as the governing semantic element in a Chinese 
word. 
2 If they have the same event type, they will be regarded as 
a single event mention. 
subject/object; 
3) The score of cosine similarity of two event 
mentions is more than a threshold3. 
Finally, for the Sequence relation, instead of 
identifying and classifying the relations clearly 
and correctly, our goal is to identify whether 
there are relevant event mentions in a long 
sentence or two adjacent short sentences who 
share arguments. Algorithm 1 illustrates a 
knowledge-based approach to identify the 
Sequence event relation in a discourse for any 
two trigger mentions tri1 and tri2 as follows: 
 
Algorithm 1 
1: input: tri1 and tri2 and their type et1 and et2 
2:  output: whether their relation is Sequence 
3:  begin 
4:      hm1 ?HM(tri1);  hm2 ?HM(tri2) 
5:  MP ?FindAllMP(hm1,et1,hm2,et2) 
6:     for any mpi in MP 
7:         if ShareArg(mpi) is true then 
8:             return true   // Sequence 
9:        end if 
10:    end for 
11:    return false 
12:  end 
 
In algorithm 1, HM(tri) is to identify the head 
morpheme in trigger tri and FindAllMP(hm1, et1, 
hm2, et2) is to find all event mention pairs in the 
training set which satisfy the condition that their 
head morphemes are hm1 and hm2, and their 
event types are et1 and et2 respectively. Besides, 
ShareArg(mpi)is used to identify whether the 
event mention pair mpi sharing at least one 
argument. In this algorithm, since the relations 
on the event types are too coarse, we introduce a 
more fine-gained Sequence relation both on the 
event types and the head morphemes of the 
triggers which can divide an event type into 
many subtypes on the head morpheme. Li and 
Zhou (2012) have ensured the effectiveness of 
using head morpheme to infer the triggers and 
our experiment results also show it is helpful for 
identifying relevant event mentions which aims 
to the higher accuracy. 
4.4 Global Argument Inference Model 
Our global argument inference model is 
composed of two steps: 1) training two sentence-
based classifiers: argument identifier (AI) and 
role determiner (RD) that estimate the score of a 
candidate acts as an argument and belongs to a 
                                                          
3 The threshold is tuned to 0.78 on the training set. 
1481
specific role following Section 3. 2) Using the 
scores of two classifiers and the event relations 
in a sentence, a discourse or a document, we 
perform global optimization to infer those 
missing or long distance arguments and their 
roles.  
To incorporate those event relations with our 
global argument inference model, we regard a 
document as a tree and divide it into three layers: 
document, discourse and sentence. A document 
is composed of a set of the discourses while a 
discourse contains three sentences. Since almost 
all arguments (~98%) of a specific event mention 
in the ACE 2005 Chinese corpus appear in the 
sentence containing the specific event mention 
and its two adjacent sentences (previous and next 
sentences), we only consider these three 
sentences as a discourse to simplify the process 
of identifying the scope of a discourse.  
We incorporate different event relations into 
our model on the different layer and the goal of 
our global argument inference model is to 
achieve the maximized scores over a document 
on its three layers and two classifiers: AI and RD. 
The score of document D is defined as 
))1))(,(1(),(
()1(
))1))((1()(
((maxarg
,,
, ,,, ,,
, ,,, ,,,
^
><><
? ?>< ><?>< ><? ?
? ?>< ><?>< ><?
??++
?+
??+
=
? ? ? ? ?
? ? ? ?
mZmZDmZmZD
DiI iIjiS jiSkjiT kjiTZA Rm
ZZIZZI
DiI iIjiS jiSkjiT kjiTZAYX
YREfYREf
XEfXEf
D
?
?
(1) 
}1,0{.. ?ZXts                                          (2) 
}1,0{, ?>< mZY                                  (3) 
RmYX mZZ ??? >< ,                       (4) 
?
?Rm
mZZ YX ><= ,                               (5) 
where Ii is the ith discourses in document D; 
S<i,j> is the jth sentences in discourse Ii; T<i,j,k> is 
the kth event mentions in sentence S<i,j>; A<i,j,k,l> 
is the lth candidate arguments in event mention 
T<i,j,k>; Z is used to denote <i,j,k,l>; fI(EZ) is the 
score of AI identifying entity mention EZ as an 
argument, where EZ is the lth entity of the kth 
event mention of the jth sentence of the ith 
discourse in document D. fD(EZ, Rm) is the score 
of RD assigning role Rm to argument EZ. Finally, 
XZ and Y<Z,m> are the indicators denoting whether 
entity EZ is an argument and whether the role Rm 
is assigned to entity EZ respectively. Besides, Eq. 
4 and Eq. 5 are the inferences to enforce that:  
1) if an entity belongs to a role, it must be an 
argument; 
2) if a entity is an argument of a specific event 
mention, it must have a role. 
Parallel relation: Sentence-based 
optimization is used to incorporate the Parallel 
relation of two event mentions into our model 
and they share all arguments in a sentence. Since 
different event type may have different role set, 
each role in a specific event should be mapped to 
the corresponding role in its Parallel event when 
they have the different event type. For example, 
the argument ??? 17 ????? (A 12-year-
old younger) in D2 acts as the Attacker role in 
the Attack event and the Agent role in the Die 
event. We learn those role-pairs from the training 
set and Table 2 shows part of the role relations 
learning from the training set. 
 
Event type pair Role pair 
Attack-Die Attacker-Agent; Target-
Victim;? 
Injure-Die Agent-Agent; Victim-
Victim;? 
Transport-
Demonstrate 
Artifact-Entity; 
Destination-Place;? 
Table 2. Part of role-pairs for those event 
mention pairs with Parallel relation. 
 
To infer the arguments and their roles on the 
Parallel relation, we enforce the consistency on 
the role-pair as follows: 
><><?
><><><><
><><><><
><><
=?>?<
????
???????
=
',',,,,,'
',,',',,,,,,,
,',',,,,
',',',,,,,,
',
,
lkjilkjihethet
kjilkjikjilkji
jikjikjiijii
mlkjimlkji
EERPmm
TATA
STTISDI
YY
(6) 
where 
'hh etetRP ?  is the set of role-pairs between 
two Parallel event mention eth and eth? and 
><>< = ',',,,,, lkjilkji EE  means they refer to the 
same entity mention. With the transitivity 
between the indicators X and Y, Eq. 6 also 
enforces the consistency on X<i,j,k,l> and X<i,j,k?,l?>. 
Coreference relation: Since the NC and EC 
relcation between two event mentions are 
different in the event expression, we introduce 
the discourse-based optimization for the former 
and document-based optimization for the latter. 
For two NC mentions, we ensure that the 
succeeding mentions can inherit the arguments 
form the previous one. To enforce this 
consistency, we just replace all fI(EZ) and fD(EZ, 
Rm) of the succeeding event mention with that of 
the previous one, since the previous one have the 
more context information. 
As for two EC event mentions, algorithm 2 
shows how to create the constraints for our 
1482
global argument inference model to infer 
arguments and roles. 
 
Algorithm 2 
1: input: two event mentions T, T? and their 
arguments set A and A? 
2:  output: the constraints set C 
3:  begin 
4:       for each argument a in A do 
5:            a??FindSim(a) 
6:    if a??? then 
7:                 ),( 'aa YYyConsistencCC ??  
8:             end if 
9:        end for 
10: end 
 
In algorithm 2, the function FindSim(a) is 
used to find a similar candidate argument a? in 
A? for a. If it?s found, we enforce the consistency 
of argument a and a? in the role by using 
Consistency(Ya,Ya?) where Ya  and Ya? are the 
indicators in Eq. 1. To evaluate the similarity 
between two candidates a and a?, we regard them 
as similar ones when they are the same word or 
in the same entity coreference chain. We use a 
coreference resolution tool to construct the entity 
coreference chains, as described in Kong et al
(2010). 
Sequence relation: For any two event 
mentions in a discourse, we use the event type 
pair with their head morphemes (e.g., Attack:?
(burst) - Die:?(die), Trial-Hearing:?(trial) - 
Sentence:?(sentence)) to search the training set 
and then obtain the probabilities of sharing the 
arguments as mentioned in algorithm 1. We 
denoted Pro<et,et?,HM(tri),HM(tri?),Rm,Rm?> as the 
probability of the trigger mentions tri and tri? 
(their event types are et and et? respectively.) 
sharing an argument whose roles are Rm and Rm? 
respectively. We propose following discourse-
based constraint to enforce the consistency 
between the roles of two arguments, which are 
related semantically, temporally, causally or 
conditionally, based on the probability of sharing 
an argument and the absolute value of the 
difference between the scores of RD: 
?
?
>
>
=?
????
=
><><
><><><><
><><><><
><><
),(),(
),),'(),(,',(Pr
',
,?
'',',',,,,
'
',',',,,,',',',
,,,',,
',',',',,,,,
mlkjiDmlkjiD
mm
lkjilkjijikji
jikjiijijii
mlkjimlkji
REfREf
RRtriHMtriHMeteto
EERmmST
STISSDI
YY
??
????
? (7) 
where ? and ? are the thresholds learned from the 
development set; tri and tri? are triggers of kth 
and k?th event mention whose event types are et 
and et? in S<i,j> and S<i,j?> respectively. 
4.5 Incorporating Argument Semantics into 
Global Argument Inference Model 
We also introduce the argument semantics, 
which represent the semantic relations of 
argument-argument pair, argument-role pair and 
argument-trigger pair, to reflect the cohesion 
inside an event. Hong et al (2011) found out that 
there is a strong argument and role consistency in 
the ACE 2005 English corpus. Those 
consistencies also occur in Chinese and they 
reveal the relation between the trigger and its 
arguments, and also explore the relation between 
the argument and its role. Besides, those entities 
act as non-argument also have the consistency 
with high probabilities.  
To let the global argument inference model 
combine those knowledges of argument 
semantics, we compute the prior probabilities 
P(X<i,j>=1) and P(Y<i,j,m>=1) that entity enj 
occurrs in a specific event type eti as an 
argument and its role is Rm respectively. To 
overcome the sparsity of the entities, we cluster 
those entities into more cohesive subtype 
following Hong et al (2011). Hence, following 
the independence assumptions described by 
Berant et al (2011), we modify the fI(EZ) and 
fD(EZ,Rm)in Eq. 1 as follows: 
)0()|1(1(
)1()|1(
log)( ==?
===
ZZZ
ZZZ
ZI XPFXP
XPFXP
Ef     (8) 
)0()|1(1(
)1()|1(
log),(
,,,
,,,
==?
===
><><><
><><><
mZmZmZ
mZmZmZ
mZD XPFXP
XPFYP
REf (9) 
where )|1( ZZ FXP =  and )|1( ,, ><>< = mZmZ FYP  
are the probabilities from the AI and AD 
respectively while FZ and F<Z,m> are the feature 
vectors. Besides, )1( , =>< mZXP  and )1( =ZXP  
are the prior probabilities learning from the 
training set. 
5 Experimentation 
In this section, we first describe the experimental 
settings and the baseline, and then evaluate our 
global argument inference model incorporating 
with relevant event mentions and argument 
semantics to infer arguments and their roles. 
5.1 Experimental Settings and Baseline 
For fair comparison, we adopt the same 
experimental settings as the state-of-the-art event 
extraction system (Li et al 2012b) and all the 
1483
evaluations are experimented on the ACE 2005 
Chinese corpus. We randomly select 567 
documents as the training set and the remaining 
66 documents as the test set. Besides, we reserve 
33 documents in the training set as the 
development set and use the ground truth entities, 
times and values for our training and testing. As 
for evaluation, we also follow the standards as 
defined in Li et al (2012b). Finally, all the 
sentences in the corpus are divided into words 
using a Chinese word segmentation tool 
(ICTCLAS) 1  with all entities annotated in the 
corpus kept. We use Berkeley Parser 2  and 
Stanford Parser 3  to create the constituent and 
dependency parse trees.  Besides, the ME tool 
(Maxent) 4  is employed to train individual 
component classifiers and lp_solver5 is used to 
construct our global argument inference model. 
Besides, all the experiments on argument 
extraction are done on the output of the trigger 
extraction system as described in Li et al 
(2012b). Table 3 shows the performance of the 
baseline trigger extraction system and Line 1 in 
Table 4 illustrates the results of argument 
identification and role determination based on 
this system. 
 
Trigger 
identification 
Event type 
determination 
P(%) R(%) F1 P(%) R(%) F1 
74.4 71.9 73.1 71.4 68.9 70.2
Table 3. Performance of the baseline on trigger 
identification and event type determination. 
5.2 Inferring Arguments on Relevant Event 
Mentions and Argument Semantics 
We develop a baseline system as mentioned in 
Section 3 and Line 2 in Table 4 shows that it 
slightly improves the F1-measure by 0.9% over 
Li et al (2012b) due to the incorporation of more 
refined features. This result indicates the 
limitation of syntactic-based feature engineering. 
Before evaluating our global argument 
inference model, we should identify the event 
relations between two mentions in a sentence, a 
discourse or a document. The experimental 
results show that the accuracies of identifying 
NC, EC, Parallel and Sequence relation are 
80.0%, 72.4%, 88.5% and 87.7% respectively. 
Those results ensure that our simple methods are 
                                                          
1http://ictclas.org/  
2 http://code.google.com/p/berkeleyparser/ 
3 http://nlp.stanford.edu/software/lex-parser.shtml 
4 http://mallet.cs.umass.edu/ 
5 http://lpsolve.sourceforge.net/5.5/ 
effective. Our statistics on the development set 
shows almost 65% of the event mentions are 
involved in those Correfrence, Parallel and 
Sequence relations, which occupy 63%, 50%, 9% 
respectively6. Most of the exceptions are isolated 
event mentions. 
 
System 
Argument 
identification 
Argument role 
determination
P(%) R(%) F1 P(%) R(%) F1
Li et al(2012b) 59.1 57.2 58.1 55.8 52.1 53.9
Baseline 60.5 57.6 59.0 55.7 53.0 54.4
BIM 59.3 60.1 59.7 54.4 55.2 54.8
BIM+RE 60.2 65.6 62.8 55.0 60.0 57.4
BIM+RE+AS 62.9 66.1 64.4 57.2 60.2 58.7
Table 4. Performance comparison of argument 
extraction on argument identification and role 
determination. 
Once the classifier AI and RD are trained, we 
would like to apply our global argument 
inference model to infer more inter-sentence 
arguments and roles. To achieve an optimal 
solution, we formulate the global inference 
problem as an Integer Linear Program (ILP), 
which leads to maximize the objective function. 
ILP is a mathematical method for constraint-
based inference to find the optimal values for a 
set of variables that maximize an objective 
function in satisfying a certain number of 
constraints. In the literature, ILP has been widely 
used in many NLP applications (e.g., Barzilay 
and Lapata, 2006; Do et al, 2012; Li et al, 
2012b).  
For our systems, we firstly evaluate the 
performance of our basic global argument 
inference model (BIM) with the Eq. 2?5 which 
enforce the consistency on AI and RD and then 
introduce the inference on the relevant event 
mentions (RE) and argument semantics (AS) to 
BIM. Table 4 shows their results and we can find 
out that: 
1) BIM only slightly improves the performance 
in F1-measure, as the result of more increase 
in recall (R) than decrease in precision (P). 
This suggests that those constraints just 
enforcing the consistency on AI and RD is not 
effective enough to infer more arguments. 
2) Compared to the BIM, our model BIM+RE 
enhances the performance of argument 
identification and role determination by 3.1% 
and 2.6% improvement in F1-measure 
respectively. This suggests the effectiveness 
                                                          
6 20% of the mentions belongs to both Coreference and 
Sequence relations. 
1484
of our global argument inference model on 
the relevant event mentions to infer inter-
sentence arguments. Table 5 shows the 
contributions of the different event relations 
while the Sequence relation gains the highest 
improvement of argument identification and 
role determination in F1-measure respectively. 
 
Constraint 
Argument 
identification 
Argument role 
determination 
P(%) R(%) F1 P(%) R(%) F1
BIM 59.3 60.1 59.7 54.4 55.2 54.8
+Parallel +0.6 +0.7 +0.6 +0.4 +0.6 +0.5
+NC +0.0 +0.8 +0.4 -0.2 +0.6 +0.2
+EC +0.6 +1.2 +0.9 +0.5 +1.0 +0.7
+ Sequence -0.3 +2.8 +1.2 -0.2 +2.6 +1.1
Table 5. Contributions of different event 
relations on argument identification and role 
determination. (Incremental) 
3) Our model BIM+ER+AS gains 1.6% 
improvement for argument identification, and 
1.3% for role determination. The results 
ensure that argument semantics not only can 
improve the performance of argument 
identification, but also is helpful to assign a 
correct role to an argument in role 
determination. 
Table 3 shows 25.6% of trigger mentions 
introduced into argument extraction are pseudo 
ones. If we use the golden trigger extraction, our 
exploration shows that the precision and recall of 
argument identification can be up to 78.6% and 
88.3% respectively. Table 6 shows the 
performance comparison of argument extraction 
on AI and RD given golden trigger extraction. 
Compared to the Baseline, our system improves 
the performance of argument identification and 
role determination by 6.4% and 5.8% 
improvement in F1-measure respectively, largely 
due to the dramatic increase in recall of 10.9% 
and 10.4%. 
 
 
System 
Argument 
identification 
Argument role 
determination 
P(%) R(%) F1 P(%) R(%) F1
Baseline 76.2 77.4 76.8 70.4 72.0 71.2
Model2 78.6 88.3 83.2 72.3 82.4 77.0
Table 6. Performance comparison of argument 
identification and type determination. (Golden 
trigger extraction) 
5.3 Discussion 
The initiation of our paper is that syntactic 
features play an important role in current 
machine learning-based approaches for English 
event extraction, however, their effectiveness is 
much reduced in Chinese. So the improvement of 
our model for English event extraction is much 
less than that of Chinese. However, our model 
can be an effective complement of the sentence-
level English argument extraction systems since 
the performance of argument extraction is still 
low in English and using discourse-level 
information is a way to improve its performance, 
especially for those event mentions whose 
arguments spread in complex sentences. 
Moreover, our exploration shows that our 
global argument inference model can mine those 
arguments within a long distance which are un-
annotated as arguments of a special event 
mention in the corpus since the annotators just 
tagged arguments in a narrow scope or omitted a 
few arguments. Actually, they are the true ones 
to our knowledge and  are more than 30.6% of 
those pseudo arguments inferred by our model. 
This ensures that our global argument inference 
model and those relations among event mentions 
is helpful to argument extraction. 
6 Conclusion 
In this paper we propose a global argument 
inference model to extract those inter-sentence 
arguments due to the nature of Chinese that it is a 
discourse-driven pro-drop language with the 
wide spread of ellipsis and the open flexible 
sentence structure. In particular, we incorporate 
various kinds of event relations and the argument 
semantics into the model in the sentence, 
discourse and document layers which represent 
the cohesion of an event or a topic. The 
experimental results ensure that our global 
argument inference model outperforms the state-
of-the-art system. 
In future work, we will focus on introducing 
more semantic information and cross-document 
information into the global argument inference 
model to improve the performance of argument 
extraction. 
Acknowledgments 
The authors would like to thank three 
anonymous reviewers for their comments on this 
paper. This research was supported by the 
National Natural Science Foundation of China 
under Grant No. 61070123, No. 61272260 and 
No. 61273320, the National 863 Project of China 
under Grant No. 2012AA011102. The co-author 
tagged with ?*? is the corresponding author. 
1485
References  
David Ahn. 2006. The Stages of Event Extraction. In 
Proc. COLING/ACL 2006 Workshop on 
Annotating and Reasoning about Time and Events. 
Pages 1-8, Sydney, Australia. 
Regina Barzilay and Miralla Lapata. 2006. 
Aggregation via Set Partitioning for Natural 
Language Generation. In Proc. NAACL 2006, 
pages 359-366, New York City, NY. 
Jonathan Berant, Ido Dagan and Jacob Goldberger. 
2011. Global Learning of Typed Entailment Rules. 
In Proc. ACL 2011, pages 610-619, Portland, OR. 
Mary Elaine Califf and Raymond J. Mooney. 2003. 
Bottom-up Relational Learning of Pattern 
Matching rules for Information Extraction. Journal 
of Machine Learning Research, 4:177?210. 
Nathanael Chambers and Dan Jurafsky. 2008. 
Unsupervised Learning of Narrative Event Chains. 
In Proc. ACL 2008, pages 789-797, Columbus, OH. 
Nathanael Chambers and Dan Jurafsky. 2011. 
Template-based Information Extraction without the 
Templates. In Proc. ACL 2011, pages 976-986, 
Portland, OR. 
Zheng Chen and Heng Ji. 2009a. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In Proc. NAACL/HLT 2009 Workshop 
on Semi-supervised Learning for Natural Language 
Processing, pages 66-74, Boulder, Colorado. 
Zheng Chen and Heng Ji. 2009b. Language Specific 
Issue and Feature Exploration in Chinese Event 
Extraction. In Proc. NAACL HLT 2009, pages 
209-212, Boulder, Colorado. 
Zhengdong Dong and Qiang Dong. 2006. HowNet 
and the Computation of Meaning. World Scientific 
Pub Co. Inc. 
Quang Xuan Do, Wei Lu and Dan Roth. 2012. Joint 
Inference for Event Timeline Construction. In Proc.  
EMNLP 2012, pages 677-687, Jeju, Korea. 
Jianfeng Fu, Zongtian Liu, Zhaoman Zhong and 
Jianfang Shan. 2010. Chinese Event Extraction 
Based on Feature Weighting. Information 
Technology Journal, 9: 184-187.  
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, 
Guodong Zhou and Qiaoming Zhu. 2011. Using 
Cross-Entity Inference to Improve Event Extraction. 
In Proc. ACL 2011, pages 1127-1136, Portland, 
OR. 
Ruihong Huang and Ellen Riloff. 2011. Peeling Back 
the Layers: Detecting Event Role Fillers in 
Secondary Contexts, In Proc. ACL 2011, pages 
1137-1147, Portland, OR. 
Ruihong Huang and Ellen Riloff. 2012. Modeling 
Textual Cohesion for Event Extraction. In Proc. 
AAAI 2012, pages 1664-1770, Toronto, Canada. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL 2008, pages 254-262, Columbus, OH. 
Fang Kong, Guodong Zhou, Longhua Qian and 
Qiaoming Zhu. 2010. Dependency-driven 
Anaphoricity Determination for Coreference 
Resolution. In Proc. COLING 2010, pages 599-607, 
Beijing, China. 
Junhui Li, Guodong Zhou and Hwee Tou Ng. 2010. 
Joint Syntactic and Semantic Parsing of Chinese. 
In Proc. ACL 2010, pages 1108-1117, Uppsala, 
Sweden. 
Peifeng Li, Guodong Zhou, Qiaoming Zhu and Libin 
Hou. 2012a. Employing Compositional Semantics 
and Discourse Consistency in Chinese Event 
Extraction. In Proc. EMNLP 2012, pages 1006-
1016, Jeju, Korea. 
Peifeng Li, Qiaoming Zhu, Hongjun Diao and 
Guodong Zhou. 2012b. Joint Modeling of Trigger 
Identification and Event Type Determination in 
Chinese Event Extraction. In Proc. COLING 2012, 
pages 1635-1652, Mumbai, India. 
Peifeng Li and Guodong Zhou. 2012. Employing 
Morphological Structures and Sememes for 
Chinese Event Extraction. In Proc. COLING 2012, 
pages 1619-1634, Mumbai, India. 
Wenjie Li, Mingliu Wu, Qin Lu, Wei Xu and Chunfa 
Yuan. 2006. Extractive Summarization using Inter- 
and Intra- Event Relevance. In Proc. 
COLING/ACL 2006, pages 369-376, Sydney, 
Australia.  
Shasha Liao and Ralph Grishman. 2010. Using 
Document Level Cross-Event Inference to Improve 
Event Extraction. In Proc. ACL 2010, pages 789-
797, Uppsala, Sweden. 
Wei Lu and Dan Roth. 2012. Automatic Event 
Extraction with Structured Preference Modeling. 
In Proc. ACL 2012, pages 835-844, Jeju, Korea. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. 
In Proc. HLT/NAACL 2007, pages 332-229,  
Rochester, NY. 
Siddharth Patwardhan and Ellen Riloff. 2007. 
Effective Information Extraction with Semantic 
Affinity Patterns and Relevant Regions. In Proc. 
EMNLP/CoNLL 2007, pages 717-727, Prague, 
Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A 
Unified Model of Phrasal and Sentential Evidence 
1486
for Information Extraction. In Proc. EMNLP 2009, 
pages 151-160, Singapore. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
Miltsakaki, Livio Robaldo, Aravind Joshi and 
Bonnie Webber. 2008. The Penn Discourse 
Treebank 2.0. In Proc. LREC 2008, pages 2961-
2968, Marrakech, Morocco. 
Bing Qin, Yanyan Zhao, Xiao Ding, Ting Liu and 
Guofu Zhai. 2010. Event Type Recognition Based 
on Trigger Expansion. Tsinghua Science and 
Technology, 15(3): 251-258, Beijing, China. 
Ellen Riloff. 1996. Automatically Generating 
Extraction Patterns from Untagged Text. In Proc. 
AAAI 1996, pages 1044?1049, Portland, OR. 
Hongye Tan, Tiejun Zhao, Jiaheng Zheng. 2008. 
Identification of Chinese Event and Their 
Argument Roles. In Proc. 2008 IEEE International 
Conference on Computer and Information 
Technology Workshops, pages 14-19, Sydney, 
Australia. 
Nianwen Xue and Yaqin Yang. 2010. Chinese 
Sentence Segmentation as Comma Classification. 
In Proc. ACL 2010, pages 631-635, Uppsala, 
Sweden. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proc. RANLP 2007 
Workshop on Multi-source, Multilingual 
Information Extraction and Summarization, pages 
41-48, Borovets, Bulgaria. 
1487
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 511?515,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Modeling of News Reader?s and Comment Writer?s Emotions?
 
 
Huanhuan Liu?  Shoushan Li??*  Guodong Zhou?  Chu-Ren Huang?  Peifeng Li? 
 
?Natural Language Processing Lab 
Soochow University, China 
{huanhuanliu.suda,shoushan.li, 
churenhuang}@gmail.com 
 
?Department of CBS 
the Hong Kong Polytechnic University 
{gdzhou,pfli}@suda.edu.cn 
 
 
Abstract 
Emotion classification can be generally done 
from both the writer?s and reader?s 
perspectives. In this study, we find that two 
foundational tasks in emotion classification, 
i.e., reader?s emotion classification on the 
news and writer?s emotion classification on 
the comments, are strongly related to each 
other in terms of coarse-grained emotion 
categories, i.e., negative and positive. On the 
basis, we propose a respective way to jointly 
model these two tasks. In particular, a co-
training algorithm is proposed to improve 
semi-supervised learning of the two tasks. 
Experimental evaluation shows the 
effectiveness of our joint modeling 
approach.* 
1 Introduction 
Emotion classification aims to predict the emo-
tion categories (e.g., happy, angry, or sad) of a 
given text (Quan and Ren, 2009; Das and Ban-
dyopadhyay, 2009). With the rapid growth of 
computer mediated communication applications, 
such as social websites and miro-blogs, the re-
search on emotion classification has been attract-
ing more and more attentions recently from the 
natural language processing (NLP) community 
(Chen et al, 2010; Purver and Battersby, 2012). 
In general, a single text may possess two kinds 
of emotions, writer?s emotion and reader?s emo-
tion, where the former concerns the emotion ex-
pressed by the writer when writing the text and 
the latter concerns the emotion expressed by a 
reader after reading the text. For example, con-
sider two short texts drawn from a news and cor-
responding comments, as shown in Figure 1. On 
                                                 
* *  Corresponding author 
one hand, for the news text, while its writer just 
objectively reports the news and thus does not 
express his emotion in the text, a reader could 
yield sad or worried emotion. On the other hand, 
for the comment text, its writer clearly expresses 
his sad emotion while the emotion of a reader 
after reading the comments is not clear (Some 
may feel sorry but others might feel careless). 
 
News:  
Today's Japan earthquake could be 
     2011 quake aftershock. ?? 
News Writer?s emotion: None 
News Reader?s emotion: sad, worried 
Comments: 
(1) I hope everything is ok, so sad. I still can 
not forget last year. 
(2) My father-in-law got to experience this 
quake... what a suffering. 
Comment Writer?s emotion: sad 
Comment Reader?s emotion: Unknown 
Figure 1: An example of writer?s and reader?s 
emotions on a news and its comments 
 
Accordingly, emotion classification can be 
grouped into two categories: reader?s emotion 
and writer?s emotion classifications. Although 
both emotion classification tasks have been 
widely studied in recent years, they are always 
considered independently and treated separately.  
However, news and their corresponding com-
ments often appear simultaneously. For example, 
in many news websites, it is popular to see a 
news followed by many comments. In this case, 
because the writers of the comments are a part of 
the readers of the news, the writer?s emotions on 
the comments are exactly certain reflection of the 
reader?s emotions on the news. That is, the 
comment writer?s emotions and the news read-
er?s emotions are strongly related. For example, 
511
in Figure 1, the comment writer?s emotion ?sad? 
is among the news reader?s emotions. 
Above observation motivates joint modeling 
of news reader?s and comment writer?s emotions. 
In this study, we systematically investigate the 
relationship between the news reader?s emotions 
and the comment writer?s emotions. Specifically, 
we manually analyze their agreement in a corpus 
collected from a news website. It is interesting to 
find that such agreement only applies to coarse-
grained emotion categories (i.e., positive and 
negative) with a high probability and does not 
apply to fine-grained emotion categories (e.g., 
happy, angry, and sad). This motivates our joint 
modeling in terms of the coarse-grained emotion 
categories. Specifically, we consider the news 
text and the comment text as two different views 
of expressing either the news reader?s or com-
ment writer?s emotions. Given the two views, a 
co-training algorithm is proposed to perform 
semi-supervised emotion classification so that 
the information in the unlabeled data can be ex-
ploited to improve the classification performance. 
2 Related Work  
2.1 Comment Writer?s Emotion Classifica-
tion 
Comment writer?s emotion classification has 
been a hot research topic in NLP during the last 
decade (Pang et al, 2002; Turney, 2002; Alm et 
al., 2005; Wilson et al, 2009) and previous stud-
ies can be mainly grouped into two categories: 
coarse-grained and fine-grained emotion classifi-
cation. 
Coarse-grained emotion classification, also 
called sentiment classification, concerns only 
two emotion categories, such as like or dislike 
and positive or negative (Pang and Lee, 2008; 
Liu, 2012). This kind of emotion classification 
has attracted much attention since the pioneer 
work by Pang et al (2002) in the NLP communi-
ty due to its wide applications (Cui et al, 2006; 
Riloff et al, 2006; Dasgupta and Ng, 2009; Li et 
al., 2010; Li et al, 2011). 
In comparison, fine-grained emotion classifi-
cation aims to classify a text into multiple emo-
tion categories, such as happy, angry, and sad. 
One main group of related studies on this task is 
about emotion resource construction, such as 
emotion lexicon building (Xu et al, 2010; 
Volkova et al, 2012) and sentence-level or doc-
ument-level corpus construction (Quan and Ren, 
2009; Das and Bandyopadhyay, 2009). Besides, 
all the related studies focus on supervised learn-
ing (Alm et al, 2005; Aman and Szpakowicz, 
2008; Chen et al, 2010; Purver and Battersby, 
2012; Moshfeghi et al, 2011), and so far, we 
have not seen any studies on semi-supervised 
learning on fine-grained emotion classification.  
2.2 News Reader?s Emotion Classification 
While comment writer?s emotion classification 
has been extensively studied, there are only a 
few studies on news reader?s emotion classifica-
tion from the NLP and related communities.  
Lin et al (2007) first describe the task of read-
er?s emotion classification on the news articles 
and then employ some standard machine learning 
approaches to train a classifier for determining 
the reader?s emotion towards a news. Their fur-
ther study, Lin et al (2008) exploit more features 
and achieve a higher performance. 
Unlike all the studies mentioned above, our 
study is the first attempt on exploring the rela-
tionship between comment writer?s emotion 
classification and news reader?s emotion classifi-
cation.  
3 Relationship between News Reader?s 
and Comment Writer?s Emotions 
To investigate the relationship between news 
reader?s and comment writer?s emotions, we col-
lect a corpus of Chinese news articles and their 
corresponding comments from Yahoo! Kimo 
News (http://tw.news.yahoo.com), where each 
news article is voted with emotion tags from 
eight categories: happy, sad, angry, meaningless, 
boring, heartwarming, worried, and useful. 
These emotion tags on each news are selected by 
the readers of the news. Note that because the 
categories of ?useful? and ?meaningless? are not 
real emotion categories, we ignore them in our 
study. Same as previous studies of Lin et al 
(2007) and Lin et al (2008), we consider the 
voted emotions as reader?s emotions on the news, 
i.e., the news reader?s emotions. We only select 
the news articles with a dominant emotion (pos-
sessing more than 50% votes) in our data. Be-
sides, as we attempt to consider the comment 
writer?s emotions, the news articles without any 
comments are filtered. 
As a result, we obtain a corpus of 3495 news 
articles together with their comments and the 
numbers of the articles of happy, sad, angry, 
boring, heartwarming, and worried are 1405, 
230, 1673, 75, 92 and 20 respectively. For 
coarse-grained categories, happy and heartwarm-
ing are merged into the positive category while 
512
sad, angry, boring and worried are merged into 
the negative category. 
Besides the tags of the reader?s emotions, each 
news article is followed by some comments, 
which can be seen as a reflection of the writer?s 
emotions (Averagely, each news is followed by 
15 comments). In order to know the exact rela-
tionship between these two kinds of emotions, 
we select 20 news from each category and ask 
two human annotators, named A and B, to manu-
ally annotate the writer?s emotion (single-label) 
according to the comments of each news. Table 1 
reports the agreement on annotators and emo-
tions, measured with Cohen?s kappa (?) value 
(Cohen, 1960). 
 ?  Value 
(Fine-grained 
emotions) 
? Value 
(Coarse-grained 
emotions) 
Annotators 0.566 0.742 
Emotions 0.504 0.756 
Table 1: Agreement on annotators and emotions 
 
Agreement between two annotators: The 
annotation agreement between the two annota-
tors is 0.566 on the fine-grained emotion catego-
ries and 0.742 on the coarse-grained emotion 
categories.  
Agreement between news reader?s and 
comment writer?s emotions: We compare the 
news reader?s emotion (automatically extracted 
from the web page) and the comment writer?s 
emotion (manually annotated by annotator A). 
The annotation agreement between the two kinds 
of emotions is 0.504 on the fine-grained emotion 
categories and 0.756 on the coarse-grained emo-
tion categories. From the results, we can see that 
the agreement on the fine-grained emotions is a 
bit low while the agreement between the coarse-
grained emotions, i.e., positive and negative, is 
very high. We find that although some fine-
grained emotions of the comments are not con-
sistent with the dominant emotion of the news, 
they belong to the same coarse-grained category.  
In a word, the agreement between news read-
er?s and comment writer?s emotions on the 
coarse-grained emotions is very high, even high-
er than the agreement between the two annota-
tors (0.754 vs. 0.742).  
In the following, we focus on the coarse-
grained emotions in emotion classification. 
4 Joint Modeling of News Reader?s and 
Comment Writer?s Emotions 
Given the importance of both news reader?s and 
comment writer?s emotion classification as de-
scribed in Introduction and the close relationship 
between news reader?s and comment writer?s 
emotions as described in last section, we system-
atically explore their joint modeling on the two 
kinds of emotion classification. 
In semi-supervised learning, the unlabeled da-
ta is exploited to improve the models with a 
small amount of the labeled data. In our ap-
proach, we consider the news text and the com-
ment text as two different views to express the 
news or comment emotion and build the two 
classifiers 
NC  and CC . Given the two-view clas-
sifiers, we perform co-training for semi-
supervised emotion classification, as shown in 
Figure 2, on both news reader?s and comment 
writer?s emotion classification. 
 
 
Input:   
NewsL  the labeled data on the news 
CommentL the labeled data  on the comments 
NewsU the unlabeled data  on the news  
CommentU  the labeled data  on the comments 
Output: 
NewsL New labeled data on the news 
CommentL  New labeled data on the comments 
 
Procedure: 
 
Loop for N iterations until
NewsU ??  or CommentU ??  
(1). Learn classifier 
NC  with NewsL  
(2). Use 
NC  to label the samples from NewsU   
(3). Choose 
1n  positive and 1n negative news 1N  
most confidently predicted by 
NC  
(4). Choose corresponding comments 
1M (the 
comments of the news in 
1N ) 
(5). Learn classifier 
CC  with CommentL  
(6). Use 
CC  to label the samples from CommentU   
(7). Choose 
2n  positive and 2n negative comments 
2M  most confidently predicted by CC  
(8). Choose corresponding comments 
2N (the news 
of the comments in 
2M ) 
(9). 
1 2News NewsL L N N? ? ?  
1 2Comment CommentL L M M? ? ? 
(10). 
1 2News NewsU U N N? ? ?
1 2Comment CommentU U M M? ? ? 
 
Figure 2: Co-training algorithm for semi-
supervised emotion classification 
513
5 Experimentation 
5.1 Experimental Settings 
Data Setting: The data set includes 3495 news 
articles (1572 positive and 1923 negative) and 
their comments as described in Section 3. Alt-
hough the emotions of the comments are not giv-
en in the website, we just set their coarse-grained 
emotion categories the same as the emotions of 
their source news due to their close relationship, 
as described in Section 3. To make the data bal-
anced, we randomly select 1500 positive and 
1500 negative news with their comments for the 
empirical study. Among them, we randomly se-
lect 400 news with their comments as the test 
data. 
Features: Each news or comment text is treat-
ed as a bag-of-words and transformed into a bi-
nary vector encoding the presence or absence of 
word unigrams. 
Classification algorithm: the maximum en-
tropy (ME) classifier implemented with the pub-
lic tool, Mallet Toolkits*. 
5.2 Experimental Results 
News reader?s emotion classifier: The classifier 
trained with the news text. 
Comment writer?s emotion classifier: The 
classifier trained with the comment text. 
Figure 3 demonstrates the performances of the 
news reader?s and comment writer?s emotion 
classifiers trained with the 10 and 50 initial la-
beled samples plus automatically labeled data 
from co-training. Here, in each iteration, we pick 
2 positive and 2 negative most confident samples, 
i.e, 
1 2 2n n? ? . From this figure, we can see that 
our co-training algorithm is very effective: using 
only 10 labeled samples in each category 
achieves a very promising performance on either 
news reader?s or comment writer?s emotion clas-
sification. Especially, the performance when us-
ing only 10 labeled samples is comparable to that 
when using more than 1200 labeled samples on 
supervised learning of comment writer?s emotion 
classification. 
   For comparison, we also implement a self-
training algorithm for the news reader?s and 
comment writer?s emotion classifiers, each of 
which automatically labels the samples from the 
unlabeled data independently. For news reader?s 
emotion classification, the performances of self-
training are 0.783 and 0.79 when 10 and 50 ini-
                                                 
* http://mallet.cs.umass.edu/ 
tial labeled samples are used. For comment writ-
er?s emotion classification, the performances of 
self-training are 0.505 and 0.508. These results 
are much lower than the performances of our co-
training approach, especially on the comment 
writer?s emotion classification i.e., 0.505 and 
0.508 vs. 0.783 and 0.805. 
 
10 Initial Labeled Samples
0.5
0.6
0.7
0.8
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data
A
c
c
u
r
a
c
y
 
50 Initial Labeled Samples
0.65
0.7
0.75
0.8
0.85
0.9
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data data
A
c
c
u
r
a
c
y
The news reader's emotion
classifier (Co-training)
The comment writer's emotion
classifier (Co-training)
 Figure 3: Performances of the news reader?s and 
comment writer?s emotion classifiers using the 
co-training algorithm 
6 Conclusion 
In this paper, we focus on two popular emotion 
classification tasks, i.e., reader?s emotion classi-
fication on the news and writer?s emotion classi-
fication on the comments. From the data analysis, 
we find that the news reader?s and comment 
writer?s emotions are highly consistent to each 
other in terms of the coarse-grained emotion cat-
egories, positive and negative. On the basis, we 
propose a co-training approach to perform semi-
supervised learning on the two tasks. Evaluation 
shows that the co-training approach is so effec-
tive that using only 10 labeled samples achieves 
nice performances on both news reader?s and 
comment writer?s emotion classification.  
514
Acknowledgments 
This research work has been partially supported 
by two NSFC grants, No.61003155, and 
No.61273320, one National High-tech Research 
and Development Program of China 
No.2012AA011102, one General Research Fund 
(GRF) sponsored by the Research Grants Coun-
cil of Hong Kong No.543810, the NSF grant of 
Zhejiang Province No.Z1110551, and one pro-
ject supported by Zhejiang Provin-cial Natural 
Science Foundation of China, No.Y13F020030.  
References  
Alm C., D. Roth and R. Sproat. 2005. Emotions from 
Text: Machine Learning for Text-based Emotion 
Prediction. In Proceedings of EMNLP-05, pp.579-
586. 
Aman S. and S. Szpakowicz. 2008. Using Roget?s 
Thesaurus for Fine-grained Emotion Recognition. 
In Proceedings of IJCNLP-08, pp.312-318. 
Chen Y., S. Lee, S. Li and C. Huang. 2010. Emotion 
Cause Detection with Linguistic Constructions. In 
Proceeding of COLING-10, pp.179-187. 
Cohen J. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Meas-
urement, 20(1):37?46. 
 Cui H., V. Mittal and M. Datar. 2006. Comparative 
Experiments on Sentiment Classification for 
Online Product Comments. In Proceedings of 
AAAI-06, pp.1265-1270. 
Das D. and S. Bandyopadhyay. 2009. Word to Sen-
tence Level Emotion Tagging for Bengali Blogs. In 
Proceedings of ACL-09, pp.149-152. 
Dasgupta S. and V. Ng. 2009. Mine the Easy, Classify 
the Hard: A Semi-Supervised Approach to Auto-
matic Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09,  pp.701-709, 2009. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Fumera G. and F. Roli. 2005. A Theoretical and Ex-
perimental Analysis of Linear Combiners for Mul-
tiple Classifier Systems. IEEE Trans. PAMI, vol.27, 
pp.942?956, 2005. 
Li S., Z. Wang, G. Zhou and S. Lee. 2011. Semi-
supervised Learning for Imbalanced Sentiment 
Classification. In Proceeding of IJCAI-11,  pp.826-
1831. 
Li S., C. Huang, G. Zhou and S. Lee.  2010. Employ-
ing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification. In Pro-
ceedings of ACL-10,  pp.414-423. 
Lin K., C. Yang and H. Chen. 2007. What Emotions 
do News Articles Trigger in Their Readers? In 
Proceeding of SIGIR-07, poster, pp.733-734. 
Lin K., C. Yang and H. Chen. 2008. Emotion Classi-
fication of Online News Articles from the Reader?s 
Perspective. In Proceeding of the International 
Conference on Web Intelligence and Intelligent 
Agent Technology, pp.220-226. 
 Liu B. 2012. Sentiment Analysis and Opinion Mining 
(Introduction and Survey). Morgan & Claypool 
Publishers, May 2012. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Moshfeghi Y., B. Piwowarski and J. Jose. 2011. Han-
dling Data Sparsity in Collaborative Filtering using 
Emotion and Semantic Based Features. In Proceed-
ings of SIGIR-11, pp.625-634. 
Pang B. and L. Lee. 2008. Opinion Mining and 
Sentiment Analysis: Foundations and Trends. 
Information Retrieval, vol.2(12), 1-135. 
Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of EMNLP-
02, pp.79-86. 
Purver M. and S. Battersby. 2012. Experimenting 
with Distant Supervision for Emotion Classifica-
tion. In Proceedings of EACL-12, pp.482-491. 
Quan C. and F. Ren. 2009. Construction of a Blog 
Emotion Corpus for Chinese Emotional Expression 
Analysis. In Proceedings of EMNLP-09, pp.1446-
1454. 
Riloff E., S. Patwardhan and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06, pp.440-448. 
Turney P. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of comments. In Proceedings of 
ACL-02, pp.417-424.  
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Volkova S., W. Dolan and T. Wilson. 2012. CLex: A 
Lexicon for Exploring Color, Concept and Emo-
tion Associations in Language. In Proceedings of 
EACL-12, pp.306-314. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433. 
Xu G., X. Meng and H. Wang. 2010. Build Chinese 
Emotion Lexicons Using A Graph-based 
Algorithm and Multiple Resources. In Proceeding 
of COLING-10, pp.1209-1217. 
515
