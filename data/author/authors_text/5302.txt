Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 351?359, Prague, June 2007. c?2007 Association for Computational Linguistics
Topic Segmentation with Hybrid Document Indexing
Irina Matveeva
Department of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
We present a domain-independent unsuper-
vised topic segmentation approach based on
hybrid document indexing. Lexical chains
have been successfully employed to evalu-
ate lexical cohesion of text segments and to
predict topic boundaries. Our approach is
based in the notion of semantic cohesion. It
uses spectral embedding to estimate seman-
tic association between content nouns over a
span of multiple text segments. Our method
significantly outperforms the baseline on the
topic segmentation task and achieves perfor-
mance comparable to state-of-the-art meth-
ods that incorporate domain specific infor-
mation.
1 Introduction
The goal of topic segmentation is to discover story
boundaries in the stream of text or audio recordings.
Story is broadly defined as segment of text contain-
ing topically related sentences. In particular, the
task may require segmenting a stream of broadcast
news, addressed by the Topic Detection and Track-
ing (TDT) evaluation project (Wayne, 2000; Allan,
2002). In this case topically related sentences belong
to the same news story. While we are considering
TDT data sets in this paper, we would like to pose
the problem more broadly and consider a domain-
independent approach to topic segmentation.
Previous research on topic segmentation has
shown that lexical coherence is a reliable indicator
of topical relatedness. Therefore, many approaches
have concentrated on different ways of estimating
lexical coherence of text segments, such as seman-
tic similarity between words (Kozima, 1993), sim-
ilarity between blocks of text (Hearst, 1994), and
adaptive language models (Beeferman et al, 1999).
These approaches use word repetitions to evaluate
coherence. Since the sentences covering the same
story represent a coherent discourse segment, they
typically contain the same or related words. Re-
peated words build lexical chains that are conse-
quently used to estimate lexical coherence. This can
be done either by analyzing the number of overlap-
ping lexical chains (Hearst, 1994) or by building a
short-range and long-range language model (Beefer-
man et al, 1999). More recently, topic segmentation
with lexical chains has been successfully applied to
segmentation of news stories, multi-party conversa-
tion and audio recordings (Galley et al, 2003).
When the task is to segment long streams of text
containing stories which may continue at a later
point in time, for example developing news stories,
building of lexical chains becomes intricate. In ad-
dition, the word repetitions do not account for syn-
onymy and semantic relatedness between words and
therefore may not be able to discover coherence of
segments with little word overlap.
Our approach aims at discovering semantic relat-
edness beyond word repetition. It is based on the
notion of semantic cohesion rather than lexical cohe-
sion. We propose to use a similarity metric between
segments of text that takes into account semantic as-
sociations between words spanning a number of seg-
ments. This method approximates lexical chains by
averaging the similarity to a number of previous text
351
segments and accounts for synonymy by using a hy-
brid document indexing scheme. Our text segmen-
tation experiments show a significant performance
improvement over the baseline.
The rest of the paper is organized as follows. Sec-
tion 2 discusses hybrid indexing. Section 3 describes
our segmentation algorithm. Section 5 reports the
experimental results. We conclude in section 6.
2 Hybrid Document Indexing
For the topic segmentation task we would like to de-
fine a similarity measure that accounts for synonymy
and semantic association between words. This simi-
larity measure will be used to evaluate semantic co-
hesion between text units and the decrease in seman-
tic cohesion will be used as an indicator of a story
boundary. First, we develop a document representa-
tion which supports this similarity measure.
Capturing semantic relations between words in
a document representation is difficult. Different
approaches tried to overcome the term indepen-
dence assumption of the bag-of-words representa-
tion (Salton and McGill, 1983) by using distribu-
tional term clusters (Slonim and Tishby, 2000) and
expanding the document vectors with synonyms, see
(Levow et al, 2005). Since content words can be
combined into semantic classes there has been a
considerable interest in low-dimensional representa-
tions. Latent Semantic Analysis (LSA) (Deerwester
et al, 1990) is one of the best known dimension-
ality reduction algorithms. In the LSA space doc-
uments are indexed with latent semantic concepts.
LSA maps all words to low dimensional vectors.
However, the notion of semantic relatedness is de-
fined differently for subsets of the vocabulary. In ad-
dition, the numerical information, abbreviations and
the documents? style may be very good indicators of
their topic. However, this information is no longer
available after the dimensionality reduction.
We use a hybrid approach to document indexing
to address these issues. We keep the notion of la-
tent semantic concepts and also try to preserve the
specifics of the document collection. Therefore, we
divide the vocabulary into two sets: nouns and the
rest of the vocabulary. The set of nouns does not
include proper nouns. We use a method of spec-
tral embedding, as described below and compute a
low-dimensional representation for documents using
only the nouns. We also compute a tf-idf represen-
tation for documents using the other set of words.
Since we can treat each latent semantic concept in
the low-dimensional representation as part of the vo-
cabulary, we combine the two vector representations
for each document by concatenating them.
2.1 Spectral Embedding
A vector space representation for documents and
sentences is convenient and makes the similarity
metrics such as cosine and distance readily avail-
able. However, those metrics will not work if they
don?t have a meaningful linguistic interpretation.
Spectral methods comprise a family of algo-
rithms that embed terms and documents in a low-
dimensional vector space. These methods use pair-
wise relations between the data points encoded in a
similarity matrix. The main step is to find an embed-
ding for the data that preserves the original similari-
ties.
GLSA We use Generalized Latent Semantic Anal-
ysis (GLSA) (Matveeva et al, 2005) to compute
spectral embedding for nouns. GLSA computes
term vectors and since we would like to use spectral
embedding for nouns, it is well-suited for our ap-
proach. GLSA extends the ideas of LSA by defining
different ways to obtain the similarities matrix and
has been shown to outperform LSA on a number of
applications (Matveeva and Levow, 2006).
GLSA begins with a matrix of pair-wise term sim-
ilarities S, computes its eigenvectors U and uses the
first k of them to represent terms and documents, for
details see (Matveeva et al, 2005). The justifica-
tion for this approach is the theorem by Eckart and
Young (Golub and Reinsch, 1971) stating that inner
product similarities between the term vectors based
on the eigenvectors of S represent the best element-
wise approximation to the entries in S. In other
words, the inner product similarity in the GLSA
space preserves the semantic similarities in S.
Since our representation will try to preserve se-
mantic similarities in S it is important to have a ma-
trix of similarities which is linguistically motivated.
352
Word Nearest Neighbors in GLSA Space
witness testify prosecutor trial testimony juror eyewitness
finance fund bank investment economy crisis category
broadcast television TV satellite ABC CBS radio
hearing hearing judge voice chatter sound appeal
surprise announcement disappointment stunning shock reaction astonishment
rest stay remain keep leave portion economy
Table 1: Words? nearest neighbors in the GLSA semantic space.
2.2 Distributional Term Similarity
PMI Following (Turney, 2001; Matveeva et al,
2005), we use point-wise mutual information (PMI)
to compute the matrix S. PMI between random vari-
ables representing the words wi and wj is computed
as
PMI(wi, wj) = log
P (Wi = 1,Wj = 1)
P (Wi = 1)P (Wj = 1)
. (1)
Thus, for GLSA, S(wi, wj) = PMI(wi, wj).
Co-occurrence Proximity An advantage of PMI
is the notion of proximity. The co-occurrence statis-
tics for PMI are typically computed using a sliding
window. Thus, PMI will be large only for words that
co-occur within a small context of fixed size.
Semantic Association vs. Synonymy Although
GLSA was successfully applied to synonymy in-
duction (Matveeva et al, 2005), we would like to
point out that the GLSA discovers semantic associ-
ation in a broad sense. Table 1 shows a few words
from the TDT2 corpus and their nearest neighbors
in the GLSA space. We can see that for ?witness?,
?finance? and ?broadcast? words are grouped into
corresponding semantic classes. The nearest neigh-
bors for ?hearing? and ?stay? represent their differ-
ent senses. Interestingly, even for the abstract noun
?surprise? the nearest neighbors are meaningful.
2.3 Document Indexing
We have two sets of the vocabulary terms: a set of
nouns, N , and the other words, T . We compute tf-idf
document vectors indexed with the words in T :
~di = (?i(w1), ?i(w2), ..., ?i(w|T |)), (2)
where ?i(wt) = tf(wt, di) ? idf(wt).
We also compute a k-dimensional representation
with latent concepts ci as a weighted linear combi-
nation of GLSA term vectors ~wt:
~di = (c1, ..., ck) =
?
t=1:|N |
?i(wt) ? ~wt, (3)
We concatenate these two representations to gener-
ate a hybrid indexing of documents:
~di = (?i(w1), ..., ?i(w|T |), c1, ...ck) (4)
In our experiments, we compute document
and sentence representation using three indexing
schemes: the tf-idf baseline, the GLSA represen-
tation and the hybrid indexing. The GLSA index-
ing computes term vectors for all vocabulary words;
document and sentence vectors are generated as lin-
ear combinations of term vectors, as shown above.
2.4 Document similarity
One can define document similarity at different lev-
els of semantic content. Documents can be similar
because they discuss the same people or events and
because they discuss related subjects and contain se-
mantically related words. Hybrid Indexing allows
us to combine both definitions of similarity. Each
representation supports a different similarity mea-
sure. tf-idf uses term-matching, the GLSA represen-
tation uses semantic association in the latent seman-
tic space computed for all words, and hybrid index-
ing uses a combination of both: term-matching for
named entities and content words other than nouns
combined with semantic association for nouns.
In the GLSA space, the inner product between
document vectors contains all pair-wise inner prod-
uct between their words, which allows one to detect
semantic similarity beyond term matching:
?~di, ~dj? =
?
w?di
?
v?dj
?i(w)?j(v)?~w,~v? (5)
353
If documents contain words which are different but
semantically related, the inner product between the
term vectors will contribute to the document similar-
ity, as illustrated with an example in section 5.
When we compare two documents indexed with
the hybrid indexing scheme, we compute a combi-
nation of similarity measures:
?~di, ~dj? =
?
nk?di
?
nm?dj
?i(nk)?j(nm)? ~nk, ~nm?+
?
t?T
?i(t) ? ?j(t).
(6)
Document similarity contains semantic association
between all pairs of nouns and uses term-matching
for the rest of the vocabulary.
3 Topic Segmentation with Semantic
Cohesion
Our approach to topic segmentation is based on
semantic cohesion supported by the hybrid index-
ing. Topic segmentation approaches use either sen-
tences (Galley et al, 2003) or blocks of words as
text units (Hearst, 1994). We used both variants
in our experiments. When using blocks, we com-
puted blocks of a fixed size (typically 20 words) slid-
ing over the documents in a fixed step size (10 or
5 words). The algorithm predicts a story boundary
when the semantic cohesion between two consecu-
tive units drops. Blocks can cross story boundaries,
thus many predicted boundaries will be displaced
with respect to the actual boundary.
Averaged similarity In our preliminary experi-
ments we used the largest difference in score to pre-
dict story boundary, following the TextTiling ap-
proach (Hearst, 1994). We found, however, that in
our document collection the word overlap between
sentences was often not large and pair-wise similar-
ity could drop to zero even for sentences within the
same story, as will be illustrated below. We could
not obtain satisfactory results with this approach.
Therefore, we used the average similarity by us-
ing a history of fixed size n. The semantic cohesion
score was computed for the position between two
text units, ti and tj as follows:
score(ti, tj) =
1
n
n?1
?
k=0
?ti?k, tj? (7)
Our approach predicts story boundaries at the min-
ima of the semantic cohesion score.
Approximating Lexical Chains One of the mo-
tivations for our cohesion score is that it approxi-
mates lexical chains, as for example in (Galley et al,
2003). Galley et al (Galley et al, 2003) define lex-
ical chains R1, .., RN by considering repetitions of
terms t1, .., tN and assigning larger weights to short
and compact chains. Then the lexical cohesion score
between two text units ti and tj is based on the num-
ber of chains that overlap both of them:
score(ti, tj) =
N
?
k=1
wk(ti)wk(tj), (8)
where wk(ti) = score(Rj) if the chain Rj over-
laps ti and zero otherwise. Our cohesion score takes
into account only the chains for words that occur in
tj and have another occurrence within n previous
sentences. Due to this simplification, we compute
the score based on inner products. Once we make
the transition to inner products, we can use hybrid
indexing and compute semantic cohesion score be-
yond term repetition.
4 Related Approaches
We compare our approach to the LCseg algorithm
which uses lexical chains to estimate topic bound-
aries (Galley et al, 2003). Hybrid indexing allows
us to compute semantic cohesion score rather than
the lexical cohesion score based on word repetitions.
Choi at al. used LSA for segmentation (Choi et
al., 2001). LSA (Deerwester et al, 1990) is a spe-
cial case of spectral embedding and Choi at al. (Choi
et al, 2001) used all vocabulary words to com-
pute low-dimensional document vectors. We use
GLSA (Matveeva et al, 2005) because it computes
term vectors as opposed to the dual document-term
representation with LSA and uses a different ma-
trix of pair-wise similarities. Furthermore, Choi
at al. (Choi et al, 2001) used clustering to predict
boundaries whereas we used the average similarity
scores.
354
s1: The Cuban news agency Prensa Latina called Clinton ?s announcement Friday that Cubans picked up
at sea will be taken to Guantanamo Bay naval base a ? new and dangerous element ? in U S immigration policy.
s2: The Cuban government has not yet publicly reacted to Clinton ?s announcement that Cuban rafters
will be turned away from the United States and taken to the U S base on the southeast tip of Cuba.
s5: The arrival of Cuban emigrants could be an ? extraordinary aggravation ? to the situation , Prensa Latina said.
s6: It noted that Cuba had already denounced the use of the base as a camp for Haitian refugees.
whom it had for many years encouraged to come to the United States.
s8: Cuba considers the land at the naval base , leased to the United States at the turn of the century,
to be illegally occupied.
s10: General Motors Corp said Friday it was recalling 5,600 1993-94 model Chevrolet Lumina, Pontiac
Trans Sport and Oldsmobile Silhouette minivans equipped with a power sliding door and built-in child seats.
s14: If this occurs , the shoulder belt may not properly retract , the carmaker said.
s15: GM is the only company to offer the power-sliding door.
s16: The company said it was not aware of any accidents or injuries related to the defect.
s17: To correct the problem , GM said dealers will install a modified interior trim piece that will reroute the seat belt.
Table 2: TDT. The first 17 sentences in the first file.
Existing approaches to hybrid indexing used dif-
ferent weights for proper nouns, nouns phrase heads
and use WordNet synonyms to expand the docu-
ments, for example (Hatzivassiloglou et al, 2000;
Hatzivassiloglou et al, 2001). Our approach does
not require linguistic resources and learning the
weights. The semantic associations between nouns
are estimated using spectral embedding.
5 Experiments
5.1 Data
The first TDT collection is part of the LCseg
toolkit1 (Galley et al, 2003) and we used it to com-
pare our approach to LCseg. We used the part of this
collection with 50 files with 22 documents each.
We also used the TDT2 collection2 of news arti-
cles from six news agencies in 1998. We used only
9,738 documents that are assigned to one topic and
have length more than 50 words. We used the Lemur
toolkit3 with stemming and stop words list for the
tf-idf indexing; we used Bikel?s parser4 to obtain
the POS-tags and select nouns; we used the PLA-
PACK package (Bientinesi et al, 2003) to compute
the eigenvalue decomposition.
1http://www1.cs.columbia.edu/ galley/tools.html
2http://nist.gov/speech/tests/tdt/tdt98/
3http://www.lemurproject.org/
4http://www.cis.upenn.edu/ dbikel/software.html
Evaluation For the TDT data we use the error
metric pk (Beeferman et al, 1999) and WindowD-
iff (Pevzner and Hearst, 2002) which are imple-
mented in the LCseg toolkit. We also used the
TDT cost metric Cseg5, with the default parameters
P(seg)=0.3, Cmiss=1, Cfa=0.3 and distance of 50
words. All these measures look at two units (words
or sentences) N units apart and evaluate how well
the algorithm can predict whether there is a bound-
ary between them or not. Lower values mean better
performance for all measures.
Global vs. Local GLSA Similarity To obtain the
PMI values we used the TDT2 collection, denoted as
GLSAlocal. Since co-occurrence statistics based on
larger collections give a better approximation to lin-
guistic similarities, we also used 700,000 documents
from the English GigaWord collection, denoted as
GLSA. We used a window of size 8.
5.2 Topic Segmentation
The first set of experiments was designed to evaluate
the advantage of the GLSA representation over the
baseline. We compare our approach to the LCseg
algorithm (Galley et al, 2003) and use sentences as
segmentation unit. To avoid the issue of parameters
setting when the number of boundaries is not known,
we provide each algorithm with the actual numbers
5www.nist.gov/speech/tests/tdt/tdt98/doc/
tdt2.eval.plan.98.v3.7.ps
355
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 2 4 6 8 10 12 14 16 18 20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
GLSA
tfidf
Figure 1: TDT. Pair-wise sentence similarities for tf-idf (left), GLSA (middle); x-axis shows story bound-
aries. Details for the first 20 sentences, table 2 (right).
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
10 21 27 45 52 65 73 89 99
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 2: TDT. Pair-wise sentence similarities for tf-idf (left), GLSA (middle) averaged over 10 preceeding
sentences; LCseg lexical cohesion scores (right). X-axis shows story boundaries.
of boundaries.
TDT We use the LCseg approach and our ap-
proach with the baseline tf-idf representation and the
GLSA representation to segment this corpus. Ta-
ble 2 shows a few sentences. Many content words
are repeated, so the lexical chains is definitely a
sound approach. As shown in Table 2, in the first
story the word ?Cuba? or ?Cuban? is repeated in ev-
ery sentence thus generating a lexical chain. On the
topic boundary, the word overlap between sentences
is very small. At the same time, the repetition of
words may also be interrupted within a story: sen-
tence 5, 6 and sentences 14, 15, 16 have little word
overlap. LCseg deals with this by defining several
parameters to control chain length and gaps. This
simple example illustrates the potential benefit of se-
mantic cohesion. Table 2 shows that ?General Mo-
tors? or ?GM? are not repeated in every sentence of
the second story. However, ?GM?, ?carmaker? and
?company? are semantically related. Making this
information available to the segmentation algorithm
allows it to establish a connection between each sen-
tence of the second story.
We computed pair-wise sentence similarities be-
tween pairs of consecutive sentences in the tf-idf and
GLSA representations. Figure 1 shows the similar-
ity values plotted for each sentence break. The pair-
wise similarities based on term-matching are very
spiky and there are many zeros within the story. The
GLSA-based similarity makes the dips in the simi-
larities at the boundaries more prominent. The last
plot gives the details for the sentences in table 2.
In the tf-idf representation sentences without word
overlap receive zero similarity but the GLSA repre-
sentation is able to use the semantic association be-
tween between ?emigrants? and ?refugees? for sen-
tences 5 and 6, and also the semantic association be-
tween ?carmaker? and ?company? for sentences 14
356
Measure tf-idf GLSA LCseg
Pmiss 0.29 0.19 N/A
Pfa 0.14 0.09 N/A
Cseg 0.18 0.08 N/A
pk 0.24 0.17 0.07
wd 0.27 0.21 0.10
Table 3: TDT segmentation results.
and 15.
This effect increases as we use the semantic cohe-
sion score as in equation 7. Figure 2 shows the simi-
larity values for tf-idf and GLSA and also the lexical
cohesion scores computed by LCseg. The GLSA-
based similarities are not quite as smooth as the LC-
seg scores, but they correctly discover the bound-
aries. LCseg parameters are fine-tuned for this doc-
ument collection. We used a general TDT2 GLSA
representation for this collection, and the only seg-
mentation parameter we used is to avoid placing
next boundary within n=3 sentences of the previ-
ous one. For this reason the predicted boundary may
be one sentence off the actual boundary. These re-
sults are summarized in Table 3. The GLSA repre-
sentation performs significantly better than the tf-idf
baseline. Its pk and WindowDiff scores with default
parameters for LCseg are worse than for LCseg. We
attribute it to the fact that we did not fine-tuned our
method to this collection and that boundaries are of-
ten placed one position off the actual boundary.
TDT2 For this collection we used three different
indexing schemes: the tf-idf baseline, the GLSA rep-
resentation and the hybrid indexing. Each represen-
tation supports a different similarity measure. Our
TDT experiments showed that the semantic cohe-
sion score based on the GLSA representation im-
proves the segmentation results. The variant of
the TDT corpus we used is rather small and well-
balanced, see (Galley et al, 2003) for details. In
the second phase of experiments we evaluate our ap-
proach on the larger TDT2 corpus. The experiments
were designed to address the following issues:
? performance comparison between GLSA and
Hybrid indexing representations. As men-
tioned before, GLSA embeds all words in
a low-dimensional space. Whereas semantic
#b known
Method Pmiss Pfa Cseg
tf-idf 0.52 0.14 0.19
GLSA 0.4 0.1 0.14
GLSA local 0.44 0.12 0.16
Hybrid 0.34 0.10 0.12
Hybrid local 0.38 0.09 0.13
LCseg 0.80 0.19 0.28
#b unknown
Method Pmiss Pfa Cseg
tf-idf 0.42 0.2 0.17
GLSA 0.37 0.13 0.14
GLSA local 0.35 0.19 0.14
Hybrid 0.26 0.16 0.11
Hybrid local 0.27 0.18 0.12
Table 4: TDT2 segmentation results. Sliding blocks
with size 20 and stepsize 10; similarity averaged
over 10 preceeding blocks.
classes for nouns have theoretical linguistic jus-
tification, it is harder to motivate a latent space
representation for example for proper nouns.
Therefore, we want to evaluate the advantage
of using spectral embedding only for nouns.
? collection dependence of similarities. The sim-
ilarity matrix S is computed using the TDT2
corpus (GLSAlocal) and using the larger Giga-
Word corpus. The larger corpus provides more
reliable co-occurrence statistics. On the other
hand, word distribution is different from that
in the TDT2 corpus. We wanted to evaluate
whether semantic similarities are collection in-
dependent.
Table 4 shows the performance evaluation. We show
the results computed using blocks containing 20
words (after preprocessing) with step size 10. We
tried other parameter values but did not achieve bet-
ter performance, which is consistent with other re-
search (Hearst, 1994; Galley et al, 2003). We show
the results for two settings: predict a known num-
ber of boundaries, and predict boundaries using a
threshold. In our experiments we used the average
of the smallest N scores as threshold, N = 4000
showing best results.
357
The spectral embedding based representations
(GLSA, Hybrid) significantly outperform the base-
line. This confirms the advantage of the semantic
cohesion score vs. term-matching. Hybrid index-
ing outperforms the GLSA representation support-
ing our intuition that semantic association is best de-
fined for nouns.
We used the GigaWord corpus to obtain the pair-
wise word associations for the GLSA and Hybrid
representations. We also computed GLSAlocal and
Hybridlocal using the TDT2 corpus to obtain the
pair-wise word associations. The co-occurrence
statistics based on the GigaWord corpus provide
more reliable estimations of semantic association
despite the difference in term distribution. The dif-
ference is larger for the GLSA case when we com-
pute the embedding for all words, GLSA performs
better than GLSAlocal. Hybridlocal performs only
slightly worse than Hybrid. This seems to support
the claim that semantic associations between nouns
are largely collection independent. On the other
hand, semantic associations for proper names are
collection dependent at least because the collections
are static but the semantic relations of proper names
may change over time. The semantic space for a
name of a president, for example, is different for the
period of time of his presidency and for the time be-
fore and after that.
Disappointingly, we could not achieve good re-
sults with LCseg. It tends to split stories into short
paragraphs. Hybrid indexing could achieve results
comparable to state-of-the art approaches, see (Fis-
cus et al, 1998) for an overview.
6 Conclusion and Future Work
We presented a topic segmentation approach based
on semantic cohesion scores. Our approach is do-
main independent, does not require training or use
of lexical resources. The scores are computed based
on the hybrid document indexing which uses spec-
tral embedding in the space of latent concepts for
nouns and keeps proper nouns and other specifics of
the documents collections unchanged. We approxi-
mate the lexical chains approach by simplifying the
definition of a chain which allows us to use inner
products as basis for the similarity score. The simi-
larity score takes into account semantic relations be-
tween nouns beyond term matching. This semantic
cohesion approach showed good results on the topic
segmentation task.
We intend to extend the hybrid indexing approach
by considering more vocabulary subsets. Syntactic
similarity is more appropriate for verbs, for exam-
ple, than co-occurrence. As a next step, we intend to
embed verbs using syntactic similarity. It would also
be interesting to use lexical chains for proper names
and learn the weights for different similarity scores.
References
J. Allan, editor. 2002. Topic Detection and Tracking:
Event-based Information Organization. Kluwer Aca-
demic Publishers.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Paolo Bientinesi, Inderjit S. Dhilon, and Robert A. van de
Geijn. 2003. A parallel eigensolver for dense sym-
metric matrices based on multiple relatively robust
representations. UT CS Technical Report TR-03-26.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text seg-
mentation. In Proceedings of EMNLP, pages 109?
117.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
J. G. Fiscus, George Doddington, John S. Garofolo, and
Alvin Martin. 1998. NIST?s 1998 topic detection and
tracking evaluation (tdt2). In Proceedings of NIST?s
1998 Topic Detection and Tracking Evaluation.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
G. Golub and C. Reinsch. 1971. Handbook for Ma-
trix Computation II, Linear Algebra. Springer-Verlag,
New York.
V. Hatzivassiloglou, Luis Gravano, and Ankineedu Mag-
anti. 2000. An investigation of linguistic features and
clustering algorithms for topical document clustering.
In Proceedings of SIGIR, pages 224?231.
V. Hatzivassiloglou, Regina Barzilay Min-Yen Kan Ju-
dith L. Klavans, Melissa L. Holcombe, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible
358
clustering tool for summarization. In Proceedings of
NAACL, pages 41?49.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9?16.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proceedings of ACL, pages
286?288.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management: Special Issue on Cross-language Infor-
mation Retrieval.
Irina Matveeva and Gina-Anne Levow. 2006. Graph-
based Generalized Latent Semantic Analysis for docu-
ment representation. In Proc. of the TextGraphs Work-
shop at HLT/NAACL.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized Latent Semantic
Analysis for term representation. In Proc. of RANLP.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Comput. Linguist., 28(1):19?36.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Research and Development in Infor-
mation Retrieval, pages 208?215.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
C. Wayne. 2000. Multilingual topic detection and track-
ing: Successful research enabled by corpora and eval-
uation. In Proceedings of Language Resources and
Evaluation Conference (LREC), pages 1487?1494.
359
Computing Term Translation Probabilities with Generalized Latent
Semantic Analysis
Irina Matveeva
Department of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
Term translation probabilities proved an
effective method of semantic smoothing in
the language modelling approach to infor-
mation retrieval tasks. In this paper, we
use Generalized Latent Semantic Analysis
to compute semantically motivated term
and document vectors. The normalized
cosine similarity between the term vec-
tors is used as term translation probabil-
ity in the language modelling framework.
Our experiments demonstrate that GLSA-
based term translation probabilities cap-
ture semantic relations between terms and
improve performance on document classi-
fication.
1 Introduction
Many recent applications such as document sum-
marization, passage retrieval and question answer-
ing require a detailed analysis of semantic rela-
tions between terms since often there is no large
context that could disambiguate words?s meaning.
Many approaches model the semantic similarity
between documents using the relations between
semantic classes of words, such as representing
dimensions of the document vectors with distri-
butional term clusters (Bekkerman et al, 2003)
and expanding the document and query vectors
with synonyms and related terms as discussed
in (Levow et al, 2005). They improve the per-
formance on average, but also introduce some in-
stability and thus increased variance (Levow et al,
2005).
The language modelling approach (Ponte and
Croft, 1998; Berger and Lafferty, 1999) proved
very effective for the information retrieval task.
Berger et. al (Berger and Lafferty, 1999) used
translation probabilities between terms to account
for synonymy and polysemy. However, their
model of such probabilities was computationally
demanding.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. Using a bag-of-words docu-
ment vectors (Salton and McGill, 1983), it com-
putes a dual representation for terms and docu-
ments in a lower dimensional space. The resulting
document vectors reside in the space of latent se-
mantic concepts which can be expressed using dif-
ferent words. The statistical analysis of the seman-
tic relatedness between terms is performed implic-
itly, in the course of a matrix decomposition.
In this project, we propose to use a combi-
nation of dimensionality reduction and language
modelling to compute the similarity between doc-
uments. We compute term vectors using the Gen-
eralized Latent Semantic Analysis (Matveeva et
al., 2005). This method uses co-occurrence based
measures of semantic similarity between terms
to compute low dimensional term vectors in the
space of latent semantic concepts. The normalized
cosine similarity between the term vectors is used
as term translation probability.
2 Term Translation Probabilities in
Language Modelling
The language modelling approach (Ponte and
Croft, 1998) proved very effective for the infor-
mation retrieval task. This method assumes that
every document defines a multinomial probabil-
ity distribution p(w|d) over the vocabulary space.
Thus, given a query q = (q1, ..., qm), the like-
lihood of the query is estimated using the docu-
ment?s distribution: p(q|d) = ?m1 p(qi|d), where
151
qi are query terms. Relevant documents maximize
p(d|q) ? p(q|d)p(d).
Many relevant documents may not contain the
same terms as the query. However, they may
contain terms that are semantically related to the
query terms and thus have high probability of
being ?translations?, i.e. re-formulations for the
query words.
Berger et. al (Berger and Lafferty, 1999) in-
troduced translation probabilities between words
into the document-to-query model as a way of se-
mantic smoothing of the conditional word proba-
bilities. Thus, they query-document similarity is
computed as
p(q|d) =
m
?
i
?
w?d
t(qi|w)p(w|d). (1)
Each document word w is a translation of a query
term qi with probability t(qi|w). This approach
showed improvements over the baseline language
modelling approach (Berger and Lafferty, 1999).
The estimation of the translation probabilities is,
however, a difficult task. Lafferty and Zhai used
a Markov chain on words and documents to es-
timate the translation probabilities (Lafferty and
Zhai, 2001). We use the Generalized Latent Se-
mantic Analysis to compute the translation proba-
bilities.
2.1 Document Similarity
We propose to use low dimensional term vectors
for inducing the translation probabilities between
terms. We postpone the discussion of how the term
vectors are computed to section 2.2. To evaluate
the validity of this approach, we applied it to doc-
ument classification.
We used two methods of computing the sim-
ilarity between documents. First, we computed
the language modelling score using term transla-
tion probabilities. Once the term vectors are com-
puted, the document vectors are generated as lin-
ear combinations of term vectors. Therefore, we
also used the cosine similarity between the docu-
ments to perform classificaiton.
We computed the language modelling score of
a test document d relative to a training document
di as
p(d|di) =
?
v?d
?
w?di
t(v|w)p(w|di). (2)
Appropriately normalized values of the cosine
similarity measure between pairs of term vectors
cos(~v, ~w) are used as the translation probability
between the corresponding terms t(v|w).
In addition, we used the cosine similarity be-
tween the document vectors
?~di, ~dj? =
?
w?di
?
v?dj
?diw ?
dj
v ?~w,~v?, (3)
where ?diw and ?
dj
v represent the weight of the
terms w and v with respect to the documents di
and dj , respectively.
In this case, the inner products between the term
vectors are also used to compute the similarity be-
tween the document vectors. Therefore, the cosine
similarity between the document vectors also de-
pends on the relatedness between pairs of terms.
We compare these two document similarity
scores to the cosine similarity between bag-of-
word document vectors. Our experiments show
that these two methods offer an advantage for doc-
ument classification.
2.2 Generalized Latent Semantic Analysis
We use the Generalized Latent Semantic Analy-
sis (GLSA) (Matveeva et al, 2005) to compute se-
mantically motivated term vectors.
The GLSA algorithm computes the term vectors
for the vocabulary of the document collection C
with vocabulary V using a large corpus W . It has
the following outline:
1. Construct the weighted term document ma-
trix D based on C
2. For the vocabulary words in V , obtain a ma-
trix of pair-wise similarities, S, using the
large corpus W
3. Obtain the matrix UT of low dimensional
vector space representation of terms that pre-
serves the similarities in S, UT ? Rk?|V |
4. Compute document vectors by taking linear
combinations of term vectors D? = UTD
The columns of D? are documents in the k-
dimensional space.
In step 2 we used point-wise mutual informa-
tion (PMI) as the co-occurrence based measure of
semantic associations between pairs of the vocab-
ulary terms. PMI has been successfully applied to
semantic proximity tests for words (Turney, 2001;
Terra and Clarke, 2003) and was also success-
fully used as a measure of term similarity to com-
pute document clusters (Pantel and Lin, 2002). In
152
our preliminary experiments, the GLSA with PMI
showed a better performance than with other co-
occurrence based measures such as the likelihood
ratio, and ?2 test.
PMI between random variables representing
two words, w1 and w2, is computed as
PMI(w1, w2) = log
P (W1 = 1,W2 = 1)
P (W1 = 1)P (W2 = 1)
.
(4)
We used the singular value decomposition
(SVD) in step 3 to compute GLSA term vectors.
LSA (Deerwester et al, 1990) and some other
related dimensionality reduction techniques, e.g.
Locality Preserving Projections (He and Niyogi,
2003) compute a dual document-term representa-
tion. The main advantage of GLSA is that it fo-
cuses on term vectors which allows for a greater
flexibility in the choice of the similarity matrix.
3 Experiments
The goal of the experiments was to understand
whether the GLSA term vectors can be used to
model the term translation probabilities. We used
a simple k-NN classifier and a basic baseline to
evalute the performance. We used the GLSA-
based term translation probabilities within the lan-
guage modelling framework and GLSA document
vectors.
We used the 20 news groups data set because
previous studies showed that the classification per-
formance on this document collection can notice-
ably benefit from additional semantic informa-
tion (Bekkerman et al, 2003). For the GLSA
computations we used the terms that occurred in
at least 15 documents, and had a vocabulary of
9732 terms. We removed documents with fewer
than 5 words. Here we used 2 sets of 6 news
groups. Groupd contained documents from dis-
similar news groups1, with a total of 5300 docu-
ments. Groups contained documents from more
similar news groups2 and had 4578 documents.
3.1 GLSA Computation
To collect the co-occurrence statistics for the sim-
ilarities matrix S we used the English Gigaword
collection (LDC). We used 1,119,364 New York
Times articles labeled ?story? with 771,451 terms.
1os.ms, sports.baseball, rec.autos, sci.space, misc.forsale,
religion-christian
2politics.misc, politics.mideast, politics.guns, reli-
gion.misc, religion.christian, atheism
Groupd Groups
#L tf Glsa LM tf Glsa LM
100 0.58 0.75 0.69 0.42 0.48 0.48
200 0.65 0.78 0.74 0.47 0.52 0.51
400 0.69 0.79 0.76 0.51 0.56 0.55
1000 0.75 0.81 0.80 0.58 0.60 0.59
2000 0.78 0.83 0.83 0.63 0.64 0.63
Table 1: k-NN classification accuracy for 20NG.
Figure 1: k-NN with 400 training documents.
We used the Lemur toolkit3 to tokenize and in-
dex the document; we used stemming and a list of
stop words. Unless stated otherwise, for the GLSA
methods we report the best performance over dif-
ferent numbers of embedding dimensions.
The co-occurrence counts can be obtained using
either term co-occurrence within the same docu-
ment or within a sliding window of certain fixed
size. In our experiments we used the window-
based approach which was shown to give better
results (Terra and Clarke, 2003). We used the win-
dow of size 4.
3.2 Classification Experiments
We ran the k-NN classifier with k=5 on ten ran-
dom splits of training and test sets, with different
numbers of training documents. The baseline was
to use the cosine similarity between the bag-of-
words document vectors weighted with term fre-
quency. Other weighting schemes such as max-
imum likelihood and Laplace smoothing did not
improve results.
Table 1 shows the results. We computed the
score between the training and test documents us-
ing two approaches: cosine similarity between the
GLSA document vectors according to Equation 3
(denoted as GLSA), and the language modelling
score which included the translation probabilities
between the terms as in Equation 2 (denoted as
3http://www.lemurproject.org/
153
LM ). We used the term frequency as an estimate
for p(w|d). To compute the matrix of translation
probabilities P , where P [i][j] = t(tj|ti) for the
LMCLSA approach, we first obtained the matrix
P? [i][j] = cos(~ti, ~tj). We set the negative and zero
entries in P? to a small positive value. Finally, we
normalized the rows of P? to sum up to one.
Table 1 shows that for both settings GLSA and
LM outperform the tf document vectors. As ex-
pected, the classification task was more difficult
for the similar news groups. However, in this
case both GLSA-based approaches outperform the
baseline. In both cases, the advantage is more
significant with smaller sizes of the training set.
GLSA and LM performance usually peaked at
around 300-500 dimensions which is in line with
results for other SVD-based approaches (Deer-
wester et al, 1990). When the highest accuracy
was achieved at higher dimensions, the increase
after 500 dimensions was rather small, as illus-
trated in Figure 1.
These results illustrate that the pair-wise simi-
larities between the GLSA term vectors add im-
portant semantic information which helps to go
beyond term matching and deal with synonymy
and polysemy.
4 Conclusion and Future Work
We used the GLSA to compute term translation
probabilities as a measure of semantic similarity
between documents. We showed that the GLSA
term-based document representation and GLSA-
based term translation probabilities improve per-
formance on document classification.
The GLSA term vectors were computed for all
vocabulary terms. However, different measures of
similarity may be required for different groups of
terms such as content bearing general vocabulary
words and proper names as well as other named
entities. Furthermore, different measures of sim-
ilarity work best for nouns and verbs. To extend
this approach, we will use a combination of sim-
ilarity measures between terms to model the doc-
ument similarity. We will divide the vocabulary
into general vocabulary terms and named entities
and compute a separate similarity score for each
of the group of terms. The overall similarity score
is a function of these two scores. In addition, we
will use the GLSA-based score together with syn-
tactic similarity to compute the similarity between
the general vocabulary terms.
References
Ron Bekkerman, Ran El-Yaniv, and Naftali Tishby.
2003. Distributional word clusters vs. words for text
categorization.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proc. of the 22rd
ACM SIGIR.
Scott C. Deerwester, Susan T. Dumais, ThomasK. Lan-
dauer, GeorgeW. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Proc. of NIPS.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proc. of the 24th
ACM SIGIR, pages 111?119, New York, NY, USA.
ACM Press.
Gina-Anne Levow, Douglas W. Oard, and Philip
Resnik. 2005. Dictionary-based techniques for
cross-language information retrieval. Information
Processing and Management: Special Issue on
Cross-language Information Retrieval.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat,
and Christian Royer. 2005. Generalized latent se-
mantic analysis for term representation. In Proc. of
RANLP.
Patrick Pantel and Dekang Lin. 2002. Document clus-
tering with committees. In Proc. of the 25th ACM
SIGIR, pages 199?206. ACM Press.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Proc.
of the 21st ACM SIGIR, pages 275?281, New York,
NY, USA. ACM Press.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw-
Hill.
Egidio L. Terra and Charles L. A. Clarke. 2003. Fre-
quency estimates for statistical word similarity mea-
sures. In Proc.of HLT-NAACL.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
154
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 235?238,
New York, June 2006. c?2006 Association for Computational Linguistics
Document Representation and Multilevel Measures of Document Similarity
Irina Matveeva
Dept. of Computer Science
University of Chicago
matveeva@cs.uchicago.edu
Abstract
We present our work on combining large-
scale statistical approaches with local lin-
guistic analysis and graph-based machine
learning techniques to compute a com-
bined measure of semantic similarity be-
tween terms and documents for applica-
tion in information extraction, question
answering, and summarisation.
1 Introduction
Document indexing and representation of term-
document relations are crucial for document classi-
fication, clustering and retrieval. In the traditional
bag-of-words vector space representation of docu-
ments (Salton and McGill, 1983) words represent
orthogonal dimensions which makes an unrealistic
assumption about their independence.
Since document vectors are constructed in a very
high dimensional vocabulary space, there has been a
considerable interest in low-dimensional document
representations to overcome the drawbacks of the
bag-of-words document vectors. Latent Semantic
Analysis (LSA) (Deerwester et al, 1990) is one of
the best known dimensionality reduction algorithms
in information retrieval.
In my research, I consider different notions of
similarity measure between documents. I use di-
mensionality reduction and statistical co-occurrence
information to define representations that support
them.
2 Dimensionality Reduction for Document
and Term Representation
A vector space representation of documents is very
convenient because it puts documents in a Euclidean
space where similarity measures such as inner prod-
uct and cosine similarity or distance are immediately
available. However, these measures will not be ef-
fective if they do not have a natural interpretation for
the original text data.
I have considered several approaches to comput-
ing a vector space representation of text data for
which inner product and distance make sense. The
general framework is to construct a matrix of pair-
wise similarities between terms or documents and
use appropriate methods of dimensionality reduc-
tion to compute low dimensional vectors. The inner
product between the resulting vectors must preserve
the similarities in the input matrix. The similarities
matrix can be computed using different notions of
similarity in the input space. Different dimensional-
ity reduction techniques impose different conditions
on how the similarities are preserved.
I investigated how external query-based similar-
ity information can be used to compute low dimen-
sional document vectors. Similar to LSA, this ap-
proach used weighted bag-of-words document vec-
tors as input which limited its effectiveness. The
next step was to develop the Generalized Latent Se-
mantic Analysis framework that allows to compute
semantically motivated term and document vectors.
235
2.1 Document Representation with the
Locality Preserving Projection Algorithm
The Locality Preserving Projection algorithm
(LPP) (He and Niyogi, 2003) is a graph-based
dimensionality reduction algorithm that computes
low dimensional document vectors by preserving
local similarities between the documents. It requires
a vector space representation of documents as
input. In addition, it uses the adjacency matrix
of the nearest neighbors graph of the data. It can
be shown, see (He and Niyogi, 2003), that the
Euclidean distance in the LPP space corresponds to
similarity in the document space.
The information about the similarity of the input
documents is contained in the adjacency matrix of
the nearest neighbors graph. In this graph, nodes
represent documents and are connected by an edge
if the documents are similar. This graph can be con-
structed using any similarity measure between the
documents, for example, the query-based similar-
ity between the documents obtained from relevance
feedback. The base case is to use inner products be-
tween the input document vectors and to connect k
nearest neighbors.
We considered several ways of modifying the
graph, see (Matveeva, 2004). We used relevance
feedback and pseudo relevance feedback from the
base line term matching retrieval to identify the top
N documents most related to the query. We added
edges to the document neighborhood graph to con-
nect these N documents. Our experiments showed
that incorporating this external relevance informa-
tion into the LPP graph improves the performance
on the information retrieval tasks, in particular at
high levels of recall. Without the use of external
information, the performance of the LPP algorithm
was comparable to the performance of the LSA al-
gorithm up to recall of 0.6?0.7. At higher levels of
recall, LSA achieves a precision that is about 0.1
better than LPP. The precision at high levels of re-
call seemed to be a weak point of LPP. Fortunately,
using the relevance feedback helped to improve the
performance in particular in this range of recall.
We found the LPP algorithm to be very sensitive
to the graph structure. It confirmed the intuition that
the Euclidean distance between the document vec-
tors in the bag-of-words representation is not a good
similarity measure. When we added query relevance
information to the graph, we introduced a similarity
metric on the document space that was closer to the
true similarity. However, this information was only
partial, because only a subset of the edges reflected
this true similarity. The next step was therefore to
develop a vector space representation for documents
which did not require the bag-of-words representa-
tion as input.
2.2 Generalized Latent Semantic Analysis
We developed the Generalized Latent Seman-
tic Analysis (GLSA) framework to compute se-
mantically motivated term and document vec-
tors (Matveeva et al, 2005). We begin with seman-
tically motivated pair-wise term similarities and use
dimensionality reduction to compute a vector space
representation for terms. Our approach is to focus on
similarity between vocabulary terms. We compute
representations and similarities for terms and con-
sider documents to be linear combinations of terms.
This shift from dual document-term representation
to terms has the following motivation.
? Terms offer a much greater flexibility in explor-
ing similarity relations than documents. The
availability of large document collections such
as the Web offers a great resource for statisti-
cal approaches. Recently, co-occurrence based
measures of semantic similarity between terms
has been shown to improve performance on
such tasks as the synonymy test, taxonomy in-
duction, etc. (Turney, 2001; Terra and Clarke,
2003; Chklovski and Pantel, 2004). On the
other hand, many semi-supervised and trans-
ductive methods based on document vectors
cannot yet handle such large document collec-
tions.
? While the vocabulary size is still quite large,
it is intuitively clear that the intrinsic dimen-
sionality of the vocabulary space is much lower.
Content bearing words are often combined into
semantic classes that correspond to particular
activities or relations and contain synonyms
and semantically related words. Therefore, it
seems very natural to represent terms as low di-
mensional vectors in the space of semantic con-
cepts.
236
2.2.1 GLSA Algorithm
The GLSA algorithm takes as input a document
collection C with vocabulary V and a large corpus
W . It has the following outline:
1. Construct the weighted term document matrix
D based on C
2. For the vocabulary words in V , obtain a ma-
trix of pair-wise similarities, S, using the large
corpus W
3. Obtain the matrix UT of low dimensional vec-
tor space representation of terms that preserves
the similarities in S, UT ? Rk?|V | . The
columns of UT are k-dimensional term vectors
4. Compute document vectors by taking linear
combinations of term vectors D? = UTD
In step 2 of the GLSA algorithm we used point-
wise mutual information (PMI) as the co-occurrence
based measure of semantic associations between
pairs of the vocabulary terms. We used the singu-
lar value decomposition in step 3 to compute GLSA
term vectors.
2.2.2 Experimental Evaluation
We used the TOEFL, TS1 and TS2 synonymy
tests to demonstrate that the GLSA vector space rep-
resentation for terms captures their semantic rela-
tions, see (Matveeva et al, 2005) for details. Our
results demonstrate that similarities between GLSA
term vectors achieve better results than PMI scores
and outperform the related PMI-IR approach (Tur-
ney, 2001; Terra and Clarke, 2003). On the TOEFL
test GLSA achieves the best precision of 0.86, which
is much better than our PMI baseline as well as
the highest precision of 0.81 reported in (Terra and
Clarke, 2003). GLSA achieves the same maximum
precision as in (Terra and Clarke, 2003) for TS1
(0.73) and higher precision on TS2 (0.82 compared
to 0.75 in (Terra and Clarke, 2003)).
We also conducted document classification exper-
iments to demonstrate the advantage of the GLSA
document vectors (Matveeva et al, 2005). We used
a k-nearest neighbors classifier for a set of 5300
documents from 6 dissimilar groups from the 20
news groups data set. The k-nn classifier achieved
higher accuracy with the GLSA document vectors
than with the traditional tf-idf document vectors, es-
pecially with fewer training examples. With 100
training examples, the k-nn classifier with GLSA
had 0.75 accuracy vs. 0.58 with the tf-idf document
vectors. With 1000 training examples the numbers
were 0.81 vs. 0.75.
The inner product between the GLSA document
vectors can be used as input to other algorithms.
The language modelling approach (Berger and Laf-
ferty, 1999) proved very effective for the informa-
tion retrieval task. Berger et. al (Berger and Laf-
ferty, 1999) used translation probabilities between
the document and query terms to account for syn-
onymy and polysemy. We proposed to use low di-
mensional term vectors for inducing the translation
probabilities between terms (Matveeva and Levow,
2006). We used the same k-nn classification task as
above. With 100 training examples, the k-nn accu-
racy based on tf-idf document vectors was 0.58 and
with the similarity based on the language modelling
with GLSA term translation probabilities the accu-
racy was 0.69. With larger training sets the differ-
ence in performance was less significant. These re-
sults illustrate that the pair-wise similarities between
the GLSA term vectors add important semantic in-
formation which helps to go beyond term matching
and deal with synonymy and polysemy.
3 Work in Progress
Many recent applications such as document sum-
marization, information extraction and question an-
swering require a detailed analysis of semantic re-
lations between terms within and across documents
and sentences. Often one has a number of sentences
or paragraphs and has to choose the candidate with
the highest level of relevance for the topic or ques-
tion. An additional requirement may be that the in-
formation content of the next candidate is different
from the sentences that are already chosen.
In these cases, it seems natural to have differ-
ent levels of document similarity. Two sentences or
paragraphs can be similar because they contain in-
formation about the same people or events. In this
case, the similarity can be based on the number of
the named entities they have in common. On the
other hand, they can be similar because they contain
synonyms or semantically related terms.
237
I am currently working on a combination of sim-
ilarity measures between terms to model document
similarity. I divide the vocabulary into general vo-
cabulary terms and named entities and compute a
separate similarity score for each group of terms.
The overall document similarity score is a function
of these two scores. To keep the vocabulary size
manageable and denoise the data, we only use the
content bearing words from the set of the general
vocabulary terms. We use a parser to identify nouns
and adjectives that participate in three types of syn-
tactic relations: subject, direct object, the head of the
noun phrase with an adjective or noun as a modifier
for nouns and the modifier of a noun for adjectives.
Currently we include only such nouns and adjectives
in the set of the content bearing vocabulary terms.
We used the TDT2 collection for preliminary
classification experiments. We used a k-nn classi-
fier to classify documents from the 10 most frequent
topics. We used tf-idf document vectors indexed
with 55,729 general vocabulary words as our base-
line. The set of the content bearing words was much
smaller and had 13,818 nouns and adjectives. The
GLSA document vectors improved the classification
accuracy over the baseline and outperformed LSA
document vectors. This validates our approach to
selecting the content bearing terms and shows the
advantage of using the GLSA framework. We are
going to extend the set of content bearing words and
to include verbs. We will take advantage of the flex-
ibility provided by our framework and use syntax
based measure of similarity in the computation of
the verb vectors, following (Lin, 1998).
Currently we are using string matching to com-
pute the named entity based measure of similar-
ity. We are planning to integrate more sophisticated
techniques in our framework.
4 Conclusion
We developed the GLSA framework for comput-
ing semantically motivated term and document vec-
tors. This framework takes advantage of the avail-
ability of large document collections and recent re-
search of corpus-based term similarity measures and
combines them with dimensionality reduction algo-
rithms.
Different measures of similarity may be required
for different groups of terms such as content bear-
ing vocabulary words and named entities. To ex-
tend the GLSA approach to computing the document
vectors, we use a combination of similarity mea-
sures between terms to model the document simi-
larity. This approach defines a fine-grained similar-
ity measure between documents and sentences. Our
goal is to develop a multilevel measure of document
similarity that will be helpful for summarization and
information extraction.
References
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proc. of the 22rd
ACM SIGIR.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Xiaofei He and Partha Niyogi. 2003. Locality preserving
projections. In Proc. of NIPS.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL, pages 768?774.
Irina Matveeva and Gina-Anne Levow. 2006. Comput-
ing term translation probabilities with generalized la-
tent semantic analysis. In Proc. of EACL.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized latent semantic
analysis for term representation. In Proc. of RANLP.
Irina Matveeva. 2004. Text representation with the lo-
cality preserving projection algorithm for information
retrieval task. In Master?s Thesis.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Egidio L. Terra and Charles L. A. Clarke. 2003. Fre-
quency estimates for statistical word similarity mea-
sures. In Proc.of HLT-NAACL.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
238
Proceedings of NAACL HLT 2007, Companion Volume, pages 113?116,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Hybrid Document Indexing with Spectral Embedding
Irina Matveeva
Department of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
Document representation has a large im-
pact on the performance of document re-
trieval and clustering algorithms. We
propose a hybrid document indexing
scheme that combines the traditional bag-
of-words representation with spectral em-
bedding. This method accounts for the
specifics of the document collection and
also uses semantic similarity information
based on a large scale statistical analysis.
Clustering experiments showed improve-
ments over the traditional tf-idf represen-
tation and over the spectral methods based
solely on the document collection.
1 Introduction
Capturing semantic relations between words in a
document representation is a difficult problem. Dif-
ferent approaches tried to overcome the term inde-
pendence assumption of the bag-of-words represen-
tation (Salton and McGill, 1983) for example by us-
ing distributional term clusters (Slonim and Tishby,
2000) and expanding the document vectors with
synonyms, see (Levow et al, 2005). Since content
words can be combined into semantic classes there
has been a considerable interest in low-dimensional
term and document representations.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. In the LSA space documents
are indexed with latent semantic concepts. LSA
showed large performance improvements over the
traditional tf-idf representation on small document
collections (Deerwester et al, 1990) but often does
not perform well on large heterogeneous collections.
LSA maps all words to low dimensional vectors.
However, the notion of semantic relatedness is de-
fined differently for subsets of the vocabulary. In ad-
dition, the numerical information, abbreviations and
the documents? style may be very good indicators of
their topic. However, this information is no longer
available after the dimensionality reduction.
We use a hybrid approach to document indexing
to address these issues. We keep the notion of la-
tent semantic concepts and also try to preserve the
specifics of the document collection. We use a low-
dimensional representation only for nouns and rep-
resent the rest of the document?s content as tf-idf
vectors.
The rest of the paper is organized as follows. Sec-
tion 2 discusses our approach. Section 3 reports the
experimental results. We conclude in section 4.
2 Hybrid Document Indexing
This section gives the general idea of our approach.
We divide the vocabulary into two sets: nouns and
the rest of the vocabulary. We use a method of spec-
tral embedding, as described below and compute a
low-dimensional representation for documents using
only the nouns. We also compute a tf-idf represen-
tation for documents using the other set of words.
Since we can treat each latent semantic concept in
the low-dimensional representation as part of the vo-
cabulary, we combine the two vector representations
for each document by concatenating them.
113
2.1 Spectral Embedding
Spectral methods comprise a family of algorithms
that use a matrix of pair-wise similarities S and per-
form its spectral analysis, such as the eigenvalue de-
composition, to embed terms and documents in a
low-dimensional vector space. S = U?UT , where
the columns of U are its eigenvectors and ? is a di-
agonal matrix with the eigenvalues.
If we have a matrix of pair-wise word similarities
S, its first k eigenvectors Uk will be used to repre-
sent the words in the latent semantic space. Seman-
tically related words will have high association with
the same latent concepts and their corresponding
vectors will be similar. Moreover, the vector similar-
ity between the word vectors will optimally preserve
the original similarities (Cox and Cox, 2001).
We use two approaches to compute spectral em-
bedding for nouns. Latent Semantic Analysis
(LSA) (Deerwester et al, 1990) and Generalized La-
tent Semantic Analysis (GLSA) (Matveeva et al,
2005). For both we used the eigenvalue decomposi-
tion as the embedding step. The difference is in the
similarities matrix which we are trying to preserve.
2.2 Distributional Term Similarity
LSA and GLSA begin with a matrix of pair-wise
term similarities S, compute its eigenvectors U and
use the first k of them to represent terms and doc-
uments, for details see (Deerwester et al, 1990;
Matveeva et al, 2005). The main difference in our
implementation of these algorithms is the matrix of
pair-wise word similarities. Since our representation
will try to preserve them it is important to have a ma-
trix of similarities which is linguistically motivated.
LSA uses the matrix of pair-wise similarities
which is based on document vectors. For two words
wi and wj in the document collection containing n
documents dk, the similarity is computed as
S(wi, wj) =
?
k=1:n
tf(wi, dk)idf(wi) ? tf(wj , dk)idf(wj),
where tf(wi, dk) is the term frequency for wi in
dk and idf(wi) is the inverse document frequency
weight for wi. LSA is a special case of spectral em-
bedding restricted to one type of term similarities
and dimensionality reduction method.
GLSA (Matveeva et al, 2005) generalizes the
idea of latent semantic space. It proposes to use
different types of similarity matrix and spectral em-
bedding methods to compute a latent space which is
closer to true semantic similarities. One way to do
so is to use a more appropriate similarities matrix S.
PMI We use point-wise mutual information (PMI)
to compute the matrix S. PMI between random vari-
ables representing the words wi and wj is computed
as
PMI(wi, wj) = log
P (Wi = 1,Wj = 1)
P (Wi = 1)P (Wj = 1)
.
Thus, for GLSA, S(wi, wj) = PMI(wi, wj).
Co-occurrence Proximity An advantage of PMI
is the notion of proximity. The co-occurrence statis-
tics for PMI are typically computed using a sliding
window. Thus, PMI will be large only for words
that co-occur within a small fixed context. Our ex-
periments show that this is a better approximation to
true semantic similarities.
2.3 Document Indexing
We have two sets of the vocabulary terms: a set of
nouns, N , and the other words, T . We compute tf-idf
document vectors indexed with the words in T :
~di = (?i(w1), ?i(w2), ..., ?i(w|T |)),
where ?i(wt) = tf(wt, di) ? idf(wt).
We also compute a k-dimensional representation
with latent concepts ci as a weighted linear combi-
nation of LSA or GLSA term vectors ~wt:
~di = (c1, ..., ck) =
?
t=1:|T |
?i(wt) ? ~wt,
We concatenate these two representations to gener-
ate a hybrid indexing of documents:
~di = (?i(w1), ..., ?i(w|T |), c1, ...ck)
3 Experiments
We performed document clustering experiments to
validate our approach.
114
Subset m-n #topics min #d max #d av. #d
5-10 19 6 10 8.2
50-150 21 55 150 94.7
500-1000 2 544 844 694.0
1000-5000 3 1367 2083 1792.3
Table 1: TDT2 topic subsets containing between m
and n documents: the number of topics per subset,
the minimum, the maximum and the average number
of documents per topic in each subset.
Indexing
All words Nouns Hybrid
tf-idf, LSA tf-idfN
GLSA, GLSA local GLSAN tf-idf+GLSAN
Table 2: Indexing schemes: with full vocabulary
(All), only nouns (Nouns) and the combination.
Data We used the TDT2 collection1 of news arti-
cles from six news agencies in 1998. We used only
10,329 documents that are assigned to one topic.
TDT2 documents are distributed over topics very
unevenly. We used subsets of the TDT2 topics that
contain between m and n documents, see Table 1.
We used the Lemur toolkit2 with stemming and stop
words list for the tf-idf indexing, Bikel?s parser3 to
obtain the set of nouns and the PLAPACK pack-
age (Bientinesi et al, 2003) to compute the eigen-
value decomposition.
Global vs. Local Similarity To obtain the PMI
values for GLSA we used the TDT2 collection, de-
noted as GLSAlocal. Since co-occurrence statistics
based on larger collections gives a better approxima-
tion to linguistic similarities, we also used 700,000
documents from the English GigaWord collection,
denoted as GLSA and GLSAN . We used a window
of size 8.
Representations For each document we com-
puted 7 representations, see Table 2. The vocabulary
size we used with the tf-idf indexing was 114,127.
For computational reasons we used the set of words
that occurred in at least 20 documents with our spec-
tral methods. We used 17,633 words for index-
1http://nist.gov/speech/tests/tdt/tdt98/
2http://www.lemurproject.org/
3http://www.cis.upenn.edu/ dbikel/software.html
ing with LSA and GLSAlocal and 17,572 words for
GLSA. We also indexed documents using only the
15,325 nouns: tf-idfN and GLSAN . The hybrid rep-
resentation was computed using the tf-idf indexing
without nouns and the GLSAN nouns vectors.
Evaluation We used the minimum squared
residue co-clustering algorithm 4. We report two
evaluation measures: accuracy and the F1-score.
The clustering algorithm assigns each document to
a cluster. We map the cluster id?s to topic labels
using the Munkres assignment algorithm (Munkres,
1957) and compute the accuracy as the ratio of the
correctly assigned labels.
The F1 score for cluster ci labeled with topic ti is
computed using F1 = 2(p?r)(p+r) where p is precision
and r is recall. For clusters C = (c1, ..., cn) and
topics T = (t1, ..., tn) we compute the total score:
F1(C, T ) =
?
t?T
Nt
N maxc?C F1(c, t).
Nt is the number of documents belonging to the
topic t and N is the total number of documents. This
measure accounts for the topic size and also corrects
the topic assignments to clusters by using the max.
4 Results and Conclusion
Table 3 shows that the spectral methods outperform
the tf-idf representations and have smaller variance.
We report the performance for four subsets. The
subset 5?10 has a large number of topics, each with
a similar number of documents. The subset 50?150
has a large number of topics with a less even distri-
bution of documents. 500 ? 1000 and 1000 ? 5000
have a couple of large topics. We ran the clustering
over 30 random initializations. To eliminate the ef-
fect of the initial conditions on the performance we
also used one document per cluster to seed the initial
assignment for the 5? 10 subset.
All methods have the worst performance for the
5?10 subset. The best performance is for the subset
500?1000. LSA and GLSAlocal indexing are com-
puted based on the TDT2 collection. GLSAlocal has
better average performance which confirms that the
co-occurrence proximity is important for distribu-
tional similarity. The GLSA indexing computed us-
ing a large corpus performs significantly worse than
4http://www.cs.utexas.edu/users/dml/Software/cocluster.html
115
All words LSA GLSAlocal GLSA onlyN GLSAN Hybrid
5-10 acc 0.56(0.11) 0.69(0.07) 0.78(0.05) 0.60(0.05) 0.63(0.05) 0.76(0.05) 0.82(0.05)
F1 0.60(0.09) 0.73(0.05) 0.81(0.04) 0.64(0.05) 0.67(0.05) 0.80(0.04) 0.85(0.04)
50-150 acc 0.75(0.05) 0.73(0.06) 0.80(0.05) 0.70(0.04) 0.68(0.04) 0.80(0.04) 0.87(0.04)
F1 0.80(0.04) 0.78(0.05) 0.84(0.04) 0.75(0.04) 0.75(0.03) 0.84(0.04) 0.90(0.03)
500-1000 acc 0.95(0.03) 0.98(0.00) 0.99(0.00) 0.97(0.00) 0.97(0.00) 0.99(0.00) 1.00(0.00)
F1 0.95(0.03) 0.98(0.00) 0.99(0.00) 0.97(0.00) 0.97(0.00) 0.99(0.00) 1.00(0.00)
1000-5000 acc 0.86(0.11) 0.88(0.04) 0.88(0.13) 0.92(0.08) 0.82(0.06) 0.92(0.00) 0.96(0.07)
F1 0.88(0.07) 0.88(0.03) 0.90(0.09) 0.93(0.06) 0.82(0.04) 0.92(0.00) 0.97(0.05)
5-10s acc 0.932 0.919 0.986 0.932 0.980 0.980 0.992
F1 0.933 0.927 0.986 0.932 0.979 0.979 0.992
Table 3: Clustering accuracy (first row) and F1 score (second row) for each indexing scheme. The measures
are averaged over 30 random initiations of the clustering algorithm, the standard deviation is shown in
brackets. For the last experiment, 5-10s, we used one document per cluster as the initial assignment.
GLSAlocal on the heterogeneous 5?10 and 50?150
subsets and performs similarly for the other two. It
supports our intuition that the document?s style and
word distribution within the collection are important
and may get lost, especially if we use a document
collection with a different word distribution to esti-
mate the similarities matrix S.
The tf-idf indexing with nouns only, onlyN , has
good performance compared to the all-words index-
ing. The semantic similarity between nouns seems
to be collection independent. The GLSAN index-
ing is significantly better than onlyN and tf-idf in
most cases and performs similar to GLSAlocal. By
using GLSAN we computed the embedding for
more nouns that we could keep in the GLSAlocal
and GLSA representations. Nouns convey impor-
tant topic membership information and it is advan-
tageous to use as many of them as possible.
We observed the same performance relation when
we used labels to make the initial cluster assign-
ment, see 5? 10s in Table 3. tf-idf, GLSA and LSA
performed similarly, GLSAlocal and GLSAN per-
formed better with the hybrid scheme being the best.
The hybrid indexing significantly outperforms tf-
idf, LSA and GLSA on three subsets. This shows the
benefits of using the spectral embedding to discover
the semantic relations between nouns and keeping
the rest of the document content as tf-idf representa-
tion to preserve other indicators of its topic member-
ship. By combining two representations the hybrid
indexing scheme defines a more complex notion of
similarity between documents. For nouns it uses the
semantic proximity in the space of latent semantic
classes and for other words it uses term-matching.
References
Paolo Bientinesi, Inderjit S. Dhilon, and Robert A. van de
Geijn. 2003. A parallel eigensolver for dense sym-
metric matrices based on multiple relatively robust
representations. UT CS Technical Report TR-03-26.
Trevor F. Cox and Micheal A. Cox. 2001. Multidimen-
sional Scaling. CRC/Chapman and Hall.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management: Special Issue on Cross-language Infor-
mation Retrieval.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized latent semantic
analysis for term representation. In Proc. of RANLP.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. SIAM, 5(1):32?38.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Research and Development in Infor-
mation Retrieval, pages 208?215.
116
A Geometric View on Bilingual Lexicon Extraction from Comparable
Corpora
E. Gaussier?, J.-M. Renders?, I. Matveeva?, C. Goutte?, H. De?jean?
?Xerox Research Centre Europe
6, Chemin de Maupertuis ? 38320 Meylan, France
Eric.Gaussier@xrce.xerox.com
?Dept of Computer Science, University of Chicago
1100 E. 58th St. Chicago, IL 60637 USA
matveeva@cs.uchicago.edu
Abstract
We present a geometric view on bilingual lexicon
extraction from comparable corpora, which allows
to re-interpret the methods proposed so far and iden-
tify unresolved problems. This motivates three new
methods that aim at solving these problems. Empir-
ical evaluation shows the strengths and weaknesses
of these methods, as well as a significant gain in the
accuracy of extracted lexicons.
1 Introduction
Comparable corpora contain texts written in differ-
ent languages that, roughly speaking, ?talk about
the same thing?. In comparison to parallel corpora,
ie corpora which are mutual translations, compara-
ble corpora have not received much attention from
the research community, and very few methods have
been proposed to extract bilingual lexicons from
such corpora. However, except for those found in
translation services or in a few international organ-
isations, which, by essence, produce parallel docu-
mentations, most existing multilingual corpora are
not parallel, but comparable. This concern is re-
flected in major evaluation conferences on cross-
language information retrieval (CLIR), e.g. CLEF1,
which only use comparable corpora for their multi-
lingual tracks.
We adopt here a geometric view on bilingual lex-
icon extraction from comparable corpora which al-
lows one to re-interpret the methods proposed thus
far and formulate new ones inspired by latent se-
mantic analysis (LSA), which was developed within
the information retrieval (IR) community to treat
synonymous and polysemous terms (Deerwester et
al., 1990). We will explain in this paper the moti-
vations behind the use of such methods for bilin-
gual lexicon extraction from comparable corpora,
and show how to apply them. Section 2 is devoted to
the presentation of the standard approach, ie the ap-
proach adopted by most researchers so far, its geo-
metric interpretation, and the unresolved synonymy
1http://clef.iei.pi.cnr.it:2002/
and polysemy problems. Sections 3 to 4 then de-
scribe three new methods aiming at addressing the
issues raised by synonymy and polysemy: in sec-
tion 3 we introduce an extension of the standard ap-
proach, and show in appendix A how this approach
relates to the probabilistic method proposed in (De-
jean et al, 2002); in section 4, we present a bilin-
gual extension to LSA, namely canonical correla-
tion analysis and its kernel version; lastly, in sec-
tion 5, we formulate the problem in terms of prob-
abilistic LSA and review different associated simi-
larities. Section 6 is then devoted to a large-scale
evaluation of the different methods proposed. Open
issues are then discussed in section 7.
2 Standard approach
Bilingual lexicon extraction from comparable cor-
pora has been studied by a number of researchers,
(Rapp, 1995; Peters and Picchi, 1995; Tanaka and
Iwasaki, 1996; Shahzad et al, 1999; Fung, 2000,
among others). Their work relies on the assump-
tion that if two words are mutual translations, then
their more frequent collocates (taken here in a very
broad sense) are likely to be mutual translations as
well. Based on this assumption, the standard ap-
proach builds context vectors for each source and
target word, translates the target context vectors us-
ing a general bilingual dictionary, and compares the
translation with the source context vector:
1. For each source word v (resp. target word w),
build a context vector ??v (resp. ??w ) consisting
in the measure of association of each word e
(resp. f ) in the context of v (resp. w), a(v, e).
2. Translate the context vectors with a general
bilingual dictionary D, accumulating the con-
tributions from words that yield identical trans-
lations.
3. Compute the similarity between source word v
and target word w using a similarity measures,
such as the Dice or Jaccard coefficients, or the
cosine measure.
As the dot-product plays a central role in all these
measures, we consider, without loss of generality,
the similarity given by the dot-product between ??v
and the translation of ??w :
???v ,????tr(w)? =
?
e
a(v, e)
?
f,(e,f)inD
a(w, f)
=
?
(e,f)?D
a(v, e) a(w, f) (1)
Because of the translation step, only the pairs (e, f)
that are present in the dictionary contribute to the
dot-product.
Note that this approach requires some general
bilingual dictionary as initial seed. One way to cir-
cumvent this requirement consists in automatically
building a seed lexicon based on spelling and cog-
nates clues (Koehn and Knight, 2002). Another ap-
proach directly tackles the problem from scratch by
searching for a translation mapping which optimally
preserves the intralingual association measure be-
tween words (Diab and Finch, 2000): the under-
lying assumption is that pairs of words which are
highly associated in one language should have trans-
lations that are highly associated in the other lan-
guage. In this latter case, the association measure
is defined as the Spearman rank order correlation
between their context vectors restricted to ?periph-
eral tokens? (highly frequent words). The search
method is based on a gradient descent algorithm, by
iteratively changing the mapping of a single word
until (locally) minimizing the sum of squared differ-
ences between the association measure of all pairs
of words in one language and the association mea-
sure of the pairs of translated words obtained by the
current mapping.
2.1 Geometric presentation
We denote by si, 1 ? i ? p and tj , 1 ? j ? q the
source and target words in the bilingual dictionary
D. D is a set of n translation pairs (si, tj), and
may be represented as a p ? q matrix M, such that
Mij = 1 iff (si, tj) ? D (and 0 otherwise).2
Assuming there are m distinct source words
e1, ? ? ? , em and r distinct target words f1, ? ? ? , fr in
the corpus, figure 1 illustrates the geometric view of
the standard method.
The association measure a(v, e) may be viewed
as the coordinates of the m-dimensional context
vector ??v in the vector space formed by the or-
thogonal basis (e1, ? ? ? , em). The dot-product in (1)
only involves source dictionary entries. The corre-
sponding dimensions are selected by an orthogonal
2The extension to weighted dictionary entries Mij ? [0, 1]
is straightforward but not considered here for clarity.
projection on the sub-space formed by (s1, ? ? ? , sp),
using a p ? m projection matrix Ps. Note that
(s1, ? ? ? , sp), being a sub-family of (e1, ? ? ? , em), is
an orthogonal basis of the new sub-space. Similarly,
??w is projected on the dictionary entries (t1, ? ? ? , tq)
using a q ? r orthogonal projection matrix Pt. As
M encodes the relationship between the source and
target entries of the dictionary, equation 1 may be
rewritten as:
S(v, w) = ???v ,????tr(w)? = (Ps??v )> M (Pt??w ) (2)
where > denotes transpose. In addition, notice that
M can be rewritten as S>T , with S an n ? p and
T an n ? q matrix encoding the relations between
words and pairs in the bilingual dictionary (e.g. Ski
is 1 iff si is in the kth translation pair). Hence:
S(v, w)=??v>P>s S>TPt??w =?SPs??v , TPt??w ? (3)
which shows that the standard approach amounts to
performing a dot-product in the vector space formed
by the n pairs ((s1, tl), ? ? ? , (sp, tk)), which are as-
sumed to be orthogonal, and correspond to transla-
tion pairs.
2.2 Problems with the standard approach
There are two main potential problems associated
with the use of a bilingual dictionary.
Coverage. This is a problem if too few corpus
words are covered by the dictionary. However, if
the context is large enough, some context words
are bound to belong to the general language, so a
general bilingual dictionary should be suitable. We
thus expect the standard approach to cope well with
the coverage problem, at least for frequent words.
For rarer words, we can bootstrap the bilingual dic-
tionary by iteratively augmenting it with the most
probable translations found in the corpus.
Polysemy/synonymy. Because all entries on ei-
ther side of the bilingual dictionary are treated as or-
thogonal dimensions in the standard methods, prob-
lems may arise when several entries have the same
meaning (synonymy), or when an entry has sev-
eral meanings (polysemy), especially when only
one meaning is represented in the corpus.
Ideally, the similarities wrt synonyms should not
be independent, but the standard method fails to ac-
count for that. The axes corresponding to synonyms
si and sj are orthogonal, so that projections of a
context vector on si and sj will in general be uncor-
related. Therefore, a context vector that is similar to
si may not necessarily be similar to sj .
A similar situation arises for polysemous entries.
Suppose the word bank appears as both financial in-
stitution (French: banque) and ground near a river
Ps
e 2
e m
v
e 1 s 1
s p
v?
(s  ,t  )
t
t f
f
f(s  ,t  )1 1
(s  ,t  ) 2
1
r
w
w?
1
p
PtS T
p k
1 i
v"
w"
Figure 1: Geometric view of the standard approach
(French: berge), but only the pair (banque, bank)
is in the bilingual dictionary. The standard method
will deem similar river, which co-occurs with bank,
and argent (money), which co-occurs with banque.
In both situations, however, the context vectors of
the dictionary entries provide some additional infor-
mation: for synonyms si and sj , it is likely that ??si
and ??sj are similar; for polysemy, if the context vec-
tors
?????banque and ???bank have few translations pairs in
common, it is likely that banque and bank are used
with somewhat different meanings. The following
methods try to leverage this additional information.
3 Extension of the standard approach
The fact that synonyms may be captured through
similarity of context vectors3 leads us to question
the projection that is made in the standard method,
and to replace it with a mapping into the sub-space
formed by the context vectors of the dictionary en-
tries, that is, instead of projecting ??v on the sub-
space formed by (s1, ? ? ? , sp), we now map it onto
the sub-space generated by (??s1 , ? ? ? ,??sp). With this
mapping, we try to find a vector space in which syn-
onymous dictionary entries are close to each other,
while polysemous ones still select different neigh-
bors. This time, if ??v is close to ??si and ??sj , si and
sj being synonyms, the translations of both si and
sj will be used to find those words w close to v.
Figure 2 illustrates this process. By denoting Qs,
respectively Qt, such a mapping in the source (resp.
target) side, and using the same translation mapping
(S, T ) as above, the similarity between source and
target words becomes:
S(v, w)=?SQs??v , TQt??w ?=??v>Q>s S>TQt??w (4)
A natural choice for Qs (and similarly for Qt) is the
following m ? p matrix:
Qs = R>s =
?
??
a(s1, e1) ? ? ? a(sp, e1)
.
.
.
.
.
.
.
.
.
a(s1, em) ? ? ? a(sp, em)
?
??
3This assumption has been experimentally validated in sev-
eral studies, e.g. (Grefenstette, 1994; Lewis et al, 1967).
but other choices, such as a pseudo-inverse of Rs,
are possible. Note however that computing the
pseudo-inverse of Rs is a complex operation, while
the above projection is straightforward (the columns
of Q correspond to the context vectors of the dic-
tionary words). In appendix A we show how this
method generalizes over the probabilistic approach
presented in (Dejean et al, 2002). The above
method bears similarities with the one described
in (Besanc?on et al, 1999), where a matrix similar
to Qs is used to build a new term-document ma-
trix. However, the motivations behind their work
and ours differ, as do the derivations and the gen-
eral framework, which justifies e.g. the choice of
the pseudo-inverse of Rs in our case.
4 Canonical correlation analysis
The data we have at our disposal can naturally be
represented as an n ? (m + r) matrix in which
the rows correspond to translation pairs, and the
columns to source and target vocabularies:
C =
e1 ? ? ? em f1 ? ? ? fr
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (s(1), t(1))
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (s(n), t(n))
where (s(k), t(k)) is just a renumbering of the trans-
lation pairs (si, tj).
Matrix C shows that each translation pair sup-
ports two views, provided by the context vectors in
the source and target languages. Each view is con-
nected to the other by the translation pair it repre-
sents. The statistical technique of canonical corre-
lation analysis (CCA) can be used to identify direc-
tions in the source view (first m columns of C) and
target view (last r columns of C) that are maximally
correlated, ie ?behave in the same way? wrt the
translation pairs. We are thus looking for directions
in the source and target vector spaces (defined by
the orthogonal bases (e1, ? ? ? , em) and (f1, ? ? ? , fr))
such that the projections of the translation pairs on
these directions are maximally correlated. Intu-
itively, those directions define latent semantic axes
se
e
e
v
f
f
f(s  ,t  )1
2
1
r
w
1
tS T
em
e1
e2
m
1
2
s
s
s
s
(s  ,t  )
1(s  ,t  )
p
1
k
i
f
fr
2f t
t
t
t
1
2
w"
v"
1
2
p
k
q
i
v
wQ Q
Figure 2: Geometric view of the extended approach
that capture the implicit relations between transla-
tion pairs, and induce a natural mapping across lan-
guages. Denoting by ?s and ?t the directions in the
source and target spaces, respectively, this may be
formulated as:
? = max
?s,?t
?
i??s,??s (i)???t,
??t (i)???
i??s,??s (i)?
?
j??t,
??t (j)?
As in principal component analysis, once the first
two directions (?1s , ?1t ) have been identified, the pro-
cess can be repeated in the sub-space orthogonal
to the one formed by the already identified direc-
tions. However, a general solution based on a set of
eigenvalues can be proposed. Following e.g. (Bach
and Jordan, 2001), the above problem can be re-
formulated as the following generalized eigenvalue
problem:
B ? = ?D ? (5)
where, denoting again Rs and Rt the first m and last
r (respectively) columns of C, we define:
B =
( 0 RtR>t RsR>s
RsR>s RtR>t 0
)
,
D =
( (RsR>s )2 0
0 (RtR>t )2
)
, ? =
( ?s
?t
)
The standard approach to solve eq. 5 is to per-
form an incomplete Cholesky decomposition of a
regularized form of D (Bach and Jordan, 2001).
This yields pairs of source and target directions
(?1s , ?1t ), ? ? ? , (?ls, ?lt) that define a new sub-space in
which to project words from each language. This
sub-space plays the same role as the sub-space de-
fined by translation pairs in the standard method, al-
though with CCA, it is derived from the corpus via
the context vectors of the translation pairs. Once
projected, words from different languages can be
compared through their dot-product or cosine. De-
noting ?s =
[
?1s , . . . ?ls
]>
, and ?t =
[
?1t , . . . ?lt
]>
,
the similarity becomes (figure 3):
S(v, w) = ??s??v , ?t??w ? = ??v>?>s ?t??w (6)
The number l of vectors retained in each language
directly defines the dimensions of the final sub-
space used for comparing words across languages.
CCA and its kernelised version were used in (Vi-
nokourov et al, 2002) as a way to build a cross-
lingual information retrieval system from parallel
corpora. We show here that it can be used to in-
fer language-independent semantic representations
from comparable corpora, which induce a similarity
between words in the source and target languages.
5 Multilingual probabilistic latent
semantic analysis
The matrix C described above encodes in each row
k the context vectors of the source (first m columns)
and target (last r columns) of each translation pair.
Ideally, we would like to cluster this matrix such
that translation pairs with synonymous words ap-
pear in the same cluster, while translation pairs with
polysemous words appear in different clusters (soft
clustering). Furthermore, because of the symmetry
between the roles played by translation pairs and vo-
cabulary words (synonymous and polysemous vo-
cabulary words should also behave as described
above), we want the clustering to behave symmet-
rically with respect to translation pairs and vocabu-
lary words. One well-motivated method that fulfills
all the above criteria is Probabilistic Latent Seman-
tic Analysis (PLSA) (Hofmann, 1999).
Assuming that C encodes the co-occurrences be-
tween vocabulary words w and translation pairs d,
PLSA models the probability of co-occurrence w
and d via latent classes ?:
P (w, d) =
?
?
P (?) P (w|?) P (d|?) (7)
where, for a given class, words and translation pairs
are assumed to be independently generated from
class-conditional probabilities P (w|?) and P (d|?).
Note here that the latter distribution is language-
independent, and that the same latent classes are
used for the two languages. The parameters of the
model are obtained by maximizing the likelihood of
the observed data (matrix C) through Expectation-
Maximisation algorithm (Dempster et al, 1977). In
ee
e
v
f
f
f
2
1
r
w
1
e
e1
e2
m
1
2
f
fr
2f
v"
v
w(CCA)
w"
(CCA)
m
(?1s , ?1t )
?1s
?is
?ls
?2s
(?ls, ?lt)
(?2s , ?2t ) ?1t
?lt
?s ?t
?2t
?it
Figure 3: Geometric view of the Canonical Correlation Analysis approach
addition, in order to reduce the sensitivity to initial
conditions, we use a deterministic annealing scheme
(Ueda and Nakano, 1995). The update formulas for
the EM algorithm are given in appendix B.
This model can identify relevant bilingual latent
classes, but does not directly define a similarity be-
tween words across languages. That may be done
by using Fisher kernels as described below.
Associated similarities: Fisher kernels
Fisher kernels (Jaakkola and Haussler, 1999) de-
rive a similarity measure from a probabilistic model.
They are useful whenever a direct similarity be-
tween observed feature is hard to define or in-
sufficient. Denoting `(w) = lnP (w|?) the log-
likelihood for example w, the Fisher kernel is:
K(w1, w2) = ?`(w1)>IF?1?`(w2) (8)
The Fisher information matrix IF =
E
(
?`(x)?`(x)>
)
keeps the kernel indepen-
dent of reparameterisation. With a suitable
parameterisation, we assume IF ? 1. For PLSA
(Hofmann, 2000), the Fisher kernel between two
words w1 and w2 becomes:
K(w1, w2) =
?
?
P (?|w1)P (?|w2)
P (?) (9)
+
?
d
P? (d|w1)P? (d|w2)
?
?
P (?|d,w1)P (?|d,w2)
P (d|?)
where d ranges over the translation pairs. The
Fisher kernel performs a dot-product in a vector
space defined by the parameters of the model. With
only one class, the expression of the Fisher kernel
(9) reduces to:
K(w1, w2) = 1 +
?
d
P? (d|w1)P? (d|w2)
P (d)
Apart from the additional intercept (?1?), this is
exactly the similarity provided by the standard
method, with associations given by scaled empir-
ical frequencies a(w, d) = P? (d|w)/
?
P (d). Ac-
cordingly, we expect that the standard method and
the Fisher kernel with one class should have simi-
lar behaviors. In addition to the above kernel, we
consider two additional versions, obtained:through
normalisation (NFK) and exponentiation (EFK):
NFK(w1, w2) =
K(w1, w2)?
K(w1)K(w2)
(10)
EFK(w1, w2) = e?
1
2 (K(w1)+K(w2)?2K(w1,w2))
where K(w) stands for K(w, w).
6 Experiments and results
We conducted experiments on an English-French
corpus derived from the data used in the multi-
lingual track of CLEF2003, corresponding to the
newswire of months May 1994 and December 1994
of the Los Angeles Times (1994, English) and Le
Monde (1994, French). As our bilingual dictionary,
we used the ELRA multilingual dictionary,4 which
contains ca. 13,500 entries with at least one match
in our corpus. In addition, the following linguis-
tic preprocessing steps were performed on both the
corpus and the dictionary: tokenisation, lemmatisa-
tion and POS-tagging. Only lexical words (nouns,
verbs, adverbs, adjectives) were indexed and only
single word entries in the dicitonary were retained.
Infrequent words (occurring less than 5 times) were
discarded when building the indexing terms and the
dictionary entries. After these steps our corpus con-
tains 34,966 distinct English words, and 21,140 dis-
tinct French words, leading to ca. 25,000 English
and 13,000 French words not present in the dictio-
nary.
To evaluate the performance of our extraction
methods, we randomly split the dictionaries into a
training set with 12,255 entries, and a test set with
1,245 entries. The split is designed in such a way
that all pairs corresponding to the same source word
are in the same set (training or test). All methods
use the training set as the sole available resource
and predict the most likely translations of the terms
in the source language (English) belonging to the
4Available through www.elra.info
test set. The context vectors were defined by com-
puting the mutual information association measure
between terms occurring in the same context win-
dow of size 5 (ie. by considering a neighborhood of
+/- 2 words around the current word), and summing
it over all contexts of the corpora. Different associ-
ation measures and context sizes were assessed and
the above settings turned out to give the best perfor-
mance even if the optimum is relatively flat. For
memory space and computational efficiency rea-
sons, context vectors were pruned so that, for each
term, the remaining components represented at least
90 percent of the total mutual information. After
pruning, the context vectors were normalised so that
their Euclidean norm is equal to 1. The PLSA-based
methods used the raw co-occurrence counts as asso-
ciation measure, to be consistent with the underly-
ing generative model. In addition, for the extended
method, we retained only the N (N = 200 is the
value which yielded the best results in our experi-
ments) dictionary entries closest to source and tar-
get words when doing the projection with Q. As
discussed below, this allows us to get rid of spuri-
ous relationships.
The upper part of table 1 summarizes the results
we obtained, measured in terms of F-1 score for
different lengths of the candidate list, from 20 to
500. For each length, precision is based on the num-
ber of lists that contain an actual translation of the
source word, whereas recall is based on the num-
ber of translations provided in the reference set and
found in the list. Note that our results differ from the
ones previously published, which can be explained
by the fact that first our corpus is relatively small
compared to others, second that our evaluation re-
lies on a large number of candidates, which can oc-
cur as few as 5 times in the corpus, whereas previous
evaluations were based on few, high frequent terms,
and third that we do not use the same bilingual dic-
tionary, the coverage of which being an important
factor in the quality of the results obtained. Long
candidate lists are justified by CLIR considerations,
where longer lists might be preferred over shorter
ones for query expansion purposes. For PLSA, the
normalised Fisher kernels provided the best results,
and increasing the number of latent classes did not
lead in our case to improved results. We thus dis-
play here the results obtained with the normalised
version of the Fisher kernel, using only one compo-
nent. For CCA, we empirically optimised the num-
ber of dimensions to be used, and display the results
obtained with the optimal value (l = 300).
As one can note, the extended approach yields
the best results in terms of F1-score. However, its
performance for the first 20 candidates are below
the standard approach and comparable to the PLSA-
based method. Indeed, the standard approach leads
to higher precision at the top of the list, but lower
recall overall. This suggests that we could gain in
performance by re-ranking the candidates of the ex-
tended approach with the standard and PLSA meth-
ods. The lower part of table 1 shows that this is
indeed the case. The average precision goes up
from 0.4 to 0.44 through this combination, and the
F1-score is significantly improved for all the length
ranges we considered (bold line in table 1).
7 Discussion
Extended method As one could expect, the ex-
tended approach improves the recall of our bilingual
lexicon extraction system. Contrary to the standard
approach, in the extended approach, all the dictio-
nary words, present or not in the context vector of a
given word, can be used to translate it. This leads to
a noise problem since spurious relations are bound
to be detected. The restriction we impose on the
translation pairs to be used (N nearest neighbors)
directly aims at selecting only the translation pairs
which are in true relation with the word to be trans-
lated.
Multilingual PLSA Even though theoretically
well-founded, PLSA does not lead to improved per-
formance. When used alone, it performs slightly
below the standard method, for different numbers
of components, and performs similarly to the stan-
dard method when used in combination with the
extended method. We believe the use of mere co-
occurrence counts gives a disadvantage to PLSA
over other methods, which can rely on more sophis-
ticated measures. Furthermore, the complexity of
the final vector space (several millions of dimen-
sions) in which the comparison is done entails a
longer processing time, which renders this method
less attractive than the standard or extended ones.
Canonical correlation analysis The results we ob-
tain with CCA and its kernel version are disappoint-
ing. As already noted, CCA does not directly solve
the problems we mentioned, and our results show
that CCA does not provide a good alternative to the
standard method. Here again, we may suffer from a
noise problem, since each canonical direction is de-
fined by a linear combination that can involve many
different vocabulary words.
Overall, starting with an average precision of 0.35
as provided by the standard approach, we were able
to increase it to 0.44 with the methods we consider.
Furthermore, we have shown here that such an im-
provement could be achieved with relatively simple
20 60 100 160 200 260 300 400 500 Avg. Prec.
standard 0.14 0.20 0.24 0.29 0.30 0.33 0.35 0.38 0.40 0.35
Ext (N=500) 0.11 0.21 0.27 0.32 0.34 0.38 0.41 0.45 0.50 0.40
CCA (l=300) 0.04 0.10 0.14 0.20 0.22 0.26 0.29 0.35 0.41 0.25
NFK(k=1) 0.10 0.15 0.20 0.23 0.26 0.27 0.28 0.32 0.34 0.30
Ext + standard 0.16 0.26 0.32 0.37 0.40 0.44 0.45 0.47 0.50 0.44
Ext + NFK(k=1) 0.13 0.23 0.28 0.33 0.38 0.42 0.44 0.48 0.50 0.42
Ext + NFK(k=4) 0.13 0.22 0.26 0.33 0.37 0.40 0.42 0.47 0.50 0.41
Ext + NFK (k=16) 0.12 0.20 0.25 0.32 0.36 0.40 0.42 0.47 0.50 0.40
Table 1: Results of the different methods; F-1 score at different number of candidate translations. Ext refers
to the extended approach, whereas NFK stands for normalised Fisher kernel.
methods. Nevertheless, there are still a number of
issues that need be addressed. The most impor-
tant one concerns the combination of the different
methods, which could be optimised on a validation
set. Such a combination could involve Fisher ker-
nels with different latent classes in a first step, and
a final combination of the different methods. How-
ever, the results we obtained so far suggest that the
rank of the candidates is an important feature. It is
thus not guaranteed that we can gain over the com-
bination we used here.
8 Conclusion
We have shown in this paper how the problem of
bilingual lexicon extraction from comparable cor-
pora could be interpreted in geometric terms, and
how this view led to the formulation of new solu-
tions. We have evaluated the methods we propose
on a comparable corpus extracted from the CLEF
colection, and shown the strengths and weaknesses
of each method. Our final results show that the com-
bination of relatively simple methods helps improve
the average precision of bilingual lexicon extrac-
tion methods from comparale corpora by 10 points.
We hope this work will help pave the way towards
a new generation of cross-lingual information re-
trieval systems.
Acknowledgements
We thank J.-C. Chappelier and M. Rajman who
pointed to us the similarity between our extended
method and the model DSIR (distributional seman-
tics information retrieval), and provided us with
useful comments on a first draft of this paper. We
also want to thank three anonymous reviewers for
useful comments on a first version of this paper.
References
F. R. Bach and M. I. Jordan. 2001. Kernel inde-
pendent component analysis. Journal of Machine
Learning Research.
R. Besanc?on, M. Rajman, and J.-C. Chappelier.
1999. Textual similarities based on a distribu-
tional approach. In Proceedings of the Tenth In-
ternational Workshop on Database and Expert
Systems Applications (DEX?99), Florence, Italy.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science, 41(6):391?407.
H. Dejean, E. Gaussier, and F. Sadat. 2002. An ap-
proach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In
International Conference on Computational Lin-
guistics, COLING?02.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the
Royal Statistical Society, Series B, 39(1):1?38.
Mona Diab and Steve Finch. 2000. A statisti-
cal word-level translation model for compara-
ble corpora. In Proceeding of the Conference
on Content-Based Multimedia Information Ac-
cess (RIAO).
Pascale Fung. 2000. A statistical view on bilingual
lexicon extraction - from parallel corpora to non-
parallel corpora. In J. Ve?ronis, editor, Parallel
Text Processing. Kluwer Academic Publishers.
G. Grefenstette. 1994. Explorations in Automatic
Thesaurus Construction. Kluwer Academic Pub-
lishers.
Thomas Hofmann. 1999. Probabilistic latent se-
mantic analysis. In Proceedings of the Fifteenth
Conference on Uncertainty in Artificial Intelli-
gence, pages 289?296. Morgan Kaufmann.
Thomas Hofmann. 2000. Learning the similarity of
documents: An information-geometric approach
to document retrieval and categorization. In Ad-
vances in Neural Information Processing Systems
12, page 914. MIT Press.
Tommi S. Jaakkola and David Haussler. 1999. Ex-
ploiting generative models in discriminative clas-
sifiers. In Advances in Neural Information Pro-
cessing Systems 11, pages 487?493.
Philipp Koehn and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora.
In ACL 2002 Workshop on Unsupervised Lexical
Acquisition.
P.A.W. Lewis, P.B. Baxendale, and J.L. Ben-
net. 1967. Statistical discrimination of the
synonym/antonym relationship between words.
Journal of the ACM.
C. Peters and E. Picchi. 1995. Capturing the com-
parable: A system for querying comparable text
corpora. In JADT?95 - 3rd International Con-
ference on Statistical Analysis of Textual Data,
pages 255?262.
R. Rapp. 1995. Identifying word translations in
nonparallel texts. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics.
I. Shahzad, K. Ohtake, S. Masuyama, and K. Ya-
mamoto. 1999. Identifying translations of com-
pound nouns using non-aligned corpora. In Pro-
ceedings of the Workshop MAL?99, pages 108?
113.
K. Tanaka and Hideya Iwasaki. 1996. Extraction of
lexical translations from non-aligned corpora. In
International Conference on Computational Lin-
guistics, COLING?96.
Naonori Ueda and Ryohei Nakano. 1995. Deter-
ministic annealing variant of the EM algorithm.
In Advances in Neural Information Processing
Systems 7, pages 545?552.
A. Vinokourov, J. Shawe-Taylor, and N. Cristian-
ini. 2002. Finding language-independent seman-
tic representation of text using kernel canonical
correlation analysis. In Advances in Neural In-
formation Processing Systems 12.
Appendix A: probabilistic interpretation of
the extension of standard approach
As in section 3, SQs??v is an n-dimensional vector,
defined over ((s1, tl), ? ? ? , (sp, tk)). The coordinate
of SQs??v on the axis corresponding to the transla-
tion pair (si, tj) is ???si ,??v ? (the one for TQt??w on
the same axis being ???tj ,??w ?). Thus, equation 4 can
be rewritten as:
S(v, w) =
?
(si,tj)
???si ,??v ????tj ,??w ?
which we can normalised in order to get a probabil-
ity distribution, leading to:
S(v, w) =
?
(si,tj)
P (v)P (si|v)P (w|tj)P (tj)
By imposing P (tj) to be uniform, and by denoting
C a translation pair, one arrives at:
S(v, w) ?
?
C
P (v)P (C|v)P (w|C)
with the interpretation that only the source, resp.
target, word in C is relevant for P (C|v), resp.
P (w|C). Now, if we are looking for those ws clos-
est to a given v, we rely on:
S(w|v) ?
?
C
P (C|v)P (w|C)
which is the probabilistic model adopted in (Dejean
et al, 2002). This latter model is thus a special case
of the extension we propose.
Appendix B: update formulas for PLSA
The deterministic annealing EM algorithm for
PLSA (Hofmann, 1999) leads to the following equa-
tions for iteration t and temperature ?:
P (?|w, d) = P (?)
?P (w|?)?P (d|?)??
?P (?)?P (w|?)?P (d|?)?
P (t+?)(?) = 1?
(w,d) n(w, d)
?
(w,d)
n(w, d)P (?|w, d)
P (t+?)(w|?) =
?
d n(w, d)P (?|w, d)?
(w,d) n(w, d)P (?|w, d)
P (t+?)(d|?) =
?
w n(w, d)P (?|w, d)?
(w,d) n(w, d)P (?|w, d)
where n(w, d) is the number of co-occurrences be-
tween w and d. Parameters are obtained by iterating
eqs 11?11 for each ?, 0 < ? ? 1.
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 20?27,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Using Morphology and Syntax Together 
in Unsupervised Learning 
Yu Hu and Irina Matveeva  
Department of  
Computer Science 
The University of Chicago 
Chicago IL 60637 
yuhu@cs.uchicago.edu
matveeva 
@uchicago.edu 
John Goldsmith 
Departments of Linguistics and 
Computer Science  
The University of Chicago 
Chicago IL 60637 
ja-goldsmith 
@uchicago.edu 
 
Colin Sprague 
Department of Linguistics 
The University of Chicago 
Chicago IL 60637 
sprague 
@uchicago.edu 
  
Abstract 
Unsupervised learning of grammar is a 
problem that can be important in many 
areas ranging from text preprocessing 
for information retrieval and 
classification to machine translation. 
We describe an MDL based grammar 
of a language that contains morphology 
and lexical categories. We use an 
unsupervised learner of morphology to 
bootstrap the acquisition of lexical 
categories and use these two learning 
processes iteratively to help and 
constrain each other. To be able to do 
so, we need to make our existing 
morphological analysis less fine 
grained. We present an algorithm for 
collapsing morphological classes 
(signatures) by using syntactic context. 
Our experiments demonstrate that this 
collapse preserves the relation between 
morphology and lexical categories 
within new signatures, and thereby 
minimizes the description length of the 
model. 
1 Introduction 
Our long term goal is the development of 
methods which will allow one to produce 
optimal analyses from arbitrary natural language 
corpora, where by optimization we understand 
an MDL (minimum description length; 
Rissanen, 1989) interpretation of the term: an 
optimal analysis is one which finds a grammar 
which simultaneously minimizes grammar 
length and data compression length. Our specific 
and primary focus is on morphology, and on 
how knowledge of morphology can be a useful 
step towards a more complete knowledge of a 
language?s linguistic structure. 
Our strategy is based on the following 
observation: knowing the rightmost suffix of a 
word is very useful information in inferring (or 
guessing) a word?s part of speech (POS), but due 
to the ambiguity of many suffixes, it is even 
better to know both a word?s suffix and the 
range of other suffixes that the word?s stem 
appears with elsewhere, i.e., its signature. As we 
will see below, this conjunction of ?better? 
information is what we call the signature 
transform, and in this paper, we explore how 
knowledge of signature transform can be merged 
with knowledge of the context vector to draw 
conclusions about morphology and syntax.  
In the distant future, we would like to be able 
to use the signature transform in a general 
process of grammar induction, but that day is 
not here; we therefore test our experiments by 
seeing how well we are able to predict POS as 
assigned by an available tagger (TreeTagger; 
Schmid 1994). In particular, we wish to decrease 
the uncertainty of a word?s POS through the 
morphological analysis described here. This 
decrease of uncertainty will enter into our 
calculation through an increase in the 
probability assigned to our test corpus once the 
corpus has been augmented with TreeTagger 
assigned POS tags. But to be clear on our 
20
process: we analyze a completely raw text 
morphologically, and use the POS tags from 
TreeTagger only to evaluate the signature 
transforms that we generate. 
We assume without argument here that any 
adequate natural language grammar will contain 
a lexicon which includes both lexical stems 
which are specified for morphological 
properties, such as the specific affixes with 
which they may occur, and affixes associated 
with lexical categories. We also explicitly note 
that many affixes are homophonous: they are 
pronounced (or written) identically, but have 
different morphological or syntactic 
characteristics, such as the English plural ?s and 
the verbal 3rd person singular present ?s. 
We focus initially on unsupervised learning 
of morphology for three reasons: first, because 
we already have a quite successful unsupervised 
morphological learner; second, the final suffix of 
a word is typically the strongest single indicator 
of its syntactic category; and third, analysis of a 
word into a stem T plus suffix F allows us 
(given our knowledge that the suffix F is a 
stronger indicator of category than the stem T) 
to collapse many distinct stems into a single 
cover symbol for purposes of analysis, 
simplifying our task, as we shall see.1 We 
eschew the use of linguistic resources with hand-
(i.e., human-)assigned morphological infor-
mation in order for this work to contribute, 
eventually, to a better theoretical understanding 
of human language acquisition. 
We present in this paper an algorithm that 
modifies the output of the morphology analyzer 
by combining redundant signatures. Since we 
ultimately want to use signatures and signature 
transforms to learn syntactic categories, we 
developed an algorithm that uses the syntactic 
contextual information. We evaluate the changes 
to the morphological analysis from the 
standpoint of efficient and adequate 
representation of lexical categories. This paper 
presents a test conducted on English, and thus 
can only be considered a preliminary step in the 
                                                          
1 See Higgins 2002 for a study similar in some ways; 
Higgins uses morphology as a bootstrap heuristic in one 
experimental set-up. This paper is heavily indebted to prior 
work on unsupervised learning of position categories such 
as Brown et al1992, Sch?tze 1997, Higgins 2002, and 
others cited there.  
eventually development of a language-
independent tool for grammar induction based 
on morphology. Nonetheless, the concepts that 
motivate the process are language-independent, 
and we are optimistic that similar results would 
be found in tests based on texts from other 
languages.  
In section 2 we discuss the notion of 
signature and signature transform, and section 3 
present a more explicit formulation of the 
general problem. In section 4 we present our 
algorithm for signature collapse. Section 5 
describes the experiments we ran to test the 
signature collapsing algorithm, and section 6 
presents and discusses our results. 
2 Signatures and signature transforms 
We employ the unsupervised learning of 
morphology developed by Goldsmith 
(Goldsmith, 2001). Regrettably, some of the 
discussion below depends rather heavily on 
material presented there, but we attempt to 
summarize the major points here. 
Two critical terms that we employ in this 
analysis are signature and signature transform. 
A signature found in a given corpus is a pair of 
lists: a stem-list and a suffix-list (or in the 
appropriate context, a prefix-list). By definition 
of signature ?, the concatenation of every stem 
in the stem-list of ? with every suffix in the 
suffix-list of ? is found in the corpus, and a 
morphological analysis of a corpus can be 
viewed as a set of signatures that uniquely 
analyze each word in the corpus. For example, a 
corpus of English that includes the words jump, 
jumps, jumped, jumping, walk, walks, walked, 
and walking might include the signature ?1 
whose stem list is { jump, walk } and whose 
suffix list is { ?, ed, ing , s }. For convenience, 
we label a signature with the concatenation of its 
suffixes separated by period ?.?. On such an 
analysis, the word jump is analyzed as belonging 
to the signature ?.ed.ing.s, and it bears the 
suffix ?. We say, then, that the signature 
transform of jump is ?.ed.ing.s_ ?, just as the 
signature transform of jumping is 
?.ed.ing.s_ing; in general, the signature 
transform of a word W, when W is morpho-
logically analyzed as stem T followed by suffix 
F, associated with signature ?, is defined as ?_F. 
21
In many of the experiments described below, 
we use a corpus in which all words whose 
frequency rank is greater than 200 have been 
replaced by their signature transforms. This 
move is motivated by the observation that high 
frequency words in natural languages tend to 
have syntactic distributions poorly predictable 
by any feature other than their specific identity, 
whereas the distribution properties of lower 
frequency words (which we take to be words 
whose frequency rank is 200 or below) are better 
predicted by category membership.  
In many cases, there is a natural connection 
between a signature transform and a lexical 
category. Our ultimate goal is to exploit this in 
the larger context of grammar induction. For 
example, consider the signature ?.er.ly, which 
occurs with stems such as strong and weak; in 
fact, words whose signature transform is 
?.er.ly_ ? are adjectives, those whose signature 
transform is ?.er.ly_er are comparative 
adjectives, and those whose signature transform 
is ?.er.ly_ly are adverbs. 
The connection is not perfect, however. 
Consider the signature ?.ed.ing.s and its four 
signature transforms. While most words whose 
? -transform is ?.ed.ing.s_s are verbs (indeed, 
3rd person singular present tense verbs, as in he 
walks funny), many are in fact plural nouns (e.g., 
walks in He permitted four walks in the eighth 
inning is a plural noun). We will refer to this 
problem as the signature purity problem?it is 
essentially the reflex of the ambiguity of 
suffixes. 
In addition, many 3rd person singular present 
tense verbs are associated with other signature 
transforms, such as ?.ing.s_s, ?.ed.s_s, and so 
forth; we will refer to this as the signature-
collapsing problem, because all other things 
being equal, we would like to collapse certain 
signatures, such as ?.ed.ing.s and ?.ed.ing, 
since a stem that is associated with the latter 
signature could have appeared in the corpus with 
an -s suffix; removing the ?.ed.ing signature and 
reassigning its stems to the ?.ed.ing.s signature 
will in general give us a better linguistic analysis 
of the corpus, one that can be better used in the 
problem of lexical category induction. This is 
the reflex of the familiar data sparsity concern.2   
Since we ultimately want to use signatures 
and signature transforms to learn syntactic 
categories, we base the similarity measure 
between the signatures on the context.   
3 A more abstract statement of the 
problem  
A minimum description length (MDL) analysis 
is especially appropriate for machine learning of 
linguistic analysis because simultaneously it 
puts a premium both on analytical simplicity and 
on goodness of fit between the model and the 
data (Rissanen 1989).  
We will present first the mathematical 
statement of the MDL model of the morphology, 
in (1), following the analysis in Goldsmith 
(2001), followed by a description of the meaning 
of the terms of the expressions, and then present 
the modified version which includes additional 
terms regarding part of speech (POS) 
information, in (2) and (3).  
(1) Morphology 
a. Grammar g =   
[ ])|(log)(minarg gDataprobgLength
Gg
?
?
 
b. =)(gLength  
 ? ?
=? <?
??
???
? +
stemsofsetTt ti itfreqt
W
||0 ][
1log
)]([
][log ?  
? ?
=? <?
+
affixesofsetFf fi iffreq||0 ][
1log  
??
?? ?
??
???
? +?+ ? ? ?
?
f f
W
f ][
][log
][
][log  
                                                          
2 The signature-collapsing problem has another side to it as 
well. An initial morphological analysis of English will 
typically give rise to a morphological analysis of words 
such as move, moves, moved, moving with a signature 
whose stems include mov and whose affixes are e.ed.es.ing. 
A successful solution to the signature-collapsing problem 
will collapse ?.ed.ing.s with e.ed.es.ing, noting that ? ~ e, 
ed ~ed, es ~ s, and ing ~ ing in an obvious sense. 
22
c. =)|(log gDataprob  
?
+=? ?
??
?
?
??
?
?
?
+
+
? ?
?
?
, ),|(log
)|(log
)(log
ftw
Dataw tfprob
tprob
prob
 
 
Equation (1a) states that our goal is to find 
the (morphological) grammar that 
simultaneously minimizes the sum of its own 
length and the compressed length of the data it 
analyzes, while (1b) specifies the grammar 
length (or model length) as the sum of the 
lengths of the links between the major 
components of the morphology: the list of letters 
(or phonemes) comprising the morphemes, the 
morphemes (stems and affixes), and the 
signatures. We use square brackets ?[.]? to 
denote the token counts in a corpus containing a 
given morpheme or word. The first line of (1b) 
expresses the notion that each stem consists of a 
pointer to its signature and a list of pointers to 
the letters that comprise it; ?(t) is the signature 
associated with stem t, and we take its 
probability to be 
][
)]([
W
t? , the empirical count of 
the words associated with ?(t) divided by the 
total count of words in the data. The second line 
expresses the idea that the morphology contains 
a list of affixes, each of which contains a list of 
pointers to the letters that comprise it. The third 
line of (1b) expresses the notion that a signature 
consists of a list of pointers to the component 
affixes. (1c) expresses the compressed length of 
each word in the data.3 
We now consider extending this model to 
include part of speech labeling, as sketched in 
(2). The principal innovation in (2) is the 
addition of part of speech tags; each affix is 
associated with one or more POS tags. As we 
                                                          
3 We do not sum over all occurrences of a word in the 
corpus; we count the compressed length of each word type 
found in the corpus. This decision was made based on the 
observation that the (compressed length of the) data term 
grows much faster than the length of the grammar as the 
corpus gets large, and the loss in ability of the model to 
predict word frequencies overwhelms any increase in 
model simplicity when we count word tokens in the data 
terms. We recognize the departure from the traditional 
understanding of MDL here, and assume the responsibility 
to explain this in a future publication. 
have seen, a path from a particular signature ? to 
a particular affix f constitutes what we have 
called a particular signature transform ?_f ; and 
we condition the probabilities of the POS tags in 
the data on the preceding signature 
transformation. As a result, our final model takes 
the form in (3). 
 
(2)  
t1
t2
t3
tn
...
Stems Signatures Affixes POSs
?1
?2
?m
...
f1
f2
f3
fk
...
?1
?2
?3
?l
...
 
(3) 
a. Grammar g =   [ ])|(log)(minarg gDataprobgLength
Gg
?
?
 
b. =)(gLength  
? ?
=? <?
??
???
? +
stemsofsetTt ti itfreqt
W
||0 ][
1log
)]([
][log ?
 
? ?
=? <?
+
affixesofsetFf fi iffreq||0 ][
1log  
?? ??? ?
?? ?
??
?
?
?
??
??
?
?
??
?
++?+
? ?
? ??
?
?
?
f
f
f
f
W
f
][
][log
][
][log
][
][log
 
c. =)|(log gDataprob  
 ?
+=? ?
??
?
?
??
?
?
?
+
+
+
? ??
?
??
, ),|(log
),|(log
)|(log)(log
ftw
Dataw fprob
tfprob
tprobprob
 
 
The differences between the models are 
found in the added final term in (3b), which 
specifies the information required to predict, or 
specify, the part of speech given the signature 
23
transform, and the corresponding term in the 
corpus compression expression (3c).  
The model in (3) implicitly assumes that the 
true POSs are known; in a more complete 
model, the POSs play a direct role in assigning a 
higher probability to the corpus (and hence a 
smaller compressed size to the data). In the 
context of such a model, an MDL-based learning 
device searches for the best assignment of POS 
tags over all possible assignments. Instead of 
doing that in this paper, we employ the 
TreeTagger (Schmid, 1994) based tags (see 
section 5 below), and make the working 
assumption that optimization of description 
length over all signature-analyses and POS tags 
can be approximated by optimization over all 
signature-analyses, given the POS tags provided 
by TreeTagger. 
4 The collapsing of signatures 
We describe in this section our proposed 
algorithm, using context vectors to collapse 
signatures together, composed of a sequence of 
operations, all but the first of which may be 
familiar to the reader:  
Replacement of words by signature-
transforms: The input to our algorithm for 
collapsing signatures is a modified version of 
the corpus which integrates the (unsupervised) 
morphological analyses in the following way. 
First of all, we leave unchanged the 200 most 
frequent words (word types). Next, we replace 
words belonging to the K most reliable 
signatures (where K=50 in these experiments) 
by their associated signature transforms, and we 
in effect ignore all other words, by replacing 
them by a distinguished ?dummy? symbol. In 
the following, we refer to our high frequency 
words and signature transforms together as 
elements?so an element is any member of the 
transformed corpus other than the ?dummy?.   
Context vectors based on mutual 
information: By reading through the corpus, we 
populate both left and right context vectors for 
each element (=signature-transform and high-
frequency word)  by observing the elements that 
occur adjacent to it. The feature indicating the 
appearance of a particular word on the left is 
always kept distinct from the feature indicating 
the appearance of the same word on the right. 
The features in a context vector are thus 
associated with the members of the element 
vocabulary (and indeed, each member of the 
element vocabulary occurs as two features: one 
on the left, one on the right). We assign the 
value of each feature y of x?s context vector as 
the pointwise mutual information of the 
corresponding element pair (x, y), defined as 
)()(
),(log
yprxpr
yxpr . 
Simplifying context vectors with ?idf?: In 
addition, because of the high dimensionality of 
the context vector and the fact that some features 
are more representative than others, we trim the 
original context vector. For each context vector, 
we sort features by their values, and then keep 
the top N (in general, we set N to 10) by setting 
these values to 1, and all others to 0. However, 
in this resulting simplified context vector, not all 
features do equally good jobs of distinguishing 
syntactical categories. As Wicentowski (2002) 
does in a similar context, we assign a weight  
if
w  to each feature fi in a fashion parallel to 
inverse document frequency (idf; see Sparck 
Jones 1973), or 
inappearsfeaturethiselements
elementsdistincttotal
#
#log . 
We view these as the diagonal elements of a 
matrix M (that is, mi,i = ifw ). We then check the 
similarity between two simplified context 
vectors by computing the weighted sum of the 
dot product of them. That is, given two 
simplified context vectors c and d, their 
similarity is defined as cTMd. If this value is 
larger than a threshold ? that is set as one 
parameter, we deem these two context vectors to 
be similar. Then we determine the similarity 
between elements by checking whether both left 
and right simplified context vectors of them are 
similar (i.e., their weighted dot products exceed 
a threshold ?). In the experiments we describe 
below, we explore four settings ? for this 
threshold: 0.8 (the most ?liberal? in allowing 
greater signature transform collapse, and hence 
greater signature collapse), 1.0, 1.2, and 1.5. 
Calculate signature similarity: To avoid 
considering many unnecessary pairs of 
signatures, we narrow the candidates into 
signature pairs in which the suffixes of one 
constitute a subset of suffixes of the other, and 
we set a limit to the permissible difference in the 
24
lengths of the signatures in the collapsed pairs, 
so that the difference in number of affixes 
cannot exceed 2. For each such pair, if all 
corresponding signature transforms are similar 
in the sense defined in the preceding paragraph, 
we deem the two signatures to be similar. 
Signature graph: Finally, we construct a 
signature graph, in which each signature is 
represented as a vertex, and an edge is drawn 
between two signatures iff they are similar, as 
just defined. In this graph, we find a number of 
cliques, each of which, we believe, indicates a 
cluster of signatures which should be collapsed. 
If a signature is a member of two or more 
cliques, then it is assigned to the largest clique 
(i.e., the one containing the largest number of 
signatures).4  
5 Experiments 
We obtain the morphological analysis of the 
Brown corpus (Ku?era and Francis, 1967) using 
the Linguistica software (http://linguistica. 
uchicago.edu), and we use the TreeTagger to 
assign a Penn TreeBank-style part-of-speech tag 
to each token in the corpus. We then carry out 
our experiment using the Brown corpus 
modified in the way we described above. Thus, 
for each token of the Brown corpus that our 
morphology analyzer analyzed, we have the 
following information: its stem, its signature 
                                                          
4 Our parameters are by design restrictive, so 
that we declare only few signatures to be similar, 
and therefore the cliques that we find in the 
graph are relatively small. One way to enlarge 
the size of collapsed signatures would be to 
loosen the similarity criterion. This, however, 
introduces too many new edges in the signatures 
graph, leading in turn to spurious collapses of 
signatures. We take a different approach, and 
apply our algorithms iteratively. The idea is that 
if in the first iteration, two cliques did not have 
enough edges between their elements to become 
a single new signature, they may be more 
strongly connected in the second iteration if 
many of their elements are sufficiently similar. 
On the other hand, cliques that were dissimilar 
in the first iteration remain weakly connected in 
the second.  
 
(i.e., the signature to which the stem is 
assigned), the suffix which the stem attains in 
this occurrence of the word (hence, the 
signature-transform), and the POS tag. For 
example, the token polymeric is analyzed into 
the stem polymer and the suffix ic, the stem is 
assigned to the signature ?.ic.s, and thus this 
particular token has the signature transform 
?.ic.s_ic. Furthermore, it was assigned POS-tag 
JJ, so that we have the following entry: 
?polymeric JJ ?.ic.s_ic?. 
Before performing signature collapsing, we 
calculate the description length of the 
morphology and the compressed length of the 
words that our algorithm analyzes and call it 
baseline description length (DL0). 
Now we apply our signature collapsing 
algorithm under several different parameter 
settings for the similarity threshold ?, and 
calculate the description length DL? of the 
resulting morphological and lexical analysis 
using  (3).  We know that the smaller the set of 
signatures, the smaller is the cost of the model. 
However, a signature collapse that combines 
signatures with different distributions over the 
lexical categories will result in a high cost of the 
data term (3c). The goal was therefore to find a 
method of collapsing signatures such that the 
reduction in the model cost will be higher than 
the increase in the compressed length of the data 
so that the total cost will decrease.  
As noted above, we perform this operation 
iteratively, and refer to the description length of 
the ith iteration, using a threshold ?, as ? iiterDL = . 
We used random collapsing in our 
experiments to ensure the expected relationship 
between appropriate collapses and description 
length. For each signature collapsing, we created 
a parallel situation in which the number of 
signatures collapsed is the same, but their choice 
is random.  We calculate the description length 
using this ?random? analysis as 
?
randomDL . We 
predict that this random collapsing will not 
produce an improvement in the total description 
length. 
25
6 Results and discussion 
Table 1 presents the description length, broken 
into its component terms (see (3)), for the 
baseline case and the alternative analyses 
resulting from our algorithm. The table shows 
the total description length of the model, as well 
as the individual terms: the signature term 
DL(?), the suffix term DL(F), the lexical 
categories term, DL(P), total morphology, 
DL(M), and the compressed length of the data, 
DL(D). We present results for two iterations for 
four threshold values (?=0.8,1.0,1.2,1.5) using 
our collapsing algorithm.  
Table 2 presents 
?
randomDL  derived from the 
random collapsing, in a fashion parallel to Table                
1. We show the results for only one iteration of 
random collapsing, since the first iteration 
already shows a substantial increase in 
description length. 
Figure 1 and Figure 2 present graphically the 
total description length from Tables 1 and 2 
respectively. The reader will see that all 
collapsing of signatures leads to a shortening of 
the description length of the morphology per se, 
and an increase in the compressed length of the 
data. This is an inevitable formal consequence of 
the MDL-style model used here. The empirical 
question that we care about is whether the 
combined description length increases or 
decreases, and what we find is that when 
collapsing the signatures in the way that we 
propose to do, the combined description length 
decreases, leading us to conclude that this is, 
overall, a superior linguistic description of the 
data. On the other hand, when signatures are 
collapsed randomly, the combined description 
length increases. This makes sense; randomly 
decreasing the formal simplicity of the 
grammatical description should not improve the 
overall analysis. Only an increase in the formal 
simplicity of a grammar that is grammatically 
sensible should have this property. Since our 
goal is to develop an algorithm that is 
completely data-driven and can operate in an  
Compa rison of DL 
362,500
363,000
363,500
364,000
364,500
365,000
365,500
366,000
DL0 DL1 DL2
?=0.8 ?=1 ?=1.2 ?=1.5  
Figure 1 Comparison of DL, 2 iterations and 4 
threshold values 
Compa rison of ra ndomly c olla psing DL
364,000
364,500
365,000
365,500
366,000
366,500
367,000
367,500
368,000
DL0 Drandom
D
L
?=0.8 ?=1 ?=1.2 ?=1.5
 
Figure 2 Comparison of DLs with random 
collapse of signatures (see text)
 DL0 8.0 1
=
=
?
iterDL  
8.0
2
=
=
?
iterDL
0.1
1
=
=
?
iterDL
0.1
2
=
=
?
iterDL
2.1
1
=
=
?
iterDL
2.1
2
=
=
?
iterDL  
5.1
1
=
=
?
iterDL  
5.1
2
=
=
?
iterDL
#? 50 41 35 41 34 44 42 46 45 
DL(?) 47,630 45,343 42,939 45,242 43,046 44,897 44,355 46,172 45,780 
DL(F) 160 156 156 153 143 158 147 163 164 
DL(P) 2,246 2,087 1,968 2,084 1,934 2,158 2,094 2,209 2,182 
DL(M) 50,218 47,768 45,244 47,659 45,304 47,395 46,777 48,724 48,306 
DL(D) 315,165 316,562 318,687 316,615 318,172 316,971 317,323 315,910 316,251 
Total 
DL 
365,383 364,330 363,931 364,275 363,476 364,367 364,101 364,635 364,558 
Table 1.   DL and its individual components for baseline and the resulting cases when collapsing 
signatures using our algorithm. 
26
 
 DL0 8.0=?
randomDL  
0.1=?
randomDL  
2.1=?
randomDL  
5.1=?
randomDL  
#? 50 41 41 44 46 
DL(?) 47,630 44,892 45,126 45,788 46,780 
DL(F) 160 201 198 187 177 
DL(P) 2,246 2,193 2,195 2,212 2,223 
DL(M) 50,218 47,468 47,700 48,369 49,362 
DL(D) 315,165 320,200 319,551 318,537 316,874 
Total DL 365,383 367,669 367,252 366,907 366,237 
Table 2. DL and its individual components for baseline and the 
resulting cases when collapsing signatures randomly.
 
 
 
unsupervised fashion, we take this evidence as 
supporting the appropriateness of our algorithm as 
a means of collapsing signatures in a 
grammatically and empirically reasonable way. 
We conclude that the collapsing of signatures 
on the basis of similarity of context vectors of 
signature transforms (in a space consisting of high 
frequency words and signature transforms) 
provides us with a useful and significant step 
towards solving the signature collapsing problem. 
In the context of the broader project, we will be 
able to use signature transforms as a more effective 
means for projecting lexical categories in an 
unsupervised way. 
As Table 1 shows, we achieve up to 30% 
decrease in the number of signatures through our 
proposed collapse. We are currently exploring 
ways to increase this value through powers of the 
adjacency matrix of the signature graph. 
In other work in progress, we explore the 
equally important signature purity problem in 
graph theoretic terms: we split ambiguous 
signature transforms into separate categories when 
we can determine that the edges connecting left-
context features and right-context features can be 
resolved into two sets (corresponding to the 
distinct categories of the transform) whose left-
features have no (or little) overlap and whose right 
features have no (or little) overlap. We employ the 
notion of minimum cut of a weighted graph to 
detect this situation.
 
References  
Brown, Peter F., Vincent J. Della Pietra, Peter V. 
deSouza, Jennifer C. Lai, and Robert L. Mercer. 
1992. Class-based n-gram models of natural 
language. Computational Linguistics, 18(4): 467-
479.  
Goldsmith, John. 2001. Unsupervised learning of the 
morphology of a natural language. Computational 
Linguistics, 27(2): 153-198.  
Higgins, Derrick. 2002. A Multi-modular Approach to 
Model Selection in Statistical NLP. University of 
Chicago Ph.D. thesis. 
Schmid, Helmut. 1994. Probabilistic part-of-speech 
tagging using decision trees.. International 
Conference on New Methods in Language 
Processing 
Kucera, Henry and W. Nelson Francis. 1967. 
Computational Analysis of Present-day American 
English. Brown University Press.  
Rissanen, Jorma. 1989. Stochastic Complexity in 
Statistical Inquiry. Singapore: World Scientific.  
Sch?tze, Hinrich. 1997. Ambiguity Resolution in 
Language Learning. CSLI Publications. Stanford 
CA.  
Sparck Jones, Karen. 1973. Index term weighting. 
Information Storage and Retrieval 9:619-33. 
Wicentowski, Richard. 2002. Modeling and Learning 
Multilingual Inflectional Morphology in a Minimally 
Supervised Framework. Johns Hopkins University 
Ph.D. thesis. 
27
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 28?35,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The SED heuristic for morpheme discovery:  
a  look at Swahili 
 
 
Yu Hu and Irina Matveeva  
Department of  
Computer Science 
The University of Chicago 
Chicago IL 60637 
yuhu@cs.uchicago.edu
matveeva 
@uchicago.edu  
John Goldsmith 
Departments of Linguistics and 
Computer Science  
The University of Chicago 
Chicago IL 60637 
ja-goldsmith 
@uchicago.edu 
 
Colin Sprague 
Department of Linguistics 
The University of Chicago 
Chicago IL 60637 
sprague 
@uchicago.edu  
  
Abstract 
This paper describes a heuristic for 
morpheme- and morphology-learning 
based on string edit distance. 
Experiments with a 7,000 word corpus 
of Swahili, a language with a rich 
morphology, support the effectiveness 
of this approach. 
1 Introduction 
This paper describes work on a technique for the 
unsupervised learning of the morphology of 
natural languages which employs the familiar 
string edit distance (SED) algorithm (Wagner 
and Fischer 1974 and elsewhere) in its first 
stage;  we refer to it here as the SED heuristic. 
The heuristic finds 3- and 4-state finite state 
automata (FSAs) from untagged corpora. We 
focus on Swahili, a Bantu language of East 
Africa, because of the very high average number 
of morphemes per word, especially in the verbal 
system, a system that presents a real challenge to 
other systems discussed in the literature.1 
In Section 2, we present the SED heuristic, 
with precision and recall figures for its 
application to a corpus of Swahili. In Section 3, 
we propose three elaborations and extensions of 
                                                     
1 An earlier version of this paper, with a more detailed 
discussion of the material presented in Section 3, is 
available at Goldsmith et al(2005). 
this approach, and in Section 4, we describe and 
evaluate the results from applying these 
extensions to the corpus of Swahili.2  
2 SED-based heuristic 
Most systems designed to learn natural language 
morphology automatically can be viewed as 
being composed of an initial heuristic 
component and a subsequent explicit model. The 
initial or bootstrapping heuristic, as the name 
suggests, is designed to rapidly come up with a 
set of candidate strings of morphemes, while the 
model consists of an explicit formulation of 
either (1) what constitutes an adequate 
morphology for a set of data, or (2) an objective 
function that must be optimized, given a corpus 
of data, in order to find the correct 
morphological analysis.  
The best known and most widely used 
heuristic is due to Zellig Harris (1955) (see also 
Harris (1967) and Hafer and Weiss (1974) for an 
evaluation based on an English corpus), using a 
notion that Harris called successor frequency 
(henceforth, SF). Harris' notion can be 
succinctly described in contemporary terms: if 
we encode all of the data in the data structure 
known as a trie, with each node in the trie 
dominating all strings which share a common 
                                                     
2 SED has been used in unsupervised language learning in a 
number of studies; see, for example, van Zaanen (2000) 
and references there, where syntactic structure is studied in 
a similar context. To our knowledge, it has not been used in 
the context of morpheme detection. 
28
string prefix,3 then each branching node in the 
trie is associated with a morpheme break. For 
example, a typical corpus of English may 
contain the words governed, governing, 
government, governor, and governs. If this data 
is encoded in the usual way in a trie, then a 
single node will exist in the trie which represents 
the string prefix govern and which dominates 
five leaves corresponding to these five words. 
Harris's SF-based heuristic algorithm would 
propose a morpheme boundary after govern on 
this basis. In contemporary terms, we can 
interpret Harris?s heuristic as providing sets of 
simple finite state automata, as in (1), which 
generate a string prefix (PF1) followed by a set 
of string suffixes (SFi) based on the 
measurement of a successor frequency greater 
than 1 (or some threshold value) at the string 
position following PF1. 
(1)  
SF1
SF3
PF1 SF2
 
A variant on the SF-based heuristic, 
predecessor frequency (henceforth, PF), calls for 
encoding words in a trie from right to left. In 
such a PF-trie, each node dominates all strings 
that share a common string suffix. In general, we 
expect SF to work best in a suffixing language, 
and PF to work best in prefixing language; 
Swahili, like all the Bantu languages, is 
primarily a prefixing language, but it has a 
significant number of important suffixes in both 
the verbal and the nominal systems. 
Goldsmith (2001) argues for using the 
discovery of signatures as the bootstrapping 
heuristic, where a signature is a maximal set of 
stems and suffixes with the property that all 
combinations of stems and suffixes are found in 
the corpus in question. We interpret Goldsmith?s 
signatures as extensions of FSAs as in (1) to 
                                                     
3 We use the terms string prefix and string suffix in the 
computer science sense: a string S is a string prefix of a 
string X iff there exists a string T such that X = S.T, where 
?.? is the string concatenation operator; under such 
conditions, T is likewise a string suffix of X. Otherwise, we 
use the terms prefix and suffix in the linguistic sense, and a 
string prefix (e.g., jump) may be a linguistic stem, as in 
jump-ing. 
FSAs as in (2); (2) characterizes Goldsmith?s 
notion of signature in term of FSAs. In 
particular, a signature is a set of forms that can 
be characterized by an FSA of 3 states. 
(2)  
PF1 SF1
PF3 SF2
PF2
 
 
We propose a simple alternative heuristic 
which utilizes the familiar dynamic 
programming algorithm for calculating string-
edit distance, and finding the best alignment 
between two arbitrary strings (Wagner and 
Fischer 1974). The algorithm finds subsets of 
the data that can be exactly-generated by 
sequential finite state automata of 3 and 4 states, 
as in (3), where the labels mi should be 
understood as cover terms for morphemes in 
general. An automaton exactly-generates a set of 
strings S if it generates all strings in S and no 
other strings; a sequential FSA is one of the 
form sketched graphically in (1)-(3), where there 
is a unique successor to each state. 
(3)  
M1 M4
M3 M6
M2
M7
M9
M5 M8
 
2.1 First stage: alignments. 
If presented with the pair of strings anapenda 
and anamupenda from an unknown language, it 
is not difficult for a human being to come up 
with the hypothesis that mu is a morpheme 
inside a larger word that is composed of at least 
two morphemes, perhaps ana- and -penda. The 
SED heuristic makes this observation explicit by 
building small FSAs of the form in (4), where at 
most one of m1 or m4 may be null, and at most 
one of m2 and m3 may be null: we refer to these 
as elementary alignments. The strings m2 and m3 
are called counterparts; the pairs of strings m1 
and m4 are called the context (of the 
counterparts). (Indeed, we consider this kind of 
string comparison to be a plausible candidate for 
human language learning; see Dahan and Brent 
1999). 
 
29
 
 
(4)  
1 432m1 m4
m3
m2
 
The first stage of the algorithm consists of 
looking at all pairs of words S, T in the corpus, 
and passing through the following steps:  
We apply several initial heuristics to 
eliminate a large proportion of the pairs of 
strings before applying the familiar SED 
algorithm to them, in view of the relative 
slowness of the SED algorithm; see Goldsmith 
et al(2005) for further details.  
We compute the optimal alignment of S and 
T using the SED algorithm, where alignment 
between two identical letters (which we call 
twins) is assigned a cost of 0, alignment between 
two different letters (which we call siblings) is 
assigned a cost of 1.5, and a letter in one string 
not aligned with a segment on the other string 
(which we call an orphan) is assigned a cost of 
1. An alignment as in (5) is thus assigned a cost 
of 5, based on a cost of 1.5 assigned to each 
broken line, and 1  to each dotted line that ends 
in a square box. 
(5)   
n i l i m u p e n d a
n i t a k a m u p e n d a  
There is a natural map from every alignment 
to a unique sequence of pairs, where every pair 
is either of the form (S[i], T[j]) (representing 
either a twin or sibling case) or of the form (S[i], 
0) or (0, T[j]) (representing the orphan case). We 
then divide the alignment up into perfect and 
imperfect spans: perfect spans are composed of 
maximal sequences of twin pairs, while 
imperfect spans are composed of maximal 
sequences of sibling or orphan pairs. This is 
illustrated in (6). 
(6)  
 
 
 
 
 
 
There is a natural equivalence between 
alignments and sequential FSAs as in (4), where 
perfect spans correspond to pairs of adjacent 
states with unique transitions and imperfect 
spans correspond to pairs of adjacent states with 
two transitions, and we will henceforth use the 
FSA notation to describe our algorithm. 
2.2 Collapsing alignments 
As we noted above (4), for any elementary 
alignment, a context is defined: the pair of 
strings (one of them possibly null) which 
surround the pair of counterparts. Our first goal 
is to collapse alignments that share their context. 
We do this in the following way. 
Let us define the set of strings associated 
with the paths leaving a state S as the production 
of state S. A four-state sequential FSA, as in (4), 
has three states with non-null productions; if this 
particular FSA corresponds to an elementary 
alignment, then two of the state-productions 
contain exactly one string?and these state-
productions define the context? and one of the 
state-productions contains exactly two strings 
(one possibly the null string)?this defines the 
counterparts. If we have two such four-state 
FSAs whose context are identical, then we 
collapse the two FSAs into a single conflated 
FSA in which the context states and their 
productions are identical, and the states that 
produced the counterparts are collapsed by 
creating a state that produces the union of the 
productions of the original states. This is 
illustrated in (7): the two FSAs in (7a) share a 
context, generated by their states 1 and 3, and 
they are collapsed to form the FSA in (7b), in 
which the context states remain unchanged, and 
the counterpart states, labeled ?2?, are collapsed 
to form a new state ?2? whose production is the 
union of the productions of the original states. 
(7)  
a.  
1 432m1 m4
1 432m1 m4
m7
m8
m3
m2
 
 
n i   l i   m u p e n d a
n i   t a k a   m u p e n d a
30
 
 
b. 
1 432m1 m4
m8
m7
m3
m2
 
2.3 Collapsing the resulting sequential 
FSAs 
We now generalize the procedure described in 
the preceding section to collapse any two 
sequential FSAs for which all but one of the 
corresponding states have exactly the same 
production. For example, the two sequential 
FSAs in (8a) are collapsed into (8b). 
Three and four-state sequential FSAs as in 
(8b), where at least two of the state-transitions 
generate more than one morpheme, form the set 
of templates derived from our bootstrapping 
heuristic. Each such template can be usefully 
assigned a quantitative score based on the 
number of letters ?saved? by the use of the 
template to generate the words, in the following 
sense. The template in (8b) summarizes four 
words: aliyesema, alimfuata, anayesema, and 
anamfuata. The total string length of these 
words is 36, while the total number of letters in 
the strings associated with the transitions in the 
FSA is 1+4+12 = 17; we say that the FSA saves 
36-17 = 19 letters. In actual practice, the 
significant templates discovered save on the 
order of 200 to 5,000 letters, and ranking them 
by the number of letters saved is a good measure 
of how significant they are in the overall 
morphology of the language. We refer to this 
score as a template?s robustness; we employ this 
quantity again in section 3.1 below. 
By this ranking, the top template found in our 
Swahili corpus of 50,000 running words was one 
that generated a and wa (class 1 and 2 subject 
markers) and followed by 246 correct verb 
continuations (all of them polymorphemic); the 
first 6 templates are summarized informally in 
Table 1. We note that the third and fourth 
template can also be collapsed to form a 
template of the form in (3), a point we return to 
below. Precision, recall, and F-score for these 
experiments are given in Table 2.  
 
(8)   
a. 
1 432a yesema
na
li
1 432a mfuata
na
li
 
 
b.  
1 432a
na
li yesema
mfuata  
 
State 1 State 2 State 3 
a, wa (sg., pl. 
human subject 
markers) 
246 stems  
ku, hu 
(infinitive, 
habitual 
markers) 
51 stems  
wa (pl. subject 
marker) 
ka, li (tense 
markers) 
25 stems 
a (sg. subject 
marker) 
ka, li (tense 
markers) 
29 stems 
a (sg. subject 
marker) 
ka, na (tense 
markers 
28 stems 
37 strings w (passive 
marker) 
a 
Table 1 Top templates in Swahili 
 
 Precision Recall  F-score 
SED 0.77 0.57 0.65 
SF 0.54 0.14 0.22 
PF 0.68 0.20 0.31 
Table 2 Results 
31
3 Further developments 
In this section, we describe three developments 
of the SED-based heuristic sketched in section 2. 
The first disambiguates which state it is that 
string material should be associated with in 
cases of ambiguity; the second collapses 
templates associated with similar morphological 
structure; the third uses the FSAs to predict 
words that do not actually occur in the corpus by 
hypothesizing stems on the basis of the 
established FSAs and as yet unanalyzed words 
in the corpus. 
3.1 Disambiguating FSAs 
In the case of a sequential FSA, when the final 
letter of the production of a (non-final) state S 
are identical, then that letter can be moved from 
being the string-suffix of all of the productions 
of state S to being the string-prefixes of all of 
the productions of the following state. More 
generally, when the n final letters of the 
productions of a state are identical, there is an n-
way ambiguity in the analysis, and the same 
holds symmetrically for the ambiguity that arises 
when the n initial letters of the production of a 
(non-initial) state.  
Thus two successive states, S and T, must (so 
to speak) fight over which will be responsible 
for generating the ambiguous string. We employ 
two steps to disambiguate these cases.  
Step 1: The first step is applicable when the 
number of distinct strings associated with states 
S and T are quite different in size (typically 
corresponding to the case where one generates 
grammatical morphemes and the other generates 
stems); in this case, we assign the ambiguous 
material to the state that generates the smaller 
number of strings. There is a natural motivation 
for this choice from the perspective of our desire 
to minimize the size of the grammar, if we 
consider the size of the grammar to be based, in 
part, on the sum of the lengths of the morphemes 
produced by each state. 
Step 2: It often happens that an ambiguity 
arises with regard to a string of one or more 
letters that could potentially be produced by 
either of a pair of successive states involving 
grammatical morphemes. To deal with this case, 
we make a decision that is also (like the 
preceding step) motivated by a desire to 
minimize the description length of the grammar. 
In this case, however, we think of the FSA as 
containing explicit strings (as we have assumed 
so far), but rather pointers to strings, and the 
?length? of a pointer to a string is inversely 
proportional to the logarithm of its frequency. 
Thus the overall use of a string in the grammar 
plays a crucial role in determining the length of 
a grammar, and we wish to maximize the 
appearance in our grammar of morphemes that 
are used frequently, and minimize the use of 
morphemes that are used rarely. 
We implement this idea by collecting a table 
of all of the morphemes produced by our FSA, 
and assigning each a score which consists of the 
sum of the robustness scores of each template 
they occur in (see discussion just above (8)). 
Thus morphemes occurring in several high 
robustness templates will have high scores; 
morphemes appearing in a small number of 
lowly ranked templates will have low scores. 
To disambiguate strings which could be 
produced by either of two successive states, we 
consider all possible parsings of the string 
between the states, and score each parsing as the 
sum of the scores of the component morphemes; 
we chose the parsing for which the total score is 
a maximum. 
 For example, Swahili has two common tense 
markers, ka and ki, and this step corrected a 
template from {ak}+{a,i}+{stems} to 
{a}+{ka,ki}+{stems}, and others of similar 
form. It also did some useful splitting of joined 
morphemes, as when it modified a template 
{wali} + {NULL, po} + {stems} to {wa} + {li, 
lipo} + {stems}. In this case, wali should indeed 
be split into wa + li (subject and tense markers, 
resp.), and while the change creates an error (in 
the sense that lipo is, in fact, two morphemes; po 
is a subordinate clause marker), the resulting 
error occurs considerably less often in the data, 
and the correct template will better be able to be 
integrated with out templates. 
3.2 Template collapsing 
From a linguistic point of view, the SED-based 
heuristic creates too many FSAs because it stays 
too close to the data provided by the corpus. The 
only way to get a more correct grammar is by 
collapsing the FSAs, which will have as a 
32
consequence the generation of new words not 
found in the corpus. We apply the following 
relatively conservative strategy for collapsing 
two templates. 
We compare templates of the same number 
of states, and distinguish between states that 
produce grammatical morphemes (five or fewer 
in number) and those that produce stems (that is, 
lexical morphemes, identified as being six or 
more in number). We collapse two templates if 
the productions of the corresponding states 
satisfy the following conditions: if the states 
generate stems, then the intersection of the 
productions must be at least two stems, while if 
the states are grammatical morphemes, then the 
productions of one pair of corresponding states 
must be identical, while for the other pair, the 
symmetric difference of the productions must be 
no greater than two in number (that is, the 
number of morphemes produced by the state of 
one template but not the other must not exceed 
2).  
3.3 Reparsing words in the corpus and 
predicting new words 
When we create robust FSAs?that is, FSAs that 
generate a large number of words?we are in a 
position to go back to the corpus and reanalyze a 
large number of words that could not be 
previously analyzed. That is, a 4-state FSA in 
which each state produced two strings generates 
8 words, and all 8 words must appear in the 
corpus for the method described so far in order 
for this particular FSA to generate any of them. 
But that condition is unlikely to be satisfied for 
any but the most common of morphemes, so we 
need to go back to the corpus and infer the 
existence of new stems (as defined operationally 
in the preceding paragraph) based on their 
occurrence in several, but not all possible, 
words.  If there exist 3 distinct words in the 
corpus which would all be generated by a 
template if a given stem were added to the 
template, we add that stem to the template. 
4 Experiments and Results 
In this section, we present three sets of 
evaluations of the refinements of the SED 
heuristics described in the preceding section. We 
used a corpus of 7,180 distinct words occurring 
in 50,000 running words from a Swahili 
translation of the Bible obtained on the internet. 
4.1 Disambiguating FSAs 
In order to evaluate the effects of the 
disambiguating of FSAs described in section 
3.1, we compare precision and recall of the 
identification of morpheme boundaries using the 
SED method with and without the 
disambiguation procedure described above. In 
Figures 1 and 2, we graph precision and recall 
for the top 10% of the templates, displayed as 
the leftmost point, for the top 20% of the 
templates, displayed as the second point from 
the left; and so on, because the higher ranked 
FSAs are more intrinsically more reliable than 
the lower ranked ones. We see that 
disambiguation repairs almost 50% of the 
previous errors, and increases recalls by about 
10%. With these increases in precision and 
recall, it is clear that the disambiguating step 
provides a considerably more accurate 
morpheme boundary discovery procedure. 
Precision
0.7
0.75
0.8
0.85
0.9
0.95
1
10 20 30 40 50 60 70 80 90 10
0
Deciles(%)
Pr
ec
is
io
n
Without
With
 
Figure 1 Comparison of precision 
 
Compare Recalls 
0.3
0.34
0.38
0.42
0.46
0.5
10 20 30 40 50 60 70 80 90 100
Deciles(%)
R
ec
al
ls
Without
With
Figure 2 Comparison of recall 
33
4.2 Template collapsing 
The second refinement discussed above 
consists of finding pairs of similar templates, 
collapsing them as appropriate, and thus creating 
patterns that generate new words that did not 
participate in the formation of the original 
templates. These new words may or may not 
themselves appear in the corpus. We are, 
however, able to judge their morphological well-
formedness by inspection. We list in Table 3 the 
entire list of eight templates that are collapsed in 
this step. 
All of the templates which are collapsed in 
this step are in fact of the same morphological 
structure (with one very minor exception4): they 
are of the form subject marker + tense marker + 
stem, and the collapsing induced in this 
procedure correctly creates larger templates of 
precisely the same structure, generating new 
words not seen in the corpus that are in fact 
correct from our (non-native speaker) 
inspection. We submitted the new words to 
Yahoo to test the words ?existence? by their 
existence on the internet, and actually found an 
average of 87% of the predicted words in a 
template; see the last column in Table 3 for 
details. 
4.3 Reparsing 
After previous refinements, we obtain a 
number of robust FSAs, for example, those 
collapsed templates in Table 3. With them, we 
then search the corpus for those words that can 
only be partly fitted into these FSAs and 
generate associated stems. Table 4 shows the 
reparsed words that had not been parsed by 
earlier templates and also newly added stems for 
some robust FSAs (the four collapsed templates 
in Table 3).  Stems such as anza ?begin? and 
fanya ?do? are thus added to the first template, 
and all words derived by prepending a tense 
marker and a subject marker are indeed accurate 
words. As the words in Table 4 suggest, the 
reparsing process adds new, common stems to 
the stem-column of the templates, thus making it 
                                                     
4 The exception involves the distinct morpheme po, a 
subordinate clause marker which must ultimately be 
analyzed as appearing in a distinct template column to the 
right of the tense markers. 
easier for the collapsing function to find 
similarities across related templates. 
In future work, we will take use the larger 
templates, populated with more stems, and input 
them to the collapsing function described in 3.2.  
5 Conclusions 
On the basis of the experiments with Swahili 
described in this paper, the SED heuristic 
appears to be a useful tool for the discovery of 
morphemes in languages with rich 
morphologies, and for the discovery of the FSAs 
that constitute the morphologies of those 
languages. 
Ultimately, the value of the heuristic is best 
tested against a range of languages with complex 
concatenative morphologies. While a thorough 
discussion would take us well beyond the limits 
of this paper, we have applied the SED heuristic 
to English, Hungarian, and Finnish as well as 
Swahili. For English, unsurprisingly, the method 
works as well as the SF and PF methods, though 
a bit more slowly, while for Hungarian and 
Finnish, the results appear promising, and a 
comparison with Creutz and Lagus (2004) for 
Finnish, for example, would be appealing. 
34
 
  
One Template 
 
The other template Collapsed Template 
% found on 
Yahoo search 
1 {a}-{ka,na}-{stems} {a}-{ka,ki}-{stems} {a}-{ka,ki,na}-{stems} 86 (37/43) 
2 {wa}-{ka,na}-{stems} {wa}-{ka,ki}-{stems} {wa}-{ka,ki,na}-{stems} 95 (21/22) 
3 {a}-{ka,ki,na}-{stems} {wa}-{ka,ki,na}-{stems} {a,wa}-{ka,ki,na}-{stems} 84 (154/183) 
4 {a}-{liye,me}-{stems} {a}-{liye,li}-{stems} {a}-{liye,li,me}-{stems} 100 (21/21) 
5 {a}-{ki,li}-{stems} {wa}-{ki,li}-{stems} {a,wa}-{ki,li}-{stems} 90 (36/40) 
6 {a}-{lipo,li}-{stems} {wa}-{lipo,li}-{stems} {a,wa}-{lipo,li}-{stems} 90 (27/30) 
7 {a,wa}-{ki,li}-{stems} {a,wa}-{lipo,li}-{stems} {a,wa}-{ki,lipo,li}-{stems} 74 (52/70) 
8 {a}-{na,naye}-{stems} {a}-{na,ta}-{stems} {a}-{na,ta,naye}-{stems} 80 (12/15) 
Table 3  Collapsed Templates and Created Words Sample. 
 
 
 
 
 Template Reparsed Words Not Parsed 
Before 
Added Stems  
1 {a, wa}-{ka,ki,na}-{stems} akawakweza, akiwa, anacho, 
akibatiza,  ? 
toka, anza, waita, fanya, enda, ? 
2 {a}-{li,liye,me }-{stems} ameinuka, ameugua, alivyo,  
aliyoniagiza,  ? 
zaliwa, kuwa, fanya, sema 
3 {a, wa}-{ki,li,lipo}-{stems} alimtoboa,  alimtaka,  
waliamini,  ? 
pata, kuwa, kaa, fanya, chukua, 
fika, ? 
4 {a} ? {na,naye,ta}-{stems} analazwa,  atanitukuza,  anaye,  
anakuita,   ? 
ingia, sema 
Table 4 Reparsed words and "discovered" stems 
 
References 
Creutz, Mathias, and Krista Lagus. (2004). Induction 
of a simple morphology for highly inflecting 
languages. Proceedings of the Workshop of 
SIGPHON (Barcelona). 
Dahan, Delphine, and Michael Brent. (1999). On the 
discovery of novel world-like units from 
utterances. Journal of Experimental Psychology: 
General  128: 165-185. 
Goldsmith, John. (2001).  Unsupervised Learning of 
the Morphology of a Natural Language. 
Computational Linguistics 27(2): 153-198. 
Goldsmith, John, Yu Hu, Irina Matveeva, and Colin 
Sprague. 2005. A heuristic for morpheme 
discovery based on string edit distance. Technical 
Report TR-2005-4. Department of Computer 
Science. University of Chicago. 
Hafer, M. A., Weiss, S. F.  (1974). Word 
segmentation by letter successor varieties. 
Information Storage and Retrieval 10: 371-385. 
Harris, Zellig. (1955). From Phoneme to Morpheme. 
Language 31: 190-222. 
Harris, Zellig. (1967). Morpheme Boundaries within 
Words: Report on a Computer Test. 
Transformations and Discourse Analysis Papers 
73.  
Oliver, Antoni, Irene Bastell?n, and Llu?s M?rquez. 
(2003). Uso de Internet para aumentar la cobertura 
de un sistema de adquisici?n l?xica del ruso. 
SEPLN 2003. 
Wagner, R. A., Fischer, M. J.  (1974). The string-to-
string correction problem. Journal of the 
Association for Computing Machinery 21(1): 168-
73. 
van Zaanen,  Menno. 2000. ABL: Alignment-Based 
Learning. Proceedings of the 17th Conference on 
Computational Linguistics, vol. 2. p. 961-67.
 
35
Workshop on TextGraphs, at HLT-NAACL 2006, pages 61?64,
New York City, June 2006. c?2006 Association for Computational Linguistics
Graph-based Generalized Latent Semantic Analysis
for Document Representation
Irina Matveeva
Dept. of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Dept. of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
Document indexing and representation of
term-document relations are very impor-
tant for document clustering and retrieval.
In this paper, we combine a graph-based
dimensionality reduction method with a
corpus-based association measure within
the Generalized Latent Semantic Analysis
framework. We evaluate the graph-based
GLSA on the document clustering task.
1 Introduction
Document indexing and representation of term-
document relations are very important issues for
document clustering and retrieval. Although the
vocabulary space is very large, content bearing
words are often combined into semantic classes that
contain synonyms and semantically related words.
Hence there has been a considerable interest in low-
dimensional term and document representations.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. The dimensions of the LSA
vector space can be interpreted as latent semantic
concepts. The cosine similarity between the LSA
document vectors corresponds to documents? sim-
ilarity in the input space. LSA preserves the docu-
ments similarities which are based on the inner prod-
ucts of the input bag-of-word documents and it pre-
serves these similarities globally.
More recently, a number of graph-based dimen-
sionality reduction techniques were successfully ap-
plied to document clustering and retrieval (Belkin
and Niyogi, 2003; He et al, 2004). The main ad-
vantage of the graph-based approaches over LSA is
the notion of locality. Laplacian Eigenmaps Embed-
ding (Belkin and Niyogi, 2003) and Locality Pre-
serving Indexing (LPI) (He et al, 2004) discover the
local structure of the term and document space and
compute a semantic subspace with a stronger dis-
criminative power. Laplacian Eigenmaps Embed-
ding and LPI preserve the input similarities only
locally, because this information is most reliable.
Laplacian Eigenmaps Embedding does not provide
a fold-in procedure for unseen documents. LPI
is a linear approximation to Laplacian Eigenmaps
Embedding that eliminates this problem. Similar
to LSA, the input similarities to LPI are based on
the inner products of the bag-of-word documents.
Laplacian Eigenmaps Embedding can use any kind
of similarity in the original space.
Generalized Latent Semantic Analysis
(GLSA) (Matveeva et al, 2005) is a frame-
work for computing semantically motivated term
and document vectors. It extends the LSA approach
by focusing on term vectors instead of the dual
document-term representation. GLSA requires a
measure of semantic association between terms and
a method of dimensionality reduction.
In this paper, we use GLSA with point-wise mu-
tual information as a term association measure. We
introduce the notion of locality into this framework
and propose to use Laplacian Eigenmaps Embed-
ding as a dimensionality reduction algorithm. We
evaluate the importance of locality for document
representation in document clustering experiments.
The rest of the paper is organized as follows. Sec-
61
tion 2 contains the outline of the graph-based GLSA
algorithm. Section 3 presents our experiments, fol-
lowed by conclusion in section 4.
2 Graph-based GLSA
2.1 GLSA Framework
The GLSA algorithm (Matveeva et al, 2005) has the
following setup. The input is a document collection
C with vocabulary V and a large corpus W .
1. For the vocabulary in V , obtain a matrix of
pair-wise similarities, S, using the corpus W
2. Obtain the matrix UT of a low dimensional
vector space representation of terms that pre-
serves the similarities in S, UT ? Rk?|V |
3. Construct the term document matrix D for C
4. Compute document vectors by taking linear
combinations of term vectors D? = UTD
The columns of D? are documents in the k-
dimensional space.
GLSA approach can combine any kind of simi-
larity measure on the space of terms with any suit-
able method of dimensionality reduction. The inner
product between the term and document vectors in
the GLSA space preserves the semantic association
in the input space. The traditional term-document
matrix is used in the last step to provide the weights
in the linear combination of term vectors. LSA is
a special case of GLSA that uses inner product in
step 1 and singular value decomposition in step 2,
see (Bartell et al, 1992).
2.2 Singular Value Decomposition
Given any matrix S, its singular value decompo-
sition (SVD) is S = U?V T . The matrix Sk =
U?kV T is obtained by setting all but the first k di-
agonal elements in ? to zero. If S is symmetric, as
in the GLSA case, U = V and Sk = U?kUT . The
inner product between the GLSA term vectors com-
puted as U?1/2k optimally preserves the similarities
in S wrt square loss.
The basic GLSA computes the SVD of S and uses
k eigenvectors corresponding to the largest eigenval-
ues as a representation for term vectors. We will re-
fer to this approach as GLSA. As for LSA, the simi-
larities are preserved globally.
2.3 Laplacian Eigenmaps Embedding
We used the Laplacian Embedding algo-
rithm (Belkin and Niyogi, 2003) in step 2 of
the GLSA algorithm to compute low-dimensional
term vectors. Laplacian Eigenmaps Embedding
preserves the similarities in S only locally since
local information is often more reliable. We will
refer to this variant of GLSA as GLSAL.
The Laplacian Eigenmaps Embedding algorithm
computes the low dimensional vectors y to minimize
under certain constraints
?
ij
||yi ? yj||2Wij .
W is the weight matrix based on the graph adjacency
matrix. Wij is large if terms i and j are similar ac-
cording to S. Wij can be interpreted as the penalty
of mapping similar terms far apart in the Laplacian
Embedding space, see (Belkin and Niyogi, 2003)
for details. In our experiments we used a binary ad-
jacency matrix W . Wij = 1 if terms i and j are
among the k nearest neighbors of each other and is
zero otherwise.
2.4 Measure of Semantic Association
Following (Matveeva et al, 2005), we primarily
used point-wise mutual information (PMI) as a mea-
sures of semantic association in step 1 of GLSA.
PMI between random variables representing two
words, w1 and w2, is computed as
PMI(w1, w2) = log
P (W1 = 1,W2 = 1)
P (W1 = 1)P (W2 = 1)
.
2.5 GLSA Space
GLSA offers a greater flexibility in exploring the
notion of semantic relatedness between terms. In
our preliminary experiments, we obtained the matrix
of semantic associations in step 1 of GLSA using
point-wise mutual information (PMI), likelihood ra-
tio and ?2 test. Although PMI showed the best per-
formance, other measures are particularly interest-
ing in combination with the Laplacian Embedding.
Related approaches, such as LSA, the Word Space
Model (WS) (Schu?tze, 1998) and Latent Relational
Analysis (LRA) (Turney, 2004) are limited to only
one measure of semantic association and preserve
the similarities globally.
62
Assuming that the vocabulary space has some un-
derlying low dimensional semantic manifold. Lapla-
cian Embedding algorithm tries to approximate this
manifold by relying only on the local similarity in-
formation. It uses the nearest neighbors graph con-
structed using the pair-wise term similarities. The
computations of the Laplacian Embedding uses the
graph adjacency matrix W . This matrix can be bi-
nary or use weighted similarities. The advantage
of the binary adjacency matrix is that it conveys
the neighborhood information without relying on in-
dividual similarity values. It is important for co-
occurrence based similarity measures, see discus-
sion in (Manning and Schu?tze, 1999).
The Locality Preserving Indexing (He et al,
2004) has a similar notion of locality but has to use
bag-of-words document vectors.
3 Document Clustering Experiments
We conducted a document clustering experiment for
the Reuters-21578 collection. To collect the co-
occurrence statistics for the similarities matrix S
we used a subset of the English Gigaword collec-
tion (LDC), containing New York Times articles la-
beled as ?story?. We had 1,119,364 documents with
771,451 terms. We used the Lemur toolkit1 to tok-
enize and index all document collections used in our
experiments, with stemming and a list of stop words.
Since Locality Preserving Indexing algorithm
(LPI) is most related to the graph-based GLSAL, we
ran experiments similar to those reported in (He et
al., 2004). We computed the GLSA document vec-
tors for the 20 largest categories from the Reuters-
21578 document collection. We had 8564 docu-
ments and 7173 terms. We used the same list of 30
TREC words as in (He et al, 2004) which are listed
in table 12. For each word on this list, we generated
a cluster as a subset of Reuters documents that con-
tained this word. Clusters are not disjoint and con-
tain documents from different Reuters categories.
We computed GLSA, GLSAL, LSA and LPI rep-
resentations. We report the results for k = 5 for
the k nearest neighbors graph for LPI and Laplacian
Embedding, and binary weights for the adjacency
1http://www.lemurproject.org/
2We used 28 words because we used stemming whereas (He
et al, 2004) did not, so that in two cases, two words were re-
duces to the same stem.
matrix. We report results for 300 embedding dimen-
sions for GLSA, LPI and LSA and 500 dimensions
for GLSAL.
We evaluate these representations in terms of how
well the cosine similarity between the document
vectors within each cluster corresponds to the true
semantic similarity. We expect documents from the
same Reuters category to have higher similarity.
For each cluster we computed all pair-wise doc-
ument similarities. All pair-wise similarities were
sorted in decreasing order. The term ?inter-pair? de-
scribes a pair of documents that have the same label.
For the kth inter-pair, we computed precision at k as:
precision(pk) =
#inter ? pairs pj, s.t. j < k
k ,
where pj refers to the jth inter-pair. The average
of the precision values for each of the inter-pairs was
used as the average precision for the particular doc-
ument cluster.
Table 1 summarizes the results. The first column
shows the words according to which document clus-
ters were generated and the entropy of the category
distribution within that cluster. The baseline was to
use the tf document vectors. We report results for
GLSA, GLSAL, LSA and LPI. The LSA and LPI
computations were based solely on the Reuters col-
lection. For GLSA and GLSALwe used the term as-
sociations computed for the Gigaword collection, as
described above. Therefore, the similarities that are
preserved are quite different. For LSA and LPI they
reflect the term distribution specific for the Reuters
collection whereas for GLSA they are more general.
By paired 2-tailed t-test, at p ? 0.05, GLSA outper-
formed all other approaches. There was no signifi-
cant difference in performance of GLSAL, LSA and
the baseline. Disappointingly, we could not achieve
good performance with LPI. Its performance varies
over clusters similar to that of other approaches but
the average is significantly lower. We would like
to stress that the comparison of our results to those
presented in (He et al, 2004) are only suggestive
since (He et al, 2004) applied LPI to each cluster
separately and used PCA as preprocessing. We com-
puted the LPI representation for the full collection
and did not use PCA.
63
word tf glsa glsaL lsa lpi
agreement(1) 0.74 0.73 0.73 0.75 0.46
american(0.8) 0.63 0.72 0.59 0.64 0.36
bank(1.4) 0.45 0.52 0.40 0.48 0.28
control(0.7) 0.78 0.82 0.80 0.80 0.58
domestic(0.8) 0.64 0.68 0.66 0.68 0.35
export(0.8) 0.64 0.65 0.70 0.67 0.37
five(1.3) 0.74 0.77 0.71 0.70 0.40
foreign(1.2) 0.51 0.58 0.55 0.56 0.28
growth(1) 0.51 0.58 0.48 0.54 0.32
income(0.5) 0.84 0.86 0.83 0.80 0.69
increase(1.3) 0.51 0.61 0.53 0.53 0.29
industrial(1.2) 0.59 0.66 0.58 0.61 0.34
internat.(1.1) 0.58 0.59 0.54 0.61 0.34
investment(1) 0.68 0.77 0.70 0.72 0.46
loss(0.3) 0.98 0.99 0.98 0.98 0.88
money(1.1) 0.70 0.62 0.71 0.65 0.38
national(1.3) 0.49 0.58 0.49 0.55 0.27
price(1.2) 0.53 0.63 0.57 0.57 0.29
production(1) 0.56 0.66 0.58 0.59 0.29
public(1.2) 0.58 0.60 0.57 0.57 0.31
rate(1.1) 0.61 0.62 0.64 0.60 0.35
report(1.2) 0.66 0.72 0.62 0.65 0.35
service(0.9) 0.59 0.66 0.56 0.61 0.39
source(1.2) 0.56 0.54 0.59 0.60 0.27
talk(0.9) 0.74 0.67 0.73 0.74 0.39
tax(0.7) 0.91 0.93 0.90 0.89 0.67
trade(1) 0.85 0.74 0.82 0.60 0.33
world(1.1) 0.63 0.65 0.68 0.66 0.33
Av. Acc 0.65 0.68 0.65 0.66 0.40
Table 1: Average inter-pairs accuracy.
The inter-pair accuracy depended on the cate-
gories distribution within clusters. For more homo-
geneous clusters, e.g. ?loss?, all methods (except
LPI) achieve similar precision. For less homoge-
neous clusters, e.g. ?national?, ?industrial?, ?bank?,
GLSA and LSA outperformed the tf document vec-
tors more significantly.
4 Conclusion and Future Work
We introduced a graph-based method of dimension-
ality reduction into the GLSA framework. Lapla-
cian Eigenmaps Embedding preserves the similar-
ities only locally, thus providing a potentially bet-
ter approximation to the low dimensional semantic
space. We explored the role of locality in the GLSA
representation and used binary adjacency matrix as
similarity which was preserved and compared it to
GLSA with unnormalized PMI scores.
Our results did not show an advantage of GLSAL.
GLSAL and LPI seem to be very sensitive to the pa-
rameters of the neighborhood graph. We tried dif-
ferent parameter settings but more experiments are
required for a thorough analysis. We are also plan-
ning to use a different document collection to elimi-
nate the possible effect of the specific term distribu-
tion in the Reuters collection. Further experiments
are needed to make conclusions about the geometry
of the vocabulary space and the appropriateness of
these methods for term and document embedding.
References
Brian T. Bartell, Garrison W. Cottrell, and Richard K.
Belew. 1992. Latent semantic indexing is an optimal
special case of multidimensional scaling. In Proc. of
the 15th ACM SIGIR, pages 161?167. ACM Press.
Mikhail Belkin and Partha Niyogi. 2003. Laplacian
eigenmaps for dimensionality reduction and data rep-
resentation. Neural Computation, 15(6):1373?1396.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying Ma.
2004. Locality preserving indexing for document rep-
resentation. In Proc. of the 27rd ACM SIGIR, pages
96?103. ACM Press.
Chris Manning and Hinrich Schu?tze. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press. Cambridge, MA.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized latent semantic
analysis for term representation. In Proc. of RANLP.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(21):97?124.
Peter D. Turney. 2004. Human-level performance on
word analogy questions by latent relational analysis.
Technical report, Technical Report ERB-1118, NRC-
47422.
64
