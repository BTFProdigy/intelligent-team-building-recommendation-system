Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331?1340,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Two Level Model for Context Sensitive Inference Rules
Oren Melamud?, Jonathan Berant?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Computer Science Department, Stanford University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
joberant@stanford.edu
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates has been commonly ad-
dressed by computing distributional simi-
larity between vectors of argument words,
operating at the word space level. A re-
cent line of work, which addresses context
sensitivity of rules, represented contexts in
a latent topic space and computed similar-
ity over topic vectors. We propose a novel
two-level model, which computes simi-
larities between word-level vectors that
are biased by topic-level context repre-
sentations. Evaluations on a naturally-
distributed dataset show that our model
significantly outperforms prior word-level
and topic-level models. We also release a
first context-sensitive inference rule set.
1 Introduction
Inference rules for predicates have been identi-
fied as an important component in semantic ap-
plications, such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y? can be useful to extract pairs of drugs and the
illnesses which they relieve, or to answer a ques-
tion like ?Which drugs relieve headache??. Along
this vein, such inference rules constitute a crucial
component in generic modeling of textual infer-
ence, under the Textual Entailment paradigm (Da-
gan et al, 2006; Dinu and Wang, 2009).
Motivated by these needs, substantial research
was devoted to automatic learning of inference
rules from corpora, mostly in an unsupervised dis-
tributional setting. This research line was mainly
initiated by the highly-cited DIRT algorithm (Lin
and Pantel, 2001), which learns inference for bi-
nary predicates with two argument slots (like the
rule in the example above). DIRT represents a
predicate by two vectors, one for each of the ar-
gument slots, where the vector entries correspond
to the argument words that occurred with the pred-
icate in the corpus. Inference rules between pairs
of predicates are then identified by measuring the
similarity between their corresponding argument
vectors. This general scheme was further en-
hanced in several directions, e.g. directional sim-
ilarity (Bhagat et al, 2007; Szpektor and Dagan,
2008) and meta-classification over similarity val-
ues (Berant et al, 2011). Consequently, several
knowledge resources of inference rules were re-
leased, containing the top scoring rules for each
predicate (Schoenmackers et al, 2010; Berant et
al., 2011; Nakashole et al, 2012).
The above mentioned methods provide a sin-
gle confidence score for each rule, which is based
on the obtained degree of argument-vector sim-
ilarities. Thus, a system that applies an infer-
ence rule to a text may estimate the validity of
the rule application based on the pre-specified rule
score. However, the validity of an inference rule
may depend on the context in which it is applied,
such as the context specified by the given predi-
cate?s arguments. For example, ?AT&T acquire T-
Mobile ? AT&T purchase T-Mobile?, is a valid
application of the rule ?X acquire Y ? X pur-
chase Y?, while ?Children acquire skills ? Chil-
dren purchase skills? is not. To address this issue, a
line of works emerged which computes a context-
sensitive reliability score for each rule application,
based on the given context.
The major trend in context-sensitive inference
models utilizes latent or class-based methods for
context modeling (Pantel et al, 2007; Szpektor et
al., 2008; Ritter et al, 2010; Dinu and Lapata,
2010b). In particular, the more recent methods
(Ritter et al, 2010; Dinu and Lapata, 2010b) mod-
eled predicates in context as a probability distribu-
tion over topics learned by a Latent Dirichlet Allo-
1331
cation (LDA) model. Then, similarity is measured
between the two topic distribution vectors corre-
sponding to the two sides of the rule in the given
context, yielding a context-sensitive score for each
particular rule application.
We notice at this point that while context-
insensitive methods represent predicates by ar-
gument vectors in the original fine-grained word
space, context-sensitive methods represent them
as vectors at the level of latent topics. This raises
the question of whether such coarse-grained topic
vectors might be less informative in determining
the semantic similarity between the two predi-
cates.
To address this hypothesized caveat of prior
context-sensitive rule scoring methods, we pro-
pose a novel generic scheme that integrates word-
level and topic-level representations. Our scheme
can be applied on top of any context-insensitive
?base? similarity measure for rule learning, which
operates at the word level, such as Cosine or
Lin (Lin, 1998). Rather than computing a single
context-insensitive rule score, we compute a dis-
tinct word-level similarity score for each topic in
an LDA model. Then, when applying a rule in a
given context, these different scores are weighed
together based on the specific topic distribution
under the given context. This way, we calculate
similarity over vectors in the original word space,
while biasing them towards the given context via
a topic model.
In order to promote replicability and equal-term
comparison with our results, we based our experi-
ments on publicly available datasets, both for un-
supervised learning of the evaluated models and
for testing them over a random sample of rule ap-
plications. We apply our two-level scheme over
three state-of-the-art context-insensitive similar-
ity measures. The evaluation compares perfor-
mances both with the original context-insensitive
measures and with recent LDA-based context-
sensitive methods, showing consistent and robust
advantages of our scheme. Finally, we release
a context-sensitive rule resource comprising over
2,000 frequent verbs and one million rules.
2 Background and Model Setting
This section presents components of prior work
which are included in our model and experiments,
setting the technical preliminaries for the rest of
the paper. We first present context-insensitive rule
learning, based on distributional similarity at the
word level, and then context-sensitive scoring for
rule applications, based on topic-level similarity.
Some further discussion of related work appears
in Section 6.
2.1 Context-insensitive Rule Learning
A predicate inference rule ?LHS ? RHS?, such
as ?X acquire Y ? X purchase Y?, specifies a
directional inference relation between two predi-
cates. Each rule side consists of a lexical pred-
icate and (two) variable slots for its arguments.1
Different representations have been used to spec-
ify predicates and their argument slots, such as
word lemma sequences, regular expressions and
dependency parse fragments. A rule can be ap-
plied when its LHS matches a predicate with a
pair of arguments in a text, allowing us to infer its
RHS, with the corresponding instantiations for the
argument variables. For example, given the text
?AT&T acquires T-Mobile?, the above rule infers
?AT&T purchases T-Mobile?.
The DIRT algorithm (Lin and Pantel, 2001)
follows the distributional similarity paradigm to
learn predicate inference rules. For each predi-
cate, DIRT represents each of its argument slots
by an argument vector. We denote the two vectors
of the X and Y slots of a predicate pred by vxpred
and vypred, respectively. Each entry of a vector vcorresponds to a particular word (or term) w that
instantiated the argument slot in a learning corpus,
with a value v(w) = PMI(pred, w) (with PMI
standing for point-wise mutual information).
To learn inference rules, DIRT considers (in
principle) each pair of binary predicates that
occurred in the corpus for a candidate rule,
?LHS ? RHS?. Then, DIRT computes a reliabil-
ity score for the rule by combining the measured
similarities between the corresponding argument
vectors of the two rule sides. Concretely, denot-
ing by l and r the predicates appearing in the two
rule sides, DIRT?s reliability score is defined as
follows:
(1)scoreDIRT(LHS ? RHS)
=
?
sim(vxl , vxr ) ? sim(v
y
l , v
y
r )
where sim(v, v?) is a vector similarity measure.
Specifically, DIRT employs the Lin similarity
1We follow most of the inference-rule learning literature,
which focused on binary predicates. However, our context-
sensitive scheme can be applied to any arity.
1332
measure from (Lin, 1998), defined as follows:
(2)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
We note that the general DIRT scheme may be
used while employing other ?base? vector similar-
ity measures. For example, the Lin measure is
symmetric, and thus using it would yield the same
reliability score when swapping the two sides of
a rule. This issue has been addressed in a sepa-
rate line of research which introduced directional
similarity measures suitable for inference rela-
tions (Bhagat et al, 2007; Szpektor and Dagan,
2008; Kotlerman et al, 2010). In our experiments
we apply our proposed context-sensitive similarity
scheme over three different base similarity mea-
sures.
DIRT and similar context-insensitive inference
methods provide a single reliability score for a
learned inference rule, which aims to predict the
validity of the rule?s applications. However, as
exemplified in the Introduction, an inference rule
may be valid in some contexts but invalid in oth-
ers (e.g. acquiring entails purchasing for goods,
but not for skills). Since vector similarity in DIRT
is computed over the single aggregate argument
vector, the obtained reliability score tends to be
biased towards the dominant contexts of the in-
volved predicates. For example, we may expect
a higher score for ?acquire ? purchase? than for
?acquire ? learn?, since the former matches a
more frequent sense of acquire in a typical corpus.
Following this observation, it is desired to obtain
a context-sensitive reliability score for each rule
application in a given context, as described next.
2.2 Context-sensitive Rule Applications
To assess the reliability of applying an inference
rule in a given context we need some model for
context representation, that should affect the rule
reliability score. A major trend in past work is
to represent contexts in a reduced-dimensionality
latent or class-based model. A couple of earlier
works utilized a cluster-based model (Pantel et al,
2007) and an LSA-based model (Szpektor et al,
2008), in a selectional-preferences style approach.
Several more recent works utilize a Latent Dirich-
let Allocation (LDA) (Blei et al, 2003) frame-
work. We now present an underlying unified view
of the topic-level models in (Ritter et al, 2010;
Dinu and Lapata, 2010b), which we follow in our
own model and in comparative model evaluations.
We note that a similar LDA model construction
was employed also in (Se?aghdha, 2010), for esti-
mating predicate-argument likelihood.
First, an LDA model is constructed, as follows.
Similar to the construction of argument vectors
in the distributional model (described above in
subsection 2.1), all arguments instantiating each
predicate slot are extracted from a large learning
corpus. Then, for each slot of each predicate, a
pseudo-document is constructed containing the set
of all argument words that instantiated this slot in
the corpus. We denote the two documents con-
structed for the X and Y slots of a predicate pred
by dxpred and dypred, respectively. In comparison tothe distributional model, these two documents cor-
respond to the analogous argument vectors vxpred
and vypred, both containing exactly the same set ofwords.
Next, an LDA model is learned from the set
of all pseudo-documents, extracted for all predi-
cates.2 The learning process results in the con-
struction of K latent topics, where each topic t
specifies a distribution over all words, denoted by
p(w|t), and a topic distribution for each pseudo-
document d, denoted by p(t|d).
Within the LDA model we can derive the
a-posteriori topic distribution conditioned on a
particular word within a document, denoted by
p(t|d,w) ? p(w|t) ? p(t|d). In the topic-level
model, d corresponds to a predicate slot and w to
a particular argument word instantiating this slot.
Hence, p(t|d,w) is viewed as specifying the rele-
vance (or likelihood) of the topic t for the predi-
cate slot in the context of the given argument in-
stantiation. For example, for the predicate slot ?ac-
quire Y? in the context of the argument ?IBM?, we
expect high relevance for a topic about companies,
while in the context of the argument ?knowledge?
we expect high relevance for a topic about abstract
concepts. Accordingly, the distribution p(t|d,w)
over all topics provides a topic-level representa-
tion for a predicate slot in the context of a particu-
lar argument w. This representation is used by the
topic-level model to compute a context-sensitive
score for inference rule applications, as follows.
2We note that there are variants in the type of LDA model
and the way the pseudo-documents are constructed in the
referenced prior work. In order to focus on the inference
methods rather than on the underlying LDA model, we use
the LDA framework described in this paper for all compared
methods.
1333
Consider the application of an inference rule
?LHS ? RHS? in the context of a particular pair
of arguments for the X and Y slots, denoted by
wx and wy, respectively. Denoting by l and r the
predicates appearing in the two rule sides, the reli-
ability score of the topic-level model is defined as
follows (we present a geometric mean formulation
for consistency with DIRT):
(3)scoreTopic(LHS ? RHS, wx, wy)
=
?
sim(dxl , dxr , wx) ? sim(d
y
l , d
y
r , wy)
where sim(d, d?, w) is a topic-distribution similar-
ity measure conditioned on a given context word.
Specifically, Ritter et al (2010) utilized the dot
product form for their similarity measure:
(4)simDC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?, w)]
(the subscript DC stands for double-conditioning,
as both distributions are conditioned on the argu-
ment word, unlike the measure below).
Dinu and Lapata (2010b) presented a slightly
different similarity measure for topic distributions
that performed better in their setting as well as in a
related later paper on context-sensitive scoring of
lexical similarity (Dinu and Lapata, 2010a). In this
measure, the topic distribution for the right hand
side of the rule is not conditioned on w:
(5)simSC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?)]
(the subscript SC stands for single-conditioning,
as only the left distribution is conditioned on the
argument word). They also experimented with a
few variants for the structure of the similarity mea-
sure and assessed that best results are obtained
with the dot product form. In our experiments,
we employ these two similarity measures for topic
distributions as baselines representing topic-level
models.
Comparing the context-insensitive and context-
sensitive models, we see that both of them mea-
sure similarity between vector representations of
corresponding predicate slots. However, while
DIRT computes sim(v, v?) over vectors in the
original word-level space, topic-level models com-
pute sim(d, d?, w) by measuring similarity of vec-
tors in a reduced-dimensionality latent space. As
conjectured in the introduction, such coarse-grain
representation might lead to loss of information.
Hence, in the next section we propose a com-
bined two-level model, which represents predicate
slots in the original word-level space while biasing
the similarity measure through topic-level context
models.
3 Two-level Context-sensitive Inference
Our model follows the general DIRT scheme
while extending it to handle context-sensitive scor-
ing of rule applications, addressing the scenario
dealt by the context-sensitive topic models. In
particular, we define the context-sensitive score
scoreWT, where WT stands for the combination
of the Word/Topic levels:
(6)scoreWT(LHS ? RHS, wx, wy)
=
?
sim(vxl , vxr , wx) ? sim(v
y
l , v
y
r , wy)
Thus, our model computes similarity over word-
level (rather than topic-level) argument vectors,
while biasing it according to the specific argu-
ment words in the given rule application con-
text. The core of our contribution is thus defining
the context-sensitive word-level vector similarity
measure sim(v, v?, w), as described in the remain-
der of this section.
Following the methods in Section 2, for each
predicate pred we construct, from the learning
corpus, its argument vectors vxpred and vypred aswell as its argument pseudo-documents dxpred and
dypred. For convenience, when referring to an ar-gument vector v, we will denote the correspond-
ing pseudo-document by dv. Based on all pseudo-
documents we learn an LDA model and obtain its
associated probability distributions.
The calculation of sim(v, v?, w) is composed of
two steps. At learning time, we compute for each
candidate rule a separate, topic-biased, similarity
score per each of the topics in the LDA model.
Then, at rule application time, we compute an
overall reliability score for the rule by combining
the per-topic similarity scores, while biasing the
score combination according to the given context
of w. These two steps are described in the follow-
ing two subsections.
3.1 Topic-biased Word-vector Similarities
Given a pair of word vectors v and v?, and
any desired ?base? vector similarity measure sim
(e.g. simLin), we compute a topic-biased sim-
ilarity score for each LDA topic t, denoted by
simt(v, v?). simt(v, v?) is computed by applying
1334
the original similarity measure over topic-biased
versions of v and v?, denoted by vt and v?t:
simt(v, v?) = sim(vt, v?t)
where
vt(w) = v(w) ? p(t|dv, w)
That is, each value in the biased vector, vt(w),
is obtained by weighing the original value v(w)
by the relevance of the topic t to the argument
word w within dv. This way, rather than replac-
ing altogether the word-level values v(w) by the
topic probabilities p(t|dv, w), as done in the topic-
level models, we use the latter to only bias the for-
mer while preserving fine-grained word-level rep-
resentations. The notation Lint denotes the simt
measure when applied using Lin as the base simi-
larity measure sim.
This learning process results in K different
topic-biased similarity scores for each candidate
rule, where K is the number of LDA topics. Ta-
ble 1 illustrates topic-biased similarities for the Y
slot of two rules involving the predicate ?acquire?.
As can be seen, the topic-biased score Lint for ?ac-
quire? learn? for t2 is higher than the Lin score,
since this topic is characterized by arguments that
commonly appear with both predicates of the rule.
Consequently, the two predicates are found to be
distributionally similar when biased for this topic.
On the other hand, the topic-biased similarity for
t1 is substantially lower, since prominent words
in this topic are likely to occur with ?acquire? but
not with ?learn?, yielding low distributional simi-
larity. Opposite behavior is exhibited for the rule
?acquire? purchase?.
3.2 Context-sensitive Similarity
When applying an inference rule, we compute
for each slot its context-sensitive similarity score
simWT(v, v?, w), where v and v? are the slot?s ar-
gument vectors for the two rule sides and w is the
word instantiating the slot in the given rule appli-
cation. This score is computed as a weighted aver-
age of the rule?s K topic-biased similarity scores
simt. In this average, each topic is weighed by
its ?relevance? for the context in which the rule is
applied, which consists of the left-hand-side pred-
icate v and the argument w. This relevance is cap-
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
acquire ? learn
Lint(v, v?) 0.040 0.334
Lin(v, v?) 0.165
acquire ? purchase
Lint(v, v?) 0.427 0.241
Lin(v, v?) 0.267
Table 1: Two characteristic topics for the Y slot of
?acquire?, along with their topic-biased Lin sim-
ilarities scores Lint, compared with the original
Lin similarity, for two rules. The relevance of each
topic to different arguments of ?acquire? is illus-
trated by showing the top 5 words in the argument
vector vyacquire for which the illustrated topic is the
most likely one.
tured by p(t|dv, w):
simWT(v, v?, w) =
?
t
[p(t|dv, w) ? simt(v, v?)]
(7)
This way, a rule application would obtain a high
score only if the current context fits those topics
for which the rule is indeed likely to be valid, as
captured by a high topic-biased similarity. The no-
tation LinWT denotes the simWT measure, when
using Lint as the topic-biased similarity measure.
Table 2 illustrates the calculation of context-
sensitive similarity scores in four rule applica-
tions, involving the Y slot of the predicate ?ac-
quire?. We observe that relative to the fixed
context-insensitive Lin score, the score of ?ac-
quire ? learn? is substantially promoted for
the argument ?skill? while being demoted for
?Skype?. The opposite behavior is observed for
?acquire ? purchase?, altogether demonstrating
how our model successfully biases the similarity
score according to rule validity in context.
4 Experimental Settings
To evaluate our model, we compare it both to
context-insensitive similarity measures as well as
to prior context-sensitive methods. Furthermore,
to better understand its applicability in typical
NLP tasks, we focus on an evaluation setting that
corresponds to a natural distribution of examples
from a large corpus.
1335
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
?acquire Skype ? learn Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.039
Lin(v, v?) 0.165
?acquire Skype ? purchase Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.417
Lin(v, v?) 0.267
?acquire skill ? learn skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.251
Lin(v, v?) 0.165
?acquire skill ? purchase skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.181
Lin(v, v?) 0.267
Table 2: Context-sensitive similarity scores (in
bold) for the Y slots of four rule applications. The
components of the score calculation are shown for
the topics of Table 1. For each rule application,
the table shows a couple of the topic-biased scores
Lint of the rule (as in Table 1), along with the topic
relevance for the given context p(t|dv, w), which
weighs the topic-biased scores in the LinWT cal-
culation. The context-insensitive Lin score is
shown for comparison.
4.1 Evaluated Rule Application Methods
We evaluated the following rule application meth-
ods: the original context-insensitive word model,
following DIRT (Lin and Pantel, 2001), as de-
scribed in Equation 1, denoted by CI; our own
topic-word context-sensitive model, as described
in Equation 6, denoted by WT. In addition, we
evaluated two variants of the topic-level context-
sensitive model, denoted DC and SC. DC follows
the double conditioned contextualized similarity
measure according to Equation 4, as implemented
by (Ritter et al, 2010), while SC follows the sin-
gle conditioned one at Equation 5, as implemented
by (Dinu and Lapata, 2010b; Dinu and Lapata,
2010a).
Since our model can contextualize various dis-
tributional similarity measures, we evaluated the
performance of all the above methods on several
base similarity measures and their learned rule-
sets, namely Lin (Lin, 1998), BInc (Szpektor and
Dagan, 2008) and vector Cosine similarity. The
Lin similarity measure is described in Equation 2.
Binc (Szpektor and Dagan, 2008) is a directional
similarity measure between word vectors, which
outperformed Lin for predicate inference (Szpek-
tor and Dagan, 2008).
To build the rule-sets and models for the tested
approaches we utilized the ReVerb corpus (Fader
et al, 2011), a large scale publicly available web-
based open extractions data set, containing about
15 million unique template extractions.3 ReVerb
template extractions/instantiations are in the form
of a tuple (x, pred, y), containing pred, a verb
predicate, x, the argument instantiation of the tem-
plate?s slot X , and y, the instantiation of the tem-
plate?s slot Y .
ReVerb includes over 600,000 different tem-
plates that comprise a verb but may also include
other words, for example ?X can accommodate up
to Y?. Yet, many of these templates share a similar
meaning, e.g. ?X accommodate up to Y?, ?X can
accommodate up to Y?, ?X will accommodate up
to Y?, etc. Following Sekine (2005), we clustered
templates that share their main verb predicate in
order to scale down the number of different pred-
icates in the corpus and collect richer word co-
occurrence statistics per predicate.
Next, we applied some clean-up preprocessing
to the ReVerb extractions. This includes discard-
ing stop words, rare words and non-alphabetical
words instantiating either the X or the Y argu-
ments. In addition, we discarded all predicates
that co-occur with less than 100 unique argument
words in each slot. The remaining corpus consists
of 7 million unique extractions and 2,155 verb
predicates.
Finally, we trained an LDA model, as described
in Section 2, using Mallet (McCallum, 2002).
Then, for each original context-insensitive simi-
larity measure, we learned from ReVerb a rule-set
comprised of the top 500 rules for every identi-
fied predicate. To complete the learning, we cal-
culated the topic-biased similarity score for each
learned rule under each LDA topic, as specified
in our context-sensitive model. We release a rule
set comprising the top 500 context-sensitive rules
that we learned for each of the verb predicates in
our learning corpus, along with our trained LDA
3ReVerb is available at http://reverb.cs.
washington.edu/
1336
Method Lin BInc Cosine
Valid 266 254 272
Invalid 545 523 539
Total 811 777 811
Table 3: Sizes of rule application test set for each
learned rule-set.
model.4
4.2 Evaluation Task
To evaluate the performance of the different meth-
ods we chose the dataset constructed by Zeich-
ner et al (2012). 5 This publicly available dataset
contains about 6,500 manually annotated predi-
cate template rule applications, each one labeled
as correct or incorrect. For example, ?Jack agree
with Jill 9 Jack feel sorry for Jill? is a rule ap-
plication in this dataset, labeled as incorrect, and
?Registration open this month? Registration be-
gin this month? is another rule application, labeled
as correct. Rule applications were generated by
randomly sampling extractions from ReVerb, such
as (?Jack?,?agree with?,?Jill?) and then sampling
possible rules for each, such as ?agree with? feel
sorry for?. Hence, this dataset provides naturally
distributed rule inferences with respect to ReVerb.
Whenever we evaluated a distributional similar-
ity measure (namely Lin, BInc, or Cosine), we
discarded instances from Zeichner et al?s dataset
in which the assessed rule is not in the context-
insensitive rule-set learned for this measure or the
argument instantiation of the rule is not in the LDA
lexicon. We refer to the remaining instances as the
test set per measure, e.g. Lin?s test set. Table 3
details the size of each such test set in our experi-
ment.
Finally, the task under which we assessed the
tested models is to rank all rule applications in
each test set, aiming to rank the valid rule appli-
cations above the invalid ones.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule appli-
cation ranking computed by this method. In order
4Our resource is available at: http://www.cs.biu.
ac.il/? nlp/downloads/wt-rules.html
5The dataset is available at: http://
www.cs.biu.ac.il/?nlp/downloads/
annotation-rule-application.htm
Method Lin BInc Cosine
CI 0.503 0.513 0.513
DC 0.451 (1200) 0.455 (1200) 0.455 (1200)
SC 0.443 (1200) 0.458 (1200) 0.452 (1200)
WT 0.562 (100) 0.584 (50) 0.565 (25)
Table 4: MAP values on corresponding test set ob-
tained by each method. Figures in parentheses in-
dicate optimal number of LDA topics.
to compute MAP values and corresponding statis-
tical significance, we randomly split each test set
into 30 subsets. For each method we computed
Average Precision on every subset and then took
the average over all subsets as the MAP value.
Since all tested context-sensitive approaches are
based on LDA topics, we varied for each method
the number of LDA topics K that optimizes its
performance, ranging from 25 to 1600 topics. We
used LDA hyperparameters ? = 0.01 and ? = 0.1
for K < 600 and ? = 50K for K >= 600.
Table 4 presents the optimal MAP performance
of each tested measure. Our main result is that
our model outperforms all other methods, both
context-insensitive and context-sensitive, by a rel-
ative increase of more than 10% for all three sim-
ilarity measures that we tested. This improvement
is statistically significant at p < 0.01 for BInc and
Lin, and p < 0.015 for Cosine, using paired t-
test. This shows that our model indeed success-
fully leverages contextual information beyond the
basic context-agnostic rule scores and is robust
across measures.
Surprisingly, both baseline topic-level context-
sensitive methods, namely DC and SC, underper-
formed compared to their context-insensitive base-
lines. While Dinu and Lapata (Dinu and Lap-
ata, 2010b) did show improvement over context-
insensitive DIRT, this result was obtained on the
verbs of the Lexical Substitution Task in SemEval
(McCarthy and Navigli, 2007), which was manu-
ally created with a bias for context-sensitive sub-
stitutions. However, our result suggests that topic-
level models might not be robust enough when ap-
plied to a random sample of inferences.
An interesting indication of the differences be-
tween our word-topic model, WT, and topic-only
models, DC and SC, lies in the optimal number of
LDA topics required for each method. The num-
ber of topics in the range 25-100 performed almost
equally well under the WT model for all base mea-
sures, with a moderate decline for higher numbers.
1337
The need for this rather small number of topics is
due to the nature of utilization of topics in WT.
Specifically, topics are leveraged for high-level
domain disambiguation, while fine grained word-
level distributional similarity is computed for each
rule under each such domain. This works best for
a relatively low number of topics. However, in
higher numbers, topics relate to narrower domains
and then topic biased word level similarity may
become less effective due to potential sparseness.
On the other hand, DC and SC rely on topics as
a surrogate to predicate-argument co-occurrence
features, and thus require a relatively large num-
ber of them to be effective.
Delving deeper into our test-set, Zeichner et al
provided a more detailed annotation for each in-
valid rule application. Specifically, they annotated
whether the context under which the rule is ap-
plied is valid. For example, in ?John bought my
car 9 John sold my car? the inference is invalid
due to an inherently incorrect rule, but the con-
text is valid. On the other hand in ?my boss raised
my salary 9 my boss constructed my salary? the
context {?my boss?, ?my salary?} for applying
?raise? construct? is invalid. Following, we split
the test-set for the base Lin measure into two test-
sets: (a) test-setvc, which includes all correct rule
applications and incorrect ones only under valid
contexts, and (b) test-setivc, which includes again
all correct rule applications but incorrect ones only
under invalid contexts.
Table 5 presents the performance of each com-
pared method on the two test sets. On test-
setivc, where context mismatches are abundant,
our model outperformed all other baselines (sta-
tistically significant at p < 0.01). In addition,
this time DC slightly outperformed CI. This re-
sult more explicitly shows the advantages of in-
tegrating word-level and context-sensitive topic-
level similarities for differentiating valid and in-
valid contexts for rule applications. Yet, many in-
valid rule applications occur under valid contexts
due to inherently incorrect rules, and we want to
make sure that also in this scenario our model
does not fall behind the context-insensitive mea-
sure. Indeed, on test-setvc, in which context mis-
matches are rare, our algorithm is still better than
the original measure, indicating that WT can be
safely applied to distributional similarity measures
without concerns of reduced performance in dif-
ferent context scenarios.
test-setivc test-setvc
Size
(valid:invalid)
432
(266:166)
645
(266:379)
CI 0.780 0.587
DC 0.796 0.498
SC 0.779 0.512
WT 0.854 0.621
Table 5: MAP results for the two split Lin test-
sets.
6 Discussion and Future Work
This paper addressed the problem of computing
context-sensitive reliability scores for predicate in-
ference rules. In particular, we proposed a novel
scheme that applies over any base distributional
similarity measure which operates at the word
level, and computes a single context-insensitive
score for a rule. Based on such a measure, our
scheme constructs a context-sensitive similarity
measure that computes a reliability score for pred-
icate inference rules applications in the context of
given arguments.
The contextualization of the base similarity
score was obtained using a topic-level LDA
model, which was used in a novel way. First,
it provides a topic bias for learning separate per-
topic word-level similarity scores between predi-
cates. Then, given a specific candidate rule ap-
plication, the LDA model is used to infer the
topic distribution relevant to the context speci-
fied by the given arguments. Finally, the context-
sensitive rule application score is computed as a
weighted average of the per-topic word-level sim-
ilarity scores, which are weighed according to the
inferred topic distribution.
While most works on context-insensitive pred-
icate inference rules, such as DIRT (Lin and Pan-
tel, 2001), are based on word-level similarity mea-
sures, almost all prior models addressing context-
sensitive predicate inference rules are based on
topic models (except for (Pantel et al, 2007),
which was outperformed by later models). We
therefore focused on comparing the performance
of our two-level scheme with state-of-the-art prior
topic-level and word-level models of distributional
similarity, over a random sample of inference rule
applications. Under this natural setting, the two-
level scheme consistently outperformed both types
of models when tested with three different base
similarity measures. Notably, our model shows
stable performance over a large subset of the data
1338
where context sensitivity is rare, while topic-level
models tend to underperform in such cases com-
pared to the base context-insensitive methods.
Our work is closely related to another research
line that addresses lexical similarity and substi-
tution scenarios in context. While we focus on
lexical-syntactic predicate templates and instanti-
ations of their argument slots as context, lexical
similarity methods consider various lexical units
that are not necessarily predicates, with their con-
text typically being the collection of words in a
window around them.
Various approaches have been proposed to ad-
dress lexical similarity. A number of works are
based on a compositional semantics approach,
where a prior representation of a target lexical unit
is composed with the representations of words in
its given context (Mitchell and Lapata, 2008; Erk
and Pado?, 2008; Thater et al, 2010). Other works
(Erk and Pado?, 2010; Reisinger and Mooney,
2010) use a rather large word window around tar-
get words and compute similarities between clus-
ters comprising instances of word windows. In ad-
dition, (Dinu and Lapata, 2010a) adapted the pred-
icate inference topic model from (Dinu and Lap-
ata, 2010b) to compute lexical similarity in con-
text.
A natural extension of our work would be to ex-
tend our two level model to accommodate context-
sensitive lexical similarity. For this purpose we
will need to redefine the scope of context in our
model, and adapt our method to compute context-
biased lexical similarities accordingly. Then we
will also be able to evaluate our model on the
Lexical Substitution Task (McCarthy and Navigli,
2007), which has been commonly used in recent
years as a benchmark for context-sensitive lexical
similarity models.
In a different NLP task, Eidelman et al (2012)
utilize a similar approach to ours for improving
the performance of statistical machine translation
(SMT). They learn an LDA model on the source
language side of the training corpus with the pur-
pose of identifying implicit sub-domains. Then
they utilize the distribution over topics inferred for
each document in their corpus to compute sepa-
rate per-topic translation probability tables. Fi-
nally, they train a classifier to translate a given
target word based on these tables and the inferred
topic distribution of the given document in which
the target word appears. A notable difference be-
tween our approach and theirs is that we use predi-
cate pseudo-documents consisting of argument in-
stantiations to learn our LDA model, while Eidel-
man et al use the real documents in a corpus.
We believe that combining these two approaches
may improve performance for both textual infer-
ence and SMT and plan to experiment with this
direction in future work.
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL.
Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorithm
for learning directionality of inference rules. In Pro-
ceedings of EMNLP-CoNLL.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Georgiana Dinu and Mirella Lapata. 2010a. Measur-
ing distributional similarity in context. In Proceed-
ings of EMNLP.
Georgiana Dinu and Mirella Lapata. 2010b. Topic
models for meaning similarity in context. In Pro-
ceedings of COLING: Posters.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings EACL.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the ACL
conference short papers.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL conference short papers.
1339
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDDConference on Knowledge Discovery
and Data Mining 2001.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SemEval.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
1340
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283?288,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Lexical Expansion to Learn Inference Rules from Sparse Data
Oren Melamud?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates is widely addressed by com-
puting distributional similarity scores be-
tween vectors of argument words. In
this scheme, prior work typically refrained
from learning rules for low frequency
predicates associated with very sparse ar-
gument vectors due to expected low reli-
ability. To improve the learning of such
rules in an unsupervised way, we propose
to lexically expand sparse argument word
vectors with semantically similar words.
Our evaluation shows that lexical expan-
sion significantly improves performance
in comparison to state-of-the-art baselines.
1 Introduction
The benefit of utilizing template-based inference
rules between predicates was demonstrated in
NLP tasks such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y?, between the templates ?X treat Y? and ?X re-
lieve Y? may be useful to identify the answer to
?Which drugs relieve stomach ache??.
The predominant unsupervised approach for
learning inference rules between templates is via
distributional similarity (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Szpektor and Da-
gan, 2008). Specifically, each argument slot in
a template is represented by an argument vector,
containing the words (or terms) that instantiate this
slot in all of the occurrences of the template in a
learning corpus. Two templates are then deemed
semantically similar if the argument vectors of
their corresponding slots are similar.
Ideally, inference rules should be learned for
all templates that occur in the learning corpus.
However, many templates are rare and occur only
few times in the corpus. This is a typical NLP
phenomenon that can be associated with either a
small learning corpus, as in the cases of domain
specific corpora and resource-scarce languages, or
with templates with rare terms or long multi-word
expressions such as ?X be also a risk factor to Y?
or ?X finish second in Y?, which capture very spe-
cific meanings. Due to few occurrences, the slots
of rare templates are represented with very sparse
argument vectors, which in turn lead to low relia-
bility in distributional similarity scores.
A common practice in prior work for learn-
ing predicate inference rules is to simply disre-
gard templates below a minimal frequency thresh-
old (Lin and Pantel, 2001; Kotlerman et al, 2010;
Dinu and Lapata, 2010; Ritter et al, 2010). Yet,
acquiring rules for rare templates may be benefi-
cial both in terms of coverage, but also in terms
of more accurate rule application, since rare tem-
plates are less ambiguous than frequent ones.
We propose to improve the learning of rules be-
tween infrequent templates by expanding their ar-
gument vectors. This is done via a ?dual? distribu-
tional similarity approach, in which we consider
two words to be similar if they instantiate similar
sets of templates. We then use these similarities
to expand the argument vector of each slot with
words that were identified as similar to the original
arguments in the vector. Finally, similarities be-
tween templates are computed using the expanded
vectors, resulting in a ?smoothed? version of the
original similarity measure.
Evaluations on a rule application task show
that our lexical expansion approach significantly
improves the performance of the state-of-the-art
DIRT algorithm (Lin and Pantel, 2001). In addi-
tion, our approach outperforms a similarity mea-
sure based on vectors of latent topics instead of
word vectors, a common way to avoid sparseness
issues by means of dimensionality reduction.
283
2 Technical Background
The distributional similarity score for an inference
rule between two predicate templates, e.g. ?X re-
sign Y? X quit Y?, is typically computed by mea-
suring the similarity between the argument vec-
tors of the corresponding X slots and Y slots of
the two templates. To this end, first the argument
vectors should be constructed and then a similarity
measure between two vectors should be provided.
We note that we focus here on binary templates
with two slots each, but this approach can be ap-
plied to any template.
A common starting point is to compute a
co-occurrence matrix M from a learning cor-
pus. M ?s rows correspond to the template slots
and the columns correspond to the various terms
that instantiate the slots. Each entry Mi,j , e.g.
Mx quit,John, contains a count of the number of
times the term j instantiated the template slot i in
the corpus. Thus, each row Mi,? corresponds to
an argument vector for slot i. Next, some func-
tion of the counts is used to assign weights to all
Mi,j entries. In this paper we use pointwise mu-
tual information (PMI), which is common in prior
work (Lin and Pantel, 2001; Szpektor and Dagan,
2008).
Finally, rules are assessed using some similar-
ity measure between corresponding argument vec-
tors. The state-of-the-art DIRT algorithm (Lin and
Pantel, 2001) uses the highly cited Lin similarity
measures (Lin, 1998) to score rules between bi-
nary templates as follows:
(1)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
(2)DIRT (l ? r)
=
?
Lin(vl:x, vr:x) ? Lin(vl:y, vr:y)
where v and v? are two argument vectors, l and
r are the templates participating in the inference
rule and vl:x corresponds to the argument vector
of slot X of template l, etc. While the original
DIRT algorithm utilizes the Lin measure, one can
replace it with any other vector similarity measure.
A separate line of research for word simi-
larity introduced directional similarity measures
that have a bias for identifying generaliza-
tion/specification relations, i.e. relations be-
tween predicates with narrow (or specific) seman-
tic meanings to predicates with broader meanings
inferred by them (unlike the symmetric Lin). One
such example is the Cover measure (Weeds and
Weir, 2003):
(3)Cover(v, v?) =
?
w?v?v? [v(w)]?
w?v?v? [v(w)]
As can be seen, in the core of the Lin and Cover
measures, as well as in many other well known
distributional similarity measures such as Jaccard,
Dice and Cosine, stand the number of shared ar-
guments vs. the total number of arguments in the
two vectors. Therefore, when the argument vec-
tors are sparse, containing very few non-zero fea-
tures, these scores become unreliable and volatile,
changing greatly with every inclusion or exclusion
of a single shared argument.
3 Lexical Expansion Scheme
We wish to overcome the sparseness issues in rare
feature vectors, especially in cases where argu-
ment vectors of semantically similar predicates
comprise similar but not exactly identical argu-
ments. To this end, we propose a three step
scheme. First, we learn lexical expansion sets for
argument words, such as the set {euros, money}
for the word dollars. Then we use these sets to ex-
pand the argument word vectors of predicate tem-
plates. For example, given the template ?X can
be exchanged for Y?, with the following argument
words instantiating slot X {dollars, gold}, and
the expansion set above, we would expand the ar-
gument word vector to include all the following
words {dollars, euros, money, gold}. Finally, we
use the expanded argument word vectors to com-
pute the scores for predicate inference rules with a
given similarity measure.
When a template is instantiated with an ob-
served word, we expect it to also be instantiated
with semantically similar words such as the ones
in the expansion set of the observed word. We
?blame? the lack of such template occurrences
only on the size of the corpus and the sparseness
phenomenon in natural languages. Thus, we uti-
lize our lexical expansion scheme to synthetically
add these expected but missing occurrences, ef-
fectively smoothing or generalizing over the ex-
plicitly observed argument occurrences. Our ap-
proach is inspired by query expansion (Voorhees,
1994) in Information Retrieval (IR), as well as by
the recent lexical expansion framework proposed
in (Biemann and Riedl, 2013), and the work by
284
Miller et al (2012) on word sense disambigua-
tion. Yet, to the best of our knowledge, this is the
first work that applies lexical expansion to distri-
butional similarity feature vectors. We next de-
scribe our scheme in detail.
3.1 Learning Lexical Expansions
We start by constructing the co-occurrence matrix
M (Section 2), where each entry Mt:s,w indicates
the number of times that word w instantiates slot
s of template t in the learning corpus, denoted by
?t:s?, where s can be either X or Y.
In traditional distributional similarity, the rows
Mt:s,? serve as argument vectors of template slots.
However, to learn expansion sets we take a ?dual?
view and consider each matrix column M?:?,w (de-
noted vw) as a feature vector for the argument
word w. Under this view, templates (or more
specifically, template slots) are the features. For
instance, for the word dollars the respective fea-
ture vector may include entries such as ?X can be
exchanged for?, ?can be exchanged for Y?, ?pur-
chase Y? and ?sell Y?.
We next learn an expansion set per each word
w by computing the distributional similarity be-
tween the vectors of w and any other argument
word w?, sim(vw, vw?). Then we take the N most
similar words as w?s expansion set with degree
N , denoted by LNw = {w?1, ..., w?N}. Any simi-
larity measure could be used, but as our experi-
ments show, different measures generate sets with
different properties, and some may be fitter for ar-
gument vector expansion than others.
3.2 Expanding Argument Vectors
Given a row count vector Mt:s,? for slot s of tem-
plate t, we enrich it with expansion sets as fol-
lows. For each w in Mt:s,?, the original count in
vt:s(w) is redistributed equally between itself and
all words in w?s expansion set, i.e. all w? ? LNw ,
(possibly yielding fractional counts) where N is a
global parameter of the model. Specifically, the
new count that is assigned to each word w is its
remaining original count after it has been redis-
tributed (or zero if no original count), plus all the
counts that were distributed to it from other words.
Next, PMI weights are recomputed according to
the new counts, and the resulting expanded vector
is denoted by v+t:s. Similarity between template
slots is now computed over the expanded vectors
instead of the original ones, e.g. Lin(v+l:x, v+r:x).
4 Experimental Settings
We constructed a relatively small learning corpus
for investigating the sparseness issues of such cor-
pora. To this end, we used a random sample from
the large scale web-based ReVerb corpus1 (Fader
et al, 2011), comprising tuple extractions of pred-
icate templates with their argument instantiations.
We applied some clean-up preprocessing to these
extractions, discarding stop words, rare words and
non-alphabetical words that instantiated either the
X or the Y argument slots. In addition, we dis-
carded templates that co-occur with less than 5
unique argument words in either of their slots, as-
suming that such few arguments cannot convey re-
liable semantic information, even with expansion.
Our final corpus consists of around 350,000 ex-
tractions and 14,000 unique templates. In this cor-
pus around one third of the extractions refer to
templates that co-occur with at most 35 unique ar-
guments in both their slots.
We evaluated the quality of inference
rules using the dataset constructed by Zeich-
ner et al (2012)2, which contains about 6,500
manually annotated template rule applications,
each labeled as correct or not. For example,
?The game develop eye-hand coordination9 The
game launch eye-hand coordination? is a rule
application in this dataset of the rule ?X develop
Y ? X launch Y?, labeled as incorrect, and
?Captain Cook sail to Australia? Captain Cook
depart for Australia? is a rule application of the
rule ?X sail to Y ? X depart for Y?, labeled as
correct. Specifically, we induced two datasets
from Zeichner et al?s dataset, denoted DS-5-35
and DS-5-50, which consist of all rule applica-
tions whose templates are present in our learning
corpus and co-occurred with at least 5 and at
most 35 and 50 unique argument words in both
their slots, respectively. DS-5-35 includes 311
rule applications (104 correct and 207 incorrect)
and DS-5-50 includes 502 rule applications (190
correct and 312 incorrect).
Our evaluation task is to rank all rule applica-
tions in each test set based on the similarity scores
of the applied rules. Optimal performance would
rank all correct rule applications above the in-
correct ones. As a baseline for rule scoring we
1http://reverb.cs.washington.edu/
2http://www.cs.biu.ac.il/nlp/
downloads/annotation-rule-application.
htm
285
used the DIRT algorithm scheme, denoted DIRT-
LE-None. We then compared between the perfor-
mance of this baseline and its expanded versions,
testing two similarity measures for generating the
expansion sets of arguments: Lin and Cover. We
denote these expanded methods DIRT-LE-SIM-N,
where SIM is the similarity measure used to gen-
erate the expansion sets and N is the lexical expan-
sion degree, e.g. DIRT-LE-Lin-2.
We remind the reader that our scheme utilizes
two similarity measures. The first measure as-
sesses the similarity between the argument vectors
of the two templates in the rule. This measure
is kept constant in our experiments and is iden-
tical to DIRT?s similarity measure (Lin). 3 The
second measure assesses the similarity between
words and is used for the lexical expansion of ar-
gument vectors. Since this is the research goal
of this paper, we experimented with two different
measures for lexical expansion: a symmetric mea-
sure (Lin) and an asymmetric measure (Cover).
To this end we evaluated their effect on DIRT?s
rule ranking performance and compared them to a
vanilla version of DIRT without lexical expansion.
As another baseline, we follow Dinu and La-
pata (2010) inducing LDA topic vectors for tem-
plate slots and computing predicate template infer-
ence rule scores based on similarity between these
vectors. We use standard hyperparameters for
learning the LDA model (Griffiths and Steyvers,
2004). This method is denoted LDA-K, where K is
the number of topics in the model.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule applica-
tion ranking computed by this method. In order
to compute MAP values and corresponding sta-
tistical significance, we randomly split each test
set into 30 subsets. For each method we com-
puted Average Precision on every subset and then
took the average as the MAP value. We varied
the degree of the lexical expansion in our model
and the number of topics in the topic model base-
line to analyze their effect on the performance of
these methods on our datasets. We note that in our
model a greater degree of lexical expansion cor-
3Experiments with Cosine as the template similarity mea-
sure instead of Lin for both DIRT and its expanded versions
yielded similar results. We omit those for brevity.
responds to more aggressive smoothing (or gen-
eralization) of the explicitly observed data, while
the same goes for a lower number of topics in the
topic model. The results on DS-5-35 and DS-5-50
are illustrated in Figure 1.
The most dramatic improvement over the base-
lines is evident in DS-5-35, where DIRT-LE-
Cover-2 achieves a MAP score of 0.577 in com-
parison to 0.459 achieved by its DIRT-LE-None
baseline. This is indeed the dataset where we ex-
pected expansion to affect most due the extreme
sparseness of argument vectors. Both DIRT-LE-
Cover-N and DIRT-LE-Lin-N outperform DIRT-
LE-None for all tested values of N , with statisti-
cal significance via a paired t-test at p < 0.05 for
DIRT-LE-Cover-N where 1 ? N ? 5, and p <
0.01 for DIRT-LE-Cover-2. On DS-5-50, improve-
ment over the DIRT-LE-None baseline is still sig-
nificant with both DIRT-LE-Cover-N and DIRT-
LE-Lin-N outperforming DIRT-LE-None. DIRT-
LE-Cover-N again performs best and achieves a
relative improvement of over 10% with statistical
significance at p < 0.05 for 2 ? N ? 3.
The above shows that expansion is effective for
improving rule learning between infrequent tem-
plates. Furthermore, the fact that DIRT-LE-Cover-
N outperforms DIRT-LE-Lin-N suggests that us-
ing directional expansions, which are biased to
generalizations of the observed argument words,
e.g. vehicle as an expansion for car, is more ef-
fective than using symmetrically related words,
such as bicycle or automobile. This conclusion
appears also to be valid from a semantic reason-
ing perspective, as given an observed predicate-
argument occurrence, such as ?drive car? we can
more likely infer that a presumed occurrence of
the same predicate with a generalization of the ar-
gument, such as ?drive vehicle?, is valid, i.e. ?drive
car ? drive vehicle?. On the other hand while
?drive car ? drive automobile? is likely to be
valid, ?drive car ? drive bicycle? and ?drive ve-
hicle? drive bicycle? are not.
Figure 1 also depicts the performance of LDA
as a vector smoothing approach. LDA-K out-
performs the DIRT-LE-None baseline under DS-
5-35 but with no statistical significance. Under
DS-5-50 LDA-K performs worst, slightly outper-
forming DIRT-LE-None only for K=450. Further-
more, under both datasets, LDA-K is outperformed
by DIRT-LE-Cover-N. These results indicate that
LDA is less effective than our expansion approach.
286
Figure 1: MAP scores on DS-5-35 and DS-5-50 for the original DIRT scheme, denoted DIRT-LE-None,
and for the compared smoothing methods as follows. DIRT with varied degrees of lexical expansion
is denoted as DIRT-LE-Lin-N and DIRT-LE-Cover-N. The topic model with varied number of topics is
denoted as LDA-K. Data labels indicate the expansion degree (N) or the number of LDA topics (K),
depending on the tested method.
One reason may be that in our model, every expan-
sion set may be viewed as a cluster around a spe-
cific word, an outstanding difference in compari-
son to topics, which provide a global partition over
all words. We note that performance improve-
ment of singleton document clusters over global
partitions was also shown in IR (Kurland and Lee,
2009).
In order to further illustrate our lexical expan-
sion scheme we focus on the rule application
?Captain Cook sail to Australia? Captain Cook
depart for Australia?, which is labeled as correct
in our test set and corresponds to the rule ?X sail
to Y ? X depart for Y?. There are 30 words in-
stantiating the X slot of the predicate ?sail to?
in our learning corpus including {Columbus, em-
peror, James, John, trader}. On the other hand,
there are 18 words instantiating the X slot of the
predicate ?depart for? including {Amanda, Jerry,
Michael, mother, queen}. While semantic simi-
larity between these two sets of words is evident,
they share no words in common, and therefore the
original DIRT algorithm, DIRT-LE-None, wrongly
assigns a zero score to the rule.
The following are descriptions of some of the
argument word expansions performed by DIRT-
LE-Cover-2 (using the notation LNw defined in Sec-
tion 3.1) for the X slot of ?sail to? L2John = {mr.,
dr.}, L2trader = {people, man}, and for the X slot
of ?depart for?, L2Michael = {John, mr.}, L2mother =
{people, woman}. Given these expansions the two
slots now share the following words {mr. ,people,
John} and the rule score becomes positive.
It is also interesting to compare the expansions
performed by DIRT-LE-Lin-2 to the above. For
instance in this case L2mother = {father, sarah},
which does not identify people as a shared argu-
ment for the rule.
6 Conclusions
We propose to improve the learning of infer-
ence rules between infrequent predicate templates
with sparse argument vectors by utilizing a novel
scheme that lexically expands argument vectors
with semantically similar words. Similarities be-
tween argument words are discovered using a dual
distributional representation, in which templates
are the features.
We tested the performance of our expansion
approach on rule application datasets that were
biased towards rare templates. Our evaluation
showed that rule learning with expanded vectors
outperformed the baseline learning with original
vectors. It also outperformed an LDA-based simi-
larity model that overcomes sparseness via dimen-
sionality reduction.
In future work we plan to investigate how our
scheme performs when integrated with manually
constructed resources for lexical expansion, such
as WordNet (Fellbaum, 1998).
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
287
References
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modeling,
1(1).
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In Proceedings
of COLING: Posters.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Oren Kurland and Lillian Lee. 2009. Clusters, lan-
guage models, and ad hoc information retrieval.
ACM Transactions on Information Systems (TOIS),
27(3):13.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
KDD.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. Proceedings of COLING,
Mumbai, India.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of NAACL.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Ellen M Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
288
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 181?190,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Probabilistic Modeling of Joint-context in Distributional Similarity
Oren Melamud
?
, Ido Dagan
?
, Jacob Goldberger
?
, Idan Szpektor
?
, Deniz Yuret
?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
? Koc? University
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com, dyuret@ku.edu.tr
Abstract
Most traditional distributional similarity
models fail to capture syntagmatic patterns
that group together multiple word features
within the same joint context. In this work
we introduce a novel generic distributional
similarity scheme under which the power
of probabilistic models can be leveraged
to effectively model joint contexts. Based
on this scheme, we implement a concrete
model which utilizes probabilistic n-gram
language models. Our evaluations sug-
gest that this model is particularly well-
suited for measuring similarity for verbs,
which are known to exhibit richer syntag-
matic patterns, while maintaining compa-
rable or better performance with respect
to competitive baselines for nouns. Fol-
lowing this, we propose our scheme as a
framework for future semantic similarity
models leveraging the substantial body of
work that exists in probabilistic language
modeling.
1 Introduction
The Distributional Hypothesis is commonly
phrased as ?words which are similar in meaning
occur in similar contexts? (Rubenstein and Good-
enough, 1965). Distributional similarity models
following this hypothesis vary in two major as-
pects, namely the representation of the context and
the respective computational model. Probably the
most prominent class of distributional similarity
models represents context as a vector of word fea-
tures and computes similarity using feature vector
arithmetics (Lund and Burgess, 1996; Turney et
al., 2010). To construct the feature vectors, the
context of each target word token
1
, which is com-
monly a word window around it, is first broken
1
We use word type to denote an entry in the vocabulary,
and word token for a particular occurrence of a word type.
into a set of individual independent words. Then
the weights of the entries in the word feature vec-
tor capture the degree of association between the
target word type and each of the individual word
features, independently of one another.
Despite its popularity, it was suggested that
the word feature vector approach misses valu-
able information, which is embedded in the co-
location and inter-relations of words (e.g. word
order) within the same context (Ruiz-Casado et al.,
2005). Following this motivation, Ruiz-Casado
et al. (2005) proposed an alternative composite-
feature model, later adopted in (Agirre et al.,
2009). This model adopts a richer context repre-
sentation by considering entire word window con-
texts as features, while keeping the same compu-
tational vector-based model. Although showing
interesting potential, this approach suffers from a
very high-dimensional feature space resulting in
data sparseness problems. Therefore, it requires
exceptionally large learning corpora to consider
large windows effectively.
A parallel line of work adopted richer context
representations as well, with a different compu-
tational model. These works utilized neural net-
works to learn low dimensional continuous vector
representations for word types, which were found
useful for measuring semantic similarity (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
These vectors are trained by optimizing the pre-
diction of target words given their observed con-
texts (or variants of this objective). Most of these
models consider each observed context as a joint
set of context words within a word window.
In this work we follow the motivation in the pre-
vious works above to exploit richer joint-context
representations for modeling distributional simi-
larity. Under this approach the set of features in
the context of each target word token is consid-
ered to jointly reflect on the meaning of the target
word type. To further facilitate this type of mod-
181
eling we propose a novel probabilistic computa-
tional scheme for distributional similarity, which
leverages the power of probabilistic models and
addresses the data sparseness challenge associated
with large joint-contexts. Our scheme is based on
the following probabilistic corollary to the distri-
butional hypothesis:
(1)?words are similar in meaning if
they are likely to occur in the same contexts?
To realize this corollary, our distributional sim-
ilarity scheme assigns high similarity scores to
word pairs a and b, for which a is likely in the con-
texts that are observed for b and vice versa. The
scheme is generic in the sense that various under-
lying probabilistic models can be used to provide
the estimates for the likelihood of a target word
given a context. This allows concrete semantic
similarity models based on this scheme to lever-
age the capabilities of probabilistic models, such
as established language models, which typically
address the modeling of joint-contexts.
We hypothesize that an underlying model that
could capture syntagmatic patterns in large word
contexts, yet is flexible enough to deal with data
sparseness, is desired. It is generally accepted
that the semantics of verbs in particular are cor-
related with their syntagmatic properties (Levin,
1993; Hanks, 2013). This provides grounds to ex-
pect that such model has the potential to excel for
verbs. To capture syntagmatic patterns, we choose
in this work standard n-gram language models as
the basis for a concrete model implementing our
scheme. This choice is inspired by recent work on
learning syntactic categories (Yatbaz et al., 2012),
which successfully utilized such language mod-
els to represent word window contexts of target
words. However, we note that other richer types
of language models, such as class-based (Brown
et al., 1992) or hybrid (Tan et al., 2012), can be
seamlessly integrated into our scheme.
Our evaluations suggest that our model is in-
deed particularly advantageous for measuring se-
mantic similarity for verbs, while maintaining
comparable or better performance with respect to
competitive baselines for nouns.
2 Background
In this section we provide additional details re-
garding previous works that we later use as base-
lines in our evaluations.
To implement the composite-feature approach,
Ruiz-Casado et al. (2005) used a Web search en-
gine to compare entire window contexts of target
word types. For example, a single feature that
could be retrieved this way for the target word like
is ?Children cookies and milk?. They showed
good results on detecting synonyms in the 80
multiple-choice questions TOEFL test. Agirre et
al. (2009) constructed composite-feature vectors
using an exceptionally large 1.6 Teraword learn-
ing corpus. They found that this approach out-
performs the traditional independent feature vec-
tor approach on a subset of the WordSim353 test-
set (Finkelstein et al., 2001), which is designed to
test the more restricted relation of semantic simi-
larity (to be distinguished from looser semantic re-
latedness). We are not aware of additional works
following this approach, of using entire word win-
dows as features.
Neural networks have been used to train lan-
guage models that are based on low dimensional
continuous vector representations for word types,
also called word embeddings (Bengio et al., 2003;
Mikolov et al., 2010). Although originally de-
signed to improve language models, later works
have shown that such word embeddings are useful
in various other NLP tasks, including measuring
semantic similarity with vector arithmetics (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
Specifically, the recent work by Mikolov et al.
(2013) introduced the CBOW and Skip-gram mod-
els, achieving state-of-the-art results in detecting
semantic analogies. The CBOW model is trained
to predict a target word given the set of context
words in a word window around it, where this
context is considered jointly as a bag-of-words.
The Skip-gram model is trained to predict each of
the context words independently given the target
word.
3 Probabilistic Distributional Similarity
3.1 Motivation
In this section we briefly demonstrate the bene-
fits of considering joint-contexts of words. As an
illustrative example, we note that the target words
like and surround may share many individual word
features such as ?school? and ?campus? in the sen-
tences ?Mary?s son likes the school campus? and
?The forest surrounds the school campus?. This
potentially implies that individual features may
not be sufficient to accurately reflect the difference
182
between such words. Alternatively, we could use
the following composite features to model the con-
text of these words, ?Mary?s son the school
campus? and ?The forest the school campus?.
This would discriminate better between like and
surround. However, in this case sentences such as
?Mary?s son likes the school campus? and ?John?s
son loves the school campus? will not provide any
evidence to the similarity between like and love,
since ?Mary?s son the school campus? is a dif-
ferent feature than ?John?s son the school cam-
pus?.
In the remainder of this section we propose
a modeling scheme and then a concrete model,
which can predict that like and love are likely to
occur in each other?s joint-contexts, whereas like
and surround are not, and then assign similarity
scores accordingly.
3.2 The probabilistic similarity scheme
We now present a computational scheme that re-
alizes our proposed corollary (1) to the distribu-
tional hypothesis and facilitates robust probabilis-
tic modeling of joint contexts. First, we slightly
rephrase this corollary as follows: ?words a and
b are similar in meaning if word b is likely in
the contexts of a and vice versa?. We denote the
probability of an occurrence of a target word b
given a joint-context c by p(b|c). For example,
p(love|?Mary?s son the school campus?) is the
probability of the word love to be the filler of the
?place-holder? in the given joint-context ?Mary?s
son the school campus?. Similarly, we denote
p(c|a) as the probability of a joint-context c given
a word a, which fills its place-holder. We now
propose p
sim
(b|a) to reflect how likely b is in the
joint-contexts of a. We define this measure as:
(2)p
sim
(b|a) =
?
c
p(c|a) ? p(b|c)
where c goes over all possible joint-contexts in the
language.
To implement this measure we need to find
an efficient estimate for p
sim
(b|a). The most
straight forward strategy is to compute sim-
ple corpus count ratio estimates for p(b|c) and
p(c|a), denoted p
#
(b|c) =
count(b,c)
count(?,c)
and
p
#
(c|a) =
count(a,c)
count(a,?)
. However, when consid-
ering large joint-contexts for c, this approach be-
comes similar to the composite-feature approach
since it is based on co-occurrence counts of tar-
get words with large joint-contexts. Therefore, we
expect in this case to encounter the data sparse-
ness problems mentioned in Section 1, where se-
mantically similar word type pairs that share only
few or no identical joint-contexts yield very low
p
sim
(b|a) estimates.
To address the data sparseness challenge and
adopt more advanced context modeling, we aim to
use a more robust underlying probabilistic model
? for our scheme and denote the probabilities es-
timated by this model by p
?
(b|c) and p
?
(c|a). We
note that contrary to the count ratio model, given a
robust model ?, such as a language model, p
?
(b|c)
and p
?
(c|a) can be positive even if the target words
b and a were not observed with the joint-context c
in the learning corpus.
While using p
?
(b|c) and p
?
(c|a) to estimate the
value of p
sim
(b|a) addresses the sparseness chal-
lenge, it introduces a computational challenge.
This is because estimating p
sim
(b|a) would re-
quire computing the sum over all of the joint-
contexts in the learning corpus regardless of
whether they were actually observed with either
word type a or b. For that reason we choose a
middle ground approach, estimating p(b|c) with
?, while using a count ratio estimate for p(c|a),
as follows. We denote the collection of all joint-
contexts observed for the target word a in the
learning corpus by C
a
, where |C
a
|= count(a, ?).
For example, C
like
= {c
1
=?Mary?s son the
school campus?, c
2
=?John?s daughter to read
poetry?,...}. We note that this collection is a multi-
set, where the same joint-context can appear more
than once.
We now approximate p
sim
(b|a) from Equation
(2) as follows:
(3)
p?
sim
?
(b|a) =
?
c
p
#
(c|a) ? p
?
(b|c) =
1
|C
a
|
?
?
c?C
a
p
?
(b|c)
We note that this formulation still addresses
sparseness of data by using a robust model, such as
a language model, to estimate p
?
(b|c). At the same
time it requires our model to sum only over the
joint-contexts in the collection C
a
, since contexts
not observed for a yield p
#
(c|a) = 0. Even so,
since the size of these context collections grows
linearly with the corpus size, considering all ob-
served contexts may still present a scalability chal-
lenge. Nevertheless, we expect our approximation
p?
sim
?
(b|a) to converge with a reasonable sample
183
size from a?s joint-contexts. Therefore, in order
to bound computational complexity, we limit the
size of the context collections used to train our
model to a maximum of N by randomly sampling
N entries from larger collections. In all our ex-
periments we use N = 10, 000. Higher values
of N yielded negligible performances differences.
Overall we see that our model estimates p?
sim
?
(b|a)
as the average probability predicted for b in (a
large sample of) the contexts observed for a.
Finally, we define our similarity measure for tar-
get word types a and b:
(4)sim
?
(a, b) =
?
p?
sim
?
(b|a) ? p?
sim
?
(a|b)
As intended, this similarity measure promotes
word pairs in which both b is likely in the con-
texts of a and vice versa. Next, we describe a
model which implements this scheme with an n-
gram language model as a concrete choice for ?.
3.3 Probabilistic similarity using language
models
In this work we focus on the word window context
representation, which is the most common. We
define a word window of order k around a target
word as a window with up to k words to each side
of the target word, not crossing sentence bound-
aries. The word window does not include the tar-
get word itself, but rather a ?place-holder? for it.
Since word windows are sequences of words,
probabilistic language models are a natural choice
of a model ? for estimating p
?
(b|c). Language
models assign likelihood estimates to sequences
of words using approximation strategies. In
this work we choose n-gram language models,
aiming to capture syntagmatic properties of the
word contexts, which are sensitive to word or-
der. To approximate the probability of long se-
quences of words, n-gram language models com-
pute the product of the estimated probability of
each word in the sequence conditioned on at most
the n ? 1 words preceding it. Furthermore, they
use ?discounting? methods to improve the esti-
mates of conditional probabilities when learning
data is sparse. Specifically, in this work we use
the Kneser-Ney n-gram model (Kneser and Ney,
1995).
We compute p
?
(b|c) as follows:
(5)p
?
(b|c) =
p
?
(b, c)
p
?
(c)
where p
?
(b, c) is the probability of the word se-
quence comprising the word window c, in which
the word b fills the place-holder. For instance, for
c = ?I drive my to work every? and b = car,
p
?
(b, c) is the estimated language model probabil-
ity of ?I drive my car to work every?. p
?
(c) is the
marginal probability of p
?
(?, c) over all possible
words in the vocabulary.
2
4 Experimental Settings
Although sometimes used interchangeably, it is
common to distinguish between semantic simi-
larity and semantic relatedness (Budanitsky and
Hirst, 2001; Agirre et al., 2009). Semantic simi-
larity is used to describe ?likeness? relations, such
as the relations between synonyms, hypernym-
hyponyms, and co-hyponyms. Semantic relat-
edness refers to a broader range of relations in-
cluding also meronymy and various other asso-
ciative relations as in ?pencil-paper? or ?penguin-
Antarctica?. In this work we focus on semantic
similarity and evaluate all compared methods on
several semantic similarity tasks.
Following previous works (Lin, 1998; Riedl and
Biemann, 2013) we use Wordnet to construct large
scale gold standards for semantic similarity evalu-
ations. We perform the evaluations separately for
nouns and verbs to test our hypothesis that our
model is particularly well-suited for verbs. To fur-
ther evaluate our results on verbs we use the verb
similarity test-set released by (Yang and Powers,
2006), which contains pairs of verbs associated
with semantic similarity scores based on human
judgements.
4.1 Compared methods
We compare our model with a traditional fea-
ture vector model, the composite-feature model
(Agirre et al., 2009), and the recent state-of-the-art
word embedding models, CBOW and Skip-gram
(Mikolov et al., 2013), all trained on the same
learning corpus and evaluated on equal grounds.
We denote the traditional feature vector baseline
by IFV
W?k
, where IFV stands for ?Independent-
Feature Vector? and k is the order of the con-
text word window considered. Similarly, we
2
Computing p
?
(c) by summing over all possible place-
holder filler words, as we did in this work, is computation-
ally intensive. However, this can be done more efficiently
by implementing customized versions of (at least some) n-
gram language models with little computational overhead,
e.g. by counting the learning corpus occurrences of n-gram
templates, in which one of the elements matches any word.
184
denote the composite-feature vector baseline by
CFV
W?k
, where CFV stands for ?Composite-
Feature Vector?. This baseline constructs
traditional-like feature vectors, but considers en-
tire word windows around target word tokens as
single features. In both of these baselines we use
Cosine as the vector similarity measure, and posi-
tive pointwise mutual information (PPMI) for the
feature vector weights. PPMI is a well-known
variant of pointwise mutual information (Church
and Hanks, 1990), and the combination of Cosine
with PPMI was shown to perform particularly well
in (Bullinaria and Levy, 2007).
We denote Mikolov?s CBOW and Skip-gram
baseline models by CBOW
W?k
and SKIP
W?k
respectively, where k denotes again the order of
the window used to train these models. We used
Mikolov?s word2vec utility
3
with standard param-
eters (600 dimensions, negative sampling 15) to
learn the word embeddings, and Cosine as the vec-
tor similarity measure between them.
As the underlying probabilistic language model
for our method we use the Berkeley implementa-
tion
4
(Pauls and Klein, 2011) of the Kneser-Ney
n-gram model with the default discount parame-
ters. We denote our model PDS
W?k
, where PDS
stands for ?Probabilistic Distributional Similar-
ity?, and k is the order of the context word win-
dow. In order to avoid giving our model an un-
fair advantage of tuning the order of the language
model n as an additional parameter, we use a fixed
n = k + 1. This means that the conditional prob-
abilities that our n-gram model learns consider a
scope of up to half the size of the window, which
is the distance in words between the target word
and either end of the window. We note that this
is the smallest reasonable value for n, as smaller
values effectively mean that there will be context
words within the window that are more than n
words away from the target word, and therefore
will not be considered by our model.
As learning corpus we used the first CD of
the freely available Reuters RCV1 dataset (Rose
et al., 2002). This learning corpus contains ap-
proximately 100M words, which is comparable in
size to the British National Corpus (BNC) (As-
ton, 1997). We first applied part-of-speech tag-
ging and lemmatization to all words. Then we
represented each word w in the corpus as the pair
3
http://code.google.com/p/word2vec
4
http://code.google.com/p/berkeleylm/
[pos(w), lemma(w)], where pos(w) is a coarse-
grained part-of-speech category and lemma(w) is
the lemmatized form of w. Finally, we converted
every pair [pos(w), lemma(w)] that occurs less
than 100 times in the learning corpus to the pair
[pos(w), ? ], which represents all rare words of the
same part-of-speech tag. Ignoring rare words is a
common practice used in order to clean up the cor-
pus and reduce the vocabulary size (Gorman and
Curran, 2006; Collobert and Weston, 2008).
The above procedure resulted in a word vocabu-
lary of 27K words. From this vocabulary we con-
structed a target verb set with over 2.5K verbs by
selecting all verbs that exist in Wordnet (Fellbaum,
2010). We repeated this procedure to create a tar-
get noun set with over 9K nouns. We used our
learning corpus for all compared methods and had
them assign a semantic similarity score for every
pair of verbs and every pair of nouns in these tar-
get sets. These scores were later used in all of our
evaluations.
4.2 Wordnet evaluation
There is a shortage of large scale test-sets for se-
mantic similarity. Popular test-sets such as Word-
Sim353 and the TOEFL synonyms test contain
only 353 and 80 test items respectively, and there-
fore make it difficult to obtain statistically signif-
icant results. To automatically construct larger-
scale test-sets for semantic similarity, we sampled
large target word subsets from our corpus and used
Wordnet as a gold standard for their semantically
similar words, following related previous evalua-
tions (Lin, 1998; Riedl and Biemann, 2013). We
constructed two test-sets for our primary evalua-
tion, one for verb similarity and another for noun
similarity.
To perform the verb similarity evaluation, we
randomly sampled 1,000 verbs from the target
verb set, where the probability of each verb to be
sampled is set to be proportional to its frequency in
the learning corpus. Next, for each sampled verb
a we constructed a Wordnet-based gold standard
set of semantically similar words. In this set each
verb a
?
is annotated as a ?synonym? of a if at least
one of the senses of a
?
is a synonym of any of the
senses of a. In addition, each verb a
?
is annotated
as a ?semantic neighbor? of a if at least one of the
senses of a
?
is a synonym, co-hyponym, or a di-
rect hypernym/hyponym of any of the senses of a.
We note that by definition all verbs annotated as
185
synonyms of a are annotated as semantic neigh-
bors as well. Next, per each verb a and an evalu-
ated method, we generated a ranked list of all other
verbs, which was induced according to the similar-
ity scores of this method.
Finally, we evaluated the compared methods
on two tasks, ?synonym detection? and ?seman-
tic neighbor detection?. In the synonym detection
task we evaluated the methods? ability to retrieve
as much verbs annotated in our gold standard as
?synonyms?, in the top-n entries of their ranked
lists. Similarly, we evaluated all methods on the
?semantic neighbors? task. The synonym detec-
tion task is designed to evaluate the ability of the
compared methods to identify a more restrictive
interpretation of semantic similarity, while the se-
mantic neighbor detection task does the same for
a somewhat broader interpretation.
We repeated the above procedure for sam-
pling 1,000 target nouns, constructing the noun
Wordnet-based gold standards and evaluating on
the two semantic similarity tasks.
4.3 VerbSim evaluation
The publicly available VerbSim test-set contains
130 verb pairs, each annotated with an average of
6 human judgements of semantic similarity (Yang
and Powers, 2006). We extracted a 107 pairs sub-
set of this dataset for which all verbs are in our
learning corpus. We followed works such as (Yang
and Powers, 2007; Agirre et al., 2009) and com-
pared the Spearman correlations between the verb-
pair similarity scores assigned by the compared
methods and the manually annotated scores in this
dataset.
5 Results
For each method and verb a in our 1,000 tested
verbs, we used the Wordnet gold standard to com-
pute the precision at top-1, top-5 and top-10 of the
ranked list generated by this method for a. We
then computed mean precision values averaged
over all verbs for each of the compared methods,
denoted as P@1, P@5 and P@10. The detailed
report of P@10 results is omitted for brevity, as
they behave very similarly to P@5. We varied the
context window order used by all methods to test
its effect on the results. We measured the same
metrics for nouns.
The results of our Wordnet-based 1,000 verbs
evaluation are presented in the upper part of Fig-
ure 1. The results show significant improvement
of our method over all baselines, with a margin
between 2 to 3 points on the synonyms detection
task and 5 to 7 points on the semantic neighbors
detection task. Our best performing configura-
tions are PDS
W?3
and PDS
W?4
, outperform-
ing all other baselines on both tasks and in all pre-
cision categories. This difference is statistically
significant at p < 0.001 using a paired t-test in all
cases except for the P@1 in the synonyms detec-
tion task. Within the baselines, the composite fea-
ture vector (CFV) performs somewhat better than
the independent feature vector (IFV) baseline, and
both methods perform best around window order
of two, with gradual decline for larger windows.
The word embedding baselines, CBOW and SKIP,
perform comparably to the feature vector base-
lines and to one another, with best performance
achieved around window order of four.
When gradually increasing the context window
order within the range of up to 4 words, our PDS
model shows improvement. This is in contrast to
the feature vector baselines, whose performance
declines for context window orders larger than 2.
This suggests that our approach is able to take ad-
vantage of larger contexts in comparison to stan-
dard feature vector models. The decline in perfor-
mance for the independent feature vector baseline
(IFV) may be related to the fact that independent
features farther away from the target word are gen-
erally more loosely related to it. This seems con-
sistent with previous works, where narrow win-
dows of the order of two words performed well
(Bullinaria and Levy, 2007; Agirre et al., 2009;
Bruni et al., 2012) and in particular so when eval-
uating semantic similarity rather than relatedness.
On the other hand, the decline in performance for
the composite feature vector baseline (CFV) may
be attributed to the data sparseness phenomenon
associated with larger windows. The performance
of the word embedding baselines (CBOW and
SKIP) starts declining very mildly only for win-
dow orders larger than 4. This might be attributed
to the fact that these models assign lower weights
to context words the farther away they are from the
center of the window.
The results of our Wordnet-based 1,000 nouns
evaluation are presented in the lower part of Fig-
ure 1. These results are partly consistent with the
results achieved for verbs, but with a couple of
notable differences. First, though our model still
186
Figure 1: Mean precision scores as a function of window order, obtained against the Wordnet-based gold
standard, on both the verb and noun test-sets with both the synonyms and semantic neighbor detection
tasks. ?P@n? stands for precision in the top-n words of the ranked lists. Note that the Y-axis scale varies
between graphs.
outperforms or performs comparably to all other
baselines, in this case the advantage of our model
over the feature vector baselines is much more
moderate and not statistically significant. Second,
the word embedding baselines generally perform
worst (with CBOW performing a little better than
SKIP), and our model outperforms them in both
P@5 and P@10 with a margin of around 2 points
for the synonyms detection task and 3-4 points for
the neighbor detection task, with statistical signif-
icance at p < 0.001.
Next, to reconfirm the particular applicability
of our model to verb similarity as apparent from
the Wordnet evaluation, we performed the Verb-
Sim evaluation and present the results in Table 1.
We compared the Spearman correlation obtained
for the top-performing window order of each of
the evaluated methods in the Wordnet verbs eval-
uation. We present two sets of results. The ?all
scores? results follow the standard evaluation pro-
cedure, considering all similarity scores produced
by each method. In the ?top-100 scores? results,
for each method we converted to zero the scores
that it assigned to word pairs, where neither of
the words is in the top-100 most similar words
of the other. Then we performed the evaluation
with these revised scores. This procedure focuses
on evaluating the quality of the methods? top-
100 ranked word lists. The results show that our
method outperforms all baselines by a nice mar-
187
Method All scores top-100 scores
PDS W-4 0.616 0.625
CFV W-2 0.477 0.497
IFV W-2 0.467 0.546
SKIP W-4 0.469 0.512
CBOW W-5 0.528 0.469
Table 1: Spearman correlation values obtained for
the VerbSim evaluation. Each method was evalu-
ated with the optimal window order found in the
Wordnet verbs evaluation.
gin of more than 8 points with the score of 0.616
and 0.625 for the ?all scores? and ?top-100 scores?
evaluations respectively. Though not statistically
significant, due to the small test-set size, these re-
sults support the ones from the Wordnet evalu-
ation, suggesting that our model performs better
than the baselines on measuring verb similarity.
In summary, our results suggest that in lack of a
robust context modeling scheme it is hard for dis-
tributional similarity models to effectively lever-
age larger word window contexts for measuring
semantic similarity. It appears that this is some-
what less of a concern when it comes to noun sim-
ilarity, as the simple feature vector models reach
near-optimal performance with small word win-
dows of order 2, but it is an important factor for
verb similarity. In his recent book, Hanks (2013)
claims that contrary to nouns, computational mod-
els that are to capture the meanings of verbs must
consider their syntagmatic patterns in text. Our
particularly good results on verb similarity sug-
gest that our modeling approach is able to cap-
ture such information in larger context windows.
We further conjecture that the reason the word em-
bedding baselines did not do as well as our model
on verb similarity might be due to their particular
choice of joint-context formulation, which is not
sensitive to word order. However, these conjec-
tures should be further validated with additional
evaluations in future work.
6 Future Directions
In this paper we investigated the potential for im-
proving distributional similarity models by model-
ing jointly the occurrence of several features under
the same context. We evaluated several previous
works with different context modeling approaches
and suggest that the type of the underlying con-
text modeling may have significant effect on the
performance of the semantic model. Further-
more, we introduced a generic probabilistic distri-
butional similarity approach, which can leverage
the power of established probabilistic language
models to effectively model joint-contexts for the
purpose of measuring semantic similarity. Our
concrete model utilizing n-gram language models
outperforms several competitive baselines on se-
mantic similarity tasks, and appears to be partic-
ularly well-suited for verbs. In the remainder of
this section we describe some potential future di-
rections that can be pursued.
First, the performance of our generic scheme
is largely inherited from the nature of its under-
lying language model. Therefore, we see much
potential in exploring the use of other types of
language models, such as class-based (Brown et
al., 1992), syntax-based (Pauls and Klein, 2012)
or hybrid (Tan et al., 2012). Furthermore, a sim-
ilar approach to ours could be attempted in word
embedding models. For instance, our syntagmatic
joint-context modeling approach could be investi-
gated by word embedding models to generate bet-
ter embeddings for verbs.
Another direction relates to the well known ten-
dency of many words, and particularly verbs, to
assume different meanings (or senses) under dif-
ferent contexts. To address this phenomenon con-
text sensitive similarity and inference models have
been proposed (Dinu and Lapata, 2010; Melamud
et al., 2013). Similarly to many semantic similar-
ity models, our current model aggregates informa-
tion from all observed contexts of a target word
type regardless of its different senses. However,
we believe that our approach is well suited to ad-
dress context sensitive similarity with proper en-
hancements, as it considers joint-contexts that can
more accurately disambiguate the meaning of tar-
get words. As an example, it is possible to con-
sider the likelihood of word b to occur in a subset
of the contexts observed for word a, which is bi-
ased towards a particular sense of a.
Finally, we note that our model is not a classic
vector space model and therefore common vec-
tor composition approaches (Mitchell and Lap-
ata, 2008) cannot be directly applied to it. In-
stead, other methods, such as similarity of com-
positions (Turney, 2012), should be investigated to
extend our approach for measuring similarity be-
tween phrases.
188
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT) and the Scien-
tific and Technical Research Council of Turkey
(T
?
UB
?
ITAK, Grant Number 112E277).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL. Association for Computational Lin-
guistics.
Guy Aston. 1997. The BNC Handbook Exploring the
British National Corpus with SARA Guy Aston and
Lou Burnard.
Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of ACL.
Alexander Budanitsky and Graeme Hirst. 2001.
Semantic distance in wordnet: An experimental,
application-oriented evaluation of five measures. In
Workshop on WordNet and Other Lexical Resources.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510?526.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP.
Christiane Fellbaum. 2010. WordNet. Springer.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web. ACM.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Pro-
ceedings of ACL.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing. IEEE.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago press.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of ACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N -Gram Language Models. In Proceedings of ACL.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of ACL.
Martin Riedl and Chris Biemann. 2013. Scaling to
large?3 data: An efficient and effective method to
compute distributional thesauri. In Proceedings of
EMNLP.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1-from Yester-
day?s News to Tomorrow?s Language Resources. In
LREC.
189
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Using context-window overlapping
in synonym discovery and ontology extension. Pro-
ceedings of RANLP.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2012. A scalable distributed syntactic, semantic,
and lexical language model. Computational Lin-
guistics, 38(3):631?671.
Peter D. Turney, Patrick Pantel, et al. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of artificial intelligence research.
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Researc,
44(1):533?585, May.
Dongqiang Yang and David M. W. Powers. 2006. Verb
similarity on the taxonomy of wordnet. In the 3rd
International WordNet Conference (GWC-06).
Dongqiang Yang and David M. W. Powers. 2007.
An empirical investigation into grammatically con-
strained contexts in predicting distributional similar-
ity. In Australasian Language Technology Workshop
2007, pages 117?124.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
EMNLP.
190
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 143?148,
Baltimore, Maryland USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Automatic Generation of Challenging Distractors
Using Context-Sensitive Inference Rules
Torsten Zesch
Language Technology Lab
University of Duisburg-Essen
torsten.zesch@uni-due.de
Oren Melamud
Computer Science Department
Bar-Ilan University
melamuo@cs.biu.ac.il
Abstract
Automatically generating challenging dis-
tractors for multiple-choice gap-fill items
is still an unsolved problem. We propose
to employ context-sensitive lexical infer-
ence rules in order to generate distractors
that are semantically similar to the gap tar-
get word in some sense, but not in the par-
ticular sense induced by the gap-fill con-
text. We hypothesize that such distrac-
tors should be particularly hard to distin-
guish from the correct answer. We focus
on verbs as they are especially difficult to
master for language learners and find that
our approach is quite effective. In our test
set of 20 items, our proposed method de-
creases the number of invalid distractors in
90% of the cases, and fully eliminates all
of them in 65%. Further analysis on that
dataset does not support our hypothesis re-
garding item difficulty as measured by av-
erage error rate of language learners. We
conjecture that this may be due to limita-
tions in our evaluation setting, which we
plan to address in future work.
1 Introduction
Multiple-choice gap-fill items as illustrated in Fig-
ure 1 are frequently used for both testing lan-
guage proficiency and as a learning device. Each
item consists of a carrier sentence that provides
the context to a target word. The target word is
blanked and presented as one possible gap-fill an-
swer together with a certain number (usually 3)
of distractors. Given a desired target word, car-
rier sentences containing it can be automatically
selected from a corpus. Some methods even select
only sentences where the target word is used in a
certain sense (Liu et al., 2005). Then, the main
problem is to pick challenging distractors that are
Figure 1: Multiple-choice gap-fill item.
reasonably hard to distinguish from the correct an-
swer (i.e. the target word) on one hand, yet cannot
be considered as correct answers on the other.
In this paper we propose to generate distrac-
tors that are semantically similar to the gap tar-
get word in some sense, but not in the particu-
lar sense induced by the gap-fill context, thereby
making them difficult to distinguish from the tar-
get word. For example, the distractor gain in Fig-
ure 1 is semantically similar to acquire, but is not
appropriate in the particular context of purchasing
companies, and therefore has high distractive po-
tential. On the other hand, the distractor purchase
is a correct answer in this context and is therefore
an invalid distractor. To generate challenging dis-
tractors, we utilize context-sensitive lexical infer-
ence rules that can discriminate between appropri-
ate substitutes of a target word given its context
and other inappropriate substitutes.
In the next section, we give an overview of pre-
vious work in order to place our contribution into
context.
2 Previous Work
The process of finding good distractors involves
two steps: Candidate Selection controls the diffi-
culty of the items, while Reliability Checking en-
sures that the items remain solvable, i.e. it ensures
143
that there is only one correct answer. We note
that this work is focused on single-word distrac-
tors rather than phrases (Gates et al., 2011), and
only on target isolated carrier sentences rather than
longer texts as in (Mostow and Jang, 2012).
2.1 Candidates Selection
In some settings the set of possible distractors is
known in advance, e.g. the set of English prepo-
sitions in preposition exercises (Lee and Seneff,
2007) or a confusion set with previously known
errors like {two, too, to}. Sakaguchi et al. (2013)
use data from the Lang-8 platform (a corpus of
manually annotated errors
1
) in order to determine
typical learner errors and use them as distractors.
However, in the common setting only the target
word is known and the set of distractors needs to
be automatically generated.
Randomly selecting distractors is a valid strat-
egy (Mostow and Jang, 2012), but it is only suit-
able for the most beginner learners. More ad-
vanced learners can easily rule out distractors that
do not fit grammatically or are too unrelated se-
mantically. Thus, more advanced approaches usu-
ally employ basic strategies, such as choosing dis-
tractors with the same part-of-speech tag as the
target word, or distractors with a corpus frequency
comparable to the target word (Hoshino and Naka-
gawa, 2007) (based on the assumption that corpus
frequency roughly correlates with word difficulty).
Pino and Eskenazi (2009) use distractors that are
morphologically, orthographically, or phonetically
similar (e.g. bread ? beard).
Another approach used in previous works to
make distractors more challenging is utilizing the-
sauri (Sumita et al., 2005; Smith and Avinesh,
2010) or taxonomies (Hoshino and Nakagawa,
2007; Mitkov et al., 2009) to select words that are
semantically similar to the target word. In addi-
tion to the target word, some approaches also con-
sider the semantic relatedness of distractors with
the whole carrier sentence or paragraph (Pino et
al., 2008; Agarwal and Mannem, 2011; Mostow
and Jang, 2012), i.e. they pick distractors that are
from the same domain as the target word.
Generally, selecting more challenging distrac-
tors usually means making them more similar to
the target word. As this increases the probability
that a distractor might actually be another correct
answer, we need a more sophisticated approach for
1
http://cl.naist.jp/nldata/lang-8/
checking the reliability of the distractor set.
2.2 Reliability Checking
In order to make sure that there is only one correct
answer to a gap-fill item, there needs to be a way
to decide for each distractor whether it fits into the
context of the carrier sentence or not. In those
cases, where we have a limited list of potential tar-
get words and distractors, e.g. in preposition exer-
cises (Lee and Seneff, 2007), a supervised classi-
fier can be trained to do this job. Given enough
training data, this approach yields very high preci-
sion, but it cannot be easily applied to open word
classes like nouns or verbs, which are much larger
and dynamic in nature.
When we do not have a closed list of potential
distractors at hand, one way to perform reliabil-
ity checking is by considering collocations involv-
ing the target word (Pino et al., 2008; Smith and
Avinesh, 2010). For example, if the target word
is strong, we can find the collocation strong tea.
Then we can use powerful as a distractor because
it is semantically similar to strong, yet *powerful
tea is not a valid collocation. This approach is ef-
fective, but requires strong collocations to discrim-
inate between valid and invalid distractors. There-
fore it cannot be used with carrier sentences that
do not contain strong collocations, such as the sen-
tence in Figure 1.
Sumita et al. (2005) apply a simple web search
approach to judge the reliability of an item. They
check whether the carrier sentence with the target
word replaced by the distractor can be found on
the web. If such a sentence is found, the distrac-
tor is discarded. We note that the applicability of
this approach is limited, as finding exact matches
for such artificial sentences can be unlikely due
to sparseness of natural languages. Therefore not
finding an exact match does not necessarily rule
out the possibility of an invalid distractor.
3 Automatic Generation of Challenging
Distractors
Our goal is to automatically generate distractors
that are as ?close? to the target word as possible,
yet do not fit the carrier sentence context. To ac-
complish this, our strategy is to first generate a set
of distractor candidates, which are semantically
similar to the target word. Then we use context-
sensitive lexical inference rules to filter candidates
that fit the context, and thus cannot be used as dis-
144
tractors. In the remainder of this section we de-
scribe this procedure in more detail.
3.1 Context-Sensitive Inference Rules
A lexical inference rule ?LHS ? RHS?, such as
?acquire ? purchase?, specifies a directional in-
ference relation between two words (or terms). A
rule can be applied when its LHS matches a word
in a text T , and then that word is substituted for
RHS, yielding the modified text H . For example,
applying the rule above to ?Microsoft acquired
Skype?, yields ?Microsoft purchased Skype?. If the
rule is true then the meaning of H is inferred from
the meaning of T . A popular way to learn lex-
ical inference rules in an unsupervised setting is
by using distributional similarity models (Lin and
Pantel, 2001; Kotlerman et al., 2010). Under this
approach, target words are represented as vectors
of context features, and the score of a rule between
two target words is based on vector arithmetics.
One of the main shortcomings of such rules is
that they are context-insensitive, i.e. they have a
single score, which is not assessed with respect to
the concrete context T under which they are ap-
plied. However, the appropriateness of an infer-
ence rule may in fact depend on this context. For
example, ?Microsoft acquire Skype ? Microsoft
purchase Skype?, is an appropriate application of
the rule ?acquire ? purchase?, while ?Children
acquire skills ? Children purchase skills? is not.
To address this issue, additional models were in-
troduced that compute a different context-sensitive
score per each context T , under which it is applied
(Dinu and Lapata, 2010; Melamud et al., 2013).
In this work, we use the resource provided
by Melamud et al. (2013), which includes both
context-sensitive and context-insensitive rules for
over 2,000 frequent verbs.
2
We use these rules to
generate challenging distractors as we show next.
3.2 Distractor Selection & Reliability
We start with the following illustrative example to
motivate our approach. While the words purchase
and acquire are considered to be almost perfect
synonyms in sentences like Microsoft acquires
Skype and Microsoft purchases Skype, this is not
true for all contexts. For example, in Children
acquire skills vs. Children purchase skills, the
meaning is clearly not equivalent. These context-
dependent senses, which are particularly typical to
2
http://www.cs.biu.ac.il/nlp/downloads/wt-rules.html
Figure 2: Filtering context-insensitive substitu-
tions with context-sensitive ones in order to get
challenging distractors.
verbs, make it difficult for learners to understand
how to properly use these words.
Acquiring such fine-grained sense distinction
skills is a prerequisite for really competent lan-
guage usage. These skills can be trained and tested
with distractors, such as purchase in the exam-
ple above. Therefore, such items are good indi-
cators in language proficiency testing, and should
be specifically trained when learning a language.
To generate such challenging distractors, we
first use the context-insensitive rules, whose LHS
matches the carrier sentence target word, to create
a distractor candidate set as illustrated on the left-
hand side of Figure 2. We include in this set the
top-n inferred words that correspond to the high-
est rule scores. These candidate words are inferred
by the target word, but not necessarily in the par-
ticular context of the carrier sentence. Therefore,
we expect this set to include both correct answers,
which would render the item unreliable, as well
as good distractors that are semantically similar to
the target word in some sense, but not in the par-
ticular sense induced by the carrier sentence.
Next, we use context-sensitive rules to generate
a distractor black-list including the top-m words
that are inferred by the target word, but this time
taking the context of the carrier sentence into con-
sideration. In this case, we expect the words in
the list to comprise only the gap-fillers that fit the
given context as illustrated on the right-hand side
of Figure 2. Such gap-fillers are correct answers
and therefore cannot be used as distractors. Fi-
nally, we subtract the black-list distractors from
the initial distractor candidate set and expect the
remaining candidates to comprise only good dis-
tractors. We consider the candidates in this final
set as our generated distractors.
145
3.3 Distractor Ranking
In case our approach returns a large number of
good distractors, we should use ranking to select
the most challenging ones. A simple strategy is
to rely on the corpus frequency of the distractor,
where less frequent means more challenging as it
will not be known to the learner. However, this
tends to put a focus on the more obscure words
of the vocabulary while actually the more frequent
words should be trained more often. Therefore, in
this work we use the scores that were assigned to
the distractors by the context-insensitive inference
rules. Accordingly, the more similar a distractor is
to the target word, the higher rank it will get (pro-
vided that it was not in the distractor black-list).
4 Experiments & Results
In our experiments we wanted to test two hy-
potheses: (i) whether context-sensitive inference
rules are able to reliably distinguish between valid
and invalid distractors, and (ii) whether the gener-
ated distractors are more challenging for language
learners than randomly chosen ones.
We used the Brown corpus (Nelson Francis and
Kuc?era, 1964) as a source for carrier sentences and
selected medium-sized (5-12 tokens long) sen-
tences that contain a main verb. We then manu-
ally inspected this set, keeping only well-formed
sentences that are understandable by a general au-
dience without requiring too much context knowl-
edge. In a production system, this manual pro-
cess would be replaced by a sophisticated method
for obtaining good carrier sentences, but this is be-
yond the scope of this paper. Finally, for this ex-
ploratory study, we only used the first 20 selected
sentences from a much larger set of possible car-
rier sentences.
4.1 Reliability
Our first goal was to study the effectiveness of our
approach in generating reliable items, i.e. items
where the target word is the only correct answer.
In order to minimize impact of pre-processing and
lemmatization, we provided the context-sensitive
inference rules with correctly lemmatized carrier
sentences and marked the target verbs. We found
that we get better results when using a distractor
black-list that is larger than the distractor candi-
date set, as this more aggressively filters invalid
distractors. We used the top-20 distractor black-
list and top-10 distractor candidate set, which lead
Only valid distractors 13/20 (65%)
Mix of valid and invalid 5/20 (25%)
Only invalid distractors 2/20 (10%)
Table 1: Reliability of items after filtering
to generating on average 3.3 distractors per item.
All our generated distractors were checked by
two native English speakers. We count a distrac-
tor as ?invalid? if it was ruled out by at least one
annotator. Table 1 summarizes the results. We
found that in 13 of the 20 items (65%) all distrac-
tors generated by our approach were valid, while
only for 2 items all generated distractors were in-
valid. For the remaining 5 items, our approach re-
turned a mix of valid and invalid distractors. We
note that the unfiltered distractor candidate set al-
ways contained invalid distractors and in 90% of
the items it contained a higher proportion of in-
valid distractors than the filtered one. This sug-
gests that the context-sensitive inference rules are
quite effective in differentiating between the dif-
ferent senses of the verbs.
A main source of error are sentences that do not
provide enough context, e.g. because the subject
is a pronoun. In She [served] one four-year term
on the national committee, it would be acceptable
to insert sold in the context of a report on po-
litical corruption, but a more precise subject like
Barack Obama would render that reading much
more unlikely. Therefore, more emphasis should
be put on selecting better carrier sentences. Se-
lecting longer sentences that provide a richer con-
text would help to rule out more distractor candi-
dates and may also lead to better results when us-
ing the context-sensitive inference rules. However,
long sentences are also more difficult for language
learners, so there will probably be some trade-off.
A qualitative analysis of the results shows that
especially for verbs with clearly distinct senses,
our approach yields good results. For example
in He [played] basketball there while working to-
ward a law degree, our method generates the dis-
tractors compose and tune which are both related
to the ?play a musical instrument? sense. An-
other example is His petition [charged] mental
cruelty, where our method generates among oth-
ers the distractors pay and collect that are both re-
lated to the ?charge taxes? reading of the verb. The
ball [floated] downstream is an example where our
method did not work well. It generated the distrac-
tors glide and travel which also fit the context and
146
Group 1 Group 2
Control Items 0.24? 0.12 0.20? 0.12
Test Items 0.18? 0.17 0.18? 0.15
Table 2: Average error rates on our dataset
should thus not be used as distractors. The verb
float is different from the previous examples, as
all its dominant senses involve some kind of ?float-
ing? even if only metaphorically used. This results
in similar senses that are harder to differentiate.
4.2 Difficulty
Next, we wanted to examine whether our approach
leads to more challenging distractors. For that
purpose we removed the distractors that our an-
notators identified as invalid in the previous step.
We then ranked the remaining distractors accord-
ing to the scores assigned to them by the context-
sensitive inference rules and selected the top-3 dis-
tractors. If our method generated less than 3 dis-
tractors, we randomly generated additional dis-
tractors from the same frequency range as the tar-
get word.
We compared our approach with randomly se-
lected distractors that are in the same order of
magnitude with respect to corpus frequency as the
distractors generated by our method. This way we
ensure that a possible change in distractor diffi-
culty cannot simply be attributed to differences in
the learners? familiarity with the distractor verbs
due to their corpus frequency. We note that ran-
dom selection repeatedly created invalid distrac-
tors that we needed to manually filter out. This
shows that better methods for checking the relia-
bility of items like in our approach are definitely
required.
We randomly split 52 participants (all non-
natives) into two groups, each assigned with a dif-
ferent test version. Table 2 summarizes the results.
For both groups, the first 7 test items were identi-
cal and contained only randomly selected distrac-
tors. Average error rate for these items was 0.24
(SD 0.12) for the first group, and 0.20 (SD 0.12)
for the second group, suggesting that the results of
the two groups on the remaining items can be com-
pared meaningfully. The first group was tested
on the remaining 13 items with randomly selected
distractors, while the second group got the same
items but with distractors created by our method.
Contrary to our hypothesis, the average error
rate for both groups was equal (0.18, SD
1
=0.17,
SD
2
=0.15). One reason might be that the English
language skills of the participants (mostly com-
puter science students or faculty) were rather high,
close to the native level, as shown by the low error
rates. Furthermore, even if the participants were
more challenged by our distractors, they might
have been able to finally select the right answer
with no measurable effect on error rate. Thus, in
future work we want measure answer time instead
of average error rate, in order to counter this effect.
We also want to re-run the experiment with lower
grade students, who might not have mastered the
kind of sense distinctions that our approach is fo-
cused on.
5 Conclusions
In this paper we have tackled the task of generating
challenging distractors for multiple-choice gap-fill
items. We propose to employ context-sensitive
lexical inference rules in order to generate distrac-
tors that are semantically similar to the gap target
word in some sense, but not in the particular sense
induced by the gap-fill context.
Our results suggest that our approach is quite ef-
fective, reducing the number of invalid distractors
in 90% of the cases, and fully eliminating all of
them in 65%. We did not find a difference in aver-
age error rate between distractors generated with
our method and randomly chosen distractors from
the same corpus frequency range. We conjecture
that this may be due to limitations in the setup of
our experiment.
Thus, in future work we want to re-run the ex-
periment with less experienced participants. We
also wish to measure answer time in addition to
error rate, as the distractive powers of a gap-filler
might be reflected in longer answer times more
than in higher error rates.
Acknowledgements
We thank all participants of the gap-fill survey,
and Emily Jamison and Tristan Miller for their
help with the annotation study. This work was
partially supported by the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement no. 287923 (EXCITE-
MENT).
147
References
Manish Agarwal and Prashanth Mannem. 2011. Au-
tomatic Gap-fill Question Generation from Text
Books. In Proceedings of the Sixth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 56?64.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing Distributional Similarity in Context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1162?1172.
Donna Gates, Margaret Mckeown, Juliet Bey, Forbes
Ave, and Ross Hall. 2011. How to Generate
Cloze Questions from Definitions : A Syntactic Ap-
proach. In Proceedings of the AAAI Fall Symposium
on Question Generation, pages 19?22.
Ayako Hoshino and Hiroshi Nakagawa. 2007. As-
sisting Cloze Test Making with a Web Application.
In Proceedings of the Society for Information Tech-
nology and Teacher Education International Confer-
ence, pages 2807? 2814.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distribu-
tional Similarity for Lexical Inference. Natural Lan-
guage Engineering, 16(4):359?389.
John Lee and Stephanie Seneff. 2007. Automatic
Generation of Cloze Items for Prepositions. In
Proceedings of INTERSPEECH, pages 2173?2176,
Antwerp, Belgium.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining 2001.
Chao-lin Liu, Chun-hung Wang, and Zhao-ming Gao.
2005. Using Lexical Constraints to Enhance the
Quality of Computer-Generated Multiple-Choice
Cloze Items. Computational Linguistics and Chi-
nese Language Processing, 10(3):303?328.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A Two Level
Model for Context Sensitive Inference Rules. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1331?1340, Sofia, Bulgaria.
Ruslan Mitkov, Le An Ha, Andrea Varga, and Luz
Rello. 2009. Semantic Similarity of Distractors in
Multiple-choice Tests: Extrinsic Evaluation. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics, pages 49?56.
Jack Mostow and Hyeju Jang. 2012. Generating
Diagnostic Multiple Choice Comprehension Cloze
Questions. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 136?146, Stroudsburg, PA, USA.
W. Nelson Francis and Henry Kuc?era. 1964. Manual
of Information to Accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers.
Juan Pino and Maxine Eskenazi. 2009. Semi-
Automatic Generation of Cloze Question Distrac-
tors Effect of Students L1. In SLaTE Workshop on
Speech and Language Technology in Education.
Juan Pino, Michael Heilman, and Maxine Eskenazi.
2008. A Selection Strategy to Improve Cloze Ques-
tion Quality. In Proceedings of the Workshop on In-
telligent Tutoring Systems for Ill-Defined Domains
at the 9th Internationnal Conference on Intelligent
Tutoring Systems.
Keisuke Sakaguchi, Yuki Arase, and Mamoru Ko-
machi. 2013. Discriminative Approach to Fill-in-
the-Blank Quiz Generation for Language Learners.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 238?242, Sofia, Bulgaria.
Simon Smith and P V S Avinesh. 2010. Gap-fill Tests
for Language Learners: Corpus-Driven Item Gener-
ation. In Proceedings of ICON-2010: 8th Interna-
tional Conference on Natural Language Processing.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring Non-native Speakers?
Proficiency of English by Using a Test with
Automatically-generated Fill-in-the-blank Ques-
tions. In Proceedings of the second workshop on
Building Educational Applications Using NLP,
EdAppsNLP 05, pages 61?68, Stroudsburg, PA,
USA.
148
