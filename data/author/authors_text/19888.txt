Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1950?1961, Dublin, Ireland, August 23-29 2014.
Why Gender and Age Prediction from Tweets is Hard:
Lessons from a Crowdsourcing Experiment
Dong Nguyen
14?
Dolf Trieschnigg
14
A. Seza Do
?
gru
?
oz
23
Rilana Gravel
4
Mari
?
et Theune
1
Theo Meder
4
Franciska de Jong
1
(1) Human Media Interaction, University of Twente, Enschede, The Netherlands
(2) Netherlands Institute for Advanced Studies, Wassenaar, NL
(3) Tilburg School of Humanities, Tilburg University, Tilburg, NL
(4) Meertens Institute, Amsterdam, The Netherlands
?
Corresponding author: d.nguyen@utwente.nl
Abstract
There is a growing interest in automatically predicting the gender and age of authors from texts.
However, most research so far ignores that language use is related to the social identity of speak-
ers, which may be different from their biological identity. In this paper, we combine insights
from sociolinguistics with data collected through an online game, to underline the importance
of approaching age and gender as social variables rather than static biological variables. In our
game, thousands of players guessed the gender and age of Twitter users based on tweets alone.
We show that more than 10% of the Twitter users do not employ language that the crowd as-
sociates with their biological sex. It is also shown that older Twitter users are often perceived
to be younger. Our findings highlight the limitations of current approaches to gender and age
prediction from texts.
1 Introduction
A major thrust of research in sociolinguistics aims to uncover the relationship between social variables
such as age and gender, and language use (Holmes and Meyerhoff, 2003; Eckert and McConnell-Ginet,
2013; Eckert, 1997; Wagner, 2012). In line with scholars from a variety of disciplines, including the so-
cial sciences and philosophy, sociolinguists consider age and gender as social and fluid variables (Eckert,
2012). Gender and age are shaped depending on the societal context, the culture of the speakers involved
in a conversation, the individual experiences and the multitude of social roles: a female teenager might
also be a high school student, a piano player, a swimmer, etc. (Eckert, 2008).
Speakers use language as a resource to construct their identity (Bucholtz and Hall, 2005). For example,
a person?s gender identity is constructed through language by using linguistic features associated with
male or female speech. These features gain social meaning in a cultural and societal context. On Twitter,
users construct their identity through interacting with other users (Marwick and boyd, 2011). Depending
on the context, they may emphasize specific aspects of their identity, which leads to linguistic variation
both within and between speakers. We illustrate this with the following three tweets:
Tweet 1: I?m walking on sunshine <3 #and don?t you feel good
Tweet 2: lalaloveya <3
Tweet 3: @USER loveyou ;D
In these tweets, we find linguistic markers usually associated with females (e.g. a heart represented
as <3). Indeed, 77% of the 181 players guessed that a female wrote these tweets in our online game.
However, this is a 16-year old biological male, whose Twitter account reveals that he mostly engages
with female friends. Therefore, he may have accommodated his style to them (Danescu-Niculescu-Mizil
et al., 2011) and as a result he employs linguistic markers associated with the opposite biological sex.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1950
Most of the NLP research focusing on predicting gender and age has approached these variables as bi-
ological and static, rather than social and fluid. For example, current approaches use supervised machine
learning models trained on tweets from males and females. However, the resulting stereotypical models
are ineffective for Twitter users who tweet differently from what is to be expected from their biological
sex.
As explained above, language use is based on social gender and age identity, and not on biological sex
and chronological age. In other words, treating gender and age as fixed biological variables in analyzing
language use is too simplistic. By comparing the biological sex and chronological age of Twitter users
with how they are perceived by the crowd (as an indication of socially constructed identities), we shed
light on the difficulty of predicting gender and age from language use and draw attention to the inherent
limitations of current approaches.
As has been demonstrated in several studies, the crowd can be used for experimentation (e.g., Munro
et al. (2010)). Our study illustrates the value of the crowd for the study of human behavior, in particular
for the experimental study of the social dimension of language use. To collect data, we created an online
game (an example of gamification (Deterding et al., 2011)) in which thousands of players (the crowd)
guessed the biological sex and age of Twitter users based on only the users? tweets. While variance
between annotators has traditionally been treated as noise, more recently variation is being treated as a
signal rather than noise (Aroyo and Welty, 2013). For example, Makatchev and Simmons (2011) analyze
how English utterances are perceived differently across language communities.
This paper follows this trend, treating variation as meaningful information. We assume that the crowd?s
perception (based on the distribution of the players? guesses) is an indication of to what extent Twitter
users emphasize their gender and age identity in their tweets. For example, when a large proportion of
the players guess the same gender for a particular user, the user is assumed to employ linguistic markers
that the crowd associates with gender-specific speech (e.g. iconic hearts used by females).
Our contributions are as follows:
? We demonstrate the use of gamification to study sociolinguistic research problems (Section 3).
? We study the difficulty of predicting an author?s gender (Section 4) and age (Section 5) from text
alone by analyzing prediction performance by the crowd. We relate our results to sociolinguistic
theories and show that approaching gender and age as fixed biological variables is too simplistic.
? Based on our findings, we reflect on current approaches to predicting age and gender from text, and
draw attention to the limitations of these approaches (Section 6).
2 Related Work
Gender Within sociolinguistics, studies on gender and language have a long history (Eckert and
McConnell-Ginet, 2013). More recently, the NLP community has become increasingly interested in
this topic. Most of the work aims at predicting the gender of authors based on their text, thereby focusing
more on prediction performance than sociolinguistic insights.
A variety of datasets have been used, including Twitter (Rao et al., 2010; Bamman et al., 2014; Fink et
al., 2012; Bergsma and Van Durme, 2013; Burger et al., 2011), blogs (Mukherjee and Liu, 2010; Schler et
al., 2005), telephone conversations (Garera and Yarowsky, 2009), YouTube (Filippova, 2012) and chats
in social networks (Peersman et al., 2011). Females tend to use more pronouns, emoticons, emotion
words, and blog words (lol, omg, etc.), while males tend to use more numbers, technology words, and
links (Rao et al., 2010; Bamman et al., 2014; Nguyen et al., 2013). These differences have also been
exploited to improve sentiment classification (Volkova et al., 2013) and cyberbullying detection (Dadvar
et al., 2012).
To the best of our knowledge, the study by Bamman et al. (2014) is the only computational study that
approaches gender as a social variable. By clustering Twitter users based on their tweets, they show that
multiple gendered styles exist. Unlike their study, we use the crowd and focus on implications for gender
and age prediction.
1951
Figure 1: Screenshot of the game. Text is translated into English (originally in Dutch). Left shows the
interface when the user needs to make a guess. Right shows the feedback interface.
Age Eckert (1997) makes a distinction between chronological (number of years since birth), biological
(physical maturity), and social age (based on life events). Most of the studies on language and age
focus on chronological age. However, speakers with the same chronological age can have very different
positions in society, resulting in variation in language use. Computational studies on language use and
age usually focus on automatic (chronological) age prediction. This has typically been modeled as
a classification problem, although this approach often suffers from ad hoc and dataset dependent age
boundaries (Rosenthal and McKeown, 2011). In contrast, recent works also explored predicting age as a
continuous variable and predicting lifestages (Nguyen et al., 2013; Nguyen et al., 2011) .
Similar to studies on gender prediction, a variety of resources have been used for age prediction, in-
cluding Twitter (Rao et al., 2010; Nguyen et al., 2013), blogs (Rosenthal and McKeown, 2011; Goswami
et al., 2009), chats in social networks (Peersman et al., 2011) and telephone conversations (Garera and
Yarowsky, 2009). Younger people use more alphabetical lengthening, more capitalization of words,
shorter words and sentences, more self-references, more slang words, and more Internet acronyms
(Rosenthal and McKeown, 2011; Nguyen et al., 2013; Rao et al., 2010; Goswami et al., 2009; Pen-
nebaker and Stone, 2003; Barbieri, 2008).
3 Data
To study how people perceive the gender and age identity of Twitter users based on their tweets, we
created an online game. Players were asked to guess the gender and age of Twitter users from tweets.
The game was part of a website (TweetGenie, www.tweetgenie.nl) that also hosted an automatic system
that predicts the gender and age of Twitter users based on their tweets (Nguyen et al., 2014). To attract
players, a link to the game was displayed on the page with the results of the automatic prediction, and
visitors were challenged to test if they were better than the automatic system (TweetGenie).
3.1 Twitter Data
We sampled Dutch Twitter users in the fall of 2012. We employed external annotators to annotate the
biological sex and chronological age (in years) using all information available through tweets, the Twitter
profile and external social media profiles such as Facebook and Linkedin. In total over 3000 Twitter users
were annotated. For more details regarding the collection of the dataset we refer to Nguyen et al. (2013).
We divided the data into train and test sets. 200 Twitter users were randomly selected from the test
set to be included in the online game (statistics are shown in Table 1). Named entities were manually
anonymized to conceal the user?s identity. Names in tweets were replaced by ?similar? names (e.g. a
first name common in a certain region in the Netherlands was replaced with another common name in
that region). This was done without knowing the actual gender and age of the Twitter users. Links were
replaced with a general [LINK] token and user mentions with @USER.
1952
Gender and age F, <20 M, <20 F, [20-40) M, [20-40) F, ?40 M, ?40
Frequency 61 60 24 23 17 15
Table 1: Statistics Twitter users in our game
3.2 Online Game
Game Setup The interface of the game is shown in Figure 1. Players guessed the biological sex (male
or female) and age (years) of a Twitter user based on only the tweets. For each user, {20, 25, 30, 35,
40} tweets were randomly selected. For a particular Twitter user, the same tweets were displayed to all
players. Twitter users were randomly selected to be displayed to the players.
To include an entertainment element, players received feedback after each guess. They were shown
the correct age and gender, the age and gender guessed by the computer, and the average guessed age
and gender distribution by the other players. In addition, a score was shown of the player versus the
computer.
Collection In May 2013, the game was launched. Media attention resulted in a large number of visitors
(Nguyen et al., 2014). We use the data collected from May 13, 2013 to August 21, 2013, resulting in
a total of 46,903 manual guesses. Players tweeted positively about the game, such as ?@USER Do you
know what is really addictive? ?Are you better than Tweetgenie? ...? and ?@USER Their game is quite
fun!? (tweets translated to English).
We filter sessions that do not seem to contain genuine guesses: when the entered age is 80 years
or above, or 8 or below. These thresholds were based on manual inspection, and chosen because it is
unlikely that the shown tweets are from users of such ages. For each guess, we registered a session ID
and an IP address. A new session started after 2 hours of inactivity. To study player performance more
robustly, we excluded multiple sessions of the same player. After three or more guesses had been made
in a session, all next sessions from the same IP address were discarded.
Statistics Statistics of the data are shown in Table 2. Figure 2 shows the distribution of the number of
guesses per session. The longest sessions consisted of 18 guesses. Some of our analyses require multiple
guesses per player. In that case, we only include players having made at least 7 guesses.
1 2 3 4 5 6 7 8 9 10 >10
Number of guesses
Freq
uenc
y
01
000
3000
5000
Figure 2: Number of guesses per session
# guesses 41,989
# sessions 15,724
Avg. time (sec) per guess 46
Avg. # guesses / session 2.67
Table 2: Statistics online game (after
cleaning)
We calculate the time taken for a guess by taking the time difference between two guesses (therefore,
no time for the first guess in each session could be measured). For each Twitter user, we calculate the
average time that was taken to guess the gender and age of the user. (Figure 3a). There is a significant
correlation (Pearson?s r = 0.291, p < 0.001) between the average time the players took to evaluate the
tweets of a Twitter user and the number of displayed tweets.
There is also a significant correlation between the average time taken for a user and the entropy over
gender guesses (Pearson?s r = 0.410, p < 0.001), and the average time taken for a user and the standard
deviation of the age guesses (Pearson?s r = 0.408, p < 0.001). Thus, on average, players spent more time
on Twitter users for whom it was more difficult to estimate gender and age.
1953
Avg time taken for Twitter user (sec)
Freq
uen
cy
0
20
40
60
80
30 35 40 45 50 55 60 65
(a) Average time taken for Twitter users
Turn
Av
era
ge 
tim
e ta
ken
30
32
34
36
2 3 4 5 6 7
(b) Average time taken per turn
Figure 3: Time taken in game
We observe that as the game progresses, players tend to take less time to make a guess. This is shown
in Figure 3b, which shows the average time taken for a turn (restricted to players with at least 7 guesses).
There was no significant correlation between time spent on a guess and the performance of players and
we did not find trends of performance increase or decrease as players progressed in the game.
3.3 Automatic Prediction
Besides studying human performance, we also compare the predictions of humans with those of an
automatic system. We split the data into train and test sets using the same splits as used by Nguyen et al.
(2013). We train a logistic regression model to predict gender (male or female), and a linear regression
model to predict the age (in years) of a person.
More specifically, given an input vector x ? R
m
, x
1
, . . . , x
m
represent features. In the case of gender
classification (e.g. y ? {?1, 1}), the model estimates a conditional distribution P (y|x, ?) = 1/(1 +
exp(?y(?
0
+ x
>
?))), where ?
0
and ? are the parameters to estimate. Age is treated as a regression
problem, and we find a prediction y? ? R for the exact age of a person y ? R using a linear regression
model: y? = ?
0
+ x
>
?. We use Ridge (also called L
2
) regularization to prevent overfitting.
We make use of the liblinear (Fan et al., 2008) and scikit-learn (Pedregosa et al., 2011) libraries.
We only use unigram features, since they have proven to be very effective for gender (Bamman et al.,
2014; Peersman et al., 2011) and age (Nguyen et al., 2013) prediction. Parameters were tuned using
cross-validation on the training set.
4 Gender
Most of the computational work on language and gender focuses on gender classification, treating gender
as fixed and classifying speakers into females and males. However, this assumes that gender is fixed and
is something people have, instead of something people do (Butler, 1990).
In this section, we first analyze the task difficulty by studying crowd performance on inferring gender
from tweets. We observe a relatively large group of Twitter users who employ language that the crowd
associates with the opposite biological sex. This, then, raises questions about the upper bound that a
prediction system based on only text can achieve.
Next, we place Twitter users on a gender continuum based on the guesses of the players and show that
treating gender as a binary variable is too simplistic. While historically gender has been treated as binary,
researchers in fields such as sociology (Lorber, 1996) and sociolinguistics (Holmes and Meyerhoff, 2003;
Bergvall et al., 1996) find this view too limited. Instead, we assume the simplest extension beyond a
binary variable: a one-dimensional gender continuum (or scale) (Bergvall et al., 1996). For example,
Bergvall (1999) talks about a ?continuum of humans? gendered practices?. While these previous studies
were based on qualitative analyses, we take a quantitative approach using the crowd.
1954
4.1 Task Difficulty
Majority vote We study crowd performance using a system based on the majority of the players?
guesses. Majority voting has proven to be a strong baseline to aggregate votes (e.g. in crowdsourcing
systems (Snow et al., 2008; Le et al., 2010)). On average, we have 210 guesses per Twitter user, providing
substantial evidence per Twitter user. A system based on majority votes achieves an accuracy of 84%
(Table 3a shows a confusion matrix). Table 3b shows a confusion matrix of the majority predictions
versus the automatic system. We find that the biological sex was predicted incorrectly by both the
majority vote system and the automatic system for 21 out of the 200 Twitter users (10.5%, not in Table).
Automatic classification systems on English tweets achieve similar performances as our majority vote
system (e.g. Bergsma and Van Durme (2013) report an accuracy of 87%, Bamman et al. (2014) 88%).
More significantly, the results suggest that 10.5% (automatic + majority) to 16% (majority) of the Dutch
Twitter users do not employ language that the crowd associates with their biological sex. As said, this
raises the question of whether we can expect much higher performances by computational systems based
on only language use.
Biological sex
Male Female
Crowd
Male 82 16
Female 16 86
(a) Crowd (majority)
Crowd
Male Female
Automatic
Male 68 22
Female 30 80
(b) Automatic vs crowd
Table 3: Confusion matrices crowd prediction
Individual players versus an automatic system When considering players with 7 or more guesses,
the average accuracy for a player is 0.71. Our automatic system achieves an accuracy of 0.69. The small
number of tweets per Twitter user in our data (20-40) makes it more difficult to automatically predict
gender.
Entropy We characterize the difficulty of inferring a user?s gender by calculating the entropy for each
Twitter user based on the gender guesses (Figure 4a). We find that the difficulty varies widely across
users, and that there are no distinct groups of ?easy? and ?difficult? users. However, we do observe an
interaction effect between the entropy of the gender guesses and the ages of the Twitter users. At an
aggregate level, we find no significant trend. Analyzing females and males separately, we observe a
significant trend with females (Pearson?s r = 0.270, p < 0.01), suggesting that older female Twitter users
tend to emphasize other aspects than their gender in tweets (as perceived by the crowd).
Persons
Ent
rop
y
0.2
0.6
1.0
0 50 100 150 200
(a) Entropy over gender guesses
0
5
10
15
20
25
0.0 0.5 1.0
Proportion of people that guessed male
Fre
que
ncy Biological 
sex
Male
Female
(b) A histogram of all Twitter users and the proportion of
players who guessed the users were male. For example,
there are 25 female users for which 10 - 20% of the players
guessed they were male.
Figure 4: Gender prediction
1955
4.2 Binarizing Gender, a Good Approach?
Using data collected through the online game we quantitatively put speakers on a gender continuum
based on how their tweets are perceived by the crowd. For each Twitter user, we calculate the proportion
of players who guessed the users were male and female. A plot is displayed in Figure 4b. We can make
the following observations:
First, the guesses by the players are based on their expectations about what kind of behaviour and
language is used by males and females. The plot shows that for some users, almost all players guessed
the same gender, indicating that these expectations are quite strong and that there are stylistic markers
and topics that the crowd strongly associates with males or females.
Second, if treating gender as a binary variable is reasonable, we would expect to see two distinct
groups. However, we observe quite an overlap between the biological males and females. There are 1)
users who conform to what is expected based on their biological sex, 2) users who deviate from what is
expected, 3) users whose tweets do not emphasize a gender identity or whose tweets have large variation
using language associated with both genders. We investigated whether this is related to their use of
Twitter (professional, personal, or both), but the number of Twitter users in our dataset who used Twitter
professionally was small and not sufficient to draw conclusions.
We now illustrate our findings using examples. The first example is a 15-year old biological female
for who the crowd guessed most strongly that she is female (96% of n=220). Three tweets from her are
shown below. She uses language typically associated with females, talking about spending time with
her girlfriends and the use of stylistic markers such as hearts and alphabetical lengthening. Thus, she
conforms strongly to what the crowd expects from her biological sex.
Tweet 4: Gezellig bij Emily en Charlotte.
Translation: Having fun with Emily and Charlotte.
Tweet 5: Hiiiiii schatjesss!
Translation: Hiiiiii cutiesss!
Tweet 6: ? @USER
Below are two tweets from a 40 year old biological female who does not employ linguistic markers
strongly associated with males or females. Therefore, only 46% of the crowd (n=200) was able to guess
that she is female.
Tweet 7: Ik viel op mijn bek. En het kabinet ook. Geinig toch? #Catshuis
Translation: I went flat on my face. And the cabinet as well. Funny right? #Catshuis
Tweet 8: Jeemig. Ik kan het bijna niet volgen allemaal.
Translation: Jeez. I almost can?t follow it all.
Twitter users vary in how much they emphasize their gender in their tweets. As a result, the difficulty
of inferring gender from tweets varies across persons, and treating gender as a binary variable ignores
much of the interesting variation within and between persons.
Automatic system We now analyze whether an automatic system is capable of capturing the position
of Twitter users on the gender continuum (as perceived by the crowd). We calculate the correlation
between the proportion of male guesses (i.e. the position on the gender continuum) and the scores of
the logistic regression classifier: ?
0
+ x
>
?. While the training data was binary (users were labeled as
male or female), a reasonable Spearman correlation of ? = 0.584 (p < 0.001) was obtained between the
classifier score and the score based on the crowd?s perception. We did not observe a significant relation
between the score of the classifier (corresponding to the confidence of the gender prediction) and age.
1956
5 Age
We start with an analysis of task difficulty, by studying crowd performance on inferring age from tweets.
Next, we show that it is particularly hard to accurately infer the chronological age of older Twitter users
from tweets.
5.1 Task Difficulty
The crowd?s average guesses As with a system based on majority vote for gender prediction, we test
the performance of a system that predicts the ages of Twitter users based on the average of all guesses.
We find that such a system achieves a Mean Absolute Error (MAE) of 4.844 years and a Pearson?s
correlation of 0.866. Although the correlation is high, the absolute errors are quite large. We find that the
crowd has difficulty predicting the ages of older Twitter users. There is a positive correlation (Pearson?s
? = 0.789) between the absolute errors and the actual age of Twitter users. There is a negative correlation
between the errors (predicted - actual age) and the actual age of Twitter users (Pearson?s ? = -0.872).
We calculate the standard deviation over all the age guesses for a user (Figure 5a) to measure the
difficulty of inferring a user?s age. There is a positive correlation between age and standard deviation of
the guesses (? = 0.691), which indicates that players have more difficulty in guessing the ages of older
Twitter users.
Individual players versus an Automatic System To estimate the performance of individual players,
we restrict our attention to players with at least 7 guesses. We find that individual players are, on average,
5.754 years off. A linear regression system achieves a MAE of 6.149 years and a Pearson correlation of
0.812. The small number of tweets in our data (20-40) increases the difficulty of the task for automatic
systems.
Age
Sta
nda
rd d
evia
tion
2
4
6
8
10
10 20 30 40 50 60
(a) Standard deviation and actual age
Actual age
Pre
dict
ed a
ge
10
20
30
40
50
60
10 20 30 40 50 60
Correct line
Human prediction
(b) Average age prediction by humans.
Figure 5: Age prediction
5.2 Inferring the Age of Older Twitter Users
Figure 5b shows the average player predictions with the actual age of the Twitter users. The red line is
the ?perfect? line, i.e. the line when the predictions would match the exact age. Black represents a fitted
LOESS curve (Cleveland et al., 1992) based on the human predictions. We find that the players tend
to overpredict the age of younger Twitter users, but even more strikingly, on average they consistently
underpredict the age of older Twitter users. The prediction errors already start at the end of the 20s, and
the gap between actual and predicted age increases with age.
This could be explained by sociolinguistic studies that have found that people between 30 and 55 years
use standard forms the most, because they experience the maximum societal pressure in the workplace to
conform (Holmes, 2013). On Twitter, this has been observed as well: Nguyen et al. (2013) found fewer
linguistic differences between older age groups than between younger age groups. This makes it difficult
for the crowd to accurately estimate the ages of older Twitter users. Younger people and retired people
use more non-standard forms (Holmes, 2013). Unfortunately, our dataset does not contain enough retired
users to analyze whether this trend is also present on Twitter.
1957
6 Discussion
We now discuss the implications of our findings for research on automatically predicting the gender and
age of authors from their texts.
Age and gender as social variables Most computational research has treated gender and age as fixed,
biological variables. The dominant approach is to use supervised machine learning methods to generalize
across a large number of examples (e.g. texts written by females and males). While the learned models
so far are effective at predicting age and gender of most people, they learn stereotypical behaviour and
therefore provide a simplistic view.
First, by using the crowd we have shown that Twitter users emphasize their gender and age in varying
degrees and in different ways, so that for example, treating gender as a binary variable is too simplistic
(Butler, 1990; Eckert and McConnell-Ginet, 2013). Many users do not employ the stereotypical language
associated with their biological sex, making models that take a static view of gender ineffective for such
users. More detailed error analyses of the prediction systems will increase understanding of the reasons
for incorrect predictions, and shed light on the relation between language use and social variables.
Second, models that assume static variables will not be able to model the interesting variation (Eisen-
stein, 2013). Models that build on recent developments in sociolinguistics will be more meaningful and
will also have the potential to contribute to new sociolinguistic insights. For example, modeling what
influences speakers to show more or less of their identity through language, or jointly modeling varia-
tion between and within speakers, are in our opinion interesting research directions. The ever increasing
amounts of social media data offer opportunities to explore these research directions.
Sampling We have shown that the difficulty of tasks such as gender and age prediction varies across
persons. Therefore, creating datasets for such tasks requires maximum attention. For example, when
a dataset is biased towards people who show a strong gender identity (e.g. by sampling followers of
accounts highly associated with males or females, such as sororities (Rao et al., 2010)), the results
obtained on such a set may not be representative of a more random set (as observed when classifying
political affiliation (Cohen and Ruths, 2013)).
Task difficulty Our study also raises the question of what level of performance can be obtained for
tasks such as predicting gender and age from only language use. Since we often form an impression
based on someone?s writing, crowd performance is a good indicator of the task difficulty. While the
crowd performance does not need to be the upper bound, it does indicate that it is difficult to predict
gender and age of a large number of Twitter users.
When taking the majority label, only 84% of the users were correctly classified according to their
biological sex. This suggests that about 16% of the Dutch Twitter users do not use language that the
crowd associates with their biological sex.
We also found that it is hard to accurately estimate the ages of older Twitter users, and we related
this to sociolinguistics studies who found less linguistic differences in older age groups due to societal
pressure in the workplace.
Limitations A limitation of our work is that we focused on language variation between persons, and not
on variation within persons. However, speakers vary their language depending on the context and their
conversation partners (e.g. accommodation effects were found in social media (Danescu-Niculescu-Mizil
et al., 2011)). For example, we assigned Twitter users an overall ?score? by placing them on a gender
continuum, ignoring the variation we find within users.
Crowdsourcing as a tool to understand NLP tasks Most research on crowdsourcing within the NLP
community has focused on how the crowd can be used to obtain fast and large amounts of annotations.
This study is an example of how the crowd can be used to obtain a deeper understanding of an NLP task.
We expect that other tasks where disagreement between annotators is meaningful (i.e. it is not only due
to noise), could potentially benefit from crowdsourcing experiments as well.
1958
7 Conclusion
In this paper, we demonstrated the successful use of the crowd to study the relation between language
use and social variables. In particular, we took a closer look at inferring gender and age from language
using data collected through an online game. We showed that treating gender and age as fixed variables
ignores the variety of ways people construct their identity through language.
Approaching age and gender as social variables will allow for richer analyses and more robust systems.
It has implications ranging from how datasets are created to how results are interpreted. We expect that
our findings also apply to other social variables, such as ethnicity and status. Instead of only focusing on
performance improvement, we encourage NLP researchers to also focus on what we can learn about the
relation between language use and social variables using computational methods.
Acknowledgements
This research was supported by the Royal Netherlands Academy of Arts and Sciences (KNAW)
and the Netherlands Organization for Scientific Research (NWO), grants IB/MP/2955 (TINPOT) and
640.005.002 (FACT). The third author is supported through the Digital Humanities research grant by
Tilburg University and a NIAS research fellowship. The authors would like to thank the players of the
TweetGenie game.
References
Lora Aroyo and Chris Welty. 2013. Crowd truth: Harnessing disagreement in crowdsourcing a relation extraction
gold standard. In Proceedings of WebSci?13.
David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2014. Gender identity and lexical variation in social
media. Journal of Sociolinguistics, 18(2):135?160.
Federica Barbieri. 2008. Patterns of age-based linguistic variation in American English. Journal of Sociolinguis-
tics, 12(1):58?88.
Shane Bergsma and Benjamin Van Durme. 2013. Using conceptual class attributes to characterize social media
users. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages
710?720.
Victoria L. Bergvall, Janet M. Bing, and Alice F. Freed. 1996. Rethinking Language and Gender Research: Theory
and Practice. Routledge.
Victoria L. Bergvall. 1999. Toward a comprehensive theory of language and gender. Language in society,
28(02):273?293.
Mary Bucholtz and Kira Hall. 2005. Identity and interaction: A sociocultural linguistic approach. Discourse
studies, 7(4-5):585?614.
John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on Twitter. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301?1309.
Judith Butler. 1990. Gender Trouble: Feminism and the Subversion of Identity. Routledge.
William S. Cleveland, Eric Grosse, and William M. Shyu. 1992. Local regression models. Statistical models in S,
pages 309?376.
Raviv Cohen and Derek Ruths. 2013. Classifying political orientation on Twitter: It?s not easy! In Proceedings of
the Seventh International AAAI Conference on Weblogs and Social Media, pages 91?99.
Maral Dadvar, Franciska de Jong, Roeland Ordelman, and Dolf Trieschnigg. 2012. Improved cyberbullying de-
tection using gender information. In Proceedings of the Twelfth Dutch-Belgian Information Retrieval Workshop
(DIR 2012), pages 23?25.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words!: linguistic style
accommodation in social media. In Proceedings of the 20th international conference on World Wide Web, pages
745?754.
1959
Sebastian Deterding, Dan Dixon, Rilla Khaled, and Lennart Nacke. 2011. From game design elements to game-
fulness: Defining ?gamification?. In Proceedings of the 15th International Academic MindTrek Conference:
Envisioning Future Media Environments, pages 9?15.
Penelope Eckert and Sally McConnell-Ginet. 2013. Language and gender. Cambridge University Press.
Penelope Eckert. 1997. Age as a sociolinguistic variable. The handbook of sociolinguistics, pages 151?167.
Penelope Eckert. 2008. Variation and the indexical field. Journal of Sociolinguistics, 12(4):453?476.
Penelope Eckert. 2012. Three waves of variation study: the emergence of meaning in the study of sociolinguistic
variation. Annual Review of Anthropology, 41:87?100.
Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of the Annual Conference
of the North American Chapter of the Association for Computational Linguistics, pages 359?369.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning Research, 9:1871?1874.
Katja Filippova. 2012. User demographics and language in an implicit social network. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language
Learning, pages 1478?1488.
Clayton Fink, Jonathon Kopecky, and Maksym Morawski. 2012. Inferring gender from the content of tweets:
A region specific example. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social
Media.
Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP, pages 710?718.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi. 2009. Stylometric analysis of bloggers? age and gender.
In Proceedings of the Third International ICWSM Conference, pages 214?217.
Janet Holmes and Miriam Meyerhoff. 2003. The handbook of language and gender. Wiley-Blackwell.
Janet Holmes. 2013. An introduction to sociolinguistics. Routledge.
John Le, Andy Edmonds, Vaughn Hester, and Lukas Biewald. 2010. Ensuring quality in crowdsourced search
relevance evaluation: The effects of training question distribution. In Proceedings of the SIGIR 2010 Workshop
on Crowdsourcing for Search Evaluation (CSE 2010), pages 21?26.
Judith Lorber. 1996. Beyond the binaries: Depolarizing the categories of sex, sexuality, and gender*. Sociological
Inquiry, 66(2):143?160.
Maxim Makatchev and Reid Simmons. 2011. Perception of personality and naturalness through dialogues by
native speakers of American English and Arabic. In Proceedings of the SIGDIAL 2011: the 12th Annual
Meeting of the Special Interest Group on Discourse and Dialogue, pages 286?293.
Alice E. Marwick and danah boyd. 2011. I tweet honestly, I tweet passionately: Twitter users, context collapse,
and the imagined audience. New Media & Society, 13(1):114?133.
Arjun Mukherjee and Bing Liu. 2010. Improving gender classification of blog authors. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing, pages 207?217.
Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler
Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic
data. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 122?130.
Dong Nguyen, Noah A Smith, and Carolyn P. Ros?e. 2011. Author age prediction from text using linear regression.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences,
and Humanities, pages 115?123.
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and Theo Meder. 2013. ?How old do you think I am??: A study
of language and age in Twitter. In Proceedings of the Seventh International AAAI Conference on Weblogs and
Social Media, pages 439?448.
1960
Dong Nguyen, Dolf Trieschnigg, and Theo Meder. 2014. Tweetgenie: Development, evaluation, and lessons
learned. In Proceedings of COLING 2014.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, Matthieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
Claudia Peersman, Walter Daelemans, and Leona Van Vaerenbergh. 2011. Predicting age and gender in online so-
cial networks. In Proceedings of the 3rd international workshop on Search and mining user-generated contents,
pages 37?44.
James W. Pennebaker and Lori D. Stone. 2003. Words of wisdom: Language use over the life span. Journal of
personality and social psychology, 85(2):291?301.
Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes
in Twitter. In Proceedings of the 2nd international workshop on Search and mining user-generated contents,
pages 37?44.
Sara Rosenthal and Kathleen McKeown. 2011. Age prediction in blogs: a study of style, content, and online be-
havior in pre- and post-social media generations. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 763?772.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker. 2005. Effects of age and gender
on blogging. In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs,
pages 199?205.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast?but is it good?:
Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 254?263.
Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring demographic language variations to
improve multilingual sentiment analysis in social media. In Proceedings of the 2013 Conference on Empirical
Methods on Natural Language Processing, pages 1815?1827.
Suzanne E. Wagner. 2012. Age grading in sociolinguistic theory. Language and Linguistics Compass, 6(6):371?
382.
1961
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 107?115,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Modeling the Use of Graffiti Style Features to Signal Social Relations 
within a Multi-Domain Learning Paradigm 
Mario Piergallini1, A. Seza Do?ru?z2, Phani Gadde1, David Adamson1, Carolyn P. Ros?1,3  
1Language Technologies 
Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
{mpiergal,pgadde, 
dadamson}@cs.cmu.edu 
2Tilburg University, TSH, 
5000 LE Tilburg, The 
Netherlands/ 
Language Technologies 
Institute, Carnegie Mellon 
University, 5000 Forbes 
Ave.,Pittsburgh PA 15213 
a.s.dogruoz@gmail.com 
3Human-Computer 
Interaction Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
cprose@cs.cmu.edu 
 
Abstract 
In this paper, we present a series of 
experiments in which we analyze the usage of 
graffiti style features for signaling personal 
gang identification in a large, online street 
gangs forum, with an accuracy as high as 83% 
at the gang alliance level and 72% for the 
specific gang.  We then build on that result in 
predicting how members of different gangs 
signal the relationship between their gangs 
within threads where they are interacting with 
one another, with a predictive accuracy as high 
as 66% at this thread composition prediction 
task.  Our work demonstrates how graffiti 
style features signal social identity both in 
terms of personal group affiliation and 
between group alliances and oppositions.  
When we predict thread composition by 
modeling identity and relationship 
simultaneously using a multi-domain learning 
framework paired with a rich feature 
representation, we achieve significantly higher 
predictive accuracy than state-of-the-art 
baselines using one or the other in isolation. 
1 Introduction 
Analysis of linguistic style in social media has 
grown in popularity over the past decade.  
Popular prediction problems within this space 
include gender classification (Argamon et al., 
2003), age classification (Argamon et al., 2007), 
political affiliation classification (Jiang & 
Argamon, 2008), and sentiment analysis (Wiebe 
et al., 2004).  From a sociolinguistic perspective, 
this work can be thought of as fitting within the 
area of machine learning approaches to the 
analysis of style (Biber & Conrad, 2009), 
perhaps as a counterpart to work by variationist 
sociolinguists in their effort to map out the space 
of language variation and its accompanying 
social interpretation (Labov, 2010; Eckert & 
Rickford, 2001).  One aspiration of work in 
social media analysis is to contribute to this 
literature, but that requires that our models are 
interpretable.  The contribution of this paper is an 
investigation into the ways in which stylistic 
features behave in the language of participants of 
a large online community for street gang 
members.  We present a series of experiments 
that reveal new challenges in modeling stylistic 
variation with machine learning approaches.  As 
we will argue, the challenge is achieving high 
predictive accuracy without sacrificing 
interpretability. 
 Gang language is a type of sociolect that has 
so far not been the focus of modeling in the area 
of social media analysis.  Nevertheless, we argue 
that the gangs forum we have selected as our 
data source provides a strategic source of data for 
exploring how social context influences stylistic 
language choices, in part because it is an area 
where the dual goals of predictive accuracy and 
interpretability are equally important. In 
particular, evidence that gang related crime may 
account for up to 80% of crime in the United 
States attests to the importance of understanding 
the social practices of this important segment of 
society (Johnsons, 2009).  Expert testimony 
attributing meaning to observed, allegedly gang-
related social practices is frequently used as 
evidence of malice in criminal investigations 
(Greenlee, 2010).  Frequently, it is police officers 
who are given the authority to serve as expert 
witnesses on this interpretation because of their 
routine interaction with gang members.  
107
Nevertheless, one must consider their lack of 
formal training in forensic linguistics (Coulthard 
& Johnson, 2007) and the extent to which the 
nature of their interaction with gang members 
may subject them to a variety of cognitive biases 
that may threaten the validity of their 
interpretation (Kahneman, 2011).   
 Gang-related social identities are known to be 
displayed through clothing, tattoos, and language 
practices including speech, writing, and gesture 
(Valentine, 1995), and even dance (Philips, 
2009).  Forensic linguists have claimed that these 
observed social practices have been over-
interpreted and inaccurately interpreted where 
they have been used as evidence in criminal trials 
and that they may have even resulted in 
sentences that are not justified by sufficient 
evidence (Greenlee, 2010).  Sociolinguistic 
analysis of language varieties associated with 
gangs and other counter-cultural groups attests to 
the challenges in reliable interpretation of such 
practices (Bullock, 1996; Lefkowitz, 1989).  If 
we as a community can understand better how 
stylistic features behave due to the choices 
speakers make in social contexts, we will be in a 
better position to achieve high predictive 
accuracy with models that are nevertheless 
interpretable.  And ultimately, our models may 
offer insights into usage patterns of these social 
practices that may then offer a more solid 
empirical foundation for interpretation and use of 
language as evidence in criminal trials. 
 In the remainder of the paper we describe our 
annotated corpus.  We then motivate the 
technical approach we have taken to modeling 
linguistic practices within the gangs forum.  
Next, we present a series of experiments 
evaluating our approach and conclude with a 
discussion of remaining challenges. 
2 The Gangs Forum Corpus 
The forum that provides data for our experiments 
is an online forum for members of street gangs. 
The site was founded in November, 2006. It was 
originally intended to be an educational resource 
compiling knowledge about the various gang 
organizations and the street gang lifestyle. Over 
time, it became a social outlet for gang members. 
There are still traces of this earlier focus in that 
there are links at the top of each page to websites 
dedicated to information about particular gangs. 
At the time of scraping its contents, it had over a 
million posts and over twelve thousand active 
users.   Our work focuses on analysis of stylistic 
choices that are influenced by social context, so 
it is important to consider some details about the 
social context of this forum.  Specifically, we 
discuss which gangs are present in the data and 
how the gangs are organized into alliances and 
rivalries.  Users are annotated with their gang 
identity at two levels of granularity, and threads 
are annotated with labels that indicate which 
gang dominates and how the participating gangs 
relate to one another.   
2.1 User-Level Annotations 
At the fine-grained level, we annotated users 
with the gang that they indicated being affiliated 
with,  including Bloods, Crips, Hoovers, 
Gangster Disciples, other Folk Nation, Latin 
Kings, Vice Lords, Black P. Stones, other People 
Nation, Trinitarios, Norte?os, and Sure?os.  
There was also an Other category for the smaller 
gangs.  For a coarser grained annotation of gang 
affiliation, we also noted the nation, otherwise 
known as gang alliance, each gang was 
associated with.   
For our experiments, a sociolinguist with 
significant domain expertise annotated the gang 
identity of 3384 users.  Information used in our 
annotation included the user?s screen name, their 
profile, which included a slot for gang affiliation, 
and the content of their posts.  We used regular 
expressions to find gang names or other 
identifiers occurring within the gang affiliation 
field and the screen names and annotated the 
users that matched.  If the value extracted for the 
two fields conflicted, we marked them as 
claiming multiple gangs.  For users whose 
affiliation could not be identified automatically, 
we manually checked their profile to see if their 
avatar (an image that accompanies their posts) or 
other fields there contained any explicit 
information.  Otherwise, we skimmed their posts 
for explicit statements of gang affiliation.   
Affiliation was unambiguously identified 
automatically for 56% of the 3384 users from 
their affiliation field.  Another 36% were 
identified automatically based on their screen 
name.  Manual inspection was only necessary in 
9% of the cases.  Users that remained ambiguous, 
were clearly fake or joke accounts, or who 
claimed multiple gangs were grouped together in 
an ?Other? category, which accounts for 6.2% of 
the total.  Thus, 94% of the users were classified 
into the 12 specific gangs mentioned above. 
108
At a coarse-grained level, users were also 
associated with a nation.  The nation category 
was inspired by the well-known gang alliances 
known as the People Nation and Folks Nation, 
which are city-wide alliances of gangs in 
Chicago. We labeled the Crips and Hoovers as a 
nation since they are closely allied gangs.  
Historically, the Hoovers began breaking away 
from the Crips and are rivals with certain subsets 
of Crips, but allies with the majority of other 
Crips gangs.  The complex inner structure of the 
Crips alliance will be discussed in Section 5 
where we interpret our quantitative results. 
There are a large number of gangs that 
comprise the People and Folks Nations. The 
major gangs within the People Nation are the 
Latin Kings, Vice Lords and Black P. Stones. 
The Folks Nation is dominated by the Gangster 
Disciples with other Folks Nation gangs being 
significantly smaller. The People Nation, Blood 
and Norte?os gangs are in a loose, national 
alliance against the opposing national alliance of 
the Folks Nation, Crips and Sure?os. Remaining 
gangs were annotated as other, such as the 
Trinitarios, that don't fit into this national 
alliance system nor even smaller alliances.   
2.2 Thread-Level Annotations 
In addition to person-level annotations of gang 
and nation, we also annotated 949 threads with 
dominant gang as well as thread composition, by 
which we mean whether the users who 
participated on the thread were only from allied 
gangs, included opposing gangs, or contained a 
mix of gangs that were neither opposing nor 
allied.  These 949 threads were ones where a 
majority of the users who posted were in the set 
of 3384 users annotated with a gang identity. 
For the dominant gang annotation at the 
gang level, we consider only participants on the 
thread for whom there was an annotated gang 
affiliation. If members of a single gang produced 
the majority of the posts in the thread, then that 
was annotated as the dominant gang of the thread. 
If no gang had a majority in the thread, it was 
instead labeled as Mixed. For dominant gang at 
the nation level, the same procedure was used, 
but instead of looking for which gang accounted 
for more of the members, we looked for which 
gang alliance accounted for the majority of users. 
For the thread composition annotation, we 
treated the Bloods, People Nation, and Norte?os 
as allied with each other as the ?Red set?.  We 
treated Crips, Hoovers, Folks Nation, and 
Sure?os as allies with each other as the ?Blue 
set?.  The Red and Blue sets oppose one another.  
The Latin Kings and Trinitarios also oppose one 
another.  Thread composition was labeled as 
Allied, Mixed or Opposing depending on the 
gangs that appeared in the thread. As with the 
dominant gang annotation, only annotated users 
were considered. If all of the posts were by users 
of the same gang or allied gangs, the thread was 
labeled as Allied.  If there were any posts from 
rival gangs, it was labeled as Opposing. 
Otherwise, it was labeled as Mixed. If the users 
were all labeled with Other as their gang it was 
also labeled as Mixed.  
3 Modeling Language Practices at the 
Feature Level 
In this section, we first describe the rich feature 
representation we developed for this work.  
Finally, we discuss the motivation for employing 
a multi-domain learning framework in our 
machine-learning experiments. 
3.1 Feature Space Design: Graffiti Style 
Features 
While computational work modeling gang-
related language practices is scant, we can learn 
lessons from computational work on other types 
of sociolects that may motivate a reasonable 
approach.  Gender prediction, for example, is a 
problem where there have been numerous 
publications in the past decade (Corney et al., 
2002; Argamon et al., 2003; Schler et al., 2005; 
Schler, 2006; Yan & Yan, 2006; Zhang et al., 
2009).  Because of the complex and subtle way 
gender influences language choices, it is a 
strategic example to motivate our work. 
 Gender-based language variation arises from 
multiple sources. Among these, it has been noted 
that within a single corpus comprised of samples 
of male and female language that the two 
genders do not speak or write about the same 
topics. This is problematic because word-based 
features such as unigrams and bigrams, which 
are very frequently used, are highly likely to pick 
up on differences in topic (Schler, 2006) and 
possibly perspective. Thus, in cases where 
linguistic style variation is specifically of 
interest, these features do not offer good 
generalizability (Gianfortoni et al., 2011). 
Similarly, in our work, members of different 
109
gangs are located in different areas associated 
with different concerns and levels of 
socioeconomic status.  Thus, in working to 
model the stylistic choices of gang forum 
members, it is important to consider how to 
avoid overfitting to content-level distinctions. 
 Typical kinds of features that have been used 
in gender prediction apart from unigram features 
include part-of-speech (POS) ngrams (Argamon 
et al., 2003), word-structure features that cluster 
words according to endings that indicate part of 
speech (Zhang et al., 2009), features that indicate 
the distribution of word lengths within a corpus 
(Corney et al., 2002), usage of punctuation, and 
features related to usage of jargon (Schler et al., 
2005). In Internet-based communication, 
additional features have been investigated such 
as usage of internet specific features including 
?internet speak? (e.g., lol, wtf, etc.), emoticons, 
and URLs (Yan & Yan, 2006).   
Transformation Origin or meaning 
b^, c^, h^, p^ ?Bloods up? Positive towards 
Bloods, Crips, Hoovers, 
Pirus, respectively 
b ? bk, c ? ck 
h ? hk, p ? pk 
Blood killer, Crip killer 
Hoover killer, Piru killer 
ck ? cc, kc Avoid use of ?ck? since it 
represents Crip killer 
o ? x, o ? ? Represents crosshairs, 
crossing out the ?0?s in a 
name like Rollin? 60s Crips 
b ? 6 Represents the six-pointed 
star. Symbol of Folk Nation 
and the affiliated Crips. 
e ? 3 Various. One is the trinity in 
Trinitario. 
s ? 5 Represents the five-pointed 
star. Symbol of People 
Nation and the affiliated 
Bloods. 
Table 1: Orthographical substitutions from gang 
graffiti symbolism 
 
 In order to place ourselves in the best position 
to build an interpretable model, our space of 
graffiti style features was designed based on a 
combination of qualitative observations of the 
gangs forum data and reading about gang 
communication using web accessible resources 
such as informational web pages linked to the 
forum and other resources related to gang 
communication (Adams & Winter, 1997; Garot, 
2007).  Specifically, in our corpus we observed 
gang members using what we refer to as graffiti 
style features to mark their identity.  Gang 
graffiti employs shorthand references to convey 
affiliation or threats (Adams & Winter, 
1997).  For example, the addition of a <k> after a 
letter representing a rival gang stands for ?killer.? 
So, writing <ck> would represent ?crip killer.? A 
summary of these substitutions can be seen in 
Table 1.  Unfortunately, only about 25% of the 
users among the 12,000 active users employ 
these features in their posts, which limits their 
ability to achieve a high accuracy, but 
nevertheless offers the opportunity to model a 
frequent social practice observed in the corpus.  
 The graffiti style features were extracted 
using a rule-based algorithm that compares 
words against a standard dictionary as well as 
using some phonotactic constraints on the 
position of certain letters.  The dictionary was 
constructed using all of the unique words found 
in the AQUAINT corpus (Graff, 2002).  If a 
word in a post did not match any word from the 
AQUAINT corpus, we tested it against each of 
the possible transformations in Table 1.  
Transformations were applied to words using 
finite state transducers.  If some combination 
transformations from that table applied to the 
observed word could produce some term from 
the AQUAINT corpus, then we counted that 
observed word as containing the features 
associated with the applied transformations. 
 The transformations were applied in the order 
of least likely to occur in normal text to the most 
likely. Since ?bk? only occurs in a handful of 
obscure words, for example, almost any 
occurrence of it can be assumed to be a 
substitution and the ?k? can safely be removed 
before the next step. By contrast, ?cc? and ?ck? 
occur in many common words so they must be 
saved for last to ensure that the final dictionary 
checks have any simultaneous substitutions 
already removed. 
 When computing values for the graffiti style 
features for a text, the value for each feature was 
computed as the number of words (tokens) that 
contained the feature divided by the total number 
of words (tokens) in the document.  We used a 
set of 13 of these features, chosen on the basis of 
how frequently they occurred and how strongly 
they distinguished gangs from one another (for 
example, substituting ?$? for ?s? was a 
transformation that was common across gangs in 
110
our qualitative analysis, and thus did not seem 
beneficial to include).  
Transformation Freq. False 
Positive 
rate 
False 
Negative 
rate 
b^, c^, h^, p^ 15103 0% 0% 
b ? bk 26923 1% 0% 
c ? ck 16144 25% 8% 
h ? hk 10053 1% 0% 
p ? pk 5669 3% 0% 
ck ? cc, kc 72086 2% 0% 
o ? x, o ? ? 13646 15% 5% 
b ? 6 2470 16% 0% 
e ? 3 8628 28% 1% 
s ? 5 13754 6% 0% 
Table 2: Evaluation of extraction of graffiti style 
features over the million post corpus 
 
 The feature-extraction approach was 
developed iteratively. After extracting the 
features over the corpus of 12,000 active users, 
we created lists of words where the features were 
detected, sorted by frequency. We then manually 
examined the words to determine where we 
observed errors occurring and then made some 
minor adjustments to the extractors.  Table 2 
displays a quantitative evaluation of the accuracy 
of the graffiti style feature extraction. 
 Performance of the style features was 
estimated for each style-feature rule.  For each 
rule, we compute a false positive and false 
negative rate.  For false positive rate, we begin 
by retrieving the list of words marked by the 
feature extraction rule containing the associated 
style marking. From the full set of words that 
matched a style feature rule, we selected the 200 
most frequently occurring word types.  We 
manually checked that complete set of word 
tokens and counted the number of misfires.  The 
false positive rate was then calculated for each 
feature by dividing the number of tokens that 
were misfires over the total number of tokens in 
the set. In all cases, we ensured that at least 55% 
of the total word tokens were covered, so 
additional words may have been examined.  
 In the case of false negatives, we started with 
the set of word types that did not match any word 
in the dictionary and also did not trigger the style 
feature rule.  Again we sorted word types in this 
list by frequency and selected the top 200 most 
frequent.  We then manually checked for missed 
instances where the associated style feature was 
used but not detected.  The false negative rate 
was then the total number of word tokens within 
this word type set divided by the total number of 
word tokens in the complete set of word types. 
 Another type of feature we used referenced 
the nicknames gangs used for themselves and 
other gangs, which we refer to as Names features.  
The intuition behind this is simple: someone who 
is a member of the Crips gang will talk about the 
Crips more often. The measure is simply how 
often a reference to a gang occurs per document. 
Some of these nicknames we included were 
gang-specific insults, with the idea that if 
someone uses insults for Crips often, they are 
likely not a Crip. The last type of reference is 
words that refer to gang alliances like the People 
Nation and Folks Nation. Members of those 
Chicago-based gangs frequently refer to their 
gang as the ?Almighty [gang name] Nation?. 
Gang Positive/Neutral 
Mentions 
Insults 
Crips crip, loc crab, ckrip, ck 
Bloods blood, damu, 
piru, ubn 
slob, bklood, 
pkiru, bk, pk 
Hoovers hoover, groover, 
crim, hgc, hcg 
snoover, 
hkoover, hk 
Gangster 
Disciples 
GD, GDN, 
Gangster 
Disciple 
gk, dk, nigka 
Folks 
Nations 
folk, folknation, 
almighty, nation 
 
People 
Nation 
people, 
peoplenation, 
almighty, nation 
 
Latin 
Kings 
alkqn, king, 
queen 
 
Black P. 
Stones 
stone, abpsn, 
moe, black p. 
 
Vice 
Lords 
vice, lord, vl, 
avln, foe, 4ch 
 
Table 3: Patterns used for gang name features.  For all 
gangs listed in the table, there are slang terms used as 
positive mentions of the gang.  For some gangs there 
are also typical insult names. 
 
We used regular expressions to capture 
occurrences of these words and variations on 
them such as the use of the orthographic 
substitutions mentioned previously, plurals, 
feminine forms, etc. Additionally, in the Blood 
and Hoover features, they sometimes use 
numbers to replace the ?o?s representing the 
street that their gang is located on. So the Bloods 
from 34th Street, say, might write ?Bl34d?. 
111
3.2 Computational Paradigm: Multi-
domain learning 
The key to training an interpretable model in our 
work is to pair a rich feature representation with 
a model that enables accounting for the structure 
of the social context explicitly.  Recent work in 
the area of multi-domain learning offers such an 
opportunity (Arnold, 2009; Daum? III, 2007; 
Finkel & Manning, 2009).  In our work, we treat 
the dominant gang of a thread as a domain for 
the purpose of detecting thread composition.  
This decision is based on the observation that 
while it is a common practice across gangs to 
express their attitudes towards allied and 
opposing gangs using stylistic features like the 
Graffiti style features, the particular features that 
serve the purpose of showing affiliation or 
opposition differ by gang.  Thus, it is not the 
features themselves that carry significance, but 
rather a combination of who is saying it and how 
it is being said. 
 As a paradigm for multi-domain learning, we 
use Daume?s Frustratingly Easy Domain 
Adaptation approach (Daum? III, 2007) as 
implemented in LightSIDE (Mayfield & Ros?, 
2013). In this work, Daum? III proposes a very 
simple ?easy adapt? approach, which was 
originally proposed in the context of adapting to 
a specific target domain, but easily generalizes to 
multi-domain learning. The key idea is to create 
domain-specific versions of the original input 
features depending on which domain a data point 
belongs to. The original features represent a 
domain-general feature space. This allows any 
standard learner to appropriately optimize the 
weights of domain-specific and domain-general 
features simultaneously.  In our work, this allows 
us to model how different gangs signal within-
group identification and across-group animosity 
or alliance using different features.  The resulting 
model will enable us to identify how gangs differ 
in their usage of style features to display social 
identity and social relations. 
 It has been noted in prior work that style is 
often expressed in a topic-specific or even 
domain-specific way (Gianfortoni et al., 2011).  
What exacerbates these problems in text 
processing approaches is that texts are typically 
represented with features that are at the wrong 
level of granularity for what is being 
modeled.  Specifically, for practical reasons, the 
most common types of features used in text 
classification tasks are still unigrams, bigrams, 
and part-of-speech bigrams, which are highly 
prone to over-fitting. When text is represented 
with features that operate at too fine-grained of a 
level, features that truly model the target style are 
not present within the model.  Thus, the trained 
models are not able to capture the style itself and 
instead capture features that correlate with that 
style within the data (Gianfortoni et al., 2011). 
 This is particularly problematic in cases 
where the data is not independent and identically 
distributed (IID), and especially where instances 
that belong to different subpopulations within the 
non-IID data have different class value 
distributions.  In those cases, the model will tend 
to give weight to features that indicate the 
subpopulation rather than features that model the 
style.   Because of this insight from prior work, 
we contrast our stylistic features with unigram 
features and our multi-domain approach with a 
single-domain approach wherever appropriate in 
our experiments presented in Section 4. 
4 Prediction Experiments 
In this section we present a series of prediction 
experiments using the annotations described in 
Section 2.  We begin by evaluating our ability to 
identify gang affiliation for individual users.  
Because we will use dominant gang as a domain 
feature in our multi-domain learning approach to 
detect thread composition, we also present an 
evaluation of our ability to automatically predict 
dominant gang for a thread.  Finally, we evaluate 
our ability to predict thread composition.  All of 
our experiments use L1 regularized Logistic 
regression. 
4.1 Predicting Gang Affiliation per User 
The first set of prediction experiments we ran 
was to identify gang affiliation.  For this 
experiment, the full set of posts contributed by a 
user was concatenated together and used as a 
document from which to extract text features.  
We conducted this experiment using a 10-fold 
cross-validation over the full set of users 
annotated for gang affiliation. Results contrasting 
alternative feature spaces at the gang level and 
nation level are displayed in Table 4.  We begin 
with a unigram feature space as the baseline.  We 
contrast this with the Graffiti style features 
described above in Section 3.1.  Because all of 
the Graffiti features are encoded in words as 
pairs of characters, we contrast the carefully 
extracted Graffiti style features with character 
112
bigrams.  Next we test the nickname features 
also described in Section 3.1.  Finally, we test 
combinations of these features.   
 Gang Nation 
Unigrams 70% 81% 
Character Bigrams 64% 76% 
Graffiti Features 44% 68% 
Name Features 63% 78% 
Name + Graffiti 67% 81% 
Unigrams + Name 70% 82% 
Unigrams + Character 
Bigrams 
71% 82% 
Unigrams + Graffiti 71% 82% 
Unigrams + Name  + 
Graffiti 
72% 83% 
Unigrams + Name  + 
Character Bigrams 
72% 79% 
Table 4: Results (percent accuracy) for gang 
affiliation prediction at the gang and nation level. 
  
     We note that the unigram space is a 
challenging feature space to beat, possibly 
because only about 25% of the users employ the 
style features we identified with any regularity.  
The character bigram space actually significantly 
outperforms the Graffiti features, in part because 
it captures aspects of both the Graffiti features, 
the name features, and also some other gang 
specific jargon.  When we combine the stylistic 
features with unigrams, we start to see an 
advantage over unigrams alone.  The best 
combination is Unigrams, Graffiti style features, 
and Name features, at 72% accuracy (.65 Kappa) 
at the gang level and 83% accuracy (.69 Kappa) 
at the nation level.  Overall the accuracy is 
reasonable and offers us the opportunity to 
expand our analysis of social practices on the 
gangs forum to a much larger sample in our 
future work than we present in this first foray. 
4.2 Predicting Dominant Gang per Thread 
In Section 4.3 we present our multi-domain 
learning approach to predicting thread 
composition.  In that work, we use dominant 
gang on a thread as a domain.  In those 
experiments, we contrast results with hand-
annotated dominant gang and automatically-
predicted dominant gang.  In order to compute an 
automatically-identified dominant gang for the 
949 threads used in that experiment, we build a 
model for gang affiliation prediction using data 
from the 2689 users who did not participate on 
any of those threads as training data so there is 
no overlap in users between train and test. 
     The feature space for that classifier included 
unigrams, character bigrams, and the gang name 
features since this feature space tied for best 
performing at the gang level in Section 4.1 and 
presents a slightly lighter weight solution than 
Unigrams, graffiti style features, and gang name 
features. We applied that trained classifier to the 
users who participated on the 949 threads.  From 
the automatically-predicted gang affiliations, we 
computed a dominant gang using the gang and 
nation level for each thread using the same rules 
that we applied to the annotated user identities 
for the annotated dominant gang labels described 
in Section 2.2.  We then evaluated our 
performance by comparing the automatically-
identified dominant gang with the more carefully 
annotated one.  Our automatically identified 
dominant gang labels were 73.3% accurate (.63 
Kappa) at the gang level and 76.6% accurate (.72 
Kappa) at the nation level. This experiment is 
mainly important as preparation for the 
experiment presented in Section 4.3. 
4.3 Predicting Thread Composition 
Our final and arguably most important prediction 
experiments were for prediction of thread 
composition.  This is where we begin to 
investigate how stylistic choices reflect the 
relationships between participants in a 
discussion.  We conducted this experiment twice, 
specifically, once with the annotated dominant 
gang labels (Table 5) and once with the 
automatically predicted ones (Table 6).  In both 
cases, we evaluate gang and nation as alternative 
domain variables.  In both sets of experiments, 
the multi-domain versions significantly 
outperform the baseline across a variety of 
feature spaces, and the stylistic features provide 
benefit above the unigram baseline.  In both 
tables the domain and nation variables are hand-
annotated. * indicates the results are significantly 
better than the no domain unigram baseline.  
Underline indicates best result per column.  And 
bold indicates overall best result.  
     The best performing models in both cases 
used a multi-domain model paired with a stylistic 
feature space rather than a unigram space.  Both 
models performed significantly better than any of 
the unigram models, even the multi-domain 
versions with annotated domains. Where gang 
was used as the domain variable and Graffiti 
style features were the features used for 
prediction, we found that the high weight 
features associated with Allied threads were 
113
either positive about gang identity for a variety 
of gangs other than their own (like B^ in a Crips 
dominated thread) or protective (like CC in a 
Bloods dominated thread).   
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 58%* 60%* 
Character 
Bigrams 
49% 55% 56% 
Graffiti 
Features 
53% 54% 61%* 
Name 
Features 
54% 63%* 66%* 
Name + 
Graffiti 
54% 61%* 65%* 
Unigrams 
+ Name 
52% 58%* 61%* 
Unigrams 
+ Graffiti 
53% 57% 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 61%* 65%* 
Table 5: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches, one 
with dominant gang as the domain variables, and the 
other with dominant nation as the domain variable. In 
this case, the domain variables are annotated. 
 
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 57% 57% 
Character 
Bigrams 
49% 53% 55% 
Graffiti 
Features 
53% 65%* 58%* 
Name 
Features 
54% 61%* 59%* 
Name + 
Graffiti 
54% 60%* 59%* 
Unigrams 
+ Name 
52% 56% 56% 
Unigrams 
+ Graffiti 
53% 58%* 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 60%* 59%* 
Table 6: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches with 
predicted domain variables, one with dominant gang 
as the domain variables, and the other with dominant 
nation as the domain variable.  
 
Crips-related features were the most frequent 
within this set, perhaps because of the complex 
social structure within the Crips alliance, as 
discussed above.  We saw neither features 
associated with negative attitudes of the gang 
towards others nor other gangs towards them in 
these Allied threads, but in opposing threads, we 
see both, for example, PK in Crips threads or BK 
in Bloods threads.  Where unigrams are used as 
the feature space, the high weight features are 
almost exclusively in the general space rather 
than the domain space, and are generally 
associated with attitude directly rather than gang 
identity.  For example, ?lol,? and ?wtf.? 
5 Conclusions  
We have presented a series of experiments in 
which we have analyzed the usage of stylistic 
features for signaling personal gang 
identification and between gang relations in a 
large, online street gangs forum.  This first foray 
into modeling the language practices of gang 
members is one step towards providing an 
empirical foundation for interpretation of these 
practices.  In embarking upon such an endeavor, 
however, we must use caution.  In machine-
learning approaches to modeling stylistic 
variation, a preference is often given to 
accounting for variance over interpretability, 
with the result that interpretability of models is 
sacrificed in order to achieve a higher prediction 
accuracy.  Simple feature encodings such as 
unigrams are frequently chosen in a (possibly 
misguided) attempt to avoid bias.  As we have 
discussed above, however, rather than cognizant 
introduction of bias informed by prior linguistic 
work, unknown bias is frequently introduced 
because of variables we have not accounted for 
and confounding factors we are not aware of, 
especially in social data that is rarely IID. Our 
results suggest that a strategic combination of 
rich feature encodings and structured modeling 
approach leads to high accuracy and 
interpretability.  In our future work, we will use 
our models to investigate language practices in 
the forum at large rather than the subset of users 
and threads used in this paper1. 
                                                          
1 An appendix with additional analysis and the 
specifics of the feature extraction rules can be found 
at http://www.cs.cmu.edu/~cprose/Graffiti.html. This 
work was funded in part by ARL 
000665610000034354.   
114
References  
Adams, K. & Winter, A. (1997). Gang graffiti as a 
discourse genre, Journal of Sociolinguistics 1/3. Pp 
337-360. 
Argamon, S., Koppel, M., Fine, J., & Shimoni, A. 
(2003). Gender, genre, and writing style in formal 
written texts, Text, 23(3), pp 321-346. 
Argamon, S., Koppel, M., Pennebaker, J., & Schler, J. 
(2007). Mining the blogosphere: age, gender, and 
the varieties of self-expression. First Monday 
12(9). 
Arnold, A. (2009). Exploiting Domain And Task 
Regularities For Robust Named Entity 
Recognition. PhD thesis, Carnegie Mellon 
University, 2009. 
Biber, D. & Conrad, S. (2009). Register, Genre, and 
Style, Cambridge University Press 
Bullock, B. (1996). Derivation and Linguistic Inquiry: 
Les Javnais, The French Review 70(2), pp 180-191. 
Corney, M., de Vel, O., Anderson, A., Mohay, G. 
(2002). Gender-preferential text mining of e-mail 
discourse, in the Proceedings of the 18th Annual 
Computer Security Applications Conference. 
Coulthard, M. & Johnson, A. (2007). An Introduction 
to Forensic Linguistics: Language as Evidence, 
Routledge 
Daum? III, H. (2007). Frustratingly Easy Domain 
Adaptation. In Proceedings of the 45th Annual 
Meeting of the Association of Computational 
Linguistics, pages 256-263. 
Eckert, P. & Rickford, J. (2001). Style and 
Sociolinguistic Variation, Cambridge: University 
of Cambridge Press. 
Finkel, J. & Manning, C. (2009). Hierarchical 
Bayesian Domain Adaptation. In Proceedings of 
Human Language Technologies: The 2009 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. 
Garot, R. (2007). ?Where You From!?: Gang Identity 
as Performance, Journal of Contemporary 
Ethnography, 36, pp 50-84. 
Gianfortoni, P., Adamson, D. & Ros?, C. P. (2011).  
Modeling Stylistic Variation in Social Media with 
Stretchy Patterns, in Proceedings of First 
Workshop on Algorithms and Resources for 
Modeling of Dialects and Language Varieties, 
Edinburgh, Scottland, UK, pp 49-59. 
Graff, D. (2002).  The AQUAINT Corpus of English 
News Text, Linguistic Data Consortium, 
Philadelphia 
Greenlee, M. (2010).  Youth and Gangs, in M. 
Coulthard and A. Johnson (Eds.). The Routledge 
Handbook of Forensic Linguistics, Routledge. 
Jiang, M. & Argamon, S. (2008). Political leaning 
categorization by exploring subjectivities in 
political blogs. In Proceedings of the 4th 
International Conference on Data Mining, pages 
647-653. 
Johnsons, K. (2009).  FBI: Burgeoning gangs behind 
up to 80% of U.S. Crime, in USA Today, January 
29, 2009. 
Kahneman,  D. (2011).  Thinking Fast and Slow, 
Farrar, Straus, and Giroux 
Krippendorff, K. (2013). Content Analysis: An 
Introduction to Its Methodology (Chapter 13), 
SAGE Publications 
Labov, W. (2010). Principles of Linguistic Change: 
Internal Factors (Volume 1), Wiley-Blackwell. 
Lefkowitz, N. (1989).  Talking Backwards in French, 
The French Review 63(2), pp 312-322. 
Mayfield, E. & Ros?, C. P. (2013). LightSIDE: Open 
Source Machine Learning for Text Accessible to 
Non-Experts, in The Handbook of Automated 
Essay Grading, Routledge Academic Press.        
http://lightsidelabs.com/research/ 
Philips, S. (2009).  Crip Walk, Villian Dance, Pueblo 
Stroll: The Embodiment of Writing in African 
American Gang Dance, Anthropological Quarterly 
82(1), pp69-97. 
Schler, J., Koppel, M., Argamon, S., Pennebaker, J. 
(2005). Effects of Age and Gender on Blogging, 
Proceedings of AAAI Spring Symposium on 
Computational Approaches for Analyzing Weblogs. 
Schler, J. (2006). Effects of Age and Gender on 
Blogging. Artificial Intelligence, 86, 82-84. 
Wiebe, J., Bruce, R., Martin, M., Wilson, T., & Ball, 
M. (2004). Learning Subjective Language, 
Computational Linguistics, 30(3). 
Yan, X., & Yan, L. (2006). Gender classification of 
weblog authors. AAAI Spring Symposium Series 
Computational Approaches to Analyzing Weblogs 
(p. 228?230). 
Zhang, Y., Dang, Y., Chen, H. (2009). Gender 
Difference Analysis of Political Web Forums : An 
Experiment on International Islamic Women?s 
Forum, Proceedings of the 2009 IEEE international 
conference on Intelligence and security 
informatics, pp 61-64. 
 
115
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 42?50,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Code-Switching in Multilingual Communication for
Immigrant Communities
Evangelos E. Papalexakis
Carnegie Mellon University
Pittsburgh, USA
epapalex@cs.cmu.edu
Dong Nguyen
University of Twente
Enschede, The Netherlands
d.nguyen@utwente.nl
A. Seza Do
?
gru
?
oz
Netherlands Institute
for Advanced Study
Wassenaar, The Netherlands
a.s.dogruoz@gmail.com
Abstract
Immigrant communities host multilingual
speakers who switch across languages
and cultures in their daily communication
practices. Although there are in-depth
linguistic descriptions of code-switching
across different multilingual communica-
tion settings, there is a need for au-
tomatic prediction of code-switching in
large datasets. We use emoticons and
multi-word expressions as novel features
to predict code-switching in a large online
discussion forum for the Turkish-Dutch
immigrant community in the Netherlands.
Our results indicate that multi-word ex-
pressions are powerful features to predict
code-switching.
1 Introduction
Multilingualism is the norm rather than an ex-
ception in face-to-face and online communica-
tion for millions of speakers around the world
(Auer and Wei, 2007). 50% of the EU popula-
tion is bilingual or multilingual (European Comis-
sion, 2012). Multilingual speakers in immigrant
communities switch across different languages
and cultures depending on the social and contex-
tual factors present in the communication envi-
ronment (Auer, 1988; Myers-Scotton, 2002; Ro-
maine, 1995; Toribio, 2002; Bullock and Toribio,
2009). Example (1) illustrates Turkish-Dutch
code-switching in a post about video games in an
online discussion forum for the Turkish immigrant
community in the Netherlands.
Example (1)
user1: <dutch>vette spellllllllll </dutch>..
<turkish>bir girdimmi cikamiyomm ..
yendikce yenesi geliyo insanin</turkish>
Translation: <dutch> awesome gameeeee
</dutch>.. <turkish>once you are in it, it is
hard to leave .. the more you win, the more
you want to win</turkish>
Mixing two or more languages is not a random
process. There are in-depth linguistic descriptions
of code-switching across different multilingual
contexts (Poplack, 1980; Silva-Corval?an, 1994;
Owens and Hassan, 2013). Although these studies
provide invaluable insights about code-switching
from a variety of aspects, there is a growing need
for computational analysis of code-switching in
large datasets (e.g. social media) where man-
ual analysis is not feasible. In immigrant set-
tings, multilingual/bilingual speakers switch be-
tween minority (e.g. Turkish) and majority (e.g.
Dutch) languages. Code-switching marks multi-
lingual, multi-cultural (Luna et al., 2008; Gros-
jean, 2014) and ethnic identities (De Fina, 2007)
of the speakers. By predicting code-switching
patterns in Turkish-Dutch social media data, we
aim to raise consciousness about mixed language
communication patterns in immigrant communi-
ties. Our study is innovative in the following ways:
? We performed experiments on the longest
and largest bilingual dataset analyzed so far.
? We are the first to predict code-switching in
social media data which allow us to investi-
gate features such as emoticons.
? We are the first to exploit multi-word expres-
sions to predict code-switching.
? We use automatic language identification at
the word level to create our dataset and fea-
tures that capture previous language choices.
The rest of this paper is structured as follows:
we discuss related work on code-switching and
multilingualism in Section 2, our dataset in Sec-
tion 3, a qualitative analysis in Section 4, our ex-
perimental setup and features in Section 5, our re-
sults in Section 6 and our conclusion in Section
7.
42
2 Related Work
Code-switching in sociolinguistics There is
rarely any consensus on the terminology about
mixed language use. Wei (1998) considers al-
ternations between languages at or above clause
levels as code-mixing. Romaine (1995) refers to
both inter-sentential and intra-sentential switches
as code-switching. Bilingual speakers may shift
from one language to another entirely (Poplack et
al., 1988) or they mix languages partially within
the single speech (Gumperz, 1982). In this study,
we focus on code-switching within the same post
in an online discussion forum used by Turkish-
Dutch bilinguals.
There are different theoretical models which
support (Myers-Scotton, 2002; Poplack, 1980) or
reject (MacSwan, 2005; Thomason and Kaufman,
2001) linguistic constraints on code-switching.
According to (Thomason and Kaufman, 2001;
Gardner-Chloros and Edwards, 2004) linguistic
factors are mostly unpredictable since social fac-
tors govern the multilingual environments in most
cases. Bhatt and Bolonyai (2011) have an exten-
sive study on socio-cognitive factors that lead to
code-switching across different multilingual com-
munities.
Although multilingual communication has been
widely studied through spoken data analyses, re-
search on online communication is relatively re-
cent. In terms of linguistic factors C?ardenas-
Claros and Isharyanti (2009) report differences
between Indonesian-English and Spanish-English
speakers in their amount of code-switching on
MSN (an instant messaging client). Durham
(2003) finds a tendency to switch to English over
time in an online multilingual (German, French,
Italian) discussion forum in Switzerland.
The media (e.g. IRC, Usenet, email, online
discussions) used for multilingual conversations
influence the amount of code-switching as well
(Paolillo, 2001; Hinrichs, 2006). Androutsopou-
los and Hinnenkamp (2001), Tsaliki (2003) and
Hinnenkamp (2008) have done qualitative anal-
yses of switch patterns across German-Greek-
Turkish, Greek-English and Turkish-German in
online environments respectively.
In terms of social factors, a number of studies
have investigated the link between topic and lan-
guage choices qualitatively (Ho, 2007; Androut-
sopoulos, 2007; Tang et al., 2011). These stud-
ies share the similar conclusion that multilingual
speakers use minority languages to discuss topics
related to their ethnic identity and reinforcing inti-
macy and self-disclosure (e.g. homeland, cultural
traditions, joke telling) whereas they use the ma-
jority language for sports, education, world poli-
tics, science and technology.
Computational approaches to code-switching
Recently, an increasing number of research within
NLP has focused on dealing with multilingual
documents. For example, corpora with multilin-
gual documents have been created to support stud-
ies on code-switching (e.g. Cotterell et al. (2014))
To enable the automatic processing and analysis
of documents with mixed languages, there is a
shift in focus toward language identification at the
word level (King and Abney, 2013; Nguyen and
Do?gru?oz, 2013; Lui et al., 2014). Most closely re-
lated to our work is the study by Solorio and Liu
(2008) who predict code-switching in recorded
English-Spanish conversations. Compared to their
work, we use a large-scale social media dataset
that enables us to explore novel features.
The task most closely related to automatic pre-
diction of code-switching is automatic language
identification (King and Abney, 2013; Nguyen and
Do?gru?oz, 2013; Lui et al., 2014). While automatic
language detection uses the words to identify the
language, automatic prediction of code-switching
involves predicting whether the language of the
next word is the same without having access to the
next word itself.
Language practices of the Turkish community
in the Netherlands Turkish has been in con-
tact with Dutch due to labor immigration since
the 1960s and the Turkish community is the
largest minority group (2% of the whole popula-
tion) in the Netherlands (Centraal Bureau voor de
Statistiek, 2013). In addition to their Dutch flu-
ency, second and third generations are also fluent
in Turkish through speaking it within the family
and community, regular family visits to Turkey
and watching Turkish TV through satellite dishes.
These speakers grow up speaking both languages
simultaneously rather than learning one language
after the other (De Houwer, 2009). In addition
to constant switches between Turkish and Dutch,
there are also literally translated Dutch multi-word
expressions (Do?gru?oz and Backus, 2007; Do?gru?oz
and Backus, 2009). Due to the religious back-
grounds of the Turkish-Dutch community, Arabic
43
words and phrases (e.g. greetings) are part of daily
communication. In addition, English words and
phrases are used both in Dutch and Turkish due to
the exposure to American and British media.
Although the necessity of studying immigrant
languages in Dutch online environments has been
voiced earlier (Dorleijn and Nortier, 2012), the
current study is the first to investigate mixed lan-
guage communication patterns of Turkish-Dutch
bilinguals in online environments.
3 Dataset
Our data comes from a large online forum
(Hababam) used by Turkish-Dutch speakers. The
forum is active since 2000 and contains 28 sub-
forums on a variety of topics (e.g. sports, poli-
tics, education). Each subforum consists of mul-
tiple threads which start with a thread title (e.g. a
statement or question) posted by a moderator or
user. The users are Turkish-Dutch bilinguals who
reside in the Netherlands. Although Dutch and
Turkish are used dominantly in the forum, English
(e.g. fixed expressions) and Arabic (e.g. prayers)
are occasionally used (less than 1%) as well. We
collected the data between June 2005 and October
2012 by crawling the forum. Statistics of our data
are shown in Table 1.
Frequency
Number of posts 4,519,869
Number of users 14,923
Number of threads 113,517
Number of subforums 29
Table 1: Dataset Statistics
The subforums Chit-Chat (1,671,436), Turkish
youth & love (447,436), and Turkish news & up-
dates (418,135) have the highest post frequency
whereas Columns (4727), Science & Philosophy
(5083) and Other Beliefs (6914) have the lowest
post frequency.
An automatic language identification tagger is
used to label the language of the words in posts
and titles of the threads. The tagger distinguishes
between Turkish and Dutch using logistic regres-
sion (Nguyen and Do?gru?oz, 2013) and achieves
a word accuracy of approximately 97%. We use
the language labels to train our classifier (since
given the labels we can determine whether there
is a switch or not), and to evaluate our model.
4 Types of Code-Switching
In this section, we provide a qualitative analysis of
code-switching in the online forum. We differen-
tiate between two types of code-switching: code-
switching across posts and code-switching within
the same post.
4.1 Code-switching across posts
Within the same discussion thread, users react
to posts of other users in different languages.
In example (2), user 1 posts in Dutch to tease
User 2. User 2 reacts to this message with a
humorous idiomatic expression in Turkish (i.e.
[adim cikmis] ?I made a name?) to indirectly
emphasize that there is no reason for her to defend
herself since she has already become famous as
the perfect person in the online community. This
type of humorous switch has also been observed
for Greek-English code-switching in face-to-face
communication (Gardner-Chloros and Finnis,
2003). The text is written with Dutch orthography
instead of conventional Turkish orthography (i.e.
[ad?m c??km?s?]). It is probably the case that the user
has a Dutch keyboard without Turkish characters.
However, writing with non-Turkish characters in
online environments is also becoming popular
among monolingual Turkish users from Turkey.
Example (2)
User1: <dutch> je hoefde niet gelijk in de
verdediging te schieten hoor </dutch> :P
Tra: ?you do not need to be immediately
defensive dear?
User2: <turkish> zaten adim cikmis
mukemmel sahane kusursuz insana, bi de
yine cikmasin </turkish> :(
Tra: ?I already have established a name as a
great amazing perfect person, I do not need
it to spread around once more?
Example (3) is taken from a thread about break-
fast traditions. The users have posted what they
had for breakfast that day. The first user talks
about his breakfast in Turkish and describes the
culture specific food items (e.g. borek ?Turkish
pastry?) prepared by his mother. The second user
describes a typical Dutch breakfast and therefore
switches to Dutch.
Example (3)
User1: <turkish>annemin peynirli borekleri
ve cay</turkish>
Tra: ?the cheese pastries of my mom and
tea?
44
User2: <dutch>Twee sneetjes geroost-
erd bruin brood met kipfilet en een glas
thee.</dutch>
Tra: ?Two pieces of roasted brown bread with
chicken filet and a cup of tea?
4.2 Code-switching within the same post
In addition to code-switching across posts, we en-
countered code-switching within the same post of
a user as well. Manual annotation of a subset of
the posts in Nguyen and Do?gru?oz (2013), suggests
that less than 20% of the posts contain a switch.
Example (4) is taken from a thread about Mother?s
Day and illustrates an intra-sentential switch. The
user starts the post in Dutch (vakantie boeken ?to
book a vacation?) and switches to Turkish since
booking a vacation through internet sites or a
travel agency is a typical activity associated with
the Dutch culture.
Example (4)
<dutch>vakantie boeken</dutch>
<turkish> yaptim annecigimee </turkish>
Tra
1
:?(I) <dutch>booked a holiday</dutch>
<turkish>for my mother.</turkish>?
Example (5) is taken from a thread about Turk-
ish marriages and illustrates an inter-sentential
switch. The user is advising the other users
in Turkish to be very careful about choosing
their partners. Since most Turkish community
members prefer Turkish partners and follow
Turkish traditions for marriage, she talks about
these topics in Turkish. However, she switches
to Dutch when she talks about getting a diploma
in the Dutch school system. Similar examples of
code-switching for emphasizing different identi-
ties based on topic have been observed for other
online and face-to-face communication as well
(Androutsopoulos, 2007; Gardner-Chloros, 2009).
Example (5)
<turkish>Allah korusun yani. Kocani iyi
sec diyim=) evlilik evcilik degildir.</turkish>
<dutch>Al zou ik wanneer ik getrouwd ben
een HBO diploma op zak hebben, zou ik
hem dan denk ik niet verlaten.</dutch>
Tra:?<turkish> May God protect you.
Choose your husband carefully. Marriage is
not a game </turkish> <dutch> Even if I
am married and have a university diploma, I
don?t think I will leave him </dutch>?
Code-switching through greetings, wishes and
formulaic expressions are commonly observed
1
It is possible to drop the subject pronoun in Turkish. As
typical in bilingual speech, an additional Turkish verb yap-
mak follows the Dutch verb boeken ?to book?.
in bilingual face-to-face communication and on-
line immigrant forums as well (Androutsopoulos,
2007; Gardner-Chloros, 2009).
5 Experimental Setup
The focus of this paper is on code-switching
within the same post. We discuss the setup and
features of our experiment in this section.
5.1 Goal
We cast the prediction of the code-switch point
within the post as a binary classification problem.
We define the i-th token of the post as an instance.
If the i + 1th token is in a different language, the
label is 1. Otherwise, the label is 0.
Obtaining language labels In order to label
each token of a post, we rely on the labels ob-
tained using automatic language identification at
the word level (see Section 3). This process may
not be the most accurate way of labeling each to-
ken of a post at a large scale. One particular arti-
fact of this procedure is that an automatic tagger
may falsely tag the language of a token in longer
posts. As a result, some lengthy posts might ap-
pear to have one or more code-switches by ac-
cident. However, since the accuracy of our tag-
ger is high (approx. 97% accuracy), we expect
the amount of such spurious code-switches to be
low. For future work, we plan to experiment on a
dataset based on automatic language identification
as well as a smaller dataset using manual annota-
tion.
5.2 Creating train and test sets
Before we attempt to train a classifier on our data,
we eliminate the biases and imbalances. The ma-
jority of posts do not contain any switches. As a
consequence, the number of instances that belong
to the ?0? class (i.e. no code-switching occurring
after the current word) grossly outnumber the in-
stances of class ?1?, where code-switching takes
place. In order to alleviate this class imbalance, for
all our experiments, we sample an equal amount
of instances from ?0? and ?1? classes randomly
2
,
both for our training and testing data. This way
the result will not favor the ?0? class even if we
randomly decide on the class label for each in-
stance. The average number of training and testing
2
We do 100 iterations and average the results of all these
independent samples.
45
instances per iteration was 4000 and 80000 respec-
tively. By drawing 100 independent samples from
the entire dataset, we cover a reasonable portion of
the full data and do not sacrifice the balance of the
two classes, which is crucially important for the
validity of our results.
5.3 Feature selection
We use the following features (see Table 2) to in-
vestigate code-switching within a post.
5.3.1 Non-linguistic features
Emoticons Emoticons are iconic symbols that
convey emotional information along with lan-
guage use in online environments (Dresner and
Herring, 2014). Emoticons have mostly been used
in the context of sentiment analysis (e.g. Volkova
et al. (2013), Chmiel et al. (2011)). Park et al.
(2014) studied how the use of emoticons differ
across cultures in Twitter data. Panayiotou (2004)
studied how bilinguals express emotions in face-
to-face environments in different languages. We
are the first to investigate the role of emoticons as a
non-linguistic factor in predicting code-switching
on social media.
Emoticons in our data are either signified by
a special tag [smiley:smiley type] or can
appear in any of the common ASCII emoticon
forms (e.g. :), :-) etc.). In order to detect the
emoticons, we used a hand picked list of ASCII
emoticons as our dictionary, as well as a filter that
searched for the special emoticon tag. Since we
rely on an automatic language tagger, the language
label of a particular emoticon depends on its sur-
rounding tokens. If an emoticon is within a block
of text that is tagged as Turkish, then the emoticon
will automatically obtain a Turkish label (and ac-
cordingly for Dutch). For future work, we will ex-
periment with labeling emoticons differently (e.g.
introducing a third, neutral label).
To assess the strength of emoticons as predic-
tors of code-switching, we generate 4 different
features (see Table 2). These features capture
whether or not there is an emoticon at or before
the token that we want to classify as the switch
boundary between Dutch and Turkish. We record
whether there was an emoticon at token i (i.e. the
token we want to classify), token i ? 1 and token
i ? 2.
The last emoticon feature records whether there
is any emoticon after the current token. We note
that this feature looks ahead (after the i-th token),
and therefore cannot be implemented in a real time
system which predicts code-switching on-the-fly.
However, we included the feature for exploratory
purposes.
5.3.2 Linguistic features
Language around the switch point We also in-
vestigate whether the knowledge of the language
of a couple of tokens before the token of inter-
est, as well as the language at the token of inter-
est, hold some predictive strength. These features
correspond to #1-3 in Table 2. Generally, the lan-
guage label is binary. However, if there are no to-
kens in positions i ? 2 or i ? 1 for features #1
and #2, we assign a third value to represent this
non-existence. Additionally, we explore whether
a previous code-switching in a post triggers a sec-
ond code-switching later in the same post. We test
this hypothesis by recording feature #4 which rep-
resents the existence of code-switching before to-
ken i.
Single word versus multi-word switch There
is an on-going discussion in multilingualism about
the classification of switched tokens (Poplack,
2004; Poplack, 2013) and whether there are
linguistic constraints on the switches (Myers-
Scotton, 2002). In addition to switches across in-
dividual lexical tokens, multilingual speakers also
switch across multi-word expressions.
Automatic identification of multi-word expres-
sions in monolingual language use have been
widely discussed (Baldwin et al., 2003; Baldwin
and Kim, 2010) but we know little about how to
predict switch points that include multi-word ex-
pressions. We are the first to include multi-word
expressions as a feature to predict code-switching.
We are mostly inspired by (Schwartz et al., 2013)
in identifying MWEs.
More specifically, we built a corpus of 3-gram
MWEs (2,241,484 in total) and selected the most
frequent 100 MWEs. We differentiate between
two types of MWEs: Let the i-th token of a post
be the switch point. For type 1, we take 3 tokens
(all in the same language) right before the switch
token (i.e. terms i? 3, i? 2, i? 1). [Allah razi ol-
sun] ?May the Lord be with you? and [met je eens]
?agree with you? are the two of the most frequent
MWEs (in Turkish and Dutch respectively).
For type 2, we take the tokens i ? 2, i ? 1, i
and the last token is in a different language (e.g.
[Turkse premier Recep] ?Turkish prime-minister
46
Table 2: Features
Feature # Feature Description
1 Language of token in position i ? 2
2 Language of token in position i ? 1
3 Language of token in position i (current token)
4 Was there code-switching before the current token?
5 Is there an emoticon in position i ? 2?
6 Is there an emoticon in position i ? 1?
7 Is there an emoticon in position i?
8 Are there any emoticons in positions after i?
9 Is the i-th token the first word of a 3-word multi-word expression?
10 Is the i-th token the second word of a 3-word multi-word expression?
11 Is the i-th token the third word of a 3-word multi-word expression?
Recep?).
The first type of MWEs captures whether an
MWE (all three words in the same language), sig-
nifies code-switching for token i or not.
The second type investigates whether there are
MWEs that ?spill over? the code-switching point
(i.e. the first two tokens of an MWE are in the
same language, but the third token is in another
language). In order to get a good estimate of the
MWEs in our corpus, we count the occurrences of
all these 3-grams and keep the top scoring ones in
terms of frequency, which end up as our dictionary
of MWEs.
6 Results
To evaluate the predictive strength of our features,
we conduct experiments using a Naive Bayes clas-
sifier.
In order to measure the performance, we train
the classifiers for various combinations of the fea-
tures shown in Table 2. As we described in the pre-
vious section, we train on randomly chosen, class-
balanced parts of the data and we test on randomly
selected balanced samples (disjoint from the train-
ing set), averaging over 100 runs. For each com-
bination of features, we measure and report aver-
age precision, recall, and F1-score, with respect to
positively predicting code-switching.
Table 3 illustrates the performance of individ-
ual features used in our classifier. Features that
concern the language of the previous tokens (i.e.
features #1 & #2) seem to perform better than
chance in predicting code-switching. On the other
hand, features #3 (language of the token in posi-
tion i) and #4 (previous code-switching) have the
worst performance. In fact, the obtained classi-
Table 3: Performance of individual features
Feature # Precision Recall F1 score
1 0.6305 1 0.7733
2 0.6362 1 0.7776
3 0 0 -
4 0 0 -
5 0.704 0.2116 0.3254
6 0.7637 0.2324 0.3564
7 0.8025 0.1339 0.0954
8 0.4879 0.3214 0.3875
9 0.5324 0.7819 0.6335
10 0.5257 0.8102 0.6376
11 0.5218 0.8396 0.6436
fier always predicts no code-switching regardless
of the value of the feature. Therefore, both pre-
cision and recall are 0. Features #1 & #2 behave
differently from features #3 & #4 because #1 & #2
have ternary values (the token language, or non-
existing). This probably forces the classifiers to
produce a non-constant decision. For instance, the
model for feature #1 decides positively for code-
switching if the language label is either Turkish or
Dutch and decides negatively if the label is non-
existing.
The rest of the individual features perform sim-
ilarly but worse than #1 and #2. Therefore, it is
necessary to use a combination of features instead
of single ones.
After examining how features perform individu-
ally, we further investigate how features behave in
groups. We first group the features into homoge-
nous categories (e.g. #1-#3 focus on the language
of tokens, #5-#8 record the presence of emoticons
and #9-#11 refer to MWEs). Subsequently, we
test the performance of these categories in differ-
ent combinations, and finally measure the effect of
47
Table 4: Performance of groups of features
Features Precision Recall F1 score
1-3 Language of tokens 0.6362 1 0.7777
1-4 Language + previous code-switching 0.6663 0.1312 0.6663
5-8 Emoticons 0.6638 0.397 0.2766
9-11 MWEs 0.5384 0.7476 0.626
5-11 Emoticons + MWEs 0.52 0.8718 0.6466
1-8 Language + previous code-switching + emoticons 0.6932 0.5114 0.4634
1-4, 9-11 Language + previous code-switching + MWEs 0.712 0.7297 0.7113
1-11 All 0.6847 0.8034 0.7106
using all our features for the task. Table 4 shows
the combinations of the features we used, as well
as the average precision, recall, and F1-score.
According to Table 4, the combination of the
language of the tokens (features #1-#3) and the
previous code-switching earlier in the post (fea-
tures #1-#4), and MWEs (features #9-#11) per-
form the highest in terms of precision/recall. Fea-
tures #3 and #4 have rather low performances on
their own but they yield a strong classifier in com-
bination with other features.
When we use features that record emoticons
(#5-#8) or MWEs (#9-#11) alone, the performance
of our classifier decreases. In general, MWEs out-
perform emoticons. We observe this performance
boost when we combine emoticon features with
other features (e.g. #1-#8) and with MWEs to-
gether in the same subset (#1-#4, #9-#11).
7 Conclusion
We focused on predicting code-switching points
for a mixed language online forum used by
the Turkish-Dutch immigrant community in the
Netherlands. For the first time, a long term data
set was used to investigate code-switching in so-
cial media. We are also the first to test new fea-
tures (e.g. emoticons and MWEs) to predict code-
switching and to identify the features with sig-
nificant predictive strength. For future work, we
will continue our investigation with exploring the
predictive value of these new features within the
Turkish-Dutch immigrant community as well as
others.
8 Acknowledgements
The first author was supported by the National Sci-
ence Foundation (NSF), Grant No. IIS-1247489.
The second author was supported by the Nether-
lands Organization for Scientific Research (NWO)
grant 640.005.002 (FACT). The third author was
supported by a Digital Humanities Research Grant
from Tilburg University and a research fellowship
from Netherlands Institute for Advanced Study.
References
Jannis Androutsopoulos and Volker Hinnenkamp.
2001. Code-switching in der bilingualen chat-
kommunikation: ein explorativer blick auf# hellas
und# turks. Beisswenger, Michael (ed.), pages 367?
401.
Jannis Androutsopoulos, 2007. The Multilingual In-
ternet, chapter Language choice and code-switching
in German-based diasporic web forums, pages 340?
361. Oxford University Press.
Peter Auer and Li Wei, 2007. Handbook of multilin-
gualism and multilingual communication., chapter
Introduction: Multilingualism as a problem? Mono-
lingualism as a problem, pages 1?14. Berlin: Mou-
ton de Gruyter.
Peter Auer. 1988. A conversation analytic ap-
proach to code-switching and transfer. Codeswitch-
ing: Anthropological and sociolinguistic perspec-
tives, 48:187?213.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. Handbook of Natural Language Pro-
cessing, second edition. Morgan and Claypool.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pages 89?96. Association for Computa-
tional Linguistics.
Rakesh M Bhatt and Agnes Bolonyai. 2011. Code-
switching and the optimal grammar of bilingual lan-
guage use. Bilingualism: Language and Cognition,
14(04):522?546.
Barbara E Bullock and Almeida Jacqueline Toribio.
2009. The Cambridge handbook of linguistic code-
switching, volume 1. Cambridge University Press
Cambridge.
48
Monica S. C?ardenas-Claros and Neny Isharyanti.
2009. Code-switching and code-mixing in inter-
net chatting: Between?yes,?ya,?and?si?-a case study.
The Jalt Call Journal, 5(3):67?78.
Centraal Bureau voor de Statistiek. 2013. Bevolking,
generatie, geslacht, leeftijd en herkomstgroepering.
2013.
Anna Chmiel, Julian Sienkiewicz, Mike Thelwall,
Georgios Paltoglou, Kevan Buckley, Arvid Kappas,
and Janusz A Ho?yst. 2011. Collective emotions
online and their influence on community life. PloS
one, 6(7):e22207.
Ryan Cotterell, Adithya Renduchintala, Naomi Saphra,
and Chris Callison-Burch. 2014. An algerian
arabic-french code-switched corpus. In LREC.
Anna De Fina. 2007. Code-switching and the con-
struction of ethnic identity in a community of prac-
tice. Language in Society, 36(03):371?392.
Annick De Houwer. 2009. Bilingual first language
acquisition. Multilingual Matters.
A Seza Do?gru?oz and Ad Backus. 2007. Postverbal el-
ements in immigrant Turkish: Evidence of change?
International Journal of Bilingualism, 11(2):185?
220.
A. Seza Do?gru?oz and Ad Backus. 2009. Innova-
tive constructions in Dutch Turkish: An assessment
of ongoing contact-induced change. Bilingualism:
Language and Cognition, 12(01):41?63.
Margreet Dorleijn and Jacomine Nortier, 2012. The
Cambridge Handbook of Linguistic Code-switching,
chapter Code-switching and the internet, pages 114?
127. Cambridge University Press.
Eli Dresner and Susan C Herring. 2014. Emoticons
and illocutionary force. In Perspectives on Theory
of Controversies and the Ethics of Communication,
pages 81?90. Springer.
Mercedes Durham. 2003. Language choice on a Swiss
mailing list. Journal of Computer-Mediated Com-
munication, 9(1):0?0.
European Comission. 2012. Europeans and their lan-
guages: Special barometer 386. Technical report,
European Comission.
Penelope Gardner-Chloros and Malcolm Edwards.
2004. Assumptions behind grammatical approaches
to code-switching: When the blueprint is a red
herring. Transactions of the Philological Society,
102(1):103?129.
Penelope Gardner-Chloros and Katerina Finnis. 2003.
How code-switching mediates politeness: Gender-
related speech among London Greek-Cypriots. So-
ciolinguistic Studies, 4(2):505?532.
Penelope Gardner-Chloros, 2009. Handbook of Code-
switching, chapter Sociolinguistic Factors in Code-
Switching, pages 97?114. Cambridge University
Press.
Francois Grosjean. 2014. Bicultural bilinguals. Inter-
national Journal of Bilingualism, xx(xx):1?15.
John J Gumperz. 1982. Discourse strategies, vol-
ume 1. Cambridge University Press.
Volker Hinnenkamp. 2008. Deutsch, Doyc or Doitsch?
Chatters as languagers?The case of a German?
Turkish chat room. International Journal of Mul-
tilingualism, 5(3):253?275.
Lars Hinrichs. 2006. Codeswitching on the Web: En-
glish and Jamaican Creole in E-mail Communica-
tion (Pragmatics & Beyond, Issn 0922-842x). John
Benjamins.
Judy Woon Yee Ho. 2007. Code-mixing: Linguistic
form and socio-cultural meaning. The International
Journal of Language Society and Culture, 21.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27?40.
David Luna, Torsten Ringberg, and Laura A Peracchio.
2008. One individual, two identities: Frame switch-
ing among biculturals. Journal of Consumer Re-
search, 35(2):279?293.
Jeff MacSwan. 2005. Codeswitching and generative
grammar: A critique of the mlf model and some
remarks on ?modified minimalism?. Bilingualism:
language and cognition, 8(01):1?22.
Carol Myers-Scotton. 2002. Contact linguistics:
Bilingual encounters and grammatical outcomes.
Oxford University Press Oxford.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of EMNLP 2013.
Jonathan Owens and Jidda Hassan, 2013. Informa-
tion Structure in Spoken Arabic, chapter Conversa-
tion markers in Arabic-Hausa code-switching, pages
207?243. Routledge Arabic Linguistics. Routledge.
Alexia Panayiotou. 2004. Switching codes, switch-
ing code: Bilinguals? emotional responses in english
and greek. Journal of multilingual and multicultural
development, 25(2-3):124?139.
John C Paolillo. 2001. Language variation on internet
relay chat: A social network approach. Journal of
sociolinguistics, 5(2):180?213.
49
Jaram Park, Young Min Baek, and Meeyoung Cha.
2014. Cross-cultural comparison of nonverbal cues
in emoticons on twitter: Evidence from big data
analysis. Journal of Communication, 64(2):333?
354.
Shana Poplack, David Sankoff, and Christopher Miller.
1988. The social correlates and linguistic processes
of lexical borrowing and assimilation. Linguistics,
26(1):47?104.
Shana Poplack. 1980. Sometimes i?ll start a sentence
in spanish y termino en espanol: toward a typology
of code-switching1. Linguistics, 18(7-8):581?618.
Shana Poplack, 2004. Soziolinguistik. An interna-
tional handbook of the science of language, chapter
Codeswitching, pages 589?597. Walter de Gruyter,
2nd edition.
Shana Poplack. 2013. ?sometimes i?ll start a sentence
in spanish y termino en espa?nol?: Toward a typology
of code-switching. Linguistics, 51(Jubilee):11?14.
Suzanne Romaine. 1995. Bilingualism (2nd edn).
Malden, MA: Blackwell Publishers.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin EP Seligman, et al.
2013. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one, 8(9):e73791.
Carmen Silva-Corval?an. 1994. Language Contact and
Change: Spanish in Los Angeles. ERIC.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Dai Tang, Tina Chou, Naomi Drucker, Adi Robertson,
William C Smith, and Jeffery T Hancock. 2011. A
tale of two languages: strategic self-disclosure via
language selection on facebook. In Proceedings of
the ACM 2011 conference on Computer supported
cooperative work, pages 387?390. ACM.
Sarah Grey Thomason and Terrence Kaufman. 2001.
Language contact. Edinburgh University Press Ed-
inburgh.
Almeida Jacqueline Toribio. 2002. Spanish-english
code-switching among us latinos. International
journal of the sociology of language, pages 89?120.
Liza Tsaliki. 2003. Globalization and hybridity: the
construction of greekness on the internet. The Media
of Diaspora, Routledge, London.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment analy-
sis in social media. In EMNLP, pages 1815?1827.
Li Wei, 1998. Codeswitching in conversation: Lan-
guage, interaction and identity, chapter The ?why?
and ?how? questions in the analysis of conversational
codeswitching, pages 156?176. Routledge.
50
