Proceedings of NAACL HLT 2009: Short Papers, pages 73?76,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Minimum Bayes Risk Combination of Translation Hypotheses from
Alternative Morphological Decompositions
Adria` de Gispert? Sami Virpioja?
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
? Helsinki University of Technology. Adaptive Informatics Research Centre
P.O.Box 5400, 02015 TKK, Finland
{sami.virpioja,mikko.kurimo}@tkk.fi
Mikko Kurimo? William Byrne?
Abstract
We describe a simple strategy to achieve trans-
lation performance improvements by combin-
ing output from identical statistical machine
translation systems trained on alternative mor-
phological decompositions of the source lan-
guage. Combination is done by means of Min-
imum Bayes Risk decoding over a shared N-
best list. When translating into English from
two highly inflected languages such as Ara-
bic and Finnish we obtain significant improve-
ments over simply selecting the best morpho-
logical decomposition.
1 Introduction
Morphologically rich languages pose significant
challenges for natural language processing. The ex-
tensive use of inflection, derivation, and composi-
tion leads to a huge vocabulary, and sparsity in mod-
els estimated from data. Statistical machine transla-
tion (SMT) systems estimated from parallel text are
affected by this. This is particularly acute when ei-
ther the source or the target language, or both, are
morphologically complex.
Owing to these difficulties and to the natural in-
terest researchers take in complex linguistic phe-
nomena, many approaches to morphological anal-
ysis have been developed and evaluated. We fo-
cus on applications to SMT in Section 1.1, but we
note the recent general survey (Roark and Sproat,
2007) and the Morpho Challenge competitive evalu-
ations1. Prior evaluations of morphological analyz-
ers have focused on determining which analyzer was
1See http://www.cis.hut.fi/morphochallenge2009/ and links
best suited for some particular task. For translation,
we take a different approach and investigate whether
competing analyzers might have complementary in-
formation. Our method is straightforward. We train
two identical SMT systems with two versions of
the same parallel corpus, each with a different mor-
phological decomposition of the source language.
We combine their translation hypotheses perform-
ing Minimum Bayes Risk decoding over merged N-
best lists. Results are reported in the NIST 2008
Arabic-to-English MT task and an European Parlia-
ment Finnish-to-English task, with significant gains
over each individual system.
1.1 Prior Work
Several earlier works investigate word segmenta-
tion and transformation schemes, which may include
Part-Of-Speech or other information, to alleviate
the effect of morphological variation on translation
models. With different training corpus sizes, they
focus on translation into English from Arabic (Lee,
2004; Habash and Sadat, 2006; Zollmann et al,
2006), Czech (Goldwater and McClosky, 2005; Tal-
bot and Osborne, 2006), German (Nie?en and Ney,
2004) or Catalan, Spanish and Serbian (Popovic
and Ney, 2004). Some address the generation
challenge when translating from English into Span-
ish (Ueffing and Ney, 2003; de Gispert and Marin?o,
2008). Unsupervised morphology learning is pro-
posed as a language-independent solution to reduce
the problems of rich morphology in (Virpioja et al,
there to earlier workshops. The combination scheme described
in this paper will be one of the evaluation tracks in the upcoming
workshop.
73
Arabic wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp fY dwrthA AlvAnyp wAlxmsyn
MADA D2 w+ qrrt >n tn$A ljnp tHDyryp jAmEp l+ AljmEyp AlEAmp fy dwrthA AlvAnyp w+ Alxmsyn
SAKHR w+ qrrt An tn$A ljnp tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn
English a preparatory committee of the whole of the general assembly is to be established at its fifty-second session
Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration.
2007). Factored models are introduced in (Koehn
and Hoang, 2007) for better integration of morpho-
syntactic information.
Gime?nez and Ma`rquez (2005) merge mul-
tiple word alignments obtained from several
linguistically-tagged versions of a Spanish-English
corpus, but only standard tokens are used in decod-
ing. Dyer et al (2008) report improvements from
multiple Arabic segmentations in translation to En-
glish translation, but their goal was to demonstrate
the value of lattice-based translation. From a model-
ing perspective their approach is unwieldy: multiple
analyses of the parallel text collections are merged
to create a large, heterogeneous training set; a sin-
gle set of models and alignments is produced; lattice
translation is then performed using a single system
to translate all morphological analyses. We find that
similar gains can be obtained much more easily.
The approach we take is Minimum Bayes Risk
(MBR) System Combination (Sim et al, 2007). N-
best lists from multiple SMT systems are merged;
the posterior distributions over the individual lists
are interpolated to form a new distribution over the
merged list. MBR hypotheses selection is then per-
formed using sentence-level BLEU score (Kumar
and Byrne, 2004). It is very likely that even greater
gains can be achieved by more complicated combi-
nation schemes (Rosti et al, 2007), although signif-
icantly more effort in tuning would be required.
2 Arabic-to-English Translation
For Arabic-to-English translation, we consider two
alternative segmentations of the Arabic words. We
first use the MADA toolkit (Habash and Rambow,
2005). After tagging, we split word prefixes and suf-
fixes according to scheme ?D2? (Habash and Sadat,
2006). Secondly, we take the segmentation gener-
ated by Sakhr Software in Egypt using their Arabic
Morphological Tagger, as an alternative segmenta-
tion into subword units. This scheme generates more
tokens as it segments all Arabic articles which other-
wise remain attached in the MADA D2 scheme (Ta-
ble 1).
Translation experiments are based on the NIST
MT08 Arabic-to-English translation task, includ-
ing all allowed parallel data as training material
(?150M English words, and 153M or 178M Arabic
words for MADA-segmented and Sakhr-segmented
text, respectively). In addition to the MT08 set itself,
we take the NIST MT02 through MT05 evaluation
sets and divide them into a development set (odd-
numbered sentences) and a test set (even-numbered
sentences), each containing ?2k sentences.
The SMT system used is HiFST, a hierarchical
phrase-based system implemented with Weighted
Finite-State Transducers (Iglesias et al, 2009). Two
identical systems are trained from each parallel cor-
pus, i.e. MADA-based and SAKHR-based. Both
systems use the same standard features and share
the first-pass English language model, a 4-gram es-
timated over the parallel text and a 965 million word
subset of monolingual data from the English Giga-
word Third Edition. Minimum Error Training pa-
rameter estimation under IBM BLEU is performed
on the development set (mt02-05-tune), and the out-
put translation lattice is rescored with large language
models estimated using ?4.7B words of English
newswire text, in the same fashion as (Iglesias et
al., 2009). Finally, the first 1000-best hypotheses
are rescored with MBR, taking the negative sentence
level BLEU score as the loss function to minimise.
For system combination, we obtain two sets of N-
best lists of depth N=500, one from each system.
Both lists are obtained after large-LM lattice rescor-
ing, i.e. prior to individual MBR. A joint MBR de-
coding is then carried out on the aggregated 1000-
best list with equal weight assigned to the posterior
distribution assigned to the hypotheses by each sys-
tem. Results are shown in Table 2.
As shown, the scores obtained via MBR combi-
nation outperform significantly those achieved via
MBR for the best-performing system (MADA). The
74
mt02-05-
-tune -test mt08
MADA-based 53.3 52.7 43.7
+MBR 53.7 53.3 44.0
SAKHR-based 52.7 52.8 43.3
+MBR 53.2 53.2 43.8
MBR-combined 54.6 54.6 45.6
Table 2: Arabic-to-English translation results. Lower-
cased IBM BLEU reported.
mixed case BLEU-4 for the MBR-combined system
on mt08 is 44.1. This is directly comparable to the
official MT08 Constrained Training Track evalua-
tion results.2
3 Finnish-to-English Translation
Finnish is a highly-inflecting, agglutinative lan-
guage. It has dozens of both inflectional and
derivational suffixes, that are concatenated together
with only moderately small changes in the sur-
face forms. For instance, one can inflect the
word ?kauppa? (shop) into ?kaupa+ssa+mme+kin?
(also in our shop) by glueing the suffixes to the
end. In addition, Finnish has many compound
words, sometimes consisting of several parts, such
as ?ulko+maa+n+kauppa+politiikka? (foreign trade
policy). Due to these properties, the number of dif-
ferent word forms that can be observed is enormous.
Morfessor (Creutz and Lagus, 2007) is a method
for modeling concatenative morphology in an un-
supervised manner. It tries to find morpheme-like
units, morphs, that are segments of the words. In-
spired by the minimum description length principle,
Morfessor tries to find a concise lexicon of morphs
that can effectively code the words in the train-
ing data. Unlike other unsupervised methods (e.g.,
Goldsmith (2001)), there is no restrictions on how
many morphs a word can have. After training the
model, the most likely segmentation of new words
to morphs can be found using the Viterbi algorithm.
There exist a few different versions of Morfessor.
The baseline algorithm has been found to be very
useful in automatic speech recognition of agglutina-
tive languages (Kurimo et al, 2006). However, it
2Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html
often oversegments morphemes that are rare or not
seen at all in the training data. Following the ap-
proach in (Virpioja et al, 2007), we use the Morfes-
sor Categories-MAP algorithm (Creutz and Lagus,
2005). It applies a hierarchical model with three sur-
face categories (prefix, stem and suffix), that allow
the algorithm to treat out-of-vocabulary words in a
convenient manner. For instance, if we encounter a
new name with a known suffix, it can usually sepa-
rate the suffix and leave the actual name intact.
Similarly to the Arabic-to-English task, we train
two identical HiFST systems. In this case, whereas
one is trained on Finnish morphs decomposed by
Morfessor (morph-based), the other is trained on
standard, unprocessed Finnish (word-based). For
this task we use the EuParl parallel corpus . Portions
from Q4/2000 was reserved for testing and Septem-
ber 2000 for development, both containing around
3,000 sentences. The training data comprised 23M
English words, and 17M or 27M Finnish tokens for
word-based or morph-based text, respectively.
The training set was also used to train the mor-
phological segmentation. The quality of the seg-
mentation is evaluated in (Virpioja et al, 2007). A
precision of 78.72% and recall of 52.29% was mea-
sured for the segmentation boundaries with respect
to a linguistic reference segmentation. As the recall
is not very high, the segmentation is more conserva-
tive than the linguistic reference. Table 4 shows an
example for a phrase in the training data.
Results are shown in Table 3, where again signifi-
cant gains are achieved when simply combining out-
put N-best lists via MBR. Only one reference was
available for scoring. In this case we did not ap-
ply large-LM rescoring, as no large additional par-
liamentary data was available. Individual MBR did
not yield gains for each of the systems.
devel test
Word-based 30.2 27.9
Morph-based 29.4 27.4
MBR-combined 30.5 28.9
Table 3: Finnish-to-English translation results. Lower-
cased IBM BLEU reported.
75
Finnish vaarallisten aineiden kuljetusten turvallisuusneuvonantaja
Morfessor vaaraSTM llistenSTM aineSTM idenSUF kuljetusPRE tenSTM turvallisuusPRE neuvoSTM nSUF antajaSTM
Linguistic vaara llis t en aine i den kuljet us t en turva llis uus neuvo n anta ja
English safety adviser for the transport of dangerous goods
Table 4: Example of Morfessor Categories-MAP segmentation and linguistic segmentation for a Finnish phrase. Sub-
scripts show the morph categories given by Morfessor: stem (STM), prefix (PRE) and suffix (SUF).
4 Conclusions
We demonstrated that multiple morphological anal-
yses can be the basis for SMT system combination.
These results will be of interest to researchers devel-
oping morphological analyzers, as it provides a new,
and potentially profitable way to evaluate compet-
ing analysers. The results should also interest SMT
researchers. SMT system combination is an active
area of research, but good gains from combination
usually require very different system architectures;
this can be a barrier to developing competitive sys-
tems. We find that the same architecture trained on
two different analyses is adequate to generate the di-
verse hypotheses needed for system combination.
Acknowledgments. This work was supported by the GALE
program of DARPA (HR0011-06-C-0022), the GSLT and AIRC
in the Academy of Finland, and the EMIME project and PAS-
CAL2 NoE in the EC?s FP7.
References
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Conf. on Adaptive Knowledge Representation
and Reasoning (AKRR).
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learning.
ACM Trans. Speech and Language Processing, 4(1).
A. de Gispert and J.B. Marin?o. 2008. On the impact
of morphology in English to Spanish statistical MT.
Speech Communication, 50.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In ACL-HLT.
J. Gime?nez and Ll. Ma`rquez. 2005. Combining linguis-
tic data views for phrase-based SMT. In ACL Work-
shop on Building and Using Parallel Texts.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
S. Goldwater and D. McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In HLT-
EMNLP.
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological disam-
biguation in one fell swoop. In ACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-
NAACL: Short Papers.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In HLT-NAACL.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In EMNLP.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
NAACL.
M. Kurimo, A. Puurula, E. Arisoy, V. Siivola, T. Hir-
sima?ki, J. Pylkko?nen, T. Aluma?e, and M. Saraclar.
2006. Unlimited vocabulary speech recognition for
agglutinative languages. In HLT-NAACL.
Y.-S. Lee. 2004. Morphological analysis for statistical
machine translation. In HLT-NAACL: Short Papers.
S. Nie?en and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic in-
formation. Computational Linguistics, 30(2).
M. Popovic and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In LREC.
B. Roark and R. Sproat. 2007. Computational Ap-
proaches to Morphology and Syntax. Oxford Univer-
sity Press.
A.V. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im-
proved word-level system combination for machine
translation. In ACL.
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. C. Wood-
land. 2007. Consensus network decoding for sta-
tistical machine translation system combination. In
ICASSP, volume 4.
D. Talbot and M. Osborne. 2006. Modelling lexical re-
dundancy for machine translation. In ACL.
N. Ueffing and H. Ney. 2003. Using POS information for
SMT into morphologically rich languages. In EACL.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine transla-
tion based on morphs induced in an unsupervised man-
ner. In MT Summit XI.
A. Zollmann, A. Venugopal, and S. Vogel. 2006. Bridg-
ing the inflection morphology gap for Arabic statistical
machine translation. In HLT-NAACL: Short Papers.
76
Proceedings of NAACL HLT 2009: Demonstrations, pages 13?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Morpho Challenge - Evaluation of algorithms for unsupervised learning of
morphology in various task s and languages
Mik k o K urimo, S ami V irpioja, V ille T urunen, T eemu H irsima?k i
Adaptive Informatics Research Centre
H elsink i U niversity of T echnolog y
F I- 0 2 0 15 , T K K , F inland
Firstname.Lastname@tkk.fi
A b strac t
After the release of the open sou rce softw are
implementation of M orfessor alg orithm, a se-
ries of several open evalu ations has b een or-
g aniz ed for u nsu pervised morpheme analy -
sis and morpheme-b ased speech recog nition
and information retrieval. T he u nsu pervised
morpheme analy sis is a particu larly attrac-
tive approach for speech and lang u ag e tech-
nolog y for the morpholog ically complex lan-
g u ag es. W hen the amou nt of distinct w ord
forms b ecomes prohib itive for the constru c-
tion of a su ffi cient lex icon, it is important
that the w ords can b e seg mented into smaller
meaning fu l lang u ag e modeling u nits. In this
presentation w e w ill demonstrate the resu lts
of the evalu ations, the b aseline sy stems b u ilt
u sing the open sou rce tools, and invite re-
search g rou ps to participate in the nex t eval-
u ation w here the task is to enhance statistical
machine translation b y morpheme analy sis.
A proposal for a T y pe I I D emo
1 Ex tended A b strac t
1 .1 T he segmentation of w ords into
morphemes
O ne of the fu ndamental task s in natu ral lang u ag e
processing applications, su ch as larg e-vocab u lary
speech recog nition (L V CS R), statistical machine
translation (S M T ) and information retrieval (IR),
is the morpholog ical analy sis of w ords. It is par-
ticu larly important for the morpholog ically com-
plex lang u ag es, w here the amou nt of different
w ord forms is su b stantially increased b y infl ection,
derivation and composition. T he decomposition of
w ords is req u ired not only for u nderstanding the sen-
tence, b u t in many lang u ag es also for ju st represent-
ing the lang u ag e b y any tractab le and trainab le sta-
tistical model and lex icon. T he manu ally composed
ru le-b ased morpholog ical analy z ers can solve these
prob lems to some ex tent, b u t only a fraction of the
ex isting lang u ag es have b een covered so far, and for
many the coverag e of the relevant content is insu ffi -
cient.
T he ob jective of the M orpho Challeng e1 is to de-
sig n and evalu ate new u nsu pervised statistical ma-
chine learning alg orithms that discover w hich mor-
phemes (smallest individu ally meaning fu l u nits of
lang u ag e) w ords consist of. T he g oal is to discover
b asic vocab u lary u nits su itab le for different task s,
su ch as L V CS R, S M T and IR. In u nsu pervised learn-
ing the list of morphemes is not pre-specifi ed for
each lang u ag e, b u t the optimal morpheme lex icon
and morpheme analy sis of all different w ord forms
is statistically optimiz ed from a larg e tex t corpu s in
a completely data-driven manner.
T he evalu ation of the morpheme analy sis alg o-
rithms is performed b oth b y a ling u istic and an ap-
plication oriented task . T he analy sis ob tained for
a long list of w ords is fi rst compared to the lin-
g u istic g old standard representing a g rammatically
correct analy sis b y verify ing that the morpheme-
sharing w ord pairs are the correct ones (K u rimo et
al., 2 0 0 7 ) . T his is repeated in different lang u ag es
and then the ob tained decomposition of w ords is
applied in state-of-the-art sy stems ru nning variou s
1S ee http://w w w .cis.hu t.fi /morphochalleng e2 0 0 9 /
13
NLP applications. The suitability of the morphemes
is v erifi ed by comparing the performance of the sys-
tems to each other and to systems using unprocessed
w ord s or conv entional w ord processing alg orithms
lik e stemming or rule-based d ecompositions.
A s a baseline method in all application, w e hav e
built systems by applying the M orfessor alg orithm,
w hich is an unsuperv ised w ord d ecomposition alg o-
rithm d ev eloped at our research g roup (C reutz and
Lag us, 20 0 2) and released as open source softw are
implementation2.
1.2 Morphemes in Information Retrieval
In information retriev al ( I R ) from tex t d ocuments a
typical task is to look for the most relev ant d ocu-
ments for a g iv en q uery. O ne of the k ey challeng es
is to red uce all the infl ected w ord forms to a common
root or stem for effectiv e ind ex ing . F rom the mor-
pheme analysis point of v iew this task is to d ecom-
pose all the w ord s in the q uery and tex t d ocuments
and fi nd out those common morphemes w hich form
the most relev ant link s.
In M orpho C halleng e the IR systems built using
the unsuperv ised morpheme analysis alg orithms are
compared in state-of-the-art C LE F task s in F innish,
G erman and E ng lish (K urimo and Turunen, 20 0 8 )
using the mean av erag e precision metric. The results
are also compared to those obtained by the g rammat-
ical morphemes as w ell as the stemming and w ord
normaliz ation method s conv entionally used in IR .
1.3 Morphemes in S peec h Rec og nition
In larg e-v ocabulary continuous speech recog nition
(LV C S R ) one k ey part of the process is the statis-
tical lang uag e mod eling w hich d etermines the prior
probabilities of all the possible w ord seq uences. A n
especially challeng ing task is to cov er all the pos-
sible w ord forms w ith suffi cient accuracy, because
any out-of-v ocabulary w ord s w ill not only be nev er
correctly recog niz ed , but also sev erely d eg rad e the
mod eling of the other nearby w ord s. B y d ecompos-
ing the w ord s into meaning ful sub-w ord units, such
as morphemes, larg e-v ocabulary lang uag e mod els
can be successfully built ev en for the most d iffi cult
ag g lutinativ e lang uag es, lik e F innish, E stonian and
Turk ish (K urimo et al, 20 0 6 b).
2S ee http://w w w .cis.hut.fi /projects/morpho/
In M orpho C halleng e the unsuperv ised mor-
pheme alg orithms hav e been compared by using
the morphemes to train statistical lang uag e mod els
and applying the mod els in state-of-the-art LV C S R
task s in F innish and Turk ish (K urimo et al, 20 0 6 a) .
B enchmark s for the same task s w ere obtained by
mod els that utiliz e the g rammatical morphemes as
w ell as trad itional w ord -based lang uag e mod els.
1.4 Morphemes in Mac hine T ranslation
The state-of-the-art statistical machine translation
(S M T) systems are affected by the morpholog ical
v ariation of w ord s at tw o d ifferent stag es (V irpi-
oja et al, 20 0 7 ) . In the fi rst stag e, the alig nment
of the source and targ et lang uag e w ord s in a par-
allel training corpus and the training of the transla-
tion mod el can benefi t from the d ecomposition of
complex w ord s into morphemes. This is particularly
important w hen either the targ et or the source lan-
g uag e, or both, are morpholog ically complex . The
fi nal stag e w here the targ et lang uag e tex t is g ener-
ated , may also req uire morpheme-based mod els, be-
cause the larg e-v ocabulary statistical lang uag e mod -
els are applied in the same w ay as in LV C S R .
In the on-g oing M orpho C halleng e 20 0 9 compe-
tition, the morpheme analysis alg orithms are com-
pared in S M T task s, w here the analysis is need ed
for the source lang uag e tex ts. The E uropean Par-
liament parallel corpus (K oehn, 20 0 5 ) is used in
the ev aluation. The source lang uag es are F innish
and G erman and the targ et in both task s is E ng lish.
To obtain a state-of-the-art performance in the task s
the morpheme-based S M T w ill be combined w ith a
w ord -based S M T using the M inimum B ayes R isk
( M B R ) interpolation of the N-best translation hy-
pothesis of both systems (d e G ispert et al, 20 0 9 ) .
1.5 Morpho C halleng e 20 0 9
A s its pred ecessors, the M orpho C halleng e 20 0 9
competition is open to all and free of charg e. The
participants? are ex pected to use their unsuperv ised
machine learning alg orithms to analyz e the w ord
lists of d ifferent lang uag es prov id ed by the org aniz -
ers and submit the results of their morpheme analy-
sis. The org aniz ers w ill then run the ling uistic ev al-
uations and build the IR and S M T systems and pro-
v id e all the results and comparisons of the d ifferent
systems. The participated alg orithms and ev aluation
14
results will be presented at the Morpho Challenge
work shop that is c urrently planned to tak e plac e
within the H L T - N A A CL 2 0 1 0 c onferenc e.
Acknowledgments
T he Morpho Challenge c om petitions and work shops
are part of the E U N etwork of E x c ellenc e P A S CA L
Challenge program and organiz ed in c ollaboration
with CL E F . W e are grateful to Mathias Creutz , E bru
A risoy , S tefan B ordag, N iz ar H abash and Majdi
S awalha for c ontributions in proc essing the training
data and c reating the gold standards. T he A c adem y
of F inland has supported the work in the projec ts
Adaptive Informatics and N ew adaptive and learn-
ing meth ods in speech recog nition.
R efer ences
M. Creutz and K . L agus. 2 0 0 2 . U nsuperv ised disc ov ery
of m orphem es. In W ork sh op on M orph olog ical and
P h onolog ical L earning of AC L - 0 2 .
A . de G ispert, S . V irpioja, M. K urim o, and W . B y rne.
2 0 0 9 . Minim um B ay es risk c om bination of translation
hy potheses from alternativ e m orphologic al dec om po-
sitions. S ubm itted to H L T - N AAC L .
P . K oehn. 2 0 0 5 . E uroparl: A parallel c orpus for statisti-
c al m ac hine translation. In M T S u mmit X .
M. K urim o and V . T urunen. 2 0 0 8 . U nsuperv ised m or-
phem e analy sis ev aluation by IR ex perim ents ? Mor-
pho Challenge 2 0 0 8 . In C L E F .
M. K urim o, M. Creutz , M. V arjok allio, E . A risoy , and M.
S arac lar. 2 0 0 6 a. U nsuperv ised segm entation of words
into m orphem es - Challenge 2 0 0 5 , an introduc tion and
ev aluation report. In P AS C AL C h alleng e W ork sh op on
U nsu pervised seg mentation of w ords into morph emes.
M. K urim o, A . P uurula, E . A risoy , V . S iiv ola, T . H ir-
sim a?k i, J . P y lk k o?nen, T . A lum a?e, and M. S arac lar.
2 0 0 6 b. U nlim ited v oc abulary speec h rec ognition for
agglutinativ e languages. In H L T - N AAC L .
M. K urim o, M. Creutz , and M. V arjok allio. 2 0 0 7 . Mor-
pho Challenge ev aluation using a linguistic G old S tan-
dard. In C L E F .
S . V irpioja, J . J . V a?y ry nen, M. Creutz , and M. S adeniem i.
2 0 0 7 . Morphology -aware statistic al m ac hine transla-
tion based on m orphs induc ed in an unsuperv ised m an-
ner. In M T S u mmit X I. D enm ark .
2 S cr ip t ou tline for th e demo p r esenta tion
In this dem o we will present the ac hiev em ents of the
Morpho Challenge 2 0 0 5 - 2 0 0 8 c om petition in graphs
and the baseline sy stem s for v arious languages de-
v eloped using the Morfessor algorithm for word de-
c om position, I R , L V CS R and S MT . T he audienc e
will also be welc om e to try their own input for these
baseline sy stem s and v iew the results.
T he sc ript is presented below for a poster-sty le
and try -it- y ourself on laptop dem o, but it will work
well as a lec ture-sty le show, too, if needed.
In the poster we illustrate the following points:
1 . B asic c harac teristic s of the unsuperv ised learn-
ing algorithm s and m orphem e analy sis results
in different languages (F innish, T urk ish, G er-
m an, E nglish, A rabic ) as in T able 1 , dem o:
h ttp://w w w .cis.h u t.fi /projects/morph o/.
2 . T he results of the ev aluations against the lin-
guistic gold standard m orphem es in different
languages, see e.g. F igure 1 .
3 . T he results of the IR ev aluations and c om par-
isons to the perform anc e of gram m atic al m or-
phem es, word-based m ethods and stem m ing in
different languages, see e.g. F igure 2 .
4 . T he results of the L V CS R ev aluations with
c om parisons to gram m atic al m orphem es and
word-based m ethods, see e.g. F igure 3 .
5 . T he c all for partic ipation in the Morpho Chal-
lenge 2 0 0 9 c om petition where the new ev alua-
tion task is using m orphem es in S MT .
F igure 1 : F - m easures for the T urk ish m orphem e analy sis.
T he laptop is used to dem onstrate the baseline
sy stem s we hav e rec ently dev eloped for different
task s that are all based on unsuperv ised m orphem es:
15
Example word M orfes s or an aly s is G old S tan dard
Finnish: lin u xiin lin u x + iin lin u x N + I L L
T u r k ish: popU lerliG in i pop + U + ler + liG in i popU ler + D ER lH g + P O S 2 S + A C C ,
popU ler + D ER lH g + P O S 3 + A C C 3
A r a b ic : A lmtH dp A l+ mtH d + p mu t aH idap P O S :P N A l+ + S G ,
mu t aH id P O S :A J A l+ + S G
G e r m a n: z u ru ec k z u b eh alten z u ru ec k + z u + b e+ h alten z u ru ec k B z u b e h alt V + I N F
E ng lish: b ab y - s itters b ab y - + s itter + s b ab y N s it V er s + P L
T ab le 1 : M orph eme an aly s is examples in differen t lan g u ag es .
F ig u re 2 : P rec is ion performan c es for th e G erman IR .
F ig u re 3 : L V C S R error rates for th e T u rk is h tas k .
1 . O n lin e L V C S R s y s tem for h ig h ly ag g lu tin ativ e
lan g u ag es , s ee e.g . s c reen s h ot in F ig u re 4 .
2 . O n lin e I R s y s tem for h ig h ly ag g lu tin ativ e lan -
g u ag es .
3 . O n lin e S M T s y s tem wh ere th e s ou rc e lan g u ag e
is a h ig h ly ag g lu tin ativ e lan g u ag e, s ee e.g .
s c reen s h ot in F ig u re 5 .
F ig u re 4 : S c reen s h ot of th e morph eme-b as ed s peec h rec -
og n iz er in ac tion for F in n is h . A n offl in e v ers ion c an b e
tried in http://www.cis.hut.fi/projects/speech/.
F ig u re 5 : S c reen s h ot of th e morph eme-b as ed mac h in e
tran s lator in ac tion for F in n is h -En g lis h . A s implifi ed web
in terfac e to th e s y s tem is als o av ailab le (pleas e email to
th e au th ors for a lin k ) .
16
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 157?165,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Web augmentation of language models for continuous speech recognition
of SMS text messages
Mathias Creutz1, Sami Virpioja1,2 and Anna Kovaleva1
1Nokia Research Center, Helsinki, Finland
2Adaptive Informatics Research Centre, Helsinki University of Technology, Espoo, Finland
mathias.creutz@nokia.com, sami.virpioja@tkk.fi, annakov@gmx.de
Abstract
In this paper, we present an efficient query
selection algorithm for the retrieval of web
text data to augment a statistical language
model (LM). The number of retrieved rel-
evant documents is optimized with respect
to the number of queries submitted.
The querying scheme is applied in the do-
main of SMS text messages. Continuous
speech recognition experiments are con-
ducted on three languages: English, Span-
ish, and French. The web data is utilized
for augmenting in-domain LMs in general
and for adapting the LMs to a user-specific
vocabulary. Word error rate reductions
of up to 6.6 % (in LM augmentation) and
26.0 % (in LM adaptation) are obtained in
setups, where the size of the web mixture
LM is limited to the size of the baseline
in-domain LM.
1 Introduction
An automatic speech recognition (ASR) system
consists of acoustic models of speech sounds and
of a statistical language model (LM). The LM
learns the probabilities of word sequences from
text corpora available for training. The perfor-
mance of the model depends on the amount and
style of the text. The more text there is, the better
the model is, in general. It is also important that
the model be trained on text that matches the style
of language used in the ASR application. Well
matching, in-domain, text may be both difficult
and expensive to obtain in the large quantities that
are needed.
A popular solution is to utilize the World Wide
Web as a source of additional text for LM train-
ing. A small in-domain set is used as seed data,
and more data of the same kind is retrieved from
the web. A decade ago, Berger and Miller (1998)
proposed a just-in-time LM that updated the cur-
rent LM by retrieving data from the web using re-
cent recognition hypotheses as queries submitted
to a search engine. Perplexity reductions of up to
10 % were reported.1 Many other works have fol-
lowed. Zhu and Rosenfeld (2001) retrieved page
and phrase counts from the web in order to update
the probabilities of infrequent trigrams that occur
in N-best lists. Word error rate (WER) reductions
of about 3 % were obtained on TREC-7 data.
In more recent work, the focus has turned to
the collection of text rather than n-gram statistics
based on page counts. More effort has been put
into the selection of query strings. Bulyko et al
(2003; 2007) first extend their baseline vocabulary
with words from a small in-domain training cor-
pus. They then use n-grams with these new words
in their web queries in order to retrieve text of a
certain genre. For instance, they succeed in ob-
taining conversational style phrases, such as ?we
were friends but we don?t actually have a relation-
ship.? In a number of experiments, word error
rate reductions of 2-3 % are obtained on English
data, and 6 % on Mandarin. The same method for
web data collection is applied by C?etin and Stolcke
(2005) in meeting and lecture transcription tasks.
The web sources reduce perplexity by 10 % and
4.3 %, respectively, and word error rates by 3.5 %
and 2.2 %, respectively.
Sarikaya et al (2005) chunk the in-domain text
into ?n-gram islands? consisting of only content
words and excluding frequently occurring stop
words. An island such as ?stock fund portfolio? is
then extended by adding context, producing ?my
stock fund portfolio?, for instance. Multiple is-
lands are combined using and and or operations to
form web queries. Significant word error reduc-
tions between 10 and 20 % are obtained; however,
the in-domain data set is very small, 1700 phrases,
1All reported percentage differences are relative unless
explicitly stated otherwise.
157
which makes (any) new data a much needed addi-
tion.
Similarly, Misu and Kawahara (2006) obtain
very good word error reductions (20 %) in spo-
ken dialogue systems for software support and
sightseeing guidance. Nouns that have high tf/idf
scores in the in-domain documents are used in the
web queries. The existing in-domain data sets
poorly match the speaking style of the task and
therefore existing dialogue corpora of different do-
mains are included, which improves the perfor-
mance considerably.
Wan and Hain (2006) select query strings by
comparing the n-gram counts within an in-domain
topic model to the corresponding counts in an out-
of-domain background model. Topic-specific n-
grams are used as queries, and perplexity reduc-
tions of 5.4 % are obtained.
It is customary to postprocess and filter the
downloaded web texts. Sentence boundaries are
detected using some heuristics. Text chunks with a
high out-of-vocabulary (OOV) rate are discarded.
Additionally, the chunks are often ranked accord-
ing to their similarity with the in-domain data, and
the lowest ranked chunks are discarded. As a sim-
ilarity measure, the perplexity of the sentence ac-
cording to the in-domain LM can be used; for in-
stance, Bulyko et al (2007). Another measure
for ranking is relative perplexity (Weilhammer et
al., 2006), where the in-domain perplexity is di-
vided by the perplexity given by an LM trained
on the web data. Also the BLEU score familiar
from the field of machine translation has been used
(Sarikaya et al, 2005).
Some criticism has been raised by Sethy et al
(2007), who claim that sentence ranking has an
inherent bias towards the center of the in-domain
distribution. They propose a data selection algo-
rithm that selects a sentence from the web set, if
adding the sentence to the already selected set re-
duces the relative entropy with respect to the in-
domain data distribution. The algorithm appears
efficient in producing a rather small subset (1/11)
of the web data, while degrading the WER only
marginally.
The current paper describes a new method for
query selection and its applications in LM aug-
mentation and adaptation using web data. The
language models are part of a continuous speech
recognition system that enables users to use
speech as an input modality on mobile devices,
such as mobile phones. The particular domain of
interest is personal communication: The user dic-
tates a message that is automatically transcribed
into text and sent to a recipient as an SMS text
message. Memory consumption and computa-
tional speed are crucial factors in mobile applica-
tions. While most studies ignore the sizes of the
LMs when comparing models, we aim at improv-
ing the LM without increasing its size when web
data is added.
Another aspect that is typically overlooked is
that the collection of web data costs time and com-
putational resources. This applies to the querying,
downloading and postprocessing of the data. The
query selection scheme proposed in this paper is
economical in the sense that it strives to download
as much relevant text from the web as possible us-
ing as few queries as possible avoiding overlap be-
tween the set of pages found by different queries.
2 Query selection and web data retrieval
Our query selection scheme involves multiple
steps. The assumption is that a batch of queries
will be created. These queries are submitted to
a search engine and the matching documents are
downloaded. This procedure is repeated for multi-
ple query batches.
In particular, our scheme attempts to maximize
the number of retrieved relevant documents, when
two restrictions apply: (1) queries are not ?free?:
each query costs some time or money; for in-
stance, the number of queries submitted within a
particular period of time is limited, and (2) the
number of documents retrieved for a particular
query is limited to a particular number of ?top
hits?.
2.1 N-gram selection and prospection
querying
Some text reflecting the target domain must be
available. A set of the most frequent n-grams oc-
curring in the text is selected, from unigrams up to
five-grams. Some of these n-grams are character-
istic of the domain of interest (such as ?Hogwarts
School of Witchcraft and Wizardry?), others are
just frequent in general (?but they did not say?);
we do not know yet which ones.
All n-grams are submitted as queries to the web
search engine. Exact matches of the n-grams are
required; different inflections or matches of the
words individually are not accepted.
158
The search engine returns the total number of
hits h(q
s
) for each query q
s
as well as the URLs
of a predefined maximum number of ?top hit? web
pages. The top hit pages are downloaded and post-
processed into plain text, from which duplicate
paragraphs and paragraphs with a high OOV rate
are removed.
N-gram language models are then trained sep-
arately on the in-domain text and the the filtered
web text. If the amount of web text is very large,
only a subset is used, which consists of the parts
of the web data that are the most similar to the
in-domain text. As a similarity measure, relative
perplexity is used. The LM trained on web data is
called a background LM to distinguish it from the
in-domain LM.
2.2 Focused querying
Next, the querying is made more specific and tar-
geted on the domain of interest. New queries are
created that consist of n-gram pairs, requiring that
a document contain two n-grams (?but they did not
say?+?Hogwarts School of Witchcraft and Wiz-
ardry?).2
If all possible n-gram pairs are formed from
the n-grams selected in Section 2.1, the number
of pairs is very large, and we cannot afford using
them all as queries. Typical approaches for query
selection include the following: (i) select pairs that
include n-grams that are relatively more frequent
in the in-domain text than in the background text,
(ii) use some extra source of knowledge for select-
ing the best pairs.
2.2.1 Extra linguistic knowledge
We first tested the second (ii) query selection ap-
proach by incorporating some simple linguistic
knowledge: In an experiment on English, queries
were obtained by combining a highly frequent n-
gram with a slightly less frequent n-gram that had
to contain a first- or second-person pronoun (I,
you, we, me, us, my, your, our). Such n-grams
were thought to capture direct speech, which is
characteristic for the desired genre of personal
communication. (Similar techniques are reported
in the literature cited in Section 1.)
Although successful for English, this scheme is
more difficult to apply to other languages, where
person is conveyed as verbal suffixes rather than
single words. Linguistic knowledge is needed for
2Higher order tuples could be used as well, but we have
only tested n-gram pairs.
every language, and it turns out that many of the
queries are ?wasted?, because they are too specific
and return only few (if any) documents.
2.2.2 Statistical approach
The other proposed query selection technique (i)
allows for an automatic identification of the n-
grams that are characteristic of the in-domain
genre. If the relative frequency of an n-gram is
higher in the in-domain data than in the back-
ground data, then the n-gram is potentially valu-
able. However, as in the linguistic approach, there
is no guarantee that queries are not wasted, since
the identified n-gram may be very rare on the In-
ternet. Pairing it with some other n-gram (which
may also be rare) often results in very few hits.
To get out the most of the queries, we pro-
pose a query selection algorithm that attempts to
optimize the relevance of the query to the target
domain, but also takes into account the expected
amount of data retrieved by the query. Thus, the
potential queries are ranked according to the ex-
pected number of retrieved relevant documents.
Only the highest ranked pairs, which are likely to
produce the highest number of relevant web pages,
are used as queries.
We denote queries that consist of two n-grams
s and t by q
s?t
. The expected number of retrieved
relevant documents for the query q
s?t
is r(q
s?t
):
r(q
s?t
) = n(q
s?t
) ? ?(q
s?t
|Q), (1)
where n(q
s?t
) is the expected number of retrieved
documents for the query, and ?(q
s?t
|Q) is the ex-
pected proportion of relevant documents within all
documents retrieved by the query. The expected
proportion of relevant documents is a value be-
tween zero and one, and as explained below, it is
dependent on all past queries, the query history Q.
Expected number of retrieved documents
n(q
s?t
). From the prospection querying phase
(Section 2.1), we know the numbers of hits for
the single n-grams s and t, separately: h(q
s
) and
h(q
t
). We make the operational, but overly simpli-
fying, assumption that the n-grams occur evenly
distributed over the web collection, independently
of each other. The expected size of the intersection
q
s?t
is then:
h?(q
s?t
) =
h(q
s
) ? h(q
t
)
N
, (2)
where N is the size of the web collection that our
n-gram selection covers (total number of docu-
159
ments). N is not known, but different estimates
can be used, for instance, N = max
?q
s
h(q
s
),
where it is assumed that the most frequent n-gram
occurs in every document in the collection (prob-
ably an underestimate of the actual value).
Ideally, the expected number of retrieved doc-
uments equals the expected number of hits, but
since the search engine returns a limited maximum
number of ?top hit? pages, M , we get:
n(q
s?t
) = min(h?(q
s?t
),M). (3)
Expected proportion of relevant documents
?(q
s?t
|Q). As in the case of n(q
s?t
), an inde-
pendence assumption can be applied in the deriva-
tion of the expected proportion of relevant docu-
ments for the combined query q
s?t
: We simply
put together the chances of obtaining relevant doc-
uments by the single n-gram queries q
s
and q
t
in-
dividually. The union equals:
?(q
s?t
|Q) =
1 ?
(
1 ? ?(q
s
|Q)
)
?
(
1 ? ?(q
t
|Q)
)
. (4)
However, we do not know the values for
?(q
s
|Q) and ?(q
t
|Q). As mentioned earlier, it is
straightforward to obtain a relevance ranking for a
set of n-grams: For each n-gram s, the LM prob-
ability is computed using both the in-domain and
the background LM. The in-domain probability is
divided by the background probability and the n-
grams are sorted, highest relative probability first.
The first n-gram is much more prominent in the
in-domain than the background data, and we wish
to obtain more text with this crucial n-gram. The
opposite is true for the last n-gram.
We need to transform the ranking into ?(?) val-
ues between zero and one. There is no absolute di-
vision into relevant and irrelevant documents from
the point of view of LM training. We use a proba-
bilistic query ranking scheme, such that we define
that of all documents containing an x% relevant
n-gram, x% are relevant. When the n-grams have
been ranked into a presumed order of relevance,
we decide that the most relevant n-gram is 100 %
relevant and the least relevant n-gram is 0 % rele-
vant; finally, we scale the relevances of the other
n-grams according to rank.
When scoring the remaining n-grams, linear
scaling is avoided, because the majority of the n-
grams are irrelevant or neutral with respect to our
domain of interest, and many of them would ob-
tain fairly high relevance values. Instead, we fix
the relevance value of the ?most domain-neutral?
n-gram (the one with the relative probability value
closest to one); we might assume that only 5 % of
all documents containing this n-gram are indeed
relevant. We then fit a polynomial curve through
the three points with known values (0, 0.05, and 1)
to get the missing ?(?) values for all q
s
.
Decay factor ?(s |Q). We noticed that if con-
stant relevance values are used, the top ranked
queries will consist of a rather small set of top
ranked n-grams that are paired with each other in
all possible combinations. However, it is likely
that each time an n-gram is used in a query, the
need for finding more occurrences of this partic-
ular n-gram decreases. Therefore, we introduced
a decay factor ?(s |Q), by which the initial ?(?)
value, written ?
0
(q
s
), is multiplied:
?(q
s
|Q) = ?
0
(q
s
) ? ?(s |Q), (5)
The decay is exponential:
?(s |Q) = (1 ? )
P
?s?Q
1. (6)
 is a small value between zero and one (for in-
stance 0.05), and ?
?s?Q
1 is the number of times
the n-gram s has occurred in past queries.
Overlap with previous queries. Some queries
are likely to retrieve the same set of documents
as other queries. This occurs if two queries share
one n-gram and there is strong correlation be-
tween the second n-grams (for instance, ?we wish
you?+?Merry Christmas? vs. ?we wish you?+
?and a Happy New Year?). In principle, when as-
sessing the relevance of a query, one should esti-
mate the overlap of that query with all past queries.
We have tested an approximate solution that al-
lows for fast computing. However, the real effect
of this addition was insignificant, and a further de-
scription is omitted in this paper.
Optimal order of the queries. We want to max-
imize the expected number of retrieved relevant
documents while keeping the number of submitted
queries as low as possible. Therefore we sort the
queries best first and submit as many queries we
can afford from the top of the list. However, the
relevance of a query is dependent on the sequence
of past queries (because of the decay factor). Find-
ing the optimal order of the queries takes O(n2)
operations, if n is the total number of queries.
A faster solution is to apply an iterative algo-
rithm: All queries are put in some initial order. For
160
each query, its r(q
s?t
) value is computed accord-
ing to Equation 1. The queries are then rearranged
into the order defined by the new r(?) values, best
first. These two steps are repeated until conver-
gence.
Repeated focused querying. Focused querying
can be run multiple times. Some ten thousands of
the top ranked queries are submitted to the search
engine and the documents matching the queries
are downloaded. A new background LM is trained
using the new web data, and a new round of fo-
cused querying can take place.
2.2.3 Comparison of the linguistic and
statistical focused querying schemes
On one language (German), the statical focused
querying algorithm (Section 2.2.2) was shown
to retrieve 50 % more unique web pages and
70 % more words than the linguistic scheme (Sec-
tion 2.2.1) for the same number of queries. Also
results from language modeling and speech recog-
nition experiments favored statistical querying.
2.3 Web collections obtained
For the speech recognition experiments described
in the current paper, we have collected web texts
for three languages: US English, European Span-
ish, and Canadian French.
As in-domain data we used 230,000 English
text messages (4 million words), 65,000 Spanish
messages (2 million words), and 60,000 French
messages (1 million words). These text messages
were obtained in data collection projects involving
thousand of participants, who used a web interface
to enter messages according to different scenarios
of personal communication situations.3 A few ex-
ample messages are shown in Figure 1.
The queries were submitted to Yahoo!?s web
search engine. The web pages that were retrieved
by the queries were filtered and cleaned and di-
vided into chunks consisting of single paragraphs.
For English, we obtained 210 million paragraphs
and 13 billion words, for Spanish 160 million
paragraphs and 12 billion words, and for French
44 million paragraphs and 3 billion words.
3Real messages sent from mobile phones would be the
best data, but are hard to get because of privacy protection.
The postprocessing of authentic messages would, however,
require proper handling of artifacts resulting from the limited
input capacities on keypads of mobile devices, such as spe-
cific acronyms: i?ll c u l8er. In our setup, we did not have to
face such issues.
I hope you have a long and happy marriage.
Congratulations!
Remember to pick up Billy at practice at five
o?clock!
Hey Eric, how was the trip with the kids over
winter vacation? Did you go to Texas?
Figure 1: Example text messages (US English).
The linguistic focused querying method was ap-
plied in the US English task (because the statisti-
cal method did not yet exist). The Spanish and
Canadian French web collections were obtained
using statistical querying. Since the French set
was smaller than the other sets (?only? 3 billion
words), web crawling was performed, such that
those web sites that had provided us with the most
valuable data (measured by relative perplexity)
were downloaded entirely. As a result, the num-
ber of paragraphs increased to 110 million and the
number of words to 8 billion.
3 Speech Recognition Experiments
We have trained language models on the in-
domain data together with web data, and these
models have been used in speech recognition ex-
periments. Two kinds of experiments have been
performed: (1) the in-domain LM is augmented
with web data, and (2) the LM is adapted to a user-
specific vocabulary utilizing web data as an addi-
tional data source.
One hundred native speakers for each language
were recorded reading held-out subsets of the in-
domain text data. The speech data was partitioned
into training and test sets, such that around one
fourth of the speakers were reserved for testing.
We use a continuous speech recognizer opti-
mized for low memory footprint and fast recog-
nition (Olsen et al, 2008). The recognizer
runs on a server (Core2 2.33 GHz) in about
one fourth of real time. The LM probabilities
are quantized and precompiled together with the
speaker-independent acoustic models (intra-word
triphones) into a finite state transducer (FST).
3.1 Language model augmentation
Each paragraph in the web data is treated as a po-
tential text message and scored according to its
similarity to the in-domain data. Relative perplex-
ity is used as the similarity measure. The para-
graphs are sorted, lowest relative perplexity first,
161
US English
FST size [MB] 10 20 40 70
In-domain 42.7 40.1 39.1 ?
Web mixture 42.0 37.6 35.7 33.8
Ppl reduction [%] 1.6 6.2 8.7 13.6
European Spanish
FST size [MB] 10 20 25 40
In-domain 68.0 64.6 64.3 ?
Web mixture 63.9 58.4 55.0 52.1
Ppl reduction [%] 6.0 9.6 14.5 19.0
Canadian French
FST size [MB] 10 20 25 50
In-domain 57.6 ? ? ?
Web mixture 51.7 47.9 45.9 44.6
Ppl reduction [%] 10.2 16.8 20.3 22.6
Table 1: Perplexities.
In the tables, the perplexity and word error rate reductions of the web mixtures are computed with
respect to the in-domain models of the same size, if such models exist; otherwise the comparison is
made to the largest in-domain model available.
and the highest ranked paragraphs are used as LM
training data. The optimal size of the set depends
on the test, but the largest chosen set contains 15
million paragraphs and 500 million words.
Separate LMs are trained on the in-domain data
and web data. The two LMs are then linearly
interpolated into a mixture model. Roughly the
same interpolation weights (0.5) are obtained for
the LMs, when the optimal value is chosen based
on a held-out in-domain development test set.
3.1.1 Test set perplexities
In Table 1, the prediction abilities of the in-domain
and web mixture language models are compared.
As an evaluation measure we use perplexity cal-
culated on test sets consisting of in-domain text.
The comparison is performed on FSTs of differ-
ent sizes. The FSTs contain the acoustic models,
language model and lexicon, but the LM makes up
for most of the size. The availability of data varies
for the different languages, and therefore the FST
sizes are not exactly the same across languages.
The LMs have been created using the SRI LM
toolkit (Stolcke, 2002). Good-Turing smoothing
with Katz backoff (Katz, 1987) has been used, and
the different model sizes are obtained by pruning
down the full models using entropy-based prun-
ing (Stolcke, 1998). N-gram orders up to five have
been tested: 5-grams always work best on the mix-
US English
FST size [MB] 10 20 40 70
In-domain 17.9 17.5 17.3 ?
Web mixture 17.5 16.7 16.4 15.8
WER reduction 2.2 4.4 5.2 8.4
European Spanish
FST size [MB] 10 20 25 40
In-domain 18.9 18.7 18.6 ?
Web mixture 18.7 17.9 17.4 16.8
WER reduction 1.4 4.1 6.6 9.7
Canadian French
FST size [MB] 10 20 25 50
In-domain 22.6 ? ? ?
Web mixture 22.1 21.7 21.3 20.9
WER reduction 2.3 4.1 5.8 7.5
Table 2: Word error rates [%].
ture models, whereas the best in-domain models
are 4- or 5-grams.
For every language and model size, the web
mixture model performs better than the corre-
sponding in-domain model. The perplexity reduc-
tions obtained increase with the size of the model.
Since it is possible to create larger mixture mod-
els than in-domain models, there are no in-domain
results for the largest model sizes.
Especially if large models can be afforded, the
perplexity reductions are considerable. The largest
improvements are observed for French (between
10.2 % and 22.6 % relative). This is not surprising,
as the French in-domain set is the smallest, which
leaves much room for improvement.
3.1.2 Word error rates
Speech recognition results for the different LMs
are given in Table 2. The results are consistent in
the sense that the web mixture models outperform
the in-domain models, and augmentation helps
more with larger models. The largest word error
rate reduction is observed for the largest Span-
ish model (9.7 % relative). All WER reductions
are statistically significant (one-sided Wilcoxon
signed-rank test; level 0.05) except the 10 MB
Spanish setup.
Although the observed word error rate reduc-
tions are mostly smaller than the corresponding
162
perplexity reductions, the results are actually very
good, when we consider the fact that consider-
able reductions in perplexity may typically trans-
late into meager word error reductions; see, for in-
stance, Rosenfeld (2000), Goodman (2001). This
suggests that the web texts are very welcome com-
plementary data that improve on the robustness of
the recognition.
3.1.3 Modified Kneser-Ney smoothing
In the above experiments, Good-Turing (GT)
smoothing with Katz backoff was used, although
modified Kneser-Ney (KN) interpolation has been
shown to outperform other smoothing methods
(Chen and Goodman, 1999). However, as demon-
strated by Siivola et al (2007), KN smoothing
is not compatible with simple pruning methods
such as entropy-based pruning. In order to make
a meaningful comparison, we used the revised
Kneser pruning and Kneser-Ney growing tech-
niques proposed by Siivola et al (2007). For the
three languages, we built KN models that resulted
in FSTs of the same sizes as the largest GT in-
domain models. The perplexities decreased 4?8%,
but in speech recognition, the improvements were
mostly negligible: the error rates were 17.0 for En-
glish, 18.7 for Spanish, and 22.5 for French.
For English, we also created web mixture mod-
els with KN smoothing. The error rates were 16.5,
15.9 and 15.7 for the 20 MB, 40 MB and 70 MB
models, respectively. Thus, Kneser-Ney outper-
formed Good-Turing, but the improvements were
small, and a statistically significant difference was
measured only for the 40 MB LMs. This was ex-
pected, as it has been observed before that very
simple smoothing techniques can perform well on
large data sets, such as web data (Brants et al,
2007).
For the purpose of demonstrating the usefulness
of our web data retrieval system, we concluded
that there was no significant difference between
GT and KN smoothing in our current setup.
3.2 Language model adaptation
In the second set of experiments we envisage a
system that adapts to the user?s own vocabulary.
Some words that the user needs may not be in-
cluded in the built-in vocabulary of the device,
such as names in the user?s contact list, names of
places or words related to some specific hobby or
other focus of interest.
Two adaptation techniques have been tested:
(1) Unigram adaptation is a simple technique, in
which user-specific words (for instance, names
from the contact list) are added to the vocabulary.
No context information is available, and thus only
unigram probabilities are created for these words.
(2) In message adaptation, the LM is augmented
selectively with paragraphs of web data that con-
tain user-specific words. Now, higher order n-
grams can be estimated, since the words occur
within passages of running text. This idea is not
new: information retrieval has been suggested as a
solution by Bigi et al (2004) among others.
In our message adaptation, we have not created
web queries dynamically on demand. Instead, we
used the large web collections described in Sec-
tion 2.3, from which we selected paragraphs con-
taining user-specific words. We have tested both
adaptation by pooling (adding the paragraphs to
the original training data), and adaptation by in-
terpolation (using the new data to train a sepa-
rate LM, which is interpolated with the original
LM). One million words from the web data were
selected for each language. The adaptation was
thought to take place off-line on a server.
3.2.1 Data sets
For each language, the adaptation takes place on
two baseline models, which are the in-domain
and web mixture LMs of Section 3.1; however,
the amount of in-domain training data is reduced
slightly (as explained below).
In order to evaluate the success of the adapta-
tion, a simulated user-specific test set is created.
This set is obtained by selecting a subset of a
larger potential test set. Words that occur both in
the training set and the potential test set and that
are infrequent in the training set are chosen as the
user-specific vocabulary. For Spanish and French,
a training set frequency threshold of one is used,
resulting in 606 and 275 user-specific words, re-
spectively. For English the threshold is 5, which
results in 99 words. All messages in the potential
test set containing any of these words are selected
into the user-specific test set. Any message con-
taining user-specific words is removed from the
in-domain training set. In this manner, we obtain
a test set with a certain over-representation of a
specific vocabulary, without biasing the word fre-
quency distribution of the training set to any no-
ticeable degree.
For comparison, performance is additionally
computed on a generic in-domain test set, as be-
163
US English, 23 MB models
Model WER (reduction)
user-specific in-domain
In-domain 29.1 (?) 17.9 (?)
+unigram adapt. 24.4 (16.3) 17.1 (4.7)
+message adapt. 21.6 (26.0) 16.8 (6.0)
Web mixture 25.7 (11.8) 16.9 (5.9)
+unigram adapt. 23.1 (20.6) 16.3 (8.8)
+message adapt. 22.2 (23.8) 16.4 (8.5)
European Spanish, 23 MB models
Model WER (reduction)
user-specific in-domain
In-domain 25.3 (?) 18.6 (?)
+unigram adapt. 23.4 (7.7) 18.5 (0.3)
+message adapt. 21.7 (14.4) 18.0 (3.2)
Web mixture 21.9 (13.7) 17.5 (5.8)
+unigram adapt. 21.5 (15.3) 17.7 (5.0)
+message adapt. 21.2 (16.5) 17.7 (4.7)
Canadian French, 21 MB models
Model WER (reduction)
user-specific in-domain
In-domain 30.3 (?) 22.6 (?)
+unigram adapt. 28.3 (6.4) 22.5 (0.4)
+message adapt. 26.6 (12.1) 22.2 (1.8)
Web mixture 26.7 (11.8) 21.4 (5.1)
+unigram adapt. 26.0 (14.3) 21.4 (5.4)
+message adapt. 26.0 (14.2) 21.6 (4.3)
Table 3: Adaptation, word error rates [%]. Six
models have been evaluated on two types of test
sets: a user-specific test set with a higher number
of user-specific words and a generic in-domain test
set. The numbers in brackets are relative WER re-
ductions [%] compared to the in-domain model.
WER values for the unigram adaptation are ren-
dered in italics, if the improvement obtained is sta-
tistically significant compared to the correspond-
ing non-adapted model. WER values for the mes-
sage adaptation are in italics, if there is a statisti-
cally significant reduction with respect to unigram
adaptation.
fore. User-specific and generic development test
sets are used for the estimation of optimal interpo-
lation weights.
3.2.2 Results
The adaptation experiments are summarized in Ta-
ble 3. Only medium sized FSTs (21?23 MB)
have been tested. The two baseline models have
been adapted using the simple unigram reweight-
ing scheme and using selective web message aug-
mentation. For the in-domain baseline, pooling
works the best, that is, adding the web messages
to the original in-domain training set. For the web
mixture baseline, a mixture model is the only op-
tion; that is, one more layer of interpolation is
added.
In the adaptation of the in-domain LMs, mes-
sage selection is almost twice as effective as uni-
gram adaptation for all data sets. Also the perfor-
mance on the generic in-domain test set is slightly
improved, because more training data is available.
Except for English, the best results on the user-
specific test sets are produced by the adaptation of
the web mixture models. The benefit of using mes-
sage adaptation instead of simple unigram adapta-
tion is smaller when we have a web mixture model
as a baseline rather than an in-domain-only LM.
On the generic test sets, the adaptation of the
web mixture makes a difference only for English.
Since there were practically no singleton words
in the English in-domain data, the user-specific
vocabulary consists of words occurring at most
five times. Thus, the English user-specific words
are more frequent than their Spanish and French
equivalents, which shows in larger WER reduc-
tions for English in all types of adaptation.
4 Discussion and conclusion
Mobile applications need to run in small memory,
but not much attention is usually paid to memory
consumption in related LM work. We have shown
that LM augmentation using web data can be suc-
cessful, even when the resulting mixture model is
not allowed to grow any larger than the initial in-
domain model. Yet, the benefit of the web data is
larger, the larger model can be used.
The largest WER reductions were observed in
the adaptation to a user-specific vocabulary. This
can be compared to Misu and Kawahara (2006),
who obtained similar accuracy improvements with
clever selection of web data, when there was ini-
tially no in-domain data available with both the
correct topic and speaking style.
We used relative perplexity ranking to filter the
downloaded web data. More elaborate algorithms
could be exploited, such as the one proposed by
Sethy et al (2007). Initially, we have experi-
mented along those lines, but it did not pay off;
maybe future refinements will be more successful.
164
References
Adam Berger and Robert Miller. 1998. Just-in-time
language modeling. In In ICASSP-98, pages 705?
708.
Brigitte Bigi, Yan Huang, and Renato De Mori. 2004.
Vocabulary and language model adaptation using in-
formation retrieval. In Proc. Interspeech 2004 ? IC-
SLP, pages 1361?1364, Jeju Island, Korea.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858?867.
Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.
2003. Getting more mileage from web text sources
for conversational speech language modeling using
class-dependent mixtures. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 7?
9, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng,
Andreas Stolcke, and ?Ozgu?r C?etin. 2007. Web
resources for language modeling in conversational
speech recognition. ACM Trans. Speech Lang. Pro-
cess., 5(1):1?25.
?Ozgu?r C?etin and Andreas Stolcke. 2005. Lan-
guage modeling in the ICSI-SRI spring 2005 meet-
ing speech recognition evaluation system. Technical
Report 05-006, International Computer Science In-
stitute, Berkeley, CA, USA, July.
S. F. Chen and J. Goodman. 1999. An empirical
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13:359?394.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15:403?434.
Slava M. Katz. 1987. Estimation of probabilities
from sparse data for the language model compo-
nent of a speech recognizer. IEEE Transactions
on Acoustics, Speech and Signal Processing, ASSP-
35(3):400?401, March.
Teruhisa Misu and Tatsuya Kawahara. 2006. A boot-
strapping approach for developing language model
of new spoken dialogue systems by selecting web
texts. In Proc. INTERSPEECH ?06, pages 9?13,
Pittsburgh, PA, USA, September, 17?21.
Jesper Olsen, Yang Cao, Guohong Ding, and Xinxing
Yang. 2008. A decoder for large vocabulary contin-
uous short message dictation on embedded devices.
In Proc. ICASSP 2008, Las Vegas, Nevada.
Ronald Rosenfeld. 2000. Two decades of language
modeling: Where do we go from here? Proceedings
of the IEEE, 88(8):1270?1278.
Ruhi Sarikaya, Augustin Gravano, and Yuqing Gao.
2005. Rapid language model development using ex-
ternal resources for new spoken dialog domains. In
Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP ?05), vol-
ume I, pages 573?576.
Abhinav Sethy, Shrikanth Narayanan, and Bhuvana
Ramabhadran. 2007. Data driven approach for lan-
guage model adaptation using stepwise relative en-
tropy minimization. In Proc. IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP ?07), volume IV, pages 177?180.
Vesa Siivola, Teemu Hirsima?ki, and Sami Virpi-
oja. 2007. On growing and pruning Kneser-
Ney smoothed n-gram models. IEEE Transac-
tions on Audio, Speech and Language Processing,
15(5):1617?1624.
A. Stolcke. 1998. Entropy-based pruning of backoff
language models. In Proc. DARPA BNTU Work-
shop, pages 270?274, Lansdowne, VA, USA.
A. Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proc. ICSLP,
pages 901?904. http://www.speech.sri.com/
projects/srilm/.
Vincent Wan and Thomas Hain. 2006. Strategies for
language model web-data collection. In Proc. IEEE
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP ?06), volume I, pages
1069?1072.
Karl Weilhammer, Matthew N. Stuttle, and Steve
Young. 2006. Bootstrapping language models for
dialogue systems. In Proc. INTERSPEECH 2006
- ICSLP Ninth International Conference on Spo-
ken Language Processing, Pittsburgh, PA, USA,
September 17?21.
Xiaojin Zhu and R. Rosenfeld. 2001. Improving tri-
gram language modeling with the world wide web.
In Proc. IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP ?01).,
volume 1, pages 533?536.
165
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1177?1185, Dublin, Ireland, August 23-29 2014.
Morfessor FlatCat: An HMM-Based Method for Unsupervised and
Semi-Supervised Learning of Morphology
Stig-Arne Gr
?
onroos
1
stig-arne.gronroos@aalto.fi
Sami Virpioja
2
sami.virpioja@aalto.fi
Peter Smit
1
peter.smit@aalto.fi
Mikko Kurimo
1
mikko.kurimo@aalto.fi
1
Department of Signal Processing and Acoustics, Aalto University
2
Department of Information and Computer Science, Aalto University
Abstract
Morfessor is a family of methods for learning morphological segmentations of words based
on unannotated data. We introduce a new variant of Morfessor, FlatCat, that applies a hid-
den Markov model structure. It builds on previous work on Morfessor, sharing model compo-
nents with the popular Morfessor Baseline and Categories-MAP variants. Our experiments show
that while unsupervised FlatCat does not reach the accuracy of Categories-MAP, with semi-
supervised learning it provides state-of-the-art results in the Morpho Challenge 2010 tasks for
English, Finnish, and Turkish.
1 Introduction
Morphological analysis is essential for automatic processing of compounding and highly-inflecting lan-
guages, for which the number of unique word forms may be very large. Apart from rule-based analyzers,
the task has been approached by machine learning methodology. Especially unsupervised methods that
require no linguistic resources have been studied widely (Hammarstr?om and Borin, 2011). Typically
these methods focus on morphological segmentation, i.e., finding morphs, the surface forms of the mor-
phemes.
For language processing applications, unsupervised learning of morphology can provide decent-
quality analyses without resources produced by human experts. However, while morphological ana-
lyzers and large annotated corpora may be expensive to obtain, a small amount of linguistic expertise is
more easily available. A well-informed native speaker of a language can often identify the different pre-
fixes, stems, and suffixes of words. Then the question is how many annotated words makes a difference.
One answer was provided by Kohonen et al. (2010), who showed that already one hundred manually
segmented words provide significant improvements to the quality of the output when comparing to a
linguistic gold standard.
The semi-supervised approach by Kohonen et al. (2010) was based on Morfessor Baseline, the sim-
plest of the Morfessor methods by Creutz and Lagus (2002; 2007). The statistical model of Morfessor
Baseline is simply a categorical distribution of morphs?a unigram model in the terms of statistical lan-
guage modeling. As the semi-supervised Morfessor Baseline outperformed all unsupervised and semi-
supervised methods evaluated in the Morpho Challenge competitions (Kurimo et al., 2010a) so far, the
next question is how the approach works for more complex models.
Another popular variant of Morfessor, Categories-MAP (CatMAP) (Creutz and Lagus, 2005), models
word formation using a hidden Markov model (HMM). The context-sensitivity of the model improves
the precision of the segmentation. For example, it can prevent splitting a single s, a common English
suffix, from the beginning of a word. Moreover, it can disambiguate between identical morphs that are
actually surface forms of different morphemes. Finally, separation of stems and affixes in the output
makes it simple to use the method as a stemmer.
In contrast to Morfessor Baseline, the lexicon of CatMAP is hierarchical: a morph that is already in
the lexicon may be used to encode the forms of other morphs. This has both advantages and drawbacks.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1177
One downside is that it mixes the prior and likelihood components of the cost function, so that the semi-
supervised approach presented by Kohonen et al. (2010) is not usable.
1.1 Hierarchical versus flat lexicons
From the viewpoint of data compression and following the two-part Minimum Description Length prin-
ciple (Rissanen, 1978), Morfessor tries to minimize the number of bits needed to encode both the model
parameters and the training data. Equivalently, the cost function L can be derived from the Maximum a
Posteriori (MAP) estimate:
?
? = argmax
?
P(? |D) = argmin
?
(
? log P(?)? log P(D | ?)
)
= argmin
?
L(?,D), (1)
where ? are the model parameters, D is the training corpus, P(?) is the prior of the parameters and
P(D | ?) is the data likelihood.
In context-independent models such as Morfessor Baseline, the parameters include only the forms and
probabilities of the morphs in the lexicon of the model. Morfessor Baseline and Categories-ML (CatML)
(Creutz and Lagus, 2004) use a flat lexicon, in which the forms of the morphs are encoded directly as
strings: each letter requires a certain number of bits to encode. Thus longer morphs are more expensive.
Encoding a long morph is worthwhile only if the morph is referred to frequently enough from the words
in the training data. If a certain string, let us say segmentation, is common enough in the training data, it
is cost-effective to have it as a whole in the lexicon. Splitting it into two items, segment and ation, would
double the number of pointers from the data, even if those morphs were already in the lexicon. The
undersegmentation of frequent words becomes evident especially if the training data is a corpus instead
of a list of unique word forms.
In contrast, Morfessor CatMAP applies a hierarchical lexicon, which makes use of the morphs that
are already in the lexicon. Instead of encoding the form of segmentation by its 12 letters, we could just
encode the form with two references to the forms of the morphs segment and ation. This may also cause
errors, for example encoding station with st and ation.
The lexicon of Morfessor CatMAP allows but does not force hierarchical encoding for the forms:
each morph has an extra parameter that indicates whether it has a hierarchical representation or not. The
problem of oversegmentation, as in st + ation, is solved using the morph categories. The categories,
which are states of the HMM, include stem, prefix, suffix, and a special non-morpheme category. The
non-morpheme category is intended to catch segments that do not fit well into the three proper morph
categories because they are fragments of a larger morph. In our example, the morph st cannot be a suffix
as it starts the word, it is unlikely to be a prefix as it directly precedes a common suffix ation, and it is
unlikely to be a stem as it is very short. Thus the algorithm is likely to use the non-morpheme state. The
hierarchy is expanded only up to the level in which there are no non-morphemes, so the final analysis is
still station. Without the hierarchy, the non-morphemes have to be removed heuristically, as in CatML
(Creutz and Lagus, 2004).
A hierarchical lexicon presents some challenges to model training. For a standard unigram or HMM
model, if you know the state and emission sequence of the training data, you can directly derive the
maximum likelihood (ML) parameters of the model: a probability of a morph is proportional to the
number of times it is referred to, conditional on the state in the HMM. But if the lexicon is partly
hierarchical, also the references within the lexicon add to the reference counts, and there is no direct way
to find the ML parameters even if the encoding of the training data is known. Similarly, semi-supervised
learning cannot be accomplished simply by adding the counts from an annotated data set, as it is not
clear when to use hierarchy instead of segmenting a word directly in the data.
Moreover, for a flat lexicon, the cost function divides into two parts that have opposing optima: the
cost of the data (likelihood) is optimal when there is minimal splitting and the lexicon consists of the
words in the training data, whereas the cost of the model (prior) is optimal when the lexicon is minimal
and consists only of the letters. In consequence, the balance of precision and recall of the segmentation
boundaries can be directly controlled by setting a weight for the data likelihood. Tuning this hyper-
parameter is a very simple form of supervision, but it has drastic effects on the segmentation results
1178
(Kohonen et al., 2010). A direct control of the balance may also be useful for some applications: Virpioja
et al. (2011) found that the performance of the segmentation algorithms in machine translation correlates
more with the precision than the recall. The weighting approach does not work for hierarchical lexicons,
for which changing the weight does not directly affect the decision whether to encode the morph with
hierarchy or not.
1.2 Morfessor FlatCat
In this paper, we introduce a new member to the Morfessor family, Morfessor FlatCat. As indicated by its
name, FlatCat uses a flat lexicon. Our hypothesis is that enabling semi-supervised learning is effective in
compensating for the undersegmentation caused by the lack of hierarchy. In particular, semi-supervised
learning can improve modeling of suffixation. In the examined languages, suffixes tend to serve syntactic
purposes, such as marking case, tense, person or number. Examples are the suffix s marking tense and
person in she writes and number in stations. Thus the suffix class is closed and has only a small number
of morphemes compared to the prefix and stem categories. As a consequence, a large coverage of suffixes
can be achieved already with a relatively small annotated data set.
The basic model of morphotactics in FlatCat is the same as in the CatML and CatMAP variants: a
hidden Markov model with states that correspond to a word boundary and four morph categories: stem,
prefix, suffix, and non-morpheme. As in CatML, we apply heuristics for removal of non-morphemes
from the final segmentation. However, because FlatCat uses MAP estimation of the parameters, these
heuristics are not necessary during the training for controlling the model complexity, but merely used as
a post-processing step to get meaningful categories.
Modeling of morphotactics improves the segmentation of compound words, by allowing the overall
level of segmentation to be increased without increasing the number of correct morphs used in incorrect
positions. As the benefits of semi-supervised learning and improved morphotactics are likely to com-
plement each other, we can expect improved performance over the semi-supervised Morfessor Baseline
method. By experimental comparison to the previous Morfessor variants, we are able to shed more light
on the effects of using an HMM versus unigram model for morphotactics, using a hierarchical versus flat
lexicon, and exploiting small amounts of annotated training data.
2 FlatCat model and algorithms
Morfessor FlatCat uses components from the older Morfessor variants. Instead of going through all the
details, we refer to the previous work and highlight only the differences. Common components between
Morfessor methods are summarized in Table 1.
As a generative model, Morfessor FlatCat describes the joint distribution P(A,W | ?) of words and
their analyses. The wordsW are observed, but their analyses, A, is a latent variable in the model. An
analysis of a word contains its morphs and morph categories: prefix, stem, suffix, and non-morpheme.
As marginalizing over all possible analyses is generally infeasible, point estimates are used during the
training. The likelihood conditioned on the current analyses is
P(D |A, ?) =
|D|
?
j=1
P(A
j
| ?). (2)
If m
i
are the morphs in A
j
, c
i
are the hidden states of the HMM corresponding to the categories of the
morphs, and # is the word boundary, P(A
j
| ?) is
P(c
1
|#)
|A
j
|
?
i=1
[
P(m
i
| c
i
) P(c
i+1
| c
i
)
]
P(# | c
|A
j
|
). (3)
Morfessor FlatCat applies an MDL-derived prior designed to control the number of non-zero param-
eters. The prior is otherwise the same as in Morfessor Baseline, but it includes the usage properties
from Morfessor CatMAP: the length of the morph and its right and left perplexity. The perplexity mea-
sures describe the predictability of the contexts in which the morph occurs. The emission probability of
1179
Morfessor method
Component Baseline CatMAP CatML FlatCat
Lexicon type Flat Hierarchy Flat Flat
Morphotactics Unigram HMM HMM HMM
Estimation MAP MAP ML MAP
Semi-supervised Implemented Not implemented Not implemented Implemented
Table 1: Overview of similarities and differences between Morfessor methods.
a morph conditioned on the morph category, P(m | c), is calculated from the properties of the morphs
similarly as in CatMAP.
2.1 Training algorithms
The parameters are optimized using a local search. Only a part of the parameters are optimized in each
step: the parameters that are used in calculating the likelihood of a certain part, unit, of the corpus. Units
vary in complexity, from all occurrences of a certain morph to the occurrences of a morph bigram whose
context fits to certain criteria.
The algorithm tries to simultaneously find the optimal segmentation for the unit and the optimal pa-
rameters consistent with that segmentation:
(A, ?) = argmin
OP(A,?)
{
L(?,A,D)
}
. (4)
The training operators OP define the units changed by the local search and the alternative segmentations
tried for each unit. There are three training operators: split, join and resegment, analogous to the similarly
named stages in CatMAP.
The split operator is applied first. It targets all occurrences of a specific morph in the corpus simultane-
ously, attempting to split it into two parts. The whole corpus is processed by sorting the current morphs
by length from shortest to longest.
The second operator attempts to join morph bigrams, grouped by the position of the bigram in the
word. The position grouped bigram counts are sorted by frequency, from most to least common.
Finally, resegmenting uses the generalized Viterbi algorithm to find the currently optimal segmentation
for one whole word at a time. This operator targets each corpus word in increasing order of frequency.
The heuristics used in FlatCat to remove non-morphemes from the final segmentation are the fol-
lowing: All consequent non-morphemes are joined together. If the resulting morph is longer than 4
characters, it is accepted as a stem. All non-morphemes preceded by a suffix and followed by only suf-
fixes or other non-morphemes are recategorized as suffixes without joining with their neighbors. If any
short non-morphemes remain, they are joined either to the preceding or following morphs (the latter only
for those in the initial position).
2.2 Semi-supervised learning
Kohonen et al. (2010) found that semi-supervised learning of Morfessor models was not effective by
only fixing the values of the analysis A for the annotated samplesD
A
. Their solution was to introduce
corpus likelihood weights ? and ?, one for the unannotated data set and one for the annotated data set.
Thus, instead of optimizing the MAP estimate, Kohonen et al. (2010) minimize the cost
L(?,A,D,D
A
) = ? log P(?)? ? log P(D |A, ?)? ? log P(D
A
|A, ?). (5)
The weights can be tuned on a development set. We use the same scheme for FlatCat.
The likelihood of the annotated data is calculated using the same HMM that is used for the unannotated
data. The morph properties are estimated only from the unannotated data. To ensure that the morphs
required for the annotated data can be emitted, a copy of each word in the annotations is added to the
1180
(a) English.
Method ? ? Pre Rec F
U Baseline 1.0 ? .88 .59 .71
U CatMAP ? ? .89 .51 .65
U FlatCat 1.0 ? .90 .57 .69
W Baseline 0.7 ? .83 .62 .71
W FlatCat 0.5 ? .84 .60 .70
SS Baseline 1.0 3000 .83 .77 .80
SS FlatCat 0.9 2000 .86 .76 .81
SS CRF+FlatCat 0.9 2000 .87 .77 .82
S CRF ? ? .92 .73 .81
(b) Finnish.
Method ? ? Pre Rec F
U Baseline 1.0 ? .84 .38 .53
U CatMAP ? ? .76 .51 .61
U FlatCat 1.0 ? .84 .38 .52
W Baseline .02 ? .62 .54 .58
W FlatCat .015 ? .66 .52 .58
SS Baseline .1 15000 .75 .72 .73
SS FlatCat .2 1500 .79 .71 .75
SS CRF+FlatCat .2 2500 .82 .76 .79
S CRF ? ? .88 .74 .80
Table 2: Boundary Precision and Recall results in comparison to gold standard segmentation. Abbrevi-
ations have been used for Unsupervised (U), likelihood weighted (W), semi-supervised (SS) and fully
supervised (S) methods. Best results for each measure have been hilighted using boldface.
unannotated data. This unannotated copy is loosely linked to the annotated word: operations that would
result in the removal of a morph required for the annotations from the lexicon cannot be selected, as such
an operation would have infinite cost.
3 Experiments
We compare Morfessor FlatCat
1
to two previous Morfessor methods and a fully supervised discrimi-
native segmentation method. The Morfessor methods used as references are the CatMAP
2
and Base-
line
3
implementations by Creutz and Lagus (2005) and Virpioja et al. (2013), respectively. Virpioja et
al. (2013) implements the semi-supervised method described by Kohonen et al. (2010). For a super-
vised discriminative model, we use a character-level conditional random field (CRF) implementation by
Ruokolainen et al. (2013)
4
.
We use the English, Finnish and Turkish data sets from Morpho Challenge 2010 (Kurimo et al.,
2010b). They include large unannotated word lists, one thousand annotated words for training, 700?
800 annotated words for parameter tuning, and 10? 1000 annotated words for testing.
For evalution, we use the BPR score by Virpioja et al. (2011). The score calculates the precision (Pre),
recall (Rec), and F
1
-score (F) of the predicted morph boundaries compared to a linguistic gold standard.
In the presence of alternative gold standard analyses, we weight each alternative equally.
We also report the mean average precision from the English and Finnish information retrieval (IR)
tasks of the Morpho Challenge. The Lemur Toolkit (Ogilvie and Callan, 2001) with Okapi BM25 rank-
ing was used. The Finnish data consists of 55K documents, 50 test queries and 23K binary relevance
assessments. The English data consists of 170K documents, 50 test queries and 20K binary relevance as-
sessments. The domain of both data sets is short newspaper articles. All word forms in both the corpora
and the queries were replaced by the morphological segmentation to be evaluated.
Morfessor FlatCat is a pipeline method that refines an initial segmentation given as input. We try two
different initializations for the semi-supervised setting: initializing with the segmentation produced by
semi-supervised Morfessor Baseline, and initializing with the CRF segmentation. All unsupervised and
likelihood-weighted results are initialized with the corresponding Baseline output.
All methods were trained using word types. The weight and perplexity threshold parameters were
optimized separately for each method, using a grid search with the held-out data set. The supervised
CRF method was trained using the one thousand word annotated training data set.
1
Available at https://github.com/aalto-speech/flatcat
2
Available at http://www.cis.hut.fi/projects/morpho/morfessorcatmap.shtml
3
Available at https://github.com/aalto-speech/morfessor
4
Available at http://users.ics.aalto.fi/tpruokol/
1181
Method ? ? Pre Rec F
U Baseline 1.0 ? .85 .36 .51
U CatMAP ? ? .83 .50 .62
U FlatCat 1.0 ? .87 .36 .51
W Baseline 0.1 ? .71 .41 .52
W FlatCat 0.3 ? .88 .38 .53
SS Baseline 0.4 2000 .86 .60 .71
SS FlatCat 0.8 2666 .87 .59 .70
SS CRF+FlatCat 1.0 3000 .87 .61 .72
S CRF ? ? .89 .58 .70
Table 3: Boundary Precision and Recall results in comparison to gold standard segmentation for Turkish.
Abbreviations have been used for Unsupervised (U), likelihood weighted (W), semi-supervised (SS) and
fully supervised (S) methods. Best results for each measure have been hilighted using boldface.
3.1 Comparison to linguistic gold standards
The results of the BPR evaluations are shown in Tables 2 (English, Finnish) and 3 (Turkish). Semi-
supervised FlatCat initialized using CRF achieves the highest F-score for both the English and Turkish
data sets. The difference between the highest and second-highest scoring methods is statistically signifi-
cant for Finnish and Turkish, but not for English (Wilcoxon signed-rank test, p < 0.01).
Table 4 shows BPR for subsets of words consisting of different morph category patterns. Each subset
consists of 500 words from the English or Finnish gold standard, with one of five selected morph patterns
as the only valid analysis. The subsets consist of words with the following morph patterns: words that
should not be segmented (STM), compound words consisting of exactly two stems (STM + STM), a
prefix followed by a stem (PRE + STM), a stem followed by a single suffix (STM + SUF) and a stem
and exactly two suffixes (STM + SUF + SUF). For the STM pattern only precision is reported, as recall
is not defined for an empty set of true boundaries.
The fact that semi-supervised FlatCat compares well against CatMAP in recall, for all morph patterns
and for the test set as a whole, indicates that supervision indeed is effective in compensating for the
undersegmentation caused by the lack of hierarchy in the lexicon. The benefit of modeling morphotactics
can be seen in improved precision for the STM + STM (for English and Finnish) and PRE + STM (for
Finnish) patterns when comparing against semi-supervised Baseline. The more aggressive segmentation
of Baseline gives better results for the English PRE + STM subset than for Finnish due to the shortness
of the English prefixes (on average 3.6 letters for the English and 5.3 for the Finnish subset). While
not directly observable in Table 4, a large part of the improvement over semi-supervised Baseline is
explained by that FlatCat does not use suffix-like morphs in incorrect positions.
Initializing the FlatCat model with CRF segmentation improves the F-scores in all subsets compared
to the initialization with Morfessor Baseline. While FlatCat cannot keep the accuracy of the suffix
boundaries at as high level as CRF, it clearly improves the stem splitting.
3.2 Information retrieval
Stemming has been shown to improve IR results (Kurimo et al., 2009), by removing inflection that is
often not relevant to the query. The morph categories make it possible to simulate stemming by removing
morphs categorized as prefixes or suffixes. As longer affixes are more likely to be meaningful, we limited
the affix removal to morphs of at most 3 letters. For methods that use morph categories, we report two
IR results: the first using all the data and the second with short affix removal (SAR) applied.
In the IR results, we include the topline methods from Morpho Challenge: Snowball Porter stemmer
(Porter, 1980) for English and ?TWOL first? for Finnish. The latter selects the lemma from the first
of the possible analyses given by the morphological analyzer FINTWOL (Lingsoft, Inc.) based on the
1182
(a) English.
STM STM + STM PRE + STM STM + SUF STM + SUF + SUF
Method Pre Pre Rec F Pre Rec F Pre Rec F Pre Rec F
U CatMAP .90 .94 .63 .75 .91 .64 .75 .87 .45 .59 .90 .51 .65
SS Baseline .64 .93 .77 .84 .82 .74 .77 .83 .86 .84 .91 .79 .85
SS FlatCat .68 .94 .65 .77 .78 .62 .69 .86 .88 .87 .94 .79 .86
SS CRF+FlatCat .68 .95 .78 .86 .78 .66 .72 .87 .89 .88 .94 .80 .87
S CRF .78 .94 .72 .81 .85 .59 .69 .92 .91 .91 .95 .82 .88
(b) Finnish.
STM STM + STM PRE + STM STM + SUF STM + SUF + SUF
Method Pre Pre Rec F Pre Rec F Pre Rec F Pre Rec F
U CatMAP .77 .90 .97 .94 .88 .96 .92 .67 .46 .54 .68 .38 .49
SS Baseline .50 .82 .88 .85 .73 .83 .78 .64 .85 .73 .76 .78 .77
SS FlatCat .49 .91 .95 .93 .80 .89 .85 .67 .84 .75 .77 .75 .76
SS CRF+FlatCat .53 .91 .96 .94 .84 .94 .88 .71 .88 .79 .80 .79 .79
S CRF .68 .88 .91 .89 .90 .91 .91 .83 .91 .87 .91 .85 .88
Table 4: Results of BPR experiments with different morph category patterns. Best results for each
measure have been hilighted using boldface.
two-level model by Koskenniemi (1983). As baseline results we also include unsegmented word forms
and truncating each word after the first five letters (First 5).
The results of the IR experiment are shown in Table 5. FlatCat provides the highest score for Finnish.
The English scores are similar to those of the semi-supervised Baseline. FlatCat performs better than
CRF for both languages. This is explained by the higher level of consistency in the segmentations
produced by FlatCat, which makes the resulting morphs more useful as query terms. The number of
morphs in the lexicons of FlatCat initialized using CRF are 108 391 (English), 46 123 (Finnish) and
74 193 (Turkish), which is much smaller than the respective morph lexicon sizes counted from the CRF
segmentation: 339 682 (English), 396 869 (Finnish) and 182 356 (Turkish). This decrease in lexicon
size indicates a more structured segmentation.
The IR performance of semi-supervised FlatCat benefits from the removal of short affixes for English
when initialized by CRF, and Finnish for both initializations. It also improves the results of unsupervised
FlatCat and CatMAP for Finnish, but lowers the precision for English. A possible explanation is that the
unsupervised methods do not analyze the suffixes with a high enough accuracy.
4 Conclusions
We have introduced a new variant of the Morfessor method, Morfessor FlatCat. It predicts both morphs
and their categories based on unannotated data, but also annotated training data can be provided. It was
shown to outperform earlier Morfessor methods in the semi-supervised learning task for English, Finnish
and Turkish.
The purely supervised CRF-based segmentation method proposed by Ruokolainen et al. (2013) outper-
forms FlatCat for Finnish and reaches the same level for English. However, we show that a discriminative
model such as CRF gives inconsistent segmentations that do not work as well in a practical application:
In English and Finnish information retrieval tasks, FlatCat clearly outperformed the CRF-based segmen-
tation.
We see two major directions for future work. Currently Morfessor FlatCat, like most Morfessor meth-
ods, assumes that words in a sentence occur independently. Making use of the sentence context in which
words occur would, however, allow making Part-Of-Speech -like distinctions. These distinctions could
1183
(a) English.
Rank Method SAR MAP
1 ? Snowball Porter ? 0.4092
2 SS Baseline ? 0.3855
3 SS FlatCat No 0.3837
4 SS FlatCat Yes 0.3821
5 SS CRF+FlatCat Yes 0.3810
6 SS CRF+FlatCat No 0.3788
7 S CRF ? 0.3771
8 W Baseline ? 0.3761
9 U Baseline ? 0.3695
10 U CatMAP No 0.3682
11 U CatMAP Yes 0.3653
12 W FlatCat No 0.3651
13 ? (First 5) ? 0.3648
14 W FlatCat Yes 0.3606
15 U FlatCat No 0.3486
16 U FlatCat Yes 0.3451
17 ? (Words) ? 0.3303
(b) Finnish.
Rank Method SAR MAP
1 W FlatCat No 0.5057
2 W FlatCat Yes 0.5029
3 SS FlatCat Yes 0.4987
4 ? TWOL first ? 0.4973
5 SS CRF+FlatCat Yes 0.4912
6 U CatMAP Yes 0.4884
7 U CatMAP No 0.4865
8 SS CRF+FlatCat No 0.4826
9 SS FlatCat No 0.4821
10 ? (First 5) ? 0.4757
11 SS Baseline ? 0.4722
12 S CRF ? 0.4660
13 W Baseline ? 0.4582
14 U Baseline ? 0.4378
15 U FlatCat Yes 0.4349
16 U FlatCat No 0.4334
17 ? (Words) ? 0.3483
Table 5: Information Retrieval results. Results of the method presented in this paper are hilighted using
boldface. Mean Average Precision is abbreviated as MAP. Short affix removal is abbreviated as SAR.
help disambiguate inflections of different lexemes that have the same surface form but should be analyzed
differently (Can and Manandhar, 2013).
The second direction is removal of the assumption that a morphology consists only of concatenative
processes. Introducing transformations to model allomorphy in a similar manner as Kohonen et al.
(2009) would allow finding the shared abstract morphemes underlying different allomorphs. This could
be especially beneficial in information retrieval and machine translation applications.
Acknowledgments
This research has been supported by European Community?s Seventh Framework Programme
(FP7/2007?2013) under grant agreement n?287678 and the Academy of Finland under the Finnish Cen-
tre of Excellence Program 2012?2017 (grant n?251170) and the LASTU Programme (grants n?256887
and 259934). The experiments were performed using computer resources within the Aalto University
School of Science ?Science-IT? project. We thank Teemu Ruokolainen for his help with the experiments.
References
Burcu Can and Suresh Manandhar. 2013. Dirichlet processes for joint learning of morphology and PoS tags. In
Proceedings of the International Joint Conference on Natural Language Processing, pages 1087?1091, Nagoya,
Japan, October.
Mathias Creutz and Krista Lagus. 2002. Unsupervised discovery of morphemes. In Mike Maxwell, editor,
Proceedings of the ACL-02Workshop onMorphological and Phonological Learning, pages 21?30, Philadelphia,
PA, USA, July. Association for Computational Linguistics.
Mathias Creutz and Krista Lagus. 2004. Induction of a simple morphology for highly-inflecting languages. In
Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology, pages
43?51, Barcelona, Spain, July. Association for Computational Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unanno-
tated text. In Timo Honkela, Ville K?on?onen, Matti P?oll?a, and Olli Simula, editors, Proceedings of AKRR?05,
1184
International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning, pages
106?113, Espoo, Finland, June. Helsinki University of Technology, Laboratory of Computer and Information
Science.
Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language Processing, 4(1):3:1?3:34, January.
Harald Hammarstr?om and Lars Borin. 2011. Unsupervised learning of morphology. Computational Linguistics,
37(2):309?350, June.
Oskar Kohonen, Sami Virpioja, and Mikaela Klami. 2009. Allomorfessor: Towards unsupervised morpheme
analysis. In Evaluating Systems for Multilingual and Multimodal Information Access: 9th Workshop of the
Cross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17?19, 2008, Revised Selected
Papers, volume 5706 of Lecture Notes in Computer Science, pages 975?982. Springer Berlin / Heidelberg,
September.
Oskar Kohonen, Sami Virpioja, and Krista Lagus. 2010. Semi-supervised learning of concatenative morphology.
In Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and
Phonology, pages 78?86, Uppsala, Sweden, July. Association for Computational Linguistics.
Kimmo Koskenniemi. 1983. Two-level morphology: A general computational model for word-form recognition
and production. Ph.D. thesis, University of Helsinki.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne. 2009. Overview and
results of Morpho Challenge 2009. In Working Notes for the CLEF 2009 Workshop, Corfu, Greece, September.
Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus. 2010a. Morpho Challenge 2005-2010: Eval-
uations and results. In Jeffrey Heinz, Lynne Cahill, and Richard Wicentowski, editors, Proceedings of the
11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 87?95,
Uppsala, Sweden, July. Association for Computational Linguistics.
Mikko Kurimo, Sami Virpioja, and Ville T. Turunen. 2010b. Overview and results of Morpho Challenge 2010. In
Proceedings of the Morpho Challenge 2010 Workshop, pages 7?24, Espoo, Finland, September. Aalto Univer-
sity School of Science and Technology, Department of Information and Computer Science. Technical Report
TKK-ICS-R37.
Paul Ogilvie and James P Callan. 2001. Experiments using the Lemur toolkit. In TREC, volume 10, pages
103?108.
Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130?137.
Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465?471.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja, and Mikko Kurimo. 2013. Supervised morphological seg-
mentation in a low-resource learning setting using conditional random fields. In Proceedings of the Seventeenth
Conference on Computational Natural Language Learning, pages 29?37, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Sami Virpioja, Ville T Turunen, Sebastian Spiegler, Oskar Kohonen, and Mikko Kurimo. 2011. Empirical com-
parison of evaluation methods for unsupervised learning of morphology. Traitement Automatique des Langues,
52(2):45?90.
Sami Virpioja, Peter Smit, Stig-Arne Gr?onroos, and Mikko Kurimo. 2013. Morfessor 2.0: Python implementation
and extensions for Morfessor Baseline. Report 25/2013 in Aalto University publication series SCIENCE +
TECHNOLOGY, Department of Signal Processing and Acoustics, Aalto University.
1185
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 21?24,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Morfessor 2.0: Toolkit for statistical morphological segmentation
Peter Smit
1
peter.smit@aalto.fi
Sami Virpioja
2
sami.virpioja@aalto.fi
Stig-Arne Gr
?
onroos
1
stig-arne.gronroos@aalto.fi
Mikko Kurimo
1
mikko.kurimo@aalto.fi
1
Department of Signal Processing and Acoustics, Aalto University
2
Department of Information and Computer Science, Aalto University
Abstract
Morfessor is a family of probabilistic ma-
chine learning methods for finding the
morphological segmentation from raw text
data. Recent developments include the de-
velopment of semi-supervised methods for
utilizing annotated data. Morfessor 2.0
is a rewrite of the original, widely-used
Morfessor 1.0 software, with well docu-
mented command-line tools and library in-
terface. It includes new features such as
semi-supervised learning, online training,
and integrated evaluation code.
1 Introduction
In the morphological segmentation task, the goal
is to segment words into morphemes, the small-
est meaning-carrying units. Morfessor is a family
of methods for unsupervised morphological seg-
mentation. The first version of Morfessor, called
Morfessor Baseline, was developed by Creutz and
Lagus (2002) its software implementation, Mor-
fessor 1.0, released by Creutz and Lagus (2005b).
A number of Morfessor variants have been devel-
oped later, including Morfessor Categories-MAP
(Creutz and Lagus, 2005a) and Allomorfessor
(Virpioja et al., 2010). Even though these algo-
rithms improve Morfessor Baseline in some areas,
the Baseline version has stayed popular as a gener-
ally applicable morphological analyzer (Spiegler
et al., 2008; Monson et al., 2010).
Over the past years, Morfessor has been used
for a wide range of languages and applications.
The applications include large vocabulary contin-
uous speech recognition (e.g. Hirsim?aki et al.,
2006), machine translation (e.g. Virpioja et al.,
2007), and speech retrieval (e.g. Arisoy et al.,
2009). Morfessor is well-suited for languages with
concatenative morphology, and the tested lan-
guages include Finnish and Estonian (Hirsim?aki
et al., 2009), German (El-Desoky Mousa et al.,
2010), and Turkish (Arisoy et al., 2009).
Morfessor 2.0 is a new implementation of the
Morfessor Baseline algorithm.
1
It has been writ-
ten in a modular manner and released as an open
source project with a permissive license to encour-
age extensions. This paper includes a summary of
the Morfessor 2.0 software and a description of the
demonstrations that will be held. An extensive de-
scription of the features in Morfessor 2.0, includ-
ing experiments, is available in the report by Vir-
pioja et al. (2013).
2 Morfessor model and algorithms
Models of the Morfessor family are generative
probabilistic models that predict compounds and
their analyses (segmentations) given the model pa-
rameters. We provide a brief overview of the
methodology; Virpioja et al. (2013) should be re-
ferred to for the complete formulas and description
of the model and its training algorithms.
Unlike older Morfessor implementations, Mor-
fessor 2.0 is agnostic in regard to the actual data
being segmented. In addition to morphological
segmentation, it can handle, for example, sentence
chunking. To reflect this we use the following
generic terms: The smallest unit that can be split
will be an atom (letter). A compound (word) is a
sequence of atoms. A construction (morph) is a
sequence of atoms contained inside a compound.
2.1 Model and cost function
The cost function of Morfessor Baseline is derived
using maximum a posteriori estimation. That is,
the goal is to find the most likely parameters ?
1
Morfessor 2.0 can be downloaded from the Mor-
pho project website (http://www.cis.hut.fi/
projects/morpho/) or GitHub repository (https:
//github.com/aalto-speech/morfessor).
21
given the observed training dataD
W
:
?
MAP
= argmax
?
p(?)p(D
W
|?) (1)
Thus we are maximizing the product of the model
prior p(?) and the data likelihood p(D
W
|?). As
usual, the cost function to minimize is set as the
minus logarithm of the product:
L(?,D
W
) = ? log p(?)? log p(D
W
|?). (2)
During training, the data likelihood is calcu-
lated using a hidden variable that contains the cur-
rent chosen analyses. Secondly, it is assumed that
the constructions in a compound occur indepen-
dently. This simplifies the data likelihood to the
product of all construction probabilities in the cho-
sen analyses. Unlike previous versions, Morfes-
sor 2.0 includes also the probabilities of the com-
pound boundaries in the data likelihood.
For prior probability, Morfessor Baseline de-
fines a distribution over the lexicon of the model.
The prior assigns higher probability to lexicons
that store fewer and shorter constructions. The
lexicon prior consists of to parts, a product over
the form probabilities and a product over the usage
probabilities. The former includes the probability
of a sequence of atoms and the latter the maxi-
mum likelihood estimates of the constructions. In
contrast to Morfessor 1.0, Morfessor 2.0 currently
supports only an implicit exponential length prior
for the constructions.
2.2 Training and decoding algorithms
A Morfessor model can be trained in multiple
ways. The standard batch training uses a local
search utilizing recursive splitting. The model is
initialized with the compounds and the full model
cost is calculated. The data structures are designed
in such way that the cost is efficient compute dur-
ing the training.
In one epoch of the algorithm, all compounds
in the training data are processed. For each com-
pound, all possible two-part segmentations are
tested. If one of the segmentations yields the low-
est cost, it is selected and the segmentation is tried
recursively on the resulting segments. In each step
of the algorithm, the cost can only decrease or stay
the same, thus guaranteeing convergence. The al-
gorithm is stopped when the cost decreases less
than a configurable threshold value in one epoch.
An extension of the Viterbi algorithm is used
for decoding, that is, finding the optimal segmen-
tations for new compound forms without changing
the model parameters.
3 New features in Morfessor 2.0
3.1 Semi-supervised extensions
One important feature that has been implemented
in Morfessor 2.0 are the semi-supervised exten-
sions as introduced by Kohonen et al. (2010)
Morfessor Baseline tends to undersegment
when the model is trained for morphological seg-
mentation using a large corpus (Creutz and Lagus,
2005b). Oversegmentation or undersegmentation
of the method are easy to control heuristically
by including a weight parameter ? for the likeli-
hood in the cost function. A low ? increases the
priors influence, favoring small construction lexi-
cons, while a high value increases the data likeli-
hood influence, favoring longer constructions.
In semi-supervised Morfessor, the likelihood of
an annotated data set is added to the cost function.
As the amount of annotated data is typically much
lower than the amount of unannotated data, its ef-
fect on the cost function may be very small com-
pared to the likelihood of the unannotated data.
To control the effect of the annotations, a sepa-
rate weight parameter ? can be included for the
annotated data likelihood.
If separate development data set is available for
automatic evaluation of the model, the likelihoods
weights can be optimized to give the best out-
put. This can be done by brute force using a grid
search. However, Morfessor 2.0 implementation
includes a simple heuristic for automatically tun-
ing the value of ? during the training, trying to
balance precision and recall. A simple heuristic,
which gives an equivalent contribution to the an-
notated data, is used for ?.
3.2 On-line training
In addition to the batch training mode, Morfes-
sor 2.0 supports on-line training mode, in which
unannotated text is processed one compound at a
time. This makes it simple to, for example, adapt
pre-trained models for new type of data. As fre-
quent compounds are encountered many times in
running text, Morfessor 2.0 includes an option for
randomly skipping compounds and constructions
that have been recently analyzed. The random
22
Figure 1: Screenshot from the Morfessor 2.0 demo.
skips can also be used to speed up the batch train-
ing.
3.3 Integrated evaluation code
One common method for evaluating the perfor-
mance of a Morfessor model is to compare it
against a gold standard segmentation using seg-
mentation boundary precision and recall. To make
the evaluation easy, the necessary tools for calcu-
lating the BPR metric by (Virpioja et al., 2011)
are included in Morfessor 2.0. For significance
testing when comparing multiple models, we have
included the Wilcoxon signed-rank test. Both the
evaluation code and statistical testing code are ac-
cessible from both the command line and the li-
brary interface.
3.4 N-best segmentation
In order to generate multiple segmentations for a
single compound, Morfessor 2.0 includes a n-best
Viterbi algorithm. It allows extraction of all possi-
ble segmentations for a compound and the proba-
bilities of the segmentations.
4 Demonstration
In the demonstration session, multiple features
and usages of Morfessor will be shown.
4.1 Web-based demonstration
A live demonstration will be given of segmenting
text with Morfessor 2.0 for different language and
training data options. In a web interface, the user
can choose a language, select the size of the train-
ing corpus and other options. After that a word
can be given which will be segmented using n-best
Viterbi, showing the 5 best results.
A list of planned languages can be found in Ta-
ble 1. A screen shot of the demo interface is shown
in Figure 1.
Languages # Words # Word forms
English 62M 384.903
Estonian 212M 3.908.820
Finnish 36M 2.206.719
German 46M 1.266.159
Swedish 1M 92237
Turkish 12M 617.298
Table 1: List of available languages for Morfessor
2.0 demonstration.
4.2 Command line interface
The new command line interface will be demon-
strated to train and evaluate Morfessor models
from texts in different languages. A diagram of
the tools is shown in Figure 2
4.3 Library interface
Interfacing with the Morfessor 2.0 Python library
will be demonstrated for building own scientific
experiments, as well as integrating Morfessor in
23
Training data
Annotation data
morfessor-train
Morfessor
model
Corpus
Gold standard
morfessor-
segment
morfessor-
evaluate
Segmented corpus
BPR-scores
Figure 2: The standard workflow for Morfessor
command line tools
bigger project. Also the code of the Web based
demonstration will be shown as an example.
Acknowledgements
The authors have received funding from the EC?s
7th Framework Programme (FP7/2007?2013) un-
der grant agreement n?287678 and the Academy
of Finland under the Finnish Centre of Excel-
lence Program 2012?2017 (grant n?251170) and
the LASTU Programme (grants n?256887 and
259934). The experiments were performed us-
ing computer resources within the Aalto Univer-
sity School of Science ?Science-IT? project.
References
E. Arisoy, D. Can, S. Parlak, H. Sak, and M. Saraclar.
2009. Turkish broadcast news transcription and re-
trieval. Audio, Speech, and Language Processing,
IEEE Transactions on, 17(5):874?883.
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Mike Maxwell, editor, Pro-
ceedings of the ACL-02 Workshop on Morphological
and Phonological Learning, pages 21?30. Associa-
tion for Computational Linguistics, July.
M. Creutz and K. Lagus. 2005a. Inducing the mor-
phological lexicon of a natural language from unan-
notated text. In Proceedings of AKRR?05, Interna-
tional and Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning, pages
106?113, Espoo, Finland, June. Helsinki University
of Technology.
M. Creutz and K. Lagus. 2005b. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical
Report A81, Publications in Computer and Informa-
tion Science, Helsinki University of Technology.
A. El-Desoky Mousa, M. Ali Basha Shaik, R. Schluter,
and H. Ney. 2010. Sub-lexical language models for
German LVCSR. In Spoken Language Technology
Workshop (SLT), 2010 IEEE, pages 171?176. IEEE.
T. Hirsim?aki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkk?onen. 2006. Unlimited vocabu-
lary speech recognition with morph language mod-
els applied to Finnish. Computer Speech & Lan-
guage, 20(4):515?541.
T. Hirsim?aki, J. Pylkk?onen, and M. Kurimo. 2009.
Importance of high-order n-gram models in morph-
based speech recognition. Audio, Speech, and
Language Processing, IEEE Transactions on,
17(4):724?732.
O. Kohonen, S. Virpioja, and K. Lagus. 2010. Semi-
supervised learning of concatenative morphology.
In Proceedings of the 11th Meeting of the ACL Spe-
cial Interest Group on Computational Morphology
and Phonology, pages 78?86, Uppsala, Sweden,
July. Association for Computational Linguistics.
C. Monson, K. Hollingshead, and B. Roark. 2010.
Simulating morphological analyzers with stochastic
taggers for confidence estimation. In Multilingual
Information Access Evaluation I. Text Retrieval Ex-
periments, pages 649?657. Springer.
S. Spiegler, B. Gol?enia, K. Shalonova, P. Flach, and
R. Tucker. 2008. Learning the morphology of zulu
with different degrees of supervision. In Spoken
Language Technology Workshop, 2008. SLT 2008.
IEEE, pages 9?12. IEEE.
S. Virpioja, J. V?ayrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine trans-
lation based on morphs induced in an unsupervised
manner. In Proceedings of the Machine Translation
Summit XI, pages 491?498, Copenhagen, Denmark,
September.
S. Virpioja, O. Kohonen, and K. Lagus. 2010. Unsu-
pervised morpheme analysis with Allomorfessor. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, volume 6241 of LNCS, pages
609?616. Springer Berlin / Heidelberg.
S. Virpioja, V. Turunen, S. Spiegler, O. Kohonen, and
M. Kurimo. 2011. Empirical comparison of evalua-
tion methods for unsupervised learning of morphol-
ogy. TAL, 52(2):45?90.
S. Virpioja, P. Smit, S. Gr?onroos, and M. Kurimo.
2013. Morfessor 2.0: Python implementation and
extensions for Morfessor Baseline. Report 25/2013
in Aalto University publication series SCIENCE +
TECHNOLOGY, Aalto University, Finland.
24
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84?89,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Painless Semi-Supervised Morphological Segmentation using Conditional
Random Fields
Teemu Ruokolainen
a
Oskar Kohonen
b
Sami Virpioja
b
Mikko Kurimo
a
a
Department of Signal Processing and Acoustics, Aalto University
b
Department of Information and Computer Science, Aalto University
firstname.lastname@aalto.fi
Abstract
We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, that is the surface
forms of morphemes. We extend a re-
cent segmentation approach based on con-
ditional random fields from purely super-
vised to semi-supervised learning by ex-
ploiting available unsupervised segmenta-
tion techniques. We integrate the unsu-
pervised techniques into the conditional
random field model via feature set aug-
mentation. Experiments on three di-
verse languages show that this straight-
forward semi-supervised extension greatly
improves the segmentation accuracy of the
purely supervised CRFs in a computation-
ally efficient manner.
1 Introduction
We discuss data-driven morphological segmenta-
tion, in which word forms are segmented into
morphs, the surface forms of morphemes. This
type of morphological analysis can be useful for
alleviating language model sparsity inherent to
morphologically rich languages (Hirsim?ki et al.,
2006; Creutz et al., 2007; Turunen and Kurimo,
2011; Luong et al., 2013). Particularly, we focus
on a low-resource learning setting, in which only
a small amount of annotated word forms are avail-
able for model training, while unannotated word
forms are available in abundance.
We study morphological segmentation using
conditional random fields (CRFs), a discrimina-
tive model for sequential tagging and segmenta-
tion (Lafferty et al., 2001). Recently, Ruoko-
lainen et al. (2013) showed that the CRFs can
yield competitive segmentation accuracy com-
pared to more complex, previous state-of-the-
art techniques. While CRFs yielded generally
the highest accuracy compared to their reference
methods (Poon et al., 2009; Kohonen et al., 2010),
on the smallest considered annotated data sets of
100 word forms, they were outperformed by the
semi-supervised Morfessor algorithm (Kohonen et
al., 2010). However, Ruokolainen et al. (2013)
trained the CRFs solely on the annotated data,
without any use of the available unannotated data.
In this work, we extend the CRF-based ap-
proach to leverage unannotated data in a straight-
forward and computationally efficient manner via
feature set augmentation, utilizing predictions of
unsupervised segmentation algorithms. Experi-
ments on three diverse languages show that the
semi-supervised extension substantially improves
the segmentation accuracy of the CRFs. The ex-
tension also provides higher accuracies on all the
considered data set sizes and languages compared
to the semi-supervised Morfessor (Kohonen et al.,
2010).
In addition to feature set augmentation, there
exists numerous approaches for semi-supervised
CRF model estimation, exemplified by minimum
entropy regularization (Jiao et al., 2006), gen-
eralized expectations criteria (Mann and McCal-
lum, 2008), and posterior regularization (He et al.,
2013). In this work, we employ the feature-based
approach due to its simplicity and the availabil-
ity of useful unsupervised segmentation methods.
Varying feature set augmentation approaches have
been successfully applied in several related tasks,
such as Chinese word segmentation (Wang et al.,
2011; Sun and Xu, 2011) and chunking (Turian et
al., 2010).
The paper is organized as follows. In Section 2,
we describe the CRF-based morphological seg-
mentation approach following (Ruokolainen et al.,
2013), and then show how to extend this approach
to leverage unannotated data in an efficient man-
ner. Our experimental setup and results are dis-
cussed in Sections 3 and 4, respectively. Finally,
84
we present conclusions on the work in Section 5.
2 Methods
2.1 Supervised Morphological Segmentation
using CRFs
We present the morphological segmentation task
as a sequential labeling problem by assigning each
character to one of three classes, namely {be-
ginning of a multi-character morph (B), middle
of a multi-character morph (M), single character
morph (S)}. We then perform the sequential label-
ing using linear-chain CRFs (Lafferty et al., 2001).
Formally, the linear-chain CRF model distribu-
tion for label sequence y = (y
1
, y
2
, . . . , y
T
) and
a word form x = (x
1
, x
2
, . . . , x
T
) is written as a
conditional probability
p (y |x;w) ?
T
?
t=2
exp
(
w ? ?(y
t?1
, y
t
, x, t)
)
,
(1)
where t indexes the character positions,w denotes
the model parameter vector, and ? the vector-
valued feature extracting function. The model pa-
rameters w are estimated discrimatively based on
a training set of exemplar input-output pairs (x, y)
using, for example, the averaged perceptron algo-
rithm (Collins, 2002). Subsequent to estimation,
the CRF model segments test word forms using
the Viterbi algorithm (Lafferty et al., 2001).
We next describe the feature set
{?
i
(y
t?1
, y
t
, x, t)}
|?|
i=1
by defining emission
and transition features. Denoting the label set {B,
M, S} as Y , the emission feature set is defined as
{?
m
(x, t)1(y
t
= y
?
t
) |m ? 1..M ,?y
?
t
? Y} ,
(2)
where the indicator function 1(y
t
= y
?
t
) returns
one if and only if y
t
= y
?
t
and zero otherwise, that
is
1(y
t
= y
?
t
) =
{
1 if y
t
= y
?
t
0 otherwise
, (3)
and {?
m
(x, t)}
M
m=1
is the set of functions describ-
ing the character position t. Following Ruoko-
lainen et al. (2013), we employ binary functions
that describe the position t of word x using all left
and right substrings up to a maximum length ?.
The maximum substring length ?
max
is considered
a hyper-parameter to be adjusted using a develop-
ment set. While the emission features associate
the input to labels, the transition feature set
{1(y
t?1
= y
?
t?1
)1(y
t
= y
?
t
) | y
?
t
, y
?
t?1
? Y} (4)
captures the dependencies between adjacent labels
as irrespective of the input x.
2.2 Leveraging Unannotated Data
In order to utilize unannotated data, we explore a
straightforward approach based on feature set aug-
mentation. We exploit predictions of unsupervised
segmentation algorithms by defining variants of
the features described in Section 2.1. The idea is
to compensate the weaknesses of the CRF model
trained on the small annotated data set using the
strengths of the unsupervised methods that learn
from large amounts of unannotated data.
For example, consider utilizing predictions of
the unsupervised Morfessor algorithm (Creutz and
Lagus, 2007) in the CRF model. In order to ac-
complish this, we first learn the Morfessor model
from the unannotated training data, and then ap-
ply the learned model on the word forms in the
annotated training set. Assuming the annotated
training data includes the English word drivers,
the Morfessor algorithm might, for instance, re-
turn a (partially correct) segmentation driv + ers.
We present this segmentation by defining a func-
tion ?(t), which returns 0 or 1, if the position t is
in the middle of a segment or in the beginning of a
segment, respectively, as in
t 1 2 3 4 5 6 7
x
t
d r i v e r s
?(t) 1 0 0 0 1 0 0
Now, given a set of U functions {?
u
(t)}
U
u=1
, we
define variants of the emission features in (2) as
{?
u
(x, t)?
m
(x, t)1(y
t
= y
?
t
) |
?u ? 1..U ,?m ? 1..M ,?y
?
t
? Y} . (5)
By adding the expanded features of form (5), the
CRF model learns to associate the output of the
unsupervised algorithms in relation to the sur-
rounding substring context. Similarly, an ex-
panded transition feature is written as
{?
u
(x, t)1(y
t?1
= y
?
t?1
)1(y
t
= y
?
t
) |
?u ? 1..U ,?y
?
t
, y
?
t?1
? Y} . (6)
After defining the augmented feature set, the
CRF model parameters can be estimated in a stan-
dard manner on the small, annotated training data
set. Subsequent to CRF training, the Morfessor
model is applied on the test instances in order to
allow the feature set augmentation and standard
decoding with the estimated CRF model. We ex-
pect the Morfessor features to specifically improve
85
segmentation of compound words (for example,
brain+storm), which are modeled with high ac-
curacy by the unsupervised Morfessor algorithm
(Creutz and Lagus, 2007), but can not be learned
from the small number of annotated examples
available for the supervised CRF training.
As another example of a means to augment the
feature set, we make use of the fact that the output
of the unsupervised algorithms does not have to be
binary (zeros and ones). To this end, we employ
the classic letter successor variety (LSV) scores
presented originally by (Harris, 1955).
1
The LSV
scores utilize the insight that the predictability of
successive letters should be high within morph
segments, and low at the boundaries. Conse-
quently, a high variety of letters following a prefix
indicates a high probability of a boundary. We use
a variant of the LSV values presented by ??ltekin
(2010), in which we first normalize the scores by
the average score at each position t, and subse-
qently logarithmize the normalized value. While
LSV score tracks predictability given prefixes, the
same idea can be utilized for suffixes, providing
the letter predecessor variety (LPV). Subsequent
to augmenting the feature set using the functions
LSV (t) and LPV (t), the CRF model learns to
associate high successor and predecessor values
(low predictability) to high probability of a seg-
ment boundary. Appealingly, the Harris features
can be obtained in a computationally inexpensive
manner, as they merely require counting statistics
from the unannotated data.
The feature set augmentation approach de-
scribed above is computationally efficient, if the
computational overhead from the unsupervised
methods is small. This is because the CRF param-
eter estimation is still based on the small amount
of labeled examples as described in Section 2.1,
while the number of features incorporated in the
CRF model (equal to the number of parameters)
grows linearly in the number of exploited unsu-
pervised algorithms.
3 Experimental Setup
3.1 Data
We perform the experiments on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al., 2009; Ku-
1
We also experimented on modifying the output of the
Morfessor algorithm from binary to probabilistic, but these
soft cues provided no consistent advantage over the standard
binary output.
English Finnish Turkish
Train (unann.) 384,903 2,206,719 617,298
Train (ann.) 1,000 1,000 1,000
Devel. 694 835 763
Test 10,000 10,000 10,000
Table 1: Number of word types in the Morpho
Challenge data set.
rimo et al., 2010) consisting of manually prepared
morphological segmentations in English, Finnish
and Turkish. We follow the experiment setup, in-
cluding data partitions and evaluation metrics, de-
scribed by Ruokolainen et al. (2013). Table 1
shows the total number of instances available for
model estimation and testing.
3.2 CRF Feature Extraction and Training
The substring features included in the CRF model
are described in Section 2.1. We include all sub-
strings which occur in the training data. The Mor-
fessor and Harris (successor and predecessor va-
riety) features employed by the semi-supervised
extension are described in Section 2.2. We ex-
perimented on two variants of the Morfessor al-
gorithm, namely, the Morfessor Baseline (Creutz
and Lagus, 2002) and Morfessor Categories-MAP
(Creutz and Lagus, 2005), CatMAP for short. The
Baseline models were trained on word types and
the perplexity thresholds of the CatMAP models
were set equivalently to the reference runs in Mor-
pho Challenge 2010 (English: 450, Finnish: 250,
Turkish: 100); otherwise the default parameters
were used. The Harris features do not require any
hyper-parameters.
The CRF model (supervised and semi-
supervised) is trained using the averaged
perceptron algorithm (Collins, 2002). The num-
ber of passes over the training set made by the
perceptron algorithm, and the maximum length of
substring features are optimized on the held-out
development sets.
The experiments are run on a standard desktop
computer using a Python-based single-threaded
CRF implementation. For Morfessor Baseline, we
use the recently published implementation by Vir-
pioja et al. (2013). For Morfessor CatMAP, we
used the Perl implementation by Creutz and La-
gus (2005).
86
3.3 Reference Methods
We compare our method?s performance with
the fully supervised CRF model and the semi-
supervised Morfessor algorithm (Kohonen et al.,
2010). For semi-supervised Morfessor, we use the
Python implementation by Virpioja et al. (2013).
4 Results
Segmentation accuracies for all languages are pre-
sented in Table 2. The columns titled Train (ann.)
and Train (unann.) denote the number of anno-
tated and unannotated training instances utilized
by the method, respectively. To summarize, the
semi-supervised CRF extension greatly improved
the segmentation accuracy of the purely super-
vised CRFs, and also provided higher accuracies
compared to the semi-supervised Morfessor algo-
rithm
2
.
Appealingly, the semi-supervised CRF exten-
sion already provided consistent improvement
over the supervised CRFs, when utilizing the com-
putationally inexpensive Harris features. Addi-
tional gains were then obtained using the Morfes-
sor features. On all languages, highest accuracies
were obtained using a combination of Harris and
CatMAP features.
Running the CRF parameter estimation (includ-
ing hyper-parameters) consumed typically up to a
few minutes. Computing statistics for the Harris
features also took up roughly a few minutes on
all languages. Learning the unsupervised Mor-
fessor algorithm consumed 3, 47, and 20 min-
utes for English, Finnish, and Turkish, respec-
tively. Meanwhile, CatMAP model estimation
was considerably slower, consuming roughly 10,
50, and 7 hours for English, Finnish and Turkish,
respectively. Training and decoding with semi-
supervised Morfessor took 21, 111, and 47 hours
for English, Finnish and Turkish, respectively.
5 Conclusions
We extended a recent morphological segmenta-
tion approach based on CRFs from purely super-
vised to semi-supervised learning. We accom-
plished this in an efficient manner using feature set
augmentation and available unsupervised segmen-
tation techniques. Experiments on three diverse
2
The improvements over the supervised CRFs and semi-
supervised Morfessor were statistically significant (confi-
dence level 0.95) according to the standard 1-sided Wilcoxon
signed-rank test performed on 10 randomly divided, non-
overlapping subsets of the complete test sets.
Method Train (ann.) Train (unann.) F1
English
CRF 100 0 78.8
S-MORF. 100 384,903 83.7
CRF (Harris) 100 384,903 80.9
CRF (BL+Harris) 100 384,903 82.6
CRF (CM+Harris) 100 384,903 84.4
CRF 1,000 0 85.9
S-MORF. 1,000 384,903 84.3
CRF (Harris) 1,000 384,903 87.6
CRF (BL+Harris) 1,000 384,903 87.9
CRF (CM+Harris) 1,000 384,903 88.4
Finnish
CRF 100 0 65.5
S-MORF. 100 2,206,719 70.4
CRF (Harris) 100 2,206,719 78.9
CRF (BL+Harris) 100 2,206,719 79.3
CRF (CM+Harris) 100 2,206,719 82.0
CRF 1,000 0 83.8
S-MORF. 1,000 2,206,719 76.4
CRF (Harris) 1,000 2,206,719 88.3
CRF (BL+Harris) 1,000 2,206,719 88.9
CRF (CM+Harris) 1,000 2,206,719 89.4
Turkish
CRF 100 0 77.7
S-MORF. 100 617,298 78.2
CRF (Harris) 100 617,298 82.6
CRF (BL+Harris) 100 617,298 84.9
CRF (CM+Harris) 100 617,298 85.5
CRF 1,000 0 88.6
S-MORF. 1,000 617,298 87.0
CRF (Harris) 1,000 617,298 90.1
CRF (BL+Harris) 1,000 617,298 91.7
CRF (CM+Harris) 1,000 617,298 91.8
Table 2: Results on test data. CRF (BL+Harris)
denotes semi-supervised CRF extension using
Morfessor Baseline and Harris features, while
CRF (CM+Harris) denotes CRF extension em-
ploying Morfessor CatMAP and Harris features.
languages showed that this straightforward semi-
supervised extension greatly improves the seg-
mentation accuracy of the supervised CRFs, while
being computationally efficient. The extension
also outperformed the semi-supervised Morfessor
algorithm on all data set sizes and languages.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012?2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).
87
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1?8. Association for Computational
Linguistics.
?agr? ??ltekin. 2010. Improving successor variety
for morphological segmentation. In Proceedings of
the 20th Meeting of Computational Linguistics in the
Netherlands.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Mike Maxwell, editor,
Proceedings of the ACL-02 Workshop on Morpho-
logical and Phonological Learning, pages 21?30,
Philadelphia, PA, USA, July. Association for Com-
putational Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Timo Honkela, Ville K?n?nen,
Matti P?ll?, and Olli Simula, editors, Proceedings of
AKRR?05, International and Interdisciplinary Con-
ference on Adaptive Knowledge Representation and
Reasoning, pages 106?113, Espoo, Finland, June.
Helsinki University of Technology, Laboratory of
Computer and Information Science.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1):3:1?3:34, January.
Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo,
Antti Puurula, Janne Pylkk?nen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Sara?lar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):3:1?3:29, December.
Zellig Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 38?46,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk?nen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515?541,
October.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 209?216. Association for Computational Lin-
guistics.
Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010. Semi-supervised learning of concatenative
morphology. In Proceedings of the 11th Meeting of
the ACL Special Interest Group on Computational
Morphology and Phonology, pages 78?86, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Mikko Kurimo, Sami Virpioja, Ville Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Mikko Kurimo, Sami Virpioja, and Ville Turunen.
2010. Overview and results of Morpho Chal-
lenge 2010. In Proceedings of the Morpho Chal-
lenge 2010 Workshop, pages 7?24, Espoo, Finland,
September. Aalto University School of Science and
Technology, Department of Information and Com-
puter Science. Technical Report TKK-ICS-R37.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Carla E. Brodley and Andrea Po-
horeckyj Danyluk, editors, Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282?289, Williamstown, MA, USA. Mor-
gan Kaufmann.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 29?37. Association for Computa-
tional Linguistics, August.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings
of ACL-08: HLT, pages 870?878. Association for
Computational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 209?217.
Association for Computational Linguistics.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
88
the Seventeenth Conference on Computational Nat-
ural Language Learning (CoNLL), pages 29?37. As-
sociation for Computational Linguistics, August.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970?979. As-
sociation for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Ville Turunen and Mikko Kurimo. 2011. Speech re-
trieval from unsegmented Finnish audio using statis-
tical morpheme-like units for segmentation, recog-
nition, and retrieval. ACM Transactions on Speech
and Language Processing, 8(1):1:1?1:25, October.
Sami Virpioja, Peter Smit, Stig-Arne Gr?nroos, and
Mikko Kurimo. 2013. Morfessor 2.0: Python im-
plementation and extensions for Morfessor Baseline.
Report 25/2013 in Aalto University publication se-
ries SCIENCE + TECHNOLOGY, Department of
Signal Processing and Acoustics, Aalto University.
Yiou Wang, Yoshimasa Tsuruoka Jun?ichi Kazama,
Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang,
and Kentaro Torisawa. 2011. Improving Chinese
word segmentation and POS tagging with semi-
supervised methods using large auto-analyzed data.
In IJCNLP, pages 309?317.
89
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195?200,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Applying morphological decomposition to statistical machine translation
Sami Virpioja and Jaakko Va?yrynen and Andre? Mansikkaniemi and Mikko Kurimo
Aalto University School of Science and Technology
Department of Information and Computer Science
PO BOX 15400, 00076 Aalto, Finland
{svirpioj,jjvayryn,ammansik,mikkok}@cis.hut.fi
Abstract
This paper describes the Aalto submission
for the German-to-English and the Czech-
to-English translation tasks of the ACL
2010 Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR.
Statistical machine translation has focused
on using words, and longer phrases con-
structed from words, as tokens in the sys-
tem. In contrast, we apply different mor-
phological decompositions of words using
the unsupervised Morfessor algorithms.
While translation models trained using the
morphological decompositions did not im-
prove the BLEU scores, we show that the
Minimum Bayes Risk combination with
a word-based translation model produces
significant improvements for the German-
to-English translation. However, we did
not see improvements for the Czech-to-
English translations.
1 Introduction
The effect of morphological variation in languages
can be alleviated by using word analysis schemes,
which may include morpheme discovery, part-of-
speech tagging, or other linguistic information.
Words are very convenient and even efficient rep-
resentation in statistical natural language process-
ing, especially with English, but morphologically
rich languages can benefit from more fine-grained
information. For instance, statistical morphs dis-
covered with unsupervised methods result in bet-
ter performance in automatic speech recognition
for highly-inflecting and agglutinative languages
(Hirsima?ki et al, 2006; Kurimo et al, 2006).
Virpioja et al (2007) applied morph-based
models in statistical machine translation (SMT)
between several language pairs without gaining
improvement in BLEU score, but obtaining re-
ductions in out-of-vocabulary rates. They uti-
lized morphs both in the source and in the tar-
get language. Later, de Gispert et al (2009)
showed that Minimum Bayes Risk (MBR) com-
bination of word-based and morph-based trans-
lation models improves translation with Arabic-
to-English and Finnish-to-English language pairs,
where only the source language utilized morph-
based models. Similar results have been shown for
Finnish-to-English and Finnish-to-German in per-
formance evaluation of various unsupervised mor-
pheme analysis algorithms in Morpho Challenge
2009 competition (Kurimo et al, 2009).
We continue the research described above and
examine how the level of decomposition affects
both the individual morph-based systems and
MBR combinations with the baseline word-based
model. Experiments are conducted with the
WMT10 shared task data for German-to-English
and Czech-to-English language pairs.
2 Methods
In this work, morphological analyses are con-
ducted on the source language data, and each dif-
ferent analysis is applied to create a unique seg-
mentation of words into morphemes. Translation
systems are trained with the Moses toolkit (Koehn
et al, 2007) from each differently segmented ver-
sion of the same source language to the target lan-
guage. Evaluation with BLEU is performed on
both the individual systems and system combina-
tions, using different levels of decomposition.
2.1 Morphological models for words
Morfessor (Creutz and Lagus, 2002; Creutz and
Lagus, 2007, etc.) is a family of methods for
unsupervised morphological segmentation. Mor-
fessor does not limit the number of morphemes
for each word, making it suitable for agglutina-
tive and compounding languages. An analysis of a
single word is a list of non-overlapping segments,
195
morphs, stored in the model lexicon. We use both
the Morfessor Baseline (Creutz and Lagus, 2005b)
and the Morfessor Categories-MAP (Creutz and
Lagus, 2005a) algorithms.1 Both are formulated
in a maximum a posteriori (MAP) framework, i.e.,
the learning algorithm tries to optimize the prod-
uct of the model prior and the data likelihood.
The generative model applied by Morfessor
Baseline assumes that the morphs are independent.
The resulting segmentation can be influenced by
using explicit priors for the morph lengths and
frequencies, but their effect is usually minimal.
The training data has a larger effect on the re-
sults: A larger data set alows a larger lexicon,
and thus longer morphs and less morphs per word
(Creutz and Lagus, 2007). Moreover, the model
can be trained with or without taking into account
the word frequencies. If the frequencies are in-
cluded, the more frequent words are usually un-
dersegmented compared to a linguistic analysis,
whereas the rare words are oversegmented (Creutz
and Lagus, 2005b). An easy way to control the
amount of segmentation is to weight the training
data likelihood by a positive factor ?. If ? > 1,
the increased likelihood results in longer morphs.
If ? < 1, the morphs will be shorter and the words
more segmented.
Words that are not present in the training data
can be segmented using an algorithm similar to
Viterbi. The algorithm can be modified to allow
new morphs types to be used by using an approx-
imative cost of adding them into the lexicon (Vir-
pioja and Kohonen, 2009). The modification pre-
vents oversegmentation of unseen word forms. In
machine translation, this is important especially
for proper nouns, for which there is usually no
need for translation.
The Morfessor Categories-MAP algorithm ex-
tends the model by imposing morph categories of
stems, prefixes and suffixes, as well as transition
probabilities between them. In addition, it applies
a hierarchical segmentation model that allows it to
construct new stems from smaller pieces of ?non-
morphemes? (Creutz and Lagus, 2007). Due to
these features, it can provide reasonable segmen-
tations also for those words that contain new mor-
phemes. The drawback of the more sophisticated
model is the slower and more complex training al-
gorithm. In addition, the amount of the segmenta-
1The respective software is available at http://www.
cis.hut.fi/projects/morpho/
tion is harder to control.
Morfessor Categories-MAP was applied to sta-
tistical machine translation by Virpioja et al
(2007) and de Gispert et al (2009). However,
Kurimo et al (2009) report that Morfessor Base-
line outperformed Categories-MAP in Finnish-to-
English and German-to-English tasks both with
and without MBR combination, although the dif-
ferences were not statistically significant. In all
the previous cases, the models were trained on
word types, i.e., without using their frequencies.
Here, we also test models trained on word tokens.
2.2 Statistical machine translation
We utilize the Moses toolkit (Koehn et al, 2007)
for statistical machine translation. The default pa-
rameter values are used except with the segmented
source language, where the maximum sentence
length is increased from 80 to 100 tokens to com-
pensate for the larger number of tokens in text.
2.3 Morphological model combination
For combining individual models, we apply Min-
imum Bayes Risk (MBR) system combination
(Sim et al, 2007). N-best lists from multiple
SMT systems trained with different morpholog-
ical analysis methods are merged; the posterior
distributions over the individual lists are interpo-
lated to form a new distribution over the merged
list. MBR hypotheses selection is then performed
using sentence-level BLEU score (Kumar and
Byrne, 2004).
In this work, the focus of the system combina-
tion is not to combine different translation systems
(e.g., Moses and Systran), but to combine systems
trained with the same translation algorithm using
the same source language data with with different
morphological decompositions.
3 Experiments
The German-to-English and Czech-to-English
parts of the ACL WMT10 shared task data were
investigated. Vanilla SMT models were trained
with Moses using word tokens for MBR combi-
nation and comparison purposes. Several different
morphological segmentation models for German
and Czech were trained with Morfessor. Each seg-
mentation model corresponds to a morph-based
SMT model trained with Moses. The word-based
vanilla Moses model is compared to each morph-
based model as well as to several MBR com-
196
binations between word-based translation models
and morph-based translation models. Quantitative
evaluation is carried out using the BLEU score
with re-cased and re-tokenized translations.
4 Data
The data used in the experiments consisted
of Czech-to-English (CZ-EN) and German-to-
English (DE-EN) parallel language data from
ACL WMT10. The data was divided into distinct
training, development, and evaluation sets. Statis-
tics and details are shown in Table 1.
Aligned data from Europarl v5 and News
Commentary corpora were included in training
German-to-English SMT models. The English
part from the same data sets was used for train-
ing a 5-gram language model, which was used in
all translation tasks. The Czech-to-English trans-
lation model was trained with CzEng v0.9 (train-
ing section 0) and News Commentary data. The
monolingual German and Czech parts of the train-
ing data sets were used for training the morph seg-
mentation models with Morfessor.
The data sets news-test2009, news-
syscomb2009 and news-syscombtune2010
from the ACL WMT 2009 and WMT 2010,
were used for development. The news-test2008,
news-test2010, and news-syscombtest2010 data
sets were used for evaluation.
4.1 Preprocessing
All data sets were preprocessed before use. XML-
tags were removed, text was tokenized and char-
acters were lowercased for every training, devel-
opment and evaluation set.
Morphological models for German and Czech
were trained using a corpus that was a combina-
tion of the respective training sets. Then the mod-
els were used for segmenting all the data sets, in-
cluding development and evaluation sets, with the
Viterbi algorithm discussed in Section 2.1. The
modification of allowing new morph types for out-
of-vocabulary words was not applied.
The Moses cleaning script performed additional
filtering on the parallel language training data.
Specifically, sentences with over 80 words were
removed from the vanilla Moses word-based mod-
els. For morph-based models the limit was set
to 100 morphs, which is the maximum limit of
the Giza++ alignment tool. After filtering with a
threshold of 100 tokens, the different morph seg-
mentations for DE-EN training data from com-
bined Europarl and News Commentary data sets
ranged from 1 613 556 to 1 624 070 sentences.
Similarly, segmented CZ-EN training data ranged
from 896 163 to 897 744 sentences. The vanilla
words-based model was trained with 1 609 998
sentences for DE-EN and 897 497 sentences for
CZ-EN.
5 Results
The details of the ACL WMT10 submissions are
shown in Table 2. The results of experiments with
different morphological decompositions and MBR
system combinations are shown in Table 3. The
significances of the differences in BLEU scores
between the word-based model (Words) and mod-
els with different morphological decompositions
was measured by dividing each evaluation data set
into 49 subsets of 41?51 sentences, and using the
one-sided Wilcoxon signed rank test (p < 0.05).
5.1 Segmentation
We created several word segmentations with Mor-
fessor baseline and Morfessor Categories-MAP
(CatMAP). Statistics for the different segmenta-
tions are given in Table 3. The amount of seg-
mentation was measured as the average number of
morphs per word (m/w) and as the percentage of
segmented words (s-%) in the training data. In-
creasing the data likelihood weight ? in Morfes-
sor Baseline increases the amount of segmentation
for both languages. However, it had little effect
on the proportion of segmented words in the three
evaluation data sets: The proportion of segmented
word tokens was 10?11 % for German and 8?9 %
for Czech, whereas the out-of-vocabulary rate was
7.5?7.8 % for German and 4.8?5.6 % for Czech.
Disregarding the word frequency information
in Morfessor Baseline (nofreq) produced more
morphs per word type and segmented nearly
all words in the training data. The Morfessor
CatMAP algorithm created segmentations with the
largest number of morphs per word, but did not
segment as many words as the Morfessor Baseline
without the frequencies.
5.2 Morph-based translation systems
The models with segmented source language per-
formed worse individually than the word-based
models. The change in the BLEU score was statis-
tically significant in almost all segmentations and
197
Data set Statistics Training Development Evaluation
Sentences Words per sentence SM LM TM
DE CZ EN DE CZ EN DE-EN CZ-EN {DE,CZ}-EN {DE,CZ}-EN
Europarl v5 1 540 549 23.2 25.2 x x x
News Commentary 100 269 21.9 18.9 21.5 x x x x x
CzEng v0.9 (training section 0) 803 286 8.3 9.9 x x
news-test2009 2 525 21.7 18.8 23.2 x
news-syscomb2009 502 19.7 17.2 21.1 x
news-syscombtune2010 455 20.2 17.3 21.0 x
news-test2008 2 051 20.3 17.8 21.7 x
news-test2010 2 489 21.7 18.4 22.3 x
news-syscombtest2010 2 034 22.0 18.6 22.6 x
Table 1: Data sets for the Czech-to-English and German-to-English SMT experiments, including the
number of aligned sentences and the average number of words per sentence in each language. The data
sets used for model training, development and evaluation are marked. Training is divided into German
(DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) training
and German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training.
Submission Segmentation model for source language BLEU-cased
(news-test2010)
aalto DE-EN WMT10 Morfessor Baseline (? = 0.5) 17.0
aalto DE-EN WMT10 CatMAP Morfessor Categories-MAP 16.5
aalto CZ-EN WMT10 Morfessor Baseline (? = 0.5) 16.2
aalto CZ-EN WMT10 CatMAP Morfessor Categories-MAP 15.9
Table 2: Our submissions for the ACL WMT10 shared task in translation. The translation models are
trained from the segmented source language into unsegmented target language with Moses.
all evaluation sets. Morfessor Baseline (? = 0.5)
was the best individual segmented model for both
German and Czech in the sense that it had the
lowest number of significant decreases the BLEU
score compared to the word-based model. Remov-
ing word frequency information with Morfessor
Baseline and using Morfessor CatMAP gave the
lowest BLEU scores with both source languages.
5.3 Translation system combination
For the DE-EN language pair, all MBR system
combinations between each segmented model and
the word-based model had slightly higher BLUE
scores than the individual word-based model.
Nearly all improvements were statistically signifi-
cant.
The BLEU scores for the MBR combinations
in the CZ-EN language pair were mostly not sig-
nificantly different from the individual word-based
model. Two scores were significantly lower.
6 Discussion
We have applied concatenative morphological
analysis, in which each original word token is seg-
mented into one or more non-overlapping morph
tokens. Our results with different levels of seg-
mentation with Morfessor suggest that the optimal
level of segmentation is language pair dependent
in machine translation.
Our approach for handling rich morphology has
not been able to directly improve the translation
quality. We assume that improvements might still
be possible by carefully tuning the amount of seg-
mentation. The experiments in this paper with
different values of the ? parameter for Morfes-
sor Baseline were conducted with the word fre-
quencies. The parameter had little effect on the
proportion of segmented words in the evaluation
data sets, as frequent words were not segmented
at all, and out-of-vocabulary words were likely to
be oversegmented by the Viterbi algorithm. Fu-
ture work includes testing a larger range of val-
ues for ?, also for models trained without the
word frequencies, and using the modification of
the Viterbi algorithm proposed in Virpioja and Ko-
honen (2009).
It might also be helpful to only segment selected
words, where the selection would be based on the
potential benefit in the translation process. In gen-
eral, the direct segmentation of words into morphs
is problematic because it increases the number
of tokens in the text and directly increases both
model training and decoding complexity. How-
ever, an efficient segmentation decreases the num-
ber of types and the out-of-vocabulary rate (Virpi-
oja et al, 2007).
We have replicated here the result that an MBR
combination of a morph-based MT system with
198
Segmentation (DE) Statistics (DE) BLEU-cased (DE-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 16.37 - 17.28 13.22 -
Morfessor Baseline (? = 0.5) 1.82 72.4% 15.19? 16.47+ 17.04? 13.28? 13.70+
Morfessor Baseline (? = 1.0) 1.65 61.0% 15.14? 16.54+ 16.87? 11.95? 13.66+
Morfessor Baseline (? = 5.0) 1.24 23.7% 15.04? 16.44? 16.63? 11.78? 13.43+
Morfessor CatMAP 2.25 67.5% 14.21? 16.42? 16.53? 11.15? 13.61+
Morfessor Baseline nofreq 2.24 91.6% 13.98? 16.47+ 16.36? 10.66? 13.58+
Segmentation (CZ) Statistics (CZ) BLEU-cased (CZ-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 14.91 - 16.73 12.75 -
Morfessor Baseline (? = 0.5) 1.19 17.7% 13.22? 14.87? 16.01? 12.60? 12.53?
Morfessor Baseline (? = 1.0) 1.09 8.1% 13.33? 14.88? 16.10? 11.29? 12.84?
Morfessor Baseline (? = 5.0) 1.03 2.9% 13.53? 14.83? 15.92? 11.17? 12.85?
Morfessor CatMAP 2.29 71.9% 11.93? 14.86? 15.79? 10.12? 10.79?
Morfessor Baseline nofreq 2.18 90.3% 12.43? 14.96? 15.82? 10.13? 12.89?
Table 3: Results for German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation models.
The source language is segmented with the shown algorithms. The amount of segmentation in the train-
ing data is measured with the average number of morphs per word (m/w) and as proportion of segmented
words (s-%) against the word-based model (Words). The trained translation systems are evaluated in-
dependently (No MBR) and in Minimum Bayes Risk system combination of word-based translation
systems (MBR). Unchanged (?), significantly higher (+) and lower (?) BLEU scores compared to the
word-based translation model (Words) are marked. The best morph-based model for each column is
emphasized.
a word-based MT system can produce a BLEU
score that is higher than from either of the indi-
vidual systems (de Gispert et al, 2009; Kurimo
et al, 2009). With the DE-EN language pair, the
improvement was statistically significant with all
tested segmentation models. However, the im-
provements were not as large as those obtained
before and the results for the CZ-EN language
pair were not significantly different in most cases.
Whether this is due to the different languages,
training data sets, the domain of the evaluation
data sets, or some problems in the model training,
is currently uncertain.
One very different approach for applying dif-
ferent levels of linguistic analysis is factor mod-
els for SMT (Koehn and Hoang, 2007), where
pre-determined factors (e.g., surface form, lemma
and part-of-speech) are stored as vectors for each
word. This provides better integration of mor-
phosyntactic information and more control of the
process, but the translation models are more com-
plex and the number and factor types in each word
must be fixed.
Our submissions to the ACL WMT10 shared
task utilize unsupervised morphological decompo-
sition models in a straightforward manner. The
individual morph-based models trained with the
source language words segmented into morphs
did not improve the vanilla word-based models
trained with the unsegmented source language.
We have replicated the result for the German-
to-English language pair that an MBR combina-
tion of a word-based and a segmented morph-
based model gives significant improvements to the
BLEU score. However, we did not see improve-
ments for the Czech-to-English translations.
Acknowledgments
This work was supported by the Academy of
Finland in the project Adaptive Informatics, the
Finnish graduate school in Language Technology,
and the IST Programme of the European Commu-
nity, under the FP7 project EMIME (213845).
References
Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL?02, pages 21?30, Philadelphia,
Pennsylvania, USA.
Mathias Creutz and Krista Lagus. 2005a. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of the AKRR?05,
Espoo, Finland.
199
Mathias Creutz and Krista Lagus. 2005b. Unsu-
pervised morpheme segmentation and morphology
induction from text corpora using Morfessor 1.0.
Technical Report A81, Publications in Computer
and Information Science, Helsinki University of
Technology.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Com-
panion Volume: Short Papers, pages 73?76, Boul-
der, USA, June. Association for Computational Lin-
guistics.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515?541.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the EMNLP 2007,
pages 868?876, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of ACL, demonstration ses-
sion, pages 177?180, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the HLT-NAACL 2004, pages
169?176.
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-
ivola, Teemu Hirsima?ki, Janne Pylkko?nen, Tanel
Aluma?e, and Murat Saraclar. 2006. Unlimited vo-
cabulary speech recognition for agglutinative lan-
guages. In Proceedings of the HLT-NAACL 2006,
pages 487?494, New York, USA.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodl. 2007. Consensus network decoding
for statistical machine translation system combina-
tion. In IEEE Int. Conf. on Acoustics, Speech, and
Signal Processing.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In Proceedings
of the Machine Translation Summit XI, pages 491?
498, Copenhagen, Denmark, September.
200
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 78?86,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Semi-supervised learning of concatenative morphology
Oskar Kohonen and Sami Virpioja and Krista Lagus
Aalto University School of Science and Technology
Adaptive Informatics Research Centre
P.O. Box 15400, FI-00076 AALTO, Finland
{oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi
Abstract
We consider morphology learning in a
semi-supervised setting, where a small
set of linguistic gold standard analyses is
available. We extend Morfessor Base-
line, which is a method for unsupervised
morphological segmentation, to this task.
We show that known linguistic segmenta-
tions can be exploited by adding them into
the data likelihood function and optimiz-
ing separate weights for unlabeled and la-
beled data. Experiments on English and
Finnish are presented with varying amount
of labeled data. Results of the linguis-
tic evaluation of Morpho Challenge im-
prove rapidly already with small amounts
of labeled data, surpassing the state-of-
the-art unsupervised methods at 1000 la-
beled words for English and at 100 labeled
words for Finnish.
1 Introduction
Morphological analysis is required in many natu-
ral language processing problems. Especially, in
agglutinative and compounding languages, where
each word form consists of a combination of stems
and affixes, the number of unique word forms in
a corpus is very large. This leads to problems in
word-based statistical language modeling: Even
with a large training corpus, many of the words en-
countered when applying the model did not occur
in the training corpus, and thus there is no infor-
mation available on how to process them. Using
morphological units, such as stems and affixes, in-
stead of complete word forms alleviates this prob-
lem. Unfortunately, for many languages morpho-
logical analysis tools either do not exist or they
are not freely available. In many cases, the prob-
lems of availability also apply to morphologically
annotated corpora, making supervised learning in-
feasible.
In consequence, there has been a need for ap-
proaches for morphological processing that would
require little language-dependent resources. Due
to this need, as well as the general interest in
language acquisition and unsupervised language
learning, the research on unsupervised learning
of morphology has been active during the past
ten years. Especially, methods that perform mor-
phological segmentation have been studied exten-
sively (Goldsmith, 2001; Creutz and Lagus, 2002;
Monson et al, 2004; Bernhard, 2006; Dasgupta
and Ng, 2007; Snyder and Barzilay, 2008b; Poon
et al, 2009). These methods have shown to pro-
duce results that improve performance in several
applications, such as speech recognition and in-
formation retrieval (Creutz et al, 2007; Kurimo et
al., 2008).
While unsupervised methods often work quite
well across different languages, it is difficult to
avoid biases toward certain kinds of languages and
analyses. For example, in isolating languages, the
average amount of morphemes per word is low,
whereas in synthetic languages the amount may be
very high. Also, different applications may need
a particular bias, for example, not analyzing fre-
quent compound words as consisting of smaller
parts could be beneficial in information retrieval.
In many cases, even a small amount of labeled data
can be used to adapt a method to a particular lan-
guage and task. Methodologically, this is referred
to as semi-supervised learning.
In semi-supervised learning, the learning sys-
tem has access to both labeled and unlabeled data.
Typically, the labeled data set is too small for su-
pervised methods to be effective, but there is a
large amount of unlabeled data available. There
are many different approaches to this class of
problems, as presented by Zhu (2005). One ap-
proach is to use generative models, which spec-
ify a join distribution over all variables in the
model. They can be utilized both in unsupervised
78
and supervised learning. In contrast, discrimina-
tive models only specify the conditional distribu-
tion between input data and labels, and therefore
require labeled data. Both, however, can be ex-
tended to the semi-supervised case. For generative
models, it is, in principle, very easy to use both la-
beled and unlabeled data. For unsupervised learn-
ing one can consider the labels as missing data and
estimate their values using the Expectation Maxi-
mization (EM) algorithm (Dempster et al, 1977).
In the semi-supervised case, some labels are avail-
able, and the rest are considered missing and esti-
mated with EM.
In this paper, we extend the Morfessor Base-
line method for the semi-supervised case. Morfes-
sor (Creutz and Lagus, 2002; Creutz and Lagus,
2005; Creutz and Lagus, 2007, etc.) is one of the
well-established methods for morphological seg-
mentation. It applies a simple generative model.
The basic idea, inspired by the Minimum Descrip-
tion Length principle (Rissanen, 1989), is to en-
code the words in the training data with a lexicon
of morphs, that are segments of the words. The
number of bits needed to encode both the morph
lexicon and the data using the lexicon should be
minimized. Morfessor does not limit the num-
ber of morphemes per word form, making it suit-
able for modeling a large variety of agglutinative
languages irrespective of them being more isolat-
ing or synthetic. We show that the model can be
trained in a similar fashion in the semi-supervised
case as in the unsupervised case. However, with
a large set of unlabeled data, the effect of the su-
pervision on the results tends to be small. Thus,
we add a discriminative weighting scheme, where
a small set of word forms with gold standard ana-
lyzes are used for tuning the respective weights of
the labeled and unlabeled data.
The paper is organized as follows: First, we
discuss related work on semi-supervised learning.
Then we describe the Morfessor Baseline model
and the unsupervised algorithm, followed by our
semi-supervised extension. Finally, we present ex-
perimental results for English and Finnish using
the Morpho Challenge data sets (Kurimo et al,
2009).
1.1 Related work
There is surprisingly little work that consider im-
proving the unsupervised models of morphology
with small amounts of annotated data. In the
related tasks that deal with sequential labeling
(word segmentation, POS tagging, shallow pars-
ing, named-entity recognition), semi-supervised
learning is more common.
Snyder and Barzilay (2008a; 2008b) consider
learning morphological segmentation with non-
parametric Bayesian model from multilingual
data. For multilingual settings, they extract 6 139
parallel short phrases from the Hebrew, Arabic,
Aramaic and English bible. Using the aligned
phrase pairs, the model can learn the segmen-
tations for two languages at the same time. In
one of the papers (2008a), they consider also
semi-supervised scenarios, where annotated data
is available either in only one language or both of
the languages. However, the amount of annotated
data is fixed to the half of the full data. This differs
from our experimental setting, where the amount
of unlabeled data is very large and the amount of
labeled data relatively small.
Poon et al (2009) apply a log-linear, undi-
rected generative model for learning the morphol-
ogy of Arabic and Hebrew. They report results
for the same small data set as Snyder and Barzilay
(2008a) in both unsupervised and semi-supervised
settings. For the latter, they use somewhat smaller
proportions of annotated data, varying from 25%
to 100% of the total data, but the amount of unla-
beled data is still very small. Results are reported
also for a larger 120 000 word Arabic data set, but
only for unsupervised learning.
A problem similar to morphological segmen-
tation is word segmentation for the languages
where orthography does not specify word bound-
aries. However, the amount of labeled data is
usually large, and unlabeled data is just an addi-
tional source of information. Li and McCallum
(2005) apply a semi-supervised approach to Chi-
nese word segmentation where unlabeled data is
utilized for forming word clusters, which are then
used as features for a supervised classifier. Xu
et al (2008) adapt a Chinese word segmentation
specifically to a machine translation task, by using
the indirect supervision from a parallel corpus.
2 Method
We present an extension of the Morfessor Baseline
method to the semi-supervised setting. Morfes-
sor Baseline is based on a generative probabilis-
tic model. It is a method for modeling concatena-
tive morphology, where the morphs?i.e., the sur-
79
face forms of morphemes?of a word are its non-
overlapping segments. The model parameters ?
encode a morph lexicon, which includes the prop-
erties of the morphs, such as their string represen-
tations. Each morph m in the lexicon has a proba-
bility of occurring in a word, P (M = m |?).1 The
probabilities are assumed to be independent. The
model uses a prior P (?), derived using the Min-
imum Description Length (MDL) principle, that
controls the complexity of the model. Intuitively,
the prior assigns higher probability to models that
store fewer morphs, where a morph is considered
stored if P (M = m |?) > 0. During model learn-
ing, ? is optimized to maximize the posterior prob-
ability:
?
MAP
= argmax
?
P (?|D
W
)
= argmax
?
{
P (?)P (D
W
|?)
}
, (1)
where D
W
includes the words in the training
data. In this section, we first consider sepa-
rately the likelihood P (D
W
|?) and the prior P (?)
used in Morfessor Baseline. Then we describe
the algorithms, first unsupervised and then semi-
supervised, for finding optimal model parameters.
Last, we shortly discuss the algorithm for seg-
menting new words after the model training.
2.1 Likelihood
The latent variable of the model, Z =
(Z
1
, . . . , Z
|D
W
|
), contains the analyses of the
words in the training data D
W
. An instance of
a single analysis for the j:th word is a sequence of
morphs, z
j
= (m
j1
, . . . ,m
j|z
j
|
). During training,
each word w
j
is assumed to have only one possible
analysis. Thus, instead of using the joint distribu-
tion P (D
W
,Z |?), we need to use the likelihood
function only conditioned on the analyses of the
observed words, P (D
W
|Z,?). The conditional
likelihood is
P (D
W
|Z = z,?)
=
|D
W
|
?
j=1
P (W = w
j
|Z = z,?)
=
|D
W
|
?
j=1
|z
j
|
?
i=1
P (M = m
ji
|?), (2)
where m
ij
is the i:th morph in word w
j
.
1We denote variables with uppercase letters and their in-
stances with lowercase letters.
2.2 Priors
Morfessor applies Maximum A Posteriori (MAP)
estimation, so priors for the model parameters
need to be defined. The parameters ? of the model
are:
? Morph type count, or the size of the morph
lexicon, ? ? Z
+
? Morph token count, or the number of morphs
tokens in the observed data, ? ? Z
+
? Morph strings (?
1
, . . . , ?
?
), ?
i
? ?
?
? Morph counts (?
1
, . . . , ?
?
), ?
i
? {1, . . . , ?},
?
i
?
i
= ?. Normalized with ?, these give
the probabilities of the morphs.
MDL-inspired and non-informative priors have
been preferred. When using such priors, morph
type count and morph token counts can be ne-
glected when optimizing the model. The morph
string prior is based on length distribution P (L)
and distribution P (C) of characters over the char-
acter set ?, both assumed to be known:
P (?
i
) = P (L = |?
i
|)
|?
i
|
?
j=1
P (C = ?
ij
) (3)
We use the implicit length prior (Creutz and La-
gus, 2005), which is obtained by removing P (L)
and using end-of-word mark as an additional char-
acter in P (C). For morph counts, the non-
informative prior
P (?
1
, . . . , ?
?
) = 1/
(
? ? 1
?? 1
)
(4)
gives equal probability to each possible combina-
tion of the counts when ? and ? are known, as
there are
(
??1
??1
)
possible ways to choose ? positive
integers that sum up to ?.
2.3 Unsupervised learning
In principle, unsupervised learning can be per-
formed by looking for the MAP estimate with the
EM-algorithm. In the case of Morfessor Baseline,
this is problematic, because the prior only assigns
higher probability to lexicons where fewer morphs
have nonzero probabilities. The EM-algorithm has
the property that it will not assign a zero probabil-
ity to any morph, that has a nonzero likelihood in
the previous step, and this will hold for all morphs
80
that initially have a nonzero probability. In con-
sequence, Morfessor Baseline instead uses a local
search algorithm, which will assign zero probabil-
ity to a large part of the potential morphs. This
is memory-efficient, since only the morphs with
nonzero probabilities need to be stored in mem-
ory. The training algorithm of Morfessor Base-
line, described by Creutz and Lagus (2005), tries
to minimize the cost function
L(?, z,D
W
) = ? lnP (?)? lnP (D
W
| z,?)
(5)
by testing local changes to z, modifying the pa-
rameters according to each change, and selecting
the best one. More specifically, one word is pro-
cessed at a time, and the segmentation that min-
imizes the cost function with the optimal model
parameters is selected:
z
(t+1)
j
= argmin
z
j
{
min
?
L(?, z
(t)
,D
W
)
}
. (6)
Next, the parameters are updated:
?
(t+1)
= argmin
?
{
L(?, z
(t+1)
,D
W
)
}
. (7)
As neither of the steps can increase the cost func-
tion, this will converge to a local optimum. The
initial parameters are obtained by adding all the
words into the morph lexicon. Due to the context
independence of the morphs within a word, the op-
timal analysis for a segment does not depend on
in which context the segment appears. Thus, it is
possible to encode z as a binary tree-like graph,
where the words are the top nodes and morphs the
leaf nodes. For each word, every possible split into
two morphs is tested in addition to no split. If the
word is split, the same test is applied recursively
to its parts. See, e.g., Creutz and Lagus (2005) for
more details and pseudo-code.
2.4 Semi-supervised learning
A straightforward way to do semi-supervised
learning is to fix the analyses z for the labeled ex-
amples. Early experiments indicated that this has
little effect on the results. The Morfessor Baseline
model only contains local parameters for morphs,
and relies on the bias given by its prior to guide
the amount of segmentation. Therefore, it may not
be well suited for semi-supervised learning. The
labeled data affects only the morphs that are found
in the labeled data, and even their analyses can be
overwhelmed by a large amount of unsupervised
data and the bias of the prior.
We suggest a fairly simple solution to this by
introducing extra parameters that guide the more
general behavior of the model. The amount of
segmentation is mostly affected by the balance
between the prior and the model. The Morfes-
sor Baseline model has been developed to ensure
this balance is sensible. However, the labeled
data gives a strong source of information regarding
the amount of segmentation preferred by the gold
standard. We can utilize this information by intro-
ducing the weight ? on the likelihood. To address
the problem of labeled data being overwhelmed by
the large amount of unlabeled data we introduce a
second weight ? on the likelihood for the labeled
data. These weights are optimized on a separate
held-out set. Thus, instead of optimizing the MAP
estimate, we minimize the following function:
L(?, z,D
W
,D
W 7?A
) =
? lnP (?)
? ?? lnP (D
W
| z,?)
? ? ? lnP (D
W 7?A
| z,?) (8)
The labeled training set D
W 7?A
may include al-
ternative analyses for some of the words. Let
A(w
j
) = {a
j1
, . . . , a
jk
} be the set of known anal-
yses for word w
j
. Assuming the training samples
are independent, and giving equal weight for each
analysis, the likelihood of the labeled data would
be
P (D
W 7?A
|?)
=
|D
W 7?A
|
?
j=1
?
a
jk
?A(w
j
)
|a
jk
|
?
i=1
P (M = m
jki
|?). (9)
However, when the analyses of the words are
fixed, the product over alternative analyses in A
is problematic, because the model cannot select
several of them at the same time. A sum over
A(w
j
):s would avoid this problem, but then the
logarithm of the likelihood function becomes non-
trivial (i.e., logarithm of sum of products) and too
slow to calculate during the training. Instead, we
use the hidden variable Z to select only one anal-
ysis also for the labeled samples, but now with the
restriction that Z
j
? A(w
j
). The likelihood func-
tion for D
W 7?A
is then equivalent to Equation 2.
Because the recursive algorithm search assumes
that a string is segmented in the same way irre-
spective of its context, the labeled data can still
81
get zero probabilities. In practice, zero probabil-
ities in the labeled data likelihood are treated as
very large, but not infinite, costs.
2.5 Segmenting new words
After training the model, a Viterbi-like algorithm
can be applied to find the optimal segmentation
of each word. As proposed by Virpioja and Ko-
honen (2009), also new morph types can be al-
lowed by utilizing an approximate cost of adding
them to the lexicon. As this enables reasonable re-
sults also when the training data is small, we use a
similar technique. The cost is calculated from the
decrease in the probabilities given in Equations 3
and 4 when a new morph is assumed to be in the
lexicon.
3 Experiments
In the experiments, we compare six different vari-
ants of the Morfessor Baseline algorithm:
? Unsupervised: The classic, unsupervised
Morfessor baseline.
? Unsupervised + weighting: A held-out set
is used for adjusting the weight of the likeli-
hood ?. When ? = 1 the method is equiva-
lent to the unsupervised baseline. The main
effect of adjusting ? is to control how many
segments per word the algorithm prefers.
Higher ? leads to fewer and lower ? to more
segments per word.
? Supervised: The semi-supervised method
trained with only the labeled data.
? Supervised + weighting: As above, but the
weight of the likelihood ? is optimized on
the held-out set. The weight can only af-
fect which segmentations are selected from
the possible alternative segmentations in the
labeled data.
? Semi-supervised: The semi-supervised
method trained with both labeled and
unlabeled data.
? Semi-supervised + weighting: As above,
but the parameters ? and ? are optimized us-
ing the the held-out set.
All variations are evaluated using the linguistic
gold standard evaluation of Morpho Challenge
2009. For supervised and semi-supervised meth-
ods, the amount of labeled data is varied be-
tween 100 and 10 000 words, whereas the held-
out set has 500 gold standard analyzes. To obtain
precision-recall curves, we calculated weighted
F0.5 and F2 scores in addition to the normal F1
score. The parameters ? and ? were optimized
also for those.
3.1 Data and evaluation
We used the English and Finnish data sets from
Competition 1 of Morpho Challenge 2009 (Ku-
rimo et al, 2009). Both are extracted from a
three million sentence corpora. For English, there
were 62 185 728 word tokens and 384 903 word
types. For Finnish, there were 36 207 308 word
tokens and 2 206 719 word types. The complexity
of Finnish morphology is indicated by the almost
ten times larger number of word types than in En-
glish, while the number of word tokens is smaller.
We applied also the evaluation method of the
Morpho Challenge 2009: The results of the mor-
phological segmentation were compared to a lin-
guistic gold standard analysis. Precision measures
whether the words that share morphemes in the
proposed analysis have common morphemes also
in the gold standard, and recall measures the op-
posite. The final score to optimize was F-measure,
i.e, the harmonic mean of the precision and re-
call.2 In addition to the unweighted F1 score, we
have applied F2 and F0.5 scores, which give more
weight to recall and precision, respectively.
Finnish gold standards are based on FINT-
WOL morphological analyzer from Lingsoft, Inc.,
that applies the two-level model by Koskenniemi
(1983). English gold standards are from the
CELEX English database. The final test sets are
the same as in Morpho Challenge, based on 10 000
English word forms and 200 000 Finnish word
forms. The test sets are divided into ten parts for
calculating deviations and statistical significances.
For parameter tuning, we applied a small held-out
set containing 500 word forms that were not in-
cluded in the test set.
For supervised and semi-supervised training,
we created sets of five different sizes: 100, 300,
1 000, 3 000, and 10 000. They did not contain any
of the word forms in the final test set, but were
otherwise randomly selected from the words for
2Both the data sets and evaluation scripts are available
from the Morpho Challenge 2009 web page: http://www.
cis.hut.fi/morphochallenge2009/
82
Figure 1: The F-measure for English as a function
of the number of labeled training samples.
which the gold standard analyses were available.
In order to use them for training Morfessor, the
morpheme analyses were converted to segmenta-
tions using the Hutmegs package by Creutz and
Linde?n (2004).
3.2 Results
Figure 1 shows a comparison of the unsupervised,
supervised and semi-supervised Morfessor Base-
line for English. It can be seen that optimiz-
ing the likelihood weight ? alone does not im-
prove much over the unsupervised case, imply-
ing that the Morfessor Baseline is well suited for
English morphology. Without weighting of the
likelihood function, semi-supervised training im-
proves the results somewhat, but it outperforms
weighted unsupervised model only barely. With
weighting, however, semi-supervised training im-
proves the results significantly already for only
100 labeled training samples. For comparison,
in Morpho Challenges (Kurimo et al, 2009), the
unsupervised Morfessor Baseline and Morfessor
Categories-MAP by Creutz and Lagus (2007) have
achieved F-measures of 59.84% and 50.50%, re-
spectively, and the all time best unsupervised re-
sult by a method that does not provide alternative
analyses for words is 66.24%, obtained by Bern-
hard (2008).3 This best unsupervised result is sur-
passed by the semi-supervised algorithm at 1000
labeled samples.
As shown in Figure 1, the supervised method
obtains inconsistent scores for English with the
3Better results (68.71%) have been achieved by Monson
et al (2008), but as they were obtained by combining of
two systems as alternative analyses, the comparison is not as
meaningful.
Figure 2: The F-measure for Finnish as a function
of the number of labeled training samples. The
semi-supervised and unsupervised lines overlap.
smallest training data sizes. The supervised al-
gorithm only knows the morphs in the training
set, and therefore is crucially dependent on the
Viterbi segmentation algorithm for analyzing new
data. Thus, overfitting to some small data sets is
not surprising. At 10 000 labeled training samples
it clearly outperforms the unsupervised algorithm.
The improvement obtained from tuning the weight
? in the supervised case is small.
Figure 2 shows the corresponding results for
Finnish. The optimization of the likelihood weight
gives a large improvement to the F-measure al-
ready in the unsupervised case. This is mainly be-
cause the standard unsupervised Morfessor Base-
line method does not, on average, segment words
into as many segments as would be appropriate for
Finnish. Without weighting, the semi-supervised
method does not improve over the unsupervised
one: The unlabeled training data is so much larger
that the labeled data has no real effect.
For Finnish, the unsupervised Morfessor Base-
line and Categories-MAP obtain F-measures of
26.75% and 44.61%, respectively (Kurimo et al,
2009). The all time best for an unsupervised
method is 52.45% by Bernhard (2008). With op-
timized likelihood weights, the semi-supervised
Morfessor Baseline achieves higher F-measures
with only 100 labeled training samples. Fur-
thermore, the largest improvement for the semi-
supervised method is achieved already from 1000
labeled training samples. Unlike English, the su-
pervised method is quite a lot worse than the un-
supervised one for small training data. This is
natural because of the more complex morphology
83
Figure 3: Precision-recall graph for English with
varying amount of labeled training data. Parame-
ters ? and ? have been optimized for three differ-
ent measures: F0.5, F1 and F2 on the held-out set.
Precision and recall values are from the final test
set, error bars indicate one standard deviation.
in Finnish; good results are not achieved just by
knowing the few most common suffixes.
Figures 3 and 4 show precision-recall graphs
of the performance of the semi-supervised method
for English and Finnish. The parameters ? and ?
have been optimized for three differently weighted
F-measures (F0.5, F1, and F2) on the held-out set.
The weight tells how much recall is emphasized;
F1 is the symmetric F-measure that emphasizes
precision and recall alike. The graphs show that
the more there are labeled training data, the more
constrained the model parameters are: With many
labeled examples, the model cannot be forced to
achieve high precision or recall only. The phe-
nomenon is more evident in the Finnish data (Fig-
ure 3), where the same amount of words contains
more information (morphemes) than in the En-
glish data. Table 1 shows the F0.5, F1 and F2
measures numerically.
Table 2 shows the values for the F1-optimal
weights ? and ? that were chosen for different
amounts of labeled data using the held-out set. As
even the largest labeled sets are much smaller than
the unlabeled training set, it is natural that ? ? ?.
The small optimal ? for Finnish explains why the
difference between unsupervised unweighted and
weighted versions in Figure 2 was so large. Gener-
ally, the more there is labeled data, the smaller ? is
needed. A possible increase in overall likelihood
cost is compensated by a smaller ?. Finnish with
100 labeled words is an exception; probably a very
Figure 4: Precision-recall graph for Finnish with
varying amount of labeled training data. Param-
eters ? and ? have been optimized for three dif-
ferent measures: F0.5, F1 and F2 on the held-out
set. Precision and recall values are from the final
test set, error bars indicate one standard deviation,
which here is very small.
high ? would end in overlearning of the small set
words at the cost of overall performance.
4 Discussion
The method developed in this paper is a straight-
forward extension of Morfessor Baseline. In the
semi-supervised setting, it should be possible to
develop a generative model that would not require
any discriminative reweighting, but could learn,
e.g., the amount of segmentation from the labeled
data. Moreover, it would be possible to learn the
morpheme labels instead of just the segmentation
into morphs, either within the current model or as
a separate step after the segmentation. We made
initial experiment with a trivial context-free label-
ing: A mapping between the segments and mor-
pheme labels was extracted from the labeled train-
ing data. If some label did not have a correspond-
ing segment, it was appended to the previous la-
bel. E.g., if the labels for ?found? are ?find V
+PAST?, ?found? was mapped to both labels. Af-
ter segmentation, each segment in the test data was
replaced by the most common label or label se-
quence whenever such was available. The results
using training data with 1 000 and 10 000 labeled
samples are shown in Table 3. Although preci-
sions decrease somewhat, recalls improve consid-
erably, and significant gains in F-measure are ob-
tained. A more advanced, context-sensitive label-
ing should perform much better.
84
English
labeled data F0.5 F1 F2
0 69.16 61.05 62.70
100 73.23 65.18 68.30
300 72.98 65.63 68.81
1000 71.86 68.29 69.68
3000 74.34 69.13 72.01
10000 76.04 72.85 73.89
Finnish
labeled data F0.5 F1 F2
0 56.81 49.07 53.95
100 58.96 52.66 57.01
300 59.33 54.92 57.16
1000 61.75 56.38 58.24
3000 63.72 58.21 58.90
10000 66.58 60.26 57.24
Table 1: The F0.5, F1 and F2 measures for the
semi-supervised + weighting method.
English Finnish
labeled data ? ? ? ?
0 0.75 - 0.01 -
100 0.75 750 0.01 500
300 1 500 0.005 5000
1000 1 500 0.05 2500
3000 1.75 350 0.1 1000
10000 1.75 175 0.1 500
Table 2: The values for the weights ? and ?
that the semisupervised algorithm chose for differ-
ent amounts of labeled data when optimizing F1-
measure.
The semi-supervised extension could easily be
applied to the other versions and extensions of
Morfessor, such as Morfessor Categories-MAP
(Creutz and Lagus, 2007) and Allomorfessor (Vir-
pioja and Kohonen, 2009). Especially the model-
ing of allomorphy might benefit from even small
amounts of labeled data, because those allomorphs
that are hardest to find (affixes, stems with irregu-
lar orthographic changes) are often more common
than the easy cases, and thus likely to be found
even from a small labeled data set.
Even without labeling, it will be interesting
to see how well the semi-supervised morphology
learning works in applications such as information
retrieval. Compared to unsupervised learning, we
obtained much higher recall for reasonably good
levels of precision, which should be beneficial to
most applications.
Segmented Labeled
English, D = 1000
Precision 69.72% 69.30%
Recall 66.92% 72.21%
F-measure 68.29% 70.72%
English, D = 10 000
Precision 77.35% 77.07%
Recall 68.85% 77.78%
F-measure 72.86% 77.42%
Finnish, D = 1000
Precision 61.03% 58.96%
Recall 52.38% 66.55%
F-measure 56.38% 62.53%
Finnish, D = 10 000
Precision 69.14% 66.90%
Recall 53.40% 74.08%
F-measure 60.26% 70.31%
Table 3: Results of a simple morph labeling after
segmentation with semi-supervised Morfessor.
5 Conclusions
We have evaluated an extension of the Morfessor
Baseline method to semi-supervised morphologi-
cal segmentation. Even with our simple method,
the scores improve far beyond the best unsuper-
vised results. Moreover, already one hundred
known segmentations give significant gain over
the unsupervised method even with the optimized
data likelihood weight.
Acknowledgments
This work was funded by Academy of Finland and
Graduate School of Language Technology in Fin-
land. We thank Mikko Kurimo and Tiina Lindh-
Knuutila for comments on the manuscript, and
Nokia foundation for financial support.
References
Delphine Bernhard. 2006. Unsupervised morpholog-
ical segmentation based on segment predictability
and word segments alignment. In Proceedings of the
PASCAL Challenge Workshop on Unsupervised seg-
mentation of words into morphemes, Venice, Italy.
PASCAL European Network of Excellence.
Delphine Bernhard. 2008. Simple morpheme labelling
in unsupervised morpheme analysis. In Advances in
Multilingual and Multimodal Information Retrieval,
8th Workshop of the CLEF, volume 5152 of Lec-
ture Notes in Computer Science, pages 873?880.
Springer Berlin / Heidelberg.
85
Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL?02, pages 21?30, Philadelphia,
Pennsylvania, USA.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical
Report A81, Publications in Computer and Informa-
tion Science, Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Mathias Creutz and Krister Linde?n. 2004. Morpheme
segmentation gold standards for Finnish and En-
glish. Technical Report A77, Publications in Com-
puter and Information Science, Helsinki University
of Technology.
Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo,
Antti Puurula, Janne Pylkko?nen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Sarac?lar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):1?29.
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In the annual conference of the North
American Chapter of the ACL (NAACL-HLT).
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society, Series B (Methodological), 39(1):1?
38.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?189.
Kimmo Koskenniemi. 1983. Two-level morphology: A
general computational model for word-form recog-
nition and production. Ph.D. thesis, University of
Helsinki.
Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.
2008. Morpho Challenge evaluation using a linguis-
tic Gold Standard. In Advances in Multilingual and
MultiModal Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum, CLEF 2007,
Budapest, Hungary, September 19-21, 2007, Re-
vised Selected Papers, Lecture Notes in Computer
Science , Vol. 5152, pages 864?873. Springer.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Wei Li and Andrew McCallum. 2005. Semi-
supervised sequence modeling with syntactic topic
models. In AAAI?05: Proceedings of the 20th na-
tional conference on Artificial intelligence, pages
813?818. AAAI Press.
Christian Monson, Alon Lavie, Jaime Carbonell, and
Lori Levin. 2004. Unsupervised induction of natu-
ral language morphology inflection classes. In Pro-
ceedings of the Workshop of the ACL Special Interest
Group in Computational Phonology (SIGPHON).
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. ParaMor: Finding paradigms
across morphology. In Advances in Multilingual
and MultiModal Information Retrieval, 8th Work-
shop of the Cross-Language Evaluation Forum,
CLEF 2007, Budapest, Hungary, September 19-21,
2007, Revised Selected Papers, Lecture Notes in
Computer Science , Vol. 5152. Springer.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 209?217. Association for Computational Lin-
guistics.
Jorma Rissanen. 1989. Stochastic Complexity in Sta-
tistical Inquiry, volume 15. World Scientific Series
in Computer Science, Singapore.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
AAAI?08: Proceedings of the 23rd national con-
ference on Artificial intelligence, pages 848?854.
AAAI Press.
Benjamin Snyder and Regina Barzilay. 2008b. Un-
supervised multilingual learning for morphological
segmentation. In Proceedings of ACL-08: HLT,
pages 737?745, Columbus, Ohio, June. Association
for Computational Linguistics.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In COLING ?08: Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics, pages 1017?1024, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Xiaojin Zhu. 2005. Semi-supervised Learning with
Graphs. Ph.D. thesis, CMU. Chapter 11, Semi-
supervised learning literature survey (updated online
version).
86
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 87?95,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Morpho Challenge competition 2005-2010: Evaluations and results
Mikko Kurimo, Sami Virpioja, Ville Turunen, Krista Lagus
Adaptive Informatics Research Centre
Aalto University, Espoo, Finland
Firstname.Lastname@tkk.fi
Abstract
Morpho Challenge is an annual evalu-
ation campaign for unsupervised mor-
pheme analysis. In morpheme analysis,
words are segmented into smaller mean-
ingful units. This is an essential part in
processing complex word forms in many
large-scale natural language processing
applications, such as speech recognition,
information retrieval, and machine trans-
lation. The discovery of morphemes is
particularly important for morphologically
rich languages where inflection, deriva-
tion and composition can produce a huge
amount of different word forms. Morpho
Challenge aims at language-independent
unsupervised learning algorithms that can
discover useful morpheme-like units from
raw text material. In this paper we de-
fine the challenge, review proposed algo-
rithms, evaluations and results so far, and
point out the questions that are still open.
1 Introduction
Many large-scale natural language processing
(NLP) applications, such as speech recognition,
information retrieval and machine translation, re-
quire that complex word forms are analyzed into
smaller, meaningful units. The discovery of these
units called morphemes is particularly important
for morphologically rich languages where the in-
flection, derivation and composition makes it im-
possible to even list all the word forms that are
used. Various tools have been developed for mor-
pheme analysis of word forms, but they are mostly
based on language-specific rules that are not eas-
ily ported to other languages. Recently, the per-
formance of tools based on language-independent
unsupervised learning from raw text material has
improved significantly and rivaled the language-
specific tools in many applications.
The unsupervised algorithms proposed so far in
Morpho Challenge typically first generate various
alternative morphemes for each word and then se-
lect the best ones based on relevant criteria. The
statistical letter successor variation (LSV) analy-
sis (Harris, 1955) and its variations are quite com-
monly used as generation methods. LSV is based
on the observation that the segment borders be-
tween the sub-word units often co-occur with the
peaks of variation for the next letter. One popu-
lar selection approach is to minimize a cost func-
tion that balances between the size of the corpus
when coded by the morphemes and the size of
the morpheme codebook needed. Selection cri-
teria that produce results resembling the linguis-
tic morpheme segmentation include, for example,
the Minimum Description Length (MDL) princi-
ple and maximum a posteriori (MAP) probability
optimization (de Marcken, 1996; Creutz and La-
gus, 2005).
The Morpho Challenge competition was
launched in 2005 to encourage the machine
learning people, linguists and specialists in NLP
applications to study this field and come together
to compare their best algorithms against each
other. The organizers selected evaluation tasks,
data and metric and performed all the evaluations.
Thus, participation was made easy for people
who were not specialists in the chosen NLP
applications. Participation was open to everybody
with no charge. The competition became popular
right from the beginning and has gained new
participants every year.
Although not all the authors of relevant mor-
pheme analysis algorithms have yet submitted
their algorithms for this evaluation campaign,
more than 50 algorithms have already been eval-
uated. After the first five years of Morpho Chal-
lenge, a lot has been learned on the various pos-
sible ways to solve the problem and how the dif-
ferent methods work in various NLP tasks. How-
87
ever, there are still open questions such as: how to
find meaning for the obtained unsupervised mor-
phemes, how to disambiguate among the alterna-
tive analyses of one word, and how to use context
in the analysis. Another recently emerged ques-
tion that is the special topic in 2010 competition
is how to utilize small amounts of labeled data
and semi-supervised learning to further improve
the analysis.
2 Definition of the challenge
2.1 Morphemes and their evaluation
Generally, the morphemes are defined as the
smallest meaningful units of language. Rather
than trying to directly specify which units are
meaningful, the Morpho Challenge aims at find-
ing units that would be useful for various practical
NLP applications. The goal is to find automatic
methods that can discover suitable units using un-
supervised learning directly on raw text data. The
methods should also not be restricted to certain
languages or include many language and applica-
tion dependent parameters that needed to be hand
tuned for each task separately. The following three
goals have been defined as the main scientific ob-
jectives for the challenge: (1) To learn of the phe-
nomena underlying word construction in natural
languages. (2) To discover approaches suitable for
a wide range of languages. (3) To advance ma-
chine learning methodology.
The evaluation tasks, metrics and languages
have been designed based on the scientific objec-
tives of the challenge. It can not be directly ver-
ified how well an obtained analysis reflects the
word construction in natural languages, but intu-
itively, the methods that split everything into let-
ters or pre-specified letter n-grams, or leave the
word forms unanalyzed, would not be very in-
teresting solutions. An interesting thing that can
be evaluated, however, is how close the obtained
analysis is to the linguistic gold standard mor-
phemes that can be obtained from CELEX or
various language-dependent rule-based analyzers.
The exact definition of the morphemes, tags, or
features available in the gold standard to be uti-
lized in the comparison should be decided and
fixed for each language separately.
To verify that a proposed algorithm works in
various languages would, ideally, require running
the evaluations on a large number of languages
that would be somehow representative of various
important language families. However, the re-
sources available for both computing and evalu-
ating the analysis in various applications and lan-
guages are limited. The suggested and applicable
compromise is to select morphologically rich lan-
guages where the morpheme analysis is most use-
ful and those languages where interesting state-of-
the-art evaluation tasks are available. By including
German, Turkish, Finnish and Arabic, many inter-
esting aspects of concatenative morphology have
already been covered.
While the comparison against the linguistic
gold standard morphemes is an interesting sub-
goal, the main interest in running the Morpho
Challenge is to find out how useful the proposed
morpheme analyses are for various practical NLP
applications. Naturally, this is best evaluated
by performing evaluations in several state-of-the-
art application tasks. Due to the limitations of
the resources, the applications have been selected
based on the importance of the morpheme analy-
sis for the application, on the availability of open
state-of-the-art evaluation tasks, and on the effort
needed to run the actual evaluations.
2.2 Unsupervised and semi-supervised
learning
Unsupervised learning is the task of learning with-
out labeled data. In the context of morphology dis-
covery, it means learning without knowing where
morpheme borders are, or which morphemes exist
in which words. Unsupervised learning methods
have many attractive features for morphological
modeling, such as language-independence, inde-
pendence of any particular linguistic theory, and
easy portability to a new language.
Semi-supervised learning can be approached
from two research directions, namely unsuper-
vised and supervised learning. In an essentially
unsupervised learning task there may exist some
labeled (classified) data, or some known links be-
tween data items, which might be utilized by the
(typically generative) learning algorithms. Turned
around, an essentially supervised learning task,
such as classification or prediction, may benefit
also from unlabeled data which is typically more
abundantly available.
In morphology modeling one might consider
the former setup to be the case: the learning task
is essentially that of unsupervised modeling, and
morpheme labels can be thought of as known links
88
between various inflected word forms.
Until 2010 the Morpho Challenge has been de-
fined only as an unsupervised learning task. How-
ever, since small samples of morphologically la-
beled data can be provided already for quite many
languages, also the semi-supervised learning task
has become of interest.
Moreover, while there exists a fair amount of
research and now even books on semi-supervised
learning (Zhu, 2005; Abney, 2007; Zhu, 2010),
it has not been as widely studied for structured
classification problems like sequence segmenta-
tion and labeling (cf. e.g. (Jiao et al, 2006)). The
semi-supervised learning challenge introduced for
Morpho Challenge 2010 can thus be viewed as an
opportunity to strengthen research in both mor-
phology modeling as well as in semi-supervised
learning for sequence segmentation and labeling
in general.
3 Review of Morpho Challenge
competitions so far
3.1 Evaluation tasks, metrics, and languages
The evaluation tasks and languages selected for
Morpho Challenge evaluations are shown in Fig-
ure 1. The languages where evaluations have been
prepared are Finnish (FIN), Turkish (TUR), En-
glish (ENG), German (GER), and Arabic (ARA).
First the morphemes are compared to linguis-
tic gold standards in direct morpheme segmen-
tation (2005) and full morpheme analysis (since
2007). The practical NLP application based eval-
uations are automatic speech recognition (ASR),
information retrieval (IR) and statistical machine
translation (SMT). Morphemes obtained by semi-
supervised learning can be evaluated in parallel
with the unsupervised morphemes. For IR, eval-
uation has also been extended for full sentences,
where the morpheme analysis can based on con-
text. The various suggested and tested evaluations
are defined in this section.
year new languages new tasks
2005 FIN, TUR, ENG segmentation, ASR
2007 GER full analysis, IR
2008 ARA context IR
2009 - SMT
2010 - semi-supervised
Table 1: The evolution of the evaluations. The
acronyms are explained in section 3.1.
3.1.1 Comparisons to linguistic gold standard
The first Morpho Challenge in 2005 (Kurimo et
al., 2006) considered unsupervised segmentation
of words into morphemes. The evaluation was
based on comparing the segmentation boundaries
given by the competitor?s algorithm to the bound-
aries obtained from a gold standard analysis.
From 2007 onwards, the task was changed to
full morpheme analysis, that is, the algorithm
should not only locate the surface forms (i.e., word
segments) of the morphemes, but find also which
surface forms are realizations (allomorphs) of the
same underlying morpheme. This generalizes the
task for finding more meaningful units than just
the realizations of morphemes that may be just in-
dividual letters or even empty strings. In applica-
tions this is useful when it is important to identify
which units carry the same meaning even if they
have different realizations in different words.
As an unsupervised algorithm cannot find the
morpheme labels that would equal to the labels in
the gold standard, the evaluation has to be based
on what word forms share the same morphemes.
The evaluation procedure samples a large num-
ber of word pairs, such that both words in the
pair have at least one morpheme in common, from
both the proposed analysis and the gold standard.
The first version of the method was applied in
2007 (Kurimo et al, 2008) and 2008 (Kurimo et
al., 2009a), and minor modifications were done in
2009 (Kurimo et al, 2009b). However, the orga-
nizers have reported the evaluation results of the
2007 and 2008 submissions also with the new ver-
sion, thus allowing a direct comparison between
them. A summary of these results for English,
Finnish, German and Turkish for the best algo-
rithms is presented in Table 2. The evaluations
in 2008 and 2009 were also performed on Arabic,
but these results and not comparable, because the
database and the gold standard was changed be-
tween the years. The exact annual results for all
participants as well as the details of the evaluation
in each year can be reviewed in the annual evalu-
ation reports (Kurimo et al, 2006; Kurimo et al,
2008; Kurimo et al, 2009a; Kurimo et al, 2009b).
Already the linguistic evaluation of Morpho
Challenge 2005 applied some principles that have
been used thereafter: (1) The evaluation is based
on a subset of the word forms given as training
data. This not only makes the evaluation proce-
dure lighter, but also allows changing the set when
89
English Finnish
Method P R F Method P R F
2009 2009
Allomorfessor 68.98 56.82 62.31 Monson PMU 47.89 50.98 49.39
Monson PMU 55.68 62.33 58.82 Monson PMM 51.75 45.42 48.38
Lignos 83.49 45.00 58.48 Spiegler PROMODES C 41.20 48.22 44.44
2008 2008
Monson P+M 69.59 65.57 67.52 Monson P+M 65.21 50.43 56.87
Monson ParaMor 63.32 51.96 57.08 Monson ParaMor 49.97 37.64 42.93
Zeman 1 67.13 46.67 55.06 Monson Morfessor 79.76 24.95 38.02
2007 2007
Monson P+M 70.09 67.38 68.71 Bernhard 2 63.92 44.48 52.45
Bernhard 2 67.42 65.11 66.24 Bernhard 1 78.11 29.39 42.71
Bernhard 1 75.61 57.87 65.56 Bordag 5a 72.45 27.21 39.56
German Turkish
Method P R F Method P R F
2009 2009
Monson PMU 52.53 60.27 56.14 Monson PMM 48.07 60.39 53.53
Monson PMM 51.07 57.79 54.22 Monson PMU 47.25 60.01 52.88
Monson PM 50.81 47.68 49.20 Monson PM 49.54 54.77 52.02
2008 2008
Monson P+M 64.06 61.52 62.76 Monson P+M 66.78 57.97 62.07
Monson Morfessor 70.73 38.82 50.13 Monson ParaMor 57.35 45.75 50.90
Monson ParaMor 56.98 42.10 48.42 Monson Morfessor 77.36 33.47 46.73
2007 2007
Monson P+M 69.96 55.42 61.85 Bordag 5a 81.06 23.51 36.45
Bernhard 2 54.02 60.77 57.20 Bordag 5 81.19 23.44 36.38
Bernhard 1 66.82 42.48 51.94 Zeman 77.48 22.71 35.13
Table 2: The summary of the best three submitted methods for years 2009, 2008 and 2007 using the
linguistic evaluation of Morpho Challenge 2009. The complete results tables by the organizers are avail-
able from http://www.cis.hut.fi/morphochallenge2009/. The three columns numbers
are precision (P), recall (R), and F-measure (F). The best F-measure for each language is in boldface,
and the best result that is not based on a direct combination of two other methods is underlined.
the old one is considered to be ?overlearned?. (2)
The frequency of the word form plays no role in
evaluation; rare and common forms are equally
likely to be selected, and have equal weight to
the score. (3) The evaluation score is balanced F-
measure, the harmonic mean of precision and re-
call. Precision measures how many of the choices
made by the algorithm are matched in gold stan-
dard; recall measures how many of the choices
in the gold standard are matched in the proposed
analysis. (4) If the linguistic gold standard has
several alternative analysis for one word, for full
precision, it is enough that one of the alternatives
is equivalent to the proposed analysis. The same
holds the other way around for recall.
All of the principles can be also criticized. For
example, evaluation based on the full set would
provide more trustworthy estimates, and common
word forms are more significant in any practical
application. However, the third and the fourth
principle have problems that can be considered to
be more serious.
Balanced F-measure favors methods that are
able to get near-to-equal precision and recall. As
many algorithms can be tuned to give either more
or less morphemes per word than in the default
case, this encourages using developments sets to
optimize the respective parameters. The winning
methods in Challenge 2009?Monson?s ParaMor-
Morfessor Union (PMU) and ParaMor-Morfessor
90
Mimic (PMM) (Monson et al, 2009), and Al-
lomorfessor (Virpioja and Kohonen, 2009)?did
this, more or less explicitly.1 Moreover, it can
be argued that the precision would be more im-
portant than recall in many applications, or, more
generally, that the optimal balance between preci-
sion and recall is application dependent. We see
two solutions for this: Either the optimization for
F-measure should be allowed with a public devel-
opment set, which means moving towards semi-
supervised direction, or precision-recall curves
should be compared, which means more complex
evaluations.
The fourth principle causes problems, if the
evaluated algorithms are allowed to have alterna-
tive analyses for each word. If several alternative
analyses are provided, the obtained precision is
about the average over the individual analyses, but
the recall is based on the best of the alternatives.
This property have been exploited in Challenges
2007 and 2008 by combining the results of two
algorithms as alternative analyses. The method,
Monson?s ParaMor+Morfessor (P+M) holds still
the best position measured in F-measures in all
languages. Combining even better-performing
methods in a similar manner would increase the
scores further. To fix this problem, either the eval-
uation metric should require matching number of
alternative analyses to get the full points, or the
symmetry of the precision and recall measures has
to be removed.
Excluding the methods that combine the anal-
yses of two other methods as alternative ones, we
see that the best F-measure (underlined in Table 2)
is held by Monson?s ParaMor-Morfessor Mimic
from 2009 (Monson et al, 2009) in Turkish and
Bernhard?s method 2 from 2007 (Bernhard, 2006)
in all the other three languages. This means that
except for Turkish, there is no improvement in the
results over the three years. Furthermore, both
of the methods are based purely on segmentation,
and so are all the other top methods presented
in Table 2 except for Bordag?s methods (Bordag,
2006) and Allomorfessor (Virpioja and Kohonen,
2009).
3.1.2 Speech recognition
A key factor in the success of large-vocabulary
continuous speech recognition is the system?s abil-
1Allomorfessor was trained with a pruned data to obtain
a higher recall, whereas ParaMor-Morfessor is explicitly op-
timized for F-measure with a separate Hungarian data set.
ity to limit the search space using a statistical lan-
guage model. The language model provides the
probability of different recognition hypothesis by
using a model of the co-occurence of its words
and morphemes. A properly smoothed n-gram is
the most conventional model. The n-gram should
consist of modeling units that are suitable for the
language, typically words or morphemes.
In Morpho Challenge state-of-the-art large-
vocabulary speech recognizers have been built for
evaluations in Finnish and Turkish (Kurimo et al,
2006). The various morpheme analysis algorithms
have been compared by measuring the recogni-
tion accuracy with different language models each
trained and optimized based on units from one of
the algorithms. The best results were quite near
to each other, but Bernhard (Bernhard, 2006) and
Morfessor Categories MAP were at the top for
both languages.
3.1.3 Information retrieval
In the information retrieval task, the algorithms
were tested by using the morpheme segmentations
for text retrieval. To return all relevant documents,
it is important to match the words in the queries to
the words in the documents irrespective of which
word forms are used. Typically, a stemming al-
gorithm or a morphological analyzer is used to re-
duce the inflected forms to their stem or base form.
The problem with these methods is that specific
rules need to be crafted for each language. How-
ever, these approaches were also tested for com-
parison purposes. The IR experiments were car-
ried out by replacing the words in the corpora and
queries by the suggested morpheme segmenta-
tions. Test corpora, queries and relevance assess-
ments were provided by Cross-Language Evalua-
tion Forum (CLEF) (Agirre et al, 2008).
To test the effect of the morpheme segmen-
tation, the number of other variables will have
to be minimized, which poses some challenges.
For example, the term weighting method will af-
fect the results and different morpheme analyz-
ers may perform optimally with different weight-
ing approaches. TFIDF and Okapi BM25 term
weighting methods have been tested. In the 2007
Challenge, it was noted that Okapi BM25 suffers
greatly if the corpus contains a lot of frequent
terms. These terms are often introduced when the
algorithms segment suffixes from stems. To over-
come this problem, a method for automatically
generating stop lists of frequent terms was intro-
91
duced. Any term that occurs more times in the cor-
pus than a certain threshold is added to the stop list
and excluded from indexing. The method is quite
simple, but it treats all morpheme analysis meth-
ods equally as it does not require the algorithm
to tag which morphemes are stems and which are
suffixes. The generated stoplists are also reason-
able sized and the results are robust with respect
to the stop list cutoff parameter. With a stop list,
Okapi BM25 clearly outperformed TFIDF rank-
ing method for all algorithms. However, the prob-
lem of choosing the term weighting approach that
treats all algorithms in an optimal way remains
open.
Another challenge is analyzing the results as it
is hard to achieve statistically significant results
with the limited number of queries (50-60) that
were available. In fact, in each language 11-17 of
the best algorithms belonged to the ?top group?,
that is, had no statistically different result to the
top performer of the language. To improve the
significance of the results, the number of queries
should be increased. This is a known problem in
the field of IR. However, it is important to test the
methods in a real life application and if an algo-
rithm gives good results across languages, there is
evidence that it is doing something useful.
Some conclusions can be drawn from the re-
sults. The language specific reference methods
(Porter stemming for English, two-layer morpho-
logical analysis for Finnish and German) give the
best results, but the best unsupervised algorithms
are almost at par and the differences are not signif-
icant. For German and Finnish, the best unsuper-
vised methods can also beat in a statistically sig-
nificant way the baseline of not doing any segmen-
tation or stemming. The best algorithms that per-
formed well across languages are ParaMor (Mon-
son et al, 2008), Bernhard (Bernhard, 2006), Mor-
fessor Baseline, andMcNamee (McNamee, 2008).
Comparing the results to the linguistic evalua-
tion (section 3.1.1), it seems that methods that per-
form well at the IR task tend to have good preci-
sion in the linguistic task, with exceptions. Thus,
in the IR task it seems important not to overseg-
ment words. One exception is the method (Mc-
Namee, 2008) which simply splits the words into
equal length letter n-grams. The method gives sur-
prisingly good results in the IR task, given the sim-
plicity, but suffers from low precision in the lin-
guistic task.
3.1.4 Machine translation
In phrase-based statistical machine translation
process there are two stages where morpheme
analysis and segmentation of the words into mean-
ingful sub-word units is needed. The first stage
is the alignment of the parallel sentences in the
source and target language for training the transla-
tion model. The second one is training a statistical
language model for the production of fluent sen-
tences in a morphologically rich target language.
In the machine translation tasks used in the
Morpho Challenge, the focus has so far been in
the alignment problem. In the evaluation tasks in-
troduced in 2009 the language-pairs were Finnish-
English and German-English. To obtain state-of-
the-art results, the evaluation consists of minimum
Bayes risk (MBR) combination of two transla-
tion systems trained on the same data, one us-
ing words and the other morphemes as the ba-
sic modeling units (de Gispert et al, 2009). The
various morpheme analysis algorithms are com-
pared by measuring the translation performance
for different two-model combinations where the
word-based model is always the same, but the
morpheme-based model is trained based on units
from each of the algorithms in turns.
Because the machine translation evaluation has
yet been tried only in 2009, it is difficult to draw
conclusions about the results yet. However, the
Morfessor Baseline algorithm seems to be partic-
ularly difficult to beat both in Finnish-German and
German-English task. The differences between
the best results are small, but the ranking in both
tasks was the same: 1. Morfessor Baseline, 2. Al-
lomorfessor, 3. The linguistic gold standard mor-
phemes (Kurimo et al, 2009b).
3.2 Evaluated algorithms
This section attempts to describe very briefly some
of the individual morpheme analysis algorithms
that have been most successful in the evaluations.
Morfessor Baseline (Creutz and Lagus, 2002):
This is a public baseline algorithm based on jointly
minimizing the size of the morph codebook and
the encoded size of the all the word forms using
the minimum description length MDL cost func-
tion. The performance is above average for all
evaluated tasks in most languages.
Allomorfessor (Kohonen et al, 2009; Virpi-
oja and Kohonen, 2009): The development of
this method was based on the observation that the
92
Finnish German English
0.25
0.3
0.35
0.4
0.45
0.5
0.55
 
 
Morfessor baseline
2007 Bernhard
2008 McNamee 4?gram
2008 Monson P+M
2009 Monson PMU
2009 Lignos
2009 Allomorfessor
Figure 1: Mean Average Precision (MAP) values for some of the best algorithms over the years in the IR
task. The upper horizontal line shows the ?goal level? for each language, i.e. the performance of the best
language specific reference method. The lower line shows the baseline reference of doing no stemming
or analysis.
morph level surface forms of one morpheme are
often very similar and the differences occur close
to the morpheme boundary. Thus, the allomor-
phemes could be modeled by simple mutations.
It has been implemented on top of the Morfessor
Baseline using maximum a posteriori (MAP) opti-
mization. This model slightly improves the perfor-
mance in the linguistic evaluation in all languages
(Kurimo et al, 2009b), but in IR and SMT there is
no improvement yet.
Morfessor Categories MAP (Creutz and La-
gus, 2005): In this method hidden Markov models
are used to incorporate morphotactic categories for
theMorfessor Baseline. The structure is optimized
by MAP and yields slight improvements in the lin-
guistic evaluation for most languages, but not for
IR or SMT tasks.
Bernhard (Bernhard, 2006): This has been one
of the best performing algorithms in Finnish, En-
glish and German linguistic evaluation and in IR
(Kurimo et al, 2008). First a list of the most likely
prefixes and suffixes is extracted and alternative
segmentations are generated for the word forms.
Then the best ones are selected based on cost func-
tions that favour most frequent analysis and some
basic morphotactics.
Bordag (Bordag, 2006): This method applies
iterative LSV and clustering of morphs into mor-
phemes. The performance in the linguistic eval-
uation is quite well for Turkish and decent for
Finnish (Kurimo et al, 2008).
ParaMor (Monson et al, 2008): This method
applies an unsupervised model for inflection rules
and suffixation for the stems by building linguisti-
cally motivated paradigms. It has obtained one of
the top performances for all languages when com-
bined with the Morfessor Baseline (Kurimo et al,
2009a). Various combination methods have been
tested: union, weighted probabilistic average and
proposing both the analyses (Monson et al, 2009).
Lignos (Lignos et al, 2009): This method is
based on the observation that the derivation of
the inflected forms can be modeled as transfor-
mations. The best transformations can be found
by optimizing the simplicity and frequency. This
method performs much better in English than in
the other languages (Kurimo et al, 2009b).
Promodes (Spiegler et al, 2009): This method
presents a probabilistic generative model that ap-
plies LSV and combines multiple analysis using a
committee. It seems to generate a large amount
of short morphemes, which is difficult for many
of the practical applications. However, it obtained
the best performance for the linguistic evaluation
in Arabic 2009 (Kurimo et al, 2009b), but did not
survive as well in other languages, and particularly
not in the IR application.
4 Open questions and challenges
Although more than 50 algorithms have already
been tested in the Morpho Challenge evaluations
and many lessons have been learned from the re-
sults and discussions, many challenges are still
open and untouched. In fact, the attempts to solve
the problem have perhaps produced even more
open questions than there were in the beginning.
93
The main new and open challenges are described
in this section.
What is the best analysis algorithm? Some
of the suggested algorithms have produced good
test results and some even in several tasks and lan-
guages, such as Bernhard (Bernhard, 2006), Mon-
son ParaMor+Morfessor (Monson et al, 2008)
and Allomorfessor (Virpioja and Kohonen, 2009).
However, none of the methods perform really well
in all the evaluation tasks and languages and their
mutual performance differences are often rather
small, even though the morphemes and the al-
gorithmic principles are totally different. Thus,
no dominant morpheme analysis algorithm have
been found. Furthermore, reaching the perfor-
mance level that rivals, or even sometimes domi-
nates, the rule-based and language-dependent ref-
erence methods does not mean that the solutions
are sufficient. Often the limited coverage or un-
suitable level of details in the analysis for the task
in the reference methods just indicates that they
are not sufficient either and better solutions are
needed. Another observation which complicates
the finding and determination of the best algorithm
is that in some tasks, such as statistical language
models for speech recognition, very different al-
gorithms can reach the same performance, because
advanced modelling methods can compensate for
unsuitable morpheme analysis.
What is the meaning of the morphemes? In
some of the fundamental applications of mor-
pheme analysis, such as text understanding, mor-
pheme segmentation alone is only part of the solu-
tion. Even more important is to find the meaning
for the obtained morphemes. The extension of the
segmentation of words into smaller units to iden-
tification of the units that correspond to the same
morpheme is a step taken to this direction, but the
question of the meaning of the morpheme is still
open. However, in the unsupervised way of learn-
ing, solutions to this may be so tightly tied to the
applications that much more complex evaluations
would be needed.
How to evaluate the alternative analyses? It
is clear that when a word form is separated from
the sentence context where it was used, the mor-
pheme analysis easily becomes ambiguous. In the
Morpho Challenge evaluations this has been taken
into account by allowing multiple alternative anal-
yses. However, in some evaluations, for exam-
ple, in the measurement of the recall of the gold
standard morphemes, this leads to unwanted re-
sults and may favour methods that always provide
a large number of alternative analysis.
How to improve the analysis using context?
A natural way to disambiguate the analysis in-
volves taking the sentence context into account.
Some of the Morpho Challenge evaluations, for
example, the information retrieval, allow this op-
tion when the source texts and queries are given.
However, this has not been widely tried yet by
the participants, probably because of the increased
computational complexity of the modelling task.
How to effectively apply semi-supervised
learning? In semi-supervised learning, a small set
of labeled data in the form of gold standard anal-
ysis for the word forms are provided. This data
can be used for improving the unsupervised solu-
tions based on unlabeled data in several ways: (1)
The labeled data is used for tuning some learning
parameters, followed by an unsupervised learning
process for the unlabeled data. (2) The labeled
morphemes are used as an ideal starting point
to bootstrap the learning on the unlabeled words
(self-training). (3) Using the EM algorithm for es-
timating a generative model, the unlabeled cases
can be treated as missing data.
The best and most practical way of using the
partly labeled data will be determined in future
when the semi-supervised task has been evaluated
in the future Morpho Challenge evaluations. For
the first time this task will be evaluated in the on-
going Morpho Challenge 2010.
Acknowledgments
We are grateful to the University of Leipzig,
University of Leeds, Computational Linguistics
Group at University of Haifa, Stefan Bordag,
Ebru Arisoy, Nizar Habash, Majdi Sawalha, Eric
Atwell, and Mathias Creutz for making the data
and gold standards in various languages available
to the Challenge. This work was supported by the
Academy of Finland in the project Adaptive In-
formatics, the graduate schools in Language Tech-
nology and Computational Methods of Informa-
tion Technology, in part by the GALE program of
the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022, and in part by
the IST Programme of the European Community,
under the FP7 project EMIME (213845) and PAS-
CAL Network of Excellence.
94
References
Steven Abney. 2007. Semisupervised Learning
for Computational Linguistics. Chapman and
Hall/CRC.
Eneko Agirre, Giorgio M. Di Nunzio, Nicola Ferro,
Thomas Mandl, and Carol Peters. 2008. CLEF
2008: Ad hoc track overview. In Working Notes for
the CLEF 2008 Workshop.
Delphine Bernhard. 2006. Unsupervised morpholog-
ical segmentation based on segment predictability
and word segments alignment. In Proc. PASCAL
Challenge Workshop on Unsupervised segmentation
of words into morphemes, Venice, Italy. PASCAL
European Network of Excellence.
Stefan Bordag. 2006. Two-step approach to unsuper-
vised morpheme segmentation. In Proc. of the PAS-
CAL Challenge Workshop on Unsupervised segmen-
tation of words into morphemes, Venice, Italy. PAS-
CAL European Network of Excellence.
Mathias Creutz and Krista Lagus. 2002. Unsu-
pervised discovery of morphemes. In Proc. SIG-
PHON/ACL?02, pages 21?30.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proc. AKRR?05, pages 106?
113.
Adria de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum bayes risk
combination of translation hypothesis from alter-
native morphological decompositions. In Proc.
NAACL?09, pages 73-76.
C. G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222. Reprinted 1970 in Pa-
pers in Structural and Transformational Linguistics,
Reidel Publishing Company, Dordrecht, Holland.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proc.
ACL?06, pages 209?216.
Oskar Kohonen, Sami Virpioja, and Mikaela Klami.
2009. Allomorfessor: Towards unsupervised mor-
pheme analysis. In Evaluating systems for Mul-
tilingual and MultiModal Information Access, 9th
Workshop of the Cross-Language Evaluation Forum,
CLEF 2008, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5706. Springer.
Mikko Kurimo, Mathias Creutz, and Krista Lagus.
2006. Unsupervised segmentation of words into
morphemes - challenge 2005, an introduction and
evaluation report. In Proc. PASCAL Challenge
Workshop on Unsupervised segmentation of words
into morphemes, Venice, Italy. PASCAL European
Network of Excellence.
Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.
2008. Morpho Challenge evaluation using a linguis-
tic Gold Standard. In Advances in Multilingual and
MultiModal Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum, CLEF 2007,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5152, pages 864?873. Springer.
Mikko Kurimo, Ville Turunen, and Matti Varjokallio.
2009a. Overview of Morpho Challenge 2008.
In Evaluating systems for Multilingual and Mul-
tiModal Information Access, 9th Workshop of the
Cross-Language Evaluation Forum, CLEF 2008,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5706. Springer.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009b.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece.
Constantine Lignos, Erwin Chan, Mitchell P. Marcus,
and Charles Yang. 2009. A rule-based unsupervised
morphology learning framework. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece.
Paul McNamee. 2008. Retrieval experiments at mor-
pho challenge 2008. InWorking Notes for the CLEF
2008 Workshop, Aarhus, Denmark, September.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. ParaMor: Finding paradigms
across morphology. In Advances in Multilingual
and MultiModal Information Retrieval, 8th Work-
shop of the Cross-Language Evaluation Forum,
CLEF 2007, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5152. Springer.
Christian Monson, Kristy Hollingshead, and Brian
Roard. 2009. Probabilistic paraMor. In Working
Notes for the CLEF 2009 Workshop, Corfu, Greece,
September.
Sebastian Spiegler, Bruno Golenia, and Peter Flach.
2009. PROMODES: A probabilistic generative
model for word decomposition. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece,
September.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme discovery with Allomorfessor. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Xiaojin Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
Xiaojin Zhu. 2010. Semi-supervised learning. In En-
cyclopedia of Machine Learning. To appear.
95
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 29?37,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Supervised Morphological Segmentation in a Low-Resource Learning
Setting using Conditional Random Fields
Teemu Ruokolainena Oskar Kohonena Sami Virpiojaa Mikko Kurimob
a Department of Information and Computer Science, Aalto University
b Department of Signal Processing and Acoustics, Aalto University
firstname.lastname@aalto.fi
Abstract
We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, the surface forms
of morphemes. Our focus is on a low-
resource learning setting, in which only a
small amount of annotated word forms are
available for model training, while unan-
notated word forms are available in abun-
dance. The current state-of-art methods
1) exploit both the annotated and unan-
notated data in a semi-supervised man-
ner, and 2) learn morph lexicons and sub-
sequently uncover segmentations by gen-
erating the most likely morph sequences.
In contrast, we discuss 1) employing only
the annotated data in a supervised man-
ner, while entirely ignoring the unanno-
tated data, and 2) directly learning to pre-
dict morph boundaries given their local
sub-string contexts instead of learning the
morph lexicons. Specifically, we em-
ploy conditional random fields, a popular
discriminative log-linear model for seg-
mentation. We present experiments on
two data sets comprising five diverse lan-
guages. We show that the fully super-
vised boundary prediction approach out-
performs the state-of-art semi-supervised
morph lexicon approaches on all lan-
guages when using the same annotated
data sets.
1 Introduction
Modern natural language processing (NLP) appli-
cations, such as speech recognition, information
retrieval and machine translation, perform their
tasks using statistical language models. For mor-
phologically rich languages, estimation of the lan-
guage models is problematic due to the high num-
ber of compound words and inflected word forms.
A successful means of alleviating this data sparsity
problem is to segment words into meaning-bearing
sub-word units (Hirsim?ki et al, 2006; Creutz et
al., 2007; Turunen and Kurimo, 2011). In lin-
guistics, the smallest meaning-bearing units of a
language are called morphemes and their surface
forms morphs. Thus, morphs are natural targets
for the segmentation.
For most languages, existing resources contain
large amounts of raw unannotated text data, only
small amounts of manually prepared annotated
training data, and no freely available rule-based
morphological analyzers. The focus of our work is
on performing morphological segmentation in this
low-resource scenario. Given this setting, the cur-
rent state-of-art methods approach the problem by
learning morph lexicons from both annotated and
unannotated data using semi-supervised machine
learning techniques (Poon et al, 2009; Kohonen
et al, 2010). Subsequent to model training, the
methods uncover morph boundaries for new word
forms by generating their most likely morph se-
quences according to the morph lexicons.
In contrast to learning morph lexicons (Poon et
al., 2009; Kohonen et al, 2010), we study mor-
phological segmentation by learning to directly
predict morph boundaries based on their local sub-
string contexts. Specifically, we apply the linear-
chain conditional random field model, a popular
discriminative log-linear model for segmentation
presented originally by Lafferty et al (2001). Im-
portantly, we learn the segmentation model from
solely the small annotated data in a supervised
manner, while entirely ignoring the unannotated
data. Despite not using the unannotated data, we
show that by discriminatively learning to predict
the morph boundaries, we are able to outperform
the previous state-of-art.
We present experiments on Arabic and Hebrew
using the data set presented originally by Snyder
and Barzilay (2008), and on English, Finnish and
29
Turkish using the Morpho Challenge 2009/2010
data sets (Kurimo et al, 2009; Kurimo et al,
2010). The results are compared against two state-
of-art techniques, namely the log-linear model-
ing approach presented by Poon et al (2009) and
the semi-supervised Morfessor algorithm (Koho-
nen et al, 2010). We show that when employ-
ing the same small amount of annotated train-
ing data, the CRF-based boundary prediction ap-
proach outperforms these reference methods on
all languages. Additionally, since the CRF model
learns from solely the small annotated data set, its
training is computationally much less demanding
compared to the semi-supervised methods, which
utilize both the annotated and the unannotated data
sets.
The rest of the paper is organized as follows. In
Section 2, we discuss related work in morpholog-
ical segmentation and methodology. In Section 3,
we describe our segmentation method. Our exper-
imental setup is described in Section 4, and the
obtained results are presented in Section 5. In Sec-
tion 6, we discuss the method and the results. Fi-
nally, we present conclusions on the work in Sec-
tion 7.
2 Related work
The CRF model has been widely used in NLP seg-
mentation tasks, such as shallow parsing (Sha and
Pereira, 2003), named entity recognition (McCal-
lum and Li, 2003), and word segmentation (Zhao
et al, 2006). Recently, CRFs were also employed
successfully in morphological segmentation for
Arabic by Green and DeNero (2012) as a com-
ponent of an English to Arabic machine trans-
lation system. While the segmentation method
of Green and DeNero (2012) and ours is very sim-
ilar, our focuses and contributions differ in sev-
eral ways. First, while in our work we consider
the low-resource learning setting, in which a small
annotated data set is available (up to 3,130 word
types), their model is trained on the Arabic Tree-
bank (Maamouri et al, 2004) constituting sev-
eral times larger training set (588,244 word to-
kens). Second, we present empirical comparison
between the CRF approach and two state-of-art
methods (Poon et al, 2009; Kohonen et al, 2010)
on five diverse languages. Third, due to being a
component of a larger system, their presentation
on the method and experiments is rather undersp-
eficied, while here we are able to provide a more
thorough description.
In the experimental section, we compare the
CRF-based segmentation approach with two state-
of-art methods, the log-linear modeling approach
presented by Poon et al (2009) and the semi-
supervised Morfessor algorithm (Kohonen et al,
2010). As stated previously, the CRF-based seg-
mentation approach differs from these methods in
that it learns to predict morph boundaries from
a small amount of annotated data, in contrast to
learning morph lexicons from both annotated and
large amounts of unannotated data.
Lastly, there exists ample work on varying un-
supervised (and semi-supervised) morphological
segmentation methods. A useful review is given
by Hammarstr?m and Borin (2011). The funda-
mental difference between our approach and these
techniques is that our method necessarily requires
manually annotated training data.
3 Methods
In this section, we describe in detail the CRF-
based approach for supervised morphological seg-
mentation.
3.1 Morphological segmentation as a
classification task
We represent the morphological segmentation task
as a structured classification problem by assign-
ing each character to one of four classes, namely
{beginning of a multi-character morph (B), mid-
dle of a multi-character morph (M), end of a multi-
character morph (E), single character morph (S)}.
For example, consider the English word form
drivers
with a corresponding segmentation
driv + er + s .
Using the classification notation, this segmenta-
tion is represented as
START B M M E B E S STOP
<w> d r i v e r s </w>
where we have assumed additional word start
and end markers <w> and </w> with respective
classes START and STOP. As another example,
consider the Finnish word form
autoilla (with cars)
with a corresponding segmentation
auto + i + lla .
Using the classification notation, this segmenta-
tion is represented as
30
START B M M E S B M E STOP
<w> a u t o i l l a </w>
Intuitively, instead of the four class set {B, M,
E, S}, a segmentation could be accomplished us-
ing only a set of two classes {B, M} as in (Green
and DeNero, 2012). However, similarly to Chi-
nese word segmentation (Zhao et al, 2006), our
preliminary experiments suggested that using the
more fine-grained four class set {B, M, E, S} per-
formed slightly better. This result indicates that
morph segments of differerent lengths behave dif-
ferently.
3.2 Linear-chain conditional random fields
We perform the above structured classification us-
ing linear-chain conditional random fields (CRFs),
a discriminative log-linear model for tagging and
segmentation (Lafferty et al, 2001). The central
idea of the linear-chain CRF is to exploit the de-
pendencies between the output variables using a
chain structured undirected graph, also referred to
as a Markov random field, while conditioning the
output globally on the observation.
Formally, the model for input x (characters in a
word) and output y (classes corresponding to char-
acters) is written as
p (y |x;w) ?
T?
t=2
exp
(
w>f(yt?1, yt,x, t)
)
,
(1)
where t indexes the characters, T denotes word
length, w the model parameter vector, and f the
vector-valued feature extracting function.
The purpose of the feature extraction function
f is to capture the co-occurrence behavior of the
tag transitions (yt?1, yt) and a set of features de-
scribing character position t of word form x. The
strength of the CRF model lies in its capability to
utilize arbitrary, non-independent features.
3.3 Feature extraction
The quality of the segmentation depends heavily
on the choice of features defined by the feature
extraction function f . We will next describe and
motivate the feature set used in the experiments.
Our feature set consists of binary indicator func-
tions describing the position t of word x using
all left and right substrings up to a maximum
length ?. For example, consider the problem
of deciding if the letter e in the word drivers
is preceded by a morph boundary. This deci-
sion is now based on the overlapping substrings
to the left and right of this potential bound-
ary position, that is {v, iv, riv, driv, <w>driv} and
{e, er, ers, ers</w>}, respectively. The substrings
to the left and right are considered indepen-
dently. Naturally, if the maximum allowed sub-
string length ? is less than five, the longest sub-
strings are discarded accordingly. In general, the
optimum ? depends on both the amount of avail-
able training data and the language.
In addition to the substring functions, we use a
bias function which returns value 1 independent
of the input x. The bias and substring features are
combined with all the possible tag transitions.
To motivate this choice of feature set, consider
formulating an intuitive segmentation rule for the
English words talked, played and speed with the
correct segmentations talk + ed, play + ed and
speed, respectively. Now, as a right context ed
is generally a strong indicator of a boundary, one
could first formulate a rule
position t is a segment boundary
if its right context is ed.
This rule would indeed correctly segment the
words talked and played, but would incorrectly
segment speed as spe + ed. This error can be re-
solved if the left contexts are utilized as inhibitors
by expanding the above rule as
position t is a segment boundary
if its right context is ed
and the left context is not spe.
Using the feature set defined above, the CRF
model can learn to perform segmentation in this
rule-like manner according to the training data.
For example, using the above example words and
segmentations for training, the CRFs could learn
to assign a high score for a boundary given that
the right context is ed and a high score for a non-
boundary given the left context spe. Subsequent to
training, making segmentation decisions for new
word forms can then be interpreted as voting based
on these scores.
3.4 Parameter estimation
The CRF model parameters w are estimated based
on an annotated training data set. Common train-
ing criteria include the maximum likelihood (Laf-
ferty et al, 2001; Peng et al, 2004; Zhao et al,
2006), averaged structured perceptron (Collins,
2002), and max-margin (Szummer et al, 2008).
In this work, we estimate the parameters using the
perceptron algorithm (Collins, 2002).
31
In perceptron training, the required graph infer-
ence can be efficiently performed using the stan-
dard Viterbi algorithm. Subsequent to training, the
segmentations for test instances are acquired again
using Viterbi search.
Compared to other training criteria, the struc-
tured perceptron has the advantage of employing
only a single hyperparameter, namely the number
of passes over training data, making model esti-
mation fast and straightforward. We optimize the
hyperparameter using a separate development set.
Lastly, we consider the longest substring length ?
a second hyperparameter optimized using the de-
velopment set.
4 Experimental setup
This section describes the data sets, evaluation
metrics, reference methods, and other details con-
cerning the evaluation of the methods.
4.1 Data sets
We evaluate the methods on two different data sets
comprising five languages in total.
S&B data. The first data set we use is the He-
brew Bible parallel corpus introduced by Snyder
and Barzilay (2008). It contains 6,192 parallel
phrases in Hebrew, Arabic, Aramaic, and English
and their frequencies (ranging from 5 to 3517).
The phrases have been extracted using automatic
word alignment. The Hebrew and Arabic phrases
have manually annotated morphological segmen-
tations, and they are used in our experiments. The
phrases are sorted according to frequency, and ev-
ery fifth phrase starting from the first phrase is
placed in the test set, every fifth starting from the
second phrase in the development set (up to 500
phrases), and the rest of the phrases in the train-
ing set. 1 The total numbers of word types in the
sets are shown in Table 1. Finally, the word forms
in the training set are randomly permuted, and the
first 25%, 50%, 75%, and 100% of them are se-
lected as subsets to study the effect of training data
size.
MC data. The second data set is based on the
Morpho Challenge 2010 (Kurimo et al, 2010).
It includes manually prepared morphological seg-
mentations in English, Finnish and Turkish. The
1We are grateful to Dr. Hoifung Poon for providing us
instructions for dividing of the data set.
Arabic Hebrew
Training 3,130 2,770
Development 472 450
Test 1,107 1,040
Table 1: The numbers of word types in S&B data
sets (Snyder and Barzilay, 2008).
English Finnish Turkish
Unannot. 384,903 2,206,719 617,298
Training 1,000 1,000 1,000
Develop. 694 835 763
Test 10?1,000 10?1,000 10?1,000
Table 2: The numbers of word types in the MC
data sets (Kurimo et al, 2009; Kurimo et al,
2010).
additional German corpus does not have segmen-
tation annotation and is therefore excluded. The
annotated data sets include training, development,
and test sets for each language. Following Virpi-
oja et al (2011), the test set results are based on
ten randomly selected 1,000 word sets. Moreover,
we divide the annotated training sets into ten par-
titions with respective sizes of 100, 200, . . . , 1000
words so that each partition is a subset of the all
larger partitions. The data is divided so that the
smallest set had every 10th word of the original
set, the second set every 10th word and the fol-
lowing word, and so forth. For reference methods
that require unannotated data, we use the English,
Finnish and Turkish corpora from Competition 1
of Morpho Challenge 2009 (Kurimo et al, 2009).
Table 2 shows the sizes of the MC data sets.
4.2 Evaluation measures
The word segmentations are evaluated by compar-
ison with linguistic morphs using precision, recall,
and F-measure. The F-measure equals the geo-
metric mean of precision (the percentage of cor-
rectly assigned boundaries with respect to all as-
signed boundaries) and recall (the percentage of
correctly assigned boundaries with respect to the
reference boundaries). While using F-measure is
a standard procedure, the prior work differ at least
in three details: (1) whether precision and recall
are calculated as micro-average over all segmenta-
tion points or as macro-average over all the word
forms, (2) whether the evaluation is based on word
types or word tokens in a corpus, and (3) if the
32
reference segmentations have alternative correct
choices for a single word type, and how to deal
with them.
For the experiments with the S&B data sets,
we follow Poon et al (2009) and apply token-
based micro-averages. For the experiments with
the MC data sets, we follow Virpioja et al (2011)
and use type-based macro-averages. However, dif-
fering from their boundary measure, we take the
best match over the alternative reference analyses
(separately for precision and recall), since none of
the methods considered here provide multiple seg-
mentations per word type. For the models trained
with the full training set, we also report the F-
measures of the boundary evaluation method by
Virpioja et al (2011) in order to compare to the
results reported in the Morpho Challenge website.
4.3 CRF feature extraction and training
The features included in the feature vector in the
CRF model (1) are described in Section 3.3. We
include all substring features which occur in the
training data.
The CRF model is trained using the averaged
perceptron algorithm as described in Section 3.4.
The algorithm initializes the model parameters
with zero vectors. The model performance, mea-
sured using F-measure, is evaluated on the devel-
opment set after each pass over the training set,
and the training is terminated when the perfor-
mance has not improved during last 5 passes. The
maximum length of substrings ? is optimized by
considering ? = 1, 2, 3, . . . , and the search is ter-
minated when the performance has not improved
during last 5 values. Finally, the algorithm returns
the parameters yielding the highest F-measure on
the development set.
For some words, the MC training sets include
several alternative segmentations. We resolve this
ambiguity by using the first given alternative and
discarding the rest. During evaluation, the alter-
native segmentations are taken into account as de-
scribed in Section 4.2.
The experiments are run on a standard desktop
computer using our own single-threaded Python-
based implementation2.
4.4 Reference methods
We compare our method?s performance on Arabic
and Hebrew data with semi-supervised Morfessor
2Available at http://users.ics.aalto.fi/
tpruokol/
(Kohonen et al, 2010) and the results reported by
Poon et al (2009). On Finnish, English and Turk-
ish data, we compare the method only with semi-
supervised Morfessor as we have no implementa-
tion of the model by Poon et al (2009).
We use a recently released Python implemen-
tation of semi-supervised Morfessor3. Semi-
supervised Morfessor was trained separately for
each training set size, always using the full unan-
notated data sets in addition to the annotated sets.
The hyperparameters, the unannotated data weight
? and the annotated data weight ?, were optimized
with a grid search on the development set. For the
S&B data, there are no separate unannotated sets.
When the annotated training set size is varied, the
remaining parts are utilized as unannotated data.
The log-linear model described in (Poon et al,
2009) and the semi-supervised Morfessor algo-
rithm are later referred to as POON-2009 and S-
MORFESSOR for brevity.
5 Results
Method performances for Arabic and Hebrew on
the S&B data are presented in Tables 3 and 4, re-
spectively. The results for the POON-2009 model
are extracted from (Poon et al, 2009). Perfor-
mances for English, Finnish and Turkish on the
MC data set are presented in Tables 5, 6 and 7,
respectively.
On the Arabic and Hebrew data sets, the CRFs
outperform POON-2009 and S-MORFESSOR
substantially on all the considered data set sizes.
On Finnish and Turkish data, the CRFs outper-
form S-MORFESSOR except for the smallest sets
of 100 instances. On English data, the CRFs out-
perform S-MORFESSOR when the training set is
500 instances or larger.
Using our implementation of the CRF model,
obtaining the results for Arabic, Hebrew, English,
Finnish, and Turkish consumed 10, 11, 22, 32,
and 28 minutes, respectively. These CPU times
include model training and hyperparameter opti-
mization. In comparison, S-MORFESSOR train-
ing is considerably slower. For Arabic and He-
brew, the S-MORFESSOR total training times
were 24 and 22 minutes, respectively, and for En-
glish, Finnish, and Turkish 4, 22, and 10 days,
respectively. The higher training times of S-
MORFESSOR are partly because of the larger
3Available at https://github.com/
aalto-speech/morfessor
33
grids in hyperparameter optimization. Further-
more, the S-MORFESSOR training time for each
grid point grows linearly with the size of the
unannotated data set, resulting in particularly slow
training on the MC data sets. All reported times
are total CPU times for single-threaded runs, while
in practice grid searches can be parallelized.
The perceptron algorithm typically converged
after 10 passes over the training set, and never re-
quired more than 40 passes to terminate. Depend-
ing on the size of the training data, the optimized
maximum lengths of substrings varied in ranges
{3,5}, {2,7}, {3,9}, {3,6}, {3,7}, for Arabic, He-
brew, English, Finnish and Turkish, respectively.
Method %Lbl. Prec. Rec. F1
CRF 25 95.5 93.1 94.3
S-MORFESSOR 25 78.7 79.7 79.2
POON-2009 25 84.9 85.5 85.2
CRF 50 96.5 94.6 95.5
S-MORFESSOR 50 87.5 91.5 89.4
POON-2009 50 88.2 86.2 87.5
CRF 75 97.2 96.1 96.6
S-MORFESSOR 75 92.8 83.0 87.6
POON-2009 75 89.6 86.4 87.9
CRF 100 98.1 97.5 97.8
S-MORFESSOR 100 91.4 91.8 91.6
POON-2009 100 91.7 88.5 90.0
Table 3: Results for Arabic on the S&B data
set (Snyder and Barzilay, 2008). The column ti-
tled %Lbl. denotes the percentage of the annotated
data used for training. In addition to the given per-
centages of annotated data, POON-2009 and S-
MORFESSOR utilized the remainder of the data
as an unannotated set.
Finally, Table 8 shows the results of the CRF
and S-MORFESSOR models trained with the full
English, Finnish, and Turkish MC data sets and
evaluated with the boundary evaluation method of
Virpioja et al (2011). That is, these numbers are
directly comparable to the BPR-F column in the
result tables presented at the Morpho Challenge
website4. For each of the three languages, CRF
clearly outperforms all the Morpho Challenge sub-
missions that have provided morphological seg-
mentations.
4http://research.ics.aalto.fi/events/
morphochallenge/
Method %Lbl. Prec. Rec. F1
CRF 25 90.5 90.6 90.6
S-MORFESSOR 25 71.5 85.3 77.8
POON-2009 25 78.7 73.3 75.9
CRF 50 94.0 91.5 92.7
S-MORFESSOR 50 82.1 81.8 81.9
POON-2009 50 82.8 74.6 78.4
CRF 75 94.0 92.7 93.4
S-MORFESSOR 75 84.0 88.1 86.0
POON-2009 75 83.1 77.3 80.1
CRF 100 94.9 94.0 94.5
S-MORFESSOR 100 85.3 91.1 88.1
POON-2009 100 83.0 78.9 80.9
Table 4: Results for Hebrew on the S&B data
set (Snyder and Barzilay, 2008). The column ti-
tled %Lbl. denotes the percentage of the annotated
data used for training. In addition to the given per-
centages of annotated data, POON-2009 and S-
MORFESSOR utilized the remainder of the data
as an unannotated set.
6 Discussion
Intuitively, the CRF-based supervised learning ap-
proach should yield high segmentation accuracy
when there are large amounts of annotated train-
ing data available. However, perhaps surprisingly,
the CRF model yields state-of-art results already
using very small amounts of training data. This
result is meaningful since for most languages it is
infeasible to acquire large amounts of annotated
training data.
The strength of the discriminatively trained
CRF model is that overlapping, non-independent
features can be naturally employed. Importantly,
we showed that simple, language-independent
substring features are sufficient for high perfor-
mance. However, adding new, task- and language-
dependent features is also easy. One might, for ex-
ample, explore features capturing vowel harmony
in Finnish and Turkish.
The CRFs was estimated using the structured
perceptron algorithm (Collins, 2002), which has
the benefit of being computationally efficient and
easy to implement. Other training criteria, such
as maximum likelihood (Lafferty et al, 2001)
or max-margin (Szummer et al, 2008), could
also be employed. Similarly, other classifiers,
such as the Maximum Entropy Markov Models
(MEMMs) (McCallum et al, 2000), are applica-
ble. However, as the amount of information in-
34
Method Train. Prec. Rec. F1
CRF 100 80.2 74.6 77.3
S-MORFESSOR 100 88.1 79.7 83.7
CRF 200 84.7 79.2 81.8
S-MORFESSOR 200 88.1 79.5 83.6
CRF 300 86.7 79.8 83.1
S-MORFESSOR 300 88.4 80.6 84.3
CRF 400 86.5 80.6 83.4
S-MORFESSOR 400 84.6 83.6 84.1
CRF 500 88.6 80.7 84.5
S-MORFESSOR 500 86.3 82.7 84.4
CRF 600 88.1 82.6 85.3
S-MORFESSOR 600 86.7 82.5 84.5
CRF 700 87.9 83.4 85.6
S-MORFESSOR 700 86.0 82.9 84.4
CRF 800 89.1 83.2 86.1
S-MORFESSOR 800 87.1 82.5 84.8
CRF 900 89.0 82.9 85.8
S-MORFESSOR 900 86.4 82.6 84.5
CRF 1000 89.8 83.5 86.5
S-MORFESSOR 1000 88.8 80.1 84.3
Table 5: Results for English on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 384,903 word types.
corporated in the model would be unchanged, the
choice of parameter estimation criterion and clas-
sifier is unlikely to have a dramatic effect on the
method performance.
In CRF training, we focused on the supervised
learning scenario, in which no unannotated data is
exploited in addition to the annotated training sets.
However, there does exist ample work on extend-
ing CRF training to the semi-supervised setting
(for example, see Mann and McCallum (2008)
and the references therein). Nevertheless, our re-
sults strongly suggest that it is crucial to use the
few available annotated training instances as ef-
ficiently as possible before turning model train-
ing burdensome by incorporating large amounts of
unannotated data.
Following previous work (Poon et al, 2009;
Kohonen et al, 2010; Virpioja et al, 2011), we
applied the boundary F-score evaluation measure,
while Green and DeNero (2012) reported charac-
ter accuracy. We consider the boundary F-score a
better measure than accuracy, since the boundary-
Method Train. Prec. Rec. F1
CRF 100 71.4 66.0 68.6
S-MORFESSOR 100 69.8 71.0 70.4
CRF 200 76.4 71.3 73.8
S-MORFESSOR 200 75.5 68.6 71.9
CRF 300 80.4 73.9 77.0
S-MORFESSOR 300 73.1 71.8 72.5
CRF 400 81.0 76.6 78.7
S-MORFESSOR 400 73.3 74.3 73.8
CRF 500 82.9 77.9 80.3
S-MORFESSOR 500 73.5 75.1 74.3
CRF 600 82.6 80.6 81.6
S-MORFESSOR 600 76.1 73.7 74.9
CRF 700 84.3 81.4 82.8
S-MORFESSOR 700 75.0 76.6 75.8
CRF 800 85.1 83.4 84.2
S-MORFESSOR 800 74.1 78.2 76.1
CRF 900 85.2 83.8 84.5
S-MORFESSOR 900 74.2 78.5 76.3
CRF 1000 86.0 84.7 85.3
S-MORFESSOR 1000 74.2 78.8 76.4
Table 6: Results for Finnish on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 2,206,719 word
types.
tag distribution is strongly skewed towards non-
boundaries. Nevertheless, for completeness, we
computed the character accuracy for our Arabic
data set, obtaining the accuracy 99.1%, which is
close to their reported accuracy of 98.6%. How-
ever, these values are not directly comparable due
to our use of the Bible corpus by Snyder and Barzi-
lay (2008) and their use of the Penn Arabic Tree-
bank (Maamouri et al, 2004).
7 Conclusions
We have presented an empirical study in data-
driven morphological segmentation employing
supervised boundary prediction methodology.
Specifically, we applied conditional random fields,
a discriminative log-linear model for segmentation
and tagging. From a methodological perspective,
this approach differs from the previous state-of-art
methods in two fundamental aspects. First, we uti-
lize a discriminative model estimated using only
annotated data. Second, we learn to predict morph
35
Method Train. Prec. Rec. F1
CRF 100 72.4 79.6 75.8
S-MORFESSOR 100 77.9 78.5 78.2
CRF 200 83.2 82.3 82.8
S-MORFESSOR 200 80.0 83.2 81.6
CRF 300 83.9 85.9 84.9
S-MORFESSOR 300 80.1 85.6 82.8
CRF 400 86.4 86.5 86.4
S-MORFESSOR 400 80.7 87.1 83.8
CRF 500 87.5 86.4 87.0
S-MORFESSOR 500 81.0 87.2 84.0
CRF 600 87.8 88.1 87.9
S-MORFESSOR 600 80.5 89.9 85.0
CRF 700 89.1 88.3 88.7
S-MORFESSOR 700 80.9 90.7 85.5
CRF 800 88.6 90.3 89.4
S-MORFESSOR 800 81.2 91.0 85.9
CRF 900 89.2 89.8 89.5
S-MORFESSOR 900 81.4 91.2 86.0
CRF 1000 89.9 90.4 90.2
S-MORFESSOR 1000 83.0 91.5 87.0
Table 7: Results for Turkish on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 617,298 word types.
boundaries based on their local character substring
contexts instead of learning a morph lexicon.
We showed that our supervised method yields
improved results compared to previous state-of-
art semi-supervised methods using the same small
amount of annotated data, while not utilizing the
unannotated data used by the reference methods.
This result has two implications. First, supervised
methods can provide excellent results in morpho-
logical segmentation already when there are only
a few annotated training instances available. This
is meaningful since for most languages it is infea-
sible to acquire large amounts of annotated train-
ing data. Second, performing morphological seg-
mentation by directly modeling segment bound-
aries can be advantageous compared to modeling
morph lexicons.
A potential direction for future work includes
evaluating the morphs obtained by our method in
real world applications, such as speech recognition
and information retrieval. We are also interested
in extending the method from fully supervised to
Method English Finnish Turkish
CRF 82.0 81.9 71.5
S-MORFESSOR 79.6 73.5 70.5
Table 8: F-measures of the Morpho Chal-
lenge boundary evaluation for CRF and S-
MORFESSOR using the full annotated training
data set.
semi-supervised learning.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012?2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).
References
M. Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), volume 10,
pages 1?8. Association for Computational Linguis-
tics.
M. Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,
J. Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Sara?lar, and A Stolcke. 2007. Morph-
based speech recognition and modeling of out-of-
vocabulary words across languages. ACM Transac-
tions on Speech and Language Processing, 5(1):3:1?
3:29, December.
S. Green and J. DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 146?155.
Association for Computational Linguistics.
H. Hammarstr?m and L. Borin. 2011. Unsupervised
learning of morphology. Computational Linguistics,
37(2):309?350, June.
T. Hirsim?ki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkk?nen. 2006. Unlimited vocabu-
lary speech recognition with morph language mod-
els applied to Finnish. Computer Speech and Lan-
guage, 20(4):515?541, October.
O. Kohonen, S. Virpioja, and K. Lagus. 2010. Semi-
supervised learning of concatenative morphology.
In Proceedings of the 11th Meeting of the ACL Spe-
cial Interest Group on Computational Morphology
36
and Phonology, pages 78?86, Uppsala, Sweden,
July. Association for Computational Linguistics.
M. Kurimo, S. Virpioja, V. Turunen, G. W. Blackwood,
and W. Byrne. 2009. Overview and results of Mor-
pho Challenge 2009. In Working Notes for the CLEF
2009 Workshop, Corfu, Greece, September.
M. Kurimo, S. Virpioja, and V. Turunen. 2010.
Overview and results of Morpho Challenge 2010. In
Proceedings of the Morpho Challenge 2010 Work-
shop, pages 7?24, Espoo, Finland, September. Aalto
University School of Science and Technology, De-
partment of Information and Computer Science.
Technical Report TKK-ICS-R37.
J. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, pages 282?289.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The penn arabic treebank: Building a large-
scale annotated arabic corpus. In NEMLAR Con-
ference on Arabic Language Resources and Tools,
pages 102?109.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of con-
ditional random fields. In Proceedings of ACL-
08: HLT, pages 870?878. Association for Compu-
tational Linguistics.
A. McCallum and W. Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 188?191. Association for Computational Lin-
guistics.
A. McCallum, D. Freitag, and F. Pereira. 2000. Max-
imum entropy Markov models for information ex-
traction and segmentation. In Pat Langley, editor,
Proceedings of the Seventeenth International Con-
ference on Machine Learning (ICML 2000), pages
591?598, Stanford, CA, USA. Morgan Kaufmann.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics (COLING 2004), page 562. Association for
Computational Linguistics.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsuper-
vised morphological segmentation with log-linear
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 209?217. Association for
Computational Linguistics.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 134?
141. Association for Computational Linguistics.
B. Snyder and R. Barzilay. 2008. Crosslingual prop-
agation for morphological analysis. In Proceedings
of the AAAI, pages 848?854.
M. Szummer, P. Kohli, and D. Hoiem. 2008. Learn-
ing CRFs using graph cuts. Computer Vision?ECCV
2008, pages 582?595.
V. Turunen and M. Kurimo. 2011. Speech retrieval
from unsegmented Finnish audio using statistical
morpheme-like units for segmentation, recognition,
and retrieval. ACM Transactions on Speech and
Language Processing, 8(1):1:1?1:25, October.
S. Virpioja, V. Turunen, S. Spiegler, O. Kohonen, and
M. Kurimo. 2011. Empirical comparison of eval-
uation methods for unsupervised learning of mor-
phology. Traitement Automatique des Langues,
52(2):45?90.
H. Zhao, C.N. Huang, and M. Li. 2006. An improved
chinese word segmentation system with conditional
random field. In Proceedings of the Fifth SIGHAN
Workshop on Chinese Language Processing, volume
1082117. Sydney: July.
37
