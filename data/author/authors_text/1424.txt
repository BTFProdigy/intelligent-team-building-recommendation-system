83
84
85
86
87
88
89
90
Prague Czech-English Dependency Treebank
Any Hopes for a Common Annotation Scheme?
Martin ?Cmejrek, Jan Cur???n, Jir??? Havelka
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics, Charles University in Prague
Malostranske? na?m. 25, Praha 1, Czech
{cmejrek,curin,havelka}@ufal.mff.cuni.cz
Abstract
The Prague Czech-English Dependency Tree-
bank (PCEDT) is a new syntactically annotated
Czech-English parallel resource. The Penn
Treebank has been translated to Czech, and its
annotation automatically transformed into de-
pendency annotation scheme. The dependency
annotation of Czech is done from plain text by
automatic procedures. A small subset of cor-
responding Czech and English sentences has
been annotated by humans. We discuss some
of the problems we have experienced during
the automatic transformation between annota-
tion schemes and hint at some of the difficulties
to be tackled by potential guidelines for depen-
dency annotation of English.
1 Introduction
The Prague Czech-English Dependency Treebank
(PCEDT) is a project of creating a Czech-English
syntactically annotated parallel corpus motivated by
research in the field of machine translation. Parallel data
are needed for designing, training, and evaluation of both
statistical and rule-based machine translation systems.
Since Czech is a language with relatively high degree
of word-order freedom, and its sentences contain certain
syntactic phenomena, such as discontinuous constituents
(non-projective constructions), which cannot be straight-
forwardly handled using the annotation scheme of Penn
Treebank (Marcus et al, 1993; Linguistic Data Consor-
tium, 1999), based on phrase-structure trees, we decided
to adopt for the PCEDT the dependency-based annotation
scheme of the Prague Dependency Treebank ? PDT (Lin-
guistic Data Consortium, 2001). The PDT is annotated
on three levels: morphological layer (lowest), analytic
layer (middle) ? surface syntactic annotation, and tec-
togrammatical layer (highest) ? level of linguistic mean-
ing. Dependency trees, representing the sentence struc-
ture as concentrated around the verb and its valency, are
used for the analytical and tectogrammatical levels, as
proposed by Functional Generative Description (Sgall et
al., 1986).
In Section 2, we describe the process of translating the
Penn Treebank into Czech. Section 3 sketches the gen-
eral procedure for transforming phrase topology of Penn
Treebank into dependency structure and describes the
specific conversions into analytical and tectogrammatical
representations. The following Section 4 describes the
automatic process of parsing of Czech into analytical rep-
resentation and its automatic conversion into tectogram-
matical representation. Section 5 briefly discusses some
of the problems of annotation from the point of view of
mutual compatibility of annotation schemes. Section 6
gives an overview of additional resources included in the
PCEDT.
2 English to Czech Translation of Penn
Treebank
When starting the PCEDT project, we chose the latter of
two possible strategies: either the parallel annotation of
already existing parallel texts, or the translation and anno-
tation of an existing syntactically annotated corpus. The
choice of the Penn Treebank as the source corpus was
also pragmatically motivated: firstly it is a widely rec-
ognized linguistic resource, and secondly the translators
were native speakers of Czech, capable of high quality
translation into their native language.
The translators were asked to translate each English
sentence as a single Czech sentence and to avoid unneces-
sary stylistic changes of translated sentences. The trans-
lations are being revised on two levels, linguistic and fac-
tual. About half of the Penn Treebank has been translated
so far (currently 21,628 sentences), the project aims at
translating the whole Wall Street Journal part of the Penn
Treebank.
DT
An
NP-SBJ-1
NN
earthquake
VBD
struck
JJ
Northern
NP
NNP
California
VP
,
,
NP-SBJ
-NONE-
*-1
S
S-ADV
VBG
killing
JJR
more
VP
QP
IN
than
CD
50
NP
NNS
people
.
.
Figure 1: Penn Treebank annotation of the sentence ?An earthquake struck Northern California, killing more than 50
people.?
For the purpose of quantitative evaluation methods,
such as NIST or BLEU, for measuring performance of
translation systems, we selected a test set of 515 sen-
tences and had them retranslated from Czech into En-
glish by 4 different translator offices, two of them from
the Czech Republic and two of them from the U.S.A.
3 Transformation of Penn Treebank
Phrase Trees into Dependency Structure
The transformation algorithm from phrase-structure
topology into dependency one, similar to transformations
described by Xia and Palmer (2001), works as follows:
? Terminal nodes of the phrase are converted to nodes
of the dependency tree.
? Dependencies between nodes are established recur-
sively: The root node of the dependency tree trans-
formed from the head constituent of a phrase be-
comes the governing node. The root nodes of the
dependency trees transformed from the right and left
siblings of the head constituent are attached as the
left and right children (dependent nodes) of the gov-
erning node, respectively.
? Nodes representing traces are removed and their
children are reattached to the parent of the trace.
3.1 Preprocessing of Penn Treebank
Several preprocessing steps preceded the transformation
into both analytical and tectogrammatical representa-
tions.
Marking of Heads in English
The concept of the head of a phrase is important dur-
ing the tranformation described above. For marking
head constituents in each phrase, we used Jason Eisner?s
scripts.
Lemmatization of English
Czech is an inflective language, rich in morphology,
therefore lemmatization (assigning base forms) is indis-
pensable in almost any linguistic application. Mostly for
reasons of symmetry with Czech data and compatibility
with the dependency annotation scheme, the English part
was automatically lemmatized by the morpha tool (Min-
nen et al, 2001) using manually assigned POS tags of the
Penn Treebank.
Unique Identification
For technical reasons, a unique identifier is assigned to
each sentence and to each token of Penn Treebank.
3.2 English Analytical Dependency Trees
This section describes the automatic process of convert-
ing Penn Treebank annotation into analytical representa-
tion.
Sent. #1
AuxS
an
Atr
earthquake
Sb
struck
Pred
Northern
Atr
California
Obj
,
AuxX
killing
Adv
more
Atr
than
AuxP
50
Atr
people
Obj
.
AuxK
Figure 2: Analytical tree for the sentence ?An earthquake
struck Northern California, killing more than 50 people.?
The structural transformation works as described
above. Because the handling of coordination in PDT is
different from the Penn Treebank annotation style and
the output of Jason Eisner?s head assigning scripts, in
the case of a phrase containing a coordinating conjunc-
tion (CC), we consider the rightmost CC as the head. The
treatment of apposition is a more difficult task, since there
is no explicit annotation of this phenomenon in the Penn
Treebank; constituents of a noun phrase enclosed in com-
mas or other delimiters (and not containing CC) are con-
sidered to be in apposition and the rightmost delimiter
becomes the head.
The information from both the phrase tree and the de-
pendency tree is used for the assignment of analytical
functions:
? Penn Treebank function tag to analytical function
mapping: some function tags of a phrase tree corre-
spond to analytic functions in an analytical tree and
can be mapped to them:
SBJ ? Sb,
{DTV, LGS, BNF, TPC, CLR} ? Obj,
{ADV, DIR, EXT, LOC, MNR, PRP, TMP, PUT} ? Adv.
? Assignment of analytical functions using local con-
text of a node: for assigning analytical functions to
the remaining nodes, we use rules looking at the cur-
rent node, its parent and grandparent, taking into ac-
count POS and the phrase marker of the constituent
in the original phrase tree headed by the node. For
example, the rule
mPOS = DT|mAF = Atr
earthquake
ACT
strike
PRED
northern
RSTR
California
PAT
&Cor;
ACT
kill
COMPL
more
CPR
50
RSTR
people
PAT
Figure 3: Tectogrammatical tree for the sentence ?An
earthquake struck Northern California, killing more than
50 people.?
assigns the analytical function Atr to every deter-
miner, the rule
mPOS = MD|pPOS = VB|mAF = AuxV
assigns the function tag AuxV to a modal verb
headed by a verb, etc. The attribute mPOS repre-
senting the POS of a node is obligatory for every
rule. The rules are examined primarily in the order
of the longest prefix of the POS of the given node
and secondarily in the order as they are listed in the
rule file. The ordering of rules is important, since
the first matching rule found assigns the analytical
function and the search is finished.
Specifics of the PDT and Penn Treebank annotation
schemes, mainly the markup of coordinations, apposi-
tions, and prepositional phrases are handled separately:
? Coordinations and appositions: the analytical func-
tion that was originally assigned to the head of a
coordination or apposition is propagated to its child
nodes by attaching the suffix Co or Ap to them, and
the head node gets the analytical function Coord or
Apos, respectively.
? Prepositional phrases: the analytical function origi-
nally assigned to the preposition node is propagated
to its child and the preposition node is labeled AuxP.
? Sentences in the PDT annotation style always con-
tain a root node labeled AuxS, which, as the only one
in the dependency tree, does not correspond to any
terminal of the phrase tree; the root node is inserted
Sent. #1
zem?t?esen?
Sb
zas?hlo
Pred
severn?
Atr
Kalifornii
Obj
a
Coord
usmrtilo
Pred
v?ce
Adv
ne?
AuxC
50
Adv
lid?
Atr
.
AuxK
Figure 4: Analytical tree for the Czech translation
?Zeme?tr?esen?? zasa?hlo severn?? Kalifornii a usmrtilo v??ce
nez? 50 lid??.?
above the original root. While in the Penn Treebank
the final punctuation is a constituent of the sentence
phrase, in the analytical tree it is moved under the
technical sentence root node.
Compare the phrase structure and the analytical repre-
sentation of a sample sentence from the Penn Treebank
in Figures 1 and 2.
3.3 English Tectogrammatical Dependency Trees
The transformation of Penn Treebank phrase trees into
tectogrammatical representation consists of a structural
transformation, and an assignment of a tectogrammat-
ical functor and a set of grammatemes to each node.
At the beginning of the structural transformation, the
initial dependency tree is created by a general transfor-
mation procedure as described above. However, func-
tional (synsemantic) words, such as prepositions, punc-
tuation marks, determiners, subordinating conjunctions,
certain particles, auxiliary and modal verbs are handled
differently. They are marked as ?hidden? and information
about them is stored in special attributes of their govern-
ing nodes (if they were to head a phrase, the head of the
other constituent became the governing node in the de-
pendency tree).
The well-formedness of a tectogrammatical tree struc-
ture requires the valency frames to be complete: apart
from nodes that are realized on surface, there are several
types of ?restored? nodes representing the non-realized
members of valency frames (cf. pro-drop property of
_ _
zem?t?esen?
ACT
earthquake
zas?hnout
PRED CO
strike
severn?
RSTR
northern
Kalifornie
PAT
California
a
CONJ
and
usmrtit
PRED CO
kill
v?ce
EXT
more_than
50
CPR
50
?lov?k
PAT
man
Figure 5: Tectogrammatical tree for the Czech translation
?Zeme?tr?esen?? zasa?hlo severn?? Kalifornii a usmrtilo v??ce
nez? 50 lid??.?
Czech and verbal condensations using gerunds and in-
finitives both in Czech and English). For a partial recon-
struction of such nodes, we can use traces, which allow
us to establish coreferential links, or restore general par-
ticipants in the valency frames.
For the assignment of tectogrammatical functors, we
can use rules taking into consideration POS tags (e.g.
PRP ? APP), function tags (JJ ? RSTR, JJR ? CPR,
etc.) and lemma (?not? ? RHEM, ?both? ? RSTR).
Grammateme Assignment ? morphological gram-
matemes (e.g. tense, degree of comparison) are assigned
to each node of the tectogrammatical tree. The assign-
ment of the morphological attributes is based on Pen-
nTreebank tags and reflects basic morphological proper-
ties of the language. At the moment, there are no auto-
matic tools for the assignment of syntactic grammatemes,
which are designed to capture detailed information about
deep syntactic structure.
The whole procedure is described in detail in
Kuc?erova? and ?Zabokrtsky? (2002).
In order to gain a ?gold standard? annotation, 1,257
sentences have been annotated manually (the 515 sen-
tences from the test set are among them). These data
are assigned morphological gramatemes (the full set of
values) and syntactic grammatemes, and the nodes are
reordered according to topic-focus articulation (informa-
tion structure).
The quality of the automatic transformation procedure
described above, based on comparison with manually an-
JJ
Such
NP-SBJ-1
NNS
loans
VBP
remain
JJ
classified
ADJP-PRD
IN
as
PP
ADJP
JJ
non-accruing
VP
,
,
NP-SBJ
-NONE-
*-1
VBG
costing
S
DT
the
NP
NN
bank
S-ADV
VP
$
$
QP
CD
10
CD
million
NP
-NONE-
*U*
.
.
Figure 6: Penn Treebank annotation of the sentence ?Such loans remain classified as non-accruing, costing the bank
$10 million.?
notated trees, is about 6% of wrongly aimed dependen-
cies and 18% of wrongly assigned functors.
See Figure 3 for the manually annotated tectogrammat-
ical representation of the sample sentence.
4 Automatic Annotation of Czech
The Czech translations of Penn Treebank were auto-
matically tokenized and morphologically tagged, each
word form was assigned a base form ? lemma by
Hajic? and Hladka? (1998) tagging tools.
Czech analytical parsing consists of a statistical
dependency parser for Czech ? either Collins parser
(Collins et al, 1999) or Charniak parser (Charniak,
1999), both adapted to dependency grammar ? and
a module for automatic analytical function assignment
( ?Zabokrtsky? et al, 2002).
When building the tectogrammatical structure, the
analytical tree structure is converted into the tectogram-
matical one. These transformations are described by lin-
guistic rules (Bo?hmova?, 2001). Then, tectogrammatical
functors are assigned by a C4.5 classifier ( ?Zabokrtsky? et
al., 2002).
The test set of 515 sentences (which have been retrans-
lated into English) has been also manually annotated on
tectogrammatical level.
See Figures 4 and 5 for automatic analytical and man-
ual tectogrammatical annotation of the Czech translation
of the sample sentence.
such
RSTR
loan
ACT
remain
PRED
&Cor;
ACT
classify
PAT
non-accruing
CPR
&Cor;
ACT
cost
COMPL
bank
ADDR
$
PAT
10
RSTR
million
RSTR
Figure 7: Tectogrammatical tree for the sentence ?Such
loans remain classified as non-accruing, costing the bank
$10 million.?
5 Problems of Dependency Annotation of
English
The manual annotation of 1,257 English sentences on tec-
togrammatical level was, to our knowledge, the first at-
tempt of its kind, and was based especially on the instruc-
tions for tectogrammatical annotation of Czech. During
the process of annotation, we have experienced both phe-
nomena that do not occur in Czech at all, and phenomena
_ _
&Gen;
ACT
obdobn?
RSTR
such
?v?r
PAT
loan
nad?le
THL
still
klasifikovat
PRED CO
clasify
&Neg;
RHEM
vyn??ej?c?
EFF
accruing
&Comma;
CONJ
co?
ACT
which
banka
PAT
bank
st?t
PRED CO
cost
10
RSTR
10
mili?n
EXT
million
dolar
MAT
dollar
Figure 8: Tectogrammatical tree for the Czech trans-
lation ?Obdobne? u?ve?ry jsou nada?le klasifikova?ny jako
nevyna?s?ej??c??, coz? banku sta?lo 10 milionu? dolaru?.?
whose counterparts in Czech occur rarely, and therefore
are not handeled thoroughly by the guidelines for tec-
togrammatical annotation designed for Czech. To men-
tion just a few, among the former belongs the annotation
of articles, certain aspects of the system of verbal tenses,
and phrasal verbs. A specimen of a roughly correspond-
ing phenomenon occurring both in Czech and English is
the gerund. It is a very common means of condensation
in English, but its counterpart in Czech (usually called
transgressive) has fallen out of use and is nowadays con-
sidered rather obsolete.
The guidelines for Czech require the transgressive to
be annotated with the functor COMPL. The reason why it is
highly problematic to apply them straightforwardly also
to the annotation of English, is that the English gerund
has a much wider range of functions than the Czech trans-
gressive. The gerund can be seen as a means of con-
densing subordinated clauses with in principle adverbial
meaning (as it is analyzed in the phrase-structure annota-
tion of Penn Treebank). Since the range of functors with
adverbial meaning is much more fine-grained, we deem it
inappropriate to mark the gerund clauses in such a simple
way on the tectogrammatical level.
From the point of view of machine translation, the
gerund constructions pose considerable difficulties be-
cause of the many syntactic constructions suitable as their
translations corresponding to their varied syntactic func-
tions.
We present two examples illustrating the issues men-
tioned above. Each example consists of three figures, the
first one presenting the Penn Treebank annotation of a (in
the second case simplified) sentence from the Penn Tree-
bank, the second one giving its tentative tectogrammatic
representation (according to the guidelines for Czech ap-
plied to English), and the third one containing the tec-
JJ
common
ADJP
CC
and
VBN
preferred
NP
NN
stock
NN
purchase
NNS
rights
Figure 9: Penn Treebank annotation of the noun phrase
?common and preferred stock purchase rights?.
togrammatical representation of its translation into Czech
(see Figures 1, 3, 5, and Figures 6, 7, 8). Note that in nei-
ther of the two examples the Czech transgressive is used
as the translation of the English gerund; a coordination
structure is used instead.
On the other hand, we have also experienced phenom-
ena in English whose Penn Treebank style of annotation
is insufficient for a successfull conversion into depen-
dency representation.
In English, the usage of constructions with nominal
premodification is very frequent, and the annotation of
such noun phrases in the Penn Treebank is often flat,
grouping together several constituents without reflecting
finer syntactic and semantic relations among them (see
Figure 9 for an example of such a noun phrase). In fact,
the possible syntactic and especially semantic relations
between the members of the noun phrase can be highly
ambiguous, but when translating such a noun phrase into
Czech, we are not usually able to preserve the ambiguity
and are forced to resolve it by choosing one of the read-
ings (see Figure 10).
Sometimes we even may be forced to insert new words
explicitly expressing the semantic relations within the
nominal group. An example of an English noun phrase
and the tectogrammatical representation of its Czech
translation with an inserted word ?podnikaj??c??? (?operat-
ing?) can be found in Figures 11 and 12.
6 Other Resources Included in PCEDT
6.1 Reader?s Digest Parallel Corpus
Reader?s Digest parallel corpus contains raw text in
53,000 aligned segments in 450 articles from the Reader?s
Digest, years 1993?1996. The Czech part is a free trans-
lation of the English version. The final selection of
data has been done manually, excluding articles whose
translations significantly differ (in length, culture-specific
facts, etc.). Parallel segments on sentential level have
been aligned by Dan Melamed?s aligning tool (Melamed,
DT
a
NNP
San
NNP
Francisco
NN
food
NNS
products
CC
and
NP
NN
building
NNS
materials
NN
marketing
CC
and
NN
distribution
NN
company
Figure 11: Penn Treebank annotation of the noun phrase ?a San Francisco food products and building materials
marketing and distribution company?.
_ _
pr?vo
PAT
right
n?kup
PAT
purchase
oby  ejn?
RSTR
common
akcie
PAT CO
stock
a
CONJ
and
prioritn?
RSTR
preferred
akcie
PAT CO
stock
Figure 10: Tectogrammatical tree for the Czech transla-
tion ?pra?vo na na?kup obyc?ejny?ch a prioritn??ch akci???.
1996). The topology is 1?1 (81%), 0?1 or 1?0 (2%), 1?2
or 2?1 (15%), 2?2 (1%), and others (1%).
6.2 Dictionaries
The PCEDT comprises also a translation dictionary com-
piled from three different Czech-English manual dictio-
naries: two of them were downloaded form the Web and
one was extracted from Czech and English EuroWord-
Nets. Entry-translation pairs were filtered and weighed
taking into account the reliability of the source dictio-
nary, the frequencies of the translations in Czech and En-
glish monolingual corpora, and the correspondence of the
Czech and English POS tags. Furthermore, by training
GIZA++ (Och and Ney, 2003) translation model on the
training part of the PCEDT extended by the manual dic-
tionaries, we obtained a probabilistic Czech-English dic-
tionary, more sensitive to the domain of financial news
specific for the Wall Street Journal.
The resulting Czech-English probabilistic dictionary
_ _
_ _
sanfrancisk?
RSTR
San_Francisco
marketingov?
RSTR CO
marketing
a
CONJ
and
distribu n?
RSTR CO
distribution
spole nost
ACT
company
podnikaj?c?
RSTR
operating
potravina
LOC CO
food_product
a
CONJ
and
stavebn?
RSTR
building
materi?l
LOC CO
material
Figure 12: Tectogrammatical tree for the Czech transla-
tion ?sanfranciska? marketingova? a distribuc?n?? spolec?nost
podnikaj??c?? v potravina?ch a stavebn??ch materia?lech?.
contains 46,150 entry-translation pairs in its lemmatized
version and 496,673 pairs of word forms in the version
where for each entry-translation pair all the correspond-
ing word form pairs have been generated.
6.3 Tools
SMT Quick Run is a package of scripts and instructions
for building statistical machine translation system from
the PCEDT or any other parallel corpus. The system uses
models GIZA++ and ISI ReWrite decoder (Germann et
al., 2001).
TrEd is a graphical editor and viewer of tree structures.
Its modular architecture allows easy handling of diverse
annotation schemes, it has been used as the principal an-
notation environment for the PDT and PCEDT.
Netgraph is a multi-platform client-server application
for browsing, querying and viewing analytical and tec-
togrammatical dependency trees, either over the Internet
or locally.
7 Conclusion
We have described the process of building the first ver-
sion of a parallel treebank for two relatively distant lan-
guages, Czech and English, during which we have also
attempted to reconcile two fairly incompatible linguistic
theories used for their description.
The resulting data collection contains data syntacti-
cally annotated on several layers of analysis. There have
already been experimental machine translation systems
MAGENTA (Hajic? et al, 2002) and DBMT ( ?Cmejrek
et al, 2003) confirming the exploitability of the corpus
and showing that we are capable of performing auto-
matic transformations from phrase structures to depen-
dency representation with an acceptable, though still not
impeccable quality.
However, for both languages, we have presented ex-
amples of phenomena, for which the ?native? annotation
scheme does not provide a sufficiently fine-grained anal-
ysis. In such cases, automatic conversion between anno-
tation schemes is not possible, and the less we can hope
for successfull machine translation.
The question of enhancing the annotation schemes to
allow for a lossless transformation between them remains
still open, and its difficulty presents a yet unfathomed
depth.
8 Acknowledgements
This research was supported by the following
grants: M?SMT ?CR Grants No. LN00A063, No.
MSM113200006, and NSF Grant No. IIS-0121285.
References
Alena Bo?hmova?. 2001. Automatic procedures in tec-
togrammatical tagging. The Prague Bulletin of Math-
ematical Linguistics, 76.
Eugene Charniak. 1999. A maximum-entropy-inspired
parser. Technical Report CS-99-12.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, College
Park, Maryland.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics, pages 228?235.
Jan Hajic? and Barbora Hladka?. 1998. Tagging Inflec-
tive Languages: Prediction of Morphological Cate-
gories for a Rich, Structured Tagset. In Proceedings of
COLING-ACL Conference, pages 483?490, Montreal,
Canada.
Jan Hajic?, Martin ?Cmejrek, Bonnie Dorr, Yuan Ding, Ja-
son Eisner, Daniel Gildea, Terry Koo, Kristen Parton,
Gerald Penn, Dragomir Radev, and Owen Rambow.
2002. Natural Language Generation in the Context of
Machine Translation. Technical report. NLP WS?02
Final Report.
Ivona Kuc?erova? and Zdene?k ?Zabokrtsky?. 2002. Trans-
forming Penn Treebank Phrase Trees into (Praguian)
Tectogrammatical Dependency Trees. Prague Bulletin
of Mathematical Linguistics, 78:77?94.
Linguistic Data Consortium. 1999. Penn Treebank 3.
LDC99T42.
Linguistic Data Consortium. 2001. Prague Dependency
Treebank 1. LDC2001T10.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
I. Dan Melamed. 1996. A geometric approach to map-
ping bitext correspondence. In Proceedings of the First
Conference on Empirical Methods in Natural Lan-
guage Processing.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
Morphological Processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia/Reidel Publishing
Company, Prague, Czech Republic/Dordrecht, Nether-
lands.
Martin ?Cmejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English Dependency-based Machine Transla-
tion. In Proceedings of the 10th Conference of The Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 83?90, Budapest, Hungary, April.
Zdene?k ?Zabokrtsky?, Petr Sgall, and Dz?eroski Sas?o. 2002.
Machine Learning Approach to Automatic Functor As-
signment in the Prague Dependency Treebank. In Pro-
ceedings of LREC 2002, volume V, pages 1513?1520,
Las Palmas de Gran Canaria, Spain.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proceedings
of HLT 2001, First International Conference on Hu-
man Language Technology Research, San Francisco.
Coling 2010: Poster Volume, pages 180?188,
Beijing, August 2010
Two Methods for Extending Hierarchical Rules from the Bilingual Chart
Parsing
Martin ?Cmejrek and Bowen Zhou
IBM T. J. Watson Research Center
{martin.cmejrek, zhou}@us.ibm.com
Abstract
This paper studies two methods for train-
ing hierarchical MT rules independently
of word alignments. Bilingual chart pars-
ing and EM algorithm are used to train bi-
text correspondences. The first method,
rule arithmetic, constructs new rules as
combinations of existing and reliable rules
used in the bilingual chart, significantly
improving the translation accuracy on the
German-English and Farsi-English trans-
lation task. The second method is pro-
posed to construct additional rules directly
from the chart using inside and outside
probabilities to determine the span of the
rule and its non-terminals. The paper also
presents evidence that the rule arithmetic
can recover from alignment errors, and
that it can learn rules that are difficult to
learn from bilingual alignments.
1 Introduction
Hierarchical phrase-based systems for machine
translation usually share the same pattern for ob-
taining rules: using heuristic approaches to ex-
tract phrase and rule pairs from word alignments.
Although these approaches are very successful
in handling local linguistic phenomena, handling
longer distance reorderings can be more difficult.
To avoid the combinatorial explosion, various re-
strictions, such as limitations of the phrase length
or non-terminal span are used, that sometimes pre-
vent from extracting good rules. Another reason
is the deterministic nature of those heuristics that
does not easily recover from errors in the word
alignment.
In this work, we learn rules for hierarchical
phrase based MT systems directly from the par-
allel data, independently of bilingual word align-
ments.
Let us have an example of a German-English
sentence pair from the Europarl corpus (Koehn,
2005).
(1) GER: die herausforderung besteht darin
diese systeme zu den besten der welt zu
machen
ENG: the challenge is to make the system
the very best
The two pairs of corresponding sequences diese
systeme ... der welt?the system ... best and zu
machen?to make are swapped. We believe that
the following rule could handle long distance re-
orderings, still with a reasonably low number of
terminals, for example:
(2) X ? ?besteht darin X1 zu X2, is to X2X1?,
There are 127 sentence pairs out of 300K of the
training data that contain this pattern, but this rule
was not learned using the conventional approach
(Chiang, 2007). There are three potential risks:
(1) alignment errors (the first zu aligned to to, or
der welt (of the world) aligned to null); (2) maxi-
mum phrase length for extracting rules lower than
11 words; (3) requirement of non-terminals span-
ning at least 2 words.
The rule arithmetic (Cmejrek et al, 2009) con-
structs the new rule (2) as a combination of good
rule usages:
(3) X ? ?besteht darin, is ?
X ? ?X1 zu X2, to X2X1?
180
The approach consists of bilingual chart parsing
(BCP) of the training data, combining rules found
in the chart using a rule arithmetic to propose new
rules, and using EM to estimate rule probabilities.
In this paper, we study the behavior of the
rule arithmetic on two different language pairs:
German-English and Farsi-English. We also pro-
pose an additional method for constructing new
rules directly from the bilingual chart, and com-
pare it with the rule arithmetic.
The paper is structured as follows: In Sec. 1, we
explain our main motivation, summarize previous
work, and briefly introduce the formalism of hi-
erarchical phrase-based translation. In Sec. 2, we
describe the bilingual chart parsing and the EM
algorithm. The rule arithmetic is introduced in
Sec. 3. The new method for proposing new rules
directly from the chart is described in Sec. 4. The
experimental setup is described in Sec. 5. Results
are thoroughly discussed in Sec. 6. Finally, we
conclude in Sec. 7.
1.1 Related work
Many previous works use the EM algorithm to
estimate probabilities of translation rules: Wu
(1997) uses EM to directly estimate joint word
alignment probabilities of Inversion Transduction
Grammar (ITG). Marcu and Wong (2002) use
EM to estimate joint phrasal translation model
(JPTM). Birch et al (2006) reduce its com-
plexity by using only concepts that match the
high-confidence GIZA++ alignments. Similarly,
Cherry and Lin (2007) use ITG for pruning. May
and Knight (2007) use EM algorithm to train tree-
to-string rule probabilities, and use the Viterbi
derivations to re-align the training data. Huang
and Zhou (2009) use EM to estimate conditional
rule probabilities P (?|?) and P (?|?) for Syn-
chronous Context-free Grammar. Others try to
overcome the deterministic nature of using bilin-
gual alignments for rule extraction by sampling
techniques (Blunsom et al, 2009; DeNero et al,
2008). Galley et al (2006) define minimal
rules for tree-to-string translation, merge them
into composed rules (similarly to the rule arith-
metic), and train weights by EM. While in their
method, word alignments are used to define all
rules, rule arithmetic proposes new rules indepen-
dently of word alignments. Similarly, Liu and
Gildea (2009) identify matching long sequences
(?big templates?) using word alignments and ?lib-
erate? matching small subtrees based on chart
probabilities. Our method of proposing rules di-
rectly from the chart does not use word alignment
at all.
1.2 Formally syntax-based models
Our baseline model follows the Chiang?s hierar-
chical model (Chiang, 2007; Chiang, 2005; Zhou
et al, 2008) based on Synchronous Context-free
Grammar (SCFG). The rules have form
X ? ??, ?,??, (4)
where X is the only non-terminal in the gram-
mar, ? and ? are source and target strings with
terminals and up to two non-terminals, ? is the
correspondence between the non-terminals. Cor-
responding non-terminals have to be expanded at
the same time.
2 Bilingual chart parsing and EM
algorithm
In this section, we briefly overview the algorithm
for bilingual chart parsing and EM estimation of
SCFG rule features.
Let e = eM1 and f = fN1 of source and tar-
get sentences. For each sentence pair e, f , the ?E?
step of the EM algorithm will use the bilingual
chart parser to enumerate all possible derivations
?, compute inside probabilities ?ijkl(X) and out-
side probabilities ?ijkl(X), and finally calculate
expected counts c(r) how many times each rule r
produced the corpus C .
The inside probabilities can be defined recur-
sively and computed dynamically during the chart
parsing:
?ijkl =
?
??tijkl
P (?.r)
?
(i?j?k?l?)??.bp
?i?j?k?l? , (5)
where tijkl represents the chart cell spanning
(eji , f lk), and the data structure ? stores the rule
?.r. If r has non-terminals, then ?.bp stores back-
pointers ?.bp1 and ?.bp2 to the cells representing
their derivations.
181
The outside probabilities can be computed re-
cursively by iterating the chart in top-down order-
ing. We start from the root cell ?1,M,1,N := 1 and
propagate the probability mass as
??.bp1+ = P (?.r)?ijkl (6)
for rules with one non-terminal, and
??.bp1 + = P (?.r)?ijkl??.bp2 , (7)
??.bp2 + = P (?.r)?ijkl??.bp1 , (8)
for rules with two non-terminals. The top-down
ordering ensures that each ?ijkl accumulates up-
dates from all cells higher in the chart before its
own outside probability is used.
The contributions to the rule expected counts
are computed as
c(?.r)+ = P (?.r)?ijkl
??.n
i=1 ??.bpi
?1,M,1,N
. (9)
Finally, rule probabilities P (r) are obtained by
normalizing expected counts in the ?M? step.
To improve the grammar coverage, the rule-
set is extended by the following rules providing
?backoff? parses and scoring for the SCFG rules:
(10) ?X1,X1f?, ?X1, fX1?, ?X1e,X1?,
?eX1,X1?,
(11) ?X1X2,X2X1?.
Rules (10) enable insertions and deletions, while
rule (11) allows for aligning swapped constituents
in addition to the standard glue rule.
3 Proposing new rules with rule
arithmetic
The main idea of this work is to propose new rules
independently of the bilingual word alignments.
We parse each sentence pair using the baseline
ruleset extended by the new rule types (10) and
(11). Then we select the most promising rule us-
ages and combine each two of them using the
rule arithmetic to propose new rules. We put the
new rules into a temporary pool, and parse and
compute probabilities and expected counts again,
this time we use rules from the baseline and from
the temporary pool. Finally, we dump expected
counts for proposed rules, and empty the tempo-
rary pool. This way we can try to propose many
rules for each sentence pair, and to filter them later
using accumulated expected counts from the EM.
The term most promising is purposefully vague
? to cover all possible approaches to filtering rule
usages. In our implementation, we are limited by
space and time, and we have to prune the number
of rules that we can combine. We use expected
counts as the main scoring criterion. When com-
puting the contributions to expected counts from
particular rule usages as described by (9), we re-
member the n-best contributors, and use them as
candidates after the expected counts for the given
sentence pair have been estimated.
The rule arithmetic combines existing rules us-
ing addition operation to create new rules. The
idea is shown in Example 12.
(12) Addition
?5, 13, 5, 11, 13, 13? ?4, 10, 6, 10, 5, 5? X ? ?X1 zu X2, to X2 X1?
?5, 11, 6, 11, 0, 0? ?6, 10, 7, 10, 0, 0? X ? ?diese X1, the X1?
1: ... 4 5 6 ... 11 12 13 3 4 5 6 7 ... 10
2: ... 0 -1 -1 ... -1 zu -2 0 to -2 -1 -1 ... -1
3: ... 0 diese -3 ... -3 0 0 0 0 0 the -3 ... -3
4: ... 0 diese -3 ... -3 zu -2 0 to -2 the -3 ... -3
5: ?5, 13, 6, 11, 13, 13? ?4, 10, 7, 10, 5, 5? X ? ?diese X1 zu X2, to X2 the X1?
First, create span projections for both source
and target sides of both rules. Use symbol 0 for
all unspanned positions, copy terminal symbols as
they are, and use symbols -1, -2, -3, and -4 to tran-
scribe X1 and X2 from the first rule, and X1 and
X2 from the second rule. Repeat the non-terminal
symbol on all spanned positions. In Example 12
line 1 shows the positions in the sentence, lines 2
and 3 show the rule span projections of the two
rules.
Second, merge source span projections (line 4),
record mappings of non-terminal symbols. We re-
quire that merged projections are continuous. We
allow substituting non-terminal symbols by termi-
nals, but we require that the whole span of the
non-terminal is fully replaced. In other words,
shortenings of non-terminal spans are not allowed.
Third, collect new rule. The merged rule us-
ages (lines 5) are generalized into rules, so that
they are not limited to the particular span for
which they were originally proposed.
The rule arithmetic can combine all types of
rules ? phrase pairs, abstract rules, glues, swaps,
insertions and deletions. However, we require that
182
at least one of the rules is either a phrase pair or
an abstract rule.
4 Proposing directly from chart
One of the issues observed while proposing new
rules with the rule arithmetic is the selection of the
best candidates. The number of all candidates that
can be combined depends on the length of the sen-
tence pair and on the number of competing pars-
ing hypotheses. Using a fixed size of the n-best
can constitute a risk of selecting bad candidates
from shorter sentences. On the other hand, the
spans of the best candidates extracted from long
sentences can be far from each other, so that most
combinations are not valid rules (e.g., the combi-
nation of two discontinuous phrasal rules is not
defined).
In our new approach we propose new rules di-
rectly from the bilingual chart, relying on the in-
side and outside probabilities computed after the
parsing of the sentence pair. The method has two
steps. In the first step we identify best matching
parallel sequences; in the second step we propose
?holes? for non-terminals.
4.1 Identifying best matching sequences
To identify the best matching sequences, we score
all sequences (eji , f lk) by a scoring function:
scoreijkl =
?ijkl?ijkl
?1,M,1,N
Lex(i, j, k, l), (13)
where the lexical score is defined as:
Lex(i, j, k, l) =
N?
j?=1
M?
i?=0
t(fj?|ei?)?ijkli?j? (14)
The t is the lexical probability from the word-to-
word translation table, and ?ijkli?j? is defined as
?ins if i? ? ?i, j? and j? ? ?k, l?, and as ?out if
i? /? ?i, j? and j? /? ?k, l?, and as 0 elsewhere.
The purpose of this function is to score only the
pairs of words that are both either from within the
sequence or from outside the sequence. Usually
0 ? ?out ? ?ins to put more weight on words
within the parallel sequence.
The scoring function is a combination of ex-
pected counts contribution of a sequence (eji , f lk)
estimated from the chart with the IBM Model 1
lexical score.
Since only the sequences spanned by filled
chart cells can have non-zero expected counts,
we can select the n-best matching sequences rela-
tively efficiently.
4.2 Proposing non-terminal positions
Similar approach can be used to propose best po-
sitions for non-terminals. We score every com-
bination of non-terminal positions. The expected
counts can be estimated using Eq. 9. Since we are
proposing new rules, the probability P (r) used in
that equation is not defined. Again, we can use
Model 1 score instead, and use the following scor-
ing function:
sijkl(bp1, bp2) = (15)
Lex(i,j,k,l,bp1,bp2)?ijkl?bp1?bp2
?1,M,1,N ,
Lex(i, j, k, l, bp1 , bp2) is defined as in Eq. 14.
This time using 0 ? ?out ? ?NT1 = ?NT2 ?
?term, restricting the IBM Model 1 to score only
word pairs that both belong either to the terminals
of the proposed rule, or to the sequences spanned
by the same non-terminal, or outside of the rule
span. The scoring function for rules with one non-
terminal is just a special case of 15.
Again, the candidates can be scored efficiently,
taking into account only those combinations of
non-terminal spans that correspond to filled cells
in the chart.
The proposed method is again independent of
bilingual alignment, but at the same time utilizes
the information obtained from the bilingual chart
parsing.
5 Experiments
We carried out experiments on two language pairs,
German-English and Farsi-English.
The German-English data is a subset (297k
sentence pairs) of the Europarl (Koehn, 2005) cor-
pus. Since we are focused on speech-to-speech
translation, the punctuation was removed, and the
text was lowercased. The dev set and test set con-
tain each 1k sentence pairs with one reference.
The word alignments were trained by GIZA++
toolkit (Och and Ney, 2000). Phrase pairs were
183
extracted using grow-diag-final (Koehn et al,
2007). The baseline ruleset was obtained as
in (Chiang, 2007). The maximum phrase length
for rule extraction was set to 10, the minimum re-
quired non-terminal span was 2.
Additional rules for insertion, deletion, and
swap were added to improve the parsability of the
data, and to help EM training and rule arithmetic.
However, these rules are not used by the decoder,
since they would degrade the performance.
New rules were proposed after the first iteration
of EM1, either by rule arithmetic or directly from
the chart.
Only non-terminal rules proposed by the rule
arithmetic from at least two different sentence
pairs and ranked (by expected counts c(r)) in the
top 100k were used. Figure 4 presents a sample of
the new rules.
New rules were also proposed directly from the
chart, using the approach in Sec. 4. 5% of best
matching parallel sequences, and 5 best scoring
rules were selected from each parallel sequence.
Non-terminal rules from the 200k-best rank were
added to the model. Figure 5 presents a sample of
the new rules.
Finally, one more iteration of EM was used to
adjust the probabilities of the new and baseline
rules. These probabilities were used as features
in the decoding.
The performance of rule arithmetic was also
verified on Farsi-English translation. The train-
ing corpus contains conversational spoken data
from the DARPA TransTac program extended
by movie subtitles and online dictionaries down-
loaded from the web (297k sentence pairs). The
punctuation was removed, and the text was low-
ercased. The dev set is 1,420 sentence pairs held
out from the training data, with one reference. The
test set provided by NIST contains 470 sentences
with 4 references. The sentences are about 30%
longer and more difficult.
The training pipeline was the same as for the
German-English experiments. 122k new non-
terminal rules were proposed using the rule arith-
metic.
1Since our initial experiments did not show any signifi-
cant gain from proposing rules after additional (lengthy) it-
erations of EM.
The feature weights were tuned on the dev
set for each translation model separately. The
translation quality was measured automatically by
BLEU score (Papineni et al, 2001).
6 Discussion of results
The BLEU score results are shown in the Ta-
ble 3. The cumulative gain of rule arithmetic and
EM (RA + EM-i0) is 1 BLEU point for German-
English translation and 2 BLEU points for Farsi-
English. The cumulative gain of rules proposed
from the chart (DC + EM-i0) is 0.2 BLEU points
for German-English. For comparison of effects of
various components of our method, we also show
scores after the first five iterations of EM (EM-i0?
EM-i4) without adding any new rules, just using
EM-trained probabilities as feature weights, and
also scores for new rules added into the baseline
without adjusting their costs by EM (RA).
The qualities of proposed rules are discussed in
this section.
6.1 German-English rules from rule
arithmetic
The Figure 4 presents a sample of new rules pro-
posed during this experiment. The table is di-
vided into three parts, presenting rules from the
top, middle, and bottom of the 100K list. The
quality of the rules is high even in the middle part
of the table, the tail part is worse.
We were surprised by seeing short rules consist-
ing of frequent words. For example ?um X1, in
order X1?. When looking into word-level align-
ments, we realized that these rules following the
pattern 16 prevent the baseline approach from ex-
tracting the rule.
(16)
GER: um Obj zu V
ENG: in order to V Obj
Similarly many other rules match the pattern of
beginning of a subordinated clause, such as that is
why, or insertions, such as of course, which both
have to be strictly followed by VSO construction
in German, in contrast to the SVO word order in
English.
We also studied the cases of rule arithmetic cor-
recting for systematic word alignment errors. For
184
example the new rule ?X1 zu koennen, to X1? was
learned from the sentence
(17)
um die in kyoto vereinbarten senkungen beibehalten zu koennen
in order to maintain the reductions agreed in kyoto
The English translation often uses a different
modality, thus the modal verb koennen is always
aligned with null. Since unaligned words are usu-
ally not allowed at the edges of sub-phrases gener-
alized into non-terminals (Chiang, 2007), this rule
cannot be learned by the baseline.
We observe that many new proposed rules cor-
respond to patterns with a non-terminal spanning
one word. For example ?um X1 zu X2, to X2
X1? corresponds to the same pattern 16, where X2
spans one verb. The line baseline min1 in the Ta-
ble 3 shows 0.3 BLEU improvement of a model
trained without the minimum non-terminal span
requirement. However, this improvement comes
at a cost of more than four times increased model
size, as shown in Table 2. We observe that us-
ing the minimum span requirement while learning
from bitext alignments combined with rule arith-
metic that can learn the most reliable rules span-
ning one word yields better performance in speed,
memory, and precision.
We can also study the new rules quantitatively.
We want to know how the rules proposed by the
rule arithmetic are used in decoding. We traced
the translation of the 1,000 test set sentences to
mark the rules that were used to generate the best
scoring hypotheses.
The stats are presented in the Table 1. The
chance that a new rules will be used in the test set
decoding (0.86%) is more than 7 times higher than
that of all rules (0.12%). Encouraging evidence is
that while the rule arithmetic rules constitute only
1.87% of total rules, they present 9.17% of rules
used in the decoding.
The Figure 1 lists the most frequently used new
rules in the decoding. We can see many rules
with 2 non-terminals that model complex verb
forms (?wird X1 haben,will have X1?), reorder-
ing in clauses (?um X1 zu gewaehrleisten, to en-
sure X1?), or reordering of verbs from the second
position in German to SVO in English (?heute X1
wir X2, today we X1 X2?).
RA Ger. DC Ger. RA Farsi
Sentences translated 1,000 1,000 417
|ALL| (all rules) 5.359,751 5.459,751 8.532,691
|NEW| (new rules) 100,000 200,000 121,784
|NEW|
|ALL| 1.87% 3.66% 1.43%
|hits ALL| 10,122 7,256 2,521
|glue| 2,910 271 267
|hits ALL unique| 6.303 6,433 2,058
|hits ALL unique|
|ALL| 0.12% 0.12% 0.02
|hits NEW| 928 1,541 125
|hits NEW unique| 858 1,504 110
|hits NEW unique|
|NEW| 0.86% 0.75 % 0.09
|hits NEW|
|hits ALL| 9.17% 21.23% 4.96%
|terminals from NEW| 4,385 7,825 407
|terminals from NEW|
|hits NEW| 4,73 5.08 3.26
Table 1: Rule hits for 1,000 test set.
Model #phrases #rules
Ger-Eng baseline 8.5M 5.3M
Ger-Eng baseline min1 8.5M 23.M
Table 2: Model sizes.
We also studied the correlation between the
rank of the proposed rules (ranked by expected
counts) and the hit rate during the decoding. The
Figure 2 measures the hit rate for each of 1,000
best ranking rules, and should be read as follows:
the rules ranking 0 to 999 were used 70 times, the
hit rate decreases as the rank grows so that there
were no hits for rules ranking 90k and more. The
rank is a good indicator of the usefulness of new
rules.
We hypothesize that the new rules are capable
of combining partial solutions to form hypothe-
ses with better word order, or better complex verb
forms so that these hypotheses are better scored
and are parts of the winning solutions more often.
6.2 German-English rules proposed directly
from the chart
We also studied why the rules proposed directly
from the bilingual chart yield smaller improve-
ment than the rule arithmetic. The number of new
rules used in the decoding (1,541) is even higher
than that of the rule arithmetic, and it constitutes
21.23% of all cases. The two experiments were
185
#hits Ger Eng
5 X1 stellt X2 dar X1 is X2
3 X1 sowohl X2 als auch X1 both X2 and
3 X1 ist es X2 it is X2 X1
3 X1 die X2 ist X1 which is X2
2 wird X1 haben will have X1
2 wir X1 damit X2 we X1 so that X2
2 was X1 hat X2 what X1 has X2
2 was X1 betrifft so as regards X1
2 und X1 muessen wir X2 and X1 we must X2
2 um X1 zu gewaehrleisten to ensure X1
2 um X1 zu X2 to X2 X1
2 sowohl X1 als auch both X1 and
2 sie X1 auch X2 they also X1 X2
2 in erster linie X1 X1 in the first instance
2 in X1 an in X1
2 ich X1 meine i X1
2 heute X1 wir X2 today we X1 X2
2 herr praesident X1 und herren mr president X1 and gentlemen
2 gleich X1 X1 a moment
2 es muss X1 werden it must be X1
Figure 1: Examples of the most frequently hit
rules during the decoding.
tuned separately, so that they used different glue
rule weights. That is why we observe the differ-
ence in the number of glues (and the number of
total rules) in the Table 1. We do not observe any
significant correlation between the rank of the rule
and the hit rate. The Figure 3 shows that the first
10k-ranked rules are hit several times, and then
the hit rate stays flat.
We offer an explanation based on our observa-
tions of rules used for the decoding. The rules
proposed directly from the chart contain a big por-
tion of content words. These rules do not capture
any important differences between the structures
of the two languages that could not be handled
by phrasal rules as well. For example, the rule
?die neuen vorschriften sollen X1,the new rules
are X1? is correct, but a combination of a baseline
phrasal rule and glue will produce the same result.
We also see many rules with non-terminals
spanning one word. For example, the sequence
(18) die europaeische kommission?the
european commission
will produce the rule
(19) ?die X1 kommission, the X1 commission?.
Although the sequence and the rule are high
scored by 13 and 15, we intuitively feel that gen-
Figure 2: Usage of new rules (RA).
Figure 3: Usage of new rules (DC).
eralizing the word european is not very helpful in
this context.
The rule arithmetic could propose the rule 19 as
(20) ?die X1, the X1? + ?kommission,
commission?,
but since the candidates for combination are se-
lected as rules with the highest expected counts
(Sec. 3), the rules 20 will most likely loose to the
phrase pair 18 and will not be selected.
To conclude our comparison, we observe that
both methods produce reliable rules that are of-
ten reused in decoding. Nevertheless, since the
rule arithmetic combines the most successful rules
from each parallel parse, the resulting rules enable
structural transformations that could not be han-
dled by baseline rules.
186
German-English Farsi-English
Model dev set test set dev set test set
baseline 23.9 25.4 41.1 38.2
RA + EM-i0 24.8 26.4 41.8 40.2
DC + EM-i0 24.6 25.6
EM-i0 24.4 26.1 40.8 39.1
EM-i1 24.4 25.8 41.3 38.5
EM-i2 24.4 25.9 41.4 38.2
EM-i3 24.4 26.0 41.3 39.3
EM-i4 24.4 26.0 41.6 39.6
RA 24.4 26.1 40.7 38.4
baseline min1 24.0 25.7
Table 3: BLEU scores
6.3 Farsi-English rules from the rule
arithmetic
Although we have only limited resources to quali-
tatively analyze the Farsi-English experiments, we
noticed that there are two major groups of new
rules.
The first group corresponds to the fact that Farsi
does not have definite article and allows pro-drop.
We observe many new rules that could not be
learned from word alignments, since some defi-
nite articles or pronouns in English were aligned
to null (and unaligned words are not allowed at the
edges of phrases). However, if the chart contains
an insertion (of the determiner or pronoun) with a
high expected count, the rule arithmetic may pro-
pose new rule by combining it with other rules.
The second group contains rules that help word
reordering. We observe rules moving verbs from
the S PP O V in Farsi into SVO in English as well
as rules reordering wh-clauses.
Most of the rules traced during the test set de-
coding belong to the second group. Figure 1
shows that the number of new rules hit during
the decoding is smaller compared to the German-
English experiments. On the other hand, the rules
have smaller number of terminals so that we as-
sume that the positive effect of these rules comes
from the reordering of non-terminals.
um X1 in order X1
natuerlich X1 of course X1
deshalb X1 this is why X1
X1 zu koennen to X1
X1 ist it is X1
nach der tagesordnung folgt die X1 the next item is the X1
herr X1 herr kommissar X2 mr X1 commissioner X2
die X1 der X2 X1 the X2
im gegenteil X1 on the contrary X1
nach der tagesordnung folgt X1 the next item is X1
X1 die X2 the X1 the X2
die X1 die the X1
ausserdem X1 in addition X1
daher X1 that is why X1
wir X1 nicht X2 we X1 not X2
die X1 der X2 the X2 X1
deshalb X1 for this reason X1
um X1 zu X2 to X2 X1
X1 nicht X2 werden X1 not be X2
Figure 4: Sample rules (RA).
ausserdem X1 wir we X1 also
die X1 des kommissars the commissioner ?s X1
den X1 ratsvorsitz the X1 presidency
ich hoffe dass X1 i would hope that X1
X1 ist zu X2 geworden X1 has become X2
die X1 des vereinigten koenigreichs the uk X1
X1 maij weggen X2 X1 maij weggen X2
X1 wir auf X2 sind X1 we are on X2
ich frage mich X1 i wonder X1
Figure 5: Sample rules (DC).
7 Conclusion
In this work, we studied two new methods for
learning hierarchical MT rules: the rule arith-
metic and proposing directly from the parse for-
est. We discussed systematic patterns where the
rule arithmetic outperforms alignment-based ap-
proaches and verified its significant improvement
on two different language pairs (German-English
and Farsi-English). We also hypothesized why the
second method ? proposing rules directly from the
chart ? improves the baseline less than the rule
arithmetic.
Acknowledgment
This work is partially supported by the DARPA
TRANSTAC program under the contract num-
ber NBCH2030007. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of DARPA.
187
References
Birch, Alexandra, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constraining the
phrase-based, joint probability statistical translation
model. In Proceedings on WSMT?06, pages 154?
157.
Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In ACL ?09, pages
782?790.
Cherry, Colin. 2007. Inversion transduction grammar
for joint phrasal translation modeling. In NAACL-
HLT?07/SSST?07.
Chiang, David. 2005. A hierarchical phrase-
based model for statistical machine translation. In
ACL?05, pages 263?270.
Chiang, David. 2007. Hierarchical phrase-based
translation. Comput. Linguist., 33(2):201?228.
Cmejrek, Martin, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG rules directly from efficient bilin-
gual chart parsing. In IWSLT?09, pages 136?143.
DeNero, John, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure under
a bayesian translation model. In EMNLP ?08, pages
314?323.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961?968.
Huang, Songfang and Bowen Zhou. 2009. An EM
algorithm for SCFG in formal syntax-based transla-
tion. In Proc. IEEE ICASSP?09, pages 4813?4816.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL.
Koehn, Philipp. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Liu, Ding and Daniel Gildea. 2009. Bayesian learning
of phrasal tree-to-string templates. In EMNLP ?09,
pages 1308?1317.
Marcu, Daniel and W Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of EMNLP?02.
May, Jonathan and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of EMNLP-CoNLL?07, pages 360?368.
Och, F. J. and H. Ney. 2000. Improved statistical
alignment models. In Proc. of ACL, pages 440?447,
Hong Kong, China, October.
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176, IBM T. J.
Watson Research Center.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Zhou, Bowen, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of the
ACL?08: HLT SSST-2, pages 19?27.
188
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138?147,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Soft Syntactic Constraints for Hierarchical Phrase-based Translation
Using Latent Syntactic Distributions
Zhongqiang Huang
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742
zqhuang@umiacs.umd.edu
Martin C?mejrek and Bowen Zhou
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{martin.cmejrek,zhou}@us.ibm.com
Abstract
In this paper, we present a novel approach
to enhance hierarchical phrase-based machine
translation systems with linguistically moti-
vated syntactic features. Rather than directly
using treebank categories as in previous stud-
ies, we learn a set of linguistically-guided la-
tent syntactic categories automatically from a
source-side parsed, word-aligned parallel cor-
pus, based on the hierarchical structure among
phrase pairs as well as the syntactic structure
of the source side. In our model, each X non-
terminal in a SCFG rule is decorated with a
real-valued feature vector computed based on
its distribution of latent syntactic categories.
These feature vectors are utilized at decod-
ing time to measure the similarity between the
syntactic analysis of the source side and the
syntax of the SCFG rules that are applied to
derive translations. Our approach maintains
the advantages of hierarchical phrase-based
translation systems while at the same time nat-
urally incorporates soft syntactic constraints.
1 Introduction
In recent years, syntax-based translation mod-
els (Chiang, 2007; Galley et al, 2004; Liu et
al., 2006) have shown promising progress in im-
proving translation quality, thanks to the incorpora-
tion of phrasal translation adopted from the widely
used phrase-based models (Och and Ney, 2004) to
handle local fluency and the engagement of syn-
chronous context-free grammars (SCFG) to handle
non-local phrase reordering. Approaches to syntax-
based translation models can be largely categorized
into two classes based on their dependency on anno-
tated corpus (Chiang, 2007). Linguistically syntax-
based models (e.g., (Yamada and Knight, 2001; Gal-
ley et al, 2004; Liu et al, 2006)) utilize structures
defined over linguistic theory and annotations (e.g.,
Penn Treebank) and guide the derivation of SCFG
rules with explicit parsing on at least one side of
the parallel corpus. Formally syntax-based mod-
els (e.g., (Wu, 1997; Chiang, 2007)) extract syn-
chronous grammars from parallel corpora based on
the hierarchical structure of natural language pairs
without any explicit linguistic knowledge or anno-
tations. In this work, we focus on the hierarchi-
cal phrase-based models of Chiang (2007), which
is formally syntax-based, and always refer the term
SCFG, from now on, to the grammars of this model
class.
On the one hand, hierarchical phrase-based mod-
els do not suffer from errors in syntactic constraints
that are unavoidable in linguistically syntax-based
models. Despite the complete lack of linguistic
guidance, the performance of hierarchical phrase-
based models is competitive when compared to lin-
guistically syntax-based models. As shown in (Mi
and Huang, 2008), hierarchical phrase-based models
significantly outperform tree-to-string models (Liu
et al, 2006; Huang et al, 2006), even when at-
tempts are made to alleviate parsing errors using
either forest-based decoding (Mi et al, 2008) or
forest-based rule extraction (Mi and Huang, 2008).
On the other hand, when properly used, syntac-
tic constraints can provide invaluable benefits to im-
prove translation quality. The tree-to-string mod-
els of Mi and Huang (2008) can actually signif-
138
icantly outperform hierarchical phrase-based mod-
els when using forest-based rule extraction together
with forest-based decoding. Chiang (2010) also ob-
tained significant improvement over his hierarchi-
cal baseline by using syntactic parse trees on both
source and target sides to induce fuzzy (not exact)
tree-to-tree rules and by also allowing syntactically
mismatched substitutions.
In this paper, we augment rules in hierarchical
phrase-based translation systems with novel syntac-
tic features. Unlike previous studies (e.g., (Zoll-
mann and Venugopal, 2006)) that directly use ex-
plicit treebank categories such as NP, NP/PP (NP
missing PP from the right) to annotate phrase pairs,
we induce a set of latent categories to capture the
syntactic dependencies inherent in the hierarchical
structure of phrase pairs, and derive a real-valued
feature vector for each X nonterminal of a SCFG
rule based on the distribution of the latent cate-
gories. Moreover, we convert the equality test of
two sequences of syntactic categories, which are ei-
ther identical or different, into the computation of
a similarity score between their corresponding fea-
ture vectors. In our model, two symbolically dif-
ferent sequences of syntactic categories could have
a high similarity score in the feature vector repre-
sentation if they are syntactically similar, and a low
score otherwise. In decoding, these feature vectors
are utilized to measure the similarity between the
syntactic analysis of the source side and the syntax
of the SCFG rules that are applied to derive trans-
lations. Our approach maintains the advantages of
hierarchical phrase-based translation systems while
at the same time naturally incorporates soft syntactic
constraints. To the best of our knowledge, this is the
first work that applies real-valued syntactic feature
vectors to machine translation.
The rest of the paper is organized as follows.
Section 2 briefly reviews hierarchical phrase-based
translation models. Section 3 presents an overview
of our approach, followed by Section 4 describing
the hierarchical structure of aligned phrase pairs and
Section 5 describing how to induce latent syntactic
categories. Experimental results are reported in Sec-
tion 6, followed by discussions in Section 7. Sec-
tion 8 concludes this paper.
2 Hierarchical Phrase-Based Translation
An SCFG is a synchronous rewriting system gener-
ating source and target side string pairs simultane-
ously based on a context-free grammar. Each syn-
chronous production (i.e., rule) rewrites a nonter-
minal into a pair of strings, ? and ?, where ? (or
?) contains terminal and nonterminal symbols from
the source (or target) language and there is a one-to-
one correspondence between the nonterminal sym-
bols on both sides. In particular, the hierarchical
model (Chiang, 2007) studied in this paper explores
hierarchical structures of natural language and uti-
lize only a unified nonterminal symbol X in the
grammar,
X ? ??, ?,??
where ? is the one-to-one correspondence between
X?s in ? and ?, and it can be indicated by un-
derscripted co-indexes. Two example English-to-
Chinese translation rules are represented as follows:
X ? ?give the pen to me,????? (1)
X ? ?giveX1 to me, X1??? (2)
The SCFG rules of hierarchical phrase-based
models are extracted automatically from corpora of
word-aligned parallel sentence pairs (Brown et al,
1993; Och and Ney, 2000). An aligned sentence pair
is a tuple (E,F,A), where E = e1 ? ? ? en can be in-
terpreted as an English sentence of length n, F =
f1 ? ? ? fm its translation of length m in a foreign lan-
guage, andA a set of links between words of the two
sentences. Figure 1 (a) shows an example of aligned
English-to-Chinese sentence pair. Widely adopted
in phrase-based models (Och and Ney, 2004), a pair
of consecutive sequences of words from E and F is
a phrase pair if all words are aligned only within the
sequences and not to any word outside. We call a se-
quence of words a phrase if it corresponds to either
side of a phrase pair, and a non-phrase otherwise.
Note that the boundary words of a phrase pair may
not be aligned to any other word. We call the phrase
pairs with all boundary words aligned tight phrase
pairs (Zhang et al, 2008). A tight phrase pair is the
minimal phrase pair among all that share the same
set of alignment links. Figure 1 (b) highlights the
tight phrase pairs in the example sentence pair.
139
65
4
3
2
1
1
2 3 4 5
(a) (b)
Figure 1: An example of word-aligned sentence pair (a)
with tight phrase pairs marked in a matrix representation
(b).
The extraction of SCFG rules proceeds as fol-
lows. In the first step, all phrase pairs below a max-
imum length are extracted as phrasal rules. In the
second step, abstract rules are extracted from tight
phrase pairs that contain other tight phrase pairs by
replacing the sub phrase pairs with co-indexed X-
nonterminals. Chiang (2007) also introduced several
requirements (e.g., there are at most two nontermi-
nals at the right hand side of a rule) to safeguard
the quality of the abstract rules as well as keeping
decoding efficient. In our example above, rule (2)
can be extracted from rule (1) with the following sub
phrase pair:
X ? ?the pen,???
The use of a unified X nonterminal makes hier-
archical phrase-based models flexible at capturing
non-local reordering of phrases. However, such flex-
ibility also comes at the cost that it is not able to
differentiate between different syntactic usages of
phrases. Suppose rule X ? ?I am readingX1, ? ? ? ?
is extracted from a phrase pair with I am reading a
book on the source side whereX1 is abstracted from
the noun phrase pair . If this rule is used to translate
I am reading the brochure of a book fair, it would
be better to apply it over the entire string than over
sub-strings such as I ... the brochure of. This is be-
cause the nonterminal X1 in the rule was abstracted
from a noun phrase on the source side of the training
data and would thus be better (more informative) to
be applied to phrases of the same type. Hierarchi-
cal phrase-based models are not able to distinguish
syntactic differences like this.
Zollmann and Venugopal (2006) attempted to ad-
dress this problem by annotating phrase pairs with
treebank categories based on automatic parse trees.
They introduced an extended set of categories (e.g.,
NP+V for she went and DT\NP for great wall, an
noun phrase with a missing determiner on the left)
to annotate phrase pairs that do not align with syn-
tactic constituents. Their hard syntactic constraint
requires that the nonterminals should match exactly
to rewrite with a rule, which could rule out poten-
tially correct derivations due to errors in the syn-
tactic parses as well as to data sparsity. For exam-
ple, NP cannot be instantiated with phrase pairs of
type DT+NN, in spite of their syntactic similarity.
Venugopal et al (2009) addressed this problem by
directly introducing soft syntactic preferences into
SCFG rules using preference grammars, but they
had to face the computational challenges of large
preference vectors. Chiang (2010) also avoided hard
constraints and took a soft alternative that directly
models the cost of mismatched rule substitutions.
This, however, would require a large number of pa-
rameters to be tuned on a generally small-sized held-
out set, and it could thus suffer from over-tuning.
3 Approach Overview
In this work, we take a different approach to intro-
duce linguistic syntax to hierarchical phrase-based
translation systems and impose soft syntactic con-
straints between derivation rules and the syntactic
parse of the sentence to be translated. For each
phrase pair extracted from a sentence pair of a
source-side parsed parallel corpus, we abstract its
syntax by the sequence of highest root categories,
which we call a tag sequence, that exactly1 domi-
nates the syntactic tree fragments of the source-side
phrase. Figure 3 (b) shows the source-side parse tree
of a sentence pair. The tag sequence for ?the pen?
is simply ?NP? because it is a noun phrase, while
phrase ?give the pen? is dominated by a verb fol-
lowed by a noun phrase, and thus its tag sequence is
?VBP NP?.
Let TS = {ts1, ? ? ? , tsm} be the set of all tag se-
quences extracted from a parallel corpus. The syntax
of each X nonterminal2 in a SCFG rule can be then
1In case of a non-tight phrase pair, we only abstract and
compare the syntax of the largest tight part.
2There are three X nonterminals (one on the left and two on
the right) for binary abstract rules, two for unary abstract rules,
and one for phrasal rules.
140
Tag Sequence Probability
NP 0.40
DT NN 0.35
DT NN NN 0.25
Table 1: The distribution of tag sequences forX1 inX ?
?I am reading X1, ? ? ? ?.
characterized by the distribution of tag sequences
~PX(TS) = (pX(ts1), ? ? ? , pX(tsm)), based on the
phrase pairs it is abstracted from. Table 1 shows
an example distribution of tag sequences for X1 in
X ? ?I am reading X1, ? ? ? ?.
Instead of directly using tag sequences, as we
discussed their disadvantages above, we represent
each of them by a real-valued feature vector. Sup-
pose we have a collection of n latent syntactic cate-
gories C = {c1, ? ? ? , cn}. For each tag sequence ts,
we compute its distribution of latent syntactic cate-
gories ~Pts(C) = (pts(c1), ? ? ? , pts(cn)). For exam-
ple, ~P?NP VP?(C) = {0.5, 0.2, 0.3} means that the la-
tent syntactic categories c1, c2, and c3 are distributed
as p(c1) = 0.5, p(c2) = 0.2, and p(c3) = 0.3 for tag
sequence ?NP VP?. We further convert the distribu-
tion to a normalized feature vector ~F (ts) to repre-
sent tag sequence ts:
~F (ts) = (f1(ts), ? ? ? , fn(ts))
=
(pts(c1), ? ? ? , pts(cn))
?(pts(c1), ? ? ? , pts(cn))?
The advantage of using real-valued feature vec-
tors is that the degree of similarity between two tag
sequences ts and ts? in the space of the latent syn-
tactic categories C can be simply computed as a dot-
product3 of their feature vectors:
~F (ts) ? ~F (ts?) =
?
1?i?n
fi(ts)fi(ts
?)
which computes a syntactic similarity score in the
range of 0 (totally syntactically different) to 1 (com-
pletely syntactically identical).
Similarly, we can represent the syntax of each X
nonterminal in a rule with a feature vector ~F (X),
computed as the sum of the feature vectors of tag
3Other measures such as KL-divergence in the probability
space are also feasible.
sequences weighted by the distribution of tag se-
quences of the nonterminal X:
~F (X) =
?
ts?TS
pX(ts)~F (ts)
Now we can impose soft syntactic constraints us-
ing these feature vectors when a SCFG rule is used
to translate a parsed source sentence. Given that aX
nonterminal in the rule is applied to a span with tag
sequence4 ts as determined by a syntactic parser, we
can compute the following syntax similarity feature:
SynSim(X, ts) = ? log(~F (ts) ? ~F (X))
Except that it is computed on the fly, this feature
can be used in the same way as the regular features
in hierarchical translation systems to determine the
best translation, and its feature weight can be tuned
in the same way together with the other features on
a held-out data set.
In our approach, the set of latent syntactic cate-
gories is automatically induced from a source-side
parsed, word-aligned parallel corpus based on the
hierarchical structure among phrase pairs along with
the syntactic parse of the source side. In what fol-
lows, we will explain the two critical aspects of
our approach, i.e., how to identify the hierarchi-
cal structures among all phrase pairs in a sentence
pair, and how to induce the latent syntactic cate-
gories from the hierarchy to syntactically explain the
phrase pairs.
4 Alignment-based Hierarchy
The aforementioned abstract rule extraction algo-
rithm of Chiang (2007) is based on the property that
a tight phrase pair can contain other tight phrase
pairs. Given two non-disjoint tight phrase pairs that
share at least one common alignment link, there are
only two relationships: either one completely in-
cludes another or they do not include one another
but have a non-empty overlap, which we call a non-
trivial overlap. In the second case, the intersection,
differences, and union of the two phrase pairs are
4A normalized uniform feature vector is used for tag se-
quences (of parsed test sentences) that are not seen on the train-
ing corpus.
141
Figure 2: A decomposition tree of tight phrase pairs with
all tight phrase pairs listed on the right. As highlighted,
the two non-maximal phrase pairs are generated by con-
secutive sibling nodes.
also tight phrase pairs (see Figure 1 (b) for exam-
ple), and the two phrase pairs, as well as their inter-
section and differences, are all sub phrase pairs of
their union.
Zhang et al (2008) exploited this property to con-
struct a hierarchical decomposition tree (Bui-Xuan
et al, 2005) of phrase pairs from a sentence pair to
extract all phrase pairs in linear time. In this pa-
per, we focus on learning the syntactic dependencies
along the hierarchy of phrase pairs. Our hierarchy
construction follows Heber and Stoye (2001).
Let P be the set of tight phrase pairs extracted
from a sentence pair. We call a sequentially-ordered
list5 L = (p1, ? ? ? , pk) of unique phrase pairs pi ? P
a chain if every two successive phrase pairs in L
have a non-trivial overlap. A chain is maximal if
it can not be extended to its left or right with other
phrase pairs. Note that any sub-sequence of phrase
pairs in a chain generates a tight phrase pair. In par-
ticular, chain L generates a tight phrase pair ?(L)
that corresponds exactly to the union of the align-
ment links in p ? L. We call the phrase pairs
generated by maximal chains maximal phrase pairs
and call the other phrase pairs non-maximal. Non-
maximal phrase pairs always overlap non-trivially
with some other phrase pairs while maximal phrase
pairs do not, and it can be shown that any non-
maximal phrase pair can be generated by a sequence
of maximal phrase pairs. Note that the largest tight
phrase pair that includes all alignment links in A is
also a maximal phrase pair.
5The phrase pairs can be sequentially ordered first by the
boundary positions of the source-side phrase and then by the
boundary positions of the target-side phrase.
give
the pen to me .
X B B B X X
X
X
X
PP
VBP
DT NN TO PRP .
NP
VP
S
give
the pen to me .
(a) (b)
X
X
B B B X X
X
X
X
X
X
X
B B B X X
X
X
X
X
VBP
X
X
B B B X X
X
X
X
X
DT
NN TO PRP
.
NP PP
CR
VP
S
I(!)
O(!)
X
X
B B B X X
X
X
X
X
VBP DT
NN TO PRP
.
S
CR
NP PP
CR
O(!)
I(!)
(c) (d)
Figure 3: (a) decomposition tree for the English side of
the example sentence pair with all phrases underlined, (b)
automatic parse tree of the English side, (c) two example
binarized decomposition trees with syntactic emissions
in depicted in (d), where the two dotted curves give an
example I(?) and O(?) that separate the forest into two
parts.
Lemma 1 Given two different maximal phrase
pairs p1 and p2, exactly one of the following alter-
natives is true: p1 and p2 are disjoint, p1 is a sub
phrase pair of p2, or p2 is a sub phrase pair of p1.
A direct outcome of Lemma 1 is that there is an
unique decomposition tree T = (N,E) covering all
of the tight phrase pairs of a sentence pair, where N
is the set of maximal phrase pairs and E is the set of
edges that connect between pairs of maximal phrase
pairs if one is a sub phrase pair of another. All of the
tight phrase pairs of a sentence pair can be extracted
directly from the nodes of the decomposition tree
(these phrase pairs are maximal), or generated by se-
quences of consecutive sibling nodes6 (these phrase
pairs are non-maximal). Figure 2 shows the decom-
position tree as well as all of the tight phrase pairs
that can be extracted from the example sentence pair
in Figure 1.
We focus on the source side of the decomposition
tree, and expand it to include all of the non-phrase
6Unaligned words may be added.
142
single words within the scope of the decomposition
tree as frontiers and attach each as a child of the low-
est node that contains the word. We then abstract the
trees nodes with two symbol, X for phrases, and B
for non-phrases, and call the result the decomposi-
tion tree of the source side phrases. Figure 3 (a) de-
picts such tree for the English side of our example
sentence pair. We further recursively binarize7 the
decomposition tree into a binarized decomposition
forest such that all phrases are directly represented
as nodes in the forest. Figure 3 (c) shows two of the
many binarized decomposition trees in the forest.
The binarized decomposition forest compactly
encodes the hierarchical structure among phrases
and non-phrases. However, the coarse abstraction
of phrases with X and non-phrases with B provides
little information on the constraints of the hierarchy.
In order to bring in syntactic constraints, we anno-
tate the nodes in the decomposition forest with syn-
tactic observations based on the automatic syntactic
parse tree of the source side. If a node aligns with
a constituent in the parse tree, we add the syntactic
category (e.g., NP) of the constituent as an emitted
observation of the node, otherwise, it crosses con-
stituent boundaries and we add a designated crossing
category CR as its observation. We call the resulting
forest a syntactic decomposition forest. Figure 3 (d)
shows two syntactic decomposition trees of the for-
est based on the parse tree in Figure 3 (b). We will
next describe how to learn finer-grained X and B
categories based on the hierarchical syntactic con-
straints.
5 Inducing Latent Syntactic Categories
If we designate a unique symbol S as the new root
of the syntactic decomposition forests introduced
in the previous section, it can be shown that these
forests can be generated by a probabilistic context-
free grammar G = (V,?, S,R, ?), where
? V = {S,X,B} is the set of nonterminals,
? ? is the set of terminals comprising treebank
categories plus the CR tag (the crossing cate-
gory),
7The intermediate binarization nodes are also labeled as ei-
ther X or B based on whether they exactly cover a phrase or
not.
? S ? V is the unique start symbol,
? R is the union of the set of production rules
each rewriting a nonterminal to a sequence of
nonterminals and the set of emission rules each
generating a terminal from a nonterminal,
? and ? assigns a probability score to each rule
r ? R.
Such a grammar can be derived from the set of
syntactic decomposition forests extracted from a
source-side parsed parallel corpus, with rule prob-
ability scores estimated as the relative frequencies
of the production and emission rules.
The X and B nonterminals in the grammar are
coarse representations of phrase and non-phrases
and do not carry any syntactic information at all.
In order to introduce syntax to these nonterminals,
we incrementally split8 them into a set of latent
categories {X1, ? ? ? , Xn} for X and another set
{B1, ? ? ? , Bn} for B, and then learn a set of rule
probabilities9 ? on the latent categories so that the
likelihood of the training forests are maximized. The
motivation is to let the latent categories learn differ-
ent preferences of (emitted) syntactic categories as
well as structural dependencies along the hierarchy
so that they can carry syntactic information. We call
them latent syntactic categories. The learned Xi?s
represent syntactically-induced finer-grained cate-
gories of phrases and are used as the set of latent
syntactic categories C described in Section 3. In re-
lated research, Matsuzaki et al (2005) and Petrov et
al. (2006) introduced latent variables to learn finer-
grained distinctions of treebank categories for pars-
ing, and Huang et al (2009) used a similar approach
to learn finer-grained part-of-speech tags for tag-
ging. Our method is in spirit similar to these ap-
proaches.
Optimization of grammar parameters to maximize
the likelihood of training forests can be achieved
8We incrementally split each nonterminal to 2, 4, 8, and fi-
nally 16 categories, with each splitting followed by several EM
iterations to tune model parameters. We consider 16 an appro-
priate number for latent categories, not too small to differentiate
between different syntactic usages and not too large for the extra
computational and storage costs.
9Each binary production rule is now associated with a 3-
dimensional matrix of probabilities, and each emission rule as-
sociated with a 1-dimensional array of probabilities.
143
by a variant of Expectation-Maximization (EM) al-
gorithm. Recall that our decomposition forests are
fully binarized (except the root). In the hypergraph
representation (Huang and Chiang, 2005), the hy-
peredges of our forests all have the same format10
?(V,W ), U?, meaning that node U expands to nodes
V and W with production rule U ? VW . Given
a forest F with root node R, we denote e(U) the
emitted syntactic category at node U and LR(U) (or
PL(W ), or PR(V ))11 the set of node pairs (V,W )
(or (U, V ), or (U,W )) such that ?(V,W ), U? is a hy-
peredge of the forest. Now consider node U , which
is either S, X , or B, in the forest. Let Ux be the
latent syntactic category12 of node U . We define
I(Ux) the part of the forest (includes e(U) but not
Ux) inside U , and O(Ux) the other part of the forest
(includes Ux but not e(U)) outside U , as illustrated
in Figure 3 (d). The inside-outside probabilities are
defined as:
PIN(Ux) = P (I(Ux)|Ux)
POUT(Ux) = P (O(Ux)|S)
which can be computed recursively as:
PIN(Ux) =
?
(V,W )?LR(U)
?
y,z
?(Ux ? e(U))
??(Ux ? VyWz)
?PIN(Vy)PIN(Wz)
POUT(Ux) =
?
(V,W )?PL(U)
?
y,z
?(Vy ? e(V ))
??(Vy ?WzUx)
?POUT(Vy)PIN(Wz)
+
?
(V,W )?PR(U)
?
y,z
?(Vy ? e(V ))
??(Vy ? UxWz)
?POUT(Vy)PIN(Wz)
In the E-step, the posterior probability of the oc-
currence of production rule13 Ux ? VyWz is com-
puted as:
P (Ux ? VyWz|F ) =
?(Ux ? e(U))
??(Ux ? VyWz)
?POUT(Ux)PIN(Vy)PIN(Ww)
PIN(R)
10The hyperedge corresponding to the root node has a differ-
ent format because it is unary, but it can be handled similarly.
When clear from context, we use the same variable to present
both a node and its label.
11LR stands for the left and right children, PL for the parent
and left children, and PR for the parent and right children.
12We never split the start symbol S, and denote S0 = S.
13The emission rules can be handled similarly.
In the M-step, the expected counts of rule Ux ?
VyWz for all latent categories Vy and Wz are accu-
mulated together and then normalized to obtain an
update of the probability estimation:
?(Ux ? VyWz) =
#(Ux ? VyWz)
?
(V ?,W ?)
?
y,z
#(Ux ? VyWz)
Recall that each node U labeled asX in a forest is
associated with a phrase whose syntax is abstracted
by a tag sequence. Once a grammar is learned, for
each such node with a corresponding tag sequence
ts in forest F , we compute the posterior probability
that the latent category of node U being Xi as:
P (Xi|ts) =
POUT(Ui)PIN(Ui)
PIN(R)
This contributes P (Xi|ts) evidence that tag se-
quence ts belongs to a Xi category. When all
of the evidences are computed and accumulated in
#(Xi, ts), they can then be normalized to obtain the
probability that the latent category of ts is Xi:
pts(Xi) =
#(Xi, ts)
?
i #(Xi, ts)
As described in Section 3, the distributions of latent
categories are used to compute the syntactic feature
vectors for the SCFG rules.
6 Experiments
We conduct experiments on two tasks, English-to-
German and English-to-Chinese, both aimed for
speech-to-speech translation. The training data for
the English-to-German task is a filtered subset of the
Europarl corpus (Koehn, 2005), containing ?300k
parallel bitext with ?4.5M tokens on each side. The
dev and test sets both contain 1k sentences with one
reference for each. The training data for the English-
to-Chinese task is collected from transcription and
human translation of conversations in travel domain.
It consists of ?500k parallel bitext with ?3M to-
kens14 on each side. Both dev and test sets contain
?1.3k sentences, each with two references. Both
14The Chinese sentences are automatically segmented into
words. However, BLEU scores are computed at character level
for tuning and evaluation.
144
corpora are also preprocessed with punctuation re-
moved and words down-cased to make them suitable
for speech translation.
The baseline system is our implementation of the
hierarchical phrase-based model of Chiang (2007),
and it includes basic features such as rule and
lexicalized rule translation probabilities, language
model scores, rule counts, etc. We use 4-gram lan-
guage models in both tasks, and conduct minimum-
error-rate training (Och, 2003) to optimize feature
weights on the dev set. Our baseline hierarchical
model has 8.3M and 9.7M rules for the English-to-
German and English-to-Chinese tasks, respectively.
The English side of the parallel data is
parsed by our implementation of the Berkeley
parser (Huang and Harper, 2009) trained on the
combination of Broadcast News treebank from
Ontonotes (Weischedel et al, 2008) and a speechi-
fied version of the WSJ treebank (Marcus et al,
1999) to achieve higher parsing accuracy (Huang et
al., 2010). Our approach introduces a new syntactic
feature and its feature weight is tuned in the same
way together with the features in the baseline model.
In this study, we induce 16 latent categories for both
X and B nonterminals.
Our approach identifies ?180k unique tag se-
quences for the English side of phrase pairs in both
tasks. As shown by the examples in Table 2, the syn-
tactic feature vector representation is able to identify
similar and dissimilar tag sequences. For instance,
it determines that the sequence of ?DT JJ NN? is
syntactically very similar to ?DT ADJP NN? while
very dissimilar to ?NN CD VP?. Notice that our la-
tent categories are learned automatically to maxi-
mize the likelihood of the training forests extracted
based on alignment and are not explicitly instructed
to discriminate between syntactically different tag
sequences. Our approach is not guaranteed to al-
ways assign similar feature vectors to syntactically
similar tag sequences. However, as the experimental
results show below, the latent categories are able to
capture some similarities among tag sequences that
are beneficial for translation.
Table 3 and 4 report the experimental results
on the English-to-German and English-to-Chinese
tasks, respectively. The addition of the syntax fea-
ture achieves a statistically significant improvement
(p ? 0.01) of 0.6 in BLEU on the test set of the
Baseline +Syntax ?
Dev 16.26 17.06 0.80
Test 16.41 17.01 0.60
Table 3: BLEU scores of the English-to-German task
(one reference).
Baseline +Syntax ?
Dev 46.47 47.39 0.92
Test 45.45 45.86 0.41
Table 4: BLEU scores of the English-to-Chinese task
(two references).
English-to-German task. This improvement is sub-
stantial given that only one reference is used for each
test sentence. On the English-to-Chinese task, the
syntax feature achieves a smaller improvement of
0.41 BLEU on the test set. One potential explanation
for the smaller improvement is that the sentences on
the English-to-Chinese task are much shorter, with
an average of only 6 words per sentence, compared
to 15 words in the English-to-German task. The
hypothesis space of translating a longer sentence is
much larger than that of a shorter sentence. There-
fore, there is more potential gain from using syn-
tax features to rule out unlikely derivations of longer
sentences, while phrasal rules might be adequate for
shorter sentences, leaving less room for syntax to
help as in the case of the English-to-Chinese task.
7 Discussions
The incorporation of the syntactic feature into the
hierarchical phrase-based translation system also
brings in additional memory load and computational
cost. In the worst case, our approach requires stor-
ing one feature vector for each tag sequence and one
feature vector for each nonterminal of a SCFG rule,
with the latter taking the majority of the extra mem-
ory storage. We observed that about 90% of the
X nonterminals in the rules only have one tag se-
quence, and thus the required memory space can be
significantly reduced by only storing a pointer to the
feature vector of the tag sequence for these nonter-
minals. Our approach also requires computing one
dot-product of two feature vectors for each nonter-
minal when a SCFG rule is applied to a source span.
145
Very similar Not so similar Very dissimilar
~F (ts) ? ~F (ts?) > 0.9 0.4 ? ~F (ts) ? ~F (ts?) ? 0.6 ~F (ts) ? ~F (ts?) < 0.1
DT JJ NN
DT NN DT JJ JJ NML NN PP NP NN
DT JJ JJ NN DT JJ CC INTJ VB NN CD VP
DT ADJP NN DT NN NN JJ RB NP IN CD
VP
VB VP PP JJ NN JJ NN TO VP
VB RB VB PP VB NN NN VB JJ WHNP DT NN
VB DT DT NN VB RB IN JJ IN INTJ NP
ADJP
JJ ADJP JJ JJ CC ADJP IN NP JJ
PDT JJ ADJP VB JJ JJ AUX RB ADJP
RB JJ ADVP WHNP JJ ADJP VP
Table 2: Examples of similar and dissimilar tag sequences.
This cost can be reduced, however, by caching the
dot-products of the tag sequences that are frequently
accessed.
There are other successful investigations to
impose soft syntactic constraints to hierarchical
phrase-based models by either introducing syntax-
based rule features such as the prior derivation
model of Zhou et al (2008) or by imposing con-
straints on translation spans at decoding time, e.g.,
(Marton and Resnik, 2008; Xiong et al, 2009;
Xiong et al, 2010). These approaches are all or-
thogonal to ours and it is expected that they can be
combined with our approach to achieve greater im-
provement.
This work is an initial effort to investigate latent
syntactic categories to enhance hierarchical phrase-
based translation models, and there are many direc-
tions to continue this line of research. First, while
the current approach imposes soft syntactic con-
straints between the parse structure of the source
sentence and the SCFG rules used to derive the
translation, the real-valued syntactic feature vectors
can also be used to impose soft constraints between
SCFG rules when rule rewrite occurs. In this case,
target side parse trees could also be used alone or to-
gether with the source side parse trees to induce the
latent syntactic categories. Second, instead of using
single parse trees during both training and decod-
ing, our approach is likely to benefit from exploring
parse forests as in (Mi and Huang, 2008). Third,
in addition to the treebank categories obtained by
syntactic parsing, lexical cues directly available in
sentence pairs could also to explored to guide the
learning of latent categories. Last but not the least,
it would be interesting to investigate discriminative
training approaches to learn latent categories that di-
rectly optimize on translation quality.
8 Conclusion
We have presented a novel approach to enhance
hierarchical phrase-based machine translation sys-
tems with real-valued linguistically motivated fea-
ture vectors. Our approach maintains the advan-
tages of hierarchical phrase-based translation sys-
tems while at the same time naturally incorpo-
rates soft syntactic constraints. Experimental results
showed that this approach improves the baseline hi-
erarchical phrase-based translation models on both
English-to-German and English-to-Chinese tasks.
We will continue this line of research and exploit
better ways to learn syntax and apply syntactic con-
straints to machine translation.
Acknowledgements
This work was done when the first author was visit-
ing IBM T. J. Watson Research Center as a research
intern. We would like to thank Mary Harper for
lots of insightful discussions and suggestions and the
anonymous reviewers for the helpful comments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
146
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics.
Binh Minh Bui-Xuan, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura?s al-
gorithm. In ISAAC.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What?s in a translation rule. In
HLT/NAACL.
Steffen Heber and Jens Stoye. 2001. Finding all common
intervals of k permutations. In CPM.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In International Workshop on Parsing Tech-
nology.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In CHSLP.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable. In
EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor, 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In EMNLP.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In NAACL.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009.
A syntax-driven bracketing model for phrase-based
translation. In ACL-IJCNLP.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In NAACL-HLT.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In COLING.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntactic
parsing and tree kernels. In SSST.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
StatMT.
147
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 545?555,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and
Forest-to-String Decoding?
Martin C?mejrek??
?IBM Prague Research Lab
V Parku 2294/4
Prague, Czech Republic, 148 00
martin.cmejrek@us.ibm.com
Haitao Mi? and Bowen Zhou?
?IBM T. J. Watson Research Center
1101 Kitchawan Rd
Yorktown Heights, NY 10598
{hmi,zhou}@us.ibm.com
Abstract
Machine translation benefits from system
combination. We propose flexible interaction
of hypergraphs as a novel technique combin-
ing different translation models within one de-
coder. We introduce features controlling the
interactions between the two systems and ex-
plore three interaction schemes of hiero and
forest-to-string models?specification, gener-
alization, and interchange. The experiments
are carried out on large training data with
strong baselines utilizing rich sets of dense
and sparse features. All three schemes signif-
icantly improve results of any single system
on four testsets. We find that specification?a
more constrained scheme that almost entirely
uses forest-to-string rules, but optionally uses
hiero rules for shorter spans?comes out as
the strongest, yielding improvement up to 0.9
(Ter-Bleu)/2 points. We also provide a de-
tailed experimental and qualitative analysis of
the results.
1 Introduction
Recent years have witnessed the success of var-
ious statistical machine translation (SMT) mod-
els using different levels of linguistic knowledge?
phrase (Koehn et al, 2003), hiero (Chiang, 2005),
and syntax-based (Liu et al, 2006; Galley et al,
2006). System combination became a promising
way of building up synergy from different SMT sys-
tems and their specific merits.
Numerous efforts that have been proposed in this
field recently can be broadly divided into two cat-
?M. C? and H. M. contributed equally to this work.
egories: Offline system combination (Rosti et al,
2007; He et al, 2008; Watanabe and Sumita, 2011;
Denero et al, 2010) aims at producing consensus
translations from the outputs of multiple individ-
ual systems. Those outputs usually contain k-best
lists of translations, which only explore a small por-
tion of the entire search space of each system. This
issue is well addressed in joint decoding (Liu et
al., 2009), or online system combination, showing
comparable improvements to the offline combina-
tion methods. Rather than finding consensus trans-
lations from the outputs of individual systems, joint
decoding works with different grammars at the de-
coding time. Although limited to individual systems
sharing the same search paradigm (e.g. left-to-right
or bottom-up), joint decoding offers many poten-
tial advatages: search through a larger space, bet-
ter efficiency, features designed once for all subsys-
tems, potential cross-system features, online sharing
of partial hypotheses, and many others.
Different approaches have different strengths in
general?hiero rules are believed to provide reliable
lexical coverage, while tree-to-string rules are good
at non-local reorderings. Different contexts present
different challenges?noun phrases usually follow
the adjacency principle, while verb phrases require
more challenging reorderings. In this work, we study
different schemes of interaction between translation
models, reflecting their specific strengths at differ-
ent (syntactic) contexts. We make five new contribu-
tions:
First, we propose a framework for joint decod-
ing by means of flexible combination of trans-
lation hypergraphs, allowing for detailed con-
545
trol of interactions between the different sys-
tems using soft constraints (Section 3).
Second, we study three interaction schemes?
special cases of joint decoding: generalization,
specification, and interchange (Section 3.3).
Third, instead of using a tree-to-string system,
we use a much stronger forest-to-string sys-
temwith fuzzy match of nonterminal categories
(Section 2.1).
Fourth, we train strong systems on a large-
scale data set, and test all methods on four test
sets. Experimental results (Section 6) show that
our new approach brings improvement of up to
0.9 points in terms of (Ter ? Bleu)/2 over the
best single system.
Fifth, we conduct a comprehensive experimen-
tal analysis, and find that joint decoding actu-
ally prefers tree-to-string rules in both shorter
and longer spans. (Section 6.3).
The paper is organized as follows: We briefly re-
view the individual models in Section 2, describe
the method of joint decoding using three alternative
interaction schemes in Section 3, describe the fea-
tures controlling the interactions and fuzzy match in
Section 4, review the related work in Section 5, and
finally, describe our experiments and give detailed
discussion of the results in Section 6.
2 Individual Models
Our individual models are two state-of-the-art sys-
tems: a hiero model (Chiang, 2005), and a forest-to-
string model (Mi et al, 2008; Mi and Huang, 2008).
We will use the following example from Chinese
to English to explain both individual and joint de-
coding algorithms throughout this paper.
SS ta?olu`nSSSSSSSS hu`iSSSSS ze?nmeya`ng
discussion/NN SSS will/VV how/VV
S discuss/VV SS meeting/NN
There are several possible meanings based on the
different POS tagging sequences:
1: NN VV VV: How is the discussion going?
2: VV NN VV: Discuss about the meeting.
3: NN NN VV: How was the discussion meeting?
4: VV VV VV: Discuss what will happen.
id rule
r1 VV(ta?olu`n) ? discuss
r2 NP(ta?olu`n) ? the discussion
r3 NP(hu`i) ? the meeting
r4 VP(ze?nmeya`ng) ? how
r?4 VP(ze?nmeya`ng) ? about
r5 IP(x1:NP x2:VP) ? x2 x1
r6 IP(x1:VV x2:IP) ? x1 x2
r7 IP(x1:NP VP(VV(hu`i) x2:VP)) ? x2 is x1 going
r11 X(x1:X ze?nmeya`ng) ? how was x1
r12 X(ze?nmeya`ng) ? what
r13 X(ta?olu`n hu`i) ? the discussion meeting
r14 X(hu`i x1:X) ? x1 will happen
r15 S(x1:S x2:X) ? x1 x2
Table 1: Translation rules. Tree-to-string (r1?r7), hiero
(r11?r14), vanilla glue (r15).
IP
x1:NP VP
VV
hu`i
x2:VP
? x2 is x1 going
Figure 1: Tree-to-string rule r7.
Table 1 shows translation rules that can generate
all four translations. We will use those rules in the
following sections.
2.1 Forest-to-string
Forest-to-string translation (Mi et al, 2008) is a lin-
guistic syntax-based system, which significantly im-
proves the translation quality of the tree-to-string
model (Liu et al, 2006; Huang et al, 2006) by using
a packed parse forest as the input instead of a single
parse tree.
Figure 1 shows a tree-to-string translation
rule (Huang et al, 2006), which is a tuple
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the source-side
tree fragment, whose internal nodes are labeled by
nonterminal symbols (like NP and VP), and whose
frontier nodes are labeled by source-language words
(like ?hu`i?) or variables from a set X = {x1, x2, . . .};
rhs(r) is the target-side string expressed in target-
language words (like ?going?) and variables; and
?(r) is a mapping from X to nonterminals. Each
546
(a)
IP0, 3
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
VP1, 3
VP2, 3
ze?nmeya`ng
Rt
? (b)
IP0, 3
X0, 2
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
X1, 3 VP1, 3
VP2, 3
ze?nmeya`ng
X0, 3
e5
e6
e7
? Rh ?
(b?)
IP0, 3
X0, 2
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
X1, 3 VP1, 3
VP2, 3
ze?nmeya`ng
X2, 3
X0, 3
e11
e14
? (c)
IP0, 3
X0, 2
VV0, 1
ta?olu`n
NP0, 1
IP1, 3
NP1, 2
hu`i
VV1, 2
X1, 3 VP1, 3
VP2, 3
ze?nmeya`ng
X2, 3
X0, 3
Figure 2: Parse and translation hypergraphs. (a) The parse forest of the example sentence. Solid hyperedges denote
the 1-best parse. (b) The corresponding translation forest F t after applying the tree-to-string translation rule set Rt.
Target lexical content is not shown. Each translation hyperedge (e.g. e7) has the same index as the corresponding rule
(r7). Gray nodes (e.g. VP1,3) became inaccessible due to the insufficient rule coverage. (b?) The translation forest Fh
after applying the hierarchical rule set Rh to the input sentence. (c) The combined translation forest Hm obtained by
superimposing b and b?. The nodes within each solid box share the same span. See Figure 3 for an example of the
internal structure of a box. The forest-to-string system can produce the translation 1 (dashed derivation: r2, r4 and r7)
and 2 (solid derivation: r1, r3, r?4, r5, and r6). Hierarchical rules generate the translation 3 (r11 and r13). The translation
4 is available by using joint decoding at X1, 3 ? IP1, 3 with the derivation: r1, r6, r12, and r14.
variable xi ? X occurs exactly once in lhs(r) and
exactly once in rhs(r). Take the rule r7 in Figure 1
for example, we have:
lhs(r7) = IP(x1:NP VP(VV(hu`i) x2:VP)),
rhs(r7) = x2 is x1 going,
?(r7) = {x1 7? NP, x2 7? VP}.
Typically, a forest-to-string system performs
translation in two steps (shown in Figure 2): pars-
ing and decoding. In the parsing step, we convert the
source language input into a parse forest (a). In the
decoding step, we first convert the parse forest into a
translation forest Ft in (b) by using the fast pattern-
matching technique (Zhang et al, 2009). For exam-
ple, we pattern-match the rule r7 rooted at IP0, 3, in
such a way that x1 spans NP0, 1 and x2 spans VP2, 3,
and add a translation hyperedge e7 in (b). Then the
decoder searches for the best derivation on the trans-
lation forest and outputs the target string.
2.2 Hiero
Hiero (hierarchical phrase-based) model (Chiang,
2005) acquires rules of synchronous context-free
grammars (SCFGs) from word-aligned parallel data,
and uses plain sequences of words as the input, with-
out any syntactic information.
547
FN
IP?1, 3
IP1, 3
BBBBSN
X?1, 3
X1, 3
EEEE
scheme interaction edges in supernode
Generalization
IP?1, 3 X?1, 3
IP1, 3 X1, 3
Specification
IP?1, 3 X?1, 3
IP1, 3 X1, 3
Interchange
IP?1, 3 X?1, 3
IP1, 3 X1, 3
Figure 3: Three interaction schemes for joint decoding.
Details of the interaction supernode for span (1, 3) shown
in Figure 2 (c). Soft constraints control the transitions.
SCFG can be formalized as a set of tuples
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the source-side
one-level CFG, whose root is X or S, and whose
frontier nodes are labeled by source-language words
(like ?hu`i?) or variables from a set X = {x1, x2, . . .};
rhs(r) is the target-side string expressed in target-
language words (like ?going?) and variables; and
?(r) is a mapping from X to nonterminals. Table 1
shows examples of hiero rules r11?r15.
Although different on source side, hiero decod-
ing can be formalized equally as forest-to-string de-
coding: First, pattern-match the input sentence into
a translation forest Fh. For example, since the rule
r11 matches ?ze?nmeya`ng? such that x1 spans the first
two words, add a hyperedge e11 in Figure 2 (b?).
Then search for the best derivation over the trans-
lation forest.
3 Joint Decoding
The goal of joint decoding is to let different MT
models collaborate within the framework of a single
decoder. This can be done by combining translation
hypergraphs of the different models at the decod-
ing time, so that online sharing of partial hypotheses
overcomes weaknesses and boosts strengths of the
systems combined.
As both forest-to-string and hiero produce trans-
lation forests that share the same hypergraph struc-
ture, we first formalize the hypergraph, then we in-
troduce an algorithm to combine different hyper-
graphs, and finally we describe three joint decoding
schemes over the merged hypergraph.
3.1 Hypergraphs
More formally, a hypergraph H is a pair ?V, E?,
where V is the set of nodes, and E the set of hyper-
edges. For a given sentence w1:l = w1 . . .wl, each
node v ? V is in the form of Y i, j, where Y is a
nonterminal in the context-free grammar1 and i, j,
0 ? i < j ? l, are string positions in the sentence
w1:l, which denote the recognition of nonterminal
Y spanning the substring from positions i through j
(that is, wi+1 . . .w j). Each hyperedge e ? E is a tuple
?tails(e), head(e), target(e)?, where head(e) ? V is
the consequent node in the deductive step, tails(e) ?
V? is the list of antecedent nodes, and target(e) is
a list of rhs(r) for rules r such that each rule r has
the same lhs(r) pattern-matched at the node head(e).
For example, the hyperedge e7 in Figure 2 (b) is
e7 = ?(NP0, 1,VP2, 3), IP0, 3, (x2 is x1 going)?,
where we can infer the mapping to be
{x1 7? NP0, 1, x2 7? VP2, 3 }.
We also denote BS(v) to be the set of incoming
hyperedges of node v, which represent the different
ways of deriving v. For example, BS(IP0, 3) is a set
of e7 and e6.
There is also a distinguished root node TOP in
each hypergraph, denoting the goal item in transla-
tion, which is simply TOP0, l.
3.2 Combining Hypergraphs
We enable interaction between translation hyper-
graphs, such as hiero Fh = ?Vh, Eh? and forest-to-
string Ft = ?V t, Et?, on nodes covering the same
span (e.g. IP1, 3 and X1, 3 in Figure 2 (c) grouped in
a box). We call such groups interaction supernodes
and show a detailed example of a supernode for span
(1, 3) in Figure 3.
The combination runs in four steps:
1In this paper, nonterminal labels X and S denote hiero
derivations, other labels are tree-to-string labels.
548
1. For each node v = Y i, j, v ? Vh ? V t, we create
a new interaction node v? = Y ?i, j with empty
BS (v?). For example, we create two nodes,
IP?1, 3 and X?1, 3, at the top of Figure 3.
2. For each hyperedge e ? BS(v), v ? V t ? Vh,
we replace each v in tails(e) with v?. For exam-
ple, e7 becomes ?(NP?0, 1,VP?2, 3), IP0, 3, (x2 is
x1 going)?.
3. All the nodes and hyperedges form the merged
hypergraph Fm, such as in Figure 2 (c).
4. Insert interaction hyperedges connecting nodes
within each interaction supernode to make Fm
connected again.
In the following subsection we present details of in-
teractions and introduce three alternative schemes.
3.3 Three Schemes of Joint Decoding
Interaction hyperedges within each supernode allow
the decoder either to stay within the same system
(e.g. in hiero using X1, 3 ? X?1, 3 in Figure 3), or to
switch to the other (e.g. to forest-to-string using X1, 3
? IP?1, 3).
For example, translation 4 can be produced as
follows: The source string ?ze?nmeya`ng? is trans-
lated by the phrase rule r12. The hiero hyperedge
e14 combines it with the translation of ?hu`i?, reach-
ing the hiero node X1, 3. Using the interaction edge
X1, 3 ? IP?1, 3 will switch into the tree-to-string
model, so that the translation can be completed with
the tree-to-string edge e6 that connects it with a par-
tial tree-to string translation of ?ta?olu`n? done by r1.
In order to achieve more precise control over the
interaction between tree-to-string and hiero deriva-
tions, we propose the following three basic inter-
action schemes: generalization, specification, in-
terchange. The schemes control the interaction be-
tween hiero and tree-to-string models by means of
soft constraints. Some schemes may even restrict
certain types of transitions. The schemes are de-
picted in Figure 3 and their details are discussed in
the following three subsections.
3.3.1 Specification
The specification decoding scheme reflects the in-
tuition of using hiero rules to translate shorter spans
and tree-to-string rules to reorder higher-level sen-
tence structures. In other words, the scheme allows
one-way switching from the hiero general nontermi-
nal into the more specific nonterminal of a tree-to-
string rule. Transitions in reverse directions are not
allowed. This is achieved by inserting specification
interaction hyperedges e leading from hiero nodes
Xi, j or Si, j into all tree-to-string interaction nodes
Y?i, j within the same supernode.
3.3.2 Generalization
In some translation domains, hiero outperforms
tree-to-string systems, as was shown in experiments
in Section 6. While local hiero or tree-to-string re-
orderings perform well, long distance reorderings
proposed by tree-to-string may be too risky (e.g. due
to parsing errors), so that monotone concatenation
of long sequences2 is the more reliable strategy. The
generalization decoding scheme, complementary to
the specification, is motivated by the idea of incorpo-
rating reliable tree-to-string translations for some se-
quences into a strong hiero translation system. This
is achieved by inserting generalization interaction
hyperedges e leading from tree-to-string nodes Yi, j
nodes into general hiero interaction nodes X?i, j and
S?i, j within the same supernode.
3.3.3 Interchange
The interchange decoding scheme is a union of
the two previous approaches. Any derivation can
freely combine hiero and tree-to-string productions.
Both specification and generalization interaction
hyperedges are inserted leading from all hiero and
tree-to-string nodes Xi, j, Si, j, and Yi, j into all inter-
action nodes X?i, j, S?i, j, and Y?i, j.
3.4 Fuzzy match
The translation rule set cannot usually cover all
hyperedges in the parse forest, thus some nodes
become inaccessible in the translation forest (e.g.
VP1, 3 in Figure 2). However, in the parse forest, as
opposed to a 1-best tree, we can find other nodes
spanning the same sequence wi: j (e.g. node IP1, 3).
In order to re-enable inaccessible nodes and to in-
crease the variability of the translation forest, we
allow reaching them from the other tree-to-string
2Monotone glue is the only possibility for very long spans
exceeding the hiero maxParse treshold.
549
nodes within the same interaction node. This can
be achieved by adding fuzzy hyperedges between
every tree-to-string state Y i, j and a differently la-
beled tree-to-string interaction state Z?i, j. For exam-
ple, in the span (0,1), we have a fuzzy hyperedge
VV0, 1 ? NP?0, 1.
While interaction hyperedges combine different
translation models, fuzzy hyperedges combine dif-
ferent derivations within the same (tree-to-string)
model.
4 Interaction Features
Our baseline systems use the log-linear framework
to estimate the probability P(D) of a derivation D
from features ?i and their weights ?i as P(D) ?
exp
(?
i ?i?i
)
. Similarly as Chiang et al (2009), our
systems use tens of dense (e.g. language models,
translation probabilities) and thousands of sparse
(e.g. lexical, fertility) features.
The features related to the joint decoding experi-
ments are the costs for specification, generalization,
interchange, and the fuzzy match. Let Lt be the set
of the labels used by the source language parser and
Lh = {S,X} be the labels used by hiero.
The generalization feature
?Y?Z = |{e; e ? D,?i, j tails(e) = {Yi, j} (1)
?head(e) = Z?i, j}|
is the total number of generalization hyperedges in
D going from tree-to-string states Y ? Lt to hiero
states Z? ? Lh.
The specification feature
?Z?Y = |{e; e ? D,?i, j tails(e) = {Zi, j} (2)
?head(e) = Y?i, j}|
is the total number of specification hyperedges in D
going from hiero states Z ? Lh to tree-to-string states
Y ? ? Lt.
The interchange feature is implemented by en-
abling the generalization and specification features
at the same time for both tuning and testing.
The fuzzy match feature
?U?W = |{e; e ? D,?i, j tails(e) = {Ui, j} (3)
?head(e) = W?i, j}|
is the total number of fuzzy match hyperedges in D
going from tree-to-tree statesU ? Lt to tree-to-string
states W? ? Lt. 3
We use MIRA to obtain weights for the new fea-
tures by tuning on the development set. The num-
ber of new parameters to tune can be estimated as
|Lh| ? |Lt| for generalization and specification, and
2 ? |Lh| ? |Lt| for interchange. For the fuzzy match
of tree-to-string nonterminals we have |Lt| ? |Lt| pa-
rameters organized as a sparse matrix, since we only
consider combinations on nonterminal labels that
cooccur in the data.4
5 Related Work
From the previous explorations of online translation
model combination, we see the work of Liu et al
(2009) proposing an unconstrained combination of
hiero and tree-to-string models as a special configu-
ration of our framework, and we also replicate it.
Denero et al (2010) combine translation mod-
els even with different search paradigms. Their ap-
proach is different, since their component systems
do not interact at decoding time, instead, each of
them provides its weighted translation forest first,
the forests are then combined to infer a new com-
bination model.
6 Experiment
In this section we describe the setup, present results,
and analyze the experiments. Finally, we propose fu-
ture directions of research.
3Here we allow U = W, which can be viewed in such a way
that exact match is a special case of fuzzy match.
4We also carried out an alternative experiment with only
three fuzzy match features estimated from the training data
parse forest by Na??ve Bayes by observing all spans in the train-
ing data, accumulating counts Cs(U) and Cs(U,W) of nonter-
minals (or pairs of nonterminals) heading the same span s. The
first two features (one for each direction) are based on condi-
tional probabilities:
?(U |W) = ? log
(
?
s?spans Cs (U,W)
?
s?spans Cs(W)
)
. (4)
The third feature is based on joint probability:
?(U,W) = ? log
(
?
s?spans Cs(U,W)
?
s?spans,A,B?Lt Cs(A, B)
)
. (5)
The average performance drops by 0.1 (Ter-Bleu)/2 points,
compared to the interchange eperiment.
550
System
GALE-web P1R6-web MT08 news MT08 web Avg.
Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2
Single
T2S 32.6 11.6 16.9 23.5 37.7 7.8 28.1 14.5 14.4
Hiero 33.7 10.2 17.0 23.1 39.2 6.3 28.8 13.7 13.3
F2S 34.0 10.3 17.3 23.2 39.6 6.3 29.2 13.6 13.4
Joint
Liu:09 34.1 9.7 17.0 23.0 38.8 6.7 29.0 13.2 13.2
Gen. 34.4 9.7 17.8 22.6 40.0 6.1 29.6 13.1 12.9
Spe. 35.1 9.4 18.1 22.2 40.2 5.8 29.6 12.9 12.6
Int. 34.9 9.4 17.9 22.3 40.0 6.2 29.6 12.9 12.7
Table 2: All results of single and joint decoding systems.
6.1 Setup
The training corpus consists of 16 million sen-
tence pairs available within the DARPA BOLT
Chinese-English task. The corpus includes a mix
of newswire, broadcast news, webblog and comes
from various sources such as LDC, HK Law, HK
Hansard and UN data. The Chinese text is seg-
mented with a segmenter trained on CTB data using
conditional random fields (CRF). Language models
are trained on the English side of the parallel cor-
pus, and on monolingual corpora, such as Gigaword
(LDC2011T07) and Google News, altogether com-
prising around 10 billion words.
We use a modified version of the Berkeley parser
(Petrov and Klein, 2007) to obtain a parse forest
for each training sentence, then we prune it with
the marginal probability-based inside-outside algo-
rithm to contain only 3n CFG nodes, where n is the
sentence length. Finally, we apply the forest-based
GHKM algorithm (Mi and Huang, 2008; Galley et
al., 2004) to extract tree-to-string translation rules
from forest-string pairs.
In the decoding step, we prune the input hyper-
graphs to 10n nodes before we use fast pattern-
matching (Zhang et al, 2009) to convert the parse
forest into the translation forest.
We tune on 1275 sentences, each with 4 refer-
ences, from the LDC2010E30 corpus, initially re-
leased under the DARPA GALE program.
All MT experiments are optimized with
MIRA (Crammer et al, 2006) to maximize
(Ter-Bleu)/2.
We test on four different test sets: GALE-web test
set from LDC2010E30 corpus (1239 sentences, 4
references), P1R6-web test set from LDC2012E124
corpus (1124 sentences, 1 reference), NIST MT08
newswire portion (691 sentences, 4 references), and
NIST MT08 web portion (666 sentences, 4 refer-
ences).
6.2 Results
Table 2 shows all results of single and joint decoding
systems. The Bleu score of the single hiero baseline
is 39.2 on MT08-news, showing that it is a strong
system. The single F2S baseline achieves compara-
ble scores on all four test sets.
Then, for reference, we present results of joint Hi-
ero and T2S decoding, which is, to our knowledge, a
strong and competitive reimplementaion of the work
described by Liu et al (2009). Finally, we present re-
sults of joint decoding of hiero and F2S in three in-
teraction schemes: generalization, specification, and
interchange.
All three combination schemes significantly im-
prove results of any single system on all four test-
sets. On average and measured in (Ter-Bleu)/2,
our systems improve the best single system by 0.4
(generalization), 0.7 (specification), and 0.6 (inter-
change).
The specification comes out as the strongest inter-
action scheme, beating the second interchange on 2
testsets by 0.1 and 0.4 (Ter-Bleu)/2 points and on 3
testsets by 0.2 Bleu points.
6.3 Discussion of Results
Interpretations of model behavior with thousands of
parameters that may possibly overlap and interfere
should be always attempted with caution. In this sec-
tion we highlight some interesting observations, ac-
551
Specification Generalization Interchange
X ? ? ? ? X X ? ? ? ? X
VP
IP
VV
NR
ADVP
QP
CC
DVP
NP
P
...
CS
CP
AD
VRD
PU
ADJP
DNP
PP
PRN
DP
0.069
0.059
0.053
0.032
0.025
0.023
0.017
0.017
0.017
0.012
...
-0.005
-0.007
-0.011
-0.012
-0.028
-0.028
-0.045
-0.064
-0.069
-0.092
QP
PP
NN
DP
NR
DNP
NP
LC
DEC
DEG
...
VV
PRN
PN
BA
VP
VRD
JJ
VC
DFL
PU
0.057
0.054
0.048
0.044
0.034
0.032
0.030
0.025
0.023
0.023
...
-0.010
-0.011
-0.013
-0.015
-0.015
-0.028
-0.035
-0.037
-0.054
-0.073
VV
VP
NN
QP
ADVP
LCP
NP
P
IP
NR
...
VSB
PN
PU
M
VRD
DNP
ADJP
PP
DP
PRN
0.062
0.044
0.034
0.025
0.022
0.021
0.018
0.017
0.016
0.016
...
-0.004
-0.004
-0.004
-0.007
-0.014
-0.023
-0.039
-0.058
-0.070
-0.080
NN
PP
CP
LCP
DEG
DP
DEC
QP
LC
NP
...
FLR
DVP
BA
JJ
AS
VRD
ADVP
PN
DFL
PU
0.048
0.041
0.035
0.035
0.031
0.028
0.027
0.027
0.021
0.019
...
-0.006
-0.009
-0.010
-0.011
-0.014
-0.017
-0.021
-0.033
-0.038
-0.103
Table 3: Examples of specification, generalization, and interchange weights. POS tags in italics.
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 13
 14
 15
 16
 17
 18
AD
VP CL
P
AD
JP
FL
R
D
FL D
P
VC
P
VS
B
VR
D
VC
D QP NP DV
P
D
N
P
LC
P PP VP CP
PR
N IP
FR
AG
Av
er
ag
e 
sp
an
 le
ng
th
Figure 4: Average span length for selected syntactic la-
bels on GALE-web test set.
companying them with our subjective judgements
and speculations.
Table 3 shows the specification and generalization
features tuned for the three combination schemes,
then sorted by their weights ?X?Y or ?Y?X . Features
shown at the top of the table are very expensive (the
#Interactions Generalization Inter. gen.
F2S ? glue 5557 4202
F2S ? hiero 695 1178
total gen. 6252 5380
Specification Inter. spec.
phrase ? F2S 2763 2235
glue ? F2S 946 841
hiero ? F2S 683 839
total spec. 4392 3915
Table 5: Rule interactions on GALE-web test set.
system tries to avoid them), while inexpensive fea-
tures are at the bottom (the system is encouraged to
use them).
The most expensive interactions for the specifi-
cation belong to constituents (IP, VP) that usually
occur higher in a syntactic tree (see Figure 4 for av-
erage span lengths of selected syntactic labels), and
often require non-local reorderings. This indicates
that the decoder is discouraged from switching from
hiero into F2S derivation at these higher-level spans.
552
rule type Generalization Specification Interchange
F2S 18,807 58% 19,399 70% 18,400 61%
Hiero 3,730 12% 2,330 8% 3,133 10%
Glue 7,367 23% 571 2% 4,714 16%
Phrase 2,274 7% 5,484 20% 3,868 13%
total 32,178 27,784 30,115
Table 4: Rule counts on GALE-web test set.
10^0
10^1
10^2
10^3
10^4
 0  5  10  15  20  25  30  35  40  45  50  55  60  65  70
N
um
be
r o
f r
ul
es
Span length
Generalization
F2S
Hiero
Glue
Phrase
10^0
10^1
10^2
10^3
10^4
 0  5  10  15  20  25  30  35  40  45  50  55  60  65  70
N
um
be
r o
f r
ul
es
Span length
Specification
F2S
Hiero
Glue
Phrase
10^0
10^1
10^2
10^3
10^4
 0  5  10  15  20  25  30  35  40  45  50  55  60  65  70
N
um
be
r o
f r
ul
es
Span length
Interchange
F2S
Hiero
Glue
Phrase
Figure 5: Rule distributions on GALE-web test set.
The third most expensive feature belongs to a
part-of-speech tag?the preterminal VV. We may
hypothesize that it shows the importance of lexical
information for the precision of reordering typically
carried out within (parent) VP nodes, and/or the im-
portance of POS information for succesful disam-
biguation of word senses in translation. Ideally, the
system can use a VP rule with a lexicalized VV. Less
preferably, the VV part has to be translated by an-
other T2S rule (losing the lexical constraint). In the
worst case, the system has to use a hiero hypothe-
sis to translate the VV part (losing the syntactic con-
straint), risking imprecise translation, since the hiero
rule is not constrained to senses corresponding to the
source POS VV. Again, the high penalty discourages
from using the hiero derivation in this context.
On the other hand, the bottom of the table shows
labels that encourage using hiero?DP, PP, DNP,
ADJP, etc.?shorter phrases that tend to be monotone
and less ambiguous.
Similar interpretations seem plausible when ex-
amining the generalization experiment. Expensive
features related to preterminals (NR, NN, CD) may
suggest two alternative principles: First, using F2S
rules for thes POS categories and then switching to
hiero is discouraged, since these contexts are more
reliably handled by hiero due to better lexical cover-
age and common adjacency in nominal categories.
Second, since there is only one attempt to switch
from F2S derivation to hiero, letting F2S complete
even larger spans (and maybe switching to hiero
later) is favorable.
The tail of generalization feature weights is more
difficult to interpret. The discount on VP encourages
decoder to use F2S for entire verb phrases before
switching to hiero, on the other hand, other verb-
related preterminals occupy the tail as well, hurrying
into early switching from F2S to hiero.
553
Finally, the feature weights tuned for the in-
terchange experiment are divided into two sub-
columns. Both generalization and specification
weights show similar trends as in the previous two
interaction schemes, although blurred (VP and IP
descending from the absolute top). Since transitions
in both ways are allowed, the search space is big-
ger and the system may behave differently. It is even
possible for a path in the hypergraph to zigzag be-
tween F2S and hiero nodes to collect interaction dis-
counts, ?diluting? the syntactic homogeneity of the
hypothesis.
Figure 5 and Tables 4 and 5 show rule distribu-
tions, total rule counts, and numbers of interactions
of different types for the three interaction schemes
on the GALE-web test set. The scope of phrase rules
is limited to 6 words. The scope of hiero rules is lim-
ited to 20 words by the commonly used maxParse
parameter, leaving longer spans to the glue rule.
The trends of F2S and glue rules show the most
obvious difference. In the generalization, F2S rules
translate spans of up to 50 words. Glue rules pre-
vail on spans longer then 7 words. The specification
is reversed, pushing the longest scope of hiero and
glue rules down to 40 words, completing the longest
sentences entirely with F2S. The interchange comes
out as a mixture of the previous two trends.
All three schemes prefer using F2S rules at
shorter spans, to the contrary of our original assump-
tion of phrasal and hiero rules being stronger on lo-
cal contexts in general. Here we may refer again
to the specification feature weights for preterminals
VV, NR, CC and P in Table 3 and to our previously
stated hypothesis about the importance of preserving
lexical and syntactic context.
Hiero rules usage on longer spans drops fastest
for specification, slowest for generalization, and in
between for interchange.
It is also interesting to notice the trends on very
short spans (2?4 words) shown by rule distributions
and reflected in numbers of interaction types. While
specification often transitions from a single phrase
rule directly into F2S, the interchange has relatively
higher counts of hiero rules, another sign of the hiero
and F2S interaction.
Synthesizing from several sources of indications
is difficult, however, we arrive at the conclusion that
joint decoding of hiero and F2S significantly im-
proves the performance. While the single systems
show similar performance, their roles are not bal-
anced in joint decoding. It seems that the role of hi-
ero consists in enabling F2S in most contexts.
We have focused on three special cases of inter-
action. We see a great potential in further studies
of other schemes, allowing more flexible interaction
than simple specification, but still more constrained
than the interchange. It seems also promising to re-
fine the interaction modeling with features taking
into account more information than a single syntac-
tic label, and to explore additional ways of parame-
ter estimation.
7 Conclusion
We have proposed flexible interaction of hyper-
graphs as a novel technique combining hiero
and forest-to-string translation models within one
decoder. We have explored three basic interac-
tion schemes?specification, generalization, and
interchange?and described soft constraints control-
ling the interactions. We have carried out experi-
ments on large training data and with strong base-
lines. Of the three schemes, the specification shows
the highest gains, achieving improvements from 0.5
to 0.9 (Ter-Bleu)/2 points over the best single sys-
tem. We have conducted a detailed analysis of each
system output based on different indications of inter-
actions, discussed possible interpretations of results,
and finally offered our conclusion and proposed fu-
ture lines of research.
Acknowledgments
We thank Jir??? Havelka for proofreading and help-
ful suggestions. We would like to acknowledge the
support of DARPA under Grant HR0011-12-C-0015
for funding part of this work. The views, opinions,
and/or findings contained in this article/presentation
are those of the author/presenter and should not be
interpreted as representing the official views or poli-
cies, either expressed or implied, of the DARPA.
References
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of HLT-NAACL, pages 218?226.
554
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, June.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
John Denero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In In Proceedings NAACL-HLT, pages 975?983.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney, Aus-
tralia, July.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-HMM-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of EMNLP,
pages 98?107, October.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66?73.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL, pages 127?133.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, August.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT, pages
192?199.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404?411.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
ACL, pages 312?319, Prague, Czech Republic, June.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
translation system combination by confusion forest. In
Proceedings of ACL 2011, pages 1249?1257.
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim
Tan. 2009. Fast translation rule matching for syntax-
based statistical machine translation. In Proceedings
of EMNLP, pages 1037?1045, Singapore, August.
555
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 227?232,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Reordering Model for Forest-to-String Machine Translation
Martin
?
Cmejrek
IBM Watson Group
Prague, Czech Republic
martin.cmejrek@us.ibm.com
Abstract
In this paper, we present a novel exten-
sion of a forest-to-string machine transla-
tion system with a reordering model. We
predict reordering probabilities for every
pair of source words with a model using
features observed from the input parse for-
est. Our approach naturally deals with the
ambiguity present in the input parse forest,
but, at the same time, takes into account
only the parts of the input forest used
by the current translation hypothesis. The
method provides improvement from 0.6 up
to 1.0 point measured by (Ter ? Bleu)/2
metric.
1 Introduction
Various commonly adopted statistical machine
translation (SMT) approaches differ in the amount
of linguistic knowledge present in the rules they
employ.
Phrase-based (Koehn et al., 2003) models are
strong in lexical coverage in local contexts, and
use external models to score reordering op-
tions (Tillman, 2004; Koehn et al., 2005).
Hierarchical models (Chiang, 2005) use lexi-
calized synchronous context-free grammar rules
to produce local reorderings. The grammatical-
ity of their output can be improved by addi-
tional reordering models scoring permutations of
the source words. Reordering model can be either
used for source pre-ordering (Tromble and Eisner,
), integrated into decoding via translation rules ex-
tension (Hayashi et al., 2010), additional lexical
features (He et al., ), or using external sources of
information, such as source syntactic features ob-
served from a parse tree (Huang et al., 2013).
Tree-to-string (T2S) models (Liu et al., 2006;
Galley et al., 2006) use rules with syntactic struc-
tures, aiming at even more grammatically appro-
priate reorderings.
Forest-to-string (F2S) systems (Mi et al., 2008;
Mi and Huang, 2008) use source syntactic forest
as the input to overcome parsing errors, and to al-
leviate sparseness of translation rules.
The parse forest may often represent several
meanings for an ambiguous input that may need
to be transtated differently using different word or-
derings. The following example of an ambiguous
Chinese sentence with ambiguous part-of-speech
labeling motivates our interest in the reordering
model for the F2S translation.
S. t?aol`un (0) SSS. h`ui (1) SSS z?enmey`ang (2)
discussion/NN SS meeting/NN how/VV
discuss/VV SSSSSwill/VV
There are several possible meanings based on
the different POS tagging sequences. We present
translations for two of them, together with the in-
dices to their original source words:
(a) NN NN VV:
How
2
was
2
the
0
discussion
0
meeting
1
?
(b) VV VV VV:
Discuss
0
what
2
will
1
happen
1
.
A T2S system starts from a single parse corre-
sponding to one of the possible POS sequences,
the same tree can be used to predict word reorder-
ings. On the other hand, a F2S system deals with
the ambiguity through exploring translation hy-
potheses for all competing parses representing the
different meanings. As our example suggests, dif-
ferent meanings also tend to reorder differently
227
id rule
r
1
NP(t?aol`un/NN) ? discussion
r
2
NP(h`ui/NN) ? meeting
r
3
NP(x
1
:NP x
2
:NP) ? the x
1
x
2
r
4
IP(x
1
:NP z?enmey`ang/VV) ? how was x
1
r
5
IP(h`ui/VV z?enmey`ang/VV) ? what will happen
r
6
IP(t?aol`un/VV x
1
:IP) ? discuss x
1
Table 1: Tree-to-string translation rules (without
internal structures).
during translation. First, the reordering model suit-
able for F2S translation should allow for trans-
lation of all meanings present in the input. Sec-
ond, as the process of deriving a partial transla-
tion hypothesis rules out some of the meanings,
the reordering model should restrict itself to fea-
tures originating in the relevant parts of the input
forest. Our work presents a novel technique satis-
fying both these requirements, while leaving the
disambuiguation decision up to the model using
global features.
The paper is organized as follows: We briefly
overview the F2S and Hiero translation models in
Section 2, present the proposed forest reordering
model in Section 3, describe our experiment and
present results in Section 4.
2 Translation Models
Forest-to-string translation (Mi et al., 2008) is an
extension of the tree-to-string model (Liu et al.,
2006; Huang et al., 2006) allowing it to use a
packed parse forest as the input instead of a sin-
gle parse tree.
Figure 1 shows a tree-to-string translation
rule (Huang et al., 2006), which is a tuple
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the source-
side tree fragment, whose internal nodes are la-
beled by nonterminal symbols (like NP), and
whose frontier nodes are labeled by source-
language words (like ?z?enmey`ang?) or variables
from a finite set X = {x
1
, x
2
, . . .}; rhs(r) is
the target-side string expressed in target-language
words (like ?how was?) and variables; and ?(r) is
a mapping from X to nonterminals. Each variable
x
i
? X occurs exactly once in lhs(r) and exactly
once in rhs(r).
The Table 1 lists all rules necessary to derive
translations (a) and (b), with their internal struc-
ture removed for simplicity.
Typically, an F2S system translates in two steps
(shown in Figure 2): parsing and decoding. In the
IP
x
1
:NP VP
VV
z?enmey`ang
? how was x
1
Figure 1: Tree-to-string rule r
4
.
parsing step, the source language input is con-
verted into a parse forest (A). In the decoding step,
we first convert the parse forest into a translation
forest F
t
in (B) by using the fast pattern-matching
technique (Zhang et al., 2009). Then the decoder
uses dynamic programing with beam search and
cube pruning to find the approximation to the best
scoring derivation in the translation forest, and
outputs the target string.
3 Forest Reordering Model
In this section, we describe the process of ap-
plying the reordering model scores. We score
pairwise translation reorderings for every pair of
source words similarly as described by Huang et
al. (2013). In their approach, an external model of
ordering distributions of sibling constituent pairs
predicts the reordering of word pairs. Our ap-
proach deals with parse forests rather than with
single trees, thus we have to model the scores dif-
ferently. We model ordering distributions for ev-
ery pair of close relatives?nodes in the parse forest
that may occur together as frontier nodes of a sin-
gle matching rule. We further condition the distri-
bution on a third node?a common ancestor of the
node pair that corresponds to the root node of the
matching rule. This way our external model takes
into acount the syntactic context of the hypothe-
sis. For example, nodes NP
0, 1
and NP
1, 2
are close
relatives, NP
0, 2
and IP
0, 3
are their common ances-
tors; NP
0, 1
and VV
2, 3
are close relatives, IP
0, 3
is
their common ancestor; NP
0, 1
and VV
1, 2
are not
close relatives.
More formally, let us have an input sentence
(w
0
, ...,w
n
) and its translation hypothesis h. For
every i and j such that 0 ? i < j ? n we as-
sume that the translations of w
i
and w
j
are in the
hypothesis h either in the same or inverted order-
ing o
i j
? {Inorder,Reorder}, with a probability
P
order
(o
i j
|h). Conditioning on h signifies that the
probabilistic model takes the current hypothesis as
a parameter. The reordering score of the entire hy-
228
(A)
IP
0, 3
NP
0, 2
NP
0, 1
t?aol`un
VV
0, 1
NP
1, 2
h`ui
VV
1, 2
IP
1, 3
z?enmey`ang
VV
2, 3
R
t
?
(B)
e
4
e
6
e
3
e
1
t?aol`un
e
2
h`ui
e
5
z?enmey`ang
Figure 2: Parse and translation hypergraphs. (A) The parse forest of the example sentence. Solid hy-
peredges denote the best parse, dashed hyperedges denote the second best parse. Unary edges were col-
lapsed. (B) The corresponding translation forest F
t
after applying the tree-to-string translation rule set R
t
.
Each translation hyperedge (e.g. e
4
) has the same index as the corresponding rule (r
4
). The forest-to-
string system can produce the example translation (a) (solid derivation: r
1
, r
2
, r
3
, and r
4
) and (b) (dashed
derivation: r
5
, r
6
).
pothesis f
order
(h) is then computed as
f
order
=
?
0?i< j?n
? log P
order
(o
i j
= o
h
i j
| h), (1)
where o
h
i j
denotes the actual ordering used in h.
The score f
order
can be computed recursively by
dynamic programing during the decoding. As an
example, we show in Table 2 reordering probabil-
ities retrieved in decoding of our sample sentence.
(a) If h is a hypothesis formed by a single trans-
lation rule r with no frontier nonterminals, we
evaluate all word pairs w
i
and w
j
covered by h
such that i < j. For each such pair we find the
frontier nodes x and y matched by r such that
x spans exactly w
i
and y spans exactly w
j
. (In
this case, x and y match preterminal nodes, each
spanning one position). We also find the node z
matching the root of r. Then we directly use the
Equation 1 to compute the score using an exter-
nal model P
order
(o
i j
|xyz) to estimate the probabil-
ity of reordering the relative nodes. For example,
when applying rule r
5
, we use the ordering dis-
tribution P
order
(o
1,2
|VV
1, 2
,VV
2, 3
, IP
1, 3
) to score
reorderings of h`ui and z?enmey`ang.
(b) If h is a hypothesis formed by a T2S rule
with one or more frontier nonterminals, we eval-
uate all word pairs as follows: If both w
i
and w
j
are spanned by the same frontier nonterminal (e.g.,
t?aol`un and h`ui when applying the rule r
4
), the
score f
order
had been already computed for the un-
derlying subhypothesis, and therefore was already
included in the total score. Otherwise, we compute
the word pair ordering cost. We find the close rel-
atives x and y representing each w
i
and w
j
. If w
i
is matched by a terminal in r, we select x as the
node matching r and spanning exactly w
i
. If w
i
is
spanned by a frontier nonterminal in r (meaning
that it was translated in a subhypothesis), we select
x as the node matching that nonterminal. We pro-
ceed identically for w
j
and y. For example, when
applying the rule r
4
, the word z?enmey`ang will be
represented by the node VV
2, 3
, while t?aol`un and
h`ui will be represented by the node NP
0, 2
.
Note that the ordering o
h
i j
cannot be determined
in some cases, sometimes a source word does not
produce any translation, or the translation of one
word is entirely surrounded by the translations of
another word. A weight corresponding to the bi-
nary discount feature f
o
unknown
is added to the score
for each such case.
The external model P
order
(o
i j
|xyz) is imple-
mented as a maximum entropy model. Features
of the model are observed from paths connecting
node z with nodes x and y as follows: First, we
pick paths z ? x and z ? y. Let z
?
be the last node
shared by both paths (the closest common ances-
tor of x and y). Then we distinguish three types of
path: (1) The common prefix z ? z
?
(it may have
zero length), the left path z ? x, and the right path
z ? y. We observe the following features on each
path: the syntactic labels of the nodes, the produc-
tion rules, the spans of nodes, a list of stop words
immediately preceding and following the span of
the node. We merge the features observed from
different paths z ? x and z ? y. This approach
229
rule word pair order probability
a) how
2
was
2
the discussion
0
meeting
1
r
3
(t?aol`un,h`ui) Inorder P
order
(
o
0,1
|NP
0, 1
,NP
1, 2
,NP
0, 2
)
r
4
(t?aol`un,z?enmey`ang) Reorder P
order
(
o
0,2
|NP
0, 2
,VV
2, 3
, IP
0, 3
)
(h`ui,z?enmey`ang) Reorder P
order
(
o
1,2
|NP
0, 2
,VV
2, 3
, IP
0, 3
)
b) discuss
0
what
2
will
1
happen
1
r
5
(h`ui, z?enmey`ang) Reorder P
order
(
o
1,2
|VV
1, 2
,VV
2, 3
, IP
1, 3
)
r
6
(t?aol`un, h`ui) Inorder P
order
(
o
0,1
|VV
0, 1
, IP
1, 3
, IP
0, 3
)
(t?aol`un, z?enmey`ang Inorder P
order
(
o
0,2
|VV
0, 1
, IP
1, 3
, IP
0, 3
)
Table 2: Example of reordering scores computed for derivations (a) and (b).
ignores the internal structure of each rule
1
, relying
on frontier node annotation. On the other hand it
is still feasible to precompute the reordering prob-
abilities for all combinations of xyz.
4 Experiment
In this section we describe the setup of the exper-
iment, and present results. Finally, we propose fu-
ture directions of research.
4.1 Setup
Our baseline is a strong F2S system (
?
Cmejrek
et al., 2013) built on large data with the full set
of model features including rule translation prob-
abilities, general lexical and provenance transla-
tion probabilities, language model, and a vari-
ety of sparse features. We build it as follows.
The training corpus consists of 16 million sen-
tence pairs available within the DARPA BOLT
Chinese-English task. The corpus includes a mix
of newswire, broadcast news, webblog data com-
ing from various sources such as LDC, HK Law,
HK Hansard and UN data. The Chinese text is seg-
mented with a segmenter trained on CTB data us-
ing conditional random fields (CRF).
Bilingual word alignments are trained and com-
bined from two sources: GIZA (Och, 2003) and
maximum entropy word aligner (Ittycheriah and
Roukos, 2005).
Language models are trained on the English
side of the parallel corpus, and on monolingual
corpora, such as Gigaword (LDC2011T07) and
Google News, altogether comprising around 10
billion words.
We parse the Chinese part of the training data
with a modified version of the Berkeley parser
1
Only to some extent, the rule still has to match the input
forest, but the reordering model decides based on the sum of
paths observed between the root and frontier nodes.
(Petrov and Klein, 2007), then prune the ob-
tained parse forests for each training sentence with
the marginal probability-based inside-outside al-
gorithm to contain only 3n CFG nodes, where n is
the sentence length.
We extract tree-to-string translation rules from
forest-string sentence pairs using the forest-based
GHKM algorithm (Mi and Huang, 2008; Galley et
al., 2004).
In the decoding step, we use larger input
parse forests than in training, we prune them to
contain 10n nodes. Then we use fast pattern-
matching (Zhang et al., 2009) to convert the parse
forest into the translation forest.
The proposed reordering model is trained on
100, 000 automatically aligned forest-string sen-
tence pairs from the parallel training data. These
sentences provide 110M reordering events that are
used by megam (Daum?e III, 2004) to train the max-
imum entropy model.
The current implementation of the reordering
model requires offline preprocessing of the input
hypergraphs to precompute reordering probabili-
ties for applicable triples of nodes (x, y, z). Since
the number of levels in the syntactic trees in T2S
rules is limited to 4, we only need to consider such
triples, where z is up to 4 levels above x or y.
We tune on 1275 sentences, each with 4 refer-
ences, from the LDC2010E30 corpus, initially re-
leased under the DARPA GALE program.
We combine two evaluation metrics for tun-
ing and testing: Bleu (Papineni et al., 2002) and
Ter (Snover et al., 2006). Both the baseline and
the reordering experiments are optimized with
MIRA (Crammer et al., 2006) to maximize (Ter-
Bleu)/2.
We test on three different test sets: GALE
Web test set from LDC2010E30 corpus (1239
sentences, 4 references), NIST MT08 Newswire
230
System GALE Web MT08 Newswire MT08 Web
Ter?Bleu
2
Bleu Ter
Ter?Bleu
2
Bleu Ter
Ter?Bleu
2
Bleu Ter
F2S 8.8 36.1 53.7 5.6 40.6 51.8 12.0 31.3 55.3
+Reordering 8.2 36.4 52.7 4.8 41.7 50.5 11.0 31.7 53.7
? -0.6 +0.3 -1.0 -0.8 +1.1 -1.3 -1.0 +0.4 -1.6
Table 3: Results.
portion (691 sentences, 4 references), and NIST
MT08 Web portion (666 sentences, 4 references).
4.2 Results
Table 3 shows all results of the baseline and the
system extended with the forest reordering model.
The (Ter ? Bleu)/2 score of the baseline system
is 12.0 on MT08 Newswire, showing that it is a
strong baseline. The system with the proposed re-
ordering model significantly improves the base-
line by 0.6, 0.8, and 1.0 (Ter ? Bleu)/2 points on
GALE Web, MT08 Newswire, and MT08 Web.
The current approach relies on frontier node
annotations, ignoring to some extent the internal
structure of the T2S rules. As part of future re-
search, we would like to compare this approach
with the one that takes into accout the internal
structure as well.
5 Conclusion
We have presented a novel reordering model for
the forest-to-string MT system. The model deals
with the ambiguity of the input forests, but also
predicts specifically to the current parse followed
by the translation hypothesis. The reordering prob-
abilities can be precomputed by an offline pro-
cess, allowing for efficient scoring in runtime. The
method provides improvement from 0.6 up to 1.0
point measured by (Ter ? Bleu)/2 metrics.
Acknowledgments
We thank Ji?r?? Havelka for proofreading and help-
ful suggestions. We would like to acknowledge
the support of DARPA under Grant HR0011-12-
C-0015 for funding part of this work. The views,
opinions, and/or findings contained in this article
are those of the author and should not be inter-
preted as representing the official views or poli-
cies, either expressed or implied, of the DARPA.
References
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the ACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7.
Hal Daum?e III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available at
http://pub.hal3.name#daume04cg-bfgs, im-
plementation available at http://hal3.name/
megam/.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of the HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the COLING-ACL.
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh,
Kevin Duh, and Seiichi Yamamoto. 2010. Hi-
erarchical Phrase-based Machine Translation with
Word-based Reordering Model. In Proceedings of
the COLING.
Zhongjun He, Yao Meng, and Hao Yu. Maximum
entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of the
EMNLP.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the AMTA.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In Proceeedings of
the EMNLP.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for arabic-english ma-
chine translation. In Proceedings of the HLT and
EMNLP.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL.
231
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 iwslt speech translation evaluation. In
Proceedings of the IWSLT.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the AMTA.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. Proceed-
ings of the HLT-NAACL.
Roy Tromble and Jason Eisner. Learning linear order-
ing problems for better translation. In Proceedings
of the EMNLP.
Martin
?
Cmejrek, Haitao Mi, and Bowen Zhou. 2013.
Flexible and efficient hypergraph interactions for
joint hierarchical and forest-to-string decoding. In
Proceedings of the EMNLP.
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim
Tan. 2009. Fast translation rule matching for
syntax-based statistical machine translation. In Pro-
ceedings of EMNLP, pages 1037?1045, Singapore,
August.
232
