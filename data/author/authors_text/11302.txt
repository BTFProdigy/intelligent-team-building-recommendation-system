Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72?80,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Effective Use of Linguistic and Contextual Information
for Statistical Machine Translation
Libin Shen and Jinxi Xu and Bing Zhang and
Spyros Matsoukas and Ralph Weischedel
BBN Technologies
Cambridge, MA 02138, USA
{lshen,jxu,bzhang,smatsouk,weisched}@bbn.com
Abstract
Current methods of using lexical features
in machine translation have difficulty in
scaling up to realistic MT tasks due to
a prohibitively large number of parame-
ters involved. In this paper, we propose
methods of using new linguistic and con-
textual features that do not suffer from
this problem and apply them in a state-of-
the-art hierarchical MT system. The fea-
tures used in this work are non-terminal
labels, non-terminal length distribution,
source string context and source depen-
dency LM scores. The effectiveness of
our techniques is demonstrated by signif-
icant improvements over a strong base-
line. On Arabic-to-English translation,
improvements in lower-cased BLEU are
2.0 on NIST MT06 and 1.7 on MT08
newswire data on decoding output. On
Chinese-to-English translation, the im-
provements are 1.0 on MT06 and 0.8 on
MT08 newswire data.
1 Introduction
Linguistic and context features, especially sparse
lexical features, have been widely used in re-
cent machine translation (MT) research. Unfor-
tunately, existing methods of using such features
are not ideal for large-scale, practical translation
tasks.
In this paper, we will propose several prob-
abilistic models to effectively exploit linguistic
and contextual information for MT decoding, and
these new features do not suffer from the scalabil-
ity problem. Our new models are tested on NIST
MT06 and MT08 data, and they provide signifi-
cant improvement over a strong baseline system.
1.1 Previous Work
The ideas of using labels, length preference and
source side context in MT decoding were explored
previously. Broadly speaking, two approaches
were commonly used in existing work.
One is to use a stochastic gradient descent
(SGD) or Perceptron like online learning algo-
rithm to optimize the weights of these features
directly for MT (Shen et al, 2004; Liang et al,
2006; Tillmann and Zhang, 2006). This method is
very attractive, since it opens the door to rich lex-
ical features. However, in order to robustly opti-
mize the feature weights, one has to use a substan-
tially large development set, which results in sig-
nificantly slower tuning. Alternatively, one needs
to carefully select a development set that simulates
the test set to reduce the risk of over-fitting, which
however is not always realistic for practical use.
A remedy is to aggressively limit the feature
space, e.g. to syntactic labels or a small fraction
of the bi-lingual features available, as in (Chiang
et al, 2008; Chiang et al, 2009), but that reduces
the benefit of lexical features. A possible generic
solution is to cluster the lexical features in some
way. However, how to make it work on such a
large space of bi-lingual features is still an open
question.
The other approach is to estimate a single score
or likelihood of a translation with rich features,
for example, with the maximum entropy (Max-
Ent) method as in (Carpuat and Wu, 2007; Itty-
cheriah and Roukos, 2007; He et al, 2008). This
method avoids the over-fitting problem, at the ex-
pense of losing the benefit of discriminative train-
ing of rich features directly for MT. However, the
feature space problem still exists in these pub-
lished models.
He et al (2008) extended the WSD-like ap-
proached proposed in (Carpuat and Wu, 2007) to
hierarchical decoders. In (He et al, 2008), lexical
72
features were limited on each single side due to the
feature space problem. In order to further reduce
the complexity of MaxEnt training, they ?trained
a MaxEnt model for each ambiguous hierarchical
LHS? (left-hand side or source side) of translation
rules. Different target sides were treated as possi-
ble labels. Therefore, the sample sets of each indi-
vidual MaxEnt model were very small, while the
number of features could easily exceed the number
of samples. Furthermore, optimizing individual
MaxEnt models in this way does not lead to global
maximum. In addition, MaxEnt models trained on
small sets are unstable.
The MaxEnt model in (Ittycheriah and Roukos,
2007) was optimized globally, so that it could bet-
ter employ the distribution of the training data.
However, one has to filter the training data ac-
cording to the test data to get competitive perfor-
mance with this model 1. In addition, the filtering
method causes some practical issues. First, such
methods are not suitable for real MT tasks, espe-
cially for applications with streamed input, since
the model has to be retrained with each new input
sentence or document and training is slow. Fur-
thermore, the model is ill-posed. The translation
of a source sentence depends on other source sen-
tences in the same batch with which the MaxEnt
model is trained. If we add one more sentence to
the batch, translations of other sentences may be-
come different due to the change of the MaxEnt
model.
To sum up, the existing models of employing
rich bi-lingual lexical information in MT are im-
perfect. Many of them are not ideal for practical
translation tasks.
1.2 Our Approach
As for our approach, we mainly use simple proba-
bilistic models, i.e. Gaussian and n-gram models,
which are more robust and suitable for large-scale
training of real data, as manifested in state-of-the-
art systems of speech recognition. The unique
contribution of our work is to design effective and
efficient statistical models to capture useful lin-
guistic and context information for MT decoding.
Feature functions defined in this way are robust
and ideal for practical translation tasks.
1According to footnote 2 of (Ittycheriah and Roukos,
2007), test set adaptation by test set sampling of the train-
ing corpus showed an advantage of more than 2 BLEU points
over a general system trained on all data.
1.2.1 Features
In this paper, we will introduce four new linguistic
and contextual feature functions. Here, we first
provide a high-level description of these features.
Details of the features are discussed in Section 2.
The first feature is based on non-terminal labels,
i.e. POS tags of the head words of target non-
terminals in transfer rules. This feature reduces
the ambiguity of translation rules. The other bene-
fit is that POS tags help to weed out bad target side
tree structures, as an enhancement to the target de-
pendency language model.
The second feature is based on the length dis-
tribution of non-terminals. In English as well as
in other languages, the same deep structure can
be represented in different syntactic structures de-
pending on the complexity of its constituents. We
model such preferences by associating each non-
terminal of a transfer rule with a probability distri-
bution over its length. Similar ideas were explored
in (He et al, 2008). However their length features
only provided insignificant improvement of 0.1
BLEU point. A crucial difference of our approach
is how the length preference is modeled. We ap-
proximate the length distribution of non-terminals
with a smoothed Gaussian, which is more robust
and gives rise to much larger improvement consis-
tently.
The third feature utilizes source side context in-
formation, i.e. the neighboring words of an input
span, to influence the selection of the target trans-
lation for a span. While the use of context infor-
mation has been explored in MT, e.g. (Carpuat
and Wu, 2007) and (He et al, 2008), the specific
technique we used by means of a context language
model is rather different. Our model is trained on
the whole training data, and it is not limited by the
constraint of MaxEnt training.
The fourth feature exploits structural informa-
tion on the source side. Specifically, the decoder
simultaneously generates both the source and tar-
get side dependency trees, and employs two de-
pendency LMs, one for the source and the other
for the target, for scoring translation hypotheses.
Our intuition is that the likelihood of source struc-
tures provides another piece of evidence about the
plausibility of a translation hypothesis and as such
would help weed out bad ones.
73
1.2.2 Baseline System and Experimental
Setup
We take BBN?s HierDec, a string-to-dependency
decoder as described in (Shen et al, 2008), as our
baseline for the following two reasons:
? It provides a strong baseline, which ensures
the validity of the improvement we would ob-
tain. The baseline model used in this paper
showed state-of-the-art performance at NIST
2008 MT evaluation.
? The baseline algorithm can be easily ex-
tended to incorporate the features proposed
in this paper. The use of source dependency
structures is a natural extension of the string-
to-tree model to a tree-to-tree model.
To ensure the generality of our results, we tested
the features on two rather different language pairs,
Arabic-to-English and Chinese-to-English, using
two metrics, IBM BLEU (Papineni et al, 2001)
and TER (Snover et al, 2006). Our experiments
show that each of the first three features: non-
terminal labels, length distribution and source side
context, improves MT performance. Surprisingly,
the source dependency feature does not produce
an improvement.
2 Linguistic and Context Features
2.1 Non-terminal Labels
In the original string-to-dependency model (Shen
et al, 2008), a translation rule is composed of a
string of words and non-terminals on the source
side and a well-formed dependency structure on
the target side. A well-formed dependency struc-
ture could be either a single-rooted dependency
tree or a set of sibling trees. As in the Hiero system
(Chiang, 2007), there is only one non-terminal X
in the string-to-dependency model. Any sub de-
pendency structure can be used to replace a non-
terminal in a rule.
For example, we have a source sentence in Chi-
nese as follows.
? jiantao zhuyao baohan liang fangmian
The literal translation for individual words is
? ?review? ?mainly? ?to consist of? ?two? ?part?
The reference translation is
? the review mainly consists of two parts
A single source word can be translated into
many English words. For example, jiantao can
be translated into a review, the review, reviews,
the reviews, reviewing, reviewed, etc. Suppose
we have source-string-to-target-dependency trans-
lation rules as shown in Figure 1. Since there is
no constraint on substitution, any translation for
jiantao could replace the X-1 slot.
One way to alleviate this problem is to limit the
search space by using a label system. We could
assign a label to each non-terminal on the target
side of the rules. Furthermore, we could assign a
label to the whole target dependency structure, as
shown in Figure 2. In decoding, each target de-
pendency sub-structure would be associated with
a label. Whenever substitution happens, we would
check whether the label of the sub-structure and
the label of the slot are the same. Substitutions
with unmatched labels would be prohibited.
In practice, we use a soft constraint by penaliz-
ing substitutions with unmatched labels. We intro-
duce a new feature: the number of times substitu-
tions with unmatched labels appear in the deriva-
tion of a translation hypothesis.
Obviously, to implement this feature we need to
associate a label with each non-terminal in the tar-
get side of a translation rule. The labels are gen-
erated during rule extraction. When we create a
rule from a training example, we replace a sub-
tree or dependency structure with a non-terminal
and associate it with the POS tag of the head word
if the non-terminal corresponds to a single-rooted
tree on the target side. Otherwise, it is assigned
the generic label X. (In decoding, all substitutions
of X are considered unmatched ones and incur a
penalty.)
2.2 Length Distribution
In English, the length of a phrase may determine
the syntactic structure of a sentence. For example,
possessive relations can be represented either as
?A?s B? or ?B of A?. The former is preferred if A
is a short phrase (e.g. ?the boy?s mother?) while
the latter is preferred if A is a complex structure
(e.g. ?the mother of the boy who is sick?).
Our solution is to build a model of length prefer-
ence for each non-terminal in each translation rule.
To address data sparseness, we assume the length
distribution of each non-terminal in a transfer rule
is a Gaussian, whose mean and variance can be
estimated from the training data. In rule extrac-
74
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X X?1 baohan
X
X?1
consists
X?2
X?2
ofmainly
zhuyao
Figure 1: Translation rules with one label X
the
X jiantao
reviews
the
X jiantao
review
X X?1 baohan
consists
NNS VBZNN
NN?1 NNS?2
X?2
ofmainly
zhuyao
Figure 2: Translation rules with multiple labels
tion, each time a translation rule is generated from
a training example, we can record the length of the
source span corresponding to a non-terminal. In
the end, we have a frequency histogram for each
non-terminal in each translation rule. From the
histogram, a Gaussian distribution can be easily
computed.
In practice, we do not need to collect the fre-
quency histogram. Since all we need to know are
the mean and the variance, it is sufficient to col-
lect the sum of the length and the sum of squared
length.
Let r be a translation rule that occurs N
r
times
in training. Let x be a specific non-terminal in that
rule. Let l(r, x, i) denote the length of the source
span corresponding to non-terminal x in the i-th
occurrence of rule r in training. Then, we can
compute the following quantities.
m
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i) (1)
s
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i)
2
, (2)
which can be subsequently used to estimate the
mean ?
r,x
and variance ?2
r,x
of x?s length distri-
bution in rule r as follows.
?
r,x
= m
r,x
(3)
?
2
r,x
= s
r,x
?m
2
r,x
(4)
Since many of the translation rules have few oc-
currences in training, smoothing of the above esti-
mates is necessary. A common smoothing method
is based on maximum a posteriori (MAP) estima-
tion as in (Gauvain and Lee, 1994).
m?
r,x
=
N
r
N
r
+ ?
m
r,x
+
?
N
r
+ ?
m?
r,x
s?
r,x
=
N
r
N
r
+ ?
s
r,x
+
?
N
r
+ ?
s?
r,x
,
where ? stands for an MAP distribution and ? rep-
resents a prior distribution. m?
r,x
and s?
r,x
can
be obtained from a prior Gaussian distribution
N (??
r,x
, ??
r,x
) via equations (3) and (4), and ? is
a weight of smoothing.
There are many ways to approximate the prior
distribution. For example, we can have one prior
for all the non-terminals or one for individual non-
terminal type. In practice, we assume ??
r,x
= ?
r,x
,
and approximate ??
r,x
as (?2
r,x
+ s
r,x
)
1
2 .
In this way, we do not change the mean, but
relax the variance with s
r,x
. We tried differ-
ent smoothing methods, but the performance did
not change much, therefore we kept this simplest
setup. We also tried the Poisson distribution, and
the performance is similar to Gaussian distribu-
tion, which is about 0.1 point lower in BLEU.
When a rule r is applied during decoding, we
compute a penalty for each non-terminal x in r
according to
P (l | r, x) =
1
?
r,x
?
2pi
e
?
(l??
r,x
)
2
2?
2
r,x
,
where l is length of source span corresponding to
x.
Our method to address the problem of length
bias in rule selection is very different from the
maximum entropy method used in existing stud-
ies, e.g. (He et al, 2008).
75
2.3 Context Language Model
In the baseline string-to-dependency system, the
probability a translation rule is selected in decod-
ing does not depend on the sentence context. In
reality, translation is highly context dependent. To
address this defect, we introduce a new feature,
called context language model. The motivation of
this feature is to exploit surrounding words to in-
fluence the selection of the desired transfer rule for
a given input span.
To illustrate the problem, we use the same ex-
ample mentioned in Section 2.1. Suppose the
source span for rule selection is zhuyao baohan,
whose literal translation is mainly and to consist
of. There are many candidate translations for this
phrase, for example, mainly consist of, mainly
consists of, mainly including, mainly includes, etc.
The surrounding words can help to decide which
translation is more appropriate for zhuyao bao-
han. We compare the following two context-based
probabilities:
? P ( jiantao | mainly consist )
? P ( jiantao | mainly consists )
Here, jiantao is the source word preceding the
source span zhuyao baohan.
In the training data, jiantao is usually trans-
lated into the review, third-person singular, then
the probability P ( jiantao | mainly consists ) will
be higher than P ( jiantao | mainly consist ), since
we have seen more context events like the former
in the training data.
Now we introduce context LM formally. Let the
source words be f
1
f
2
..f
i
..f
j
..f
n
. Suppose source
sub-string f
i
..f
j
is translated into e
p
..e
q
. We can
define tri-gram probabilities on the left and right
sides of the source span:
? left : P
L
(f
i?1
|e
p
, e
p+1
)
? right : P
R
(f
j+1
|e
q
, e
q?1
)
In our implementation, the left and right context
LMs are estimated from the training data as part
of the rule extraction procedure. When we exact a
rule, we collect two 3-gram events, one for the left
side and the other for the right side.
In decoding, whenever a partial hypothesis is
generated, we calculate the context LM scores
based on the leftmost two words and the rightmost
two words of the hypothesis as well as the source
context. The product of the left and right context
LM scores is used as a new feature in the scoring
function.
Please note that our approach is very different
from other approaches to context dependent rule
selection such as (Ittycheriah and Roukos, 2007)
and (He et al, 2008). Instead of using a large num-
ber of fine grained features with weights optimized
using the maximum entropy method, we treat con-
text dependency as an ngram LM problem, and it
is smoothed with Witten-Bell discounting. The es-
timation of the context LMs is very efficient and
robust.
The benefit is two fold. The estimation of the
context LMs is very efficient. It adds only one new
weight to the scoring function.
2.4 Source Dependency Language Model
The context LM proposed in the previous sec-
tion only employs source words immediately be-
fore and after the current source span in decod-
ing. To exploit more source context, we use a
source side dependency language model as an-
other feature. The motivation is to take advantage
of the long distance dependency relations between
source words in scoring a translation theory.
We extended string-to-dependency rules in
the baseline system to dependency-to-dependency
rules. In each dependency-to-dependency rule, we
keep record of the source string as well as the
source dependency structure. Figure 3 shows ex-
amples of dependency-to-dependency rules.
We extended the string-to-dependency decod-
ing algorithm in the baseline to accommodate
dependency-to-dependency theories. In decoding,
we build both the source and the target depen-
dency structures simultaneously in chart parsing
over the source string. Thus, we can compute the
source dependency LM score in the same way we
compute the target side score, using a procedure
described in (Shen et al, 2008).
We introduce two new features for the source
side dependency LM as follows, in a way similar
to the target side.
? Source dependency LM score
? Discount on ill-formed source dependency
structures
The source dependency LM is trained on the
source side of the bi-lingual training data with
Witten-Bell smoothing. The source dependency
LM score represents the likelihood of the source
76
X?1 X?2zhuyao
baohan
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X
X X?1
consists
X?2ofmainly
Figure 3: Dependency-to-dependency translation rules
dependency tree generated by the decoder. The
source dependency tree with the highest score is
the one that is most likely to be generated by the
dependency model that created the source side of
the training data.
Source dependency trees are composed of frag-
ments embedded in the translation rules. There-
fore, a source dependency LM score can be
viewed as a measure whether the translation rules
are put together in a way similar to the training
data. Therefore, a source dependency LM score
serves as a feature to represent structural con-
text information that is capable of modeling long-
distance relations.
However, unlike source context LMs, the struc-
tural context information is used only when two
partial dependency structures are combined, while
source context LMs work as a look-ahead feature.
3 Experiments
We designed our experiments to show the impact
of each feature separately as well as their cumula-
tive impact:
? BASE: baseline string-to-dependency system
? SLM: baseline + source dependency LM
? CLM: baseline + context LM
? LEN: baseline + length distribution
? LBL: baseline + syntactic labels
? LBL+LEN: baseline + syntactic labels +
length distribution
? LBL+LEN+CLM: baseline + syntactic labels
+ length distribution + context LM
All the models were optimized on lower-cased
IBM BLEU with Powell?s method (Powell, 1964;
Brent, 1973) on n-best translations (Ostendorf et
al., 1991), but evaluated on both IBM BLEU and
TER. The motivation is to detect if an improve-
ment is artificial, i.e., specific to the tuning met-
ric. For both Arabic-to-English and Chinese-to-
English MT, we tuned on NIST MT02-05 and
tested on MT06 and MT08 newswire sets.
The training data are different from what was
usd at MT06 or MT08. Our Arabic-to-English
data contain 29M Arabic words and 38M En-
glish words from 11 corpora: LDC2004T17,
LDC2004T18, LDC2005E46, LDC2006E25,
LDC2006G05, LDC2005E85, LDC2006E36,
LDC2006E82, LDC2006E95, Sakhr-A2E and
Sakhr-E2A. The Chinese-to-English data contain
107M Chinese words and 132M English words
from eight corpora: LDC2002E18, LDC2005T06,
LDC2005T10, LDC2006E26, LDC2006G05,
LDC2002L27, LDC2005T34 and LDC2003E07.
They are available under the DARPA GALE
program. Traditional 3-gram and 5-gram string
LMs were trained on the English side of the
parallel data plus the English Gigaword corpus
V3.0 in a way described in (Bulyko et al, 2007).
The target dependency LMs were trained on the
English side of the parallel training data. For that
purpose, we parsed the English side of the parallel
data. Two separate models were trained: one for
Arabic from the Arabic training data and the other
for Chinese from the Chinese training data.
To compute the source dependency LM for
Chinese-to-English MT, we parsed the Chinese
side of the Chinese-to-English parallel data. Due
to the lack of a good Arabic parser compatible
with the Sakhr tokenization that we used on the
source side, we did not test the source dependency
LM for Arabic-to-English MT.
When extracting rules with source dependency
structures, we applied the same well-formedness
constraint on the source side as we did on the tar-
get side, using a procedure described by (Shen
et al, 2008). Some candidate rules were thrown
away due to the source side constraint. On the
77
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08
CLM 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92
LEN 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45
LBL 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57
LBL+LEN 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16
LBL+LEN+CLM 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80
Rescoring (5-gram LM)
BASE 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15
CLM 51.57 49.54 41.74 43.88 51.44 49.37 41.63 43.74
LEN 52.05 50.01 41.50 43.72 51.88 49.89 41.51 43.47
LBL 51.80 49.69 41.54 43.76 51.93 49.86 41.27 43.33
LBL+LEN 51.90 49.76 41.41 43.70 52.42 50.29 40.93 43.00
LBL+LEN+CLM 52.61 50.51 40.77 43.03 52.60 50.56 40.69 42.81
Table 1: BLEU and TER percentage scores on MT06 and MT08 Arabic-to-English newswire sets.
other hand, one string-to-dependency rule may
split into several dependency-to-dependency rules
due to different source dependency structures. The
size of the dependency-to-dependency rule set is
slightly smaller than the size of the string-to-
dependency rule set.
Tables 1 and 2 show the BLEU and TER per-
centage scores on MT06 and MT08 for Arabic-
to-English and Chinese-to-English translation re-
spectively. The context LM feature, the length
feature and the syntax label feature all produce
a small improvement for most of the conditions.
When we combined the three features, we ob-
served significant improvements over the baseline.
For Arabic-to-English MT, the LBL+LEN+CLM
system improved lower-cased BLEU by 2.0 on
MT06 and 1.7 on MT08 on decoding output.
For Chinese-to-English MT, the improvements in
lower-cased BLEU were 1.0 on MT06 and 0.8 on
MT08. After re-scoring, the improvements be-
came smaller, but still noticeable, ranging from 0.7
to 1.4. TER scores were also improved noticeably
for all conditions, suggesting there was no metric
specific over-tuning.
Surprisingly, source dependency LM did not
provide any improvement over the baseline. There
are two possible reasons for this. One is that
the source and target parse trees were generated
by two stand-alone parsers, which may cause in-
compatible structures on the source and target
sides. By applying the well-formed constraints
on both sides, a lot of useful transfer rules are
discarded. A bi-lingual parser, trained on paral-
lel treebanks recently made available to the NLP
community, may overcome this problem. The
other is that the search space of dependency-to-
dependency decoding is much larger, since we
need to add source dependency information into
the chart parsing states. We will explore tech-
niques to address these problems in the future.
4 Discussion
Linguistic information has been widely used in
SMT. For example, in (Wang et al, 2007), syntac-
tic structures were employed to reorder the source
language as a pre-processing step for phrase-based
decoding. In (Koehn and Hoang, 2007), shallow
syntactic analysis such as POS tagging and mor-
phological analysis were incorporated in a phrasal
decoder.
In ISI?s syntax-based system (Galley et al,
2006) and CMU?s Hiero extension (Venugopal et
al., 2007), non-terminals in translation rules have
labels, which must be respected by substitutions
during decoding. In (Post and Gildea, 2008; Shen
et al, 2008), target trees were employed to im-
prove the scoring of translation theories. Mar-
ton and Resnik (2008) introduced features defined
on constituent labels to improve the Hiero system
(Chiang, 2005). However, due to the limitation of
MER training, only part of the feature space could
used in the system. This problem was fixed by
78
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69
SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46
CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77
LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41
LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49
LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65
LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51
Rescoring (5-gram LM)
BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60
SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21
CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28
LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51
LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48
LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46
LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98
Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets.
Chiang et al (2008), which used an online learn-
ing method (Crammer and Singer, 2003) to handle
a large set of features.
Most SMT systems assume that translation
rules can be applied without paying attention to
the sentence context. A few studies (Carpuat and
Wu, 2007; Ittycheriah and Roukos, 2007; He et
al., 2008; Hasan et al, 2008) addressed this de-
fect by selecting the appropriate translation rules
for an input span based on its context in the in-
put sentence. The direct translation model in (It-
tycheriah and Roukos, 2007) employed syntactic
(POS tags) and context information (neighboring
words) within a maximum entropy model to pre-
dict the correct transfer rules. A similar technique
was applied by He et al (2008) to improve the Hi-
ero system.
Our model differs from previous work on the
way in which linguistic and contextual informa-
tion is used.
5 Conclusions and Future Work
In this paper, we proposed four new linguistic
and contextual features for hierarchical decoding.
The use of non-terminal labels, length distribution
and context LM features gave rise to significant
improvement on Arabic-to-English and Chinese-
to-English translation on NIST MT06 and MT08
newswire data over a state-of-the-art string-to-
dependency baseline. Unlike previous work, we
employed robust probabilistic models to capture
useful linguistic and contextual information. Our
methods are more suitable for practical translation
tasks.
In future, we will continue this work in two
directions. We will employ a Gaussian model
to unify various linguistic and contextual fea-
tures. We will also improve the dependency-to-
dependency method with a better bi-lingual parser.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
R. P. Brent. 1973. Algorithms for Minimization With-
out Derivatives. Prentice-Hall.
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
M. Carpuat and D. Wu. 2007. Context-dependent
phrasal translation lexicons for statistical machine
translation. In Proceedings of Machine Translation
Summit XI.
79
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proceedings of the 2008
Conference of Empirical Methods in Natural Lan-
guage Processing.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of the 43th Annual Meeting of the Association for
Computational Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic models.
In COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
J.-L. Gauvain and Chin-Hui Lee. 1994. Maximum a
posteriori estimation for multivariate gaussian mix-
tureobservations of markov chains. IEEE Transac-
tions on Speech and Audio Processing, 2(2).
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Proceedings of the 2008 Conference
of Empirical Methods in Natural Language Process-
ing.
Z. He, Q. Liu, and S. Lin. 2008. Improving statistical
machine translation using lexicalized rule selection.
In Proceedings of COLING ?08: The 22nd Int. Conf.
on Computational Linguistics.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In Proceedings of the 2007 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Conference of
Empirical Methods in Natural Language Process-
ing.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In COLING-ACL ?06: Proceed-
ings of 44th Annual Meeting of the Association for
Computational Linguistics and 21st Int. Conf. on
Computational Linguistics.
Y. Marton and P. Resnik. 2008. Soft syntactic con-
straints for hierarchical phrased-based translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language.
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
M. Post and D. Gildea. 2008. Parsers as language
models for statistical machine translation. In The
Eighth Conference of the Association for Machine
Translation in the Americas.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
7(2).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proceedings of
the 2004 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A New
String-to-Dependency Machine Translation Algo-
rithm with a Target Dependency Language Model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Ameri-
cas.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical mt. In
COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An efficient two-pass approach to synchronous-cfg
driven statistical mt. In Proceedings of the 2007 Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
80
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708?717,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Discriminative Corpus Weight Estimation for Machine Translation
Spyros Matsoukas and Antti-Veikko I. Rosti and Bing Zhang
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,arosti,bzhang}@bbn.com
Abstract
Current statistical machine translation
(SMT) systems are trained on sentence-
aligned and word-aligned parallel text col-
lected from various sources. Translation
model parameters are estimated from the
word alignments, and the quality of the
translations on a given test set depends
on the parameter estimates. There are
at least two factors affecting the parame-
ter estimation: domain match and training
data quality. This paper describes a novel
approach for automatically detecting and
down-weighing certain parts of the train-
ing corpus by assigning a weight to each
sentence in the training bitext so as to op-
timize a discriminative objective function
on a designated tuning set. This way, the
proposed method can limit the negative ef-
fects of low quality training data, and can
adapt the translation model to the domain
of interest. It is shown that such discrim-
inative corpus weights can provide sig-
nificant improvements in Arabic-English
translation on various conditions, using a
state-of-the-art SMT system.
1 Introduction
Statistical machine translation (SMT) systems rely
on a training corpus consisting of sentences in
the source language and their respective reference
translations to the target language. These paral-
lel sentences are used to perform automatic word
alignment, and extract translation rules with asso-
ciated probabilities. Typically, a parallel training
corpus is comprised of collections of varying qual-
ity and relevance to the translation problem of in-
terest. For example, an SMT system applied to
broadcast conversational data may be trained on
a corpus consisting mostly of United Nations and
newswire data, with only a very small amount of
in-domain broadcast news/conversational data. In
this case, it would be desirable to down-weigh the
out-of-domain data relative to the in-domain data
during the rule extraction and probability estima-
tion. Similarly, it would be good to assign a lower
weight to data of low quality (e.g., poorly aligned
or incorrectly translated sentences) relative to data
of high quality.
In this paper, we describe a novel discrimina-
tive training method that can be used to estimate a
weight for each sentence in the training bitext so as
to optimize an objective function ? expected trans-
lation edit rate (TER) (Snover et al, 2006) ? on a
held-out development set. The training bitext typ-
ically consists of millions of (parallel) sentences,
so in order to ensure robust estimation we express
each sentence weight as a function of sentence-
level features, and estimate the parameters of this
mapping function instead. Sentence-level fea-
tures may include the identifier of the collection or
genre that the sentence belongs to, the number of
tokens in the source or target side, alignment infor-
mation, etc. The mapping from features to weights
can be implemented via any differentiable func-
tion, but in our experiments we used a simple per-
ceptron. Sentence weights estimated in this fash-
ion are applied directly to the phrase and lexical
counts unlike any previously published method to
the author?s knowledge. The tuning framework is
developed for phrase-based SMT models, but the
tuned weights are also applicable to the training of
a hierarchical model. In cases where the tuning set
used for corpus weight estimation is a close match
to the test set, this method yields significant gains
in TER, BLEU (Papineni et al, 2002), and ME-
TEOR (Lavie and Agarwal, 2007) scores over a
state-of-the-art hierarchical baseline.
The paper is organized as follows. Related work
on data selection, data weighting, and model adap-
tation is presented in Section 2. The corpus weight
708
approach and estimation algorithm are described
in Section 3. Experimental evaluation of the ap-
proach is presented in Sections 4 and 5. Section 6
concludes the paper with a few directions for fu-
ture work.
2 Related Work
Previous work related to corpus weighting may
be split into three categories: data selection, data
weighting, and translation model adaptation. The
first two approaches may improve the quality
of the word alignment and prevent phrase-pairs
which are less useful for the domain to be learned.
The model adaptation, on the other hand, may
boost the weight of the more relevant phrase-
pairs or introduce translations for unseen source
phrases.
Resnik and Smith (2003) mined parallel text
from the web using various filters to identify likely
translations. The filtering may be viewed as a
data selection where poor quality translation are
discarded before word alignment. Yasuda et al
(2008) selected subsets of an existing parallel cor-
pus to match the domain of the test set. The dis-
carded sentence pairs may be valid translations
but they do not necessarily improve the translation
quality on the test domain. Mandal et al (2008)
used active learning to select suitable training data
for human translation. Hildebrand et al (2005) se-
lected comparable sentences from parallel corpora
using information retrieval techniques.
Lu et al (2007) proposed weighting compara-
ble portions of the parallel text before word align-
ment based on information retrieval. The relevant
portions of the parallel text were given a higher in-
teger weight in GIZA++ word alignment. Similar
effect may be achieved by replicating the relevant
subset in the training data.
Lu et al (2007) also proposed training adapted
translation models which were interpolated with a
model trained on the entire parallel text. Snover
et al (2008) used cross-lingual information re-
trieval to identify possible bias-rules to improve
the coverage on the source side. These rules may
cover source phrases for which no translations
were learned from the available parallel text.
Koehn and Schroeder (2007) described a pro-
cedure for domain adaptation that was using two
translation models in decoding, one trained on
in-domain data and the other on out-of-domain
data. Phrase translation scores from the two mod-
els where combined in a log-linear fashion, with
weights estimated based on minimum error rate
training (Och, 2003) on a designated tuning set.
The method described in this paper can also be
viewed as data filtering or (static) translation adap-
tation, but it has the following advantages over
previously published techniques:
1. The estimated corpus weights are discrim-
inative and are computed so as to directly
optimize an MT performance metric on a
pre-defined development set. Unlike the do-
main adaptation technique in (Koehn and
Schroeder, 2007), which also estimates the
adaptation parameters discriminatively, our
proposed method does not require a man-
ual specification of the in-domain and out-
of-domain training data collections. Instead,
it automatically determines which collections
are most relevant to the domain of interest,
and increases their weight while decreasing
the weight assigned to less relevant collec-
tions.
2. All sentences in the parallel corpus can in-
fluence the translation model, as opposed
to filtering/discarding data. However, the
proposed method can still assign very low
weights to parts of the corpus, if it determines
that it helps improve MT performance.
3. The framework used for estimating the cor-
pus weights can be easily extended to support
discriminative alignment link-level weights,
thus allowing the system to automatically
identify which portions of the training sen-
tences are most useful.
Naturally, as with any method, the proposed
technique has certain limitations. Specifically, it
is only concerned with influencing the translation
rule probabilities via the corpus weights; it does
not change the set of rules extracted. Thus, it is
unable to add new translation rules as in Snover
et al (2008). Also, it can potentially lead to pa-
rameter over-fitting, especially if the function that
maps sentence features to weights is complex and
based on a large number of parameters, or if the
development set used for estimating the mapping
function does not match the characteristics of the
test set.
709
3 Corpus Weights Estimation
3.1 Feature Extraction
The purpose of feature extraction is to identify,
for each sentence in the parallel training data, a
set of features that can be useful in estimating a
weight that is correlated with quality or relevance
to the MT task at hand. Starting from sentence-
aligned, word-aligned parallel training data, one
could extract various types of sentence-level fea-
tures. For example, we could specify features that
describe the two sides of the parallel data or the
alignment between them, such as collection id,
genre id, number of source tokens, number of tar-
get tokens, ratio of number of source and target
tokens, number of word alignment links, fraction
of source tokens that are unaligned, and fraction
of target tokens that are unaligned. Additionally,
we could include information retrieval (IR) related
features that reflect the relevance of a training sen-
tence to the domain of interest, e.g., by measur-
ing vector space model (VSM) distance of the sen-
tence to the current tuning set, or its log likelihhod
with respect to an in-domain language model.
Note that the collection and genre identifiers
(ids) mentioned above are bit vectors. Each col-
lection in the training set is mapped to a number.
A collection may consist of sentences from multi-
ple genres (e.g., newswire, web, broadcast news,
broadcast conversations). Genres are also mapped
to a unique number across the whole training set.
Then, given a sentence in the training bitext, we
can extract a binary vector that contains two non-
zero bits, one indicating the collection id, and an-
other denoting the genre id.
It is worth mentioning that in the experiments
reported later in this paper we made use of only the
collection and genre ids as features, although the
framework supports general sentence-level fea-
tures.
3.2 Mapping Features to Weights
As mentioned previously, one way to map a fea-
ture vector to a weight is to use a perceptron.
A multi-layer neural network may also be used,
but at the expense of slower training. In this
work, all of the experiments carried out made use
of a perceptron mapping function. However, it
is also possible to cluster the training sentences
into classes by training a Gaussian mixture model
(GMM) on their respective feature vectors1. Then,
given a feature vector we can compute the (poste-
rior) probability that it was generated by one of
the N Gaussians in the GMM, and use this N-
dimensional vector of posteriors as input to the
perceptron. This is similar to having a neural net-
work with a static hidden layer and Gaussian acti-
vation functions.
Given the many choices available in mapping
features to weights, we will describe the mapping
function in general terms. Let f
i
be the n ? 1
feature vector corresponding to sentence i. Let
?(x;?) denote a function Rn ? (0, 1) that is pa-
rameterized in terms of the parameter vector ? and
maps a feature vector x to a scalar weight in (0, 1).
The goal of the automatic corpus weight estima-
tion procedure is to estimate the parameter vector
? so as to optimize an objective function on a de-
velopment set.
3.3 Training with Weighted Corpora
Once the sentence features have been mapped to
weights, the translation rule extraction and prob-
ability estimation can proceed as usual, but with
weighted counts. For example, let w
i
= ?(f
i
;?)
be the weight assigned to sentence i. Let (s, t) be
a source-target phrase pair that can be extracted
from the corpus, and A(s) and B(t) indicating the
sets of sentences that s and t occur in. Then,
P (s|t) =
?
j?A(s)?B(t)
w
j
c
j
(s, t)
?
j?B(t)
w
j
c
j
(t)
(1)
where c
j
(?) denotes the number of occurrences of
the phrase (or phrase-pair) in sentence j.
3.4 Optimizing the Mapping Function
Estimation of the parameters ? of the mapping
function ? can be performed by directly optimiz-
ing a suitable objective function on a development
set. Ideally, we would like to estimate the param-
eters of the mapping function so as to directly op-
timize an automatic MT performance evaluation
metric, such as TER or BLEU on the full transla-
tion search space. However, this is extremely com-
putationally intensive for two reasons: (a) opti-
mizing in the full translation search space requires
a new decoding pass for each iteration of opti-
mization; and (b) a direct optimization of TER or
1Note that in order to train such a GMM it may be nec-
essary to first apply a decorrelating, dimensionality reducing,
transform (e.g., principal component analysis) to the features.
710
BLEU requires the use of a derivative free, slowly
converging optimization method such as MERT
(Och, 2003), because these objective functions are
not differentiable.
In our case, for every parameter vector update
we need to essentially retrain the translation model
(reestimate the phrase and lexical translation prob-
abilities based on the updated corpus weights), so
the cost of each iteration is significantly higher
than in a typical MERT application. For these rea-
sons, in this work we chose to minimize the ex-
pected TER over a translation N-best on a desig-
nated tuning set, which is a continuous and differ-
entiable function and can be optimized with stan-
dard gradient descent methods in a small number
of iterations. Note, that using expected TER is not
the only option here; any criterion that can be ex-
pressed as a continuous function of the phrase or
lexical translation probabilities can be used to op-
timize ?.
Given an N-best of translation hypotheses over
a development set of S sentences, we can define
the expected TER as follows
T =
?
S
s=1
?
N
s
j=1
p
sj

sj
?
S
s=1
r
s
(2)
where N
s
is the number of translation hypothe-
ses available for segment s; 
sj
is the minimum
raw edit distance between hypothesis j of seg-
ment s (or h
sj
, for short) and the reference transla-
tion(s) corresponding to segment s; r
s
is the aver-
age number of reference translation tokens in seg-
ment s, and p
sj
is the posterior probability of hy-
pothesis h
sj
in the N-best. The latter is computed
as follows
p
sj
=
e
?L
sj
?
N
s
k=1
e
?L
sk
(3)
where L
sj
is the total log likelihood of hypothe-
sis h
sj
, and ? is a tunable scaling factor that can
be used to change the dynamic range of the likeli-
hood scores and hence the distribution of posteri-
ors over the N-best. The hypothesis likelihood L
sj
is typically computed as a dot product of a decod-
ing weight vector and a vector of various ?feature?
scores, such as log phrase translation probability,
log lexical translation probability, log n-gram lan-
guage model probability, and number of tokens in
the hypothesis. However, in order to simplify this
presentation we will assume that it contains a sin-
gle translation model score, the log phrase transla-
tion probability of source given target. This score
is a sum of log conditional probabilities, similar
to the one defined in Equation 1. Therefore, L
sj
is indirectly a function of the training sentence
weights.
In order to minimize the expected TER T , we
need to compute the derivative of T with respect
to the mapping function parameters ?. Using the
chain rule, we get equations (4)-(8), where the
summation in Equation 6 is over all source-target
phrase pairs in the derivation of hypothesis h
sm
, ?
is the decoding weight assigned to the log phrase
translation score, and the summation in Equation
7 is over all training sentences2.
Thus, in order to compute the derivative of
the objective function we first need to calculate
? lnP (s
k
|t
k
)
??
for every phrase pair (s
k
, t
k
) in the
translation N-best based on Equations 7 and 8,
which requires time proportional to the number of
occurrences of these phrases in the parallel train-
ing data. After that, we can compute ?Lsm
??
for
each hypothesis h
sm
, based on Equation 6. Fi-
nally, we calculate ? ln psj
??
and ?T
??
based on Equa-
tions 5 and 4, respectively.
3.5 Implementation Issues
In our system, the corpus weights were trained
based on N-best translation hypotheses generated
by a phrase-based MT system on a designated tun-
ing set. Each translation hypothesis in the N-best
has a score that is a (linear) function of the fol-
lowing log translation probabilities: target phrase
given source phrase, source phrase given target
phrase, and lexical smoothing term. Additionally,
each hypothesis specifies information about its
derivation, i.e., which source-target phrase pairs it
consists of. Therefore, given an N-best, we can
identify the set of unique phrase pairs and use this
information in order to perform a filtered accumu-
lation of the statistics needed for calculating the
derivative in Equation 8. This reduces the storage
needed for the sufficient statistics significantly.
Minimization of the expected TER of the N-
best hypotheses was performed using the limited-
memory BFGS algorithm (Liu and Nocedal,
1989). Typically, the parameter vector ? required
about 30 iterations of LBFGS to converge.
Since the N-best provides only a limited repre-
sentation of the MT hypothesis search space, we
regenerated the N-best after every 30 iterations
2In the general case where L
sj
includes other translation
scores, e.g., lexical translation probabilities, the derivative
?L
sm
??
will have to include additional terms.
711
?T
??
=
S
?
s=1
N
s
?
j=1
?T
? ln p
sj
? ln p
sj
??
=
(
1
?
S
s=1
r
s
)
S
?
s=1
N
s
?
j=1
p
sj

sj
? ln p
sj
??
(4)
? ln p
sj
??
=
N
s
?
m=1
? ln p
sj
?L
sm
?L
sm
??
= ?
(
?L
sj
??
?
N
s
?
m=1
p
sm
?L
sm
??
)
(5)
?L
sm
??
=
?
(s
k
,t
k
)?h
sm
?L
sm
? lnP (s
k
|t
k
)
? lnP (s
k
|t
k
)
??
=
?
(s
k
,t
k
)?h
sm
?
? lnP (s
k
|t
k
)
??
(6)
? lnP (s
k
|t
k
)
??
=
?
i
? lnP (s
k
|t
k
)
?w
i
?w
i
??
(7)
? lnP (s
k
|t
k
)
?w
i
=
?
j?A(s
k
)?B(t
k
)
? (j ? i) c
j
(s
k
, t
k
)
?
j?A(s
k
)?B(t
k
)
w
j
c
j
(s
k
, t
k
)
?
?
j?B(t
k
)
? (j ? i) c
j
(t
k
)
?
j?B(t
k
)
w
j
c
j
(t
k
)
(8)
?(x) =
{
1 x = 0
0 x 6= 0
(9)
of LBFGS training, merging new hypotheses with
translations from previous iterations. The overall
training procedure is described in more detail be-
low:
1. Initialize parameter vector ? to small random
values, so that all training sentences receive
approximately equal weights.
2. Initialize phrase-based MT decoding weights
to previously tuned values.
3. Perform weighted phrase rule extraction as
described in Equation 1, to estimate the
phrase and lexical translation probabilities.
4. Decode the tuning set, generating N-best.
5. Merge N-best hypotheses from previous iter-
ations to current N-best.
6. Tune decoding weights so as to minimize
TER on merged N-best, using a derivative
free optimization method. In our case, we
used Powell?s algorithm (Powell, 1964) mod-
ified by Brent as described in (Brent, 1973) 3.
7. Identify set of unique source-target phrase
pairs in merged N-best.
8. Extract sufficient statistics from training data
for all phrases identified in step 7.
3This method was first used for N-best based parameter
optimization in (Ostendorf et al, 1991).
9. Run the LBFGS algorithm to minimize the
expected TER in the merged N-best, using
the derivative equations described previously.
10. Assign a weight to each training sentence
based on the ? values optimized in 9.
11. Go to step 3.
Typically, the corpus weights converge in about
4-5 main iterations. The calculation of the deriva-
tive is parallelized to speed up computation, re-
quiring about 10 minutes per iteration of LBFGS.
4 Experimental Setup
In this section we describe the setup that was used
for all experiments reported in this paper. Specif-
ically, we provide details about the training data,
development sets, and MT systems (phrase-based
and hierarchical).
4.1 Training Data
All MT training experiments made use of an
Arabic-English corpus of approximately 200 mil-
lion tokens (English side). Most of the collections
in this corpus are available through the Linguis-
tic Data Consortium (LDC) and are regularly part
of the resources specified for the constrained data
track of the NIST MT evaluation4.
4For a list of the NIST MT09 constrained train-
ing condition resources, see http://www.itl.
nist.gov/iad/mig/tests/mt/2009/MT09_
ConstrainedResources.pdf
712
The corpus includes data from multiple gen-
res, as shown in Table 1. The ?Sakhr? newswire
collection is a set of Arabic-to-English and
English-to-Arabic data provided by Sakhr Soft-
ware, totaling about 30.8 million tokens, and
is only available to research teams participat-
ing in the Defense Advanced Research Projects
Agency (DARPA) Global Autonomous Language
Exploitation (GALE) program. The ?LDC Giga-
word (ISI)? collection was produced by automati-
cally detecting and extracting portions of parallel
text from the monolingual LDC Arabic and En-
glish Gigaword collections, using a method devel-
oped at the Information Sciences Institute (ISI) of
the University of Southern California.
Data Origin Style Size(K tokens)
LDC pre-GALE
U. Nations 118049
Newswire 2700
Treebank 685
LDC post-GALE
Newswire 14344
Treebank 292
Web 478
Broad. News 573
Broad. Conv. 1003
Web-found text Lexicons 436Quran 406
Sakhr Newswire 30790
LDC Gigaword Newswire 29169(ISI)
Table 1: Composition of the Arabic-English par-
allel corpus used for MT training.
It is easy to see that most of the parallel train-
ing data are either newswire or from United Na-
tions. The amount of web text or broadcast
news/conversations is only a very small fraction
of the total corpus. In total, there are 31 collec-
tions in the training bitext. Some collections (es-
pecially those released recently by LDC for the
GALE project) consist of data from multiple gen-
res. The total number of unique genres (or data
types) in the training set is 10.
Besides the above bitext, we also used approxi-
mately 8 billion words of English text for language
model (LM) training (3.7B words from the LDC
Gigaword corpus, 3.3B words of web-downloaded
text, and 1.1B words of data from CNN archives).
This data was used to train two language mod-
els: an entropy-pruned trigram LM, used in decod-
ing, and an unpruned 5-gram LM used in N-best
rescoring. Kneser-Ney smoothing was applied to
the n-grams in both cases.
4.2 Development Sets
The development sets used for tuning and testing
the corpus weights and other MT settings were
comprised of documents from previous Arabic-
English NIST MT evaluation sets and from GALE
development/evaluation sets.
Specifically, the newswire Tune and Test sets
consist of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evaluation
sets, and the GALE P2 and P3 development sets.
The web Tune and Test sets are made of docu-
ments from NIST MT06 and MT08, the GALE P1
and P2 evaluation sets, the GALE P2 and P3 devel-
opment sets, and a held-out portion of the GALE
year 1 quarter 4 web training data release.
The audio Tune and Test sets consist of roughly
equal parts of news and conversations broadcast
from November 2005 through May 2007 by ma-
jor Arabic-speaking television and radio stations
(e.g., Al-Jazeera, Al-Arabiya, Syrian TV), totaling
approximately 14 hours of speech. The audio was
processed through automated speech recognition
(ASR) in order to produce (errorful) transcripts
that were used as input to all MT decoding experi-
ments reported in this paper. However, the corpus
weight estimation was carried out based on N-best
MT of the Arabic audio reference transcriptions
(i.e., the transcripts had no speech recognition er-
rors, and contained full punctuation).
It is important to note that some of the docu-
ments in the above devsets have multiple reference
translations (usually 4), while others have only
one. Most of the documents in the newswire sets
have 4 references, but unfortunately the web and
audio sets have, on average, less than 2 reference
translations per segment. More details are listed in
Table 2.
Another important note is that, although the au-
dio sets consist of both broadcast news (BN) and
broadcast conversations (BC), we did not perform
BN or BC-specific tuning. Corpus weights and
MT decoding parameters were optimized based on
a single Tune set, on a mix of BN and BC data.
However, when we report speech translation re-
sults in later sections, we break down the perfor-
713
Genre Tune Test#segs #tokens #refs/seg #segs #tokens #refs/seg
Newswire 1994 72359 3.94 3149 115700 3.67
Web 3278 99280 1.69 4425 125795 2.08
Audio BN 897 32990 1.00 1530 53067 1.00
Audio BC 765 24607 1.00 1416 44435 1.00
Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabic
newswire, web, and audio. The audio sets include material from both broadcast news and broadcast
conversations.
mance by genre.
4.3 MT Systems
Experiments were performed using two types of
statistical MT systems: a phrase-based system,
similar to Pharaoh (Koehn, 2004), and a state-
of-the-art, hierarchical string-to-dependency-tree
system, similar to (Shen et al, 2008).
The phrase-based MT system employs a pruned
3-gram LM in decoding, and can optionally gen-
erate N-best unique translation hypotheses which
are used to estimate the corpus weights, as de-
scribed in Section 3.
The hierarchical MT system performs decoding
with the same 3-gram LM, generates N-best of
unique translation hypotheses, and then rescores
them using a large, unpruned 5-gram LM in order
to select the best scoring translation. It is worth
mentioning that this hierarchical MT system pro-
vides a very strong baseline; it achieves a case-
sensitive BLEU score of 52.20 on the newswire
portion of the NIST MT08 evaluation set, which
is similar to the score of the second-best system
that participated in the unconstrained data track of
the NIST MT08 evaluation.
Both types of models were trained on the same
word alignments generated by GIZA++ (Och and
Ney, 2003).
5 Results
In this section we report results on the Arabic
newswire, web, and audio development sets, us-
ing both phrase-based and hierarchical MT sys-
tems, in terms of TER, BLEU5, and METEOR
(Lavie and Agarwal, 2007). Whenever corpus
weights are used, they were estimated on the des-
ignated Tune set using the phrase-based MT sys-
5The brevity penalty was calculated using the formula in
the original IBM paper, rather than the more recent definition
implemented in the NIST mteval-v11b.pl script.
tem. Only the collection and genre ids were used
as sentence features in order to estimate the corpus
weights. As mentioned in Section 4.1, the train-
ing bitext consists of 31 collections and 10 gen-
res, so each training sentence was assigned a 41-
dimensional binary vector indicating its particu-
lar collection/genre combination. That vector was
then mapped into a single weight using a percep-
tron.
5.1 Phrase-based MT
Results using the phrase-based MT system are
shown in Table 3. In all cases, the decoding
weights were optimized so as to minimize TER
on the designated Tune set. On newswire, the
discriminative corpus weights provide 0.8% abso-
lute gain in TER, in both Tune and Test sets. On
web, the TER gain is 0.9% absolute on Tune and
0.5% on Test. On the audio Test set, the TER gain
is 0.5% on BN and 1.4% on BC. Significant im-
provements were also obtained in the BLEU and
METEOR scores, on all sets and conditions.
5.2 Hierarchical MT
Results using the hierarchical MT system are
shown in Table 4. The hierarchical system
used different tuning criteria in each genre. On
newswire, the decoding weights were optimized
so as to maximize BLEU, while on web and audio
the tuning was based on 0.5TER+0.5(1?BLEU)
(referred to as TERBLEU in what follows). Note
that these were the criteria for tuning the decoding
weights; whenever corpus weights were used, they
were taken from the phrase-based system.
It is interesting to see that gains from discrimi-
native corpus weights carry over to the more pow-
erful hierarchical MT system. On newswire Test,
the gain in BLEU is 0.8; on web Test, the gain in
TERBLEU is 0.3. On the audio Test set, the cor-
pus weights provide 0.7 and 0.75 TERBLEU re-
duction on BN and BC, respectively. As with the
714
Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTR
Tune No 42.3 48.2 67.5 60.0 21.9 51.3Yes 41.5 49.6 68.7 59.1 22.8 52.3
Test No 43.2 46.2 66.5 58.6 24.2 52.2Yes 42.4 47.5 67.8 58.1 25.4 52.9
(a) Results on Arabic text.
Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTR
Tune No 56.0 22.9 55.5 57.3 21.7 55.0Yes 55.0 25.0 57.1 56.1 23.6 56.4
Test No 53.0 25.3 57.7 55.9 22.9 55.4Yes 52.5 26.6 58.8 54.5 24.7 56.8
(b) Results on Arabic audio.
Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets. Decoding
weights were optimized on the Tune set in order to directly minimize TER. Corpus weights were also
optimized on Tune set, but based on expected TER.
phrase-based system, all metrics improve from the
use of corpus weights, in all sets/conditions.
6 Conclusions
We have described a novel approach for estimat-
ing a weight for each sentence in a parallel train-
ing corpus so as to optimize MT performance of a
phrase-based statistical MT system. The sentence
weights influence MT performance by being ap-
plied to the phrase and lexical counts during trans-
lation rule extraction and probability estimation.
In order to ensure robust training of the weights,
we expressed them as a function of sentence-level
features. Then, we defined the process for opti-
mizing the parameters of that function based on
the expected TER of a translation hypothesis N-
best on a designated tuning set.
The proposed technique was evaluated in the
context of Arabic-English translation, on multiple
conditions. It was shown that encouraging results
were obtained by just using collection and genre
ids as features. Interestingly, the discriminative
corpus weights were found to be generally appli-
cable and provided gains in a state-of-the-art hi-
erarchical string-to-dependency-tree MT system,
even though they were trained using the phrase-
based MT system.
Next step is to include other sentence-level fea-
tures, as described in Section 3.1. Finally, the
technique described in this paper can be extended
to address the estimation of weights at the align-
ment link level, based on link-level features. We
believe that this will have a larger impact on the
lexical and phrase translation probabilities, since
there is a large number of parallel training sen-
tences that are partially correct, i.e., they contain
parts that are aligned and translated correctly, and
parts that are wrong. The current procedure tries
to assign a single weight to such sentences, so
there is no way to distinguish between the ?good?
and ?bad? portions of each sentence. Pushing the
weight estimation at the alignment link level will
alleviate this problem and will make the discrimi-
native training more targeted.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
Richard P. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
715
Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTR
Tune No 39.5 54.4 70.3 58.2 25.2 53.8Yes 38.8 55.6 71.2 58.0 25.5 54.0
Test No 40.7 52.1 69.3 57.0 28.3 54.7Yes 40.1 52.9 69.8 56.6 28.5 55.0
(a) Results on Arabic text.
Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTR
Tune No 54.9 27.3 58.0 55.8 26.1 57.4Yes 53.6 28.2 59.0 54.9 26.9 58.0
Test No 51.6 29.9 60.0 54.4 27.6 57.7Yes 50.7 30.4 60.7 53.2 27.9 58.7
(b) Results on Arabic audio.
Table 4: Hierarchical 5-gram rescoring results on the Arabic text and audio development sets. Decod-
ing/rescoring weights were optimized on the Tune set in order to directly maximize BLEU (for newswire)
or minimize TERBLEU (for web and audio). Corpus weights were the same as the ones used in the cor-
responding phrase-based decodings.
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
Annual Conference of European Association for Ma-
chine Translation, pages 133?142.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 224?227.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 343?350.
Arindam Mandal, Dimitra Vergyri, Wen Wang, Jing
Zheng, Andreas Stolcke, Gokhan Tur, Dilek
Hakkani-Tu?r, and Necip Fazil Ayan. 2008. Effi-
cient data selection for machine translation. In Pro-
ceedings of the Second IEEE/ACL Spoken Language
Technology Workshop, pages 261?264.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language, pages 83?87.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
pages 155?162.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
716
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577?585.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 857?866.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, volume II, pages 655?660.
717
Proceedings of the Third Workshop on Statistical Machine Translation, pages 183?186,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Incremental Hypothesis Alignment for Building Confusion Networks with
Application to Machine Translation System Combination
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
Confusion network decoding has been the
most successful approach in combining out-
puts from multiple machine translation (MT)
systems in the recent DARPA GALE and
NIST Open MT evaluations. Due to the vary-
ing word order between outputs from differ-
ent MT systems, the hypothesis alignment
presents the biggest challenge in confusion
network decoding. This paper describes an
incremental alignment method to build confu-
sion networks based on the translation edit rate
(TER) algorithm. This new algorithm yields
significant BLEU score improvements over
other recent alignment methods on the GALE
test sets and was used in BBN?s submission to
the WMT08 shared translation task.
1 Introduction
Confusion network decoding has been applied in
combining outputs from multiple machine transla-
tion systems. The earliest approach in (Bangalore
et al, 2001) used edit distance based multiple string
alignment (MSA) (Durbin et al, 1988) to build the
confusion networks. The recent approaches used
pair-wise alignment algorithms based on symmetric
alignments from a HMM alignment model (Matusov
et al, 2006) or edit distance alignments allowing
shifts (Rosti et al, 2007). The alignment method
described in this paper extends the latter by incre-
mentally aligning the hypotheses as in MSA but also
allowing shifts as in the TER alignment.
The confusion networks are built around a ?skele-
ton? hypothesis. The skeleton hypothesis defines
the word order of the decoding output. Usually, the
1-best hypotheses from each system are considered
as possible skeletons. Using the pair-wise hypoth-
esis alignment, the confusion networks are built in
two steps. First, all hypotheses are aligned against
the skeleton independently. Second, the confusion
networks are created from the union of these align-
ments. The incremental hypothesis alignment algo-
rithm combines these two steps. All words from the
previously aligned hypotheses are available, even if
not present in the skeleton hypothesis, when align-
ing the following hypotheses. As in (Rosti et al,
2007), confusion networks built around all skeletons
are joined into a lattice which is expanded and re-
scored with language models. System weights and
language model weights are tuned to optimize the
quality of the decoding output on a development set.
This paper is organized as follows. The incre-
mental TER alignment algorithm is described in
Section 2. Experimental evaluation comparing the
incremental and pair-wise alignment methods are
presented in Section 3 along with results on the
WMT08 Europarl test sets. Conclusions and future
work are presented in Section 4.
2 Incremental TER Alignment
The incremental hypothesis alignment is based on
an extension of the TER algorithm (Snover et al,
2006). The extension allows using a confusion net-
work as the reference. First, the algorithm finds the
minimum edit distance between the hypothesis and
the reference network by considering all word arcs
between two consecutive nodes in the reference net-
work as possible matches for a hypothesis word at
183
1 2 3 4 5 6I (3)
NULL (2)
like (3)
NULL (2)
big blue (1)
balloons (2)
blue (1) kites (1)
Figure 1: Network after pair-wise TER alignment.
that position. Second, shifts of blocks of words that
have an exact match somewhere else in the network
are tried in order to find a new hypothesis word or-
der with a lower TER. Each shifted block is con-
sidered a single edit. These two steps are executed
iteratively as a greedy search. The final alignment
between the re-ordered hypothesis and the reference
network may include matches, substitutions, dele-
tions, and insertions.
The confusion networks are built by creating a
simple confusion network from the skeleton hypoth-
esis. If the skeleton hypothesis has   words, the
initial network has   arcs and   nodes. Each
arc has a set of system specific confidence scores.
The score for the skeleton system is set to  and
the confidences for other systems are set to zeros.
For each non-skeleton hypothesis, a TER alignment
against the current network is executed as described
above. Each match found will increase the system
specific word arc confidence by 
	 where 
is the rank of the hypothesis in that system?s   -best
list. Each substitution will generate a new word arc
at the corresponding position in the network. The
word arc confidence for the system is set to 
	
and the confidences for other systems are set to ze-
ros. Each deletion will generate a new NULL word
arc unless one exists at the corresponding position
in the network. The NULL word arc confidences are
adjusted as in the case of a match or a substitution
depending on whether the NULL word arc exists or
not. Finally, each insertion will generate a new node
and two word arcs at the corresponding position in
the network. The first word arc will have the in-
serted word with the confidence set as in the case
of a substitution and the second word arc will have
a NULL word with confidences set by assuming all
previously aligned hypotheses and the skeleton gen-
erated the NULL word arc.
After all hypotheses have been added into the con-
fusion network, the system specific word arc confi-
dences are scaled to sum to one over all arcs between
1 2 3 4 5 6I (3) like (3)
kites (1)
NULL (2) NULL (1)
big (1) blue (2)
balloons (2)
Figure 2: Network after incremental TER alignment.
each set of two consecutive nodes. Other scores for
the word arc are set as in (Rosti et al, 2007).
2.1 Benefits over Pair-Wise TER Alignment
The incremental hypothesis alignment guarantees
that insertions between a hypothesis and the cur-
rent confusion network are always considered when
aligning the following hypotheses. This is not the
case in any pair-wise hypothesis alignment algo-
rithm. During the pair-wise hypothesis alignment,
an identical word in two hypotheses may be aligned
as an insertion or a substitution in a different posi-
tion with respect to the skeleton. This will result in
undesirable repetition and lower confidence for that
word in the final confusion network. Also, multiple
insertions are not handled implicitly.
For example, three hypotheses ?I like balloons?,
?I like big blue balloons?, and ?I like blue kites?
might be aligned by the pair-wise alignment, assum-
ing the first as the skeleton, as follows:
I like NULL balloons NULL
I like big blue balloons NULL
I like NULL balloons NULL
I like NULL blue kites
which results in the confusion network shown in
Figure 1. The number of hypotheses proposing each
word is shown in parentheses. The alignment be-
tween the skeleton and the second hypothesis has
two consecutive insertions ?big blue? which are not
available for matching when the third hypothesis is
aligned against the skeleton. Therefore, the word
?blue? appears twice in the confusion network. If
many hypotheses have multiple insertions at the
same location with respect to the skeleton, they have
to be treated as phrases or a secondary alignment
process has to be applied.
Assuming the same hypotheses as above, the in-
cremental hypothesis alignment may yield the fol-
lowing alignment:
184
System TER BLEU MTR
worst 53.26 33.00 63.15
best 42.30 48.52 67.71
syscomb pw 39.85 52.00 68.73
syscomb giza 40.01 52.24 68.68
syscomb inc 39.25 52.73 68.97
oracle 21.68 64.14 78.18
Table 1: Results on the Arabic GALE Phase 2 system
combination tuning set with four reference translations.
I like NULL NULL balloons
I like big blue balloons
I like NULL blue kites
which results in the confusion network shown in
Figure 2. In this case the word ?blue? is available
for matching when the third hypothesis is aligned.
It should be noted that the final confusion network
depends on the order in which the hypotheses are
added. The experiments so far have indicated that
different alignment order does not have a significant
influence on the final combination results as mea-
sured by the automatic evaluation metrics. Usually,
aligning the system outputs in the decreasing order
of their TER scores on the development set yields
the best scores.
2.2 Confusion Network Oracle
The extended TER algorithm can also be used to
estimate an oracle TER in a confusion network by
aligning the reference translations against the con-
fusion network. The oracle hypotheses can be ex-
tracted by finding a path with the maximum number
of matches. These hypotheses give a lower bound
on the TER score for the hypotheses which can be
generated from the confusion networks.
3 Experimental Evaluation
The quality of the final combination output depends
on many factors. Combining very similar outputs
does not yield as good gains as combining out-
puts from diverse systems. It is also important that
the development set used to tune the combination
weights is as similar to the evaluation set as possi-
ble. This development set should be different from
the one used to tune the individual systems to avoid
bias toward any system that may be over-tuned. Due
System TER BLEU MTR
worst 59.09 20.74 57.24
best 48.18 31.46 62.61
syscomb pw 46.31 33.02 63.18
syscomb giza 46.03 33.39 63.21
syscomb inc 45.45 33.90 63.45
oracle 27.53 49.10 71.81
Table 2: Results on the Arabic GALE Phase 2 evaluation
set with one reference translation.
to the tight schedule for the WMT08, there was no
time to experiment with many configurations. As
more extensive experiments have been conducted in
the context of the DARPA GALE program, results
on the Arabic GALE Phase 2 evaluation setup are
first presented. The translation quality is measured
by three MT evaluation metrics: TER (Snover et al,
2006), BLEU (Papineni et al, 2002), and METEOR
(Lavie and Agarwal, 2007).
3.1 Results on Arabic GALE Outputs
For the Arabic GALE Phase 2 evaluation, nine sys-
tems were combined. Five systems were phrase-
based, two hierarchical, one syntax-based, and one
rule-based. All statistical systems were trained on
common parallel data, tuned on a common genre
specific development set, and a common English to-
kenization was used. The English bi-gram and 5-
gram language models used in the system combina-
tion were trained on about 7 billion words of English
text. Three iterations of bi-gram decoding weight
tuning were performed followed by one iteration of
5-gram re-scoring weight tuning. All weights were
tuned to minimize the sum of TER and 1-BLEU.
The final 1-best outputs were true-cased and deto-
kenized before scoring.
The results on the newswire system combination
development set and the GALE Phase 2 evaluation
set are shown in Tables 1 and 2. The first two
rows show the worst and best scores from the in-
dividual systems. The scores may be from different
systems as the best performing system in terms of
TER was not necessarily the best performing system
in terms of the other metrics. The following three
rows show the scores of three combination outputs
where the only difference was the hypothesis align-
ment method. The first, syscomb pw, corresponds
185
BLEU
System de-en fr-en
worst 11.84 16.31
best 28.30 33.13
syscomb 29.05 33.63
Table 3: NIST BLEU scores on the German-English (de-
en) and French-English (fr-en) Europarl test2008 set.
to the pair-wise TER alignment described in (Rosti
et al, 2007). The second, syscomb giza, cor-
responds to the pair-wise symmetric HMM align-
ments from GIZA++ described in (Matusov et al,
2006). The third, syscomb inc, corresponds to
the incremental TER alignment presented in this pa-
per. Finally, oracle corresponds to an estimate of
the lower bound on the translation quality obtained
by extracting the TER oracle output from the con-
fusion networks generated by the incremental TER
alignment. It is unlikely that there exists a set of
weights that would yield the oracle output after de-
coding, though. The incremental TER alignment
yields significant improvements over all individual
systems and the combination outputs using the pair-
wise alignment methods.
3.2 Results on WMT08 Europarl Outputs
On the WMT08 shared translation task, transla-
tions for two language pairs and two tasks were
provided for the system combination experiments.
Twelve systems participated in the German-English
and fourteen in the French-English translation tasks.
The translations of the Europarl test (test2008) were
provided as the development set outputs and the
translations of the News test (newstest2008) were
provided as the evaluation set outputs. An English
bi-gram, 4-gram, and true-caser language models
were trained by using all English text available for
the WMT08 shared task, including Europarl mono-
lingual and news commentary parallel training sets.
The outputs were tokenized and lower-cased before
combination, and the final combination output was
true-cased and detokenized.
The results on the Europarl test set for both lan-
guage pairs are shown in table 3. The first two rows
have the NIST BLEU scores of the worst and the
best individual systems. The last row, syscomb,
corresponds to the system combination using the in-
cremental TER alignment. The improvements in the
NIST BLEU scores are fairly modest which is prob-
ably due to low diversity of the system outputs. It is
also unlikely that these weights are optimal for the
out-of-domain News test set outputs.
4 Conclusions
This paper describes a novel hypothesis alignment
algorithm for building confusion networks from
multiple machine translation system outputs. The al-
gorithm yields significant improvements on the Ara-
bic GALE evaluation set outputs and was used in
BBN?s submission to the WMT08 shared translation
task. The hypothesis alignment may benefit from
using stemming and synonymy in matching words.
Also, special handling of punctuation may improve
the alignment further. The future work will inves-
tigate the influence of better alignment to the final
combination outputs.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program.
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proc. ASRU, pages 351?354.
R. Durbin, S.R. Eddy, A. Krogh, and G. Mitchison. 1988.
Biological Sequence Analysis: Probabilistic Models of
Proteins and Nucleic Acids. Cambridge Univ. Press.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for MT evaluation with high levels of cor-
relation with human judgments. In Proc. ACL/WMT,
pages 228?231.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proc. EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311?318.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proc. ACL 2007, pages 312?319.
M. Snover, B. Dorr, R. Schwartz, L. Micciula, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA, pages
223?231.
186
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 61?65,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Incremental Hypothesis Alignment with Flexible Matching for Building
Confusion Networks: BBN System Description for WMT09 System
Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
This paper describes the incremental hy-
pothesis alignment algorithm used in the
BBN submissions to the WMT09 system
combination task. The alignment algo-
rithm used a sentence specific alignment
order, flexible matching, and new shift
heuristics. These refinements yield more
compact confusion networks compared to
using the pair-wise or incremental TER
alignment algorithms. This should reduce
the number of spurious insertions in the
system combination output and the sys-
tem combination weight tuning converges
faster. System combination experiments
on the WMT09 test sets from five source
languages to English are presented. The
best BLEU scores were achieved by comb-
ing the English outputs of three systems
from all five source languages.
1 Introduction
Machine translation (MT) systems have different
strengths and weaknesses which can be exploited
by system combination methods resulting in an
output with a better performance than any indi-
vidual MT system output as measured by auto-
matic evaluation metrics. Confusion network de-
coding has become the most popular approach to
MT system combination. The first confusion net-
work decoding method (Bangalore et al, 2001)
was based on multiple string alignment (MSA)
(Durbin et al, 1988) borrowed from biological
sequence analysis. However, MSA does not al-
low re-ordering. The translation edit rate (TER)
(Snover et al, 2006) produces an alignment be-
tween two strings and allows shifts of blocks of
words. The availability of the TER software has
made it easy to build a high performance system
combination baseline (Rosti et al, 2007).
The pair-wise TER alignment originally de-
scribed by Sim et al (2007) has various limita-
tions. First, the hypotheses are aligned indepen-
dently against the skeleton which determines the
word order of the output. The same word from
two different hypotheses may be inserted in differ-
ent positions w.r.t. the skeleton and multiple inser-
tions require special handling. Rosti et al (2008)
described an incremental TER alignment to miti-
gate these problems. The incremental TER align-
ment used a global order in which the hypotheses
were aligned. Second, the TER software matches
words with identical surface strings. The pair-
wise alignment methods proposed by Ayan et al
(2008), He et al (2008), and Matusov et al (2006)
are able to match also synonyms and words with
identical stems. Third, the TER software uses a set
of heuristics which is not always optimal in de-
termining the block shifts. Karakos et al (2008)
proposed using inversion transduction grammars
to produce different pair-wise alignments.
This paper is organized as follows. A refined
incremental alignment algorithm is described in
Section 2. Experimental evaluation comparing
the pair-wise and incremental TER alignment al-
gorithms with the refined alignment algorithm on
WMT09 system combination task is presented in
Section 3. Conclusions and future work are pre-
sented in Section 4.
2 Incremental Hypothesis Alignment
with Flexible Matching
2.1 Sentence Specific Alignment Order
Rosti et al (2008) proposed incremental hypothe-
sis alignment using a system specific order. This
is not likely to be optimal since one MT system
may have better output on one sentence and worse
on another. More principled approach is similar to
MSA where the order is determined by the edit
distance of the hypothesis from the network for
61
17
0
1NULL(6.2e-7)
9
NULL(0.9999)
2cereal
NULL
3
thomas
4
jefferson
edison
5
says
6
eat
7
your
8
NULL
vegetables
NULL
10
eat
11
your 12cereal
NULL
13
thomas 14
edison
jefferson
15
says 16vegetables
NULL
NULL
(a) Alignment using the standard TER shift heuristics.
150
1NULL(0.5)
8
NULL(0.5)
2
thomas
3jefferson
edison
4
says
5
eat
6
your
7
vegetables
cereal NULL
9
eat
10
your
11
cereal
vegetables
12
thomas 13edison
jefferson
14
says
NULL
(b) Alignment using the modified shift heuristics.
Figure 1: Combined confusion networks using different shift heuristics. The initial NULL arcs include
the prior probability estimates in parentheses.
each sentence. The TER scores of the remaining
unaligned hypotheses using the current network as
the reference are computed. The hypothesis with
the lowest edit cost w.r.t. the network is aligned.
Given  systems, this increases the number of
alignments performed from  to 	
 .
2.2 Flexible Matching
The TER software assigns a zero cost for match-
ing tokens and a cost of one for all errors includ-
ing insertions, deletions, substitutions, and block
shifts. Ayan et al (2008) modified the TER soft-
ware to consider substitutions of synonyms with
a reduced cost. Recently, Snover et al (2009)
extended the TER algorithm in a similar fashion
to produce a new evaluation metric, TER plus
(TERp), which allows tuning of the edit costs in
order to maximize correlation with human judg-
ment. The incremental alignment with flexible
matching uses WordNet (Fellbaum, 1998) to find
all possible synonyms and words with identical
stems in a set of hypotheses. Substitutions involv-
ing synonyms and words with identical stems are
considered with a reduced cost of 0.2.
2.3 Modified Shift Heuristics
The TER is computed by trying shifts of blocks of
words that have an exact match somewhere else in
the reference in order to find a re-ordering of the
hypothesis with a lower edit distance to the refer-
ence. Karakos et al (2008) showed that the shift
heuristics in TER do not always yield an optimal
alignment. Their example used the following two
hypotheses:
1. thomas jefferson says eat your vegetables
2. eat your cereal thomas edison says
A system combination lattice using TER align-
ment is shown in Figure 1(a). The blocks
?eat your? are shifted when building both con-
fusion networks. Using the second hypothe-
sis as the skeleton seems to give a better align-
ment. The lower number of edits also results in a
higher skeleton prior shown between nodes 0 and
9. There are obviously some undesirable paths
through the lattice but it is likely that a language
model will give a higher score to the reasonable
hypotheses.
Since the flexible matching allows substitutions
with a reduced cost, the standard TER shift heuris-
tics have to be modified. A block of words may
have some words with identical matches and other
words with synonym matches. In TERp, synonym
and stem matches are considered as exact matches
for the block shifts, otherwise the TER shift con-
straints are used. In the flexible matching, the shift
heuristics were modified to allow any block shifts
62
that do not increase the edit cost. A system combi-
nation lattice using the modified shift heuristics is
shown in Figure 1(b). The optimal shifts of blocks
?eat your cereal? and ?eat your vegetables? were
found and both networks received equal skeleton
priors. TERp would yield this alignment only
if these blocks appear in the paraphrase table or
if ?cereal? and ?vegetables? are considered syn-
onyms. This example is artificial and does not
guarantee that optimal shifts are always found.
3 Experimental Evaluation
System combination experiments combining the
English WMT09 translation task outputs were per-
formed. A total of 96 English outputs were pro-
vided including primary, contrastive, and  -best
outputs. Only the primary  -best outputs were
combined due to time constraints. The numbers
of primary systems per source language were: 3
for Czech, 15 for German, 9 for Spanish, 15 for
French, and 3 for Hungarian. The English bigram
and 5-gram language models were interpolated
from four LM components trained on the English
monolingual Europarl (45M tokens) and News
(510M tokens) corpora, and the English sides of
the News Commentary (2M tokens) and Giga-
FrEn (683M tokens) parallel corpora. The interpo-
lation weights were tuned to minimize perplexity
on news-dev2009 set. The system combination
weights ? one for each system, LM weight, and
word and NULL insertion penalties ? were tuned
to maximize the BLEU (Papineni et al, 2002)
score on the tuning set (newssyscomb2009).
Since the system combination was performed on
tokenized and lower cased outputs, a trigram-
based true caser was trained on all News training
data. The tuning may be summarized as follows:
1. Tokenize and lower case the outputs;
2. Align hypotheses incrementally using each
output as a skeleton;
3. Join the confusion networks into a lattice
with skeleton specific prior estimates;
4. Extract a  -best list from the lattice given
the current weights;
5. Merge the  -best list with the hypotheses
from the previous iteration;
6. Tune new weights given the current merged
 -best list;
7. Iterate 4-6 three times;
8. Extract a  -best list from the lattice given
the best decoding weights and re-score hy-
potheses with a 5-gram;
9. Tune re-scoring weights given the final  -
best list;
10. Extract  -best hypotheses from the  -best
list given the best re-scoring weights, re-case,
and detokenize.
After tuning the system combination weights, the
outputs on a test set may be combined using the
same steps excluding 4-7 and 9. The hypothesis
scores and tuning are identical to the setup used in
(Rosti et al, 2007).
Case insensitive TER and BLEU scores for the
combination outputs using the pair-wise and in-
cremental TER alignment as well as the flexible
alignment on the tuning (dev) and test sets are
shown in Table 1. Only case insensitive scores
are reported since the re-casers used by different
systems are very different and some are trained
using larger resources than provided for WMT09.
The scores of the worst and best individual sys-
tem outputs are also shown. The best and worst
TER and BLEU scores are not necessarily from
the same system output. Both incremental
and flexible alignments used sentence spe-
cific alignment order. Combinations using the in-
cremental and flexible hypothesis alignment algo-
rithms consistently outperform the ones using the
pair-wise TER alignment. The flexible alignment
is slightly better than the incremental alignment on
Czech, Spanish, and Hungarian, and significantly
better on French to English test set scores.
Since the test sets for each language pair consist
of translations of the same documents, it is pos-
sible to combine outputs from many source lan-
guages to English. There were a total of 46 En-
glish primary  -best system outputs. Using all 46
outputs would have required too much memory in
tuning, so a subset of 11 outputs was chosen. The
11 outputs consist of google, uedin, and uka
outputs on all languages. Case insensitive TER
and BLEU scores for the xx-en combination are
shown in Table 2. In addition to incremental
and flexible alignment methods which used
sentence specific alignment order, scores for in-
cremental TER alignment with a fixed alignment
order used in the BBN submissions to WMT08
63
dev cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.30 17.63 82.01 6.83 65.64 19.74 69.19 15.21 78.70 10.33
best 58.16 23.12 57.24 23.20 53.02 29.48 49.78 32.27 66.77 13.59
pairwise 59.60 24.01 56.35 26.04 53.11 29.49 51.03 31.65 69.58 14.60
incremental 59.22 24.31 55.73 26.73 53.05 29.72 50.72 32.09 70.15 14.85
flexible 59.38 24.18 55.51 26.71 52.62 30.24 50.22 32.58 69.83 14.88
test cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.74 16.37 82.39 6.81 65.44 19.04 71.44 14.49 81.21 9.90
best 59.53 21.18 59.41 21.30 53.34 28.69 51.33 31.14 68.32 12.75
pairwise 61.02 21.25 58.75 23.41 53.65 28.15 53.17 29.83 71.50 13.39
incremental 60.63 21.67 58.13 23.96 53.47 28.38 52.51 30.45 71.69 13.60
flexible 60.34 21.87 58.05 23.86 53.13 28.57 51.98 31.30 71.17 13.84
Table 1: Case insensitive TER and BLEU scores on newssyscomb2009 (dev) and newstest2009
(test) for five source languages.
(Rosti et al, 2008) are marked as incr-wmt08.
The sentence specific alignment order yields about
a half BLEU point gain on the tuning set and a
one BLEU point gain on the test set. All system
combination experiments yield very good BLEU
gains on both sets. The scores are also signifi-
cantly higher than any combination from a single
source language. This shows that the outputs from
different source languages are likely to be more di-
verse than outputs from different MT systems on a
single language pair. The combination is not guar-
anteed to be the best possible as the set of outputs
was chosen arbitrarily.
The compactness of the confusion networks
may be measured by the average number of
nodes and arcs per segment. All xx-en con-
fusion networks for newssyscomb2009 and
newstest2009 after the incremental TER
alignment had on average 44.5 nodes and 112.7
arcs per segment. After the flexible hypothesis
alignment, there were on average 41.1 nodes and
104.6 arcs per segment. The number of NULL
word arcs may also be indicative of the alignment
quality. The flexible hypothesis alignment reduced
the average number of NULL word arcs from 29.0
to 24.8 per segment. The rate of convergence in
the  -best list based iterative tuning may be mon-
itored by the number of new hypotheses in the
merged  -best lists from iteration to iteration. By
the third tuning iteration, there were 10% fewer
new hypotheses in the merged  -best list when
using the flexible hypothesis alignment.
xx-en dev test
System TER BLEU TER BLEU
worst 74.21 12.80 75.84 12.05
best 49.78 32.27 51.33 31.14
pairwise 46.10 35.95 47.77 33.53
incr-wmt08 44.58 36.84 46.60 33.61
incremental 44.59 37.30 46.42 34.61
flexible 44.54 37.38 45.82 34.48
Table 2: Case insensitive TER and BLEU
scores on newssyscomb2009 (dev) and
newstest2009 (test) for xx-en combination.
4 Conclusions
This paper described a refined incremental hy-
pothesis alignment algorithm used in the BBN
submissions to the WMT09 system combination
task. The new features included sentence specific
alignment order, flexible matching, and modified
shift heuristics. The refinements yield more com-
pact confusion networks which should allow fewer
spurious insertions in the output and faster conver-
gence in tuning. The future work will investigate
tunable edit costs and methods to choose an opti-
mal subset of outputs for combination.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
64
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 33?
40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceed-
ings of the Automatic Speech Recognition and Un-
derstanding Workshop (ASRU), pages 351?354.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1988. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107.
Damianos Karakos, Jason Eisner, Sanjeev Khundan-
pur, and Markus Dreyer. 2008. Machine trans-
lation system combination using ITG-based align-
ments. In Proceedings of ACL-08: HLT, pages 81?
84.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Proceedings of the 11th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 33?40.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine
translation system combination. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 105?108.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation.
65
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616?625,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Statistical Machine Translation with a Factorized Grammar
Libin Shen and Bing Zhang and Spyros Matsoukas and
Jinxi Xu and Ralph Weischedel
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{lshen,bzhang,smatsouk,jxu,weisched}@bbn.com
Abstract
In modern machine translation practice, a sta-
tistical phrasal or hierarchical translation sys-
tem usually relies on a huge set of trans-
lation rules extracted from bi-lingual train-
ing data. This approach not only results in
space and efficiency issues, but also suffers
from the sparse data problem. In this paper,
we propose to use factorized grammars, an
idea widely accepted in the field of linguis-
tic grammar construction, to generalize trans-
lation rules, so as to solve these two prob-
lems. We designed a method to take advantage
of the XTAG English Grammar to facilitate
the extraction of factorized rules. We experi-
mented on various setups of low-resource lan-
guage translation, and showed consistent sig-
nificant improvement in BLEU over state-of-
the-art string-to-dependency baseline systems
with 200K words of bi-lingual training data.
1 Introduction
A statistical phrasal (Koehn et al, 2003; Och and
Ney, 2004) or hierarchical (Chiang, 2005; Marcu
et al, 2006) machine translation system usually re-
lies on a very large set of translation rules extracted
from bi-lingual training data with heuristic methods
on word alignment results. According to our own
experience, we obtain about 200GB of rules from
training data of about 50M words on each side. This
immediately becomes an engineering challenge on
space and search efficiency.
A common practice to circumvent this problem
is to filter the rules based on development sets in the
step of rule extraction or before the decoding phrase,
instead of building a real distributed system. How-
ever, this strategy only works for research systems,
for which the segments for translation are always
fixed.
However, do we really need such a large rule set
to represent information from the training data of
much smaller size? Linguists in the grammar con-
struction field already showed us a perfect solution
to a similar problem. The answer is to use a fac-
torized grammar. Linguists decompose lexicalized
linguistic structures into two parts, (unlexicalized)
templates and lexical items. Templates are further
organized into families. Each family is associated
with a set of lexical items which can be used to lex-
icalize all the templates in this family. For example,
the XTAG English Grammar (XTAG-Group, 2001),
a hand-crafted grammar based on the Tree Adjoin-
ing Grammar (TAG) (Joshi and Schabes, 1997) for-
malism, is a grammar of this kind, which employs
factorization with LTAG e-tree templates and lexical
items.
Factorized grammars not only relieve the burden
on space and search, but also alleviate the sparse
data problem, especially for low-resource language
translation with few training data. With a factored
model, we do not need to observe exact ?template
? lexical item? occurrences in training. New rules
can be generated from template families and lexical
items either offline or on the fly, explicitly or im-
plicitly. In fact, the factorization approach has been
successfully applied on the morphological level in
previous study on MT (Koehn and Hoang, 2007). In
this work, we will go further to investigate factoriza-
tion of rule structures by exploiting the rich XTAG
English Grammar.
We evaluate the effect of using factorized trans-
lation grammars on various setups of low-resource
language translation, since low-resource MT suffers
greatly on poor generalization capability of trans-
616
lation rules. With the help of high-level linguis-
tic knowledge for generalization, factorized gram-
mars provide consistent significant improvement
in BLEU (Papineni et al, 2001) over string-to-
dependency baseline systems with 200K words of
bi-lingual training data.
This work also closes the gap between compact
hand-crafted translation rules and large-scale unor-
ganized automatic rules. This may lead to a more ef-
fective and efficient statistical translation model that
could better leverage generic linguistic knowledge
in MT.
In the rest of this paper, we will first provide a
short description of our baseline system in Section 2.
Then, we will introduce factorized translation gram-
mars in Section 3. We will illustrate the use of the
XTAG English Grammar to facilitate the extraction
of factorized rules in Section 4. Implementation de-
tails are provided in Section 5. Experimental results
are reported in Section 6.
2 A Baseline String-to-Tree Model
As the baseline of our new algorithm, we use a
string-to-dependency system as described in (Shen
et al, 2008). There are several reasons why we take
this model as our baseline. First, it uses syntactic
tree structures on the target side, which makes it easy
to exploit linguistic information. Second, depen-
dency structures are relatively easier to implement,
as compared to phrase structure grammars. Third,
a string-to-dependency system provides state-of-the-
art performance on translation accuracy, so that im-
provement over such a system will be more convinc-
ing.
Here, we provide a brief description of the base-
line string-to-dependency system, for the sake of
completeness. Readers can refer to (Shen et al,
2008; Shen et al, 2009) for related information.
In the baseline string-to-dependency model, each
translation rule is composed of two parts, source and
target. The source sides is a string rewriting rule,
and the target side is a tree rewriting rule. Both
sides can contain non-terminals, and source and tar-
get non-terminals are one-to-one aligned. Thus, in
the decoding phase, non-terminal replacement for
both sides are synchronized.
Decoding is solved with a generic chart parsing
algorithm. The source side of a translation rule is
used to detect when this rule can be applied. The tar-
get side of the rule provides a hypothesis tree struc-
ture for the matched span. Mono-lingual parsing can
be viewed as a special case of this generic algorithm,
for which the source string is a projection of the tar-
get tree structure.
Figure 1 shows three examples of string-to-
dependency translation rules. For the sake of con-
venience, we use English for both source and target.
Upper-cased words represent source, while lower-
cased words represent target. X is used for non-
terminals for both sides, and non-terminal alignment
is represented with subscripts.
In Figure 1, the top boxes mean the source side,
and the bottom boxes mean the target side. As for
the third rule, FUN Q stands for a function word in
the source language that represents a question.
3 Translation with a Factorized Grammar
We continue with the example rules in Figure 1.
Suppose, we have ?... HATE ... FUN Q? in a given
test segment. There is no rule having both HATE
and FUN Q on its source side. Therefore, we have
to translate these two source words separately. For
example, we may use the second rule in Figure 1.
Thus, HATE will be translated into hates, which is
wrong.
Intuitively, we would like to have translation rule
that tell us how to translate X1 HATE X2 FUN Q
as in Figure 2. It is not available directly from the
training data. However, if we obtain the three rules
in Figure 1, we are able to predict this missing rule.
Furthermore, if we know like and hate are in the
same syntactic/semantic class in the source or target
language, we will be very confident on the validity
of this hypothesis rule.
Now, we propose a factorized grammar to solve
this generalization problem. In addition, translation
rules represented with the new formalism will be
more compact.
3.1 Factorized Rules
We decompose a translation rule into two parts,
a pair of lexical items and an unlexicalized tem-
plate. It is similar to the solution in the XTAG En-
glish Grammar (XTAG-Group, 2001), while here we
617
X1  LIKE  X2
likes
X1 X2
X1  HATE  X2
hates
X1 X2
X1  LIKE X2  FUN_Q
like
does X1 X2
Figure 1: Three examples of string-to-dependency translation rules.
X1  V  X2
VBZ
X1 X2
X1  V  X2
VBZ
X1 X2
X1  V  X2  FUN_Q
VB
does X1 X2
Figure 3: Templates for rules in Figure 1.
X1  HATE  X2  FUN_Q
hate
does X1 X2
Figure 2: An example of a missing rule.
work on two languages at the same time.
For each rule, we first detect a pair of aligned head
words. Then, we extract the stems of this word pair
as lexical items, and replace them with their POS
tags in the rule. Thus, the original rule becomes an
unlexicalized rule template.
As for the three example rules in Figure 1, we will
extract lexical items (LIKE, like), (HATE, hate) and
(LIKE, like) respectively. We obtain the same lexical
items from the first and the third rules.
The resultant templates are shown in Figure 3.
Here, V represents a verb on the source side, VB
stands for a verb in the base form, and VBZ means
a verb in the third person singular present form as
in the Penn Treebank representation (Marcus et al,
1994).
In the XTAG English Grammar, tree templates for
transitive verbs are grouped into a family. All transi-
tive verbs are associated with this family. Here, we
assume that the rule templates representing struc-
tural variations of the same word class can also be
organized into a template family. For example, as
shown in Figure 4, templates and lexical items are
associated with families. It should be noted that
a template or a lexical item can be associated with
more than one family.
Another level of indirection like this provides
more generalization capability. As for the missing
618
X1  V  X2
VBZ
X1 X2
Family Transitive_3
X1  V  X2  FUN_Q
VB
does X1 X2
X1  V  FUN_Past
VBD
X1
Family Intransitive_2
( LIKE, like ) ( HATE, hate ) ( OPEN, open ) ( HAPPEN, happen )
Figure 4: Templates and lexical items are associated with families.
rule in Figure 2, we can now generate it by replac-
ing the POS tags in the second template of Figure
4 with lexical items (HATE, hate) with their correct
inflections. Both the template and the lexical items
here are associated with the family Transitive 3..
3.2 Statistical Models
Another level of indirection also leads to a desirable
back-off model. We decompose a rule R into to two
parts, its template PR and its lexical items LR. As-
suming they are independent, then we can compute
Pr(R) as
Pr(R) = Pr(PR)Pr(LR), or
Pr(R) =
?
F Pr(PR|F )Pr(LR|F )Pr(F ), (1)
if they are conditionally independent for each fam-
ily F . In this way, we can have a good estimate for
rules that do not appear in the training data. The
second generative model will also be useful for un-
supervised learning of families and related probabil-
ities.
In this paper, we approximate families by using
target (English) side linguistic knowledge as what
we will explain in Section 4, so this changes the def-
inition of the task. In short, we will be given a list of
families. We will also be given an association table
B(L,F ) for lexical items L and families F , such
that B(L,F ) = true if and only L is associated
with F , but we do not know the distributions.
Let S be the source side of a rule or a rule tem-
plate, T the target side of a rule of a rule template.
We define Prb, the back-off conditional model of
templates, as follows.
Prb(PS |PT , L) =
?
F :B(L,F ) #(PS , PT , F )
?
F :B(L,F ) #(PT , F )
, (2)
where # stands for the count of events.
Let P and L be the template and lexical items of
R respectively. Let Prt be the MLE model obtained
from the training data. The smoothed probability is
then defined as follows.
Pr(RS |RT ) = (1 ? ?)Prt(RS |RT )
+?Prb(PS |PT , L), (3)
where ? is a parameter. We fix it to 0.1 in later ex-
periments. Conditional probability Pr(RT |RS) is
defined in a similar way.
3.3 Discussion
The factorized models discussed in the previous sec-
tion can greatly alleviate the sparse data problem,
especially for low-resource translation tasks. How-
ever, when the training data is small, it is not easy to
619
learn families. Therefore, to use unsupervised learn-
ing with a model like (1) somehow reduces a hard
translation problem to another one of the same diffi-
culty, when the training data is small.
However, in many cases, we do have extra infor-
mation that we can take advantage of. For example,
if the target language has rich resources, although
the source language is a low-density one, we can ex-
ploit the linguistic knowledge on the target side, and
carry it over to bi-lingual structures of the translation
model. The setup of X-to-English translation tasks
is just like this. This will be the topic of the next
section. We leave unsupervised learning of factor-
ized translation grammars for future research.
4 Using A Mono-Lingual Grammar
In this section, we will focus on X-to-English trans-
lation, and explain how to use English resources to
build a factorized translation grammar. Although we
use English as an example, this approach can be ap-
plied to any language pairs that have certain linguis-
tic resources on one side.
As shown in Figure 4, intuitively, the families
are intersection of the word families of the two lan-
guages involved, which means that they are refine-
ment of the English word families. For example,
a sub-set of the English transitive families may be
translated in the same way, so they share the same
set of templates. This is why we named the two fam-
ilies Transitive 3 and Intransitive 2 in Figure 4.
Therefore, we approximate bi-lingual families
with English families first. In future, we can use
them as the initial values for unsupervised learning.
In order to learn English families, we need to take
away the source side information in Figure 4, and
we end up with a template?family?word graph as
shown in Figure 5. We can learn this model on large
mono-lingual data if necessary.
What is very interesting is that there already exists
a hand-crafted solution for this model. This is the
XTAG English Grammar (XTAG-Group, 2001).
The XTAG English Grammar is a large-scale En-
glish grammar based on the TAG formalism ex-
tended with lexicalization and unification-based fea-
ture structures. It consists of morphological, syn-
tactic, and tree databases. The syntactic database
contains the information that we have represented
in Figure 5 and many other useful linguistic annota-
tions, e.g. features.
The XTAG English grammar contains 1,004 tem-
plates, organized in 53 families, and 221 individual
templates. About 30,000 lexical items are associ-
ated with these families and individual templates 1.
In addition, it also has the richest English morpho-
logical lexicon with 317,000 inflected items derived
from 90,000 stems. We use this resource to predict
POS tags and inflections of lexical items.
In our applications, we select all the verb fami-
lies plus one each for nouns, adjectives and adverbs.
We use the families of the English word as the fam-
ilies of bi-lingual lexical items. Therefore, we have
a list of about 20 families and an association table
as described in Section 3.2. Of course, one can use
other linguistic resources if similar family informa-
tion is provided, e.g. VerbNet (Kipper et al, 2006)
or WordNet (Fellbaum, 1998).
5 Implementation
Nowadays, machine translation systems become
more and more complicated. It takes time to write
a decoder from scratch and hook it with various
modules, so it is not the best solution for research
purpose. A common practice is to reduce a new
translation model to an old one, so that we can use
an existing system, and see the effect of the new
model quickly. For example, the tree-based model
proposed in (Carreras and Collins, 2009) used a
phrasal decoder for sub-clause translation, and re-
cently, DeNeefe and Knight (2009) reduced a TAG-
based translation model to a CFG-based model by
applying all possible adjunction operations offline
and stored the results as rules, which were then used
by an existing syntax-based decoder.
Here, we use a similar method. Instead of build-
ing a new decoder that uses factorized grammars,
we reduce factorized rules to baseline string-to-
dependency rules by performing combination of
templates and lexical items in an offline mode. This
is similar to the rule generation method in (DeNeefe
and Knight, 2009). The procedure is as follows.
In the rule extraction phase, we first extract all the
string-to-dependency rules with the baseline system.
1More information about XTAG is available online at
http://www.cis.upenn.edu/?xtag .
620
VBZ
X1 X2
Family Transitive
VB
does X1 X2
VBD
X1
Family Intransitive
like hate open happen
Figure 5: Templates, families, and words in the XTAG English Grammar.
For each extracted rule, we try to split it into various
?template?lexical item? pairs by choosing different
aligned words for delexicalization, which turns rules
in Figure 1 into lexical items and templates in Fig-
ure 3. Events of templates and lexical items are
counted according to the family of the target En-
glish word. If an English word is associated with
more than one family, the count is distributed uni-
formly among these families. In this way, we collect
sufficient statistics for the back-off model in (2).
For each family, we keep the top 200 most fre-
quent templates. Then, we apply them to all the
lexical items in this families, and save the gener-
ated rules. We merge the new rules with the original
one. The conditional probabilities for the rules in the
combined set is smoothed according to (2) and (3).
Obviously, using only the 200 most frequent tem-
plates for each family is just a rough approxima-
tion. An exact implementation of a new decoder for
factorized grammars can make better use of all the
templates. However, the experiments will show that
even an approximation like this can already provide
significant improvement on small training data sets,
i.e. with no more than 2M words.
Since we implement template application in an of-
fline mode, we can use exactly the same decoding
and optimization algorithms as the baseline. The de-
coder is a generic chart parsing algorithm that gen-
erates target dependency trees from source string in-
put. The optimizer is an L-BFGS algorithm that
maximizes expected BLEU scores on n-best hy-
potheses (Devlin, 2009).
6 Experiments on Low-Resource Setups
We tested the performance of using factorized gram-
mars on low-resource MT setups. As what we noted
above, the sparse data problem is a major issue when
there is not enough training data. This is one of the
cases that a factorized grammar would help.
We did not tested on real low-resource languages.
Instead, we mimic the low-resource setup with two
of the most frequently used language pairs, Arabic-
to-English and Chinese-to-English, on newswire
and web genres. Experiments on these setups will
be reported in Section 6.1. Working on a language
which actually has more resources allows us to study
the effect of training data size. This will be reported
in Section 6.2. In Section 6.3, we will show exam-
ples of templates learned from the Arabic-to-English
training data.
6.1 Languages and Genres
The Arabic-to-English training data contains about
200K (target) words randomly selected from an
LDC corpus, LDC2006G05 A2E set, plus an
Arabic-English dictionary with about 89K items.
We build our development sets from GALE P4 sets.
There are one tune set and two test sets for the MT
systems 2. TEST-1 has about 5000 segments and
TEST-2 has about 3000 segments.
2One of the two test sets will later be used to tune an MT
combination system.
621
MODEL
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English newswire
baseline 21.07 12.41 43.77 19.96 11.42 42.79 21.09 11.03 43.74
factorized 21.70 13.17 44.85 20.52 11.70 43.83 21.36 11.77 44.72
Arabic-to-English web
baseline 10.26 5.02 32.78 9.40 4.87 31.26 14.11 7.34 35.93
factorized 10.67 5.34 33.83 9.74 5.20 32.52 14.66 7.69 37.11
Chinese-to-English newswire
baseline 13.17 8.04 44.70 19.62 9.32 48.60 14.53 6.82 45.34
factorized 13.91 8.09 45.03 20.48 9.70 48.61 15.16 7.37 45.31
Chinese-to-English web
baseline 11.52 5.96 42.18 11.44 6.07 41.90 9.83 4.66 39.71
factorized 11.98 6.31 42.84 11.72 5.88 42.55 10.25 5.34 40.34
Table 1: Experimental results on Arabic-to-English / Chinese-to-English newswire and web data. %BL stands for
BLEU scores for documents whose BLEU scores are in the bottom 75% to 90% range of all documents. MET stands
for METEOR scores.
The Chinese-to-English training data contains
about 200K (target) words randomly selected from
LDC2006G05 C2E set, plus a Chinese-English dic-
tionary (LDC2002L27) with about 68K items. The
development data setup is similar to that of Arabic-
to-English experiments.
Chinese-to-English translation is from a morphol-
ogy poor language to a morphology rich language,
while Arabic-to-English translation is in the oppo-
site direction. It will be interesting to see if factor-
ized grammars help on both cases. Furthermore, we
also test on two genres, newswire and web, for both
languages.
Table 1 lists the experimental results of all the four
conditions. The tuning metric is expected BLEU.
We are also interested in the BLEU scores for doc-
uments whose BLEU scores are in the bottom 75%
to 90% range of all documents. We mark it as %BL
in the table. This metric represents how a system
performances on difficult documents. It is important
to certain percentile evaluations. We also measure
METEOR (Banerjee and Lavie, 2005) scores for all
systems.
The system using factorized grammars shows
BLEU improvement in all conditions. We measure
the significance of BLEU improvement with paired
bootstrap resampling as described by (Koehn, 2004).
All the BLEU improvements are over 95% confi-
dence level. The new system also improves %BL
and METEOR in most of the cases.
6.2 Training Data Size
The experiments to be presented in this section
are designed to measure the effect of training data
size. We select Arabic web for this set of experi-
ments. Since the original Arabic-to-English train-
ing data LDC2006G05 is a small one, we switch to
LDC2006E25, which has about 3.5M target words
in total. We randomly select 125K, 250K, 500K, 1M
and 2M sub-sets from the whole data set. A larger
one always includes a smaller one. We still tune on
expected BLEU, and test on BLEU, %BL and ME-
TEOR.
The average BLEU improvement on test sets is
about 0.6 on the 125K set, but it gradually dimin-
ishes. For better observation, we draw the curves of
BLEU improvement along with significance test re-
sults for each training set. As shown in Figure 6 and
7, more improvement is observed with fewer train-
ing data. This fits well with fact that the baseline MT
model suffers more on the sparse data problem with
smaller training data. The reason why the improve-
ment diminishes on the full data set could be that the
rough approximation with 200 most frequent tem-
plates cannot fully take advantage of this paradigm,
which will be discussed in the next section.
622
MODEL SIZE
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English web
baseline
125K
8.54 2.96 28.87 7.41 2.82 26.95 11.29 5.06 31.37
factorized 8.99 3.44 30.40 7.92 3.57 28.63 12.04 6.06 32.87
baseline
250K
10.18 4.70 32.21 8.94 4.35 30.31 13.71 6.93 35.14
factorized 10.57 4.96 33.22 9.34 4.78 31.51 14.02 7.28 36.25
baseline
500K
12.18 5.84 35.59 10.82 5.77 33.62 16.48 8.30 38.73
factorized 12.40 6.01 36.15 11.14 5.96 34.38 16.76 8.53 39.27
baseline
1M
13.95 7.17 38.49 12.48 7.12 36.56 18.86 10.00 42.18
factorized 14.14 7.41 38.99 12.66 7.34 37.14 19.11 10.29 42.56
baseline
2M
15.74 8.38 41.15 14.18 8.17 39.26 20.96 11.95 45.18
factorized 15.92 8.81 41.51 14.34 8.25 39.68 21.42 12.05 45.51
baseline
3.5M
16.95 9.76 43.03 15.47 9.08 41.28 22.83 13.24 47.05
factorized 17.07 9.99 43.18 15.49 8.77 41.41 22.72 13.10 47.23
Table 2: Experimental results on Arabic web. %BL stands for BLEU scores for documents whose BLEU scores are
in the bottom 75% to 90% range of all documents. MET stands for METEOR scores.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-1
Figure 6: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-1.
6.3 Example Templates
Figure 8 lists seven Arabic-to-English templates
randomly selected from the transitive verb family.
TMPL 151 is an interesting one. It helps to alleviate
the pronoun dropping problem in Arabic. However,
we notice that most of the templates in the 200 lists
are rather simple. More sophisticated solutions are
needed to go deep into the list to find out better tem-
plates in future.
It will be interesting to find an automatic or
semi-automatic way to discover source counterparts
of target treelets in the XTAG English Grammar.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-2
Figure 7: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-2.
Generic rules like this will be very close to hand-
craft translate rules that people have accumulated for
rule-based MT systems.
7 Conclusions and Future Work
In this paper, we proposed a novel statistical ma-
chine translation model using a factorized structure-
based translation grammar. This model not only al-
leviates the sparse data problem but only relieves the
burden on space and search, both of which are im-
minent issues for the popular phrasal and/or hierar-
chical MT systems.
623
VVB
TMPL_1
X1  V
VBD
X1
TMPL_121
TMPL_31
V  X1
for
VBG
X1
TMPL_151
TMPL_61
V  X1
VBN
by
X1
TMPL_181
TMPL_91
X1  V
VBD
X1
the
V  X1
VBD
he X1
X1  V  X2
VBZ
X1 X2
Figure 8: Randomly selected Arabic-to-English templates from the transitive verb family.
We took low-resource language translation, espe-
cially X-to-English translation tasks, for case study.
We designed a method to exploit family informa-
tion in the XTAG English Grammar to facilitate the
extraction of factorized rules. We tested the new
model on low-resource translation, and the use of
factorized models showed significant improvement
in BLEU on systems with 200K words of bi-lingual
training data of various language pairs and genres.
The factorized translation grammar proposed here
shows an interesting way of using richer syntactic
resources, with high potential for future research.
In future, we will explore various learning meth-
ods for better estimation of families, templates and
lexical items. The target linguistic knowledge that
we used in this paper will provide a nice starting
point for unsupervised learning algorithms.
We will also try to further exploit the factorized
representation with discriminative learning. Fea-
tures defined on templates and families will have
good generalization capability.
Acknowledgments
This work was supported by DARPA/IPTO Contract
HR0011-06-C-0022 under the GALE program3. We
thank Aravind Joshi, Scott Miller, Richard Schwartz
and anonymous reviewers for valuable comments.
3Distribution Statement ?A? (Approved for Public Release,
Distribution Unlimited). The views, opinions, and/or find-
ings contained in this article/presentation are those of the au-
thor/presenter and should not be interpreted as representing the
official views or policies, either expressed or implied, of the De-
fense Advanced Research Projects Agency or the Department of
Defense.
624
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of the
43th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 101?104, Ann Arbor,
MI.
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of the 2009 Conference of Empirical
Methods in Natural Language Processing, pages 200?
209, Singapore.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 263?270, Ann Ar-
bor, MI.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference of Empirical Methods in Natural
Language Processing, pages 727?736, Singapore.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, Univ. of Maryland.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. The MIT Press.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer-Verlag.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extensive classifications of en-
glish verbs. In Proceedings of the 12th EURALEX In-
ternational Congress.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proceedings of the 2007 Conference of Empiri-
cal Methods in Natural Language Processing.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 48?54, Edmonton,
Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference of Empirical Methods in Natu-
ral Language Processing, pages 388?395, Barcelona,
Spain.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference of Empirical
Methods in Natural Language Processing, pages 44?
52, Sydney, Australia.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Kishore Papineni, Salim Roukos, and Todd Ward. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report, RC22176.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical Ma-
chine Translation. In Proceedings of the 2009 Confer-
ence of Empirical Methods in Natural Language Pro-
cessing, pages 72?80, Singapore.
XTAG-Group. 2001. A lexicalized tree adjoining gram-
mar for english. Technical Report 01-03, IRCS, Univ.
of Pennsylvania.
625
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 321?326,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BBN System Description for WMT10 System Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination out-
puts for Czech-English, German-English,
Spanish-English, French-English, and All-
English language pairs. All combinations
were based on confusion network decod-
ing. An incremental hypothesis alignment
algorithm with flexible matching was used
to build the networks. The bi-gram de-
coding weights for the single source lan-
guage translations were tuned directly to
maximize the BLEU score of the decod-
ing output. Approximate expected BLEU
was used as the objective function in gra-
dient based optimization of the combina-
tion weights for a 44 system multi-source
language combination (All-English). The
system combination gained around 0.4-
2.0 BLEU points over the best individual
systems on the single source conditions.
On the multi-source condition, the system
combination gained 6.6 BLEU points.
1 Introduction
The BBN submissions to the WMT10 system
combination task were based on confusion net-
work decoding. The confusion networks were
built using the incremental hypothesis alignment
algorithm with flexible matching introduced in the
BBN submission for the WMT09 system combi-
nation task (Rosti et al, 2009). This year, the
system combination weights were tuned to max-
imize the BLEU score (Papineni et al, 2002) of
the 1-best decoding output (lattice based BLEU
tuning) using downhill simplex method (Press et
al., 2007). A 44 system multi-source combina-
tion was also submitted. Since the gradient-free
optimization algorithms do not seem to be able to
handle more than 20-30 weights, a gradient ascent
to maximize an approximate expected BLEU ob-
jective was used to optimize the larger number of
weights.
The lattice based BLEU tuning may be imple-
mented using any optimization algorithm that does
not require the gradient of the objective function.
Due to the size of the lattices, the objective func-
tion evaluation may have to be distributed to mul-
tiple servers. The optimizer client accumulates the
BLEU statistics of the 1-best hypotheses from the
servers for given search weights, computes the fi-
nal BLEU score, and passes it to the optimiza-
tion algorithm which returns a new set of search
weights. The lattice based tuning explores the en-
tire search space and does not require multiple de-
coding iterations with N -best list merging to ap-
proximate the search space as in the standard min-
imum error rate training (Och, 2003). This allows
much faster turnaround in weight tuning.
Differentiable approximations of BLEU have
been proposed for consensus decoding. Tromble
et al (2008) used a linear approximation and Pauls
et al (2009) used a closer approximation called
CoBLEU. CoBLEU is based on the BLEU for-
mula but the n-gram counts are replaced by ex-
pected counts over a translation forest. Due to the
min-functions required in converting the n-gram
counts to matches and a non-differentiable brevity
penalty, a sub-gradient ascent must be used. In
this work, an approximate expected BLEU (Exp-
BLEU) defined over N -best lists was used as a
differentiable objective function. ExpBLEU uses
expected BLEU statistics where the min-function
is not needed as the statistics are computed off-
line and the brevity penalty is replaced by a dif-
ferentiable approximation. The ExpBLEU tun-
ing yields comparable results to direct BLEU tun-
ing using gradient-free algorithms on combina-
tions of small number of systems (fewer than 20-
30 weights). Results on a 44 system combination
show that the gradient based optimization is more
robust with larger number of weights.
321
This paper is organized as follows. Section
2 reviews the incremental hypothesis alignment
algorithm used to built the confusion networks.
Decoding weight optimization using direct lattice
1-best BLEU tuning and N -best list based Exp-
BLEU tuning are presented in Section 3. Exper-
imental results on combining single source lan-
guage to English outputs and all 44 English out-
puts are detailed in Section 4. Finally, Section 5
concludes this paper with some ideas for future
work.
2 Hypothesis Alignment
The confusion networks were built by using the
incremental hypothesis alignment algorithm with
flexible matching introduced in Rosti et al (2009).
The algorithm is reviewed in more detail here. It
is loosely related to the alignment performed in
the calculation of the translation edit rate (TER)
(Snover et al, 2006) which estimates the edit
distance between two strings allowing shifts of
blocks of words in addition to insertions, dele-
tions, and substitutions. Calculating an exact TER
for strings longer than a few tokens1 is not compu-
tationally feasible, so the tercom2 software uses
heuristic shift constraints and pruning to find an
upper bound of TER. In this work, the hypothe-
ses were aligned incrementally with the confusion
network, thus using tokens from all previously
aligned hypotheses in computing the edit distance.
Lower substitution costs were assigned to tokens
considered equivalent and the heuristic shift con-
straints of tercom were relaxed3.
First, tokens from all hypotheses are put into
equivalence classes if they belong to the same
WordNet (Fellbaum, 1998) synonym set or have
the same stem. The 1-best hypothesis from each
system is used as the confusion network skeleton
which defines the final word order of the decod-
ing output. Second, a trivial confusion network
is generated from the skeleton hypothesis by gen-
erating a single arc for each token. The align-
ment algorithm explores shifts of blocks of words
that minimize the edit distance between the cur-
rent confusion network and an unaligned hypothe-
1Hypotheses are tokenized and lower-cased prior to align-
ment. Tokens generally refer to words and punctuation.
2http://www.cs.umd.edu/?snover/tercom/
current version 0.7.25.
3This algorithm is not equivalent to an incremental TER-
Plus (Snover et al, 2009) due to different shift constraints and
the lack of paraphrase matching
30 1cat(1) 2sat(1) mat(1)
(a) Skeleton hypothesis.
40 1cat(1,1) 2sat(1,1) 3on(0,1)NULL(1,0) mat(1,1)
(b) Two hypotheses (insertion).
40 1cat(1,1,0)NULL(0,0,1) 2sat(1,1,1) 3on(0,1,0)NULL(1,0,1) mat(1,1,1)
(c) Three hypotheses (deletion).
40 1cat(1,1,0,1)NULL(0,0,1,0) 2sat(1,1,1,1) 3on(0,1,0,0)NULL(1,0,1,1) mat(1,1,1,0)hat(0,0,0,1)
(d) Four hypotheses (substitution).
Figure 1: Example of incrementally aligning ?cat
sat mat?, ?cat sat on mat?, ?sat mat?, and ?cat sat
hat?.
sis. Third, the hypothesis with the lowest edit dis-
tance to the current confusion network is aligned
into the network. The heuristically selected edit
costs used in the WMT10 system were 1.0 for
insertions, deletions, and shifts, 0.2 for substitu-
tions of tokens in the same equivalence class, and
1.0001 for substitutions of non-equivalent tokens.
An insertion with respect to the network always
results in a new node and two new arcs. The first
arc contains the inserted token and the second arc
contains a NULL token representing the missing
token from all previously aligned hypotheses. A
substitution/deletion results in a new token/NULL
arc or increase in the confidence of an existing to-
ken/NULL arc. The process is repeated until all
hypotheses are aligned into the network.
For example, given the following hypotheses
from four systems: ?cat sat mat?, ?cat sat on mat?,
?sat mat?, and ?cat sat hat?, an initial network in
Figure 1(a) is generated. The following two hy-
potheses have a distance of one edit from the initial
network, so the second can be aligned next. Figure
1(b) shows the additional node created and the two
new arcs for ?on? and ?NULL? tokens. The third
hypothesis has deleted token ?cat? and matches the
322
?NULL? token between nodes 2 and 3 as seen in
Figure 1(c). The fourth hypothesis matches all but
the final token ?hat? which becomes a substitution
for ?mat? in Figure 1(d). The binary vectors in
the parentheses following each token show which
system generated the token aligned to that arc. If
the systems generated N -best hypotheses, a frac-
tional increment could be added to these vectors
as in (Rosti et al, 2007). Given these system spe-
cific scores are normalized to sum to one over all
arcs connecting two consecutive nodes, they may
be viewed as system specific word arc posterior
estimates. Note, for 1-best hypotheses the scores
sum to one without normalization.
Given system outputs E = {E1, . . . , ENs},
an algorithm to build a set of Ns confusion
networks C = {C1, . . . , CNs} may be written
as:
for n = 1 to Ns do
Cn ? Init(En) {initialize confusion net-
work from the skeleton}
E ? ? E ? En {set of unaligned hypotheses}
while E ? 6= ? do
Em ? argminE?E ? Dist(E,Cn)
{compute edit distances}
Cn ? Align(Em, Cn) {align closest hy-
pothesis}
E ? ? E ? ? Em {update set of unaligned
hypotheses}
end while
end for
The set of Ns confusion networks are expanded to
separate paths with distinct bi-gram contexts and
connected in parallel into a big lattice with com-
mon start and end nodes with NULL token arcs.
A prior probability estimate is assigned to the sys-
tem specific word arc confidences connecting the
common start node and the first node in each sub-
network. A heuristic prior is estimated as:
pn =
1
Z
exp(?100
en
Nn
) (1)
where en is the total cost of aligning all hypothe-
ses when using system n as the skeleton, Nn is
the number of nodes in the confusion network be-
fore bi-gram expansion, and Z is a scaling factor
to guarantee pn sum to one. This gives a higher
prior for a network with fewer alignment errors
and longer expected decoding output.
3 Weight Optimization
Standard search algorithms may be used to find N -
best hypotheses from the final lattice. The score
for arc l is computed as:
sl = log
( Ns?
n=1
?nsnl
)
+ ?L(wl|wP (l)) + ?S(wl)
(2)
where ?n are the system weights constrained to
sum to one, snl are the system specific arc pos-
teriors, ? is a language model (LM) scaling fac-
tor, L(wl|wP (l)) is the bi-gram log-probability for
the token wl on the arc l given the token wP (l)
on the arc P (l) preceding the arc l, ? is the word
insertion scaling factor, and S(wl) is zero if wl
is a NULL token and one otherwise. The path
with the highest total score under summation is
the 1-best decoding output. The decoding weights
? = {?1, . . . , ?Ns , ?, ?} are tuned to optimize two
objective functions described next.
3.1 Lattice Based BLEU Optimization
Powell?s method (Press et al, 2007) on N -best
lists was used in system combination weight tun-
ing in Rosti et al (2007). This requires multiple
decoding iterations and merging the N -best lists
between tuning runs to approximate the full search
space as in Och (2003). To speed up the tuning
process, a distributed optimization method can be
used. The lattices are divided into multiple chunks
each of which are loaded into memory by a server.
A client runs the optimization algorithm relying
on the servers for parallelized objective function
evaluation. The client sends a new set of search
weights to the servers which decode the chunks
of lattices and return the 1-best hypothesis BLEU
statistics back to the client. The client accumulates
the BLEU statistics from all servers and computes
the final BLEU score used as the objective func-
tion by the optimization algorithm. Results similar
to Powell?s method can be obtained with fewer it-
erations by using the downhill simplex method in
multi-dimensions (Amoeba) (Press et al, 2007).
To enforce the sum to one constraint of the sys-
tem weights ?n, the search weights are restricted
to [0, 1] by assigning a large penalty if any cor-
responding search weight breaches the limits and
these restricted search weights are scaled to sum
to one before the objective function evaluation.
After optimizing the bi-gram decoding weights
directly on the lattices, a 300-best list are gener-
323
ated. The 300-best hypotheses are re-scored using
a 5-gram LM and another set of re-scoring weights
are tuned on the development set using the stan-
dard N -best list based method. Multiple random
restarts may be used in both lattice and N-best list
based optimization to decrease chances of finding
a local minimum. Twenty sets of initial weights
(the weights from the previous tuning and 19 ran-
domly perturbed weights) were used in all experi-
ments.
3.2 Approximate Expected BLEU
Optimization
The gradient-free optimization algorithms like
Powell?s method and downhill simplex work well
for up to around 20-30 weights. When the number
of weights is larger, the algorithms often get stuck
in local optima even if multiple random restarts
are used. The BLEU score for a 1-best output is
defined as follows:
BLEU =
4?
n=1
(?
i m
n
i?
i h
n
i
) 1
4
?
(
1 ?
?
i ri
?
i h
1
i
)
(3)
where mni is the number of n-gram matches be-
tween the hypothesis and reference for segment
i, hni is the number of n-grams in the hypothesis,
ri is the reference length (or the reference length
closest to the hypothesis if multiple references are
available), and ?(x) = min(1.0, ex) is the brevity
penalty. The first term in Equation 3 is a harmonic
mean of the n-gram precisions up to n = 4. The
selection of 1-best hypotheses is discrete and the
brevity penalty is not continuous, so the BLEU
score is not differentiable and gradient based op-
timization cannot be used. Given a posterior dis-
tribution over all possible decoding outputs could
be defined, an expected BLEU could be optimized
using gradient ascent. However, this posterior dis-
tribution can only be approximated by expensive
sampling methods.
A differentiable objective function over N -best
lists to approximate the BLEU score can be de-
fined using expected BLEU statistics and a con-
tinuous approximation of the brevity penalty. The
posterior probability for hypothesis j of segment i
is simply the normalized decoder score:
pij =
e?Sij
?
k e?Sik
(4)
where ? is a posterior scaling factor and Sij is the
total score of hypothesis j of segment i. The pos-
terior scaling factor controls the shape of the pos-
terior distribution: ? > 1.0 moves the probability
mass toward the 1-best hypothesis and ? < 1.0
flattens the distribution. The BLEU statistics in
Equation 3 are replaced by the expected statistics;
for example, m?ni =
?
j pijmij , and the brevity
penalty ?(x) is approximated by:
?(x) =
ex ? 1
e1000x + 1
+ 1 (5)
ExpBLEU has a closed form solution for the gra-
dient, provided the total decoder score is differen-
tiable.
The penalty used to restrict the search weights
corresponding to the system weights ?n in
gradient-free BLEU tuning is not differentiable.
For expected BLEU tuning, the search weights ?n
are unrestricted but the system weights are ob-
tained by a sigmoid transform and normalized to
sum to one:
?n =
?(?n)
?
m ?(?m)
(6)
where ?(?n) = 1/(1 + e??n).
The expected BLEU tuning is performed on N -
best lists in similar fashion to direct BLEU tuning.
Tuned weights from one decoding iteration are
used to generate a new N -best list, the new N -best
list is merged with the N -best list from the previ-
ous tuning run, and a new set of weights are op-
timized using limited memory Broyden-Fletcher-
Goldfarb-Shanno method (lBFGS) (Liu and No-
cedal, 1989). Since the posterior distribution is
affected by the size of the N -best list and differ-
ent decoding weights, the posterior scaling factor
can be set for each tuning run so that the perplex-
ity of the posterior distribution given the merged
N -best list is constant. A target perplexity of 5.0
was used in the experiments. Four iterations of
bi-gram decoding weight tuning were performed
using 300-best lists. The final 300-best list was re-
scored with a 5-gram and another set of re-scoring
weights was tuned on the development set.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined. Unpruned En-
glish bi-gram and 5-gram language model com-
ponents were trained using the WMT10 corpora:
EuroParl, GigaFrEn, NewsCommentary,
and News. Additional six Gigaword v4 com-
ponents were trained: AFP, APW, XIN+CNA,
324
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.99 13.85 68.45 15.07 60.86 21.02 71.17 15.00
best 56.77 22.84 57.76 25.05 51.81 30.10 53.66 28.64
syscomb 57.31 25.11 54.97 27.75 50.46 31.54 51.35 31.16
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.65 14.29 67.50 15.66 60.52 21.86 68.36 16.82
best 56.13 23.56 58.12 24.34 51.45 30.56 52.16 29.79
syscomb 56.89 25.12 55.60 26.38 50.33 31.59 51.36 30.16
Table 1: Case insensitive TER and BLEU scores on syscombtune (tune) and syscombtest (test)
for combinations of outputs from four source languages.
LTW, NYT, and Headlines+Datelines. In-
terpolation weights for the ten components
were tuned so as to minimize perplexity on
the newstest2009-ref.en development set.
The LMs used modified Kneser-Ney smoothing.
On the multi-source condition (xx-en) another
LM was trained from the system outputs and in-
terpolated with the general LM using an interpola-
tion weight 0.3 for the LM trained on the system
outputs. This LM is referred to as biasLM later.
A tri-gram true casing model was trained using all
available English data. This model was used to
restore the case of the lower-case system combi-
nation output.
All six 1-best system outputs on cz-en, 16
outputs on de-en, 8 outputs on es-en, and
14 outputs on fr-en were combined. The lat-
tice based BLEU tuning was used to optimize the
bi-gram decoding weights and N-best list based
BLEU tuning was used to optimize the 5-gram re-
scoring weights. Results for these single source
language experiments are shown in Table 1. The
gains on syscombtune were similar to those on
syscombtest for all but French-English. The
tuning set contained only 455 segments but ap-
peared to be well matched with the larger (2034
segments) test set. The characteristics of the indi-
vidual system outputs were probably different for
the tuning and test sets on French-English transla-
tion. In our experience, optimizing system com-
bination weights using the ExpBLEU tuning for
a small number of systems yields similar results
to lattice based BLEU tuning. The lattice based
BLEU tuning is faster as there is no need for mul-
tiple decoding and tuning iterations. Using the bi-
asLM on the single source combinations did not
xx-en tune test
System TER BLEU TER BLEU
worst 71.17 13.85 68.65 14.29
best 51.81 30.10 51.45 30.56
lattice 43.15 35.72 43.79 35.29
expBLEU 44.07 36.91 44.35 36.62
+biasLM 43.63 37.61 44.50 37.12
Table 2: Case insensitive TER and BLEU scores
on syscombtune (tune) and syscombtest
(test) for xx-en combination. Combinations us-
ing lattice BLEU tuning, expected BLEU tuning,
and after adding the system output biased LM are
shown.
yield any gains. The output for these conditions
probably did not contain enough data for biasLM
training given the small tuning set and small num-
ber of systems.
Finally, experiments combining all 44 1-best
system outputs were performed to produce a
multi-source combination output. The first experi-
ment used the lattice based BLEU tuning and gave
a 5.6 BLEU point gain on the tuning set as seen in
Table 2. The ExpBLEU tuning gave an additional
1.2 point gain which suggests that the direct lattice
based BLEU tuning got stuck in a local optimum.
Using the system output biased LM gave an addi-
tional 0.7 point gain. The gains on the test set were
similar and the best combination gave a 6.6 point
gain over the best individual system.
5 Conclusions
The BBN submissions for WMT10 system com-
bination task were described in this paper. The
combination was based on confusion network de-
325
coding. The confusion networks were built us-
ing an incremental hypothesis alignment algo-
rithm with flexible matching. The bi-gram de-
coding weights for the single source conditions
were optimized directly to maximize the BLEU
scores of the 1-best decoding outputs and the 5-
gram re-scoring weights were tuned on 300-best
lists. The BLEU gains over the best individual
system outputs were around 1.5 points on cz-en,
2.0 points on de-en, 1.0 points on es-en, and
0.4 points on fr-en. The system combination
weights on xx-en were tuned to maximize Exp-
BLEU, and a system output biased LM was used.
The BLEU gain over the best individual system
was 6.6 points. Future work will investigate tuning
of the edit costs used in the alignment. A lattice
based ExpBLEU tuning will be investigated. Also,
weights for more complicated functions with addi-
tional features may be tuned using ExpBLEU.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Math-
ematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Adam Pauls, John DeNero, and DanKlein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
recipes: the art of scientific computing. Cambridge
University Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 61?65.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing,
pages 620?629.
326
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 159?165,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Expected BLEU Training for Graphs: BBN System Description for
WMT11 System Combination Task
Antti-Veikko I. Rosti? and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination outputs
for Czech-English, German-English, Spanish-
English, and French-English language pairs.
All combinations were based on confusion
network decoding. The confusion networks
were built using incremental hypothesis align-
ment algorithm with flexible matching. A
novel bi-gram count feature, which can penal-
ize bi-grams not present in the input hypothe-
ses corresponding to a source sentence, was
introduced in addition to the usual decoder
features. The system combination weights
were tuned using a graph based expected
BLEU as the objective function while incre-
mentally expanding the networks to bi-gram
and 5-gram contexts. The expected BLEU
tuning described in this paper naturally gen-
eralizes to hypergraphs and can be used to
optimize thousands of weights. The com-
bination gained about 0.5-4.0 BLEU points
over the best individual systems on the official
WMT11 language pairs. A 39 system multi-
source combination achieved an 11.1 BLEU
point gain.
1 Introduction
The confusion networks for the BBN submissions
to the WMT11 system combination task were built
using incremental hypothesis alignment algorithm
?This work was supported by DARPA/I2O Contract No.
HR0011-06-C-0022 under the GALE program (Approved for
Public Release, Distribution Unlimited). The views, opinions,
and/or findings contained in this article/presentation are those of
the author/presenter and should not be interpreted as represent-
ing the official views or policies, either expressed or implied,
of the Defense Advanced Research Projects Agency or the De-
partment of Defense.
with flexible matching (Rosti et al, 2009). A novel
bi-gram count feature was used in addition to the
standard decoder features. The N-best list based ex-
pected BLEU tuning (Rosti et al, 2010), similar to
the one proposed by Smith and Eisner (2006), was
extended to operate on word lattices. This method is
closely related to the consensus BLEU (CoBLEU)
proposed by Pauls et al (2009). The minimum oper-
ation used to compute the clipped counts (matches)
in the BLEU score (Papineni et al, 2002) was re-
placed by a differentiable function, so there was
no need to use sub-gradient ascent as in CoBLEU.
The expected BLEU (xBLEU) naturally generalizes
to hypergraphs by simply replacing the forward-
backward algorithm with inside-outside algorithm
when computing the expected n-gram counts and
sufficient statistics for the gradient.
The gradient ascent optimization of the xBLEU
appears to be more stable than the gradient-free di-
rect 1-best BLEU tuning or N -best list based min-
imum error rate training (Och, 2003), especially
when tuning a large number of weights. On the of-
ficial WMT11 language pairs with up to 30 weights,
there was no significant benefit from maximizing
xBLEU. However, on a 39 system multi-source
combination (43 weights total), it yielded a signif-
icant gain over gradient-free BLEU tuning and N -
best list based expected BLEU tuning.
2 Hypothesis Alignment and Features
The incremental hypothesis alignment with flexible
matching (Rosti et al, 2009) produces a confusion
network for each system output acting as a skele-
ton hypothesis for the ith source sentence. A con-
fusion network is a graph where all paths visit all
159
vertices. Consecutive vertices are connected by one
or more edges representing alternatives. Each edge
l is associated with a token and a set of scores. A to-
ken may be a word, punctuation symbol, or special
NULL token indicating a deletion in the alignment.
The set of scores includes a vector ofNs system spe-
cific confidences, siln, indicating whether the token
was aligned from the output of the system n.1 Other
scores may include a language model (LM) score
as well as non-NULL and NULL token indicators
(Rosti et al, 2007). As Rosti et al (2010) described,
the networks for all skeletons are connected to a start
and end vertex with NULL tokens in order to form
a joint lattice with multiple parallel networks. The
edges connecting the start vertex to the initial ver-
tices in each network have a heuristic prior estimated
from the alignment statistics at the confidence cor-
responding to the skeleton system. The edges con-
necting the final vertices of each network to the end
vertex have all system confidences set to one, so the
final edge does not change the score of any path.
A single word confidence is produced from the
confidence vector by taking an inner product with
the system weights ?n which are constrained to sum
to one,2
?
n ?n = 1. The total edge score is pro-
duced by a log-linear interpolation of the word con-
fidence with other features film:
sil = log
( Ns?
n=1
?nsiln
)
+
?
m
?mfilm (1)
The usual features film include the LM score as well
as non-NULL and NULL token indicators. Based
on an analysis of the system combination outputs, a
large number of bi-grams not present in any input
hypothesis are often produced, some of which are
clearly ungrammatical despite the LM. These novel
bi-grams are due to errors in hypothesis alignment
and the confusion network structure where any word
from the incoming edges of a vertex can be followed
by any word from the outgoing edges. After expand-
ing and re-scoring the joint lattice with a bi-gram, a
new feature indicating the presence of a novel bi-
gram may be added on the edges. A negative weight
1The confidences are binary when aligning 1-best outputs.
More elaborate confidences may be estimated fromN -best lists;
see for example Rosti et al (2007).
2See (Rosti et al, 2010) for a differentiable constraint.
for this feature discourages novel bi-grams in the
output during decoding.
3 Weight Optimization
The most common objective function used in ma-
chine translation is the BLEU-N score (Papineni et
al., 2002) defined as follows:3
BLEU =
N?
n=1
(?
i m
n
i?
i h
n
i
) 1
N
?
(
1?
?
i ri
?
i h
1
i
)
(2)
where N is the maximum n-gram order (typically
N = 4), mni is the number of n-gram matches
(clipped counts) between the hypothesis ei and ref-
erence e?i for segment i, hni is the number of n-grams
in the hypothesis, ri is the reference length,4 and
?(x) = min(1.0, ex) is the brevity penalty. Using
gn to represent an arbitrary n-gram, cign to repre-
sent the count of gn in hypothesis ei, and c?ign to
represent the count of gn in reference e?i, the BLEU
statistics can be defined as follows:
mni =
?
gn
min(cign , c?ign) (3)
hni =
?
gn
cign (4)
The unigram count h1i is simply the hypothesis
length and higher order n-gram counts can be ob-
tained by hni = h
n?1
i ? 1. The reference n-gram
counts for each sentence can be stored in an n-gram
trie for efficient scoring.5
The BLEU score is not differentiable due to the
minimum operations on the matches mni and brevity
penalty ?(x). Therefore gradient-free optimization
algorithms, such as Powell?s method or downhill
simplex (Press et al, 2007), are often employed in
weight tuning (Och, 2003). System combination
weights tuned using the downhill simplex method
to directly optimize 1-best BLEU score of the de-
coder outputs served as the first baseline in the ex-
periments. The distributed optimization approach
used here was first described in (Rosti et al, 2010).
3Superscripts indicate the n-gram order in all variables in
this paper. They are used as exponents only for the constant e.
4If multiple references are available, ri is the reference
length closest to the hypothesis length, h1i .
5If multiple references are available, the maximum n-gram
counts are stored.
160
A set of system combination weights was first tuned
for unpruned lattices re-scored with a bi-gram LM.
Another set of re-scoring weights was tuned for 300-
best lists re-scored with a 5-gram LM.
3.1 Graph expected BLEU
Gradient-free optimization algorithms work well
with a relatively small number of weights. Weight
optimization for a 44 system combination in Rosti
et al (2010) was shown to be unstable with down-
hill simplex algorithm. Instead, an N-best list based
expected BLEU tuning with gradient ascent yielded
better results. This served as the second baseline in
the experiments. The objective function is defined
by replacing the n-gram statistics with expected n-
gram counts and matches as in (Smith and Eisner,
2006), and brevity penalty with a differentiable ap-
proximation:
?(x) =
ex ? 1
1 + e1000x
+ 1 (5)
An N-best list represents a subset of the search space
and multiple decoding iterations with N-best list
merging is required to improve convergence. In this
work, expected BLEU tuning is extended for lat-
tices by replacing the minimum operation in n-gram
matches with another differentiable approximation.
The expected n-gram statistics for path j, which cor-
respond to the standard statistics in Equations 3 and
4, are defined as follows:
m?ni =
?
gn
?
( ?
j?Ji
Pijcijgn , c?ign
)
(6)
h?ni =
?
gn
?
j?Ji
Pijcijgn (7)
where Ji is the set of all paths in a lattice or all
derivations in a hypergraph for the ith source sen-
tence, Pij is the posterior of path j, and cijgn is
the count of n-grams gn in hypothesis eij on path
j. The path posterior and approximate minimum are
defined by:
Pij =
?
l?j e
?sil
?
j??Ji
?
l?j? e?sil
(8)
?(x, c) =
x? c
1 + e1000(x?c)
+ c (9)
where sil is the total score on edge l defined in Equa-
tion 1 and ? is an edge score scaling factor. The
scaling factor affects the shape of the edge posterior
distribution; ? > 1.0 makes the edge posteriors on
the 1-best path higher than edge posteriors on other
paths and ? < 1.0 makes the posteriors on all paths
more uniform.
The graph expected BLEU can be factored as
xBLEU = ePB where:
P =
1
N
N?
n=1
(
log
?
i
m?ni ? log
?
i
h?ni
)
(10)
B = ?
(
1?
?
i ri
?
i h?
1
i
)
(11)
and ri is the reference length.6 This objective func-
tion is closely related to CoBLEU (Pauls et al,
2009). Unlike CoBLEU, xBLEU is differentiable
and standard gradient ascent algorithms can be used
to find weights that maximize the objective.
Note, the expected counts can be expressed in
terms of edge posteriors as:
?
j?Ji
Pijcijgn =
?
l?Li
pil?(c
n
il, g
n) (12)
where Li is the set of all edges for the ith sentence,
pil is the edge posterior, ?(x, c) is the Kronecker
delta function which is 1 if x = c and 0 if x 6= c, and
cnil is the n-gram context of edge l. The edge posteri-
ors can be computed via standard forward-backward
algorithm for lattices or inside-outside algorithm for
hypergraphs. As with the BLEU statistics, only ex-
pected unigram counts h?1i need to be accumulated
for the hypothesis n-gram counts in Equation 7 as
h?ni = h?
n?1
i ? 1 for n > 1. Also, the expected
n-gram counts for each graph can be stored in an
n-gram trie for efficient gradient computation.
3.2 Gradient of graph expected BLEU
The gradient of the xBLEU with respect to weight ?
can be factored as:
?xBLEU
??
=
?
i
?
l?Li
?sil
??
?
j?Ji
?xBLEU
? logPij
? logPij
?sil
(13)
where the gradient of the log-path-posterior with re-
spect to the edge score is given by:
? logPij
?sil
= ?
(
?(l ? j)? pil
)
(14)
6If multiple reference are available, ri is the reference length
closest to the expected hypothesis length h?1i .
161
?xBLEU
??
= ?eP
(
B
N
N?
n=1
?
i
(
m?nik ?m
n
ik
mn
?
h?nik ? h
n
ik
hn
))
+ C??(1? C)
?
i
h?1ik ? h
1
ik
h1
(15)
and ?(l ? j) is one if edge l is on path j, and zero
otherwise. Using the factorization xBLEU = ePB,
Equation 13 can be expressed using sufficient statis-
tics as shown in Equation 15, where ??(x) is the
derivative of ?(x) with respect to x, mn =
?
i m?
n
i ,
hn =
?
i h?
n
i , C =
?
ri/
?
i h?
1
i , and the remaining
sufficient statistics are given by:
??ign = ?
?
( ?
j?Ji
Pijcijgn , c?ign
)
mnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
??igncijgn
)
m?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
??igncijgn
hnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
cijgn
)
h?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
cijgn
where ??(x, c) is the derivative of ?(x, c) with re-
spect to x, and the parentheses in the equations for
mnik and h
n
ik signify that the second terms do not de-
pend on the edge l.
3.3 Forward-backward algorithm under
expectation semiring
The sufficient statistics for graph expected BLEU
can be computed using expectation semirings (Li
and Eisner, 2009). Instead of computing single
forward/backward or inside/outside scores, addi-
tional n-gram elements are tracked for matches and
counts. For example in a bi-gram graph, the ele-
ments for edge l are represented by a 5-tuple7 sl =
?pl, r1lh, r
2
lh, r
1
lm, r
2
lm? where pl = e
?sil and:
rnlh =
?
gn
?(cnil, g
n)e?sil (16)
rnlm =
?
gn
??igne
?sil (17)
Assuming the lattice is topologically sorted, the for-
ward algorithm8 under expectation semiring for a 3-
7The sentence index i is dropped for brevity.
8For inside-outside algorithm, see (Li and Eisner, 2009).
tuple9 sl = ?pl, r1lh, r
1
lm? is defined by:
?0 = ?1, 0, 0? (18)
?v =
?
l?Iv
?u(l) ? sl (19)
where Iv is the set of all edges with target vertex
v and u(l) is the source vertex for edge l, and the
operations are defined by:
s1 ? s2 = ?p1 + p2, r
1
1h + r
1
2h, r
1
1m + r
1
2m?
s1 ? s2 = ?p1p2, p1r
1
2h + p2r
1
1h, p1r
1
2m + p2r
1
1m?
The backward algorithm for ?u can be implemented
via the forward algorithm in reverse through the
graph. The sufficient statistics for the gradient can
be accumulated during the backward pass noting
that:
?
j?Ji
Pij
?
gn
??igncijgn =
rnm(?0)
p(?0)
(20)
?
j?Ji
Pij
?
gn
cijgn =
rnh(?0)
p(?0)
(21)
where rnm(?) and r
n
h(?) extract the nth order r ele-
ments from the tuple for matches and counts, respec-
tively, and p(?) extracts the p element. The statistics
for the paths traveling via edge l can be computed
by:
?
j:l?Ji
Pij
?
gn
??igncijgn =
rnm(?u ? sl ? ?v)
p(?0)
(22)
?
j:l?Ji
Pij
?
gn
cijgn =
rnh(?u ? sl ? ?v)
p(?0)
(23)
where the u and v subscripts in ?u and ?v are the
start and end vertices for edge l. To avoid under-
flow, all the computations can be carried out in log
domain.
9A 3-tuple for uni-gram counts is used as an example in or-
der to save space. In a 5-tuple for bi-gram counts, all r elements
are computed independently of other r elements with the same
operations. Similarly, tri-gram counts require 7-tuples and four-
gram counts require 9-tuples.
162
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 66.03 18.09 69.03 16.28 60.56 21.02 62.75 21.83
best 53.75 28.36 58.39 24.28 50.26 30.55 50.48 30.87
latBLEU 53.99 29.25 56.70 26.49 48.34 34.55 48.90 33.90
nbExpBLEU 54.43 29.04 56.36 27.33 48.44 34.73 48.58 34.23
latExpBLEU 53.89 29.37 56.24 27.36 48.27 34.93 48.53 34.24
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 65.35 17.69 69.03 15.83 61.22 19.79 62.36 21.36
best 52.21 29.54 58.00 24.16 50.15 30.14 50.15 30.32
latBLEU 52.80 29.89 55.87 26.22 48.29 33.91 48.51 32.93
nbExpBLEU 52.97 29.93 55.77 26.52 48.39 33.86 48.25 32.94
latExpBLEU 52.68 29.99 55.74 26.62 48.30 34.10 48.17 32.91
Table 1: Case insensitive TER and BLEU scores on newssyscombtune (tune) and newssyscombtest (test)
for combinations of outputs from four source languages. Three tuning methods were used: lattice BLEU (latBLEU),
N-best list based expected BLEU (nbExpBLEU), and lattice expected BLEU (latExpBLEU).
3.4 Entropy on a graph
Expanding the joint lattice to n-gram orders above
n = 2 is often impractical without pruning. If the
edge posteriors are not reliable, which is usually
the case for unoptimized weights, pruning might re-
move good quality paths from the graph. As a com-
promise, an incremental expansion strategy may be
adopted by first expanding and re-scoring the lattice
with a bi-gram, optimizing weights for xBLEU-2,
and then expanding and re-scoring the lattice with
a 5-gram. Pruning should be more reliable with the
edge posteriors computed using the tuned bi-gram
weights. A second set of weights may be tuned with
the 5-gram graph to maximize xBLEU-4.
When the bi-gram weights are tuned, it may be
beneficial to increase the edge score scaling factor
to focus the edge posteriors to the 1-best path. On
the other hand, a lower scaling factor may be bene-
ficial when tuning the 5-gram weights. Rosti et al
(2010) determined the scaling factor automatically
by fixing the perplexity of the merged N -best lists
used in tuning. Similar strategy may be adopted in
incremental n-gram expansion of the lattices.
Entropy on a graph can also be computed using
the expectation semiring formalism (Li and Eisner,
2009) by defining sl = ?pl, rl? where pl = e?sil and
rl = log pl. The entropy is given by:
Hi = log p(?0)?
r(?0)
p(?0)
(24)
where p(?0) and r(?0) extract the p and r elements
from the 2-tuple ?0, respectively. The average target
entropy over all sentences was set manually to 3.0
in the experiments based on the tuning convergence
and size of the pruned 5-gram lattices.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined (cz-en,
de-en, es-en, and fr-en). Unpruned English
bi-gram and 5-gram language model compo-
nents were trained using the WMT11 corpora:
EuroParl, GigaFrEn, UNDoc Es, UNDoc Fr,
NewsCommentary, News2007, News2008,
News2009, News2010, and News2011.
Additional six Gigaword v4 components in-
cluded: AFP, APW, XIN+CNA, LTW, NYT, and
Headlines+Datelines. The total number
of words used to train the LMs was about 6.4
billion. Interpolation weights for the sixteen
components were tuned to minimize perplexity on
the newstest2010-ref.en development set.
The modified Kneser-Ney smoothing (Chen and
163
Goodman, 1998) was used in training. Experiments
using a LM trained on the system outputs and inter-
polated with the general LM were also conducted.
The interpolation weights between 0.1 and 0.9 were
tried, and the weight yielding the highest BLEU
score on the tuning set was selected. A tri-gram true
casing model was trained on all the LM training
data. This model was used to restore the case of the
lower-case system combination output.
All twelve 1-best system outputs on cz-en, 26
outputs on de-en, 16 outputs on es-en, and 24
outputs on fr-en were combined. Three different
weight optimization methods were tried. First, lat-
tice based 1-best BLEU optimization of the bi-gram
decoding weights followed by N-best list based
BLEU optimization of 5-gram re-scoring weights
using 300-best lists, both using downhill simplex.
Second, N-best list based expected BLEU optimiza-
tion of the bi-gram and 5-gram weights using 300-
best lists with merging between bi-gram decoding
iterations. Third, lattice based expected BLEU opti-
mization of bi-gram and 5-gram decoding weights.
The L-BFGS (Liu and Nocedal, 1989) algorithm
was used in gradient ascent. Results for all four sin-
gle source experiments are shown in Table 1, includ-
ing case insensitive TER (Snover et al, 2006) and
BLEU scores for the worst and best systems, and
the system combination outputs for the three tuning
methods. The gains on tuning and test sets were con-
sistent, though relatively smaller on cz-en due to
a single system (online-B) dominating the other
systems by about 5-6 BLEU points. The tuning
method had very little influence on the test set scores
apart from de-en where the lattice BLEU opti-
mization yields slightly lower BLEU scores. This
seems to suggest that the gradient free optimization
is not as stable with a larger number of weights.10
The novel bi-gram feature did not have significant
influence on the TER or BLEU scores, but the num-
ber of novel bi-grams was reduced by up to 100%.
Finally, experiments combining 39 system out-
puts by taking the top half of the outputs from each
language pair were performed. The selection was
based on case insensitive BLEU scores on the tun-
ing set. Table 2 shows the scores for seven combi-
10A total number of 30 weights, 26 system and 4 feature
weights, were tuned for de-en.
xx-en tune test
System TER BLEU TER BLEU
worst 62.81 21.19 62.92 20.29
best 51.11 30.87 50.80 30.32
latBLEU 40.95 40.75 41.06 39.81
+biasLM 41.18 40.90 41.16 39.90
nbExpBLEU 40.81 41.36 41.05 40.15
+biasLM 40.72 41.99 40.65 40.89
latExpBLEU 40.57 41.68 40.62 40.60
+biasLM 40.42 42.23 40.52 41.38
-nBgF 40.85 41.41 40.88 40.55
Table 2: Case insensitive TER and BLEU scores on
newssyscombtune (tune) and newssyscombtest
(test) for xx-en combination. Combinations using lat-
tice BLEU tuning (latBLEU), N-best list based expected
BLEU tuning (nbExpBLEU), and lattice expected BLEU
tuning (latExpBLEU) with and without the system out-
put biased LM (biasLM) are shown. Final row, marked
nBgF, corresponds to the above tuning without the novel
bi-gram feature.
nations using the three tuning methods with or with-
out the system output biased LM, and finally without
the novel bi-gram count feature. There is a clear ad-
vantage from the expected BLEU tuning on the tun-
ing set, and lattice tuning yields better scores than
N-best list based tuning. The difference between
latBLEU and nbExpBLEU without biasLM is
not quite as large on the test set but latExpBLEU
yields significant gains over both. The biasLM also
yields significant gains on all but latBLEU tuning.
Finally, removing the novel bi-gram count feature
results in a significant loss, probably due to the large
number of input hypotheses. The number of novel
bi-grams in the test set output was reduced to zero
when using this feature.
5 Conclusions
The BBN submissions for WMT11 system combi-
nation task were described in this paper together
with a differentiable objective function, graph ex-
pected BLEU, which scales well for a large number
of weights and can be generalized to hypergraphs.
System output biased language model and a novel
bi-gram count feature also gave significant gains on
a 39 system multi-source combination.
164
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Computer Science
Group Harvard University.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 40?51.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system combi-
nation for machine translation. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothe-
sis alignment with flexible matching for building con-
fusion networks: BBN system description for WMT09
system combination task. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
61?65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for WMT10 system combination task. In Proceedings
of the Fifth Workshop on Statistical Machine Transla-
tion, pages 321?326.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 787?
794.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
165
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
