Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 274?283,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Who is Who and What is What:  
Experiments in Cross-Document Co-Reference  
Alex Baron 
BBN Technologies 
10 Moulton Street 
Cambridge, MA 02138 
abaron@bbn.com 
Marjorie Freedman  
BBN Technologies 
10 Moulton Street 
Cambridge, MA 02138 
mfreedma@bbn.com 
 
Abstract 
This paper describes a language-independent, 
scalable system for both challenges of cross-
document co-reference: name variation and 
entity disambiguation. We provide system re-
sults from the ACE 2008 evaluation in both 
English and Arabic. Our English system?s ac-
curacy is 8.4% relative better than an exact 
match baseline (and 14.2% relative better over 
entities mentioned in more than one docu-
ment). Unlike previous evaluations, ACE 
2008 evaluated both name variation and entity 
disambiguation over naturally occurring 
named mentions.  An information extraction 
engine finds document entities in text. We de-
scribe how our architecture designed for the 
10K document ACE task is scalable to an 
even larger corpus.  Our cross-document ap-
proach uses the names of entities to find an 
initial set of document entities that could refer 
to the same real world entity and then uses an 
agglomerative clustering algorithm to disam-
biguate the potentially co-referent document 
entities. We analyze how different aspects of 
our system affect performance using ablation 
studies over the English evaluation set. In ad-
dition to evaluating cross-document co-
reference performance, we used the results of 
the cross-document system to improve the ac-
curacy of within-document extraction, and 
measured the impact in the ACE 2008 within-
document evaluation.  
1 Introduction 
Cross-document entity co-reference is the problem 
of identifying whether mentions from different 
documents refer to the same or distinct entities. 
There are two principal challenges: the same entity 
can be referred to by more than one name string 
(e.g. Mahmoud Abbas and Abu Mazen) and the 
same name string can be shared by more than one 
entity (e.g. John Smith). Algorithms for solving the 
cross-document co-reference problem are neces-
sary for systems that build knowledge bases from 
text, question answering systems, and watch list 
applications.  
There are several challenges in evaluating and 
developing systems for the cross-document co-
reference task. (1) The annotation process required 
for evaluation and for training is expensive; an an-
notator must cluster a large number of entities 
across a large number of documents. The annotator 
must read the context around each instance of an 
entity to make reliable judgments. (2) On randomly 
selected text, a baseline of exact string match will 
do quite well, making it difficult to evaluate pro-
gress. (3) For a machine, there can easily be a scal-
ability challenge since the system must cluster a 
large number of entities.  
Because of the annotation challenges, many 
previous studies in cross-document co-reference 
have focused on only the entity disambiguation 
problem (where one can use string retrieval to col-
lect many documents that contain same name); or 
have used artificially ambiguated data. 
Section 2 describes related work; section 3 in-
troduces ACE, where the work was evaluated; sec-
tion 4 describes the underlying information 
extraction engine; sections 5 and 6 address the 
challenges of coping with name variation and dis-
ambiguating entities; sections 7, 8, and 9 present 
empirical results, improvement of entity extraction 
274
within documents using cross-document corefer-
ence, and a difference in performance on person 
versus organization entities. Section 10 discusses 
the scalability challenge. Section 11 concludes.  
2 Related Work 
Person disambiguation given a person name 
string. Bagga and Baldwin (1998b) produced one 
of the first works in cross-document co-reference. 
Their work presented a vector space model for the 
problem of entity disambiguation, clustering 197 
articles that contained the name ?John Smith?.  
Participants in the 2007 Sem-Eval Web People 
Search(WEPS) task clustered 100-document sets 
based on which person a name string of interest 
referenced. WEPS document sets were collected 
by selecting the top 100 web search results to que-
ries about a name string (Artiles, et al, 2007).  
Mann and Yarowsky (2003) and Gooi and 
Allan (2004) used artificially ambiguous data to 
allow for much larger experiments in clustering 
documents around a known person of interest.  
Clustering different variants of the same name. 
Lloyd et. al (2006) use a combination of ?morpho-
logical similarity? and ?contextual similarity? to 
cluster name variants that refer to the same entity.  
Clustering and disambiguation. The John Hop-
kins 2007 Summer Workshop produced a cross-
document annotated version of the ACE 2005 cor-
pus (18K document entities, 599 documents) con-
sisting of 5 entity types (Day, et. al, 2007). There 
was little ambiguity or variation in the corpus. Par-
ticipants demonstrated that disambiguation im-
provements could be achieved with a Metropolis-
Hastings clustering algorithm. The study assumed 
human markup of document-level entities. 
Our work. The work reported in this paper ad-
dresses both entity clustering and name variation 
for both persons and organizations in a corpus of 
10K naturally occurring documents selected to be 
far richer than the ACE 2005 data by NIST and 
LDC. We investigated a new approach in both 
English and Arabic, and evaluated on document-
level entities detected by information extraction. 
3 ACE Evaluation 
NIST?s ACE evaluation measures system perform-
ance on a predetermined set of entities, relations, 
and events. For the 2008 global entity detection 
and recognition task (GEDR)1, system perform-
ance was measured on named instances of person 
and organization entities. The GEDR task was run 
over both English and Arabic documents. Partici-
pants processed over 10K documents for each lan-
guage. References were produced for about 400 
documents per language (NIST, 2008). The evalua-
tion set included documents from several genres 
over a 10 year time period. Document counts are 
provided in Table 1. This evaluation differed from 
previous community cross-document coreference 
evaluations in that it (a) covered both organizations 
and people; (b) required processing a relatively 
large data set; (c) evaluated entity disambiguation 
and name variation simultaneously; and (d) meas-
ured cross-document co-reference over system-
detected document-level entities and mentions.  
 
 English Arabic 
broadcast conversation 8 38 
broadcast news  72 19 
meeting  18 --- 
newswire 237 314 
telephone 18 12 
usenet 15 15 
weblog 47 14 
Table 1: Documents per genre in ACE2008 test set 
 
The evaluation set was selected to include in-
teresting cases for cross-document co-reference 
(e.g cases with spelling variation and entities with 
shared names). This is necessary because annota-
tion is difficult to produce and naturally sampled 
data has a high percentage of entities resolvable 
with string match. The selection techniques were 
unknown to ACE participants.  
4 Extraction System Overview 
Our cross-document co-reference system relies on 
SERIF, a state-of-the-art information extraction 
(IE) system (Ramshaw, et. al, 2001) for document-
level information extraction. The IE system uses 
statistically trained models to detect and classify 
mentions, link mentions into entities, and detect 
and classify relations and events. English and Ara-
bic SERIF share the same general models, al-
though there are differences in the specific features 
used by the models.  Arabic SERIF does not per-
form event detection. While Arabic SERIF does 
                                                          
1
 NIST?s evaluation of cross-document co-reference. 
275
make use of some morphological features, the 
cross-document co-reference system, which fo-
cused specifically on entity names, does not use 
these features.   
Figure 1 and Figure 2 illustrate the architecture 
and algorithms of the cross-document co-reference 
system respectively. Our system separately ad-
dresses two aspects of the cross-document co-
reference problem: name variation (Section  5) and 
entity disambiguation (Section  6). This leads to a 
scalable solution as described in Section  10. 
 
Figure 1: Cross-document Co-reference Architechure 
 
The features used by the cross-document co-
reference system can be divided into four classes: 
World Knowledge (W), String Similarity (S), Pre-
dictions about Document Context (C), and Meta-
data (M). Name variation (V) features operate over 
unique corpus name strings. Entity disambiguation 
features (D) operate over document-level entity 
instances. During disambiguation, the agglomera-
tive clustering algorithm merges two clusters when 
conditions based on the features are met. For ex-
ample, two clusters are merged when they share at 
least half the frequently occurring nouns that de-
scribe an entity (e.g. president).  As shown in 
Table 2, features from the same class were often 
used in both variation and disambiguation. All 
classes of features were used in both English and 
Arabic. Because very little training data was avail-
able, both the name variation system and the dis-
ambiguation system use manually tuned heuristics 
to combine the features. Tuning was done using 
the ACE2008 pilot data (LDC, 2008b), documents 
from the SemEval WEPS task (Artiles, et al, 
2007), and some internally annotated documents. 
Internal annotation was similar in style to the 
WEPS annotation and did not include full ACE 
annotation. Annotators simply clustered documents 
based on potentially confusing entities. Internal 
annotation was done for ~100 names in both Eng-
lish and Arabic. 
Feature Class Stage Class 
Wikipedia knowledge D, V W 
Web-mined aliases V W 
Word-based similarity  D, V S 
Character-based similarity V S 
Translation dictionaries V S 
Corpus Mined Aliases D, V C 
SERIF extraction D,V C 
Predicted Document Topics D C 
Metadata (source, date, etc.) D M 
Table 2: Features for Cross-Document Co-Reference 
5 Name Variation 
The name variation component (Block 1 of Figure 
1) collects all name strings that appear in the 
document set and provides a measure of similarity 
between each pair of name strings.2 Regions (A) 
and (B) of Figure 2 illustrate the input and output 
of the name variation component.  
This component was initially developed for 
question answering applications, where when 
asked the question ?Who is George Bush?? relevant 
answers can refer to both George W and George 
HW (the question is ambiguous). However when 
asked ?Who leads al Qaeda?? the QA system must 
be able to identify spelling variants for the name al 
Qaeda. For the cross-document co-reference prob-
lem, separating the name variation component 
from the disambiguation component improves the 
scalability of the system (described in Section  10). 
The name variation component makes use of a 
variety of features including web-mined alias lists, 
aliases mined from the corpus (e.g ?John aka J?), 
statistics about the relations and co-reference deci-
sions predicted by SERIF, character-based edit 
distance, and token subset trees. The token subset 
trees algorithm measures similarity using word 
overlap by building tree-like structures from the 
unique corpus names based on overlapping tokens. 
Translation dictionaries (pulled from machine 
                                                          
2
 For the majority of pairs, this similarity score will be 0.  
Input 
Documents 
IE System 
Cross-Document  
Name Variation 
Entity 
Featurizer 
 
Name Similarity 
DB 
 
Entity-based 
Feature DB 
 
Clusters DB 
World 
Knowledge DB 
Output 
Documents 
Entity 
Disambiguation 
 
Information 
Extraction DB 
(1) 
 
(2) 
 
276
translation training and cross-language links in 
Wikipedia) account for names that have a canoni-
cal form in one language but may appear in many 
forms in another language.   
 
 
Figure 2: Cross-document Co-reference Process 
 
The features are combined with hand-tuned 
weights resulting in a unidirectional similarity 
score for each pair of names. The similarity be-
tween two name strings is also influenced by the 
similarity between the contexts in which the two 
names appear (for example the modifiers or titles 
that precede a name). This information allows the 
system to be more lenient with edit distance when 
the strings appear in a highly similar context, for 
example increasing the similarity score between 
?Iranian President Ahmadinejad? and ?Iranian 
President Nejad.? 
6 Entity Disambiguation  
We use a complete link agglomerative cluster-
ing algorithm for entity disambiguation. To make 
agglomerative clustering feasible over a 10K 
document corpus, rather than clustering all docu-
ment-level entities together, we run agglomerative 
clustering over subsets of the corpus entities. For 
each name string, we select the set of names that 
the variation component chose as valid variants. In 
Figure 2 region C, we have selected Mahmoud 
Abbas and 3 variants.  
We then run a three stage agglomerative clus-
tering algorithm over the set of document entities 
that include any of the name string variants or the 
original name. Figure 2 region D illustrates three 
document-level entities. 
The name variation links are not transitive, and 
therefore a name string can be associated with 
more than one clustering instance. Furthermore 
document-level entities can include more than one 
name string. However once a document-level en-
tity has been clustered, it remains linked to entities 
that were a part of that initial clustering. Because 
of this, the order in which the algorithm selects 
name strings is important. We sort the name strings 
so that those names about which we have the most 
information and believe are less likely to be am-
biguous are clustered first. Name strings that are 
more ambiguous or about which less information is 
available are clustered later.  
 The clustering procedure starts by initializing 
singleton clusters for each document entity, except 
those document entities that have already partici-
pated in an agglomerative clustering process. For 
those entities that have already been clustered, the 
clustering algorithm retrieves the existing clusters.  
The merging decisions are based on the similar-
ity between two clusters as calculated through fea-
ture matches. Many features are designed to 
capture the context of the document in which enti-
ties appear. These features include the document 
topics (as predicted by the unsupervised topic de-
tection system (Sista, et al, 2002), the publication 
date and source of a document, and the other 
names that appear in the document (as predicted by 
SERIF).  Other features are designed to provide 
information about the specific context in which an 
entity appears for example: the noun phrases that 
refer to an entity and the relationships and events 
in which an entity participates (as predicted by 
SERIF).  Finally some features, such as the 
uniqueness of a name in Wikipedia are designed to 
provide the disambiguation component with world 
knowledge about the entity. Since each cluster 
represents a global entity, as clusters grow through 
merges, the features associated with the clusters 
expand. For example, the set of associated docu-
ment topics the global entity participates in grows.   
While we have experimented with statistically 
learning the threshold for merging, because of the 
small amount of available training data, this 
threshold was set manually for the evaluation.  
Abu Abbas, Abu Mazen, Adam Smith, 
A Smith, Andy Smith, Mahmoud Abbas,  
Muhammed Abbas ?. 
(A) Name Strings:  
(B) Name String 
Pairs with Score:  
0.9 Mahmoud AbbasAbu Mazen 
0.7 Mahmoud AbbasAbu Abbas 
0.8 Mahmoud AbbasMuhammad Abbas  
 ?.  
(C) Set of Equivalent 
Name Strings:  
Abu Mazen,  
Mahmoud Abbas,  
 Muhammed Abbas,  
Abu Abbas 
(D) Document Entity  
Mentions:  
Palestinian President Mahmoud Abbas ... Abbas said 
Abu Abbas was arrested ? Abbas hijacked  
? election of Abu Mazen 
? 
(E) Entity Clusters:  
Abu Mazen 
Mahmoud Abbas 
Palestinian Leader 
convicted terrorist 
Muhammed Abbas  
Abu Abbas 
277
Clustering over these subsets of similar strings 
has the additional benefit of limiting the number of 
global decisions that are affected by a mistake in 
the within-document entity linking. For example, if 
in one document, the system linked Hillary Clinton 
to Bill Clinton; assuming that the two names are 
not chosen as similar variants, we are likely to end 
up with a cluster made largely of mentions of 
Hillary with one spurious mention of Bill and a 
separate cluster that contains all other mentions of 
Bill. In this situation, an agglomerative clustering 
algorithm that linked over the full set of document-
level entities is more likely to be led astray and 
create a single ?Bill and Hillary? entity. 
7 Experimental Results 
Table 3 and Table 4  include preliminary ACE 
results3 for the highest, lowest, and average system 
in the local and cross-document tasks respectively. 
While a single participant could submit more than 
one entry, these numbers reflect only the primary 
submissions. The ACE scorer maps system pro-
duced entities to reference entities and produces 
several metrics. For the within-document task, 
metrics include ACE Value, B3, and a variant of 
B3 weighted to reflect ACE value weightings.  For 
the cross-document task, the B3 metric is replaced 
with F (NIST, 2008). ACE value has traditionally 
been the official metric of the ACE evaluation. It 
puts a higher cost on certain classes of entities (e.g. 
people are more important than facilities), certain 
classes of mentions (e.g. names are more important 
than pronouns), and penalizes systems for mistakes 
in type and subtype detection as well as linking 
mistakes. Assigning a mention to the wrong entity 
is very costly in terms of value score. If the men-
tion is a name, a system is penalized 1.0 for the 
missed mention and an additional 0.75 for a men-
tion false alarm. We will report ACE Value and 
value weighted B3/F. Scores on the local task are 
not directly comparable to scores on the global 
task. The local entity detection and recognition 
task (LEDR) includes entity detection for five 
(rather than two) classes of entities and includes 
pronoun and nominal (e.g. ?the group?) mentions in 
addition to names. 
 
                                                          
3
 Results in this paper use v2.1 of the references and v17 of 
the ACE scorer. Final results will be posted to 
http://www.nist.gov/speech/tests/ace/2008/ 
 English Arabic 
 Val B3Val Val B3Val 
Top 52.6 71.5 43.6 69.1 
Average -53.3 50.0 17.3 47.6 
Low4 -269.1 25.8 -9.1 26.1 
BBN-A-edropt 52.1 71.5 43.0 68.9 
BBN-B-st-mg 52.6 71.5 43.6 69.1 
BBN-B-st-mg-
fix5 
57.2 77.4 44.6 71.3 
Table 3: ACE 2008 Within-Document Results (LEDR) 
 
 English Arabic 
 Val FVal Val FVal 
Top 53.0 73.8 28.2 58.7 
Average 21.1 59.1 24.7 56.8 
Low -64.1 31.6 21.2 54.8 
BBN-B-med 53.0 73.8 28.2 58.7 
BBN-B-low 53.2 73.8 28.7 59.3 
BBN-B-med-fix5 61.7 77 31.4 60.1 
Table 4: ACE 2008 Cross-Document Results (GEDR) 
 
Our cross-document co-reference system used 
BBN-A-edropt as input. BBN-B-st-mg is the result 
of using cross-document co-reference to improve 
local results (Section  9). For cross-document co-
reference, our primary submission, BBN-B-med, 
was slightly outperformed by an alternate system 
BBN-B-low. The two submissions differed only in 
a parameter setting for the topic detection system 
(BBN-B-low requires more documents to predict a 
?topic?). BBN-A-st-mg-fix and BBN-B-med-fix 
are the result of post-processing the BBN output to 
account for a discrepancy between the training and 
evaluation material.5   
In addition to releasing results, NIST also re-
leased the references. Table 5 includes the ACE 
score for our submitted English system and the 
score when the system was run over only the 415 
documents with references. The system performs 
slightly better when operating over the full docu-
ment set. This suggests that the system is using 
information from the corpus even when it is not 
directly scored.  
                                                          
4
 There was a swap in rank between metrics, so the low num-
bers reflect two different systems.   
5
 There were discrepancies between the ACE evaluation and 
training material with respect to the portions of text that 
should be processed.  Therefore our initial system included a 
number of spurious entities. NIST has accepted revised output 
that removes these entities. Experiments in this paper reflect 
the corrected system.   
278
  
 FVal 
10K documents processed (415 scored) 
(BBN-B-med-fix) 
77 
Only 415 documents processed 76.3 
Table 5: Full English System ACE Evaluation Results 
 
We have run a series of ablation experiments 
over the 415 files in the English test set to evaluate 
the effectiveness of different feature classes. These 
experiments were run using only the annotated 
files (and not the full 10K document set). We ran 
two simple baselines. The first baseline (?No 
Link?) does not perform any cross-document co-
reference, all document entities are independent 
global entities. The second baseline (?Exact 
Match?) links document-level entities using exact 
string match. We ran 6 variations of our system: 
o Configuration 1 is the most limited system. It 
uses topics and IE system output for disambigua-
tion, and aliases mined from the documents for 
the name variation component.  
o Configuration 2 includes Configuration 1 fea-
tures with the addition of string similarity (edit 
distance, token subset trees) algorithms for the 
name variation stage.  
o Configuration 3 includes Configuration 2 fea-
tures and adds context-based features (e.g. titles 
and premodifiers) for name variation.  
o Configuration 4 adds information from docu-
ment metadata to the disambiguation component.  
o Configuration 5 adds web-mined information 
(alias lists, Wikipedia, etc.) to both the variation 
and disambiguation components. This is the con-
figuration that was used for our NIST submission.  
o Configuration 5a is identical to Configuration 
5 except that the string-based edit distance was 
removed from the name variation component.  
As noted previously, the ACE collection was 
selected to include challenging entities. The selec-
tion criteria of the corpus (which are not known by 
ACE participants) can affect the importance of fea-
tures. For example, a corpus that included very few 
transliterated names would make less use of fea-
tures based on edit distance.  
Figure 3 and Figure 4 show performance (with 
value weighted F) on the eight conditions over sys-
tem predicted within-document extraction and ref-
erence within-document extraction respectively. 
Figure 3 also includes configuration 5 run over all 
10K documents. We provide two sets of results. 
The first evaluates system performance over all 
entities. The relatively high score of the ?No Link? 
baseline indicates that a high percentage of the 
document-level entities in the corpus are only men-
tioned in one document. The second set of num-
bers measures system performance on those 
entities appearing in more than one reference 
document. While this metric does not give a com-
plete picture of the cross-document co-reference 
task (sometimes a singleton entity must be disam-
biguated from a large entity that shares the same 
name); it does provide useful insights given the 
frequency of singleton entities. 
System Document Level Entities
30
40
50
60
70
80
90
100
Sp
lit A
ll
Ex
ac
t M
atc
h 1 2 3 4 5a 5
5 (1
0k
 
do
cs
)
Configuration
Va
lu
e 
W
e
ig
ht
ed
 
F
All Entities
Entities in > 1
Documents
 
Figure 3: Performance on System Document Entities 
 
Reference Document Level Entities
30
40
50
60
70
80
90
100
Split All Exact
Match
1 2 3 4 5a 5
Configuration
Va
lu
e
 
W
e
ig
ht
ed
 
F
All Entities
Entities in >1
Documents
 
Figure 4: Performance on Perfect Document Entities 
 
Overall system performance improved as fea-
tures were added. Configuration 1, which disam-
biguated entities with a small set of features, 
performed worse than a more aggressive exact 
string match strategy. The nature of our agglom-
erative clustering algorithm leads to entity merges 
only when there is sufficient evidence for the 
merge. The relatively high performance of the ex-
act match strategy suggests that in the ACE corpus, 
most entities that shared a name string referred to 
279
the same entity, and therefore aggressive merging 
leads to better performance. As additional features 
are added, our system becomes more confident and 
merges more document-level entities.  
With the addition of string similarity measures 
(Configuration 2) our system outperforms the exact 
match baseline. The submitted results on system 
entities (Configuration 5) provide a 8.4% relative 
reduction in error over the exact match baseline. If 
scored only on entities that occur in more than one 
document, Configuration 5 gives a 14.2% relative  
redution in error over the exact match baseline.  
The context based features (Configuration 3) al-
low for more aggressive edit-distance-based name 
variation when two name strings frequently occur 
in the same context. In Configuration 3, ?Sheik 
Hassan Nasrallah? was a valid variant of ?Hassan 
Nasrallah? because both name strings were com-
monly preceded by ?Hezbollah leader?. Similarly, 
?Dick Cheney? became a valid variant of ?Richard 
Bruce Cheney? because both names were preceded 
by ?vice president?. In Configuration 2 the entities 
included in both sets of name strings had remained 
unmerged because the strings were not considered 
valid variants. With the addition of contextual in-
formation (Configuration 3), the clustering algo-
rithm created a single global entity. For the ?Dick 
Cheney? cluster, this was correct. ?Sheik Hassan 
Nassrallah? was a more complex instance, in some 
cases linking was correct, in others it was not.  
The impact of the metadata features (Configu-
ration 4) was both positive and negative. An article 
about the ?Arab League Secretary General Amru 
Moussa? was published on the same day in the 
same source as an article about ?Intifada Fatah 
movement leader Abu Moussa?. With the addition 
of metadata features, these two distinct global enti-
ties were merged. However, the addition of meta-
data features correctly led to the merging of three 
instances of the name ?Peter? in ABC news text 
(all referring ABC?s Peter Jennings).  
Web-mined information (Configuration 5) pro-
vides several variation and disambiguation fea-
tures. As we observed, the exact match baseline 
has fairly high accuracy but is obviously also too 
aggressive of a strategy. However, for certain very 
famous global entities, any reference to the name 
(especially in corpora made of primarily news text) 
is likely to be a reference to a single global entity. 
Because these people/organizations are famous, 
and commonly mentioned, many of the topic and 
extraction based features will provide insufficient 
evidence for merging. The same famous person 
will be mentioned in many different contexts. We 
use Wikipedia as a resource for such entities. If a 
name is unambiguous in Wikipedia, then we merge 
all instances of this name string. In the evaluation 
corpus, this led to the merging of many different 
instances of ?Osama Bin Laden? into a single en-
tity. Web-mined information is also a resource for 
aliases and acronyms. These alias lists, allowed us 
to merge ?Abu Muktar? with ?Khadafi Montanio? 
and ?National Liberation Army? with ?ELN?. 
Interestingly, removing the string edit distance 
algorithm (System 5a), is a slight improvement 
over System 5. Initial error analysis has shown that 
while the string edit distance algorithm did im-
prove accuracy on some entities (e.g linking ?Sam 
Alito? with ?Sam Elito? and linking ?Andres Pas-
trana? with ?Andreas Pastrana?); in other cases, 
the algorithm allowed the system to overlink two 
entities, for example linking ?Megawati Soekar-
noputri? and her sister ?Rachmawati Sukarnoputri?.  
8 Improving Document-Level Extraction 
with Global Information  
In addition to evaluating the cross-document sys-
tem performance on the GEDR task, we ran a pre-
liminary set of experiments using the cross-
document co-reference system to improve within-
document extraction. Global output modified 
within-document extraction in two ways. 
First, the cross-document co-reference system 
was used to modify the within-document system?s 
subtype classification. In addition to evaluating 
entity links and type classification, the ACE task 
measures subtype classification. For example, for 
organization entities, systems distinguish between 
Media and Entertainment organizations. The IE 
system uses all mentions in a given entity to assign 
a subtype. The cross-document co-reference sys-
tem has merged several document-level entities, 
and therefore has even more information with 
which to assign subtypes. The cross-document sys-
tem also has access to a set of manual labels that 
have been assigned to Wikipedia categories.  
Secondly, we used the cross-document co-
reference system?s linking decisions to merge 
within-document entities. If the cross-document 
co-reference system merged two entities in the 
280
same document, then those entities were merged in 
the within-document output.  
Table 6 includes results for our within-
document IE system, the IE system with improved 
subtypes, and the IE system with improved sub-
types and merged entities.  
 
 B3Val Val 
Local 77.3 56.7 
+ Subtypes 77.3 56.9 
+ Merge 77.4 57.2 
Table 6: Within-document Results 
 
While these preliminary experiments yield rela-
tively small improvements in accuracy, an analysis 
of the system?s output suggests that the merging 
approach is quite promising. The output that has 
been corrected with global merges includes the 
linking entities with ?World Knowledge? acronyms 
(e.g. linking ?FARC? with ?Armed Revolutionary 
Forces of Colombia?); linking entities despite 
document-level extraction mistakes (e.g. ?Lady 
Thatcher? with ?Margaret Thatcher?); and linking 
entities despite spelling mistakes in a document 
(e.g linking ?Avenajado? with ?Robert Aventa-
jado?). However, as we have already seen, the 
cross-document co-reference system does make 
mistakes and these mistakes can propagate to the 
within-document output.  
In particular, we have noticed that the cross-
document system has a tendency to link person 
names with the same last name when both names 
appear in a single document. As we think about the 
set of features used for entity disambiguation, we 
can see why this would be true. These names may 
have enough similarity to be considered equivalent 
names. Because they appear in the same document, 
they will have the same publication date, document 
source, and document topics. Adjusting the cross-
document system to either use a slightly different 
approach to cluster document-level entities from 
the same document or at the very least to be more 
conservative in applying merges that are the result 
primarily of document metadata and context to the 
within-document output could improve accuracy.  
9 Effect of LEDR on GEDR 
Unlike previous evaluations of cross-document co-
reference performance, the ACE 2008 evaluation 
included both person and organization entities. We 
have noticed that the performance of the cross-
document co-reference system on organizations 
lags behind the performance of the system on peo-
ple. In contrast, for LEDR, the extraction system?s 
performance is quite similar between the two entity 
classes. Furthermore, the difference between 
global organization and person accuracy in the 
GEDR is smaller when the GEDR task performed 
with perfect document-level extraction. Scores are 
shown in Table 7. These differences suggest that 
part of the reason for the low performance on or-
ganizations in GEDR is within-document accuracy.  
 
 
LEDR GEDR-  
System 
GEDR-
Perfect 
 B3Val Val FVal Val FVal Val 
Org 75.1 51.7 67.8 45.9 91.5 84.0 
Per 76.2 52.9 83.2 71.4 94.3 89.5 
Table 7: Performance on ORG and PER Entities 
 
The LEDR task evaluates names, nominals, and 
pronouns. GEDR, however only evaluates over 
name strings. To see if this was a part of the differ-
ence in accuracy, we removed all pronoun and 
nominal mentions from both the IE system?s local 
output and the reference set. As shown in Table 8, 
the gap in performance between organizations and 
people is much larger in this setting.  
 
 LEDR- Name Only 
 B3Val Val 
ORG 82.6 83.0 
PER 90.1 90.4 
Table 8: Local Performance on Name Only Task 
 
Because the GEDR task focuses exclusively on 
names and excludes nominals and pronouns, mis-
takes in mention type labeling (e.g. labeling a 
name as a nominal) become misses and false 
alarms rather than type substitutions. As the task is 
currently defined, type substitutions are much less 
costly than a missing or false alarm entity.  
Intuitively, correctly labeling the name of a per-
son as a name and not a nominal is simple. The 
distinction for organizations may be fuzzier. For 
example the string ?the US Department of Justice? 
could conceivably contain one name, two names, 
or a name and a nominal. The ACE guidelines 
(LDC, 2008a) suggest that this distinction can be 
difficult to make, and in fact have a lengthy set of 
rules for classifying such cases. However, these 
rules can seem unintuitive, and may be difficult for 
machines to learn. For example ?Justice Depart-
ment? is not a name but ?Department of Justice? is. 
In some sense, this is an artificial distinction en-
forced by the task definition, but the accuracy 
281
numbers suggest that the distinction has a negative 
effect on system evaluation.  
10 Scalability 
One of the challenges for systems participating 
in the ACE task was the need to process a rela-
tively large document set (10K documents). In 
question answering applications, our name varia-
tion algorithms have been applied to even larger 
corpora (up to 1M documents). There are two fac-
tors that make our solution scalable.  
First, much of the name variation work is 
highly parallelizable. Most of the time spent in this 
algorithm is spent in the name string edit distance 
calculation. This is also the only algorithm in the 
name variation component that scales quadratically 
with the number of name strings. However, each 
calculation is independent, and could be done si-
multaneously (with enough machines). For the 
10K document set, we ran this algorithm on one 
machine, but when working with larger document 
sets, these computations were run in parallel.  
Second, the disambiguation algorithm clusters 
subsets of document-level entities, rather than run-
ning the clustering over all entities in the document 
set. In the English ACE corpus, the IE system 
found more than 135K document-level entities that 
were candidates for global entity resolution. There 
were 62,516 unique name strings each of which 
was used to initialize an agglomerative clustering 
instance. As described in Section  6, a document 
entity is only clustered one time. Consequently, 
36% of these clustering instances are ?skipped? 
because they contain only already clustered docu-
ment entities. Even the largest clustering instance 
contained only 1.4% of the document-level enti-
ties.  
The vast majority of agglomerative clustering 
instances disambiguated a small number of docu-
ment-level entities and ran quickly. 99.7% of the 
agglomerative clustering runs took less than 1 sec-
ond. 99.9% took 90 seconds or less.  
A small number of clustering instances in-
cluded a large number of document entities, and 
took significant time. The largest clustering in-
stance, initialized with the name string ?Xinhua,? 
contained 1848 document-level entities (1.4% of 
the document-level entities in the corpus). This 
instance took 2.6 hours (27% of the total time 
spent running agglomerative clustering). Another 
frequent entity ?George Bush? took 1.2 hours.  
As described in Section  6, the clustering proce-
dure can combine unresolved document-level enti-
ties into existing global entities. For large cluster 
sets (e.g entities referred to by the string ?Xinhua?), 
speed would be improved by running many smaller 
clustering instances on subsets of the document-
level entities and then merging the results.  
11 Conclusions and Future Work 
We have presented a cross-document co-reference 
clustering algorithm for linking entities across a 
corpus of documents that  
? addresses both the challenges of name varia-
tion and entity disambiguation. 
? is language-independent, 
? is scalable  
As measured in ACE 2008, for English our sys-
tem produced an .8.4% relative reduction in error 
over a baseline that used exact match of name 
strings. When measured on only entities that ap-
peared in more than one document, the system 
gave a 14.2% relative reduction in error. For the 
Arabic task, our system produced a 7% reduction 
in error over exact match (12.4% when scored over 
entities that appear in more than one document). 
We have shown how a variety of features are im-
portant for addressing different aspects of the 
cross-document co-reference problem. Our current 
features are merged with hand-tuned weights. As 
additional development data becomes available, we 
believe it would be feasible to statistically learn the 
weights. With statistically learned weights, a larger 
feature set could improve accuracy even further.  
 Global information from the cross-document 
co-reference system improved within-document 
information extraction. This suggests both that a 
document-level IE system operating over a large 
corpus text can improve its accuracy with informa-
tion that it learns from the corpus; and also that 
integrating an IE system more closely with a 
source of world knowledge (e.g. a knowledge 
base) could improve extraction accuracy.  
Acknowledgements 
This work was supported by the United States De-
partment of Homeland Security. Elizabeth Boschee 
and Ralph Weischedel provided useful insights 
during this work.   
282
References 
 
Artiles, Javier, Julio Gonzalo. & Felisa Verdejo.. 2005. 
A Testbed for People Searching Strategies. In the 
WWW. SIGIR 2005 Conference. Salvador, Brazil. 
Artiles, Javier, Julio Gonzalo. & Satochi Sekine.. 2007. 
The SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 64?69, 
Prague, Czech.  
Bagga, Amit & Breck Baldwin. 1998a. Algorithms for 
Scoring Coreference Chains. In Proceedings of the 
Linguistic Coreference Workshop at the First Inter-
national Conference on Language Resources and 
Evaluation (LREC'98), pages 563-566. 
Bagga, Amit & Breck Baldwin. 1998b. Entity-Based 
Cross-Document Coreferencing Using the Vector 
Space Model. In Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics and the 17th International Conference on 
Computational Linguistics (COLING-ACL?98), pages 
79-85. 
Day, David.,Jason Duncan, Claudio Guiliano, Rob Hall, 
Janet Hitzeman,Su Jian, Paul McNamee, Gideon 
Mann, Stanley Yong & Mike Wick. 2007. CDC Fea-
tures. Johns Hopkins Summer Workshop on Cross-
Document Entity Disambiguation. 
http://www.clsp.jhu.edu/ws2007/groups/elerfed/docu
ments/fullCDED.ppt 
Gooi, Chung Heong & James Allan. 2004. Cross-
document coreference on a large scale corpus. In 
Human Language Technology Conf. North American 
Chapter Association for Computational Linguistics, 
pages 9?16, Boston, Massachusetts, USA. 
Lloyd, Levon., Andrew Mehler & Steven Skiena 2006. 
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching. 2006, pages 
12-23, Barcelona, Spain. 
Linguistic Data Consortium 2008a. ACE (Automatic 
Content Extraction) English Annotation Guidelines 
for Entities Version 6.6 2008.06.13. .  Linguistic 
Data Consortium, Philadelphia. 
http://projects.ldc.upenn.edu/ace/docs/English-
Entities-Guidelines_v6.6.pdf 
Linguistic Data Consortium, 2008b. ACE 2008 XDOC 
Pilot Data V2.1. LDC2007E64.  Linguistic Data 
Consortium, Philadelphia. 
Mann, Gideon S. & Yarowsky, David. 2003. Unsuper-
vised Personal Name Disambiguation In Proceedings 
of the seventh conference on Natural language learn-
ing at HLT-NAACL, pages 33-40. 
 
 
NIST Speech Group. 2008. The ACE 2008 evaluation 
plan: Assessment of Detection and Recognition of 
Entities and Relations Within and Across Docu-
ments. 
http://www.nist.gov/speech/tests/ace/2008/doc/ace08
-evalplan.v1.2d.pdf 
Ramshaw, Lance, E. Boschee, S. Bratus, S. Miller, R. 
Stone, R. Weischedel and A. Zamanian: ?Experi-
ments in Multi-Modal Automatic Content Extrac-
tion?; in Proc. of HLT-01, San Diego, CA, 2001. 
Sista, S, R. Schwartz, T. Leek, and J. Makhoul. An Al-
gorithm for Unsupervised Topic Discovery from 
Broadcast News Stories. In Proceedings of ACM 
HLT, San Diego, CA, 2002. 
 
 
283
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1437?1446,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Extreme Extraction -- Machine Reading in a Week 
 
Marjorie Freedman, Lance Ramshaw, Elizabeth Boschee, Ryan Gabbard,  
Gary Kratkiewicz, Nicolas Ward, Ralph Weischedel 
Raytheon BBN Technologies 
10 Moulton St. 
Cambridge, MA 02138 
mfreedma,lramshaw,eboschee,rgabbard,kratkiewicz, 
nward,weischedel@bbn.com 
 
The views expressed are those of the author and do not reflect the official policy or position of the Depart-
ment of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.   
  
 
Abstract 
We report on empirical results in extreme 
extraction. It is extreme in that (1) from re-
ceipt of the ontology specifying the target 
concepts and relations, development is li-
mited to one week and that (2) relatively 
little training data is assumed. We are able 
to surpass human recall and achieve an F1 
of 0.51 on a question-answering task with 
less than 50 hours of effort using a hybrid 
approach that mixes active learning, boot-
strapping, and limited (5 hours) manual 
rule writing. We compare the performance 
of three systems: extraction with handwrit-
ten rules, bootstrapped extraction, and a 
combination. We show that while the recall 
of the handwritten rules surpasses that of 
the learned system, the learned system is 
able to improve the overall recall and F1.      
1 Introduction 
Throughout the Automatic Content Extraction 1 
(ACE) evaluations and the Message Understanding 
Conferences2 (MUC), teams typically had a year or 
more from release of the target to submitting sys-
tem results. One exception was MUC-6 (Grishman 
& Sundheim, 1996), in which scenario templates 
for changing positions were extracted given only 
one month. Our goal was to confine development 
to a calendar week, in fact, <50 person hours. This 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://www-nlpir.nist.gov/related_projects/muc/ 
is significant in two ways: the less effort it takes to 
bring up a new domain, (1) the more broadly ap-
plicable the technology is and (2) the less effort 
required to run a diagnostic research experiment. 
Our second goal concerned minimizing training 
data. Rather than approximately 250k words of 
entity and relation annotation as in ACE, only ~20 
example pairs per relation-type were provided as 
training. Reducing the training requirements has 
the same two desirable outcomes: demonstrating 
that the technology can be broadly applicable and 
reducing the overhead for running experiments. 
The system achieved recall of 0.49 and precision 
of 0.53 (for an F1 of 0.51) on a blind test set of 60 
queries of the form Ri(arg1, arg2), where Ri is one 
of the 5 new relations and exactly one of arg1 or 
arg2 is a free variable for each query. 
Key to this achievement was a hybrid of:   
? a variant of (Miller, et al, 2004) to learn two 
new classes of entities via automatically induced 
word classes and active learning (6 hours) 
? bootstrap relation learning (Freedman et al 
2010) to learn 5 new relation classes (2.5 hours),  
? handwritten patterns over predicate-argument 
structure (5 hours), and 
? coreference (20 hours) 
Our bootstrap learner is initialized with relation 
tuples (not annotated text) and uses LDC?s Giga-
word and Wikipedia as a background corpus to 
learn patterns for relation detection that are based 
on normalized predicate argument structure as well 
as surface strings.  
These early empirical results suggest the follow-
ing: (1) It is possible to specify a domain, adapt 
our system, and complete manual scoring, includ-
1437
ing human performance, within a month. Experi-
ments in machine reading (and in extraction) can 
be performed much more quickly and cheaply than 
ever before. (2) Through machine learning and 
limited human pattern writing (6 hours), we 
adapted a machine reading system within a week 
(using less than 50 person hours), achieving ques-
tion answering performance with an F1 of 0.5 and 
with recall 11% higher (relative) to a human read-
er. (3) Unfortunately, machine learning, though 
achieving 80% precision,3 significantly lags behind 
a gifted human pattern writer in recall. Thus, boot-
strap learning with much higher recall at minimal 
sacrifice in precision is highly desirable. 
2 Related Work 
This effort is evaluated extrinsically via formal 
questions expressed as a binary relation with one 
free variable. This contrasts with TREC Question 
Answering, 4  where the questions are in natural 
language, and not restricted to a single binary rela-
tion. Like the ?list? queries of TREC QA, the re-
quirement is to find all answers, not just one. 
Though question interpretation is not required in 
our work, interpretation of the text corpus is. 
The goal of rapid adaptation has been tested in 
other contexts. In 2003, a series of experiments in 
adapting to a new language in less than month 
tested system performance on Cebuano and Hindi. 
The primary goal was to adapt to a new language, 
rather than a new domain. The extraction partici-
pants focused on named-entity recognition, not 
relation extraction (May, et al 2003; Sekine & 
Grishman, 2003; Li & McCallum, 2003; Maynard 
et al 2003). The scenario templates of MUC-6 
(Grishman & Sundheim, 1996) are more similar to 
our relation extraction task, although the domain is 
quite different. Our experiment allowed for 1 week 
of development time, while MUC-6 allowed a 
month. The core entities in the MUC-6 task 
(people and organizations) had been worked on 
previously. In contrast all of our relations included 
at least one novel class. While MUC-6 systems 
tended to use finite-state patterns, they did not in-
corporate bootstrapping or patterns based on the 
output of a statistical parser.   
                                                          
3 Handwritten patterns achieved 52% precision. 
4 http://trec.nist.gov/data/qamain.html 
For learning entity classes, we follow Miller, et 
al., (2004), using word clustering and active learn-
ing to train a perceptron model, but unlike that 
work we apply the technique not just to names but 
also to descriptions. An alternative approach to 
learning classes, applying structural patterns to 
bootstrap description recognition without active 
learning, is seen in Riloff (1996) and Kozareva et 
al., (2008)   
Much research (e.g. Ramshaw 2001) has fo-
cused on learning relation extractors using large 
amounts of supervised training, as in ACE. The 
obvious weakness of such approaches is the result-
ing reliance on manually annotated examples, 
which are expensive and time-consuming to create.  
Others have explored bootstrap relation learn-
ing from seed examples. Agichtein & Gravano 
(2000) and Ravichandran & Hovy (2002) reported 
results for generating surface patterns for relation 
identification; others have explored similar ap-
proaches (e.g. Pantel & Pennacchiotti, 2006). Mit-
chell et al (2009) showed that for macro-reading, 
precision and recall can be improved by learning a 
large set of interconnected relations and concepts 
simultaneously. None use coreference to find train-
ing examples; all use surface (word) patterns. 
Freedman et. al (2010) report improved perfor-
mance from using predicate structure for boot-
strappped relation learning.  
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran 
and Hovy (2002) report results in TREC QA, 
where extracting one instance of a relation can be 
sufficient, rather than detecting all instances. Mit-
chell et al (2009), while demonstrating high preci-
sion, do not measure recall. 
By contrast, our work emphasizes recall, not 
just precision. Our question answering task asks 
list-like questions that require multiple answers.  
We also include the results of a secondary, extrac-
tion evaluation which requires that the system 
identify every mention of the relations in a small 
set of documents. This evaluation is loosely based 
on the relation mention detection task in ACE.  
3 Task Set-Up and Evaluation 
Our effort was divided into four phases. During the 
first phase, a third party produced an ontology and 
the resources, which included: brief (~1 paragraph) 
guidelines for each relation and class in the ontolo-
1438
gy; ~20 examples for each relation in the ontology; 
2K documents that are rich in domain relations. 
Table 1 lists the 5 new relations and number of ex-
amples provided for each. Arguments in italics 
were known by the system prior to the evaluation.  
Relation Ex. 
possibleTreatment(Substance, Condition) 23 
expectedDateOnMarket(Substance, Date) 11 
responsibleForTreatment(Substance, Agent) 19 
studiesDisease(Agent, Condition) 16 
hasSideEffect(Substance, Condition) 27 
Table 1: New Relations and Number of Examples 
In phase two, we spent one week extending our 
extraction system for the new ontology. During the 
third phase, we ran our system over 10K docu-
ments to extract all instances of domain relations 
from those documents. In the fourth phase, our 
question answering system used the extracted in-
formation to answer queries.  
4 Approach to Domain Specialization 
Our approach to extracting domain relations inte-
grated novel relation and class detectors into an 
existing extraction system, designed primarily 
around the ACE tasks. The existing system uses a 
discriminatively trained classifier to detect the enti-
ty and value types of ACE. It also produces a syn-
tactic parse for each sentence; normalizes these 
parses to find logical predicate argument structure; 
and detects and coreferences pronominal, nominal, 
and name mentions for each of the 7 ACE entity 
types (Person, Organization, Geopolitical Entity, 
Location, Facility, Weapon, and Vehicle).5  
The extraction system has three components that 
allow for rapid adaptation to a new domain:  
? Class detectors trained using word classes de-
rived from unsupervised clustering and sentence-
selected training data. 
? A bootstrap relation learner which given a few 
seed examples learns patterns that indicate the 
presence of relations.  
? An expressive pattern language which allows a 
developer to express rules for relation extraction 
in a simple, but fast manner.  
 
 
Component Approach Effort 
Class Recognizer Active Learning 6 hrs 
                                                          
5 The extraction system detects relations and events in the 
ACE ontology, but these were not used in the current work.  
 
Class Recognizer Web-Mined List 1 hrs 
Relation Recognizer 
Semi-supervised 
Bootstrapping 
8.5 hrs 
Relation Recognizer Manual Patterns 5 hrs 
Coreference Heuristics 20 hrs 
Table 2: Effort and Approach for New Domain 
4.1 Class Extraction  
Each of the relations in the new domain included at 
least one argument that was new. While question 
answering requires the system to identify the 
classes only when they appear in a relation, know-
ledge of when a class is present provides important 
information for relation extraction. For example in 
our ontology, Y is a treatment for X only if Y is a 
substance. Thus, ?Group counseling sessions are 
effective treatments for depression? does not con-
tain an instance of possibleTreatment(), while 
?SSRIs are effective treatments for depression? 
does. The bootstrap learner allows constraints 
based on argument type. To use this capability, we 
trained the recognizer at the beginning of the week 
of domain adaptation and used the predicted 
classes during learning.  
We annotated 1064 sentences (~31K words) us-
ing active learning combined with unsupervised 
word clusters (Miller,et al, 2004) for the following 
classes: Substance-Name, Substance-Description, 
Condition-Name, and Condition-Description. Ge-
neric noun-phrases like new drugs, the illness, etc 
were labeled as descriptors. Because of the time 
frame, we did not develop extensive guidelines nor 
measure inter-annotator agreement. Annotation 
took 6 hours. We supplemented our annotation 
with lists of substances and treatments from the 
web, which took 1 hour.  
4.2 Coreference 
Providing a name reference is generally preferable 
to a non-specific string (e.g. the drugs), but not 
always feasible; for instance, reports of new re-
search may appear without a name for the drug. 
Our existing system?s coreference algorithms op-
erate only on mentions of ACE entity types (per-
sons, organizations, GPEs, other locations, 
facilities, vehicles, and weapons). During the week 
of domain adaption we developed new heuristics 
for coreference over non-ACE types.  Most of our 
heuristics are domain independent (e.g. linking the 
parts of an appositive). Our decision to annotate 
names and descriptions separately was driven par-
1439
tially by the need to select the best reference (i.e. 
name) for co-referent clusters. Adding coreference 
heuristics for the two new entity types was the sin-
gle most time-consuming activity, taking 20 of the 
total 43 hours. 
4.3 Relation Extraction  
For relation extraction, we used both pattern 
learning and handwritten patterns. We initialized 
our bootstrap relation learner with the example 
instances provided with the domain ontology; Ta-
ble 3 includes examples of the instances provided 
to the system as training. Our bootstrap relation 
learner finds instances of the relation argument 
pairs in text and then proposes both predicate-
argument structure and word-based connections 
between the arguments as possible new patterns for 
the relation. The learner automatically prunes po-
tential patterns using information about the number 
of known-to-be true and novel instances matched 
by a proposed pattern. By running the pattern ex-
tractor over a large corpus, the proposed patterns 
generate new seeds which are in turn are used to 
propose new patterns. For this experiment, we in-
corporated a small amount of supervision during 
the bootstrapping process (roughly 1 hour total per 
relation); we also performed ~30 minutes total in 
pruning domain patterns at the end of learning.  
 Relation Arg-1 Arg-2 
possTreatmnt AZT AIDS 
studyDisease Dr Henri Joyeux cancer 
studyDisease Samir Khleif cancer 
Table 3: Sample Instances for Initializing Learner 
We also used a small amount of human effort 
creating rules for detecting the relations. The pat-
tern writer was given the guidelines, the examples, 
and a 2K document background corpus and spent 1 
hour per relation writing rules.  
The learned patterns use a subset of the full pat-
tern language used by the pattern-writer. The lan-
guage operates over surface-strings as well as 
predicate-argument structure. Figure 1 illustrates 
learned and handwritten patterns for the possible-
TreatmentRelation(). The patterns in rectangles 
match surface-string patterns; the tree-like patterns 
match normalized predicate argument structure.  
The ?WORD- token indicates a wild card of 1-3 
words.  The blue rectangles at the root of the trees 
in the handwritten patterns are sets of predicates 
that can be matched by the pattern. 
5 Evaluation  
Our question answering evaluation was inspired 
by the evaluation in DARPA?s machine reading 
program, which requires systems to map the in-
formation in text into a formal ontology and an-
swer questions based on that ontology. Unlike 
ACE, this allows evaluators to measure perfor-
mance without exhaustively annotating documents, 
allows for balance between rare and common rela-
tions, and implicitly measures coreference without 
requiring explicit annotation of answer keys for 
coreference. However because the evaluation only 
measures performance on the set of queries, many 
relation instances will be unscored. Furthermore, 
the system is not rewarded for finding the same 
relation multiple times; finding 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat) is 
the same as finding 1 (or 10) instances.  
 
Figure 1: Sample Patterns for possibleTreatment() 
 
The evaluation included only queries of the type 
Find all instances for which the relation P(X, Z) is 
true where one of X or Z is constant. For example, 
Find possible treatments for diabetes; or What is 
expected date to market for Abilify? There were 60 
queries in the evaluation set to be answered from a 
10K document corpus. To produce a preliminary 
answer key, annotators were given the queries and 
corpus indexed by Google Desktop. Annotators 
were given 1 hour to find potential answers to each 
query. If no answers were found after 1 hour, the 
annotators were given a second hour to look for 
answers. For two queries, both of the form Find 
treatments with an expected date to market of MM-
YYYY, even after two hours of searching the anno-
tators were unable to find any answers.6  
Annotator answers served as the initial gold-
standard. Given this initial answer key, annotators 
reviewed system answers and aligned them with 
gold-standard answers. System output not aligned 
with the initial gold standard was assessed as cor-
rect or incorrect. We assume that the final gold-
standard constitutes a complete answer key, and 
                                                          
6 Evaluators wanted some queries with no answers. 
1440
are thus able to calculate recall for our system and 
for humans7. Because we had only one annotator 
for each query and because we assumed that any 
answer found by an annotator was correct, we 
could not estimate human precision on this task.  
Answers can be specific named concepts (e.g. 
Penicillin) or generic descriptions (e.g. drug, ill-
ness). Given the sentence, ACME produces a wide 
range of drugs including treatments for malaria 
and athletes foot,? our reading system would ex-
tract the relations responsibleForTreatment(drugs, 
ACME), possibleTreatment(drugs, malaria), pos-
sibleTreatment(drugs, athletes foot). When a name 
was available in the document, annotators marked 
the answer as correct, but underspecified. We cal-
culated precision and recall treating underspecified 
answers as incorrect and separately calculated pre-
cision and recall counting underspecified answers 
as correct. When treated as correct, there was less 
than a 0.05 absolute increase in both precision and 
recall. Unless otherwise specified, all scores re-
ported here use the stricter condition which treats 
underspecified answers as incorrect.  
We also evaluated extracting all information in a 
small document collection (here human search of 
the 10k documents does not play a role in finding 
answers). Individuals were asked to annotate every 
instance of the 5 relations in a set of 102 docu-
ments. Recall, Precision, and F were calculated by 
aligning system responses to the answer key. Sys-
tem answers that aligned are correct; those that did 
not are incorrect; and answers in the key that were 
not found by the system are misses. Unlike the 
question answering evaluation, this evaluation 
measures the ability to find every instance of a 
fact. If the gold standard includes 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat), re-
call will decrease for each instance missed. The 
?extraction? evaluation does not penalize systems 
for missing coreference.  
6 Results 
6.1 Class Detection 
                                                          
7 The answer key may contain some answers that were found 
neither by the annotator nor by the systems described here, 
since the answer key includes answers pooled from other sys-
tems not reported in this paper. The system reported here was 
the highest performing of all those participating in the experi-
ment. Furthermore, if a system answer is marked as correct, 
but underspecified, the specific  answer is put in the key. 
The recall, precision, and F1 for class detection 
using 10-fold cross validation of the ~1K anno-
tated sentences appear in the 3-5th columns of Table 
4. Given the amount of training, our results are 
lower than in Miller et al(2004) (an F1 of 90 with 
less than 25K words of training). Several factors 
could explain this: Finding boundaries and types 
for descriptions is more complex than for names in 
English. 8  Our classes, pharmaceutical substances 
and physiological conditions, may have been more 
difficult to learn. Our classes are less common in 
news reporting; as such, both word-class clusters 
and active learning may have been less effective. 
Finally, our evaluation was done on a 10-fold split 
of the active-learning selected data; bias in select-
ing the data could explain at least a part of our 
lower performance.  
Type 
# in 
GS 
Without Lists With Lists 
R P F R P F 
Subst-D 789 77 85 80.8 78 85 81.3 
Subst-N 410 70 82 75.5 77 81 78.9 
Cond-D 427 72 78 74.9 72 77 74.4 
Cond-N 963 80 87 83.4 84 83 83.5 
Table 4: Cross Validation:  Condition & Substance 
We noticed that the system frequently reported 
country names to be substance-names. Surprising-
ly, we found that our well-trained name finder 
made the opposite mistake, occasionally reporting 
drugs as geo-political entities.  
We incorporated lists of known substances and 
conditions to improve recall. Performance on the 
same cross-validation split is shown in the final 
three columns of Table 4. Incorporating the lists led 
to recall gains for both substance-name and condi-
tion-name. Because a false-alarm in class recogni-
tion only leads to an incorrect relation extraction if 
it appears in a context indicating a domain relation, 
false alarms of classes may be less important in the 
question answering and extraction evaluations.   
6.2 Question Answering and Extraction 
Figure 2 and Table 6 show system performance 
using only handwritten rules (HW), only learned 
patterns (L), and combining both (C). Figure 2  
includes scores calculated with all of the systems? 
answers (in the dotted boxes), and with just those 
answers that were deemed useful (discussed be-
                                                          
8 English names are capitalized; person names have a typical 
form and are frequently signaled by titles; organization names 
frequently have clear signal words, such as Corp. 
1441
low). We include annotator recall. Handwritten 
patterns outperform learned patterns consistently 
with much higher recall. Encouragingly, however, 
1. The combined system?s recall and F-Score 
are noticeably higher for 3 of the relations.  
2. The learned patterns generate answers not 
found by handwritten patterns.  
3. The learned patterns have high precision.9 
There is variation across the different relations. 
The two best performing relations possibleTreat-
ment() and studiesDisease() have F1 more than 
twice as high as the two worst performing rela-
tions, expectedDateToMarket() and hasSideEf-
fect(). This is primarily due to differences in recall.  
 
Figure 2: Overall Q/A Performance: All answers in  
dotted boxes; 'Useful Answers' unboxed 
The combined system?s recall (0.49), while low, 
is higher than that of the annotators (0.44). While 
hardly surprising that a machine can process in-
formation much more quickly than a person, it is 
encouraging that higher recall is achieved even 
with only one week?s effort. In the context of our 
pooled answer-key, the relatively low recall of 
both the system and the annotator suggests that 
there was little overlap between the answers found 
by the annotator and those found by the system.  
As already described, the system answers can 
include both specific references (e.g. Prozac) and 
more generic references (the drug). When a more 
specific answer is present in the document, generic 
references have been treated as incorrect. Howev-
er, sometimes there is not a more specific refer-
ence; for example an article written before a drug 
has been released may never name the drug. Scores 
reported thus far treat such answers as correct. 
These answers would be useful when answering 
more complex queries. For example, given the sen-
                                                          
9 The learned patterns' high precision is to be expected for two 
reasons. First, a few bad patterns were manually removed for 
each relation. More importantly, the learning algorithm strong-
ly favors high precision patterns because it needs to maintain a 
seed set with low noise in order to learn effectively.  
tence ?ACME spent 5 years developing a pill to 
treat the flu which it will release next week,? ex-
tracting relations involving ?the pill?  would allow 
a system to answer questions that use multiple rela-
tions in the ontology to for example ask about  or-
ganizations developing treatments for the flu, or 
the expected date of release for ACME?s drugs. 
However, in our simple question answering 
framework such generic answers never convey 
novel information and thus were probably ignored 
by human annotators.  
 To measure the impact of treating these generic 
references as correct,10 we did additional annota-
tion on the correct answers, marking answers as 
?useful? (specific) and ?not-useful? (generic). The 
unboxed bars in Figure 2 show performance when 
?not-useful? answers are removed from the answer-
key and the responses. For the four relations where 
there was a change Table 5 provides the relative 
change performance when only ?useful? answers 
are considered. The annotator?s recall increases 
noticeably while the combined system?s drops. 
This results in the overall recall of annotators sur-
passing that of the combined system.   
Relation 
Recall Precision 
A C H L C H L 
possTreat 12 10 10 14 -10 -11 -3 
respTreat 9 0 -5 8 -4 -4 -1 
studyDis 12 -6 -9 13 -11 -13 0 
hasSidEff 3 4 4 4 0 0 0 
Total 11 -2 -4 6 -9 -10 -2 
Table 5: Relative Change in Recall and Precision When 
Non-Useful Answers are Removed 
Table 7 shows the total number of answers pro-
duced by annotators and by each system, as well as 
the percentage of queries with at least one correct 
answer for each system. For one relation expec-
tedDateOnMarket(), the learned system did not 
find any answers. This relation had far fewer an-
swers found by annotators and occurred far more 
rarely in the fully annotated extraction set (see Ta-
ble 8). Anecdotally, extracting this relation fre-
quently required co-referencing ?it? (e.g. ?It will be 
released in March 2011?). Our heuristics for core-
ference of the new classes did not account for pro-
nouns. Learning from such examples would 
require coreference during bootstrapping. Most 
likely, the learned system was unable to generate 
enough novel instances to continue bootstrapping 
                                                          
10 Generic answers were treated as correct only if a more spe-
cific reference was not available in the document.  
1442
and was thus unable to learn the relation.  
Relation Type 
(# Queries; # Correct Ans.) 
Recall Precision F 
A C HW L C HW L C HW L 
possTreatment (10;247) 0.27 0.63 0.50 0.34 0.51 0.47 0.83 0.56 0.48 0.48 
respForTreat (15;134) 0.73 0.33 0.24 0.22 0.66 0.78 0.73 0.44 0.37 0.33 
expectDateMarkt (11;60) 0.90 0.17 0.17 0.00 0.77 0.83 0.00 0.27 0.28 0 
studiesDisease (13;292) 0.23 0.67 0.59 0.09 0.51 0.50 0.79 0.58 0.54 0.16 
hasSideEffect (11;104) 0.80 0.10 0.13 0.02 0.83 0.70 1.00 0.17 0.23 0.04 
Total (60;837) 0.44 0.49 0.42 0.17 0.53 0.52 0.80 0.51 0.46 0.28 
Table 6: Question Answering Results by Relation Type 
Relation Type 
 
Total Number of Answers % Queries with At Least 1 Corr. Ans 
A C HW L A C HW L 
possTreatment  66 303 261 100 100.0% 90.0% 90.0% 90.0% 
respForTreat  98 67 41 40 100.0% 66.7% 60.0% 60.0% 
expectDateMarkt  54 13 12 0 72.7% 45.5% 45.5% 0.0% 
studiesDisease  68 379 347 33 100.0% 61.5% 46.2% 46.2% 
hasSideEffect  83 12 20 2 72.7% 36.4% 45.5% 18.2% 
Total  369 774 681 175 90.0% 60.0% 56.7% 43.3% 
Table 7: Number of Answers and Number of Queries Answered 
Overall, the system did better on relations hav-
ing more correct answers. Bootstrap learning has 
an easier time discovering new instances and new 
patterns when there are more examples to work 
with. Even a human pattern writer will have more 
examples to generalize from for common relations.  
While possibleTreatment() and hasSideEffect() 
have similar F-scores, their performance is very 
different at the query level. The system was able to 
find at least one correct answer to every possible-
Treatment() query; however only 72.7% of the stu-
diesDisease() queries were answered.  
Table 8 presents results from the extraction 
evaluation where a set of ~100 documents were 
annotated for all mentions of the 5 relations. Be-
cause every mention in the document set must be 
found, the system cannot rely on finding the easiest 
answers for common relations. The results in Table 
8 are significantly lower than for the question ans-
wering tasks; yet some of the same trends are 
present. Handwritten rules outperform learned pat-
terns. For at least some relations, the combination 
of the two improves performance. The three rela-
tions for which the learned system has the lowest 
performance on the question-answering task have 
the fewest instances annotated in the document set. 
Fewer instance in the large corpus make bootstrap-
ping more difficult?the learner is less able to gen-
erate novel instances to expand its pattern set.  
7 Discussion 
7.1 Sources of Error 
The most common source of error is pattern cover-
age. In the following figure, the system identified 
responsibleForTreatment(Janssen Pharmaceutical, 
Sporanox), but missed the corresponding relation 
between Novartis and Lamisil.  
 
 
 
 
 
Relation Type # Relations Found Recall Precision F 
GS C HW L C HW L C HW L C HW L 
possibleTreatment 518 225 187 68 0.15 0.10 0.09 0.34 0.28 0.66 0.21 0.15 0.15 
respForTreatment 387 101 77 36 0.10 0.08 0.05 0.41 0.40 0.50 0.17 0.13 0.08 
expDateOnMarket 66 13 13 0 0.06 0.06 0.00 0.31 0.31 0.00 0.10 0.10 0.00 
studiesDisease 136 95 91 4 0.08 0.09 0.00 0.12 0.13 0.00 0.10 0.11 0.00 
hasSideEffect 256 26 25 2 0.04 0.04 0.00 0.39 0.40 0.50 0.07 0.07 0.01 
Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations 
Sporanox is made by Janssen Pharmaceutica Inc., 
of Titusville, N.J. Lamisil is a product of Novartis 
Pharmaceuticals of East Hanover, N.J. 
 
 
1443
Missed class instances contribute to errors, some-
times originating in errors in tokenization (e.g. not 
removing the ?_? in each drug name in a bulleted 
list of the form ?_Trovan, an antibiotic...; etc.) 
However, many drug-names are simply missed: 
 
The system correctly identifies Rebif and Aricept 
as drugs, but misses Pregabalin and Serono. In 
both misses, the immediately preceding and fol-
lowing words provide little evidence that the word 
refers to a drug rather than some other product. 
Substance detection might be better served with a 
web-scale, list-learning approach like the doubly 
anchored patterns described in (Kozareva et al, 
2008). Alternatively, our approach may need to be 
extended to include a larger context window. 
7.2 Learned Patterns  
One of the ways in which learned patterns supple-
ment handwritten ones is learning highly specific 
surface-string patterns that are insensitive to errors 
in parsing. Figure 3 illustrates two examples of 
what appear to be easy cases of possibleTreat-
ment(). Because the handwritten patterns are not 
exhaustive and make extensive use of syntactic 
structure, parse errors prevented the system based 
on handwritten rules from firing. Learned surface-
string patterns were able to find these relations.  
Even when the syntactic structure is correct, 
learned patterns capture expressions not common 
enough to have been noticed by the rule writer. For 
example, while the handwritten patterns included 
?withdrew? as a predicate indicating a company 
was responsible for a drug, they did not include 
?pulled.? By including ?pulled?, learned patterns 
extracted responsibleForTreatment() from ?Ameri-
can Home Products pulled Duract, a painkiller.? 
Similarly, the learned patterns include an explicit 
pattern ?CONDITION drug called SUBSTANCE?, 
and thus extracted a possibleTreatment() relation 
from ?newly approved narcolepsy drug called 
modafinil? without relying on the coreference 
component to link drug to modafinil.  
Handwritten Patterns 
Despite the examples above of successfully learned 
patterns, handwritten patterns perform significantly 
better. In the active-learning context used for these 
experiments, the handwritten rules also required 
less manual effort. This comparison is not entirely 
fair-- while learned patterns required more hours, 
supervising the bootstrapping algorithm requires 
no training. The handwritten patterns, in contrast, 
require a trained expert.  
 
Figure 3: Extractions Missed by Handwritten Rules & 
the Erroneous Parses that Hid them 
While handwritten rules and learned patterns use 
the same language, they make use of it differently. 
The handwritten patterns group similar concepts 
together. A human pattern writer adds relevant 
synonyms, as well as words that are not synonym-
ous but in the pattern context can be used inter-
changeably. In Figure 4, the handwritten patterns 
include three word-sets: (patient*, people, partici-
pant*); (given, taken, took, using); and (report*, 
experience*, develop*, suffer*). The ?*? serves as a 
wild-card to further generalize a pattern. The word-
sets in Figure 4 illustrate challenges for a learned 
system: the words are not synonyms, but rather are 
words that can be used to imply the relation.  
A human pattern writer frequently generates 
new classes not in the domain ontology. In Figure 
4, the circled patterns form a class of ?people tak-
ing a substance.? The handwritten patterns for stu-
diesDisease() include classes targeting scientists 
and researchers. These classes are not necessarily 
triggered by nouns. Such classes allow the pattern 
writer to include complex patterns as in Figure 4 
and to write relatively precise, but open-ended pat-
terns such as: if there is a single named-drug and a 
named, non-side-effect disease in the same sen-
tence, the drug is a treatment for the disease.  
Pfizer also hopes to introduce Pregabalin next 
year for treatment of neuropathic pain, epilepsy 
and anxiety?Other deals include co-promoting 
Rebif for multiple sclerosis with its discoverer, 
Serono, and marketing Aricept for Alzheimer's 
disease with its developer, Eisai Co. 
1444
 
Figure 4: Learned and Handwritten Patterns for  
hasSideEffect() 
A final difference between handwritten and 
learned patterns is the level of predicate-argument 
complexity used. In general, handwritten patterns 
account for larger spans of predicate argument 
structure while learned patterns tend to limit them-
selves to the connections between the arguments of 
the relation with minor extensions.  
8 Conclusions and Lessons Learned 
First, it is encouraging that the synthesis of learn-
ing algorithms and handwritten algorithms can 
achieve an F1 of 0.51 in a new domain in a week 
(<50 hours of effort). Second, it is exciting that so 
little training data is required: ~20 relation pairs 
out of context (~2.5 hours of effort) and ~6 hours 
of active learning for the new classes.  
Third, the effectiveness of learning algorithms is 
still not competitive with handwritten patterns 
based on predicate-argument structure (~5 hours of 
effort on top of active learning for entities). 
Though the learned patterns have high precision 
(0.80 on average), recall is low (0.17) and varied 
greatly across the relations. Though the dominant 
factor in missing relations is pattern coverage, 
missing instances of classes contributed to low re-
call. Comparing learned patterns to manually writ-
ten patterns, (1) synonyms or other lexical 
alternatives that a human pattern writer would in-
clude, (2) the creation of subclasses for argument 
types, and (3) the scope of patterns11 are each ma-
jor sources of the disparity in coverage. Research 
on learning approaches to raise recall without sig-
nificant sacrifice in precision seems essential.  
Fourth, despite the disparity in performance of 
learned versus manual patterns, and despite the low 
                                                          
11 Learned patterns tend to focus on the structure that appears 
between the two arguments, rather than structure surrounding 
the left and right arguments. 
recall of learned patterns, the combined system?s 
recall and F-Score are higher for three of the rela-
tions because the learned patterns generated an-
swers not found by handwritten patterns. We found 
examples where highly specific, learned, surface-
level patterns (lexical patterns) occasionally found 
information missed by handwritten patterns due to 
parsing errors or general low coverage. 
Fifth, the effort for coreference was the most 
time-consuming, given that every new relation 
contained at least one of the new argument types. 
While we included this in our estimate of domain 
adaptation, the infrastructure we built is domain 
generic. Improving generic coreference will reduce 
domain specific effort in future.  
Perhaps most significant of all, running a com-
plete experiment from definition of the domain 
through creation of training data and measurement 
of end-to-end performance of the system can be 
completed in a month. The ability to rapidly, 
cheaply, and empirically measure the impact of 
extraction research could prove a significant spur 
to research across the board. 
These experiments suggest three possible direc-
tions for improving the ability to quickly develop 
information extraction technology for a new set of 
relations: (1) reducing the amount of supervision 
provided to the bootstrap-learner; (2) improving 
the bootstrapping approach to reach the level of 
recall achieved by the human pattern writer elimi-
nating the need for a trained expert during domain 
adaptation; and (3) focusing improvements to the 
bootstrapping approach on techniques that allow it 
to find more of the instances missed by the pattern 
writer, thus improving the accuracy of the hybrid 
system.   
Acknowledgments 
This work was supported, in part, by DARPA un-
der AFRL Contract FA8750-09-C-179. Distribu-
tion Statement ?A? (Approved for Public Release, 
Distribution Unlimited) Thank you to the review-
ers for your insightful comments and to Michelle 
Franchini for coordinating the assessment effort. 
References 
E. Agichtein and L. Gravano. Snowball: extracting rela-
tions from large plain-text collections. In Proceed-
ings of the ACM Conference on Digital Libraries, pp. 
85-94, 2000.  
1445
A. Blum and T. Mitchell. Combining Labeled and Un-
labeled Data with Co-Training. In Proceedings of the 
1998 Conference on Computational Learning 
Theory, July 1998.  
E. Boschee, V. Punyakanok, R. Weischedel. An Explo-
ratory Study Towards ?Machines that Learn to Read?. 
Proceedings of AAAI BICA Fall Symposium, No-
vember 2008. 
J. Chen, D. Ji, C. Tan and Z. Niu. (2006). Relation ex-
traction using label propagation based semi-
supervised learning. COLING-ACL 2006: 129-136. 
July 2006. 
M. Freedman, E. Loper, E. Boschee, and R. Weischedel. 
Empirical Studies in Learning to Read. Proceedings 
of NAACL 2010 Workshop on Formalisms and Me-
thodology for Learning by Reading, pp. 61-69, June 
2010. 
W. Li and A. McCallum.  Rapid development of Hindi 
named entity recognition using conditional random 
fields and feature induction. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
R Grishman and B. Sundheim. Message Understanding 
Conference-6 : A Brief History", in COLING-96, 
Proc . of the Int'l Conj. on Computational Linguis-
tics, 1996.  
Z. Kozareva and E. Hovy. Not All Seeds Are Equal: 
Measuring the Quality of Text Mining Seeds. Human 
Language Technologies: The 2010 Annual Confe-
rence of the North American Chapter of the Associa-
tion for Computational Linguistics, June, 2010, pp. 
618-626. 
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic 
class learning from the web with hyponym pattern 
linkage graphs. In Proceedings of ACL-08: HLT, 
pages 1048?1056.  
J. May, A. Brunstein, P. Natarajan,  and R. Weischedel.  
Surprise! What's in a Cebuano or Hindi Name? 
Transactions on Asian Language Information 
Processing (TALIP), Volume 2 Issue 3  September, 
2003. 
D. Maynard, V. Tablan, K. Bontcheva, and H. Cun-
ningham. Rapid customization of an information ex-
traction system for a surprise language. Transactions 
on Asian Language Information Processing (TALIP), 
Volume 2 Issue 3  September, 2003. 
S. Miller, J. Guinness, and A. Zamanian, ?Name Tag-
ging with Word Cluster and Discriminative Train-
ing?, Proceedings of HLT/NAACL 2004, pp. 337-
342, 2004 
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and 
R. Wang. ?Populating the Semantic Web by Macro-
Reading Internet Text. Invited paper, Proceedings of 
the 8th International Semantic Web Conference 
(ISWC 2009).  
NIST, ACE 2007: 
http://www.itl.nist.gov/iad/mig/tests/ace/2007/softwa
re.html 
P. Pantel and M. Pennacchiotti. Espresso: Leveraging 
Generic Patterns for Automatically Harvesting Se-
mantic Relations. In Proceedings of Conference on 
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120. 
Sydney, Australia, 2006.  
L. Ramshaw , E. Boschee, S. Bratus, S. Miller, R. 
Stone, R. Weischedel, A. Zamanian, ?Experiments in 
multi-modal automatic content extraction?, Proceed-
ings of Human Technology Conference, March 2001.  
D. Ravichandran and E. Hovy. Learning surface text 
patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), 
pages 41?47, Philadelphia, PA, 2002.  
E. Riloff. Automatically generating extraction patterns 
from untagged text. In Proceedings of the Thirteenth 
National Conference on Artificial Intelligence, pages 
1044-1049, 1996.  
S. Sekine and R. Grishman.  Hindi-English cross-lingual 
question-answering  system. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
G. Zhou, J. Li, L. Qian, Q. Zhu. Semi-Supervised 
Learning for Relation Extraction. Proceedings of the 
Third International Joint Conference on Natural 
Language Processing: Volume-I. 2008. 
 
1446
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288?293,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
 Coreference for Learning to Extract Relations:  
Yes, Virginia, Coreference Matters 
 
Ryan Gabbard 
rgabbard@bbn.com 
 
Marjorie Freedman 
mfreedma@bbn.com 
 
Ralph Weischedel 
weischedel@bbn.com 
 
Raytheon BBN Technologies, 10 Moulton St., Cambridge, MA 02138 
 
The views expressed are those of the author and do not reflect the official policy or position of the De-
partment of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.  
 
 
Abstract 
As an alternative to requiring substantial su-
pervised relation training data, many have ex-
plored bootstrapping relation extraction from 
a few seed examples. Most techniques assume 
that the examples are based on easily spotted 
anchors, e.g., names or dates. Sentences in a 
corpus which contain the anchors are then 
used to induce alternative ways of expressing 
the relation. We explore whether coreference 
can improve the learning process. That is, if 
the algorithm considered examples such as his 
sister, would accuracy be improved? With co-
reference, we see on average a 2-fold increase 
in F-Score. Despite using potentially errorful 
machine coreference, we see significant in-
crease in recall on all relations. Precision in-
creases in four cases and decreases in six.  
1 Introduction 
As an alternative to requiring substantial super-
vised relation training data (e.g. the ~300k words 
of detailed, exhaustive annotation in Automatic 
Content Extraction (ACE) evaluations1) many have 
explored bootstrapping relation extraction from a 
few (~20) seed instances of a relation. Key to such 
approaches is a large body of unannotated text that 
can be iteratively processed as follows:  
1. Find sentences containing the seed instances. 
2. Induce patterns of context from the sentences. 
3. From those patterns, find more instances. 
4. Go to 2 until some condition is reached. 
Most techniques assume that relation instanc-
es, like hasBirthDate(Wolfgang Amadeus Mozart, 
                                                          
1
 http://www.nist.gov/speech/tests/ace/ 
1756), are realized in the corpus as relation texts2 
with easily spotted anchors like Wolfgang 
Amadeus Mozart was born in 1756.  
In this paper we explore whether using corefer-
ence can improve the learning process. That is, if 
the algorithm considered texts like his birth in 
1756 for the above relation, would performance of 
the learned patterns be better? 
2 Related Research 
There has been much work in relation extraction 
both in traditional supervised settings and, more 
recently, in bootstrapped, semi-supervised settings. 
To set the stage for discussing related work, we 
highlight some aspects of our system. Our work 
initializes learning with about 20 seed relation in-
stances and uses about 9 million documents of un-
annotated text3 as a background bootstrapping 
corpus. We use both normalized syntactic structure 
and surface strings as features. 
Much has been published on learning relation 
extractors using lots of supervised training, as in 
ACE, which evaluates system performance in de-
tecting a fixed set of concepts and relations in text. 
Researchers have typically used this data to incor-
porate a great deal of structural syntactic infor-
mation in their models (e.g. Ramshaw, 2001), but 
the obvious weakness of these approaches is the 
resulting reliance on manually annotated examples, 
which are expensive and time-consuming to create. 
                                                          
2
 Throughout we will use relation instance to refer to a fact 
(e.g. ORGHasEmployee(Apple, Steve Jobs)), while we will use 
relation text to refer a particular sentence entailing a relation 
instance (e.g. Steve Jobs is Apple?s CEO).  
3
 Wikipedia and the LDC?s Gigaword newswire corpus. 
288
Others have explored automatic pattern genera-
tion from seed examples. Agichtein & Gravano 
(2000) and Ravichandran & Hovy (2002) reported 
results for generating surface patterns for relation 
identification; others have explored similar ap-
proaches (e.g. Pantel & Pennacchiotti, 2006). 
Mitchell et al (2009) showed that for macro-
reading, precision and recall can be improved by 
learning a large set of interconnected relations and 
concepts simultaneously. In all cases, the ap-
proaches used surface (word) patterns without co-
reference. In contrast, we use the structural 
features of predicate-argument structure and em-
ploy coreference. Section 3 describes our particular 
approach to pattern and relation instance scoring 
and selection.  
Another research strand (Chen et al, 2006 & 
Zhou et al, 2008) explores semi-supervised rela-
tion learning using the ACE corpus and assuming 
manual mention markup. They measure the accu-
racy of relation extraction alone, without including 
the added challenge of resolving non-specific rela-
tion arguments to name references. They limit their 
studies to the small ACE corpora where mention 
markup is manually encoded.  
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran 
and Hovy (2002) report results in the Text Retriev-
al Conference (TREC) Question Answering track, 
where extracting one text of a relation instance can 
be sufficient, rather than detecting all texts. Mitch-
ell et al (2009), while demonstrating high preci-
sion, do not measure recall. 
In contrast, our study has emphasized recall. A 
primary focus on precision allows one to ignore 
many relation texts that require coreference or 
long-distance dependencies; one primary goal of 
our work is to measure system performance in ex-
actly those areas. There are at least two reasons to 
not lose sight of recall. For the majority of entities 
there will be only a few mentions of that entity in 
even a large corpus. Furthermore, for many infor-
mation-extraction problems the number documents 
at runtime will be far less than web-scale.  
3 Approach 
Figure 1 depicts our approach for learning patterns 
to detect relations. At each iteration, the steps are:   
(1) Given the current relation instances, find possi-
ble texts that entail the relation by finding sentenc-
es in the corpus containing all arguments of an in-
stance.  
(2)  As in Freedman et al (2010) and Boschee et 
al. (2008), induce possible patterns using the con-
text in which the arguments appear. Patterns in-
clude both surface strings and normalized syntactic 
structures.4 Each proposed pattern is applied to the 
corpus to find a set of hypothesized texts.
 
For each 
pattern, a confidence score is assigned using esti-
mated precision5 and recall.  The highest confi-
dence patterns are added to the pattern set.6 
(3) The patterns are applied to the corpus to find 
additional possible relation instances. For each 
proposed instance, we estimate a score using a Na-
ive Bayes model with the patterns as the features.  
When using coreference, this score is penalized if 
an instance?s supporting evidence involves low-
confidence coreference links. The highest scoring 
instances are added to the instance set. 
 (4) After the desired number of iterations (in these 
experiments, 20) is complete, a human reviews the 
resulting pattern set and removes those patterns 
which are clearly incorrect (e.g. ?X visited Y? for 
hasBirthPlace).7   
 
Figure 1: Approach to learning relations 
We ran this system in two versions: ?Coref has 
no access to coreference information, while +Coref 
(the original system) does. The systems are other-
wise identical. Coreference information is provided 
by BBN?s state-of-the-art information extraction 
                                                          
4
 Surface text patterns with wild cards are not proposed until 
the third iteration.  
5
 Estimated recall is the weighted fraction of known instances 
found. Estimated precision is the weighted average of the 
scores of matched instances; scores for unseen instances are 0.   
6
 As more patterns are accepted in a given iteration, we raise 
the confidence threshold.  Usually, ~10 patterns are accepted 
per iteration.  
7
 This takes about ten minutes per relation, which is less than 
the time to choose the initial seed instances. 
pattern 
database
proposed
instances
proposed 
patterns
proposed
pairs
retrieve from 
corpus
retrieve from corpus
induce
prune 
and add
granted
patent
obj
INVENTOR
INVENTION
iobj
for
Thomas Edison ? light bulb
Alexander G. Bell ... telephone
Ben Franklin ? lightning rod
Edison invented the light bulb
Bell built the first telephone
Edison was granted a U.S. patent 
for the light bulb
Franklin invented the lightning rod
example pairs
instances
289
system (Ramshaw, et al, 2011; NIST, 2007) in a 
mode which sacrifices some accuracy for speed 
(most notably by reducing the parser?s search 
space). The IE system processes over 50MB/hour 
with an  average EDR Value score when evaluated 
on an 8-fold cross-validation of the ACE 2007.  
+Coref can propose relation instances from text 
in which the arguments are expressed as either 
name or non-name mentions. When the text of an 
argument of a proposed instance is a non-name, the 
system uses coreference to resolve the non-name to 
a name. -Coref can only propose instances based 
on texts where both arguments are names.8 
This has several implications: If a text that en-
tails a relation instance expresses one of the argu-
ments as a non-name mention (e.g. ?Sue?s husband 
is here.?), -Coref will be unable to learn an in-
stance from that text. Even when all arguments are 
expressed as names, -Coref may need to use more 
specific, complex patterns to learn the instance 
(e.g. ?Sue asked her son, Bob, to set the table?). 
We expect the ability to run using a ?denser,? more 
local space of patterns to be a significant advantage 
of +Coref. Certain types of patterns (e.g. patterns 
involving possessives) may also be less likely to be 
learned by -Coref. Finally, +Coref has access to 
much more training data at the outset because it 
can find more matching seed instances,9 potentially 
leading to better and more stable training. 
4 Evaluation Framework 
Estimating recall for bootstrapped relation learning 
is a challenge except for corpora small enough for 
complete annotation to be feasible, e.g., the ACE 
corpora. ACE typically had a test set of ~30,000 
words and ~300k for training. Yet, with a small 
corpus, rare relations will be inadequately repre-
sented.10 Macro-reading evaluations (e.g. Mitchell, 
2009) have not estimated recall, but have measured 
precision by sampling system output and determin-
ing whether the extracted fact is true in the world. 
                                                          
8
 An instance like hasChild(his father, he) would be useful 
neither during training nor (without coreference) at runtime. 
9
 An average of 12,583 matches versus 2,256 matches. If mul-
tiple mentions expressing an argument occur in one sentence, 
each match is counted, inflating the difference. 
10
 Despite being selected to be rich in the 18 ACE relation 
subtypes, the 10 most frequent subtypes account for over 90% 
of the relations with the 4 most frequent accounting for 62%; 
the 5 least frequent relation subtypes occur less than 50 times. 
Here we extend this idea to both precision and re-
call in a micro-reading context.  
Precision is measured by running the system 
over the background corpus and randomly sam-
pleing 100 texts that the system believes entail 
each relation. From the mentions matching the ar-
gument slots of the patterns, we build a relation 
instance.  If these mentions are not names (only 
possible for +Coref), they are resolved to names 
using system coreference.  For example, given the 
passage in Figure 2 and the pattern ?(Y, poss:X)?, 
the system would match the mentions X=her and 
Y=son, and build the relation instance 
hasChild(Ethel Kennedy, Robert F. Kennedy Jr.). 
During assessment, the annotator is asked 
whether, in the context of the whole document, a 
given sentence entails the relation instance. We 
thus treat both incorrect relation extraction and 
incorrect reference resolution as mistakes.  
To measure recall, we select 20 test relation in-
stances and search the corpus for sentences con-
taining all arguments of a test instance (explicitly 
or via coreference). We randomly sampled from 
this set, choosing at most 10 sentences for each test 
instance, to form a collection of at most 200 sen-
tences likely to be texts expressing the desired rela-
tion. These sentences were then manually 
annotated in the same manner as the precision an-
notation. Sentences that did not correctly convey 
the relation instance were removed, and the re-
maining set of sentences formed a recall set.  We 
consider a recall set instance to be found by a sys-
tem if the system finds a relation of the correct 
type in the sentence. We intentionally chose to 
sample 10 sentences from each test example, rather 
than sampling from the set of all sentences found. 
This prevents one or two very commonly ex-
pressed instances from dominating the recall set. 
As a result, the recall test set is biased away from 
?true? recall, because it places a higher weight on 
the ?long tail? of instances. However, this gives a 
more accurate indication of the system?s ability to 
find novel instances of a relation.  
Ethel Kennedy says that when the family gathered 
for Thanksgiving she wanted the children to know 
what a real turkey looked like. So she sent her son, 
Robert F. Kennedy Jr., to a farm to buy two birds. 
Figure 2: Passage entailing hasChild relation 
290
5 Empirical Results 
Table 1 gives results for precision, recall, and F 
for +Coref (+) and ?Coref (-). In all cases remov-
ing coreference causes a drop in recall, ranging 
from only 33%(hasBirthPlace) to over 90% 
(GPEEmploys). The median drop is 68%. 
 
5.1 Recall 
There are two potential sources of ?Coref?s 
lower recall. For some relation instances, the text 
will contain only non-named instances, and as a 
result -Coref will be unable to find the instance.     
-Coref is also at a disadvantage while learning, 
since it has access to fewer texts during bootstrap-
ping.  Figure 311 presents the fraction of instances 
in the recall test set for which both argument 
names appear in the sentence.  Even with perfect 
patterns, -Coref has no opportunity to find roughly 
25% of the relation texts because at least one ar-
gument is not expressed as a name.   
To further understand -Coref?s lower perfor-
mance, we created a third system, *Coref, which 
used coreference at runtime but not during train-
ing.12 In a few cases, such as hasBirthPlace, 
*Coref is able to almost match the recall of the 
system that used coreference during learning 
(+Coref), but on average the lack of coreference at 
runtime accounts for only about 25% of the differ-
ence, with the rest accounted for by differences in 
the pattern sets learned. 
Figure 4 shows the distribution of argument 
mention types for +Coref on the recall set.  Com-
paring this to Figure 3, we see that +Coref uses 
name-name pairs far less often than it could (less 
                                                          
11
 Figures 3 & 4 do not include hasBirthDate: There is only 1 
potential named argument for this relation, the other is a date.  
12
 *Coref was added after reading paper reviews, so there was 
not time to do annotation for a precision evaluation for it. 
than 50% of the time overall).  Instead, even when 
two names are present in a sentence that entails the 
relation, +Coref chooses to find the relation in 
name-descriptor and name-pronoun contexts which 
are often more locally related in the sentences. 
 
Figure 4: Distribution of argument mention types for 
+Coref matches on the recall set 
For the two cases with the largest drops in re-
call, ORGEmploys and GPEEmploys, +Coref and ?
Coref have very different trajectories during train-
ing.  For example, in the first iteration, ?Coref 
learns patterns involving director, president, and 
head for ORGEmploys, while +Coref learns pat-
terns involving joined and hired.  We speculate 
that ?Coref may become stuck because the most 
frequent name-name constructions, e.g. ORG/GPE 
title PERSON (e.g. Brazilian President Lula da 
Silva), are typically used to introduce top officials. 
For such cases, even without co-reference, system 
specific effort and tuning could potentially have 
improved ?Coref?s ability to learn the relations.  
5.2 Precision 
Results on precision are mixed. While for 4 of 
the relations +Coref is higher, for the 6 others the 
addition of coreference reduces precision. The av-
erage precisions for +Coref and ?Coref are 82.2 
and 87.8, and the F-score of +Coref exceeded that 
 
0 %
20 %
40 %
60 %
80 %
100 %
1 2 3 4 5 6 7 8 9
Other
Combi nati ons
Both Desc
Name & Pron
Name & Desc
Both Na me
 P+ P- R+ R- R* F+ F- 
attendSchool (1) 83 97 49 16 27 62 27 
GPEEmploy(2) 91 96 29 3 3 44 5 
GPELeader (3) 87 99 48 28 30 62 43 
hasBirthPlace (4) 87 97 57 37 53 69 53 
hasChild (5) 70 60 37 17 11 48 27 
hasSibling (6) 73 69 67 17 17 70 28 
hasSpouse (7) 61 96 72 22 31 68 36 
ORGEmploys(8) 92 82 22 4 7 35 7 
ORGLeader (9) 88 97 73 32 42 80 48 
hasBirthDate (10) 90 85 45 13 32 60 23 
Table 1: Precision, Recall, and F scores 
 
Figure 3: Fraction of recall instances with name 
mentions present in the sentence for both arguments. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1 2 3 4 5 6 7 8 9
%
 R
ec
al
l I
ns
ta
nc
es
291
of ?Coref for all relations. Thus while +Coref pays 
a price in precision for its improved recall, in many 
applications it may be a worthwhile tradeoff. 
Though one might expect that errors in coref-
erence would reduce precision of +Coref, such er-
rors may be balanced by the need to use longer 
patterns in ?Coref. These patterns often include 
error-prone wildcards which lead to a drop in pre-
cision. Patterns with multiple wildcards were also 
more likely to be removed as unreliable in manual 
pattern pruning, which may have harmed the recall 
of ?Coref, while improving its precision. 
5.3 Further Analysis 
Our analysis thus far has focused on micro-
reading which requires a system find all mentions 
of an instance relation ? i,e, in our evaluation Or-
gLeader(Apple, Steve Jobs) might occur in as 
many as 20 different contexts.  While ?Coref per-
forms poorly at micro-reading, it could still be ef-
fective for macro-reading, i.e. finding at least one 
instance of the relation OrgLeader(Apple, Steve 
Jobs). As a rough measure of this, we also evaluat-
ed recall by counting the number of test instances 
for which at least one answer was found by the two 
systems. With this method, +Coref?s recall is still 
higher for all but one relation type, although the 
gap between the systems narrows somewhat. 
 
In addition to our recall evaluation, we meas-
ured the number of sentences containing relation 
instances found by each of the systems when ap-
plied to 5,000 documents (see Table 3).  For al-
most all relations, +Coref matches many more 
sentences, including finding more sentences for 
those relations for which it has higher precision. 
6 Conclusion 
Our experiments suggest that in contexts where 
recall is important incorporating coreference into a 
relation extraction system may provide significant 
gains. Despite being noisy, coreference infor-
mation improved F-scores for all relations in our 
test, more than doubling the F-score for 5 of the 
10.  
Why is the high error rate of coreference not 
very harmful to +Coref?  We speculate that there 
are two reasons. First, during training, not all co-
reference is treated equally.  If the only evidence 
we have for a proposed instance depends on low 
confidence coreference links, it is very unlikely to 
be added to our instance set for use in future itera-
tions.  Second, for both training and runtime, many 
of the coreference links relevant for extracting the 
relation set examined here are fairly reliable, such 
as wh-words in relative clauses. 
There is room for more investigation of the 
question, however. It is also unclear if the same 
result would hold for a very different set of rela-
tions, especially those which are more event-like 
than relation-like. 
Acknowledgments 
This work was supported, in part, by DARPA un-
der AFRL Contract FA8750-09-C-179. The views 
expressed are those of the authors and do not re-
flect the official policy or position of the Depart-
ment of Defense or the U.S. Government. We 
would like to thank our reviewers for their helpful 
comments and Martha Friedman, Michael Heller, 
Elizabeth Roman, and Lorna Sigourney for doing 
our evaluation annotation.  
  
 +Coref -Coref #Test 
Instances 
ORGEmploys 8 2 20 
GPEEmploys 12 3 19 
hasSibling 11 4 19 
hasBirthDate 12 5 17 
hasSpouse 15 9 20 
ORGLeader 14 9 19 
attendedSchool 17 12 20 
hasBirthPlace 19 15 20 
GPELeader 15 13 19 
hasChild 6 6 19 
Table 2: Number of test seeds where at least one 
instance is found in the evaluation. 
Prec Number of Sentences 
Relation P+ P- +Cnt -Cnt *Cnt 
attendedSchool 83 97 541 212 544 
hasChild 91 96 661 68 106 
hasSpouse 87 99 1262 157 282 
hasSibling 87 97 313 72 272 
GPEEmploys 70 60 1208 308 313 
GPELeader 73 69 1018 629 644 
ORGEmploys 61 96 1698 142 209 
ORGLeader 92 82 1095 207 286 
hasBirthDate 88 97 231 131 182 
hasBirthPlace 90 85 836 388 558 
Table 3: Number of sentences in which each system 
found relation instances   
292
References 
E. Agichtein and L. Gravano. 2000. Snowball: extract-
ing relations from large plain-text collections. In 
Proceedings of the ACM Conference on Digital Li-
braries, pp. 85-94. 
M. Banko, M. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proceedings of the International 
Joint Conference on Artificial Intelligence. 
A. Baron and M. Freedman. 2008. Who is Who and 
What is What: Experiments in Cross Document Co-
Reference. In Empirical Methods in Natural Lan-
guage Processing.  
A. Blum and T. Mitchell. 1998. Combining Labeled and 
Unlabeled Data with Co-Training. In Proceedings of 
the 1998 Conference on Computational Learning 
Theory. 
E. Boschee, V. Punyakanok, R. Weischedel. 2008. An 
Exploratory Study Towards ?Machines that Learn to 
Read?. Proceedings of AAAI BICA Fall Symposium. 
J. Chen, D. Ji, C. Tan and Z. Niu. 2006. Relation extrac-
tion using label propagation based semi-supervised 
learning. COLING-ACL 2006: 129-136. 
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and 
R. Wang. 2009. Populating the Semantic Web by 
Macro-Reading Internet Text. Invited paper, Pro-
ceedings of the 8th International Semantic Web Con-
ference (ISWC 2009).  
National Institute of Standards and Technology.  2007. 
NIST 2007 Automatic Content Extraction Evaluation 
Official Results. http://www.itl.nist.gov/iad/mig/ 
tests/ace/2007/doc/ace07_eval_official_results 
_20070402.html 
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging Generic Patterns for Automatically Harvesting 
Semantic Relations. In Proceedings of Conference on 
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120. 
Sydney, Australia.  
L. Ramshaw, E. Boschee, S. Bratus, S. Miller, R. Stone, 
R. Weischedel, A. Zamanian. 2001. Experiments in 
multi-modal automatic content extraction, In Pro-
ceedings of Human Language Technology Confer-
ence.  
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride, 
R. Weischedel, A. Zamanian. 2011. SERIF Language 
Processing ? Efficient Trainable Language Under-
standing. In Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global 
Autonomous Language Exploitation. Springer. 
D. Ravichandran and E. Hovy. 2002. Learning surface 
text patterns for a question answering system. In 
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2002), 
pages 41?47, Philadelphia, PA.  
E. Riloff. 1996. Automatically generating extraction 
patterns from untagged text. In Proceedings of the 
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044-1049.  
G. Zhou, J. Li, L. Qian, Q. Zhu. 2008. Semi-Supervised 
Learning for Relation Extraction. Proceedings of the 
Third International Joint Conference on Natural 
Language Processing: Volume-I.  
Z. Kozareva and E. Hovy. Not All Seeds Are Equal: 
Measuring the Quality of Text Mining Seeds. 2010. 
Human Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the As-
sociation for Computational Linguistics pp. 618-626. 
 
 
293
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 341?345,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language Use: What can it Tell us? 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
 
 
Abstract 
For 20 years, information extraction has fo-
cused on facts expressed in text. In contrast, 
this paper is a snapshot of research in progress 
on inferring properties and relationships 
among participants in dialogs, even though 
these properties/relationships need not be ex-
pressed as facts. For instance, can a machine 
detect that someone is attempting to persuade 
another to action or to change beliefs or is as-
serting their credibility? We report results on 
both English and Arabic discussion forums. 
1 Introduction 
Extracting explicitly stated information has been 
tested in MUC1 and ACE2 evaluations. For exam-
ple, for the text Mushaima'a, head of the opposi-
tion Haq movement, an ACE system extracts the 
relation LeaderOf(Mushaima'a, HaqMovement). In 
TREC QA3 systems answered questions, e.g.  
?When was Mozart born??, for which the answer is 
contained in one or a few extracted text phrases.  
Sentiment analysis uses implicit meaning of 
text, but has focused primarily on text known to be 
rich in opinions (product reviews, editorials) and 
delves into only one aspect of implicit meaning.  
Our long-term goal is to predict social roles in 
informal group discussion from language uses 
(LU), even if those roles are not explicitly stated; 
for example, using the communication during a 
meeting, identify the leader of a group. This paper 
provides a snapshot of preliminary, ongoing re-
search in predicting two classes of language use: 
                                                          
1
 http://www-nlpir.nist.gov/related_projects/muc/ 
2
 http://www.nist.gov/speech/tests/ace/ 
3
 http://trec.nist.gov/data/qa.html 
Establish-Credibility and Attempt-To-Persuade. 
Technical challenges include dealing with the facts 
that those LUs are rare and subjective and that hu-
man judgments have low agreement.  
Our hybrid statistical & rule-based approach 
detects those two LUs in English and Arabic. Our 
results are that (1) annotation at the message (turn) 
level provides training data useful for predicting 
rare phenomena at the discussion level while re-
ducing the requirement for turn-level predictions to 
be accurate; (2)weighing subjective judgments 
overcomes the need for high annotator consistency. 
Because the phenomena are rare, always predicting 
the absence of a LU is a very high baseline. For 
English, the system beats those baselines. For Ara-
bic, more work is required, since only 10-20% of 
the amount of training data exists so far.  
2 Language Uses (LUs) 
A language use refers to an aspect of the social 
intention of how a communicator uses language.  
The information that supports a decision about an 
implicit social action or role is likely to be distrib-
uted over more than one turn in a dialog; therefore, 
a language use is defined, annotated, and predicted 
across a thread in the dialog. Because our current 
work uses discussion forums, threads provide a 
natural, explicit unit of analysis. Our current work 
studies two language uses.  
An Attempt-to-Persuade occurs when a poster 
tries to convince other participants to change their 
beliefs or actions over the course of a thread. Typi-
cally, there is at least some resistance on the part of 
the posters being persuaded. To distinguish be-
tween actual persuasion and discussions that in-
volve differing opinions, a poster needs to engage 
341
in multiple persuasion posts (turns) to be consid-
ered exhibiting the LU.  
Establish-Credibility occurs when a poster at-
tempts to increase their standing within the group. 
This can be evidenced with any of several moves, 
e.g., explicit statements of authority, demonstration 
expertise through knowledge, providing verifiable 
information (e.g., from a trusted source or citing 
confirmable facts), or providing a justified opinion 
(e.g., a logical argument or personal experience).  
3 Challenges 
There were two significant challenges: (a) sparsity 
of the LUs, and (b) inter-annotator agreement. To 
address the sparsity of data, we tried to automati-
cally select data that was likely to contain content 
of interest. Data selection focused on the number 
of messages and posters in a thread, as well as the 
frequency of known indicators like quotations. 
(withheld). Despite these efforts, the LUs of inter-
est were rare, especially in Arabic.  
Annotation was developed using cycles of 
guideline development, annotation, evaluation of 
agreement, and revision of guidelines. Elsewhere, 
similar, iterative annotation processes have yielded 
significant improvements in agreement for word 
sense and coreference (Hovy et al, 2006). While 
LUs were annotated for a poster over the full 
thread, annotators also marked specific messages 
in the thread for presence of evidence of the lan-
guage use. Table 1 includes annotator consistency 
at both the evidence (message) and LU level.   
 English Arabic 
 Msg LU Msg LU 
 Agr # Agr # Agr # Agr # 
Per. 0.68 4722 0.75 2151 0.57 652 0.49 360 
Cred. 0.66 3594 0.68 1609 0.35 652 0.45 360 
Table 1: Number of Annotated Data Units and Annota-
tor Agreement (measured as F) 
The consistency numbers for this task were sig-
nificantly lower than we have seen in other lan-
guage processing tasks. Discussions suggested that 
disagreement did not come from a misunderstand-
ing of the task but was the result of differing intui-
tions about difficult-to-define labels. In the 
following two sections, we describe how the eval-
uation framework and system development pro-
ceeded despite low levels of consistency.  
4 Evaluation Framework 
Task. The task is to predict for every participant in 
a given thread, whether the participant exhibits 
Attempt-to-Persuade and/or Establish-Credibility. 
If there is insufficient evidence of an LU for a par-
ticipant, then the LU value for that poster is nega-
tive. The external evaluation measured LU 
predictions. Internally we measured predictions of 
message-level evidence as well. 
Corpora. For English, 139 threads from 
Google Groups and LiveJournal have been anno-
tated for Attempt-to-Persuade, and 103 threads for 
Attempt-to-Establish-Credibility. For Arabic, 
threads were collected from al-handasa.net.4 31 
threads were annotated for both tasks. Counts of 
annotated messages appear in Table 1. 
Measures. Due to low annotator agreement, at-
tempting to resolve annotation disagreement by the 
standard adjudication process was too time-
consuming. Instead, the evaluation scheme, similar 
to the pyramid scheme used for summarization 
evaluation, assigns scores to each example based 
on its level of agreement among the annotators. 
Specifically, each example is assigned positive and 
negative scores, p = n+/N and n = n-/N, where n+ is 
the number of annotators that annotate the example 
as positive, and n- for the negative. N is the total 
number of annotators. A system that outputs posi-
tive on the example results in p correct and n incor-
rect. The system gets p incorrect and n correct for 
predicting negative. Partial accuracy and F-
measure can then be computed. 
Formally, let X = {xi} be a set of examples. 
Each example xi is associated with positive and 
negative scores, pi and ni. Let ri = 1 if the system 
outputs positive for example xi and 0 for negative. 
The partial accuracy, recall, precision, and F-
measure can be computed by: 
pA = 100??i(ripi+(1-ri)ni) / ?i(pi+ni) 
pR = 100??iripi / ?ipi 
pP = 100? ?iripi / ?iri 
pF = 2 pR pP/(pR+pP) 
The maximum pA and pF may be less than 100 
when there is disagreement between annotators. To 
achieve accuracy and F scores on a scale of 100, 
pA and pF are normalized using the maximum 
achievable scores with respect to the data. 
npA = 100?pA/max(pA) 
npF = 100?pF/max(pF) 
                                                          
4
 URLs and judgments are available by email. 
342
5 System and Empirical Results 
Our architecture is shown in Figure 1. We process 
a thread in three stages: (1) linguistic analysis of 
each message (post) to yield features, (2) Predic-
tion of message-level properties using an SVM on 
the extracted features, and (3) Simple rules that 
predict language uses over the thread.  
 
Figure 1: Message and LU Prediction 
Phase 1: The SERIF Information Extraction 
Engine extracts features which are designed to cap-
ture different aspects of the posts. The features in-
clude simple features that can be extracted from 
the surface text of the posts and the structure of the 
posts within the threads. These may correlate di-
rectly or indirectly correlate to the language uses. 
In addition, more syntactic and semantic-driven 
features are also used. These can indicate the spe-
cific purpose of the sentences; specifically target-
ing directives, imperatives, or shows authority. The 
following is a partial list of features which are used 
both in isolation and in combination with each oth-
er. 
Surface and structural features: average sen-
tence length; number of names, pronouns, and dis-
tinct entities; number of sentences, URLs (links), 
paragraphs and out-of-vocabulary words; special 
styles (bold, italics, stereotypical punctuation e.g. 
!!!! ), depth in thread, and presence of a quotation. 
Syntactic and semantic features: predicate-
argument structure including the main verb, sub-
ject, object, indirect object, adverbial modifier, 
modal modifier, and negation, imperative verbs, 
injection words, subjective words, and mentions of 
attack events. 
Phase 2: Given training data from the message 
level (Section 3), an SVM predicts if the post con-
tains evidence for an LU. The motivation for this 
level is (1) Posts provide a compact unit with reli-
ably extractable, specific, explicit features. (2) 
There is more training data at the post level. (3) 
Pointing to posts offers a more clear justification 
for the predictions. (4) In our experiments, errors 
here do not seem to percolate to the thread level. In 
fact, accuracy at the message level is not directly 
predictive of accuracy at the thread level. 
Phase 3: Given the infrequency of the Attempt-
to-Persuade and Establish-Credibility LUs, we 
wrote a few rules to predict LUs over threads, giv-
en the predictions at the message level. For in-
stance, if the number of messages with evidence 
for persuasion is greater than 2 from a given partic-
ipant, then the system predicts AttemptToPer-
suade. Phase 3 is by design somewhat robust to 
errors in Phase 2. To predict that a poster is exhib-
iting the Attempt-to-Persuade LU, the system need 
not find every piece of evidence that the LU is pre-
sent, but rather just needs to find sufficient evi-
dence for identifying the LU.  
Our message level classifiers were trained with 
an SVM that optimizes F-measure (Joachims, 
2005). Because annotation disagreement is a major 
challenge, we experimented with various ways to 
account for (and make use of) noisy, dual annotat-
ed text. Initially, we resolved the disagreement au-
tomatically, i.e. removing examples with 
disagreement; treating an example as negative if 
any annotator marked the example negative; and 
treating an example as positive if any annotator 
marked the example as positive. An alternative 
(and more principled) approach is to incorporate 
positive and negative scores for each example into 
the optimization procedure. Because each example 
was annotated by the same number of annotators (2 
in this case), we are able to treat each annotator?s 
decision as an independent example without aug-
menting the SVM optimization process.  
The results below use the training procedure 
that performed best on the leave-one-thread-out 
cross validation results (Table 23 and Table 34). 
Counts of threads appear in Section 4. We compare 
our system?s performance (S) with two simple 
baselines. Baseline-A (A) always predicts absent 
for the LU/evidence. Baseline-P (P) predicts posi-
tive (present) for all messages/LUs. Table 4Table 3 
shows results for predicting message level evi-
dence of an LU (Phase 2). Table 5Table 4 shows 
performance on the task of predicting an LU for 
each poster. 
The results show significantly worse perfor-
mance in Arabic than English-- not surprising con-
sidering 5-10-fold difference in training examples. 
Additionally, Arabic messages are much shorter, 
and the phenomena is even more rare (as illustrated 
by the high npA, accuracy, of the A baseline).  
343
  Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 72.5 83.2 0.0 0.0 77.6 95.0 0.0 0.0 
P 40.4 29.7 61.1 50.7 33.9 14.4 54.5 30.9 
S 86.5 81.3 79.2 61.9 86.7 95.5 73.9 54.0 
Table 43: Performance on Message Level Evidence 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 90.9 86.7 0.0 0.0 87.7 90.2 0.0 0.0 
P 12.1 27.0 23.8 48.2 18.0 21.5 33.7 41.1 
S 94.6 88.3 76.8 38.8 95.1 92.4 80.0 36.0 
Table 54: Cross Validation Performance on Poster LUs  
Table 6Table 5 shows LU prediction results 
from an external evaluation on held out data. Un-
like our dataset, each example in the external eval-
uation dataset was annotated by 3 annotators. The 
results are similar to our internal experiment. 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 96.2 98.4 0.0 0.0 93.6 94.0 93.6 0.0 
P 13.1 4.2 27.6 11.7 11.1 10.1 11.1 22.2 
S 96.5 94.6 75.1 59.1 97.7 92.5 97.7 24.7 
Table 65: External, Held-Out Results on Poster LUs  
6 Related Research 
Research in authorship profiling (Chung & Penne-
baker, 2007; Argamon et al in press; and Abbasi 
and Chen, 2005) has identified traits, such as sta-
tus, sex, age, gender, and native language. Models 
and predictions in this field have primarily used 
simple word-based features, e.g. occurrence and 
frequency of function words. 
Social science researchers have studied how so-
cial roles develop in online communities (Fisher, et 
al., 2006), and have attempted to categorize these 
roles in multiple ways (Golder and Donath 2004; 
Turner et al, 2005). Welser et al (2007) have in-
vestigated the feasibility of detecting such roles 
automatically using posting frequency (but not the 
content of the messages). 
Sentiment analysis requires understanding the 
implicit nature of the text. Work on perspective 
and sentiment analysis frequently uses a corpus 
known to be rich in sentiment such as reviews or 
editorials (e.g. (Hardisty, 2010), (Somasundaran& 
Weibe, 2009). The MPQA corpus (Weibe, 2005) 
annotates polarity for sentences in newswire, but 
the focus of this corpus is at the sentence level. 
Both the MPQA corpus and the various corpora of 
editorials and reviews have tended towards more 
formal, edited, non-conversational text. Our work 
in contrast, specifically targets interactive discus-
sions in an informal setting. Work outside of com-
putational linguistics that has looked at persuasion 
has tended to examine language in a persuasive 
context (e.g. sales, advertising, or negotiations).  
Like the current work, Strzalkowski, et al 
(2010) investigates language uses over informal 
dialogue. Their work focuses on chat transcripts in 
an experimental setting designed to be rich in the 
phenomena of interest. Like our work, their predic-
tions operate over the conversation, and not a sin-
gle utterance. The specific language uses in their 
work (topic/task control, involvement, and disa-
greement) are different than those discussed here. 
Our work also differs in the data type of interest. 
We work with threaded online discussions in 
which the phenomena in question are rare. Our 
annotators and system must distinguish between 
the language use and text that is opinionated with-
out an intention to persuade or establish credibility.   
7 Conclusions and Future Work 
In this work in progress, we presented a hybrid 
statistical & rule-based approach to detecting prop-
erties not explicitly stated, but evident from lan-
guage use. Annotation at the message (turn) level 
provided training data useful for predicting rare 
phenomena at the discussion level while reducing 
the need for turn-level predictions to be accurate. 
Weighing subjective judgments overcame the need 
for high annotator consistency. For English, the 
system beats both baselines with respect to accura-
cy and F, despite the fact that because the phenom-
ena are rare, always predicting the absence of a 
language use is a high baseline. For Arabic, more 
work is required, particularly since only 10-20% of 
the amount of training data exists so far. 
This work has explored LUs, the implicit, social 
purpose behind the words of a message. Future 
work will explore incorporating LU predictions to 
predict the social roles played by the participants in 
a thread, for example using persuasion and credi-
bility to establish which participants in a discus-
sion are serving as informal leaders.  
344
Acknowledgement 
This research was funded by the Office of the Director 
of National Intelligence (ODNI), Intelligence Advanced 
Research Projects Activity (IARPA), through the _____.  
All statements of fact, opinion or conclusions contained 
herein are those of the authors and should not be con-
strued as representing the official views or policies of 
IARPA, the ODNI or the U.S. Government. 
References 
Argamon, S., Koppel, M., Pennebaker, J.W., and Schler, 
J. (2009). ?Automatically profiling the author of 
an anonymous text?. Communications of the Asso-
ciation for Computing Machinery (CACM). Vol-
ume 52 Issue 2. 
Abbasi A., and Chen H. (2005). ?Applying authorship 
analysis to extremist-group web forum messages?. 
In IEEE Intelligent Systems, 20(5), pp. 67?75. 
Boyd, D, Golder, S, and Lotan, G. (2010). ?Tweet, 
Tweet, Retweet: Conversational Aspects of Re-
tweeting on Twitter.? HICSS-43. IEEE: Kauai, HI. 
Chung, C.K., and Pennebaker, J.W. (2007). ?The psy-
chological functions of function words?. In K. 
Fiedler (Ed.), Social communication, pp. 343-359. 
New York: Psychology Press. 
Golder S., and Donath J. (2004) "Social Roles in Elec-
tronic Communities," presented at the Association 
of Internet Researchers (AoIR). Brighton, England 
Hovy E., Marcus M., Palmer M., Ramshaw L., and 
Weischedel R. (2006). ?Ontonotes: The 90% solu-
tion?. In Proceedings of the Human Language 
Technology Conference of the NAACL, Compan-
ion Volume: Short Papers, pp. 57?60. Association 
for Computational Linguistics, New York City, 
USA. 
Joachims, T. (2005), ?A Support Vector Method for 
Multivariate Performance Measures?, Proceedings 
of the International Conference on Machine 
Learning (ICML). 
Kelly, J., Fisher, D., Smith, D., (2006) ?Friends, foes, 
and fringe: norms and structure in political discus-
sion networks?, Proceedings of the 2006 interna-
tional conference on Digital government research.  
NIST Speech Group. (2008). ?The ACE 2008 evalua-
tion plan: Assessment of Detection and Recogni-
tion of Entities and Relations Within and Across 
Documents?. 
http://www.nist.gov/speech/tests/ace/2008/doc/ace
08 -evalplan.v1.2d.pdf 
Ranganath, R., Jurafsky, D., and McFarland, D. (2009) 
?It?s Not You, it?s Me: Detecting Flirting and its 
Misperception in Speed-Dates? Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pages 334?342. 
Somasundaran, S & Wiebe, J
 
(2009). Recognizing 
Stances in Online Debates. ACL-IJCNLP 2009. 
Strzalkowski, T, Broadwell, G, Stromer-Galley, J, 
Shaikh, S, Taylor, S and Webb, N. (2010) ?Model-
ing Socio-Cultural Phenomena in Discourse?. 
Proceedings of the 23rd International Conference 
on Computational Linguistics (Coling 2010), pag-
es 1038?1046, Beijing, August 2010 
Turner T. C., Smith M. A., Fisher D., and Welser H. T. 
(2005) ?Picturing Usenet: Mapping computer-
mediated collective action?. In Journal of Com-
puter-Mediated Communication, 10(4). 
Voorhees, E. & Tice, D. (2000)."Building a Question 
Answering Test Collection", Proceedings of 
SIGIR, pp. 200-207. 
Welser H. T., Gleave E., Fisher D., and Smith M., 
(2007). "Visualizing the signatures of social roles in 
online discussion groups," In The Journal of Social 
Structure, vol. 8, no. 2. 
Wiebe, J, Wilson, T and Cardie, C (2005). Annotating 
expressions of opinions and emotions in language. 
Language Resources and Evaluation, volume 39, is-
sue 2-3, pp. 165-210. 
 
 
345
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 61?69,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Empirical Studies in Learning to Read 
Marjorie Freedman, Edward Loper, Elizabeth Boschee, Ralph Weischedel 
BBN Raytheon Technologies 
10 Moulton St 
Cambridge, MA 02139 
{mfreedma, eloper, eboschee, weischedel}@bbn.com 
Abstract 
In this paper, we present empirical results on 
the challenge of learning to read. That is, giv-
en a handful of examples of the concepts and 
relations in an ontology and a large corpus, 
the system should learn to map from text to 
the concepts/relations of the ontology. In this 
paper, we report contrastive experiments on 
the recall, precision, and F-measure (F) of the 
mapping in the following conditions: (1) em-
ploying word-based patterns, employing se-
mantic structure, and combining the two; and 
(2) fully automatic learning versus allowing 
minimal questions of a human informant. 
1 Introduction 
This paper reports empirical results with an algo-
rithm that ?learns to read? text and map that text 
into concepts and relations in an ontology specified 
by the user. Our approach uses unsupervised and 
semi-supervised algorithms to harness the diversity 
and redundancy of the ways concepts and relations 
are expressed in document collections. Diversity 
can be used to automatically generate patterns and 
paraphrases for new concepts and relations to 
boost recall. Redundancy can be exploited to au-
tomatically check and improve the accuracy of 
those patterns, allowing for system learning with-
out human supervision.  
For example, the system learns how to recog-
nize a new relation (e.g. invent), starting from 5-20 
instances (e.g. Thomas Edison + the light bulb). 
The system iteratively searches a collection of 
documents to find sentences where those instances 
are expressed (e.g. ?Thomas Edison?s patent for 
the light bulb?), induces patterns over textual fea-
tures found in those instances (e.g. pa-
tent(possessive:A, for:B)), and repeats the cycle by 
applying the generated patterns to find additional 
instances followed by inducing more patterns from 
those instances. Unsupervised measures of redun-
dancy and coverage are used to estimate the relia-
bility of the induced patterns and learned instances; 
only the most reliable are added, which minimizes 
the amount of noise introduced at each step.  
There have been two approaches to evaluation 
of mapping text to concepts and relations: Auto-
matic Content Extraction (ACE)1 and Knowledge 
Base Population (KBP)2. In ACE, complete ma-
nual annotation for a small corpus (~25k words) 
was possible; thus, both recall and precision could 
be measured across every instance in the test set. 
This evaluation can be termed micro reading in 
that it evaluates every concept/relation mention in 
the corpus. In ACE, learning algorithms had 
roughly 300k words of training data.  
By contrast, in KBP, the corpus of documents 
in the test set was too large for a complete answer 
key. Rather than a complete answer key, relations 
were extracted for a list of entities; system output 
was pooled and judged manually. This type of 
reading has been termed macro reading3, since 
finding any instance of the relation in the 1.3M 
document corpus is measured success, rather than 
finding every instance. Only 118 queries were pro-
vided, though several hundred were created and 
distributed by participants.  
In the study in this paper, recall, precision, and 
F are measured for 11 relations under the following 
contrastive conditions 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://apl.jhu.edu/~paulmac/kbp.html  
3 See http://rtw.ml.cmu.edu/papers/mitchell-iswc09.pdf   
61
1. Patterns based on words vs. predicate-
argument structure vs. combining both. 
2. Fully automatic vs. a few periodic res-
ponses by humans to specific queries. 
Though many prior studies have focused on 
precision, e.g., to find any text justification to an-
swer a question, we focus equally on recall and 
report recall performance as well as precision. This 
addresses the challenge of finding information on 
rarely mentioned entities (no matter how challeng-
ing the expression). We believe the effect will be 
improved technology overall. We evaluate our sys-
tem in a micro-reading context on 11 relations. In a 
fully automatic configuration, the system achieves 
an F of .48 (Recall=.37, Precision=.68). With li-
mited human intervention, F rises to .58 (Re-
call=.49, Precision=.70). We see that patterns 
based on predicate-argument structure (text 
graphs) outperform patterns based on surface 
strings with respect to both precision and recall. 
Section 2 describes our approach; section 3, 
some challenges; section 4, the implementation; 
section 5, evaluation; section 6, empirical results 
on extraction type; section 7, the effect of periodic, 
limited human feedback; section 8, related work; 
and section 9, lessons learned and conclusions. 
2 Approach 
Our approach for learning patterns that can be used 
to detect relations is depicted in Figure 1. Initially, 
a few instances of the relation tuples are provided, 
along with a massive corpus, e.g., the web or the 
gigaword corpus from the Linguistic Data Consor-
tium (LDC). The diagram shows three inventor-
invention pairs, beginning with Thomas Edi-
son?light bulb. From these, we find candidate 
sentences in the massive corpus, e.g., Thomas Edi-
son invented the light bulb. Features extracted from 
the sentences retrieved, for example features of the 
text-graph (the predicate-argument structure con-
necting the two arguments), provide a training in-
stance for pattern induction. The induced patterns 
are added to the collection (database) of patterns.
Running the extended pattern collection over the 
corpus finds new, previously unseen relation 
tuples. From these new tuples, additional sentences 
which express those tuples can be retrieved, and 
the cycle of learning can continue. 
There is an analogous cycle of learning con-
cepts from instances and the large corpus; the ex-
periments in this paper do not report on that paral-
lel learning cycle. 
Figure 1: Approach to Learning Relations 
At the ith iteration, the steps are 
1. Given the set of hypothesized instances of the 
relation (triples HTi), find instances of such 
triples in the corpus. (On the first iteration, 
?hypothesized? triples are manually-generated 
seed examples.) 
2. Induce possible patterns. For each proposed 
pattern P: 
a. Apply pattern P to the corpus to generate a 
set of triples TP
b. Estimate precision as the confidence-
weighted average of the scores of the 
triples in TP. Reduce precision score by the 
percentage of triples in TP that violate us-
er-specified relation constraints (e.g. arity 
constraints described in 4.3)  
c. Estimate recall as the confidence-weighted 
percentage of triples in HTi found by the 
pattern 
3. Identify a set of high-confidence patterns HPi
using cutoffs automatically derived from rank-
based curves for precision, recall, and F-
measure (?=0.7) 
4. Apply high-confidence patterns to a Web-scale 
corpus to hypothesize new triples. For each 
proposed triple T 
a. Estimate score(T) as the expected proba-
bility that T is correct, calculated by com-
bining the respective precision and recall 
scores of all of the patterns that did or did 
not return it (using the Na?ve Bayes as-
sumption that all patterns are independent) 
b. Estimate confidence(T) as the percentage 
of patterns in HPi by which T was found 
5. Identify a set of high-confidence triples HTi+1

	














	


	 







	



	

	
	
 

	 
