Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 243?247,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Toward Automatically Assembling Hittite-Language Cuneiform Tablet
Fragments into Larger Texts
Stephen Tyndall
University of Michigan
styndall@umich.edu
Abstract
This paper presents the problem within Hit-
tite and Ancient Near Eastern studies of frag-
mented and damaged cuneiform texts, and
proposes to use well-known text classification
metrics, in combination with some facts about
the structure of Hittite-language cuneiform
texts, to help classify a number of fragments of
clay cuneiform-script tablets into more com-
plete texts. In particular, I propose using
Sumerian and Akkadian ideogrammatic signs
within Hittite texts to improve the perfor-
mance of Naive Bayes and Maximum Entropy
classifiers. The performance in some cases
is improved, and in some cases very much
not, suggesting that the variable frequency of
occurrence of these ideograms in individual
fragments makes considerable difference in
the ideal choice for a classification method.
Further, complexities of the writing system
and the digital availability of Hittite texts com-
plicate the problem.
1 Introduction
The Hittite empire, in existence for about 600 years
between 1800 and 1200 BCE, left numerous histori-
cal, political, and literary documents behind, written
in cuneiform in clay tablets. There are a number of
common problems that confront Hittite scholars in-
terested in any subdiscipline of Hittitology, be it his-
tory, philology, or linguistics. Horst Klengel sum-
marizes the issue most crucial to this paper:
Some general problems, affecting both
philologists and historians, are caused by
the Hittite textual tradition itself. First,
the bulk of the cuneiform material is frag-
mentary. The tablets, discovered in var-
ious depots in the Hittite capital and in
some provincial centers, normally were of
a larger size. When the archives were de-
stroyed, the tablets for the most part broke
into many pieces. Therefore, the joining
of fragments became an important prereq-
uisite for interpretation(Klengel, 2002).
Most Hittite texts are broken, but a number exist
in more than one fragmentary copy.
Figure 1 shows a photograph, taken from the
University of Meinz Konkordanz der hethitischen
Texte1, of a typical Hittite cuneiform fragment.
Complete or partially-complete texts are assem-
bled from collections of fragments based on shape,
writing size and style, and sentence similarity. Joins
between fragments are not made systematically, but
are usually discovered by scholars assembling large
numbers of fragments that reference a specific sub-
ject, like some joins recently made in Hittite treaty
documents in (Beckman, 1997).
Joins are thus fairly rare compared to the fre-
quency of new publishing of fragments. Such joins
and the larger texts created therewith are catalogued
according to a CTH (Catalogue des Textes Hittites2)
number. Each individual text is composed of one or
more cuneiform fragments belonging to one or more
copies of a single original work.
1available at http://www.hethport.
uni-wuerzburg.de/HPM/hethportlinks.html
2available at http://www.hethport.
uni-wuerzburg.de/CTH/
243
Figure 2 shows a published join in hand-copied
cuneiform fragments. In this case, the fragments are
not contiguous, and only the text on the two frag-
ments was used to make the join.
The task then, for the purposes of this paper, is
to connect unknown fragments of Hittite cuneiform
tablets with larger texts. I?m viewing this as a text
classification task, where larger, CTH-numbered
texts are the categories, and small fragments are the
bits of text to be assigned to these categories.
2 The Corpus of Hittite
Hittite cuneiform consists of a mix of syllabic writ-
ing for Hittite words and logographic writing, typ-
ically Sumerian ideograms, standing in for Hittite
words. Most words are written out phonologically
using syllabic signs, in structure mostly CV and VC,
and a few CVC. Some common words are written
with logograms from other Ancient Near Eastern
languages, e.g. Hittite antuhs?a- ?man? is commonly
written with the Sumerian-language logogram tran-
scribed LU?. Such writings are called Sumerograms
or Akkadograms, depending on the language from
which the ideogram is taken.
The extant corpus of Hittite consists of more than
30,000 clay tablets and fragments excavated at sites
in Turkey, Syria, and Egypt (Hoffner and Melchert,
2008, 2-3). Many of these fragments are assigned to
one of the 835 texts catalogued in the CTH.
3 Prior Work
A large number of prior studies on text classifica-
tion have informed the progress of this study. Cat-
egorization of texts into genres is very well studied
(Dewdney et al, 2001). Other related text classi-
fication studies have looked at classifying text by
source, in contexts of speech, as in an attempt to
classify some segments of speech into native and
non-native speaker categories (Tomokiyo and Jones,
2001), and writing and authorship, as in the fa-
mous Federalist Papers study(Mosteller and Wal-
lace, 1984), and context, as in a categorization of
a set of articles according to which newspaper they
appeared in (Argamon-Engelson et al, 1998).
Measures of similarity among sections of a single
document bear a closer relation to this project than
the works above. Previous studies have examined in-
Figure 1: Photograph of a Hittite Tablet Fragment
Figure 2: Published Fragment Join
244
ternal document similarity, using some vector-based
metrics to judge whether documents maintain the
same subject throughout (Nicholson, 2009).
Very little computational work on cuneiform lan-
guages or texts exists. The most notable example
is a study that examined grapheme distribution as
a way to understand Hurrian substratal interference
in the orthography of Akkadian-language cuneiform
texts written in the Hurrian-speaking town of Nuzi
(Smith, 2007). Smith?s work, though using different
classifying methods and and an enormously differ-
ent corpus on a language with different characteris-
tics, is the most similar to this study, since both are
attempts to classify cuneiform fragments into cat-
egories - in Smith?s case, into Hurrian-influenced
Nuzi Akkadian and non-Nuzi standard Akkadian.
4 The Project Corpus
For this project, I use a corpus of neo-Hittite
fragment transcriptions available from H. Craig
Melchert (Melchert, ). The corpus is one large text
file, divided into CTH numbered sections, which
themselves are divided into fragments labeled by
their publication numbers - mostly KUB, which
stands for Keilschrifturkunden aus Boghazko?i or
KBo, Keilschrifttexte aus Boghazko?i, the two major
publications for Hittite text fragments.
I restricted the fragments used in this project to
fragments belonging to texts known to exist in at
least two copies, a choice that produces a larger
number of fragments per text without requiring a
judgment about what number of fragments in a text
constitutes ?fragmented enough? for a legitimate test
of this task. This leaves 36 total CTH-numbered
texts, consisting of 389 total fragments.
The fragments themselves are included as plain
text, with restorations by the transcribers left intact
and set off by brackets, in the manner typical of
cuneiform transcription. In transcription, signs with
phonemic value are written in lower case characters,
while ideograms are represented in all caps. Sign
boundaries are represented by a hyphen, indicating
the next sign is part of the current word, by an equals
sign, indicating the next sign is a clitic, or a space,
indicating that the next sign is part of a new word.
{KUB XXXI 25; DS 29}
x
[ ]A-NA KUR URUHa[t-ti?
[ i]s-tar-ni=sum-m[i
[ ]x nu=kn ki-x[
[ ] KUR URUMi-iz-ri=y[a
[is-tar-ni]=sum-mi e-es-du [
[ ] nu=kn A-NA KUR URUMi-iz-ri[
[A-NA EGI]R UDmi is-tar-ni=su[m-mi
This fragment, KUB XXI25, is very small and
broken on both sides. The areas between brackets
are sections of the text broken off or effaced by ero-
sion of tablet surface material. Any text present be-
tween brackets has been inferred from context and
transcriber experience with usual phrasing in Hittite.
In the last line, the sign EGIR, a Sumerian ideogram,
which is split by a bracket, was partially effaced but
still recognizable to the transcriber, and so is split by
a bracket.
5 Methods
For this project, I used both Naive Bayes and Max-
imum Entropy classifiers as implemented by the
MAchine Learning for LanguagE Toolkit, MAL-
LET(McCallum, 2002).
Two copies of the corpus were prepared. In
one, anything in brackets or partially remaining after
brackets was removed, leaving only characters actu-
ally preserved on the fragment. This copy is called
Plain Cuneiform in the results section. The other
has all bracket characters removed, leaving all actual
characters and all characters suggested by the tran-
scribers. This corpus is called Brackets Removed in
the results section. By removing the brackets but
leaving the suggested characters, I hoped to use the
transcribers? intuitions about Hittite texts to further
improve the performance of both classifiers.
The corpora were tokenized in two ways:
1. The tokens were defined only by spaces, cap-
turing all words in the corpus.
2. The tokens were defined as a series of capital
letters and punctuation marks, capturing only
the Sumerian and Akkadian ideograms in the
text, i.e. the very common Sumerian ideogram
DINGER.MES?, ?the gods?.
The training and tests were all performed using
MALLET?s standard algorithms, cross-validated,
245
Table 1: Results for Plain Corpus
Tokenization Naive Bayes Max Ent
All Tokens .55 .61
Ideograms Only .44 .51
Table 2: Results for Tests on Corpus with Brackets Re-
moved
Tokenization Naive Bayes Max Ent
All Tokens .64 .67
Ideograms Only .49 .54
splitting the data randomly into ten parts, and using
9 parts of the data as a training set and 1 part of the
data as a test set. This means that each set was tested
ten times, with all of the data eventually being used
as part of the testing phase.
6 Results and Discussion
Accuracy values from the classifiers using the Plain
corpus, and from the corpus with the Brackets Re-
moved, are presented in Tables 1 and 2, respec-
tively. The measures are raw accuracy, the fraction
of the test fragments that the methods categorized
correctly.
The results for the Plain Corpus show that the
Naive Bayes classifier was 55% accurate with all to-
kens, and 44% accurate with ideograms alone. The
Maximum Entropy classifier was 61% accurate with
all tokens, and 51% accurate with ideograms only.
Both classifiers performed better with the Brack-
ets Removed corpus. The Naive Bayes classifier was
accurate 64% of the time with all tokens and 49% of
the time with ideograms only. The Maximum En-
tropy classifier was 67% accurate with all tokens,
and 54% accurate with ideograms only.
The predicted increase in accuracy using
ideograms was not upheld by the above tests. It may
be the case that Sumerograms and Akkadograms
are insufficiently frequent, particularly in smaller
fragments, to allow for correct categorization.
Some early tests suggested occasional excellent
results for this tokenization scheme, including a
single random 90-10 training/test run that showed
a test accuracy of .86, much higher than any larger
cross-validated test included above. This suggests,
perhaps unsurprisingly, that the accuracy of classi-
fication using Sumerograms and Akkadograms is
heavily dependent on the structure of the fragments
in question.
Maximum Entropy classification proved to be
slightly better, in every instance, than Naive Bayes
classification, a fact that will prove useful in future
tests and applications.
The fact that removing the brackets and includ-
ing the transcribers? additions improved the perfor-
mance of all classifiers will likewise prove useful,
since transcriptions of fragments are typically pub-
lished with such bracketed additions. It also seems
to demonstrate the quality of these additions made
by transcribers.
Overall, these tests suggest that in general, the
?use-everything? approach is better for accurate clas-
sification of Hittite tablet fragments with larger CTH
texts. However, in some cases, when the fragments
in question have a large number of Sumerograms
and Akkadograms, using them exclusively may be
the right choice.
7 Implications and Further Work
In the future, I hope to continue with a number of
other approaches to this problem, including lemma-
tizing the various Hittite noun and verb paradigms.
Additionally, viewing the problem in other ways,
e.g. regarding tablet fragments as elements for con-
nection by clustering algorithms, might work well.
Given the large number of small fragments now
coming to light, this method could speed the pro-
cess of text assembly considerably. A new set of
archives, recently discovered in the Hittite city of
S?apinuwa, are only now beginning to see publica-
tion. This site contains more than 3000 new Hit-
tite tablet fragments, with excavations ongoing(Su?el,
2002). The jumbled nature of the dig site means that
the process of assembling new texts from this site
will be one of the major tasks in for Hittite schol-
ars in the near future. This attempt at speeding the
task is only the beginning of what I hope will be a
considerable body of work to help build more com-
plete texts, and therefore more complete literatures
and histories, of not only Hittite, but other cuneiform
languages like Akkadian and Sumerian, some of the
world?s earliest written languages.
246
References
S. Argamon-Engelson, M. Koppel, and G. Avneri. 1998.
Style-based text categorization: What newspaper am i
reading. In Proc. of the AAAI Workshop on Text Cate-
gorization, pages 1?4.
G. Beckman. 1997. New Joins to Hittite Treaties.
Zeitschrift fu?r Assyriologie und Vorderasiatische
Archa?ologie, 87(1):96?100.
N. Dewdney, C. VanEss-Dykema, and R. MacMillan.
2001. The form is the substance: Classification of gen-
res in text. In Proceedings of the workshop on Human
Language Technology and Knowledge Management-
Volume 2001, pages 1?8. Association for Computa-
tional Linguistics.
H.A. Hoffner and H.C. Melchert. 2008. A grammar of
the Hittite language. Eisenbrauns.
Horst Klengel. 2002. Problems in hittite history, solved
and unsolved. In Simrit Dhesi K. Aslihan Yener, Harry
A. Hoffner Jr., editor, Recent developments in Hittite
archaeology and history: papers in memory of Hans
G. Gu?terbock, pages 101?109. Eisenbrauns.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
H. Craig Melchert. Anatolian databases. http:
//www.linguistics.ucla.edu/people/
Melchert/webpage/AnatolianDatabases.
htm.
F. Mosteller and D.L. Wallace. 1984. Applied bayesian
and classical inference: The case of the federalist pa-
pers.
C. Nicholson. 2009. Judging whether a document
changes in subject. In Southeastcon, 2009. SOUTH-
EASTCON?09. IEEE, pages 189?194. IEEE.
S.P. Smith. 2007. Hurrian Orthographic Interfer-
ence in Nuzi Akkadian: A Computational Comparative
Graphemic Analysis. Ph.D. thesis, Harvard University
Cambridge, Massachusetts.
A. Su?el. 2002. Ortako?y-sapinuwa. In Simrit Dhesi
K. Aslihan Yener, Harry A. Hoffner Jr., editor, Recent
developments in Hittite archaeology and history: pa-
pers in memory of Hans G. Gu?terbock, pages 157?165.
Eisenbrauns.
L.M. Tomokiyo and R. Jones. 2001. You?re not from
?round here, are you?: naive bayes detection of non-
native utterance text. In Second meeting of the North
American Chapter of the Association for Computa-
tional Linguistics on Language technologies 2001,
pages 1?8. Association for Computational Linguistics.
247
BALLGAME: A Corpus for Computational Semantics
Ezra Keshet, Terry Szymanski, and Stephen Tyndall
University of Michigan
E-mail: {ekeshet,tdszyman,styndall}@umich.edu
Abstract
In this paper, we describe the Baseball Announcers? Language Linked with General Annotation of
Meaningful Events (BALLGAME) project ? a text corpus for research in computional semantics.
We collected pitch-by-pitch event data for a sample of baseball games and used this data to build an
annotated corpus composed of transcripts of radio broadcasts of these games. Our annotation links
text from the broadcast to events in a formal representation of the semantics of the baseball game. We
describe our corpus model, the annotation tool used to create the corpus, and conclude by discussing
applications of this corpus in semantics research and natural language processing.
1 Introduction
The use of large annotated corpora and treebanks has led to many fruitful research programs in compu-
tational linguistics. At the time of this writing, Marcus et al (1993), which introduces the University of
Pennsylvania Treebank,1 has been cited by over 3000 subsequent papers.2 Such treebanks are invaluable
for the training and testing of large-scale syntactic parsers and numerous other applications in the field
of Computational Syntax.
Unfortunately for the field of Computational Semantics, there are few corresponding annotated cor-
pora or treebanks representing the formalized meaning of natural language sentences, mainly because
there is very little agreement on what such a representation of meaning would look like for arbitrary
text. To overcome this obstacle, several recent studies have turned to the arena of sports, pairing natural
language with game statistics in several domains, including RoboCup soccer (Liang et al, 2009; Chen
et al, 2010), soccer (Theune and Klabbers, 1998; Saggion et al, 2003), American football (Barzilay and
Lapata, 2005; Liang et al, 2009), and baseball (Fleischman, 2007).
We have adapted this approach in the creation of a semantics-oriented corpus, using the domain of
major-league baseball. The information state of a baseball game can be represented with a small number
of variables, such as who is on which base, who is batting, who is playing each position, and the current
score and inning. There is even a standard way of representing updates to this information state.3 This
makes baseball a logical stepping stone to a fuller representation of the world. We also chose baseball
for this corpus because of the volume of data available, in the form of both natural language descriptions
of events and language-independent game statistics. Most of professional baseball?s thousands of games
per year have at least two television broadcasts (home and away) and at least two radio broadcasts, often
in multiple languages. The scorecard statistics for each game are also kept and made available on the
internet, along with complete ordered lists of in-game events. These resources, coupled with a high-
coverage syntactic parser, allow one to link natural language utterances with representations of their
syntax and semantics.
1http://www.cis.upenn.edu/?treebank/
2http://scholar.google.com/scholar?cites=7124559111460341353
3See example scorecards at http://swingleydev.com/baseball/tutorial.php.
340
2 Corpus Design
The basic design of the BALLGAME corpus is a mapping between spans of text and events in a baseball
game. The raw text comes from the transcribed speech of announcers broadcasting the radio play-by-
play of a professional baseball game. This text is chunked into spans, and these spans are then labeled
according to the following scheme:
? Event is the label given to a span that describes an event in our representation of the game for the
first time. (Examples of events are simultaneous descriptions of pitches, plays, and stolen bases.)
? Recap is the label given to a span that correlates with prior events in the game. (Examples of recaps
are when the announcer states the current score or strike count, or summarizes the current batter?s
previous at-bats.)
? Banter is the label given to a span that does not relate to an event in the game. The majority of
spans are labeled as banter. (Examples of banters are ?color? commentary, any discussion of the
day?s news, other baseball games, advertisements, etc.)
The term ?span? has no linguistic significance, although spans often turn out to be sentences or clauses.
Each span from the text that is labeled as an event is linked to one or more events in the model of the
game as shown in Figure 1. Not every event is linked to a span of text, since some events go unmentioned
by the announcers.
Figure 1: Illustration of a portion of the corpus: event spans of the text (on the left) are associated with
events from a standardized description of the ballgame (on the right).
We model each game as a time-ordered sequence of baseball events, designed so that the state of the
game at any given point, while not explicitly represented, can be computed given the events from the
start of the game up to that point. We use a simple event model inspired by the comprehensive scoring
system developed by Retrosheet,4 but modified to match our needs and data resources. For example,
most baseball scoring systems are at-bat-based, but this system is too coarse-grained for our purposes.
Therefore, we use a system in which the fundamental event type is the pitch. Every baseball action from
the start of the pitcher?s motion until the end of the play (a hit or an out) is categorized as a PITCH event.
Several other event types exist to accommodate other plays (e.g. balks, pick-offs), non-play actions (e.g.
coaching visits to the mound, rain delays), and procedural activities (e.g. ejections, player substitutions).
In addition to a category, each event has multiple attribute values. The possible attributes depend on
the category. A PITCH event, for example, has attributes describing the type, speed, and location of the
pitch as well as whether it results in a ball, strike, play, etc. If the result is a play, then there are additional
4http://www.retrosheet.org
341
attributes describing the fielders involved in the defensive play. On the other hand, a PICKOFF event has
different attributes, describing which base the ball was thrown to, whether it resulted in an out, etc.
dealsCC curveballa low
advmodnsubj dobj det
<PLAYER pos='pitcher' team='home' firstname='CC' ...>
<PITCH type='curve' zone='12' result='ball' ...>
Events:
...
Figure 2: Example of a dependency parsed transcript line and corresponding events.
In the future, we plan to add syntactic parse information for each span such as that generated using
the Stanford Parser (De Marneffe et al, 2006). Using an explicit syntactic representation, like the one
illustrated in figure 2, it will be possible to label more detailed correlations between the text and the
meaning. Even without explicit annotation, statistical learning methods could be used to infer, e.g., that
the word ?curveball? in the sentence in figure 2 correlates with the semantic attribute type=?curve?,
or that the word ?CC? correlates with a specific PLAYER entity. While the annotations in the corpus
exist only at the sentence or phrase level, this type of further processing could push the annotation down
to the word level, facilitating the study of lexical semantics and semantic transformations of syntactic
structures.
3 Corpus Creation
Student transcribers use a custom-created transcription and annotation tool, illustrated in Figure 3, to add
data to the corpus. They listen to and transcribe the radio broadcast, while simultaneously chunking the
text into spans as described above. Each span is labeled banter, event, or recap, and, if the span describes
an event, the student selects the corresponding event(s) from the event column.
Annotators have access to a style guide to encourage consistency. This guide sets out two main prin-
ciples: first, the transcript of an inning, taken as a whole, should be read like a well-edited, consistently
formatted document; and second, all and only the events explicitly mentioned by the radio announcers
should be linked to events in the game model.
Although spans are displayed as separate lines in the transcription tool, in order to maintain this
first style principle, we ask the students to imagine that all spans of the transcript are pasted together in
sequence to form a normal transcript of the game. Thus, they are asked not to put ellipses or dashes at
the end of spans nor to capitalize the beginnings of spans that do not begin sentences. Also included in
this principle is a standardized formatting style for baseball statistics, such as strike counts, scores, and
batting averages, so that, for instance, ?the count is two and oh? is transcribed ?the count is 2-0?.
The second principle set out in the annotation style guide is meant to ensure that the events linked to
a particular utterance are as close as possible to the ?meaning? of that utterance. Integral to this process
is consistently distinguishing the categories of event, recap and banter. Since recap and banter spans do
not relate to events in the model, it is important to keep them separate from the event spans to get the
most accurate data. Even given the descriptions of these categories from section 2, ambiguous cases still
do arise on occasion. For instance, one common difficulty is distinguishing event from recap when an
announcer discusses a play immediately after it happens. In such cases, in keeping with our annotation
principle, we use the rule of thumb that only new information is annotated as event; old information is
recap. We also adopt the rule that only game events that are explicitly stated by the announcer should
be linked to spans; for example, if the announcer merely states the name of the batter (e.g. ?Cust takes a
first-pitch strike?) in the process of describing the first pitch of his at-bat, then this should not reference
the ATBAT event that indicates the arrival of a new batter at the plate. On the other hand, an explicit
mention (e.g. ?Here?s Cust.?) should.
In the final steps of the annotation process, each transcript is reviewed and corrected by a second
annotator to reduce errors and further promote consistency across annotators.
342
Figure 3: Screen shot of online annotation tool.
4 Potential Applications
Since this corpus links natural language utterances with complete semantic representations which fully
describe the state of the baseball game, it has a number of applications for research in computational
semantics. While the domain is limited, and the ?meaning? of a baseball game does not approach the
complexity of the possible ?meanings? in the real world, nevertheless this corpus should be a useful
resource both for developing NLP tools and for studying theories of language and meaning.
One application domain for this type of data is natural language generation and understanding, and
much prior work connecting sports commentaries to statistics or events falls into this domain. One
related generation task is to generate textual summaries of complete games: Theune and Klabbers (1998)
generated spoken Dutch summaries of soccer matches, and Barzilay and Lapata (2005) investigate the
relationship between textual NFL recaps and the box scores of the games. More similar to our project
is the RoboCup announcer system of Chen et al (2010), which produces play-by-play commentary (in
English and Korean) of simulated RoboCup soccer matches. Our corpus could certainly be used to train
systems that predict the event structure given the text of the commentary, or vice-versa.
In the domain of information extraction, our corpus could be used to train systems to infer repre-
sentations of meaning from texts. In many domains, the same word or phrase can appear in a variety
of different contexts with different ramifications. For example, the phrase ?home run? in a baseball
commentary may mean that a home run has just occurred, or it may refer to a home run in a previous
game, or a player?s home-run totals for the season, etc.. Fleischman (2007), using a collection of video
343
broadcasts of baseball games, combines natural language processing with artificial vision technology to
resolve when events like home runs actually occur, in order to facilitate retrieval of relevant video clips.
Using our corpus, one could design a system to perform the same task based purely on the textual data,
perhaps to extend this same task to radio broadcasts as well as television broadcasts. Given the corpus
labels of event, recap, and banter, a classifier could be built to identify only the event regions, and an
extraction system could identify the relevant semantic features (e.g. player names, types of events).
While generation and understanding are tasks most applicable to this corpus, we hope researchers
will find additional innovative uses of the corpus. For example, given that we plan to incorporate a num-
ber of baseball games with commentary both in English and Spanish, there is a potential connection to
machine translation, particularly approaches that utilize comparable (rather than parallel) corpora. In our
corpus, the comparable sections (i.e. the event-labeled regions) are explicitly aligned with one another,
which is not usually the case in comparable corpora. Also, the corpus could prove useful for research on
formal semantics, despite the fact the meaning representation is not particularly rich compared to modern
semantic theory, and the jargon and speech styles are very specific to the domain of baseball sportscasts.
5 Conclusion
We have presented an overview of the BALLGAME annotated corpus for research in computational
semantics, as well as a description of our procedure for annotation and the specialized annotation tool
we developed for this purpose. To date, the corpus contains sixteen three- to four-hour-long major
league baseball radio broadcasts, transcribed and annotated as described above. This represents 237,100
transcribed words in 13,382 spans (6,511 banter; 3,994 event; 2,877 recap). Work is ongoing, and the
goal is to complete fifty games by the end of the year. We believe this corpus, by pairing natural language
text with formalized representations of meaning, will prove useful for many types of NLP research.
References
Barzilay, R. and M. Lapata (2005). Collective content selection for concept-to-text generation. In Pro-
ceedings of HLT/EMNLP, pp. 331?338.
Chen, D., J. Kim, and R. Mooney (2010). Training a multilingual sportscaster: Using perceptual context
to learn language. Journal of Artificial Intelligence Research 37(1), 397?436.
De Marneffe, M., B. MacCartney, and C. Manning (2006). Generating typed dependency parses from
phrase structure parses. In LREC 2006.
Fleischman, M. (2007). Situated models of meaning for sports video retrieval. In NAACL-HLT 2007, pp.
37?40.
Liang, P., M. Jordan, and D. Klein (2009). Learning semantic correspondences with less supervision. In
Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pp. 91?99.
Marcus, M., B. Santorini, and M. Marcinkiewicz (1993). Building a large annotated corpus of English:
The Penn Treebank. Computational linguistics 19(2), 313?330.
Saggion, H., J. Kuper, H. Cunningham, T. Declerck, P. Wittenburg, M. Puts, E. Hoenkamp, F. de Jong,
and Y. Wilks (2003). Event-coreference across multiple, multi-lingual sources in the Mumis project.
In Proceedings of the tenth conference on European chapter of the Association for Computational
Linguistics: Volume 2, pp. 239?242.
Theune, M. and E. Klabbers (1998). GoalGetter: Generation of spoken soccer reports. In Proceedings
of the Ninth International Workshop on Natural Language Generation, pp. 292?295.
344
