Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 968?976,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
  Tree Kernel-based Negation and Speculation Scope Detection with 
Structured Syntactic Parse Features 
 
 
Bowei Zou       Guodong Zhou       Qiaoming Zhu* 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
zoubowei@gmail.com, {gdzhou,qmzhu}@suda.edu.cn 
 
  
 
Abstract 
Scope detection is a key task in information ex-
traction. This paper proposes a new approach for 
tree kernel-based scope detection by using the 
structured syntactic parse information. In addi-
tion, we have explored the way of selecting 
compatible features for different part-of-speech 
cues. Experiments on the BioScope corpus show 
that both constituent and dependency structured 
syntactic parse features have the advantage in 
capturing the potential relationships between 
cues and their scopes. Compared with the state 
of the art scope detection systems, our system 
achieves substantial improvement.* 
1 Introduction 
The task of scope detection is to detect the linguis-
tic scope dominated by a specific cue. Current re-
searches in this field focus on two semantic as-
pects: negation and speculation. The negative 
scope detection is to detect the linguistic scope 
which is repudiated by a negative word (viz., nega-
tive cue, e.g., ?not?). In other side, the speculative 
scope detection is to detect the uncertain part in a 
sentence corresponding to the speculative word 
(viz., speculative cue, e.g., ?seems?). See the sen-
tence 1) below, the negative cue ?not? dominates 
the scope of ?not expensive?. Similarly, the specu-
lative cue ?possible? in sentence 2) dominates the 
uncertain scope ?the possible future scenarios?. 
1) The chair is [not expensive] but comfortable.  
2) Considering all that we have seen, what are now 
[the possible future scenarios]? 
                                                 
*	Corresponding	author	
The negative and speculative scope detection 
task consists of two basic stages. The first one is to 
identify the sentences involving negative or specu-
lative meaning. The second stage is to detect the 
linguistic scope of the cue in sentences (Velldal et 
al, 2012). In this paper, we focus on the second 
stage. That is, by given golden cues, we detect 
their linguistic scopes. 
We propose a tree kernel-based negation and 
speculation scope detection with structured syntac-
tic parse features. In detail, we regard the scope 
detection task as a binary classification issue, 
which is to classify the tokens in a sentence as be-
ing inside or outside the scope. In the basic 
framework, we focus on the analysis and applica-
tion of structured syntactic parse features as fol-
lows: 
Both constituent and dependency syntactic fea-
tures have been proved to be effective in scope 
detection (?zg?r et al 2009; ?vrelid et al 2010). 
However, these flat features are hardly to reflect 
the information implicit in syntactic parse tree 
structures. Our intuition is that the segments of the 
syntactic parse tree around a negative or specula-
tive cue is effective for scope detection. The relat-
ed structures normally underlay the indirect clues 
to identify the relations between cues and their 
scopes, e.g., in sentence 1), ?but something?, as a 
frequently co-occurred syntactic structure with 
?not something?, is an effective clue to determine 
the linguistic scope of ?not?. 
The tree kernel classifier (Moschitti, 2006) 
based on support vector machines uses a kernel 
function between two trees, affording a compari-
son between their substructures. Therefore, a tree 
kernel-based scope detection approach with struc-
tured syntactic parse tree is employed. The tree 
968
kernel has been already proved to be effective in 
semantic role labeling (Che et al 2006) and rela-
tion extraction (Zhou et al 2007). 
In addition, the empirical observation shows 
that features have imbalanced efficiency for scope 
classification, which is normally affected by the 
part-of-speech (abbr., POS) of cues. Hence, we 
build the discriminative classifiers for each kind of 
POS of cues, then explore and select the most 
compatible features for them. 
We construct a scope detection system by using 
the structured syntactic parse features based tree 
kernel classification. Compared with the state of 
the art scope detection systems, our system 
achieves the performance of accuracy 76.90% on 
negation and 84.21% on speculation (on Abstracts 
sub-corpus). Additionally, we test our system on 
different sub-corpus (Clinical Reports and Full 
Papers). The results show that our approach has 
better cross-domain performance. 
The rest of this paper is organized as follows: 
Section 2 reviews related work. Section 3 intro-
duces the corpus and corresponding usage in our 
experiments. Section 4 describes our approach and 
the experiments are presented in Section 5. Finally, 
there is a conclusion in Section 6. 
2 Related Work 
Most of the previous studies on negation and spec-
ulation scope detection task can be divided into 
two main aspects: the heuristic rule based methods 
and the machine learning based methods. We re-
spectively introduce the aspects in below. 
2.1 Heuristic Rule based Methods 
The initial studies for scope detection are to com-
pile effective heuristic rules (Chapman et al 2001; 
Goldin et al 2003). Recently, the heuristic rule 
based methods have further involved the syntactic 
features. 
Huang et al(2007) implemented a hybrid ap-
proach to automated negation scope detection. 
They combined the regular expression matching 
with grammatical parsing: negations are classified 
on the basis of syntactic categories and located in 
parse trees. Their hybrid approach is able to identi-
fy negated concepts in radiology reports even 
when they are located at some distance from the 
negative term. 
?zg?r et al(2009) hypothesized that the scope 
of a speculation cue can be characterized by its 
part-of-speech and the syntactic structure of the 
sentence and developed rules to map the scope of a 
cue to the nodes in the syntactic parse tree. By 
given golden speculation cues, their rule-based 
method achieves the accuracies of 79.89% and 
61.13% on the Abstracts and the Full-Papers sub-
corpus, respectively. 
?vrelid et al(2010) constructed a small set of 
heuristic rules which define the scope for each cue. 
In developing these rules, they made use of the 
information provided by the guidelines for scope 
annotation in the BioScope corpus, combined with 
manual inspection of the training data in order to 
further generalize over the phenomena discussed 
by Vincze et al(2008) and work out interactions of 
constructions for various types of cues. 
Apostolova et al(2011) presented a linguistical-
ly motivated rule-based system for the detection of 
negation and speculation scopes that performs on 
par with state-of-the-art machine learning systems. 
The rules are automatically extracted from the Bi-
oScope corpus and encode lexico-syntactic pat-
terns in a user-friendly format. While their system 
was developed and tested using a biomedical cor-
pus, the rule extraction mechanism is not domain-
specific. 
The heuristic rule based methods have bad ro-
bustness in detecting scopes crossing different 
meaning aspects (e.g., negative vs. speculative) 
and crossing different linguistic resources (e.g., 
Technical Papers vs. Clinical Reports). 
2.2 Machine Learning based Methods 
The machine learning based methods have been 
ignored until the release of the BioScope corpus 
(Szarvas et al 2008), where the large-scale data of 
manually annotated cues and corresponding scopes 
can support machine learning well. 
Morante et al(2008) formulated scope detection 
as a chunk classification problem. It is worth not-
ing that they also proposed an effective proper 
post-processing approach to ensure the consecu-
tiveness of scope. Then, for further improving the 
scope detection, Morante et al(2009a) applied a 
meta-learner that uses the predictions of the three 
classifiers (TiMBL/SVM/CRF) to predict the 
scope. 
For the competitive task in CoNLL?2010 (Far-
kas et al 2010), Morante et al(2010) used a 
969
memory-based classifier based on the k-nearest 
neighbor rule to determine if a token is the first 
token in a scope sequence, the last, or neither. 
Therefore, in order to guarantee that all scopes are 
continuous sequences of tokens they apply a first 
post-processing step that builds the sequence of 
scope. 
The existing machine learning based approaches 
substantially improve the robustness of scope de-
tection, and have nearly 80% accuracy. However, 
the approaches ignore the availability of the struc-
tured syntactic parse information. This information 
involves more clues which can well reflect the re-
lations between cues and scopes. S?nchez et al
(2010) employed a tree kernel based classifier with 
CCG structures to identify speculative sentences 
on Wikipedia dataset. However, in S?nchez?s ap-
proach, not all sentences are covered by the classi-
fier. 
3 Corpus 
We have employed the BioScope corpus (Szarvas 
et al 2008; Vincze et al 2008)1, an open resource 
from the biomedical domain, as the benchmark 
corpus. The corpus contains annotations at the to-
ken level for negative and speculative cues and at 
the sentence level for their linguistic scope (as 
shown in Figure 1). 
 (Note: <Sentence> denotes one sentence and the tag ?id? denotes its 
serial number; <xcope> denotes the scope of a cue; <cue> denotes the 
cue, the tag ?type? denotes the specific kind of cues and the tag ?ref? 
is the cue?s serial number.) 
Figure 1. An annotated sentence in BioScope. 
The BioScope corpus consists of three sub-
corpora: biological Full Papers from FlyBase and 
BMC Bioinformatics, biological paper Abstracts 
from the GENIA corpus (Collier et al 1999), and 
Clinical Reports. Among them, the Full Papers 
sub-corpus and the Abstracts sub-corpus come 
from the same genre. In comparison, the Clinical 
Reports sub-corpus consists of clinical radiology 
reports with short sentences. 
                                                 
1 http://www.inf.u-szeged.hu/rgai/bioscope 
In our experiments, if there is more than one cue 
in a sentence, we treat them as different cue and 
scope (two independent instances). The statistical 
data for our corpus is presented in Table 1 in be-
low. 
The average length of sentences in the negation 
portion is almost as long as that in speculation, 
while the average length of scope in negation is 
shorter than that in speculation. In addition, the 
length of sentence and scope in both Abstracts and 
Full Papers sub-corpora is comparative. But in 
Clinical Reports sub-corpus, it is shorter than that 
in Abstracts and Full Papers. Thus, looking for the 
effective features in short sentences is especially 
important for improving the robustness for scope 
detection. 
(Note: ?Av. Len? stands for average length.) 
Table 1. Statistics for our corpus in BioScope. 
4 Methodology 
We regard the scope detection task as a binary 
classification problem, which is to classify each 
token in sentence as being the element of the scope 
or not. Under this framework, we describe the flat 
syntactic features and employ them in our bench-
mark system. Then, we propose a tree kernel-
based scope detection approach using the struc-
tured syntactic parse features. Finally, we con-
struct the discriminative classifier for each kind of 
POS of cues, and select the most compatible fea-
tures for each classifier. 
4.1 Flat Syntactic Features 
In our benchmark classification system, the fea-
tures relevant to the cues or tokens are selected. 
Then, we have explored the constituent and de-
pendency syntactic features for scope detection. 
These features are all flat ones which reflect the 
characteristic of tokens, cues, scopes, and the rela-
tion between them. 
 Abstract Paper Clinical
Nega-
tion 
Sentences 1594 336 441 
Words 46849 10246 3613 
Scopes 1667 359 442 
Av. Len Sentence 29.39 30.49 8.19 
Av. Len Scope 9.62 9.36 5.28 
Specu-
lation
Sentences 2084 519 854 
Words 62449 16248 10241
Scopes 2693 682 1137 
Av. Len Sentence 29.97 31.31 11.99
Av. Len Scope 17.24 15.58 6.99 
<sentence id=?S26.8?> These findings <xcope id=?X26.8.2?> 
<cue type=?speculation? ref=?X26.8.2?> indicate that </cue> 
<xcope id=?X26.8.1?> corticosteroid resistance in bronchial 
asthma <cue type=?negation? ref=?X26.8.1?> can not </cue> 
be explained by abnormalities in corticosteroid receptor char-
acteristics </xcope></xcope> . </sentence> 
970
Basic Features: Table 2 shows the basic fea-
tures which directly relate to the characteristic of 
cues or tokens in our basic classification. 
Feature Remark 
B1 Cue. 
B2 Candidate token. 
B3 Part-of-speech of candidate token. 
B4 Left token of candidate token. 
B5 Right token of candidate token. 
B6 Positional relation between cue and token.
   Table 2. Basic features. 
Constituent Syntactic Features: For improv-
ing the basic classification, we employ 10 constit-
uent features belonging to two aspects. On the one 
hand, we regard the linguistic information of the 
neighbor locating around the candidate tokens as 
the coherent features (CS1~CS6 in Table 3). These 
features are used for detecting the close coopera-
tion of a candidate token co-occurring with its 
neighbors in a scope. On the other hand, we regard 
the linguistic characteristics of the candidate to-
kens themselves in a syntactic tree as the inherent 
features (CS7~CS10 in Table 3). These features 
are used for determining whether the token has the 
direct relationship with the cue or not. 
Features Remarks 
CS1 POS of left token. 
CS2 POS of right token. 
CS3 Syntactic category of left token. 
CS4 Syntactic category of right token. 
CS5 Syntactic path from left token to the cue. 
CS6 Syntactic path from right token to the cue. 
CS7 Syntactic category of the token. 
CS8 Syntactic path from the token to the cue. 
CS9 Whether the syntactic category of the token is 
the ancestor of the cue. 
CS10 Whether the syntactic category of the cue is the 
ancestor of the token. 
Table 3. Constituent syntactic features. 
Features Remarks 
DS1 Dependency direction (?head?or ?dependent?). 
DS2 Dependency syntactic path from the token to cue. 
DS3 The kind of dependency relation between the token 
and cue. 
DS4 Whether the token is the ancestor of the cue. 
DS5 Whether the cue is the ancestor of the token. 
Table 4. Dependency syntactic features. 
Dependency Syntactic Features: For the effec-
tiveness to obtain the syntactic information far 
apart from cues, we use 5 dependency syntactic 
features which emphasize the dominant relation-
ship between cues and tokens by dependency arcs 
as shown in Table 4. 
The features in Table 2, 3, and 4 have imbal-
anced classification for the scope classification. 
Therefore, we adopt the greedy feature selection 
algorithm as described in Jiang et al(2006) to pick 
up positive features incrementally according to 
their contributions. The algorithm repeatedly se-
lects one feature each time, which contributes most, 
and stops when adding any of the remaining fea-
tures fails to improve the performance. 
4.2 Structured Syntactic Features 
Syntactic trees involve not only the direct bridge 
(e.g., syntactic path) between cue and its scope but 
also the related structures to support the bridge 
(e.g., sub-tree). The related structures normally 
involve implicit clues which underlay the relation 
between cue and its scope. Therefore, we use the 
constituent and dependency syntactic structures as 
the supplementary features to further improve the 
benchmark system. 
Furthermore, we employ the tree kernel-based 
classifier to capture the structured information 
both in constituent and dependency parsing trees. 
The results of the constituent syntactic parser are 
typical trees which always consist of the syntactic 
category nodes and the terminal nodes. Thus, the 
constituent syntactic tree structures could be used 
in tree kernel-based classifier directly, but not for 
the dependency syntactic tree structures. As Figure 
2 shows, in sentence ?The chair is not expensive 
but comfortable.? the tree kernels cannot represent 
the relations on the arcs (e.g., ?CONJ? between 
?expensive? and ?comfortable?). It is hard to use 
the relations between tokens and cues in tree ker-
nels. 
 Figure 2: The dependency tree of sentence ?The 
chair is not expensive but comfortable.? 
971
 Figure 3. Two transformational rules. 
To solve the problem, we transform the depend-
ency tree into other two forms capable of being 
used directly as the compatible features in tree-
kernel based classification. The transformational 
rules are described as below: 
(1) Extracting the dependency relations to gen-
erate a tree of pure relations (named dependency 
relational frame), where the tokens on the nodes of 
original dependency tree are ignored and only the 
relation labels are used. E.g., the tokens ?chair?, 
?is?, etc in Figure 2 are all deleted and replaced by 
the corresponding relation labels. E.g., ?NSUBJ?, 
?COP?, etc are used as nodes in the dependency 
relational frame, see (1a) & (1b) in Figure 3. 
(2) Inserting the tokens which have been deleted 
in step (1) into the dependency relational frame 
and making them follow and link with their origi-
nal dependency relations. E.g., the tokens ?chair?, 
?is?, etc are added below the nodes ?NSUBJ?, 
?COP?, etc, see (2a) & (2b) in Figure 3. 
 
Figure 4. Two transformations for tree-kernel. 
Within the constituent and dependency syntactic 
trees, we have employed both the Completed Sub-
Tree and the Critical Path as the syntactic structure 
features for our classification. The former is a min-
imum sub-tree that involves the cues and the to-
kens, while the latter is the path from the cues to 
the tokens in the completed tree containing the 
primary structural information. Figure 4 shows 
them. 
4.3 Part-of-Speech Based Classification Op-
timization 
Motivating in part by the rule-based approach of 
?zg?r et al(2009), we infer that features have im-
balanced efficiency for scope classification, nor-
mally affected by the part-of-speech (POS) of cues.  
POS of Cues Number POS of Cues Number
CC 157 VB 31 
IN 115 VBD 131 
JJ 238 VBG 225 
MD 733 VBN 112 
NN 43 VBP 561 
RB 137 VBZ 207 
Table 5. Distribution of different POSs of specula-
tive cues in Abstracts sub-corpus. 
Table 5 shows the distribution for different 
POSs of cues in the Abstracts sub-corpus of Bio-
Scope for speculation detection task. The cues of 
different POS usually undertake different syntactic 
roles. Thus, there are different characteristics in 
triggering linguistic scopes. See the two examples 
below: 
3) TCF-1 contained a single DNA box in the [putative 
mammalian sex-determining gene SRY]. 
4) The circadian rhythm of plasma cortisol [either 
disappeared or was inverted]. 
The speculative cue ?putative? in sentence 3) is 
an adjective. The corresponding scope is its modi-
ficatory structure (?putative mammalian sex-
determining gene SRY?). In sentence 4), ?ei-
ther?or?? is a conjunction speculation cue. Its 
scope is the two connected components (?either 
disappeared or was inverted?). Thus, the effective 
features for the adjectival cue are normally the de-
pendency features, e.g., the features of DS1 and 
DS5 in Table 4, while the features for the conjunc-
tion cue are normally the constituent information, 
e.g., the features of CS9 in Table 3.  
In Table 5, considering the different function of 
verb voice, we cannot combine the ?VB(*)? POS. 
For instance, the POS of ?suggest? in sentence 5) 
is ?VBP? (the verb present tense). The correspond-
ing scope does not involve the sentence subject. 
972
The POS of ?suggested? in sentence 6) is ?VBN? 
(the past participle). The scope involves the sub-
ject ?An age-related decrease?. 
5) These results [suggest that the genes might be in-
volved in terminal granulocyte differentiation]. 
6) [An age-related decrease was suggested between 
subjects younger than 20 years]. 
As a result, we have built a discriminative clas-
sifier for each kind of POS of cues, and then ex-
plored and selected the most compatible features 
for each classifier. 
5 Experiments and Results 
5.1 Experimental Setting 
Considering the effectiveness of different features, 
we have split the Abstracts sub-corpus into 5 equal 
parts, within which 2 parts are used for feature 
selection (Feature Selection Data) and the rest for 
the scope detection experiments (Scope Detection 
Data). The Feature Selection Data are divided into 
5 equal parts, within which 4 parts for training and 
the rest for developing. In our scope detection ex-
periments, we divide the Scope Detection Data 
into 10 folds randomly, so as to perform 10-fold 
cross validation. As the experiment data is easily 
confusable, Figure 5 illustrates the allocation. 
Checking the validity of our method, we use the 
Abstracts sub-corpus in Section 5.2, 5.3 and 5.4, 
while in Section 5.5 we use all of the three sub-
corpora (Abstracts, Full Papers, and Clinical Re-
ports) to test the robustness of our system when 
applied to different text types within the same do-
main. 
 Figure 5. The allocation for experiment data. 
The evaluation is made using the precision, re-
call and their harmonic mean, F1-score. Addition-
ally, we report the accuracy in PCS (Percentage of 
Correct Scopes) applied in CoNLL?2010, within 
which a scope is fully correct if all tokens in a sen-
tence have been assigned to the correct scope class 
for a given cue. The evaluation in terms of preci-
sion and recall measures takes a token as a unit, 
whereas the evaluation in terms of PCS takes a 
scope as a unit. The key toolkits for scope classifi-
cation include: 
Constituent and Dependency Parser: All the 
sentences in BioScope corpus are tokenized and 
parsed using the Berkeley Parser (Petrov et al 
2007) 2  which have been trained on the GENIA 
TreeBank 1.0 (Tateisi et al 2005)3, a bracketed 
corpus in PTB style. 10-fold cross-validation on 
GTB1.0 shows that the parser achieves 87.12% in 
F1-score. On the other hand, we obtain the de-
pendency relations by the Stanford Dependencies 
Parser4. 
Support Vector Machine Classifier: SVMLight5 
is selected as our classifier, which provides a way 
to combine the tree kernels with the default and 
custom SVMLight kernels. We use the default pa-
rameter computed by SVMLight. 
Besides, according to the guideline of the Bio-
Scope corpus, scope must be a continuous chunk. 
The scope classifier may result in discontinuous 
blocks, as each token may be classified inside or 
outside the scope. Therefore, we perform the rule 
based post-processing algorithm proposed by Mo-
rante et al(2008) to obtain continuous scopes. 
5.2 Results on Flat Syntactic Features 
Relying on the results of the greedy feature selec-
tion algorithm (described in Section 4.1), we ob-
tain 9 effective features {B1, B3, B6, CS3, CS4, 
CS9, DS1, DS3, DS5} (see Table 2, 3 and 4) for 
negation scope detection and 13 effective features 
{B3, B4, B5, B6, CS1, CS5, CS6, CS8, CS9, CS10, 
DS1, DS4, DS5} for speculation. Table 6 lists the 
performances on the Scope Detection Data by per-
forming 10-fold cross validation. It shows that flat 
constituent and dependency syntactic features sig-
nificantly improve the basic scope detection by 
13.48% PCS for negation and 30.46% for specula-
tion (?2; p < 0.01). It demonstrates that the selected 
syntactic features are effective for scope detection. 
 
 
                                                 
2 http://code.google.com/p/berkeleyparser 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://svmlight.joachims.org 
973
Negation 
Features P R F PCS
Basic 89.89 68.72 77.86 39.50
Con.  85.72 67.80 75.66 41.81
Dep.  90.31 69.01 78.19 40.08
Bas.&Con.  88.86 79.07 83.61 51.64
Bas.&Dep. 90.44 73.62 81.17 49.36
All 91.21 76.57 83.25 52.98
Speculation 
Features P R F PCS
Basic 89.67 86.86 88.24 40.09
Con.  96.43 87.46 91.72 66.57
Dep.  90.84 87.04 88.89 44.45
Bas.&Con.  95.66 92.08 93.83 69.59
Bas.&Dep. 92.39 88.27 90.28 67.49
All 95.71 92.09 93.86 70.55
(Note: ?Bas.? denotes basic features; ?Con.? denotes Constituent 
features; ?Dep.? denotes Dependency features; ?All? contains Basic, 
Constituent, and Dependency features being selected.) 
Table 6. Performance of flat syntactic features. 
The results also show that the speculative scope 
detection achieves higher performance (16.98% 
higher in PCS) (?2; p < 0.01) than the negation 
scope detection. The main reason is that although 
the average sentence length of negation and specu-
lation are comparable (29.97 vs. 29.39 words, in 
Table 1), the average length of speculation scopes 
is much longer than the negation (17.24 vs. 9.62 
words, in Table 1) in Abstracts sub-corpus. With 
the shorter scopes in training data, the classifier 
inevitably have more negative samples. Thus, by 
using a token as the basic unit in our classification, 
the imbalanced samples will seriously mislead the 
classifier and result in bias on the negative samples. 
In addition, both constituent and dependency 
flat features can improve the scope classification, 
for the reason that the constituent features usually 
provide the nearer syntactic information of the 
cues, and that the further syntactic information 
between cues and scopes have been obtained by 
the dependency features. 
5.3 Results on Structured Syntactic Parse 
Features 
Table 7 and Table 8 give the scope detection per-
formance using the different structured syntactic 
parse features on negation and speculation respec-
tively. Compared to the optimal system (using all 
of the selected flat features in Table 6) in Section 
5.2, the structured syntactic parse features at best 
improve the scope classification nearly 17.29% on 
negation (PCS=70.27%) and 12.32% on specula-
tion (PCS=82.87%) (?2; p < 0.01). It indicates that 
the structured syntactic parse features can provide 
more implicit linguistic information, as supple-
mentary clues, to support scope classification. 
The improvements also show that both the com-
pleted syntactic sub-trees and critical paths in con-
stituent and dependency parsing trees are effective. 
The reason is that the completed syntactic sub-
trees contain the surrounding information related 
to cues and tokens, while there are more direct 
syntactic information in the critical paths between 
cue and its scope. 
Features P R F PCS 
Con. CT 91.12 83.25 86.89 54.57 
Con. CT&CP 93.31 89.32 91.20 66.58 
Dep. T1 CT 87.29 84.37 85.81 53.07 
Dep. T1 CT&CP 90.03 86.77 88.37 59.53 
Dep. T2 CT 88.17 84.58 86.34 53.76 
Dep. T2 CT&CP 91.09 87.31 89.16 60.11 
All 93.84 91.94 92.88 70.27 
(Note: ?Con.? denotes Constituent features; ?Dep.? denotes Depend-
ency features; ?T1? use the transformational rule (1) in Section 4.2 to 
get the dependency tree; ?T2? use the transformational rule (2) in 
Section 4.2 to get the dependency tree; CT-?Completed syntactic sub-
Tree?; CP-?Critical Path?; ?All? contains Con CT&CP, Dep T1 
CT&CP and Dep T2 CT&CP) 
Table 7. Performance of structured syntactic parse 
features on negation. 
Features P R F PCS 
Con. CT 95.89 93.37 94.61 75.17 
Con. CT&CP 96.05 94.36 95.20 76.73 
Dep. T1 CT 93.24 90.77 91.99 72.31 
Dep. T1 CT&CP 94.28 92.30 93.28 73.75 
Dep. T2 CT 93.76 89.68 91.67 73.06 
Dep. T2 CT&CP 95.29 94.55 94.92 75.69 
All 96.93 96.86 96.89 82.87 
Table 8. Performance of structured syntactic parse 
features on speculation. 
5.4 Results on Part-of-Speech Based Classifi-
cation 
To confirm the assumption in Section 4.3, we have 
built a discriminative classifier for each kind of 
POS of cues. Considering that the features involv-
ing the global structured syntactic parse infor-
mation in Section 4.2 are almost effective to all 
instances, we only use the flat syntactic features in 
Section 4.1. 
Negation
System P R F PCS
All Features 91.21 76.57 83.25 52.98
POS Classifier 91.79 78.29 84.50 56.77
Specula-
tion 
System P R F PCS
All Features  95.71 92.09 93.86 70.55
POS Classifier 95.79 93.13 94.44 71.68
(Note: ?All Features? System is the optimal system in Section 5.2) 
Table 9. Performances of POS based classification. 
Table 9 shows the performance of POS based 
classification. Compared with the system which 
only uses one classifier for all cues in Section 5.2, 
974
the POS based classification improves 1.13% on 
PCS (?2; p < 0.01), as different POS kinds of cues 
involve respectively effective features with more 
related clues between cue and its scope. 
Table 10 lists the performance of each POS kind 
of cues in speculation scope classification. There 
are still some low performances in some kinds of 
POS of cues. We consider it caused by two reasons. 
Firstly, some kinds of POS of cues  (e.g. NN etc.) 
have fewer samples (just 43 samples shown in Ta-
ble 5). For this reason, the training for classifier is 
limited. Then, for these low performance kinds of 
POS of cues, we may have not found the effective 
features for them. Although there are some kinds 
of cues with low performance, the whole perfor-
mance of part-of-speech based classification is 
improved. 
Cue?s 
POS 
B1~B6 
  1   2   3   4   5   6 
CS1~CS10 
1   2   3   4    5   6   7   8   9 10 
DS1~DS5 
1   2   3   4   5 PCS 
CC ?    ? ?    ? ?   ? ? ? ? ? 38.45
IN ?    ? ?    ? ? ?  ? ?  ? ? ? 87.99
JJ ?     ?    ?  ?  ?    ? 31.83
MD    ? ? ?  ?  ?  ? ? ? ? ?  ? ? 79.84
NN ?     ? ? ?   ?   ?  ?  ? 65.83
RB      ? ? ?   ? ?    ?    ? 37.03
VB      ? ?       ? ?   44.29
VBD    ? ?    ? ? ? ? ? ? ? ?  ? 63.57
VBG    ? ?    ? ? ?  ? ? ? ?  ? ? 82.89
VBN    ? ?    ?  ?  ? ? ? ?  ? 66.38
VBP    ? ? ? ?   ? ?  ? ? ? ? ?  ? 81.91
VBZ    ? ? ? ?   ? ?  ? ? ? ? ?  ? 77.16
Table 10. Performance of each POS kind of cues 
in speculation scope classification. 
5.5 Results of Comparison Experiments 
To get the final performance of our approach, we 
train the classifiers respectively by different effec-
tive features in Section 4.1 for POS kinds of cues, 
and use the structured syntactic parse features in 
Section 4.2 on Abstracts sub-corpus by performing 
10-fold cross validation. 
Negation 
System Abstract Paper Clinical
Morante (2008) 57.33 N/A N/A 
Morante (2009a) 73.36 50.26 87.27 
Ours 76.90 61.19 85.31 
Specula-
tion 
System Abstract Paper Clinical
Morante (2009b) 77.13 47.94 60.59 
?zg?r (2009) 79.89 61.13 N/A 
Ours 84.21 67.24 72.92 
Table 11. Performance comparison of our system 
with the state-of-the-art ones in PCS. 
The results in Table 11 show that our system 
outperforms the state of the art ones both on nega-
tion and speculation scope detection. Results also 
show that the system is portable to different types 
of documents, although performance varies de-
pending on the characteristics of the corpus. 
In addition, on both negation and speculation, 
the results on Clinical Reports sub-corpus are bet-
ter than those on Full Papers sub-corpus. It is 
mainly due to that the clinical reports are easier to 
process than full papers and abstracts. The average 
length of sentence for negative clinical reports is 
8.19 tokens, whereas for abstracts it is 29.39 and 
for full papers 30.49. Shorter sentences imply 
shorter scopes. The more unambiguous sentence 
structure of short sentence can make the structured 
constituent and dependency syntactic features eas-
ier to be processed. 
6 Conclusion 
This paper proposes a new approach for tree ker-
nel-based scope detection by using the structured 
syntactic parse information. In particular, we have 
explored the way of selecting compatible features 
for different part-of-speech cues. Experiments 
show substantial improvements of our scope clas-
sification and better robustness. 
However, the results on the Full Papers and the 
Clinical Reports sub-corpora are lower than those 
on the Abstracts sub-corpus for both negation and 
speculation. That is because the structured syntac-
tic parse features contain some complicated and 
lengthy components, and the flat features cross 
corpus are sparse. Our future work will focus on 
the pruning algorithm for the syntactic structures 
and analyzing errors in depth in order to get more 
effective features for the scope detection on differ-
ent corpora. 
Acknowledgments 
This research is supported by the National Natural 
Science Foundation of China, No.61272260, 
No.61373097, No.61003152, the Natural Science 
Foundation of Jiangsu Province, No.BK2011282, 
the Major Project of College Natural Science 
Foundation of Jiangsu Province, No.11KJA520003 
and the Graduates Project of Science and Innova-
tion, No.CXZZ12_0818. Besides, thanks to Yu 
Hong and the three anonymous reviewers for their 
valuable comments on an earlier draft. 
 
975
References  
Emilia Apostolova, Noriko Tomuro and Dina Demner-
Fushman. 2011. Automatic Extraction of Lexico-
Syntactic Patterns for Detection of Negation and 
Speculation Scopes. In Proceedings of ACL-HLT 
short papers, pages 283-287. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34 (5): 301-310. 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A Hybrid Convolution Tree Kernel for Seman-
tic Role Labeling. In Proceedings of ACL, pages 73-
80. 
Nigel Collier, Hyun S. Park, Norihiro Ogata, et al 1999. 
The GENIA Project: Corpus-Based Knowledge Ac-
quisition and Information Extraction from Genome 
Research Papers. In Proceedings of EACL. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-2010 
Shared Task: Learning to Detect Hedges and their 
Scope in Natural Language Text. In Proceedings of 
CoNLL: Shared Task, pages 1-12. 
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to Detect Negation with ?Not? in Medical Texts. 
In SIGIR Workshop: Text Analysis and Search for 
Bioinformatics. 
Yang Huang and Henry Lowe. 2007. A Novel Hybrid 
Approach to Automated Negation Detection in Clin-
ical Radiology Reports. Journal of the American 
Medical Informatics Association, 14(3):304-311. 
Zhengping Jiang and Hwee T. Ng. 2006. Semantic Role 
Labeling of NomBank: A Maximum Entropy Ap-
proach. In Proceedings of EMNLP, pages 138-145. 
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bio-
medical Texts. In Proceedings of EMNLP, pages 
715-724. 
Roser Morante and Walter Daelemans. 2009a. A Met-
alearning Approach to Processing the Scope of Ne-
gation. In Proceedings of CoNLL, pages 21-29. 
Roser Morante and Walter Daelemans. 2009b. Learning 
the Scope of Hedge Cues in Biomedical Texts. In 
Proceedings of the BioNLP Workshop, pages 28-36. 
Roser Morante, Vincent Van Asch and Walter Daele-
mans. 2010. Memory-Based Resolution of In-
Sentence Scopes of Hedge Cues. In Proceedings of 
CoNLL Shared Task, pages 40-47. 
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proceedings of 
the 11th Conference of the European Chapter of the 
Association for Computational Linguistics, pages 
113-120. 
Lilja ?vrelid, Erik Velldal, and Stephan Oepen. 2010. 
Syntactic Scope Resolution in Uncertainty Analysis. 
In Proceedings of COLING, pages 1379-1387. 
Arzucan ?zg?r and Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific Text. 
In Proceedings of EMNLP, pages 1398-1407. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. In Proceedings of NAACL, 
pages 404-411. 
Liliana M. S?nchez, Baoli Li, Carl Vogel. 2007. Ex-
ploiting CCG Structures with Tree Kernels for Spec-
ulation Detection. In Proceedings of the Fourteenth 
Conference on Computational Natural Language 
Learning: Shared Task, pages 126-131. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, and 
J?nos Csirik. 2008. The BioScope corpus: Annota-
tion for Negation, Uncertainty and their Scope in Bi-
omedical Texts. In Proceedings of BioNLP, pages 
38-45. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. In Proceedings of IJCNLP, Com-
panion volume, pages 222-227. 
Erik Velldal, Lilja ?vrelid, Jonathon Read and Stephan 
Oepen. 2012. Speculation and Negation: Rules, 
Rankers, and the Role of Syntax. Computational 
Linguistics, 38(2):369-410. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra and J?nos Csirik. 2008. The BioScope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(Suppl 11):S9. 
Guodong Zhou, Min Zhang, Donghong Ji, and Qi-
aoming Zhu. 2007. Tree Kernel-based Relation Ex-
traction with Context-Sensitive Structured Parse 
Tree Information. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages, 728-736. 
 
976
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 522?530,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Negation Focus Identification with Contextual Discourse Information 
 
 
Bowei Zou        Qiaoming Zhu       Guodong Zhou* 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
zoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cn 
 
  
 
Abstract 
Negative expressions are common in natural 
language text and play a critical role in in-
formation extraction. However, the perfor-
mances of current systems are far from satis-
faction, largely due to its focus on intra-
sentence information and its failure to con-
sider inter-sentence information. In this paper, 
we propose a graph model to enrich intra-
sentence features with inter-sentence features 
from both lexical and topic perspectives. 
Evaluation on the *SEM 2012 shared task 
corpus indicates the usefulness of contextual 
discourse information in negation focus iden-
tification and justifies the effectiveness of our 
graph model in capturing such global infor-
mation. * 
1 Introduction 
Negation is a grammatical category which com-
prises various kinds of devices to reverse the 
truth value of a proposition (Morante and 
Sporleder, 2012). For example, sentence (1) 
could be interpreted as it is not the case that he 
stopped. 
(1) He didn't stop. 
Negation expressions are common in natural 
language text. According to the statistics on bio-
medical literature genre (Vincze et al, 2008), 
19.44% of sentences contain negative expres-
sions. The percentage rises to 22.5% on Conan 
Doyle stories (Morante and Daelemans, 2012). It 
is interesting that a negative sentence may have 
both negative and positive meanings. For exam-
ple, sentence (2) could be interpreted as He 
stopped, but not until he got to Jackson Hole 
with positive part he stopped and negative part 
until he got to Jackson Hole. Moreover, a nega-
                                                 
* Corresponding author 
tive expression normally interacts with some 
special part in the sentence, referred as negation 
focus in linguistics. Formally, negation focus is 
defined as the special part in the sentence, which 
is most prominently or explicitly negated by a 
negative expression. Hereafter, we denote nega-
tive expression in boldface and negation focus 
underlined. 
(2) He didn't stop until he got to Jackson Hole. 
While people tend to employ stress or intona-
tion in speech to emphasize negation focus and 
thus it is easy to identify negation focus in 
speech corpora, such stress or intonation infor-
mation often misses in the dominating text cor-
pora. This poses serious challenges on negation 
focus identification. Current studies (e.g., Blanco 
and Moldovan, 2011; Rosenberg and Bergler, 
2012) sort to various kinds of intra-sentence in-
formation, such as lexical features, syntactic fea-
tures, semantic role features and so on, ignoring 
less-obvious inter-sentence information. This 
largely defers the performance of negation focus 
identification and its wide applications, since 
such contextual discourse information plays a 
critical role on negation focus identification. 
Take following sentence as an example. 
(3) Helen didn?t allow her youngest son to 
play the violin. 
In sentence (3), there are several scenarios on 
identification of negation focus, with regard to 
negation expression n?t, given different contexts: 
Scenario A: Given sentence But her husband did 
as next sentence, the negation focus should be 
Helen, yielding interpretation the person who 
didn?t allow the youngest son to play the violin is 
Helen but not her husband. 
Scenario B: Given sentence She thought that he 
didn?t have the artistic talent like her eldest son 
as next sentence, the negation focus should be 
the youngest son, yielding interpretation Helen 
522
thought that her eldest son had the talent to play 
the violin, but the youngest son didn?t. 
Scenario C: Given sentence Because of her 
neighbors? protests as previous sentence, the ne-
gation focus should be play the violin, yielding 
interpretation Helen didn?t allow her youngest 
son to play the violin, but it didn?t show whether 
he was allowed to do other things. 
In this paper, to well accommodate such con-
textual discourse information in negation focus 
identification, we propose a graph model to en-
rich normal intra-sentence features with various 
kinds of inter-sentence features from both lexical 
and topic perspectives. Besides, the standard 
PageRank algorithm is employed to optimize the 
graph model. Evaluation on the *SEM 2012 
shared task corpus (Morante and Blanco, 2012) 
justifies our approach over several strong base-
lines. 
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
presents several strong baselines on negation fo-
cus identification with only intra-sentence fea-
tures. Section 4 introduces our topic-driven 
word-based graph model with contextual dis-
course information. Section 5 reports the exper-
imental results and analysis. Finally, we con-
clude our work in Section 6. 
2 Related Work 
Earlier studies of negation were almost in lin-
guistics (e.g. Horn, 1989; van der Wouden, 
1997), and there were only a few in natural lan-
guage processing with focus on negation recog-
nition in the biomedical domain. For example, 
Chapman et al (2001) developed a rule-based 
negation recognition system, NegEx, to deter-
mine whether a finding mentioned within narra-
tive medical reports is present or absent. Since 
the release of the BioScope corpus (Vincze et al, 
2008), a freely available resource consisting of 
medical and biological texts, machine learning 
approaches begin to dominate the research on 
negation recognition (e.g. Morante et al, 2008; 
Li et al, 2010). 
Generally, negation recognition includes three 
subtasks: cue detection, which detects and identi-
fies possible negative expressions in a sentence, 
scope resolution, which determines the grammat-
ical scope in a sentence affected by a negative 
expression, and focus identification, which iden-
tifies the constituent in a sentence most promi-
nently or explicitly negated by a negative expres-
sion. This paper concentrates on the third subtask, 
negation focus identification. 
Due to the increasing demand on deep under-
standing of natural language text, negation 
recognition has been drawing more and more 
attention in recent years, with a series of shared 
tasks and workshops, however, with focus on cue 
detection and scope resolution, such as the Bi-
oNLP 2009 shared task for negative event detec-
tion (Kim et al, 2009) and the ACL 2010 Work-
shop for scope resolution of negation and specu-
lation (Morante and Sporleder, 2010), followed 
by a special issue of Computational Linguistics 
(Morante and Sporleder, 2012) for modality and 
negation. 
The research on negation focus identification 
was pioneered by Blanco and Moldovan (2011), 
who investigated the negation phenomenon in 
semantic relations and proposed a supervised 
learning approach to identify the focus of a nega-
tion expression. However, although Morante and 
Blanco (2012) proposed negation focus identifi-
cation as one of the *SEM?2012 shared tasks, 
only one team (Rosenberg and Bergler, 2012) 1 
participated in this task. They identified negation 
focus using three kinds of heuristics and 
achieved 58.40 in F1-measure. This indicates 
great expectation in negation focus identification. 
The key problem in current research on nega-
tion focus identification is its focus on intra-
sentence information and large ignorance of in-
ter-sentence information, which plays a critical 
role in the success of negation focus identifica-
tion. For example, Ding (2011) made a qualita-
tive analysis on implied negations in conversa-
tion and attempted to determine whether a sen-
tence was negated by context information, from 
the linguistic perspective. Moreover, a negation 
focus is always associated with authors? intention 
in article. This indicates the great challenges in 
negation focus identification. 
3 Baselines 
Negation focus identification in *SEM?2012 
shared tasks is restricted to verbal negations an-
notated with MNEG in PropBank, with only the 
constituent belonging to a semantic role selected 
as negation focus. Normally, a verbal negation 
expression (not or n?t) is grammatically associat-
ed with its corresponding verb (e.g., He didn?t 
stop). For details on annotation guidelines and 
                                                 
1 In *SEM?2013, the shared task is changed with focus on 
"Semantic Textual Similarity". 
523
examples for verbal negations, please refer to 
Blanco and Moldovan (2011). 
For comparison, we choose the state-of-the-art 
system described in Blanco and Moldovan 
(2011), which employed various kinds of syntac-
tic features and semantic role features, as one of 
our baselines. Since this system adopted C4.5 for 
training, we name it as BaselineC4.5. In order to 
provide a stronger baseline, besides those fea-
tures adopted in BaselineC4.5, we added more re-
fined intra-sentence features and adopted ranking 
Support Vector Machine (SVM) model for train-
ing. We name it as BaselineSVM. 
Following is a list of features adopted in the 
two baselines, for both BaselineC4.5 and Base-
lineSVM, 
? Basic features: first token and its part-of-
speech (POS) tag of the focus candidate; the 
number of tokens in the focus candidate; 
relative position of the focus candidate 
among all the roles present in the sentence; 
negated verb and its POS tag of the negative 
expression;  
? Syntactic features: the sequence of words 
from the beginning of the governing VP to 
the negated verb; the sequence of POS tags 
from the beginning of the governing VP to 
the negated verb; whether the governing VP 
contains a CC; whether the governing VP 
contains a RB. 
? Semantic features: the syntactic label of se-
mantic role A1; whether A1 contains POS 
tag DT, JJ, PRP, CD, RB, VB, and WP, as 
defined in Blanco and Moldovan (2011); 
whether A1 contains token any, anybody, an-
ymore, anyone, anything, anytime, anywhere, 
certain, enough, full, many, much, other, 
some, specifics, too, and until, as defined in 
Blanco and Moldovan (2011); the syntactic 
label of the first semantic role in the sentence; 
the semantic label of the last semantic role in 
the sentence; the thematic role for 
A0/A1/A2/A3/A4 of the negated predicate. 
and for BaselineSVM only, 
? Basic features: the named entity and its type 
in the focus candidate; relative position of the 
focus candidate to the negative expression 
(before or after). 
? Syntactic features: the dependency path and 
its depth from the focus candidate to the neg-
ative expression; the constituent path and its 
depth from the focus candidate to the nega-
tive expression; 
4 Exploring Contextual Discourse In-
formation for Negation Focus Identi-
fication 
While some of negation focuses could be identi-
fied by only intra-sentence information, others 
must be identified by contextual discourse in-
formation. Section 1 illustrates the necessity of 
such contextual discourse information in nega-
tion focus identification by giving three scenarios 
of different discourse contexts for negation ex-
pression n?t in sentence (3). 
For better illustration of the importance of 
contextual discourse information, Table 1 shows 
the statistics of intra- and inter-sentence infor-
mation necessary for manual negation focus 
identification with 100 instances randomly ex-
tracted from the held-out dataset of *SEM'2012 
shared task corpus. It shows that only 17 instanc-
es can be identified by intra-sentence information. 
It is surprising that inter-sentence information is 
indispensable in 77 instances, among which 42 
instances need only inter-sentence information 
and 35 instances need both intra- and inter-
sentence information. This indicates the great 
importance of contextual discourse information 
on negation focus identification. It is also inter-
esting to note 6 instances are hard to determine 
even given both intra- and inter-sentence infor-
mation. 
Info Number
#Intra-Sentence Only 17 
#Inter-Sentence Only 42 
#Both 35 
#Hard to Identify 6 
(Note: "Hard to Identify" means that it is hard for a 
human being to identify the negation focus even 
given both intra- and inter-sentence information.) 
Table 1. Statistics of intra- and inter-sentence 
information on negation focus identification. 
Statistically, we find that negation focus is al-
ways related with what authors repeatedly states 
in discourse context. This explains why contex-
tual discourse information could help identify 
negation focus. While inter-sentence information 
provides the global characteristics from the dis-
course context perspective and intra-sentence 
information provides the local features from lex-
ical, syntactic and semantic perspectives, both 
have their own contributions on negation focus 
identification. 
In this paper, we first propose a graph model 
to gauge the importance of contextual discourse 
524
information. Then, we incorporate both intra- 
and inter-sentence features into a machine learn-
ing-based framework for negation focus identifi-
cation. 
4.1 Graph Model 
Graph models have been proven successful in 
many NLP applications, especially in represent-
ing the link relationships between words or sen-
tences (Wan and Yang, 2008; Li et al, 2009). 
Generally, such models could construct a graph 
to compute the relevance between document 
theme and words. 
In this paper, we propose a graph model to 
represent the contextual discourse information 
from both lexical and topic perspectives. In par-
ticular, a word-based graph model is proposed to 
represent the explicit relatedness among words in 
a discourse from the lexical perspective, while a 
topic-driven word-based model is proposed to 
enrich the implicit relatedness between words, by 
adding one more layer to the word-based graph 
model in representing the global topic distribu-
tion of the whole dataset. Besides, the PageRank 
algorithm (Page et al, 1998) is adopted to opti-
mize the graph model. 
Word-based Graph Model: 
A word-based graph model can be defined as 
Gword (W, E), where W={wi} is the set of words in 
one document and E={eij|wi, wj ?W} is the set of 
directed edges between these words, as shown in 
Figure 1. 
 Figure 1. Word-based graph model. 
In the word-based graph model, word node wi 
is weighted to represent the correlation of the 
word with authors? intention. Since such correla-
tion is more from the semantic perspective than 
the grammatical perspective, only content words 
are considered in our graph model, ignoring 
functional words (e.g., the, to,?). Especially, the 
content words limited to those with part-of-
speech tags of JJ, NN, PRP, and VB. For sim-
plicity, the weight of word node wi is initialized 
to 1. 
In addition, directed edge eij is weighted to 
represent the relatedness between word wi and 
word wj in a document with transition probability 
P(j|i) from i to j, which is normalized as follows: 
???|?? ? ??????,???? ??????,????                    (1) 
where k represents the nodes in discourse, and 
Sim(wi,wj) denotes the similarity between wi and 
wj. In this paper, two kinds of information are 
used to calculate the similarity between words. 
One is word co-occurrence (if word wi and word 
wj occur in the same sentence or in the adjacent 
sentences, Sim(wi,wj) increases 1), and the other 
is WordNet (Miller, 1995) based similarity. 
Please note that Sim(wi,wi) = 0 to avoid self-
transition, and Sim(wi,wj) and Sim(wj,wi) may not 
be equal. 
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows: 
???????????? ? 1 
?????????????? ? ? ? ???????????? ???? ???|?? ?
																																		?1 ? ??                                       (2) 
where d is the damping factor as in the PageRank 
algorithm. 
Topic-driven Word-based Graph Model 
While the above word-based graph model can 
well capture the relatedness between content 
words, it can only partially model the focus of a 
negation expression since negation focus is more 
directly related with topic than content. In order 
to reduce the gap, we propose a topic-driven 
word-based model by adding one more layer to 
refine the word-based graph model over the 
global topic distribution, as shown in Figure 2.  
 Figure 2. Topic-driven word-based graph model. 
525
Here, the topics are extracted from all the doc-
uments in the *SEM 2012 shared task using the 
LDA Gibbs Sampling algorithm (Griffiths, 2002). 
In the topic-driven word-based graph model, the 
first layer denotes the relatedness among content 
words as captured in the above word-based graph 
model, and the second layer denotes the topic 
distribution, with the dashed lines between these 
two layers indicating the word-topic model re-
turn by LDA. 
Formally, the topic-driven word-based two-
layer graph is defined as Gtopic (W, T, Ew, Et), 
where W={wi} is the set of words in one docu-
ment and T={ti} is the set of topics in all docu-
ments; Ew={ewij|wi, wj ?W} is the set of directed 
edges between words and Et ={etij|wi?W, tj ?T} 
is the set of undirected edges between words and 
topics; transition probability Pw(j|i) of ewij is de-
fined as the same as P(j|i) of the word-based 
graph model. Besides, transition probability Pt 
(i,m) of etij in the word-topic model is defined as: 
????, ?? ? ??????,???? ??????,????                 (3) 
where Rel(wi, tm) is the weight of word wi in top-
ic tm calculated by the LDA Gibbs Sampling al-
gorithm.  On the basis, the transition probability 
Pw (j|i) of ewij is updated by calculating as fol-
lowing: 
?????|?? ? ? ? ????|?? ? ?1 ? ?? ? ????,???????,??? ????,???????,???   
(4) 
where k represents all topics linked to both word 
wi and word wj, and ??[0,1] is the coefficient 
controlling the relative contributions from the 
lexical information in current document and the 
topic information in all documents. 
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows: 
???????????? ? 1 
?????????????? ? ? ? ???????????? ???? ?????|?? ?
																																		?1 ? ??                                       (5) 
where d is the damping factor as in the PageRank 
algorithm. 
4.2 Negation Focus Identification via 
Graph Model 
Given the graph models and the PageRank opti-
mization algorithm discussed above, four kinds 
of contextual discourse information are extracted 
as inter-sentence features (Table 2). 
In particular, the total weight and the max 
weight of words in the focus candidate are calcu-
lated as follows: 
??????????? ? ? ?????????????????         (6) 
????????? ? max? ????????????????    (7) 
where i represents the content words in the focus 
candidate. These two kinds of weights focus on 
different aspects about the focus candidate with 
the former on the contribution of content words, 
which is more beneficial for a long focus candi-
date, and the latter biased towards the focus can-
didate which contains some critical word in a 
discourse. 
No Feature 
1 Total weight of words in the focus candi-date using the co-occurrence similarity. 
2 Max weight of words in the focus candi-date using the co-occurrence similarity. 
3 Total weight of words in the focus candi-date using the WordNet similarity. 
4 Max weight of words in the focus candi-date using the WordNet similarity. 
Table 2. Inter-sentence features extracted from 
graph model. 
For evaluating the contribution of contextual 
discourse information on negation focus identifi-
cation directly, we incorporate the four inter-
sentence features from the topic-driven word-
based graph model into a negation focus identifi-
er. 
5 Experimentation 
In this section, we describe experimental settings 
and systematically evaluate our negation focus 
identification approach with focus on exploring 
the effectiveness of contextual discourse infor-
mation. 
5.1 Experimental Settings 
Dataset 
In all our experiments, we employ the 
*SEM'2012 shared task corpus (Morante and 
Blanco, 2012)2 . As a freely downloadable re-
source, the *SEM shared task corpus is annotated 
on top of PropBank, which uses the WSJ section 
of the Penn TreeBank. In particular, negation 
focus annotation on this corpus is restricted to 
verbal negations (with corresponding mark 
                                                 
2 http://www.clips.ua.ac.be/sem2012-st-neg/ 
526
MNEG in PropBank). On 50% of the corpus an-
notated by two annotators, the inter-annotator 
agreement was 0.72 (Blanco and Moldovan, 
2011). Along with negation focus annotation, 
this corpus also contains other annotations, such 
as POS tag, named entity, chunk, constituent tree, 
dependency tree, and semantic role. 
In total, this corpus provides 3,544 instances 
of negation focus annotations. For fair compari-
son, we adopt the same partition as *SEM?2012 
shared task in all our experiments, i.e., with 
2,302 for training, 530 for development, and 712 
for testing. Although for each instance, the cor-
pus only provides the current sentence, the pre-
vious and next sentences as its context, we sort to 
the Penn TreeBank3 to obtain the corresponding 
document as its discourse context. 
Evaluation Metrics 
Same as the *SEM'2012 shared task, the evalua-
tion is made using precision, recall, and F1-score. 
Especially, a true positive (TP) requires an exact 
match for the negation focus, a false positive (FP) 
occurs when a system predicts a non-existing 
negation focus, and a false negative (FN) occurs 
when the gold annotations specify a negation 
focus but the system makes no prediction. For 
each instance, the predicted focus is considered 
correct if it is a complete match with a gold an-
notation. 
Beside, to show whether an improvement is 
significant, we conducted significance testing 
using z-test, as described in Blanco and Moldo-
van (2011). 
Toolkits 
In our experiments, we report not only the de-
fault performance with gold additional annotated 
features provided by the *SEM'2012 shared task 
corpus and the Penn TreeBank, but also the per-
formance with various kinds of features extracted 
automatically, using following toolkits: 
? Syntactic Parser: We employ the Stanford 
Parser4 (Klein and Manning, 2003; De Marn-
effe et al, 2006) for tokenization, constituent 
and dependency parsing. 
? Named Entity Recognizer: We employ the 
Stanford NER5 (Finkel et al, 2005) to obtain 
named entities. 
                                                 
3 http://www.cis.upenn.edu/~treebank/ 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://nlp.stanford.edu/ner/ 
? Semantic Role Labeler: We employ the se-
mantic role labeler, as described in Punyaka-
nok et al(2008). 
? Topic Modeler: For estimating transition 
probability Pt(i,m), we employ 
GibbsLDA++6, an LDA model using Gibbs 
Sampling technique for parameter estimation 
and inference. 
? Classifier: We employ SVMLight 7 with default 
parameters as our classifier. 
5.2 Experimental Results 
With Only Intra-sentence Information 
Table 3 shows the performance of the two base-
lines, the decision tree-based classifier as in 
Blanco and Moldovan (2011) and our ranking 
SVM-based classifier. It shows that our ranking 
SVM-based baseline slightly improves the F1-
measure by 2.52% over the decision tree-based 
baseline, largely due to the incorporation of more 
refined features.  
System P(%) R(%) F1 
BaselineC4.5 66.73 49.93 57.12
BaselineSVM 60.22 59.07 59.64
Table 3. Performance of baselines with only 
intra-sentence information. 
Error analysis of the ranking SVM-based 
baseline on development data shows that 72% of 
them are caused by the ignorance of inter-
sentence information. For example, among the 
42 instances listed in the category of ?#Inter-
Sentence Only? in Table 1, only 7 instances can 
be identified correctly by the ranking SVM-
based classifier. With about 4 focus candidates in 
one sentence on average, this percentage is even 
lower than random. 
With Only Inter-sentence Information 
For exploring the usefulness of pure contextual 
discourse information in negation focus identifi-
cation, we only employ inter-sentence features 
into ranking SVM-based classifier. First of all, 
we estimate two parameters for our topic-driven 
word-based graph model: topic number T for 
topic model and coefficient ? between Pw(j|i) and 
Pt (i,m) in Formula 4. 
Given the LDA Gibbs Sampling model with 
parameters ? = 50/T and ? = 0.1, we vary T from 
20 to 100 with an interval of 10 to find the opti-
                                                 
6 http://gibbslda.sourceforge.net/ 
7 http://svmlight.joachims.org 
527
mal T. Figure 3 shows the experiment results of 
varying T (with ? = 0.5) on development data. It 
shows that the best performance is achieved 
when T = 50 with 51.11 in F1). Therefore, we set 
T as 50 in our following experiments. 
 Figure 3. Performance with varying T. 
For parameter ?, a trade-off between the tran-
sition probability Pw(j|i) (word to word) and the 
transition probability Pt (i,m) (word and topic) to 
update P?w(j|i), we vary it from 0 to 1 with an 
interval of 0.1. Figure 4 shows the experiment 
results of varying ? (with T=50) on development 
data. It shows that the best performance is 
achieved when ? = 0.6, which are adopted here-
after in all our experiments. This indicates that 
direct lexical information in current document 
contributes more than indirect topic information 
in all documents on negation focus identification. 
It also shows that direct lexical information in 
current document and indirect topic information 
in all documents are much complementary on 
negation focus identification. 
 Figure 4. Performance with varying ?. 
System P(%) R(%) F1 
using word-based graph 
model  45.62 42.02 43.75
using topic-driven word-
based graph model 54.59 50.76 52.61
Table 4. Performance with only inter-sentence 
information. 
Table 4 shows the performance of negation 
focus identification with only inter-sentence fea-
tures. It also shows that the system with inter-
sentence features from the topic-driven word-
based graph model significantly improves the 
F1-measure by 8.86 over the system with inter-
sentence features from the word-based graph 
model, largely due to the usefulness of topic in-
formation. 
In comparison with Table 3, it shows that the 
system with only intra-sentence features achieves 
better performance than the one with only inter-
sentence features (59.64 vs. 52.61 in F1-
measure). 
With both Intra- and Inter-sentence In-
formation 
Table 5 shows that enriching intra-sentence fea-
tures with inter-sentence features significantly 
(p<0.01) improve the performance by 9.85 in F1-
measure than the better baseline. This indicates 
the usefulness of such contextual discourse in-
formation and the effectiveness of our topic-
driven word-based graph model in negation fo-
cus identification.  
System P(%) R(%) F1 
BaselineC4.5 with intra 
feat. only 66.73 49.93 57.12
BaselineSVM with intra 
feat. only 60.22 59.07 59.64
Ours with Both feat. 
using word-based GM 64.93 62.47 63.68
Ours  with  Both   feat. 
using    topic-driven 
word-based GM
71.67 67.43 69.49
(Note: ?feat.? denotes features; ?GM? denotes graph model.) 
Table 5. Performance comparison of systems on 
negation focus identification. 
System P(%) R(%) F1 
BaselineC4.5 with intra 
feat. only (auto) 60.94 44.62 51.52
BaselineSVM with intra 
feat. Only (auto) 53.81 51.67 52.72
Ours with Both feat. 
using word-based GM 
(auto) 
58.77 57.19 57.97
Ours  with  Both   feat. 
using    topic-driven 
word-based GM (auto) 
66.74 64.53 65.62
Table 6. Performance comparison of systems on 
negation focus identification with automatically 
extracted features. 
528
Besides, Table 6 shows the performance of 
our best system with all features automatically 
extracted using the toolkits as described in Sec-
tion 5.1. Compared with our best system employ-
ing gold additional annotated features (the last 
line in Table 5), the homologous system with 
automatically extracted features (the last line in 
Table 6) only decrease of less than 4 in F1-
measure. This demonstrates the achievability of 
our approach. 
In comparison with the best-reported perfor-
mance on the *SEM?2012 shared task (Rosen-
berg and Bergler, 2012), our system performs 
better by about 11 in F-measure.  
5.3 Discussion 
While this paper verifies the usefulness of con-
textual discourse information on negation focus 
identification, the performance with only inter-
sentence features is still weaker than that with 
only intra-sentence features. There are two main 
reasons. On the one hand, the former employs an 
unsupervised approach without prior knowledge 
for training. On the other hand, the usefulness of 
inter-sentence features depends on the assump-
tion that a negation focus relates to the meaning 
of which is most relevant to authors? intention in 
a discourse. If there lacks relevant information in 
a discourse context, negation focus will become 
difficult to be identified only by inter-sentence 
features.  
Error analysis also shows that some of the ne-
gation focuses are very difficult to be identified, 
even for a human being. Consider the sentence (3) 
in Section 1, if given sentence because of her 
neighbors' protests, but her husband doesn?t 
think so as its following context, both Helen and 
to play the violin can become the negation focus. 
Moreover, the inter-annotator agreement in the 
first round of negation focus annotation can only 
reach 0.72 (Blanco and Moldovan, 2011). This 
indicates inherent difficulty in negation focus 
identification. 
6 Conclusion 
In this paper, we propose a graph model to enrich 
intra-sentence features with inter-sentence fea-
tures from both lexical and topic perspectives. In 
this graph model, the relatedness between words 
is calculated by word co-occurrence, WordNet-
based similarity, and topic-driven similarity. 
Evaluation on the *SEM 2012 shared task corpus 
indicates the usefulness of contextual discourse 
information on negation focus identification and 
our graph model in capturing such global infor-
mation. 
In future work, we will focus on exploring 
more contextual discourse information via the 
graph model and better ways of integrating intra- 
and inter-sentence information on negation focus 
identification. 
Acknowledgments 
This research is supported by the National Natu-
ral Science Foundation of China, No.61272260, 
No.61331011, No.61273320, the Natural Science 
Foundation of Jiangsu Province, No. BK2011282, 
the Major Project of College Natural Science 
Foundation of Jiangsu Province, 
No.11KIJ520003, and the Graduates Project of 
Science and Innovation, No. CXZZ12_0818. The 
authors would like to thank the anonymous re-
viewers for their insightful comments and sug-
gestions. Our sincere thanks are also extended to 
Dr. Zhongqing Wang for his valuable discus-
sions during this study. 
Reference 
Eduardo Blanco and Dan Moldovan. 2011. Semantic 
Representation of Negation Using Focus Detection. 
In Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics, pages 
581-589, Portland, Oregon, June 19-24, 2011. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. 
A simple algorithm for identifying negated find-
ings and diseases in discharge summaries. Journal 
of Biomedical Informatics, 34:301-310. 
Marie-Catherine De Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
In Proceedings of  LREC?2006. 
Yun Ding. 2011. Implied Negation in Discourse. 
Journal of Theory and Practice in Language Stud-
ies, 1(1): 44-51, Jan 2011. 
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating non-local infor-
mation into information extraction systems by 
gibbs sampling. In Proceedings of the 43rd Annual 
Meeting on Association for Computational Lin-
guistics, pages 363-370, Stroudsburg, PA, USA. 
Tom Griffiths. 2002. Gibbs sampling in the generative 
model of Latent Dirichlet Allocation. Tech. rep., 
Stanford University. 
Laurence R Horn. 1989. A Natural History of Nega-
tion. Chicago University Press, Chicago, IL. 
529
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan 
Zhu. 2009. Answering Opinion Questions with 
Random Walks on Graphs. In Proceedings of the 
47th Annual Meeting of the ACL and the 4th 
IJCNLP of the AFNLP, pages 737-745, Suntec, 
Singapore, 2-7 Aug 2009. 
Junhui Li, Guodong Zhou, Hongling Wang, and Qi-
aoming Zhu. 2010. Learning the Scope of Negation 
via Shallow Semantic Parsing. In Proceedings of 
the 23rd International Conference on Computa-
tional Linguistics. Stroudsburg, PA, USA: Associa-
tion for Computational Linguistics, 671-679. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun'ichi Tsujii. 2009. Over-
view of BioNLP'09 Shared Task on Event Extrac-
tion. In Proceedings of the BioNLP'2009 Workshop 
Companion Volume for Shared Task. Stroudsburg, 
PA, USA: Association for Computational Linguis-
tics, 1-9. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 
41st Meeting of the Association for Computational 
Linguistics, pages 423-430. 
George A. Miller. 1995. Wordnet: a lexical database 
for english. Commun. ACM, 38(11):39-41. 
Roser Morante, Anthony Liekens and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bi-
omedical Texts. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 715-724, Honolulu, Oc-
tober 2008. 
Roser Morante and Caroline Sporleder, editors. 2010. 
In Proceedings of the Workshop on Negation and 
Speculation in Natural Language Processing. Uni-
versity of Antwerp, Uppsala, Sweden. 
Roser Morante and Eduardo Blanco. 2012. *SEM 
2012 Shared Task: Resolving the Scope and Focus 
of Negation. In Proceedings of the First Joint Con-
ference on Lexical and Computational Semantics 
(*SEM), pages 265-274, Montreal, Canada, June 7-
8, 2012. 
Roser Morante and Caroline Sporleder. 2012. Modali-
ty and Negation: An Introduction to the Special Is-
sue. Computational Linguistics, 2012, 38(2): 223-
260. 
Roser Morante and Walter Daelemans. 2012. Conan 
Doyle-neg: Annotation of negation cues and their 
scope in Conan Doyle stories. In Proceedings of 
LREC 2012, Istambul. 
Lawrence Page, Sergey Brin, Rajeev Motwani, and 
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report, 
Stanford University. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. 
The importance of syntactic parsing and inference 
in semantic role labeling. Computational Linguis-
tics, 34(2):257-287, June. 
Sabine Rosenberg and Sabine Bergler. 2012. UCon-
cordia: CLaC Negation Focus Detection at *Sem 
2012. In Proceedings of the First Joint Conference 
on Lexical and Computational Semantics (*SEM), 
pages 294-300, Montreal, Canada, June 7-8, 2012. 
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity, and Multiple Negation. 
Routledge, London. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for un-
certainty,negation and their scopes. BMC Bioin-
formatics, 9(Suppl 11):S9. 
Xiaojun Wan and Jianwu Yang. 2008. Multi-
document summarization using cluster-based link 
analysis. In Proceedings of the 31st annual inter-
national ACM SIGIR conference on Research and 
development in information retrieval, pages 299-
306. 
 
530
