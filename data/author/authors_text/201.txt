Looking Under the Hood: Tools for Diagnosing Your Question
Answering Engine
Eric Breck?, Marc Light?, Gideon S. Mann?, Ellen Riloff?,
Brianne Brown?, Pranav Anand?, Mats Rooth?, Michael Thelen?
? The MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730, {ebreck,light}@mitre.org
? Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, gsm@cs.jhu.edu
? School of Computing, University of Utah, Salt Lake City, UT 84112, {riloff,thelenm}@cs.utah.edu
? Bryn Mawr College, Bryn Mawr, PA 19010, bbrown@brynmawr.edu
? Department of Mathematics, Harvard University, Cambridge, MA 02138, anand@fas.harvard.edu
? Department of Linguistics, Cornell University, Ithaca, NY 14853, mr249@cornell.edu
Abstract
In this paper we analyze two question
answering tasks : the TREC-8 ques-
tion answering task and a set of reading
comprehension exams. First, we show
that Q/A systems perform better when
there are multiple answer opportunities
per question. Next, we analyze com-
mon approaches to two subproblems:
term overlap for answer sentence iden-
tification, and answer typing for short
answer extraction. We present general
tools for analyzing the strengths and
limitations of techniques for these sub-
problems. Our results quantify the limi-
tations of both term overlap and answer
typing to distinguish between compet-
ing answer candidates.
1 Introduction
When building a system to perform a task, the
most important statistic is the performance on
an end-to-end evaluation. For the task of open-
domain question answering against text collec-
tions, there have been two large-scale end-to-
end evaluations: (TREC-8 Proceedings, 1999)
and (TREC-9 Proceedings, 2000). In addition, a
number of researchers have built systems to take
reading comprehension examinations designed to
evaluate children?s reading levels (Charniak et al,
2000; Hirschman et al, 1999; Ng et al, 2000;
Riloff and Thelen, 2000; Wang et al, 2000). The
performance statistics have been useful for deter-
mining how well techniques work.
However, raw performance statistics are not
enough. If the score is low, we need to under-
stand what went wrong and how to fix it. If the
score is high, it is important to understand why.
For example, performance may be dependent on
characteristics of the current test set and would
not carry over to a new domain. It would also be
useful to know if there is a particular character-
istic of the system that is central. If so, then the
system can be streamlined and simplified.
In this paper, we explore ways of gaining
insight into question answering system perfor-
mance. First, we analyze the impact of having
multiple answer opportunities for a question. We
found that TREC-8 Q/A systems performed bet-
ter on questions that had multiple answer oppor-
tunities in the document collection. Second, we
present a variety of graphs to visualize and ana-
lyze functions for ranking sentences. The graphs
revealed that relative score instead of absolute
score is paramount. Third, we introduce bounds
on functions that use term overlap1 to rank sen-
tences. Fourth, we compute the expected score of
a hypothetical Q/A system that correctly identifies
the answer type for a question and correctly iden-
tifies all entities of that type in answer sentences.
We found that a surprising amount of ambiguity
remains because sentences often contain multiple
entities of the same type.
1Throughout the text, we use ?overlap? to refer to the
intersection of sets of words, most often the words in the
question and the words in a sentence.
2 The data
The experiments in Sections 3, 4, and 5 were per-
formed on two question answering data sets: (1)
the TREC-8 Question Answering Track data set
and (2) the CBC reading comprehension data set.
We will briefly describe each of these data sets
and their corresponding tasks.
The task of the TREC-8 Question Answering
track was to find the answer to 198 questions us-
ing a document collection consisting of roughly
500,000 newswire documents. For each question,
systems were allowed to return a ranked list of
5 short (either 50-character or 250-character) re-
sponses. As a service to track participants, AT&T
provided top documents returned by their retrieval
engine for each of the TREC questions. Sec-
tions 4 and 5 present analyses that use all sen-
tences in the top 10 of these documents. Each
sentence is classified as correct or incorrect auto-
matically. This automatic classification judges a
sentence to be correct if it contains at least half
of the stemmed, content-words in the answer key.
We have compared this automatic evaluation to
the TREC-8 QA track assessors and found it to
agree 93-95% of the time (Breck et al, 2000).
The CBC data set was created for the Johns
Hopkins Summer 2000 Workshop on Reading
Comprehension. Texts were collected from the
Canadian Broadcasting Corporation web page for
kids (http://cbc4kids.ca/). They are an average
of 24 sentences long. The stories were adapted
from newswire texts to be appropriate for ado-
lescent children, and most fall into the follow-
ing domains: politics, health, education, science,
human interest, disaster, sports, business, crime,
war, entertainment, and environment. For each
CBC story, 8-12 questions and an answer key
were generated.2 We used a 650 question sub-
set of the data and their corresponding 75 stories.
The answer candidates for each question in this
data set were all sentences in the document. The
sentences were scored against the answer key by
the automatic method described previously.
2This work was performed by Lisa Ferro and Tim Bevins
of the MITRE Corporation. Dr. Ferro has professional expe-
rience writing questions for reading comprehension exams
and led the question writing effort.
3 Analyzing the number of answer
opportunities per question
In this section we explore the impact of multiple
answer opportunities on end-to-end system per-
formance. A question may have multiple answers
for two reasons: (1) there is more than one differ-
ent answer to the question, and (2) there may be
multiple instances of each answer. For example,
?What does the Peugeot company manufacture??
can be answered by trucks, cars, or motors and
each of these answers may occur in many sen-
tences that provide enough context to answer the
question. The table insert in Figure 1 shows that,
on average, there are 7 answer occurrences per
question in the TREC-8 collection.3 In contrast,
there are only 1.25 answer occurrences in a CBC
document. The number of answer occurrences
varies widely, as illustrated by the standard devia-
tions. The median shows an answer frequency of
3 for TREC and 1 for CBC, which perhaps gives
a more realistic sense of the degree of answer fre-
quency for most questions.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5 6 7 9 1 2 1 4 1 8 2 7 2 8 6 1 6 7
# Answers
%
 Q
ue
st
io
ns
TREC-8
5 0
3 5 2
7.04
3
12.94
CBC
2 1 9
2 7 4
1.25
1
0.61
# Questions
# Answers
Mean
Median
Standard Dev.
Figure 1: Frequency of answers in the TREC-8
(black bars) and CBC (white bars) data sets
To gather this data we manually reviewed 50
randomly chosen TREC-8 questions and identi-
fied all answers to these questions in our text col-
lection. We defined an ?answer? as a text frag-
ment that contains the answer string in a context
sufficient to answer the question. Figure 1 shows
the resulting graph. The x-axis displays the num-
ber of answer occurrences found in the text col-
lection per question and the y-axis shows the per-
3We would like to thank John Burger and John Aberdeen
for help preparing Figure 1.
centage of questions that had x answers. For ex-
ample, 26% of the TREC-8 questions had only
1 answer occurrence, and 20% of the TREC-8
questions had exactly 2 answer occurrences (the
black bars). The most prolific question had 67
answer occurrences (the Peugeot example men-
tioned above). Figure 1 also shows the analysis
of 219 CBC questions. In contrast, 80% of the
CBC questions had only 1 answer occurrence in
the targeted document, and 16% had exactly 2 an-
swer occurrences.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 1 0 2 0 3 0 4 0 5 0 6 0 7 0
# answers occurences per question
%
 o
f s
ys
te
m
s 
w
ith
 a
t l
ea
st
 o
ne
 c
or
re
ct
 re
sp
on
se
Point per question
Mean correct per occurrence #
Figure 2: Answer repetition vs. system response
correctness for TREC-8
Figure 2 shows the effect that multiple answer
opportunities had on the performance of TREC-8
systems. Each solid dot in the scatter plot repre-
sents one of the 50 questions we examined.4 The
x-axis shows the number of answer opportunities
for the question, and the y-axis represents the per-
centage of systems that generated a correct an-
swer5 for the question. E.g., for the question with
67 answer occurrences, 80% of the systems pro-
duced a correct answer. In contrast, many ques-
tions had a single answer occurrence and the per-
centage of systems that got those correct varied
from about 2% to 60%.
The circles in Figure 2 represent the average
percentage of systems that answered questions
correctly for all questions with the same number
of answer occurrences. For example, on average
about 27% of the systems produced a correct an-
swer for questions that had exactly one answer oc-
4We would like to thank Lynette Hirschman for suggest-
ing the analysis behind Figure 2 and John Burger for help
with the analysis and presentation.
5For this analysis, we say that a system generated a cor-
rect answer if a correct answer was in its response set.
currence, but about 50% of the systems produced
a correct answer for questions with 7 answer op-
portunities. Overall, a clear pattern emerges: the
performance of TREC-8 systems was strongly
correlated with the number of answer opportuni-
ties present in the document collection.
4 Graphs for analyzing scoring
functions of answer candidates
Most question answering systems generate sev-
eral answer candidates and rank them by defin-
ing a scoring function that maps answer candi-
dates to a range of numbers. In this section,
we analyze one particular scoring function: term
overlap between the question and answer can-
didate. The techniques we use can be easily
applied to other scoring functions as well (e.g.,
weighted term overlap, partial unification of sen-
tence parses, weighted abduction score, etc.). The
answer candidates we consider are the sentences
from the documents.
The expected performance of a system that
ranks all sentences using term overlap is 35% for
the TREC-8 data. This number is an expected
score because of ties: correct and incorrect can-
didates may have the same term overlap score. If
ties are broken optimally, the best possible score
(maximum) would be 54%. If ties are broken
maximally suboptimally, the worst possible score
(minimum) would be 24%. The corresponding
scores on the CBC data are 58% expected, 69%
maximum, and 51% minimum. We would like to
understand why the term overlap scoring function
works as well as it does and what can be done to
improve it.
Figures 3 and 4 compare correct candidates and
incorrect candidates with respect to the scoring
function. The x-axis plots the range of the scor-
ing function, i.e., the amount of overlap. The
y-axis represents Pr(overlap=x | correct) and
Pr(overlap=x | incorrect), where separate curves
are plotted for correct and incorrect candidates.
The probabilities are generated by normalizing
the number of correct/incorrect answer candidates
with a particular overlap score by the total number
of correct/incorrect candidates, respectively.
Figure 3 illustrates that the correct candidates
for TREC-8 have term overlap scores distributed
between 0 and 10 with a peak of 24% at an over-
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0 2 4 6 8 10 12 14 16 18 20N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
overlap
incorrect
correct
Figure 3: Pr(overlap=x|[in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 5 10 15 20 25 30No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
overlap
incorrect
correct
Figure 4: Pr(overlap=x|[in]correct) for CBC
lap of 2. However, the incorrect candidates have
a similar distribution between 0 and 8 with a peak
of 32% at an overlap of 0. The similarity of the
curves illustrates that it is unclear how to use the
score to decide if a candidate is correct or not.
Certainly no static threshold above which a can-
didate is deemed correct will work. Yet the ex-
pected score of our TREC term overlap system
was 35%, which is much higher than a random
baseline which would get an expected score of
less than 3% because there are over 40 sentences
on average in newswire documents.6
After inspecting some of the data directly, we
posited that it was not the absolute term overlap
that was important for judging candidate but how
the overlap score compares to the scores of other
candidates. To visualize this, we generated new
graphs by plotting the rank of a candidate?s score
6We also tried dividing the term overlap score by the
length of the question to normalize for query length but did
not find that the graph was any more helpful.
on the x-axis. For example, the candidate with
the highest score would be ranked first, the can-
didate with the second highest score would be
ranked second, etc. Figures 5 and 6 show these
graphs, which display Pr(rank=x | correct) and
Pr(rank=x | incorrect) on the y-axis. The top-
ranked candidate has rank=0.
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
-
10
00
-
90
0
-
80
0
-
70
0
-
60
0
-
50
0
-
40
0
-
30
0
-
20
0
-
10
0 0
N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
ranked overlap
incorrect
correct
Figure 5: Pr(rank=x | [in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
-45 -40 -35 -30 -25 -20 -15 -10 -5 0No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
ranked overlap
incorrect
correct
Figure 6: Pr(rank=x | [in]correct) for CBC
The ranked graphs are more revealing than the
graphs of absolute scores: the probability of a
high rank is greater for correct answers than in-
correct ones. Now we can begin to understand
why the term overlap scoring function worked as
well as it did. We see that, unlike classification
tasks, there is no good threshold for our scor-
ing function. Instead relative score is paramount.
Systems such as (Ng et al, 2000) make explicit
use of relative rank in their algorithms and now
we understand why this is effective.
Before we leave the topic of graphing scoring
functions, we want to introduce one other view of
the data. Figure 7 plots term overlap scores on
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
0 2 4 6 8 10 12 14
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
lo
g-
od
ds
 o
f c
or
re
ct
ne
ss
m
a
ss
overlap
log-odds
mass curve
Figure 7: TREC-8 log odds correct given overlap
the x-axis and the log odds of being correct given
a score on the y-axis. The log odds formula is:
log Pr(correct|overlap)Pr(incorrect|overlap)
Intuitively, this graph shows how much more
likely a sentence is to be correct versus incorrect
given a particular score. A second curve, labeled
?mass,? plots the number of answer candidates
with each score. Figure 7 shows that the odds of
being correct are negative until an overlap of 10,
but the mass curve reveals that few answer candi-
dates have an overlap score greater than 6.
5 Bounds on scoring functions that use
term overlap
The scoring function used in the previous sec-
tion simply counts the number of terms shared
by a question and a sentence. One obvious mod-
ification is to weight some terms more heavily
than others. We tried using inverse document fre-
quence based (IDF) term weighting on the CBC
data but found that it did not improve perfor-
mance. The graph analogous to Figure 6 but with
IDF term weighting was virtually identical.
Could another weighting scheme perform bet-
ter? How well could an optimal weighting
scheme do? How poorly would the maximally
suboptimal scheme do? The analysis in this sec-
tion addresses these questions. In essence the an-
swer is the following: the question and the can-
didate answers are typically short and thus the
number of overlapping terms is small ? conse-
quently, many candidate answers have exactly the
same overlapping terms and no weighting scheme
could differentiate them. In addition, subset rela-
tions often hold between overlaps. A candidate
whose overlap is a subset of a second candidate
cannot score higher regardless of the weighting
scheme.7 We formalize these overlap set relations
and then calculate statistics based on them for the
CBC and TREC data.
Question: How much was Babe Belanger paid to play
amateur basketball?
S1: She was a member of the winningest
basketball team Canada ever had.
S2: Babe Belanger never made a cent for her
skills.
S3: They were just a group of young women
from the same school who liked to
play amateur basketball.
S4: Babe Belanger played with the Grads from
1929 to 1937.
S5: Babe never talked about her fabulous career.
MaxOsets : ( {S2, S4}, {S3} )
Figure 8: Example of Overlap Sets from CBC
Figure 8 presents an example from the CBC
data. The four overlap sets are (i) Babe Belanger,
(ii) basketball, (iii) play amateur basketball, and
(iv) Babe. In any term-weighting scheme with
positive weights, a sentence containing the words
Babe Belanger will have a higher score than sen-
tences containing just Babe, and sentences with
play amateur basketball will have a higher score
than those with just basketball. However, we can-
not generalize with respect to the relative scores
of sentences containing Babe Belanger and those
containing play amateur basketball because some
terms may have higher weights than others.
The most we can say is that the highest scor-
ing candidate must be a member of {S2, S4} or
{S3}. S5 and S1 cannot be ranked highest be-
cause their overlap sets are a proper subset of
competing overlap sets. The correct answer is
S2 so an optimal weighting scheme would have
a 50% chance of ranking S2 first, assuming that
it identified the correct overlap set {S2, S4} and
then randomly chose between S2 and S4. A max-
imally suboptimal weighting scheme could rank
S2 no lower than third.
We will formalize these concepts using the fol-
lowing variables:
7Assuming that all term weights are positive.
q: a question (a set of words)
s: a sentence (a set of words)
w,v: sets of intersecting words
We define an overlap set (ow,q) to be a set of
sentences (answer candidates) that have the same
words overlapping with the question. We define a
maximal overlap set (Mq) as an overlap set that is
not a subset of any other overlap set for the ques-
tion. For simplicity, we will refer to a maximal
overlap set as a MaxOset.
ow,q = {s|s ? q = w}
?q = all unique overlap sets for q
maximal(ow,q) if ?ov,q ? ?q, w 6? v
Mq = {ow,q ? ?q | maximal(ow,q)}
Cq = {s|s correctly answers q}
We can use these definitions to give upper
and lower bounds on the performance of term-
weighting functions on our two data sets. Table 1
shows the results. The max statistic is the per-
centage of questions for which at least one mem-
ber of its MaxOsets is correct. The min statis-
tic is the percentage of questions for which all
candidates of all of its MaxOsets are correct (i.e.,
there is no way to pick a wrong answer). Finally
the expectedmax is a slightly more realistic up-
per bound. It is equivalent to randomly choosing
among members of the ?best? maximal overlap
set, i.e., the MaxOset that has the highest percent-
age of correct members. Formally, the statistics
for a set of questions Q are computed as:
max = |{q|?o ? Mq,?s ? o s.t. s ? Cq}||Q|
min = |{q|?o ? Mq,?s ? o s ? Cq}||Q|
exp. max = 1|Q| ?
?
q?Q
max
o?Mq
|{s ? o and s ? Cq}|
|o|
The results for the TREC data are considerably
lower than the results for the CBC data. One ex-
planation may be that in the CBC data, only sen-
tences from one document containing the answer
are considered. In the TREC data, as in the TREC
task, it is not known beforehand which docu-
ments contain answers, so irrelevant documents
exp. max max min
CBC training 72.7% 79.0% 24.4%
TREC-8 48.8% 64.7% 10.1%
Table 1: Maximum overlap analysis of scores
may contain high-scoring sentences that distract
from the correct sentences.
In Table 2, we present a detailed breakdown
of the MaxOset results for the CBC data. (Note
that the classifications overlap, e.g., questions that
are in ?there is always a chance to get it right?
are also in the class ?there may be a chance to
get it right.?) 21% of the questions are literally
impossible to get right using only term weight-
ing because none of the correct sentences are in
the MaxOsets. This result illustrates that maxi-
mal overlap sets can identify the limitations of a
scoring function by recognizing that some candi-
dates will always be ranked higher than others.
Although our analysis only considered term over-
lap as a scoring function, maximal overlap sets
could be used to evaluate other scoring functions
as well, for example overlap sets based on seman-
tic classes rather than lexical items.
In sum, the upper bound for term weighting
schemes is quite low and the lower bound is
quite high. These results suggest that methods
such as query expansion are essential to increase
the feature sets used to score answer candidates.
Richer feature sets could distinguish candidates
that would otherwise be represented by the same
features and therefore would inevitably receive
the same score.
6 Analyzing the effect of multiple
answer type occurrences in a sentence
In this section, we analyze the problem of extract-
ing short answers from a sentence. Many Q/A
systems first decide what answer type a question
expects and then identify instances of that type in
sentences. A scoring function ranks the possible
answers using additional criteria, which may in-
clude features of the surrounding sentence such
as term overlap with the question.
For our analysis, we will assume that two short
answers that have the same answer type and come
from the same sentence are indistinguishable to
the system. This assumption is made by many
number of percentage
questions of questions
Impossible to get it wrong 159 24%
(?ow ? Mq, ?s ? ow, s ? Cq)
There is always a chance to get it right 45 7%
(?ow ? Mq, ?s ? ow s.t. s ? Cq)
There may be a chance to get it right 310 48%
(?ow ? Mq s.t. ?s ? ow s.t. s ? Cq)
The wrong answers will always be weighted too highly 137 21%
(?ow ? Mq, ?s ? ow, s 6? Cq)
There are no correct answers with any overlap with Q 66 10%
(?s ? d, s is incorrect or s has 0 overlap)
There are no correct answers (auto scoring error) 12 2%
(?s ? d, s is incorrect)
Table 2: Maximal Overlap Set Analysis for CBC data
Q/A systems: they do not have features that can
prefer one entity over another of the same type in
the same sentence.
We manually annotated data for 165 TREC-
9 questions and 186 CBC questions to indicate
perfect question typing, perfect answer sentence
identification, and perfect semantic tagging. Us-
ing these annotations, we measured how much
?answer confusion? remains if an oracle gives you
the correct question type, a sentence containing
the answer, and correctly tags all entities in the
sentence that match the question type. For exam-
ple, the oracle tells you that the question expects
a person, gives you a sentence containing the cor-
rect person, and tags all person entities in that sen-
tence. The one thing the oracle does not tell you
is which person is the correct one.
Table 3 shows the answer types that we used.
Most of the types are fairly standard, except for
the Defaultnp and Defaultvp which are default
tags for questions that desire a noun phrase or
verb phrase but cannot be more precisely typed.
We computed an expected score for this hy-
pothetical system as follows: for each question,
we divided the number of correct candidates (usu-
ally one) by the total number of candidates of the
same answer type in the sentence. For example,
if a question expects a Location as an answer and
the sentence contains three locations, then the ex-
pected accuracy of the system would be 1/3 be-
cause the system must choose among the loca-
tions randomly. When multiple sentences contain
a correct answer, we aggregated the sentences. Fi-
nally, we averaged this expected accuracy across
all questions for each answer type.
TREC CBC
Answer Type Score Freq Score Freq
defaultnp .33 47 .25 28
organization .50 1 .72 3
length .50 1 .75 2
thingname .58 14 .50 1
quantity .58 13 .77 14
agent .63 19 .40 23
location .70 24 .68 29
personname .72 11 .83 13
city .73 3 n/a 0
defaultvp .75 2 .42 15
temporal .78 16 .75 26
personnoun .79 7 .53 5
duration 1.0 3 .67 4
province 1.0 2 1.0 2
area 1.0 1 n/a 0
day 1.0 1 n/a 0
title n/a 0 .50 1
person n/a 0 .67 3
money n/a 0 .88 8
ambigbig n/a 0 .88 4
age n/a 0 1.0 2
comparison n/a 0 1.0 1
mass n/a 0 1.0 1
measure n/a 0 1.0 1
Overall .59 165 .61 186
Overall-dflts .69 116 .70 143
Table 3: Expected scores and frequencies for each
answer type
Table 3 shows that a system with perfect ques-
tion typing, perfect answer sentence identifica-
tion, and perfect semantic tagging would still
achieve only 59% accuracy on the TREC-9 data.
These results reveal that there are often multi-
ple candidates of the same type in a sentence.
For example, Temporal questions received an ex-
pected score of 78% because there was usually
only one date expression per sentence (the correct
one), while Default NP questions yielded an ex-
pected score of 25% because there were four noun
phrases per question on average. Some common
types were particularly problematic. Agent ques-
tions (most Who questions) had an answer con-
fusability of 0.63, while Quantity questions had a
confusability of 0.58.
The CBC data showed a similar level of an-
swer confusion, with an expected score of 61%,
although the confusability of individual answer
types varied from TREC. For example, Agent
questions were even more difficult, receiving a
score of 40%, but Quantity questions were easier
receiving a score of 77%.
Perhaps a better question analyzer could assign
more specific types to the Default NP and De-
fault VP questions, which skew the results. The
Overall-dflts row of Table 3 shows the expected
scores without these types, which is still about
70% so a great deal of answer confusion remains
even without those questions. The confusability
analysis provides insight into the limitations of
the answer type set, and may be useful for com-
paring the effectiveness of different answer type
sets (somewhat analogous to the use of grammar
perplexity in speech research).
Q1: What city is Massachusetts General Hospital located
in?
A1: It was conducted by a cooperative group of on-
cologists from Hoag, Massachusetts General Hospital
in Boston, Dartmouth College in New Hampshire, UC
San Diego Medical Center, McGill University in Montreal
and the University of Missouri in Columbia.
Q2: When was Nostradamus born?
A2: Mosley said followers of Nostradamus, who lived
from 1503 to 1566, have claimed ...
Figure 9: Sentences with Multiple Items of the
Same Type
However, Figure 9 shows the fundamental
problem behind answer confusability. Many sen-
tences contain multiple instances of the same
type, such as lists and ranges. In Q1, recognizing
that the question expects a city rather than a gen-
eral location is still not enough because several
cities are in the answer sentence. To achieve bet-
ter performance, Q/A systems need use features
that can more precisely target an answer.
7 Conclusion
In this paper we have presented four analyses of
question answering system performance involv-
ing: multiple answer occurence, relative score for
candidate ranking, bounds on term overlap perfor-
mance, and limitations of answer typing for short
answer extraction. We hope that both the results
and the tools we describe will be useful to others.
In general, we feel that analysis of good perfor-
mance is nearly as important as the performance
itself and that the analysis of bad performance can
be equally important.
References
E.J. Breck, J.D. Burger, L. Ferro, L. Hirschman, D. House,
M. Light, and I. Mani. 2000. How to Evaluate your
Question Answering System Every Day and Still Get
Real Work Done. In Proceedings of the Second Con-
ference on Language Resources and Evaluation (LREC-
2000).
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kos-
mala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy,
Z. Yang, S. Zeller, and L. Zorn. 2000. Reading Compre-
hension Programs in a Statistical-Language-Processing
Class. In ANLP/NAACL Workshop on Reading Com-
prehension Tests as Evaluation for Computer-Based Lan-
guage Understanding Systems.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep Read: A Reading Comprehension System. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
H.T. Ng, L.H. Teo, and J.L.P. Kwan. 2000. A Machine
Learning Approach to Answering Questions for Reading
Comprehension Tests. In Proceedings of EMNLP/VLC-
2000 at ACL-2000.
E. Riloff and M. Thelen. 2000. A Rule-based Question
Answering System for Reading Comprehension Tests.
In ANLP/NAACL Workshop on Reading Comprehension
Tests as Evaluation for Computer-Based Language Un-
derstanding Systems.
TREC-8 Proceedings. 1999. Proceedings of the Eighth
Text Retrieval Conference (TREC8). National Institute of
Standards and Technology, Special Publication 500-246,
Gaithersburg, MD.
TREC-9 Proceedings. 2000. Proceedings of the Ninth Text
Retrieval Conference (forthcoming). National Institute
of Standards and Technology, Special Publication 500-
XXX, Gaithersburg, MD.
W. Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandy-
berry, and M.P. Harper. 2000. A Question Answering
System Developed as a Project in a Natural Language
Processing Course. In ANLP/NAACL Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592?596,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Stance Classification using Dialogic Properties of Persuasion
Marilyn A. Walker, Pranav Anand, Robert Abbott and Ricky Grant
Baskin School of Engineering & Linguistics Department
University of California Santa Cruz
Santa Cruz, Ca. 95064, USA
maw,panand,abbott,rgrant@soe.ucsc.edu
Abstract
Public debate functions as a forum for both
expressing and forming opinions, an impor-
tant aspect of public life. We present results
for automatically classifying posts in online
debate as to the position, or STANCE that the
speaker takes on an issue, such as Pro or Con.
We show that representing the dialogic struc-
ture of the debates in terms of agreement rela-
tions between speakers, greatly improves per-
formance for stance classification, over mod-
els that operate on post content and parent-
post context alone.
1 Introduction
Public debate functions as a forum for both express-
ing and forming opinions. Three factors affect opin-
ion formation, e.g. the perlocutionary uptake of de-
bate arguments (Cialdini, 2000; Petty and Cacioppo,
1988; Petty et al, 1981). First, there is the ARGU-
MENT itself, i.e. the propositions discussed along
with the logical relations between them. Second is
the SOURCE of the argument (Chaiken, 1980), e.g.
the speaker?s expertise, or agreement relations be-
tween speakers. The third factor consists of proper-
ties of the AUDIENCE such as prior beliefs, social
identity, personality, and cognitive style (Davies,
1998). Perlocutionary uptake in debates primar-
ily occurs in the audience, who may be undecided,
while debaters typically express a particular position
or STANCE on an issue, e.g. Pro or Con, as in the
online debate dialogues in Figs. 1, 2, and 3.
Previous computational work on debate covers
three different debate settings: (1) congressional de-
Post Stance Utterance
P1 PRO I feel badly for your ignorance because although
there maybe a sliver of doubt that mankind may
have evolved from previous animals, there is no
doubt that the Earth and the cosmos have gone
through evolution and are continuing to do so
P2 CON As long as there are people who doubt evolu-
tion, both lay and acedamia, then evolution is in
doubt. And please don?t feel bad for me. I am
perfectly secure in my ?ignorance?.
P3 PRO By that measure, as long as organic chemistry,
physics and gravity are in doubt by both lay and
acedamia, then organic chemistry, physics and
gravity are in doubt. Gravity is a theory. Why
aren?t you giving it the same treatment you do
to evolution? Or is it because you are ignorant?
Angelic Falling anyone?
P4 CON I?m obviously ignorant. Look how many times
i?ve been given the title. ?Gravity is a theory.
Why aren?t you giving it the same treatment you
do to evolution?? Because it doesn?t carry the
same weight. ;P
Figure 1: All posts linked via rebuttal links. The topic
was ?Evolution?, with sides ?Yes, I Believe? vs. ?No, I
Dont Believe?.
bates (Thomas et al, 2006; Bansal et al, 2008;
Yessenalina et al, 2010; Balahur et al, 2009; Bur-
foot et al, 2011); (2) company-internal discussion
sites (Murakami and Raymond, 2010; Agrawal et
al., 2003); and (3) online social and political public
forums (Somasundaran and Wiebe, 2009; Somasun-
daran and Wiebe, 2010; Wang and Rose?, 2010; Bi-
ran and Rambow, 2011). Debates in online public
forums (e.g. Fig. 1) differ from debates in congress
and on company discussion sites in two ways.
First, the language is different. Online debaters
are highly involved, often using emotional and col-
orful language to make their points. These debates
are also personal, giving a strong sense of the indi-
592
vidual making the argument, and whether s/he fa-
vors emotive or factual modes of expression, e.g.
Let me answer.... NO! (P2 in Fig. 3). Other com-
mon features are sarcasm, e.g. I?m obviously igno-
rant. Look how many times i?ve been given the ti-
tle (P4 in Fig. 1), questioning another?s evidence or
assumptions: Yes there is always room for human
error, but is one accident that hasn?t happened yet
enough cause to get rid of a capital punishment? (P2
in Fig. 3), and insults: Or is it because you are ig-
norant? (P3 in Fig. 1). These properties may func-
tion to engage the audience and persuade them to
form a particular opinion, but they make computa-
tional analysis of such debates challenging, with the
best performance to date averaging 64% over several
topics (Somasundaran and Wiebe, 2010).
Post Stance Utterance
P1 Superman Batman is no match for superman. Not
only does he have SUPERnatural powers as
opposed to batman?s wit and gadgetry, but
his powers have increased in number over
the years. For example, when Superman?s
prowess was first documented in the comics
he did not have x-ray vision. It wasn?t until
his story was told on radio that he could see
through stuff. So no matter what new weapon
batman could obtain, Superman would add an-
other SUPERnatural weapon to foil the Caped
crusader.
P2 Batman Superman GAVE Batman a krytonite ring so
that Batman could take him down should he
need to. Superman did this because he knows
Batman is the only guy that could do it.
P3 Superman But, not being privy to private conversations
with S-man, you wouldn?t know that, being the
humble chap that he is, S-man allowed batman
the victory because he likes the bat and wanted
him to mantain some credibility. Honest.
P4 Batman Hmmm, this is confusing. Since we all know
that Supes doesn?t lie and yet at the time of
him being beaten by Batman he was under the
control of Poison Ivy and therefore could NOT
have LET Batman win on purpose. I have to
say that I am beginning to doubt you really are
friends with Supes at all.
Figure 2: All posts linked via rebuttal links. The topic
was ?Superman vs. Batman?
Second, the affordances of different online debate
sites provide differential support for dialogic rela-
tions between forum participants. For example, the
research of Somasundaran and Wiebe (2010), does
not explicitly model dialogue or author relations.
However debates in our corpus vary greatly by topic
on two dialogic factors: (1) the percent of posts that
are rebuttals to prior posts, and (2) the number of
Post Stance Utterance
P1 CON 69 people have been released from death row
since 1973 these people could have been killed
if there cases and evidence did not come up rong
also these people can have lost 20 years or more
to a false coviction. it is only a matter of time till
some one is killed yes u could say there doing
a good job now but it has been shown so many
times with humans that they will make the hu-
man error and cost an innocent person there life.
P2 PRO Yes there is always room for human error, but
is one accident that hasn?t happened yet enough
cause to get rid of a capital punishment? Let me
answer...NO! If you ban the death penalty crime
will skyrocket. It is an effective deterannce for
crime. The states that have strict death penalty
laws have less crime than states that don?t (Texas
vs. Michigan) Texas?s crime rate is lower than
Michigan and Texas has a higher population!!!!
Figure 3: Posts linked via rebuttal links. The topic was
?Capital Punishment?, and the argument was framed as
?Yes we should keep it? vs. ?No we should not?.
posts per author. The first 5 columns of Table 2
shows the variation in these dimensions by topic.
In this paper we show that information about di-
alogic relations between authors (SOURCE factors)
improves performance for STANCE classification,
when compared to models that only have access to
properties of the ARGUMENT. We model SOURCE
relations with a graph, and add this information to
classifiers operating on the text of a post. Sec. 2
describes the corpus and our approach. Our cor-
pus is publicly available, see (Walker et al, 2012).
We show in Sec. 3 that modeling source properties
improves performance when the debates are highly
dialogic. We leave a more detailed comparison to
previous work to Sec. 3 so that we can contrast pre-
vious work with our approach.
2 Experimental Method and Approach
Our corpus consists of two-sided debates from Con-
vinceme.net for 14 topics that range from play-
ful debates such as Superman vs. Batman (Fig. 2
to more heated political topics such as the Death
Penalty (Fig. 3. In total the corpus consists of 2902
two-sided debates (36,307 posts), totaling 3,080,874
words; the topic labelled debates which we use in
our experiments contain 575,818 words. On Con-
vinceme, a person starts a debate by posting a topic
or a question and providing sides such as for vs.
against. Debate participants can then post argu-
ments for one side or the other, essentially self-
593
labelling their post for stance. Convinceme pro-
vides three possible sources of dialogic structure,
SIDE, REBUTTAL LINKS and TEMPORAL CONTEXT.
Timestamps for posts are only available by day and
there are no agreement links. Here, we use the self-
labelled SIDE as the stance to be predicted.
Set/Factor Description
Basic Number of Characters in post, Average Word
Length, Unigrams, Bigrams
Sentiment LIWC counts and frequencies, Opinion De-
pendencies, LIWC Dependencies, negation
Argument Cue Words, Repeated Punctuation, Context,
POS-Generalized Dependencies, Quotes
Table 1: Feature Sets
We construct features from the posts, along with a
representation of the parent post as context, and use
those features in several base classifiers. As shown
in Table 1, we distinguish between basic features,
such as length of the post and the words and bi-
grams in the post, and features capturing sentiment
and subjectivity, including using the LIWC tool for
emotion labelling (Pennebaker et al, 2001) and de-
riving generalized dependency features using LIWC
categories, as well as some limited aspects of the
argument structure, such as cue words signalling
rhetorical relations between posts, POS generalized
dependencies, and a representation of the parent post
(context). Only rebuttal posts have a parent post, and
thus values for the context features.
50-10
Figure 4: Sample maxcut to ConvinceMe siding. Sym-
bols (circle, cross, square, triangles) indicate authors and
fill colors (white,black) indicate true side. Rebuttal links
are marked by black edges, same-author links by red;
weights are 50 and -10, respectively. Edges in the max-
cut are highlighted in yellow, and the nodes in each cut
set are bounded by the green dotted line.
We then construct a graph (V,E) representing the
dialogue structure, using the rebuttal links and au-
thor identifiers from the forums site. Each node V
of the graph is a post, and edges E indicate dialogic
relations of agreement and disagreement between
posts. We assume only that authors always agree
with themselves, and that rebuttal links indicate dis-
agreement. Agreement links based on the inference
that if A, B disagree with C they agree with each
other were not added to the graph.
Maxcut attempts to partition a graph into two
sides. Fig. 4 illustrates a sample result of applying
MaxCut. Edges connecting the partitions are said
to be cut, while those within partitions are not. The
goal is to maximize the sum of cut edge weights. By
making edge weights high we reward the algorithm
for cutting the edge, by making edge weights nega-
tive we penalize the algorithm for cutting the edge.
Rebuttal links were assigned a weight +100/(num-
ber of rebuttals). Same author links were assigned a
weight -60/(number of posts by author). If author A
rebutted author B at some point, then a weight of 50
was assigned to all edges connecting posts by author
A and posts by author B. If author B rebutted author
A as well, that 50 was increased to 100. We applied
the MaxCut partitioning algorithm to this graph, and
then we orient each of the components automati-
cally using a traditional supervised classifier. We
consider each component separately where compo-
nents are defined using the original (pre-MaxCut)
graph. For each pair of partition side p ? {P0, P1}
and classifier label l ? {L0, L1}, we compute a
score Sp,l by summing the margins of all nodes as-
signed to that partition and label. We then compute
and compare the score differences for each partition.
Dp = Sp,L1 ? Sp,L0 If DP0 < DP1 , then nodes in
partition P0 should be assigned label L0 and nodes
in P1 should be assigned label L1. Likewise, if
DP0 > DP1 , then nodes in partition P0 should be as-
signed label L1 and nodes in P1 should be assigned
label L0. If DP0 = DP1 , then we orient the compo-
nent with a coin flip.
3 Results and Discussion
Table 2 summarizes our results for the base classi-
fier (JRIP) compared to using MaxCut over the so-
cial network defined by author and rebuttal links.
We report results for experiments using all the fea-
594
Topic Characteristics MaxCut Algorithm JRIP Algorithm
Topic Posts Rebs P/A A> 1p MLE Acc F1 P R Acc F1 P R
Abortion 607 64% 2.73 42% 53% 82% 0.82 0.78 0.88 55% 0.55 0.52 0.59
Cats v. Dogs 162 40% 1.60 24% 53% 80% 0.78 0.80 0.76 61% 0.55 0.59 0.51
Climate Change 207 65% 2.92 41% 50% 64% 0.66 0.63 0.69 61% 0.62 0.60 0.63
Comm. v. Capitalism 214 62% 2.97 46% 55% 70% 0.67 0.66 0.68 53% 0.49 0.48 0.49
Death Penalty 331 60% 2.40 45% 56% 35% 0.31 0.29 0.34 55% 0.46 0.48 0.44
Evolution 818 66% 3.74 53% 58% 82% 0.78 0.78 0.79 56% 0.49 0.48 0.50
Existence Of God 852 76% 4.16 51% 56% 75% 0.73 0.70 0.76 52% 0.49 0.47 0.51
Firefox v. IE 233 38% 1.27 15% 79% 76% 0.47 0.44 0.49 72% 0.33 0.34 0.33
Gay Marriage 560 56% 2.01 28% 65% 84% 0.77 0.74 0.81 60% 0.43 0.43 0.44
Gun Control 135 59% 2.08 45% 63% 37% 0.24 0.21 0.27 53% 0.24 0.30 0.20
Healthcare 112 79% 3.11 53% 55% 73% 0.71 0.69 0.72 60% 0.49 0.56 0.44
Immigration 78 58% 1.95 33% 54% 33% 0.21 0.23 0.19 53% 0.39 0.48 0.33
Iphone v. Blackberry 25 44% 1.14 14% 67% 88% 0.80 0.86 0.75 71% 0.46 0.60 0.38
Israel v. Palestine 64 33% 3.37 53% 58% 85% 0.82 0.79 0.85 49% 0.48 0.42 0.56
Mac v. PC 126 37% 1.85 24% 52% 19% 0.18 0.17 0.18 46% 0.46 0.45 0.48
Marijuana legalization 229 45% 1.52 25% 71% 73% 0.56 0.52 0.60 63% 0.34 0.35 0.34
Star Wars vs. LOTR 102 44% 1.38 26% 53% 63% 0.62 0.60 0.65 63% 0.62 0.60 0.65
Superman v. Batman 146 30% 1.39 20% 54% 50% 0.40 0.44 0.37 56% 0.47 0.52 0.43
Table 2: Results. KEY: Number of posts on the topic (Posts). Percent of Posts linked by Rebuttal links (Rebs). Posts
per author (P/A). Authors with more than one post (A > 1P). Majority Class Baseline (MLE).
tures with ?2 feature selection; we use JRIP as the
base classifier because margins are used by the auto-
matic MaxCut graph orientation algorithm. Exper-
iments with different learners (NB, SVM) did not
yield significant differences from JRIP. The results
show that, in general, representing dialogic infor-
mation in terms of a network of relations between
posts yields very large improvements. In the few
topics where performance is worse (Death Penalty,
Gun Control, Mac vs. PC, Superman vs. Batman),
the MaxCut graph gets oriented to the stance sides
the wrong way, so that the cut actually groups the
posts correctly into sides, but then assigns them to
the wrong side. For Maxcut, as expected, there are
significant correlations between the % of Rebuttals
in a debate and Precision (R = .16 ) and Recall (R=
.22), as well as between Posts/Author and Precision
(R = .25) and Recall (R = .43). This clearly indi-
cates that the degree of dialogic behavior (the graph
topology) has a strong influence on results per topic.
These results would be even stronger if all MaxCut
graphs were oriented correctly.
(Somasundaran and Wiebe, 2010) present an un-
supervised approach using ICA to stance classifica-
tion, showing that identifying argumentation struc-
ture improves performance, with a best performance
averaging 64% accuracy over all topics, but as high
as 70% for some topics. Other research classifies
the speaker?s side in a corpus of congressional floor
debates (Thomas et al, 2006; Bansal et al, 2008;
Balahur et al, 2009; Burfoot et al, 2011). Thomas
et al(2006) achieved accuracies of 71.3% by using
speaker agreement information in the graph-based
MinCut/Maxflow algorithm, as compared to accura-
cies around 70% via an an SVM classifier operating
on content alone. The best performance to date on
this corpus achieves accuracies around 82% for dif-
ferent graph-based approaches as compared to 76%
accuracy for content only classification (Burfoot et
al., 2011). Other work applies MaxCut to the reply
structure of company discussion forums, showing
that rules for identifying agreement (Murakami and
Raymond, 2010), defined on the textual content of
the post yield performance improvements over using
reply structures alone (Malouf and Mullen, 2008;
Agrawal et al, 2003)
Our results are not strictly comparable since we
use a different corpus with different properties, but
to our knowledge this is the first application of Max-
Cut to stance classification that shows large perfor-
mance improvements from modeling dialogic rela-
tions. In future work, we plan to explore whether
deeper linguistic features can yield large improve-
ments in both the base classifier and in MaxCut re-
sults, and to explore better ways of automatically
orienting the MaxCut graph to stance side. We also
hope to develop much better context features and to
make even more use of dialogue structure.
595
References
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
A. Balahur, Z. Kozareva, and A. Montoyo. 2009. Deter-
mining the polarity and source of opinions expressed
in political debates. Computational Linguistics and
Intelligent Text Processing, pages 468?480.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13?16.
O. Biran and O. Rambow. 2011. Identifying justifica-
tions in written dialogs. In 2011 Fifth IEEE Inter-
national Conference on Semantic Computing (ICSC),
pages 162?168. IEEE.
C. Burfoot, S. Bird, and T. Baldwin. 2011. Collective
classification of congressional floor-debate transcripts.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1506?1515. As-
sociation for Computational Linguistics.
S. Chaiken. 1980. Heuristic versus systematic informa-
tion processing and the use of source versus message
cues in persuasion. Journal of personality and social
psychology, 39(5):752.
Robert B. Cialdini. 2000. Influence: Science and Prac-
tice (4th Edition). Allyn & Bacon.
M.F. Davies. 1998. Dogmatism and belief formation:
Output interference in the processing of supporting
and contradictory cognitions. Journal of personality
and social psychology, 75(2):456.
R. Malouf and T. Mullen. 2008. Taking sides: User clas-
sification for informal online political discourse. In-
ternet Research, 18(2):177?190.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
Richard E. Petty and John T. Cacioppo. 1988. The ef-
fects of involvement on responses to argument quan-
tity and quality: Central and peripheral routes to per-
suasion. Journal of Personality and Social Psychol-
ogy, 46(1):69?81.
R.E. Petty, J.T. Cacioppo, and R. Goldman. 1981. Per-
sonal involvement as a determinant of argument-based
persuasion. Journal of Personality and Social Psy-
chology, 41(5):847.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proceedings of
the 2006 conference on empirical methods in natural
language processing, pages 327?335. Association for
Computational Linguistics.
M. Walker, P. Anand, J. Fox Tree, R. Abbott, and J. King.
2012. A corpus for research on deliberation and
debate. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
LREC 2012, Istanbul, Turkey, May 23-25, 2012.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1046?1056. Association for Computational
Linguistics.
596
Extracting Contextual Evaluativity
Kevin Reschke
University of California, Santa Cruz
kreschke@ucsc.edu
Pranav Anand
University of California, Santa Cruz
panand@ucsc.edu
Abstract
Recent work on evaluativity or sentiment in the language sciences has focused on the contri-
butions that lexical items provide. In this paper, we discuss contextual evaluativity, stance that is
inferred from lexical meaning and pragmatic environments. Focusing on assessor-grounding claims
like We liked him because he so clearly disliked Margaret Thatcher, we build a corpus and construct a
system employing compositional principles of evaluativity calculation to derive that we dislikes Mar-
garet Thatcher. The resulting system has an F-score of 0.90 on our dataset, outperforming reasonable
baselines, and indicating the viability of inferencing in the evaluative domain.
1 Contextual Evaluativity
A central aim of contemporary research on sentiment or evaluative language is the extraction of eval-
uative triples: ?evaluator, target, evaluation?. To date, both formal (e.g., Martin and White 2005, Potts
2005) and computational approaches (e.g., Pang and Lee 2008) have focused on how such triples are
lexically encoded (e.g., the negative affect of scoundrel or dislike). While lexical properties are a key
source of evaluative information, word-based considerations alone can miss pragmatic inferences result-
ing from context. (1), for example, communicates that the referent of we bears not only positive stance
towards the referent of him, but also negative stance towards Margaret Thatcher:
(1) We liked him because he so clearly disliked Margaret Thatcher.
LEXICAL EVALUATIVITY: ?we, him, +?; ?he, M.T., -?
CONTEXTUAL EVALUATIVITY: ?we, M.T., -?
This paper argues for a compositional approach to contextual evaluativity similar to the compositional
methods adopted for lexical evaluativity in Moilanen and Pulman (2007) and Nasukawa and Yi (2003).
At the the heart of the approach is the treatment of verbal predicates (dislike in (1)) as evaluativity
functors which relate argument/entity-level evaluativity to event-level evaluativity.
As discussed in ?2, the utitlity of such a model surfaces in cases where the event-level evaluativity
is known from context, and thus new information about the contextual evaluativity of the event partic-
ipants (e.g. Margaret Thatcher) can be inferred. Consequently, the empirical focus of this paper is on
structures like (1), where the second clause provides grounds for the sentiment encoded in the first, and
hence has a predictable event-level evaluation from the first clause?s evaluator. In ?3 we describe the
collection and annotation of a corpus of such assessment-grounding configurations from large-scale web
data. This annotated corpus serves as a test bed for experimental evaluation of various implementations
of the proposed compositional approach. The results of these experiments (?4) strongly support a com-
positional approach to contextual evaluativity inference. A simple compositional algorithm based on
a small, manually created evaluativity functor lexicon demonstrated significantly better precision than
non-compositional baselines. Moreover, a method for automatically expanding coverage to novel predi-
cates based on similarity with the manually created lexicon is shown to increase recall dramatically with
modest reduction in precision.
2 A Framework For Inferring Contextual Polarity
Evaluativity is concerned with determining private states (e.g., judgment or emotion) that a particular
evaluator bears towards a target entity, event, or proposition. This may be represented as a three place
370
Table 1: Evaluativity functors for verbs of having, withholding, disliking, and liking
x y Ehave Elack Ewthhld Edprv Espr Edislike Elike
+ + + - - - # - +
+ - - + + # + + -
- + - + + + # - +
- - + - - # - + -
x have/lack y a withhold/deprive/spare x of y x dislike/like y
relation, R ? De ?D??DE , where ? is of variable type and E is the type of evaluative stance, assumed
here to be binary. Lexical approaches to evaluativity (see Pang and Lee 2008 for a review) have focused
on those relations that are determinable from word-internal meaning alone. For example, describing
an event e as coddling gives rise to two triples: ?AGENT(e), PATIENT(e),+? and ?SPEAKER, e,??.1
These lexical inferences then become part of the feature set for classifying phrasal stance (e.g., the
author?s overall evaluativity in a sentence). A contrasting line of research (Moilanen and Pulman 2007,
Nasukawa and Yi 2003) analyzes phrasal stance as a compositional product of the polarities toward event
participants. For example, the evaluative polarity of the speaker toward the event in (2a) is positively
correlated with the polarity toward the subject, and negatively so in (2b).
(2) a. My {ally, enemy} was deprived shelter.
b. My {ally, enemy} was spared a dangerous mission.
Compositional proposals rely on mapping each n-ary predicate P an n-ary evaluativity functor EP :
DEn ? DE . Anand and Reschke (2011) argue that evaluativity functors largely group into classes,
depending on whether the predicates in question entail final states of possession and/or affectedness. For
example, the functors for predicates of withholding, including deprive and spare, are partial cases of the
functor for lack (partiality reflects lexical idiosyncracies about e.g., deprivation and positive objects), as
shown in Table 1.
While compositional systems are designed to compute phrasal stances bottom-up, their calculi straight-
forwardly allow inference to participant polarities as well, assuming knowledge of the event polarity and
all but one participant. Consider the sentence He disliked Margaret Thatcher. By the evaluativity con-
ditions in Table 1, Edislike is positive iff the evaluator has negative evaluation of Thatcher. Thus, given
knowledge of the event polarity, we can infer the evaluator?s stance with respect to Thatcher. In (1), this
information is provided by the preceding assessing clause (+, from Elike ). As the second clause serves
as grounds for the assessment in the first clause, the event described in the second clause is predictably
also assessed as + by the evaluator we. In our experiments we exploited this construction in particular,
but the general procedure does not require it (thus, for example, evaluative adverbs such as fortunately
and regrettably could provide an additional construction type). This procedure is sketched for (1) below:
(3) We likedelike him because he so clearly dislikededislike Margaret Thatcher.
LEXICAL EVALUATIVITY: ?we, him, +?; ?he, M.T., -?
PRAGMATIC INFERENCE: ?we, edislike, +? (edislike justifies ?we, him, +?)
COMPOSITIONAL INFERENCE: Edislike(+, y) = + iff y = +
therefore, y is regarded as +, or ?we, M.T., -?
Note that for this application, we may simplify the compositional picture and treat functors as either
preservers or reversers of the polarity of the object of interest, as is done in Moilanen and Pulman (2007)
and Nasukawa and Yi (2003): preservers (such as verbs of liking) match the object polarity with the
event polarity, and reverses negate it.
When the assessing clause evaluator is not affiliated with the speaker, this procedure can produce
markedly different results from lexical markers (which often show speaker evaluativity). Thus, in (4),
the speaker?s assessment of Obama?s cuts (indicated by the lexical much-needed) stands in sharp contrast
with NASA?s (determined by inference):
1Here, we simplify regarding potential evaluators outside of the speaker.
371
(4) NASA got angry at Obama because he imposed some much-needed cuts.
LEXICAL EVALUATIVITY: ?NASA, Obama, -?; ?SPEAKER, some much needed cuts, +?
CONTEXTUAL EVALUATIVITY: ?NASA, some much needed cuts, -?
The assessment-grounding configuration in (1) and (4) is highly productive. Behaviorally, implicit
causality predicates (including predicates of assessment, as well as praise and scolding) are frequently
understood by experimental subjects as describing an event involving the assessment target, especially
when followed by because (Garvey and Carmazza, 1974; Koornneef and van Berkum, 2006). In addition,
Somasundaran and Weibe (2009) exploited a similar construction to gather reasons for people?s product
assessments from online reviews. These together suggest that such constructions could be simultaneously
high-precision sources for evaluativity inference and easily obtainable from large corpora.
3 Data Gathering and Annotation
We developed a corpus of assessment-grounding excerpts from documents across the web to evaluate
the potential of the framework in ?2. 73 positive and 120 negative assessment predicates (like, adore,
hate, loathe, etc.) were selected from the MPQA subjectivity lexicon (Wilson et al, 2005). These were
expanded accross inflectional variants to produce 826 assessment templates, half with explicit because,
half without (e.g. terrified by X because he). These templates were filled with personal pronouns and
the names of 26 prominent political figures and issued as websearch queries to the Yahoo! Search
API.2 A total of 440,000 webdocument results were downloaded and processed using an 1152 core Sun
Microsystems blade cluster. The relevant sentences from each document were extracted, and those under
80 characters in length were parsed using the Stanford Dependency Parser.3
This produced 60,000 parsed assessment-grounding sentences, 6,000 of which (excluding duplicates)
passed the additional criterion that the grounding clause should contain a verb with a direct object. This
restriction ensured that each item in our corpus had a target for contextual polarity inference. An addi-
tional 3,300 cases were excluded because the target in the grounding clause shared possible coreference
with the experiencer (subject) of the assessment clause. We avoided these coreferring cases because,
from the perspective of a potential application, inferences about an experiencer?s stance towards himself
are less valuable than inferences about his stance towards others. Finally, the list was manually short-
ened to include only those sentences marked as assessment-grounding configurations according to two
annotators (? = 0.82); the classification task of whether this pragmatic connection occurs is beyond the
scope of this paper. 57% of the data was removed in this pass, 14% from tokens with because and 43%
from tokens without. Implicit causality verbs not followed by because have been shown experimentally
to give rise to a much weaker preference for justification (Au, 1986), and this is confirmed in our corpus
search. The result of this procedure was a final corpus size of 1,160.
The corpus was annotated for inferred contextual polarity. One of the authors and another annotator
coded sentences for evaluator stance toward the object (+,-, unknown); agreement was high: ? = 0.90.
The 48 unresolved cases were adjudicated by a third annotator. 27 cases were uniformly judged un-
known, involving predicates of change, disclosure (reveal, expose), and understanding (know). These
were removed from the corpus, leaving 1,133 sentences for training and testing.
4 System and Experimental Results
Restricting ourselves to the assessment-grounding configuration discussed above, we treat contextual
polarity inference as a binary classification problem with two inputs: the INPUT EVENT event-level
polarity (derived from the assessment clause) and the main verb of the grounding clause (henceforth
FUNCTOR VERB). The goal of the classifier is to correctly predict the polarity of the target NP (direct
object to the functor verb) given these inputs.
2http://developer.yahoo.com/search/
3http://nlp.stanford.edu/software/lex-parser.shtml
372
Table 2: Examples of verbs marked as
preserver/reverser and their sources
EXAMPLE CLASS SOURCE
reward preserver MPQA subj. lex.
hamper reverser MPQA subj. lex.
tutor preserver (benefit) FrameNet
batter reverser (injury) FrameNet
Table 3: Performance of systems and baselines
for contextual evaluativity classification
SYSTEM PREC. RECALL F-SCORE
B-Functor 0.39 0.24 0.30
B-Input 0.69 1.0 0.82
B-MLE 0.75 1.0 0.86
SYS 0.88 0.57 0.69
SYS-MPQA 0.88 0.24 0.38
SYS-Frame 0.89 0.41 0.56
SYS+Maj 0.82 1.0 0.90
SYS+Sim 0.84 0.97 0.90
As mentioned in ?2, we may categorize the functor verbs in our lexicon into preservers and reversers.
Two sources populate our lexicon. First, positively subjective verbs from the MPQA subjectivity lexicon
were marked as preservers and negatively subjective verbs were marked as reversers (1249 verbs total).
For example, Edislike is a reverser. Second, 487 verbs were culled from FrameNet (Ruppenhofer et al,
2005) based on their membership in six entailment classes: verbs of injury, destruction, lacking, benefit,
creation, and having. Class membership was determined by identifying 124 FrameNet frames aligning
with one or more classes, then manually selecting from these frames verbs whose class membership
was unambiguous. Verbs of benefit, creation, and having were marked as preservers. Verbs of injury,
destruction, and lacking were marked as reversers (Table 2). Our system (SYS) classifies objects in
context as follows: If the functor verb is a preserver, the target NP is assigned the same polarity as the
input event polarity. If the functor verb is a reverser, the target NP is assigned the opposite of the input
event polarity. This procedure is modulated by the presence of negation, as detected by a neg relation in
the dependency parse. Under negation, a preserver acts like a reverser, and vice versa.
We tested the performance of this system (SYS) on our annotated corpus against three baselines.
The first baseline (B-Functor) attempts to determine the importance of the input event to the calculation.
It thus ignores the preceding context, and attempts to classify the target object from the functor verb
directly, based on the verb?s polarity in the MPQA subjectivity lexicon. It has poor precision and recall,4
reflecting both the importance of the assessment context for object polarity and the fact that the functor
verbs are often not lexically sentiment bearing (e.g., predicates of possession). The second baseline
(B-Input), conversely, ignores the functor verb and uses the input event polarity as listed in the MPQA
lexicon (modulo negation) for object classification. The purpose of this baseline is to approximate a
classifier that predicts target polarity solely from the global/contextual polarity of the preceding clause.
This has sharply increased precision, indicating contextual information?s importance. The third baseline
(B-MLE) picked the majority object class (+), and had the highest precision, indicating the general bias
in our corpus for positive objects. Table 3 shows the performance (precision vs. recall) of our system
compared to the three baselines. Its precision is significantly higher, but its F-score is limited by the lower
coverage of our manually constructed lexicon. SYS-MPQA and SYS-Frame show the performance of
the system when the functor lexicon is limited to the MPQA and Framenet predicates, respectively. Both
are high precision sources of functor prediction, and pick out somewhat distinct predicates (given the
recall gain of combining them). SYS+Maj and SYS+Sim are attempts to handle the low recall of SYS
caused by functor verbs in the test data which aren?t in the system?s lexicon. SYS+Maj simply assigns
these out-of-vocabulary verbs to the majority class: preservers. SYS+Sim classifies out-of-vocabulary
verbs as preservers or reversers based on their relative similarity to the known preservers and reversers
selected from FrameNet ? an unknown verb is categorized as a preserver if its average similarity to
preservers is greater than its average similarity to reversers. Similarity was determined according to the
Jiang-Conrath distance measure (Jiang and Conrath, 1997), which based on links in WordNet (Fellbaum,
1998). (Note: this process cannot occur for words not found in WordNet ? e.g. misspellings ? hence the
4Low recall occurs when items are left unclassified due to out-of-vocabulary functor verbs. Low precision occurs when a +
item is classified as ? or vice versa.
373
less than perfect recall). These two systems outperform all baselines, but have indistinguishable F-scores
(if misspellings are excluded, SYS+Sim has a Recall of 0.99 and F-score of 0.91).
Most of the precision errors incurred by our systems were syntactic: incorrect parsing, incorrect
extraction of the object, or faulty negation handling (e.g., negative quantifiers or verbs). 26% of errors
are due to word-sense disambiguation. The verbs spoil and own each have positive and negative uses
(own can mean defeat), but only one sense was registered in the lexicon, leading to errors. The lion?s
share of these errors (22%) were due to the use of hate and similar expressions to convey jealousy (e.g.
I was mad at him because he had both Boardwalk and Park Place). In these scenarios, although the
assessment is negative, the event-level polarity of the grounding clause event type is positive (because it
is desired), a fact which our current system cannot handle. One way forward would be to apply WSD
techniques to distinguish jealous from non-jealous uses of predicates of dislike.
5 Conclusion
We have described a system for the extraction of what we termed contextual evaluativity ? evaluations
of objects that arise from the understanding of pragmatic inferences. This system, once we incorporate
procedures to automatically infer evaluativity functor class, significantly outperforms reasonable base-
lines on a corpus of assessor-grounding extracts from web documents. The system operates by running
a compositional approach to phrasal evaluativity in reverse, and is thus an instance of the potential com-
putational value of such treatments of evaluativity.
References
Anand, P. and K. Reschke (2011). Verb classes as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. to appear.
Au, T. K. (1986). A verb is worth a thousand words. Journal of Memory and Language 25, 104?122.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Garvey, C. and A. Carmazza (1974). Implict causality in verbs. Linguistic Inquiry 5, 459?484.
Jiang, J. and C. Conrath (1997). Semantic similarity based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research in Computational Linguistics.
Koornneef, A. W. and J. J. A. van Berkum (2006). On the use of verb-based implicit causality in sentence
comprehension. Journal of Memory and Language 54, 445?465.
Martin, J. R. and P. R. R. White (2005). Language of Evaluation: Appraisal in English. Palgrave
Macmillan.
Moilanen, K. and S. Pulman (2007). Sentiment composition. In Proceedings of RANLP 2007.
Nasukawa, T. and J. Yi (2003). Sentiment analysis: Capturing favorability using natural language pro-
cessing. In Proceedings of the 2nd international conference on Knowledge capture.
Pang, B. and L. Lee (2008). Opinion mining and sentiment analysis. Foundations and Trends in Infor-
mation Retrieval 2(1-2), 1?135.
Potts, C. (2005). The Logic of Conventional Implicature. Oxford University Press.
Ruppenhofer, J., M. Ellsworth, M. R. L. Petruck, and C. R. Johnson (2005). Framenet ii: Extended
theory and practice. Technical report, ICSI Technical Report.
Somasundaran, S. and J. Weibe (2009). Recognizing stances in online debates. In Proceedings of ACL-
47, pp. 226?234.
Wilson, T., J. Weibe, and P. Hoffman (2005). Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of EMNLP-05.
374
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 2?11,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
How can you say such things?!?:
Recognizing Disagreement in Informal Political Argument
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree,
Robeson Bowmani, and Joseph King
University of California Santa Cruz
abbott|maw@soe.ucsc.edu,panand|foxtree@ucsc.edu
Abstract
The recent proliferation of political and so-
cial forums has given rise to a wealth of freely
accessible naturalistic arguments. People can
?talk? to anyone they want, at any time, in
any location, about any topic. Here we use
a Mechanical Turk annotated corpus of forum
discussions as a gold standard for the recog-
nition of disagreement in online ideological
forums. We analyze the utility of meta-post
features, contextual features, dependency fea-
tures and word-based features for signaling
the disagreement relation. We show that us-
ing contextual and dialogic features we can
achieve accuracies up to 68% as compared to
a unigram baseline of 63%.
1 Introduction
The recent proliferation of political and social fo-
rums has given rise to a wealth of freely accessible
naturalistic arguments. People can ?talk? to anyone
they want, at any time, in any location, about any
topic. Their conversations range from current polit-
ical topics such as national health care to religious
questions such as the meaning of biblical passages.
See Figure 1. We aim to automatically derive rep-
resentations of the discourse structure of such argu-
ments and to gain a deeper theoretical and empirical
understanding of the linguistic reflexes of perlocu-
tionary acts such as persuasion (Austin, 1965).
The study of the structure of argumentative com-
munication has a long lineage in psychology (Cial-
dini, 2000) and rhetoric (Hunter, 1987), but the his-
torical lack of a large corpus of naturalistic exam-
Topic Q-R: Post
Evolution Q: How can you say such things? The Bible says that
God CREATED over and OVER and OVER again! And
you reject that and say that everything came about by
evolution? If you reject the literal account of the Cre-
ation in Genesis, you are saying that God is a liar! If you
cannot trust God?s Word from the first verse, how can
you know that the rest of it can be trusted?
R: It?s not a literal account unless you interpret it that
way.
Gay
mar-
riage
Q: Gavin Newsom- I expected more from him when I
supported him in the 2003 election. He showed himself
as a family-man/Catholic, but he ended up being the ex-
act oppisate, supporting abortion, and giving homosexu-
als marriage licenses. I love San Francisco, but I hate the
people. Sometimes, the people make me want to move
to Sacramento or DC to fix things up.
R: And what is wrong with giving homosexuals the right
to settle down with the person they love? What is it to
you if a few limp-wrists get married in San Francisco?
Homosexuals are people, too, who take out their garbage,
pay their taxes, go to work, take care of their dogs, and
what they do in their bedroom is none of your business.
Abortion Q: Equality is not defined by you or me. It is defined by
the Creator who created men.
R: Actually I think it is defined by the creator who cre-
ated all women. But in reality your opinion is gibberish.
Equality is, like every other word, defined by the people
who use the language. Currently it means ?the same?.
People aren?t equal because they are not all the same.
Any attempt to argue otherwise is a display of gross stu-
pidity.
Figure 1: Sample Quote/Response Pairs
ples has limited empirical work to a handful of gen-
res (e.g., editorials or simulated negotiations). Ar-
gumentation is above all tactical. Thus being able
to effectively model it would afford us a glimpse
of pragmatics beyond the conversational turn. More
practically, an increasing portion of information and
opinion exchange online occurs in natural dialogue,
in forums, in webpage comments, and in the back
2
and forth of short messages (e.g., Facebook status
updates, tweets, etc.) Effective models of argumen-
tative discourse thus have clear applications in auto-
matic summarization, information retrieval, or pre-
dicting real-world events such as how well a new
product is being received or the outcome of a popu-
lar vote on a topic (Bollen et al, 2011).
In this paper, we focus on an important initial task
for the recognition of argumentative structure: auto-
matic identification of agreement and disagreement.
We introduce the ARGUE corpus, an annotated col-
lection of 109,553 forum posts (11,216 discussion
threads) from the debate website 4forums.com. On
4forums, a person starts a discussion by posting a
topic or a question in a particular category, such as
society, politics, or religion. Some example topics
can be seen in Table 1. Forum participants can then
post their opinions, choosing whether to respond di-
rectly to a previous post or to the top level topic (start
a new thread). These discussions are essentially di-
alogic; however the affordances of the forum such
as asynchrony, and the ability to start a new thread
rather than continue an existing one, leads to dia-
logic structures that are different than other multi-
party informal conversations (Fox Tree, 2010). An
additional source of dialogic structure in these dis-
cussions, above and beyond the thread structure, is
the use of the quote mechanism, in which partici-
pants often break a previous post down into the com-
ponents of its argument and respond to each compo-
nent in turn. Many posts include quotations of previ-
ous posts. Because we hypothesize that these posts
are more targeted at a particular proposition that the
poster wants to comment on, than posts and replies
in general, we focus here on understanding the rela-
tionship between a quoted text and a response, and
the linguistic reflexes of those relationships. Exam-
ples of quote/response pairs for several of our topics
are provided in Figure 1.
The most similar work to our own is that of
Wang & Rose (2010) who analyzed Usenet fo-
rum quote/response structures. This work did not
distinguish agreement vs. disagreement across
quote/response pairs. Rather they show that they can
use a variant of LSA to improve accuracy for identi-
fying a parent post, given a response post, with 70%
accuracy. Other similar work uses Congressional
debate transcripts or blogs or other social media to
develop methods for distinguishing agreement from
disagreement or to distinguish rebuttals from out-of-
context posts (Thomas et al, 2006; Bansal et al,
2008; Awadallah et al, 2010; Walker et al, ; Bur-
foot, 2008; Mishne and Glance, 2006; Popescu and
Pennacchiotti, 2010). These methods are directly
applicable, but the genre of the language is so dif-
ferent from our informal forums that the results are
not directly comparable. Work by Somasundaran &
Wiebe (2009, 2010) has examined debate websites
and focused on automatically determining the stance
of a debate participant with respect to a particular is-
sue. This work has treated each post as a text to be
classified in terms of stance, for a particular topic,
and shown that discourse relations such as conces-
sions and the identification of argumentation triggers
improves performance . Their work, along with oth-
ers, also indicates that for such tasks it is difficult to
beat a unigram baseline (Pang and Lee, 2008). Other
work has focused on the social network structure
of online forums (Murakami and Raymond, 2010;
Agrawal et al, 2003). However, Agarwal?s work as-
sumed that adjacent posts always disagree, and did
not use any of the information in the text. Murakami
& Raymond (2010) show that simple rules defined
on the textual content of the post can improve over
Agarwal?s results.
Section 2 discusses our corpus in more detail, de-
scribes how we collected annotations using Mechan-
ical Turk, and presents results of a corpus analysis
of the use of particular discourse cues. Section 3 de-
scribes how we set up classification experiments for
distinguishing agreement from disagreement, and
Section 4 presents our results for agreement classifi-
cation. We also characterize the linguistic reflexes of
this relation. We analyze the utility of meta-post fea-
tures, contextual features, dependency features and
word-based features for signaling the disagreement
relation. We show that using contextual and dia-
logic features we can achieve accuracies up to 68%
as compared to a unigram baseline of 63%.
2 Data and Corpus Analysis
Table 1 provides an overview of some of the charac-
teristics of our corpus by topic. Figure 2 shows the
wording of the survey questions that we posted for
each quote/response as Mechanical Turk hits.
3
Topic Discs Posts NumA P/A A>1P PL Agree Sarcasm Emote Attack Nasty
evolution 872 10292 580 17.74 76% 576 10% 6% 16% 13% 9%
gun control 825 7968 411 19.39 66% 521 11% 8% 21% 16% 12%
abortion 564 7354 574 12.81 69 % 454 9% 6% 31% 16% 12%
gay marriage 305 3586 342 10.49 69% 522 13% 9% 23% 12% 8%
existence of God 105 1581 258 6.13 66% 569 11 % 7% 26% 14% 10%
healthcare 81 702 112 6.27 64% 522 14% 10% 34% 17% 17%
communism vs. capitalism 38 585 110 5.32 59% 393 23% 8% 15% 8% 0%
death penalty 25 500 138 3.62 62% 466 25% 5% 5% 5% 5%
climate change 40 361 116 3.11 55% 375 20% 9% 17% 26% 17%
marijuana legalization 13 160 72 2.22 38% 473 5% 2% 20% 5% 5%
Table 1: Characteristics of Different Topics. KEY: Number of discussions and posts on the topic (Discs, Posts).
Number of authors (NumA). Posts per author (P/A). Authors with more than one post (A > 1P). Median post Length
in Characters (PL). The remainder of the columns are the annotations shown in Figure 2. Percentage of posts that
agree (Agree%), use sarcasm (Sarcasm%), are emotional (Emote), attack the previous poster (Attack), and are
nasty (Nasty). The scalar values are threshholded at -1,1.
Our corpus is derived from a debate oriented in-
ternet forum called 4forums.com. It is a typical in-
ternet forum built on the vBulletin software. People
initiate discussions (threads) and respond to others?
posts. Each thread has a tree-like dialogue structure.
Each post has author information and a timestamp
with minute resolution. Many posts include quota-
tions of previous posts. For this work we chose to
focus on quotations because they establish a clear re-
lationship between the quoted text and the response.
Our corpus consists of 11,216 discussions and
109,553 posts by 2764 authors. We hand annotated
discussions for topic from a set of previously identi-
fied contentious political and social issues. The web-
site is tailored to a US audience and our topics are
somewhat US centric. Table 1 describes features of
our topics in order of decreasing discussion count.
When restricted to these topics, the corpus consists
of 2868 discussions, 33,089 posts, and 1302 authors.
Many posts include quotations. Overall 60,382
posts contain one or more quotation. Within our
topics of interest, nearly 20,000 posts contain quota-
tions. We defined a quote-response pair (Q-R pair)
where the response was the portion of the respond-
ing post directly following a quotation but preceding
any additional quotations.
We selected 10,003 Q-R pairs from the topics
of interest for a Mechanical Turk annotation task.
These were biased by cue word to ensure adequate
data for discourse marker analysis (See Section 2.1.
For this task we showed annotators seven Q-R pairs
and asked them to judge Agreement/Disagreement
and a set of other measures as shown in Figure 2.
Most of our measures were scalar; we chose to do
this because previous work on estimating the rela-
tionship between MTurk annotations and expert an-
notations suggest that taking the means of scalar
annotations could be a good way to reduce noise
in MTurk annotations (Snow et al, 2008). For all
of the measures annotated, the Turkers were not
given additional definitions of their meaning. For
example, we let Turkers to use their native intu-
itions about what it means for a post to be sarcas-
tic, since previous work suggests that non-specialists
tend to collapse all forms of verbal irony under the
term sarcastic (Bryant and Fox Tree, 2002). We
did not ask Turkers to distinguish between sarcasm
and other forms of verbal irony such as hyperbole,
understatement, rhetorical questions and jocularity
(Gibbs, 2000).
Agreement was a scalar judgment on an 11 point
scale [-5,5] implemented with a slider. The anno-
tators were also able to signal uncertainty with an
CAN?T TELL option. Each of the pairs was anno-
tated by 5-7 annotators. We showed the first 155
characters of each quote and each response. We also
provided a SHOW MORE button which expanded the
post to its full length. After annotation, we removed
a number of Q-R pairs in cases where a clear link
between the quote and a previous post could not be
established, e.g. the source quoted was not another
post, but the NY Times. This left us with 8,242 Q-R
pairs for our final analysis. Resampling to a natural
distribution left us with 2,847 pairs which we used
to build our machine learning test set. We used the
remaining annotated and unannotated pairs for de-
4
velopment.
Type ? Survey Question
S 0.62 Agree/Disagree: Does the respondent agree or dis-
agree with the prior post?
S 0.32 Fact/Emotion: Is the respondent attempting to
make a fact based argument or appealing to feel-
ings and emotions?
S 0.42 Attack/Insult: Is the respondent being support-
ive/respectful or are they attacking/insulting in
their writing?
B 0.22 Sarcasm: Is the respondent using sarcasm?
S 0.46 Nice/Nasty: Is the respondent attempting to be nice
or is their attitude fairly nasty?
Figure 2: Mechanical Turk Annotations (Binary = B and
Scalar = S) and level of agreement as Krippendorff?s ?.
Figure 3 provides examples from the end
points and means of the annotations for three
of the questions, Respect/Insult, Sarcasm, and
Fact/Emotion. Nice/Nasty and Respect/Insult are
strongly correlated by worker annotations r(54003)
= 0.84, p < 2.2e-16 and both weakly corre-
lated with Agree/Disagree ratings (r(54003) = 0.32
and r(54003)=0.36, respectively; p < 2.2e-16)
and Fact/Emotion ratings (r(54003) = 0.32 and
r(54003)=0.31, respectively; p < 2.2e-16), while
Agree/Disagree and Fact/Emotion ratings show the
smallest correlation, r(54003)=0.11, p < 2.2e-16.
For the linguistic marker correlations discussed be-
low we averaged scores across annotators, a process
which sharpened correlations (e.g., Respect/Insult
means correlate with Agree/Disagree means more
strongly (r(5393) = 0.51) as well as Nice/Nasty
means (r(5393) = 0.91; Agree/Disagree is far less
correlated with Fact/Emotion (r(5393) = 0.07). In-
terannotator agreement was computed using Krip-
pendorff?s ? (due to the variability in number of an-
notators that completed each hit), assuming an ordi-
nal scale for all measures except sarcasm; see Fig-
ure 2. The low agreement for Sarcasm accords with
native intuition ? it is the class with the least de-
pendence on lexicalization and the most subject to
inter-speaker stylistic variation. The relatively low
results for Fact/Emotion is perhaps due to the emo-
tional charge many ideological arguments engender;
informal examination of posts that showed the most
disagreement in this category often showed a cut-
ting comment or a snide remark at the end of a post,
which was was ignored by some annotators and ev-
idence for others (one Emotional post in Figure 3 is
clearly an insult, but was uniformly labeled as -5 by
all annotators).
2.1 Discourse Markers
Both psychological research on discourse processes
(Fox Tree and Schrock, 1999; Fox Tree and Schrock,
2002; Groen et al, 2010) and computational work
on agreement (Galley et al, 2004) indicate that dis-
course markers are strongly associated with partic-
ular pragmatic functions. Because of their salient
position, we test the role of turn-initial markers in
predicting upcoming content (Fox Tree and Schrock,
2002; Groen et al, 2010). Based on manual inspec-
tion of a subset of the corpus, we constructed a list of
20 discourse markers; 17 of these occurred at least
50 times in a quote response (upper bound of 700
samples): actually, and, because, but, I believe, I
know, I see, I think, just, no, oh, really, so, well,
yes, you know, you mean. All of their occurrences
became part of the 10,003 Q-R pairs annotated.
The top discourse markers highlighting disagree-
ment were really (67% read a response beginning
with this marker as prefacing a disagreement with
a prior post), no (66%), actually (60%), but (58%),
so (58%), and you mean (57%). At this point, the
next most disagreeable category was the unmarked
category, with about 50% of respondents interpret-
ing an unmarked post as disagreeing. On the other
hand, the most agreeable marker was yes (73% read
a response beginning with this marker as prefacing
an agreement) followed by I know (64%), I believe
(62%), I think (61%), and just (57%). The other
markers were close to the unmarked category: and
(50%), because (51%), oh (51%), I see (52%), you
know (54%), and well (55%).
The overall agreement on sarcasm was low, as in
other computational work on recognizing sarcasm
(Davidov et al, 2010). At most, only 31% of re-
spondents agreed that the material after a discourse
marker was sarcastic, with the most sarcastic mark-
ers being you mean (31%), oh (29%), really (24%),
so (22%), and I see (21%). Only 15% of respon-
dents rated the unmarked category as sarcastic (e.g.,
fewer than 1 out of 6 respondents). The cues I think
(10%), I believe (9%), and actually (10%) were the
least sarcastic markers.
Taken together, these ratings suggest that the cues
really, you mean, and so can be used to indicate both
5
Class Very High Degree Neutral Very Low Degree
Insult
or
Attack
Well, you have proven yoruself to be a
man with no brain, that is for sure. The
definition that was given was the one that
scientists use, not the layperson.
The empire you defend is tyrannical.
They are responsible for the death of mil-
lions.
Very well put.
Is that what you said right be-
fore they started banning assault
weapons?...Obviously, you?re gullible.
Since you?re such a brainiac and all, why
don?t you visit the UN website and see
what your beloved UN is up to?
Bad comparisons. A fair comparison
would be comparing the total number of
defensive gun uses to the total number
of gun crimes (not just limiting it to gun
homicides).
In some cases yes, in others no. If the
mutation gives a huge advantage, then
there will be a decline in the size of the
gene pool for a while (eg when the Aus-
tralian rabbit population...
Sarcasm My pursuit of happiness is denied by
trees existing. Let?s burn them down and
destroy the environment. It?s much bet-
ter than me being unhappy.
An interesting analysis of that article you
keep quoting from the World Net Daily
[url]
I would suggest you look at the faero is-
land mouse then. That is a new species,
and it is not man doing it, but rather na-
ture itself.
Like the crazy idea the Earth goes around
the Sun.
Indeed there is no diffrence it is still a
dead baby but throwing a baby in a trash
can and leaving it for dead is far more
cruel than abortion.
Too late, drug usage has already created
those epidemics. Legalizing drugs may
increase some of them temporarily, but
they already exist.
Emotion-
based
Argu-
ment
Really! You can prove that most pro-
lifers don?t care about women?...it is id-
iotic thinking like this that makes me re-
spect you less and less.
Fine by me. First, I don?t consider hav-
ing a marriage recognized by govern-
ment to be a ?right?. Second, I?ve said
many times I don?t think government
should be in the marriage business at all.
Sure. Here is an explanation. The 14C
Method. That is from the Radiocarbon
WEB info site by the Waikato Radio-
carbon Dating Lab of the University of
Waikato (New Zeland).
I love Jesus John the Beloved is my most
favorite writer throughout time If you
think I have a problem with a follower
of Jesus your wrong. I have a problem
with the Christians
I agree that the will to survive is an amaz-
ing phenomenon when put to the test.
But I do not agree with your statement
of life at *any* cost. There will always
be a time when the humane/loving thing
to do is to let an infant/child/adult go.
Heller is about determining the answer to
a long standing question on the nature of
the Second Amendment, and how much
gun control is legally allowed. Roe v.
Wade is about finding legal precedent for
the murder of unborn children. I see ab-
solutely no comparison between the two.
Figure 3: Sample Responses for the Insult, Sarcasm, and Fact/Feeling spectrums
disagreement and sarcasm. However, but, no, and
actually can be used for disagreement, but not sar-
casm. And I know (14% sarcastic, similar to None),
I believe, and I think can be used for non-sarcastic
agreement.
From informal analyses, we hypothesized that re-
ally and oh might indicate sarcasm. While we found
evidence supporting this for really, it was not the
case for oh. Instead, oh was used to indicate emo-
tion; it was the discourse marker with the highest
ratings of feeling over fact.
Despite the fact that it would seem that disagree-
ment would be positively correlated with sarcasm,
disagreement and sarcasm were not related. There
were two tests possible. One tested the percentage of
people who identified an item as disagreeing against
the percentage of people who identified it as sar-
casm, r(16) = -.27, p = .27 (tested on 17 discourse
markers plus the None category). The other tested
the degree of disagreement (from -5 to +5) against
the percentage of people who identified the post as
sarcastic, r(16) = -.33, p = .18.
However, we did observe relationships between
sarcasm and other variables. Two results support the
argument that sarcasm is emotional and personal.
The more sarcastic, the nastier (rather than nicer),
r(16) = .87, p < .001. In addition, the more sarcas-
tic, the more emotional (over factual) respondents
were judged to be, r(16) = .62, p = .006 Taken to-
gether, these analyses suggest that sarcasm is emo-
tional and personal, but not necessarily a sign of dis-
agreement.
3 Machine Learning Experimental Setup
For our experiments we used the Weka machine
learning toolkit. All results are from 10 fold cross-
validation on a balanced test set. Unless otherwise
mentioned, we used thresholds of 1 and -1 on the
mean agreement judgment to determine agreement
6
and disagreement respectively. We omitted those Q-
R pairs which were judged neutral (mean annotator
judgment in the (-1,1) range).
As described above, from the original 10,003 Q-
R pairs we applied certain constraints (notably re-
quirement that we be able to identify the originating
post) which left us with 8,242. We then resampled
to obtain a natural distribution leaving us with 2,847
pairs. Applying the (-1,1) threshold and balancing
the result yielded a test set of 682 Q-R pairs.
3.1 Classifiers
Our experiments used two simple classifiers: Naive-
Bayes and JRip. NaiveBayes makes a strict indepen-
dence assumption and can be swamped by the sheer
number of features we used, but it is a solid baseline
and does a decent job of suggesting which features
are more powerful. JRip is a rule based classifier
which produces a compact model suitable for hu-
man consumption and quick application. JRip is not
without its own limitations but, for our task, it shows
better results than NaiveBayes. The model it builds
uses only a handful of features.
3.2 Feature Extraction
Our aim was to develop features for the automatic
identification of agreement and disagreement that
would do well on the task and provide useful base-
lines for comparisons with previous and future work.
Features are grouped into sets as shown in Table 2
and discussed in more detail below.
Set Description/Examples
MetaPost Non-lexical features. E.g. posterid, time be-
tween posts, etc.
Unigrams,
Bigrams
Word and Word Pair frequencies
Cue Words Initial unigram, bigram, and trigram
Punctuation Collapsed into one of the following: ??, !!, ?!
LIWC LIWC measures and frequencies
Dependencies Dependencies derived from the Stanford
Parser.
Generalized De-
pendencies
Dependency features generalized with re-
spect to POS of the head word and opinion
polarity of both words.
Table 2: Feature Sets, Descriptions, and Examples
Unigrams, Bigrams, Trigrams. Results of pre-
vious work suggest that a unigram baseline can be
difficult to beat for certain types of debates (Walker
et al, ; Somasundaran and Wiebe, 2010). Thus we
derived both unigrams and bigrams as features. We
captured the final token as a feature by padding with
-nil- tokens when building the bigrams. See below
for comments on initial uni/bi/tri-grams.
MetaPost Info. Previous work suggested that
non-lexical features like poster ids and the time be-
tween posts might contain indicators of disagree-
ment. People on these forums get to know one an-
other and often enjoy repeatedly arguing with the
same person. In addition, we hypothesized that the
?heat? of a particular conversation could be corre-
lated with rapid-fire exchanges, as indicated by short
time periods between posts.
Thus these features involve structure outside of
the quote/response text. This includes author infor-
mation, time between posts, the log10 of the time
between posts, the number of other quotes in the
response, whether the quote responds to a post by
the response?s author, the percent of the quoted post
which is actually quoted, whether the quoted post is
by the same author as the response (there were only
an handful of these), whether the response mentions
the quote author by name, and whether the response
is longer than the quote.
The forum software effectively does this annota-
tion for us so there is no reason not to consider it as
a clue in our quest to understand and interpret online
dialogue.
Discourse Markers. Previous work on dialogue
analysis has repeatedly noted the discourse func-
tions of particular discourse markers, and our corpus
analysis above also suggests their use in this par-
ticular dataset (Hirschberg and Litman, 1993; Fox
Tree, 2010; Schiffrin, 1987; Di Eugenio et al, 1997;
Moser and Moore, 1995). However, because dis-
course markers can be stacked up Oh, so really we
decided to represent this feature as post initial uni-
grams, bigrams and trigrams.
Repeated Punctuation. Informal analyses of our
data suggested that repeated sequential use of partic-
ular types of punctuation such as !! and ?? did not
mean the same thing as simple counts or frequen-
cies of punctuation across a whole post. Thus we
developed distinct features for a subset of these rep-
etitions.
LIWC. We also derived features using the Lin-
guistics Inquiry Word Count tool (LIWC-2001)
(Pennebaker et al, 2001). LIWC classifies words
7
into 69 categories and counts how many words get
classified into each category. Some LIWC features
that we expect to be important are words per sen-
tence (WPS), pronominal forms, and positive and
negative emotion words.
Dependency and Generalized Dependency. We
used the Stanford parser to extract dependency fea-
tures for each quote and response (De Marneffe et
al., 2006; Klein and Manning, 2003). The depen-
dency parse for a given sentence is a set of triples,
composed of a grammatical relation and the pair
of words for which the grammatical relation holds
(reli, wj , wk), where reli is the dependency relation
among words wj and wk. The word wj is the HEAD
of the dependency relation.
Following (Joshi and Penstein-Rose?, 2009) we ex-
tracted generalized dependency features by leaving
one dependency element lexicalized and generaliz-
ing the other to part of speech. Joshi & Rose?s re-
sults suggested that this approach would work better
than either fully lexicalized or fully generalized de-
pendency features.
Opinion Dependencies. Somasundaran & Wiebe
(2009) introduce the concept of features that iden-
tify the TARGET of opinion words. Inspired by this
approach, we used the MPQA dictionary of opinion
words to select the subset of dependency and gen-
eralized dependency features in which those opin-
ion words appear. For these features we replace the
opinion words with their positive or negative polar-
ity equivalents.
Cosine Similarity. This feature is based on previ-
ous work on threading. We derive cosine-similarity
measure using tf-idf vectors where the document
frequency was derived from the entire topic re-
stricted corpus.
Annotations. We also add features represent-
ing information that we do not currently derive au-
tomatically, but which might be automatically de-
rived in future work based on annotations in the cor-
pus. These include the topic and Mechanical Turk
annotations for Fact/Emotion, Respect/Insult, Sar-
casm, and Nasty/Nice, which could reasonably be
expected to be recognized independently of Agree-
ment/Disagreement.
Feature
type
Selected Features
Meta number-of-other-quotes, percent-quoted, author-quote-
USERNAME
Initial
n-gram
yes, so, I agree, well said, really?, I don?t know
Bigram that you, ? -nil-, you have, evolution is
Depend-
ency
dep-nsubj(agree, i), dep-nsubj(think, you), dep-prep-
with(agree, you)
Opinion
Depen-
dency
dep-opinion-nsubj(negative, you), dep-opinion-
dep(proven, negative), dep-opinion-aux(positive,
to)
Anno-
tations
topic-gay marriage, mean-response-nicenasty, mean-
unsure-sarcasm
Table 3: Some of the more useful features for each cate-
gory, using ?2 for feature selection.
Figure 4: Sample model learned using JRip. The num-
bers represent (total instances covered by a rule / number
incorrectly labeled). This particular model was built on
development data.
4 Results
Table 3 shows features which were selected for each
of our feature categories using a ?2 test for fea-
ture selection. These results vindicate our interest
in discourse markers as cues to argument structure,
as well as the importance of the generalized depen-
dency features and opinion target pairs (Wang and
Rose?, 2010; Somasundaran and Wiebe, 2009). Fig-
ure 4 shows a sample model learned using JRip.
We limit our pair-wise comparisons between clas-
sifiers and feature sets to those corresponding to par-
8
Feats NB JRip?2
Uni,UniCue 0.578 0.626
BOW 0.598 0.654
Meta 0.579 0.588
Response Local 0.600 0.666
Quote Local 0.531 0.588
Both Local 0.601 0.682
Meta+Local 0.603 0.654
All 0.603 0.632
Just Annotations 0.765 0.814
All+Annotations 0.603 0.795
Table 4: Accuracies on a balanced test set (random base-
line: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?2
feature selection on the training set during cross valida-
tion. BOW = Unigrams, CueWords, Bigrams, Trigrams,
LIWC, Repeated Punctuation. Response/Quote/Both
Local uses only those features which exist in the text of
the response or quote respectively. It consists of LIWC,
dependencies, generalized dependencies, the various n-
grams, and length measures.
ticular hypotheses. We conducted five tests with
Bonferroni correction to .01 for a .05 level of sig-
nificance.
While we hypothesized that more sophisticated
linguistic features would improve over unigram fea-
tures alone, a paired t-test using the results in Table 4
indicate that there is no statistical difference be-
tween the performance of JRip using only response
local features (JRip,ResponseLocal), as compared to
the Unigram,UniCue features (t(9) = 2.18, p = .06).
However, a paired t-test using the results in
Table 4 indicate that there is a statistical dif-
ference between the performance of JRip using
local features from both the quote and the re-
sponse, (JRip,BothLocal) as compared to the Uni-
gram,UniCue features (t(9) = 3.94, p =.003). This
shows that the contextual features do matter, even
though (JRip,BothLocal) does not provide signifi-
cant improvements over (JRip,Response Local) (t(9)
= .92, p = .38).
In general, examination of the table suggests
that the JRip classifier performs better than Naive
Bayes. A paired t-test indicates that there is a sta-
tistical difference between the performance of JRip
using local features from both the quote and the
response, (JRip,BothLocal) (JRip,BothLocal) and
Naive Bayes using local features from both the quote
and the response, (NB,BothLocal) (t(9) = 3.43, p =
.007).
In addition, with an eye toward the future, we ex-
amined whether automatic recognition of sarcasm,
attack/insult, fact/feeling nice/nasty could possibly
improve results for recognizing disagreement. Us-
ing the human annotations as a proxy for automatic
results, we get classification accuracies of over 81%
(JRip,JustAnnotations). This suggests it might be
possible to improve results over our best current re-
sults (JRip,BothLocal) (t(9) = 6.09, p < .001).
Another interesting fact, is that despite its use in
previous work for threading, the cosine similarity
between the quote and response did not improve ac-
curacy for the classifiers we tested, over and above
the use of text-based contextual features. Further
investigation is required to draw conclusions about
this or similar metrics (LSA, PMI, etc.).
5 Discussion and Conclusion
In this paper, we have introduced a new collection
of internet forum posts, the ARGUE corpus, col-
lected across a range of ideological topics, and con-
taining scalar Agreement/Disagreement annotations
over quote-response pairs within a post. We have
demonstrated that we can achieve a significant im-
provement over a unigram baseline agreement de-
tection system using features from both a response
and the quote being responded to.
Beyond agreement, the ARGUE corpus contains
finer-grained annotations for degrees of insult, nas-
tiness, and emotional appeal, as well as the pres-
ence of sarcasm. We have demonstrated that these
classes (especially insult and nastiness) correlate
with agreement. While the utility of these classes
as features for agreement detection is dependent on
how easily they are learned, in closing we note that
they also afford us a richer understanding of how ar-
gumentative conversation flows. In section 2.1.2, we
outlined how they can yield understanding of the po-
tential functions of a discourse particle within a par-
ticular post. They may as allow us to understand the
extent to which participants react in kind, rewarding
insult with insult or kindness in turn. In future work,
we hope to turn to these conversational dynamics.
In future work, it would be useful to
build a ternary classifier which labels
9
Agree/Disagree/Neutral, thus reflecting the true
distribution of these dialogue acts in the data.
Additionally, the proportion of agreeing utterances
varies widely across media so it may be desirable to
add an appropriate prior when adapting the model
to a new dataset.
Acknowledgments
This work was funded by Grant NPS-BAA-03 to
UCSC and and through the Intelligence Advanced
Research Projects Activity (IARPA) through the
Army Research Laboratory. We?d like to thank
Craig Martell for helpful discussions over the course
of this project, and the anonymous reviewers for use-
ful feedback. We would also like to thank Michael
Minor and Jason Aumiller for their contributions to
scripting and the database.
References
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
J.L. Austin. 1965. How to do things with words. Oxford
University Press, New York.
R. Awadallah, M. Ramanath, and G. Weikum. 2010.
Language-model-based pro/con classification of polit-
ical text. In Proceeding of the 33rd international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 747?748. ACM.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13?16.
J. Bollen, H. Mao, and X. Zeng. 2011. Twitter mood
predicts the stock market. Journal of Computational
Science.
G.A. Bryant and J.E. Fox Tree. 2002. Recognizing ver-
bal irony in spontaneous speech. Metaphor and sym-
bol, 17(2):99?119.
C. Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of polit-
ical transcripts. In Australasian Language Technology
Association Workshop 2008, volume 6, pages 11?18.
Robert B. Cialdini. 2000. Influence: Science and Prac-
tice (4th Edition). Allyn & Bacon.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twitter
and amazon. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 107?116. Association for Computational Lin-
guistics.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454. Citeseer.
Barbara Di Eugenio, Johanna D. Moore, and Massimo
Paolucci. 1997. Learning features that predict cue
usage. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguistics,
ACL/EACL 97, pages 80?87.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J.E. Fox Tree and J.C. Schrock. 2002. Basic mean-
ings of you know and I mean. Journal of Pragmatics,
34(6):727?747.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):113.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in
conversational speech: Use of bayesian networks to
model pragmatic dependencies. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 669?es. Association for Com-
putational Linguistics.
R.W. Gibbs. 2000. Irony in talk among friends.
Metaphor and Symbol, 15(1):5?27.
M. Groen, J. Noyes, and F. Verstraten. 2010. The Effect
of Substituting Discourse Markers on Their Role in
Dialogue. Discourse Processes: A Multidisciplinary
Journal, 47(5):33.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics, 19(3):501?530.
John E. Hunter. 1987. A model of compliance-
gaining message selection. Communication Mono-
graphs, 54(1):54?63.
M. Joshi and C. Penstein-Rose?. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313?316. Association for Computational
Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
G. Mishne and N. Glance. 2006. Leave a reply: An anal-
ysis of weblog comments. In Third annual workshop
on the Weblogging ecosystem. Citeseer.
10
Margaret G. Moser and Johanna Moore. 1995. Inves-
tigating cue selection and placement in tutorial dis-
course. In ACL 95, pages 130?137.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
A.M. Popescu and M. Pennacchiotti. 2010. Detecting
controversial events from twitter. In Proceedings of
the 19th ACM international conference on Information
and knowledge management, pages 1873?1876. ACM.
Deborah Schiffrin. 1987. Discourse Markers. Cam-
bridge University Press, Cambridge, U.K.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proceedings of
the 2006 conference on empirical methods in natural
language processing, pages 327?335. Association for
Computational Linguistics.
Marilyn Walker, Rob Abbott, Pranav Anand, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. Cats
Rule and Dogs Drool: Classifying Stance in Online
Debate.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
11
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 1?9,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Cats Rule and Dogs Drool!: Classifying Stance in Online Debate
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree,
Robeson Bowmani, and Michael Minor
University of California Santa Cruz
Abstract
A growing body of work has highlighted the
challenges of identifying the stance a speaker
holds towards a particular topic, a task that in-
volves identifying a holistic subjective dispo-
sition. We examine stance classification on
a corpus of 4873 posts across 14 topics on
ConvinceMe.net, ranging from the playful to
the ideological. We show that ideological de-
bates feature a greater share of rebuttal posts,
and that rebuttal posts are significantly harder
to classify for stance, for both humans and
trained classifiers. We also demonstrate that
the number of subjective expressions varies
across debates, a fact correlated with the per-
formance of systems sensitive to sentiment-
bearing terms. We present results for iden-
tifing rebuttals with 63% accuracy, and for
identifying stance on a per topic basis that
range from 54% to 69%, as compared to un-
igram baselines that vary between 49% and
60%. Our results suggest that methods that
take into account the dialogic context of such
posts might be fruitful.
1 Introduction
Recent work has highlighted the challenges of iden-
tifying the STANCE that a speaker holds towards a
particular political, social or technical topic. Clas-
sifying stance involves identifying a holistic subjec-
tive disposition, beyond the word or sentence (Lin
et al, 2006; Malouf and Mullen, 2008; Greene and
Resnik, 2009; Somasundaran and Wiebe, 2009; So-
masundaran and Wiebe, 2010). Our work is inspired
by the large variety of such conversations now freely
available online, and our observation that the contex-
tual affordances of different debate and discussion
websites vary a great deal. One important contex-
tual variable, discussed at length below, is the per-
centage of posts that are rebuttals to previous posts,
which varies in our data from 34% to 80%. The abil-
ity to explicitly rebut a previous post gives these de-
bates both monologic and dialogic properties (Biber,
1991; Crystal, 2001; Fox Tree, 2010); Compare Fig-
ure 1 to Figure 2. We believe that discussions con-
taining many rebuttal links require a different type of
analysis than other types of debates or discussions.
Dialogic Capital Punishment
Studies have shown that using the death penalty saves 4 to 13 lives
per execution. That alone makes killing murderers worthwhile.
What studies? I have never seen ANY evidence that capital pun-
ishment acts as a deterrant to crime. I have not seen any evidence
that it is ?just? either.
When Texas and Florida were executing people one after the other
in the late 90?s, the murder rates in both states plunged, like Rosie
O?donnel off a diet.. .
That?s your evidence? What happened to those studies? In the
late 90s a LOT of things were different than the periods preceding
and following the one you mention. We have no way to determine
what of those contributed to a lower murder rate, if indeed there
was one. You have to prove a cause and effect relationship and
you have failed.
Figure 1: Capital Punishment discussions with posts
linked via rebuttal links.
This paper utilizes 1113 two-sided debates (4873
posts) from Convinceme.net for 14 different debate
topics. See Table 1. On Convinceme, a person starts
a debate by posting a topic or a question and provid-
ing sides such as for vs. against. Debate participants
can then post arguments for one side or the other, es-
sentially self-labelling their post for stance. These
debates may be heated and emotional, discussing
weighty issues such as euthanasia and capital pun-
ishment, such as the example in Figure 1. But they
also appear to be a form of entertainment via playful
1
debate. Popular topics on Convinceme.net over the
past 4 years include discussions of the merits of Cats
vs. Dogs, or Pirates vs. Ninjas (almost 1000 posts).
See Figure 3.
Monologic Capital Punishment
I value human life so much that if someone takes one than his
should be taken. Also if someone is thinking about taking a life
they are less likely to do so knowing that they might lose theirs
Death Penalty is only a costlier version of a lifetime prison sen-
tence, bearing the exception that it offers euthanasia to criminals
longing for an easy escape, as opposed to a real punishment.
There is no proof that the death penalty acts as a deterrent, plus
due to the finalty of the sentence it would be impossible to amend
a mistaken conviction which happens with regualrity especially
now due to DNA and improved forensic science.
Actually most hardened criminals are more afraid to live-then die.
I?d like to see life sentences without parole in lieu of capital pun-
ishment with hard labor and no amenities for hard core repeat
offenders, the hell with PC and prisoner?s rights-they lose priv-
eledges for their behaviour.
Figure 2: Posts on the topic Capital punishment without
explicit link structure. The discussion topic was ?Death
Penalty?, and the argument was framed as yes we should
keep it vs. no we should not.
Our long term goal is to understand the dis-
course and dialogic structure of such conversations.
This could be useful for: (1) creating automatic
summaries of each position on an issue (Sparck-
Jones, 1999); (2) gaining a deeper understanding
of what makes an argument persuasive (Marwell
and Schmitt, 1967); and (3) identifying the lin-
guistic reflexes of perlocutionary acts such as per-
suasion and disagreement (Walker, 1996; Greene
and Resnik, 2009; Somasundaran and Wiebe, 2010;
Marcu, 2000). As a first step, in this paper we aim
to automatically identify rebuttals, and identify the
speaker?s stance towards a particular topic.
Dialogic Cats vs. Dogs
Since we?re talking much of $hit, then Dogs rule! Cat poo is ex-
tremely foul to one?s nostrils you?ll regret ever handling a cat.
Stick with dogs, they?re better for your security, and poo?s not too
bad. Hah!
Dog owners seem infatuated with handling sh*t. Cat owners don?t
seem to share this infatuation.
Not if they?re dog owners who live in the country. If your dog
sh*ts in a field you aren?t going to walk out and pick it up.
Cat owners HAVE to handle sh*t, they MUST clean out a litter
box...so suck on that!
Figure 3: Cats vs. Dogs discussions with posts linked by
rebuttal links.
The most similar work to our own is that of So-
masundaran & Wiebe (2009, 2010) who also focus
on automatically determining the stance of a debate
participant with respect to a particular issue. Their
data does not provide explicit indicators of dialogue
structure such as are provided by the rebuttal links
in Convinceme. Thus, this work treats each post as
a monologic text to be classified in terms of stance,
for a particular topic. They show that discourse re-
lations such as concessions and the identification of
argumentation triggers improves performance over
sentiment features alone (Somasundaran and Wiebe,
2009; Somasundaran and Wiebe, 2010). This work,
along with others, indicates that for such tasks it is
difficult to beat a unigram baseline (Pang and Lee,
2008).
Other similar related work analyzes Usenet forum
quote/response structures (Wang and Rose?, 2010).
We believe quote/response pairs have a similar dis-
course structure to the rebuttal post pairs in Con-
vinceme, but perhaps with the linguistic reflexes
of stance expressed even more locally. However
agreement vs. disagreement is not labelled across
quote/response pairs and Wang & Rose (2010) do
not attempt to distinguish these different discourse
relations. Rather they show that they can use a vari-
ant of LSA to identify a parent post, given a response
post, with approximately 70% accuracy. A recent
paper by (Abbott et al, 2011) examines agreement
and disagreement in quote/response pairs in idealog-
ical and nonidealogical online forum discussions,
and shows that you can distinguish the agreement
relation with 68% accuracy. Their results indicate
that contextual features do improve performance for
identifying the agreement relation between quotes
and responses.
Other work has utilized the social network struc-
ture of online forums, either with or without tex-
tual features of particular posts (Malouf and Mullen,
2008; Mishne and Glance, 2006; Murakami and
Raymond, 2010; Agrawal et al, 2003). However
this work does not examine the way that the dia-
logic structure varies by topic, as we do, and the
threading structure of their debates does not dis-
tinguish between agreement and disagreement re-
sponses. (Mishne and Glance, 2006) show that most
replies to blog posts are disagreements, while Agar-
wal?s work assumed that adjacent posts always dis-
agree, and did not use any of the information in the
text. Murakami & Raymond (2010) show that sim-
ple rules for identifying disagreement, defined on
the textual content of the post, can improve over
Agarwal?s results and (Malouf and Mullen, 2008)
show that a combination of textual and social net-
2
work features provides the best performance. We
leave the incorporation of social network informa-
tion for stance classification to future work.
Section 3 discusses our corpus in more detail, and
presents the results of a human debate-side classi-
fication task conducted on Mechanical Turk. Sec-
tion 3 describes two different machine learning ex-
periments: one for identifying rebuttals and the other
for automatically determining stance. Section 4
presents our results. We show that we can iden-
tify rebuttals with 63% accuracy, and that using sen-
timent, subjectivity and dialogic features, we can
achieve debate-side classification accuracies, on a
per topic basis, that range from 54% to 69%, as com-
pared to unigram baselines that vary between 49%
and 60%.
2 Corpus Description and Analysis
Table 1 provides an overview of our corpus. Our
corpus consists of 1113 two-sided debates (4873
posts) from Convinceme.net for 12 topics ranging
from playful debates such as Cats vs. Dogs to more
heated political topics such as Capital Punishment.
In Table 1, the topics above the line are either tech-
nical or playful, while the topics below the line are
ideological. In total the corpus consists of 2,722,340
words; the topic labeled debates which we use in our
experiments contain 507,827 words.
Convinceme provides three possible sources of
dialogic structure: (1) the SIDE that a post is placed
on indicates the poster?s stance with respect to the
original debate topic, and thus can be considered as a
response to that post; (2) REBUTTAL LINKS between
posts which are explicitly indicated by the poster us-
ing the affordances of the site; and (3) the TEMPO-
RAL CONTEXT of the debate, i.e. the state of the
debate at a particular point in time, which a debate
participant orients to in framing their post.
Topics vary a great deal in terms of their dialogic
structure and linguistic expression. In Table 1, the
columns providing counts for different variables are
selected to illustrate ways in which topics differ in
the form and style of the argument and in its sub-
jective content. One important variable is the per-
centage of the topic posts that are linked into a re-
buttal dialogic structure (Rebuttals). Some of these
differences can be observed by comparing the dia-
logic and monologic posts for the Capital Punish-
ment topic in Figures 1 and 2 to those for the Cats
vs. Dogs topic in Figures 3 and 4. Ideological
Monologic Cats vs. Dogs
First of all, cats are about a thousand times easier to care for.
You don?t have to walk them or bathe them because they?re smart
enough to figure out all that stuff on their own. Plus, they have the
common courtesy to do their business in the litter box, instead of
all over your house and yard. Just one of the many reasons cats
rule and dogs, quite literally drool!
Say, you had a bad day at work, or a bad breakup, you just wanna
go home and cry. A cat would just look at you like ?oh ok, you?re
home? and then walk away. A dog? Let?s see, the dog would most
likely wiggle its tail, with tongue sticking out and head tilted - the
?you?re home! i missed you so much, let?s go snuggle in front of
the TV and eat ice-cream? look. What more do I need to say?
Figure 4: Posts on the topic Cats vs. Dogs without ex-
plicit rebuttal links.
topics display more author investment; people feel
more strongly about these issues. This is shown by
the fact that there are more rebuttals per topic and
more posts per author (P/A) in the topics below the
line in Table 1. It follows that these topics have a
much higher degree of context-dependence in each
post, since posts respond directly to the parent post.
Rebuttals exhibit more markers of dialogic interac-
tion: greater pronominalization (especially you as
well as propositional anaphora such as that and it),
ellipsis, and dialogic cue words; Figure 5 shows the
difference in counts of ?you? between rebuttals and
non-rebuttals (Rebuttals x? = 9.6 and Non-Rebuttals
x? = 8.5, t(27) = 24.94, p < .001). Another indi-
cation of author investment is the percentage of au-
thors with more than one post (A > 1P). Post Length
(PL), on the other hand, is not significantly corre-
lated with degree of investment in the topic.
Figure 5: Kernel density estimates for ?you? counts across
rebuttals (green) and non-rebuttals (red).
Other factors we examined were words per sen-
3
Post and Threading Variables Normalized LIWC Variables
Topic Posts Rebuttals P/A A > 1p PL Pro WPS 6LTR PosE NegE
Cats v. Dogs 148 40% 1.68 26% 242 3.30 -1.95 -2. 43 1.70 .30
Firefox vs. IE 218 40% 1.28 16% 167 -0.11 -0.84 0.53 1.23 -0.81
Mac vs. PC 126 47% 1.85 24% 347 0.52 0.28 -0.85 -0.11 -1.05
Superman/Batman 140 34% 1.41 21% 302 -0.57 -1.78 -0.43 1.21 .99
2nd Amendment 134 59% 2.09 45% 385 -1.38 1.74 0.58 -1.04 0.38
Abortion 594 70% 2.82 43% 339 0.63 -0.27 -0.41 -0.95 0.68
Climate Change 202 69% 2.97 40% 353 -0.74 1.23 0.57 -1.25 -0.63
Communism vs. Capitalism 212 70% 3.03 47% 348 -0.76 -0.15 1.09 0.39 -0.55
Death Penalty 324 62% 2.44 45% 389 -0.15 -0.40 0.49 -1.13 2.90
Evolution 798 76% 3.91 55% 430 -0.80 -1.03 1.34 -0.57 -0.94
Exist God 844 77% 4.24 52% 336 0.43 -0.10 0.34 -0.24 -0.32
Gay Marriage 505 65% 2.12 29% 401 -0.13 .86 .85 -0.42 -0.01
Healthcare 110 80% 3.24 56% 280 0.28 1.54 .99 0.14 -0.42
Marijuana Legalization 214 52% 1.55 26% 423 0.14 0.37 0.53 -0.86 0.50
Table 1: Characteristics of Different Topics. Topics below the line are considered ?ideological?. Normalized LIWC
variable z-scores are significant when more than 1.94 standard deviations away from the mean (two-tailed).
KEY: Number of posts on the topic (Posts). Percent of Posts linked by Rebuttal links (Rebuttals). Posts per author
(P/A). Authors with more than one post (A > 1P). Post Length in Characters (PL). Pro = percent of the words as
pronominals. WPS = Words per sentence. 6LTR = percent of words that are longer than 6 letters. PosE positive
emotion words. NegE negative emotion words.
tence (WPS), the length of words used (6LTR)
which typically indicates scientific or low frequency
words, the use of pronominal forms (Pro), and
the use of positive and negative emotion words
(PosE,NegE) (Pennebaker et al, 2001). For exam-
ple, Table 1 shows that discussions about Cats vs.
Dogs consist of short simple words in short sen-
tences with relatively high usage of positive emo-
tion words and pronouns, whereas 2nd amendment
debates use relatively longer sentences, and death
penalty debates (unsurprisingly) use a lot of nega-
tive emotion words.
Human Topline. The best performance for sid-
ing ideological debates in previous work is approx-
imately 64% accuracy over all topics, for a collec-
tion of 2nd Amendment, Abortion, Evolution, and
Gay Rights debate posts (Somasundaran and Wiebe,
2010). Their best performance is 70% for the 2nd
amendment topic. The website that these posts were
collected from apparently did not support dialogic
threading, and thus there are no explicitly linked re-
buttals in this data set. Given the dialogic nature
of our data, as indicated by the high percentage of
rebuttals in the ideological debates, we first aim to
determine how difficult it is for humans to side an
individual post from a debate without context. To
our knowledge, none of the previous work on de-
bate side classification has attempted to establish a
human topline.
We set up a Mechanical Turk task by randomly se-
lected a subset of our data excluding the first post on
each side of a debate and debates with fewer than 6
posts on either side. Each of our 12 topics consists of
more than one debate: each debate was mapped by
hand to the topic and topic-siding (as in (Somasun-
daran and Wiebe, 2010)). We selected equal num-
bers of posts for each topic for each side, and cre-
ated 132 tasks (Mechanical Turk HITs). Each HIT
consisted of choosing the correct side for 10 posts
divided evenly, and selected randomly without re-
placement, from two debates. For each debate we
presented a title, side labels, and the initial post on
each side. For each post we presented the first 155
characters with a SEE MORE button which expanded
the post to its full length. Each HIT was judged by 9
annotators using Mechanical Turk with each anno-
tator restricted to at most 30 HITS (300 judgments).
Since many topics were US specific and we wanted
annotators with a good grasp of English, we required
Turkers to have a US IP address.
Figure 6 plots the number of annotators over all
topics who selected the ?true siding? as the side that
the post was on. We defined ?true siding? for this
purpose as the side that the original poster placed
their post. Figure 6 illustrates that humans often
placed the post on the wrong side. The majority of
posters agreed with the true siding 78.26% of the
time. The Fleiss? kappa statistic was 0.2656.
Importantly and interestingly, annotator accuracy
varied across topics in line with rebuttal percentage.
Annotators correctly labeled 94 of 100 posts for Cats
vs. Dogs but only managed 66 of 100 for the Cli-
4
Figure 6: Accuracies of Human Mechanical Turk judges
at selecting the True Siding of a post without context.
mate Change topic. This suggests that posts may
be difficult to side without context, which is what
one might expect given their dialogic nature. Rebut-
tals were clearly harder to side: annotators correctly
sided non-rebuttals 87% of the time, but only man-
aged 73% accuracy for rebuttals. Since all of the less
serious topics consisted of ?50% rebuttals while all
of the more serious ideological debates had >50%
rebuttals, 76% of ideological posts were sided cor-
rectly, while 85% of non-ideological posts were cor-
rectly sided. See Table 2.
Class Correct Total Accuracy
Rebuttal 606 827 0.73
Non-Rebuttal 427 493 0.87
Table 2: Human Agreement on Rebuttal Classification
Looking at the data by hand revealed that when
nearly all annotators agreed with each other but dis-
agreed with the self-labeled side, the user posted on
the wrong side (either due to user error, or because
the user was rebutting an argument the parent post
raised, not the actual conclusion).
The difficult-to-classify posts (where only 4-6 an-
notators were correct) were more complex. Our
analysis suggests that in 28% of these cases, the an-
notators were simply wrong, perhaps only skimming
a post when the stance indicator was buried deep in-
side it. Our decision to show only the first 155 char-
acters of each post by default (with a SHOW MORE
button) may have contributed to this error. An ad-
ditional 39% were short comments or ad hominem
responses, that showed disagreement, but no indi-
cation of side and 17% were ambiguous out of con-
text. A remaining 10% were meta-debate comments,
either about whether there were only two sides, or
whether the argument was meaningful. Given the
differences in siding difficulty depending on rebut-
tal status, in Section 4 we present results for both
rebuttal and stance classification.
3 Features and Learning Methods
Our experiments were conducted with the Weka
toolkit. All results are from 10 fold cross-validation
on a balanced test set. In the hand examination of
annotators siding performance, 101 posts were de-
termined to have incorrect self-labeling for side. We
eliminated these posts and their descendants from
the experiments detailed below. This resulted in a
dataset of 4772 posts. We used two classifiers with
different properties: NaiveBayes and JRip. JRip is
a rule based classifier which produces a compact
model suitable for human consumption and quick
application. Table 3 provides a summary of the fea-
tures we extract for each post. We describe and mo-
tivate these feature sets below.
Set Description/Examples
Post Info IsRebuttal, Poster
Unigrams Word frequencies
Bigrams Word pair frequencies
Cue Words Initial unigram, bigram, and trigram
Repeated
Punctuation
Collapsed into one of the following: ??, !!, ?!
LIWC LIWC measures and frequencies
Dependencies Dependencies derived from the Stanford Parser.
Generalized
Dependen-
cies
Dependency features generalized with respect to
POS of the head word and opinion polarity of
both words.
Opinion De-
pendencies
Subset of Generalized Dependencies with opin-
ion words from MPQA.
Context Fea-
tures
Matching Features used for the post from the par-
ent post.
Table 3: Feature Sets, Descriptions, and Examples
Counts, Unigrams, Bigrams. Previous work
suggests that the unigram baseline can be difficult to
beat for certain types of debates (Somasundaran and
Wiebe, 2010). Thus we derived both unigrams and
bigrams as features. We also include basic counts
such as post length.
Cue Words. We represent each posts initial un-
igram, bigram and trigram sequences to capture the
useage of cue words to mark responses of particular
type, such as oh really, so, and well; these features
were based on both previous work and our exami-
nation of the corpus (Fox Tree and Schrock, 1999;
Fox Tree and Schrock, 2002; Groen et al, 2010).
5
Repeated Punctuation. Our informal analyses
suggested that repeated sequential use of particular
types of punctuation such as !! and ?? did not mean
the same thing as simple counts or frequencies of
punctuation across a whole post. Thus we developed
distinct features for a subset of these repetitions.
LIWC. We also derived features using the Lin-
guistics Inquiry Word Count tool (LIWC-2001)
(Pennebaker et al, 2001). LIWC provides meta-
level conceptual categories for words to use in word
counts. Some LIWC features that we expect to be
important are words per sentence (WPS), pronomi-
nal forms (Pro), and positive and negative emotion
words (PosE) and (NegE). See Table 1.
Syntactic Dependency. Previous research in
this area suggests the utility of dependency struc-
ture to determine the TARGET of an opinion word
(Joshi and Penstein-Rose?, 2009; Somasundaran and
Wiebe, 2009; Somasundaran and Wiebe, 2010). The
dependency parse for a given sentence is a set of
triples, composed of a grammatical relation and the
pair of words for which the grammatical relation
holds (reli, wj , wk), where reli is the dependency
relation among words wj and wk. The word wj is
the HEAD of the dependency relation. We use the
Stanford parser to parse the utterances in the posts
and extract dependency features (De Marneffe et al,
2006; Klein and Manning, 2003).
Generalized Dependency. To create generalized
dependencies, we ?back off? the head word in each
of the above features to its part-of-speech tag (Joshi
and Penstein-Rose?, 2009). Joshi & Rose?s results
suggested that this approach would work better than
either fully lexicalized or fully generalized depen-
dency features. We call these POS generalized de-
pendencies in the results below.
Opinion Dependencies. Somasundaran & Wiebe
(2009) introduced features that identify the TAR-
GET of opinion words. Inspired by this approach,
we used the MPQA dictionary of opinion words
to select the subset of dependency and generalized
dependency features in which those opinion words
appear. For these features we replace the opinion
words with their positive or negative polarity equiv-
alents (Lin et al, 2006).
Context Features. Given the difficulty annota-
tors had in reliably siding rebuttals as well as their
prevalence in the corpus, we hypothesize that fea-
tures representing the parent post could be helpful
for classification. Here, we use a naive represen-
tation of context, where for all the feature types in
Table 3, we construct both parent features and post
features. For top-level parentless posts, the parent
features were null.
Figure 7: Model for distinguishing rebuttals vs. nonre-
buttals across all topics.
4 Results
The primary aim of our experiments was to deter-
mine the potential contribution, to debate side clas-
sification performance, of contextual dialogue fea-
tures, such as linguistic reflexes indicating a poster?s
orientation to a previous post or information from a
parent post. Because we believed that identification
of whether a post is a rebuttal or not might be help-
ful in the long term for debate-side classification, we
also establish a baseline for rebuttal classification.
4.1 Rebuttal Classification Results
The differences in human performance for siding de-
pended on rebuttal status. Our experiments on re-
buttal classification using the rule-based JRip clas-
sifer on a 10-fold cross-validation of our dataset pro-
6
duced 63% accuracy. Figure 7 illustrates a sample
model learned for distinguishing rebuttals from non-
rebuttals across all topics. The Figure shows that,
although we used the full complement of lexical and
syntactic features detailed above, the learned rules
were almost entirely based on LIWC and unigram
lexical features, such as 2nd person pronouns (7/8
rules), quotation marks (4/8 rules), question marks
(3/8), and negation (4/8), all of which correlated
with rebuttals. Other features that are used at several
places in the tree are LIWC Social Processes, LIWC
references to people, and LIWC Inclusive and Ex-
clusive. One tree node reflects the particular concern
with bodily functions that characterizes the Cats vs.
Dogs debate as illustrated in Figure 3.
4.2 Automatic Debate-Side Classification
Results
We first compared accuracies using Naive Bayes to
JRip for all topics for all feature sets. A paired t-test
showed that Naive Bayes over all topics and feature
sets was consistently better than JRip (p < .0001).
Thus the rest of our analysis and the results in Ta-
ble 4 focus on the Naive Bayes results.
Table 4 presents results for automatic debate
side classification using different feature sets and the
Naive Bayes learner which performs best over all
topics. In addition to classifying using only post-
internal features, we ran a parallel set of experiments
adding contextual features representing the parent
post, as described in Section 3. The results in Table
4 are divided under the headers Without Context and
With Context depending on whether features from
the parent post were used if it existed (e.g. in the
case of rebuttals).
We conducted paired t-tests over all topics simul-
taneously to examine the utility of different feature
sets. We compared unigrams to LIWC, opinion gen-
eralized dependencies, POS generalized dependen-
cies, and all features. We also compared experi-
ments using context features to experiments using
no contextual features. In general, our results in-
dicate that if the data are aggregated over all top-
ics, that indeed it is very difficult to beat the uni-
gram baseline. Across all topics there are generally
no significant differences between experiments con-
ducted with unigrams and other features. The mean
accuracies across all topics for unigrams vs. LIWC
features was 54.35% for unigrams vs. 52.83% for
LIWC. The mean accuracies for unigram vs POS
generalized dependencies was 54.35% vs. 52.64%,
and for unigrams vs. all features was Unigram
54.35% vs 54.62%. The opinion generalized de-
pendencies features actually performed significantly
worse than unigrams with an accuracy of 49% vs.
54.35% (p < .0001).
It is interesting to note that in general the unigram
accuracies are significantly below what Somasun-
daran and Wiebe achieve (who report overall uni-
gram of 62.5%). This suggests a difference between
the debate posts in their corpus and the Convinceme
data we used which may be related to the proportion
of rebuttals.
The overal lack of impact for either the POS gen-
eralized dependency features (GDepP) or the Opin-
ion generalized dependency features (GDep0) is
surprising given that they improve accuracy for other
similar tasks (Joshi and Penstein-Rose?, 2009; Soma-
sundaran and Wiebe, 2010). While our method of
extracting the GDepP features is identical to (Joshi
and Penstein-Rose?, 2009), our method for extracting
GDepO is an approximation of the method of (So-
masundaran and Wiebe, 2010), that does not rely on
selecting particular patterns indicating the topics of
arguing by using a development set.
The LIWC feature set, which is based on a lexi-
cal hierarchy that includes social features, negative
and positive emotion, and psychological processes,
is the only feature set that appears to have the po-
tential to systematically show improvement over a
good range of topics. We believe that further analy-
sis is needed; we do not want to handpick topics for
which particular feature sets perform well.
Our results also showed that context did not seem
to help uniformly over all topics. The mean per-
formance over all topics for contextual features us-
ing the combination of all features and the Naive
Bayes learner was 53.0% for context and 54.62%
for no context (p = .15%, not significant). Interest-
ing, the use of contextual features provided surpris-
ingly greater performance for particular topics. For
example for 2nd Amendment, unigrams with con-
text yield a performance of 69.23% as opposed to
the best performing without context features using
LIWC of 64.10%. The best performance of (So-
masundaran and Wiebe, 2010) is also 70% for the
2nd amendment topic. For the Healthcare topic,
LIWC with context features corresponds to an accu-
racy of 60.64% as opposed to GDepP without con-
text performance of 54.26%. For Communism vs.
Capitism, LIWC with context features gives an ac-
curacy of 56.55% as opposed to accuracies actually
7
Without Context With Context
Turk Uni LIWC GdepO GdepP All Uni LIWC GdepO GdepP All
Cats v. Dogs 94 59.23 55.38 56.15 61.54 62.31 50.77 56.15 55.38 60.77 50.00
Firefox vs. IE 74 51.25 53.75 43.75 48.75 50.00 51.25 53.75 52.50 52.50 51.25
Mac vs. PC 76 53.33 56.67 55.00 50.83 56.67 53.33 55.83 56.67 49.17 54.17
Superman Batman 89 54.84 45.97 42.74 45.97 54.03 50.00 57.26 43.55 50.81 53.23
2nd Amendment 69 56.41 64.10 51.28 58.97 57.69 69.23 61.54 44.87 52.56 67.95
Abortion 75 50.97 51.56 50.58 52.14 51.17 51.36 53.70 51.75 53.70 50.78
Climate Change 66 53.65 58.33 38.02 46.35 50.52 48.96 56.25 38.02 38.54 48.96
Comm vs. Capitalism 68 48.81 47.02 46.43 47.02 48.81 45.83 56.55 47.02 51.19 48.81
Death Penalty 79 51.80 53.96 46.76 49.28 52.52 51.80 56.12 56.12 57.55 53.24
Evolution 72 57.24 48.36 54.93 56.41 57.24 54.11 46.22 50.82 52.14 52.96
Existence of God 73 52.71 51.14 49.72 52.42 51.99 52.28 52.28 50.14 53.42 51.42
Gay Marriage 88 60.28 56.11 56.11 58.61 59.44 56.94 52.22 54.44 53.61 54.72
Healthcare 86 52.13 51.06 51.06 54.26 52.13 45.74 60.64 59.57 57.45 53.19
MJ Legalization 81 57.55 46.23 43.40 53.77 59.43 52.83 46.23 49.06 49.06 50.94
Table 4: Accuracies achieved using different feature sets and 10-fold cross validation as compared to the human
topline from MTurk. Best accuracies are shown in bold for each topic in each row. KEY: Human topline results
(Turk). Unigram features (Uni). Linguistics Inquiry Word Count features (LIWC). Generalized dependency features
containing MPQA terms (GdepO) & POS tags (GdepP). NaiveBayes was used, no attribute selection was applied.
below the majority class baseline for all of the fea-
tures without context.
Should we conclude anything from the fact that
6 of the topics are idealogical, out of the 7 topics
where contextual features provide the best perfor-
mance? We believe that the significantly greater per-
centage of rebuttals for these topics should give a
greater weight to contextual features, so it would be
useful to examine stance classification performance
on the subset of the posts that are rebuttals. We be-
lieve that context is important; our conclusion is that
our current contextual features are naive ? they are
not capturing the relationship between a post and a
parent post. Sequential models or at least better con-
textual features are needed.
The fact that we should be able to do much better
is indicated clearly by the human topline, shown in
the column labelled Turk in Table 4. Even without
context, and with the difficulties siding rebuttals, the
human annotators achieve accuracies ranging from
66% to 94%.
5 Discussion
This paper examines two problems in online-
debates: rebuttal classification and debate-side or
stance classification. Our results show that we can
identify rebuttals with 63% accuracy, and that using
lexical and contextual features such as those from
LIWC, we can achieve debate-side classification ac-
curacies on a per topic basis that range from 54% to
69%, as compared to a unigram baselines that vary
between 49% and 60%. These are the first results
that we are aware of that establish a human topline
for debate side classification. These are also the first
results that we know of for identifying rebuttals in
such debates.
Our results for stance classification are mixed.
While we show that for many topics we can beat
a unigram baseline given more intelligent features,
we do not beat the unigram baseline when we com-
bine our data across all topics. In addition, we are
not able to show across all topics that our contex-
tual features make a difference, though clearly use of
context should make a difference in understanding
these debates, and for particular topics, classifica-
tion results using context are far better than the best
feature set without any contextual features. In fu-
ture work, we hope to develop more intelligent fea-
tures for representing context and improve on these
results. We also plan to make our corpus available
to other researchers in the hopes that it will stimu-
late further work analyzing the dialogic structure of
such debates.
Acknowledgments
This work was funded by Grant NPS-BAA-03 to
UCSC and Intelligence Advanced Research Projects
Activity (IARPA) through the Army Research Lab-
oratory to UCSC by subcontract from the University
of Maryland. We?d like to thank Craig Martell and
Joseph King for helpful discussions over the course
of this project, and the anonymous reviewers for use-
ful feedback. We would also like to thank Jason Au-
miller for his contributions to the database.
8
References
Rob Abbott, Marilyn Walker, Jean E. Fox Tree, Pranav
Anand, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing Dis-
agreement in Informal Political Argument. In Pro-
ceedings of the ACL Workshop on Language and So-
cial Media.
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
D. Biber. 1991. Variation across speech and writing.
Cambridge Univ Pr.
David Crystal. 2001. Language and the Internet. Cam-
bridge University Press.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454. Citeseer.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J.E. Fox Tree and J.C. Schrock. 2002. Basic mean-
ings of you know and I mean. Journal of Pragmatics,
34(6):727?747.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):113.
S. Greene and P. Resnik. 2009. More than words: Syn-
tactic packaging and implicit sentiment. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
503?511. Association for Computational Linguistics.
M. Groen, J. Noyes, and F. Verstraten. 2010. The Effect
of Substituting Discourse Markers on Their Role in
Dialogue. Discourse Processes: A Multidisciplinary
Journal, 47(5):33.
M. Joshi and C. Penstein-Rose?. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313?316. Association for Computational
Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
W.H. Lin, T. Wilson, J. Wiebe, and A. Hauptmann. 2006.
Which side are you on?: identifying perspectives at
the document and sentence levels. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 109?116. Association for
Computational Linguistics.
R. Malouf and T. Mullen. 2008. Taking sides: User clas-
sification for informal online political discourse. In-
ternet Research, 18(2):177?190.
Daniel Marcu. 2000. Perlocutions: The Achilles?
heel of Speech Act Theory. Journal of Pragmatics,
32(12):1719?1741.
G. Marwell and D. Schmitt. 1967. Dimensions of
compliance-gaining behavior: An empirical analysis.
sociomety, 30:350?364.
G. Mishne and N. Glance. 2006. Leave a reply: An anal-
ysis of weblog comments. In Third annual workshop
on the Weblogging ecosystem. Citeseer.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
Karen Sparck-Jones. 1999. Automatic summarizing;
factors and directions. In Inderjeet Mani and Mark
Maybury, editors, Advances in Automatic Text Summa-
rization. MIT Press.
Marilyn A. Walker. 1996. Inferring acceptance and re-
jection in dialogue by default rules of inference. Lan-
guage and Speech, 39-2:265?304.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
9
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 84?88,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
POLITICAL-ADS: An annotated corpus of event-level evaluativity
Kevin Reschke
Department of Computer Science
Stanford University
Palo Alto, CA 94305 USA
reschkek@gmail.com
Pranav Anand
Department of Linguistics
University of California, Santa Cruz
Santa Cruz, CA 95064 USA
panand@ucsc.edu
Abstract
This paper presents a corpus targeting eval-
uative meaning as it pertains to descriptions
of events. The corpus, POLITICAL-ADS is
drawn from 141 television ads from the 2008
U.S. presidential race and contains 3945 NPs
and 1549 VPs annotated for scalar sentiment
from three different perspectives: the narra-
tor, the annotator, and general society. We
show that annotators can distinguish these per-
spectives reliably and that correlation between
the annotator?s own perspective and that of a
generic individual is higher than those with
the narrator. Finally, as a sample application,
we demonstrate that a simple compositional
model built off of lexical resources outper-
forms a lexical baseline.
1 Introduction
In the past decade, the semantics of evaluative lan-
guage has received renewed attention in both formal
and computational linguistics (Martin and White,
2005; Potts, 2005; Pang and Lee, 2008; Jackend-
off, 2007). This work has focused on evaluativity
at either the lexical level or the phrasal/event level
stance, without bridging between the two. A par-
allel tradition of compositional event polarity ((Na-
sukawa and Yi, 2003; Moilanen and Pulman, 2007;
Choi and Cardie, 2008; Neviarouskaya et al, 2010))
has grown up analogous to approaches to composi-
tionality in formal semantics: event predicates are
not of constant polarity, but provide functions from
the polarities of their arguments to event polarities.
Little work exists assessing the relative advantages
of a compositional account, in part because no re-
source annotating both NP level polarity and event-
level polarity in context exists. This paper intro-
duces such a corpus, POLITICAL-ADS, a collec-
tion of 2008 U.S. presidential race television ads
with scalar sentiment annotations at the NP and VP
level. After describing the corpus creation and char-
acteristics in sections 3 and 4, in section 5, we show
that a compositional system achieves an accuracy of
84.2%, above a lexical baseline of 65.1%.
2 Background
While many sentiment models handle negation
quasi-compositionally (Pang and Lee, 2008; Polanyi
and Zaenen, 2005), Nasukawa & Yi (Nasukawa and
Yi, 2003) first noted that predicates like prevent
are ?flippers?, conveying that their subject and ob-
ject have opposite polarity ? since trouble is nega-
tive, something that prevents trouble is good. Re-
cent work has expanded that idea into a fully com-
positional system (Moilanen and Pulman, 2007;
Neviarouskaya et al, 2010). Moilanen and Pulman
construct a system of compositional rules that builds
polarityin terms of a hand-built lexicon of predicates
as flippers or preservers. However, this system con-
flates two different assessment perspectives, that of
the Narrator and of some mentioned NP (NP-to-NP
perspective). The latter include psychological pred-
icates such as love and hate, and those of admira-
tion or censure (e.g., admonish, praise). Thus, they
would mark John dislikes scary movies as negative, a
correct NP-to-NP claim, but not necessarily correct
for the Narrator. Recognizing this, Neviarouskaya
et al (Neviarouskaya et al, 2010) develop a pair of
84
Announcer: In tough times, who will help Michigan?s
auto industry? Barack Obama favors loan guarantees to
help Detroit retool and revitalize. But John McCain re-
fused to support loan guarantees for the auto industry.
Now he?s just paying lip service. Not talking straight.
And McCain voted repeatedly for tax breaks for compa-
nies that ship jobs overseas, selling out American anno-
tators. We just can?t afford more of the same.
Figure 1: Transcript of POLITICAL-ADS ad #57
 14 
 Figure 5: Snapshot of Mechanical Turk form for Transcript #57 (Dem.)  
  Figure 6: Instruction for completing annotation form.  Our Goals: The purpose of this HIT is to help us document the words people use to persuade others. Overview: We ask you to read transcripts of political ads from the 2008 US presidential campaign (you can watch videos of the ads as well). Then you will answer questions about different highlighted portions of the ad. The questions are designed to determine how different pieces of text contribute to the overall message of the ad. You will answer the same four questions for each highlighted portion: 1. How does the narrator want you to feel about the highlighted expression? 2. How do you to feel about the highlighted expression? 3. In your opinion, how controversial is the highlighted expression in American society? 
Figure 2: POLITICAL-ADS annotation interface
compositional rules over both perspectives. Impor-
tantly, neither of these appr ache have been vali-
dated against a suffici ntly nuanced dataset. M ila-
nen and Pulman test against the SemEval-07 Head-
lines Corpus, which asks annotators to give an over-
all impression of sentiment. This approach allows a
headline such as Outcry in N Korea ?nuclear test?
to be arked negative, even though outcry over
military provocations is arguably good. Similarly,
Neviarouskaya et al evaluate only against NP-to-
NP data as well. While the MPQA corpus (Wiebe
et al, 2005), which annotates the source of each
sentiment annotation, separates these two sentiment
sources, work trained on it has not (Choi and Cardie,
2008; Moilanen et al, 2010). In addition, existing
annotation schemes are not designed to tease apart
perspectival differences. For example, MPQA in-
cludes a notion of Narrator-oriented evaluativity, but
it does not include the perspectives of you and the
general public.
3 The corpus
POLITICAL-ADS, is drawn from politics, a rich
and recently evolving domain for evaluativity re-
search that we hypothesized would involve a high
volume of sentiment claims subject to perspecti-
val differences. POLITICAL-ADS is a collec-
tion of 141 television ads that ran during the 2008
U.S. presidential race between Democratic candi-
date Barack Obama and Republican candidate John
McCain. The collection consists of 81 ads from
Democratic side and 60 ads from Republican side.
Figure 1 provides a sample transcript.
Each transcript was parsed using the Stanford
Parser and all NPs and VPs excluding those headed
by auxiliaries were extracted. VP annotations were
assumed to represent phrasal/event-level polarity
and NP ones argument-level polarity. The annota-
tion interface is shown in Figure 2. Annotators were
shown a transcript and a movie clip, and navigated
through the NPs and VPs within the document. At
each point they were asked to rate their response
on a [-1,1] scale for the following four questions
about the highlighted expression: 1) how the nar-
rator wants them to feel; 2) how they feel; 3) how
people in general feel; 4) how controversial the is-
sue is (included to test the whether sense of contro-
versy yields sharper differences between the various
assessment perspectives). Finally, because phrases
were not prefiltered, a ?Doesn?t Make Sense? button
was provided for each question.
206 annotators on Mechanical Turk completed
985 transcripts at $0.40 per transcript; each tran-
script was annotated by an average of 4.8 different
annotators living in the U.S. We then filtered anno-
tators by 200 phrases we deemed relatively uncon-
troversial in 20 randomly selected transcripts. To do
this, we scored each annotator in terms of the ab-
solute difference between their mean response and
the median (each annotator?s scores were first nor-
malized by mean absolute value) in the Narrator
question. We found when we thresholded annota-
tors at a score above 0.5, agreement with our gold
standard was 83.5% and dropped substantially after-
wards. This threshold excluded 74 annotators, leav-
ing 132 high-quality, or HQ, annotators (the full data
is available in the corpus).
The corpus consists of 5494 phrases (1549 VPs
and 3945 NPs) annotated 6.3 times on average, for
a total of 34, 692 annotations (9800 VP and 24892
NP). Each phrase was annotated by at least 3 HQ
annotators (average 3.9 annotators), and such an-
notators contributed 5960 VP and 15238 NP an-
85
notations. Of these, 12.1% HQ NP and 5.4% of
HQ VP responses were marked as ?Doesn?t Make
Sense? (DMS) for the narrator question. In general,
controversy and narrator questions had the highest
and lowest rates of DMS, respectively; NPs showed
higher response rates than VPs; and HQ annotators
had higher rates of button presses.1 In sections 4 and
5, we will ignore the DMS responses.
4 Corpus Findings
Table 1 provides summary statistics for the corpus.
Across the board, the three perspective questions av-
eraged close to 0, and in general HQ annotators are
closer to 0 (non-HQ annotators tended to provide
positive responses). VPs had slightly higher vari-
ance than NPs, at marginal probability (p < .04),
suggesting that VP responses were more extreme
than NP ones. You and Generic assessments are
highly correlated (Pearson?s ? = 0.85), but Narra-
tor is less so (? = .76/.74). All three are weakly
correlated with Controversy (? = .25/.26/.29 for
Narr., You, Gen., respectively). Narrator has the
highest standard deviations for the raw data, but the
lowest for the normed data. In the raw data, many
annotators recognized the narrators intensely parti-
san views and rated accordingly (|x| > 0.8), but
were more tempered when providing their perspec-
tive (|x| ? 0.35), leading to lower ?. This intensity
difference is factored out in normalization, yielding
the opposite pattern.
The response data was collected from our anno-
tators in scalar form, but applications (e.g., evalu-
ative polarity classification) it is the polarity of the
response that matters. Ignoring magnitude, Table 3
shows the polarity breakdown for all HQ phrasal an-
notations. Positive responses are the dominant class
across the board. Neutral responses are less frequent
for Narrator than for the other types. NPs have fewer
negatives and more neutrals than VPs.
Table 2 shows average standard deviations (i.e.,
agreement) by worker, question, and XP type. Note
both that NPs show less variance than VPs and that
non-HQ annotators less than HQ annotators (non-
HQ annotators gave more 0 responses).
1In a QUESTION + PHRASE TYPE + QUESTION + ANNOTA-
TOR TYPE linear model with annotator as a random effect, all
of the above effects are significant. This was the simplest model
COND ALL HQ ONLY
RAW RAW NORMED
Narr. .10 (.45) .05 (.62) .08 (.87)
You .10 (.34) .06 (.46) .09 (.85)
Gen. .10 (.33) .05 (.45) .08 (.86)
Contr. .17 (.22) .13 (.30) .17 (.60)
Table 1: Mean response by category and worker type
COND HQ ANNOTATORS
RAW NORMED
ALL VP NP ALL VP NP
Narr. .69 .75 .67 .96 1.06 .93
You .57 .63 .55 .99 1.12 .94
Gen. .53 .58 .51 .99 1.13 .94
Contr. .53 .58 .51 1.01 1.15 .96
ALL ANNOTATORS
ALL VP NP
Narr. .63 .68 .62
You .54 .59 .53
Gen. .52 .56 .51
Contr. .54 .56
Table 2: Average Standard Deviations For HQ and all
annotators
5 Comparing lexical and compositional
treatments
While compositional models of event-level evalua-
tivity are logically defensible, the extent to which
these models apply in the wild is an open ques-
tion. Because other compositional lexicons are not
freely available, we used the system described in
(Reschke and Anand, 2011), which induces flippers
and preservers from the MPQA subjectivity lexi-
con and FrameNet (Ruppenhofer et al, 2005). The
MPQA lexicon is a collection of over 8,000 words
marked for polarity. Our functor lexicon uses the
following heuristic: verbs marked positive in MPQA
are preservers; verbs marked negative are flippers.
For example, dislike has negative MPQA polarity;
therefore, it is marked as a flipper in our lexicon.
This gives us 1249 predicates: 869 flippers and 380
preservers. 329 additional verbs were added from
FrameNet according to their membership in five en-
according to ?w model comparison.
86
COND POL VP NP
Narr. + 2874 (51%) 6877 (51%)
- 2654 (47%) 5590 (42%)
0 111 (2%) 932 (7%)
You + 2714 (49%) 6573 (50%)
- 2466 (45%) 4967 (38%)
0 337 (6%) 1575 (12%)
Gen. + 2615 (48%) 6350 (49%)
- 2541 (48%) 5125 (39%)
0 332 (6%) 1558 (12%)
Contr. + 3095 (57%) 6522 (51%)
- 1755 (32%) 4159 (33%)
0 558 (10%) 2051 (16%)
Table 3: Polarity breakdowns for HQ annotations
tailment classes (Reschke and Anand, 2011): verbs
of injury/destruction, lacking, benefit, creation, and
having. 124 frames across these classes were identi-
fied, and then verbs of benefit, creation, and having
(aid, generate, have) were marked as preservers and
the complement set (forget, arrest, lack) as flippers.
As a lexical baseline, the MPQA polarity of each
verb was used ? flippers correspond to baseline neg-
ative events and preservers to positive ones.
A 635 VP test subset of POLITICAL-ADS was
constructed by omitting intransitive VPs and VPs
with non-NP complements. Gold standard labels
were determined from average normed HQ annota-
tor data. This yielded 329 positive, 284 negative,
and 2 neutral events. NPs, determined similarly, di-
vided into 393 positive, 230 negative, and 12 neutral.
Of the 635 VPs in the test set, only 272 (43.5%)
are in our FrameNet/MPQA lexicon and we hence
compare the two systems on this subset. On this
subset, the compositional system has an accuracy of
84.2%, while the lexical baseline has an accuracy
of 65.1%; there were 72 instances where the com-
positional model outperformed the lexical baseline
and 22 where the lexical outperformed the composi-
tional. Typical examples where the compositional
system won involve MPQA negatives like break,
cut, and hate and positives like want and trust. The
lexical model marks VPs like breaks the grip of for-
eign oil and want a massive government as negative
and positive, respectively ? because the NPs in ques-
tion are negative, the answers should be reversed. In
contrast, the lexical model wins on cases like grow
the economy and reform Wall Street correct. These
exemplify a robust pattern in the errors: cases where
the event is marked positive while the NP is marked
negative. In examples like grow Washington, the
idea that grow is a preserver is reasonable. However,
in grow the economy, the negativity of the economy
is arguably measuring the state of some constant en-
tity. While reform is marked positive in MPQA, it
is arguably a reverser; this shows the problems with
our lexicon induction.
At an intuitive level, we expect agent evalu-
ativity to mirror event-level evaluativity because
positive/negative entities tend to commit posi-
tive/negative acts, and this is borne out. For flip-
pers or preservers, the average VP evaluativity is
correlated with the average subject evaluativity. For
flippers the correlation is 0.57; for preservers it is
0.52. Although our model ignored subject evalua-
tivity, we performed a generalized linear regression
with subject and object evaluativity as predictors
and event-level evaluativity as outcome. For flip-
pers the regression coefficients were 0.52 for subject
(p < 4e? 4) and?0.52 for object (p < 1e? 5). For
preservers the coefficients were 0.27 (p < 1e?5) for
subject and 0.93 for object (p < 2e? 7). Thus, sub-
ject polarity is an important factor for flipper events
(e.g., the hero/villain defeated the enemy, but less so
for preservers (e.g. the hero/villain helped the en-
emy.).
6 Conclusion
In this paper we have presented POLITICAL-ADS,
a new resource for investigating the relationships be-
tween NP sentiment and VP sentiment systemati-
cally. We have demonstrated that annotators can re-
liably annotate political data with sentiment at the
phrasal level from multiple perspectives. We have
also shown that in the present data set that self-
reporting and judging generic positions are highly
correlated, while correlation with narrators is ap-
preciably weaker, as narrators are seen as more ex-
treme. We have also shown that the controversy of a
phrase does not correlate with annotators? disagree-
ments with the narrator. Finally, as a sample appli-
cation, we demonstrated that a simple compositional
model built off of lexical resources outperforms a
purely lexical baseline.
87
References
Y. Choi and C Cardie. 2008. Learning with compo-
sitional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of EMNLP
2008.
Ray Jackendoff. 2007. Language, consciousness, cul-
ture. MIT Press.
J. R. Martin and P. R. R. White. 2005. Language of Eval-
uation: Appraisal in English. Palgrave Macmillan.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of RANLP 2007.
K. Moilanen, S. Pulman, and Y Zhang. 2010. Packed
feelings and ordered sentiments: Sentiment pars-
ing with quasi-compositional polarity sequencing and
compression. In Proceedings of WASSA 2010, EACI
2010.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Cap-
turing favorability using natural language processing.
In Proceedings of the 2nd international conference on
Knowledge capture.
A. Neviarouskaya, H. Prendinger, , and M. Ishizuka.
2010. Recognition of affect, judgment, and appreci-
ation in text. In Proceedings of COLING 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
L. Polanyi and A. Zaenen. 2005. Contextual valence
shifters. in computing attitude and affect in text. In
Janyce Wiebe James G. Shanahan, Yan Qu, editor,
Computing Attitude and Affect in Text: Theory and
Application. Springer Verlag, Dordrecht, The Nether-
lands.
Chris Potts. 2005. The Logic of Conventional Implica-
ture. Oxford University Press.
K. Reschke and P. Anand. 2011. Extracting contextual
evaluativity. In Proceedings of ICWS 2011.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, and Christopher R. Johnson. 2005. Framenet
ii: Extended theory and practice. Technical report,
ICSI Technical Report.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language. In
Proceedings of LREC 2005.
88
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 65?69, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Annotating the Focus of Negation in terms of Questions Under Discussion
Pranav Anand
Department of Linguistics
University of California, Santa Cruz
Santa Cruz, CA 95064 USA
panand@ucsc.edu
Craig Martell
Department of Computer Science
Naval Postgraduate School
Monterey, CA USA
cmartell@nps.edu
Abstract
Blanco & Moldovan (Blanco and Moldovan,
2011) have empirically demonstrated that
negated sentences often convey implicit pos-
itive inferences, or focus, and that these in-
ferences are both human annotatable and ma-
chine learnable. Concentrating on their anno-
tation process, this paper argues that the focus-
based implicit positivity should be separated
from concepts of scalar implicature and neg-
raising, as well as the placement of stress. We
show that a model making these distinctions
clear and which incorporates the pragmatic
notion of question under discussion yields ?
rates above .80, but that it substantially de-
flates the rates of focus of negation in text.
1 Introduction
The recent paper by Blanco & Moldovan (Blanco
and Moldovan, 2011) has highlighted the fact that
negation in natural language is more that just a
propositional logic operator. The central claims of
the paper are that negation conveys implicit positiv-
ity more than half of the time and that such positivity
is both reliably annotatable by humans and promis-
ingly learnable by machine. In this paper, we eval-
uate their annotation process and propose a differ-
ent model that incorporates the pragmatic concept
that discourse is guided by questions under discus-
sion (QUDs), often implicit issues that hearers and
speakers are attending to. We concentrate on the
corpus used in (Blanco and Moldovan, 2011), PB-
FOC.1
1PB-FOC was released as part of *SEM 2012
Shared Task: Resolving the Scope and Focus of Nega-
Our animating concern can be seen concretely by
comparing the examples2 from the corpus provided
below.
(1) a. ?They were willing to mistreat us be-
cause we hadn?t shown any moxie, any
resistance,? says William Queenan, a
DC-10 pilot and 14-year Federal vet-
eran. (ex. 939)
b. ?I won?t be throwing 90 mph, but I will
throw 80-plus,? he says. (ex. 1)
c. ?Some shows just don?t impress, he
says, and this is one of them.? (ex. 30)
d. ?But we don?t believe there is enough
of a difference to be clinically signifi-
cant,? Dr. Sobel said. (ex. 426)
We believe these examples are incorrectly anno-
tated, but in somewhat different ways. Following
Blanco & Moldovan, assume that focus of negation
is diagnosed by an implication that some alternative
to the focus would make a sentence true. Then in
(1a), in which the focus is annotated as being on the
negative polarity item any moxie, any resistance, it
is not clear that there is focus at all. If there were,
the sentence would imply that the pilots in ques-
tion showed something but not some moxie. This
doesn?t seem to be the meaning intended. In con-
trast, in (1b), we agree that focus is present, but
take it to be on the phrase 90 mph, as is confirmed
tionhttp://www.clips.ua.ac.be/sem2012-st-neg/
2The citation (ex. n) will refer to the nth annotated instance
in the PB-FOC dataset. In these and following examples, we
indicate the PB-FOC focus by emboldening and our suggested
alternative (if present) by italics
65
by the overt contrast that follows. Finally, (1c) and
(1d) both show something more complex; in (1c) the
scalar quantifier some is not in the scope of negation
(lest it mean no shows impress), and thus cannot be
a focus. Nonetheless, we agree that a positive im-
plicature arises here (namely, that some shows do
impress), but we suggest that this is simply a fact
about scalar implicatures. Finally, in (1d), in which
the verb believe is a so-called neg-raiser (a predicate
P such that ?P (x) ? P (?x)), the implicit posi-
tivity about a belief the doctors have is not due to
pragmatic focus, but a lexical property of the verb in
question.
In sum, what worried us was the variety of con-
structions being considered equivalent. In order to
respond to these concerns, we reannotated 2304 sen-
tences from the development subcorpus, being care-
ful to try to tease apart the relevant distinctions men-
tioned above. This paper documents that effort. Our
central finding is that the PB-FOC data contains an
overabundance of focus-marked phrases (i.e., cases
like (1a)): the PB-FOC rate of focus marking in our
subcorpus is 74% (somewhat higher than the 65%
for the whole dataset), while we observed a rate of
50%. Although the reduction in focus-marking oc-
curs across all Propbank role types, we show that it
is highest with the A1 and AM-MNR roles. One
central reason for the overmarking, we argue, is
that the definition of focus of negation Blanco &
Moldovan use is somewhat vague, allowing one to
confuse emphasis with implicit positivity. We ar-
gue instead that although they are right to correlate
stress with focus (by and large), focus is connected
to referencing a QUD (Rooth, 1996; Roberts, 1996;
Kadmon, 2001), and only indirectly leads to positiv-
ity.
2 Delimiting Focus of Negation
2.1 What Focus of Negation is
Following (Huddleston and Pullman, 2002), Blanco
& Moldovan define the focus of negation as ?that
part of the scope [of negation] that is most promi-
nently or explicitly negated.? They further argue that
when there is a focus of negation, it yields a cor-
responding positive inference. This idea has roots
in Jackendoff?s seminal theory of focus (Jackendoff,
1972). Jackendoff proposes a) that focus in general
(with or without negation) partitions a sentence into
a function, obtained by lambda abstracting over the
focused constituent and b) that negation is a focus-
sensitive operator, stating that the function applied to
the focused constituent yields falsity. To capture the
positive inference cases, Jackendoff initially claims
that focus always presupposes that there is some el-
ement in the function?s domain (i.e., there is some
way to make the sentence true).
(2) Bill likes Mary. 7? ??x Bill likes x, Mary?
(3) not(?f, x?) = 0.
(4) focus presupposition: ?y[f(y) = 1].
While 4 might be correct for focus-sensitive op-
erators like only, it is clearly not for negation. As
Jackendoff himself points out, the sentence
(5) Bill doesn?t like anybody.
clearly does not lead to the inference that Bill likes
someone, even when anybody is strongly stressed.
More contemporary work (Rooth, 1996; Roberts,
1996) has instead argued that what focus presup-
poses is that there is a relevant question under dis-
cussion (QUD). In the case of 2, it is the question
(6) Who does Bill like?
The QUD model assumes that dialogue is struc-
tured in terms of currently relevant (often implicit)
questions, which serve to explain how a coherent
discourse arises. Focus is thus coherent in context
if the corresponding QUD is relevant. This serves to
explain Jackendoff?s counterexample (5) ? anybody
is focused because the question (6) is currently rele-
vant. Under this account, focus of negation does not
automatically yield an existential positive inference,
but only if the corresponding QUD is assumed to ex-
clude negative answers (i.e., if it is assumed that no
one is not a suitable answer to Who does Bill like?).
Adopting the QUD model thus means that in deter-
mining the positive inferences from a negated sen-
tence, we must ask two questions:
a) What is the relevant QUD for this sentence/sub-
sentence?
b) Does that QUD in context prohibit negative an-
swers?
66
2.2 What isn?t Focus of Negation
Thus, we see that the positive inference resulting
from a negated sentence is the result of an inter-
play of the general meaning of focus (referencing
a relevant QUD) and context (furnishing an assump-
tion that some non-negative answer to the QUD ex-
ists). However, there is another way of yielding pos-
itive inferences to negated sentences, relying merely
on the familiar theory of scalar implicature. Con-
sider (7) below, which involves the scalar expres-
sion much (roughly equivalent to a lot). In positive
assertions, using the quantifier a lot entails the corre-
sponding alternative with some, and using all entails
a lot. In the scope of negation, these patterns reverse,
giving rise to opposite implicatures. Thus, (7) impli-
cates that the stronger alternative (8) is false and thus
(9) ? that some but not much of a clue is given.
(7) assertion: However, it doesn?t give much of
a clue as to whether a recession is on the
horizon. (ex. 122)
(8) stronger alternative: It doesn?t give any clue
as to whether a recession is on the horizon.
(9) implicature: It gives some clue as to whether
the recession is on the horizon.
A different problem occurs with ?neg-raising?
predicates like believe, expect, think, seem, and
want. Since (Filmore, 1963), it has been noted that
some clausal embedding predicates seem to interpret
a superordinate negation inside their scope ? that is,
BILL DOESN?T THINK MARY IS HERE seems to be
equivalent to BILL THINKS MARY ISN?T HERE.
While neg-raising is defeasible in certain contexts
and its explanation is contentious (see (Gajewski,
2007) for discussion), it does not seem to be depen-
dent on focus per se. In particular, putting focus on
any element in the complement clause seems to en-
gender a different positive inference. For example,
in (10), this would give rise to the inference that Bill
wants to talk to someone else, not simply that he
wants to not talk to Mary.
(10) Bill doesn?t want to talk to Mary.
In short, neg-raising cases should be considered
more properly to be cases where the scope of nega-
tion is semantically lower than it appears, not cases
of focus driven inference.
3 Reannotation
We annotated 2304 examples from the shared task
training corpus. As in the original study, annotators
were shown a target sentence as well as the prior
and following sentence and were asked to mark the
focus of negation in the target. Annotators followed
a three step process. First, they were instructed to
?move? the negation around the sentence to various
constituents, as exemplified below, introducing an
existential quantificational some. . . but not.
(11) a. [She]A0 didn?t have [hot water]A1 [for
five days]AM?TMP . (ex. 1925)
b. Someone but not her had hot water for
five days.
c. She had something but not hot water for
five days.
d. She had hot water but not for five days.
They were then asked to determine which if any
of these was most relevant, given the surrounding
context and mark that as the focus. In determining
which was most relevant, annotators asked whether
the question corresponding to each altered sentence
(e.g., Who had hot water for five days?) appeared to
be under discussion in context.3
Three linguist annotators were selected and
trained on 20 examples randomly drawn from the
training set, including 5 examples of scalar ?focus?,
3 of neg-raising, and 5 instances of no focus. An-
notators were given explicit feedback on each trial
annotated. The annotators then annotated the re-
maining 2284 examples in our subcorpus with 100%
overlap and 2 annotators per token.
3.1 Results
Figure 1 summarizes the differences between PB-
FOC and our annotation by role4. Our annotators
achieved a pairwise ? of 0.82. Our agreement with
PB-FOC was significantly lower: ? = 0.48 if we
exclude scalars and neg-raisers and 0.59 if we count
them as focused.
3The QUD model in general allows multiple foci, e.g., Who
had hot water when? We did not consider multiple foci in the
present study.
4Other consists of C-A1, AM-PNC, AM-LOC, A4, R-A1,
AM-EXT, A3, R-A0, AM-DIR, AM-DIS, R-AM-LOC
67
PB-FOC ROLE COUNT AGREED SCALAR NEG-RAISING NO FOCUS OTHER
A1 920 332 54 101 372 61
NO FOCUS 591 532 0 0 AGREED 59
AM-TMP 160 116 0 0 29 15
AM-MNR 125 51 28 0 40 6
A2 112 43 1 0 47 21
A0 88 24 20 0 23 21
AM-ADV 77 30 3 0 26 18
No Role 69 42 2 0 19 6
Other 161 42 8 20 75 16
TOTAL 2303 1212 116 121 631 223
Figure 1: Overall comparison of roles
As Figure 1 shows, the central reason for this
discrepancy is the 631 examples where our annota-
tors did not find focus where PB-FOC indicated that
there was some; in contrast, only 59 examples that
PB-FOC labeled as focusless were disagreed with.
There are two interesting trends. First, we found
an abundance of cases where the the question pro-
duced by the PB-FOC focus yielded an uninforma-
tive question (12% of disagreements), often in cases
containing predicates of possession (e.g., have, con-
tain). For example, in (12), the PB-FOC label would
be answer the question What do American Brands
conclude they have under the contract?, which does
not seem relevant in context.
(12) possession (7%): ?We have previously had
discussions with representatives of Pinker-
ton?s Inc. concerning the (sale of the com-
pany) and we concluded that we did not have
liability under the contract,? says American
Brands. (ex. 181)
An additional 4% of the disagreements involved
idiomatic expressions, where neither the syntactic
nor the semantic sub-constituents could be mean-
ingfully separated; in (13), take kindly to that as a
whole is negated, and focusing on any one part will
upset the idiom. Although of small number, the bi-
ased questions exemplified in (14) are illustrative of
negation?s chimerical lives; in these questions, nega-
tion?s function is at the discourse level and it has no
propositional negative force.
(13) idioms (4%): But media-stock analyst
Richard J. MacDonald of MacDonald
Grippo Riely says Wall Street won?t take
kindly to that. (ex. 2081)
(14) biased questions (10 instances): But
wouldn?t a president who acted despite Sen-
ate objections be taking grave political
risks? (ex. 489)
4 Conclusion
We have argued that while the study of the focus of
negation is of compelling interest to the computa-
tional community, more work is needed at theory-
and annotation-building levels before we can effec-
tively ask machine learning questions. We have sug-
gested that one promising route for pursuing this
is to operationalize the question under discussion
model of focus?s contribution to a sentence, and
that such a procedure yields a marked decrease in
the prevalence of focus of negation in PB-FOC.
This partly follows from our decision on linguistic
grounds to separate focus of negation from scalar
implicature and neg-raising. From an engineering
perspective, if our goal is to extract any positive in-
ference from negated clauses, such distinctions may
be academic. We suspect, however, that the linguis-
tic heterogeneity substantially complicates annota-
tor?s task. We have shown that by explicitly telling
annotators what the differences are, agreement rises,
and we think future work should incorporate such a
model. Finally, we plan on annotating foci that do
not yield positive inferences, since it has the hope of
giving us a window into when and how focus gives
rise to positivity.
68
References
Eduardo Blanco and Dan Moldovan. 2011. Semantic
Representation of Negation Using Focus Detection. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), Portland, OR,
USA.
Charles Filmore. 1963. The position of embedding trans-
formations in grammar. Word, 19:208?231.
Jan Robert Gajewski. 2007. Neg-raising and polarity.
Linguistics and Philosophy, 30:289?328.
Rodney Huddleston and Geoffrey K. Pullman. 2002. The
Cambridge Grammar of the English Langauge. Cam-
bridge University Press.
Ray Jackendoff. 1972. Semantic Interpretation in Gen-
erative Grammar. MIT Press, Cambridge, Mass.
Nirit Kadmon. 2001. Formal Pragmatics: Seman-
tics, Pragmatics, Presupposition, and Focus. Wiley-
Blackwell.
Craige Roberts. 1996. Information structure: Towards
an integrated theory of formal pragmatics. In Jae-Hak
Yoon and Andreas Kathol, editors, OSU Working Pa-
pers in Linguistics, Volume 49: Papers in Semantics,
pages 91?136. The Ohio State University Department
of Linguistics.
Mats Rooth. 1996. Focus. In Shalom Lappin, editor, The
Handbook of Contemporary Semantic Theory, pages
271?298. Blackwell, Oxford.
69
