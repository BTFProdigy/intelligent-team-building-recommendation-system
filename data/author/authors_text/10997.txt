Proceedings of ACL-08: HLT, pages 1030?1038,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extraction of Entailed Semantic Relations Through
Syntax-based Comma Resolution
Vivek Srikumar 1 Roi Reichart2 Mark Sammons1 Ari Rappoport2 Dan Roth1
1University of Illinois at Urbana-Champaign
{vsrikum2|mssammon|danr}@uiuc.edu
2Institute of Computer Science, Hebrew University of Jerusalem
{roiri|arir}@cs.huji.ac.il
Abstract
This paper studies textual inference by inves-
tigating comma structures, which are highly
frequent elements whose major role in the ex-
traction of semantic relations has not been
hitherto recognized. We introduce the prob-
lem of comma resolution, defined as under-
standing the role of commas and extracting the
relations they imply. We show the importance
of the problem using examples from Tex-
tual Entailment tasks, and present A Sentence
Transformation Rule Learner (ASTRL), a ma-
chine learning algorithm that uses a syntac-
tic analysis of the sentence to learn sentence
transformation rules that can then be used to
extract relations. We have manually annotated
a corpus identifying comma structures and re-
lations they entail and experimented with both
gold standard parses and parses created by a
leading statistical parser, obtaining F-scores of
80.2% and 70.4% respectively.
1 Introduction
Recognizing relations expressed in text sentences is
a major topic in NLP, fundamental in applications
such as Textual Entailment (or Inference), Question
Answering and Text Mining. In this paper we ad-
dress this issue from a novel perspective, that of un-
derstanding the role of the commas in a sentence,
which we argue is a key component in sentence
comprehension. Consider for example the following
three sentences:
1. Authorities have arrested John Smith, a retired
police officer.
2. Authorities have arrested John Smith, his friend
and his brother.
3. Authorities have arrested John Smith, a retired
police officer announced this morning.
Sentence (1) states that John Smith is a retired
police officer. The comma and surrounding sen-
tence structure represent the relation ?IsA?. In (2),
the comma and surrounding structure signifies a list,
so the sentence states that three people were ar-
rested: (i) John Smith, (ii) his friend, and (iii) his
brother. In (3), a retired police officer announced
that John Smith has been arrested. Here, the comma
and surrounding sentence structure indicate clause
boundaries.
In all three sentences, the comma and the sur-
rounding sentence structure signify relations essen-
tial to comprehending the meaning of the sentence,
in a way that is not easily captured using lexical-
or even shallow parse-level information. As a hu-
man reader, we understand them easily, but auto-
mated systems for Information Retrieval, Question
Answering, and Textual Entailment are likely to en-
counter problems when comparing structures like
these, which are lexically similar, but whose mean-
ings are so different.
In this paper we present an algorithm for comma
resolution, a task that we define to consist of (1) dis-
ambiguating comma type and (2) determining the
relations entailed from the sentence given the com-
mas? interpretation. Specifically, in (1) we assign
each comma to one of five possible types, and in
(2) we generate a set of natural language sentences
that express the relations, if any, signified by each
comma structure. The algorithm uses information
extracted from parse trees. This work, in addition to
having immediate significance for natural language
processing systems that use semantic content, has
potential applications in improving a range of auto-
1030
mated analysis by decomposing complex sentences
into a set of simpler sentences that capture the same
meaning. Although there are many other widely-
used structures that express relations in a similar
way, commas are one of the most commonly used
symbols1. By addressing comma resolution, we of-
fer a promising first step toward resolving relations
in sentences.
To evaluate the algorithm, we have developed an-
notation guidelines, and manually annotated sen-
tences from the WSJ PennTreebank corpus. We
present a range of experiments showing the good
performance of the system, using gold-standard and
parser-generated parse trees.
In Section 2 we motivate comma resolution
through Textual Entailment examples. Section 3 de-
scribes related work. Sections 4 and 5 present our
corpus annotation and learning algorithm. Results
are given in Section 6.
2 Motivating Comma Resolution Through
Textual Entailment
Comma resolution involves not only comma dis-
ambiguation but also inference of the arguments
(and argument boundaries) of the relationship repre-
sented by the comma structure, and the relationships
holding between these arguments and the sentence
as a whole. To our knowledge, this is the first pa-
per that deals with this problem, so in this section
we motivate it in depth by showing its importance
to the semantic inference task of Textual Entailment
(TE) (Dagan et al, 2006), which is increasingly rec-
ognized as a crucial direction for improving a range
of NLP tasks such as information extraction, ques-
tion answering and summarization.
TE is the task of deciding whether the meaning
of a text T (usually a short snippet) can be inferred
from the meaning of another text S. If this is the
case, we say that S entails T . For example2, we say
that sentence (1) entails sentence (2):
1. S: Parviz Davudi was representing Iran at a
meeting of the Shanghai Co-operation Orga-
nization (SCO), the fledgling association that
1For example, the WSJ corpus has 49K sentences, among
which 32K with one comma or more, 17K with two or more,
and 7K with three or more.
2The examples of this section are variations of pairs taken
from the Pascal RTE3 (Dagan et al, 2006) dataset.
binds two former Soviet republics of central
Asia, Russia and China to fight terrorism.
2. T: SCO is the fledgling association that binds
several countries.
To see that (1) entails (2), one must understand
that the first comma structure in sentence (1) is an
apposition structure, and does not indicate the begin-
ning of a list. The second comma marks a boundary
between entities in a list. To make the correct infer-
ence one must determine that the second comma is a
list separator, not an apposition marker. Misclassify-
ing the second comma in (1) as an apposition leads
to the conclusion that (1) entails (3):
3. T: Russia and China are two former Soviet re-
publics of central Asia .
Note that even to an educated native speaker of
English, sentence 1 may be initially confusing; dur-
ing the first reading, one might interpret the first
comma as indicating a list, and that ?the Shanghai
Co-operation Organization? and ?the fledgling asso-
ciation that binds...? are two separate entities that are
meeting, rather than two representations of the same
entity.
From these examples we draw the following con-
clusions: 1. Comma resolution is essential in com-
prehending natural language text. 2. Explicitly rep-
resenting relations derived from comma structures
can assist a wide range of NLP tasks; this can be
done by directly augmenting the lexical-level rep-
resentation, e.g., by bringing surface forms of two
text fragments with the same meaning closer to-
gether. 3. Comma structures might be highly am-
biguous, nested and overlapping, and consequently
their interpretation is a difficult task. The argument
boundaries of the corresponding extracted relations
are also not easy to detect.
The output of our system could be used to aug-
ment sentences with an explicit representation of en-
tailed relations that hold in them. In Textual Entail-
ment systems this can increase the likelihood of cor-
rect identification of entailed sentences, and in other
NLP systems it can help understanding the shallow
lexical/syntactic content of a sentence. A similar ap-
proach has been taken in (Bar-Haim et al, 2007; de
Salvo Braz et al, 2005), which augment the source
sentence with entailed relations.
1031
3 Related Work
Since we focus on extracting the relations repre-
sented by commas, there are two main strands of
research with similar goals: 1) systems that directly
analyze commas, whether labeling them with syn-
tactic information or correcting inappropriate use in
text; and 2) systems that extract relations from text,
typically by trying to identify paraphrases.
The significance of interpreting the role of com-
mas in sentences has already been identified by (van
Delden and Gomez, 2002; Bayraktar et al, 1998)
and others. A review of the first line of research is
given in (Say and Akman, 1997).
In (Bayraktar et al, 1998) the WSJ PennTreebank
corpus (Marcus et al, 1993) is analyzed and a very
detailed list of syntactic patterns that correspond to
different roles of commas is created. However, they
do not study the extraction of entailed relations as
a function of the comma?s interpretation. Further-
more, the syntactic patterns they identify are unlexi-
calized and would not support the level of semantic
relations that we show in this paper. Finally, theirs
is a manual process completely dependent on syn-
tactic patterns. While our comma resolution system
uses syntactic parse information as its main source
of features, the approach we have developed focuses
on the entailed relations, and does not limit imple-
mentations to using only syntactic information.
The most directly comparable prior work is that
of (van Delden and Gomez, 2002), who use fi-
nite state automata and a greedy algorithm to learn
comma syntactic roles. However, their approach dif-
fers from ours in a number of critical ways. First,
their comma annotation scheme does not identify
arguments of predicates, and therefore cannot be
used to extract complete relations. Second, for each
comma type they identify, a new Finite State Au-
tomaton must be hand-encoded; the learning com-
ponent of their work simply constrains which FSAs
that accept a given, comma containing, text span
may co-occur. Third, their corpus is preprocessed by
hand to identify specialized phrase types needed by
their FSAs; once our system has been trained, it can
be applied directly to raw text. Fourth, they exclude
from their analysis and evaluation any comma they
deem to have been incorrectly used in the source
text. We include all commas that are present in the
text in our annotation and evaluation.
There is a large body of NLP literature on punctu-
ation. Most of it, however, is concerned with aiding
syntactic analysis of sentences and with developing
comma checkers, much based on (Nunberg, 1990).
Pattern-based relation extraction methods (e.g.,
(Davidov and Rappoport, 2008; Davidov et al,
2007; Banko et al, 2007; Pasca et al, 2006; Sekine,
2006)) could in theory be used to extract relations
represented by commas. However, the types of
patterns used in web-scale lexical approaches cur-
rently constrain discovered patterns to relatively
short spans of text, so will most likely fail on
structures whose arguments cover large spans (for
example, appositional clauses containing relative
clauses). Relation extraction approaches such as
(Roth and Yih, 2004; Roth and Yih, 2007; Hirano
et al, 2007; Culotta and Sorenson, 2004; Zelenko et
al., 2003) focus on relations between Named Enti-
ties; such approaches miss the more general apposi-
tion and list relations we recognize in this work, as
the arguments in these relations are not confined to
Named Entities.
Paraphrase Acquisition work such as that by (Lin
and Pantel, 2001; Pantel and Pennacchiotti, 2006;
Szpektor et al, 2004) is not constrained to named
entities, and by using dependency trees, avoids the
locality problems of lexical methods. However,
these approaches have so far achieved limited accu-
racy, and are therefore hard to use to augment exist-
ing NLP systems.
4 Corpus Annotation
For our corpus, we selected 1,000 sentences con-
taining at least one comma from the Penn Treebank
(Marcus et al, 1993) WSJ section 00, and manu-
ally annotated them with comma information3. This
annotated corpus served as both training and test
datasets (using cross-validation).
By studying a number of sentences from WSJ (not
among the 1,000 selected), we identified four signif-
icant types of relations expressed through commas:
SUBSTITUTE, ATTRIBUTE, LOCATION, and LIST.
Each of these types can in principle be expressed us-
ing more than a single comma. We define the notion
3The guidelines and annotations are available at http://
L2R.cs.uiuc.edu/
?
cogcomp/data.php.
1032
of a comma structure as a set of one or more commas
that all relate to the same relation in the sentence.
SUBSTITUTE indicates an IS-A relation. An ex-
ample is ?John Smith, a Renaissance artist, was fa-
mous?. By removing the relation expressed by the
commas, we can derive three sentences: ?John Smith
is a Renaissance artist?, ?John Smith was famous?,
and ?a Renaissance artist was famous?. Note that in
theory, the third relation will not be valid: one exam-
ple is ?The brothers, all honest men, testified at the
trial?, which does not entail ?all honest men testified
at the trial?. However, we encountered no examples
of this kind in the corpus, and leave this refinement
to future work.
ATTRIBUTE indicates a relation where one argu-
ment describes an attribute of the other. For ex-
ample, from ?John, who loved chocolate, ate with
gusto?, we can derive ?John loved chocolate? and
?John ate with gusto?.
LOCATION indicates a LOCATED-IN relation. For
example, from ?Chicago, Illinois saw some heavy
snow today? we can derive ?Chicago is located in
Illinois? and ?Chicago saw some heavy snow today?.
LIST indicates that some predicate or property
is applied to multiple entities. In our annotation,
the list does not generate explicit relations; instead,
the boundaries of the units comprising the list are
marked so that they can be treated as a single unit,
and are considered to be related by the single rela-
tion ?GROUP?. For example, the derivation of ?John,
James and Kelly all left last week? is written as
?[John, James, and Kelly] [all left last week]?.
Any commas not fitting one of the descriptions
above are designated as OTHER. This does not in-
dicate that the comma signifies no relations, only
that it does not signify a relation of interest in this
work (future work will address relations currently
subsumed by this category). Analysis of 120 OTHER
commas show that approximately half signify clause
boundaries, which may occur when sentence con-
stituents are reordered for emphasis, but may also
encode implicit temporal, conditional, and other re-
lation types (for example, ?Opening the drawer, he
found the gun.?). The remainder comprises mainly
coordination structures (for example, ?Although he
won, he was sad?) and discourse markers indicating
inter-sentence relations (such as ?However, he soon
cheered up.?). While we plan to develop an anno-
Rel. Type Avg. Agreement # of Commas # of Rel.s
SUBSTITUTE 0.808 243 729
ATTRIBUTE 0.687 193 386
LOCATION 0.929 71 140
LIST 0.803 230 230
OTHER 0.949 909 0
Combined 0.869 1646 1485
Table 1: Average inter-annotator agreement for identify-
ing relations.
tation scheme for such relations, this is beyond the
scope of the present work.
Four annotators annotated the same 10% of the
WSJ sentences in order to evaluate inter-annotator
agreement. The remaining sentences were divided
among the four annotators. The resulting corpus was
checked by two judges and the annotation corrected
where appropriate; if the two judges disagreed, a
third judge was consulted and consensus reached.
Our annotators were asked to identify comma struc-
tures, and for each structure to write its relation type,
its arguments, and all possible simplified version(s)
of the original sentence in which the relation implied
by the comma has been removed. Arguments must
be contiguous units of the sentence and will be re-
ferred to as chunks hereafter. Agreement statistics
and the number of commas and relations of each
type are shown in Table 4. The Accuracy closely ap-
proximates Kappa score in this case, since the base-
line probability of chance agreement is close to zero.
5 A Sentence Tranformation Rule Learner
(ASTRL)
In this section, we describe a new machine learning
system that learns Sentence Transformation Rules
(STRs) for comma resolution. We first define the
hypothesis space (i.e., STRs) and two operations ?
substitution and introduction. We then define the
feature space, motivating the use of Syntactic Parse
annotation to learn STRs. Finally, we describe the
ASTRL algorithm.
5.1 Sentence Transformation Rules
A Sentence Transformation Rule (STR) takes a
parse tree as input and generates new sentences. We
formalize an STR as the pair l ? r, where l is a
tree fragment that can consist of non-terminals, POS
tags and lexical items. r is a set {ri}, each ele-
ment of which is a template that consists of the non-
1033
terminals of l and, possibly, some new tokens. This
template is used to generate a new sentence, called a
relation.
The process of applying an STR l ? r to a parse
tree T of a sentence s begins with finding a match for
l in T . A match is said to be found if l is a subtree
of T . If matched, the non-terminals of each ri are
instantiated with the terminals that they cover in T .
Instantiation is followed by generation of the output
relations in one of two ways: introduction or sub-
stitution, which is specified by the corresponding ri.
If an ri is marked as an introductory one, then the
relation is the terminal sequence obtained by replac-
ing the non-terminals in ri with their instantiations.
For substitution, firstly, the non-terminals of the ri
are replaced by their instantiations. The instantiated
ri replaces all the terminals in s that are covered by
the l-match. The notions of introduction and substi-
tution were motivated by ideas introduced in (Bar-
Haim et al, 2007).
Figure 1 shows an example of an STR and Figure
2 shows the application of this STR to a sentence. In
the first relation, NP1 and NP2 are instantiated with
the corresponding terminals in the parse tree. In the
second and third relations, the terminals of NP1 and
NP2 replace the terminals covered by NPp.
LHS: NPp
NP1 , NP2 ,
RHS:
1. NP1 be NP2 (introduction)
2. NP1 (substitution)
3. NP2 (substitution)
Figure 1: Example of a Sentence Transformation Rule. If
the LHS matches a part of a given parse tree, then the
RHS will generate three relations.
5.2 The Feature Space
In Section 2, we discussed the example where there
could be an ambiguity between a list and an apposi-
tion structure in the fragment two former Soviet re-
publics, Russia and China. In addition, simple sur-
face examination of the sentence could also identify
the noun phrases ?Shanghai Co-operation Organi-
zation (SCO)?, ?the fledgling association that binds
S
NPp
NP1
John Smith
, NP2
a renaissance
artist
,
V P
was
famous
RELATIONS:
1 [John Smith]/NP1 be [a renaissance artist]/NP2
2 [John Smith] /NP1 [was famous]
3 [a renaissance artist]/NP2 [was famous]
Figure 2: Example of application of the STR in Figure 1.
In the first relation, an introduction, we use the verb ?be?,
without dealing with its inflections. NP1 and NP2 are
both substitutions, each replacing NPp to generate the
last two relations.
two former Soviet Republics?, ?Russia? and ?China?
as the four members of a list. To resolve such ambi-
guities, we need a nested representation of the sen-
tence. This motivates the use of syntactic parse trees
as a logical choice of feature space. (Note, however,
that semantic and pragmatic ambiguities might still
remain.)
5.3 Algorithm Overview
In our corpus annotation, the relations and their ar-
gument boundaries (chunks) are explicitly marked.
For each training example, our learning algorithm
first finds the smallest valid STR ? the STR with the
smallest LHS in terms of depth. Then it refines the
LHS by specializing it using statistics taken from
the entire data set.
5.4 Generating the Smallest Valid STR
To transform an example into the smallest valid
STR, we utilize the augmented parse tree of the
sentence. For each chunk in the sentence, we find
the lowest node in the parse tree that covers the
chunk and does not cover other chunks (even par-
tially). It may, however, cover words that do not
belong to any chunk. We refer to such a node as
a chunk root. We then find the lowest node that cov-
ers all the chunk roots, referring to it as the pat-
tern root. The initial LHS consists of the sub-
tree of the parse tree rooted at the pattern root and
whose leaf nodes are all either chunk roots or nodes
that do not belong to any chunk. All the nodes are
labeled with the corresponding labels in the aug-
1034
mented parse tree. For example, if we consider the
parse tree and relations shown in Figure 2, then do-
ing the above procedure gives us the initial LHS
as S (NPp(NP1, NP2, ) V P ). The three relations
gives us the RHS with three elements ?NP1 be
NP2?, ?NP1 V P ? and ?NP1 V P ?, all three being
introduction.
This initial LHS need not be the smallest one that
explains the example. So, we proceed by finding the
lowest node in the initial LHS such that the sub-
tree of the LHS at that node can form a new STR
that covers the example using both introduction and
substitution. In our example, the initial LHS has a
subtree, NPp(NP1, NP2, ) that can cover all the re-
lations with the RHS consisting of ?NP1 be NP2?,
NP1 and NP2. The first RHS is an introduction,
while the second and the third are both substitutions.
Since no subtree of this LHS can generate all three
relations even with substitution, this is the required
STR. The final step ensures that we have the small-
est valid STR at this stage.
5.5 Statistical Refinement
The STR generated using the procedure outlined
above explains the relations generated by a single
example. In addition to covering the relations gen-
erated by the example, we wish to ensure that it does
not cover erroneous relations by matching any of the
other comma types in the annotated data.
Algorithm 1 ASTRL: A Sentence Transformation
Rule Learning.
1: for all t: Comma type do
2: Initialize STRList[t] = ?
3: p = Set of annotated examples of type t
4: n = Annotated examples of all other types
5: for all x ? p do
6: r = Smallest Valid STR that covers x
7: Get fringe of r.LHS using the parse tree
8: S = Score(r,p,n)
9: Sprev = ??
10: while S 6= Sprev do
11: if adding some fringe node to r.LHS causes a signifi-
cant change in score then
12: Set r = New rule that includes that fringe node
13: Sprev = S
14: S = Score(r,p,n)
15: Recompute new fringe nodes
16: end if
17: end while
18: Add r to STRList[t]
19: Remove all examples from p that are covered by r
20: end for
21: end for
For this purpose, we specialize the LHS so that it
covers as few examples from the other comma types
as possible, while covering as many examples from
the current comma type as possible. Given the most
general STR, we generate a set of additional, more
detailed, candidate rules. Each of these is obtained
from the original rule by adding a single node to
the tree pattern in the rule?s LHS, and updating the
rule?s RHS accordingly. We then score each of the
candidates (including the original rule). If there is
a clear winner, we continue with it using the same
procedure (i.e., specialize it). If there isn?t a clear
winner, we stop and use the current winner. After
finishing with a rule (line 18), we remove from the
set of positive examples of its comma type all exam-
ples that are covered by it (line 19).
To generate the additional candidate rules that we
add, we define the fringe of a rule as the siblings
and children of the nodes in its LHS in the original
parse tree. Each fringe node defines an additional
candidate rule, whose LHS is obtained by adding
the fringe node to the rule?s LHS tree. We refer to
the set of these candidate rules, plus the original one,
as the rule?s fringe rules. We define the score of an
STR as
Score(Rule,p,n) = Rp|p| ?
Rn
|n|
where p and n are the set of positive and negative
examples for this comma type, and Rp and Rn are
the number of positive and negative examples that
are covered by the STR. For each example, all exam-
ples annotated with the same comma type are pos-
itive while all examples of all other comma types
are negative. The score is used to select the win-
ner among the fringe rules. The complete algorithm
we have used is listed in Algorithm 1. For conve-
nience, the algorithm?s main loop is given in terms
of comma types, although this is not strictly nec-
essary. The stopping criterion in line 11 checks
whether any fringe rule has a significantly better
score than the rule it was derived from, and exits the
specialization loop if there is none.
Since we start with the smallest STR, we only
need to add nodes to it to refine it and never have
to delete any nodes from the tree. Also note that the
algorithm is essentially a greedy algorithm that per-
forms a single pass over the examples; other, more
1035
complex, search strategies could also be used.
6 Evaluation
6.1 Experimental Setup
To evaluate ASTRL, we used the WSJ derived cor-
pus. We experimented with three scenarios; in two
of them we trained using the gold standard trees
and then tested on gold standard parse trees (Gold-
Gold), and text annotated using a state-of-the-art sta-
tistical parser (Charniak and Johnson, 2005) (Gold-
Charniak), respectively. In the third, we trained and
tested on the Charniak Parser (Charniak-Charniak).
In gold standard parse trees the syntactic cate-
gories are annotated with functional tags. Since cur-
rent statistical parsers do not annotate sentences with
such tags, we augment the syntactic trees with the
output of a Named Entity tagger. For the Named
Entity information, we used a publicly available NE
Recognizer capable of recognizing a range of cat-
egories including Person, Location and Organiza-
tion. On the CoNLL-03 shared task, its f-score is
about 90%4. We evaluate our system from different
points of view, as described below. For all the eval-
uation methods, we performed five-fold cross vali-
dation and report the average precision, recall and
f-scores.
6.2 Relation Extraction Performance
Firstly, we present the evaluation of the performance
of ASTRL from the point of view of relation ex-
traction. After learning the STRs for the different
comma types using the gold standard parses, we
generated relations by applying the STRs on the test
set once. Table 2 shows the precision, recall and
f-score of the relations, without accounting for the
comma type of the STR that was used to generate
them. This metric, called the Relation metric in fur-
ther discussion, is the most relevant one from the
point of view of the TE task. Since a list does not
generate any relations in our annotation scheme, we
use the commas to identify the list elements. Treat-
ing each list in a sentence as a single relation, we
score the list with the fraction of its correctly identi-
fied elements.
In addition to the Gold-Gold and Gold-Charniak
4A web demo of the NER is at http://L2R.cs.uiuc.
edu/
?
cogcomp/demos.php.
settings described above, for this metric, we also
present the results of the Charniak-Charniak setting,
where both the train and test sets were annotated
with the output of the Charniak parser. The improve-
ment in recall in this setting over the Gold-Charniak
case indicates that the parser makes systematic er-
rors with respect to the phenomena considered.
Setting P R F
Gold-Gold 86.1 75.4 80.2
Gold-Charniak 77.3 60.1 68.1
Charniak-Charniak 77.2 64.8 70.4
Table 2: ASTRL performance (precision, recall and f-
score) for relation extraction. The comma types were
used only to learn the rules. During evaluation, only the
relations were scored.
6.3 Comma Resolution Performance
We present a detailed analysis of the performance of
the algorithm for comma resolution. Since this paper
is the first one that deals with the task, we could not
compare our results to previous work. Also, there
is no clear baseline to use. We tried a variant of
the most frequent baseline common in other disam-
biguation tasks, in which we labeled all commas as
OTHER (the most frequent type) except when there
are list indicators like and, or and but in adjacent
chunks (which are obtained using a shallow parser),
in which case the commas are labeled LIST. This
gives an average precision 0.85 and an average recall
of 0.36 for identifying the comma type. However,
this baseline does not help in identifying relations.
We use the following approach to evaluate the
comma type resolution and relation extraction per-
formance ? a relation extracted by the system is con-
sidered correct only if both the relation and the type
of the comma structure that generated it are correctly
identified. We call this metric the Relation-Type
metric. Another way of measuring the performance
of comma resolution is to measure the correctness of
the relations per comma type. In both cases, lists are
scored as in the Relation metric. The performance of
our system with respect to these two metrics are pre-
sented in Table 3. In this table, we also compare the
performance of the STRs learned by ASTRL with
the smallest valid STRs without further specializa-
tion (i.e., using just the procedure outlined in Sec-
tion 5.4).
1036
Type Gold-Gold Setting Gold-Charniak Setting
Relation-Type metric
Smallest Valid STRs ASTRL Smallest Valid STRs ASTRL
P R F P R F P R F P R F
Total 66.2 76.1 70.7 81.8 73.9 77.6 61.0 58.4 59.5 72.2 59.5 65.1
Relations Metric, Per Comma Type
ATTRIBUTE 40.4 68.2 50.4 70.6 59.4 64.1 35.5 39.7 36.2 56.6 37.7 44.9
SUBSTITUTE 80.0 84.3 81.9 87.9 84.8 86.1 75.8 72.9 74.3 78.0 76.1 76.9
LIST 70.9 58.1 63.5 76.2 57.8 65.5 58.7 53.4 55.6 65.2 53.3 58.5
LOCATION 93.8 86.4 89.1 93.8 86.4 89.1 70.3 37.2 47.2 70.3 37.2 47.2
Table 3: Performance of STRs learned by ASTRL and the smallest valid STRs in identifying comma types and
generating relations.
There is an important difference between the Re-
lation metric (Table 2) and the Relation-type met-
ric (top part of Table 3) that depends on the seman-
tic interpretation of the comma types. For example,
consider the sentence ?John Smith, 59, went home.?
If the system labels the commas in this as both AT-
TRIBUTE and SUBSTITUTE, then, both will gener-
ate the relation ?John Smith is 59.? According to
the Relation metric, there is no difference between
them. However, there is a semantic difference be-
tween the two sentences ? the ATTRIBUTE relation
says that being 59 is an attribute of John Smith while
the SUBSTITUTE relation says that John Smith is the
number 59. This difference is accounted for by the
Relation-Type metric.
From this standpoint, we can see that the special-
ization step performed in the full ASTRL algorithm
greatly helps in disambiguating between the AT-
TRIBUTE and SUBSTITUTE types and consequently,
the Relation-Type metric shows an error reduction
of 23.5% and 13.8% in the Gold-Gold and Gold-
Charniak settings respectively. In the Gold-Gold
scenario the performance of ASTRL is much better
than in the Gold-Charniak scenario. This reflects the
non-perfect performance of the parser in annotating
these sentences (parser F-score of 90%).
Another key evaluation question is the per-
formance of the method in identification of the
OTHER category. A comma is judged to be as
OTHER if no STR in the system applies to it.
The performance of ASTRL in this aspect is pre-
sented in Table 4. The categorization of this cate-
gory is important if we wish to further classify the
OTHER commas into finer categories.
Setting P R F
Gold-Gold 78.9 92.8 85.2
Gold-Charniak 72.5 92.2 81.2
Table 4: ASTRL performance (precision, recall and f-
score) for OTHER identification.
7 Conclusions
We defined the task of comma resolution, and devel-
oped a novel machine learning algorithm that learns
Sentence Transformation Rules to perform this task.
We experimented with both gold standard and parser
annotated sentences, and established a performance
level that seems good for a task of this complexity,
and which will provide a useful measure of future
systems developed for this task. When given au-
tomatically parsed sentences, performance degrades
but is still much higher than random, in both sce-
narios. We designed a comma annotation scheme,
where each comma unit is assigned one of four types
and an inference rule mapping the patterns of the
unit with the entailed relations. We created anno-
tated datasets which will be made available over the
web to facilitate further research.
Future work will investigate four main directions:
(i) studying the effects of inclusion of our approach
on the performance of Textual Entailment systems;
(ii) using features other than those derivable from
syntactic parse and named entity annotation of the
input sentence; (iii) recognizing a wider range of im-
plicit relations, represented by commas and in other
ways; (iv) adaptation to other domains.
Acknowledgement
The UIUC authors were supported by NSF grant
ITR IIS-0428472, DARPA funding under the Boot-
strap Learning Program and a grant from Boeing.
1037
References
M. Banko, M. Cafarella, M. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proc. of IJCAI, pages 2670?2676.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic level.
In Proc. of AAAI, pages 871?876.
M. Bayraktar, B. Say, and V. Akman. 1998. An analysis
of english punctuation: The special case of comma.
International Journal of Corpus Linguistics, 3(1):33?
57.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
of the Annual Meeting of the ACL, pages 173?180.
A. Culotta and J. Sorenson. 2004. Dependency tree ker-
nels for relation extraction. In Proc. of the Annual
Meeting of the ACL, pages 423?429.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In Proc. of the Annual Meeting of the
ACL.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Proc. of the Annual Meeting
of the ACL, pages 232?239.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for seman-
tic entailment in natural language. In Proc. of AAAI,
pages 1678?1679.
T. Hirano, Y. Matsuo, and G. Kikui. 2007. Detecting
semantic relations between named entities in text using
contextual features. In Proc. of the Annual Meeting of
the ACL, pages 157?160.
D. Lin and P. Pantel. 2001. DIRT: discovery of inference
rules from text. In Proc. of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining 2001,
pages 323?328.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
G. Nunberg. 1990. CSLI Lecture Notes 18: The Lin-
guistics of Punctuation. CSLI Publications, Stanford,
CA.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Proc. of the Annual Meeting of the
ACL, pages 113?120.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proc. of the Annual Meeting of
the ACL, pages 809?816.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 1?8. Association for
Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
B. Say and V. Akman. 1997. Current approaches to
punctuation in computational linguistics. Computers
and the Humanities, 30(6):457?469.
S. Sekine. 2006. On-demand information extraction. In
Proc. of the Annual Meeting of the ACL, pages 731?
738.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based of entailment relations. In Proc. of
EMNLP, pages 49?56.
S. van Delden and F. Gomez. 2002. Combining finite
state automata and a greedy learning algorithm to de-
termine the syntactic roles of commas. In Proc. of IC-
TAI, pages 293?300.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research, 3:1083?1106.
1038
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 129?139,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Joint Model for Extended Semantic Role Labeling
Vivek Srikumar and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL 61801
{vsrikum2, danr}@illinois.edu
Abstract
This paper presents a model that extends se-
mantic role labeling. Existing approaches in-
dependently analyze relations expressed by
verb predicates or those expressed as nominal-
izations. However, sentences express relations
via other linguistic phenomena as well. Fur-
thermore, these phenomena interact with each
other, thus restricting the structures they artic-
ulate. In this paper, we use this intuition to
define a joint inference model that captures
the inter-dependencies between verb seman-
tic role labeling and relations expressed us-
ing prepositions. The scarcity of jointly la-
beled data presents a crucial technical chal-
lenge for learning a joint model. The key
strength of our model is that we use existing
structure predictors as black boxes. By en-
forcing consistency constraints between their
predictions, we show improvements in the per-
formance of both tasks without retraining the
individual models.
1 Introduction
The identification of semantic relations between
sentence constituents has been an important task in
NLP research. It finds applications in various natural
language understanding tasks that require complex
inference going beyond the surface representation.
In the literature, semantic role extraction has been
studied mostly in the context of verb predicates, us-
ing the Propbank annotation of Palmer et al (2005),
and also for nominal predicates, using the Nombank
corpus of Meyers et al (2004).
However, sentences express semantic relations
through other linguistic phenomena. For example,
consider the following sentence:
(1) The field goal by Brien changed the game in the
fourth quarter.
Verb centered semantic role labeling would identify
the arguments of the predicate change as (a) The
field goal by Brien (A0, the causer of the change),
(b) the game (A1, the thing changing), and (c) in
the fourth quarter (temporal modifier). However,
this does not tell us that the scorer of the field goal
was Brien, which is expressed by the preposition by.
Also, note that the in indicates a temporal relation,
which overlaps with the verb?s analysis.
In this paper, we propose an extension of the stan-
dard semantic role labeling task to include relations
expressed by lexical items other than verbs and nom-
inalizations. Further, we argue that there are interac-
tions between the different phenomena which sug-
gest that there is a benefit in studying them together.
However, one key challenge is that large jointly la-
beled corpora do not exist. This motivates the need
for novel learning and inference schemes that ad-
dress the data problem and can still benefit from the
interactions among the phenomena.
This paper has two main contributions.
1. From the machine learning standpoint, we pro-
pose a joint inference scheme to combine exist-
ing structure predictors for multiple linguistic
phenomena. We do so using hard constraints
that involve only the labels of the phenomena.
The strength of our model is that it is easily
129
extensible, since adding new phenomena does
not require fully retraining the joint model from
scratch. Furthermore, our approach minimizes
the need for extensive jointly labeled corpora
and, instead, uses existing predictors as black
boxes.
2. From an NLP perspective, we motivate the ex-
tension of semantic role labeling beyond verbs
and nominalizations. We instantiate our joint
model for the case of extracting preposition and
verb relations together. Our model uses exist-
ing systems that identify verb semantic roles
and preposition object roles and jointly pre-
dicts the output of the two systems in the pres-
ence of linguistic constraints that enforce co-
herence between the predictions. We show that
using constraints to combine models improves
the performance on both tasks. Furthermore,
since the constraints depend only on the labels
of the two tasks and not on any specific dataset,
our experiments also demonstrate that enforc-
ing them allows for better domain adaptation.
The rest of the paper is organized as follows: We
motivate the need for extending semantic role label-
ing and the necessity for joint inference in Section 2.
In Section 3, we describe the component verb SRL
and preposition role systems. The global model is
defined in Section 4. Section 5 provides details on
the coherence constraints we use and demonstrates
the effectiveness of the joint model through experi-
ments. Section 6 discusses our approach in compar-
ison to existing work and Section 7 provides con-
cluding remarks.
2 Problem Definition and Motivation
Semantic Role Labeling has been extensively stud-
ied in the context of verbs and nominalizations.
While this analysis is crucial to understanding a
sentence, it is clear that in many natural language
sentences, information is conveyed via other lexi-
cal items. Consider, for example, the following sen-
tences:
(2) Einstein?s theory of relativity changed physics.
(3) The plays of Shakespeare are widely read.
(4) The bus, which was heading for Nairobi
in Kenya, crashed in the Kabale district of
Uganda.
The examples contain information that cannot be
captured by analyzing the verbs and the nominaliza-
tions. In sentence (2), the possessive form tells us
that the theory of relativity was discovered by Ein-
stein. Furthermore, the theory is on the subject of
relativity. The usage of the preposition of is dif-
ferent in sentence (3), where it indicates a creator-
creation relationship. In the last sentence, the same
preposition tells us that the Kabale district is located
in Uganda. Prepositions, compound nouns, posses-
sives, adjectival forms and punctuation marks of-
ten express relations, the identification of which is
crucial for text understanding tasks like recognizing
textual entailment, paraphrasing and question an-
swering.
The relations expressed by different linguistic
phenomena often overlap. For example, consider the
following sentence:
(5) Construction of the library began in 1968.
The relation expressed by the nominalization con-
struction recognizes the library as the argument of
the predicate construct. However, the same analy-
sis can also be obtained by identifying the sense of
the preposition of, which tells us that the subject of
the preposition is a nominalization of the underlying
verb. A similar redundancy can be observed with
analyses of the verb began and the preposition in.
The above example motivates the following key in-
tuition: The correct interpretation of a sentence is
the one that gives a consistent analysis across all
the linguistic phenomena expressed in it.
An inference mechanism that simultaneously pre-
dicts the structure for different phenomena should
account for consistency between the phenomena. A
model designed to address this has the following
desiderata:
1. It should account for the dependencies between
phenomena.
2. It should be extensible to allow easy addition of
new linguistic phenomena.
130
3. It should be able to leverage existing state-of-
the-art models with minimal use of jointly la-
beled data, which is expensive to obtain.
Systems that are trained on each task indepen-
dently do not account for the interplay between
them. One approach for tackling this is to define
pipelines, where the predictions for one of the tasks
acts as the input for another. However, a pipeline
does not capture the two-way dependency between
the tasks. Training a fully joint model from scratch
is also unrealistic because it requires text that is an-
notated with all the tasks, thus making joint train-
ing implausible from a learning theoretic perspective
(See Punyakanok et al (2005) for a discussion about
the learning theoretic requirements of joint training.)
3 Tasks and Individual Systems
Before defining our proposed model that captures
the requirements listed in the previous section, we
introduce the tasks we consider and their indepen-
dently trained systems that we improve using the
joint system. Though the model proposed here is
general and can be extended to several linguistic
phenomena, in this paper, we focus on relations ex-
pressed by verbs and prepositions. This section de-
scribes the tasks, the data sets we used for our exper-
iments and the current state-of-the-art systems for
these tasks.
We use the following sentence as our running ex-
ample to illustrate the phenomena: The company
calculated the price trends on the major stock mar-
kets on Monday.
3.1 Preposition Relations
Prepositions indicate a relation between the attach-
ment point of the preposition and its object. As we
have seen, the same preposition can indicate dif-
ferent types of relations. In the literature, the pol-
ysemy of prepositions is addressed by The Prepo-
sition Project1 of Litkowski and Hargraves (2005),
which is a large lexical resource for English that la-
bels prepositions with their sense. This sense inven-
tory formed the basis of the SemEval-2007 task of
preposition word sense disambiguation of Litkowski
and Hargraves (2007). In our example, the first on
1http://www.clres.com/prepositions.html
would be labeled with the sense 8(3) which identifies
the object of the preposition as the topic, while the
second instance would be labeled as 17(8), which
indicates that argument is the day of the occurrence.
The preposition sense inventory, while useful to
identify the fine grained distinctions between prepo-
sition usage, defines a unique sense label for each
preposition by indexing the definitions of the prepo-
sitions in the Oxford Dictionary of English. For ex-
ample, in the phrase at noon, the at would be labeled
with the sense 2(2), while the preposition in I will
see you in an hour will be labeled 4(3). Note that
both these (and also the second on in our running ex-
ample) indicate a temporal relation, but are assigned
different labels based on the preposition. To counter
this problem we collapsed preposition senses that
are semantically similar to define a new label space,
which we refer to as Preposition Roles.
We retrained classifiers for preposition sense for
the new label space. Before describing the prepo-
sition role dataset, we briefly describe the datasets
and the features for the sense problem. The best
performing system at the SemEval-2007 shared task
of preposition sense disambiguation (Ye and Bald-
win (2007)) achieves a mean precision of 69.3% for
predicting the fine grained senses. Tratz and Hovy
(2009) and Hovy et al (2010) attained significant
improvements in performance using features derived
from the preposition?s neighbors in the parse tree.
We extended the feature set defined in the former
for our independent system. Table 1 summarizes the
rules for identifying the syntactically related words
for each preposition. We used dependencies from
the easy-first dependency parser of Goldberg and El-
hadad (2010).
For each word extracted from these rules, the fea-
tures include the word itself, its lemma, the POS
tag, synonyms and hypernyms of the first WordNet
sense and an indicator for capitalization. These fea-
tures improved the accuracy of sense identification
to 75.1% on the SemEval test set. In addition, we
also added the following new features for each word:
1. Indicators for gerunds and nominalizations of
verbs.
2. The named entity tag (Person, Location or Or-
ganization) associated with a word, if any. We
131
Id. Feature
1. Head noun/verb that dominates the
preposition along with its modifiers
2. Head noun/verb that is dominated by
the preposition along with its modifiers
3. Subject, negator and object(s) of the
immediately dominating verb
4. Heads of sibling prepositions
5. Words withing a window of 5 centered
at the preposition
Table 1: Features for preposition relation from Tratz and
Hovy (2009). These rules were used to identify syntacti-
cally related words for each preposition.
used the state-of-the-art named entity tagger of
Ratinov and Roth (2009) to label the text.
3. Gazetteer features, which are active if a word is
a part of a phrase that belongs to a gazetteer list.
We used the gazetteer lists which were used
by the NER system. We also used the CBC
word clusters of Pantel and Lin (2002) as ad-
ditional gazetteers and Brown cluster features
as used by Ratinov and Roth (2009) and Koo et
al. (2008).
Dahlmeier et al (2009) annotated senses for the
prepositions at, for, in, of, on, to and with in the sec-
tions 2-4 and 23 of the Wall Street Journal portion of
the Penn Treebank2. We trained sense classifiers on
both datasets using the Averaged Perceptron algo-
rithm with the one-vs-all scheme using the Learning
Based Java framework of Rizzolo and Roth (2010)3.
Table 2 reports the performance of our sense disam-
biguation systems for the Treebank prepositions.
As mentioned earlier, we collapsed the sense la-
bels onto the newly defined preposition role labels.
Table 3 shows this label set alng with frequencies
of the labels in the Treebank dataset. According to
this labeling scheme, the first on in our running ex-
ample will be labeled TOPIC and the second one will
2This dataset does not annotate all prepositions and re-
stricts itself mainly to prepositions that start a Propbank ar-
gument. The data is available at http://nlp.comp.nus.
edu.sg/corpora
3Learning Based Java can be downloaded from http://
cogcomp.cs.illinois.edu.
Test set
Train Treebank Sec. 23 SemEval
Penn Treebank 61.41 38.22
SemEval 47.00 78.25
Table 2: Preposition sense performance. This table re-
ports accuracy of sense prediction on the prepositions that
have been annotated for the Penn Treebank dataset.
Role Train Test
ACTIVITY 57 23
ATTRIBUTE 119 51
BENEFICIARY 78 17
CAUSE 255 116
CONCOMITANT 156 74
ENDCONDITION 88 66
EXPERIENCER 88 42
INSTRUMENT 37 19
LOCATION 1141 414
MEDIUMOFCOMMUNICATION 39 30
NUMERIC/LEVEL 301 174
OBJECTOFVERB 365 112
OTHER 65 49
PARTWHOLE 485 133
PARTICIPANT/ACCOMPANIER 122 58
PHYSICALSUPPORT 32 18
POSSESSOR 195 56
PROFESSIONALASPECT 24 10
RECIPIENT 150 70
SPECIES 240 58
TEMPORAL 582 270
TOPIC 148 54
Table 3: Preposition role data statistics for the Penn Tree-
bank preposition dataset.
be labeled TEMPORAL4. We re-trained the sense
disambiguation system to predict preposition roles.
When trained on the Treebank data, our system at-
tains an accuracy of 67.82% on Section 23 of the
Treebank. We use this system as our independent
baseline for preposition role identification.
3.2 Verb SRL
The goal of verb Semantic Role Labeling (SRL)
is to identify the predicate-argument structure de-
fined by verbs in sentences. The CoNLL Shared
Tasks of 2004 and 2005 (See Carreras and Ma`rquez
4The mapping from the preposition senses to the roles de-
fines a new dataset and is available for download at http:
//cogcomp.cs.illinois.edu/.
132
(2004), Carreras and Ma`rquez (2005)) studied the
identification of the predicate-argument structure of
verbs using the PropBank corpus of Palmer et al
(2005). Punyakanok et al (2008) and Toutanova et
al. (2008) used global inference to ensure that the
predictions across all arguments of the same predi-
cate are coherent. We re-implemented the system of
Punyakanok et al (2008), which we briefly describe
here, to serve as our baseline verb semantic role la-
beler 5. We refer the reader to the original paper for
further details.
The verb SRL system of Punyakanok et al (2008)
consists of four stages ? candidate generation, argu-
ment identification, argument classification and in-
ference. The candidate generation stage involves us-
ing the heuristic of Xue and Palmer (2004) to gener-
ate an over-complete set of argument candidates for
each predicate. The identification stage uses a clas-
sifier to prune the candidates. In the argument clas-
sification step, the candidates that remain after the
identification step are assigned scores for the SRL
arguments using a multiclass classifier. One of the
labels of the classifier is ?, which indicates that the
candidate is, in fact, not an argument. The inference
step produces a combined prediction for all argu-
ment candidates of a verb proposition by enforcing
global constraints.
The inference enforces the following structural
and linguistic constraints: (1) Each candidate can
have at most one label. (2) No duplicate core argu-
ments. (3) No overlapping or embedding arguments.
(4) Given the predicate, some argument classes are
illegal. (5) If a candidate is labeled as an R-arg,
then there should be one labeled as arg. (6) If a
candidate is labeled as a C-arg, there should be one
labeled arg that occurs before the C-arg.
Instead of using the identifier to filter candidates
for the classifier, in our SRL system, we added
the identifier to the global inference and enforced
consistency constraints between the identifier and
the argument classifier predictions ? the identifier
should predict that a candidate is an argument if,
and only if, the argument classifier does not predict
the label ?. This change is in keeping with the idea
of using joint inference to combine independently
5The verb SRL system be downloaded from http://
cogcomp.cs.illinois.edu/page/software
learned systems, in this case, the argument identifier
and the role classifier. Furthermore, we do not need
to explicitly tune the identifier for high recall.
We phrase the inference task as an integer lin-
ear program (ILP) following the approach devel-
oped in Roth and Yih (2004). Integer linear pro-
grams were used by Roth and Yih (2005) to add gen-
eral constraints for inference with conditional ran-
dom fields. ILPs have since been used successfully
in many NLP applications involving complex struc-
tures ? Punyakanok et al (2008) for semantic role
labeling, Riedel and Clarke (2006) and Martins et al
(2009) for dependency parsing and several others6.
Let vCi,a be the Boolean indicator variable that de-
notes that the ith argument candidate for a predicate
is assigned a label a and let ?Ci,a represent the score
assigned by the argument classifier for this decision.
Similarly, let vIi denote the identifier decision for the
ith argument candidate of the predicate and ?Ii de-
note its identifier score. Then, the objective of infer-
ence is to maximize the total score of the assignment
max
vC ,vI
?
i,a
?Ci,avCi,a +
?
i
?Ii vIi (1)
Here, vC and vI denote all the argument classifier
and identifier variables respectively. This maximiza-
tion is subject to the constraints described above,
which can be transformed to linear (in)equalities.
We denote these constraints as CSRL. In addition
to CSRL which were defined by Punyakanok et al
(2008), we also have the constraints linking the pre-
dictions of the identifier and classifier:
vCv,i,? + vIv,i = 1; ?v, i. (2)
Inference in our baseline SRL system is, thus, the
maximization of the objective defined in (1) sub-
ject to constraints CSRL, the identifier-classifier con-
straints defined in (2) and the restriction of the vari-
ables to take values in {0, 1}.
To train the classifiers, we used parse trees from
the Charniak and Johnson (2005) parser with the
6The primary advantage of using ILP for inference is that
this representation enables us to add arbitrary coherence con-
straints between the phenomena. If the underlying optimization
problem itself is tractable, then so is the corresponding integer
program. However, other approaches to solve the constrained
maximization problem can also be used for inference.
133
same feature representation as in the original sys-
tem. We trained the classifiers on the standard
Propbank training set using the one-vs-all extension
of the average Perceptron algorithm. As with the
preposition roles, we implemented our system using
Learning Based Java of Rizzolo and Roth (2010).
We normalized all classifier scores using the soft-
max function. Compared to the 76.29% F1 score
reported by Punyakanok et al (2008) using single
parse tree predictions from the parser, our system
obtained 76.22% F1 score on section 23 of the Penn
Treebank.
4 A Joint Model for Verbs and
Prepositions
We now introduce our model that captures the needs
identified in Section 2. The approach we develop
in this paper follows the one proposed by Roth and
Yih (2004) of training individual models and com-
bining them at inference time. Our joint model
is a Constrained Conditional Model (See Chang et
al. (2011)), which allows us to build upon existing
learned models using declarative constraints.
We represent our component inference problems
as integer linear program instances. As we saw in
Section 3.2, the inference for SRL is instantiated as
an ILP problem. The problem of predicting prepo-
sition roles can be easily transformed into an ILP
instance. Let vRp,r denote the decision variable that
encodes the prediction that the preposition p is as-
signed a role r and let ?Rp,r denote its score. Let
vR denote all the role variables for a sentence. Then
role prediction is equivalent to the following maxi-
mization problem:
max
vR
?
p,r
?Rp,r ? vRp,r (3)
subj. to ?
r
vRp,r = 1, ?p (4)
vRp,r ? {0, 1}, ?p, r. (5)
In general, let p denote a linguistic structure pre-
diction task of interest and let P denote all such
tasks. Let Zp denote the set of labels that the parts
of the structure associated with phenomenon p can
take. For example, for the SRL argument classifica-
tion component, the parts of the structure are all the
candidates that need to be labeled for a given sen-
tence and the set Zp is the set of all argument labels.
For each phenomenon p ? P , we use vp to denote
its set of inference variables for a given sentence.
Each inference variable vpZ,y ? vp corresponds to
the prediction that the part y has the label Z in the
final structure. Each variable is associated with a
score ?pZ,y that is obtained from a learned score pre-
dictor. Let Cp denote the structural constraints that
are ?local? to the phenomenon. Thus, for verb SRL,
these would be the constraints defined in the previ-
ous section, and for preposition role, the only local
constraint would be the constraint (4) defined above.
The independent inference problem for the phe-
nomenon p is the following integer program:
max
vp
?
Z?Zp
?
vp
vpZ,y ??
p
Z,y, (6)
subj. to Cp(vp), (7)
vpZ,y ? {0, 1}, ?v
p
Z,y. (8)
As a technical point, this defines one inference
problem per sentence, rather than per predicate
as in the verb SRL system of Punyakanok et al
(2008). This simple extension enabled Surdeanu et
al. (2007) to study the impact of incorporating cross-
predicate constraints for verb SRL. In this work, this
extension allows us to incorporate cross-phenomena
inference.
4.1 Joint inference
We consider the problem of jointly predicting sev-
eral phenomena incorporating linguistic knowledge
that enforce consistency between the output labels.
Suppose p1 and p2 are two phenomena. If zp11 is a la-
bel associated with the former and zp21 , zp22 , ? ? ? are
labels associated with the latter, we consider con-
straints of the form
zp11 ? zp21 ? zp22 ? ? ? ? ? zp2n (9)
We expand this language of constraints by allowing
the specification of pre-conditions for a constraint to
apply. This allows us to enforce constraints of the
form ?If an argument that starts with the preposi-
tion ?at? is labeled AM-TMP, then the preposition
can be labeled either NUMERIC/LEVEL or TEMPO-
RAL.? This constraint is universally quantified for
134
all arguments that satisfy the precondition of start-
ing with the preposition at.
Given a first-order constraint in this form and an
input sentence, suppose the inference variable vp11 is
a grounding of zp11 and vp21 , vp22 , ? ? ? are groundings
of the right hand labels such that the preconditions
are satisfied, then the constraint can be phrased as
the following linear inequality.
?vp11 +
?
i
vp2i ? 0
In the context of the preposition role and verb
SRL, we consider constraints between labels for a
preposition and SRL argument candidates that begin
with that preposition. This restriction forms the pre-
condition for all the joint constraints considered in
this paper. Since the joint constraints involve only
the labels, they can be derived either manually from
the definition of the tasks or using statistical rela-
tion learning techniques. In addition to mining con-
straints of the form (9), we also use manually spec-
ified joint constraints. The constraints used in our
experiments are described further in Section 5.
In general, let J denote a set of pairwise joint
constraints. The joint inference problem can be
phrased as that of maximizing the score of the as-
signment subject to the structural constraints of each
phenomenon (Cp) and the joint linguistic constraints
(J). However, since, the individual tasks were not
trained on the same datasets, the scoring functions
need not be in the same numeric scale. In our model,
each labelZ for a phenomenon p is associated with a
scoring function ?pZ,y for a part y. To scale the scor-
ing functions, we associate each label with a param-
eter ?pZ . This gives us the following integer linear
program for joint inference:
max
v
?
p?P
?
Z?Zp
?pZ
(
?
yp
vpZ,y ??
p
Z,y
)
, (10)
subj. to Cp(vp), ?p ? P (11)
J(v), (12)
vpZ,y ? {0, 1}, ?v
p
Z,y. (13)
Here, v is the vector of inference variables which
is obtained by stacking all the inference variables of
each phenomena.
For our experiments, we use a cutting plane solver
to solve the integer linear program as in Riedel
(2009). This allows us to solve the inference prob-
lem without explicitly having to instantiate all the
joint constraints.
4.2 Learning to rescale the individual systems
Given the individual models and the constraints, we
only need to learn the scaling parameters ?pZ . Note
that the number of scaling parameters is the total
number of labels. When we jointly predict verb SRL
and preposition role, we have 22 preposition roles
(from table 3), one SRL identifier label and 54 SRL
argument classifier labels. Thus we learn only 77
parameters for our joint model. This means that we
only need a very small dataset that is jointly anno-
tated with all the phenomena.
We use the Structure Perceptron of Collins (2002)
to learn the scaling weights. Note that for learning
the scaling weights, we need each label to be associ-
ated with a real-valued feature. Given an assignment
of the inference variables v, the value of the feature
corresponding to the label Z of task p is given by the
sum of scores of all parts in the structure for p that
have been assigned this label, i.e. ?
yp
vpZ,y??
p
Z,y. This
feature is computed for the gold and the predicted
structures and is used for updating the weights.
5 Experiments
In this section, we describe our experimental setup
and evaluate the performance of our approach. The
research question addressed by the experiments is
the following: Given independently trained systems
for verb SRL and preposition roles, can their per-
formance be improved using joint inference between
the two tasks? To address this, we report the results
of the following two experiments:
1. First, we compare the joint system against the
baseline systems and with pipelines in both di-
rections. In this setting, both base systems are
trained on the Penn Treebank data.
2. Second, we show that using joint inference can
provide strong a performance gain even when
the underlying systems are trained on different
domains.
In all experiments, we report the F1 measure for
the verb SRL performance using the CoNLL 2005
135
evaluation metric and the accuracy for the preposi-
tion role labeling task.
5.1 Data and Constraints
For both the verb SRL and preposition roles, we
used the first 500 sentences of section 2 of the Penn
Treebank corpus to train our scaling parameters. For
the first set of experiments, we trained our underly-
ing systems on the rest of the available Penn Tree-
bank training data for each task. For the adaptation
experiment, we train the role classifier on the Se-
mEval data (restricted to the same Treebank prepo-
sitions). In both cases, we report performance on
section 23 of the Treebank.
We mined consistency constraints from the sec-
tions 2, 3 and 4 of the Treebank data. As mentioned
in Section 4.1, we considered joint constraints re-
lating preposition roles to verb argument candidates
that start with the preposition. We identified the fol-
lowing types of constraints: (1) For each preposi-
tion, the set of invalid verb arguments and prepo-
sition roles. (2) For each preposition role, the set
of allowed verb argument labels if the role occurred
more than ten times in the data, and (3) For each
verb argument, the set of allowed preposition roles,
similarly with a support of ten. Note that, while the
constraints were obtained from jointly labeled data,
the constraints could be written down because they
encode linguistic intuition about the labels.
The following is a constraint extracted from the
data, which applies to the preposition with:
srlarg(A2) ? prep-role(ATTRIBUTE)
? prep-role(CAUSE)
? prep-role(INSTRUMENT)
? prep-role(OBJECTOFVERB)
? prep-role(PARTWHOLE)
? prep-role(PARTICIPANT/ACCOMPAINER)
? prep-role(PROFESSIONALASPECT).
This constraint says that if any candidate that starts
with with is labeled as an A2, then the preposition
can be labeled only with one of the roles on the right
hand side.
Some of the mined constraints have negated vari-
ables to enforce that a role or an argument label
should not be allowed. These can be similarly con-
verted to linear inequalities. See Rizzolo and Roth
(2010) for a further discussion about converting log-
ical expressions into linear constraints.
In addition to these constraints that were mined
from data, we also enforce the following hand-
written constraints: (1) If the role of a verb at-
tached preposition is labeled TEMPORAL, then there
should be a verb predicate for which this preposi-
tional phrase is labeled AM-TMP. (2) For verb at-
tached prepositions, if the preposition is labeled with
one of ACTIVITY, ENDCONDITION, INSTRUMENT
or PROFESSIONALASPECT, there should be at least
one predicate for which the corresponding preposi-
tional phrase is not labeled ?.
The conversion of the first constraint to a linear
inequality is similar to the earlier cases. For each
of the roles in the second constraint, let r denote a
role variable that assigns the label to some prepo-
sition. Suppose there are n SRL candidates across
all verb predicates begin with that preposition, and
let s1, s2, ? ? ? , sn denote the SRL variables that as-
sign these candidates to the label ?. Then the second
constraint corresponds to the following inequality:
r +
n?
i=1
si ? n
5.2 Results of joint learning
First, we compare our approach to the performance
of the baseline independent systems and to pipelines
in both directions in Table 4. For one pipeline, we
added the prediction of the baseline preposition role
system as an additional feature to both the identifier
and the argument classifier for argument candidates
that start with a preposition. Similarly, for the sec-
ond pipeline, we added the SRL predictions as fea-
tures for prepositions that were the first word of an
SRL argument. In all cases, we performed five-fold
cross validation to train the classifiers.
The results show that both pipelines improve per-
formance. This justifies the need for a joint sys-
tem because the pipeline can improve only one of
the tasks. The last line of the table shows that the
joint inference system improves upon both the base-
lines. We achieve this improvement without retrain-
ing the underlying models, as done in the case of the
pipelines.
On analyzing the output of the systems, we found
that the SRL precision improved by 2.75% but the
136
Setting SRL Preposition Role
(F1) (Accuracy)
Baseline SRL 76.22 ?
Baseline Prep. ? 67.82
Prep. ? SRL 76.84 ?
SRL? Prep. ? 68.55
Joint inference 77.07 68.39
Table 4: Performance of the joint system, compared to
the individual systems and the pipelines. All performance
measures are reported on Section 23 of the Penn Tree-
bank. The verb SRL systems were trained on sections
2-21, while the preposition role classifiers were trained
on sections 2-4. For the joint inference system, the scal-
ing parameters were trained on the first 500 sentences of
section 2, which were held out. All the improvements in
this table are statistically significant at the 0.05 level.
recall decreased by 0.98%, contributing to the over-
all F1 improvement. The decrease in recall is due to
the joint hard constraints that prohibit certain assign-
ments to the variables which would have otherwise
been possible. Note that, for a given sentence, even
if the joint constraints affect only a few argument
candidates directly, they can alter the labels of the
other candidates via the ?local? SRL constraints.
Consider the following example of the system
output which highlights the effect of the constraints.
(6) Weatherford said market conditions led to the
cancellation of the planned exchange.
The independent preposition role system incor-
rectly identifies the to as a LOCATION. The semantic
role labeling component identifies the phrase to the
cancellation of the planned exchange as the A2 of
the verb led. One of the constraints mined from the
data prohibits the label LOCATION for the preposi-
tion to if the argument it starts is labeled A2. This
forces the system to change the preposition label
to the correct one, namely ENDCONDITION. Both
the independent and the joint systems also label the
preposition of as OBJECTOFVERB, which indicates
that the phrase the planned exchange is the object of
the deverbal noun cancellation.
5.3 Effect of constraints on adaptation
Our second experiment compares the performance
of the preposition role classifier that has been trained
on the SemEval dataset with and without joint con-
straints. Note that Table 2 in Section 3, shows
the drop in performance when applying the prepo-
sition sense classifier. We see that the SemEval-
trained preposition role classifier (baseline in the ta-
ble) achieves an accuracy of 53.29% when tested on
the Treebank dataset. Using this classifier jointly
with the verb SRL classifier via joint constraints gets
an improvement of almost 3 percent in accuracy.
Setting Preposition Role
(Accuracy)
Baseline 53.29
Joint inference 56.22
Table 5: Performance of the SemEval-trained preposition
role classifier, when tested on the Treebank dataset with
and without joint inference with the verb SRL system.
The improvement, in this case is statistically significant
at the 0.01 level using the sign test.
The primary reason for this improvement, even
without re-training the classifier, is that the con-
straints are defined using only the labels of the sys-
tems. This avoids the standard adaptation problems
of differing vocabularies and unseen features.
6 Discussion and Related work
Roth and Yih (2004) formulated the problem of ex-
tracting entities and relations as an integer linear
program, allowing them to use global structural con-
straints at inference time even though the component
classifiers were trained independently. In this pa-
per, we use this idea to combine classifiers that were
trained for two different tasks on different datasets
using constraints to encode linguistic knowledge.
In the recent years, we have seen several joint
models that combine two or more NLP tasks . An-
drew et al (2004) studied verb subcategorization
and sense disambiguation of verbs by treating it as
a problem of learning with partially labeled struc-
tures and proposed to use EM to train the joint
model. Finkel and Manning (2009) modeled the task
of named entity recognition together with parsing.
Meza-Ruiz and Riedel (2009) modeled verb SRL,
predicate identification and predicate sense recogni-
tion jointly using Markov Logic. Henderson et al
(2008) was designed for jointly learning to predict
syntactic and semantic dependencies. Dahlmeier et
137
al. (2009) addressed the problem of jointly learning
verb SRL and preposition sense using the Penn Tree-
bank annotation that was introduced in that work.
The key difference between these and the model
presented in this paper lies in the simplicity of our
model and its easy extensibility because it leverages
existing trained systems. Moreover, our model has
the advantage that the complexity of the joint param-
eters is small, hence does not require a large jointly
labeled dataset to train the scaling parameters.
Our approach is conceptually similar to that of
Rush et al (2010), which combined separately
trained models by enforcing agreement using global
inference and solving its linear programming relax-
ation. They applied this idea to jointly predict de-
pendency and phrase structure parse trees and on the
task of predicting full parses together with part-of-
speech tags. The main difference in our approach is
that we treat the scaling problem as a separate learn-
ing problem in itself and train a joint model specifi-
cally for re-scaling the output of the trained systems.
The SRL combination system of Surdeanu et al
(2007) studied the combination of three different
SRL systems using constraints and also by training
secondary scoring functions over the individual sys-
tems. Their approach is similar to the one presented
in this paper in that, unlike standard reranking, as
in Collins (2000), we entertain all possible solutions
during inference, while reranking approaches train
a discriminative scorer for the top-K solutions of
an underlying system. Unlike the SRL combination
system, however, our approach spans multiple phe-
nomena. Moreover, in contrast to their re-scoring
approaches, we do not define joint features drawn
from the predictions of the underlying components
to define our global model.
We consider the tasks verb SRL and preposition
roles and combine their predictions to provide a
richer semantic annotation of text. This approach
can be easily extended to include systems that pre-
dict structures for other linguistic phenomena be-
cause we do not retrain the underlying systems. The
semantic relations can be enriched by incorporating
more linguistic phenomena such as nominal SRL,
defined by the Nombank annotation scheme of Mey-
ers et al (2004), the preposition function analysis
of O?Hara and Wiebe (2009) and noun compound
analysis as defined by Girju (2007) and Girju et al
(2009) and others. This presents an exciting direc-
tion for future work.
7 Conclusion
This paper presents a strategy for extending seman-
tic role labeling without the need for extensive re-
training or data annotation. While standard seman-
tic role labeling focuses on verb and nominal re-
lations, sentences can express relations using other
lexical items also. Moreover, the different relations
interact with each other and constrain the possible
structures that they can take. We use this intuition
to define a joint model for inference. We instanti-
ate our model using verb semantic role labeling and
preposition role labeling and show that, using lin-
guistic constraints between the tasks and minimal
joint learning, we can improve the performance of
both tasks. The main advantage of our approach
is that we can use existing trained models without
re-training them, thus making it easy to extend this
work to include other linguistic phenomena.
Acknowledgments
The authors thank the members of the Cognitive
Computation Group at the University of Illinois for
insightful discussions and the anonymous reviewers
for valuable feedback.
This research is supported by the Defense Ad-
vanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-
09-C-0181. Any opinions, ndings, and conclusion
or recommendations expressed in this material are
those of the authors and do not necessarily reect the
view of the DARPA, AFRL, or the US government.
References
G. Andrew, T. Grenager, and C. D. Manning. 2004.
Verb sense and subcategorization: Using joint infer-
ence to improve performance on complementary tasks.
In Proceedings of EMNLP.
X. Carreras and L. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared tasks: Semantic role labeling. In
Proceedings of CoNLL-2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-2005.
138
M. Chang, L. Ratinov, and D. Roth. 2011. Structured
learning with constrained conditional models. Ma-
chine Learning (To appear).
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
ACL.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In ICML.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In EMNLP.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources and
Evaluation.
R. Girju. 2007. Improving the interpretation of noun
phrases with cross-linguistic information. In ACL.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In NAACL.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A
latent variable model of synchronous parsing for syn-
tactic and semantic dependencies. In CoNLL.
D. Hovy, S. Tratz, and E. Hovy. 2010. What?s in a prepo-
sition? dimensions of sense disambiguation for an in-
teresting word class. In Coling 2010: Posters.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
K. Litkowski and O. Hargraves. 2005. The preposition
project. In Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of Preposi-
tions and their Use in Computational Linguistics For-
malisms and Applications.
K. Litkowski and O. Hargraves. 2007. Semeval-2007
task 06: Word-sense disambiguation of prepositions.
In SemEval-2007: 4th International Workshop on Se-
mantic Evaluations.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation.
I Meza-Ruiz and S. Riedel. 2009. Jointly identifying
predicates, arguments and senses using markov logic.
In NAACL.
T. O?Hara and J. Wiebe. 2009. Exploiting semantic role
resources for preposition disambiguation. Computa-
tional Linguistics, 35(2), June.
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In The Eighth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In IJ-
CAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
S. Riedel. 2009. Cutting plane map inference for markov
logic. In SRL 2009.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Language Re-
sources and Evaluation.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
CoNLL.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP. Association for Computational Linguistics.
M. Surdeanu, L. Ma`rquez, X. Carreras, and P. R. Comas.
2007. Combination strategies for semantic role label-
ing. J. Artif. Int. Res., 29:105?151, June.
K. Toutanova, A. Haghighi, and C. D. Manning. 2008. A
global joint model for semantic role labeling. Compu-
tational Linguistics, 34(2).
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features.
In NAACL: Student Research Workshop and Doctoral
Consortium.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In EMNLP.
P. Ye and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Features.
In SemEval-2007.
139
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1114?1124, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
On Amortizing Inference Cost for Structured Prediction
Vivek Srikumar? and Gourab Kundu? and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801
{vsrikum2, kundu2, danr}@illinois.edu
Abstract
This paper deals with the problem of predict-
ing structures in the context of NLP. Typically,
in structured prediction, an inference proce-
dure is applied to each example independently
of the others. In this paper, we seek to op-
timize the time complexity of inference over
entire datasets, rather than individual exam-
ples. By considering the general inference
representation provided by integer linear pro-
grams, we propose three exact inference the-
orems which allow us to re-use earlier solu-
tions for certain instances, thereby completely
avoiding possibly expensive calls to the infer-
ence procedure. We also identify several ap-
proximation schemes which can provide fur-
ther speedup. We instantiate these ideas to the
structured prediction task of semantic role la-
beling and show that we can achieve a speedup
of over 2.5 using our approach while retain-
ing the guarantees of exactness and a further
speedup of over 3 using approximations that
do not degrade performance.
1 Introduction
Typically, in structured prediction applications, ev-
ery example is treated independently and an infer-
ence algorithm is applied to each one of them. For
example, consider a dependency parser that uses the
maximum spanning tree algorithm (McDonald et al
2005) or its integer linear program variants (Riedel
and Clarke, 2006; Martins et al 2009) to make pre-
dictions. Given a trained model, the parser addresses
* These authors contributed equally to this work.
each sentence separately and runs the inference al-
gorithm to predict the parse tree. Thus, the time
complexity of inference over the test set is linear in
the size of the corpus.
In this paper, we ask the following question: For
a given task, since the inference procedure predicts
structures from the same family of structures (depen-
dency trees, semantic role structures, etc.), can the
fact that we are running inference for a large num-
ber of examples help us improve the time complexity
of inference? In the dependency parsing example,
this question translates to asking whether, having
parsed many sentences, we can decrease the parsing
time for the next sentence.
Since any combinatorial optimization problem
can be phrased as an integer linear program (ILP),
we frame inference problems as ILPs for the purpose
of analysis. By analyzing the objective functions
of integer linear programs, we identify conditions
when two ILPs have the same solution. This allows
us to reuse solutions of previously solved problems
and theoretically guarantee the optimality of the so-
lution. Furthermore, in some cases, even when the
conditions are not satisfied, we can reuse previous
solutions with high probability of being correct.
Given the extensive use of integer linear programs
for structured prediction in Natural Language Pro-
cessing over the last few years, these ideas can be ap-
plied broadly to NLP problems. We instantiate our
improved inference approaches in the structured pre-
diction task of semantic role labeling, where we use
an existing implementation and a previous trained
model that is based on the approach of (Punyakanok
et al 2008). We merely modify the inference pro-
1114
cess to show that we can realize the theoretical gains
by making fewer calls to the underlying ILP solver.
Algorithm Speedup
Theorem 1 2.44
Theorem 2 2.18
Theorem 3 2.50
Table 1: The speedup for semantic role labeling cor-
responding to the three theorems described in this
paper. These theorems guarantee the optimality of
the solution, thus ensuring that the speedup is not
accompanied by any loss in performance.
Table 1 presents a preview of our results, which
are discussed in Section 4. All three approaches in
this table improve running time, while guaranteeing
optimum solutions. Allowing small violations to the
conditions of the theorems provide an even higher
improvement in speedup (over 3), without loss of
performance.
The primary contributions of this paper are:
1. We pose the problem of optimizing inference
costs over entire datasets rather than individ-
ual examples. Our approach is agnostic to the
underlying models and allows us to use pre-
trained scoring functions.
2. We identify equivalence classes of ILP prob-
lems and use this notion to prove exact con-
ditions under which no inference is required.
These conditions lead to algorithms that can
speed up inference problem without losing the
exactness guarantees. We also use these con-
ditions to develop approximate inference algo-
rithms that can provide a further speedup.
3. We apply our approach to the structured pre-
diction task of semantic role labeling. By not
having to perform inference on some of the in-
stances, those that are equivalent to previously
seen instances, we show significant speed up in
terms of the number of times inference needs to
be performed. These gains are also realized in
terms of wall-clock times.
The rest of this paper is organized as follows: In
section 2, we formulate the problem of amortized
inference and provide motivation for why amortized
gains can be possible. This leads to the theoretical
discussion in section 3, where we present the meta-
algorithm for amortized inference along with sev-
eral exact and approximate inference schemes. We
instantiate these schemes for the task of semantic
role labeling (Section 4). Section 5 discusses related
work and future research directions.
2 Motivation
Many NLP tasks can be phrased as structured pre-
diction problems, where the goal is to jointly assign
values to many inference variables while account-
ing for possible dependencies among them. This de-
cision task is a combinatorial optimization problem
and can be solved using a dynamic programming ap-
proach if the structure permits. In general, the infer-
ence problem can be formulated and solved as inte-
ger linear programs (ILPs).
Following (Roth and Yih, 2004) Integer linear
programs have been used broadly in NLP. For exam-
ple, (Riedel and Clarke, 2006) and (Martins et al
2009) addressed the problem of dependency pars-
ing and (Punyakanok et al 2005; Punyakanok et
al., 2008) dealt with semantic role labeling with this
technique.
In this section, we will use the ILP formulation
of dependency parsing to introduce notation. The
standard approach to framing dependency parsing as
an integer linear program was introduced by (Riedel
and Clarke, 2006), who converted the MST parser
of (McDonald et al 2005) to use ILP for inference.
The key idea is to build a complete graph consist-
ing of tokens of the sentence where each edge is
weighted by a learned scoring function. The goal
of inference is to select the maximum spanning tree
of this weighted graph.
2.1 Problem Formulation
In this work, we consider the general inference prob-
lem of solving a 0-1 integer linear program. To per-
form inference, we assume that we have a model that
assigns scores to the ILP decision variables. Thus,
our work is applicable not only in cases where in-
ference is done after a separate learning phase, as in
(Roth and Yih, 2004; Clarke and Lapata, 2006; Roth
and Yih, 2007) and others, but also when inference
is done during the training phase, for algorithms like
1115
the structured perceptron of (Collins, 2002), struc-
tured SVM (Tsochantaridis et al 2005) or the con-
straints driven learning approach of (Chang et al
2007).
Since structured prediction assigns values to a
collection of inter-related binary decisions, we de-
note the ith binary decision by yi ? {0, 1} and the
entire structure as y, the vector composed of all the
binary decisions. In our running example, each edge
in the weighted graph generates a single decision
variable (for unlabeled dependency parsing). For
each yi, let ci ? < denote the weight associated with
it. We denote the entire collection of weights by the
vector c, forming the objective for this ILP.
Not all assignments to these variables are valid.
Without loss of generality, these constraints can be
expressed using linear inequalities over the infer-
ence variables, which we write as MTy ? b for
a real valued matrix M and a vector b. In depen-
dency parsing, for example, these constraints ensure
that the final output is a spanning tree.
Now, the overall goal of inference is to find the
highest scoring structure. Thus, we can frame infer-
ence as an optimization problem p with n inference
variables as follows:
arg max
y?{0,1}n
cTy (1)
subject to MTy ? b. (2)
For brevity, we denote the space of feasible solutions
that satisfy the constraints for the ILP problem p as
Kp = {y ? {0, 1}n|MTy ? b}. Thus, the goal of
inference is to find
arg max
y?Kp
cTy.
We refer to Kp as the feasible set for the inference
problem p and yp as its solution.
In the worst case, integer linear programs are
known to be NP-hard. Hence, solving large prob-
lems, (that is, problems with a large number of con-
straints and/or variables) can be infeasible.
For structured prediction problems seen in NLP,
we typically solve many instances of inference prob-
lems. In this paper, we investigate whether an infer-
ence algorithm can use previous predictions to speed
up inference time, thus giving us an amortized gain
in inference time over the lifetime of the program.
We refer to inference algorithms that have this capa-
bility as amortized inference algorithms.
In our running example, each sentence corre-
sponds to a separate ILP. Over the lifetime of the
dependency parser, we create one inference instance
(that is, one ILP) per sentence and solve it. An amor-
tized inference algorithm becomes faster at parsing
as it parses more and more sentences.
2.2 Why can inference costs be amortized over
datasets?
In the rest of this section, we will argue that the time
cost of inference can be amortized because of the
nature of inference in NLP tasks. Our argument is
based on two observations, which are summarized in
Figure (1): (1) Though the space of possible struc-
tures may be large, only a very small fraction of
these occur. (2) The distribution of observed struc-
tures is heavily skewed towards a small number of
them.
x?s p?s y?s
ILP
formulation
Inference
Examples
ILPs
Solutions
Figure 1: For a structured prediction task, the infer-
ence problem p for an example x needs to be for-
mulated before solving it to get the structure y. In
structured prediction problems seen in NLP, while
an exponential number of structures is possible for a
given instance, in practice, only a small fraction of
these ever occur. This figure illustrates the empirical
observation that there are fewer inference problems
p?s than the number of examples and the number of
observed structures y?s is even lesser.
As an illustration, consider the problem of part-
of-speech tagging. With the standard Penn Treebank
tag set, each token can be assigned one of 45 labels.
Thus, for a sentence of size n, we could have 45n
structures out of which the inference process needs
to choose one. However, a majority of these struc-
tures never occur. For example, we cannot have a
1116
 0
 100000
 200000
 300000
 400000
 500000
 0  10  20  30  40  50Number of tokens
Part-of-speech statistics, using tagged Gigaword text
Number of examples of size Number of unique POS tag sequences
(a) Part-of-speech tagging
 0
 100000
 200000
 300000
 400000
 500000
 0  10  20  30  40  50Size of sentence
Unlabeled dependency parsing statistics, using tagged Gigaword text
Number of examples of sizeNumber of unique dependency trees
(b) Unlabeled dependency parsing
 0
 20000
 40000
 60000
 80000
 100000
 120000
 140000
 160000
 0  1  2  3  4  5  6  7  8Size of the input (number of argument candidates)
SRL statistics , using tagged Gigaword text
Number of examples of sizeNumber of unique SRL structures
(c) Semantic role labeling
Figure 2: Number of inference instances for different input sizes (red solid lines) and the number of unique
structures for each size (blue dotted lines). The x-axis indicates the size of the input (number of tokens
for part of speech and dependency, and number of argument candidates for SRL.) Note that the number of
instances is not the number of unique examples of a given length, but the number of times an inference
procedure is called for an input of a given size.
 0
 2
 4
 6
 8
 10
 12
 0  5000  10000  15000  20000Solution Id
Log frequency of solutions for sentences with 5 tokens
(a) Sentence length = 5
-1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  50000  100000  150000  200000  250000Solution Id
Log frequency of solutions for sentences with 10 tokens
(b) Sentence length = 10
-1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  50000  100000  150000  200000  250000  300000  350000Solution Id
Log frequency of solutions for sentences with 15 tokens
(c) Sentence length = 15
Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentences
with five, ten and fifteen tokens. The x-axes list different unique part-of-speech tag sequences for the entire
sentence. These plots show that for sentences of a given length, most structures (solutions) that are possible
never occur, or occur very infrequently; only a few of the possible structures (solutions) actually occur
frequently.
sentence where all the tokens are determiners.
Furthermore, many sentences of the same size
share the same part-of-speech tag sequence. To
quantify the redundancy of structures, we part-of-
speech tagged the English Gigaword corpus (Graff
and Cieri, 2003). Figure (2a) shows the number
of sentences in the corpus for different sentence
lengths. In addition, it also shows the number of
unique part-of-speech tag sequences (over the en-
tire sentence) for each size. We see that the number
of structures is much fewer than the number of in-
stances for any sentence size. Note that 45n quickly
outgrows the number of sentences as n increases.
The figures (2b) and (2c) show similar statistics for
unlabeled dependency parsing and semantic role la-
beling. In the former case, the size of the instance is
the number of tokens in a sentence, while in the lat-
ter, the size is the number of argument candidates
that need to be labeled for a given predicate. In
both cases, we see that the number of empirically
observed structures is far fewer than the number of
instances to be labeled.
Thus, for any given input size, the number of in-
stances of that size (over the lifetime of the program)
far exceeds the number of observed structures for
that size. Moreover, the number of observed struc-
tures is significantly smaller than the number of the-
oretically possible structures. Thus, we have a small
number of structures that form optimum structures
for many inference instances of the same size.
Our second observation deals with the distribu-
tion of structures for a given input size. Figure (3)
1117
shows the log frequencies of part-of-speech tagging
sequences for sentences of lengths five, ten and fif-
teen. In all cases, we see that a few structures are
most frequent. We observed similar distributions of
structures for all input sizes for dependency parsing
and semantic role labeling as well.
Since the number of structures for a given exam-
ple size is small, many examples x?s, and hence
many inference problems p?s, are associated with
the same structure y. These observations suggest the
possibility of getting an amortized gain in inference
time by characterizing the set of inference problems
that produce the same structure. Then, for a new in-
ference problem, if we can identify that it belongs to
a known set, that is, will yield a solution that we have
already seen, we do not have to run inference at all.
The second observation also suggests that this char-
acterization of sets of problems that have the same
solution can be done in a data-driven way because
characterizing a small number of structures can give
us high coverage.
3 Amortizing inference costs
In this section, we present different schemes for
amortized inference leading up to an inference meta-
algorithm. The meta-algorithm is both agnostic to
the underlying inference algorithm that is used by
the problem and maintains the exactness properties
of the underlying inference scheme. That is, if we
have an exact/approximate inference algorithm with
a certain guarantees, the meta-algorithm will have
the same guarantees, but with a speedup.
3.1 Notation
For an integer linear program p with np variables,
we denote its objective coefficients by cp and its fea-
sible set byKp. We denote its solution as as yp. We
represent vectors by boldfaced symbols and their ith
component using subscripts.
We consider many instantiations of the inference
problem and use superscripts to denote each indi-
vidual instance. Thus, we have a large collection of
inference instances P = {p1,p2, ? ? ? } along with
their respective solutions {y1p,y
2
p, ? ? ? }.
Definition 1 (Equivalence classes of ILPs). Two in-
teger linear programs are said to be in the same
equivalence class if they have the same number of
inference variables and the same feasible set.
We square brackets to denote equivalence classes.
If [P ] is an equivalence class of ILPs, we use the
notation K[P ] to denote its feasible set and n[P ] to
denote the number of variables. Also, for a program
p, we use the notation p ? [P ] to indicate that it
belongs to the equivalence class [P ].
3.2 Exact theorems
Our goal is to characterize the set of objective func-
tions which will have the same solution for a given
equivalence class of problems.
Suppose we have solved an ILP p to get a solution
yp. For every inference variable that is active in the
solution (i.e., whose value is 1), increasing the corre-
sponding objective value will not change the optimal
assignment to the variables. Similarly, for all other
variables (whose value in the solution is 0), decreas-
ing the objective value will not change the optimal
solution. This intuition gives us our first theorem for
checking whether two ILPs have the same solution
by looking at the difference between their objective
coefficients.
Theorem 1. Let p denote an inference problem
posed as an integer linear program belonging to an
equivalence class [P ]. Let q ? [P ] be another infer-
ence instance in the same equivalence class. Define
?c = cq ? cp to be the difference of the objective
coefficients of the ILPs. Then, yp is the solution of
the problem q if for each i ? {1, ? ? ? , np}, we have
(2yp,i ? 1)?ci ? 0 (3)
The condition in the theorem, that is, inequal-
ity (3), requires that the objective coefficients corre-
sponding to values yp,i that are set to 1 in p increase,
and those that correspond to values of yp,i set to 0,
decrease. Under these conditions, if yp is the max-
imizer of the original objective, then it maximizes
the new objective too.
Theorem 1 identifies perturbations of an ILP?s ob-
jective coefficients that will not change the optimal
assignment. Next, we will characterize the sets of
objective values that will have the same solution us-
ing a criterion that is independent of the actual so-
lution. Suppose we have two ILPs p and q in an
equivalence class [P ] whose objective values are cp
and cq respectively. Suppose y? is the solution to
1118
both these programs. That is, for every y ? K[P ],
we have cTpy ? c
T
py
? and cTqy ? c
T
qy
?. Multiply-
ing these inequalities by any two positive real num-
bers x1 and x2 and adding them shows us that y?
is also the solution for the ILP in [P ] which has the
objective coefficients x1cp + x2cq. Extending this
to an arbitrary number of inference problems gives
us our next theorem.
Theorem 2. Let P denote a collection
{p1,p2, ? ? ? ,pm} of m inference problems in
the same equivalence class [P ] and suppose that
all the problems have the same solution, yp. Let
q ? [P ] be a new inference program whose optimal
solution is y. Then y = yp if there is some x ? <m
such that x ? 0 and
cq =
?
j
xjcjp. (4)
From the geometric perspective, the pre-condition
of this theorem implies that if the new coefficients
lie in the cone formed by the coefficients of the pro-
grams that have the same solution, then the new pro-
gram shares the solution.
Theorems 1 and 2 suggest two different ap-
proaches for identifying whether a new ILP can
use the solution of previously solved inference in-
stances. These theorems can be combined to get a
single criterion that uses the objective coefficients of
previously solved inference problems and their com-
mon solution to determine whether a new inference
problem will have the same solution. Given a collec-
tion of solved ILPs that have the same solution, from
theorem 2, we know that an ILP with the objective
coefficients c =
?
j xjc
j
p will share the solution.
Considering an ILP whose objective vector is c and
applying theorem 1 to it gives us the next theorem.
Theorem 3. Let P denote a collection
{p1,p2, ? ? ? ,pm} of m inference problems
belonging to the same equivalence class [P ].
Furthermore, suppose all the programs have the
same solution yp. Let q ? [P ] be a new inference
program in the equivalence class. For any x ? <m,
define ?c(x) = cq ?
?
j xjc
j
p. The assignment
yp is the optimal solution of the problem q if there
is some x ? <m such that x ? 0 and for each
i ? {1, np}, we have
(2yp,i ? 1)?ci ? 0 (5)
Theorem Condition
Theorem 1 ?i ? {1, ? ? ? , np},
(2yp,i ? 1)?ci ? 0; ?i.
Theorem 2 ? x ? <m, such that
x ? 0 and cq =
?
j xjc
j
p
Theorem 3 ? x ? <m, such that
x ? 0 and (2yp,i ? 1)?ci ? 0; ?i.
Table 2: Conditions for checking whether yp is the
solution for an inference problem q ? [P ] according
to theorems 1, 2 and 3. Please refer to the statements
of the theorems for details about the notation.
3.3 Implementation
Theorems 1, 2 and 3 each specify a condition that
checks whether a pre-existing solution is the opti-
mal assignment for a new inference problem. These
conditions are summarized in Table 2. In all cases,
if the condition matches, the theorems guarantee that
the two solutions will be the same. That is, applying
the theorems will not change the performance of the
underlying inference procedure. Only the number of
inference calls will be decreased.
In our implementation of the conditions, we used
a database1 to cache ILPs and implemented the
retrieval of equivalence classes and solutions as
queries to the database. To implement theorem 1,
we iterate over all ILPs in the equivalence class and
check if the condition is satisfied for one of them.
The conditions of theorems 2 and 3 check whether a
collection of linear (in)equalities has a feasible solu-
tion using a linear program solver.
We optimize the wall-clock time of theorems 2
and 3 by making two observations. First, we do not
need to solve linear programs for all possible ob-
served structures. Given an objective vector, we only
need consider the highest scoring structures within
an equivalence class. (All other structures cannot
be the solution to the ILP.) Second, since theorem
2 checks whether an ILP lies within a cone, we can
optimize the cache for theorem 2 by only storing the
ILPs that form on the boundary of the cone. A sim-
ilar optimization can be performed for theorem 3 as
well. Our implementation uses the following weaker
version of this optimization: while caching ILPs, we
1We used the H2 database engine, which can be downloaded
from http://h2database.com, for all caching.
1119
do not add an instance to the cache if it already satis-
fies the theorem. This optimization reduces the size
of the linear programs used to check feasibility.
3.4 Approximation schemes
So far, in the above three theorems, we retain the
guarantees (in terms of exactness and performance)
of the underlying inference procedure. Now, we will
look at schemes for approximate inference. Unlike
the three theorems listed above, with the following
amortized inference schemes, we are not guaranteed
an optimal solution.
3.4.1 Most frequent solution
The first scheme for approximation uses the ob-
servation that the most frequent solution occurs an
overwhelmingly large number of times, compared to
the others. (See the discussion in section 2.2 and fig-
ures 3a, 3b and 3c for part-of-speech tagging.) Un-
der this approximation scheme, given an ILP prob-
lem, we simply pick the most frequent solution for
that equivalence class as the solution, provided this
solution has been seen a sufficient number of times.
If the support available in the cache is insufficient,
we call the underlying inference procedure.
3.4.2 Top-K approximation
The previous scheme for approximate amortized
inference is agnostic to the objective coefficients of
integer linear program to be solved and uses only
its equivalence class to find a candidate structure.
The top-K approach extends this by scoring the K
most frequent solutions using the objective coeffi-
cients and selecting the highest scoring one as the
solution to the ILP problem. As with the previous
scheme, we only consider solutions that have suffi-
cient support.
3.4.3 Approximations to theorems 1 and 3
The next approximate inference schemes relaxes
the conditions in theorems 1 and 3 by allowing the
inequalities to be violated by . That is, the inequal-
ity (3) from Theorem 1 now becomes
(2yp,i ? 1)?ci +  ? 0. (6)
The inequality (5) from Theorem 3 is similarly re-
laxed as follows:
(2yp,i ? 1)?ci +  ? 0 (7)
3.5 Amortized inference algorithm
Each exact and approximate inference approach de-
scribed above specifies a condition to check whether
an inference procedure should be called for a
new problem. This gives us the following meta-
algorithm for amortized inference, parameterized by
the actual scheme used: If the given input instance p
satisfies the condition specified by the scheme, then
use the cached solution. Otherwise, call the infer-
ence procedure and cache the solution for future use.
4 Experiments
In this section, we apply the theory from Section 3 to
the structure prediction problem of semantic role la-
beling. Since the inference schemes presented above
are independent of the learning aspects, we use an
off-the-shelf implementation and merely modify the
inference as discussed in Section 3.5.
The goal of the experiments is to show that us-
ing an amortized inference algorithm, we can make
fewer calls to the underlying inference procedure.
For the exact inference algorithms, doing so will not
change the performance as compared to the under-
lying system. For the approximations, we can make
a trade-off between the inference time and perfor-
mance.
4.1 Experimental setup
Our goal is to simulate a long-running NLP process
that can use a cache of already solved problems to
improve inference time. Given a new input problem,
our theorems require us to find all elements in the
equivalence class of that problem along with their
solutions. Intuitively, we expect a higher probability
of finding members of an arbitrary equivalence class
if the size of the cache is large. Hence, we processed
sentences from the Gigaword corpus and cached the
inference problems for our task.
The wall-clock time is strongly dependent on such
specific implementation of the components, which
are independent of the main contributions of this
work. Also, in most interesting applications, the
computation time for each step will be typically
dominated by the number of inference steps, espe-
cially with efficient implementations of caching and
retrieval. Hence, the number of calls to the underly-
ing procedure is the appropriate complexity param-
1120
eter. Let NBase be the number of times we would
need to call the underlying inference procedure had
we not used an amortized algorithm. (This is the
same as the number of inference problems.) Let NA
be the number of times the underlying inference pro-
cedure is actually called using an amortized algo-
rithm A. We define the speedup of A as
Speedup(A) =
NBase
NA
. (8)
We also report the clock speedup of our implemen-
tation for all algorithms, which is the ratio of the
wall-clock time taken by the baseline algorithm to
that of the amortized algorithm. For measuring time,
we only measure the time for inference as the other
aspects (feature extraction, scoring, etc.) are not
changed.
4.2 Semantic Role Labeling
The goal of Semantic Role Labeling (SRL) (Palmer
et al 2010) is to identify and assign semantic roles
to arguments of verb predicates in a sentence. For
example, consider the the sentence John gave the
ball to Mary. The verb give takes three arguments,
John, the ball and to Mary, which are labeled A0,
A1 and A2 respectively.
We used the system of (Punyakanok et al 2008)
as our base SRL system. It consists of two classi-
fiers trained on the Propbank corpus. The first one,
called the argument identifier, filters argument can-
didates which are generated using a syntactic parse-
based heuristic. The second model scores each can-
didate that has not been filtered for all possible argu-
ment labels. The scores for all candidates of a pred-
icate are combined via inference. As in the system
of (Punyakanok et al 2008), the softmax function
is applied to the raw classifier scores to ensure that
they are in the same numeric range.
Inference mandates that certain structural and
linguistic constraints hold over the full predicate-
argument structure for a verb. (Punyakanok et al
2008) modeled inference via an integer linear pro-
gram instance, where each assignment of labels
to candidates corresponds to one decision variable.
Given a set of argument candidates, the feasible set
of decisions is dependent of the number of argument
candidates and the verb predicate. Thus, in terms
of the notation used in this paper, the equivalence
classes are defined by the pair (predicate, number of
argument candidates).
We ran the semantic role labeler on 225,000 verb
predicates from the Gigaword corpus and cached
the equivalence classes, objective coefficients and
solutions generated by the SRL system. We re-
port speedup for the various amortized inference
schemes on the standard Penn Treebank test set. On
this data, the unaltered baseline system, processes
5127 integer linear programs and achieves an F1 of
75.85%.
Table 3 shows the speedup and performance for
the various inference schemes. The most frequent
and top-K systems are both naive solutions that take
advantage of the cache of stored problems. In spite
of their simplicity, they attain F1 scores of 62%
and 70.06% because few structures occur most fre-
quently, as described in section 2.2. We see that all
the exact theorems attain a speedup higher than two
without losing performance. (The variation in F1 be-
tween them is because of the existence of different
equivalent solutions in terms of the objective value.)
This shows us that we can achieve an amortized gain
in inference. Note that a speedup of 2.5 indicates
that the solver is called only for 40% of the exam-
ples. The approximate versions of theorems 1 and 3
(with  = 0.3 in both cases, which was not tuned)
attain an even higher gain in speedup over the base-
line than the base versions of the theorems. Interest-
ingly, the SRL performance in both cases does not
decline much even though the conditions of the the-
orems may be violated.
5 Related work and Future directions
In recent years, we have seen several approaches to
speeding up inference using ideas like using the cut-
ting plane approach (Riedel, 2009), dual decompo-
sition and Lagrangian relaxation (Rush et al 2010;
Chang and Collins, 2011). The key difference be-
tween these and the work in this paper is that all
these approaches solve one instance at a time. Since
we can use any inference procedure as a underlying
system, the speedup reported in this paper is appli-
cable to all these algorithms.
Decomposed amortized inference In this paper,
we have taken advantage of redundancy of struc-
tures that can lead to the re-use of solutions. In the
1121
Type Algorithm # instances # solver Speedup Clock F1
calls speedup
Exact Baseline 5127 5217 1.0 1.0 75.85
Exact Theorem 1 5127 2134 2.44 1.54 75.90
Exact Theorem 2 5127 2390 2.18 1.14 75.79
Exact Theorem 3 5127 2089 2.50 1.36 75.77
Approx. Most frequent (Support = 50) 5127 2812 1.86 1.57 62.00
Approx. Top-10 solutions (Support = 50) 5127 2812 1.86 1.58 70.06
Approx. Theorem 1 (approx,  = 0.3) 5127 1634 3.19 1.81 75.76
Approx. Theorem 3 (approx,  = 0.3) 5127 1607 3.25 1.50 75.46
Table 3: Speedup and performance for various inference methods for the task of Semantic Role Labeling.
All the exact inference algorithms get a speedup higher than two. The speedup of the approximate version
of the theorems is even higher without loss of performance. The clock speedup is defined as the ratio of the
inference times of the baseline and the given algorithm. All numbers are averaged over ten trials.
part of speech example, we showed redundancy of
structures at the sentence level (Figure 2a). How-
ever, for part-of-speech tagging, the decisions are
rarely, if at all, dependent on a very large context.
One direction of future work is to take advantage of
the fact that the inference problem can be split into
smaller sub-problems. To support this hypothesis,
we counted the number of occurrences of ngrams
of tokens (including overlapping and repeated men-
tions) for n <= 10 and compared this to the number
of unique part-of-speech ngrams of this length. Fig-
ure 4 shows these two counts. Following the argu-
ment in Section 2.2, this promises a large amortized
gain in inference time. We believe that such decom-
position can also be applied to other, more complex
structured prediction tasks.
The value of approximate inference From the
experiments, we see that the first two approximate
inference schemes (most frequent solution and the
top-K scheme) can speed up inference with the
only computational cost being the check for pre-
conditions of the exact theorems. Effectively, these
algorithms have parameters (i.e., the support param-
eter) that allow us to choose between the inference
time and performance. Figure 5 shows the perfor-
mance of the most frequent and top-K baselines for
different values of the support parameter, which in-
dicates how often a structure must occur for it to be
considered. We see that for lower values of support,
we can get a very high speedup but pay with poorer
performance.
 0
 1e+06
 2e+06
 3e+06
 4e+06
 5e+06
 6e+06
 7e+06
 0  2  4  6  8  10
Number of tokens
Part-of-speech ngram statistics, using tagged Gigaword text
Number of instances for size Number of unique structures for given length
Figure 4: The red line shows the number of ngrams
of tokens (including overlapping and repeated oc-
currences) in the Gigaword corpus and the blue line
shows the number of unique POS tag sequences.
However, the prediction of the approximate al-
gorithms can be used to warm-start any solver that
can accept an external initialization. Warm-starting
a solver can give a way to get the exact solution and
yet take advantage of the frequency of structures that
have been observed.
Lifted inference The idea of amortizing inference
time over the dataset is conceptually related to the
idea of lifted inference (de Salvo Braz et al 2005).
We abstract many instances into equivalence classes
and deal with the inference problem with respect to
the equivalence classes in the same way as done in
lifted inference algorithms.
1122
 1
 1.5
 2
 2.5
 3
 3.5
 0  200  400  600  800  1000 50
 55
 60
 65
 70
 75
 80
Support
Performance of the most frequent and top-K schemes for different values of support
SpeedupPerformance of most frequent solution (F1)Performance of top-K solution (F1)
Figure 5: Most frequent solutions and top-K:
Speedup and SRL performance (F1) for different
values of the support parameter, using the most-
frequent solutions (dashed blue line) and the top-
K scheme (thick gray line). Support indicates how
many times a structure should be seen for it to be
considered. Note that the speedup values for both
schemes are identical (red line).
6 Conclusion
In this paper, we addressed structured prediction in
the context of NLP and proposed an approach to im-
prove inference costs over an entire dataset, rather
than individual instances. By treating inference
problems as instances of integer linear programs, we
proposed three exact theorems which identify exam-
ples for which the inference procedure need not be
called at all and previous solutions can be re-used
with the guarantee of optimality. In addition, we
also proposed several approximate algorithms. We
applied our algorithms, which are agnostic to the
actual tasks, to the problem semantic role labeling,
showing significant decrease in the number of infer-
ence calls without any loss in performance. While
the approach suggested in this paper is evaluated in
semantic role labeling, it is generally applicable to
any NLP task that deals with structured prediction.
Acknowledgements
The authors wish to thank Sariel Har-Peled and the members
of the Cognitive Computation Group at the University of Illi-
nois for insightful discussions and the anonymous reviewers for
their valuable feedback. This research is sponsored by the Army
Research Laboratory (ARL) under agreement W911NF-09-2-
0053. The authors also gratefully acknowledge the support
of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Labo-
ratory (AFRL) prime contract no. FA8750-09-C-0181. This
work is also supported by the Intelligence Advanced Research
Projects Activity (IARPA) Foresight and Understanding from
Scientific Exposition (FUSE) Program via Department of In-
terior National Business Center contract number D11PC2015.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of ARL, DARPA, AFRL, IARPA,
or the US government.
References
Y-W. Chang and M. Collins. 2011. Exact decoding
of phrase-based translation models through lagrangian
relaxation. EMNLP.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
R. de Salvo Braz, E. Amir, and D. Roth. 2005. Lifted
first-order probabilistic inference. In IJCAI.
D Graff and C. Cieri. 2003. English gigaword.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In EMNLP, pages 523?530, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic Role
Labeling, volume 3. Morgan & Claypool Publishers.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
IJCAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
S. Riedel. 2009. Cutting Plane MAP Inference for
Markov Logic. Machine Learning.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, CoNLL.
1123
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research.
1124
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499?1510,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Modeling Biological Processes for Reading Comprehension
Jonathan Berant
?
, Vivek Srikumar
?
, Pei-Chun Chen, Brad Huang and Christopher D. Manning
Stanford University, Stanford
Abby Vander Linden and Brittany Harding
University of Washington, Seattle
Abstract
Machine reading calls for programs that
read and understand text, but most current
work only attempts to extract facts from
redundant web-scale corpora. In this pa-
per, we focus on a new reading compre-
hension task that requires complex reason-
ing over a single document. The input is
a paragraph describing a biological pro-
cess, and the goal is to answer questions
that require an understanding of the re-
lations between entities and events in the
process. To answer the questions, we first
predict a rich structure representing the
process in the paragraph. Then, we map
the question to a formal query, which is
executed against the predicted structure.
We demonstrate that answering questions
via predicted structures substantially im-
proves accuracy over baselines that use
shallower representations.
1 Introduction
The goal of machine reading is to develop pro-
grams that read text to learn about the world
and make decisions based on accumulated knowl-
edge. Work in this field has focused mostly on
macro-reading, i.e., processing large text collec-
tions and extracting knowledge bases of facts (Et-
zioni et al., 2006; Carlson et al., 2010; Fader et al.,
2011). Such methods rely on redundancy, and are
thus suitable for answering common factoid ques-
tions which have ample evidence in text (Fader et
al., 2013). However, reading a single document
(micro-reading) to answer comprehension ques-
tions that require deep reasoning is currently be-
yond the scope of state-of-the-art systems.
In this paper, we introduce a task where given
a paragraph describing a process, the goal is to
?
Both authors equally contributed to the paper.
answer reading comprehension questions that test
understanding of the underlying structure. In par-
ticular, we consider processes in biology text-
books such as this excerpt and the question that
follows:
?. . . Water is split, providing a source of elec-
trons and protons (hydrogen ions, H
+
) and giv-
ing off O
2
as a by-product. Light absorbed by
chlorophyll drives a transfer of the electrons
and hydrogen ions from water to an acceptor
called NADP
+
. . . ?
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
This excerpt describes a process in which a com-
plex set of events and entities are related to one
another. A system trying to answer this ques-
tion must extract a rich structure spanning multi-
ple sentences and reason that water splitting com-
bined with light absorption leads to transfer of
ions. Note that shallow methods, which rely on
lexical overlap or text proximity, will fail. Indeed,
both answers are covered by the paragraph and the
wrong answer is closer in the text to the question.
We propose a novel method that tackles this
challenging problem (see Figure 1). First, we train
a supervised structure predictor that learns to ex-
tract entities, events and their relations describing
the biological process. This is a difficult prob-
lem because events have complex interactions that
span multiple sentences. Then, treating this struc-
ture as a small knowledge-base, we map ques-
tions to formal queries that are executed against
the structure to provide the answer.
Micro-reading is an important aspect of natural
language understanding (Richardson et al., 2013;
Kushman et al., 2014). In this work, we focus
specifically on modeling processes, where events
and entities relate to one another through com-
plex interactions. While we work in the biology
1499
?. . . Water is split, providing a source of elec-
trons and protons (hydrogen ions, H
+
) and
giving off O
2
as a by-product. Light ab-
sorbed by chlorophyll drives a transfer of
the electrons and hydrogen ions from water
to an acceptor called NADP+ . . . ?
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
water
split
THEME
absorb
light
THEME
transfer ions
THEME
ENABLE
CAUSE
water
split
absorb
light
THEME
(CAUSE|ENABLE)
+
THEME
water
split
transfer ions
THEME
(CAUSE|ENABLE)
+
THEME
Step 1
Step 2
Step 3: Answer = b
Figure 1: An overview of our reading comprehension system. First, we predict a structure from the input paragraph (the
top right portion shows a partial structure skipping some arguments for brevity). Circles denote events, squares denote argu-
ments, solid arrows represent event-event relations, and dashed arrows represent event-argument relations. Second, we map
the question paired with each answer into a query that will be answered using the structure. The bottom right shows the query
representation. Last, the two queries are executed against the structure, and a final answer is returned.
domain, processes are abundant in domains such
as chemistry, economics, manufacturing, and even
everyday events like shopping or cooking, and our
model can be applied to these domains as well.
The contributions of this paper are:
1. We propose a reading comprehension task
which requires deep reasoning over struc-
tures that represent complex relations be-
tween multiple events and entities.
2. We present PROCESSBANK, a new dataset
consisting of descriptions of biological pro-
cesses, fully-annotated with rich process
structures, and accompanied by multiple-
choice questions.
3. We present a novel method for answer-
ing questions, by predicting process struc-
tures and mapping questions to queries. We
demonstrate that by predicting structures we
can improve reading comprehension accu-
racy over baselines that do not exploit the un-
derlying structure.
The data and code for this paper are avail-
able at http://www-nlp.stanford.edu/
software/bioprocess.
2 Task Definition and Setup
This section describes the reading comprehension
task we address and the accompanying dataset.
We will use the example in Figure 1 as our run-
ning example throughout the paper.
Our goal is to tackle a complex reading com-
prehension setting that centers on understanding
the underlying meaning of a process description.
We target a multiple-choice setting in which each
input consists of a paragraph of text describing a
biological process, a question, and two possible
answers. The goal is to identify the correct answer
using the text (Figure 1, left). We used the 148
paragraphs from the textbook Biology (Campbell
and Reece, 2005) that were manually identified by
Scaria et al. (2013). We extended this set to 200
paragraphs by including additional paragraphs that
describe biological processes. Each paragraph in
the collection represents a single biological pro-
cess and describes a set of events, their partici-
pants and their interactions.
Because we target understanding of paragraph
meaning, we use the following desiderata for
building the corpus of questions and answers:
1. The questions should focus on the events and
entities participating in the process described
in the paragraph, and answering the questions
should require reasoning about the relations
between those events and entities.
2. Both answers should have similar lexical
overlap with the paragraph. Moreover, names
of entities and events in the question and an-
swers should appear as in the paragraph and
not using synonyms. This is to ensure that the
task revolves around reading comprehension
rather than lexical variability.
1
A biologist created the question-answer part of
1
Lexical variability is an important problem in NLP, but
is not the focus of this task.
1500
the corpus comprising of 585 questions spread
over the 200 paragraphs. A second annotator val-
idated 326 randomly chosen questions and agreed
on the correct answer with the first annotator in
98.1% of cases. We provide the annotation guide-
lines in the supplementary material.
Figure 1 (left) shows an excerpt of a paragraph
describing a process and an example of a ques-
tion based on it. In general, questions test an un-
derstanding of the interactions between multiple
events (such as causality, inhibition, temporal or-
dering), or between events and entities (i.e., roles
of entities in events), and require complex reason-
ing about chains of event-event and event-entity
relations.
3 The Structure of Processes
A natural first step for answering reading compre-
hension questions is to identify a structured rep-
resentation of the text. In this section, we define
this structure. We broadly follow the definition of
Scaria et al. (2013), but modify important aspects,
highlighted at the end of this section.
A paragraph describing a process is a sequence
of tokens that describes events, entities and their
relations (see Figure 1, top right). A process is
a directed graph (T ,A, E
tt
, E
ta
), where the nodes
T are labeled event triggers, the nodes A are ar-
guments, E
tt
are labeled edges describing event-
event relations, and E
ta
are labeled edges from
triggers to arguments denoting semantic roles (see
Figure 1 top right for a partial structure of the run-
ning example). The goal of process extraction is
to generate the process graph given the input para-
graph.
Triggers and arguments A trigger is a token
span denoting the occurrence of an event. In Fig-
ure 1, split, absorbed and transfer are event trig-
gers. In rare cases, a trigger denotes the non-
occurrence of an event. For example, in ?sym-
patric speciation can occur when gene flow is
blocked?, sympatric speciation occurs if gene flow
does not happen. Thus, nodes in T are labeled as
either a T-YES or T-NO to distinguish triggers of
events that occur from triggers of events that do
not occur. Arguments are token spans denoting
entities that participate in the process (such as wa-
ter, light and ions in Figure 1).
Semantic roles The edges E
ta
from triggers
to arguments are labeled by the semantic roles
AGENT, THEME, SOURCE, DESTINATION, LO-
CATION, RESULT, and OTHER for all other roles.
Our running example shows three THEME seman-
tic roles for the three triggers. For brevity, the fig-
ure does not show the RESULT of the event split,
namely, both source of electrons and protons (hy-
drogen ions, H
+
) and O
2
.
Event-event relations The directed edges E
tt
between triggers are labeled by one of eight pos-
sible event-event relations. These relations are
central to answering reading comprehension ques-
tions, which test understanding of the depen-
dencies and causal relations between the process
events. We first define three relations that express
a dependency between two event triggers u and v.
1. CAUSE denotes that u starts before v, and if
u happens then v happens (Figure 1).
2. ENABLE denotes that u creates conditions
necessary for the occurrence of v. This
means that u starts before v and v can only
happen if u happens (Figure 1).
2
3. PREVENT denotes that u starts before v and
if u happens, then v does not happen.
In processes, events sometimes depend on more
than one other event. For example, in Figure 1
(right top) transfer of ions depends on both water
splitting as well as light absorption. Conversely,
in Figure 2, the shifting event results in either one
of two events but not both. To express both con-
junctions and disjunctions of related events we
add the relations CAUSE-OR, ENABLE-OR and
PREVENT-OR, which express disjunctions, while
the default CAUSE, ENABLE, and PREVENT ex-
press conjunction (Compare the CAUSE-OR rela-
tions in Figure 2 with the relations in Figure 1).
We define the SUPER relation to denote that
event u is part of event v. (In Figure 2, slip-
page is a sub-event of replication.) Last, we use
the event coreference relation SAME to denote two
event mentions referring to the same event.
Notice that the assignments of relation labels in-
teract across different pairs of events. As an ex-
ample, if event u causes event v, then v can not
cause u. Our inference algorithm uses such struc-
tural constraints when predicting process structure
(Section 4).
2
In this work, we do not distinguish causation from facil-
itation, where u can help v but is not absolutely required. We
instructed the annotators to ignore the inherent uncertainty in
these cases and use CAUSE.
1501
Figure 2: Partial example of a process, as annotated in our dataset.
Avg Min Max
# of triggers 7.0 2 18
# of arguments 11.3 1 36
# of relation 7.9 1 37
Table 1: Statistics of triggers, arguments and rela-
tions over the 200 annotated paragraphs.
Three biologists annotated the same 200 para-
graphs described in Section 2 using the brat anno-
tation tool (Stenetorp et al., 2012). For each para-
graph, one annotator annotated the process, and
a second validated its correctness. Importantly,
the questions and answers were authored sepa-
rately by a different annotator, thus ensuring that
the questions and answers are independent from
the annotated structures. Table 1 gives statistics
over the dataset. The annotation guidelines are in-
cluded in the supplementary material.
Relation to Scaria et al. (2013) Scaria et al.
(2013) also defined processes as graphs where
nodes are events and edges describe event-event
relations. Our definition differs in a few important
aspects.
First, the set of event-event relations in that
work included temporal relations in addition to
causal ones. In this work, we posit that because
events in a process are inter-related, causal depen-
dencies are sufficient to capture the relevant tem-
poral ordering between them. Figure 1 illustrates
this phenomenon, where the temporal ordering be-
tween the events of water splitting and light ab-
sorption is unspecified. It does not matter whether
one happens before, during, or after the other. Fur-
thermore, the incoming causal links to transfer im-
ply that the event should happen after splitting and
absorption.
A second difference is that Scaria et al. (2013)
do not include disjunctions and conjunctions of
events in their formulation. Last, Scaria et al.
(2013) predict only relations given input triggers,
while we predict a full process structure.
4 Predicting Process Structures
We now describe the first step of our algorithm.
Given an input paragraph we predict events, their
arguments and event-event relations (Figure 1,
top). We decompose this into three sub-problems:
1. Labeling trigger candidates using a multi-
class classifier (Section 4.1).
2. For each trigger, identifying an over-
complete set of possible arguments, using a
classifier tuned for high recall (Section 4.2).
3. Jointly assigning argument labels and rela-
tion labels for all trigger pairs (Section 4.3).
The event-event relations CAUSE, ENABLE,
CAUSE-OR and ENABLE-OR, form a semantic
cluster: If (u, v) is labeled by one of these, then
the occurrence of v depends on the occurrence of
u. Since our dataset is small, we share statistics by
collapsing all four labels to a single ENABLE la-
bel. Similarly, we collapse the PREVENT and
PREVENT-OR labels, overall reducing the number
of relations to four.
For brevity, in what follows we only provide
a flavor of the features we extract, and refer the
reader to the supplementary material for details.
4.1 Predicting Event Triggers
The first step is to identify the events in the pro-
cess. We model the trigger detector as a multi-
class classifier that labels all content words in
the paragraph as one of T-YES, T-NO or NOT-
TRIGGER (Recall that a word can trigger an event
that occurred, an event that did not occur, or not
be a trigger at all). For simplicity, we model trig-
gers as single words, but in the gold annotation
about 14% are phrases (such as gene flow). Thus,
we evaluate trigger prediction by taking heads of
gold phrases. To train the classifier, we extract
1502
the lemma and POS tag of the word and adja-
cent words, dependency path to the root, POS
tag of children and parent in the dependency tree,
and clustering features from WordNet (Fellbaum,
1998), Nomlex (Macleod et al., 1998), Levin verb
classes (Levin, 1993), and a list of biological pro-
cesses compiled from Wikipedia.
4.2 Filtering Argument Candidates
Labeling trigger-argument edges is similar to se-
mantic role labeling. Following the standard ap-
proach (Punyakanok et al., 2008), for each trigger
we collect all constituents in the same sentence to
build an over-complete set of plausible candidate
arguments. This set is pruned with a binary classi-
fier that is tuned for high recall (akin to the argu-
ment identifier in SRL systems). On the develop-
ment set we filter more than half of the argument
candidates, while achieving more than 99% recall.
This classifier is trained using argument identifica-
tion features from Punyakanok et al. (2008).
At the end of this step, each trigger has a set of
candidate arguments which will be labeled during
joint inference. In further discussion, the argument
candidates for trigger t are denoted by A
t
.
4.3 Predicting Arguments and Relations
Given the output of the trigger classifier, our goal
is to jointly predict event-argument and event-
event relations. We model this as an integer linear
program (ILP) instance described below. We first
describe the inference setup assuming a model that
scores inference decisions and defer description of
learning to Section 4.4. The ILP has two types of
decision variables: arguments and relations.
Argument variables These variables capture
the decision that a candidate argument a, belong-
ing to the set A
t
of argument candidates, takes a
label A (from Section 3). We denote the Boolean
variables by y
t,a,A
, which are assigned a score
b
t,a,A
by the model. We include an additional label
NULL-ARG, indicating that the candidate is not an
argument for the trigger.
Event-event relation variables These variables
capture the decision that a pair of triggers t
1
and
t
2
are connected by a directed edge (t
1
, t
2
) labeled
by the relation R. We denote these variables by
z
t
1
,t
2
,R
, which are associated with a score c
t
1
,t
2
,R
.
Again, we introduce a label NULL-REL to indicate
triggers that are not connected by an edge.
Name Description
Unique labels Every argument candidate and trigger pair has ex-
actly one label.
Argument overlap Two arguments of the same trigger cannot overlap.
Relation symmetry The SAME relation is symmetric. All other rela-
tions are anti-symmetric, i.e., for any relation la-
bel other than SAME, at most one of (t
i
, t
j
) or
(t
j
, t
i
) can take that label and the other is assigned
the label NULL-REL.
Max arguments per
trigger
Every trigger can have no more than two arguments
with the same label.
Max triggers per ar-
gument
The same span of text can not be an argument for
more than two triggers.
Connectivity The triggers must form a connected graph, framed
as flow constraints as in Magnanti and Wolsey
(1995) and Martins et al. (2009).
Shared arguments If the same span of text is an argument of two trig-
gers, then the triggers must be connected by a rela-
tion that is not NULL-REL. This ensures that trig-
gers that share arguments are related.
Unique parent For any trigger, at most one outgoing edge can be
labeled SUPER.
Table 2: Constraints for joint inference.
Formulation Given the two sets of variables,
the objective of inference is to find a global as-
signment that maximizes the score. That is, the
objective can be stated as follows:
max
y,z
?
t,a?A
t
,A
b
t,a,A
? y
t,a,A
+
?
t
1
,t
2
,R
c
t
1
,t
2
,R
? z
t
1
,t
2
,R
Here, y and z refer to all the argument and rela-
tion variables respectively.
Clearly, all possible assignments to the infer-
ence variables are not feasible and there are both
structural as well as prior knowledge constraints
over the output space. Table 2 states the con-
straints we include, which are expressed as linear
inequalities over output variables using standard
techniques (e.g., (Roth and Yih, 2004)).
4.4 Learning in the Joint Model
We train both the trigger classifier and the argu-
ment identifier using L
2
-regularized logistic re-
gression. For the joint model, we use a linear
model for the scoring functions, and train jointly
using the structured averaged perceptron algo-
rithm (Collins, 2002).
Since argument labeling is similar to semantic
role labeling (SRL), we extract standard SRL fea-
tures given the trigger and argument from the syn-
tactic tree for the corresponding sentence. In ad-
dition, we add features extracted from an off-the-
shelf SRL system. We also include all feature con-
junctions. For event relations, we include the fea-
tures described in Scaria et al. (2013), as well as
context features for both triggers, and the depen-
dency path between them, if one exists.
1503
5 Question Answering via Structures
This section describes our question answering sys-
tem that, given a process structure, a question and
two answers, chooses the correct answer (steps 2
and 3 in Figure 1).
Our strategy is to treat the process structure as
a small knowledge-base. We map each answer
along with the question into a structured query that
we compare against the structure. The query can
prove either the correctness or incorrectness of the
answer being considered. That is, either we get a
valid match for an answer (proving that the cor-
responding answer is correct), or we get a refu-
tation in the form of a contradicted causal chain
(thus proving that the other answer is correct).
This is similar to theorem proving approaches sug-
gested in the past for factoid question answering
(Moldovan et al., 2003).
The rest of this section is divided into three
parts: Section 5.1 defines the queries we use, Sec-
tion 5.2 describes a rule-based algorithm for con-
verting a question and an answer into a query and
finally, 5.3 describes the overall algorithm.
5.1 Queries over Processes
We model a query as a directed graph path with
regular expressions over edge labels. The bot-
tom right portion of Figure 1 shows examples of
queries for our running example. In general, given
a question and one of the answer candidates, one
end of the path is populated by a trigger/argument
found in the question and the other is populated
with a trigger/ argument from the answer.
We define a query to consist of three parts:
1. A regular expression over relation labels, de-
scribing permissible paths,
2. A source trigger/argument node, and
3. A target trigger/argument node.
For example, the bottom query in Figure 1 looks
for paths labeled with CAUSE or ENABLE edges
from the event split to the event transfer.
Note that the representation of questions as di-
rected paths is a modeling choice and did not influ-
ence the authoring of the questions. Indeed, while
most questions do fit this model, there are rare
cases that require a more complex query structure.
5.2 Query Generation
Mapping a question and an answer into a query
involves identifying the components of the query
listed above. We do this in two phases: (1) In the
alignment phase, we align triggers and arguments
in the question and answer to the process structure
to give us candidate source and target nodes. (2)
In the query construction phase, we identify the
regular expression and the direction of the query
using the question, the answer and the alignment.
We identify three broad categories of QA pairs
(see Table 3) that can be identified using simple
lexical rules: (a) Dependency questions ask which
event or argument depends on another event or ar-
gument, (b) Temporal questions ask about tempo-
ral ordering of events, and (c) True-false questions
ask whether some fact is true. Below, we describe
the two phases of query generation primarily in the
context of dependency questions with a brief dis-
cussion about temporal and true-false questions at
the end of the section.
Alignment Phase We align triggers in the struc-
ture to the question and the answer by matching
lemmas or nominalizations. In case of multiple
matches, we use the context to disambiguate and
resolve ties using the highest matching candidate
in the syntactic dependency tree.
We align arguments in the question and the an-
swer in a similar manner. Since arguments are
typically several words long, we prefer maximal
spans. Additionally, if a question (or an answer)
contains an aligned trigger, we prefer to align
words to its arguments.
Query Construction Phase We construct a
query using the aligned question and answer trig-
gers/arguments. We will explain query construc-
tion using our running example (reproduced as the
dependency question in Table 3).
First, we identify the source and the target of
the query. We select either the source or the tar-
get to be a question node and populate the other
end of the query path with an answer node. To
make the choice between source or target for the
question node, we use the main verb in the ques-
tion, its voice and relative position of the question
word with respect to the main verb. In our exam-
ple, the main verb lead to is in active voice and the
question word what is not in subject position. This
places the trigger from the question as the source
of the query path (see both queries in the bottom
right portion of the running example). In contrast,
had the verb been require, the trigger would be the
target of the query. We construct two verb clusters
that indicate query direction using a small seed set
1504
Type Example # (%)
Dependency Q: What can the splitting of water lead to? 407 (69.57%)
a: Light absorption
b: Transfer of ions
Temporal Q: What is the correct order of events? 57 (9.74%)
a: PDGF binds to tyrosine kinases, then cells divide, then wound healing
b: Cells divide, then PDGF binds to tyrosine kinases, then wound healing
True-False Q: Cdk associates with MPF to become cyclin 121 (20.68%)
a: True
b: False
Table 3: Examples and statistics for each of the three coarse types of questions.
Is main verb trigger?
Condition Regular Exp.
Wh- word subjective? AGENT
Wh- word object? THEME
Condition Regular Exp.
default (ENABLE|SUPER)
+
DIRECT (ENABLE|SUPER)
PREVENT (ENABLE|SUPER)
?
PREVENT(ENABLE|SUPER)
?
Yes No
Figure 3: Rules for determining the regular expressions for queries concerning two triggers. In each table, the condition
column decides the regular expression to be chosen. In the left table, we make the choice based on the path from the root to
the Wh- word in the question. In the right table, if the word directly modifies the main trigger, the DIRECT regular expression
is chosen. If the main verb in the question is in the synset of prevent, inhibit, stop or prohibit, we select the PREVENT regular
expression. Otherwise, the default one is chosen. We omit the relation label SAME from the expressions, but allow going
through any number of edges labeled by SAME when matching expressions to the structure.
that we expand using WordNet.
The final step in constructing the query is to
identify the regular expression for the path con-
necting the source and the target. Due to paucity
of data, we do not map a question and an answer
to arbitrary regular expressions. Instead, we con-
struct a small set of regular expressions, and build
a rule-based system that selects one. We used the
training set to construct the regular expressions
and we found that they answer most questions (see
Section 6.4). We determine the regular expression
based on whether the main verb in the sentence is
a trigger and whether the source and target of the
path are triggers or arguments. Figure 3 shows the
possible regular expressions and the procedure for
choosing one when both the source and target are
triggers. If either of them are argument nodes, we
append the appropriate semantic role to the regu-
lar expression, based on whether the argument is
the source or the target of the path (or both).
True-false questions are treated similarly, ex-
cept that both source and target are chosen from
the question. For temporal questions, we seek to
identify the ordering of events in the answers. We
use the keywords first, then, or simultaneously to
identify the implied order in the answer. We use
the regular expression SUPER
+
for questions ask-
ing about simultaneous events and ENABLE
+
for
those asking about sequential events.
5.3 Answering Questions
We match the query of an answer to the process
structure to identify the answer. In case of a match,
the corresponding answer is chosen. The matching
path can be thought of as a proof for the answer.
If neither query matches the graph (or both do),
we check if either answer contradicts the struc-
ture. To do so, we find an undirected path from
the source to the target. In the event of a match, if
the matching path traverses any ENABLE edge in
the incorrect direction, we treat this as a refutation
for the corresponding answer and select the other
one. In our running example, in addition to the
valid path for the second query, for the first query
we see that there is an undirected path from split
to absorb through transfer that matches the first
query. This tells us that light absorption cannot
be the answer because it is not along a causal path
from split.
Finally, if none of the queries results in a match,
we look for any unlabeled path between the source
and the target, before backing off to a dependency-
based proximity baseline described in Section 6.
When there are multiple aligning nodes in the
question and answer, we look for any proof or
refutation before backing off to the baselines.
1505
6 Empirical Evaluation
In this section we aim to empirically evaluate
whether we can improve reading comprehension
accuracy by predicting process structures. We first
provide details of the experimental setup.
6.1 Experimental setup
We used 150 processes (435 questions) for train-
ing and 50 processes (150 questions) as the test
set. For development, we randomly split the train-
ing set 10 times (80%/20%), and tuned hyper-
parameters by maximizing average accuracy on
question answering. We preprocessed the para-
graphs with the Stanford CoreNLP pipeline ver-
sion 3.4 (Manning et al., 2014) and Illinois SRL
(Punyakanok et al., 2008; Clarke et al., 2012). We
used the Gurobi optimization package
3
for infer-
ence.
We compare our system PROREAD to baselines
that do not have access to the process structure:
1. BOW: For each answer, we compute the
proportion of content word lemmas covered
by the paragraph and choose the one with
higher coverage. For true-false questions, we
compute the coverage of the question state-
ment, and answer ?True? if it is higher than a
threshold tuned on the development set.
2. TEXTPROX: For dependency questions, we
align content word lemmas in both the ques-
tion and answer against the text and select the
answer whose aligned tokens are closer to the
aligned tokens of the question. For tempo-
ral questions, we return the answer for which
the order of events is identical to their order
in the paragraph. For true-false questions, we
return ?True? if the number of bigrams from
the question covered in the text is higher than
a threshold tuned on the development set.
3. SYNTPROX: For dependency questions, we
use proximity as in TEXTPROX, except that
distance is measured using dependency tree
edges. To support multiple sentences we con-
nect roots of adjacent sentences with bidi-
rectional edges. For temporal questions this
baseline is identical to TEXTPROX. For true-
false questions, we compute the number of
dependency tree edges in the question state-
ment covered by edges in the paragraph (an
edge has a source lemma, relation, and target
lemma), and answer ?True? if the coverage is
3
http://www.gurobi.com/
Method Depen. Temp. True-
false
All
PROREAD 68.1 80.0 55.6 66.7
SYNTPROX 61.9 70.0 48.1 60.0
TEXTPROX 58.4 70.0 33.3 54.7
BOW 47.8 40.0 44.4 46.7
GOLD 77.9 80.0 70.4 76.7
Table 4: Reading comprehension test set accuracy. The All
column shows overall accuracy across all questions. The first
three columns show accuracy for each coarse type.
higher than a threshold tuned on the training
set.
To separate the contribution of process struc-
tures from the performance of our structure pre-
dictor, we also run our QA system given manually
annotated gold standard structures (GOLD).
4
6.2 Reading Comprehension Task
We evaluate our system using accuracy, i.e., the
proportion of questions answered correctly. Ta-
ble 4 presents test set results, where we break
down questions by their coarse-type.
PROREAD improves accuracy compared to the
best baseline by 6.7 absolute points (last column).
Most of the gain is due to improvement on de-
pendency questions, which are the most common
question type. The performance of BOW indicates
that lexical coverage alone does not distinguish the
correct answer from the wrong answer. In fact,
guessing the answer with higher lexical overlap
results in performance that is slightly lower than
random. Text proximity and syntactic proximity
provide a stronger cue, but exploiting predicted
process structures substantially outperforms these
baselines.
Examining results using gold information high-
lights the importance of process structures inde-
pendently of the structure predictor. Results of
GOLD demonstrate that given gold structures we
can obtain a dramatic improvement of almost 17
points compared to the baselines, using our sim-
ple deterministic QA system.
Results on true-false questions are low for
PROREAD and all the baselines. True-false ques-
tions are harder for two main reasons. First, in
dependency and temporal questions, we create a
query for both answers, and can find a proof or
a refutation for either one of them. In true-false
4
We also ran an experiment where gold triggers are
given and arguments and relations are predicted. We found
that this results in slightly higher performance compared to
PROREAD.
1506
Precision Recall F
1
Triggers 75.4 73.9 74.6
Arguments 43.4 34.4 38.3
Relations 27.0 22.5 24.6
Table 5: Structured prediction test set results.
questions we must determine given a single state-
ment whether it holds. Second, an analysis of true-
false questions reveals that they focus less on re-
lations between events and entities in the process,
and require modeling lexical variability.
5
6.3 Structure Prediction Task
Our evaluation demonstrates that gold structures
improve accuracy substantially more than pre-
dicted structures. To examine this, we now di-
rectly evaluate the structure predictor by com-
paring micro-average precision, recall and F
1
be-
tween predicted and gold structures (Table 5).
While performance for trigger identification is
reasonable, performance on argument and relation
prediction is low. This explains the higher perfor-
mance obtained in reading comprehension given
gold structures. Note that errors in trigger predic-
tion propagate to argument and relation prediction
? a relation cannot be predicted correctly if either
one of the related triggers is not previously identi-
fied. One reason for low performance is the small
size of the dataset. Thus, training process predic-
tors with less supervision is an important direction
for future work. Furthermore, the task of process
prediction is inherently difficult, because often re-
lations are expressed only indirectly in text. For
example, in Figure 1 the relation between water
splitting and transfer of ions is only recoverable
by understanding that water provides the ions that
need to be transferred.
Nevertheless, we find that questions can often
be answered correctly even if the structure con-
tains some errors. For example, the gold structure
for the sentence ?Some . . . radioisotopes have
long half-lives, allowing . . . ?, contains the trigger
long half-lives, while we predict have as a trigger
and long half-lives as an argument. This is good
enough to answer questions related to this part of
the structure correctly, and overall, to improve per-
formance using predicted structures.
5
The low performance of TEXTPROX and SYNTPROX on
true-false questions can also be attributed to the fact that we
tuned a threshold parameter on the training set, and this did
not generalize well to the test set.
Reason GOLD PROREAD
Alignment 35% 15%
Missing from annotation 25% 10%
Entity coreference 20% 10%
Missing regular expression 10%
Lexical variability 5% 10%
Error in predicted structure 55%
Other 5%
Table 6: Error analysis results. An explanation of the vari-
ous categories are in the body of the paper.
6.4 Error Analysis
This section presents the results of an analysis of
20 sampled errors of GOLD (gold structures), and
20 errors of PROREAD (predicted structures). We
have categorized the primary reason for error in
Table 6.
As expected, the main problem when using pre-
dicted structures, is structure errors which account
for more than half of the errors.
Errors in GOLD are distributed across various
categories, which we briefly describe. Alignment
errors occur due to multiple words aligning to mul-
tiple triggers and arguments. For example, in the
question ?What is the result of gases being pro-
duced in the lysosome??, the answer ?engulfed
pathogens are poisoned? is incorrectly aligned to
the trigger engulfed rather than to poisoned.
Another reason for errors are cases where ques-
tions are asked about parts of the paragraph that
are missing from annotation. This is possible since
questions were authored independently of struc-
ture annotation. Two other causes for errors are
entity coreference errors, where a referent for an
entity is missing from the structure, and lexical
variability, where the author of questions uses
names for triggers or arguments that are missing
from the paragraph, and so alignment fails.
Last, in 10% of the cases in GOLD we found
that the answer could not be retrieved using the set
of regular expressions that are currently used by
our QA system.
7 Discussion
This work touches on several strands of work in
NLP including information extraction, semantic
role labeling, semantic parsing and reading com-
prehension.
Event and relation extraction have been studied
via the ACE data (Doddington et al., 2004) and
related work. The BioNLP shared tasks (Kim et
al., 2009; Kim et al., 2011; Riedel and McCal-
1507
lum, 2011) focused on biomedical data to extract
events and their arguments. Event-event relations
have been mostly studied from the perspective of
temporal ordering; e.g., (Chambers and Jurafsky,
2008; Yoshikawa et al., 2009; Do et al., 2012; Mc-
Closky and Manning, 2012). The process struc-
ture predicted in this work differs from these lines
of work in two important ways: First, we predict
events, arguments and their interactions from mul-
tiple sentences, while most earlier work focused
on one or two of these components. Second, we
model processes, and thus target causal relations
between events, rather than temporal order only.
Our semantic role annotation is similar to ex-
isting SRL schemes such as PropBank (Palmer et
al., 2005), FrameNet (Ruppenhofer et al., 2006)
and BioProp (Chou et al., 2006). However, in con-
trast to PropBank and FrameNet, we do not allow
all verbs to trigger events and instead let the an-
notators decide on biologically important triggers,
which are not restricted to verbs (unlike BioProp,
where 30 pre-specified verbs were selected for an-
notation). Like PropBank and BioProp, the argu-
ment labels are not trigger specific.
Mapping questions to queries is effectively a se-
mantic parsing task. In recent years, several lines
of work addressed semantic parsing using vari-
ous formalisms and levels of supervision (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2006; Clarke et al., 2010; Berant et al., 2013).
In particular, Krishnamurthy and Kollar (2013)
learned to map natural language utterances to ref-
erents in an image by constructing a KB from the
image and then mapping the utterance to a query
over the KB. This is analogous to our process of
constructing a process structure and performing
QA by querying that structure. In our work, we
parse questions into graph-based queries, suitable
for modeling processes, using a rule-based heuris-
tic. Training a statistical semantic parser that will
replace the QA system is an interesting direction
for future research.
Multiple choice reading comprehension tests
are a natural choice for evaluating machine read-
ing. Hirschman et al. (1999) presented a bag-of-
words approach to retrieving sentences for read-
ing comprehension. Richardson et al. (2013) re-
cently released the MCTest reading comprehen-
sion dataset that examines understanding of fic-
tional stories. Their work shares our goal of ad-
vancing micro-reading, but they do not focus on
process understanding.
Developing programs that perform deep reason-
ing over complex descriptions of processes is an
important step on the road to fulfilling the higher
goals of machine reading. In this paper, we present
an end-to-end system for reading comprehen-
sion of paragraphs which describe biological pro-
cesses. This is, to the best of our knowledge, the
first system to both predict a rich structured rep-
resentation that includes entities, events and their
relations, and utilize this structure for answering
reading comprehension questions. We also created
a new dataset, PROCESSBANK, which contains
200 paragraphs that are both fully-annotated with
process structure, as well as accompanied by ques-
tions. We empirically demonstrated that model-
ing biological processes can substantially improve
reading comprehension accuracy in this domain.
Acknowledgments
The authors would like to thank Luke
Amuchastegui for authoring the multiple-choice
questions, and also the anonymous reviewers for
their constructive feedback. We thank the Allen
Institute for Artificial Intelligence for assistance
in funding this work.
References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of EMNLP.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of AAAI.
Nathanael Chambers and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves
temporal ordering. In Proceedings of EMNLP.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan
Su, Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu.
2006. A semi-automatic method for annotating a
biomedical proposition bank. In Proceedings of the
Workshop on Frontiers in Linguistically Annotated
Corpora 2006, July.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of CoNLL.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
1508
Learned to Stop Worrying and Love NLP Pipelines).
In Proceedings of LREC.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of ACL.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program-Tasks, Data, and
Evaluation. In Proceedings of LREC.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In Proceedings of AAAI.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-Driven Learning for Open Ques-
tion Answering. In Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Lynette Hirschman, Marc Light, Eric Breck, and
John D. Burger. 1999. Deep read: A reading com-
prehension system. In Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of
BioNLP shared task 2011. In Proceedings of
BioNLP.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connect-
ing natural language to the physical world. TACL,
1:193?206.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
ACL.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX.
Thomas L. Magnanti and Laurence A. Wolsey. 1995.
Optimal trees. Handbooks in operations research
and management science, 7:503?615.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of ACL:
System Demonstrations.
Andr?e L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extrac-
tion. In Proceedings of EMNLP-CoNLL.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. Cogex: A logic prover
for question answering. In Proceedings of NAACL-
HLT.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. MCTest: A challenge dataset for
the open-domain machine comprehension of text. In
Proceedings of EMNLP.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL.
Josef Ruppenhofer, Michael Ellsworth, Miriam RL
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended theory and
practice. Berkeley FrameNet Release, 1.
Aju Thalappillil Scaria, Jonathan Berant, Mengqiu
Wang, Peter Clark, Justin Lewis, Brittany Harding,
and Christopher D. Manning. 2013. Learning bi-
ological processes with global constraints. In Pro-
ceedings of EMNLP.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the demonstra-
tions at EACL.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of HLT-NAACL.
1509
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with Markov logic. In Pro-
ceedings of ACL/IJCNLP.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
1510
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358?367,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Correcting Grammatical Verb Errors
Alla Rozovskaya
Columbia University
New York, NY 10115
ar3366@columbia.edu
Dan Roth
University of Illinois
Urbana, IL 61801
danr@illinois.edu
Vivek Srikumar
Stanford University
Stanford, CA 94305
svivek@cs.stanford.edu
Abstract
Verb errors are some of the most com-
mon mistakes made by non-native writers
of English but some of the least studied.
The reason is that dealing with verb er-
rors requires a new paradigm; essentially
all research done on correcting grammat-
ical errors assumes a closed set of trig-
gers ? e.g., correcting the use of prepo-
sitions or articles ? but identifying mis-
takes in verbs necessitates identifying po-
tentially ambiguous triggers first, and then
determining the type of mistake made and
correcting it. Moreover, once the verb is
identified, modeling verb errors is chal-
lenging because verbs fulfill many gram-
matical functions, resulting in a variety of
mistakes. Consequently, the little earlier
work done on verb errors assumed that the
error type is known in advance.
We propose a linguistically-motivated ap-
proach to verb error correction that makes
use of the notion of verb finiteness to iden-
tify triggers and types of mistakes, before
using a statistical machine learning ap-
proach to correct these mistakes. We show
that the linguistically-informed model sig-
nificantly improves the accuracy of the
verb correction approach.
1 Introduction
We address the problem of correcting grammati-
cal verb mistakes made by English as a Second
Language (ESL) learners. Recent work in ESL er-
ror correction has focused on errors in article and
preposition usage (Han et al., 2006; Felice and
Pulman, 2008; Gamon et al., 2008; Tetreault et
al., 2010; Gamon, 2010; Rozovskaya and Roth,
2010b; Dahlmeier and Ng, 2011).
While verb errors occur as often as article and
preposition mistakes, with a few exceptions (Lee
and Seneff, 2008; Gamon et al., 2009; Tajiri et al.,
2012), there has been little work on verbs. There
are two reasons for why it is difficult to deal with
verb mistakes. First, in contrast to articles and
prepositions, verbs are more difficult to identify
in text, as they can often be confused with other
parts of speech, and processing tools are known to
make more errors on noisy ESL data (Nagata et al.,
2011). Second, verbs are more complex linguisti-
cally: they fulfill several grammatical functions,
and these different roles imply different types of
errors.
These difficulties have led all previous work
on verb mistakes to assume prior knowledge of
the mistake type; however, identifying the specific
category of a verb error is nontrivial, since the sur-
face form of the verb may be ambiguous, espe-
cially when that verb is used incorrectly. Consider
the following examples of verb mistakes:
1. ?We discusses*/discuss this every time.?
2. ?I will be lucky if I {will find}*/find something that
fits.?
3. ?They wanted to visit many places without
spend*/spending a lot of money.?
4. ?They arrived early to organized*/organize every-
thing?.
These examples illustrate three grammatical
verb properties: Agreement, Tense, and non-finite
Form choice that encompass the most common
grammatical verb problems for ESL learners. The
first two examples show mistakes on verbs that
function as main verbs in a clause: sentence (1)
shows an example of subject-verb Agreement er-
ror; (2) is an example of a Tense mistake where
the ambiguity is between {will find} (Future tense)
358
and find (Present tense). Examples (3) and (4) dis-
play Form mistakes: confusing the infinitive and
gerund forms in (3) and including an inflection on
an infinitive verb in (4).
This paper addresses the specific challenges of
verb error correction that have not been addressed
previously ? identifying candidates for mistakes
and determining which class of errors is present,
before proceeding to correct the error. The ex-
perimental results show that our linguistically-
motivated approach benefits verb error correction.
In particular, in order to determine the error type,
we build on the notion of verb finiteness to distin-
guish between finite and non-finite verbs (Quirk et
al., 1985), that correspond to Agreement and Tense
mistakes (examples (1) and (2) above) and Form
mistakes (examples (3) and (4) above), respec-
tively (see Sec. 3). The approach presented in this
work was evaluated empirically and competitively
in the context of the CoNLL shared task on error
correction (Ng et al., 2013) where it was imple-
mented as part of the highest-scoring University
of Illinois system (Rozovskaya et al., 2013) and
demonstrated superior performance on the verb er-
ror correction sub-task.
This paper makes the following contributions:
?We present a holistic, linguistically-motivated
framework for correcting grammatical verb mis-
takes; our approach ?starts from scratch? with-
out any knowledge of which mistakes should be
corrected or of the mistake type; in doing that
we show that the specific challenges of verb error
correction are better addressed by first identifying
the finiteness of the verb in the error identification
stage.
? Within the proposed model, we describe and
evaluate several methods of selecting verb candi-
dates, an algorithm for determining the verb type,
and a type-driven verb error correction system.
?We annotate a subset of the FCE data set with
gold verb candidates and gold verb type.
1
2 Related Work
Earlier work in ESL error correction follows the
methodology of the context-sensitive spelling cor-
rection task (Golding and Roth, 1996; Golding
and Roth, 1999; Banko and Brill, 2001; Carlson
et al., 2001; Carlson and Fette, 2007). Most of
the effort in ESL error correction so far has been
1
The annotation is available at http://cogcomp.cs.illinois.
edu/page/publication view/743
on article and preposition usage errors, as these
are some of the most common mistakes among
non-native English speakers (Dalgish, 1985; Lea-
cock et al., 2010). These phenomena are generally
modeled as multiclass classification problems: a
single classifier is trained for a given error type
where the set of classes includes all articles or the
top n most frequent English prepositions (Izumi
et al., 2003; Han et al., 2006; Felice and Pul-
man, 2008; Gamon et al., 2008; Tetreault et al.,
2010; Rozovskaya and Roth, 2010b; Rozovskaya
and Roth, 2011; Dahlmeier and Ng, 2011).
Mistakes on verbs have attracted significantly
less attention in the error correction literature.
Moreover, the little earlier work done on verb er-
rors only considered subsets of these errors and
assumed the error sub-type is known in advance.
Gamon et al. (2009) mentioned a model for learn-
ing gerund/infinitive confusions and auxiliary verb
presence/choice. Lee and Seneff (2008) proposed
an approach based on pattern matching on trees
combined with word n-gram counts for correcting
agreement misuse and some types of verb form
errors. However, they excluded tense mistakes,
which is the most common error category for ESL
learners (40% of all verb errors, Sec. 3). Tajiri
et al. (2012) considered only tense mistakes. In
the above studies, it was assumed that the type of
mistake that needs to be corrected is known, and
irrelevant verb errors were excluded (e.g., Tajiri
et al. (2012) addressed only tense mistakes and
excluded from the evaluation other kinds of verb
errors). In other words, it was assumed that part
of the task was solved. But, unlike in article and
preposition error correction where the type of mis-
take is known based on the surface form of the
word, in verb error correction, it is not obvious.
The key distinction of our work is that we pro-
pose a holistic approach that starts from ?scratch?
and, given an instance, first detects a mistake and
identifies its type, and then proceeds to correct
it. We also evaluate several methods for select-
ing verb candidates and show the significance of
this step for improving verb error correction per-
formance, while earlier studies do not discuss this
aspect of the problem. In the CoNLL shared task
(Ng et al., 2013) that included verb errors in agree-
ment and form, the participating teams did not pro-
vide details on how specific challenges were han-
dled, but the University of Illinois system obtained
the highest score on the verb sub-task, even though
359
Tag Error type Rel. freq. (%)
TV Tense 40.0
FV Form 22.3
AGV Verb-subject agreement 11.5
MV Missing verb 11.7
UV Unneccesary verb 7.3
IV Inflection 5.4
DV Derivation 1.8
Total 6640
Table 1: Grammatical verb errors in FCE.
all teams used similar resources (Ng et al., 2013).
3 Verb Errors in ESL Writing
Verb-related errors are very prominent among
non-native English speakers: grammatical mis-
use of verbs constitutes one of the most com-
mon errors in several learner corpora, including
those previously used (Izumi et al., 2003; Lee
and Seneff, 2008) and the one employed in this
work. We study verb errors using the FCE cor-
pus (Yannakoudakis et al., 2011). The corpus
possesses several desirable characteristics: it is
large (500,000 words), has been annotated by na-
tive English speakers, and contains data by learn-
ers of multiple first-language backgrounds. The
FCE corpus contains 5056 determiner errors, 5347
preposition errors, and 6640 grammatical verb
mistakes (Table 1).
3.1 Verb Finiteness
There are many grammatical categories for which
English verbs can be marked. The linguistic no-
tion of verb finiteness or verb type (Radford, 1988;
Quirk et al., 1985) distinguishes between verbs
that function on their own in a clause as main verbs
(finite) and those that do not (non-finite). Gram-
matical properties associated with each group are
mutually exclusive: tense and agreement markers,
for example, do not apply to non-finite verbs; non-
finite verbs are not marked for many grammatical
functions but may appear in several forms.
The most common verb problems for ESL
learners ? Tense, Agreement, non-finite Form ?
involve verbs both in finite and non-finite roles.
Table 2 illustrates contexts that license finite and
non-finite verbs.
Our intuition is that, because properties associ-
ated with each verb type are mutually exclusive,
verb finiteness should benefit verb error correc-
tion models: an observed verb error may be due
to several grammatical phenomena, and knowing
which phenomena are active depends on the func-
tion of the verb in the current context. Note that
Agreement, Tense, and Form errors account for
Category Agreement Kappa Random
Correct verbs 0.97 0.95 0.51
Erroneous verbs 0.88 0.81 0.41
Table 3: Inter-annotator agreement based on 250 verb
errors and 250 correct verbs, randomly selected.
about 74% of all grammatical verb errors in Ta-
ble 1 but the finiteness distinction applies to all
English verbs ? every verb is either finite or non-
finite in a specific syntactic context ? and is also
relevant for the remaining mistakes not addressed
here.
2
4 Annotation for Verb Finiteness
In order to evaluate the quality of the algorithm
for verb finiteness and of the candidate selection
methods, we annotated all verbs ? correct and er-
roneous ? in a random set of 124 documents from
our corpus with the information about verb finite-
ness. We refer to these 124 documents as gold sub-
set. We also annotated erroneous verbs in the re-
maining 1120 documents of the corpus. The anno-
tation was performed by two students with back-
ground in Linguistics. The inter-annotator agree-
ment is shown in Table 3 and is high.
Annotating Verb Errors For each verb error that
was tagged as Tense (TV), Agreement (AGV), and
Form (FV), the annotators marked verb finiteness.
Additionally, the annotators also specified the type
of error (Tense, Agreement, or Form) (Table 4),
since the FCE tags do not always correspond to
the three error types we study here. For exam-
ple, the FV tag may mark errors on finite verbs.
Overall, about 7% of verb errors have to do with
phenomena different from the three verb proper-
ties considered in this work and thus are excluded
from the present study.
Annotating Correct Verbs Correct verbs were
identified in text using an automated proce-
dure that relies on part-of-speech information
(Sec. 5.1). Valid candidates were specified for
verb finiteness. The candidates that were iden-
tified incorrectly due to mistakes by the part-of-
speech tagger were marked as invalid.
5 The Computational Model
The verb error correction problem is formulated
as a classification task in the spirit of the learn-
2
For instance, the missing verb errors (MV, 11.7%) re-
quire an additional step to identify contexts for missing verbs,
and then appropriate verb properties need to be determined
based on verb finiteness.
360
Verb type Example Verb properties
Agreement Tense Form
Finite
?He discussed this with me last week? - Past Simple -
?He discusses this with me every week.? 3rd person,Sing. Present Simple -
Non-finite
?He left without discussing it with me.? - - Gerund
?They let him discuss this with me.? - - Infinitive
?To discuss this now would be ill-advised.? - - to-Infinitive
Table 2: Contexts that license finite and non-finite verbs and the corresponding active properties.
Error on Verb Type Subcategory Example
Finite (67.7%)
Agreement (20%) ?We discusses*/discuss this every time.?
Tense (80%) ?If you buy something, you {would be}*/{will be} happy.?
Non-finite (25.3%)
?If one is famous he has to accept the disadvantages of be*/being famous.? ?I am very
glad {for receiving}*/{to receive} it.?
?They arrived early to organized*/organize everything.?
Other errors (7.0%)
Passive/Active(42.3%) ?Our end-of-conference party {is included}*/includes dinner and dancing.?
Compound (40.7%) ?You ask me for some informations*/information- here they*/it are*/is.?
Other (16.8%) ?Nobody {has to be}*/{should be} late.?
Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus.
ing paradigm commonly used for correcting other
ESL errors (Sec. 2), with the exception that the
verb model includes additional components. All
of the components are listed below:
1. Candidate selection (5.1)
2. Verb finiteness prediction (5.2)
3. Feature generation (5.3)
4. Error identification (5.4)
5. Error correction (5.5)
After verb candidates are selected, verb finite-
ness is determined and features are generated for
each candidate. The finiteness prediction is used
in the error identification component. Given the
output of the error identification stage, the corre-
sponding classifiers for each error type are invoked
to propose an appropriate correction.
We split the corpus documents into two equal
parts ? training and test. We chose a train-test split
and not cross-validation, since the FCE data set is
quite large to allow for such a split. The training
data is also used to develop the components for
candidate selection and verb finiteness prediction.
5.1 Candidate Selection
This stage selects the set of verb instances that
are presented as input to the classifier. A verb in-
stance refers to the verb, including its auxiliaries
or the infinitive marker (e.g. ?found?, ?will find?,
?to find?). Candidate selection is a crucial step for
models that correct mistakes on open-class words
because those errors that are missed at this stage
have no chance of being detected. We implement
four candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as identi-
fied by a shallow parser (Punyakanok and Roth,
2001).
3
Method (2) also includes words tagged
with one of the verb tags: {VB, VBN, VBG,
VBD, VBP, VBZ} predicted by the POS tagger.
4
However, relying on the POS information is not
good enough, since the POS tagger performance
on ESL data is known to be suboptimal (Nagata et
al., 2011). For example, verbs lacking agreement
markers are likely to be mistagged as nouns (Lee
and Seneff, 2008). Methods (3) and (4) address
the problem of pre-processing errors. Method (3)
adds words that are on the list of valid English
verb lemmas; the lemma list is constructed us-
ing a POS-tagged version of the NYT section of
the Gigaword corpus and contains about 2,600 of
frequently-occurring words tagged as VB; for ex-
ample, (3) will add shop but not shopping, but (4)
will add both.
For methods (3) and (4), we developed verb-
Morph,
5
a tool that performs morphological anal-
ysis on verbs and is used to lemmatize verbs and
to generate morphological variants. The module
makes uses of (1) the verb lemma list and (2) a list
of irregular English verbs.
The quality of the candidate selection methods
is evaluated in Table 5 on the gold subset by com-
puting the recall, i.e. the percentage of erroneous
verbs that have been selected as candidates. Meth-
ods that address pre-processing mistakes are able
to recover more erroneous verb candidates in text.
It is also interesting to note that across all methods,
the highest recall is obtained for tense errors. This
suggests that the POS tagger is more prone to fail-
3
http://cogcomp.cs.illinois.edu/demo/shallowparse
4
http://cogcomp.cs.illinois.edu/page/software view/POS
5
The tool and more detail about it can be found at
http://cogcomp.cs.illinois.edu/page/publication view/743
361
Method Recall Recall by error group (%)
(%) Agr. Tense Form
(1) All verb phrases 83.00 86.62 93.55 59.08
(2) + tokens tagged as verbs 91.96 90.30 94.33 87.79
(3) + tokens that are valid
verb lemmas
95.50 95.99 96.46 93.23
(4) + tokens with inflections
that are valid verb lemmas
96.09 96.32 96.62 94.84
Table 5: Candidate selection methods performance.
ure due to errors in agreement and form. The eval-
uation in Table 5 uses recall, as the goal is to assess
the ability of the methods to select erroneous verbs
as candidates. In Sec. 6.1, the contribution of each
method to error identification is evaluated.
5.2 Predicting Verb Finiteness
Predicting verb finiteness is not trivial, as almost
all English verbs can occur in both finite and non-
finite form and the surface forms of a verb in finite
and non-finite form may be the same (see Table 2).
While we cannot learn verb type automatically
due to lack of annotation, we show, however, that,
for the majority of verbs, finiteness can be reliably
predicted using linguistic knowledge. We imple-
ment a decision-list classifier that makes use of
linguistically-motivated rules (Table 6). The algo-
rithm covers about 92% of all verb candidates, ab-
staining on the remaining highly-ambiguous 8%.
The evaluation of the method on the gold sub-
set (last column in Table 6) shows that despite its
simplicity, this method is highly effective: 98% on
correct verbs and over 89% on errors.
5.3 Features
The baseline features are word n-grams in the 4-
word window around the verb instance. Addi-
tional features are intended to characterize a given
error type and are selected based on previous stud-
ies: for Agreement and Form errors, we use a
parser (Klein and Manning, 2003) and define fea-
tures that reflect dependency relations between the
verb and its neighbors. We denote these features
by syntax. Syntactic knowledge via tree patterns
has been shown useful for Agreement mistakes
(Lee and Seneff, 2008). Features for Tense in-
clude temporal adverbs in the sentence and tenses
of other verbs in the sentence and are similar to
the features used in other verb classification tasks
(Reichart and Rappoport, 2010; Lee, 2011; Tajiri
et al., 2012). The features are shown in Table 7.
5.4 Error Identification
The goal of this stage is to identify errors and to
predict their type. We define a linear model where,
given a verb, a weight vector w assigns a score
to each label in the label space {Correct, Form,
Agreement, Tense}. The prediction of the classi-
fier is the label with the highest score.
The baseline error identification model, called
combined, is agnostic to the type of the verb. In
the combined model, for each verb v and label l,
we generate a feature vector, ?(v, l) and the best
label is predicted as
argmax
l
w
T
?(v, l).
The combined model makes use of all the fea-
tures we have defined earlier for each verb.
The type-based model uses the verb finiteness
prediction made by the verb finiteness classifier.
A soft way to use the finiteness prediction is to
add the predicted finiteness value as a feature. The
other ? hard-decision approach ? is to use only
a subset of the features depending on the pre-
dicted finiteness: Agreement and Tense for the fi-
nite verbs, and Form features for non-finite. The
hard-decision type-driven approach defines a fea-
ture vector for a verb based on its type. Thus,
given the verb v and its type t, we define fea-
tures ?(v, t, l) for each label l. Thus, the label is
predicted as
argmax
l
w
T
?(v, t, l).
5.5 Error Correction
The correction module consists of three compo-
nents, one for each type of mistake. Given the
output of the error identification model, the ap-
propriate correction component is run for each in-
stance predicted to be a mistake.
6
The verb finite-
ness prediction is used to select finite instances for
training the Agreement and Tense components and
non-finite ? for the Form component. The label
space for Tense specifies tense and aspect prop-
erties of the English verbs (see Tajiri et al., 2012
for more detail), the Agreement component spec-
ifies the person and number properties, while the
Form component includes the commonly confus-
able non-finite English forms (see Table 2). These
components are trained as multiclass classifiers.
6
We assume that each verb contains at most one mistake.
Less than 1% of all erroneous verbs have more than one error
present.
362
A verb is Non-Finite if any of the following hold: A verb is Finite if any of the following hold Accuracy on
Correct Erroneous
verbs verbs
(1) All verbs identified by shallow parser
98.01 89.4
(1) [numTokens = 2] ? [firstToken = to] (2) can; could
(2) firstToken = be (3) [numTokens = 1] ? [pos ? {V BD, V BP, V BZ}]
(3) [numTokens = 1] ? [pos = V BG] (4) [numTokens = 2] ? [firstToken! = to]
(5) numTokens > 2
Table 6: Algorithm for determining verb type. numTokens denotes the number of tokens in the verb instance, e.g., for the
verb instance ?to go?, numTokens = 2. Verbs not covered by the rules, e.g. those that are not tagged with a verb-related POS
in methods (3) and (4), are not assigned any verb type. The last column shows algorithm accuracy on the gold subset separately
for correct and incorrect verbs.
Agreement Description
(1) subjHead, subjPOS The surface form and the POS tag of the subject head
(2) subjDet {those,this,..} Determiner of the subject phrase
(3) subjDistance Distance between the verb and the subject head
(4) subjNumber {Sing, Pl} Sing ? singular pronouns and nouns; Pl ? plural pronouns and nouns
(5) subjPerson {3rdSing, Not3rdSing, 1stSing} 3rdSing ? she,he,it,singular nouns; Not3rdSing ? we,you,they, plural nouns; 1stSing ? ?I?
(6) conjunctions (1)&(3);(4)&(5)
Tense Description
(1) verb phrase (VP) verb lemma, negation, surface forms and POS tags of all words in the verb phrase
(2) verbs in sentence(4 features) tenses and lemmas of the finite verbs preceding and following the verb instance
(3) time adverbs (2 features) temporal adverb before and after the verb instance
(4) bag-of-words (BOW) (8 features) Includes the following words in the sentence: {if, when, since, then, wish, hope, when, since,
after}
Form Description
(1) closest word surface form, lemma, POS tag, and distance of the closest open-class word to the left of the
verb
(2) governor surface form, POS tag and dependency type of the target
(3) preposition if the verb is preceded by a preposition: preposition itself and the surface form, POS tag and
dependency of the governor of the preposition
(4) pos and lemma POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word
ngrams
Table 7: Features used, grouped by error type.
6 Experiments
The main goal of this work is to propose a uni-
fied framework for correcting verb mistakes and
to address the specific challenges of the problem.
We thus do not focus on features or on the spe-
cific learning algorithm. Our experimental study
addresses the following research questions:
I. Linguistic questions: (i) candidate selection
methods; (ii) verb finiteness contribution to
error identification
II. Computational Framework: error identifi-
cation vs. correction
III. Gold annotation: (i) using gold candidates
and verb type vs. automatic; (ii) performance
comparison by error type
Learning Framework There is a lot of under-
standing for which algorithmic methods work
best for ESL correction tasks, how they compare
among themselves, and how they compare to n-
gram based methods. Specifically, despite their in-
tuitive appeal, language models were shown to not
work well on these tasks, while the discriminative
learning framework has been shown to be superior
to other approaches and thus is commonly used
for error correction tasks (see Sec. 2). Since we
do not address the algorithmic aspect of the prob-
lem, we refer the reader to Rozovskaya and Roth
(2011) for a discussion of these issues. We train
all our models with the SVM learning algorithm
implemented in JLIS (Chang et al., 2010).
Evaluation We report both Precision/Recall
curves and AAUC (as a summary). Error cor-
rection is generally evaluated using F1 (Dale et
al., 2012); Precision and Recall (Gamon, 2010;
Tajiri et al., 2012); or Average Area Under Curve
(AAUC) (Rozovskaya and Roth, 2011). For a dis-
cussion on these metrics with respect to error cor-
rection tasks, we refer the reader to Rozovskaya
(2013). AAUC (Hanley and McNeil, 1983)) is a
measure commonly used to generate a summary
statistic, computed as an average precision value
over a range of recall points. In this paper, AAUC
is computed over the first 15 recall points:
AAUC =
1
15
?
15
?
i=1
Precision(i).
6.1 Linguistic Questions
Candidate Selection Methods The contribution
of the candidate selection component with respect
to error identification is evaluated in Table 8, us-
ing the methods presented in Sec. 5.1. Overall,
363
Recall of candidate AAUC
selection method (%) Combined Type-based
(1) (83.00) 73.38 79.49
(2) (91.96) 80.36 86.48
(3) (95.50) 81.39 87.05
(4) (96.09) 81.27 86.81
Table 8: Impact of candidate selection methods on error
identification performance. The first column shows the per-
centage of erroneous verbs selected by each method. Type-
based models are discussed in Sec. 6.1.
Correct verbs Erroneous verbs Error rate
Training 41721 1981 4.75%
Test 41836 2014 4.81%
Table 9: Training and test data statistics. Candidates are
selected using method (3).
better performance is achieved by methods with
higher recall, with the exception of method (4); its
performance on error identification is behind that
of method (3), perhaps due to the amount of noise
that is also added. While the difference is small,
method (3) is also simpler than method (4). We
thus use method (3) in the rest of the paper. Table
9 shows the number of verb instances in training
and test selected with this method.
Verb Finiteness Sec. 5.4 presented two ways of
adding verb finiteness: (1) adding the predicted
verb type as a feature and (2) selecting only the
relevant features depending on the finiteness of the
verb. Table 10 shows the results of using verb type
in the error identification stage. While the first
approach does not provide improvement over the
combined model, the second method is very ef-
fective. We conjecture that because verb type pre-
diction is quite accurate, the second, hard-decision
approach is preferred, as it provides knowledge in
a direct way. Henceforth, we will use the second
method in the type-based model.
Fig. 1 compares the performance of the com-
bined and the hard-decision type-based models
shown in Table 10. Precision/Recall curves are
generated by varying the threshold on the confi-
dence of the classifier. This graph reveals the be-
havior of the systems at multiple recall points: we
observe that at every recall point the type-based
classifier has higher precision.
So far, the models used all features defined in
Sec. 5.3. Table 11 reveals that the type-driven
Model AAUC
Combined 81.39
Type-based I (soft) 81.11
Type-based II (hard) 87.05
Table 10: Verb finiteness contribution to error identifi-
cation.
 
70
 
75
 
80
 
85
 
90
 
95  0
 
2
 
4
 
6
 
8
 
10
 
12
 
14
PRECISION
RECAL
L
Comb
ined
Type-
based
Figure 1: Verb finiteness contribution to error identifi-
cation: key result. AAUC shown in Table 10. The combined
model uses no verb type information. In the hard-decision
type-based model, each verb uses the features according to
its finiteness. The differences are statistically significant (Mc-
Nemar?s test, p < 0.0001).
Feature set AAUC
Combined Type-based
Baseline 46.62 49.72
All?Syntax 79.47 84.88
Full feature set 81.39 87.05
Table 11: Verb finiteness contribution to error identifi-
cation for different features.
approach is superior to the combined approach
across different feature sets, and the performance
gap increases with more sophisticated feature sets,
which is to be expected, since more complex fea-
tures are tailored toward relevant verb errors. Fur-
thermore, adding features specific to each error
type significantly improves the performance over
the word n-gram features. The rest of the experi-
ments use all features (denoted Full feature set).
6.2 Identification vs. Correction
After running the error identification component,
we apply the appropriate correction models to
those instances identified as errors. The results
for identification and correction are shown in Ta-
ble 12. The correction models are also finiteness-
aware models trained on the relevant verb in-
stances (finite or non-finite), as predicted by the
verb finiteness classifier.
We evaluate the correction components by fix-
ing a recall point in the error identification stage.
7
We observe the relatively low recall obtained by
the models. Error correction models tend to have
low recall (see, for example, the recent shared
tasks on ESL error correction (Dale and Kilgar-
riff, 2011; Dale et al., 2012; Ng et al., 2013)). The
key reason for the low recall is the error sparsity:
over 95% of verbs are correct, as shown in Table 9.
7
We can increase recall using a different threshold but
higher precision is preferred in error correction tasks.
364
Error type Correction Identification
P R F1 P R F1
Agreement 90.62 9.70 17.52 90.62 9.70 17.52
Tense 60.51 7.47 13.31 86.62 10.70 19.05
Form 81.82 16.34 27.24 83.47 16.67 27.79
Total 71.94 10.24 17.94 85.81 12.22 21.20
Table 12: Performance of the complete model after the
correction stage. The results on Agreement mistakes are the
same, since Agreement errors are always binary decisions,
unlike Tense and Form mistakes.
The only way to improve over this 95% baseline is
by forcing the system to have very good precision
(at the expense of recall). The performance shown
in Table 12 corresponds to an accuracy of 95.60%
in identification (error reduction of 8.7%) and
95.40% in correction (error reduction of 4.5%)
over the baseline of 95.19%.
6.3 Analysis on Gold Data
To further study the impact of each step of the sys-
tem, we analyze our model on the gold subset of
the data. The gold subset contains two additional
pieces of information not available for the rest of
the corpus: gold verb candidates and gold verb
finiteness (Sec. 4). The set contains 7784 gold
verbs, including 464 errors. Experiments are run
in 10-fold cross-validation where on each run 90%
of the documents are used for training and the re-
maining 10% are used for evaluation. The gold
annotation can be used instead of automatic pre-
dictions in two system components: (1) candidate
selection and (2) verb finiteness.
Table 13 shows the performance on error identi-
fication when gold vs. automatic settings are used.
As expected, using the gold verb type is more ef-
fective than using the automatic one, both with au-
tomatic and gold candidates. The same is true for
candidate selection. For instance, the combined
model improves by 14 AAUC points (from 55.90
to 69.86) with gold candidates. These results indi-
cate that candidate selection is an important com-
ponent of the verb error correction system.
Note that compared to the performance on the
entire data set (Table 10), the performance of the
models shown here that use automatic components
is lower, since the training size is smaller. On the
other hand, because of the smaller training size,
the gain due to the type-based approach is larger
on the gold subset (19 vs. 6 AAUC points).
Finally, in Table 14, we evaluate the contribu-
tion of verb finiteness to error identification by er-
ror type. While performance varies by error, it is
clear that all errors benefit from verb typing.
Candidate selection Verb type prediction AAUC
Automatic
None 55.90
Automatic 74.72
Gold 89.45
Gold
None 69.86
Automatic 90.89
Gold 96.42
Table 13: Gold subset: error identification with gold vs.
automatic candidates and finiteness information. Value
None for verb type prediction denotes the combined model.
Error type AAUC
Combined Type-based Type-based
Automatic Gold
Agreement 86.80 88.43 89.21
Tense 18.07 25.62 26.87
Form 97.08 98.23 98.36
Table 14: Gold subset: gold vs. automatic finiteness con-
tribution to error identification by error type.
7 Conclusion
Verb errors are commonly made by ESL writers
but difficult to address due to to their diversity
and the fact that identifying verbs in (noisy) text
may itself be difficult. We develop a linguistically-
inspired approach that first identifies verb candi-
dates in noisy learner text and then makes use
of verb finiteness to identify errors and character-
ize the type of mistake. This is important, since
most errors made by non-native speakers cannot
be identified by considering only closed classes
(e.g., prepositions and articles). Our model inte-
grates a statistical machine learning approach with
a rule-based system that encodes linguistic knowl-
edge to yield the first general correction approach
to verb errors (that is, one that does not assume
prior knowledge of which mistake was made).
This work thus provides a first step in consider-
ing more general algorithmic paradigms for cor-
recting grammatical errors and paves the way for
developing models to address other ?open-class?
mistakes.
Acknowledgments
The authors thank Graeme Hirst, Julia Hockenmaier, Mark
Sammons, and the anonymous reviewers for their helpful
feedback. This work was done while the first and the third
authors were at the University of Illinois. This material is
based on research sponsored by DARPA under agreement
number FA8750-13-2-0008 and by the Army Research Lab-
oratory (ARL) under agreement W911NF-09-2-0053. Any
opinions, findings, conclusions or recommendations are those
of the authors and do not necessarily reflect the view of the
agencies.
365
References
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of 39th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 26?33,
Toulouse, France, July.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Pro-
ceedings of the IEEE International Conference on
Machine Learning and Applications (ICMLA).
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of
the National Conference on Innovative Applications
of Artificial Intelligence (IAAI), pages 45?50.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Dahlmeier and H. T. Ng. 2011. Grammatical er-
ror correction with alternating structure optimiza-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 915?923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
R. De Felice and S. Pulman. 2008. A classifier-based
approach to preposition and determiner error correc-
tion in L2 English. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 169?176, Manchester, UK,
August.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings
of IJCNLP.
M. Gamon, C. Leacock, C. Brockett, W. B. Dolan,
J. Gao, D. Belenko, and A. Klementiev. 2009. Us-
ing statistical techniques and web search to correct
ESL errors. CALICO Journal, Special Issue on Au-
tomatic Analysis of Learner Language, 26(3):491?
511.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?
171, Los Angeles, California, June.
A. R. Golding and D. Roth. 1996. Applying Winnow
to context-sensitive spelling correction. In Proc. of
the International Conference on Machine Learning
(ICML), pages 182?190.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115?129.
J. Hanley and B. McNeil. 1983. A method of com-
paring the areas under receiver operating character-
istic curves derived from the same cases. Radiology,
148(3):839?843.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in
the Japanese learners? English spoken data. In The
Companion Volume to the Proceedings of 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 145?148, Sapporo, Japan, July.
T.-H. Kao, Y.-W. Chang, H. w. Chiu, T-.H. Yen, J. Bois-
son, J. c. Wu, and J.S. Chang. 2013. Conll-2013
shared task: Grammatical error correction nthu sys-
tem description. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 20?25, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Pro-
cessing Systems 15 NIPS, pages 3?10. MIT Press.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan and Claypool Publish-
ers.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
J. Lee. 2011. Verb tense generation. Social and Be-
havioral Sciences, 27:122?130.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210?1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
366
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995?1001. MIT Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
A. Radford. 1988. Transformational Grammar. Cam-
bridge University Press.
R. Reichart and A. Rappoport. 2010. Tense sense
disambiguation: A new syntactic polysemy task.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
325?334, Cambridge, MA, October. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Training
paradigms for correcting errors in grammar and us-
age. In Proceedings of the NAACL-HLT.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL, Portland, Oregon, 6. Association for Com-
putational Linguistics.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya. 2013. Automated Methods for Text
Correction. Ph.D. thesis.
T. Tajiri, M. Komachi, and Y. Matsumoto. 2012. Tense
and aspect error correction for esl learners using
global context. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 198?202,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Us-
ing parse features for preposition selection and error
detection. In ACL.
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011.
A new dataset and method for automatically grading
esol texts. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 180?189, Portland, Oregon, USA, June.
Association for Computational Linguistics.
367
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429?437,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Discriminative Learning over Constrained Latent Representations
Ming-Wei Chang and Dan Goldwasser and Dan Roth and Vivek Srikumar
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang,goldwas1,danr,vsrikum2}@uiuc.edu
Abstract
This paper proposes a general learning frame-
work for a class of problems that require learn-
ing over latent intermediate representations.
Many natural language processing (NLP) de-
cision problems are defined over an expressive
intermediate representation that is not explicit
in the input, leaving the algorithm with both
the task of recovering a good intermediate rep-
resentation and learning to classify correctly.
Most current systems separate the learning
problem into two stages by solving the first
step of recovering the intermediate representa-
tion heuristically and using it to learn the final
classifier. This paper develops a novel joint
learning algorithm for both tasks, that uses the
final prediction to guide the selection of the
best intermediate representation. We evalu-
ate our algorithm on three different NLP tasks
? transliteration, paraphrase identification and
textual entailment ? and show that our joint
method significantly improves performance.
1 Introduction
Many NLP tasks can be phrased as decision prob-
lems over complex linguistic structures. Successful
learning depends on correctly encoding these (of-
ten latent) structures as features for the learning sys-
tem. Tasks such as transliteration discovery (Kle-
mentiev and Roth, 2008), recognizing textual en-
tailment (RTE) (Dagan et al, 2006) and paraphrase
identification (Dolan et al, 2004) are a few proto-
typical examples. However, the input to such prob-
lems does not specify the latent structures and the
problem is defined in terms of surface forms only.
Most current solutions transform the raw input into
a meaningful intermediate representation1, and then
encode its structural properties as features for the
learning algorithm.
Consider the RTE task of identifying whether the
meaning of a short text snippet (called the hypoth-
esis) can be inferred from that of another snippet
(called the text). A common solution (MacCartney
et al, 2008; Roth et al, 2009) is to begin by defining
an alignment over the corresponding entities, pred-
icates and their arguments as an intermediate rep-
resentation. A classifier is then trained using fea-
tures extracted from the intermediate representation.
The idea of using a intermediate representation also
occurs frequently in other NLP tasks (Bergsma and
Kondrak, 2007; Qiu et al, 2006).
While the importance of finding a good inter-
mediate representation is clear, emphasis is typi-
cally placed on the later stage of extracting features
over this intermediate representation, thus separat-
ing learning into two stages ? specifying the la-
tent representation, and then extracting features for
learning. The latent representation is obtained by an
inference process using predefined models or well-
designed heuristics. While these approaches often
perform well, they ignore a useful resource when
generating the latent structure ? the labeled data for
the final learning task. As we will show in this pa-
per, this results in degraded performance for the ac-
tual classification task at hand. Several works have
considered this issue (McCallum et al, 2005; Gold-
wasser and Roth, 2008b; Chang et al, 2009; Das
and Smith, 2009); however, they provide solutions
1In this paper, the phrases ?intermediate representation? and
?latent representation? are used interchangeably.
429
that do not easily generalize to new tasks.
In this paper, we propose a unified solution to the
problem of learning to make the classification deci-
sion jointly with determining the intermediate rep-
resentation. Our Learning Constrained Latent Rep-
resentations (LCLR) framework is guided by the in-
tuition that there is no intrinsically good intermedi-
ate representation, but rather that a representation is
good only to the extent to which it improves perfor-
mance on the final classification task. In the rest of
this section we discuss the properties of our frame-
work and highlight its contributions.
Learning over Latent Representations This pa-
per formulates the problem of learning over latent
representations and presents a novel and general so-
lution applicable to a wide range of NLP applica-
tions. We analyze the properties of our learning
solution, thus allowing new research to take advan-
tage of a well understood learning and optimization
framework rather than an ad-hoc solution. We show
the generality of our framework by successfully ap-
plying it to three domains: transliteration, RTE and
paraphrase identification.
Joint Learning Algorithm In contrast to most
existing approaches that employ domain specific
heuristics to construct intermediate representations
to learn the final classifier, our algorithm learns to
construct the optimal intermediate representation to
support the learning problem. Learning to represent
is a difficult structured learning problem however,
unlike other works that use labeled data at the in-
termediate level, our algorithm only uses the binary
supervision supplied for the final learning problem.
Flexible Inference Successful learning depends
on constraining the intermediate representation with
task-specific knowledge. Our framework uses the
declarative Integer Linear Programming (ILP) infer-
ence formulation, which makes it easy to define the
intermediate representation and to inject knowledge
in the form of constraints. While ILP has been ap-
plied to structured output learning, to the best of our
knowledge, this is the first work that makes use of
ILP in formalizing the general problem of learning
intermediate representations.
2 Preliminaries
We introduce notation using the Paraphrase Iden-
tification task as a running example. This is the bi-
nary classification task of identifying whether one
sentence is a paraphrase of another. A paraphrase
pair from the MSR Paraphrase corpus (Quirk et al,
2004) is shown in Figure 1. In order to identify
that the sentences paraphrase each other , we need
to align constituents of these sentences. One possi-
ble alignment is shown in the figure, in which the
dotted edges correspond to the aligned constituents.
An alignment can be specified using binary variables
corresponding to every edge between constituents,
indicating whether the edge is included in the align-
ment. Different activations of these variables induce
the space of intermediate representations.
The notification was first reported Friday by MSNBC.
MSNBC.com first reported the CIA request on Friday.
Figure 1: The dotted lines represent a possible intermediate
representation for the paraphrase identification task. Since dif-
ferent representation choices will impact the binary identifica-
tion decision directly, our approach chooses the representation
that facilitates the binary learning task.
To formalize this setting, let x denote the input
to a decision function, which maps x to {?1, 1}.
We consider problems where this decision depends
on an intermediate representation (for example, the
collection of all dotted edges in Figure 1), which can
be represented by a binary vector h.
In the literature, a common approach is to sepa-
rate the problem into two stages. First, a genera-
tion stage predicts h for each x using a pre-defined
model or a heuristic. This is followed by a learn-
ing stage, in which the classifier is trained using h.
In our example, if the generation stage predicts the
alignment shown, then the learning stage would use
the features computed based on the alignments. For-
mally, the two-stage approach uses a pre-defined in-
ference procedure that finds an intermediate repre-
sentation h?. Using features ?(x,h?) and a learned
weight vector ?, the example is classified as positive
if ?T?(x,h?) ? 0.
However, in the two stage approach, the latent
representation, which is provided to the learning al-
gorithm, is determined before learning starts, and
without any feedback from the final task. It is dic-
tated by the intuition of the developer. This approach
makes two implicit assumptions: first, it assumes
430
the existence of a ?correct? latent representation and,
second, that the model or heuristic used to generate
it is the correct one for the learning problem at hand.
3 Joint Learning with an Intermediate
Representation
In contrast to two-stage approaches, we use the
annotated data for the final classification task to
learn a suitable intermediate representation which,
in turn, helps the final classification.
Choosing a good representation is an optimization
problem that selects which of the elements (features)
of the representation best contribute to success-
ful classification given some legitimacy constraints;
therefore, we (1) set up the optimization framework
that finds legitimate representations (Section 3.1),
and (2) learn an objective function for this optimiza-
tion problem, such that it makes the best final deci-
sion (Section 3.2.)
3.1 Inference
Our goal is to correctly predict the final label
rather than matching a ?gold? intermediate repre-
sentation. In our framework, attempting to learn the
final decision drives both the selection of the inter-
mediate representation and the final predictions.
For each x, let ?(x) be the set of all substructures
of all possible intermediate representations. In Fig-
ure 1, this could be the set of all alignment edges
connecting the constituents of the sentences. Given
a vocabulary of such structures of sizeN , we denote
intermediate representations by h ? {0, 1}N , which
?select? the components of the vocabulary that con-
stitute the intermediate representation. We define
?s(x) to be a feature vector over the substructure
s, which is used to describe the characteristics of s,
and define a weight vector u over these features.
Let C denote the set of feasible intermediate repre-
sentations h, specified by means of linear constraints
over h. While ?(x) might be large, the set of those
elements in h that are active can be constrained by
controlling C. After we have learned a weight vec-
tor u that scores intermediate representations for the
final classification task, we define our decision func-
tion as
fu(x) = max
h?C
uT
?
s??(x)
hs?s(x), (1)
and classify the input as positive if fu(x) ? 0.
In Eq. (1), uT?s(x) is the score associated with
the substructure s, and fu(x) is the score for the en-
tire intermediate representation. Therefore, our de-
cision function fu(x) ? 0 makes use of the interme-
diate representation and its score to classify the in-
put. An input is labeled as positive if its underlying
intermediate structure allows it to cross the decision
threshold. The intermediate representation is cho-
sen to maximize the overall score of the input. This
design is especially beneficial for many phenomena
in NLP, where only positive examples have a mean-
ingful underlying structure. In our paraphrase iden-
tification example, good alignments generally exist
only for positive examples.
One unique feature of our framework is that we
treat Eq. (1) as an Integer Linear Programming
(ILP) instance. A concrete instantiation of this set-
ting to the paraphrase identification problem, along
with the actual ILP formulation is shown in Section
4.
3.2 Learning
We now present an algorithm that learns the
weight vector u. For a loss function ` : R ? R,
the goal of learning is to solve the following opti-
mization problem:
min
u
?
2
?u?2 +
?
i
` (?yifu(xi)) (2)
Here, ? is the regularization parameter. Substituting
Eq. (1) into Eq. (2), we get
min
u
?
2
?u?2+
?
i
`
?
??yi max
h?C
uT
?
s??(x)
hs?s(xi)
?
? (3)
Note that there is a maximization term inside the
global minimization problem, making Eq. (3) a non-
convex problem. The minimization drives u towards
smaller empirical loss while the maximization uses
u to find the best representation for each example.
The algorithm for Learning over Constrained La-
tent Representations (LCLR) is listed in Algorithm
1. In each iteration, first, we find the best feature
representations for all positive examples (lines 3-5).
This step can be solved with an off-the-shelf ILP
solver. Having fixed the representations for the pos-
itive examples, we update the u by solving Eq. (4)
at line 6 in the algorithm. It is important to observe
431
Algorithm 1 LCLR :The algorithm that optimizes (3)
1: initialize: u? u0
2: repeat
3: for all positive examples (xi, yi = 1) do
4: Find h?i ? arg maxh?C
?
s
hsuT?s(xi)
5: end for
6: Update u by solving
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?C
uT
?
s
hs?s(xi)) (4)
7: until convergence
8: return u
that for positive examples in Eq. (4), we use the in-
termediate representations h? from line 4.
Algorithm 1 satisfies the following property:
Theorem 1 If the loss function ` is a non-
decreasing function, then the objective function
value of Eq. (3) is guaranteed to decrease in every
iteration of Algorithm 1. Moreover, if the loss func-
tion is also convex, then Eq. (4) in Algorithm 1 is
convex.
Due to the space limitation, we omit the proof.
Theoretically, we can use any loss function that
satisfies the conditions of the theorem. In the exper-
iments in this paper, we use the squared-hinge loss
function: `(?yfu(x)) = max(0, 1? yfu(x))2.
Recall that Eq. (4) is not the traditional SVM or
logistic regression formulation. This is because in-
side the inner loop, the best representation for each
negative example must be found. Therefore, we
need to perform inference for every negative exam-
ple when updating the weight vector solution. In-
stead of solving a difficult non-convex optimization
problem (Eq. (3)), LCLR iteratively solves a series
of easier problems (Eq. (4)). This is especially true
for our loss function because Eq. (4) is convex and
can be solved efficiently.
We use a cutting plane algorithm to solve Eq. (4).
A similar idea has been proposed in (Joachims et al,
2009). The algorithm for solving Eq. (4) is presented
as Algorithm 2. This algorithm uses a ?cache? Hj
to store all intermediate representations for negative
examples that have been seen in previous iterations
Algorithm 2 Cutting plane algorithm to optimize Eq. (4)
1: for each negative example xj , Hj ? ?
2: repeat
3: for each negative example xj do
4: Find h?j ? arg maxh?C
?
s hsu
T?s(xj)
5: Hj ? Hj ? {h?j}
6: end for
7: Solve
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?Hj
uT
?
s
hs?s(xi)) (5)
8: until no new element is added to any Hj
9: return u
(lines 3-6) 2. The difference between Eq. (5) in line
7 of Algorithm 2 and Eq. (4) is that in Eq. (5), we do
not search over the entire space of intermediate rep-
resentations. The search space for the minimization
problem Eq. (5) is restricted to the cache Hj . There-
fore, instead of solving the minimization problem
Eq. (4), we can now solve several simpler problems
shown in Eq. (5). The algorithm is guaranteed to
stop (line 8) because the space of intermediate rep-
resentations is finite. Furthermore, in practice, the
algorithm needs to consider only a small subset of
?hard? examples before it converges.
Inspired by (Hsieh et al, 2008), we apply an effi-
cient coordinate descent algorithm for the dual for-
mulation of (5) which is guaranteed to find its global
minimum. Due to space considerations, we do not
present the derivation of dual formulation and the
details of the optimization algorithm.
4 Encoding with ILP: A Paraphrase
Identification Example
In this section, we define the latent representation
for the paraphrase identification task. Unlike the ear-
lier example, where we considered the alignment of
lexical items, we describe a more complex interme-
diate representation by aligning graphs created using
semantic resources.
An input example is represented as two acyclic
2In our implementation, we keep a global cache Hj for each
negative example xj . Therefore, in Algorithm 2, we start with
a non-empty cache improving the speed significantly.
432
graphs, G1 and G2, corresponding to the first
and second input sentences. Each vertex in the
graph contains word information (lemma and part-
of-speech) and the edges denote dependency rela-
tions, generated by the Stanford parser (Klein and
Manning, 2003). The intermediate representation
for this task can now be defined as an alignment be-
tween the graphs, which captures lexical and syntac-
tic correlations between the sentences.
We use V (G) and E(G) to denote the set of ver-
tices and edges in G respectively, and define four
hidden variable types to encode vertex and edge
mappings between G1 and G2.
? The word-mapping variables, denoted by
hv1,v2 , define possible pairings of vertices,
where v1 ? V (G1) and v2 ? V (G2).
? The edge-mapping variables, denoted by
he1,e2 , define possible pairings of the graphs
edges, where e1 ? E(G1) and e2 ? E(G2).
? The word-deletion variables hv1,? (or h?,v2) al-
low for vertices v1 ? V (G1) (or v2 ? V (G2))
to be deleted. This accounts for omission of
words (like function words).
? The edge-deletion variables, he1,? (or h?,e2) al-
low for deletion of edges from G1 (or G2).
Our inference problem is to find the optimal set of
hidden variable activations, restricted according to
the following set of linear constraints
? Each vertex inG1 (orG2) can either be mapped
to a single vertex in G2 (or G1) or marked as
deleted. In terms of the word-mapping and
word-deletion variables, we have
?v1 ? V (G1);hv1,? +
?
v2?V (G2)
hv1,v2 = 1 (6)
?v2 ? V (G2);h?,v2 +
?
v1?V (G1)
hv1,v2 = 1 (7)
? Each edge in G1 (or G2) can either be mapped
to a single edge in G2 (or G1) or marked as
deleted. In terms of the edge-mapping and
edge-deletion variables, we have
?e1 ? E(G1);he1,? +
?
e2?E(G2)
he1,e2 = 1 (8)
?e2 ? E(G2);h?,e2 +
?
e1?E(G1)
he1,e2 = 1 (9)
? The edge mappings can be active if, and only
if, the corresponding node mappings are ac-
tive. Suppose e1 = (v1, v?1) ? E(G1) and
e2 = (v2, v?2) ? E(G2), where v1, v
?
1 ? V (G1)
and v2, v?2 ? V (G2). Then, we have
hv1,v2 + hv?1,v?2 ? he1,e2 ? 1 (10)
hv1,v2 ? he1,e2 ;hv?1,v?2 ? he1,e2 (11)
These constraints define the feasible set for the in-
ference problem specified in Equation (1). This in-
ference problem can be formulated as an ILP prob-
lem with the objective function from Equation (1):
max
h
?
s
hsuT?s(x)
subject to (6)-(11); ?s;hs ? {0, 1} (12)
This example demonstrates the use of integer linear
programming to define intermediate representations
incorporating domain intuition.
5 Experiments
We applied our framework to three different NLP
tasks: transliteration discovery (Klementiev and
Roth, 2008), RTE (Dagan et al, 2006), and para-
phrase identification (Dolan et al, 2004).
Our experiments are designed to answer the fol-
lowing research question: ?Given a binary classifi-
cation problem defined over latent representations,
will the joint LCLR algorithm perform better than a
two-stage approach?? To ensure a fair comparison,
both systems use the same feature functions and def-
inition of intermediate representation. We use the
same ILP formulation in both configurations, with a
single exception ? the objective function parameters:
the two stage approach uses a task-specific heuristic,
while LCLR learns it iteratively.
The ILP formulation results in very strong two
stage systems. For example, in the paraphrase iden-
tification task, even our two stage system is the cur-
rent state-of-the-art performance. In these settings,
the improvement obtained by our joint approach is
non-trivial and can be clearly attributed to the su-
periority of the joint learning algorithm. Interest-
ingly, we find that our more general approach is bet-
ter than specially designed joint approaches (Gold-
wasser and Roth, 2008b; Das and Smith, 2009).
Since the objective function (3) of the joint ap-
proach is not convex, a good initialization is re-
quired. We use the weight vector learned by the two
433
stage approach as the starting point for the joint ap-
proach. The algorithm terminates when the relative
improvement of the objective is smaller than 10?5.
5.1 Transliteration Discovery
Transliteration discovery is the problem of iden-
tifying if a word pair, possibly written using two
different character sets, refers to the same underly-
ing entity. The intermediate representation consists
of all possible character mappings between the two
character sets. Identifying this mapping is not easy,
as most writing systems do not perfectly align pho-
netically and orthographically; rather, this mapping
can be context-dependent and ambiguous.
For an input pair of words (w1, w2), the interme-
diate structure h is a mapping between their charac-
ters, with the latent variable hij indicating if the ith
character in w1 is aligned to the jth character in w2.
The feature vector associated with the variable hij
contains unigram character mapping, bigram char-
acter mapping (by considering surrounding charac-
ters). We adopt the one-to-one mapping and non-
crossing constraint used in (Chang et al, 2009).
We evaluated our system using the English-
Hebrew corpus (Goldwasser and Roth, 2008a),
which consists of 250 positive transliteration pairs
for training, and 300 pairs for testing. As negative
examples for training, we sample 10% from random
pairings of words from the positive data. We report
two evaluation measurements ? (1) the Mean Recip-
rocal Rank (MRR), which is the average of the mul-
tiplicative inverse of the rank of the correct answer,
and (2) the accuracy (Acc), which is the percentage
of the top rank candidates being correct.
We initialized the two stage inference process as
detailed in (Chang et al, 2009) using a Romaniza-
tion table to assign uniform weights to prominent
character mappings. This initialization procedure
resembles the approach used in (Bergsma and Kon-
drak, 2007). An alignment is first built by solving
the constrained optimization problem. Then, a sup-
port vector machine with squared-hinge loss func-
tion is used to train a classifier using features ex-
tracted from the alignment. We refer to this two
stage approach as Alignment+Learning.
The results summarized in Table 1 show the sig-
nificant improvement obtained by the joint approach
(95.4% MRR) compared to the two stage approach
Transliteration System Acc MRR
(Goldwasser and Roth,
2008b)
N/A 89.4
Alignment + Learning 80.0 85.7
LCLR 92.3 95.4
Table 1: Experimental results for transliteration. We compare
a two-stage system: ?Alignment+Learning? with LCLR, our
joint algorithm. Both ?Alignment+Learning? and LCLR use
the same features and the same intermediate representation def-
inition.
(85.7%). Moreover, LCLR outperforms the joint
system introduced in (Goldwasser and Roth, 2008b).
5.2 Textual Entailment
Recognizing Textual Entailment (RTE) is an im-
portant textual inference task of predicting if a given
text snippet, entails the meaning of another (the hy-
pothesis). In many current RTE systems, the entail-
ment decision depends on successfully aligning the
constituents of the text and hypothesis, accounting
for the internal linguistic structure of the input.
The raw input ? the text and hypothesis ? are
represented as directed acyclic graphs, where ver-
tices correspond to words. Directed edges link verbs
to the head words of semantic role labeling argu-
ments produced by (Punyakanok et al, 2008). All
other words are connected by dependency edges.
The intermediate representation is an alignment be-
tween the nodes and edges of the graphs. We used
three hidden variable types from Section 4 ? word-
mapping, word-deletion and edge-mapping, along
with the associated constraints as defined earlier.
Since the text is typically much longer than the hy-
pothesis, we create word-deletion latent variables
(and features) only for the hypothesis.
The second column of Table 2 lists the resources
used to generate features corresponding to each hid-
den variable type. For word-mapping variables, the
features include a WordNet based metric (WNSim),
indicators for the POS tags and negation identifiers.
We used the state-of-the-art coreference resolution
system of (Bengtson and Roth, 2008) to identify the
canonical entities for pronouns and extract features
accordingly. For word deletion, we use only the POS
tags of the corresponding tokens (generated by the
LBJ POS tagger3) to generate features. For edge
3
http://L2R.cs.uiuc.edu/?cogcomp/software.php
434
Hidden RTE Paraphrase
Variable features features
word-mapping WordNet, POS,
Coref, Neg
WordNet, POS,
NE, ED
word-deletion POS POS, NE
edge-mapping NODE-INFO NODE-INFO,
DEP
edge-deletion N/A DEP
Table 2: Summary of latent variables and feature resources for
the entailment and paraphrase identification tasks. See Section
4 for an explanation of the hidden variable types. The linguistic
resources used to generate features are abbreviated as follows ?
POS: Part of speech, Coref: Canonical coreferent entities; NE:
Named Entity, ED: Edit distance, Neg: Negation markers, DEP:
Dependency labels, NODE-INFO: corresponding node align-
ment resources, N/A: Hidden variable not used.
Entailment System Acc
Median of TAC 2009 systems 61.5
Alignment + Learning 65.0
LCLR 66.8
Table 3: Experimental results for recognizing textual entail-
ment. The first row is the median of best performing systems of
all teams that participated in the RTE5 challenge (Bentivogli et
al., 2009). Alignment + Learning is our two-stage system im-
plementation, and LCLR is our joint learning algorithm. Details
about these systems are provided in the text.
mapping variables, we include the features of the
corresponding word mapping variables, scaled by
the word similarity of the words forming the edge.
We evaluated our system using the RTE-5
data (Bentivogli et al, 2009), consisting of 600 sen-
tence pairs for training and testing respectively, in
which positive and negative examples are equally
distributed. In these experiments the joint LCLR al-
gorithm converged after 5 iterations.
For the two stage system, we used WN-
Sim to score alignments during inference. The
word-based scores influence the edge variables
via the constraints. This two-stage system (the
Alignment+Learning system) is significantly better
than the median performance of the RTE-5 submis-
sions. Using LCLR further improves the result by al-
most 2%, a substantial improvement in this domain.
5.3 Paraphrase Identification
Our final task is Paraphrase Identification, dis-
cussed in detail at Section 4. We use all the four
hidden variable types described in that section. The
features used are similar to those described earlier
Paraphrase System Acc
Experiments using (Dolan et al, 2004)
(Qiu et al, 2006) 72.00
(Das and Smith, 2009) 73.86
(Wan et al, 2006) 75.60
Alignment + Learning 76.23
LCLR 76.41
Experiments using Extended data set
Alignment + Learning 72.00
LCLR 72.75
Table 4: Experimental Result For Paraphrasing Identification.
Our joint LCLR approach achieves the best results compared
to several previously published systems, and our own two stage
system implementation (Alignment + Learning). We evaluated
the systems performance across two datasets: (Dolan et al,
2004) dataset and the Extended dataset, see the text for details.
Note that LCLR outperforms (Das and Smith, 2009), which is a
specifically designed joint approach for this task.
for the RTE system and are summarized in Table 2.
We used the MSR paraphrase dataset of (Dolan
et al, 2004) for empirical evaluation. Additionally,
we generated a second corpus (called the Extended
dataset) by sampling 500 sentence pairs from the
MSR dataset for training and using the entire test
collection of the original dataset. In the Extended
dataset, for every sentence pair, we extended the
longer sentence by concatenating it with itself. This
results in a more difficult inference problem because
it allows more mappings between words. Note that
the performance on the original dataset sets the ceil-
ing on the second one.
The results are summarized in Table 4. The first
part of the table compares the LCLR system with
a two stage system (Alignment + Learning) and
three published results that use the MSR dataset.
(We only list single systems in the table4) Inter-
estingly, although still outperformed by our joint
LCLR algorithm, the two stage system is able per-
form significantly better than existing systems for
that dataset (Qiu et al, 2006; Das and Smith, 2009;
Wan et al, 2006). We attribute this improvement,
consistent across both the ILP based systems, to the
intermediate representation we defined.
We hypothesize that the similarity in performance
between the joint LCLR algorithm and the two stage
4Previous work (Das and Smith, 2009) has shown that com-
bining the results of several systems improves performance.
435
(Alignment + Learning) systems is due to the limited
intermediate representation space for input pairs in
this dataset. We evaluated these systems on the more
difficult Extended dataset. Results indeed show that
the margin between the two systems increases as the
inference problem becomes harder.
6 Related Work
Recent NLP research has largely focused on two-
stage approaches. Examples include RTE (Zanzotto
and Moschitti, 2006; MacCartney et al, 2008; Roth
et al, 2009); string matching (Bergsma and Kon-
drak, 2007); transliteration (Klementiev and Roth,
2008); and paraphrase identification (Qiu et al,
2006; Wan et al, 2006).
(MacCartney et al, 2008) considered construct-
ing a latent representation to be an independent task
and used manually labeled alignment data (Brockett,
2007) to tune the inference procedure parameters.
While this method identifies alignments well, it does
not improve entailment decisions. This strengthens
our intuition that the latent representation should be
guided by the final task.
There are several exceptions to the two-stage ap-
proach in the NLP community (Haghighi et al,
2005; McCallum et al, 2005; Goldwasser and Roth,
2008b; Das and Smith, 2009); however, the interme-
diate representation and the inference for construct-
ing it are closely coupled with the application task.
In contrast, LCLR provides a general formulation
that allows the use of expressive constraints, mak-
ing it applicable to many NLP tasks.
Unlike other latent variable SVM frameworks
(Felzenszwalb et al, 2009; Yu and Joachims, 2009)
which often use task-specific inference procedure,
LCLR utilizes the declarative inference framework
that allows using constraints over intermediate rep-
resentation and provides a general platform for a
wide range of NLP tasks.
The optimization procedure in this work and
(Felzenszwalb et al, 2009) are quite different.
We use the coordinate descent and cutting-plane
methods ensuring we have fewer parameters and
the inference procedure can be easily parallelized.
Our procedure also allows different loss functions.
(Cherry and Quirk, 2008) adopts the Latent SVM al-
gorithm to define a language model. Unfortunately,
their implementation is not guaranteed to converge.
In CRF-like models with latent variables (McCal-
lum et al, 2005), the decision function marginal-
izes over the all hidden states when presented with
an input example. Unfortunately, the computational
cost of applying their framework is prohibitive with
constrained latent representations. In contrast, our
framework requires only the best hidden representa-
tion instead of marginalizing over all possible repre-
sentations, thus reducing the computational effort.
7 Conclusion
We consider the problem of learning over an inter-
mediate representation. We assume the existence of
a latent structure in the input, relevant to the learn-
ing problem, but not accessible to the learning algo-
rithm. Many NLP tasks fall into these settings and
each can consider a different hidden input structure.
We propose a unifying thread for the different prob-
lems and present a novel framework for Learning
over Constrained Latent Representations (LCLR).
Our framework can be applied to many different la-
tent representations such as parse trees, orthographic
mapping and tree alignments. Our approach con-
trasts with existing work in which learning is done
over a fixed representation, as we advocate jointly
learning it with the final task.
We successfully apply the proposed framework to
three learning tasks ? Transliteration, Textual En-
tailment and Paraphrase Identification. Our joint
LCLR algorithm achieves superior performance in
all three tasks. We attribute the performance im-
provement to our novel training algorithm and flex-
ible inference procedure, allowing us to encode do-
main knowledge. This presents an interesting line of
future work in which more linguistic intuitions can
be encoded into the learning problem. For these rea-
sons, we believe that our framework provides an im-
portant step forward in understanding the problem
of learning over hidden structured inputs.
Acknowledgment We thank James Clarke and Mark Sam-
mons for their insightful comments. This research was partly sponsored
by the Army Research Laboratory (ARL) (accomplished under Cooper-
ative Agreement Number W911NF-09-2-0053) and by Air Force Re-
search Laboratory (AFRL) under prime contract no. FA8750-09-C-
0181. Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not necessarily
reflect the view of the ARL or of AFRL.
436
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In Proc. of TAC Work-
shop.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In ACL.
C. Brockett. 2007. Aligning the RTE 2006 corpus.
In Technical Report MSR-TR-2007-77, Microsoft Re-
search.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
C. Cherry and C. Quirk. 2008. Discriminative, syntactic
language modeling through latent svms. In Proc. of
the Eighth Conference of AMTA.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
ACL.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrimina-
tively trained part based models. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In ACL. Short
Paper.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In EMNLP.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In HLT-EMNLP.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural svms. Machine
Learning.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
NIPS.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Cyril Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Translation.
B. MacCartney, M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural language
inference. In EMNLP.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In UAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
EMNLP.
D. Roth, M. Sammons, and V.G. Vydiswaran. 2009. A
framework for entailed relation recognition. In ACL.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the p?ara-farceo?ut
of paraphrase. In Proc. of the Australasian Language
Technology Workshop (ALTW).
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In ICML.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In ACL.
437
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905?913,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Margin-based Decomposed Amortized Inference
Gourab Kundu? and Vivek Srikumar? and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801
{kundu2, vsrikum2, danr}@illinois.edu
Abstract
Given that structured output prediction is
typically performed over entire datasets,
one natural question is whether it is pos-
sible to re-use computation from earlier
inference instances to speed up inference
for future instances. Amortized inference
has been proposed as a way to accomplish
this. In this paper, first, we introduce a new
amortized inference algorithm called the
Margin-based Amortized Inference, which
uses the notion of structured margin to
identify inference problems for which pre-
vious solutions are provably optimal. Sec-
ond, we introduce decomposed amortized
inference, which is designed to address
very large inference problems, where ear-
lier amortization methods become less ef-
fective. This approach works by decom-
posing the output structure and applying
amortization piece-wise, thus increasing
the chance that we can re-use previous so-
lutions for parts of the output structure.
These parts are then combined to a global
coherent solution using Lagrangian relax-
ation. In our experiments, using the NLP
tasks of semantic role labeling and entity-
relation extraction, we demonstrate that
with the margin-based algorithm, we need
to call the inference engine only for a third
of the test examples. Further, we show that
the decomposed variant of margin-based
amortized inference achieves a greater re-
duction in the number of inference calls.
1 Introduction
A wide variety of NLP problems can be natu-
rally cast as structured prediction problems. For
* These authors contributed equally to this work.
some structures like sequences or parse trees, spe-
cialized and tractable dynamic programming algo-
rithms have proven to be very effective. However,
as the structures under consideration become in-
creasingly complex, the computational problem of
predicting structures can become very expensive,
and in the worst case, intractable.
In this paper, we focus on an inference tech-
nique called amortized inference (Srikumar et al,
2012), where previous solutions to inference prob-
lems are used to speed up new instances. The
main observation that leads to amortized inference
is that, very often, for different examples of the
same size, the structures that maximize the score
are identical. If we can efficiently identify that two
inference problems have the same solution, then
we can re-use previously computed structures for
newer examples, thus giving us a speedup.
This paper has two contributions. First, we de-
scribe a novel algorithm for amortized inference
called margin-based amortization. This algorithm
is on an examination of the structured margin of
a prediction. For a new inference problem, if this
margin is larger than the sum of the decrease in the
score of the previous prediction and any increase
in the score of the second best one, then the previ-
ous solution will be the highest scoring one for the
new problem. We formalize this intuition to derive
an algorithm that finds provably optimal solutions
and show that this approach is a generalization of
previously identified schemes (based on Theorem
1 of (Srikumar et al, 2012)).
Second, we argue that the idea of amortization
is best exploited at the level of parts of the struc-
tures rather than the entire structure because we
expect a much higher redundancy in the parts.
We introduce the notion of decomposed amor-
tized inference, whereby we can attain a significant
improvement in speedup by considering repeated
sub-structures across the dataset and applying any
amortized inference algorithm for the parts.
905
We evaluate the two schemes and their combi-
nation on two NLP tasks where the output is en-
coded as a structure: PropBank semantic role la-
beling (Punyakanok et al, 2008) and the problem
of recognizing entities and relations in text (Roth
and Yih, 2007; Kate and Mooney, 2010). In these
problems, the inference problem has been framed
as an integer linear program (ILP). We compare
our methods with previous amortized inference
methods and show that margin-based amortization
combined with decomposition significantly out-
performs existing methods.
2 Problem Definition and Notation
Structured output prediction encompasses a wide
variety of NLP problems like part-of-speech tag-
ging, parsing and machine translation. The lan-
guage of 0-1 integer linear programs (ILP) pro-
vides a convenient analytical tool for representing
structured prediction problems. The general set-
ting consists of binary inference variables each of
which is associated with a score. The goal of in-
ference is to find the highest scoring global assign-
ment of the variables from a feasible set of assign-
ments, which is defined by linear inequalities.
While efficient inference algorithms exist for
special families of structures (like linear chains
and trees), in the general case, inference can be
computationally intractable. One approach to deal
with the computational complexity of inference
is to use an off-the-shelf ILP solver for solv-
ing the inference problem. This approach has
seen increasing use in the NLP community over
the last several years (for example, (Roth and
Yih, 2004; Clarke and Lapata, 2006; Riedel and
Clarke, 2006) and many others). Other approaches
for solving inference include the use of cutting
plane inference (Riedel, 2009), dual decomposi-
tion (Koo et al, 2010; Rush et al, 2010) and
the related method of Lagrangian relaxation (Rush
and Collins, 2011; Chang and Collins, 2011).
(Srikumar et al, 2012) introduced the notion of
an amortized inference algorithm, defined as an
inference algorithm that can use previous predic-
tions to speed up inference time, thereby giving an
amortized gain in inference time over the lifetime
of the program.
The motivation for amortized inference comes
from the observation that though the number of
possible structures could be large, in practice, only
a small number of these are ever seen in real
data. Furthermore, among the observed structures,
a small subset typically occurs much more fre-
quently than the others. Figure 1 illustrates this
observation in the context of part-of-speech tag-
ging. If we can efficiently characterize and iden-
tify inference instances that have the same solu-
tion, we can take advantage of previously per-
formed computation without paying the high com-
putational cost of inference.
Figure 1: Comparison of number of instances and the num-
ber of unique observed part-of-speech structures in the Gi-
gaword corpus. Note that the number of observed structures
(blue solid line) is much lower than the number of sentences
(red dotted line) for all sentence lengths, with the difference
being very pronounced for shorter sentences. Embedded in
the graph are three histograms that show the distribution of
observed structures for sentences of length 15, 20 and 30. In
all cases, we see that a small number of tag sequences are
much more frequent than the others.
We denote inference problems by the bold-
faced letters p and q. For a problem p, the goal
of inference is to jointly assign values to the parts
of the structure, which are represented by a col-
lection of inference variables y ? {0, 1}n. For all
vectors, subscripts represent their ith component.
Each yi is associated with a real valued cp,i ? <
which is the score for the variable yi being as-
signed the value 1. We denote the vector com-
prising of all the cp,i as cp. The search space
for assignments is restricted via constraints, which
can be written as a collection of linear inequalities,
MTy ? b. For a problem p, we denote this fea-
sible set of structures by Kp.
The inference problem is that of finding the fea-
sible assignment to the structure which maximizes
the dot product cTy. Thus, the prediction problem
can be written as
arg max
y?Kp
cTy. (1)
906
We denote the solution of this maximization prob-
lem as yp.
Let the set P = {p1,p2, ? ? ? } denote previously
solved inference problems, along with their re-
spective solutions {y1p,y2p, ? ? ? }. An equivalence
class of integer linear programs, denoted by [P ],
consists of ILPs which have the same number of
inference variables and the same feasible set. Let
K[P ] denote the feasible set of an equivalence class
[P ]. For a program p, the notation p ? [P ] indi-
cates that it belongs to the equivalence class [P ].
(Srikumar et al, 2012) introduced a set of amor-
tized inference schemes, each of which provides a
condition for a new ILP to have the same solu-
tion as a previously seen problem. We will briefly
review one exact inference scheme introduced in
that work. Suppose q belongs to the same equiv-
alence class of ILPs as p. Then the solution to q
will be the same as that of p if the following con-
dition holds for all inference variables:
(2yp,i ? 1)(cq,i ? cp,i) ? 0. (2)
This condition, referred to as Theorem 1 in that
work, is the baseline for our experiments.
In general, for any amortization scheme
A, we can define two primitive operators
TESTCONDITIONA and SOLUTIONA. Given
a collection of previously solved prob-
lems P and a new inference problem q,
TESTCONDITIONA(P,q) checks if the solu-
tion of the new problem is the same as that
of some previously solved one and if so,
SOLUTIONA(P,q) returns the solution.
3 Margin-based Amortization
In this section, we will introduce a new method
for amortizing inference costs over time. The key
observation that leads to this theorem stems from
the structured margin ? for an inference problem
p ? [P ], which is defined as follows:
? = min
y?K[P ],y 6=yp
cTp(yp ? y). (3)
That is, for all feasible y, we have cTpyp ? cTpy+
?. The margin ? is the upper limit on the change in
objective that is allowed for the constraint setK[P ]
for which the solution will not change.
For a new inference problem q ? [P ], we define
? as the maximum change in objective value that
can be effected by an assignment that is not the
A B = yp
cp
cq?
?
decrease in
value of yp
inc
rea
sin
go
bje
cti
ve
cpTyp
Two assignments
Figure 2: An illustration of the margin-based amortization
scheme showing the very simple case with only two compet-
ing assignments A and B. Suppose B is the solution yp for
the inference problem p with coefficients cp, denoted by the
red hyperplane, and A is the second-best assignment. For a
new coefficient vector cq, if the margin ? is greater than the
sum of the decrease in the objective value of yp and the max-
imum increase in the objective of another solution (?), then
the solution to the new inference problem will still be yp. The
margin-based amortization theorem captures this intuition.
solution. That is,
? = max
y?K[P ],y 6=yp
(cq ? cp)T y (4)
Before stating the theorem, we will provide an in-
tuitive explanation for it. Moving from cp to cq,
consider the sum of the decrease in the value of
the objective for the solution yp and ?, the maxi-
mum change in objective value for an assignment
that is not the solution. If this sum is less than the
margin ?, then no other solution will have an ob-
jective value higher than yp. Figure 2 illustrates
this using a simple example where there are only
two competing solutions.
This intuition is captured by our main theorem
which provides a condition for problems p and q
to have the same solution yp.
Theorem 1 (Margin-based Amortization). Let p
denote an inference problem posed as an inte-
ger linear program belonging to an equivalence
class [P ] with optimal solution yp. Let p have
a structured margin ?, i.e., for any y, we have
cTpyp ? cTpy + ?. Let q ? [P ] be another infer-
ence instance in the same equivalence class and
let ? be defined as in Equation 4. Then, yp is the
solution of the problem q if the following holds:
?(cq ? cp)Typ + ? ? ? (5)
907
Proof. For some feasible y, we have
cTqyp ? cTqy ? cTqyp ? cTpy ??
? cTqyp ? cTpyp + ? ??
? 0
The first inequality comes from the definition of ?
in (4) and the second one follows from the defini-
tion of ?. The condition of the theorem in (5) gives
us the final step. For any feasible y, the objective
score assigned to yp is greater than the score as-
signed to y according to problem q. That is, yp is
the solution to the new problem.
The margin-based amortization theorem pro-
vides a general, new amortized inference algo-
rithm. Given a new inference problem, we check
whether the inequality (5) holds for any previously
seen problems in the same equivalence class. If so,
we return the cached solution. If no such problem
exists, then we make a call to an ILP solver.
Even though the theorem provides a condition
for two integer linear programs to have the same
solution, checking the validity of the condition re-
quires the computation of ?, which in itself is an-
other integer linear program. To get around this,
we observe that if any constraints in Equation 4
are relaxed, the value of the resulting maximum
can only increase. Even with the increased ?, if
the condition of the theorem holds, then the rest
of the proof follows and hence the new problem
will have the same solution. In other words, we
can solve relaxed, tractable variants of the maxi-
mization in Equation 4 and still retain the guaran-
tees provided by the theorem. The tradeoff is that,
by doing so, the condition of the theorem will ap-
ply to fewer examples than theoretically possible.
In our experiments, we will define the relaxation
for each problem individually and even with the
relaxations, the inference algorithm based on the
margin-based amortization theorem outperforms
all previous amortized inference algorithms.
The condition in inequality (5) is, in fact, a strict
generalization of the condition for Theorem 1 in
(Srikumar et al, 2012), stated in (2). If the latter
condition holds, then we can show that ? ? 0 and
(cq ? cp)Typ ? 0. Since ? is, by definition, non-
negative, the margin-based condition is satisfied.
4 Decomposed Amortized Inference
One limitation in previously considered ap-
proaches for amortized inference stems from the
expectation that the same full assignment maxi-
mizes the objective score for different inference
problems, or equivalently, that the entire structure
is repeated multiple times. Even with this assump-
tion, we observe a speedup in prediction.
However, intuitively, even if entire structures
are not repeated, we expect parts of the assign-
ment to be the same across different instances. In
this section, we address the following question:
Can we take advantage of the redundancy in com-
ponents of structures to extend amortization tech-
niques to cases where the full structured output is
not repeated? By doing so, we can store partial
computation for future inference problems.
For example, consider the task of part of speech
tagging. While the likelihood of two long sen-
tences having the same part of speech tag sequence
is not high, it is much more likely that shorter sec-
tions of the sentences will share the same tag se-
quence. We see from Figure 1 that the number of
possible structures for shorter sentences is much
smaller than the number of sentences. This im-
plies that many shorter sentences share the same
structure, thus improving the performance of an
amortized inference scheme for such inputs. The
goal of decomposed amortized inference is to ex-
tend this improvement to larger problems by in-
creasing the size of equivalence classes.
To decompose an inference problem, we use the
approach of Lagrangian Relaxation (Lemare?chal,
2001) that has been used successfully for various
NLP tasks (Chang and Collins, 2011; Rush and
Collins, 2011). We will briefly review the under-
lying idea1. The goal is to solve an integer linear
program q, which is defined as
q : max
MTy?b
cTqy
We partition the constraints into two sets, say C1
denoting M1Ty ? b1 and C2, denoting con-
straints M2Ty ? b2. The assumption is that in
the absence the constraints C2, the inference prob-
lem becomes computationally easier to solve. In
other words, we can assume the existence of a sub-
routine that can efficiently compute the solution of
the relaxed problem q?:
q? : max
M1Ty?b1
cTqy
1For simplicity, we only write inequality constraints in
the paper. However, all the results here are easily extensible
to equality constraints by removing the non-negativity con-
straints from the corresponding dual variables.
908
We define Lagrange multipliers ? ? 0, with one
?i for each constraint in C2. For problem q, we
can define the Lagrangian as
L(y,?) = cTqy ? ?T
(
M2Ty ? b2
)
Here, the domain of y is specified by the constraint
set C1. The dual objective is
L(?) = max
M1Ty?b1
cTqy ? ?T
(
M2Ty ? b2
)
= max
M1Ty?b1
(
cq ? ?TM2
)T y + ?Tb2.
Note that the maximization in the definition of the
dual objective has the same functional form as q?
and any approach to solve q? can be used here to
find the dual objective L(?). The dual of the prob-
lem q, given by min??0 L(?), can be solved us-
ing subgradient descent over the dual variables.
Relaxing the constraints C2 to define the prob-
lem q? has several effects. First, it can make the re-
sulting inference problem q? easier to solve. More
importantly, removing constraints can also lead to
the merging of multiple equivalence classes, lead-
ing to fewer, more populous equivalence classes.
Finally, removing constraints can decompose the
inference problem q? into smaller independent
sub-problems {q1,q2, ? ? ? } such that no constraint
that is inC1 has active variables from two different
sets in the partition.
For the sub-problem qi comprising of variables
yi, let the corresponding objective coefficients be
cqi and the corresponding sub-matrix of M2 be
Mi2. Now, we can define the dual-augmented sub-
problem as
max
Mi1
Ty?bi1
(
cqi ? ?TMi2
)T
yi (6)
Solving all such sub-problems will give us a com-
plete assignment for all the output variables.
We can now define the decomposed amortized
inference algorithm (Algorithm 1) that performs
sub-gradient descent over the dual variables. The
input to the algorithm is a collection of previ-
ously solved problems with their solutions, a new
inference problem q and an amortized inference
scheme A (such as the margin-based amortization
scheme). In addition, for the task at hand, we first
need to identify the set of constraints C2 that can
be introduced via the Lagrangian.
First, we check if the solution can be obtained
without decomposition (lines 1?2). Otherwise,
Algorithm 1 Decomposed Amortized Inference
Input: A collection of previously solved infer-
ence problems P , a new problem q, an amor-
tized inference algorithm A.
Output: The solution to problem q
1: if TESTCONDITION(A, q, P ) then
2: return SOLUTION(A, q, P )
3: else
4: Initialize ?i ? 0 for each constraint in C2.
5: for t = 1 ? ? ?T do
6: Partition the problem q into sub-
problems q1,q2, ? ? ? such that no con-
straint in C1 has active variables from
two partitions.
7: for partition qi do
8: yi ? Solve the maximization prob-
lem for qi (Eq. 6) using the amortized
scheme A.
9: end for
10: Let y? [y1;y2; ? ? ? ]
11: if M2y ? b2 and (b2 ?M2y)i?i = 0
then
12: return y
13: else
14: ??
[
?? ?t
(
b2 ?M2Ty
)]
+
15: end if
16: end for
17: return solution of q using a standard infer-
ence algorithm
18: end if
we initialize the dual variables ? and try to ob-
tain the solution iteratively. At the tth itera-
tion, we partition the problem q into sub-problems
{q1,q2, ? ? ? } as described earlier (line 6). Each
partition defines a smaller inference problem with
its own objective coefficients and constraints. We
can apply the amortization scheme A to each sub-
problem to obtain a complete solution for the re-
laxed problem (lines 7?10). If this solution satis-
fies the constraints C2 and complementary slack-
ness conditions, then the solution is provably the
maximum of the problem q. Otherwise, we take a
subgradient step to update the value of ? using a
step-size ?t, subject to the constraint that all dual
variables must be non-negative (line 14). If we do
not converge to a solution in T iterations, we call
the underlying solver on the full problem.
In line 8 of the algorithm, we make multiple
calls to the underlying amortized inference pro-
cedure to solve each sub-problem. If the sub-
909
problem cannot be solved using the procedure,
then we can either solve the sub-problem using a
different approach (effectively giving us the stan-
dard Lagrangian relaxation algorithm for infer-
ence), or we can treat the full instance as a cache
miss and make a call to an ILP solver. In our ex-
periments, we choose the latter strategy.
5 Experiments and Results
Our experiments show two results: 1. The margin-
based scheme outperforms the amortized infer-
ence approaches from (Srikumar et al, 2012).
2. Decomposed amortized inference gives further
gains in terms of re-using previous solutions.
5.1 Tasks
We report the performance of inference on two
NLP tasks: semantic role labeling and the task of
extracting entities and relations from text. In both
cases, we used an existing formulation for struc-
tured inference and only modified the inference
calls. We will briefly describe the problems and
the implementation and point the reader to the lit-
erature for further details.
Semantic Role Labeling (SRL) Our first task is
that of identifying arguments of verbs in a sen-
tence and annotating them with semantic roles
(Gildea and Jurafsky, 2002; Palmer et al, 2010)
. For example, in the sentence Mrs. Haag plays
Eltiani., the verb plays takes two arguments: Mrs.
Haag, the actor, labeled as A0 and Eltiani, the
role, labeled as A1. It has been shown in prior
work (Punyakanok et al, 2008; Toutanova et al,
2008) that making a globally coherent prediction
boosts performance of SRL.
In this work, we used the SRL system of (Pun-
yakanok et al, 2008), where one inference prob-
lem is generated for each verb and each infer-
ence variables encodes the decision that a given
constituent in the sentence takes a specific role.
The scores for the inference variables are obtained
from a classifier trained on the PropBank cor-
pus. Constraints encode structural and linguistic
knowledge about the problem. For details about
the formulations of the inference problem, please
see (Punyakanok et al, 2008).
Recall from Section 3 that we need to define a
relaxed version of the inference problem to effi-
ciently compute ? for the margin-based approach.
For a problem instance with coefficients cq and
cached coefficients cp, we take the sum of the
highest n values of cq ? cp as our ?, where n is
the number of argument candidates to be labeled.
To identify constraints that can be relaxed for
the decomposed algorithm, we observe that most
constraints are not predicate specific and apply for
all predicates. The only constraint that is predi-
cate specific requires that each predicate can only
accept roles from a list of roles that is defined for
that predicate. By relaxing this constraint in the
decomposed algorithm, we effectively merge all
the equivalence classes for all predicates with a
specific number of argument candidates.
Entity-Relation extraction Our second task is
that of identifying the types of entities in a sen-
tence and the relations among them, which has
been studied by (Roth and Yih, 2007; Kate and
Mooney, 2010) and others. For the sentence
Oswald killed Kennedy, the words Oswald and
Kennedy will be labeled by the type PERSON, and
the KILL relation exists between them.
We followed the experimental setup as de-
scribed in (Roth and Yih, 2007). We defined one
inference problem for each sentence. For every
entity (which is identified by a constituent in the
sentence), an inference variable is introduced for
each entity type. For each pair of constituents, an
inference variable is introduced for each relation
type. Clearly, the assignment of types to entities
and relations are not independent. For example, an
entity of type ORGANIZATION cannot participate
in a relation of type BORN-IN because this rela-
tion label can connect entities of type PERSON and
LOCATION only. Incorporating these natural con-
straints during inference were shown to improve
performance significantly in (Roth and Yih, 2007).
We trained independent classifiers for entities and
relations and framed the inference problem as in
(Roth and Yih, 2007). For further details, we refer
the reader to that paper.
To compute the value of ? for the margin-based
algorithm, for a new instance with coefficients cq
and cached coefficients cp, we define ? to be the
sum of all non-negative values of cq ? cp.
For the decomposed inference algorithm, if the
number of entities is less than 5, no decomposi-
tion is performed. Otherwise, the entities are par-
titioned into two sets: set A includes the first four
entities and set B includes the rest of the entities.
We relaxed the relation constraints that go across
these two sets of entities to obtain two independent
inference problems.
910
5.2 Experimental Setup
We follow the experimental setup of (Srikumar et
al., 2012) and simulate a long-running NLP pro-
cess by caching problems and solutions from the
Gigaword corpus. We used a database engine to
cache ILP and their solutions along with identi-
fiers for the equivalence class and the value of ?.
For the margin-based algorithm and the Theo-
rem 1 from (Srikumar et al, 2012), for a new in-
ference problem p ? [P ], we retrieve all infer-
ence problems from the database that belong to
the same equivalence class [P ] as the test prob-
lem p and find the cached assignment y that has
the highest score according to the coefficients of
p. We only consider cached ILPs whose solution
is y for checking the conditions of the theorem.
This optimization ensures that we only process a
small number of cached coefficient vectors.
In a second efficiency optimization, we pruned
the database to remove redundant inference prob-
lems. A problem is redundant if solution to that
problem can be inferred from the other problems
stored in the database that have the same solution
and belong to the same equivalence class. How-
ever, this pruning can be computationally expen-
sive if the number of problems with the same so-
lution and the same equivalence class is very large.
In that case, we first sampled a 5000 problems ran-
domly and selected the non-redundant problems
from this set to keep in the database.
5.3 Results
We compare our approach to a state-of-the-art ILP
solver2 and also to Theorem 1 from (Srikumar
et al, 2012). We choose this baseline because
it is shown to give the highest improvement in
wall-clock time and also in terms of the num-
ber of cache hits. However, we note that the re-
sults presented in our work outperform all the pre-
vious amortization algorithms, including the ap-
proximate inference methods.
We report two performance metrics ? the per-
centage decrease in the number of ILP calls, and
the percentage decrease in the wall-clock infer-
ence time. These are comparable to the speedup
and clock speedup defined in (Srikumar et al,
2012). For measuring time, since other aspects
of prediction (like feature extraction) are the same
across all settings, we only measure the time taken
for inference and ignore other aspects. For both
2We used the Gurobi optimizer for our experiments.
tasks, we report the runtime performance on sec-
tion 23 of the Penn Treebank. Note that our amor-
tization schemes guarantee optimal solution. Con-
sequently, using amortization, task accuracy re-
mains the same as using the original solver.
Table 1 shows the percentage reduction in the
number of calls to the ILP solver. Note that for
both the SRL and entity-relation problems, the
margin-based approach, even without using de-
composition (the columns labeled Original), out-
performs the previous work. Applying the de-
composed inference algorithm improves both the
baseline and the margin-based approach. Overall,
however, the fewest number of calls to the solver is
made when combining the decomposed inference
algorithm with the margin-based scheme. For the
semantic role labeling task, we need to call the
solver only for one in six examples while for the
entity-relations task, only one in four examples re-
quire a solver call.
Table 2 shows the corresponding reduction in
the wall-clock time for the various settings. We
see that once again, the margin based approach
outperforms the baseline. While the decomposed
inference algorithm improves running time for
SRL, it leads to a slight increase for the entity-
relation problem. Since this increase occurs in
spite of a reduction in the number of solver calls,
we believe that this aspect can be further improved
with an efficient implementation of the decom-
posed inference algorithm.
6 Discussion
Lagrangian Relaxation in the literature In the
literature, in applications of the Lagrangian relax-
ation technique (such as (Rush and Collins, 2011;
Chang and Collins, 2011; Reichart and Barzilay,
2012) and others), the relaxed problems are solved
using specialized algorithms. However, in both the
relaxations considered in this paper, even the re-
laxed problems cannot be solved without an ILP
solver, and yet we can see improvements from de-
composition in Table 1.
To study the impact of amortization on running
time, we modified our decomposition based infer-
ence algorithm to solve each sub-problem using
the ILP solver instead of amortization. In these ex-
periments, we ran Lagrangian relaxation for until
convergence or at most T iterations. After T itera-
tions, we call the ILP solver and solve the original
problem. We set T to 100 in one set of exper-
911
% ILP Solver calls required
Method Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 ? 100 ?
(Srikumar et al, 2012) 41 24.4 59.5 57.0
Margin-based 32.7 16.6 28.2 25.4
Table 1: Reduction in number of inference calls
% time required compared to ILP Solver
Method Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 ? 100 ?
(Srikumar et al, 2012) 54.8 40.0 81 86
Margin-based 45.9 38.1 58.1 61.3
Table 2: Reduction in inference time
iments (call it Lag1) and T to 1 (call it Lag2).
In SRL, compared to solving the original problem
with ILP Solver, both Lag1 and Lag2 are roughly
2 times slower. For entity relation task, compared
to ILP Solver, Lag1 is 186 times slower and Lag2
is 1.91 times slower. Since we used the same im-
plementation of the decomposition in all experi-
ments, this shows that the decomposed inference
algorithm crucially benefits from the underlying
amortization scheme.
Decomposed amortized inference The decom-
posed amortized inference algorithm helps im-
prove amortized inference in two ways. First,
since the number of structures is a function of its
size, considering smaller sub-structures will allow
us to cache inference problems that cover a larger
subset of the space of possible sub-structures. We
observed this effect in the problem of extracting
entities and relations in text. Second, removing a
constraint need not always partition the structure
into a set of smaller structures. Instead, by re-
moving the constraint, examples that might have
otherwise been in different equivalence classes be-
come part of a combined, larger equivalence class.
Increasing the size of the equivalence classes in-
creases the probability of a cache-hit. In our ex-
periments, we observed this effect in the SRL task.
7 Conclusion
Amortized inference takes advantage of the reg-
ularities in structured output to re-use previous
computation and improve running time over the
lifetime of a structured output predictor. In this pa-
per, we have described two approaches for amor-
tizing inference costs over datasets. The first,
called the margin-based amortized inference, is a
new, provably exact inference algorithm that uses
the notion of a structured margin to identify previ-
ously solved problems whose solutions can be re-
used. The second, called decomposed amortized
inference, is a meta-algorithm over any amortized
inference that takes advantage of previously com-
puted sub-structures to provide further reductions
in the number of inference calls. We show via ex-
periments that these methods individually give a
reduction in the number of calls made to an infer-
ence engine for semantic role labeling and entity-
relation extraction. Furthermore, these approaches
complement each other and, together give an addi-
tional significant improvement.
Acknowledgments
The authors thank the members of the Cognitive Computa-
tion Group at the University of Illinois for insightful discus-
sions and the anonymous reviewers for valuable feedback.
This research is sponsored by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053. The authors
also gratefully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. This material also
is based on research sponsored by DARPA under agreement
number FA8750-13-2-0008. This work has also been sup-
ported by the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Interior National Business
Center contract number D11PC20155. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright an-
notation thereon. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the view of ARL,
DARPA, AFRL, IARPA, DoI/NBC or the US government.
912
References
Y-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. EMNLP.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics.
R. Kate and R. Mooney. 2010. Joint entity and relation
extraction using card-pyramid parsing. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 203?212. Asso-
ciation for Computational Linguistics.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and
D. Sontag. 2010. Dual decomposition for parsing
with non-projective head automata. In EMNLP.
C. Lemare?chal. 2001. Lagrangian Relaxation. In
Computational Combinatorial Optimization, pages
112?156.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic
Role Labeling, volume 3. Morgan & Claypool Pub-
lishers.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics.
R. Reichart and R. Barzilay. 2012. Multi event extrac-
tion guided by global constraints. In NAACL, pages
70?79.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In EMNLP.
S. Riedel. 2009. Cutting plane MAP inference for
Markov logic. Machine Learning.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Hwee Tou Ng and Ellen Riloff, editors,
CoNLL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
A.M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through Lagrangian re-
laxation. In ACL, pages 72?82, Portland, Oregon,
USA, June.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
V. Srikumar, G. Kundu, and D. Roth. 2012. On amor-
tizing inference cost for structured prediction. In
EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2008.
A global joint model for semantic role labeling.
Computational Linguistics, 34:161?191.
913
Transactions of the Association for Computational Linguistics, 1 (2013) 231?242. Action Editor: Noah Smith.
Submitted 11/2012; Revised 2/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Modeling Semantic Relations Expressed by Prepositions
Vivek Srikumar and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801.
{vsrikum2, danr}@illinois.edu
Abstract
This paper introduces the problem of predict-
ing semantic relations expressed by preposi-
tions and develops statistical learning models
for predicting the relations, their arguments
and the semantic types of the arguments. We
define an inventory of 32 relations, build-
ing on the word sense disambiguation task
for prepositions and collapsing related senses
across prepositions. Given a preposition in
a sentence, our computational task to jointly
model the preposition relation and its argu-
ments along with their semantic types, as a
way to support the relation prediction. The an-
notated data, however, only provides labels for
the relation label, and not the arguments and
types. We address this by presenting two mod-
els for preposition relation labeling. Our gen-
eralization of latent structure SVM gives close
to 90% accuracy on relation labeling. Further,
by jointly predicting the relation, arguments,
and their types along with preposition sense,
we show that we can not only improve the re-
lation accuracy, but also significantly improve
sense prediction accuracy.
1 Introduction
This paper addresses the problem of predicting se-
mantic relations conveyed by prepositions in text.
Prepositions express many semantic relations be-
tween their governor and object. Predicting these
can help advancing text understanding tasks like
question answering and textual entailment. Consider
the sentence:
(1) The book of Prof. Alexander on primary school
methods is a valuable teaching resource.
Here, the preposition on indicates that the book
and primary school methods are connected by the
relation Topic and of indicates the Creator-
Creation relation between Prof. Alexander and
the book. Predicting these relations can help answer
questions about the subject of the book and also rec-
ognize the entailment of sentences like Prof. Alexan-
der has written about primary school methods.
Being highly polysemous, the same preposition
can indicate different kinds of relations, depending
on its governor and object. Furthermore, several
prepositions can indicate the same semantic relation.
For example, consider the sentence:
(2) Poor care led to her death from pneumonia.
The preposition from in this sentence expresses the
relation Cause(death, pneumonia). In a differ-
ent context, it can denote other relations, as in the
phrases copied from the film (Source) and recog-
nized from the start (Temporal). On the other
hand, the relation Cause can be expressed by sev-
eral prepositions; for example, the following phrases
express a Cause relation: died of pneumonia and
tired after the surgery.
We characterize semantic relations expressed by
transitive prepositions and develop accurate models
for predicting the relations, identifying their argu-
ments and recognizing the semantic types of the ar-
guments. Building on the word sense disambigua-
tion task for prepositions, we collapse semantically
related senses across prepositions to derive our re-
lation inventory. These relations act as predicates
in a predicate-argument representation, where the
arguments are the governor and the object of the
231
preposition. While ascertaining the arguments is a
largely syntactic decision, we point out that syn-
tactic parsers do not always make this prediction
correctly. However, as illustrated in the examples
above, identifying the relation depends on the gov-
ernor and object of the preposition.
Given a sentence and a preposition, our goal is
to model the predicate (i.e. the preposition rela-
tion) and its arguments (i.e. the governor and ob-
ject). Very often, the relation label is not influenced
by the surface form of the arguments but rather by
their semantic types. In sentence (2) above, we
want the predicate to be Cause when the object of
the preposition is any illness. We thus suggest to
model the argument types along with the preposi-
tion relations and arguments, using different notions
of types. These three related aspects of the rela-
tion prediction task are further explained in Section
3 leading up to the problem definition.
Though we wish to predict relations, arguments
and types, there is no corpus which annotates all
three. The SemEval 2007 shared task of word sense
disambiguation for prepositions provides sense an-
notations for prepositions. We use this data to gen-
erate training and test corpora for the relation la-
bels. In Section 4, we present two models for the
prepositional relation identification problem. The
first model considers all possible argument candi-
dates from various sources along with all argument
types to predict the preposition relation label. The
second model treats the arguments and types as la-
tent variables during learning using a generalization
of the latent structural SVM of (Yu and Joachims,
2009). We show in Section 5 that this model not
only predicts the arguments and types, but also im-
proves relation prediction performance.
The primary contributions of this paper are:
1. We introduce a new inventory of preposition
relations that covers the 34 prepositions that
formed the basis of the SemEval 2007 task of
preposition sense disambiguation.
2. We model preposition relations, arguments and
their types jointly and propose a learning algo-
rithm that learns to predict all three using train-
ing data that annotates only relation labels.
3. We show that jointly predicting relations with
word sense not only improves the relation pre-
dictor, but also gives a significant improvement
in sense prediction.
2 Prepositions & Predicate-Argument
Semantics
Semantic role labeling (cf. (Gildea and Jurafsky,
2002; Palmer et al, 2010; Punyakanok et al, 2008)
and others) is the task of converting text into a
predicate-argument representation. Given a trigger
word or phrase in a sentence, this task solves two
related prediction problems: (a) identifying the rela-
tion label, and (b) identifying and labeling the argu-
ments of the relation.
This problem has been studied in the con-
text of verb and nominal triggers using the Prop-
Bank (Palmer et al, 2005) and NomBank (Meyers
et al, 2004) annotations over the Penn Treebank,
and also using the FrameNet lexicon (Fillmore et
al., 2003), which allows arbitrary words to trigger
semantic frames.
This paper focuses on semantic relations ex-
pressed by transitive prepositions1. We can define
the two prediction tasks for prepositions as follows:
identifying the relation label for a preposition, and
predicting the arguments of the relation. Preposi-
tions can mark arguments (both core and adjunct)
for verbal and nominal predicates. In addition, they
can also trigger relations that are not part of other
predicates. For example, in sentence (3) below, the
prepositional phrase starting with to is an argument
of the verb visit, but the in triggers an independent
relation indicating the location of the aquarium.
(3) The children enjoyed the visit to the aquarium
in Coney Island.
FrameNet covers some prepositional relations, but
allows only temporal, locative and directional senses
of prepositions to evoke frames, accounting for only
3% of the targets in the SemEval 2007 shared task
of FrameNet parsing. In fact, the state-of-the-art
FrameNet parser of (Das et al, 2010) does not con-
sider any frame inducing prepositions.
(Baldwin et al, 2009) highlights the importance
of studying prepositions for a complete linguistic
1By transitive prepositions we refer to the standard usage of
prepositions that take an object. In particular, we do not con-
sider prepositional particles in our analysis.
232
analysis of sentences and surveys work in the NLP
literature that addresses the syntax and semantics
of prepositions. One line of work (Ye and Bald-
win, 2006) addressed the problem of preposition
semantic role labeling by considering prepositional
phrases that act as arguments of verbs according
to the PropBank annotation. They built a system
that predicts the labels of these prepositional phrases
alone. However, by definition, this covered only
verb-attached prepositions. (Zapirain et al, 2012)
studied the impact of automatically learned selec-
tional preferences for predicting arguments of verbs
and showed that modeling prepositional phrases sep-
arately improves the performance of argument pre-
diction.
Preposition semantics has also been studied
via the Preposition Project (Litkowski and Har-
graves, 2005) and the related SemEval 2007 shared
task of word sense disambiguation of prepositions
(Litkowski and Hargraves, 2007). The Preposi-
tion Project identifies preposition senses based on
their definitions in the Oxford Dictionary of English.
There are 332 different labels to be predicted with a
wide variance in the number of senses per preposi-
tion ranging from 2 (during and as) to 25 (on). For
example, according to the preposition sense inven-
tory, the preposition from in sentence (2) above will
be labeled with the sense from:12(9) to indicate a
cause. (Dahlmeier et al, 2009) added sense anno-
tation to seven prepositions in four sections of the
Penn Treebank with the goal of studying their inter-
action with verb arguments.
Using the SemEval data, (Tratz and Hovy, 2009)
and (Hovy et al, 2010) showed that the arguments
offer an important cue to identify the sense of the
preposition and (Tratz, 2011) showed further im-
provements by refining the sense inventory. How-
ever, though these works used a dependency parser
to identify arguments, in order to overcome parsing
errors, they augment the parser?s predictions using
part-of-speech based heuristics.
We argue that, while disambiguating the sense
of a preposition does indeed reveal nuances of its
meaning, it leads to a proliferation of labels to be
predicted. Most importantly, sense labels do not
transfer to other prepositions that express the same
meaning. For example, both finish lunch before
noon and finish lunch by noon express a Temporal
relation. According to the Preposition Project, the
sense label for the first preposition is before:1(1),
and that for the second is by:17(4). This both de-
feats the purpose of identifying the relations to aid
natural language understanding and makes the pre-
diction task harder than it should be: using the stan-
dard word sense classification approach, we need to
train a separate classifier for each word because the
labels are defined per-preposition. In other words,
we cannot share features across the different prepo-
sitions. This motivates the need to combine such
senses of prepositions into the same class label.
In this direction, (O?Hara and Wiebe, 2009) de-
scribes an inventory of preposition relations ob-
tained using Penn Treebank function tags and frame
elements from FrameNet. (Srikumar and Roth,
2011) merged preposition senses of seven preposi-
tions into relation labels. (Litkowski, 2012) also
suggests collapsing the definitions of prepositions
into a smaller set of semantic classes. To aid bet-
ter generalization and to reduce the label complex-
ity, we follow this line of work to define a set of rela-
tion labels which abstract word senses across prepo-
sitions2.
3 Preposition-triggered Relations
This section describes the inventory of preposition
relations introduced in this paper, and then identifies
the components of the preposition relation extraction
problem.
3.1 Preposition Relation Inventory
We build our relation inventory using the sense an-
notation in the Preposition Project, focusing on the
34 prepositions3 annotated for the SemEval-2007
shared task of preposition sense disambiguation.
As discussed in Section 2, we construct the in-
ventory of preposition relations by collapsing se-
mantically related preposition senses across differ-
2Since the preposition sense data is annotated over
FrameNet sentences, sense annotation can be used to extend
FrameNet (Litkowski, 2012). We believe that the abstract la-
bels proposed in this paper can further help in this effort.
3We consider the following prepositions: about, above,
across, after, against, along, among, around, as, at, before, be-
hind, beneath, beside, between, by, down, during, for, from, in,
inside, into, like, of, off, on, onto, over, round, through, to, to-
wards, and with. This does not include multi-word prepositions
such as because of and due to.
233
ent prepositions. For each sense that is defined,
the Preposition Project also specifies related prepo-
sitions. These definitions and related prepositions
provide a starting point to identify senses that can
be merged across prepositions. We followed this
with a manual cleanup phase. Some senses do not
cleanly align with a single relation because the def-
initions include idiomatic or figurative usage. For
example, the sense in:7(5) of the preposition in, ac-
cording to the definition, includes both spatial and
figurative notions of the spatial sense (that is, both
in London and in a film). In such cases, we sam-
pled 20 examples from the SemEval 2007 training
set and assigned the relation label based on majority.
If sufficient examples could not be sampled, these
senses were added to the label Other, which is not
a semantically coherent category and represents the
?overflow? case.
Overall, we have 32 labels, which are listed in
Table 14. A companion publication (available on
the authors? website) provides detailed definitions
of each relation and the senses that were merged to
create each label. Since we define relations to be
groups of preposition sense labels, each sense can
be uniquely mapped to a relation label. Hence, we
can use the annotated sense data from SemEval 2007
to obtain a corpus of relation-labeled sentences.
To validate the labeling scheme, two native speak-
ers of English annotated 200 sentences from the
SemEval training corpus using only the definitions
of the labels as the annotation guidelines. We mea-
sured Cohen?s kappa coefficient (Cohen, 1960) be-
tween the annotators to be 0.75 and also between
each annotator and the original corpus to be 0.76 and
0.74 respectively.
3.2 Preposition Relation Extraction
The input to the prediction problem consists of a
preposition in a sentence and the goal is to jointly
model the following: (i) The relation expressed by
the preposition, and (ii) The arguments of the rela-
tion, namely the governor and the object.
We use sentence (2) in the introduction as our run-
ning example the following discussion. In our run-
4Note that, even though we do not consider intransitive
prepositions, the definitions of some relations in Table 1 could
be extended apply to prepositional particles such drive down
(Direction) and run about (Manner).
Relation Name Example
Activity good at boxing
Agent opened by Annie
Attribute walls of stone
Beneficiary fight for Napoleon
Cause died of cancer
Co-Participants pick one among these
Destination leaving for London
Direction drove towards the border
EndState driven to tears
Experiencer warm towards her
Instrument cut with a knife
Journey travel by road
Location living in London
Manner scream like an animal
MediumOfCommunication new show on TV
Numeric increase by 10%
ObjectOfVerb murder of the boys
Opponent/Contrast fight with him
Other all others
Participant/Accompanier steak with wine
PartWhole member of gang
PhysicalSupport lean against the wall
Possessor son of a friend
ProfessionalAspect works in publishing
Purpose tools for making it
Recipient unkind to her
Separation ousted from power
Source purchased from the shop
Species city of Prague
StartState recover from illness
Temporal arrived on Monday
Topic books on Shakespeare
Table 1: List of preposition relations
ning example, the relation label is Cause. We rep-
resent the predicted relation label by r.
Arguments The relation label crucially depends
on correctly identifying the arguments of the prepo-
sition, which are death and pneumonia in our run-
ning example. While a parser can identify the argu-
ments of a preposition, simply relying on the parser
may impose an upper limit on the accuracy of rela-
tion prediction.
We build an oracle experiment to highlight this
limitation. Table 2 shows the recall of the easy-first
dependency parser of (Goldberg and Elhadad, 2010)
on Section 23 of the Penn Treebank for identifying
the governor and object of prepositions.
We define heuristics that generate a candidate
governors and objects for a preposition. For the gov-
234
ernor, this set includes the previous verb or noun
and for the object, it includes only the next noun.
The row labeled Best(Parser, Heuristics) shows the
performance of an oracle predictor which selects the
true governor/object if present among the parser?s
prediction and the heuristics. We see that, even for
the in-domain case, if we are able to re-rank the can-
didates, we could achieve a big improvement in ar-
gument identification.
Recall
Governor Object
Parser 88.88 92.37
Best(Parser, Heuristics) 92.50 93.06
Table 2: Identifying governor and object of prepositions
in the Penn Treebank data. Here, Best(Parser, Heuris-
tics) reports the performance of an oracle that picks the
true governor and object, if present among the candidates
presented by the parser and the heuristic. This presents
an in-domain upper bound for governor and object detec-
tion. See text for further details.
To overcome erroneous parser decisions, we en-
tertain governor and object candidates proposed
both by the parser and the heuristics. In the follow-
ing discussion, we denote the chosen governor and
object by g and o respectively.
Argument types While the primary purpose of
this work is to model preposition relations and their
arguments, the relation prediction is strongly depen-
dent on the semantic type of the arguments. To il-
lustrate this, consider the following incomplete sen-
tence: The message was delivered at ? ? ? . This
preposition can express both a Temporal or a
Location relation depending on the object (for ex-
ample, noon vs. the doorstep).
(Agirre et al, 2008) shows that modeling the se-
mantic type of the arguments jointly with attachment
can improve PP attachment accuracy. In this work,
we point out that argument types should be modeled
jointly with both aspects of the problem of preposi-
tion relation labeling.
Types are an abstraction that capture common
properties of groups of entities. For example, Word-
Net provides generalizations of words in the form of
their hypernyms. In our running example, we wish
to generalize the relation label for death from pneu-
monia to include cases such as suffering from flu.
Figure 1 shows the hypernym hierarchy for the word
pneumonia. In this case, synsets in the hypernym
hierarchy, like pathological state or physical condi-
tion, would also include ailments like flu.
pneumonia
=> respiratory disease
=> disease
=> illness
=> ill health
=> pathological state
=> physical condition
=> condition
=> state
=> attribute
=> abstraction
=> entity
Figure 1: Hypernym hierarchy for the word pneumonia
We define a semantic type to be a cluster of words.
In addition to WordNet hypernyms, we also cluster
verbs, nouns and adjectives using the dependency-
based word similarity of (Lin, 1998) and treat cluster
membership as types. These are described in detail
in Section 5.1.
Relation prediction involves not only identifying
the arguments, but also selecting the right semantic
type for them, which together, help predicting the
relation label. Given an argument candidate and a
collection of possible types (given by WordNet or
the similarity based clusters), we need to select one
of the types. For example, in the WordNet case, we
need to pick one of the hypernyms in the hypernym
hierarchy. Thus, for the governor and object, we
have a set of type labels, comprised of one element
for each type category. We denote this by tg (gover-
nor type) and to (object type) respectively.
3.3 Problem definition
The input to our prediction task is a preposition in
a sentence. Our goal is to jointly model the relation
it expresses, the governor and the object of the rela-
tion and the types of each argument (both WordNet
hypernyms and cluster membership). We denote the
input by x, which consists not only of the prepo-
sition but also a set of candidates for the governor
and the object and, for each type category, the list of
types for the governor and candidate.
235
The prediction, which we denote by y, consists
of the relation r, which can be one of the valid re-
lation labels in Table 1 and the governor and object,
denoted by g and o, each of which is one of text seg-
ments proposed by the parser or the heuristics. Ad-
ditionally, y also consists of type predictions for the
governor and object, denoted by tg and to respec-
tively, each of which is a vector of labels, one for
each type category. Table 3 summarizes the nota-
tion described above. We refer to the ith element of
vectors using subscripts and use the superscript ? to
denote gold labels. Recall that we have gold labels
only for the relation labels and not for arguments and
their types.
Symbol Meaning
x Input (pre-processed sentence and
preposition)
r relation label for the preposition
g, o governor and object of the relation
tg, to vectors of type assignments for
governor and object respectively
y Full structure (r, g, o, tg, to)
Table 3: Summary of notation
4 Learning preposition relations
A key challenge in modeling preposition relations is
that our training data only annotates the relation la-
bels and not the arguments and types. In this section,
we introduce two approaches for predicting preposi-
tion relations using this data.
4.1 Feature Representation
We use the notation ?(x,y) to indicate the feature
function for an input x and the full output y. We
build ? using the features of the components of y:
1. Arguments: For g and o, which represent an
assignment to the governor and object, we de-
note the features extracted from the arguments
as ?A(x, g) and ?A(x, o) respectively.
2. Types: Given a type assignment tgi to the ith
type category of the governor, we define fea-
tures ?T (x, g, tgi ). Similarly, we define features
?T (x, o, toi ) for the types of the object.
We combine the argument and their type features to
define the features for classifying the relation, which
we denote by ?(x, g, o, tg, to):
? =
?
a?{g,o}
(
?A(x, a) +
?
i
?T (x, a, tai )
)
(1)
Section 5 describes the actual features used in our
experiments.
Observe that given the arguments and their types,
the task of predicting relations is simply a multiclass
classification problem. Thus, following the standard
convention for multiclass classification, the overall
feature representation for the relation and argument
prediction is defined by conjoining the relation r
with features for the corresponding arguments and
types, ?. This gives us the full feature representa-
tion, ?(x,y).
4.2 Model 1: Predicting only relations
The first model aims at predicting only the rela-
tion labels and not the arguments and types. This
falls into the standard multiclass classification set-
ting, where we wish to predict one of 32 labels. To
do so, we sum over all the possible assignments to
the rest of the structure and define features for the
inputs as
??(x) =
?
g,o,tg ,to
?(x, g, o, tg, to) (2)
Effectively, doing so uses all the governor and ob-
ject candidates and all their semantic types to get
a feature representation for the relation classifica-
tion problem. Once again, for a relation label r, the
overall feature representation is defined by conjoin-
ing the relation r with the features for that relation
??, which we write as ?R(x, r). Note that this sum-
mation is computationally inexpensive in our case
because the sum decomposes according to equation
(1). With a learned weight vector w, the relation
label is predicted as
r = arg max
r?
wT?R(x, r?) (3)
We use a structural SVM (Tsochantaridis et al,
2004) to train a weight vector w that predicts the re-
lation label as above. The training is parameterized
by C, which represents the tradeoff between gener-
alization and the hinge loss.
236
4.3 Model 2: Learning from partial
annotations
In the second model, even though our annotation
does not provide gold labels for arguments and
types, our goal is to predict them. At inference time,
if we had a weight vector w, we could predict the
full structure using inference as follows:
y = arg max
y?
wT?(x,y) (4)
We propose an iterative learning algorithm to learn
this weight vector.
In the following discussion, for a labeled example
(x,y?), we refer to the missing part of its structure
as h(y?). That is, h(y?) is the assignment to the
arguments of the relation and their types. We use the
notation r(y) to denote the relation label specified
by a structure y.
Our learning algorithm is closely related to re-
cently developed latent variable based frameworks
(Yu and Joachims, 2009; Chang et al, 2010a; Chang
et al, 2010b), where the supervision provides only
partial annotation. We begin by defining two addi-
tional inference procedures:
1. Latent Inference: Given a weight vector w
and a partially labeled example (x,y?), we can
?complete? the rest of the structure by infer-
ring the highest scoring assignment to the miss-
ing parts. In the algorithm, we call this pro-
cedure LatentInf(w,x,y?), which solves the
following maximization problem:
y? = arg maxy wT?(x,y), (5)
s.t. r(y) = r(y?).
2. Loss augmented inference: This is a variant
of the the standard loss augmented inference
for structural SVMs, which solves the follow-
ing maximization problem for a given x and
fully labeled y?:
arg max
y
wT?(x,y) + ?(y,y?) (6)
Here, ?(y,y?) denotes the loss function. In
the standard structural SVMs, the loss is over
the entire structure. In the Latent Structural
SVM formulation of (Yu and Joachims, 2009),
the loss is defined only over the part of the
structure with the gold label. In this work, we
use the standard Hamming loss over the entire
structure, but scale the loss for the elements of
h(y) by a parameter ? < 1. This is a gen-
eralization of the latent structural SVM, which
corresponds to the setting ? = 0. The intu-
ition behind having a non-zero ? is that in ad-
dition to penalizing the learning algorithm if it
violates the annotated part of the structure, we
also incorporate a small penalty for the rest of
the structure.
Using these two inference procedures, we define
the learning algorithm as Algorithm 1. The weight
vector is initialized using Model 1. The algorithm
then finds the best arguments and types for all ex-
amples in the training set (steps 3-5). Doing so
gives an estimate of the arguments and types for
each example, giving us ?fully labeled? structured
data. The algorithm then proceeds to use this data to
train a new weight vector using the standard struc-
tural SVM with the loss augmented inference listed
above (step 6). These two steps are repeated several
times. Note that as with the summation in Model
1, solving the inference problems described above is
computationally inexpensive.
Algorithm 1 Algorithm for learning Model 2
Input: Examples D = {xi, r(y?i )}, where exam-
ples are labeled only with the relation labels.
1: Initialize weight vector w using Model 1
2: for t = 1, 2, ? ? ? do
3: for (xi,y?i ) ? D do
4: y?i ? LatentInf(w,xi,y?i ) (Eq. 5)
5: end for
6: w ? LearnSSVM({xi, y?i}) with the loss
augmented inference of Eq. 6
7: end for
8: return w
Algorithm 1 is parameterized by C and ?. The
parameter ? controls the extent to which the hypoth-
esized labels according to the previous iteration?s
weight vector influence the learning.
237
4.4 Joint inference between preposition senses
and relations
By defining preposition relations as disjoint sets of
preposition senses, we effectively have a hierarchi-
cal relationship between senses and relations. This
suggests that joint inference can be employed be-
tween sense and relation predictions with a validity
constraint connecting the two. The idea of employ-
ing inference to combine independently trained pre-
dictors to obtain a coherent output structure has been
used for various NLP tasks in recent years, starting
with the work of (Roth and Yih, 2004; Roth and Yih,
2007).
We use the features defined by (Hovy et al, 2010),
which we write as ?s(x, s) for a given input x and
sense label s, and train a separate preposition sense
model on the SemEval data with features ?s(x, s)
using the structural SVM algorithm. Thus, we have
two weight vectors ? the one for predicting preposi-
tion relations described earlier, and the preposition
sense weight vector. At prediction time, for a given
input, we find the highest scoring joint assignment to
the relation, arguments and types and the sense, sub-
ject to the constraint that the sense and the relation
agree according to the definition of the relations.
5 Experiments and Results
The primary research goal of our experiments is to
evaluate the different models (Model 1, Model 2 and
joint relation-sense inference) for predicting prepo-
sition relations. In additional analysis experiments,
we also show that the definition of preposition rela-
tions indeed captures cross-preposition semantics by
taking advantage of shared features and also high-
light the need for going beyond the syntactic parser.
5.1 Types and Features
Types As described in Section 3, we use WordNet
hypernyms as one of the type categories. We use all
hypernyms within four levels in the hypernym hier-
archy for all senses.
The second type category is defined by word-
similarity driven clusters. We briefly describe the
clustering process here. The thesaurus of (Lin,
1998) specifies similar lexical items for a given
word along with a similarity score from 0 to 1. It
treats nouns, verbs and adjectives separately. We
use the score to cluster groups of similar words us-
ing a greedy set-covering approach. Specifically,
we randomly select a word which is not yet in a
cluster as the center of a new cluster and add all
words whose score is greater than ? to it. We re-
peat this process till all words are in some clus-
ter. A word can appear in more than one cluster
because all words similar to the cluster center are
added to the cluster. We repeat this process for
? ? {0.1, 0.125, 0.15, 0.175, 0.2, 0.25}. By increas-
ing the value of ?, the clusters become more selec-
tive and hence smaller. Table 4 shows example noun
clusters created using ? = 0.15. For a given word,
identifiers for clusters to which the word belongs
serve as type label candidates for this type category5.
Features Our argument features, denoted by ?A
in Section 4.1, are derived from the preposition
sense feature set of (Hovy et al, 2010) and extract
the following from the argument: 1. Word, part-of-
speech, lemma and capitalization indicator, 2. Con-
flated part-of-speech (one of Noun, Verb, Adjective,
Adverb, and Other), 3. Indicator for existence in
WordNet, 4. WordNet synsets for the first and all
senses, 5. WordNet lemma, lexicographer file names
and part, member and substance holonyms, 6. Roget
thesaurus divisions for the word, 7. The first and last
two and three letters, and 8. Indicators for known af-
fixes. Our type features (?T ) are simply indicators
for the type label, conjoined with the type category.
One advantage of abstracting word senses into re-
lations is that we can share features across different
prepositions. The base feature set (for both types
and arguments) defined above does not encode in-
formation about the preposition to be classified. We
do so by conjoining the features with the preposi-
tion. In addition, since the relation labels are shared
across all prepositions, we include the base features
as a shared representation between prepositions.
We consider two variants of our feature sets.
We refer to the features described above as the
typed features. In addition, we define the
typed+gen features by conjoining argument and
type features of typed with the name of the genera-
tor that proposes the argument. Recall that governor
candidates are proposed by the dependency parser,
or by the heuristics described earlier. Hence, for
5The clusters can be downloaded from the authors? website.
238
Jimmy Carter; Ronald Reagan; richard nixon; George Bush; Lyndon Johnson; Richard M. Nixon; Gerald Ford
metalwork; porcelain; handicraft; jade; bronzeware; carving; pottery; ceramic; earthenware; jewelry; stoneware; lacquerware
degradation; erosion; pollution; logging; desertification; siltation; urbanization; felling; poaching; soil erosion; depletion;
water pollution; deforestation
expert; Wall Street analyst; analyst; economist; telecommunications analyst; strategist; media analyst
fox news channel; NBC News; MSNBC; Fox News; CNBC; CNNfn; C-Span
Tuesdays; Wednesdays; weekday; Mondays; Fridays; Thursdays; sundays; Saturdays
Table 4: Examples of noun clusters generated using the set-covering approach for ? = 0.15
a governor, the typed+gen features would conjoin
the corresponding typed features with one of parser,
previous-verb, previous-noun, previous-adjective, or
previous-word.
5.2 Experimental setup and data
All our experiments are based on the Sem-
Eval 2007 data for preposition sense disambigua-
tion (Litkowski and Hargraves, 2007) comprising
word sense annotation over 16176 training and
8058 examples of prepositions labeled with their
senses. We pre-processed sentences with part-of-
speech tags using the Illinois POS tagger and de-
pendency graphs using the parser of (Goldberg and
Elhadad, 2010)6. For the experiments described be-
low, we used the relation-annotated training set to
train the models and evaluate accuracy of prediction
on the test set.
We chose the structural SVM parameter C using
five-fold cross-validation on a 1000 random exam-
ples chosen from the training set. For Model 2, we
picked ? = 0.1 using a validation set consisting of
a separate set of 1000 training examples. We ran
Algorithm 1 for 20 rounds.
Predicting the most frequent relation for a prepo-
sition gives an accuracy of 21.18%. Even though
the performance of the most-frequent relation label
is poor, it does not represent the problem?s difficulty
and is not a good baseline. To compare, for prepo-
sition senses, using features from the neighboring
words, (Ye and Baldwin, 2007) obtained an accuracy
of 69.3%, and with features designed for the prepo-
sition sense task, (Hovy et al, 2010) get up to 84.8%
accuracy for the task. Our re-implementation of the
latter system using a different set of pre-processing
tools gets an accuracy of 83.53%.
For preposition relations, our baseline system for
6We used the Curator (Clarke et al, 2012) for all pre-
processing.
relation labeling uses the typed feature set, but with-
out any type information. This produces an accuracy
of 88.01% with Model 1 and 88.64% with Model 2.
We report statistical significance of results using our
implementation of Dan Bikel?s stratified-shuffling
based statistical significance tester7.
5.3 Main results: Relation prediction
Our main result, presented in Table 5, compares the
baseline model (without types) against other sys-
tems, using both models described in Section 4.
First, we see that adding type information (typed)
improves performance over the baseline. Expand-
ing the feature space (typed+gen) gives further im-
provements. Finally, jointly predicting the relations
with preposition senses gives another improvement.
Setting AccuracyModel 1 Model 2
No types 88.01 88.64
typed 88.77 89.14
typed+gen 89.90? 89.43?
Joint typed+gen & sense 89.99? 90.26??
Table 5: Main results: Accuracy of relation labeling.
Results in bold are statistically significant (p < 0.01)
improvements over the system that is unaware of types.
Superscripts ? and ? indicate significant improvements
over typed and typed+gen respectively at p < 0.01. For
Model 2, the improvement of typed over the model with-
out types is significant at p < 0.05.
Our objective is not predicting preposition sense.
However, we observe that with Model 2, jointly pre-
dicting the sense and relations improves not only the
performance of relation identification, but via joint
inference between relations and senses also leads to
a large improvement in sense prediction accuracy.
Table 6 shows the accuracy for sense prediction. We
7http://www.cis.upenn.edu/?dbikel/software.html
239
see that while Model 1 does not lead to a significant
improvement in the accuracy, Model 2 gives an ab-
solute improvement of over 1%.
Setting Sense accuracy
Hovy (re-implementation) 83.53
Joint + Model 1 83.78
Joint + Model 2 84.78?
Table 6: Sense prediction performance. Joint inference
with Model 1, while improving relation performance,
does not help sense accuracy in comparison to our re-
implementation of the Hovy sense disambiguation sys-
tem. However, with Model 2, the improvement is statis-
tically significant at p < 0.01.
5.4 Ablation experiments
Feature sharing across prepositions In our first
analysis experiment, we seek to highlight the utility
of sharing features between different prepositions.
To do so, we compare the performance of a sys-
tem trained without shared features against the type-
independent system, which uses shared features. To
discount the influence of other factors, we use Model
1 in the typed setting without any types. Table 7
reports the accuracy of relation prediction for these
two feature sets. We observed a similar improve-
ment in performance even when type features are
added or the setting is changed to typed+gen or with
Model 2.
Setting Accuracy
Independent 87.17
+ Shared 88.01
Table 7: Comparing the effect of feature sharing across
prepositions. We see that having a shared representation
that goes across prepositions improves accuracy of rela-
tion prediction (p < 0.01).
Different argument candidate generators Our
second ablation study looks at the effect of the var-
ious argument candidate generators. Recall that in
addition to the dependency governor and object, our
models also use the previous word, the previous
noun, adjective and verb as governor candidates and
the next noun as object candidate. We refer to the
candidates generated by the parser as Parser only
and the others as Heuristics only. Table 8 compares
the performance of these two argument candidate
generators against the full set using Model 1 in both
the typed and typed+gen settings.
We see that the heuristics give a better accu-
racy than the parser based system. This is because
the heuristics often contain the governor/object pre-
dicted by the dependency. This is not always the
case, though, because using all generators gives a
slightly better performing system (not statistically
significant). In the overall system, we retain the de-
pendency parser as one of the generators in order to
capture long-range governor/object candidates that
may not be in the set selected by the heuristics.
Feature sets
Generator typed typed+gen
Parser only 87.12 87.12
Heuristics only 87.63 88.84
All 88.01 89.12
Table 8: The performance of different argument candi-
date generators. We see that considering a larger set of
candidate generators gives a big accuracy improvement.
6 Discussion
There are two key differences between Model 1 and
2. First, the former predicts only the relation label,
while the latter predicts the entire structure. Table 9
shows example predictions of Model 2 for relation
label and WordNet argument types. These examples
show how the argument types can be thought of as
an explanation for the choice of relation label.
Input Relation Hypernyms
governor object
died of pneumonia Cause experience disease
suffered from flu Cause experience disease
recovered from flu StartState change disease
Table 9: Example predictions according to Model 2. The
hypernyms column shows a representative of the synset
chosen for the WordNet types. We see that in the com-
bination of experience and disease suggests the relation
Cause while the change and disease indicate the rela-
tion StartState.
The main difference between the two models is
in the treatment of the unlabeled (or latent) parts of
the structure (namely, the arguments and the types)
during training and inference. During training, for
240
each example, Model 1 aggregates features from all
governors and objects even if they are possibly ir-
relevant, which may lead to a much bigger model in
terms of the number of active weights. On the other
hand, for Model 2, Algorithm 1 uses the single high-
est scoring prediction of the latent variables, accord-
ing to the current parameters, to refine the parame-
ters. Indeed, in our experiments, we observed that
the number of non-zero weights in the weight vec-
tor of Model 2 is much smaller than that of Model
1. For instance, in the typed setting, the weight vec-
tor for Model 1 had 2.57 million elements while that
for Model 2 had only 1.0 million weights. Similarly,
for the typed+gen setting, Model 1 had 5.41 million
non-zero elements in the weight vector while Model
2 had only 2.21 million non-zero elements.
The learning algorithm itself is a generalization
of the latent structural SVM of (Yu and Joachims,
2009). By setting ? to zero, we get the latent struc-
ture SVM. However, we found via cross-validation
that this is not the best setting of the parameter. A
theoretical understanding of the sparsity of weights
learned by the algorithm and a study of its conver-
gence properties is an avenue of future research.
7 Conclusion
We addressed the problem of modeling semantic re-
lations expressed by prepositions. We approached
this task by defining a set of preposition relations
that combine preposition senses across prepositions.
Doing so allowed us to leverage existing annotated
preposition sense data to induce a corpus for prepo-
sition labels. We modeled preposition relations in
terms of its arguments, namely the governor and ob-
ject of the preposition, and the semantic types of
the arguments. Using a generalization of the latent
structural SVM, we trained a relation, argument and
type predictor using only annotated relation labels.
This allowed us to get an accuracy of 89.43% on re-
lation prediction. By employing joint inference with
a preposition sense predictor, we further improved
the relation accuracy to 90.23%.
Acknowledgments
The authors wish to thank Martha Palmer, Nathan Schneider,
the anonymous reviewers and the editor for their valuable feed-
back. The authors gratefully acknowledge the support of the
Defense Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. This material
is also based on research sponsored by DARPA under agree-
ment number FA8750-13-2-0008. The U.S. Government is au-
thorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. The
views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or implied, of
DARPA, AFRL or the U.S. Government.
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 317?325, Columbus, USA.
T. Baldwin, V. Kordoni, and A. Villavicencio. 2009.
Prepositions in applications: A survey and introduc-
tion to the special issue. Computational Linguistics,
35(2):119?149.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained latent
representations. In Proceedings of the Annual Meet-
ing of the North American Association of Computa-
tional Linguistics (NAACL), pages 429?437, Los An-
geles, USA.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010b. Structured output learning with indirect super-
vision. In Proceedings of the International Conference
on Machine Learning (ICML), pages 199?206, Haifa,
Israel.
J. Clarke, V. Srikumar, M. Sammons, and D. Roth. 2012.
An NLP Curator (or: How I Learned to Stop Wor-
rying and Love NLP Pipelines). In Proceedings of
the International Conference on Language Resources
and Evaluation (LREC), pages 3276?3283, Istanbul,
Turkey.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In Proceedings of the Confer-
ence on Empirical Methods for Natural Language Pro-
cessing (EMNLP), pages 450?458, Singapore.
D. Das, N. Schneider, D. Chen, and N. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceedings
of Human Language Technologies: The 2010 Annual
241
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 948?
956, Los Angeles, USA.
C. Fillmore, C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. International Journal of Lexi-
cography, 16(3):235?250.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proceedings of Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 742?750, Los Angeles, USA.
D. Hovy, S. Tratz, and E. Hovy. 2010. What?s in a prepo-
sition? dimensions of sense disambiguation for an in-
teresting word class. In Coling 2010: Posters, pages
454?462, Beijing, China.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 768?774, Montreal, Canada.
K. Litkowski and O. Hargraves. 2005. The Preposition
Project. In ACL-SIGSEM Workshop on the Linguistic
Dimensions of Prepositions and their Use in Computa-
tional Linguistics Formalisms and Applications, pages
171?179, Colchester, UK.
K. Litkowski and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation of Prepositions.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 24?
29, Prague, Czech Republic.
K. Litkowski. 2012. Proposed Next Steps for The Prepo-
sition Project. Technical Report 12-01, CL Research.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24?
31, Boston, USA.
T. O?Hara and J. Wiebe. 2009. Exploiting semantic role
resources for preposition disambiguation. Computa-
tional Linguistics, 35(2):151?184.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?106.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic Role
Labeling, volume 3. Morgan & Claypool Publishers.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
In Proceedings of the Annual Conference on Compu-
tational Natural Language Learning (CoNLL), pages
1?8, Boston, USA.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. Introduction to Statistical Rela-
tional Learning.
V. Srikumar and D. Roth. 2011. A joint model for
extended semantic role labeling. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Edinburgh, Scotland.
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium, pages 96?100, Boulder, USA.
S. Tratz. 2011. Semantically-enriched Parsing for Natu-
ral Language Understanding. Ph.D. thesis, University
of Southern California.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interde-
pendent and structured output spaces. In Proceedings
of the International Conference on Machine Learning
(ICML), pages 104?111, Banff, Canada.
P. Ye and T. Baldwin. 2006. Semantic role labeling
of prepositional phrases. ACM Transactions on Asian
Language Information Processing (TALIP), 5(3):228?
244.
P. Ye and T. Baldwin. 2007. MELB-YB: Preposition
sense disambiguation using rich semantic features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 241?
244, Prague, Czech Republic.
C. Yu and T. Joachims. 2009. Learning structural SVMs
with latent variables. In Proceedings of the Inter-
national Conference on Machine Learning (ICML),
pages 1?8, Montreal, Canada.
B. Zapirain, E. Agirre, L. Ma`rquez, and M. Surdeanu.
2012. Selectional preferences for semantic role classi-
fication. Computational Linguistics, pages 1?33.
242
