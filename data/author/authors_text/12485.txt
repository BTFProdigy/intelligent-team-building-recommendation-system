Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 459?467,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Employing Topic Models for Pattern-based Semantic Class Discovery 
 
 
Huibin Zhang1*     Mingjie Zhu2*     Shuming Shi3     Ji-Rong Wen3 
1Nankai University 
2University of Science and Technology of China 
3Microsoft Research Asia 
{v-huibzh, v-mingjz, shumings, jrwen}@microsoft.com 
 
  
 
Abstract? 
 
A semantic class is a collection of items 
(words or phrases) which have semantically 
peer or sibling relationship. This paper studies 
the employment of topic models to automati-
cally construct semantic classes, taking as the 
source data a collection of raw semantic 
classes (RASCs), which were extracted by ap-
plying predefined patterns to web pages. The 
primary requirement (and challenge) here is 
dealing with multi-membership: An item may 
belong to multiple semantic classes; and we 
need to discover as many as possible the dif-
ferent semantic classes the item belongs to. To 
adopt topic models, we treat RASCs as ?doc-
uments?, items as ?words?, and the final se-
mantic classes as ?topics?. Appropriate 
preprocessing and postprocessing are per-
formed to improve results quality, to reduce 
computation cost, and to tackle the fixed-k 
constraint of a typical topic model. Experi-
ments conducted on 40 million web pages 
show that our approach could yield better re-
sults than alternative approaches. 
1 Introduction 
Semantic class construction (Lin and Pantel, 
2001; Pantel and Lin, 2002; Pasca, 2004; Shinza-
to and Torisawa, 2005; Ohshima et al, 2006) 
tries to discover the peer or sibling relationship 
among terms or phrases by organizing them into 
semantic classes. For example, {red, white, 
black?} is a semantic class consisting of color 
instances. A popular way for semantic class dis-
covery is pattern-based approach, where prede-
fined patterns (Table 1) are applied to a 
                                                   
? This work was performed when the authors were interns at 
Microsoft Research Asia 
collection of web pages or an online web search 
engine to produce some raw semantic classes 
(abbreviated as RASCs, Table 2). RASCs cannot 
be treated as the ultimate semantic classes, be-
cause they are typically noisy and incomplete, as 
shown in Table 2. In addition, the information of 
one real semantic class may be distributed in lots 
of RASCs (R2 and R3 in Table 2). 
 
Type Pattern 
SENT NP {, NP}*{,} (and|or) {other} NP 
TAG <UL>  <LI>item</LI>  ?  <LI>item</LI>  </UL> 
TAG <SELECT> <OPTION>item?<OPTION>item </SELECT> 
* SENT: Sentence structure patterns; TAG: HTML Tag patterns 
Table 1. Sample patterns 
 
R1: {gold, silver, copper, coal, iron, uranium} 
R2: {red, yellow, color, gold, silver, copper} 
R3: {red, green, blue, yellow} 
R4: {HTML, Text, PDF, MS Word, Any file type} 
R5: {Today, Tomorrow, Wednesday, Thursday, Friday, 
Saturday, Sunday} 
R6: {Bush, Iraq, Photos, USA, War} 
Table 2. Sample raw semantic classes (RASCs) 
 
This paper aims to discover high-quality se-
mantic classes from a large collection of noisy 
RASCs. The primary requirement (and chal-
lenge) here is to deal with multi-membership, i.e., 
one item may belong to multiple different seman-
tic classes. For example, the term ?Lincoln? can 
simultaneously represent a person, a place, or a 
car brand name. Multi-membership is more pop-
ular than at a first glance, because quite a lot of 
English common words have also been borrowed 
as company names, places, or product names. 
For a given item (as a query) which belongs to 
multiple semantic classes, we intend to return the 
semantic classes separately, rather than mixing 
all their items together. 
Existing pattern-based approaches only pro-
vide very limited support to multi-membership. 
For example, RASCs with the same labels (or 
hypernyms) are merged in (Pasca, 2004) to gen-
459
erate the ultimate semantic classes. This is prob-
lematic, because RASCs may not have (accurate) 
hypernyms with them. 
In this paper, we propose to use topic models 
to address the problem. In some topic models, a 
document is modeled as a mixture of hidden top-
ics. The words of a document are generated ac-
cording to the word distribution over the topics 
corresponding to the document (see Section 2 for 
details). Given a corpus, the latent topics can be 
obtained by a parameter estimation procedure. 
Topic modeling provides a formal and conve-
nient way of dealing with multi-membership, 
which is our primary motivation of adopting top-
ic models here. To employ topic models, we treat 
RASCs as ?documents?, items as ?words?, and 
the final semantic classes as ?topics?. 
There are, however, several challenges in ap-
plying topic models to our problem. To begin 
with, the computation is intractable for 
processing a large collection of RASCs (our da-
taset for experiments contains 2.7 million unique 
RASCs extracted from 40 million web pages). 
Second, typical topic models require the number 
of topics (k) to be given. But it lacks an easy way 
of acquiring the ideal number of semantic classes 
from the source RASC collection. For the first 
challenge, we choose to apply topic models to 
the RASCs containing an item q, rather than the 
whole RASC collection. In addition, we also per-
form some preprocessing operations in which 
some items are discarded to further improve effi-
ciency. For the second challenge, considering 
that most items only belong to a small number of 
semantic classes, we fix (for all items q) a topic 
number which is slightly larger than the number 
of classes an item could belong to. And then a 
postprocessing operation is performed to merge 
the results of topic models to generate the ulti-
mate semantic classes. 
Experimental results show that, our topic 
model approach is able to generate higher-quality 
semantic classes than popular clustering algo-
rithms (e.g., K-Medoids and DBSCAN). 
We make two contributions in the paper: On 
one hand, we find an effective way of construct-
ing high-quality semantic classes in the pattern-
based category which deals with multi-
membership. On the other hand, we demonstrate, 
for the first time, that topic modeling can be uti-
lized to help mining the peer relationship among 
words. In contrast, the general related relation-
ship between words is extracted in existing topic 
modeling applications. Thus we expand the ap-
plication scope of topic modeling. 
2 Topic Models 
In this section we briefly introduce the two wide-
ly used topic models which are adopted in our 
paper. Both of them model a document as a mix-
ture of hidden topics. The words of every docu-
ment are assumed to be generated via a 
generative probability process. The parameters of 
the model are estimated from a training process 
over a given corpus, by maximizing the likelih-
ood of generating the corpus. Then the model can 
be utilized to inference a new document. 
pLSI: The probabilistic Latent Semantic In-
dexing Model (pLSI) was introduced in Hof-
mann (1999), arose from Latent Semantic 
Indexing (Deerwester et al, 1990). The follow-
ing process illustrates how to generate a docu-
ment d in pLSI: 
1. Pick a topic mixture distribution ?(? |?). 
2. For each word wi in d 
a. Pick a latent topic z with the probabil-
ity ?(?|?) for wi 
b. Generate wi with probability ?(?? |?) 
So with k latent topics, the likelihood of gene-
rating a document d is 
 ?(?) =  ? ?? ? ?(?|?)
??
 (2.1) 
LDA (Blei et al, 2003): In LDA, the topic 
mixture is drawn from a conjugate Dirichlet prior 
that remains the same for all documents (Figure 
1). The generative process for each document in 
the corpus is, 
1. Choose document length N from a Pois-
son distribution Poisson(?). 
2. Choose ?  from a Dirichlet distribution 
with parameter ?. 
3. For each of the N words wi. 
a. Choose a topic z from a Multinomial 
distribution with parameter ?. 
b. Pick a word wi from ? ??  ?,? . 
So the likelihood of generating a document is 
 ?(?) =  ?(?|?)
?
  ?(?|?)? ?? ?,? ??
??
 (2.2) 
 
 
Figure 1. Graphical model representation of LDA, 
from Blei et al (2003) 
 
w? z?
?
N
M
460
3 Our Approach 
The source data of our approach is a collection 
(denoted as CR) of RASCs extracted via applying 
patterns to a large collection of web pages. Given 
an item as an input query, the output of our ap-
proach is one or multiple semantic classes for the 
item. To be applicable in real-world dataset, our 
approach needs to be able to process at least mil-
lions of RASCs. 
3.1 Main Idea 
As reviewed in Section 2, topic modeling pro-
vides a formal and convenient way of grouping 
documents and words to topics. In order to apply 
topic models to our problem, we map RASCs to 
documents, items to words, and treat the output 
topics yielded from topic modeling as our seman-
tic classes (Table 3). The motivation of utilizing 
topic modeling to solve our problem and building 
the above mapping comes from the following 
observations. 
1) In our problem, one item may belong to 
multiple semantic classes; similarly in topic 
modeling, a word can appear in multiple top-
ics. 
2) We observe from our source data that 
some RASCs are comprised of items in mul-
tiple semantic classes. And at the same time, 
one document could be related to multiple 
topics in some topic models (e.g., pLSI and 
LDA). 
 
Topic modeling Semantic class construction 
word item (word or phrase) 
document RASC 
topic semantic class 
Table 3. The mapping from the concepts in topic 
modeling to those in semantic class construction 
 
Due to the above observations, we hope topic 
modeling can be employed to construct semantic 
classes from RASCs, just as it has been used in 
assigning documents and words to topics. 
There are some critical challenges and issues 
which should be properly addressed when topic 
models are adopted here. 
Efficiency: Our RASC collection CR contains 
about 2.7 million unique RASCs and 26 million 
(1 million unique) items. Building topic models 
directly for such a large dataset may be computa-
tionally intractable. To overcome this challenge, 
we choose to apply topic models to the RASCs 
containing a specific item rather than the whole 
RASC collection. Please keep in mind that our 
goal in this paper is to construct the semantic 
classes for an item when the item is given as a 
query. For one item q, we denote CR(q) to be all 
the RASCs in CR containing the item. We believe 
building a topic model over CR(q) is much more 
effective because it contains significantly fewer 
?documents?, ?words?, and ?topics?. To further 
improve efficiency, we also perform preprocess-
ing (refer to Section 3.4 for details) before build-
ing topic models for CR(q), where some low-
frequency items are removed. 
Determine the number of topics: Most topic 
models require the number of topics to be known 
beforehand1. However, it is not an easy task to 
automatically determine the exact number of se-
mantic classes an item q should belong to. Ac-
tually the number may vary for different q. Our 
solution is to set (for all items q) the topic num-
ber to be a fixed value (k=5 in our experiments) 
which is slightly larger than the number of se-
mantic classes most items could belong to. Then 
we perform postprocessing for the k topics to 
produce the final properly semantic classes. 
In summary, our approach contains three 
phases (Figure 2). We build topic models for 
every CR(q), rather than the whole collection CR. 
A preprocessing phase and a postprocessing 
phase are added before and after the topic model-
ing phase to improve efficiency and to overcome 
the fixed-k problem. The details of each phase 
are presented in the following subsections. 
 
 
Figure 2. Main phases of our approach 
 
3.2 Adopting Topic Models 
For an item q, topic modeling is adopted to 
process the RASCs in CR(q) to generate k seman-
tic classes. Here we use LDA as an example to 
                                                   
1 Although there is study of non-parametric Bayesian mod-
els (Li et al, 2007) which need no prior knowledge of topic 
number, the computational complexity seems to exceed our 
efficiency requirement and we shall leave this to future 
work. 
R580 
R1 
R2 
CR 
Item q 
Preprocessing 
?400
?  
?1
? 
?2
? 
T5 
T1 
T2 
C3 
C1 
C2 
Topic  
modeling 
Postprocessing 
T3 
T4 
CR(q) 
461
illustrate the process. The case of other genera-
tive topic models (e.g., pLSI) is very similar. 
According to the assumption of LDA and our 
concept mapping in Table 3, a RASC (?docu-
ment?) is viewed as a mixture of hidden semantic 
classes (?topics?). The generative process for a 
RASC R in the ?corpus? CR(q) is as follows, 
1) Choose a RASC size (i.e., the number of 
items in R): NR ~ Poisson(?). 
2) Choose a k-dimensional vector ??  from a 
Dirichlet distribution with parameter ?. 
3) For each of the NR items an: 
a) Pick a semantic class ??  from a mul-
tinomial distribution with parameter 
?? . 
b) Pick an item an from ?(?? |?? ,?) , 
where the item probabilities are pa-
rameterized by the matrix ?. 
There are three parameters in the model: ? (a 
scalar), ?  (a k-dimensional vector), and ?  (a 
? ? ? matrix where V is the number of distinct 
items in CR(q)). The parameter values can be ob-
tained from a training (or called parameter esti-
mation) process over CR(q), by maximizing the 
likelihood of generating the corpus. Once ?  is 
determined, we are able to compute ?(?|?,?), 
the probability of item a belonging to semantic 
class z. Therefore we can determine the members 
of a semantic class z by selecting those items 
with high ? ? ?,?  values. 
The number of topics k is assumed known and 
fixed in LDA. As has been discussed in Section 
3.1, we set a constant k value for all different 
CR(q). And we rely on the postprocessing phase 
to merge the semantic classes produced by the 
topic model to generate the ultimate semantic 
classes. 
When topic modeling is used in document 
classification, an inference procedure is required 
to determine the topics for a new document. 
Please note that inference is not needed in our 
problem. 
One natural question here is: Considering that 
in most topic modeling applications, the words 
within a resultant topic are typically semantically 
related but may not be in peer relationship, then 
what is the intuition that the resultant topics here 
are semantic classes rather than lists of generally 
related words? The magic lies in the ?docu-
ments? we used in employing topic models. 
Words co-occurred in real documents tend to be 
semantically related; while items co-occurred in 
RASCs tend to be peers. Experimental results 
show that most items in the same output seman-
tic class have peer relationship. 
It might be noteworthy to mention the exchan-
geability or ?bag-of-words? assumption in most 
topic models. Although the order of words in a 
document may be important, standard topic mod-
els neglect the order for simplicity and other rea-
sons2. The order of items in a RASC is clearly 
much weaker than the order of words in an ordi-
nary document. In some sense, topic models are 
more suitable to be used here than in processing 
an ordinary document corpus. 
3.3 Preprocessing and Postprocessing 
Preprocessing is applied to CR(q) before we build 
topic models for it. In this phase, we discard 
from all RASCs the items with frequency (i.e., 
the number of RASCs containing the item) less 
than a threshold h. A RASC itself is discarded 
from CR(q) if it contains less than two items after 
the item-removal operations. We choose to re-
move low-frequency items, because we found 
that low-frequency items are seldom important 
members of any semantic class for q. So the goal 
is to reduce the topic model training time (by 
reducing the training data) without sacrificing 
results quality too much. In the experiments sec-
tion, we compare the approaches with and with-
out preprocessing in terms of results quality and 
efficiency. Interestingly, experimental results 
show that, for some small threshold values, the 
results quality becomes higher after preprocess-
ing is performed. We will give more discussions 
in Section 4. 
In the postprocessing phase, the output seman-
tic classes (?topics?) of topic modeling are 
merged to generate the ultimate semantic classes. 
As indicated in Sections 3.1 and 3.2, we fix the 
number of topics (k=5) for different corpus CR(q) 
in employing topic models. For most items q, 
this is a larger value than the real number of se-
mantic classes the item belongs to. As a result, 
one real semantic class may be divided into mul-
tiple topics. Therefore one core operation in this 
phase is to merge those topics into one semantic 
class. In addition, the items in each semantic 
class need to be properly ordered. Thus main 
operations include, 
1) Merge semantic classes 
2) Sort the items in each semantic class 
Now we illustrate how to perform the opera-
tions. 
Merge semantic classes: The merge process 
is performed by repeatedly calculating the simi-
                                                   
2 There are topic model extensions considering word order 
in documents, such as Griffiths et al (2005). 
462
larity between two semantic classes and merging 
the two ones with the highest similarity until the 
similarity is under a threshold. One simple and 
straightforward similarity measure is the Jaccard 
coefficient, 
 ??? ?1 ,?2 =
 ?1 ? ?2 
 ?1 ? ?2 
 (3.1) 
where ?1 ? ?2  and ?1 ? ?2  are respectively the 
intersection and union of semantic classes C1 and 
C2. This formula might be over-simple, because 
the similarity between two different items is not 
exploited. So we propose the following measure, 
 ??? ?1 ,?2 =
  ???(?, ?)???2???1
 ?1 ?  ?2 
 (3.2) 
where |C| is the number of items in semantic 
class C, and sim(a,b) is the similarity between 
items a and b, which will be discussed shortly. In 
Section 4, we compare the performance of the 
above two formulas by experiments. 
Sort items: We assign an importance score to 
every item in a semantic class and sort them ac-
cording to the importance scores. Intuitively, an 
item should get a high rank if the average simi-
larity between the item and the other items in the 
semantic class is high, and if it has high similari-
ty to the query item q. Thus we calculate the im-
portance of item a in a semantic class C as 
follows, 
 ? ?|? = ? ?sim(a,C)+(1-?) ?sim(a,q) (3.3) 
where ? is a parameter in [0,1], sim(a,q) is the 
similarity between a and the query item q, and 
sim(a,C) is the similarity between a and C, calcu-
lated as, 
 ??? ?,? =
 ???(?, ?)???
 ? 
 (3.4) 
Item similarity calculation: Formulas 3.2, 
3.3, and 3.4 rely on the calculation of the similar-
ity between two items. 
One simple way of estimating item similarity 
is to count the number of RASCs containing both 
of them. We extend such an idea by distinguish-
ing the reliability of different patterns and pu-
nishing term similarity contributions from the 
same site. The resultant similarity formula is, 
 ???(?,?) = log(1 + ?(?(?? ,? ))
??
?=1
)
?
?=1
 (3.5) 
where Ci,j is a RASC containing both a and b, 
P(Ci,j) is the pattern via which the RASC is ex-
tracted, and w(P) is the weight of pattern P. As-
sume all these RASCs belong to m sites with Ci,j 
extracted from a page in site i, and ki being the 
number of RASCs corresponding to site i. To 
determine the weight of every type of pattern, we 
randomly selected 50 RASCs for each pattern 
and labeled their quality. The weight of each 
kind of pattern is then determined by the average 
quality of all labeled RASCs corresponding to it. 
The efficiency of postprocessing is not a prob-
lem, because the time cost of postprocessing is 
much less than that of the topic modeling phase. 
3.4 Discussion 
3.4.1 Efficiency of processing popular items 
Our approach receives a query item q from users 
and returns the semantic classes containing the 
query. The maximal query processing time 
should not be larger than several seconds, be-
cause users would not like to wait more time. 
Although the average query processing time of 
our approach is much shorter than 1 second (see 
Table 4 in Section 4), it takes several minutes to 
process a popular item such as ?Washington?, 
because it is contained in a lot of RASCs. In or-
der to reduce the maximal online processing 
time, our solution is offline processing popular 
items and storing the resultant semantic classes 
on disk. The time cost of offline processing is 
feasible, because we spent about 15 hours on a 4-
core machine to complete the offline processing 
for all the items in our RASC collection. 
3.4.2 Alternative approaches 
One may be able to easily think of other ap-
proaches to address our problem. Here we dis-
cuss some alternative approaches which are 
treated as our baseline in experiments. 
RASC clustering: Given a query item q, run a 
clustering algorithm over CR(q) and merge all 
RASCs in the same cluster as one semantic class. 
Formula 3.1 or 3.2 can be used to compute the 
similarity between RASCs in performing cluster-
ing. We try two clustering algorithms in experi-
ments: K-Medoids and DBSCAN. Please note k-
means cannot be utilized here because coordi-
nates are not available for RASCs. One draw-
back of RASC clustering is that it cannot deal 
with the case of one RASC containing the items 
from multiple semantic classes. 
Item clustering: By Formula 3.5, we are able 
to construct an item graph GI to record the 
neighbors (in terms of similarity) of each item. 
Given a query item q, we first retrieve its neigh-
bors from GI, and then run a clustering algorithm 
over the neighbors. As in the case of RASC clus-
tering, we try two clustering algorithms in expe-
riments: K-Medoids and DBSCAN. The primary 
disadvantage of item clustering is that it cannot 
assign an item (except for the query item q) to 
463
multiple semantic classes. As a result, when we 
input ?gold? as the query, the item ?silver? can 
only be assigned to one semantic class, although 
the term can simultaneously represents a color 
and a chemical element. 
4 Experiments 
4.1 Experimental Setup 
Datasets: By using the Open Directory Project 
(ODP3) URLs as seeds, we crawled about 40 mil-
lion English web pages in a breadth-first way. 
RASCs are extracted via applying a list of sen-
tence structure patterns and HTML tag patterns 
(see Table 1 for some examples). Our RASC col-
lection CR contains about 2.7 million unique 
RASCs and 1 million distinct items. 
Query set and labeling: We have volunteers 
to try Google Sets4, record their queries being 
used, and select overall 55 queries to form our 
query set. For each query, the results of all ap-
proaches are mixed together and labeled by fol-
lowing two steps. In the first step, the standard 
(or ideal) semantic classes (SSCs) for the query 
are manually determined. For example, the ideal 
semantic classes for item ?Georgia? may include 
Countries, and U.S. states. In the second step, 
each item is assigned a label of ?Good?, ?Fair?, 
or ?Bad? with respect to each SSC. For example, 
?silver? is labeled ?Good? with respect to ?col-
ors? and ?chemical elements?. We adopt metric 
MnDCG (Section 4.2) as our evaluation metric. 
Approaches for comparison: We compare 
our approach with the alternative approaches dis-
cussed in Section 3.4.2. 
LDA: Our approach with LDA as the topic 
model. The implementation of LDA is based 
on Blei?s code of variational EM for LDA5. 
pLSI: Our approach with pLSI as the topic 
model. The implementation of pLSI is based 
on Schein, et al (2002). 
KMedoids-RASC: The RASC clustering ap-
proach illustrated in Section 3.4.2, with the 
K-Medoids clustering algorithm utilized. 
DBSCAN-RASC: The RASC clustering ap-
proach with DBSCAN utilized. 
KMedoids-Item: The item clustering ap-
proach with the K-Medoids utilized. 
DBSCAN-Item: The item clustering ap-
proach with the DBSCAN clustering algo-
rithm utilized. 
                                                   
3 http://www.dmoz.org 
4 http://labs.google.com/sets 
5 http://www.cs.princeton.edu/~blei/lda-c/ 
K-Medoids clustering needs to predefine the 
cluster number k. We fix the k value for all dif-
ferent query item q, as has been done for the top-
ic model approach. For fair comparison, the same 
postprocessing is made for all the approaches. 
And the same preprocessing is made for all the 
approaches except for the item clustering ones 
(to which the preprocessing is not applicable). 
4.2 Evaluation Methodology 
Each produced semantic class is an ordered list 
of items. A couple of metrics in the information 
retrieval (IR) community like Precision@10, 
MAP (mean average precision), and nDCG 
(normalized discounted cumulative gain) are 
available for evaluating a single ranked list of 
items per query (Croft et al, 2009). Among the 
metrics, nDCG (Jarvelin and Kekalainen, 2000) 
can handle our three-level judgments (?Good?, 
?Fair?, and ?Bad?, refer to Section 4.1), 
 ????@? =
 ? ? /log(? + 1)??=1
 ?? ? /log(? + 1)??=1
 (4.1) 
where G(i) is the gain value assigned to the i?th 
item, and G*(i) is the gain value assigned to the 
i?th item of an ideal (or perfect) ranking list. 
Here we extend the IR metrics to the evalua-
tion of multiple ordered lists per query. We use 
nDCG as the basic metric and extend it to 
MnDCG. 
Assume labelers have determined m SSCs 
(SSC1~SSCm, refer to Section 4.1) for query q 
and the weight (or importance) of SSCi is wi. As-
sume n semantic classes are generated by an ap-
proach and n1 of them have corresponding SSCs 
(i.e., no appropriate SSC can be found for the 
remaining n-n1 semantic classes). We define the 
MnDCG score of an approach (with respect to 
query q) as, 
 ????? ? =
?1
?
?
 ?? ? ?????(SSC?)
?
i=1
 ??
m
i=1
 (4.2) 
where 
 ????? ???? =  
0                                         ?? ?? = 0
1
??
max
? ?[1, ??]
(???? ?? ,?  )  ?? ?? ? 0
  (4.3) 
In the above formula, nDCG(Gi,j) is the nDCG 
score of semantic class Gi,j; and ki denotes the 
number of semantic classes assigned to SSCi. For 
a list of queries, the MnDCG score of an algo-
rithm is the average of all scores for the queries. 
The metric is designed to properly deal with 
the following cases, 
464
i). One semantic class is wrongly split into 
multiple ones: Punished by dividing ??  in 
Formula 4.3; 
ii). A semantic class is too noisy to be as-
signed to any SSC: Processed by the 
?n1/n? in Formula 4.2; 
iii). Fewer semantic classes (than the number 
of SSCs) are produced: Punished in For-
mula 4.3 by assigning a zero value. 
iv). Wrongly merge multiple semantic 
classes into one: The nDCG score of the 
merged one will be small because it is 
computed with respect to only one single 
SSC. 
The gain values of nDCG for the three relev-
ance levels (?Bad?, ?Fair?, and ?Good?) are re-
spectively -1, 1, and 2 in experiments. 
4.3 Experimental  Results 
4.3.1 Overall performance comparison 
Figure 3 shows the performance comparison be-
tween the approaches listed in Section 4.1, using 
metrics MnDCG@n (n=1?10). Postprocessing 
is performed for all the approaches, where For-
mula 3.2 is adopted to compute the similarity 
between semantic classes. The results show that 
that the topic modeling approaches produce 
higher-quality semantic classes than the other 
approaches. It indicates that the topic mixture 
assumption of topic modeling can handle the 
multi-membership problem very well here. 
Among the alternative approaches, RASC clus-
tering behaves better than item clustering. The 
reason might be that an item cannot belong to 
multiple clusters in the two item clustering ap-
proaches, while RASC clustering allows this. For 
the RASC clustering approaches, although one 
item has the chance to belong to different seman-
tic classes, one RASC can only belong to one 
semantic class. 
 
 
Figure 3. Quality comparison (MnDCG@n) among 
approaches (frequency threshold h = 4 in preprocess-
ing; k = 5 in topic models) 
4.3.2 Preprocessing experiments 
Table 4 shows the average query processing time 
and results quality of the LDA approach, by va-
rying frequency threshold h. Similar results are 
observed for the pLSI approach. In the table, h=1 
means no preprocessing is performed. The aver-
age query processing time is calculated over all 
items in our dataset. As the threshold h increases, 
the processing time decreases as expected, be-
cause the input of topic modeling gets smaller. 
The second column lists the results quality 
(measured by MnDCG@10). Interestingly, we 
get the best results quality when h=4 (i.e., the 
items with frequency less than 4 are discarded). 
The reason may be that most low-frequency 
items are noisy ones. As a result, preprocessing 
can improve both results quality and processing 
efficiency; and h=4 seems a good choice in pre-
processing for our dataset. 
 
h 
Avg. Query Proc. 
Time (seconds) 
Quality 
(MnDCG@10) 
1 0.414 0.281 
2 0.375 0.294 
3 0.320 0.322 
4 0.268 0.331 
5 0.232 0.328 
6 0.210 0.315 
7 0.197 0.315 
8 0.184 0.313 
9 0.173 0.288 
Table 4. Time complexity and quality comparison 
among LDA approaches of different thresholds 
 
4.3.3 Postprocessing experiments 
 
Figure 4. Results quality comparison among topic 
modeling approaches with and without postprocessing 
(metric: MnDCG@10) 
 
The effect of postprocessing is shown in Figure 
4. In the figure, NP means no postprocessing is 
performed. Sim1 and Sim2 respectively mean 
Formula 3.1 and Formula 3.2 are used in post-
processing as the similarity measure between 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 2 3 4 5 6 7 8 9 10
pLSI LDA KMedoids-RASC
DBSCAN-RASC KMedoids-Item DBSCAN-Item
n
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
LDA pLSI
NP
Sim1 
Sim2
465
semantic classes. The same preprocessing (h=4) 
is performed in generating the data. It can be 
seen that postprocessing improves results quality. 
Sim2 achieves more performance improvement 
than Sim1, which demonstrates the effectiveness 
of the similarity measure in Formula 3.2. 
4.3.4 Sample results 
Table 5 shows the semantic classes generated by 
our LDA approach for some sample queries in 
which the bad classes or bad members are hig-
hlighted (to save space, 10 items are listed here, 
and the query itself is omitted in the resultant 
semantic classes).  
 
Query Semantic Classes 
apple 
C1: ibm, microsoft, sony, dell, toshiba,  sam-
sung, panasonic, canon, nec, sharp ? 
C2: peach, strawberry, cherry, orange, bana-
na, lemon, pineapple, raspberry, pear, grape 
? 
gold 
C1: silver, copper, platinum, zinc, lead, iron, 
nickel, tin, aluminum, manganese ? 
C2: silver, red, black, white, blue, purple, 
orange, pink, brown, navy ? 
C3: silver, platinum, earrings, diamonds, 
rings, bracelets, necklaces, pendants, jewelry, 
watches ? 
C4: silver, home, money, business, metal, 
furniture, shoes, gypsum, hematite, fluorite 
?  
lincoln 
C1: ford, mazda, toyota, dodge, nissan, hon-
da, bmw, chrysler, mitsubishi, audi ? 
C2: bristol, manchester, birmingham, leeds, 
london, cardiff, nottingham, newcastle, shef-
field, southampton ? 
C3: jefferson, jackson, washington, madison, 
franklin, sacramento, new york city, monroe, 
Louisville, marion ? 
computer 
science 
C1: chemistry, mathematics, physics, biolo-
gy, psychology, education, history, music, 
business, economics ? 
Table 5. Semantic classes generated by our approach 
for some sample queries (topic model = LDA) 
 
5 Related Work 
Several categories of work are related to ours. 
The first category is about set expansion (i.e., 
retrieving one semantic class given one term or a 
couple of terms). Syntactic context information is 
used (Hindle, 1990; Ruge, 1992; Lin, 1998) to 
compute term similarities, based on which simi-
lar words to a particular word can directly be 
returned. Google sets is an online service which, 
given one to five items, predicts other items in 
the set. Ghahramani and Heller (2005) introduce 
a Bayesian Sets algorithm for set expansion. Set 
expansion is performed by feeding queries to 
web search engines in Wang and Cohen (2007) 
and Kozareva (2008). All of the above work only 
yields one semantic class for a given query. 
Second, there are pattern-based approaches in the 
literature which only do limited integration of 
RASCs (Shinzato and Torisawa, 2004; Shinzato 
and Torisawa, 2005; Pasca, 2004), as discussed 
in the introduction section. In Shi et al (2008), 
an ad-hoc approach was proposed to discover the 
multiple semantic classes for one item. The third 
category is distributional similarity approaches 
which provide multi-membership support (Har-
ris, 1985; Lin  and Pantel, 2001; Pantel and Lin, 
2002). Among them, the CBC algorithm (Pantel 
and Lin, 2002) addresses the multi-membership 
problem. But it relies on term vectors and centro-
ids which are not available in pattern-based ap-
proaches. It is therefore not clear whether it can 
be borrowed to deal with multi-membership here. 
Among the various applications of topic 
modeling, maybe the efforts of using topic model 
for Word Sense Disambiguation (WSD) are most 
relevant to our work. In Cai et al(2007), LDA is 
utilized to capture the global context information 
as the topic features for better performing the 
WSD task. In Boyd-Graber et al (2007), Latent 
Dirichlet with WordNet (LDAWN) is developed 
for simultaneously disambiguating a corpus and 
learning the domains in which to consider each 
word. They do not generate semantic classes. 
6 Conclusions 
We presented an approach that employs topic 
modeling for semantic class construction. Given 
an item q, we first retrieve all RASCs containing 
the item to form a collection CR(q). Then we per-
form some preprocessing to CR(q) and build a 
topic model for it. Finally, the output semantic 
classes of topic modeling are post-processed to 
generate the final semantic classes. For the CR(q) 
which contains a lot of RASCs, we perform of-
fline processing according to the above process 
and store the results on disk, in order to reduce 
the online query processing time. 
We also proposed an evaluation methodology 
for measuring the quality of semantic classes. 
We show by experiments that our topic modeling 
approach outperforms the item clustering and 
RASC clustering approaches. 
 
Acknowledgments 
We wish to acknowledge help from Xiaokang 
Liu for mining RASCs from web pages, Chan-
gliang Wang and Zhongkai Fu for data process.  
  
466
References  
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res., 3:993?1022. 
Bruce Croft, Donald Metzler, and Trevor Strohman. 
2009. Search Engines: Information Retrieval in 
Practice. Addison Wesley.  
Jordan Boyd-Graber, David Blei, and Xiaojin 
Zhu.2007. A topic model for word sense disambig-
uation. In Proceedings EMNLP-CoNLL 2007, pag-
es 1024?1033, Prague, Czech Republic, June. 
Association for Computational Linguistics. 
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. 
NUS-ML: Improving word sense disambiguation 
using topic features. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, volume 
4. 
Scott Deerwester, Susan T. Dumais, GeorgeW. Fur-
nas, Thomas K. Landauer, and Richard Harshman. 
1990. Indexing by latent semantic analysis. Journal 
of the American Society for Information Science, 
41:391?407. 
Zoubin Ghahramani and Katherine A. Heller. 2005. 
Bayesian Sets. In Advances in Neural Information 
Processing Systems (NIPS05). 
Thomas L. Griffiths, Mark Steyvers, David M. 
Blei,and Joshua B. Tenenbaum. 2005. Integrating 
topics and syntax. In Advances in Neural Informa-
tion Processing Systems 17, pages 537?544. MIT 
Press 
Zellig Harris. Distributional Structure. The Philoso-
phy of Linguistics. New York: Oxford University 
Press. 1985. 
Donald Hindle. 1990. Noun Classification from Pre-
dicate-Argument Structures. In Proceedings of 
ACL90, pages 268?275.  
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR99, pages 50?57, New York, 
NY, USA. ACM. 
Kalervo Jarvelin, and Jaana Kekalainen. 2000. IR 
Evaluation Methods for Retrieving Highly Rele-
vant Documents. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on 
Research and Development in Information Retriev-
al (SIGIR2000). 
Zornitsa Kozareva, Ellen Riloff and Eduard Hovy. 
2008. Semantic Class Learning from the Web with 
Hyponym Pattern Linkage Graphs, In Proceedings 
of ACL-08. 
Wei Li, David M. Blei, and Andrew McCallum. Non-
parametric Bayes Pachinko Allocation. In Proceed-
ings of Conference on Uncertainty in Artificial In-
telligence (UAI), 2007. 
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of COLING-
ACL98, pages 768-774. 
Dekang Lin and Patrick Pantel. 2001. Induction of 
Semantic Classes from Natural Language Text. In 
Proceedings of SIGKDD01, pages 317-322.  
Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tana-
ka. 2006. Searching coordinate terms with their 
context from the web. In WISE06, pages 40?47. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text. In Proceedings of 
SIGKDD02.  
Marius Pasca. 2004. Acquisition of Categorized 
Named Entities for Web Search. In Proc. of 2004 
CIKM.  
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. In Information 
Processing & Management, 28(3), pages 317-32. 
Andrew I. Schein,  Alexandrin Popescul,  Lyle H. 
Ungar and David M. Pennock. 2002. Methods and 
metrics for cold-start recommendations. In Pro-
ceedings of SIGIR02, pages  253-260. 
Shuming Shi, Xiaokang Liu and Ji-Rong Wen. 2008. 
Pattern-based Semantic Class Discovery with Mul-
ti-Membership Support. In CIKM2008, pages 
1453-1454.  
Keiji Shinzato and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In 
HLT/NAACL04, pages 73?80. 
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple 
WWW-based Method for Semantic Word Class 
Acquisition. In RANLP05.  
Richard C. Wang and William W. Cohen. 2007. Lan-
gusage-Independent Set Expansion of Named Enti-
ties Using the Web. In ICDM2007. 
 
467
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Anchor Text Extraction for Academic Search 
 
 
Shuming Shi1     Fei Xing2*     Mingjie Zhu3*     Zaiqing Nie1     Ji-Rong Wen1 
1Microsoft Research Asia 
2Alibaba Group, China 
3University of Science and Technology of China 
{shumings, znie, jrwen}@microsoft.com 
fei_c_xing@yahoo.com; mjzhu@ustc.edu 
 
 
 
Abstract* 
 
Anchor text plays a special important role in 
improving the performance of general Web 
search, due to the fact that it is relatively ob-
jective description for a Web page by poten-
tially a large number of other Web pages. 
Academic Search provides indexing and 
search functionality for academic articles. It 
may be desirable to utilize anchor text in aca-
demic search as well to improve the search re-
sults quality. The main challenge here is that 
no explicit URLs and anchor text is available 
for academic articles. In this paper we define 
and automatically assign a pseudo-URL for 
each academic article. And a machine learning 
approach is adopted to extract pseudo-anchor 
text for academic articles, by exploiting the ci-
tation relationship between them. The ex-
tracted pseudo-anchor text is then indexed and 
involved in the relevance score computation of 
academic articles. Experiments conducted on 
0.9 million research papers show that our ap-
proach is able to dramatically improve search 
performance. 
1 Introduction 
Anchor text is a piece of clickable text that links 
to a target Web page. In general Web search, 
anchor text plays an extremely important role in 
improving the search quality. The main reason 
for this is that anchor text actually aggregates the 
opinion (which is more comprehensive, accurate, 
and objective) of a potentially large number of 
people for a Web page. 
                                                 
* This work was performed when Fei Xing and Mingjie Zhu 
were interns at Microsoft Research Asia. 
In recent years, academic search (Giles et al, 
1998; Lawrence et al, 1999; Nie et al, 2005; 
Chakrabarti et al, 2006) has become an impor-
tant supplement to general web search for re-
trieving research articles. Several academic 
search systems (including Google Scholar?, Cite-
seer?, DBLP?, Libra**, ArnetMiner??, etc.) have 
been deployed. In order to improve the results 
quality of an academic search system, we may 
consider exploiting the techniques which are 
demonstrated to be quite useful and critical in 
general Web search. In this paper, we study the 
possibility of extracting anchor text for research 
papers and using them to improve the search per-
formance of an academic search system. 
 
 
Figure 1. An example of one paper citing other papers 
 
The basic search unit in most academic search 
systems is a research paper. Borrowing the con-
cepts of URL and anchor-text in general Web 
search, we may need to assign a pseudo-URL for 
one research paper as its identifier and to define 
the pseudo-anchor text for it by the contextual 
description when this paper is referenced (or 
mentioned). The pseudo-URL of a research pa-
per could be the combination of its title, authors 
and publication information. Figure-1 shows an 
excerpt where one paper cites a couple of other 
                                                 
? http://scholar.google.com/ 
? http://citeseerx.ist.psu.edu/ 
? http://www.informatik.uni-trier.de/~ley/db/ 
** http://libra.msra.cn/ 
?? http://www.arnetminer.org/ 
10
papers. The grayed text can be treated as the 
pseudo-anchor text of the papers being refe-
renced. Once the pseudo-anchor text of research 
papers is acquired, it can be indexed and utilized 
to help ranking, just as in general web search. 
However it remains a challenging task to cor-
rectly identify and extract these pseudo-URLs 
and pseudo-anchor texts. First, unlike the situa-
tion in general web search where one unique 
URL is assigned to each web page as a natural 
identifier, the information of research papers 
need to be extracted from web pages or PDF files. 
As a result, in constructing pseudo-URLs for 
research papers, we may face the problem of ex-
traction errors, typos, and the case of one re-
search paper having different expressions in dif-
ferent places. Second, in general Web search, 
anchor text is always explicitly specified by 
HTML tags (<a> and </a>). It is however much 
harder to perform anchor text extraction for re-
search papers. For example, human knowledge 
may be required in Figure-1 to accurately identi-
fy the description of every cited paper. 
To address the above challenges, we propose 
an approach for extracting and utilizing pseudo-
anchor text information in academic search to 
improve the search results quality. Our approach 
is composed of three phases. In the first phase, 
each time a paper is cited in another paper, we 
construct a tentative pseudo-URL for the cited 
paper and extract a candidate anchor block for it. 
The tentative pseudo-URL and the candidate 
anchor block are allowed to be inaccurate. In the 
second phase, we merge the tentative pseudo-
URLs that should represent the same paper. All 
candidate anchor blocks belong to the same pa-
per are grouped accordingly in this phase. In the 
third phase, the final pseudo-anchor text of each 
paper is generated from all its candidate blocks, 
by adopting a SVM-based machine learning me-
thodology. We conduct experiments upon a data-
set containing 0.9 million research papers. The 
experimental results show that lots of useful anc-
hor text can be successfully extracted and accu-
mulated using our approach, and the ultimate 
search performance is dramatically improved 
when anchor information is indexed and used for 
paper ranking. 
The remaining part of this paper is organized 
as follows. In Section 2, we describe in detail our 
approach for pseudo-anchor text extraction and 
accumulation. Experimental results are reported 
in Section 3. We discuss related work in Section 
4 and finally conclude the paper in Section 5. 
2 Our Approach 
2.1 Overview 
Before describing our approach in detail, we first 
recall how anchor text is processed in general 
Web search. Assume that there have been a col-
lection of documents being crawled and stored 
on local disk. In the first step, each web page is 
parsed and the out links (or forward links) within 
the page are extracted. Each link is comprised of 
a URL and its corresponding anchor text. In the 
second step, all links are accumulated according 
to their destination URLs (i.e. the anchor texts of 
all links pointed to the same URL are merged). 
Thus, we can get al anchor text corresponding to 
each web page. Figure-2 (a) demonstrates this 
process. 
 
 
Figure 2. The main process of extracting (a) anchor 
text in general web search and (b) pseudo-anchor text 
in academic search 
 
For academic search, we need to extract and 
parse the text content of papers. When a paper A 
mentions another paper B, it either explicitly or 
implicitly displays the key information of B to let 
the users know that it is referencing B instead of 
other papers. Such information can be extracted 
to construct the tentative pseudo-URL of B. The 
pseudo-URLs constructed in this phase are tenta-
tive because different tentative pseudo-URLs 
may be merged to generate the same final pseu-
do-URL. All information related to paper B in 
different papers can be accumulated and treated 
Web pages 
HTML parsing 
Links 
Anchor text 
for pages 
 
Group by link 
destination 
Papers 
Paper parsing 
Tentative pseudo-URLs 
Candidate anchor blocks 
Anchor block accumulation 
Papers with their  
candidate anchor blocks 
Papers with their  
pseudo-anchor text 
Anchor-text learning 
11
as the potential anchor text of B. Our goal is to 
get the anchor text related to each paper. 
Our approach for pseudo-anchor text extrac-
tion is shown in Figure-2 (b). The key process is 
similar to that in general Web search for accumu-
lating and utilizing page anchor text. One prima-
ry difference between Figure-2 (a) and (b) is the 
latter accumulates candidate anchor blocks rather 
than pieces of anchor text. A candidate anchor 
block is a piece of text that contains the descrip-
tion of one paper. The basic idea is: Instead of 
extracting the anchor text for a paper directly (a 
difficult task because of the lack of enough in-
formation), we first construct a candidate anchor 
block to contain the "possible" or "potential" de-
scription of the paper. After we accumulate all 
candidate anchor blocks, we have more informa-
tion to provide a better estimation about which 
pieces of texts are anchor texts. Following this 
idea, our proposed approach adopts a three-phase 
methodology to extract pseudo-anchor text. In 
the first phase, each time a paper B appearing in 
another paper A, a candidate anchor block is ex-
tracted for B. All candidate anchor blocks belong 
to the same paper are grouped in the second 
phase. In the third phase, the final pseudo-anchor 
text of each paper is selected among all candidate 
blocks. 
Extracting tentative pseudo-URLs and can-
didate anchor blocks: When one paper cites 
another paper, a piece of short text (e.g. "[1]" or 
?(xxx et al, 2008)?) is commonly inserted to 
represent the paper to be cited, and the detail in-
formation (key attributes) of it are typically put 
at the end of the document (in the references sec-
tion). We call each paper listed in the references 
section a reference item. The references section 
can be located by searching for the last occur-
rence of term 'reference' or 'references' in larger 
fonts. Then, we adopt a rule-based approach to 
divide the text in the references section into ref-
erence items. Another rule-based approach is 
used to extract paper attributes (title, authors, 
year, etc) from a reference item. We observed 
some errors in our resulting pseudo-URLs caused 
by the quality of HTML files converted from 
PDF format, reference item extraction errors, 
paper attribute extraction errors, and other fac-
tors. We also observed different reference item 
formats for the same paper. The pseudo-URL for 
a paper is defined according to its title, authors, 
publisher, and publication year, because these 
four kinds of information can readily be used to 
identify a paper. 
For each citation of a paper, we treat the sen-
tence containing the reference point (or citation 
point) as one candidate anchor block. When mul-
tiple papers are cited in one sentence, we treat 
the sentence as the candidate anchor block of 
every destination paper. 
Candidate Anchor Block Accumulation: 
This phase is in charge of merging all candidate 
blocks of the same pseudo-URL. As has been 
discussed, tentative pseudo-URLs are often inac-
curate; and different tentative pseudo-URLs may 
correspond to the same paper. The primary chal-
lenge here is perform the task in an efficient way 
and with high accuracy. We will address this 
problem in Subsection 2.2. 
Pseudo-Anchor Generation: In the previous 
phase, all candidate blocks of each paper have 
been accumulated. This phase is to generate the 
final anchor text for each paper from all its can-
didate blocks. Please refer to Subsection 2.3 for 
details. 
2.2 Candidate Anchor Block Accumulation 
via Multiple Feature-String Hashing 
Consider this problem: Given a potentially huge 
number of tentative pseudo-URLs for papers, we 
need to identify and merge the tentative pseudo-
URLs that represent the same paper. This is like 
the problems in the record linkage (Fellegi and 
Sunter, 1969), entity matching, and data integra-
tion which have been extensively studied in da-
tabase, AI, and other areas. In this sub-section, 
we will first show the major challenges and the 
previous similar work on this kind of problem. 
Then a possible approach is described to achieve 
a trade-off between accuracy and efficiency. 
 
 
Figure 3. Two tentative pseudo-URLs representing 
the same paper 
 
2.2.1 Challenges and candidate techniques 
Two issues should be addressed for this problem: 
similarity measurement, and the efficiency of the 
algorithm. On one hand, a proper similarity func-
tion is needed to identify two tentative pseudo-
URLs representing the same paper. Second, the 
12
integration process has to be accomplished effi-
ciently. 
We choose to compute the similarity between 
two papers to be a linear combination of the si-
milarities on the following fields: title, authors, 
venue (conference/journal name), and year. The 
similarity function on each field is carefully de-
signed. For paper title, we adopt a term-level edit 
distance to compute similarity. And for paper 
authors, person name abbreviation is considered. 
The similarity function we adopted is fairly well 
in accuracy (e.g., the similarity between the two 
pseudo-URLs in Figure-3 is high according to 
our function); but it is quite time-consuming to 
compute the similarity for each pair of papers 
(roughly 1012 similarity computation operations 
are needed for 1 million different tentative pseu-
do-URLs). 
Some existing methods are available for de-
creasing the times of similarity calculation opera-
tions. McCallum et al (2000) addresses this high 
dimensional data clustering problem by dividing 
data into overlapping subsets called canopies 
according to a cheap, approximate distance mea-
surement. Then the clustering process is per-
formed by measuring the exact distances only 
between objects from the same canopy. There are 
also other subspace methods (Parsons et al, 2004) 
in data clustering areas, where data are divided 
into subspaces of high dimensional spaces first 
and then processing is done in these subspaces. 
Also there are fast blocking approaches for 
record linkage in Baxter et al (2003). Though 
they may have different names, they hold similar 
ideas of dividing data into subsets to reduce the 
candidate comparison records. The size of data-
set used in the above papers is typically quite 
small (about thousands of data items). For effi-
ciency issue, Broder et al (1997) proposed a 
shingling approach to detect similar Web pages. 
They noticed that it is infeasible to compare 
sketches (which are generated by shingling) of 
all pairs of documents. So they built an inverted 
index that contains a list of shingle values and 
the documents they appearing in. With the in-
verted index, they can effectively generate a list 
of all the pairs of documents that share any shin-
gles, along with the number of shingles they 
have in common. They did experiments on a da-
taset containing 30 million documents. 
By adopting the main ideas of the above tech-
niques to our pseudo-URL matching problem, a 
possible approach can be as follows. 
 
 
Figure 4. The Multiple Feature-String Hashing algo-
rithm for candidate anchor block accumulation 
 
2.2.2 Method adopted 
The method utilized here for candidate anchor 
block accumulation is shown in Figure 4. The 
main idea is to construct a certain number of fea-
ture strings for a tentative pseudo-URL (abbre-
viated as TP-URL) and do hash for the feature 
strings. A feature string of a paper is a small 
piece of text which records a part of the paper?s 
key information, satisfying the following condi-
tions: First, multiple feature strings can typically 
be built from a TP-URL. Second, if two TP-
URLs are different representations of the same 
paper, then the probability that they have at least 
one common feature string is extremely high. We 
can choose the term-level n-grams of paper titles 
(referring to Section 3.4) as feature strings. 
The algorithm maintains an in-memory hash-
table which contains a lot of slots each of which 
is a list of TP-URLs belonging to this slot. For 
each TP-URL, feature strings are generated and 
hashed by a specified hash function. The TP-
URL is then added into some slots according to 
the hash values of its feature strings. Any two 
TP-URLs belonging to the same slot are further 
compared by utilizing our similarity function. If 
their similarity is larger than a threshold, the two 
TP-URLs are treated as being the same and 
therefore their corresponding candidate anchor 
blocks are merged. 
The above algorithm tries to achieve good bal-
ance between accuracy and performance. On one 
hand, compared with the na?ve algorithm of per-
forming one-one comparison between all pairs of 
TP-URLs, the algorithm needs only to compute 
Algorithm Multiple Feature-String Hashing for candidate anchor 
block accumulation 
Input: A list of papers (with their tentative pseudo-URLs 
and candidate anchor blocks) 
Output: Papers with all candidate anchor blocks of the 
same paper aggregated 
 
Initial: An empty hashtable h (each slot of h is a list of pa-
pers) 
For each paper A in the input list { 
For each feature-string of A { 
Lookup by the feature-string in h to get a slot s; 
Add A into s; 
} 
} 
For each slot s with size smaller than a threshold { 
For any two papers A1, A2 in s { 
float fSim = Similarity(A1, A2); 
if(fSim > the specified threshold) { 
Merge A1 and A2; 
} 
} 
} 
13
the similarity for the TP-URLs that share a 
common slot. On the other hand, because of the 
special property of feature strings, most TP-
URLs representing the same paper can be de-
tected and merged. 
The basic idea of dividing data into over-
lapped subsets is inherited from McCallum et al 
(2000), Broder et al (1997), and some subspace 
clustering approaches. Slightly different, we do 
not count the number of common feature strings 
between TP-URLs. Common bins (or inverted 
indices) between data points are calculated in 
McCallum et al (2000) as a ?cheap distance? for 
creating canopies. The number of common Shin-
gles between two Web documents is calculated 
(efficiently via inverted indices), such that Jac-
card similarity could be used to measure the si-
milarity between them. In our case, we simply 
compare any two TP-URLs in the same slot by 
using our similarity function directly. 
The effective and efficiency of this algorithm 
depend on the selection of feature strings. For a 
fixed feature string generation method, the per-
formance of this algorithm is affected by the size 
of each slot, especially the number and size of 
big slots (slots with size larger than a threshold). 
Big slots will be discarded in the algorithm to 
improve performance, just like removing com-
mon Shingles in Broder et al (1997). In Section 
4, we conduct experiments to test the perfor-
mance of the above algorithm with different fea-
ture string functions and different slot size thre-
sholds. 
2.3 Pseudo-Anchor Text Learning 
In this subsection, we address the problem of 
extracting the final pseudo-anchor text for a pa-
per, given all its candidate anchor blocks (see 
Figure 5 for an example). 
2.3.1 Problem definition 
A candidate anchor block is a piece of text with 
one or some reference points (a reference point is 
one occurrence of citation in a paper) specified, 
where a reference point is denoted by a 
<start_pos, end_pos> pair (means start position 
and end position respectively): ref = <start_pos, 
end_pos>. We represent a candidate anchor 
block to be the following format, 
AnchorBlock = (Text, ref1, ref2, ?) 
We define a block set to be a set of candidate 
anchor blocks for a paper, 
BlockSet = {AnchorBlock1, AnchorBlock2, ?} 
Now the problem is: Given a block set con-
taining N elements, extract some text excerpts 
from them as the anchor text of the paper. 
2.3.2 Learn term weights 
We adopt a machine-learning approach to assign, 
for each term in the anchor blocks, a discrete de-
gree of being anchor text. The main reasons for 
taking such an approach is twofold: First, we 
believe that assigning each term a fuzzy degree 
of being anchor text is more appropriate than a 
binary judgment as either an anchor-term or non-
anchor-term. Second, since the importance of a 
term for a ?link? may be determined by many 
factors in paper search, a machine-learning could 
be more flexible and general than the approaches 
that compute term degrees by a specially de-
signed formula. 
 
 
Figure 5. The candidate pseudo-anchor blocks of a 
paper 
 
The features used for learning are listed in Ta-
ble-1. 
We observed that it would be more effective if 
some of the above features are normalized before 
being used for learning. For a term in candidate 
anchor block B, its TF are normalized by the 
BM25 formula (Robertson et al, 1999), 
 
TFL
Bbbk
TFkTFnorm ?????
???
)||)1((
)1(
1
1
 
 
where L is average length of the candidate blocks, 
|B| is the length of B, and k1, b are parameters. 
DF is normalized by the following formula, 
 )1log( DF
NIDF ??
  
where N is the number of elements in the block 
set (i.e. total number of candidate anchor blocks 
for the current paper). 
Features RefPos and Dist are normalized as, 
 
RefPosnorm = RefPos / |B| 
Distnorm = (Dist-RefPos) / |B| 
 
And the feature BlockLen is normalized as, 
14
 BlockLennorm = log(1+BlockLen)  
 
Features Description 
DF 
Document frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of all papers. It is used to indicate whether the 
term is a stop word or not. 
BF 
Block frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of this paper. 
CTF 
Collection term frequency: Total number of times the 
term appearing in the blocks. For multiple times of 
occurrences in one block, all of them are counted. 
IsInURL 
Specify whether the term appears in the pseudo-URL 
of the paper. 
TF 
Term frequency: Number of times the terms appearing 
in the candidate block. 
Dist 
Directed distance from the nearest reference point to 
the term location 
RefPos 
Position of the nearest reference point in the candidate 
pseudo-anchor block. 
BlockLen Length of the candidate pseudo-anchor block 
Table 1. Features for learning 
 
We set four term importance levels, from 1 
(unrelated terms or stop words) to 4 (words par-
ticipating in describing the main ideas of the pa-
per). 
We choose support vector machine (SVM) for 
learning term weights here, because of its power-
ful classification ability and well generalization 
ability (Burges, 1998). We believe some other 
machine learning techniques should also work 
here. The input of the classifier is a feature vec-
tor of a term and the output is the importance 
level of the term. Given a set of training data 
? ?liii levelfeature 1, ?, a decision function f(x) can be 
acquired after training. Using the decision func-
tion, we can assign an importance level for each 
term automatically. 
 
3 Experiments 
3.1 Experimental Setup 
Our experimental dataset contains 0.9 million 
papers crawled from the web. All the papers are 
processed according to the process in Figure-2 
(b). We randomly select 300 queries from the 
query log of Libra (libra.msra.cn) and retrieve 
the results in our indexing and ranking system 
with/without the pseudo-anchors generated by 
our approach. Then the volunteer researchers and 
students in our group are involved to judge the 
search results. The top 30 results of different 
ranking algorithms for each query are labeled 
and assigned a relevance value from 1 (meaning 
'poor match') to 5 (meaning 'perfect match'). The 
search results quality is measured by NDCG 
(Jarvelin and Kekalainen, 2000). 
3.2 Overall Effect of our Approach 
Figure 6 shows the performance comparison be-
tween the results of two baseline paper ranking 
algorithms and the results of including pseudo-
anchor text in ranking. 
 
0.466
0.426
0.388
0.597
0.619
0.689
0.673 0.672
0.627
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
NDCG@1 NDCG@3 NDCG@10
Base(Without CitationCount)
Base
Pseudo-Anchor Included
 
Figure 6. Comparison between the baseline approach 
and our approach (measure: nDCG) 
 
The ?Base? algorithm considers the title, ab-
stract, full-text and static-rank (which is a func-
tion of the citation count) of a paper. In a bit 
more detail, for each paper, we adopt the BM25 
formula (Robertson et al, 1999) over its title, 
abstract, and full-text respectively. And then the 
resulting score is linearly combined with the stat-
ic-rank to get its final score. The static-rank is 
computed as follows, 
 StaticRank = log(1+CitationCount) (3.1) 
To test the performance of including pseudo-
anchor text in ranking, we compute an anchor 
score for each paper and linearly combine it with 
its baseline score (i.e. the score computed by the 
baseline algorithm). 
We tried two kinds of ways for anchor score 
computation. The first is to merge all pieces of 
anchor excerpts (extracted in the previous section) 
into a larger piece of anchor text, and use BM25 
to compute its relevance score. In another ap-
proach called homogeneous evidence combina-
tion (Shi et al, 2006), a relevance score is com-
puted for each anchor excerpt (still using BM25), 
and all the scores for the excerpts are sorted des-
cending and then combined by the following 
formula, 
 ?
?
?????
m
i
ianchor sicS 1 2))1(1(
1 (3.2) 
where si (i=1, ?, m) are scores for the m anchor 
excerpts, and c is a parameter. The primary idea 
15
here is to let larger scores to have relative greater 
weights. Please refer to Shi et al (2006) for a 
justification of this approach. As we get slightly 
better results with the latter way, we use it as our 
final choice for computing anchor scores. 
From Figure 6, we can see that the overall per-
formance is greatly improved by including pseu-
do-anchor information. Table 2 shows the t-test 
results, where a ?>? indicates that the algorithm 
in the row outperforms that in the column with a 
p-value of 0.05 or less, and a ?>>? means a p-
value of 0.01 or less. 
 
 
 
Base 
Base (without 
CitationCount) 
Our approach > >> 
Base  >> 
Base (without Cita-
tionCount) 
  
Table 2. Statistical significance tests (t-test over 
nDCG@3) 
 
Table 3 shows the performance comparison by 
using some traditional IR measures based on bi-
nary judgments. Since the results of not includ-
ing CitationCount are much worse than the other 
two, we omit it in the table. 
 
Measure 
Approach 
MAP MRR P@1 P@10 
Base (including 
CitationCount) 
0.364 0.727 0.613 0.501 
Our Approach 0.381 0.734 0.625 0.531 
Table 3. Performance compassion using binary judg-
ment measures 
 
3.3 Sample Query Analysis 
Here we analyze some sample queries to get 
some insights about why and how pseudo-anchor 
improves search performance. Figure-7 and Fig-
ure-8 show the top-3 results of two sample que-
ries: {TF-IDF} and {Page Rank}. 
For query "TF-IDF", the top results of the 
baseline approach have keyword "TF-IDF" ap-
peared in the title as well as in other places of the 
papers. Although the returned papers are relevant 
to the query, they are not excellent because typi-
cally users may want to get the first TF-IDF pa-
per or some papers introducing TF-IDF. When 
pseudo-anchor information is involved, some 
excellent results (B1, B2, B3) are generated. The 
main reason for getting the improved results is 
that these papers (or books) are described with 
"TF-IDF" when lots of other papers cite them. 
 
 
Figure 7. Top-3 results for query TF-IDF 
 
 
Figure 8. Top-3 results for query Page Rank 
 
Figure-8 shows another example about how 
pseudo-anchor helps to improve search results 
quality. For query "Page Rank" (note that there is 
a space in between), the results returned by the 
baseline approach are not satisfactory. In the pa-
pers returned by our approach, at least B1 and B2 
are very good results. Although they did not la-
bel themselves "Page Rank", other papers do so 
in citing them. Interestingly, although the result 
B3 is not about the "PageRank" algorithm, it de-
scribes another popular "Page Rank" algorithm 
in addition to PageRank. 
Another interesting observation from the two 
figures is that our approach retrieves older papers 
than the baseline method, because old papers 
tend to have more anchor text (due to more cita-
tions). So our approach may not be suitable for 
retrieve newer papers. To overcome this problem, 
maybe publication year should be considered in 
our ranking functions. 
3.4 Anchor Accumulation Experiments 
We conduct experiments to test the effectiveness 
and efficiency of the multiple-feature-string-
hashing algorithm presented in Section 2.2. The 
duplication detection quality of this algorithm is 
determined by the appropriate selection of fea-
A1. V Safronov, M Parashar, Y Wang et al Optimizing Web servers 
using Page rank prefetching for clustered accesses. Information 
Sciences. 2003. 
A2. AO Mendelzon, D Rafiei. An autonomous page ranking method for 
metasearch engines. WWW, 2002. 
A3. FB Kalhoff. On formally real Division Algebras and Quasifields of 
Rank two. 
(a) Without anchor 
B1. S Brin, L Page. The Anatomy of a Large-Scale Hypertextual Web 
Search Engine. WWW, 1998 
B2. L Page, S Brin, R Motwani, T Winograd. The pagerank citation 
ranking: Bringing order to the web. 1998. 
B3. JM Kleinberg. Authoritative sources in a hyperlinked environment. 
Journal of the ACM, 1999. 
(b) With anchor 
 
A1. K Sugiyama, K Hatano, M Yoshikawa, S Uemura. Refinement of TF-
IDF schemes for web pages using their hyperlinked neighboring pages. 
Hypertext?03 
A2. A Aizawa. An information-theoretic perspective of tf-idf measures. 
IPM?03. 
A3. N Oren. Reexamining tf.idf based information retrieval with Genet-
ic Programming. SAICSIT?02. 
(a) Without anchor 
B1. G Salton, MJ McGill. Introduction to Modern Information Retriev-
al. McGraw-Hill, 1983. 
B2. G Salton and C Buckley. Term weighting approaches in automatic 
text retrieval. IPM?98. 
B3. R Baeza-Yates, B Ribeiro-Neto. Modern Information Retrieval. 
Addison-Wesley, 1999 
(b) With anchor 
 
16
ture strings. When feature strings are fixed, the 
slot size threshold can be used to tune the tra-
deoff between accuracy and performance. 
 
Feature Strings 
Slot Distr. 
Ungram Bigram Trigram 4-gram 
# of Slots 1.4*105 1.2*106 2.8*106 3.4*106 
# of Slots with 
size > 100 
5240 6806 1541 253 
# of Slots with 
size > 1000 
998 363 50 5 
# of Slots with 
size > 10000 
59 11 0 0 
Table 4. Slot distribution with different feature strings 
 
We take all the papers extracted from PDF 
files as input to run the algorithm. Identical TP-
URLs are first eliminated (therefore their candi-
date anchor blocks are merged) by utilizing a 
hash table. This pre-process step results in about 
1.46 million distinct TP-URLs. The number is 
larger than our collection size (0.9 million), be-
cause some cited papers are not in our paper col-
lection. We tested four kinds of feature strings all 
of which are generated from paper title: uni-
grams, bigrams, trigrams, and 4-grams. Table-4 
shows the slot size distribution corresponding to 
each kind of feature strings. The performance 
comparison among different feature strings and 
slot size thresholds is shown in Table 5. It seems 
that bigrams achieve a good trade-off between 
accuracy and performance. 
 
Feature 
Strings 
Slot Size 
Threshold 
Dup. papers 
Detected 
Processing 
Time (sec) 
Unigram 
5000 529,717  119,739.0  
500 327,357 7,552.7  
Bigram 500 528,981 8,229.6  
Trigram 
Infinite 518,564 8,420.4  
500 516,369 2,654.9  
4-gram 500 482,299 1,138.2  
Table 5. Performance comparison between different 
feature strings and slot size thresholds 
 
4 Related Work 
There has been some work which uses anchor 
text or their surrounding text for various Web 
information retrieval tasks. It was known at the 
very beginning era of internet that anchor text 
was useful to Web search (McBryan, 1994). 
Most Web search engines now use anchor text as 
primary and power evidence for improving 
search performance. The idea of using contextual 
text in a certain vicinity of the anchor text was 
proposed in Chakrabarti et al (1998) to automat-
ically compile some lists of authoritative Web 
resources on a range of topics. An anchor win-
dow approach is proposed in Chakrabarti et al
(1998) to extract implicit anchor text. Following 
this work, anchor windows were considered in 
some other tasks (Amitay  et al, 1998; Haveli-
wala et al, 2002; Davison, 2002; Attardi et al, 
1999). Although we are inspired by these ideas, 
our work is different because research papers 
have many different properties from Web pages. 
From the viewpoint of implicit anchor extraction 
techniques, our approach is different from the 
anchor window approach. The anchor window 
approach is somewhat simpler and easy to im-
plement than ours. However, our method is more 
general and flexible. In our approach, the anchor 
text is not necessarily to be in a window. 
Citeseer (Giles et al, 1998; Lawrence  et al, 
1999) has been doing a lot of valuable work on 
citation recognition, reference matching, and pa-
per indexing. It has been displaying contextual 
information for cited papers. This feature has 
been shown to be helpful and useful for re-
searchers. Differently, we are using context de-
scription for improving ranking rather than dis-
play purpose. In addition to Citeseer, some other 
work (McCallum et al, 1999; Nanba and Oku-
mura, 1999; Nanba et al, 2004; Shi et al, 2006) 
is also available for extracting and accumulating 
reference information for research papers. 
5 Conclusions and Future Work 
In this paper, we propose to improve academic 
search by utilizing pseudo-anchor information. 
As pseudo-URL and pseudo-anchor text are not 
as explicit as in general web search, more efforts 
are needed for pseudo-anchor extraction. Our 
machine-learning approach has proven success-
ful in automatically extracting implicit anchor 
text. By using the pseudo-anchors in our academ-
ic search system, we see a significant perfor-
mance improvement over the basic approach. 
 
 
Acknowledgments 
We would like to thank Yunxiao Ma and Pu 
Wang for converting paper full-text from PDF to 
HTML format. Jian Shen has been helping us do 
some reference extraction and matching work. 
Special thanks are given to the researchers and 
students taking part in data labeling. 
 
 
 
 
17
References 
E. Amitay. 1998. Using common hypertext links to 
identify the best phrasal description of target web 
documents. In Proc. of the SIGIR'98 Post Confe-
rence Workshop on Hypertext Information Re-
trieval for the Web, Melbourne, Australia. 
G. Attardi, A. Gulli, and F. Sebastiani. 1999. Theseus: 
categorization by context. In Proceedings of the 8th 
International World Wide Web Conference. 
A. Baxter, P. Christen, T. Churches. 2003. A compar-
ison of fast blocking methods for record linkage. In 
ACM SIGKDD'03 Workshop on Data Cleaning, 
Record Linkage and Object consolidation. Wash-
ington DC. 
A. Broder, S. Glassman, M. Manasse, and G. Zweig. 
1997. Syntactic clustering of the Web. In Proceed-
ings of the Sixth International World Wide Web 
Conference, pp. 391-404. 
C.J.C. Burges. 1998. A tutorial on support vector ma-
chines for pattern recognition. Data Mining and 
Knowledge Discovery, 2, 121-167. 
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. 
Raghavan, and S. Rajagopalan. 1998. Automatic 
resource list compilation by analyzing hyperlink 
structure and associated text. In Proceedings of the 
7th International World Wide Web Conference. 
K. Chakrabarti, V. Ganti, J. Han, and D. Xin. 2006. 
Ranking objects based on relationships. In SIG-
MOD ?06: Proceedings of the 2006 ACM SIG-
MOD international conference on Management of 
data, pages 371?382, New York, NY, USA. ACM. 
B. Davison. 2000. Topical locality in the web. In SI-
GIR'00: Proceedings of the 23rd annual interna-
tional ACM SIGIR conference on Research and 
development in information retrieval, pages 272- 
279, New York, NY, USA. ACM. 
I.P. Fellegi, and A.B. Sunter. A Theory for Record 
Linkage, Journal of the American Statistical Asso-
ciation, 64, (1969), 1183-1210. 
C. L. Giles, K. Bollacker, and S. Lawrence. 1998. 
CiteSeer: An automatic citation indexing system. 
In IanWitten, Rob Akscyn, and Frank M. Shipman 
III, editors, Digital Libraries 98 - The Third ACM 
Conference on Digital Libraries, pages 89?98, 
Pittsburgh, PA, June 23?26. ACM Press. 
T.H. Haveliwala, A. Gionis, D. Klein, and P. Indyk. 
2002. Evaluating strategies for similarity search on 
the web. In WWW ?02: Proceedings of the 11th in-
ternational conference on World Wide Web, pages 
432?442, New York, NY, USA. ACM. 
K. Jarvelin, and J. Kekalainen. 2000. IR Evaluation 
Methods for Retrieving Highly Relevant Docu-
ments. In Proceedings of the 23rd Annual Interna-
tional ACM SIGIR Conference on Research and 
Development in Information Retrieval (SI-
GIR2000). 
S. Lawrence, C.L. Giles, and K. Bollacker. 1999. Dig-
ital libraries and Autonomous Citation Indexing. 
IEEE Computer, 32(6):67?71. 
A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 
1999. Building Domain-specific Search Engines 
with Machine Learning Techniques. In Proceed-
ings of the AAAI-99 Spring Symposium on Intelli-
gent Agents in Cyberspace. 
A. McCallum, K. Nigam, and L. Ungar. 2000. Effi-
cient clustering of high-dimensional data sets with 
application to reference matching. In Proc. 6th 
ACM SIGKDD Int. Conf. on Knowledge Discov-
ery and Data Mining. 
O.A. McBryan. 1994. Genvl and wwww: Tools for 
taming the web. In In Proceedings of the First In-
ternational World Wide Web Conference, pages 
79-90. 
H. Nanba, M. Okumura. 1999. Towards Multi-paper 
Summarization Using Reference Information. In 
Proc. of the 16th International Joint Conference on 
Artificial Intelligence, pp.926-931. 
H. Nanba, T. Abekawa, M. Okumura, and S. Saito. 
2004. Bilingual PRESRI: Integration of Multiple 
Research Paper Databases. In Proc. of RIAO 2004, 
195-211. 
L. Parsons, E. Haque, H. Liu. 2004. Subspace cluster-
ing for high dimensional data: a review. SIGKDD 
Explorations 6(1): 90-105. 
S.E. Robertson, S. Walker, and M. Beaulieu. 1999. 
Okapi at TREC-7: automatic ad hoc, filtering, VLC 
and filtering tracks. In Proceedings of TREC?99. 
S. Shi, R. Song, and J-R Wen. 2006. Latent Additivity: 
Combining Homogeneous Evidence. Technique 
report, MSR-TR-2006-110, Microsoft Research, 
August 2006. 
S. Shi, F. Xing, M. Zhu, Z.Nie, and J.-R. Wen. 2006. 
Pseudo-Anchor Extraction for Search Vertical Ob-
jects. In Proc. of the 2006 ACM 15th Conference 
on Information and Knowledge Management. Ar-
lington, USA. 
Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. 2005. 
Object-level ranking: bringing order to web objects. 
InWWW?05: Proceedings of the 14th international 
conference on World Wide Web, pages 567?574, 
New York, NY, USA. ACM. 
 
18
