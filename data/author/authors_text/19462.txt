Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 872?883, Dublin, Ireland, August 23-29 2014.
Utilizing Microblogs for Automatic News Highlights Extraction
Zhongyu Wei
The Chinese University of Hong Kong
Shatin, N.T.
Hong Kong
zywei@se.cuhk.edu.hk
Wei Gao
Qatar Computing Research Institute
Qatar Foundation
Daha, Qatar
wgao@qf.org.qa
Abstract
Story highlights form a succinct single-document summary consisting of 3-4 highlight sentences
that reflect the gist of a news article. Automatically producing news highlights is very challeng-
ing. We propose a novel method to improve news highlights extraction by using microblogs. The
hypothesis is that microblog posts, although noisy, are not only indicative of important pieces of
information in the news story, but also inherently ?short and sweet? resulting from the artificial
compression effect due to the length limit. Given a news article, we formulate the problem as two
rank-then-extract tasks: (1) we find a set of indicative tweets and use them to assist the ranking
of news sentences for extraction; (2) we extract top ranked tweets as a substitute of sentence ex-
traction. Results based on our news-tweets pairing corpus indicate that the method significantly
outperform some strong baselines for single-document summarization.
1 Introduction
People in this era are overloaded by their daily exposure to large amount of online information. To make
life easier, some news websites like CNN.com and USAToday.com provide ?Story Highlights? in their
news articles for readers to get the gist of story quickly. The highlights of an article typically contain
3-4 summary sentences in bullet-points form that are representative of and shorter than the original new
sentences in the article. An example of story highlights of an article is shown in Figure 1 (marked in red
rectangle) that are written in a compact, almost telegraphic style. In contrast to the original content of
the article, significant compression is obtained by shortening and paraphrasing.
Unfortunately, the production of such good-quality highlights needs to be done manually which is
very expensive. Existing methods face grand technical challenges for automating the process. The task is
complex in nature due to a broad range of linguistic constraints which ultimately requires wide-coverage
of language understanding beyond the capabilities of current NLP technology (Woodsend and Lapata,
2010). Most automatic systems simplify the problem using extractive approach. By using linguistic
or statistical information or both, the key units or concepts can be identified from sentences or across
multiple documents, and then the sentences are scored and extracted according to their informativeness
with the presence of the key components.
The extractive approach has two salient problems: (1) it is commonly ineffective to locate key sen-
tences, meaning that the presence of linguistically and/or statistically important units does not necessarily
indicate a highlight sentence. This is evidenced by the fact that sophisticated systems for Document Un-
derstanding Conference (DUC) summarization task cannot significantly outperform a trivial baseline that
simply selects first n sentences of the document (Nenkova, 2005); (2) sentence extracts as highlights are
extraordinarily verbose in general, which need to be post-processed for substantial compression. But
sentence compression may breach the readability or grammaticality (Clarke and Lapata, 2008).
With the popularity of social media, online news providers are moving towards offering more inter-
action with news readers via microblogging service like Twitter. Many Twitter users also post tweets
Work conducted at Qatar Computing Research Institute (QCRI) when the first author was employed as an intern
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
872
Figure 1: A CNN news article with story highlights (Highlights are marked by red rectangle, and the
news sentences related to the highlights are enclosed in green rectangles) and some relevant tweets one
can observe independently on Twitter (marked by light blue rectangles on the left)
about news together with their URLs. Such increased cross-media interaction recasts the role of different
information sources that are useful for this task in a sense that interesting correlations between the news
and relevant microblogs could be captured and leveraged to boost the performance.
To address these considerations, we make two hypotheses based on our observation that can be crucial
to highlights extraction. (1) Indicative effect: microblog users? mentioning about the pieces of news is
indicative of the importance of the corresponding sentences; (2) Human compression effect: important
portions of a news article have been rewritten by microblog users in a more condensed style owing to
length limit. Accordingly, we formulate our problem as two independent rank-then-extract tasks: firstly,
we find a set of indicative tweets and use them to assist the ranking of news sentences for extraction;
secondly, we extract top-ranked tweets (with the help of news sentences) as a substitute of sentences
extraction since they are typically shorter. Based on our news-tweets pairing corpus, the results of ex-
periments following both directions indicate that our methods outperform some strong baselines for
single-document summarization.
2 Related Work
Our work intersects the summarization of single document and microblogs. Single-document summa-
rization has been studied for years starting from Luhn and Peter (1958). Based on local content informa-
tion of a document (Wong et al., 2008; Barzilay et al., 1997; Marcu, 1997), researchers proposed various
statistical or semantic approaches using classification (Wong et al., 2008), Integer Linear Programming
(ILP) (Li et al., 2013), sequential models (Shen et al., 2007) and graphical models (Litvak and Last,
2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was
used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection
of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013).
Summarizing microblog content is to distill the large quantities of tweets into a concise and represen-
tative description of a target event. Sharifi et al. (2010) proposed a graph-based phrase reinforcement
873
algorithm (PRA) to generate a one-sentence summary from a collection of tweets. By using linguistic
features, Judd and Kalita (2013) improved the performance of PRA. Sharifi et al. (2010) and Inouye
et al. (2011) presented a hybrid TF-IDF approach for extracting tweets with the presence of important
terms. More fine-grained summarization was proposed by considering sub-events and combining the
summaries extracted from each sub-topic (Nichols et al., 2012; Zubiaga et al., 2012; Duan et al., 2012).
The research for coupling news and microblogs attracted much attention recently. Suba?si?c and
Berendt (2011) and Zhao et al. (2011) independently compared tweets to online news to identify fea-
tures for news detection in tweets. Phelan et al. (2011) used tweets to recommend news articles based on
user preferences. Gao et al. (2012) produced cross-media news summaries by capturing the complemen-
tary information from both sides. Kothari et al. (2013) and
?
Stajner et al. (2013) investigated detecting
news comments from Twitter for extending news information provided. Guo et al. (2013) proposed a
graphical model to identify news for a given tweet to provide contextual support for NLP tasks.
Some work attempted to use different kinds of resources to help document summarization, such as
Wikipedia and query log of search engine (Svore et al., 2007), clickthrough data (Sun et al., 2005),
users? comments on news (Hu et al., 2008), and social media context of the articles (Yang et al., 2011).
Our work is closely related to Svore et al. (2007) that considered incorporating third-party resource in
the ranking process, but the access to query logs is extremely limited, and Wikipedia content is relatively
static which cannot reflect timely information like social media.
We also share the same testbed with Woodsend and Lapata (2010). They selected and compressed
news sentences with a joint model using ILP by considering phrase as basic extract element. Their
method requires a large training corpus for deriving accurate salient scores of phrases, and also the
feasible solution of ILP model with hard constraints does not necessarily exist.
Yang et al. (2011) proposed a unified supervised model called dual wing factor graph to simultaneously
summarize Web documents and tweets based on structural mining from social context. Despite of similar
motivation, our work has some key differences from theirs: (1) Our ground-truth come from standard
news highlights, and our target summary keeps consistent no matter which source of information our
highlights are extracted from. They built ground-truth summaries separately for each side by manually
choosing no less than 5 tweets and 10 news sentences. So, our standard is more difficult to reach since
our ground-truth summaries are not extracts of the original sentences or tweets; (2) Our approach is very
different. We use ranking-based algorithm which is more adequate than their classification approach
because there are much fewer positive candidates than negative ones, and the class distribution is very
imbalanced (like information retrieval tasks). Also, they were focused on mining the implicit structural
information from retweeting and user following networks, while we focus on content-based correlations.
3 Corpus Construction
There is no news-tweets coupling data set publicly available for the purpose of news highlights produc-
tion
1
. We constructed the first of such corpus for this application by our own, for which an event-oriented
strategy was adopted to collect the highlights-document-tweets couplings by using a social search en-
gine. We manually identified 17 salient news events taking place in recent two years. For each event,
we manually generated a set of core queries which were used to retrieve the relevant tweets via Topsy
2
search API. Then we gathered the retrieved tweets containing embedded URLs that point to the news
articles on CNN and USAToday websites that provide story highlights, and extracted the content of the
news articles and the associated highlights.
For each article, we collected all the tweets in the retrieved tweet set above that contain links to the
article to form our highlights-document-tweets couplings based on the following rules: (1) We delete
those extremely short tweets with less than 5 tokens and the tweets that are suspected copies from news
title and highlights. For example, we try our best to remove all the suspectable tweets including the cases
1
We realize the news-tweets coupling data set released recently for NLP tasks by Guo et al. (Guo et al., 2013). However,
this data set is not suitable for our task for two reasons: (1) There are 12,704 news articles but only 34,888 tweets. Although
part of the news are from CNN which contain story highlights, the number of tweets per article is too limited, not to mention
finding useful candidates; (2) The full text of news content is not provided, with only the first few sentences of articles instead.
2
http://topsy.com
874
Documents Highlights Tweets
Total # 121 455 78,419
Sentence # per news 53.6?25.6 3.7?0.4 648.1?1161.7
Token # per news 1123.0?495.8 49.6?10.0 10364.5?24749.2
Token # per sentence 21.0?11.6 13.2?3.2 16.0?5.3
Table 1: Overview statistics on the corpus (mean and standard deviation)
Event Doc # Highlight # Tweet # Event Doc # Highlight # Tweet #
Aurora shooting 14 54 12,463 African runner murder 8 29 9,461
Boston bombing 38 147 21,683 Syria chemical weapons use 1 4 331
Connecticut shooting 13 47 3,021 US military in Syria 2 7 719
Edward Snowden 5 17 1,955 DPRK Nuclear Test 2 8 3,329
Egypt balloon crash 3 12 836 Asiana Airlines Flight 214 11 42 8,353
Hurricane Sandy 4 15 607 Moore Tornado 5 19 1,259
Russian meteor 3 11 6,841 US Flu Season 7 23 6,304
Chinese Computer Attacks 2 8 507 Williams Olefins Explosion 1 4 268
cause of the Super Bowl blackout 2 8 482 Total 121 455 78,419
Table 2: Distribution of documents, highlights and tweets with respect to different events
like ?RT @someone HIGHLIGHT URL?; (2) If there are more than 100 tweets linked to an article, the
article is kept, otherwise the artcile is removed. Note that using explicit hyperlinks is not the only way for
identifying the couplings but the most straightforward one. Here we simply resort to this straightforward
method to build the corpus for verifying our two hypotheses raised in Section 1. Thorough investigation
on the construction of an enhanced highlights-oriented coupling corpus is left for our future work.
The statistics of the resulted corpus are given in Table 1 which is also made accessible
3
. As shown in
the table, the average number of relevant tweets to a document is about 648. Since some of the events
are much more popular than others, the standard deviation of the number of tweets associated with a
document is as high as 1,162. The highlights are characterized as high compression rate compared to the
length of news articles. In addition, a single highlight sentence on average is only 2/3 the length of a news
sentence, and more interestingly the average length of tweets is very close to that of highlight sentences,
which suggests that the relevant tweets can be a reasonable source of candidates for extraction.
Table 2 shows the distribution of documents, highlights and tweets with respect to the 17 news events
we collected.
4 Our Approach
Given a news article containing n sentences S = {s
1
, s
2
, ...s
n
} and a set of m relevant tweets T =
{t
1
, t
2
, ..., t
m
}, we aim to extract x sentences from the set S or the same number of tweets from set T as
highlights covering the main theme of the article. We define the two tasks as follows:
? Task 1 ? sentences extraction: Given auxiliary T , extract x elements H(S) =
{s
(1)
, s
(2)
, ..., s
(x)
|s
(i)
? S, 1 ? i ? x} from S as highlights.
? Task 2 ? tweets extraction: Given auxiliary S, extract x elementsH(T ) = {t
(1)
, t
(2)
, ..., t
(x)
|t
(i)
?
T, 1 ? i ? x} from T as highlights.
Most single-document summarization methods (Woodsend and Lapata, 2010; Yang et al., 2011) treat
the extraction as a classification problem which assigns either positive or negative label to the extract
candidates. We argue that it is more adequate to model it as a ranking problem because there is far
more unsuitable candidates than suitable ones for being the highlights. Such kind of imbalanced class
distribution makes classification a secondary solution.
Our model learns to rank all the candidate sentences in task 1 or candidate tweets in task 2, and then
extracts the top-x ranked instances as output highlights. We adopt an effective pair-wise ranking model
RankBoost (Freund et al., 2003) for that using the RankLib package
4
. RankBoost takes pairs of instances
3
http://www1.se.cuhk.edu.hk/
?
zywei/data/hilightextraction.zip
4
http://sourceforge.net/p/lemur/wiki/RankLib/
875
Category Name Description
Local Sentence Feature (LSF)
IsFirst Whether s is the first sentence in the news
Pos The position of s in the news
TitleSimi Token overlap between s and news title
ImportUnigram Importance score of s according to the unigram distribution in the news
ImportBigram Importance score of s according to the bigram distribution in the news
Local Tweet Feature (LTF)
Length Token number in t
HashTag HashTag related features (presence and count)
URL URL related features (count)
Mention Mention related features (presence and count)
ImportTFIDF Importance score of t based on unigram Hybrid TF-IDF algorithm (Sharifi et al., 2010)
ImportPRA Importance score of t based on phrase reinforcement algorithm (Sharifi et al., 2010)
TopicNE Named entity related features (NE count and seven binary values indicating the presence of each category)
TopicLDA LDA-based topic model features (maximum relevance with sub-topics, etc.)
QualityOOV Out-of-vocabulary words related features (count and percentage)
QualityLM Quality score of t according to language model (Unigram, bigram and trigram)
QualityDepend Quality score of t according to dependency bank (Han and Baldwin, 2011)
Cross-Media Feature (CCF)
MaxCosine Maximum cosine value between the target instance and auxiliary instances
MaxROUGE1F Maximum ROUGE-1 F score between the target instance and auxiliary instances
MaxROUGE1P Maximum ROUGE-1 precision value between the target instance and auxiliary instances
MaxROUGE1R Maximum ROUGE-1 recall value between the target instance and auxiliary instances
LeadSenSimi
?
ROUGE-1 F score between leading news sentences and t
TitleSimi
?
ROUGE-1 F score between news title and t
MaxSenPos
?
The position of sentences that obtain maximum ROUGE-1 F score with t
SimiUnigram Similarity based on the distribution of (local) unigram frequency in the auxiliary resource
SimiUniTFIDF Similarity based on the distribution of (local) unigram TF-IDF in the auxiliary resource
SimiTopEntity Similarity based on the (local) presence and count of most frequent entities in the auxiliary resource
SimiTopUnigram Similarity based on the (local) presence and count of most frequent unigrams in the auxiliary resource
Table 3: Feature description (t: a tweet; s: a news sentence; *: features used in task 2 only)
(I
i
, I
j
) as input for training and their preference order as labels. In our case, instance pair can be the pair
of sentences or tweets, and the pairwise order is determined by the salient score of each instance that is
the maximum ROUGE-1 (Lin, 2004) F-value between the instance and the corresponding ground-truth
highlight sentences. Given the gold standard highlights H
g
= {h
1
, h
2
, ..., h
x
}, the salient score of an
instance is calculated as score(I
i
) = max
k
{ROUGE-1(I
i
, h
k
)}.
Note that in task 2 the number of tweets pairs generated in training can be extremely large because
of the number of tweets in popular topical news articles (see Table 2) that may degrade the efficiency of
training. Some ad-hoc workaround is employed to make the problem tractable. As opposed to using all
the possible pairs, we divide the tweets into b bins, where the bins are bounded by continuous ranges of
salient scores. We fix the length of different ranges by fitting the distributions of salient score values.
Tuned on a subset with 20% randomly selected training instances, the value of b is determined as 4.
Then, the pairs are formed across these brackets.
5 Feature Design
The feature space of the two tasks are designed to intersect at the cross-media correlation part. The local
features describe the instance to be ranked (i.e., either a news sentence or a tweet), and the cross-media
correlation features capture the similarity of the instance with the counterparts in the auxiliary resource.
The features consist of three subsets of informativeness measures including local sentence features
(LSF), local tweet features (LTF) and cross-media correlation features (CCF). In task 1, we can use
LSF or both LSF and CCF for rank learning; and in task 2, we can use LTF or combine LTF and CCF.
The full feature list is described in Table 5. For local sentence features, we implement the 5 document
features defined in (Svore et al., 2007) for single-document summarization task. This is for the ease of
comparison with the existing approach. In this section, we will only describe the local tweet features and
the cross-media correlation features in more detail.
5.1 Local Tweet Features
Local tweet features are proposed to capture the importance of a tweet based on local information in
three aspects, including twitter-specific, topic-related, and writing-quality measures.
876
5.1.1 Twitter-specific measures
Twitter-specific features indicate the basic content-based characteristics of a tweet such as length, the
characteristics specifically provided by Twitter platform such as hashtags, mentions and embedded urls,
and two scoring functions used by state-of-the-art tweet summarization algorithms including Hybrid TF-
IDF (Sharifi et al., 2010) and PRA (Sharifi et al., 2010). Hybrid TF-IDF is a variant of traditional TF-IDF
weighting for tweets collection which treats each tweet as a document when computing IDF while the
whole tweets set as a document when computing TF. We calculate the feature ImportTFIDF of a tweet
based on the TF and IDF values of its tokens. PRA is a phrase reinforcement algorithm that can produce
a one-sentence summary for a given tweets set. We follow the idea of PRA to generate the token graph
of our tweets set and compute the weight for each token node. We then measure the importance of a
tweet by summating the weights of all its tokens, which becomes the ImportPRA feature.
5.1.2 Topic-related measures
Topic-related features are used to capture important tweets based on the topical information embodied
by named entities (NE) or latent topic semantics. TopicNE is proposed to utilize NE as indicator for
describing an event. We resort to Stanford Name Entity Recognizer
5
to extract seven types of named
entities including time, location, organization, person, money, percent and date. Based on that, we count
entities in the tweet, and then obtain seven additional binary values indicating the presence of each
category. TopicLDA is used to capture sub-topics. Intuitively, if a tweet is highly related to some sub-
topic in the event, it is more important. We use LDA (Blei et al., 2003) to identify the sub-topics in
the tweets set. Based on the resulted sub-topics and term distribution, we first calculate the maximum
relevance value between the tweet and all sub-topics as a feature. Then, we obtain the distribution of
relevance values of the tweet with respect to all sub-topics and compute the entropy of this distribution
as another feature. The lower the entropy is, the higher the degree of topical concentration for the tweet.
We use the default setting of the toolkit mallet
6
and set the number of sub-topics as 10 empirically.
5.1.3 Writing-quality measures
Writing-quality features indicate if a tweet is written in a formal way. Intuitively if more formally a
tweet is written, it is more likely to be extracted. QualityOOV measures to what extent a tweet contains
out-of-vocabulary (OOV) tokens. We simply calculate the number and the percentage of the OOV words
in the tweet as features
7
. QualityLM measures writing quality of a tweet based on language model.
We train uni-gram, bi-gram and tri-gram language models using maximum-likelihood estimation. By
summating the probabilities of all the tokens in the tweet regarding the three different language models,
we obtain three n-gram-based writing-quality features. QualityDepend measures the writing quality
based on dependency relation. The dependency feature is generated following Han et al. (2011). Instead
of using the technique for normalizing tweet text, we apply it for assessing the grammaticality of tweets
8
.
5.2 Cross-media Correlation Features
We observe that Twitter users like to quote or rewrite the important pieces of new content in the posts.
If a news sentence is referred or paraphrased by many tweets, it is assumed to be indicated as more
important. On the other hand, a tweet, besides its local importance indicator, may be more important if
it is similar to the theme of the news content. Therefore, cross-media correlation features are designed
to incorporate the auxiliary information source for helping instance ranking. In task 1, news articles are
local content and the corresponding tweets are considered auxiliary, and in task 2 their roles are reversed.
5.2.1 Instance-level similarities
Instance-level similarities indicate if there are auxiliary instances similar to the current local instance
and to what extent they are similar. These features reveal if the current instance has strong correlation
5
http://nlp.stanford.edu/software/CRF-NER.shtml
6
http://mallet.cs.umass.edu/index.php
7
The words not found in a common English dictionary, GNU aspell dictionary v0.60.6, are treated as OOV
8
Both dependency bank and language model here are based on New York Times corpus (http://www.ldc.upenn.
edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19)
877
across the media boundary. We use four general metrics including cosine, ROUGE-1 F-value, ROUGE-1
precision score and ROUGE-1 recall score to measure the surface similarity between news sentence and
tweet. And the other three features, namely LeadSenSimi, TitleSimi and MaxSenPos are only used in task
2 for ranking tweets when news sentences are considered as auxiliary. This is because leading sentences
and title of news are considered as the most informative content. The more similar a tweet to them, the
more important it can be. Also, position information is often used for document summarization. We
borrow the position of the most similar sentence as bridge to measure the importance of a given tweet.
5.2.2 Semantic-space-level similarities
Semantic-space-level similarities reflect the importance of the current local instance based on the distri-
bution of its semantic units in the auxiliary resource. We propose two features to represent the distribution
of the semantic units that are based on unigram frequency and unigram TF-IDF, and named as SimiU-
nigram and SimiUniTFIDF, respectively. We first obtain a unigram distribution on the auxiliary space,
and compute the similarity of a local instance by summing over the probabilities of all its unigrams in
the distribution. Additionally, we also identify some most frequent named entities and unigrams in the
auxiliary information source, and then compute the presence and the count of them in the current local
instance as additional features, which are named as SimiTopEntity and SimiTopUnigram.
6 Experiments and Results
6.1 Setup
Task 1 extracts highlights from news articles. For comparison, we use the following approaches: (1)
Lead sentence chooses the first x sentences from the given news article, which is a strong baseline
that no DUC system could beat with large margin (Nenkova, 2005); (2) Phrase ILP (Woodsend and
Lapata, 2010) generates highlights from news with the joint model combining sentence compression
and selection, which treats phrases and clauses as extract unit; (3) Sentence ILP (Woodsend and Lapata,
2010) is a variant of Phrase ILP that treats sentence as extract unit; (4) LexRank (news) summarizes
the given news using the typical multi-document summarization algorithm LexRank (Erkan and Radev,
2004); (5) Ours (LSF) is our ranking method based on the local sentence features which are equivalent to
the features used by Svore et al. (2007); (6) Ours (LSF+CCF) is our method combining LSF and CCF.
Task 2 extracts highlights from tweets where we use the following approaches: (1) LexRank (tweets)
uses LexRank (Erkan and Radev, 2004) with tweets as the mere input; (2) Ours (LTF) is our ranking
method based on local tweet features; (3) Ours (LTF+CCF) is our method combining LTF and CCF.
Unlike single news document where redundant sentences are rare, the redundancy of tweets is serious.
Many summarization algorithms are sensitive to redundancy in the input. It is thus problematic for
tweets as the source of extraction. Hence we apply Maximal Marginal Relevance (MMR) (Carbonell
and Goldstein, 1998) for reducing tweets redundancy in task 2. The parameter in MMR used to gauge
the threshold of redundancy is tuned based on 20% randomly selected training data. Overall, we conduct
5-fold cross-validation for evaluation. The highlights of each news article are used as ground truth. In
the output, we fix the number of highlights extracted x as 4. We report ROUGE-1 and ROUGE-2 scores
with ROUGE-1 as the major evaluation metric.
6.2 Results
The overall performance can be seen in Table 4, from which we have the following findings:
? Indeed, Lead sentence is a very strong baseline that performs much better than most of other meth-
ods. It is only a little worse than LexRank (news) and much worse than Ours (LSF+CCF).
? LexRank (news) performs the second best in task 1. However, the performance of LexRank (tweets)
is the worst in task 2. This is because LexRank is proposed for summarizing regular documents and its
performance is affected seriously by the short, noisy texts like tweets.
? Sentence ILP and Phrase ILP perform similarly and do not show clear advantage over other base-
lines. This is different from what Woodsend and Lapata (2010) has obtained. This implies that their
model is sensitive to the size of training data where the ILP model may be undertrained here with the
878
Approach
ROUGE-1 ROUGE-2
F P R F P R
Lead sentence 0.263 0.211 0.374 0.101 0.080 0.147
LexRank (news) 0.264 0.226 0.332 0.088 0.074 0.112
Sentence ILP 0.238 0.209 0.293 0.068 0.058 0.088
Phrase ILP 0.236 0.215 0.281 0.069 0.061 0.086
Ours (LSF) 0.256 0.214 0.345 0.093 0.076 0.129
Ours (LSF+CCF) 0.292 0.239 0.398 0.110 0.089 0.155
LexRank (tweets) 0.212 0.204 0.226 0.064 0.061 0.068
Ours (LTF) 0.264 0.280 0.274 0.095 0.106 0.098
Ours (LTF+CCF) 0.295 0.320 0.295 0.105 0.118 0.105
Table 4: Overall performance (Bold: best performance of the task; Underlined: significance (p < 0.01)
compared to our best model; Italic: significance (p < 0.05) compared to our best model)
amount of training data available. In addition, we find there are lots of infeasible solutions for the ILP
model, indicating that the hard constraints are not relaxed enough for the relatively small data set.
? Ours (LSF+CCF) and Ours (LTF+CCF) achieve the best performance on task1 and task2, respec-
tively, and they significantly outperform all other methods in terms of ROUGE-1 F-score based on the
result of paired two-tailed t-test. By incorporating CCF, we improve the performance of local features
significantly. This justifies that cross-media correlations are indeed useful for improving the quality of
exaction from both directions.
? Comparing Ours (LSF+CCF) and Ours (LTF+CCF), although their ROUGE-1 F-scores are compa-
rable, the former is better on ROUGE-1 recall and the ROUGE-1 precision of the latter is much higher.
This is because news sentences are usually longer than tweets. So the highlights extracted from news
article cover more highlight tokens than those from tweets. The length of generated summary and ground
truth can be seen in Table 5, where tweet extracts are much closer to the ground-truth highlights. And
tweets appear to be a more suitable source for highlights extraction because of the human compression
effect on the tweets.
Tokens # per sentence Tokens # per summary
Ground-truth highlights 13.2?3.2 49.6?10.0
Ours (LSF+CCF) 24.3?11.8 91.3?18.4
Ours (LTF+CCF) 16.1?5.4 55.3?16.1
Table 5: Comparison of the length of extracted highlights and that of ground truth
6.3 Analysis
Table 6 shows an example for analyzing our extracted highlights compared to the ground-truth. In
example 1 (left column), with the help of tweets, Ours (LSF+CCF) can output good highlight sentences
N2 and N3 which cannot be extracted by Ours (LSF). On the side of tweets, T2 is newly extracted by
Ours (LTF+CCF) after considering CCF. Furthermore, highlights extracted from tweets also bring extra
good highlight T3 which is similar to H1. We find that H1 is rewritten from an original sentence which
is three times longer, so it is difficult for extractive method to locate the original sentence in the article.
Even if the sentence could be identified, the information was verbose still. Interestingly, some Twitter
user produces a tweet like T3 by paraphrasing and shortening which is captured by the algorithm.
Although cross-media correlations are helpful, two out of four ground-truth highlight sentences are
covered by the extracted good highlights in example 1. Also, the good extracts from different sources
may not cover the same set of ground-truth. Therefore, maybe we can try to combine the extracts from
both sides for further improvement.
879
1: Positive example 2: Negative example
H1. Luxor province bans all hot air balloon flights until further notice HH1. Snowden grew up in Elizabeth City, N.C., but family moved to Ellicott
City, Md.
H2. The Tuesday accident was the world?s deadliest hot air balloon accident
in at least 20 years
HH2. In 2003, he enlisted in the Army, but broke both his legs during Special
Forces training
H3. Officials: Passengers in the balloon included 19 foreign tourists HH3. His first NSA job was as a security guard at an agency facility at the
University of Maryland
H4. No foul play is suspected, official says
N1. Cairo An official investigation into the cause of a balloon accident that
killed 19 people in Egypt could take two weeks, ...
NN1. A 29-year-old former CIA employee who admitted responsibility Sun-
day for one of the most extraordinary ...
N2. [+] The Tuesday accident was the world?s deadliest hot air balloon
accident in at least 20 years.
NN2. He told the newspaper he is willing to stand behind his actions in
public because ?I know I have done nothing wrong.?
N3. [+] Tuesday?s crash prompted the governor to ban all hot air balloon
flights until further notice.
NN3. He told the newspaper that the NSA ?routinely lies? to Congress about
the scope of its surveillance in the United States.
N4. How safe is hot air ballooning? NN4. [+] I can?t in good conscience allow the U.S. government to destroy
privacy, internet freedom and basic...
NN. [-] His first NSA job was as a security guard at an agency facility at
the University of Maryland in College Park, ...
T1. CNN: official investigation into yesterday air balloon accident in Luxor
could take 2 weeks
TT1. I can?t in good conscience allow the U.S. government to destroy pri-
vacy, Snowden told the Guardian.
T2. [+] Governor bans all hot air balloon flights until further notice. TT2. whistleblower Edward Snowden: I do not expect to see home again,
though that is what I want.
T3. Foul play not suspected in fatal balloon accident TT3. More on ex CIA Snowden: I have done nothing wrong
T4. Official: Egypt balloon explosion probe can take 2 weeks TT4. Ex-CIA employee: Obama advanced surveillance policies, not re-
formed them.
Table 6: Examples of extracted highlights (H&HH items are the ground-truth highlights, N&NN items
are the highlights extracted from news by Ours (LSF+CCF), and T&TT items are the highlights ex-
tracted from tweets by Ours (LTF+CCF); Bold: Good highlight; [+]: Newly extracted highlights using
correlation features; [-]: Lost highlights after adding correlation features)
Example 2 (right column) shows tweets may not be always useful. Ours (LSF+CCF) adds a bad
highlight NN4 but removes a good one NN. We find that NN4 is very similar to TT1. So the introduction
of NN4 is believed as the result of influence from TT1. NN is squeezed out of the summary since we
find it lack of tweets in our set similar to NN. Currently, we only use explicit links for tweets-document
couplings. It might be helpful if we could expand the set to cover more informative tweets.
6.4 Contribution of Features
We further investigate the contribution of different features in our feature set (see Table 5) to the learned
ranking models. We choose the best models from the two tasks, i.e., Ours (LSF+CCF) and Ours
(LTF+CC), and find out the top-10 weighted features for each model. To get the feature weights, for
each feature we aggregate the weight values of its corresponding weak ranker selected during the itera-
tion in RankBoost training, that is, for a weak ranker repeatedly selected in different rounds, its weights
obtained from those rounds are added up to obtain as the feature weight. Table 7 lists the top-10 features
and their corresponding weight values.
Cross-media correlation features, which are underlined, appear overwhelmingly important to the sen-
tences extraction task with the modelOurs (LSF+CCF), where they take eight places in the top-10 feature
list. This confirms the indicative effect of tweets. In tweets extraction task, the model Ours (LTF+CCF)
does not seem to be so dependent on the cross-media correlation features, but still there are five of them
appearing important in the list. In particular, the similarities between tweets and the leading news sen-
tences such as SimiTopUnigram and LeadSenSimi are shown very helpful. This is because the leading
part of the article can be more indicative of important tweets. Besides, the writing-quality measures of
tweets are also very useful as it is shown that all the three quality-related features are among the top ten.
7 Conclusion and Future work
In this paper, we explore to utilize microblogs for automatic highlights extraction from two perspectives
using learning-based ranking models. Firstly, we extract important sentences from news article by using
a set of relevant tweets that provide indicative support for the informativeness of candidate sentences;
Secondly, we extract important tweets from the relevant tweets set associated with the given article by
taking the advantage of the fact that tweets are comparably concise as highlights. The results show
that our methods significantly outperform state-of-the-art baseline approaches for single-document sum-
880
Task 1: Ours (LSF+CCF) Task 2: Ours (LTF+CCF)
Feature Weight Feature Weight
ImportUnigram 4.7912 SimiTopUnigram (count) 1.9300
MaxROUGE1R 2.1049 LeadSenSimi (third) 1.8367
MaxROUGE1F 0.6511 QualityLM (Bigram) 1.4513
SimiTopUnigram (count) 0.6260 MaxROUGE1R 1.1925
SimiUnigram 0.5424 QualityLM (Unigram) 0.9441
MaxROUGE1P 0.1922 LeadSenSimi (second) 0.9224
SimiTFIDF 0.1534 QualityDepend 0.8306
SimiTopEntity (count) 0.0311 TopicNE (person) 0.7937
SimiTopEntity (presence) 0.0051 ImportTFIDF 0.7423
TitleSimi 0.0050 LeadSenSimi (fourth) 0.6072
Table 7: Top 10 features and their weights resulting from the best ranking models in the two tasks
(underline: Cross-media correlation features)
marization. Our feature study further discovers that the cross-media correlations are overwhelmingly
important to sentence extraction, and for tweets extraction the quality-related features are comparably
important as cross-media correlation measures. Also, tweets extraction appears more suitable for pro-
ducing highlights owing to the human compression effect of tweets.
For the future work, we plan to enlarge the relevant tweets collection by including relevant tweets
not linked by URLs; we can combine the extracts from both sides for further improvement; we can also
strengthen our model by capturing some deeper or latent linguistic and semantic correlations with deep
learning formalism.
Acknowledgments
This work is supported by the QCRI-MIT collaboration program. We appreciate the helpful discussions
with Regina Barzilay, Karthik Narasimhan and Lanjun Zhou at early stage of the project. Zhongyu Wei
is partially financed by the General Research Fund of Hong Kong (417112) and the Shenzhen Funda-
mental Research Program (JCYJ20130401172046450) of China. We thank anonymous reviewers for
their insightful comments.
References
Regina Barzilay, Michael Elhadad, et al. 1997. Using lexical chains for text summarization. In Proceedings of the
ACL Workshop on Intelligent Scalable Text Summarization, number 1, pages 10?17.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 335?336. ACM.
James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Research, 34:637?674.
Yajuan Duan, Zhimin Chen, Furu Wei, Ming Zhou, and Heung-Yeung Shum. 2012. Twitter topic summarization
by ranking tweets using social influence and content quality. In Proceedings of COLING, pages 763?780.
G?unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research, 22:457?479.
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining
preferences. Journal of Machine Learning Research, 4:933?969.
881
Wei Gao, Peng Li, and Kareem Darwish. 2012. Joint topic modeling for event summarization across news and
social media streams. In Proceedings of the 21st ACM International Conference on Information and Knowledge
Management, pages 1173?1182. ACM.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013. Linking tweets to news: A framework to enrich short text
data in social media. In Proceedings of the ACL, pages 239?249.
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In
Proceedings of ACL, pages 368?378.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-
document summarization as a tree knapsack problem. In Proceedings of EMNLP, pages 1515?1520.
Meishan Hu, Aixin Sun, and Ee-Peng Lim. 2008. Comments-oriented document summarization: understanding
documents with readers? feedback. In Proceedings of the 31st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 291?298. ACM.
David Inouye and Jugal K Kalita. 2011. Comparing twitter summarization algorithms for multiple post summaries.
In Proceedings of 2011 IEEE Third International Conference on Social Computing (SocialCom), pages 298?
306. IEEE.
Joel Judd and Jugal Kalita. 2013. Better twitter summaries? In Proceedings of NAACL-HLT, pages 445?449.
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to
sentence compression. Artificial Intelligence, 139(1):91?107.
Alok Kothari, Walid Magdy, Ahmed Mourad Kareem Darwish, and Ahmed Taei. 2013. Detecting comments on
news articles in microblogs. In Proceedings of ICWSM, pages 293?302.
Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In
Proceedings of ACL, pages 1004?1013.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?81.
Marina Litvak and Mark Last. 2008. Graph-based keyword extraction for single-document summarization. In
Proceedings of the workshop on multi-source multilingual information extraction and summarization, pages
17?24. Association for Computational Linguistics.
Hans Peter Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development,
2(2):159?165.
Daniel Marcu. 1997. From discourse structures to text summaries. In Proceedings of ACL, pages 82?88.
Ani Nenkova. 2005. Automatic text summarization of newswire: lessons learned from the document under-
standing conference. In Proceedings of the 20th International Conference on Artificial Intelligence, pages
1436?1441. AAAI Press.
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews. 2012. Summarizing sporting events using twitter. In Pro-
ceedings of the 2012 ACM International Conference on Intelligent User Interfaces, pages 189?198. ACM.
Owen Phelan, Kevin McCarthy, Mike Bennett, and Barry Smyth. 2011. Terms of a feather: Content-based news
recommendation and discovery using twitter. In Advances in Information Retrieval, pages 448?459. Springer.
Beaux Sharifi, M-A Hutton, and Jugal K Kalita. 2010. Experiments in microblog summarization. In Proceedings
of 2010 IEEE Second International Conference on Social Computing (SocialCom), pages 49?56. IEEE.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional
random fields. In Proceeding of IJCAI, pages 2862?2867.
Tadej
?
Stajner, Bart Thomee, Ana-Maria Popescu, Marco Pennacchiotti, and Alejandro Jaimes. 2013. Automatic
selection of social media responses to news. In Proceedings of the 19th ACM SIGKDD international Conference
on Knowledge Discovery and Data Mining, pages 50?58. ACM.
Ilija Suba?si?c and Bettina Berendt. 2011. Peddling or creating? investigating the role of twitter in news reporting.
In Advances in Information Retrieval, pages 207?213. Springer.
882
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang, Yuchang Lu, and Zheng Chen. 2005. Web-page summa-
rization using clickthrough data. In Proceedings of the 28th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 194?201. ACM.
Krysta Marie Svore, Lucy Vanderwende, and Christopher JC Burges. 2007. Enhancing single-document summa-
rization by combining ranknet and third-party sources. In Proceedings of EMNLP-CoNLL, pages 448?457.
Kam-Fai Wong, Mingli Wu, andWenjie Li. 2008. Extractive summarization using supervised and semi-supervised
learning. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 985?992.
Association for Computational Linguistics.
Kristian Woodsend and Mirella Lapata. 2010. Automatic generation of story highlights. In Proceedings of
the 48th Annual Meeting of the Association for Computational Linguistics, pages 565?574. Association for
Computational Linguistics.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and Juanzi Li. 2011. Social context summarization. In
Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 255?264. ACM.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic models. In Advances in Information Retrieval, pages
338?349. Springer.
Arkaitz Zubiaga, Damiano Spina, Enrique Amig?o, and Julio Gonzalo. 2012. Towards real-time summarization of
scheduled events from twitter streams. In Proceedings of the 23rd ACM Conference on Hypertext and Social
Media, pages 319?320. ACM.
883
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 162?171,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Discourse Relations for Eliminating
Intra-sentence Polarity Ambiguities
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, Kam-Fai Wong
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
Shatin, NT, Hong Kong, China
Key Laboratory of High Confidence Software Technologies
Ministry of Education, China
{ljzhou, byli, wgao, zywei, kfwong}@se.cuhk.edu.hk
Abstract
Polarity classification of opinionated sen-
tences with both positive and negative senti-
ments1 is a key challenge in sentiment anal-
ysis. This paper presents a novel unsuper-
vised method for discovering intra-sentence
level discourse relations for eliminating polar-
ity ambiguities. Firstly, a discourse scheme
with discourse constraints on polarity was de-
fined empirically based on Rhetorical Struc-
ture Theory (RST). Then, a small set of cue-
phrase-based patterns were utilized to collect
a large number of discourse instances which
were later converted to semantic sequential
representations (SSRs). Finally, an unsuper-
vised method was adopted to generate, weigh
and filter new SSRs without cue phrases for
recognizing discourse relations. Experimen-
tal results showed that the proposed methods
not only effectively recognized the defined
discourse relations but also achieved signifi-
cant improvement by integrating discourse in-
formation in sentence-level polarity classifica-
tion.
1 Introduction
As an important task of sentiment analysis, polar-
ity classification is critically affected by discourse
structure (Polanyi and Zaenen, 2006). Previous re-
search developed discourse schema (Asher et al,
2008) (Somasundaran et al, 2008) and proved that
the utilization of discourse relations could improve
the performance of polarity classification on dia-
logues (Somasundaran et al, 2009). However, cur-
1Defined as ambiguous sentences in this paper
rent state-of-the-art methods for sentence-level po-
larity classification are facing difficulties in ascer-
taining the polarity of some sentences. For example:
(a) [Although Fujimori was criticized by the international
community]?[he was loved by the domestic population]?
[because people hated the corrupted ruling class]. (??
??????????????????????
??????????????????????)
Example (a) is a positive sentence holding a Con-
trast relation between first two segments and a
Cause relation between last two segments. The po-
larity of "criticized", "hated" and "corrupted" are rec-
ognized as negative expressions while "loved" is rec-
ognized as a positive expression. Example (a) is dif-
ficult for existing polarity classification methods for
two reasons: (1) the number of positive expressions
is less than negative expressions; (2) the importance
of each sentiment expression is unknown. However,
consider Figure 1, if we know that the polarity of
the first two segments holding a Contrast relation
is determined by the nucleus (Mann and Thompson,
1988) segment and the polarity of the last two seg-
ments holding aCause relation is also determined by
the nucleus segment, the polarity of the sentence will
be determined by the polarity of "[he...population]".
Thus, the polarity of Example (a) is positive.
Statistics showed that 43% of the opinionated
sentences in NTCIR2 MOAT (Multilingual Opinion
Analysis Task) Chinese corpus3 are ambiguous. Ex-
isting sentence-level polarity classification methods
ignoring discourse structure often give wrong results
for these sentences. We implemented state-of-the-
2http://research.nii.ac.jp/ntcir/
3Including simplified Chinese and traditional Chinese cor-
pus from NTCIR-6 MOAT and NTCIR-7 MOAT
162
Figure 1: Discourse relations for Example (a). (n and s
denote nucleus and satellite segment, respectively)
art method (Xu and Kit, 2010) in NTCIR-8 Chinese
MOAT as the baseline polarity classifier (BPC) in
this paper. Error analysis of BPC showed that 49%
errors came from ambiguous sentences.
In this paper, we focused on the automation of
recognizing intra-sentence level discourse relations
for polarity classification. Based on the previous
work of Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988), a discourse scheme with dis-
course constraints on polarity was defined empiri-
cally (see Section 3). The scheme contains 5 rela-
tions: Contrast, Condition, Continuation, Cause and
Purpose. From a raw corpus, a small set of cue-
phrase-based patterns were used to collect discourse
instances. These instances were then converted to
semantic sequential representations (SSRs). Finally,
an unsupervised SSR learner was adopted to gener-
ate, weigh and filter high quality new SSRs with-
out cue phrases. Experimental results showed that
the proposed methods could effectively recognize
the defined discourse relations and achieve signifi-
cant improvement in sentence-level polarity classi-
fication comparing to BPC.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Sec-
tion 3 presents the discourse scheme with discourse
constraints on polarity. Section 4 gives the detail of
proposed method. Experimental results are reported
and discussed in Section 5 and Section 6 concludes
this paper.
2 Related Work
Research on polarity classification were generally
conducted on 4 levels: document-level (Pang et al,
2002), sentence-level (Riloff et al, 2003), phrase-
level (Wilson et al, 2009) and feature-level (Hu and
Liu, 2004; Xia et al, 2007).
There was little research focusing on the auto-
matic recognition of intra-sentence level discourse
relations for sentiment analysis in the literature.
Polanyi and Zaenen (2006) argued that valence cal-
culation is critically affected by discourse struc-
ture. Asher et al (2008) proposed a shallow se-
mantic representation using a feature structure and
use five types of rhetorical relations to build a fine-
grained corpus for deep contextual sentiment anal-
ysis. Nevertheless, they did not propose a com-
putational model for their discourse scheme. Sny-
der and Barzilay (2007) combined an agreement
model based on contrastive RST relations with a lo-
cal aspect model to make a more informed over-
all decision for sentiment classification. Nonethe-
less, contrastive relations were only one type of dis-
course relations which may help polarity classifica-
tion. Sadamitsu et al (2008) modeled polarity re-
versal using HCRFs integrated with inter-sentence
discourse structures. However, our work is on intra-
sentence level and our purpose is not to find polar-
ity reversals but trying to adapt general discourse
schemes (e.g., RST) to help determine the overall
polarity of ambiguous sentences.
The most closely related works were (Somasun-
daran et al, 2008) and (Somasundaran et al, 2009),
which proposed opinion frames as a representation
of discourse-level associations on dialogue andmod-
eled the scheme to improve opinion polarity clas-
sification. However, opinion frames was difficult
to be implemented because the recognition of opin-
ion target was very challenging in general text. Our
work differs from their approaches in two key as-
pects: (1) we distinguished nucleus and satellite in
discourse but opinion frames did not; (2) our method
for discourse discovery was unsupervised while their
method needed annotated data.
Most research works about discourse classifica-
tion were not related to sentiment analysis. Su-
pervised discourse classification methods (Soricut
and Marcu, 2003; Duverle and Prendinger, 2009)
needed manually annotated data. Marcu and Echi-
habi (2002) presented an unsupervised method to
recognize discourse relations held between arbitrary
spans of text. They showed that lexical pairs ex-
tracted from massive amount of data can have a
major impact on discourse classification. Blair-
Goldensohn et al (2007) extended Marcu's work by
using parameter opitimization, topic segmentation
and syntactic parsing. However, syntactic parsers
163
were usually costly and impractical when dealing
with large scale of text. Thus, in additional to lex-
ical features, we incorporated sequential and seman-
tic information in proposed method for discourse re-
lation classification. Moreover, our method kept the
characteristic of language independent, so it could be
applied to other languages.
3 Discourse Scheme for Eliminating
Polarity Ambiguities
Since not all of the discourse relations in RST
would help eliminate polarity ambiguities, the dis-
course scheme defined in this paper was on a much
coarser level. In order to ascertain which relations
should be included in our scheme, 500 ambigu-
ous sentences were randomly chosen from NTCIR
MOAT Chinese corpus and the most common dis-
course relations for connecting independent clauses
in compound sentences were annotated. We found
that 13 relations from RST occupied about 70% of
the annotated discourse relations which may help
eliminate polarity ambiguities. Inspired by Marcu
and Echihabi (2002), to construct relatively low-
noise discourse instances for unsupervised methods
using cue phrases, we grouped the 13 relations into
the following 5 relations:
Contrast is a union of Antithesis, Concession, Oth-
erwise and Contrast from RST.
Condition is selected from RST.
Continuation is a union of Continuation, Parallel
from RST.
Cause is a union of Evidence, Volitional-Cause,
Nonvolitional-Cause, Volitional-result and
Nonvolitional-result from RST.
Purpose is selected from RST.
The discourse constraints on polarity presented
here were based on the observation of annotated dis-
course instances: (1) discourse instances holding
Contrast relation should contain two segments with
opposite polarities; (2) discourse instances hold-
ing Continuation relation should contain two seg-
ments with the same polarity; (3) the polarity of dis-
course instances holdingContrast,Condition,Cause
or Purpose was determined by the nucleus segment;
(4) the polarity of discourse instances holding Con-
tinuation was determined by either segment.
Relation Cue Phrases(English Translation)
Contrast although1, but2, however2
Condition if1, (if1?then2)
Continuation and, further more,(not only, but also)
Cause because
1, thus2, accordingly2,
as a result2
Purpose in order to
2, in order that2,
so that2
1 means CUE1 and 2 means CUE2
Table 1: Examples of cue phrases
4 Methods
The proposed methods were based on two as-
sumptions: (1) Cue-phrase-based patterns could be
used to find limited number of high quality discourse
instances; (2) discourse relations were determined
by lexical, structural and semantic information be-
tween two segments.
Cue-phrase-based patterns could find only lim-
ited number of discourse instances with high pre-
cision (Marcu and Echihabi, 2002). Therefore, we
could not rely on cue-phrase-based patterns alone.
Moreover, there was no annotated corpus similar to
Penn Discourse TreeBank (Miltsakaki et al, 2004)
in other languages such as Chinese. Thus, we pro-
posed a language independent unsupervised method
to identify discourse relations without cue phrases
while maintaining relatively high precision. For
each discourse relation, we started with several cue-
phrase-based patterns and collected a large number
of discourse instances from raw corpus. Then, dis-
course instances were converted to semantic sequen-
tial representations (SSRs). Finally, an unsupervised
method was adopted to generate, weigh and filter
common SSRswithout cue phrases. Themined com-
mon SSRs could be directly used in our SSR-based
classifier in unsupervised manner or be employed as
effective features for supervised methods.
4.1 Gathering and representing discourse
instances
A discourse instance, denoted by Di, consists of
two successive segments (Di[1], Di[2]) within a sen-
tence. For example:
D1: [Although Boris is very brilliant at math]s, [he
164
BOS... ?[CUE2]...EOS
BOS [CUE1]... ?...EOS
BOS... ?[CUE1]...EOS
BOS [CUE1]... ?[CUE2]...EOS
Table 2: Cue-phrase-based patterns. BOS and EOS de-
noted the beginning and end of two segments.
is a horrible teacher]n
D2: [John is good at basketball]s, [but he lacks team
spirit]n
In D1, "although" indicated the satellite section
while inD2, "but" indicated the nucleus section. Ac-
cordingly, different cue phrases may indicate differ-
ent segment type. Table 1 listed some examples of
cue phrases for each discourse relation. Some cue
phrases were singleton (e.g. "although" and "as a re-
sult") and some were used as a pair (e.g. "not only,
but also"). "CUE1" indicated satellite segments and
"CUE2" indicated nucleus segments. Note that we
did not distinguish satellite from nucleus for Con-
tinuation in this paper because the polarity could be
determined by either segment.
Table 2 listed cue-phrase-based patterns for all re-
lations. To simplify the problem of discourse seg-
mentation, we split compound sentences into dis-
course segments using commas and semicolons. Al-
though we collected discourse instances from com-
pound sentences only, the number of instances for
each discourse relation was large enough for the pro-
posed unsupervised method. Note that we only col-
lected instances containing at least one sentiment
word in each segment.
In order to incorporate lexical and semantic infor-
mation in our method, we represented each word in
a discourse instance using a part-of-speech tag, a se-
mantic label and a sentiment tag. Then, all discourse
instances were converted to SSRs. The rules for con-
verting were as follows:
(1) Cue phrases and punctuations were ingored.
But the information of nucleus(n) and satellite(s)
was preserved.
(2) Adverbs(RB) appearing in sentiment lexicon,
verbs(V ), adjectives(JJ ) and nouns(NN) were repre-
sented by their part-of-speech (pos) tag with seman-
tic label (semlabel) if available.
(3) Named entities (NE; PER: person name;ORG:
organization), pronouns (PRP), and function words
were represented by their corresponding named en-
tity tags and part-of-speech tags, respectively.
(4) Added sentiment tag (P : Positive; N : Nega-
tive) to all sentiment words.
By applying above rules, the SSRs forD1 andD2
would be:
d1: [PERV|Ja01 RB|Ka01 JJ|Ee14|P IN NN|Dk03]s
, [PRP V|Ja01 DT JJ|Ga16|N NN|Ae13 ]n
d2: [PER V|Ja01 JJ|Ee14|P IN NN|Bp12]s, [PRP
V|He15|N NN|Di10 NN|Dd08 ]n
Refer to d1 and d2, "Boris" could match "John"
in SSRs because they were converted to "PER" and
they all appeared at the beginning of discourse in-
stances. "Ja01", "Ee14" etc. were semantic labels
from Chinese synonym list extended version (Che et
al., 2010). There were similar resources in other lan-
guages such asWordnet(Fellbaum, 1998) in English.
The next problem became how to start from current
SSRs and generate new SSRs for recognizing dis-
course relations without cue phrases.
4.2 Mining common SSRs
Recall assumption (2), in order to incorporate lex-
ical, structural and semantic information for the sim-
ilarity calculation of two SSRs holding the same
discourse relation, three types of matches were de-
fined for {(u, v)|u ? di[k], v ? dj[k], k = 1, 2}:
(1)Full match: (i) u = v or (ii) u.pos = v.pos and
u.semlabel=v.semlabel or (iii) u.pos=v.pos and
u had a sentiment tag and v had a sentiment tag or
(iv) u.pos and v.pos?{PRP, PER, ORG} (2) Partial
match: u.pos = v.pos but not Full match; (3) Mis-
match: u.pos ?= v.pos.
Generating common SSRs
Intuitively, a simple way of estimating the simi-
larity between two SSRs was using the number of
mismatches. Therefore, we utilized match(di, dj)
where i ?= j, which integrated the three types of
matches defined above to calculate the number of
mismatches and generate common SSRs. Consider
Table 3, in common SSRs, full matches were pre-
served, partial matches were replaced by part of
speech tags and mismatches were replaced by '*'s.
The common SSRs generated during the calculation
of match(di, dj) consisted of two parts. The first
part was generated by di[1] and dj[1] and the second
part was generated by di[2] and dj[2]. We stipulated
165
d1 d2 mis conf ssr
PER PER 0 0 PER
V|Ja01 V|Ja01 0 0 V|Ja01
RB|Ka01 +1 ?0.298 *
JJ|Ee14|P JJ|Ee14|P 0 0 JJ|Ee14|P
IN IN 0 0 IN
NN|Dk03 NN|Bp12 0 ?0.50 NN
conf(ssr[1]) = ?0.798
PRP PRP 0 0 PRP
V|Ja01 V|He15|N 0 ?0.50 V
DT +1 ?0.184 *
JJ|Ga16|N +1 ?1.0 *
NN|Ae13 NN|Di10 0 ?0.50 NN
NN|Dd08 +1 ?1.0 *
conf(ssr[2]) = ?3.184
Table 3: Calculation of match(d1, d2). ssr denoted
the common SSR between d1 and d2 , conf(ssr[1]) and
conf(ssr[2]) denoted the confidence of ssr.
that di and dj could generate a common SSR if and
only if the orders of nucleus segment and satellite
segment were the same.
In order to guarantee relatively high quality com-
mon SSRs, we empirically set the upper threshold
of the number of mismatches as 0.5 (i.e., ? 1/2 of
the number of words in the generated SSR). It's not
difficult to figure out that the number of mismatches
generated in Table 3 satisfied this requirement. As a
result, for each discourse relation rn, a correspond-
ing common SSR set Sn could be obtained by adopt-
ing match(di, dj) where i ?= j for all discourse in-
stances. An advantage of match(d1, d2) was that
the generated common SSRs preserved the sequen-
tial structure of original discourse instances. And
common SSRs allows us to build high precision dis-
course classifiers (See Section 5).
Weighing and filtering common SSRs
A problem of match(di, dj) was that it ignored
some important information by treating different
mismatches equally. For example, the adverb "very"
in "very brilliant" of D1 was not important for dis-
course recognition. In other words, the number of
mismatches inmatch(di, dj) could not precisely re-
flect the confidence of the generated common SSRs.
Therefore, it was needed to weigh different mis-
matches for the confidence calculation of common
SSRs.
Intuitively, if a partial match or a mismatch (de-
noted by um) occurred very frequently in the gener-
ation of common SSRs, the importance of um tends
to diminish. Inspired by the tf-idf model, given
ssri?Sn, we utilized the following equation to esti-
mate the weight (denoted by wm) of um.
wm = ?ufm ? log (|Sn|/ssrfm )
where ufm denoted the frequency of um during the
generation of ssri, |Sn| denoted the size of Sn and
ssrfm denoted the number of common SSRs in Sn
containing um . All weights were normalized to
[?1, 0).
Nouns (except for named entities) and verbs were
most representative words in discourse recognition
(Marcu and Echihabi, 2002). In addition, adjectives
and adverbs appearing in sentiment lexicons were
important for polarity classification. Therefore, for
these 4 kinds of words, we utilized ?1.0 for a mis-
match and ?0.50 for a partial match.
As we had got the weights for all partial matches
and mismatches, the confidence of ssri?Sn could be
calculated using the cumulation of weights of par-
tial matches and mismatches in ssri[1] and ssri[2].
Recall Table 3, conf(ssr[1]) and conf(ssr[2]) rep-
resented the confidence scores ofmatch(di[1], dj[1])
and match(di[2], dj[2]), respectively. In order to
control the quantity and quality of mined SSRs, a
threshold minconf was introduced. ssri will be
preserved if and only if conf(ssri[1]) ?minconf
and conf(ssri[2]) ? minconf . The value of
minconf was tuned using the development data.
Finally, we combined adjacent '*'s and preserved
SSRs containing at least one notional word and at
least two words in each segment to meet the de-
mand of maintaining high precision (e.g., "[* DT
*]", "[PER *]" will be dropped). Moreover, since
many of the SSRs were duplicated, we ranked all
the generated SSRs according to their occurrences
and dropped those appearing only once in order to
preserve common SSRs. At last, SSRs appearing in
more than one common SSR set were removed for
maintaining the uniqueness of each set. The com-
mon SSR set Sn for each discourse relation rn could
be directly used in SSR-based unsupervised classi-
fiers or be employed as effective features in super-
vised methods.
166
Relation Occurrence
Contrast 86 (8.2%)
Condition 27 (2.6%)
Continuation 445 (42.2%)
Cause 123 (11.7%)
Purpose 55 (5.2%)
Others 318 (30.2%)
Table 4: Distribution of discourse relations on NTC-7.
Others represents discourse relations not included in our
discourse scheme.
5 Experiments
5.1 Annotation work and Data
We extracted all compound sentences which may
contain the defined discourse relations from opinion-
ated sentences (neutral ones were dropped) of NT-
CIR7MOAT simplified Chinese training data. 1,225
discourse instances were extracted and two annota-
tors were trained to annotate discourse relations ac-
cording to the discourse scheme defined in Section 3.
Note that we annotate both explicit and implicit dis-
course relations. The overall inter annotator agree-
ment was 86.05% and the Kappa-value was 0.8031.
Table 4 showed the distribution of annotated dis-
course relations based on the inter-annotator agree-
ment. The proportion of occurrences of each dis-
course relations varied greatly. For example, Con-
tinuation was the most common relation in anno-
tated corpus, but the occurrences of Condition rela-
tion were rare.
The experiments of this paper were performed us-
ing the following data sets:
NTC-7 contained manually annotated discourse
instances (shown in Table 4). The experiments of
discourse identification were performed on this data
set.
NTC-8 contained all opinionated sentences (neu-
tral ones were dropped) extracted from NTCIR8
MOAT simplified Chinese test data. The experi-
ments of polarity ambiguity elimination using the
identified discourse relations were performed on this
data set.
XINHUA contained simplified Chinese raw news
text from Xinhua.com (2002-2005). A word seg-
mentation tool, a part-of-speech tagging tool, a
named entity recognizer and a word sense disam-
biguation tool (Che et al, 2010) were adopted to all
sentences. The common SSRs were mined from this
data set.
5.2 Experimental Settings
Discourse relation identification
In order to systematically justify the effectiveness
of proposed unsupervised method, following exper-
iments were performed on NTC-7:
Baseline used only cue-phrase-based patterns.
M&E proposed by Marcu and Echihabi (2002).
Given a discourse instance Di, the probabilities:
P (rk|(Di[1], Di[2])) for each relation rk were esti-
mated on all text from XINHUA. Then, the most
likely discourse relation was determined by taking
the maximum over argmaxk{P (rk|(Di[1], Di[2])}.
cSSR used both cue-phrase-based patterns to-
gether with common SSRs for recognizing discourse
relations. Common SSRs were mined from dis-
course instances extracted fromXINHUAusing cue-
phrase-based patterns. Development data were ran-
domly selected for tuning minconf .
SVM was trained utilizing cue phrases, probabil-
ities from M&E, topic similarity, structure overlap,
polarity of segments and mined common SSRs (Op-
tional). The parameters of the SVM classifier were
set by a grid search on the training set. We performed
4-fold cross validation on NTC-7 to get an average
performance.
The purposes of introducing SVM in our experi-
ment were: (1) to compare the performance of cSSR
to supervised method; (2) to examine the effective-
ness of integrating common SSRs as features for su-
pervised methods.
Polarity ambiguity elimination
BPC was trained mainly utilizing punctuation,
uni-gram, bi-gram features with confidence score
output. Discourse classifiers such as Baseline, cSSR
or SVM were adopted individually for the post-
processing of BPC. Given an ambiguous sentence
which contained more than one segment, an intuitive
three-step method was adopted to integrated a dis-
course classifier and discourse constraints on polar-
ity for the post-processing of BPC:
(1) Recognize all discourse relations together with
nucleus and satellite information using a discourse
classifier. The nucleus and satellite information is
167
Figure 2: Influences of different values of minconf to
the performance of cSSR
acquired by cSSR if a segment pair could match a
cSSR. Otherwise, we use the annotated nucleus and
satellite information.
(2) Apply discourse constraints on polarity to
ascertain the polarity for each discourse instance.
There may be conflicts between polarities acquired
by BPC and discourse constraints on polarity (e.g.,
Two segments with the same polarity holding a Con-
trast relation). To handle this problem, we chose
the segment with higher polarity confidence and ad-
justed the polarity of the other segment using dis-
course constraints on polarity.
(3) If there was more than one discourse instance
in a single sentence, the overall polarity of the sen-
tence was determined by voting of polarities from
each discourse instance under the majority rule.
5.3 Experimental Results
Refer to Figure 2, the performance of cSSR was
significantly affected by minconf . Note that we
performed the tuning process ofminconf on differ-
ent development data (1/4 instances randomly se-
lected from NTC-7) and Figure 2 showed the av-
erage performance. cSSR became Baseline when
minconf =0. A significant drop of precision was
observed when minconf was less than ?2.5. The
recall remained around 0.495 when minconf ?
?4.0. The best performance was observed when
minconf=?3.5. As a result, ?3.5 was utilized as
the threshold value for cSSR in the following exper-
iments.
Table 5 presented the experimental results for dis-
course relation classification. it showed that:
(1) Cue-phrase-based patterns could find only lim-
ited number of discourse relations (34.1% of average
BPC Baseline cSSR SVM+SSRs
Precision 0.7661 0.7982 0.8059 0.8113
Recall 0.7634 0.7957 0.8038 0.8091
F-score 0.7648 0.7970 0.8048 0.8102
Table 6: Performance of integrating discourse classifiers
and constraints to polarity classification. Note that the
experiments were performed on NTC-8 which contained
only opinionated sentences.
recall) with a very high precision (96.17% of average
precision). This is a proof of assumption (1) given
in Section 4. On the other side, M&E which only
considered word pairs between two segments of dis-
course instances got a higher recall with a large drop
of precision. The drop of precision may be caused
by the neglect of structural and semantic information
of discourse instances. However, M&E still outper-
formed Baseline in average F -score.
(2) cSSR enhanced Baseline by increasing the av-
erage recall by about 15% with only a small drop of
precision. The performance of cSSR demonstrated
that our method could effectively discover high qual-
ity common SSRs. The most remarkable improve-
ment was observed on Continuation in which the re-
call increased by almost 20% with only a minor drop
of precision. Actually, cSSR outperformed Baseline
in all discourse relations except forContrast. In Dis-
course Tree Bank (Carlson et al, 2001) only 26%
of Contrast relations were indicated by cue phrases
while in NTC-7 about 70% of Contrast were indi-
cated by cue phrases. A possible reason was that
we were dealing with Chinese news text which were
usually well written. Another important observation
was that the performance of cSSR was very close to
the result of SVM.
(3) SVM+SSRs achieved the best F -score on
Continuation and average performance. The integra-
tion of SSRs to the feature set of SVM contributed to
a remarkable increase in average F -score. The re-
sults of cSSR and SVM+SSRs demonstrated the ef-
fectiveness of common SSRs mined by the proposed
unsupervised method.
Table 6 presented the performance of integrat-
ing discourse classifiers to polarity classification.
For Baseline and cSSR, the information of nucleus
and satellite could be obtained directly from cue-
168
Relation Baseline M&E cSSR SVM SVM+SSRs
Contrast
P 0.9375 0.4527 0.7531 0.9375 0.9375
R 0.6977 0.7791 0.7093 0.6977 0.6977
F 0.8000 0.5726 0.7305 0.8000 0.8000
Condition
P 1.0000 0.4444 0.6774 1.0000 0.7083
R 0.5556 0.8889 0.7778 0.5185 0.6296
F 0.7143 0.5926 0.7241 0.6829 0.6667
Continuation
P 0.9831 0.6028 0.9761 0.6507 0.7266
R 0.2607 0.5865 0.4584 0.6697 0.6629
F 0.4120 0.5945 0.6239 0.6600 0.6933
Cause
P 1.0000 0.5542 0.9429 1.0000 0.9412
R 0.2114 0.3740 0.2683 0.2114 0.2602
F 0.3489 0.4466 0.4177 0.3489 0.4076
Purpose
P 0.8947 0.3704 0.8163 0.9167 0.7193
R 0.6182 0.7273 0.7273 0.6000 0.7455
F 0.7312 0.4908 0.7692 0.7253 0.7321
Average
P 0.9617 0.5302 0.8864 0.7207 0.7607
R 0.3410 0.5951 0.4878 0.5856 0.6046
F 0.5035 0.5608 0.6293 0.6461 0.6737
Table 5: Performance of recognizing discourse relations. (The evaluation criteria are Precision, Recall and F-score)
phrase-based patterns and SSRs, respectively. For
SVM+cSSR, the nucleus and satellite information
was acquired by cSSR if a segment pair could match
a cSSR. Otherwise, we used manually annotated nu-
cleus and satellite information. It's clear that the
performance of polarity classification was enhanced
with the improvement of discourse relation recogni-
tion. M&E was not included in this experiment be-
cause the performance of polarity classification was
decreased by the mis-classified discourse relations.
SVM+SSRs achieved significant (p<0.01) improve-
ment in polarity classification compared to BPC.
5.4 Discussion
Effect of weighing and filtering
To assess the contribution of weighing and filter-
ing in mining SSRs using a minimum confidence
threshold, i.e. minconf , we implemented cSSR?
without weighing and filtering on the same data set.
Consider Table 7, cSSR achieved obvious improve-
ment in Precision and F -score than cSSR?. More-
over, the total number of SSRs was greatly reduced
in cSSR with only a minor drop of recall. This was
because cSSR? was affected by thousands of low
quality common SSRs which would be filtered in
cSSR. The result in Table 7 proved that weighing and
cSSR? cSSR
Precision 0.6182 0.8864
Recall 0.5014 0.4878
F-score 0.5537 0.6293
NOS > 1 million ? 0.12 million
Table 7: Comparison of cSSR? and cSSR. "NOS" denoted
the number of mined common SSRs.
filtering were essential in our proposed method.
We further analyzed how the improvement was
achieved in cSSR. In our experiment, the most com-
mon mismatches were auxiliary words, named enti-
ties, adjectives or adverbs without sentiments (e.g.,
"green", "very", etc.), prepositions, numbers and
quantifiers. It's straightforward that these words
were insignificant in discourse relation classification
purpose. Moreover, these words did not belong to
the 4 kinds of most representative words. In other
words, the weights of most mismatches were calcu-
lated using the equation presented in Section 4.2 in-
stead of utilizing a unified value, i.e. ?1. Recall
Table 3, the weight of "RB|Ka01" (original: "very")
was ?0.298 and "DT" (original: 'a') was ?0.184.
Comparing to the weights of mismatches for most
representative words (?1.0), the proposed method
successfully down weighed the words which were
169
Figure 3: Improvement from individual discourse rela-
tions. N denoted the number of ambiguities eliminated.
not important for discourse identification. There-
fore, weighing and filtering were able to preserve
high quality SSRs while filter out low quality SSRs
by setting the confidence threshold, i.e. minconf .
Contribution of different discourse relations
We also analyzed the contribution of different dis-
course relations in eliminating polarity ambiguities.
Refer to Figure 3, the improvement of polarity classi-
fication mainly came from three discourse relations:
Contrast, Continuation and Cause. It was straight-
forward that Contrast relation could eliminate po-
larity ambiguities because it held between two seg-
ments with opposite polarities. The contribution of
Cause relation also result from two segments holding
different polarities such as example (a) in Section 1.
However, recall Table 4, although Cause occurred
more often than Contrast, only a part of discourse
instances holding Cause relation contained two seg-
ments with the opposite polarities. Another impor-
tant relation in eliminating ambiguity was Continu-
ation. We investigated sentences with polarities cor-
rected by Continuation relation. Most of them fell
into two categories: (1) sentences with mistakenly
classified sentiments by BPC; (2) sentences with im-
plicit sentiments. For example:
(b) [France and Germany have banned human cloning at
present]?[on 20th, U.S. President George W. Bush called
for regulations of the same content to Congress] (???
????????????????????? 20
???????????????????)
The first segment of example (b) was negative
("banned" expressed a negative sentiment) and a
Continuation relation held between these two seg-
ments. Consequently, the polarity of the second seg-
ment should be negative.
6 Conclusions and Future work
This paper focused on unsupervised discovery
of intra-sentence discourse relations for sentence
level polarity classification. We firstly presented a
discourse scheme based on empirical observations.
Then, an unsupervised method was proposed start-
ing from a small set of cue-phrase-based patterns to
mine high quality common SSRs for each discourse
relation. The performance of discourse classification
was further improved by employing SSRs as features
in supervisedmethods. Experimental results showed
that our methods not only effectively recognized dis-
course relations but also achieved significant im-
provement (p<0.01) in sentence level polarity clas-
sification. Although we were dealing with Chinese
text, the proposed unsupervised method could be
easily generalized to other languages.
The future work will be focused on (1) integrating
more semantic and syntactic information in proposed
unsupervised method; (2) extending our method to
inter-sentence level and then jointly modeling intra-
sentence level and inter-sentence level discourse
constraints on polarity to reach a global optimal in-
ference for polarity classification.
Acknowledgments
This work is partially supported by National 863
program of China (Grant No. 2009AA01Z150),
the Innovation and Technology Fund of Hong Kong
SAR (Project No. GHP/036/09SZ) and 2010/11
CUHK Direct Grants (Project No. EE09743).
References
N. Asher, F. Benamara, and Y.Y. Mathieu. 2008. Distill-
ing opinion in discourse: A preliminary study. Coling
2008: Companion volume: Posters and Demonstra-
tions, pages 5--8.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and refining rhetorical-semantic
relationmodels. InProceedings of NAACLHLT, pages
428--435.
L. Carlson, D.Marcu, andM.E. Okurowski. 2001. Build-
ing a discourse-tagged corpus in the framework of
170
rhetorical structure theory. In Proceedings of the Sec-
ond SIGdial Workshop on Discourse and Dialogue-
Volume 16, pages 1--10. Association for Computa-
tional Linguistics.
W. Che, Z. Li, and T. Liu. 2010. Ltp: A chinese language
technology platform. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Demonstrations, pages 13--16. Association for Com-
putational Linguistics.
D.A. Duverle and H. Prendinger. 2009. A novel dis-
course parser based on support vector machine classi-
fication. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2, pages 665--673. Associ-
ation for Computational Linguistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168--177. ACM.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text organi-
zation. Text-Interdisciplinary Journal for the Study of
Discourse, 8(3):243--281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 368--375. Associa-
tion for Computational Linguistics.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
The penn discourse treebank. InProceedings of the 4th
International Conference on Language Resources and
Evaluation. Citeseer.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing-
Volume 10, pages 79--86. Association for Computa-
tional Linguistics.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: The-
ory and applications, pages 1--10.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In Proceedings of the seventh conference on Natu-
ral language learning at HLT-NAACL 2003-Volume 4,
pages 25--32. Association for Computational Linguis-
tics.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008. Sen-
timent analysis based on probabilistic models using
inter-sentence information.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings of
NAACL HLT, pages 300--307.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 801--808. Association for
Computational Linguistics.
S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor.
2009. Supervised and unsupervised methods in em-
ploying discourse relations for improving opinion po-
larity classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 170--179. Association for Compu-
tational Linguistics.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 149--156. Association for Computational Lin-
guistics.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399--433.
Y.Q. Xia, R.F. Xu, K.F. Wong, and F. Zheng. 2007. The
unified collocation framework for opinion mining. In
International Conference on Machine Learning and
Cybernetics, volume 2, pages 844--850. IEEE.
R. Xu and C. Kit. 2010. Incorporating feature-based and
similarity-based opinion mining--ctl in ntcir-8 moat.
InProceedings of the 8th NTCIRWorkshop, pages 276-
-281.
171
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1159?1168,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploiting Community Emotion for Microblog Event Detection
Gaoyan Ou
1,2
, Wei Chen
1,2,
, Tengjiao Wang
1,2
, Zhongyu Wei
1,3
,
Binyang Li
4
, Dongqing Yang
1,2
and Kam-Fai Wong
1,3
1
Key Laboratory of High Confidence Software Technologies, Ministry of Education, China
2
School of Electronics Engineering and Computer Science, Peking University, China
3
Shenzhen Research Institute, The Chinese University of Hong Kong, China
4
Dept. of Information Science & Technology, University of International Relations, China
pekingchenwei@pku.edu.cn
Abstract
Microblog has become a major plat-
form for information about real-world
events. Automatically discovering real-
world events from microblog has attracted
the attention of many researchers. Howev-
er, most of existing work ignore the impor-
tance of emotion information for event de-
tection. We argue that people?s emotion-
al reactions immediately reflect the occur-
ring of real-world events and should be im-
portant for event detection. In this study,
we focus on the problem of community-
related event detection by community e-
motions. To address the problem, we pro-
pose a novel framework which include
the following three key components: mi-
croblog emotion classification, community
emotion aggregation and community emo-
tion burst detection. We evaluate our ap-
proach on real microblog data sets. Exper-
imental results demonstrate the effective-
ness of the proposed framework.
1 Introduction
Microblog has become a popular and convenient
platform for people to share information about so-
cial events in real time. When an external even-
t occurs, it will be quickly propagated between
microblog users. During propagation process of
an event, sufficient amount of users will express
their emotions. Taking Sina Weibo
1
as an exam-
ple, more than 12 percent of users use emoticons
2
when reposting an event-related microblog mes-
sage.
The emotion information can not only help us
better understand a given event, but also be u-
tilized to discover new events. Figure 1 shows
1
http://weibo.com/
2
An icon to indicate user?s emotion, as shown in Table 1.
 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 58?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Study on Uncertainty Identification in Social Media Context
Zhongyu Wei1, Junwen Chen1, Wei Gao2,
Binyang Li1, Lanjun Zhou1, Yulan He3, Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
3School of Engineering & Applied Science, Aston University, Birmingham, UK
{zywei,jwchen,byli,ljzhou,kfwong}@se.cuhk.edu.hk
wgao@qf.org.qa, y.he@cantab.net
Abstract
Uncertainty text detection is important
to many social-media-based applications
since more and more users utilize social
media platforms (e.g., Twitter, Facebook,
etc.) as information source to produce
or derive interpretations based on them.
However, existing uncertainty cues are in-
effective in social media context because
of its specific characteristics. In this pa-
per, we propose a variant of annotation
scheme for uncertainty identification and
construct the first uncertainty corpus based
on tweets. We then conduct experiments
on the generated tweets corpus to study the
effectiveness of different types of features
for uncertainty text identification.
1 Introduction
Social media is not only a social network tool for
people to communicate but also plays an important
role as information source with more and more
users searching and browsing news on it. People
also utilize information from social media for de-
veloping various applications, such as earthquake
warning systems (Sakaki et al, 2010) and fresh
webpage discovery (Dong et al, 2010). How-
ever, due to its casual and word-of-mouth pecu-
liarities, the quality of information in social me-
dia in terms of factuality becomes a premier con-
cern. Chances are there for uncertain information
or even rumors flooding in such a context of free
form. We analyzed a tweet dataset which includes
326,747 posts (Details are given in Section 3) col-
lected during 2011 London Riots, and result re-
veals that at least 18.91% of these tweets bear un-
certainty characteristics1. Therefore, distinguish-
ing uncertain statements from factual ones is cru-
cial for users to synthesize social media informa-
tion to produce or derive reliable interpretations,
1The preliminary study was done based on a manually de-
fined uncertainty cue-phrase list. Tweets containing at least
one hedge cue were treated as uncertain.
and this is expected helpful for applications like
credibility analysis (Castillo et al, 2011) and ru-
mor detection (Qazvinian et al, 2011) based on
social media.
Although uncertainty has been studied theoret-
ically for a long time as a grammatical phenom-
ena (Seifert and Welte, 1987), the computational
treatment of uncertainty is a newly emerging area
of research. Szarvas et al (2012) pointed out that
?Uncertainty - in its most general sense - can be
interpreted as lack of information: the receiver of
the information (i.e., the hearer or the reader) can-
not be certain about some pieces of information?.
In recent years, the identification of uncertainty
in formal text, e.g., biomedical text, reviews or
newswire, has attracted lots of attention (Kilicoglu
and Bergler, 2008; Medlock and Briscoe, 2007;
Szarvas, 2008; Light et al, 2004). However, un-
certainty identification in social media context is
rarely explored.
Previous research shows that uncertainty identi-
fication is domain dependent as the usage of hedge
cues varies widely in different domains (Morante
and Sporleder, 2012). Therefore, the employment
of existing out-of-domain corpus to social media
context is ineffective. Furthermore, compared to
the existing uncertainty corpus, the expression of
uncertainty in social media is fairly different from
that in formal text in a sense that people usu-
ally raise questions or refer to external informa-
tion when making uncertain statements. But, nei-
ther of the uncertainty expressions can be repre-
sented based on the existing types of uncertainty
defined in the literature. Therefore, a different un-
certainty classification scheme is needed in social
media context.
In this paper, we propose a novel uncertainty
classification scheme and construct the first uncer-
tainty corpus based on social media data ? tweets
in specific here. And then we conduct experi-
ments for uncertainty post identification and study
the effectiveness of different categories of features
based on the generated corpus.58
2 Related work
We introduce some popular uncertainty corpora
and methods for uncertainty identification.
2.1 Uncertainty corpus
Several text corpora from various domains have
been annotated over the past few years at different
levels (e.g., expression, event, relation, sentence)
with information related to uncertainty.
Sauri and Pustejovsky (2009) presented a cor-
pus annotated with information about the factu-
ality of events, namely Factbank, which is con-
structed based on TimeBank2 containing 3,123 an-
notated sentences from 208 news documents with
8 different levels of uncertainty defined.
Vincze et al (2008) constructed the BioSocpe
corpus, which consists of medical and biological
texts annotated for negation, uncertainty and their
linguistic scope. This corpus contains 20,924 sen-
tences.
Ganter et al (2009) generated Wikipedia
Weasels Corpus, where Weasel tags in Wikipedia
articles is adopted readily as labels for uncertainty
annotation. It contains 168,923 unique sentences
with 437 weasel tags in total.
Although several uncertainty corpora exist,
there is not a uniform set of standard for uncer-
tainty annotation. Szarvas et al (2012) normal-
ized the annotation of the three corpora aforemen-
tioned. However, the context of these corpora
is different from that of social media. Typically,
these documents annotated are grammatically cor-
rect, carefully punctuated, formally structured and
logically expressed.
2.2 Uncertainty identification
Previous work on uncertainty identification fo-
cused on classifying sentences into uncertain
or definite categories. Existing approaches are
mainly based on supervised methods (Light et
al., 2004; Medlock and Briscoe, 2007; Medlock,
2008; Szarvas, 2008) using the annotated corpus
with different types of features including Part-Of-
Speech (POS) tags, stems, n-grams, etc..
Classification of uncertain sentences was con-
solidated as a task in the 2010 edition of CoNLL
shared task on learning to detect hedge cues
and their scope in natural language text (Farkas
et al, 2010). The best system for Wikipedia
data (Georgescul, 2010) employed Support Vector
Machine (SVM), and the best system for biolog-
ical data (Tang et al, 2010) adopted Conditional
2http://www.timeml.org/site/timebank/
timebank.html
Random Fields (CRF).
In our work, we conduct an empirical study of
uncertainty identification on tweets dataset and ex-
plore the effectiveness of different types of fea-
tures (i.e., content-based, user-based and Twitter-
specific) from social media context.
3 Uncertainty corpus for microblogs
3.1 Types of uncertainty in microblogs
Traditionally, uncertainty can be divided into
two categories, namely Epistemic and Hypothet-
ical (Kiefer, 2005). For Epistemic, there are two
sub-classes Possible and Probable. For Hypotheti-
cal, there are four sub-classes including Investiga-
tion, Condition, Doxastic andDynamic. The detail
of the classification is described as below (Kiefer,
2005):
Epistemic: On the basis of our world knowledge
we cannot decide at the moment whether the
statement is true or false.
Hypothetical: This type of uncertainty includes
four sub-classes:
? Doxastic: Expresses the speaker?s be-
liefs and hypotheses.
? Investigation: Proposition under inves-
tigation.
? Condition: Proposition under condi-
tion.
? Dynamic: Contains deontic, disposi-
tional, circumstantial and buletic modal-
ity.
Compared to the existing uncertainty corpora,
social media authors enjoy free form of writing.
In order to study the difference, we annotated a
small set of 827 randomly sampled tweets accord-
ing to the scheme of uncertainty types above, in
which we found 65 uncertain tweets. And then,
we manually identified all the possible uncertain
tweets, and found 246 really uncertain ones out of
these 827 tweets, which means that 181 uncertain
tweets are missing based on this scheme. We have
the following three salient observations:
? Firstly, there is no tweet found with the type of
Investigation. We find people seldom use words
like ?examine? or ?test? (indicative words of In-
vestigation category) when posting tweets. Once
they do this, the statement should be considered
as highly certain. For example, @dobibid I have
tested the link, it is fake!
? Secondly, people frequently raise questions
about some specific topics for confirmation which
expresses uncertainty. For example, @ITVCentral59
Can you confirm that Birmingham children?s hos-
pital has/hasn?t been attacked by rioters?
? Thirdly, people tend to post message with exter-
nal information (e.g., story from friends) which re-
veals uncertainty. For example, Friend who works
at the children?s hospital in Birmingham says the
riot police are protecting it.
Based on these observations, we propose a vari-
ant of uncertainty types in social media context
by eliminating the category of Investigation and
adding the category of Question and External un-
der Hypothetical, as shown in Table 3.1. Note
that our proposed scheme is based on Kiefer?s
work (2005) which was previously extended to
normalize uncertainty corpora in different genres
by Szarvas et al (2012). But we did not try these
extended schema for specific genres since even the
most general one (Kiefer, 2005) was proved un-
suitable for social media context.
3.2 Annotation result
The dataset we annotated was collected from Twit-
ter using Streaming API during summer riots
in London during August 6-13 2011, including
326,747 tweets in total. Search criteria include
hashtags like #ukriots, #londonriots, #prayforlon-
don, and so on. We further extracted the tweets
relating to seven significant events during the riot
identified by UK newspaper The Guardian from
this set of tweets. We annotated all the 4,743 ex-
tracted tweets for the seven events3.
Two annotators were trained to annotate the
dataset independently. Given a collection of
tweets T = {t1, t2, t3...tn}, the annotation task is
to label each tweet ti as either uncertain or cer-
tain. Uncertainty assertions are to be identified
in terms of the judgements about the author?s in-
tended meaning rather than the presence of uncer-
tain cue-phrase. For those tweets annotated as un-
certain, sub-class labels are also required accord-
ing to the classification indicated in Table 3.1 (i.e.,
multi-label is allowed).
The Kappa coefficient (Carletta, 1996) indi-
cating inter-annotator agreement was 0.9073 for
the certain/uncertain binary classification and was
0.8271 for fine-grained annotation. The conflict
labels from the two annotators were resolved by a
third annotator. Annotation result is displayed in
Table 3.2, where 926 out of 4,743 tweets are la-
beled as uncertain accounting for 19.52%. Ques-
tion is the uncertainty category with most tweets,
followed by External. Only 21 tweets are labeled
3http://www.guardian.co.uk/
uk/interactive/2011/dec/07/
london-riots-twitter
Tweet# 4743
Uncertainty# 926
Epistemic Possible# 16Probable# 129
Hypothetical
Condition# 71
Doxastic# 48
Dynamic# 21
External# 208
Question# 488
Table 2: Statistics of annotation result
as Dynamic and all of them are buletic modal-
ity4 which shares similarity with Doxastic. There-
fore, we consider Dynamic together with Domes-
tic in the error analysis for simplicity. During
the preliminary annotation, we found that uncer-
tainty cue-phrase is a good indicator for uncer-
tainty tweets since tweets labeled as uncertain al-
ways contain at least one cue-phrase. Therefore,
annotators are also required identify cue-phrases
which trigger the sense of uncertainty in the tweet.
All cue-phrases appearing more than twice are col-
lected to form a uncertainty cue-phrase list.
4 Experiment and evaluation
We aim to identify those uncertainty tweets from
tweet collection automatically based on machine
learning approaches. In addition to n-gram fea-
tures, we also explore the effectiveness of three
categories of social media specific features includ-
ing content-based, user-based and Twitter-specific
ones. The description of the three categories of
features is shown in Table 4. Since the length of
tweet is relatively short, we therefore did not carry
out stopwords removal or stemming.
Our preliminary experiments showed that com-
bining unigrams with bigrams and trigrams gave
better performance than using any one or two of
these three features. Therefore, we just report the
result based on the combination of them as n-gram
features. Five-fold cross validation is used for
evaluation. Precision, recall and F-1 score of un-
certainty category are used as the metrics.
4.1 Overall performance
The overall performance of different approaches
is shown in Table 4.1. We used uncertainty cue-
phrase matching approach as baseline, denoted
by CP. For CP, we labeled tweets containing at
least one entry in uncertainty cue-phrase list (de-
scribed in Section 3) as uncertain. All the other
approaches are supervised methods using SVM
based on different feature sets. n-gram stands for
n-gram feature set, C means content-based feature
set, U denotes user-based feature set, T represents
4Proposition expresses plans, intentions or desires.60
Category Subtype Cue Phrase Example
Epistemic Possible, etc. may, etc. It may be raining.Probable likely, etc. It is probably raining.
Hypothetical
Condition if, etc. If it rains, we?ll stay in.
Doxastic believe, etc. He believes that the Earth is flat.
Dynamic hope, etc. fake picture of the london eye on fire... i hope
External someone said, etc. Someone said that London zoo was attacked.
Question seriously?, etc. Birmingham riots are moving to the children hospital?! seriously?
Table 1: Classification of uncertainty in social media context
Category Name Description
Content-based
Length Length of the tweet
Cue Phrase Whether the tweet contains a uncertainty cue
OOV Ratio Ratio of words out of vocabulary
Twitter-specific
URL Whether the tweet contains a URL
URL Count Frequency of URLs in corpus
Retweet Count How many times has this tweet been retweeted
Hashtag Whether the tweet contains a hashtag
Hashtag Count Number of Hashtag in tweets
Reply Is the current tweet a reply tweet
Rtweet Is the current tweet a retweet tweet
User-based
Follower Count Number of follower the user owns
List Count Number of list the users owns
Friend Count Number of friends the user owns
Favorites Count Number of favorites the user owns
Tweet Count Number of tweets the user published
Verified Whether the user is verified
Table 3: Feature list for uncertainty classification
Approach Precision Recall F-1
CP 0.3732 0.9589 0.5373
SVMn?gram 0.7278 0.8259 0.7737
SVMn?gram+C 0.8010 0.8260 0.8133
SVMn?gram+U 0.7708 0.8271 0.7979
SVMn?gram+T 0.7578 0.8266 0.7907
SVMn?gram+ALL 0.8162 0.8269 0.8215
SVMn?gram+Cue Phrase 0.7989 0.8266 0.8125SVMn?gram+Length 0.7372 0.8216 0.7715SVMn?gram+OOV Ratio 0.7414 0.8233 0.7802
Table 4: Result of uncertainty tweets identification
Twitter-specific feature set and ALL is the combi-
nation of C, U and T.
Table 4.1 shows that CP achieves the best recall
but its precision is the lowest. The learning based
methods with different feature sets give some sim-
ilar recalls. Compared to CP, SVMn?gram in-
creases the F-1 score by 43.9% due to the salient
improvement on precision and small drop of re-
call. The performance improves in terms of pre-
cision and F-1 score when the feature set is ex-
panded by adding C, U or T onto n-gram, where
+C brings the highest gain, and SVMn?gram+ALL
performs best in terms of precision and F-1 score.
We then study the effectiveness of the three
content-based features, and result shows that the
presence of uncertain cue-phrase is most indica-
tive for uncertainty tweet identification.
4.2 Error analysis
We analyze the prediction errors based on
SVMn?gram+ALL. The distribution of errors in
terms of different types of uncertainty is shown
Type Poss. Prob. D.&D. Cond. Que. Ext.
Total# 16 129 69 71 488 208
Error# 11 20 18 11 84 40
% 0.69 0.16 0.26 0.15 0.17 0.23
Table 5: Error distributions
in Table 4.2. Our method performs worst on the
type of Possible and on the combination of Dy-
namic and Doxastic because these two types have
the least number of samples in the corpus and the
classifier tends to be undertrained without enough
samples.
5 Conclusion and future work
In this paper, we propose a variant of classification
scheme for uncertainty identification in social me-
dia and construct the first uncertainty corpus based
on tweets. We perform uncertainty identification
experiments on the generated dataset to explore
the effectiveness of different types of features. Re-
sult shows that the three categories of social media
specific features can improve uncertainty identifi-
cation. Furthermore, content-based features bring
the highest improvement among the three and the
presence of uncertain cue-phrase contributes most
for content-based features.
In future, we will explore to use uncertainty
identification for social media applications.
6 Acknowledgement
This work is partially supported by General Re-
search Fund of Hong Kong (No. 417112).61
References
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249?254.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th International Conference
on World Wide Web, pages 675?684.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 331?340. ACM.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The conll-
2010 shared task: learning to detect hedges and their
scope in natural language text. In Proceedings of
the 14th Conference on Computational Natural Lan-
guage Learning?Shared Task, pages 1?12. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009, pages 173?
176. Association for Computational Linguistics.
Maria Georgescul. 2010. A hedgehop over a max-
margin framework using hedge cues. In Proceed-
ings of the 14th Conference on Computational Natu-
ral Language Learning?Shared Task, pages 26?31.
Association for Computational Linguistics.
Ferenc Kiefer. 2005. Lehetoseg es szuk-
segszeruseg[Possibility and necessity]. Tinta Kiado,
Budapest.
H. Kilicoglu and S. Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a
linguistically motivated perspective. BMC bioinfor-
matics, 9(Suppl 11):S10.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings
of BioLink 2004 workshop on linking biological lit-
erature, ontologies and databases: tools for users,
pages 17?24.
B. Medlock and T. Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific litera-
ture. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
992?999.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational Linguistics, 38(2):223?260.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589?1599.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, pages 851?860. ACM.
R. Saur?? and J. Pustejovsky. 2009. Factbank: A cor-
pus annotated with event factuality. Language Re-
sources and Evaluation, 43(3):227?268.
Stephan Seifert and Werner Welte. 1987. A basic bib-
liography on negation in natural language, volume
313. Gunter Narr Verlag.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the 14th Conference on Compu-
tational Natural Language Learning?Shared Task,
pages 13?17. Association for Computational Lin-
guistics.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and
J. Csirik. 2008. The bioscope corpus: biomedical
texts annotated for uncertainty, negation and their
scopes. BMC bioinformatics, 9(Suppl 11):S9.
62
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 97?102,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Web Information Mining and Decision Support Platform for the  
Modern Service Industry 
 
Binyang Li1,2, Lanjun Zhou2,3, Zhongyu Wei2,3, Kam-fai Wong2,3,4,  
Ruifeng Xu5, Yunqing Xia6 
 
1 Dept. of Information Science & Technology, University of International Relations, China 
2Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong 
Kong, Shatin, N.T., Hong Kong  
3MoE Key Laboratory of High Confidence Software Technologies, China 
4 Shenzhen Research Institute, The Chinese University of Hong Kong 
5Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China 
6Department of Computer Science & Technology, TNList, Tsinghua University, China 
{byli,ljzhou,zywei,kfwong}@se.cuhk.edu.hk  
 
 
Abstract 
This demonstration presents an intelligent infor-
mation platform MODEST. MODEST will pro-
vide enterprises with the services of retrieving 
news from websites, extracting commercial in-
formation, exploring customers? opinions, and 
analyzing collaborative/competitive social net-
works. In this way, enterprises can improve the 
competitive abilities and facilitate potential col-
laboration activities. At the meanwhile, MOD-
EST can also help governments to acquire in-
formation about one single company or the entire 
board timely, and make prompt strategies for 
better support. Currently, MODEST is applied to 
the pillar industries of Hong Kong, including 
innovative finance, modem logistics, information 
technology, etc. 
1 Introduction 
With the rapid development of Web 2.0, the 
amount of information is exploding. There are 
millions of events towards companies and bil-
lions of opinions on products generated every 
day (Liu, 2012). Such enormous information 
cannot only facilitate companies to improve their 
competitive abilities, but also help government to 
make prompt decisions for better support or 
timely monitor, e.g. effective risk management. 
For this reason, there is a growing demand of 
Web information mining and intelligent decision 
support services for the industries. Such services 
are collectively referred as modern service, 
which includes the following requirements: 
(1) To efficiently retrieve relevant information 
from the websites; 
(2) To accurately determine the latest business 
news and trends of the company; 
(3) To identify and analyze customers? opinions 
towards the company; 
(4) To explore the collaborative and competitive 
relationship with other companies; 
(5) To leverage the knowledge mined from the 
business news and company social network 
for decision support. 
In this demonstration, we will present a Web 
information mining and decision support plat-
form, MODEST1. The objective of MODEST is 
to provide modern services for both enterprises 
and government, including collecting Web in-
formation, making deep analysis, and providing 
supporting decision. The innovation of MOD-
EST is focusing on deep analysis which incor-
porates the following functions: 
? Topic detection and tracking function is to 
cluster the hot events and capture the rela-
tionship between the relevant events based on 
the collected data from websites (event also 
referred as topic in this paper). In order to re-
alize this function, Web mining techniques 
are adopted, e.g. topic clustering, heuristics 
algorithms, etc. 
? The second function is to identify and analyze 
customers? opinions about the company. 
Opinion mining technology (Zhou et al., 2010) 
is adopted to determine the polarity of those 
news, which can help the company timely and 
appropriately adjust the policy to strengthen 
the dominant position or avoid risks. 
                                                          
1 This work is supported by the Innovation and Technology 
Fund of Hong Kong SAR. 
97
? The third function is to explore and analyze 
social network based on the company centric. 
We utilize social network analysis (SNA) 
technology (Xia et al., 2010) to discover the 
relationships, and we further analyze the con-
tent in fine-grained granularity to identify its 
potential partners or competitors.  
With the help of MODEST, the companies can 
acquire modern service-related information, and 
timely adjust corporate policies and marketing 
plan ahead. Hence, the ability of information ac-
quisition and the competitiveness of the enter-
prises can be improved accordingly. 
In this paper, we will use a practical example 
to illustrate our platform and evaluate the per-
formance of main functions.  
The rest of this paper is organized as follows. 
Section 2 will introduce the system description 
as well as the main functions implementation. 
The practical case study will be illustrated in 
Section 3. The performance of MODEST will be 
evaluated in Section 4.Finally, this paper will be 
concluded in Section 5. 
2 System Description 
In this section, we first outline the system archi-
tecture of MODEST, and then describe the im-
plementation of the main functionality in detail.  
2.1 Architecture and Workflow 
The MODEST system consists of three modules: 
data acquisition, data analysis, and result display. 
The system architecture is shown in Figure 1. 
 
Figure 1: System architecture. (The module in 
blue is data acquisition, the module in orange is 
data analysis, and the module in light green is 
result display) 
(1) The core technique in the data acquisition 
module is the crawler, which is developed to 
collect raw data from websites, e.g. news portals, 
blogosphere. Then the system parse the raw web 
pages and extract information to store in the local 
database for further processing. 
(2) The data analysis module can be divided into 
two parts:  
? NLP pre-processor: utilizes NLP (natural 
language processing) techniques and some 
toolkits to perform the pre-processing on the 
raw data in (1), including word segmenta-
tion, part-of-speech (POS) tagging1, stop-
word removal, and named entity recognition 
(NER)2. We then create knowledgebase for 
individual industry, such as domain-specific 
sentiment word lexicon, name entity collec-
tion, and so on. 
? Miner?makes use of data mining techniques 
to realize four functions, topic detection and 
tracking (TDT), multi-document summari-
zation 3  (MDS), social network analysis 
(SNA), and opinion mining (OM). The re-
sults of data analysis are also stored in the 
database.  
(3) The result display module read out the analy-
sis results from the database and display them to 
users in the form of plain text, charts, figures, as 
well as video. 
2.2 Function Implementation 
Since the innovation of MODEST is focusing on 
the module of data analysis, we will describe its 
main functions in detail, including topic detec-
tion and tracking, opinion mining, and social 
networks analysis. 
2.2.1 Topic Detection and Tracking 
The TDT function targets on detecting and 
tracking the hot topics for each individual com-
pany. Given a period of data collected from web-
sites, there are various discussions about the 
company. In order to extract these topics, clus-
tering methods (Viermetz et al., 2007 and Yoon 
et al., 2009) are implemented to explore the top-
ics. Note that during the period of data collection, 
different topics with respect to the same compa-
ny may have relations. We, therefore, utilize hi-
erarchical clustering methods4to capture the po-
tential relations.  
Due to the large amount of data, it is impossi-
ble to view all the topics at a snapshot. MODEST 
utilizes topic tracking technique (Wang et al., 
2008) to identify related stories with a stream of 
                                                          
1 www.ictclas.org 
2http://ir.hit.edu.cn/demo/ltp 
3http://libots.sourceforge.net/ 
4http://dragon.ischool.drexel.edu/ 
Raw
files
Pre-processed
files
Database
Crawler UI
Word
segmentation
Web
NER
Stopword
removal
POS
tagging
NLP pre-p ocessor
TDT OM
MDS SNS
Miner
Data Layer
98
media. It is convenient for the users to see the 
latest information about the company.  
In summary, TDT function provides the ser-
vices of detecting and tracking the latest and 
emergent topics, analyzing the relationships of 
topics on the dynamics of the company. It meets 
the aforementioned demand, ?to accurately grasp 
the latest business news and trends of the com-
pany?. 
2.2.2 Opinion Mining 
The objective of OM function is to discover 
opinions towards a company and classify the 
opinions into positive, negative, or neutral. 
The opinion mining function is redesigned 
based on our own opinion mining engine (Zhou 
et al., 2010). It separates opinion identification 
and polarity classification into two stages.  
Given a set of documents that are relevant to 
the company, we first split the documents into 
sentences, and then identify whether the sentence 
is opinionated or not. We extract the features 
shown in Table 1 for opinion identification. 
(Zhou et al., 2010) 
Table 1: Features adopted in the opinionated 
sentence classifier 
Punctuation level features 
The presence of direct quote punctuation "?" and "?"  
The presence of other punctuations: "?" and "!" 
Word-Level and entity-level features 
The presence of known opinion operators 
The percentage of known opinion word in sentence 
Presence of a named entity 
Presence of pronoun 
Presence of known opinion indicators 
Presence of known degree adverbs 
Presence of known conjunctions 
Bi-gram features 
Named entities + opinion operators 
Pronouns + opinion operators 
Nouns or named entities + opinion words 
Pronouns + opinion words 
Opinion words (adjective) + opinion words(noun) 
Degree adverbs + opinion words 
Degree adverbs + opinion operators 
These features are then combined using a ra-
dial basis function (RBF) kernel and a support 
vector machine (SVM) classifier (Drucker et al., 
1997) is trained based on the NTCIR 8training 
data for opinion identification (Kando, 2010). 
For those opinionated sentences, we then clas-
sify them into positive, negative, or neutral. In 
addition to the features shown in Table 1, we 
incorporate features of s-VSM (Sentiment Vector 
Space Model) (Xia et al., 2008) to enhance the 
performance. The principles of the s-VSM are 
listed as follows: (1) Only sentiment-related 
words are used to produce sentiment features for 
the s-VSM. (2) The sentiment words are appro-
priately disambiguated with the neighboring ne-
gations and modifiers. (3) Negations and modifi-
ers are included in the s-VSM to reflect the func-
tions of inversing, strengthening and weakening. 
Sentiment unit is the appropriate element com-
plying with the above principles. (Zhou et al., 
2010) 
In addition to polarity classification, opinion 
holder and target are also recognized in OM 
function for further identifying the relationship 
that two companies have, e.g. collaborative or 
competitive. Both of the dependency parser and 
the semantic role labeling1 (SRL) tool are in-
corporated to identify the semantic roles of each 
chunk based on verbs in the sentence. 
The OM function provides the company with 
services of analyzing the social sentimental 
feedback on the dynamics of the company. It 
meets the aforementioned demand, ?to identify 
and analyze customers? opinions towards the 
company?. 
2.2.3 Social Network Analysis  
SNA function aims at producing the commercial 
network of companies that are hidden within the 
articles.  
To achieve this goal, we maintain two lexicons, 
the commercial named entity lexicon and com-
mercial relation lexicon. Commercial named en-
tity are firstly located within the text and then 
recorded in the commercial entity lexicon in the 
pre-processor NER. Commercial relation lexicon 
record the articles/documents that involve the 
commercial relations. Note that the commercial 
relation lexicon (Table 2) is manually compiled. 
In this work, we consider only two general 
commercial relations, namely cooperation and 
competition.  
 
Table 2: Statistics on relation lexicon. 
Type Amount Examples 
Competition 20 ??(challenge), ??
(compete), ? ?
(opponent) 
Collaboration 18 ??(collaborate),??
(coordinate), ? ?
(cooperate) 
SNA function produces the social network of a 
centric company, which can provide the compa-
                                                          
1http://ir.hit.edu.cn/demo/ltp 
99
ny with the impact analysis and decision-making 
chain tracking. It meets the aforementioned de-
mand, ?to explore the collaborative and competi-
tive relationship between companies?. 
3 Practical Example 
In this section, we use a case study to illustrate 
our system and further evaluate the performance 
of the main functions with respect to those com-
panies. Due to the limited space, we just illus-
trate the main functions of topic detection, opin-
ion mining and social network analysis. 
3.1 Topic Detection and Opinion Mining 
Figure 2(a) showed the results of topic detection 
and opinion mining functions for a Hong Kong 
local financial company Sun Hung Kai Proper-
ties (?????). On top of the figure are the 
results of topic detection and tracking function. 
Multi-document summary of the latest news is 
provided for the company and more news with 
the similar topics can be found by pressing the 
button ???? (more). Since there are a lot of 
duplicates of a piece of news on the websites, the 
summary is a direct way to acquire the recent 
news, which can improve the effectiveness of the 
company.  
The results of opinion mining function are 
shown at the bottom of Figure 2(a), where the 
green line indicates negative while the red line 
indicates positive. In order to give a dynamic 
insight of public opinions, we provide the 
amount changes of positive and negative articles 
with time variant. This is very helpful for the 
company to capture the feedback of their mar-
keting policies. As shown in Figure 2(a), there 
were 14 negative articles (????) on Oct. 29, 
2012, which achieved negative peak within the 6 
months. The users would probably read those 14 
articles and adjust the company strategy accord-
ingly.  
3.2 Social Network Analysis 
Figure 2(b) shows the social network based on 
the centric company in yellow, Sun Hung Kai 
Properties (?????). We only list the half 
of the connected companies with collaborative 
relationship from Sun Hung Kai Properties, and 
remove the competitive ones due to limited space. 
The thickness of the line indicates the strength of 
the collaboration between the two companies. 
The social network can explore the potential 
partners/competitors of a company. Furthermore, 
users are allowed to adjust the depth and set the 
nodes count of the network. The above analysis 
can provide a richer insight in to a company.  
In the following section, we will make exper-
iments to investigate the performance of the 
above functions.
 
(a) Topic detection and opinion mining of Sun Hung Kai Properties (?????). (For convenience, 
we translate the texts on the button in English) 
Opinion Mining 
Topic Detection 
100
 (b)Social network of Sun Hung Kai Properties (?????). (The rectangle in yellow is the centric) 
 
Figure 2: Screenshot of the MODEST system. 
4 Experiment and Result 
In our evaluation, the experiments were made 
based on 17692 articles collected from 52 Hong 
Kong websites during 6 months (1/7/2012~ 
31/12/2012). We investigate the performance of 
MODEST based on the standard metrics pro-
posed by NIST1, including precision, recall, and 
F-score. 
Precision (P) is the fraction of detected articles 
(U) that are relevantto the topic (N). 
  
 
 
      
Recall (R) is the fraction of the articles (T) that 
are relevant to the topic that are successfully de-
tected (N). 
  
 
 
      
Usually, there is an inverse relationship be-
tween precision and recall, where it is possible to 
increase one at the cost of reducing the other. 
Therefore, precision and recall scores are not 
discussed in isolation. Instead, F-Score (F) is 
proposed to combine precision and recall, which 
is the harmonic meanof precision and recall. 
  
 
 
 
 
 
 
      
     
   
      
4.1 Topic Detection and Tracking 
We first assess the performance of the topic de-
tection function. The data is divided into 6 parts 
                                                          
1http://trec.nist.gov/ 
according to the time. For different companies, 
the amount of articles vary a lot. Therefore, we 
calculate the metrics for each individual dataset, 
and then compute the weighted mean value. The 
experimental results are shown in Table 3.  
Table 3: Experimental results on topic detection. 
Dataset Recall Precision F-Score 
1/7/12-31/7/12 85.71% 89.52% 85.38% 
1/8/12-31/8/12 93.10% 93.68% 92.49% 
1/9/12-30/9/12 76.50% 83.13% 76.56% 
1/10/12-31/10/12 83.32% 88.53% 85.84% 
1/11/12-30/11/12 86.11% 89.94% 87.98% 
1/12/12-31/12/12 84.26% 87.65% 85.92% 
Average 85.13% 88.78% 85.69% 
From the experimental results, we can find 
that the average F-Score is about 85.69%.The 
dataset in the second row achieves the best per-
formance while the dataset in the third only get 
76.56% in F-Score. It is because that the amount 
of articles is smaller than the others and the re-
call value is very low. As far as we know, the 
best run of topic detection in (Allan et al., 2007) 
achieved 84%. The performance of topic detec-
tion in MODEST is comparable. 
4.2 Opinion Mining 
We then evaluate the performance of opinion 
mining function. We manually annotated 1568 
articles, which is further divided into 8 datasets 
randomly. Precision, recall, and F-score are also 
used as the metrics for the evaluation. The ex-
perimental results are shown in Table 4. 
101
  From Table 4, we can find that the average 
F-Score can reach 74.09%. Note that the opinion 
mining engine of MODEST is the implementa-
tion of (Zhou et al., 2010), which achieved the 
best run in NTCIR. However, the engine is 
trained on NTCIR corpus, which consists of arti-
cles of general domain, while the test set focuses 
on the financial domain. We further train our 
engine on the data from the financial domain and 
the average F-Score improves to over 80%. 
5 Conclusions 
This demonstration presents an intelligent infor-
mation platform designed to mine Web infor-
mation and provide decisions for modern service, 
MODEST. MODEST can provide the services of 
retrieving news from websites, extracting com-
mercial information, exploring customers? opin-
ions about a given company, and analyzing its 
collaborative/competitive social networks. Both 
enterprises and government are the target cus-
tomers. For enterprise, MODEST can improve 
the competitive abilities and facilitate potential 
collaboration. For government, MODEST can 
collect information about the entire industry, and 
make prompt strategies for better support. 
In this paper, we first introduce the system ar-
chitecture design and the main functions imple-
mentation, including topic detection and tracking, 
opinion mining, and social network analysis. 
Then a case study is given to illustrate the func-
tions of MODEST. In order to evaluate the per-
formance of MODEST, we also conduct the ex-
periments based on the data from 52 Hong Kong 
websites, and the results show the effectiveness 
of the above functions. 
In the future, MODEST will be improved in 
two directions: 
? Extend to other languages, e.g. English, 
Simplified Chinese, etc. 
? Enhance the compatibility to implement 
on mobile device.  
The demo of MODEST and the related 
toolkits can be found on the homepage: 
http://sepc111.se.cuhk.edu.hk:8080/adcom_hk/ 
Acknowledgements 
This research is partially supported by General Re-
search Fund of Hong Kong (417112), Shenzhen Fun-
damental Research Program (JCYJ201304011720464 
50, JCYJ20120613152557576), KTO(TBF1ENG007), 
National Natural Science Foundation of China 
(61203378, 61370165), and Shenzhen International 
Cooperation Funding (GJHZ20120613110641217). 
References: 
James Allan, Jaime Carbonell, George Doddington, 
Jonathan Yamron, and Yiming Yang. 1998. Topic 
Detection and Tracking Pilot Study: Final Report. 
Proceedings of the DARPA Broadcast News Tran-
scription and Understanding Workshop. 
Harris Drucker, Chris J.C. Burges, Linda Kaufman, 
Alex Smola, and Vladimir Vpnik. 1997. Support 
Vector Regression Machines. Proceedings of Ad-
vances in Neural Information Processing Systems, 
pp. 155-161. 
Noriko Kando.2010. Overview of the Eighth NTCIR 
Workshop. Proceedings of NTCIR-8 Workshop. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Proceedings of Synthesis Lectures on Hu-
man Language Technologies, pp. 1-167. 
Maximilian Viermetz, and Michal Skubacz. 2007. 
Using Topic Discovery to Segment Large Commu-
nication Graphs for Social Network Analysis. Pro-
ceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence, pp. 95-99. 
Canhui Wang, Min Zhang, Liyun Ru, and Shaoping 
Ma. 2008. Automatic Online News Topic Ranking 
Using Media Focus and User Attention based on 
Aging Theory. Proceedings of the Conference on 
Information and Knowledge Management. 
Yunqing Xia, Nianxing Ji, Weifeng Su, and Yi Liu. 
2010. Mining Commercial Networks from Online 
Financial News. Proceedings of the IEEE Interna-
tional Conference on E-Business Engineering, pp. 
17-23. 
Ruifeng Xu, Kam-fai Wong, and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining-WIA in NTCIR-7 
MOAT Task. In NTCIR-7 Workshop, pp. 307-313. 
Seok-Ho Yoon, Jung-Hwan Shin, Sang-Wook Kim, 
and Sunju Park. 2009. Extraction of a Latent Blog 
Community based on Subject. Proceeding of the 18th 
ACM Conference on Information and Knowledge 
Management, pp. 1529-1532. 
Lanjun Zhou, Yunqing Xia, Binyang Li, and Kam-fai 
Wong. 2010. WIA-Opinmine System in NTCIR-8 
MOAT Evaluation. Proceedings of NTCIR-8 
Workshop Meeting, pp. 286-292. 
Table 4: Experimental results on opinion mining. 
Dataset Size Precision Recall F-Score 
dataset-1 200 76.57% 78.26% 76.57% 
dataset-2 200 83.55% 89.64% 86.07% 
dataset-3 200 69.12% 69.80% 69.44% 
dataset-4 200 77.13% 75.40% 75.67% 
dataset-5 200 76.21% 77.65% 76.74% 
dataset-6 200 63.76% 66.22% 64.49% 
dataset-7 200 78.56% 78.41% 78.43% 
dataset-8 168 65.72% 65.15% 65.32% 
Average 196 73.83% 75.07% 74.09% 
102
