Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 177?185,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Sentence Fusion via Dependency Graph Compression
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present a novel unsupervised sentence fu-
sion method which we apply to a corpus of bi-
ographies in German. Given a group of related
sentences, we align their dependency trees and
build a dependency graph. Using integer lin-
ear programming we compress this graph to
a new tree, which we then linearize. We use
GermaNet and Wikipedia for checking seman-
tic compatibility of co-arguments. In an eval-
uation with human judges our method out-
performs the fusion approach of Barzilay &
McKeown (2005) with respect to readability.
1 Introduction
Automatic text summarization is a rapidly develop-
ing field in computational linguistics. Summariza-
tion systems can be classified as either extractive or
abstractive ones (Spa?rck Jones, 1999). To date, most
systems are extractive: sentences are selected from
one or several documents and then ordered. This
method exhibits problems, because input sentences
very often overlap and complement each other at the
same time. As a result there is a trade-off between
non-redundancy and completeness of the output. Al-
though the need for abstractive approaches has been
recognized before (e.g. McKeown et al (1999)), so
far almost all attempts to get closer to abstractive
summarization using scalable, statistical techniques
have been limited to sentence compression.
The main reason why there is little progress on ab-
stractive summarization is that this task seems to re-
quire a conceptual representation of the text which is
not yet available (see e.g. Hovy (2003, p.589)). Sen-
tence fusion (Barzilay & McKeown, 2005), where a
new sentence is generated from a group of related
sentences and where complete semantic and con-
ceptual representation is not required, can be seen
as a middle-ground between extractive and abstrac-
tive summarization. Our work regards a corpus of
biographies in German where multiple documents
about the same person should be merged into a sin-
gle one. An example of a fused sentence (3) with the
source sentences (1,2) is given below:
(1) Bohr
Bohr
studierte
studied
an
at
der
the
Universita?t
University
Kopenhagen
Copenhagen
und
and
erlangte
got
dort
there
seine
his
Doktorwu?rde.
PhD
?Bohr studied at the University of Copenhagen
and got his PhD there?
(2) Nach
After
dem
the
Abitur
school
studierte
studied
er
he
Physik
physics
und
and
Mathematik
mathematics
an
at
der
the
Universita?t
University
Kopenhagen.
Copenhagen
?After school he studied physics and mathemat-
ics at the University of Copenhagen?
(3) Nach
After
dem
the
Abitur
school
studierte
studied
Bohr
Bohr
Physik
physics
und
and
Mathematik
mathematics
an
at
der
the
Universita?t
University
Kopenhagen
Copenhagen
und
and
erlangte
got
dort
there
seine
his
Doktorwu?rde.
PhD
?After school Bohr studied physics and mathe-
matics at the University of Copenhagen and got
his PhD there?
177
Having both (1) and (2) in a summary would make
it redundant. Selecting only one of them would not
give all the information from the input. (3), fused
from both (1) and (2), conveys the necessary infor-
mation without being redundant and is more appro-
priate for a summary.
To this end, we present a novel sentence fusion
method based on dependency structure alignment
and semantically and syntactically informed phrase
aggregation and pruning. We address the problem in
an unsupervised manner and use integer linear pro-
gramming (ILP) to find a globally optimal solution.
We argue that our method has three important advan-
tages compared to existing methods. First, we ad-
dress the grammaticality issue empirically by means
of knowledge obtained from an automatically parsed
corpus. We do not require such resources as subcat-
egorization lexicons or hand-crafted rules, but de-
cide to retain a dependency based on its syntactic
importance score. The second point concerns inte-
grating semantics. Being definitely important, ?this
source of information remains relatively unused in
work on aggregation1 within NLG? (Reiter & Dale,
2000, p.141). To our knowledge, in the text-to-text
generation field, we are the first to use semantic in-
formation not only for alignment but also for aggre-
gation in that we check coarguments? compatibility.
Apart from that, our method is not limited to sen-
tence fusion and can be easily applied to sentence
compression. In Filippova & Strube (2008) we com-
press English sentences with the same approach and
achieve state-of-the-art performance.
The paper is organized as follows: Section 2 gives
an overview of related work and Section 3 presents
our data. Section 4 introduces our method and Sec-
tion 5 describes the experiments and discusses the
results of the evaluation. The conclusions follow in
the final section.
2 Related Work
Most studies on text-to-text generation concern sen-
tence compression where the input consists of ex-
actly one sentence (Jing, 2001; Hori & Furui, 2004;
Clarke & Lapata, 2008, inter alia). In such set-
ting, redundancy, incompleteness and compatibility
1We follow Barzilay & McKeown (2005) and refer to aggre-
gation within text-to-text generation as sentence fusion.
issues do not arise. Apart from that, there is no
obvious way of how existing sentence compression
methods can be adapted to sentence fusion.
Barzilay & McKeown (2005) present a sentence
fusion method for multi-document news summariza-
tion which crucially relies on the assumption that in-
formation appearing in many sources is important.
Consequently, their method produces an intersec-
tion of input sentences by, first, finding the centroid
of the input, second, augmenting it with informa-
tion from other sentences and, finally, pruning a pre-
defined set of constituents (e.g. PPs). The resulting
structure is not necessarily a tree and allows for ex-
traction of several trees, each of which can be lin-
earized in many ways.
Marsi & Krahmer (2005) extend the approach of
Barzilay & McKeown to do not only intersection
but also union fusion. Like Barzilay & McKeown
(2005), they find the best linearization with a lan-
guage model which, as they point out, often pro-
duces inadequate rankings being unable to deal with
word order, agreement and subcategorization con-
straints. In our work we aim at producing a valid
dependency tree structure so that most grammatical-
ity issues are resolved before the linearization stage.
Wan et al (2007) introduce a global revision
method of how a novel sentence can be generated
from a set of input words. They formulate the prob-
lem as a search for a maximum spanning tree which
is incrementally constructed by connecting words or
phrases with dependency relations. The grammat-
icality issue is addressed by a number of hard con-
straints. As Wan et al point out, one of the problems
with their method is that the output built up from
dependencies found in a corpus might have a mean-
ing different from the intended one. Since we build
our trees from the input dependencies, this problem
does not arise with our method. Apart from that, in
our opinion, the optimization formulation we adopt
is more appropriate as it allows to integrate many
constraints without complex rescoring rules.
3 Data
The comparable corpus we work with is a collection
of about 400 biographies in German gathered from
178
the Internet2. These biographies describe 140 differ-
ent people, and the number of articles for one person
ranges from 2 to 4, being 3 on average. Despite ob-
vious similarities between articles about one person,
neither identical content nor identical ordering of in-
formation can be expected.
Fully automatic preprocessing in our system com-
prises the following steps: sentence boundaries are
identified with a Perl CPAN module3. Then the
sentences are split into tokens and the TnT tagger
(Brants, 2000) and the TreeTagger (Schmid, 1997)
are used for tagging and lemmatization respectively.
Finally, the biographies are parsed with the CDG de-
pendency parser (Foth & Menzel, 2006). We also
identify references to the biographee (pronominal as
well as proper names) and temporal expressions (ab-
solute and relative) with a few rules.
4 Our Method
Groups of related sentences serve as input to a sen-
tence fusion system and thus need to be identified
first (4.1). Then the dependency trees of the sen-
tences are modified (4.2) and aligned (4.3). Syntac-
tic importance (4.4) and word informativeness (4.5)
scores are used to extract a new dependency tree
from a graph of aligned trees (4.6). Finally, the tree
is linearized (4.7).
4.1 Sentence Alignment
Sentence alignment for comparable corpora requires
methods different from those used in machine trans-
lation for parallel corpora. For example, given two
biographies of a person, one of them may follow the
timeline from birth to death whereas the other may
group events thematically or tell only about the sci-
entific contribution of the person. Thus one can-
not assume that the sentence order or the content
is the same in two biographies. Shallow methods
like word or bigram overlap, (weighted) cosine or
Jaccard similarity are appealing as they are cheap
and robust. In particular, Nelken & Schieber (2006)
2http://de.wikipedia.org, http://home.
datacomm.ch/biografien, http://biographie.
net/de, http://www.weltchronik.de/ws/bio/
main.htm, http://www.brockhaus-suche.de/
suche
3http://search.cpan.org/?holsten/
Lingua-DE-Sentence-0.07/Sentence.pm
demonstrate the efficacy of a sentence-based tf*idf
score when applied to comparable corpora. Follow-
ing them, we define the similarity of two sentences
sim(s1, s2) as
S1 ? S2
|S1| ? |S2|
=
?
t wS1(t) ? wS2(t)
?
?
t w2S1(t)
?
t w2S2(t)
(1)
where S is the set of all lemmas but stop-words from
s, and wS(t) is the weight of the term t:
wS(t) = S(t)
1
Nt
(2)
where S(t) is the indicator function of S, Nt is the
number of sentences in the biographies of one per-
son which contain t. We enhance the similarity mea-
sure by looking up synonymy in GermaNet (Lem-
nitzer & Kunze, 2002).
We discard identical or nearly identical sen-
tences (sim(s1, s2) > 0.8) and greedily build
sentence clusters using a hierarchical groupwise-
average technique. As a result, one sentence may
belong to one cluster at most. These sentence clus-
ters serve as input to the fusion algorithm.
4.2 Dependency Tree Modification
We apply a set of transformations to a dependency
tree to emphasize its important properties and elim-
inate unimportant ones. These transformations are
necessary for the compression stage. An example of
a dependency tree and its modifed version are given
in Fig. 1.
PREP preposition nodes (an, in) are removed and
placed as labels on the edges to the respective
nouns;
CONJ a chain of conjuncts (Mathematik und
Physik) is split and each node is attached to the
parent node (studierte) provided they are not
verbs;
APP a chain of words analyzed as appositions by
CDG (Niels Bohr) is collapsed into one node;
FUNC function words like determiners (der), aux-
iliary verbs or negative particles are removed
from the tree and memorized with their lexical
heads (memorizing negative particles preserves
negation in the output);
179
Bohr
Mathematik
und
Physik
an in
Kopenhagen
der
Uni
studierte
subj obja pp
pp
kon
cj
pn
pn
det
(a) Dependency tree
studierte
root
s
bio
Mathematik
Physik Uni
Kopenhagen
obja
obja
an
in
subj
(b) Modified tree
Figure 1: The dependency tree of the sentence Bohr studierte Mathematik und Physik an der Uni in Kopenhagen
(Bohr studied mathematics and physics at university in Copenhagen) as produced by the parser (a) and after all
transformations applied (b)
ROOT every dependency tree gets an explicit root
which is connected to every verb node;
BIO all occurrences of the biographee (Niels Bohr)
are replaced with the bio tag.
4.3 Node Alignment
Once we have a group of two to four strongly related
sentences and their transformed dependency trees,
we aim at finding the best node alignment. We use
a simple, fast and transparent method and align any
two words provided that they
1. are content words;
2. have the same part-of-speech;
3. have identical lemmas or are synonyms.
In case of multiple possibilities, which are extremely
rare in our data, the choice is made randomly. By
merging all aligned nodes we get a dependency
graph which consists of all dependencies from the
input trees. In case it contains a cycle, one of the
alignments from the cycle is eliminated.
We prefer this very simple method to bottom-up
ones (Barzilay & McKeown, 2005; Marsi & Krah-
mer, 2005) for two main reasons. Pursuing local
subtree alignments, bottom-up methods may leave
identical words unaligned and thus prohibit fusion
of complementary information. On the other hand,
they may force alignment of two unrelated words if
the subtrees they root are largely aligned. Although
in some cases it helps discover paraphrases, it con-
siderably increases chances of generating ungram-
matical output which we want to avoid at any cost.
4.4 Syntactic Importance Score
Given a dependency graph we want to get a new de-
pendency tree from it. Intuitively, we want to re-
tain obligatory dependencies (e.g. subject) while re-
moving less important ones (e.g. adv). When de-
ciding on pruning an argument, previous approaches
either used a set of hand-crafted rules (e.g. Barzilay
& McKeown (2005)), or utilized a subcategorization
lexicon (e.g. Jing (2001)). The hand-crafted rules
are often too general to ensure a grammatical argu-
ment structure for different verbs (e.g. PPs can be
pruned). Subcategorization lexicons are not readily
available for many languages and cover only verbs.
E.g. they do not tell that the noun son is very of-
ten modified by a PP using the preposition of, as in
the son of Niels Bohr, and that the NP without a PP
modifier may appear incomplete.
To overcome these problems, we decide on prun-
ing an edge by estimating the conditional proba-
bility of its label given its head, P (l|h)4. For ex-
ample, P (subj|studieren) ? the probability of the
label subject given the verb study ? is higher than
P (in|studieren), and therefore the subject will be
preserved whereas the prepositional label and thus
the whole PP can be pruned, if needed. Table 1
presents the probabilities of several labels given that
the head is studieren and shows that some preposi-
tions are more important than other ones. Note that
if we did not apply the PREP modification we would
be unable to distinguish between different prepo-
sitions and could only calculate P (pp|studieren)
4The probabilities are calculated from a corpus of approx.
3,000 biographies from Wikipedia which we annotated auto-
matically as described in Section 3.
180
which would not be very informative.
subj obja in an nach mit zu
0.88 0.74 0.44 0.42 0.09 0.02 0.01
Table 1: Probabilities of subj, obja(ccusative), in, at, af-
ter, with, to given the verb studieren (study)
4.5 Word Informativeness Score
We also want to retain informative words in the out-
put tree. There are many ways in which word im-
portance can be defined. Here, we use a formula
introduced by Clarke & Lapata (2008) which is a
modification of the significance score of Hori & Fu-
rui (2004):
I(wi) =
l
N ? fi log
FA
Fi
(3)
wi is the topic word (either noun or verb), fi is the
frequency of wi in the aligned biographies, Fi is the
frequency of wi in the corpus, and FA is the sum
of frequencies of all topic words in the corpus. l is
the number of clause nodes above w and N is the
maximum level of embedding of the sentence which
w belongs to. By defining word importance differ-
ently, e.g. as relatedness of a word to the topic, we
could apply our method to topic-based summariza-
tion (Krahmer et al, 2008).
4.6 New Sentence Generation
We formulate the task of getting a tree from a depen-
dency graph as an optimization problem and solve
it with ILP5. In order to decide which edges of the
graph to remove, for each directed dependency edge
from head h to word w we introduce a binary vari-
able xlh,w, where l stands for the label of the edge:
xlh,w =
{
1 if the dependency is preserved
0 otherwise
(4)
The goal is to find a subtree of the graph which
gets the highest score of the objective function (5) to
which both the probability of dependencies (P (l|h) )
and the importance of dependent words (I(w)) con-
tribute:
5We use lp solve in our implementation http://
sourceforge.net/projects/lpsolve.
f(X) =
?
x
xlh,w ? P (l|h) ? I(w) (5)
The objective function is subject to four types of
constraints presented below (W stands for the set of
graph nodes minus root, i.e. the set of words).
STRUCTURAL constraints allow to get a tree from
the graph: (6) ensures that each word has one head
at most. (7) ensures connectivity in the tree. (8) is
optional and restricts the size of the resulting tree to
? words (? = min(0.6? ? |W |, 10)).
?w ? W,
?
h,l
xlh,w ? 1 (6)
?w ? W,
?
h,l
xlh,w ?
1
|W |
?
u,l
xlw,u ? 0 (7)
?
x
xlh,w ? ? (8)
SYNTACTIC constraints ensure the syntactic validity
of the output tree and explicitly state which argu-
ments should be preserved. We have only one syn-
tactic constraint which guarantees that a subordinat-
ing conjunction (sc) is preserved (9) if and only if the
clause it belongs to serves as a subordinate clause
(sub) in the output.
?xscw,u,
?
h,l
xsubh,w ? xscw,u = 0 (9)
SEMANTIC constraints restrict coordination to se-
mantically compatible elements. The idea behind
these constraints is the following (see Fig. 2). It
can be that one sentence says He studied math and
another one He studied physics, so the output may
unite the two words under coordination: He studied
math and physics. But if the input sentences are He
studied physics and He studied sciences, then one
should not unite both, because sciences is the gen-
eralization of physics. Neither should one unite two
unrelated words: He studied with pleasure and He
studied with Bohr cannot be fused into He studied
with pleasure and Bohr.
To formalize these intuitions we define two func-
tions hm(w,u) and rel(w,u): hm(w,u) is a binary func-
tion, whereas rel(w,u) returns a value from [0, 1]. We
181
root
s
studied
sciencesbio
pleasure
mathphysics
subj
with
with
obja
obja
obja
Bohr
Figure 2: Graph obtained from sentences He studied sci-
ences with pleasure and He studied math and physics with
Bohr
also introduce additional variables ylw,u (represented
by dashed lines in Fig. 2):
ylw,u =
{
1 if ?h, l : xlh,w = 1 ? xlh,u = 1
0 otherwise
(10)
For two edges sharing a head and having identical
labels to be retained we check in GermaNet and
in the taxonomy derived from Wikipedia (Kassner
et al, 2008) that their dependents are not in the
hyponymy or meronymy relation (11). We prohibit
verb coordination unless it is found in one of the
input sentences. If the dependents are nouns, we
also check that their semantic relatedness as mea-
sured with WikiRelate! (Strube & Ponzetto, 2006)
is above a certain threshold (12). We empirically
determined the value of ? = 0.36 by calculating an
average similarity of coordinated nouns in the cor-
pus.
?ylw,u, hm(w, u) ? ylw,u = 0 (11)
?ylw,u, (rel(w, u) ? ?) ? ylw,u ? 0 (12)
(11) prohibits that physics (or math) and sciences ap-
pear together since, according to GermaNet, physics
(Physik) is a hyponym of science (Wissenschaft).
(12) blocks taking both pleasure (Freude) and Bohr
because rel(Freude,Bohr) = 0.17. math and physics
are neither in ISA, nor part-of relation and are suffi-
ciently related (rel(Mathematik, Physik) = 0.67) to
become conjuncts.
META constraints (equations (13) and (14)) guar-
antee that ylw,u = xlh,w ? xlh,u i.e. they ensure that
the semantic constraints are applied only if both the
labels from h to w and from h to u are preserved.
?ylw,u, xlh,w + xlh,u ? 2ylw,u (13)
?ylw,u, 1 ? xlh,w + 1 ? xlh,u ? 1 ? ylw,u (14)
4.7 Linearization
The ?overgenerate-and-rank? approach to statisti-
cal surface realization is very common (Langk-
ilde & Knight, 1998). Unfortunately, in its sim-
plest and most popular version, it ignores syntac-
tical constraints and may produce ungrammatical
output. For example, an inviolable rule of Ger-
man grammar states that the finite verb must be in
the second position in the main clause. Since it is
hard to enforce such rules with an ngram language
model, syntax-informed linearization methods have
been developed for German (Ringger et al, 2004;
Filippova & Strube, 2007). We apply our recent
method to order constituents and, using the CMU
toolkit (Clarkson & Rosenfeld, 1997), build a tri-
gram language model from Wikipedia (approx. 1GB
plain text) to find the best word order within con-
stituents. Some constraints on word order are in-
ferred from the input. Only interclause punctuation
is generated.
5 Experiments and Evaluation
We choose Barzilay & McKeown?s system as a non-
trivial baseline since, to our knowledge, there is no
other system which outperforms theirs (Sec. 5.1). It
is important for us to evaluate the fusion part of our
system, so the input and the linearization module of
our method and the baseline are identical. We are
also interested in how many errors are due to the lin-
earization module and thus define the readability up-
per bound (Sec. 5.2). We further present and discuss
the experiments (Sec. 5.3 and 5.5).
5.1 Baseline
The algorithm of Barzilay & McKeown (2005) pro-
ceeds as follows: Given a group of related sentences,
a dependency tree is built for each sentence. These
trees are modified so that grammatical features are
eliminated from the representation and memorized;
noun phrases are flattened to facilitate alignment.
A locally optimal pairwise alignment of modified
182
dependency trees is recursively found with Word-
Net and a paraphrase lexicon. From the alignment
costs the centroid of the group is identified. Then
this tree is augmented with information from other
trees given that it appears in at least half of the sen-
tences from this group. A rule-based pruning mod-
ule prunes optional constituents, such as PPs or rel-
ative clauses. The linearization of the resulting tree
(or graph) is done with a trigram language model.
To adapt this system to German, we use the Ger-
maNet API (Gurevych & Niederlich, 2005) instead
of WordNet. We do not use a paraphrase lexicon,
because there is no comparable corpus of sufficient
size available for German. We readjust the align-
ment parameters of the system to prevent dissimi-
lar nodes from being aligned. The input to the al-
gorithm is generated as described in Sec. 4.1. The
linearization is done as described in Sec. 4.7. In
cases when there is a graph to linearize, all possible
trees covering the maximum number of nodes are
extracted from it and linearized. The most probable
string is selected as the final output with a language
model. For the rest of the reimplementation we fol-
low the algorithm as presented.
5.2 Readability Upper Bound
To find the upper bound on readability, we select one
sentence from the input randomly, parse it and lin-
earize the dependency tree as described in Sec. 4.7.
This way we obtain a sentence which may differ in
form from the input sentences but whose content is
identical to one of them.
5.3 Experiments
It is notoriously difficult to evaluate generation and
summarization systems as there are many dimen-
sions in which the quality of the output can be as-
sessed. The goal of our present evaluation is in the
first place to check whether our method is able to
produce sensible output.
We evaluated the three systems (GRAPH-
COMPRESSION, BARZILAY & MCKEOWN and
READABILITY UB) with 50 native German speakers
on 120 fused sentences generated from 40 randomly
drawn related sentences groups (3 ? 40). In an
online experiment, the participants were asked to
read a fused sentence preceded by the input and
to rate its readability (read) and informativity in
respect to the input (inf ) on a five point scale. The
experiment was designed so that every participant
rated 40 sentences in total. No participant saw
two sentences generated from the same input. The
results are presented in Table 2. len is an average
length in words of the output.
read inf len
READABILITY UB 4.0 3.5 12.9
BARZILAY & MCKEOWN 3.1 3.0 15.5
GRAPH-COMPRESSION 3.7 3.1 13.0
Table 2: Average readability and informativity on a five
point scale, average length in words
5.4 Error Analysis
The main disadvantage of our method, as well as
other methods designed to work on syntactic struc-
tures, is that it requires a very accurate parser. In
some cases, errors in the preprocessing made ex-
tracting a valid dependency tree impossible. The
poor rating of READABILITY UB also shows that er-
rors of the parser and of the linearization module af-
fect the output considerably.
Although the semantic constraints ruled out
many anomalous combinations, the limited cover-
age of GermaNet and the taxonomy derived from
Wikipedia was the reason for some semantic oddi-
ties in the sentences generated by our method. For
example, it generated phrases like aus England und
Gro?britannien (from England and Great Britain).
A larger taxonomy would presumably increase the
recall of the semantic constraints which proved help-
ful. Such errors were not observed in the output of
the baseline because it does not fuse within NPs.
Both the baseline and our method made subcate-
gorization errors, although these are more common
for the baseline which aligns not only synonyms
but also verbs which share some arguments. Also,
the baseline pruned some PPs necessary for a sen-
tence to be complete. For example, it pruned an
der Atombombe (on the atom bomb) and generated
an incomplete sentence Er arbeitete (He worked).
For the baseline, alignment of flattened NPs instead
of words caused generating very wordy and redun-
dant sentences when the input parse trees were in-
correct. In other cases, our method made mistakes
183
in linearizing constituents because it had to rely on a
language model whereas the baseline used unmod-
ified constituents from the input. Absense of intra-
clause commas caused a drop in readability in some
otherwise grammatical sentences.
5.5 Discussion
A paired t-test revealed significant differences be-
tween the readability ratings of the three systems
(p = 0.01) but found no significant differences be-
tween the informativity scores of our system and the
baseline. Some participants reported informativity
hard to estimate and to be assessable for grammat-
ical sentences only. The higher readability rating
of our method supports our claim that the method
based on syntactic importance score and global con-
straints generates more grammatical sentences than
existing systems. An important advantage of our
method is that it addresses the subcategorization is-
sue directly without shifting the burden of selecting
the right arguments to the linearization module. The
dependency structure it outputs is a tree and not a
graph as it may happen with the method of Barzi-
lay & McKeown (2005). Moreover, our method can
distinguish between more and less obligatory argu-
ments. For example, it knows that at is more impor-
tant than to for study whereas for go it is the other
way round. Unlike our differentiated approach, the
baseline rule states that PPs can generally be pruned.
Since the baseline generates a new sentence by
modifying the tree of an input sentence, in some
cases it outputs a compression of this sentence. Un-
like this, our method is not based on an input tree
and generates a new sentence without being biased
to any of the input sentences.
Our method can also be applied to non-trivial sen-
tence compression, whereas the baseline and similar
methods, such as Marsi & Krahmer (2005), would
then boil down to a few very general pruning rules.
We tested our method on the English compression
corpus6 and evaluated the compressions automati-
cally the same way as Clarke & Lapata (2008) did.
The results (Filippova & Strube, 2008) were as good
as or significantly better than the state-of-the-art, de-
pending on the choice of dependency parser.
6The corpus is available from http://homepages.
inf.ed.ac.uk/s0460084/data.
6 Conclusions
We presented a novel sentence fusion method which
formulates the fusion task as an optimization prob-
lem. It is unsupervised and finds a globally optimal
solution taking semantics, syntax and word informa-
tiveness into account. The method does not require
hand-crafted rules or lexicons to generate grammat-
ical output but relies on the syntactic importance
score calculated from an automatically parsed cor-
pus. An experiment with native speakers demon-
strated that our method generates more grammatical
sentences than existing systems.
There are several directions to explore in the fu-
ture. Recently query-based sentence fusion has been
shown to be a better defined task than generic sen-
tence fusion (Krahmer et al, 2008). By modify-
ing the word informativeness score, e.g. by giving
higher scores to words semantically related to the
query, one could force our system to retain words
relevant to the query in the output. To generate co-
herent texts we plan to move beyond sentence gen-
eration and add discourse constraints to our system.
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). Part of the data has been used
with a permission of Bibliographisches Institut & F.
A. Brockhaus AG, Mannheim, Germany. We would
like to thank the participants in our online evalua-
tion. We are also grateful to Regina Barzilay and the
three reviewers for their helpful comments.
References
Barzilay, Regina & Kathleen R. McKeown (2005). Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?327.
Brants, Thorsten (2000). TnT ? A statistical Part-of-
Speech tagger. In Proceedings of the 6th Confer-
ence on Applied Natural Language Processing, Seat-
tle, Wash., 29 April ? 4 May 2000, pp. 224?231.
Clarke, James & Mirella Lapata (2008). Global inference
for sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 31:399?429.
Clarkson, Philip & Ronald Rosenfeld (1997). Statis-
tical language modeling using the CMU-Cambridge
toolkit. In Proceedings of the 5th European Con-
ference on Speech Communication and Technology,
184
Rhodes, Greece, 22-25 September 1997, pp. 2707?
2710.
Filippova, Katja & Michael Strube (2007). Generating
constituent order in German clauses. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics, Prague, Czech Republic, 23?30
June 2007, pp. 320?327.
Filippova, Katja & Michael Strube (2008). Dependency
tree based sentence compression. In Proceedings of
the 5th International Conference on Natural Language
Generation, Salt Fork, Ohio, 12?14 June 2008, pp. 25?
32.
Foth, Kilian & Wolfgang Menzel (2006). Hybrid pars-
ing: Using probabilistic models as predictors for a
symbolic parser. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, Sydney, Australia, 17?21 July 2006,
pp. 321?327.
Gurevych, Iryna & Hendrik Niederlich (2005). Access-
ing GermaNet data and computing semantic related-
ness. In Companion Volume to the Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, Mich., 25?30 June
2005, pp. 5?8.
Hori, Chiori & Sadaoki Furui (2004). Speech summa-
rization: An approach through word extraction and a
method for evaluation. IEEE Transactions on Infor-
mation and Systems, E87-D(1):15?25.
Hovy, Eduard (2003). Text summarization. In Ruslan
Mitkov (Ed.), The Oxford Handbook of Computational
Linguistics, pp. 583?598. Oxford, U.K.: Oxford Uni-
versity Press.
Jing, Hongyan (2001). Cut-and-Paste Text Summariza-
tion, (Ph.D. thesis). Computer Science Department,
Columbia University, New York, N.Y.
Kassner, Laura, Vivi Nastase & Michael Strube (2008).
Acquiring a taxonomy from the German Wikipedia.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation, Marrakech, Mo-
rocco, 26 May ? 1 June 2008.
Krahmer, Emiel, Erwin Marsi & Paul van Pelt (2008).
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Companion Volume to the Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, Columbus, Ohio, 15?20 June
2008, pp. 193?196.
Langkilde, Irene & Kevin Knight (1998). Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 17th International Conference on
Computational Linguistics and 36th Annual Meet-
ing of the Association for Computational Linguistics,
Montre?al, Que?bec, Canada, 10?14 August 1998, pp.
704?710.
Lemnitzer, Lothar & Claudia Kunze (2002). GermaNet
? representation, visualization, application. In Pro-
ceedings of the 3rd International Conference on Lan-
guage Resources and Evaluation, Las Palmas, Canary
Islands, Spain, 29?31 May 2002, pp. 1485?1491.
Marsi, Erwin & Emiel Krahmer (2005). Explorations in
sentence fusion. In Proceedings of the European Work-
shop on Natural Language Generation, Aberdeen,
Scotland, 8?10 August, 2005, pp. 109?117.
McKeown, Kathleen R., Judith L. Klavans, Vassileios
Hatzivassiloglou, Regina Barzilay & Eleazar Eskin
(1999). Towards multidocument summarization by re-
formulation: Progress and prospects. In Proceedings
of the 16th National Conference on Artificial Intelli-
gence, Orlando, Flo., 18?22 July 1999, pp. 453?460.
Nelken, Rani & Stuart Schieber (2006). Towards robust
context-sensitive sentence alignment for monolingual
corpora. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Trento, Italy, 3?7 April 2006, pp.
161?168.
Reiter, Ehud & Robert Dale (2000). Building Natu-
ral Language Generation Systems. Cambridge, U.K.:
Cambridge University Press.
Ringger, Eric, Michael Gamon, Robert C. Moore, David
Rojas, Martine Smets & Simon Corston-Oliver (2004).
Linguistically informed statistical models of con-
stituent structure for ordering in sentence realization.
In Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzerland,
23?27 August 2004, pp. 673?679.
Schmid, Helmut (1997). Probabilistic Part-of-Speech
tagging using decision trees. In Daniel Jones & Harold
Somers (Eds.), New Methods in Language Processing,
pp. 154?164. London, U.K.: UCL Press.
Spa?rck Jones, Karen (1999). Automatic summarizing:
Factors and directions. In Inderjeet Mani & Mark T.
Maybury (Eds.), Advances in Automatic Text Summa-
rization, pp. 1?12. Cambridge, Mass.: MIT Press.
Strube, Michael & Simone Paolo Ponzetto (2006).
WikiRelate! Computing semantic relatedness using
Wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence, Boston, Mass., 16?
20 July 2006, pp. 1419?1424.
Wan, Stephen, Robert Dale, Mark Dras & Cecile Paris
(2007). Global revision in summarization: Generating
novel sentences with Prim?s algorithm. In Proceedings
of the 10th Conference of the Pacific Association for
Computational Linguistics, Melbourne, Australia, 19?
21 September, 2007, pp. 226?235.
185
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 246?254,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Company-Oriented Extractive Summarization of Financial News?
Katja Filippova?, Mihai Surdeanu?, Massimiliano Ciaramita?, Hugo Zaragoza?
?EML Research gGmbH ?Yahoo! Research
Schloss-Wolfsbrunnenweg 33 Avinguda Diagonal 177
69118 Heidelberg, Germany 08018 Barcelona, Spain
filippova@eml-research.de,{mihais,massi,hugoz}@yahoo-inc.com
Abstract
The paper presents a multi-document sum-
marization system which builds company-
specific summaries from a collection of fi-
nancial news such that the extracted sen-
tences contain novel and relevant infor-
mation about the corresponding organiza-
tion. The user?s familiarity with the com-
pany?s profile is assumed. The goal of
such summaries is to provide information
useful for the short-term trading of the cor-
responding company, i.e., to facilitate the
inference from news to stock price move-
ment in the next day. We introduce a
novel query (i.e., company name) expan-
sion method and a simple unsupervized al-
gorithm for sentence ranking. The sys-
tem shows promising results in compari-
son with a competitive baseline.
1 Introduction
Automatic text summarization has been a field of
active research in recent years. While most meth-
ods are extractive, the implementation details dif-
fer considerably depending on the goals of a sum-
marization system. Indeed, the intended use of the
summaries may help significantly to adapt a par-
ticular summarization approach to a specific task
whereas the broadly defined goal of preserving rel-
evant, although generic, information may turn out
to be of little use.
In this paper we present a system whose goal is
to extract sentences from a collection of financial
?This work was done during the first author?s internship
at Yahoo! Research. Mihai Surdeanu is currently affiliated
with Stanford University (mihais@stanford.edu).
Massimiliano Ciaramita is currently at Google
(massi@google.com).
news to inform about important events concern-
ing companies, e.g., to support trading (i.e., buy or
sell) the corresponding symbol on the next day, or
managing a portfolio. For example, a company?s
announcement of surpassing its earnings? estimate
is likely to have a positive short-term effect on its
stock price, whereas an announcement of job cuts
is likely to have the reverse effect. We demonstrate
how existing methods can be extended to achieve
precisely this goal.
In a way, the described task can be classified
as query-oriented multi-document summarization
because we are mainly interested in information
related to the company and its sector. However,
there are also important differences between the
two tasks.
? The name of the company is not a query,
e.g., as it is specified in the context of the
DUC competitions1, and requires an exten-
sion. Initially, a query consists exclusively
of the ?symbol?, i.e., the abbreviation of the
name of a company as it is listed on the stock
market. For example, WPO is the abbrevia-
tion used on the stock market to refer to The
Washington Post?a large media and educa-
tion company. Such symbols are rarely en-
countered in the news and cannot be used to
find all the related information.
? The summary has to provide novel informa-
tion related to the company and should avoid
general facts about it which the user is sup-
posed to know. This point makes the task
related to update summarization where one
has to provide the user with new information
1http://duc.nist.gov; since 2008 TAC: http:
//www.nist.gov/tac.
246
given some background knowledge2. In our
case, general facts about the company are as-
sumed to be known by the user. Given WPO,
we want to distinguish between The Wash-
ington Post is owned by The Washington Post
Company, a diversified education and media
company and The Post recently went through
its third round of job cuts and reported an
11% decline in print advertising revenues for
its first quarter, the former being an example
of background information whereas the lat-
ter is what we would like to appear in the
summary. Thus, the similarity to the query
alone is not the decisive parameter in com-
puting sentence relevance.
? While the summaries must be specific for a
given organization, important but general fi-
nancial events that drive the overall market
must be included in the summary. For exam-
ple, the recent subprime mortgage crisis af-
fected the entire economy regardless of the
sector.
Our system proceeds in the three steps illus-
trated in Figure 1. First, the company symbol is
expanded with terms relevant for the company, ei-
ther directly ? e.g., iPod is directly related to Apple
Inc. ? or indirectly ? i.e., using information about
the industry or sector the company operates in. We
detail our symbol expansion algorithm in Section
3. Second, this information is used to rank sen-
tences based on their relatedness to the expanded
query and their overall importance (Section 4). Fi-
nally, the most relevant sentences are re-ranked
based on the degree of novelty they carry (Section
5).
The paper makes the following contributions.
First, we present a new query expansion tech-
nique which is useful in the context of company-
dependent news summarization as it helps identify
sentences important to the company. Second, we
introduce a simple and efficient method for sen-
tence ranking which foregrounds novel informa-
tion of interest. Our system performs well in terms
of the ROUGE score (Lin & Hovy, 2003) com-
pared with a competitive baseline (Section 6).
2 Data
The data we work with is a collection of financial
news consolidated and distributed by Yahoo! Fi-
2See the DUC 2007 and 2008 update tracks.
nance3 from various sources4. Each story is la-
beled as being relevant for a company ? i.e., it
appears in the company?s RSS feed ? if the story
mentions either the company itself or the sector the
company belongs to. Altogether the corpus con-
tains 88,974 news articles from a period of about
5 months (148 days). Some articles are labeled
as being relevant for several companies. The total
number of (company name, news collection) pairs
is 46,444.
The corpus is cleaned of HTML tags, embed-
ded graphics and unrelated information (e.g., ads,
frames) with a set of manually devised rules. The
filtering is not perfect but removes most of the
noise. Each article is passed through a language
processing pipeline (described in (Atserias et al,
2008)). Sentence boundaries are identified by
means of simple heuristics. The text is tokenized
according to Penn TreeBank style and each to-
ken lemmatized using Wordnet?s morphological
functions. Part of speech tags and named entities
(LOC, PER, ORG, MISC) are identified by means
of a publicly available named-entity tagger5 (Cia-
ramita & Altun, 2006, SuperSense). Apart from
that, all sentences which are shorter than 5 tokens
and contain neither nouns nor verbs are sorted out.
We apply the latter filter as we are interested in
textual information only. Numeric information
contained, e.g., in tables can be easily and more
reliably obtained from the indices tables available
online.
3 Query Expansion
In company-oriented summarization query expan-
sion is crucial because, by default, our query con-
tains only the symbol, that is the abbreviation of
the name of the company. Unfortunately, exist-
ing query expansion techniques which utilize such
knowledge sources as WordNet or Wikipedia are
not useful for symbol expansion. WordNet does
not include organizations in any systematic way.
Wikipedia covers many companies but it is unclear
how it can be used for expansion.
3http://finance.yahoo.com
4http://biz.yahoo.com, http://www.
seekingalpha.com, http://www.marketwatch.
com, http://www.reuters.com, http://www.
fool.com, http://www.thestreet.com, http:
//online.wsj.com, http://www.forbes.com,
http://www.cnbc.com, http://us.ft.com,
http://www.minyanville.com
5http://sourceforge.net/projects/
supersensetag
247
Expansion
Query
Expanded
Query
Relatedness
to Query
Filtering
Relevant
Sentences
Ranking
Novelty
Company
Profile
Yahoo! Finance
Symbol
Summary
News
Figure 1: System architecture
Intuitively, a good expansion method should
provide us with a list of products, or properties,
of the company, the field it operates in, the typi-
cal customers, etc. Such information is normally
found on the profile page of a company at Yahoo!
Finance6. There, so called ?business summaries?
provide succinct and financially relevant informa-
tion about the company. Thus, we use business
summaries as follows. For every company sym-
bol in our collection, we download its business
summary, split it into tokens, remove all words
but nouns and verbs which we then lemmatize.
Since words like company are fairly uninforma-
tive in the context of our task, we do not want to
include them in the expanded query. To filter out
such words, we compute the company-dependent
TF*IDF score for every word on the collection of
all business summaries:
score(w) = tfw,c ? log
?
N
cfw
?
(1)
where c is the business summary of a company,
tfw,c is the frequency of w in c, N is the total
number of business summaries we have, cfw is
the number of summaries that contain w. This
formula penalizes words occurring in most sum-
maries (e.g., company, produce, offer, operate,
found, headquarter, management). At the mo-
ment of running the experiments, N was about
3,000, slightly less than the total number of sym-
6http://finance.yahoo.com/q/pr?s=AAPL
where the trading symbol of any company can be used
instead of AAPL.
bols because some companies do not have a busi-
ness summary on Yahoo! Finance. It is impor-
tant to point out that companies without a business
summary are usually small and are seldom men-
tioned in news articles: for example, these compa-
nies had relevant news articles in only 5% of the
days monitored in this work.
Table 1 gives the ten high scoring words for
three companies (Apple Inc. ? the computer and
software manufacture, Delta Air Lines ? the air-
line, and DaVita ? dyalisis services). Table 1
shows that this approach succeeds in expanding
the symbol with terms directly related to the com-
pany, e.g., ipod for Apple, but also with more gen-
eral information like the industry or the company
operates in, e.g., software and computer for Apple.
All words whose TF*IDF score is above a certain
threshold ? are included in the expanded query (?
was tuned to a value of 5.0 on the development
set).
4 Relatedness to Query
Once the expanded query is generated, it can be
used for sentence ranking. We chose the system of
Otterbacher et al (2005) as a a starting point for
our approach and also as a competitive baseline
because it has been successfully tested in a simi-
lar setting?it has been applied to multi-document
query-focused summarization of news documents.
Given a graph G = (S,E), where S is the set
of all sentences from all input documents, and E is
the set of edges representing normalized sentence
similarities, Otterbacher et al (2005) rank all sen-
248
AAPL DAL DVA
apple air dialysis
music flight davita
mac delta esrd
software lines kidney
ipod schedule inpatient
computer destination outpatient
peripheral passenger patient
movie cargo hospital
player atlanta disease
desktop fleet service
Table 1: Top 10 scoring words for three companies
tence nodes based on the inter-sentence relations
as well as the relevance to the query q. Sentence
ranks are found iteratively over the set of graph
nodes with the following formula:
r(s, q) = ?
rel(s|q)
P
t?S rel(t|q)
+(1??)
X
t?S
sim(s, t)
P
v?S sim(v, t)
r(t, q) (2)
The first term represents the importance of a sen-
tence defined in respect to the query, whereas the
second term infers the importance of the sentence
from its relation to other sentences in the collec-
tion. ? ? (0, 1) determines the relative importance
of the two terms and is found empirically. Another
parameter whose value is determined experimen-
tally is the sentence similarity threshold ? , which
determines the inclusion of a sentence in G. Ot-
terbacher et al (2005) report 0.2 and 0.95 to be
the optimal values for ? and ? respectively. These
values turned out to produce the best results also
on our development set and were used in all our
experiments. Similarity between sentences is de-
fined as the cosine of their vector representations:
sim(s, t) =
P
w?s?t weight(w)
2
q
P
w?s weight(w)2 ?
q
P
w?t weight(w)2
(3)
weight(w) = tfw,sidfw,S (4)
idfw,S = log
( |S| + 1
0.5 + sfw
)
(5)
where tfw,s is the frequency of w in sentence s,
|S| is the total number of sentences in the docu-
ments from which sentences are to be extracted,
and sfw is the number of sentences which contain
the word w (all words in the documents as well
as in the query are stemmed and stopwords are re-
moved from them). Relevance to the query is de-
fined in Equation (6) which has been previously
used for sentence retrieval (Allan et al, 2003):
rel(s|q) =
X
w?q
log(tfw,s + 1) ? log(tfw,q + 1) ? idfw,S (6)
where tfw,x stands for the number of times w ap-
pears in x, be it a sentence (s) or the query (q). If
a sentence shares no words other than stopwords
with the query, the relevance becomes zero. Note
that without the relevance to the query part Equa-
tion 2 takes only inter-sentence similarity into ac-
count and computes the weighted PageRank (Brin
& Page, 1998).
In defining the relevance to the query, in Equa-
tion (6), words which do not appear in too many
sentences in the document collection weigh more.
Indeed, if a word from the query is contained in
many sentences, it should not count much. But it
is also true that not all words from the query are
equally important. As it has been mentioned in
Section 3, words like product or offer appear in
many business summaries and are equally related
to any company. To penalize such words, when
computing the relevance to the query, we multiply
the relevance score of a given word w with the in-
verted document frequency of w on the corpus of
business summaries Q ? idfw,Q:
idfw,Q = log
( |Q|
qfw
)
(7)
We also replace tfw,s with the indicator function
s(w) since it has been reported to be more ad-
equate for sentences, in particular for sentence
alignment (Nelken & Shieber, 2006):
s(w) =
{
1 if s contains w
0 otherwise
(8)
Thus, the modified formula we use to compute
sentence ranks is as follows:
rel(s|q) =
X
w?q
s(w) ? log(tfw,q + 1) ? idfw,S ? idfw,Q (9)
We call these two ranking algorithms that use
the formula in (2) OTTERBACHER and QUERY
WEIGHTS, the difference being the way the rel-
evance to the query is computed: (6) or (9). We
use the OTTERBACHER algorithm as a baseline in
the experiments reported in Section 6.
249
5 Novelty Bias
Apart from being related to the query, a good sum-
mary should provide the user with novel infor-
mation. According to Equation (2), if there are,
say, two sentences which are highly similar to the
query and which share some words, they are likely
to get a very high score. Experimenting with the
development set, we observed that sentences about
the company, such as e.g., DaVita, Inc. is a lead-
ing provider of kidney care in the United States,
providing dialysis services and education for pa-
tients with chronic kidney failure and end stage re-
nal disease, are ranked high although they do not
contribute new information. However, a non-zero
similarity to the query is indeed a good filter of the
information related to the company and to its sec-
tor and can be used as a prerequisite of a sentence
to be included in the summary. These observations
motivate our proposal for a ranking method which
aims at providing relevant and novel information
at the same time.
Here, we explore two alternative approaches to
add the novelty bias to the system:
? The first approach bypasses the relatedness
to query step introduced in Section 4 com-
pletely. Instead, this method merges the dis-
covery of query relatedness and novelty into
a single algorithm, which uses a sentence
graph that contains edges only between sen-
tences related to the query, (i.e., sentences for
which rel(s|q) > 0). All edges connecting
sentences which are unrelated to the query
are skipped in this graph. In this way we limit
the novelty ranking process to a subset of sen-
tences related to the query.
? The second approach models the problem
in a re-ranking architecture: we take the
top ranked sentences after the relatedness-to-
query filtering component (Section 4) and re-
rank them using the novelty formula intro-
duced below.
The main difference between the two approaches
is that the former uses relatedness-to-query and
novelty information but ignores the overall impor-
tance of a sentence as given by the PageRank al-
gorithm in Section 4, while the latter combines all
these aspects ?i.e., importance of sentences, relat-
edness to query, and novelty? using the re-ranking
architecture.
To amend the problem of general information
ranked inappropriately high, we modify the word-
weighting formula (4) so that it implements a nov-
elty bias, thus becoming dependent on the query.
A straightforward way to define the novelty weight
of a word would be to draw a line between the
?known? words, i.e., words appearing in the busi-
ness summary, and the rest. In this approach all
the words from the business summary are equally
related to the company and get the weight of 0:
weight(w) =
{
0 if Q contains w
tfw,sidfw,S otherwise
(10)
We call this weighting scheme SIMPLE. As
an alternative, we also introduce a more elab-
orate weighting procedure which incorporates
the relatedness-to-query (or rather distance from
query) in the word weight formula. Intuitively, the
more related to the query a word is (e.g., DaVita,
the name of the company), the more familiar to the
user it is and the smaller its novelty contribution
is. If a word does not appear in the query at all, its
weight becomes equal to the usual tfw,sidfw,S :
weight(w) =
 
1 ? tfw,q ? idfw,QP
wi?q
tfwi,q ? idfwi,Q
!
? tfw,sidfw,S (11)
The overall novelty ranking formula is based
on the query-dependent PageRank introduced in
Equation (2). However, since we already incorpo-
rate the relatedness to the query in these two set-
tings, we focus only on related sentences and thus
may drop the relatedness to the query part from
(2):
r?(s, q) = ? + (1 ? ?)
?
t?S
sim(s, t, q)
?
u?S sim(t, u, q)
(12)
We set ? to the same value as in OTTERBACHER.
We deliberately set the sentence similarity thresh-
old ? to a very low value (0.05) to prevent the
graph from becoming exceedingly bushy. Note
that this novelty-ranking formula can be equally
applied in both scenarios introduced at the begin-
ning of this section. In the first scenario, S stands
for the set of nodes in the graph that contains only
sentences related to the query. In the second sce-
nario, S contains the highest ranking sentences
detected by the relatedness-to-query component
(Section 4).
250
5.1 Redundancy Filter
Some sentences are repeated several times in the
collection. Such repetitions, which should be
avoided in the summary, can be filtered out ei-
ther before or after the sentence ranking. We ap-
ply a simple repetition check when incrementally
adding ranked sentences to the summary. If a sen-
tence to be added is almost identical to the one
already included in the summary, we skip it. Iden-
tity check is done by counting the percentage of
non-stop word lemmas in common between two
sentences. 95% is taken as the threshold.
We do not filter repetitions before the rank-
ing has taken place because often such repetitions
carry important and relevant information. The re-
dundancy filter is applied to all the systems de-
scribed as they are equally prone to include repe-
titions.
6 Evaluation
We randomly selected 23 company stock names,
and constructed a document collection for each
containing all the news provided in the Yahoo! Fi-
nance news feed for that company in a period of
two days (the time period was chosen randomly).
The average length of a news collection is about
600 tokens. When selecting the company names,
we took care of not picking those which have only
a few news articles for that period of time. This
resulted into 9.4 news articles per collection on av-
erage. From each of these, three human annotators
independently selected up to ten sentences. All an-
notators had average to good understanding of the
financial domain. The annotators were asked to
choose the sentences which could best help them
decide whether to buy, sell or retain stock for the
company the following day and present them in
the order of decreasing importance. The anno-
tators compared their summaries of the first four
collections and clarified the procedure before pro-
ceeding with the other ones. These four collec-
tions were then later used as a development set.
All summaries ? manually as well as automat-
ically generated ? were cut to the first 250 words
which made the summaries 10 words shorter on
average. We evaluated the performance automat-
ically in terms of ROUGE-2 (Lin & Hovy, 2003)
using the parameters and following the methodol-
ogy from the DUC events. The results are pre-
sented in Table 2. We also report the 95% confi-
dence intervals in brackets. As in DUC, we used
METHOD ROUGE-2
Otterbacher 0.255 (0.226 - 0.285)
Query Weights 0.289 (0.254 - 0.324)
Novelty Bias (simple) 0.315 (0.287 - 0.342)
Novelty Bias 0.302 (0.277 - 0.329)
Manual 0.472 (0.415 - 0.531)
Table 2: Results of the four extraction methods
and human annotators
jackknife for each (query, summary) pair and com-
puted a macro-average to make human and au-
tomatic results comparable (Dang, 2005). The
scores computed on summaries produced by hu-
mans are given in the bottom line (MANUAL) and
serve as upper bound and also as an indicator for
the inter-annotator agreement.
6.1 Discussion
From Table 2 follows that the modifications we
applied to the baseline are sensible and indeed
bring an improvement. QUERY WEIGHTS per-
forms better than OTTERBACHER and is in turn
outperformed by the algorithms biased to novel in-
formation (the two NOVELTY systems). The over-
lap between the confidence intervals of the base-
line and the simple version of the novelty algo-
rithm is minimal (0.002).
It is remarkable that the achieved improvement
is due to a more balanced relatedness to the query
ranking (9), as well as to the novelty bias re-
ranking. The fact that the simpler novelty weight-
ing formula (10) produced better results than the
more elaborated one (11) requires a deeper anal-
ysis and a larger test set to explain the difference.
Our conjecture so far is that the SIMPLE approach
allows for a better combination of both novelty
and relatedness to query. Since the more complex
novelty ranking formula penalizes terms related
to the query (Equation (11)), it favors a scenario
where novelty is boosted in detriment of related-
ness to query, which is not always realistic.
It is important to note that, compared with the
baseline, we did not do any parameter tuning for
? and the inter-sentence similarity threshold. The
improvement between the system of Otterbacher
et al (2005) and our best model is statistically
significant.
251
6.2 System Combination
Recall from Section 5 that the motivation for pro-
moting novel information came from the fact that
sentences with background information about the
company obtained very high scores: they were re-
lated but not novel. The sentences ranked by OT-
TERBACHER or QUERY WEIGHTS required a re-
ranking to include related and novel sentences in
the summary. We checked whether novelty re-
ranking brings an improvement if added on top
of a system which does not have a novelty bias
(baseline or QUERY WEIGHTS) and compared it
with the setting where we simply limit the novelty
ranking to all the sentences related to the query
(NOVELTY SIMPLE and NOVELTY). In the simi-
larity graph, we left only edges between the first
30 sentences from the ranked list produced by
one of the two algorithms described in Section 4
(OTTERBACHER or QUERY WEIGHTS). Then we
ranked the sentences biased to novel information
the same way as described in Section 5. The re-
sults are presented in Table 3. What we evalu-
ate here is whether a combination of two methods
performs better than the simple heuristics of dis-
carding edges between sentences unrelated to the
query.
METHOD ROUGE-2
Otterbacher + Novelty simple 0.280 (0.254 - 0.306)
Otterbacher + Novelty 0.273 (0.245 - 0.301)
Query Weights + Novelty simple 0.275 (0.247 - 0.302)
Query Weights + Novelty 0.265 (0.242 - 0.289)
Table 3: Results of the combinations of the four
methods
From the four possible combinations, there is
an improvement over the baseline only (0.255 vs.
0.280 resp. 0.273). None of the combinations per-
forms better than the simple novelty bias algo-
rithm on a subset of edges. This experiment sug-
gests that, at least in the scenario investigated here
(short-term monitoring of publicly-traded compa-
nies), novelty is more important than relatedness
to query. Hence, the simple novelty bias algo-
rithm, which emphasizes novelty and incorporates
relatedness to query only through a loose con-
straint (rel(s|q) > 0) performs better than com-
plex models, which are more constrained by the
relatedness to query.
7 Related Work
Summarization has been extensively investigated
in recent years and to date there exists a multi-
tude of very different systems. Here, we review
those that come closest to ours in respect to the
task and that concern extractive multi-document
query-oriented summarization. We also mention
some work on using textual news data for stock
indices prediction which we are aware of.
Stock market prediction: Wu?thrich et al
(1998) were among the first who introduced an au-
tomatic stock indices prediction system which re-
lies on textual information only. The system gen-
erates weighted rules each of which returns the
probability of the stock going up, down or remain-
ing steady. The only information used in the rules
is the presence or absence of certain keyphrases
provided by a human expert who ?judged them
to be influential factors potentially moving stock
markets?. In this approach, training data is re-
quired to measure the usefulness of the keyphrases
for each of the three classes. More recently, Ler-
man et al (2008) introduced a forecasting system
for prediction markets that combines news anal-
ysis with a price trend analysis model. This ap-
proach was shown to be successful for the fore-
casting of public opinion about political candi-
dates in such prediction markets. Our approach
can be seen as a complement to both these ap-
proaches, necessary especially for financial mar-
kets where the news typically cover many events,
only some related to the company of interest.
Unsupervized summarization systems extract
sentences whose relevance can be inferred from
the inter-sentence relations in the document col-
lection. In (Radev et al, 2000), the centroid of
the collection, i.e., the words with the highest
TF*IDF, is considered and the sentences which
contain more words from the centroid are ex-
tracted. Mihalcea & Tarau (2004) explore sev-
eral methods developed for ranking documents
in information retrieval for the single-document
summarization task. Similarly, Erkan & Radev
(2004) apply in-degree and PageRank to build a
summary from a collection of related documents.
They show that their method, called LexRank,
achieves good results. In (Otterbacher et al, 2005;
Erkan, 2006) the ranking function of LexRank is
extended to become applicable to query-focused
summarization. The rank of a sentence is deter-
mined not just by its relation to other sentences in
252
the document collection but also by its relevance
to the query. Relevance to the query is defined as
the word-based similarity between query and sen-
tence.
Query expansion has been used for improv-
ing information retrieval (IR) or question answer-
ing (QA) systems with mixed results. One of the
problems is that the queries are expanded word
by word, ignoring the context and as a result the
extensions often become inadequate7. However,
Riezler et al (2007) take the entire query into ac-
count when adding new words by utilizing tech-
niques used in statistical machine translation.
Query expansion for summarization has not yet
been explored as extensively as in IR or QA.
Nastase (2008) uses Wikipedia and WordNet for
query expansion and proposes that a concept can
be expanded by adding the text of all hyper-
links from the first paragraph of the Wikipedia
article about this concept. The automatic eval-
uation demonstrates that extracting relevant con-
cepts from Wikipedia leads to better performance
compared with WordNet: both expansion systems
outperform the no-expansion version in terms of
the ROUGE score. Although this method proved
helpful on the DUC data, it seems less appropriate
for expanding company names. For small compa-
nies there are short articles with only a few links;
the first paragraphs of the articles about larger
companies often include interesting rather than
relevant information. For example, the text pre-
ceding the contents box in the article about Apple
Inc. (AAPL) states that ?Fortune magazine named
Apple the most admired company in the United
States?8. The link to the article about the For-
tune magazine can be hardly considered relevant
for the expansion of AAPL. Wikipedia category
information, which has been successfully used in
some NLP tasks (Ponzetto & Strube, 2006, inter
alia), is too general and does not help discriminate
between two companies from the same sector.
Our work suggests that query expansion is
needed for summarization in the financial domain.
In addition to previous work, we also show that an-
other key factor for success in this task is detecting
and modeling the novelty of the target content.
7E.g., see the proceedings of TREC 9, TREC 10: http:
//trec.nist.gov.
8Checked on September 17, 2008.
8 Conclusions
In this paper we presented a multi-document
company-oriented summarization algorithm
which extracts sentences that are both relevant for
the given organization and novel to the user. The
system is expected to be useful in the context of
stock market monitoring and forecasting, that is,
to help the trader predict the move of the stock
price for the given company. We presented a
novel query expansion method which works par-
ticularly well in the context of company-oriented
summarization. Our sentence ranking method is
unsupervized and requires little parameter tuning.
An automatic evaluation against a competitive
baseline showed supportive results, indicating that
the ranking algorithm is able to select relevant
sentences and promote novel information at the
same time.
In the future, we plan to experiment with po-
sitional features which have proven useful for
generic summarization. We also plan to test the
system extrinsically. For example, it would be of
interest to see if a classifier may predict the move
of stock prices based on a set of features extracted
from company-oriented summaries.
Acknowledgments: We would like to thank the
anonymous reviewers for their helpful feedback.
References
Allan, James, Courtney Wade & Alvaro Bolivar
(2003). Retrieval and novelty detection at the
sentence level. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval Toronto, On., Canada, 28 July ? 1 Au-
gust 2003, pp. 314?321.
Atserias, Jordi, Hugo Zaragoza, Massimiliano
Ciaramita & Giuseppe Attardi (2008). Se-
mantically annotated snapshot of the English
Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and
Evaluation, Marrakech, Morocco, 26 May ? 1
June 2008.
Brin, Sergey & Lawrence Page (1998). The
anatomy of a large-scale hypertextual web
search engine. Computer Networks and ISDN
Systems, 30(1?7):107?117.
Ciaramita, Massimiliano & Yasemin Altun
(2006). Broad-coverage sense disambiguation
253
and information extraction with a supersense
sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia,
22?23 July 2006, pp. 594?602.
Dang, Hoa Trang (2005). Overview of DUC
2005. In Proceedings of the 2005 Document
Understanding Conference held at the Human
Language Technology Conference and Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 9?
10 October 2005.
Erkan, Gu?nes? (2006). Using biased random walks
for focused summarization. In Proceedings
of the 2006 Document Understanding Confer-
ence held at the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,,
New York, N.Y., 8?9 June 2006.
Erkan, Gu?nes? & Dragomir R. Radev (2004).
LexRank: Graph-based lexical centrality as
salience in text summarization. Journal of Arti-
ficial Intelligence Research, 22:457?479.
Lerman, Kevin, Ari Gilder, Mark Dredze & Fer-
nando Pereira (2008). Reading the markets:
Forecasting public opinion of political candi-
dates by news analysis. In Proceedings of
the 22st International Conference on Computa-
tional Linguistics, Manchester, UK, 18?22 Au-
gust 2008, pp. 473?480.
Lin, Chin-Yew & Eduard H. Hovy (2003). Au-
tomatic evaluation of summaries using N-gram
co-occurrence statistics. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, Edmonton, Alberta,
Canada, 27 May ?1 June 2003, pp. 150?157.
Mihalcea, Rada & Paul Tarau (2004). Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Nat-
ural Language Processing, Barcelona, Spain,
25?26 July 2004, pp. 404?411.
Nastase, Vivi (2008). Topic-driven multi-
document summarization with encyclopedic
knowledge and activation spreading. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Hon-
olulu, Hawaii, 25?27 October 2008. To appear.
Nelken, Rani & Stuart M. Shieber (2006). To-
wards robust context-sensitive sentence align-
ment for monolingual corpora. In Proceedings
of the 11th Conference of the European Chapter
of the Association for Computational Linguis-
tics, Trento, Italy, 3?7 April 2006, pp. 161?168.
Otterbacher, Jahna, Gu?nes? Erkan & Dragomir
Radev (2005). Using random walks for
question-focused sentence retrieval. In Pro-
ceedings of the Human Language Technology
Conference and the 2005 Conference on Empir-
ical Methods in Natural Language Processing,
Vancouver, B.C., Canada, 6?8 October 2005,
pp. 915?922.
Ponzetto, Simone Paolo & Michael Strube (2006).
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Pro-
ceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
New York, N.Y., 4?9 June 2006, pp. 192?199.
Radev, Dragomir R., Hongyan Jing & Malgorzata
Budzikowska (2000). Centroid-based summa-
rization of mutliple documents: Sentence ex-
traction, utility-based evaluation, and user stud-
ies. In Proceedings of the Workshop on Au-
tomatic Summarization at ANLP/NAACL 2000,
Seattle, Wash., 30 April 2000, pp. 21?30.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal & Yi Liu (2007).
Statistical machine translation for query expan-
sion in answer retrieval. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Re-
public, 23?30 June 2007, pp. 464?471.
Wu?thrich, B, D. Permunetilleke, S. Leung, V. Cho,
J. Zhang & W. Lam (1998). Daily prediction of
major stock indices from textual WWW data. In
In Proceedings of the 4th International Confer-
ence on Knowledge Discovery and Data Mining
- KDD-98, pp. 364?368.
254
Proceedings of NAACL HLT 2009: Short Papers, pages 225?228,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tree Linearization in English:
Improving Language Model Based Approaches
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We compare two approaches to dependency
tree linearization, a task which arises in many
NLP applications. The first one is the widely
used ?overgenerate and rank? approach which
relies exclusively on a trigram language model
(LM); the second one combines language
modeling with a maximum entropy classifier
trained on a range of linguistic features. The
results provide strong support for the com-
bined method and show that trigram LMs are
appropriate for phrase linearization while on
the clause level a richer representation is nec-
essary to achieve comparable performance.
1 Introduction
To date, many natural language processing appli-
cations rely on syntactic representations and also
modify them by compressing, fusing, or translating
into a different language. A syntactic tree emerg-
ing as a result of such operations has to be lin-
earized to a string of words before it can be out-
put to the end-user. The simple and most widely
used trigram LM has become a standard tool for
tree linearization in English (Langkilde & Knight,
1998). For languages with less rigid word order,
LM-based approaches have been shown to perform
poorly (e.g., Marsi & Krahmer (2005) for Dutch),
and methods relying on a range of linguistic fea-
tures have been successfully applied instead (see
Uchimoto et al (2000) and Ringger et al (2004),
Filippova & Strube (2007) for Japanese and German
resp.). To our knowledge, none of the linearization
studies have compared a LM-based method with
an alternative. Thus, it would be of interest to
draw such a comparison, especially on English data,
where LMs are usually expected to work well.
As an improvement to the LM-based approach,
we propose a combined method which distinguishes
between the phrase and the clause levels:
? it relies on a trigram LM to order words within
phrases;
? it finds the order of clause constituents (i.e.,
constituents dependent on a finite verb) with a
maximum entropy classifier trained on a range
of linguistic features.
We show that such a differentiated approach is ben-
eficial and that the proposed combination outper-
forms the method which relies solely on a LM.
Hence, our results challenge the widespread attitude
that trigram LMs provide an appropriate way to lin-
earize syntactic trees in English but also indicate
that they perform well in linearizing subtrees cor-
responding to phrases.
2 LM-based Approach
Trigram models are easy to build and use, and it has
been shown that more sophisticated n-gram models
(e.g., with higher n, complex smoothing techniques,
skipping, clustering or caching) are often not worth
the effort of implementing them due to data sparse-
ness and other issues (Goodman, 2001). This ex-
plains the popularity of trigram LMs in a variety
of NLP tasks (Jurafsky & Martin, 2008), in partic-
ular, in tree linearization where they have become
225
brothers
predet det
the of
neighbor
my
poss
pobj
prep
all
Figure 1: A tree of the noun phrase all the brothers of my
neighbor
de facto the standard tree linearization tool in ac-
cordance with the ?overgenerate and rank? principle:
given a syntactic tree, one needs to consider all pos-
sible linearizations and then choose the one with the
lowest entropy. Given a projective dependency tree1,
all linearizations can be found recursively by gener-
ating permutations of a node and its children. Unfor-
tunately, the number of possible permutations grows
factorially with the branching factor. Hence it is
highly desirable to prohibit generation of clearly un-
acceptable permutations by putting hard constraints
encoded in the English grammar. The constraints
which we implement in our study are the following:
determiners, possessives, quantifiers and noun or ad-
jective modifiers always precede their heads. Con-
junctions, coordinated elements, prepositional ob-
jects always follow their heads. These constraints
allow us to limit, e.g., the total of 96 (2 ? 2 ? 4!)
possibilities for the tree corresponding to the phrase
all the brothers of my neighbor (see Figure 1) to only
two (all the brothers of my neighbor, the all brothers
of my neighbor).
Still, even with such constraints, in some cases the
list of possible linearizations is too long and has to
be reduced to the first N , where N is supposed to be
sufficiently large. In our experiments we break the
permutation generation process if the limit of 20,000
variants is reached.
3 Combined Approach
The LM approach described above has at least two
disadvantages: (1) long distance dependencies are
not captured, and (2) the list of all possible lineariza-
tions can be huge which makes the search for the
1Note that a phrase structure tree can be converted into a
dependency tree, and some PCFG parsers provide this option.
best string unfeasible. However, our combined ap-
proach is based on the premise that trigram LMs are
well-suited for finding the order within NPs, PPs and
other phrases where the head is not a finite verb.
E.g., given a noun modified by the words big, red
and the, a LM can reliably rank the correct order
higher than incorrect ones ( the big red N vs. the red
big N, etc.).
Next, on the clause level, for every finite verb in
the tree we find the order of its dependents using the
method which we originally developed for German
(Filippova & Strube, 2007), which utilizes a range
of such linguistic features as PoS tag, syntactic role,
length in words, pronominalization, semantic class,
etc.2 For the experiments presented in this paper, we
train two maximum entropy classifiers on all but the
semantic features:
1. The first classifier determines the best starting
point for a sentence: for each constituent de-
pendent on the verb it returns the probability of
this constituent being the first one in a sentence.
The subject and also adjuncts (e.g. temporal ad-
juncts like yesterday) are usually found in the
beginning of the sentence.
2. The second classifier is trained to determine
whether the precedence relation holds between
two adjacent constituents and is applied to all
constituents but the one selected by the first
classifier. The precedence relation defined by
this classifier has been shown to be transitive
and thus can be used to sort randomly ordered
constituents. Note that we do not need to con-
sider all possible orders to find the best one.
Once the order within clause constituents as well as
the order among them is found, the verb is placed
right after the subject. The verb placing step com-
pletes the linearization process.
The need for two distinct classifiers can be illus-
trated with the following example:
(1) a [Earlier today] [she] sent [him] [an email].
b [She] sent [him] [an email] [earlier today].
c *[She] sent [earlier today] [him] [an email].
2See the cited paper for the full list of features and imple-
mentation details.
226
(1a,b) are grammatical while (1c) is hardly accept-
able, and no simple precedence rule can be learned
from pairs of constituents in (1a) and (1b): the tem-
poral adjunct earlier today can precede or follow
each of the other constituents dependent on the verb
(she, him, an email). Thus, the classifier which
determines the precedence relation is not enough.
However, an adequate rule can be inferred with
an additional classifier trained to find good starting
points: a temporal adjunct may appear as the first
constituent in a sentence; if it is not chosen for this
position, it should be preceded by the pronominal-
ized subject (she), the indirect object (him) and the
short non-pronominalized object (an email).
4 Experiments
The goal of our experiments is to check the follow-
ing hypotheses:
1. That trigram LMs are well-suited for phrase
linearization.
2. That there is a considerable drop in perfor-
mance when one uses them for linearization on
the clause level.
3. That an approach which uses a richer represen-
tation on the clause level is more appropriate.
4.1 Data
We take a subset of the TIPSTER3 corpus ? all Wall
Street Journal articles from the period of 1987-92
(approx. 72 mill. words) ? and automatically anno-
tate them with sentence boundaries, part of speech
tags and dependency relations using the Stanford
parser (Klein & Manning, 2003). We reserve a
small subset of about 600 articles (340,000 words)
for testing and use the rest to build a trigram LM
with the CMU toolkit (Clarkson & Rosenfeld, 1997,
with Good-Turing smoothing and vocabulary size of
30,000). To train the maximum entropy classifiers
we use about 41,000 sentences.
4.2 Evaluation
To test the trigram-based approach, we generate all
possible permutations of clause constituents, place
3Description at http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC93T3A.
the verb right after the subject and then rank the re-
sulting strings with the LM taking the information
on sentence boundaries into account. To test the
combined approach, we find the best candidate for
the first position in the clause, then put the remain-
ing constituents in a random order, and finally sort
them by consulting the second classifier.
The purpose of the evaluation is to assess how
good a method is at reproducing the input from its
dependency tree. We separately evaluate the perfor-
mance on the phrase and the clause levels. When
comparing the two methods on the clause level, we
take the clause constituents as they are presented
in the input sentence. Although English allows for
some minor variation in word order and it might
happen that the generated order is not necessarily
wrong if different from the original one, we do not
expect this to happen often and evaluate the perfor-
mance rigorously: only the original order counts as
the correct one. The default evaluation metric is per-
phrase/per-clause accuracy:
acc = |correct||total|
Other metrics we use to measure how different a
generated order of N elements is from the correct
one are:
1. Kendall?s ? , ? = 1 ? 4 tN(N?1) where t is
the minimum number of interchanges of con-
secutive elements to achieve the right order
(Kendall, 1938; Lapata, 2006).
2. Edit distance related di, di = 1 ? mN where m
is the minimum number of deletions combined
with insertions to get to the right order (Ringger
et al, 2004).
E.g., on the phrase level, the incorrectly generated
phrase the all brothers of my neighbor (?1-0-2-3-4-
5?) gets ? = 0.87, di = 0.83. Likewise, given the
input sentence from (1a), the incorrectly generated
order of the four clause constituents in (1c) ? ?1-0-
2-3? ? gets ? of 0.67 and di of 0.75.
4.3 Results
The results of the experiments on the phrase and the
clause levels are presented in Tables 1 and 2 respec-
tively. From the total of 5,000 phrases, 55 (about
227
1%) were discarded because the number of admis-
sible linearizations exceeded the limit of 20,000. In
the first row of Table 1 we give the results for cases
where, with all constraints applied, there were still
several possible linearizations (non-triv; 1,797); the
second row is for all phrases which were longer than
one word (> 1; 2,791); the bottom row presents the
results for the total of 4,945 phrases (all).
acc ? di
non-triv 76% 0.85 0.94
> 1 85% 0.90 0.96
all 91% 0.94 0.98
Table 1: Results of the trigram method on the phrase level
Table 2 presents the results of the trigram-based
(TRIGRAM) and combined (COMBINED) methods on
the clause level. Here, we filtered out trivial cases
and considered only clauses which had at least two
constituents dependent on the verb (approx. 5,000
clauses in total).
acc ? di
TRIGRAM 49% 0.49 0.81
COMBINED 67% 0.71 0.88
Table 2: Results of the two methods on the clause level
4.4 Discussion
The difference in accuracy between the performance
of the trigram model on the phrase and the clause
level is considerable ? 76% vs. 49%. The accuracy
of 76% is remarkable given that the average length
of phrases which counted as non-triv is 6.2 words,
whereas the average clause length in constituents is
3.3. This statistically significant difference in per-
formance supports our hypothesis that the ?overgen-
erate and rank? approach advocated in earlier studies
is more adequate for finding the optimal order within
phrases. The ? value of 0.85 also indicates that many
of the wrong phrase linearizations were near misses.
On the clause level, where long distance dependen-
cies are frequent, an approach which takes a range
of grammatical features into account is more appro-
priate ? this is confirmed by the significantly better
results of the combined method (67%).
5 Conclusions
We investigated two tree linearization methods in
English: the mainstream trigram-based approach
and the one which combines a trigram LM on the
phrase level with two classifiers trained on a range
of linguistic features on the clause level. The results
demonstrate (1) that the combined approach repro-
duces the word order more accurately, and (2) that
the performance of the trigram LM-based method on
phrases is significantly better than on clauses.
Acknowledgments: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would like to thank the
anonymous reviewers for their feedback.
References
Clarkson, P. & R. Rosenfeld (1997). Statistical language
modeling using the CMU-Cambridge toolkit. In Proc.
of EUROSPEECH-97, pp. 2707?2710.
Filippova, K. & M. Strube (2007). Generating constituent
order in German clauses. In Proc. of ACL-07, pp. 320?
327.
Goodman, J. T. (2001). A bit of progress in language
modeling. Computer Speech and Language, pp. 403?
434.
Jurafsky, D. & J. H. Martin (2008). Speech and Language
Processing. Upper Saddle River, N.J.: Prentice Hall.
Kendall, M. G. (1938). A new measure of rank correla-
tion. Biometrika, 30:81?93.
Klein, D. & C. D. Manning (2003). Accurate unlexical-
ized parsing. In Proc. of ACL-03, pp. 423?430.
Langkilde, I. & K. Knight (1998). Generation that ex-
ploits corpus-based statistical knowledge. In Proc. of
COLING-ACL-98, pp. 704?710.
Lapata, M. (2006). Automatic evaluation of information
ordering: Kendall?s tau. Computational Linguistics,
32(4):471?484.
Marsi, E. & E. Krahmer (2005). Explorations in sentence
fusion. In Proc. of ENLG-05, pp. 109?117.
Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets
& S. Corston-Oliver (2004). Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proc. of COLING-04, pp.
673?679.
Uchimoto, K., M. Murata, Q. Ma, S. Sekine & H. Isahara
(2000). Word order acquisition from corpora. In Proc.
of COLING-00, pp. 871?877.
228
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320?327,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generating Constituent Order in German Clauses
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We investigate the factors which determine
constituent order in German clauses and pro-
pose an algorithm which performs the task
in two steps: First, the best candidate for
the initial sentence position is chosen. Then,
the order for the remaining constituents is
determined. The first task is more difficult
than the second one because of properties
of the German sentence-initial position. Ex-
periments show a significant improvement
over competing approaches. Our algorithm
is also more efficient than these.
1 Introduction
Many natural languages allow variation in the word
order. This is a challenge for natural language gen-
eration and machine translation systems, or for text
summarizers. E.g., in text-to-text generation (Barzi-
lay & McKeown, 2005; Marsi & Krahmer, 2005;
Wan et al, 2005), new sentences are fused from de-
pendency structures of input sentences. The last step
of sentence fusion is linearization of the resulting
parse. Even for English, which is a language with
fixed word order, this is not a trivial task.
German has a relatively free word order. This
concerns the order of constituents1 within sentences
while the order of words within constituents is rela-
tively rigid. The grammar only partially prescribes
how constituents dependent on the verb should be
ordered, and for many clauses each of the n! possi-
ble permutations of n constituents is grammatical.
1Henceforth, we will use this term to refer to constituents
dependent on the clausal top node, i.e. a verb, only.
In spite of the permanent interest in German word
order in the linguistics community, most studies
have limited their scope to the order of verb argu-
ments and few researchers have implemented ? and
even less evaluated ? a generation algorithm. In this
paper, we present an algorithm, which orders not
only verb arguments but all kinds of constituents,
and evaluate it on a corpus of biographies. For
each parsed sentence in the test set, our maximum-
entropy-based algorithm aims at reproducing the or-
der found in the original text. We investigate the
importance of different linguistic factors and sug-
gest an algorithm to constituent ordering which first
determines the sentence initial constituent and then
orders the remaining ones. We provide evidence
that the task requires language-specific knowledge
to achieve better results and point to the most diffi-
cult part of it. Similar to Langkilde & Knight (1998)
we utilize statistical methods. Unlike overgenera-
tion approaches (Varges & Mellish, 2001, inter alia)
which select the best of all possible outputs ours is
more efficient, because we do not need to generate
every permutation.
2 Theoretical Premises
2.1 Background
It has been suggested that several factors have an in-
fluence on German constituent order. Apart from
the constraints posed by the grammar, information
structure, surface form, and discourse status have
also been shown to play a role. It has also been
observed that there are preferences for a particular
order. The preferences summarized below have mo-
320
tivated our choice of features:
? constituents in the nominative case precede
those in other cases, and dative constituents
often precede those in the accusative case
(Uszkoreit, 1987; Keller, 2000);
? the verb arguments? order depends on the
verb?s subcategorization properties (Kurz,
2000);
? constituents with a definite article precede
those with an indefinite one (Weber & Mu?ller,
2004);
? pronominalized constituents precede non-
pronominalized ones (Kempen & Harbusch,
2004);
? animate referents precede inanimate ones (Pap-
pert et al, 2007);
? short constituents precede longer ones (Kim-
ball, 1973);
? the preferred topic position is right after the
verb (Frey, 2004);
? the initial position is usually occupied by
scene-setting elements and topics (Speyer,
2005).
? there is a default order based on semantic prop-
erties of constituents (Sgall et al, 1986):
Actor < Temporal < SpaceLocative < Means < Ad-
dressee < Patient < Source < Destination < Purpose
Note that most of these preferences were identified
in corpus studies and experiments with native speak-
ers and concern the order of verb arguments only.
Little has been said so far about how non-arguments
should be ordered.
German is a verb second language, i.e., the po-
sition of the verb in the main clause is determined
exclusively by the grammar and is insensitive to
other factors. Thus, the German main clause is di-
vided into two parts by the finite verb: Vorfeld (VF),
which contains exactly one constituent, and Mit-
telfeld (MF), where the remaining constituents are
located. The subordinate clause normally has only
MF. The VF and MF are marked with brackets in
Example 1:
(1) [Au?erdem]
Apart from that
entwickelte
developed
[Lummer
Lummer
eine
a
Quecksilberdampflampe,
Mercury-vapor lamp
um
to
monochromatisches
monochrome
Licht
light
herzustellen].
produce.
?Apart from that, Lummer developed a
Mercury-vapor lamp to produce monochrome
light?.
2.2 Our Hypothesis
The essential contribution of our study is that we
treat preverbal and postverbal parts of the sentence
differently. The sentence-initial position, which in
German is the VF, has been shown to be cognitively
more prominent than other positions (Gernsbacher
& Hargreaves, 1988). Motivated by the theoretical
work by Chafe (1976) and Jacobs (2001), we view
the VF as the place for elements which modify the
situation described in the sentence, i.e. for so called
frame-setting topics (Jacobs, 2001). For example,
temporal or locational constituents, or anaphoric ad-
verbs are good candidates for the VF. We hypoth-
esize that the reasons which bring a constituent to
the VF are different from those which place it, say,
to the beginning of the MF, for the order in the MF
has been shown to be relatively rigid (Keller, 2000;
Kempen & Harbusch, 2004). Speakers have the
freedom of selecting the outgoing point for a sen-
tence. Once they have selected it, the remaining con-
stituents are arranged in the MF, mainly according to
their grammatical properties.
This last observation motivates another hypothe-
sis we make: The cumulation of the properties of
a constituent determines its salience. This salience
can be calculated and used for ordering with a sim-
ple rule stating that more salient constituents should
precede less salient ones. In this case there is no
need to generate all possible orders and rank them.
The best order can be obtained from a random one
by sorting. Our experiments support this view. A
two-step approach, which first selects the best can-
didate for the VF and then arranges the remaining
constituents in the MF with respect to their salience
performs better than algorithms which generate the
order for a sentence as a whole.
321
3 Related Work
Uszkoreit (1987) addresses the problem from a
mostly grammar-based perspective and suggests
weighted constraints, such as [+NOM] ? [+DAT],
[+PRO] ? [?PRO], [?FOCUS] ? [+FOCUS], etc.
Kruijff et al (2001) describe an architecture
which supports generating the appropriate word or-
der for different languages. Inspired by the findings
of the Prague School (Sgall et al, 1986) and Sys-
temic Functional Linguistics (Halliday, 1985), they
focus on the role that information structure plays
in constituent ordering. Kruijff-Korbayova? et al
(2002) address the task of word order generation in
the same vein. Similar to ours, their algorithm rec-
ognizes the special role of the sentence-initial po-
sition which they reserve for the theme ? the point
of departure of the message. Unfortunately, they did
not implement their algorithm, and it is hard to judge
how well the system would perform on real data.
Harbusch et al (2006) present a generation work-
bench, which has the goal of producing not the most
appropriate order, but all grammatical ones. They
also do not provide experimental results.
The work of Uchimoto et al (2000) is done on
the free word order language Japanese. They de-
termine the order of phrasal units dependent on the
same modifiee. Their approach is similar to ours in
that they aim at regenerating the original order from
a dependency parse, but differs in the scope of the
problem as they regenerate the order of modifers for
all and not only for the top clausal node. Using a
maximum entropy framework, they choose the most
probable order from the set of all permutations of n
words by the following formula:
P (1|h) = P ({Wi,i+j = 1|1 ? i ? n? 1, 1 ? j ? n? i}|h)
?
n?1
Y
i=1
n?i
Y
j=1
P (Wi,i+j = 1|hi,i+j)
=
n?1
Y
i=1
n?i
Y
j=1
PME(1|hi,i+j)
(1)
For each permutation, for every pair of words , they
multiply the probability of their being in the correct2
order given the history h. Random variable Wi,i+j
2Only reference orders are assumed to be correct.
is 1 if word wi precedes wi+j in the reference sen-
tence, 0 otherwise. The features they use are akin
to those which play a role in determining German
word order. We use their approach as a non-trivial
baseline in our study.
Ringger et al (2004) aim at regenerating the or-
der of constituents as well as the order within them
for German and French technical manuals. Utilizing
syntactic, semantic, sub-categorization and length
features, they test several statistical models to find
the order which maximizes the probability of an or-
dered tree. Using ?Markov grammars? as the start-
ing point and conditioning on the syntactic category
only, they expand a non-terminal node C by predict-
ing its daughters from left to right:
P (C|h) =
n
Y
i=1
P (di|di?1, ..., di?j , c, h) (2)
Here, c is the syntactic category of C, d and h
are the syntactic categories of C?s daughters and the
daughter which is the head of C respectively.
In their simplest system, whose performance is
only 2.5% worse than the performance of the best
one, they condition on both syntactic categories and
semantic relations (?) according to the formula:
P (C|h) =
n
Y
i=1
?
P (?i|di?1, ?i?1, ...di?j , ?i?j , c, h)
?P (di|?i, di?1, ?i?1..., di?j , ?i?j , c, h)
?
(3)
Although they test their system on German data,
it is hard to compare their results to ours directly.
First, the metric they use does not describe the per-
formance appropriately (see Section 6.1). Second,
while the word order within NPs and PPs as well as
the verb position are prescribed by the grammar to a
large extent, the constituents can theoretically be or-
dered in any way. Thus, by generating the order for
every non-terminal node, they combine two tasks of
different complexity and mix the results of the more
difficult task with those of the easier one.
4 Data
The data we work with is a collection of biogra-
phies from the German version of Wikipedia3. Fully
automatic preprocessing in our system comprises
the following steps: First, a list of people of a
certain Wikipedia category is taken and an article
is extracted for every person. Second, sentence
3http://de.wikipedia.org
322
entwickelte
um herzustellen SUB
monochromatisches Licht
eine Quecksilberdampflampe OBJAau?erdem ADV (conn)Lummer SUBJ (pers)
Figure 1: The representation of the sentence in Example 1
boundaries are identified with a Perl CPAN mod-
ule4 whose performance we improved by extend-
ing the list of abbreviations. Next, the sentences
are split into tokens. The TnT tagger (Brants, 2000)
and the TreeTagger (Schmid, 1997) are used for tag-
ging and lemmatization. Finally, the articles are
parsed with the CDG dependency parser (Foth &
Menzel, 2006). Named entities are classified accord-
ing to their semantic type using lists and category
information from Wikipedia: person (pers), location
(loc), organization (org), or undefined named entity
(undef ne). Temporal expressions (Oktober 1915,
danach (after that) etc.) are identified automatically
by a set of patterns. Inevitable during automatic an-
notation, errors at one of the preprocessing stages
cause errors at the ordering stage.
Distinguishing between main and subordinate
clauses, we split the total of about 19 000 sentences
into training, development and test sets (Table 1).
Clauses with one constituent are sorted out as trivial.
The distribution of both types of clauses according
to their length in constituents is given in Table 2.
train dev test
main 14324 3344 1683
sub 3304 777 408
total 17628 4121 2091
Table 1: Size of the data sets in clauses
2 3 4 5 6+
main 20% 35% 27% 12% 6%
sub 49% 35% 11% 2% 3%
Table 2: Proportion of clauses with certain lengths
4http://search.cpan.org/?holsten/Lingua-DE-Sentence-
0.07/Sentence.pm
Given the sentence in Example 1, we first trans-
form its dependency parse into a more general
representation (Figure 15) and then, based on the
predictions of our learner, arrange the four con-
stituents. For evaluation, we compare the arranged
order against the original one.
Note that we predict neither the position of the
verb, nor the order within constituents as the former
is explicitly determined by the grammar, and the lat-
ter is much more rigid than the order of constituents.
5 Baselines and Algorithms
We compare the performance of two our algorithms
with four baselines.
5.1 Random
We improve a trivial random baseline (RAND) by
two syntax-oriented rules: the first position is re-
served for the subject and the second for the direct
object if there is any; the order of the remaining con-
stituents is generated randomly (RAND IMP).
5.2 Statistical Bigram Model
Similar to Ringger et al (2004), we find the order
with the highest probability conditioned on syntac-
tic and semantic categories. Unlike them we use de-
pendency parses and compute the probability of the
top node only, which is modified by all constituents.
With these adjustments the probability of an order
O given the history h, if conditioned on syntactic
functions of constituents (s1...sn), is simply:
P (O|h) =
n
?
i=1
P (si|si?1, h) (4)
Ringger et al (2004) do not make explicit, what
their set of semantic relations consists of. From the
5OBJA stands for the accusative object.
323
example in the paper, it seems that these are a mix-
ture of lexical and syntactic information6. Our anno-
tation does not specify semantic relations. Instead,
some of the constituents are categorized as pers, loc,
temp, org or undef ne if their heads bear one of these
labels. By joining these with possible syntactic func-
tions, we obtain a larger set of syntactic-semantic
tags as, e.g., subj-pers, pp-loc, adv-temp. We trans-
form each clause in the training set into a sequence
of such tags, plus three tags for the verb position (v),
the beginning (b) and the end (e) of the clause. Then
we compute the bigram probabilities7.
For our third baseline (BIGRAM), we select from
all possible orders the one with the highest probabil-
ity as calculated by the following formula:
P (O|h) =
n
?
i=1
P (ti|ti?1, h) (5)
where ti is from the set of joined tags. For Example
1, possible tag sequences (i.e. orders) are ?b subj-
pers v adv obja sub e?, ?b adv v subj-pers obja sub
e?, ?b obja v adv sub subj-pers e?, etc.
5.3 Uchimoto
For the fourth baseline (UCHIMOTO), we utilized a
maximum entropy learner (OpenNLP8) and reim-
plemented the algorithm of Uchimoto et al (2000).
For every possible permutation, its probability is es-
timated according to Formula (1). The binary clas-
sifier, whose task was to predict the probability that
the order of a pair of constituents is correct, was
trained on the following features describing the verb
or hc ? the head of a constituent c9:
vlex, vpass, vmod the lemma of the root of the
clause (non-auxiliary verb), the voice of the
verb and the number of constituents to order;
lex the lemma of hc or, if hc is a functional word,
the lemma of the word which depends on it;
pos part-of-speech tag of hc;
6E.g. DefDet, Coords, Possr, werden
7We use the CMU Toolkit (Clarkson & Rosenfeld, 1997).
8http://opennlp.sourceforge.net
9We disregarded features which use information specific to
Japanese and non-applicable to German (e.g. on postpositional
particles).
sem if defined, the semantic class of c; e.g. im April
1900 and mit Albert Einstein (with Albert Ein-
stein) are classified temp and pers respectively;
syn, same the syntactic function of hc and whether
it is the same for the two constituents;
mod number of modifiers of hc;
rep whether hc appears in the preceding sentence;
pro whether c contains a (anaphoric) pronoun.
5.4 Maximum Entropy
The first configuration of our system is an extended
version of the UCHIMOTO baseline (MAXENT). To
the features describing c we added the following
ones:
det the kind of determiner modifying hc (def, indef,
non-appl);
rel whether hc is modified by a relative clause (yes,
no, non-appl);
dep the depth of c;
len the length of c in words.
The first two features describe the discourse status
of a constituent; the other two provide information
on its ?weight?. Since our learner treats all values
as nominal, we discretized the values of dep and len
with a C4.5 classifier (Kohavi & Sahami, 1996).
Another modification concerns the efficiency of
the algorithm. Instead of calculating probabilities
for all pairs, we obtain the right order from a random
one by sorting. We compare adjacent elements by
consulting the learner as if we would sort an array of
numbers. Given two adjacent constituents, ci < cj ,
we check the probability of their being in the right
order, i.e. that ci precedes cj : Ppre(ci, cj). If it is
less than 0.5, we transpose the two and compare ci
with the next one.
Since the sorting method presupposes that the pre-
dicted relation is transitive, we checked whether this
is really so on the development and test data sets. We
looked for three constituents ci, cj , ck from a sen-
tence S, such that Ppre(ci, cj) > 0.5, Ppre(cj , ck) >
0.5, Ppre(ci, ck) < 0.5 and found none. Therefore,
unlike UCHIMOTO, where one needs to make exactly
N ! ? N(N ? 1)/2 comparisons, we have to make
N(N ? 1)/2 comparisons at most.
324
5.5 The Two-Step Approach
The main difference between our first algorithm
(MAXENT) and the second one (TWO-STEP) is that
we generate the order in two steps10 (both classifiers
are trained on the same features):
1. For the VF, using the OpenNLP maximum en-
tropy learner for a binary classification (VF vs.
MF), we select the constituent c with the high-
est probability of being in the VF.
2. For the MF, the remaining constituents are put
into a random order and then sorted the way it
is done for MAXENT. The training data for the
second task was generated only from the MF of
clauses.
6 Results
6.1 Evaluation Metrics
We use several metrics to evaluate our systems and
the baselines. The first is per-sentence accuracy
(acc) which is the proportion of correctly regener-
ated sentences. Kendall?s ? , which has been used for
evaluating sentence ordering tasks (Lapata, 2006),
is the second metric we use. ? is calculated as
1? 4 tN(N?1) , where t is the number of interchangesof consecutive elements to arrange N elements in
the right order. ? is sensitive to near misses and
assigns abdc (almost correct order) a score of 0.66
while dcba (inverse order) gets ?1. Note that it is
questionable whether this metric is as appropriate
for word ordering tasks as for sentence ordering ones
because a near miss might turn out to be ungrammat-
ical whereas a more different order stays acceptable.
Apart from acc and ? , we also adopt the metrics
used by Uchimoto et al (2000) and Ringger et al
(2004). The former use agreement rate (agr) cal-
culated as 2pN(N?1) : the number of correctly orderedpairs of constituents over the total number of all pos-
sible pairs, as well as complete agreement which is
basically per-sentence accuracy. Unlike ? , which
has ?1 as the lowest score, agr ranges from 0 to 1.
Ringger et al (2004) evaluate the performance only
in terms of per-constituent edit distance calculated
as mN , where m is the minimum number of moves11
10Since subordinate clauses do not have a VF, the first step is
not needed.
11A move is a deletion combined with an insertion.
needed to arrange N constituents in the right order.
This measure seems less appropriate than ? or agr
because it does not take the distance of the move into
account and scores abced and eabcd equally (0.2).
Since ? and agr, unlike edit distance, give higher
scores to better orders, we compute inverse distance:
inv = 1 ? edit distance instead. Thus, all three met-
rics (? , agr, inv) give the maximum of 1 if con-
stituents are ordered correctly. However, like ? , agr
and inv can give a positive score to an ungrammat-
ical order. Hence, none of the evaluation metrics
describes the performance perfectly. Human eval-
uation which reliably distinguishes between appro-
priate, acceptable, grammatical and ingrammatical
orders was out of choice because of its high cost.
6.2 Results
The results on the test data are presented in Table
3. The performance of TWO-STEP is significantly
better than any other method (?2, p < 0.01). The
performance of MAXENT does not significantly dif-
fer from UCHIMOTO. BIGRAM performed about as
good as UCHIMOTO and MAXENT. We also checked
how well TWO-STEP performs on each of the two
sub-tasks (Table 4) and found that the VF selection
is considerably more difficult than the sorting part.
acc ? agr inv
RAND 15% 0.02 0.51 0.64
RAND IMP 23% 0.24 0.62 0.71
BIGRAM 51% 0.60 0.80 0.83
UCHIMOTO 50% 0.65 0.82 0.83
MAXENT 52% 0.67 0.84 0.84
TWO-STEP 61% 0.72 0.86 0.87
Table 3: Per-clause mean of the results
The most important conclusion we draw from the
results is that the gain of 9% accuracy is due to the
VF selection only, because the feature sets are iden-
tical for MAXENT and TWO-STEP. From this fol-
lows that doing feature selection without splitting
the task in two is ineffective, because the importance
of a feature depends on whether the VF or the MF is
considered. For the MF, feature selection has shown
syn and pos to be the most relevant features. They
alone bring the performance in the MF up to 75%. In
contrast, these two features explain only 56% of the
325
cases in the VF. This implies that the order in the MF
mainly depends on grammatical features, while for
the VF all features are important because removal of
any feature caused a loss in accuracy.
acc ? agr inv
TWO-STEP VF 68% - - -
TWO-STEP MF 80% 0.92 0.96 0.95
Table 4: Mean of the results for the VF and the MF
Another important finding is that there is no need
to overgenerate to find the right order. Insignificant
for clauses with two or three constituents, for clauses
with 10 constituents, the number of comparisons is
reduced drastically from 163,296,000 to 45.
According to the inv metric, our results are con-
siderably worse than those reported by Ringger et al
(2004). As mentioned in Section 3, the fact that they
generate the order for every non-terminal node se-
riously inflates their numbers. Apart from that, they
do not report accuracy, and it is unknown, how many
sentences they actually reproduced correctly.
6.3 Error Analysis
To reveal the main error sources, we analyzed incor-
rect predictions concerning the VF and the MF, one
hundred for each. Most errors in the VF did not lead
to unacceptability or ungrammaticality. From lexi-
cal and semantic features, the classifier learned that
some expressions are often used in the beginning of
a sentence. These are temporal or locational PPs,
anaphoric adverbials, some connectives or phrases
starting with unlike X, together with X, as X, etc.
Such elements were placed in the VF instead of the
subject and caused an error although both variants
were equally acceptable. In other cases the classi-
fier could not find a better candidate but the subject
because it could not conclude from the provided fea-
tures that another constituent would nicely introduce
the sentence into the discourse. Mainly this con-
cerns recognizing information familiar to the reader
not by an already mentioned entity, but one which is
inferrable from what has been read.
In the MF, many orders had a PP transposed with
the direct object. In some cases the predicted order
seemed as good as the correct one. Often the algo-
rithm failed at identifying verb-specific preferences:
E.g., some verbs take PPs with the locational mean-
ing as an argument and normally have them right
next to them, whereas others do not. Another fre-
quent error was the wrong placement of superficially
identical constituents, e.g. two PPs of the same size.
To handle this error, the system needs more spe-
cific semantic information. Some errors were caused
by the parser, which created extra constituents (e.g.
false PP or adverb attachment) or confused the sub-
ject with the direct verb.
We retrained our system on a corpus of newspaper
articles (Telljohann et al, 2003, Tu?Ba-D/Z) which is
manually annotated but encodes no semantic knowl-
edge. The results for the MF were the same as on the
data from Wikipedia. The results for the VF were
much worse (45%) because of the lack of semantic
information.
7 Conclusion
We presented a novel approach to ordering con-
stituents in German. The results indicate that a
linguistically-motivated two-step system, which first
selects a constituent for the initial position and then
orders the remaining ones, works significantly better
than approaches which do not make this separation.
Our results also confirm the hypothesis ? which has
been attested in several corpus studies ? that the or-
der in the MF is rather rigid and dependent on gram-
matical properties.
We have also demonstrated that there is no need
to overgenerate to find the best order. On a prac-
tical side, this finding reduces the amount of work
considerably. Theoretically, it lets us conclude that
the relatively fixed order in the MF depends on the
salience which can be predicted mainly from gram-
matical features. It is much harder to predict which
element should be placed in the VF. We suppose that
this difficulty comes from the double function of the
initial position which can either introduce the ad-
dressation topic, or be the scene- or frame-setting
position (Jacobs, 2001).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would also like to thank
Elke Teich and the three anonymous reviewers for
their useful comments.
326
References
Barzilay, R. & K. R. McKeown (2005). Sentence fusion for
multidocument news summarization. Computational Lin-
guistics, 31(3):297?327.
Brants, T. (2000). TnT ? A statistical Part-of-Speech tagger. In
Proceedings of the 6th Conference on Applied Natural Lan-
guage Processing, Seattle, Wash., 29 April ? 4 May 2000,
pp. 224?231.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, sub-
jects, topics, and point of view. In C. Li (Ed.), Subject and
Topic, pp. 25?55. New York, N.Y.: Academic Press.
Clarkson, P. & R. Rosenfeld (1997). Statistical language mod-
eling using the CMU-Cambridge toolkit. In Proceedings
of the 5th European Conference on Speech Communication
and Technology, Rhodes, Greece, 22-25 September 1997, pp.
2707?2710.
Foth, K. & W. Menzel (2006). Hybrid parsing: Using proba-
bilistic models as predictors for a symbolic parser. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, Sydney, Australia, 17?
21 July 2006, pp. 321?327.
Frey, W. (2004). A medial topic position for German. Linguis-
tische Berichte, 198:153?190.
Gernsbacher, M. A. & D. J. Hargreaves (1988). Accessing sen-
tence participants: The advantage of first mention. Journal
of Memory and Language, 27:699?717.
Halliday, M. A. K. (1985). Introduction to Functional Gram-
mar. London, UK: Arnold.
Harbusch, K., G. Kempen, C. van Breugel & U. Koch (2006).
A generation-oriented workbench for performance grammar:
Capturing linear order variability in German and Dutch. In
Proceedings of the International Workshop on Natural Lan-
guage Generation, Sydney, Australia, 15-16 July 2006, pp.
9?11.
Jacobs, J. (2001). The dimensions of topic-comment. Linguis-
tics, 39(4):641?681.
Keller, F. (2000). Gradience in Grammar: Experimental
and Computational Aspects of Degrees of Grammaticality,
(Ph.D. thesis). University of Edinburgh.
Kempen, G. & K. Harbusch (2004). How flexible is con-
stituent order in the midfield of German subordinate clauses?
A corpus study revealing unexpected rigidity. In Proceed-
ings of the International Conference on Linguistic Evidence,
Tu?bingen, Germany, 29?31 January 2004, pp. 81?85.
Kimball, J. (1973). Seven principles of surface structure parsing
in natural language. Cognition, 2:15?47.
Kohavi, R. & M. Sahami (1996). Error-based and entropy-based
discretization of continuous features. In Proceedings of the
2nd International Conference on Data Mining and Knowl-
edge Discovery, Portland, Oreg., 2?4 August, 1996, pp. 114?
119.
Kruijff, G.-J., I. Kruijff-Korbayova?, J. Bateman & E. Teich
(2001). Linear order as higher-level decision: Information
structure in strategic and tactical generation. In Proceedings
of the 8th European Workshop on Natural Language Gener-
ation, Toulouse, France, 6-7 July 2001, pp. 74?83.
Kruijff-Korbayova?, I., G.-J. Kruijff & J. Bateman (2002). Gen-
eration of appropriate word order. In K. van Deemter &
R. Kibble (Eds.), Information Sharing: Reference and Pre-
supposition in Language Generation and Interpretation, pp.
193?222. Stanford, Cal.: CSLI.
Kurz, D. (2000). A statistical account on word order variation
in German. In A. Abeille?, T. Brants & H. Uszkoreit (Eds.),
Proceedings of the COLING Workshop on Linguistically In-
terpreted Corpora, Luxembourg, 6 August 2000.
Langkilde, I. & K. Knight (1998). Generation that exploits
corpus-based statistical knowledge. In Proceedings of the
17th International Conference on Computational Linguistics
and 36th Annual Meeting of the Association for Computa-
tional Linguistics, Montre?al, Que?bec, Canada, 10?14 August
1998, pp. 704?710.
Lapata, M. (2006). Automatic evaluation of information order-
ing: Kendall?s tau. Computational Linguistics, 32(4):471?
484.
Marsi, E. & E. Krahmer (2005). Explorations in sentence fu-
sion. In Proceedings of the European Workshop on Nat-
ural Language Generation, Aberdeen, Scotland, 8?10 Au-
gust, 2005, pp. 109?117.
Pappert, S., J. Schliesser, D. P. Janssen & T. Pechmann (2007).
Corpus- and psycholinguistic investigations of linguistic
constraints on German word order. In A. Steube (Ed.),
The discourse potential of underspecified structures: Event
structures and information structures. Berlin, New York:
Mouton de Gruyter. In press.
Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets &
S. Corston-Oliver (2004). Linguistically informed statistical
models of constituent structure for ordering in sentence real-
ization. In Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzerland, 23?27
August 2004, pp. 673?679.
Schmid, H. (1997). Probabilistic Part-of-Speech tagging using
decision trees. In D. Jones & H. Somers (Eds.), New Methods
in Language Processing, pp. 154?164. London, UK: UCL
Press.
Sgall, P., E. Hajic?ova? & J. Panevova? (1986). The Meaning of the
Sentence in Its Semantic and Pragmatic Aspects. Dordrecht,
The Netherlands: D. Reidel.
Speyer, A. (2005). Competing constraints on Vorfeldbesetzung
in German. In Proceedings of the Constraints in Discourse
Workshop, Dortmund, 3?5 July 2005, pp. 79?87.
Telljohann, H., E. W. Hinrichs & S. Ku?bler (2003). Stylebook
for the Tu?bingen treebank of written German (Tu?Ba-D/Z.
Technical Report: Seminar fu?r Sprachwissenschaft, Univer-
sita?t Tu?bingen, Tu?bingen, Germany.
Uchimoto, K., M. Murata, Q. Ma, S. Sekine & H. Isahara
(2000). Word order acquisition from corpora. In Proceedings
of the 18th International Conference on Computational Lin-
guistics, Saarbru?cken, Germany, 31 July ? 4 August 2000,
pp. 871?877.
Uszkoreit, H. (1987). Word Order and Constituent Structure in
German. CSLI Lecture Notes. Stanford: CSLI.
Varges, S. & C. Mellish (2001). Instance-based natural lan-
guage generation. In Proceedings of the 2nd Conference of
the North American Chapter of the Association for Compu-
tational Linguistics, Pittsburgh, Penn., 2?7 June, 2001, pp.
1?8.
Wan, S., R. Dale, M. Dras & C. Paris (2005). Searching for
grammaticality and consistency: Propagating dependencies
in the Viterbi algorithm. In Proceedings of the 10th Euro-
pean Workshop on Natural Language Generation, Aberdeen,
Scotland, 8?10 August, 2005, pp. 211?216.
Weber, A. & K. Mu?ller (2004). Word order variation in Ger-
man main clauses: A corpus analysis. In Proceedings of
the 5th International Workshop on Linguistically Interpreted
Corpora, 29 August, 2004, Geneva, Switzerland, pp. 71?77.
327
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 267?274,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Linguistically Motivated Features
for Paragraph Boundary Identification
Katja Filippova and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
In this paper we propose a machine-
learning approach to paragraph boundary
identification which utilizes linguistically
motivated features. We investigate the re-
lation between paragraph boundaries and
discourse cues, pronominalization and in-
formation structure. We test our algorithm
on German data and report improvements
over three baselines including a reimple-
mentation of Sporleder & Lapata?s (2006)
work on paragraph segmentation. An
analysis of the features? contribution sug-
gests an interpretation of what paragraph
boundaries indicate and what they depend
on.
1 Introduction
Our work is concerned with multi-document sum-
marization, namely with the merging of multiple
documents about the same topic taken from the
web. We view summarization as extraction of im-
portant sentences from the text. As a consequence
of the merging process the layout of the documents
is lost. In order to create the layout of the out-
put, the document structure (Power et al, 2003)
has to be regenerated. One aspect of this struc-
ture is of particular importance for our work: the
paragraph structure. In web documents paragraph
boundaries are used to anchor figures and illustra-
tions, so that the figures are always aligned with
the same paragraph even when the font size or the
window size is changed. Since we want to include
figures in the generated summaries, paragraph seg-
mentation is an important subtask in our applica-
tion.
Besides multi-document summarization of web
documents, paragraph boundary identification
(PBI) could be useful for a number of different ap-
plications, such as producing the layout for tran-
scripts provided by speech recognizers and opti-
cal character recognition systems, and determin-
ing the layout of documents generated for output
devices with different screen size.
Though related to the task of topic segmenta-
tion which stimulated a large number of studies
(Hearst, 1997; Choi, 2000; Galley et al, 2003,
inter alia), paragraph segmentation has not been
thoroughly investigated so far. We explain this by
the fact that paragraphs are considered a stylistic
phenomenon and that there is no unanimous opin-
ion on what the function of the paragraph is. Some
authors (Irmscher (1972) as cited by Stark (1988))
suggest that paragraph structure is arbitrary and
can not be determined based solely on the prop-
erties of the text. Still, psycholinguistic studies
report that humans agree, at least to some extent,
on placing boundaries between paragraphs. These
studies also note that paragraph boundaries are in-
formative and make the reader perceive paragraph-
initial sentences as being important (Stark, 1988).
In contrast to topic segmentation, paragraph seg-
mentation has the advantage that large amounts of
annotated data are readily availabe for supervised
learning.
In this paper we describe our approach to para-
graph segmentation. Previous work (Sporleder &
Lapata, 2004; 2006) mainly focused on superficial
and easily obtainable surface features like punctu-
ation, quotes, distance and words in the sentence.
Their approach was claimed to be domain- and
language-independent. Our hypothesis, however,
is that linguistically motivated features, which we
compute automatically, provide a better paragraph
segmentation than Sporleder & Lapata?s surface
ones, though our approach may loose some of the
267
domain-independence. We test our hypothesis on
a corpus of biographies downloaded from the Ger-
man Wikipedia1. The results we report in this pa-
per indicate that linguistically motivated features
outperform surface features significantly. It turned
out that pronominalization and information struc-
ture contribute to the determination of paragraph
boundaries while discourse cues have a negative
effect.
The paper is organized as follows: First, we de-
scribe related work in Section 2, then in Section
3 our data is introduced. The baselines, the ma-
chine learners, the features and the experimental
setup are given in Section 4. Section 5 reports and
discusses the results.
2 Related Work
Compared to other text segmentation tasks, e.g.
topic segmentation, PBI has received relatively lit-
tle attention. We are aware of three studies which
approach the problem from different perspectives.
Bolshakov & Gelbukh (2001) assume that split-
ting text into paragraphs is determined by text co-
hesion: The link between a paragraph initial sen-
tence and the preceding context is weaker than the
links between sentences within a paragraph. They
evaluate text cohesion using a database of collo-
cations and semantic links and insert paragraph
boundaries where the cohesion is low.
The algorithm of Sporleder & Lapata (2004,
2006) uses surface, syntactic and language model
features and is applied to three different languages
and three domains (fiction, news, parliament).
This study is of particular interest to us since one
of the languages the algorithm is tested on is Ger-
man. They investigate the impact of different fea-
tures and data size, and report results significantly
better than a simple baseline. However, their re-
sults vary considerably between the languages and
the domains. Also, the features determined impor-
tant is different for each setting. So, it may be
possible that Sporleder & Lapata do not provide
conclusive results.
Genzel (2005) considers lexical and syntactic
features and reports accuracy obtained from En-
glish fiction data as well as from the WSJ corpus.
He points out that lexical coherence and structural
features turn out to be the most useful for his algo-
rithm. Unfortunately, the only evaluation measure
he provides is accuracy which, for the PBI task,
1http://de.wikipedia.org
does not describe the performance of a system suf-
ficiently.
In comparison to the mentioned studies, our
goal is to examine the influence of cohesive fea-
tures on the choice of paragraph boundary inser-
tion. Unlike Bolshakov & Gelbukh (2001), who
have similar motivation but measure cohesion by
collocations, we explore the role of discourse cues,
pronominalization and information structure.
The task of topic segmentation is closely related
to the task of paragraph segmentation. If there
is a topic boundary, it is very likely that it coin-
cides with a paragraph boundary. However, the
reverse is not true and one topic can extend over
several paragraphs. So, if determined reliably,
topic boundaries could be used as high precision,
low recall predictors for paragraph boundaries.
Still, there is an important difference: While work
on topic segmentation mainly depends on content
words (Hearst, 1997) and relations between them
which are computed using lexical chains (Galley
et al, 2003), paragraph segmentation as a stylistic
phenomenon may depend equally likely on func-
tion words. Hence, paragraph segmentation is
a task which encompasses the traditional borders
between content and style.
3 Data
The data we used is a collection of biographies
from the German version of Wikipedia. We se-
lected all biographies under the Wikipedia cate-
gories of physicists, chemists, mathematicians and
biologists and obtained 970 texts with an average
length of 20 sentences and 413,776 tokens in total.
Although our corpus is substantially smaller
than the German corpora of Sporleder & Lapata
(2006), it should be big enough for a fair com-
parison between their algorithm and the algorithm
proposed here. Having investigated the effect of
the training size, Sporleder & Lapata (2006) came
to the conclusion that their system performs well
being trained on a small data set. In particular,
the learning curve for German shows an improve-
ment of only about 2% when the amount of train-
ing data is increased from 20%, which in case of
German fiction approximately equals 370,000 to-
kens, to 100%.
Fully automatic preprocessing in our system
comprises the following stages: First, a list of peo-
ple of a certain Wikipedia category is taken and
for every person an article is extracted The text
268
training development test
tokens 347,763 39,228 19,943
sentences 15,583 1,823 922
paragraphs 5,323 654 362
Table 1: Number of tokens and sentences per set
is purged from Wiki tags and comments, the in-
formation on subtitles and paragraph structure is
preserved. Second, sentence boundaries are iden-
tified with a Perl CPAN module2 whose perfor-
mance we improved by extending the list of abbre-
viations and modifying the output format. Next,
the sentences are split into tokens. The TnT tag-
ger (Brants, 2000) and the TreeTagger (Schmid,
1997) are used for tagging and lemmatizing. Fi-
nally, the texts are parsed with the CDG depen-
dency parser (Foth & Menzel, 2006). Thus, the
text is split on three levels: paragraphs, sentences
and tokens, and morphological and syntactic in-
formation is provided.
A publicly available list of about 300 discourse
connectives was downloaded from the Internet site
of the Institute for the German Language3 (Insti-
tut fu?r Deutsche Sprache, Mannheim) and slightly
extended. These are identified in the text and an-
notated automatically as well. Named entities are
classified according to their type using informa-
tion from Wikipedia: person, location, organiza-
tion or undefined. Given the peculiarity of our cor-
pus, we are able to identify all mentions of the bi-
ographee in the text by simple string matching. We
also annotate different types of referring expres-
sions (first, last, full name) and resolve anaphora
by linking personal pronouns to the biographee
provided that they match in number and gender.
The annotated corpus is split into training
(85%), development (10%) and testing (5%) sets.
Distribution of data among the three sets is pre-
sented in Table 1. Sentences which serve as sub-
titles in a text are filtered out because they make
identifying a paragraph boundary for the follow-
ing sentence trivial.
4 Experiments
4.1 Machine Learners
The PBI task was reformulated as a binary classifi-
cation problem: every training instance represent-
2http://search.cpan.org/?holsten/Lingua-DE-Sentence-
0.07/Sentence.pm
3http://hypermedia.ids-mannheim.de/index.html
ing a sentence was classified either as paragraph-
initial or not.
We used two machine learners: BoosTexter
(Schapire & Singer, 2000) and TiMBL (Daele-
mans et al, 2004). BoosTexter was developed
for text categorization, and combines simple rules
(decision stumps) in a boosting manner. Sporleder
& Lapata used this learner because it has the abil-
ity to combine many only moderately accurate
hypotheses. TiMBL is a memory-based learner
which classifies every test instance by finding the
most similar examples in the training set, hence it
does not abstract from the data and is well suited
to handle features with many values, e.g. the list
of discourse cues. For both classifiers, all experi-
ments were run with the default settings.
4.2 Baselines
We compared the performance of our algorithm
against three baselines. The first one (distance)
trivially inserts a paragraph break after each third
sentence, which is the average number of sen-
tences in a paragraph. The second baseline (Gal-
ley) hypothesizes that paragraph breaks coincide
with topic boundaries and utilizes Galley et al?s
(2003) topic boundary identification tool LCseg.
The third baseline (Sporleder) is a reimplementa-
tion of Sporleder & Lapata?s 2006 algorithm with
the following features:
Word and Sentence Distances from the current
sentence to the previous paragraph break;
Sentence Length and Relative Position (relPos)
of the sentence in a text;
Quotes encodes whether this and the previous
sentences contain a quotation, and whether
the quotation is continued in the current sen-
tence or not;
Final Punctuation of the previous sentence;
Words ? the first (word1), the first two (word2),
the first three and all words from the sen-
tence;
Parsed has positive value in case the sentence is
parsed, negative otherwise;
Number of S, VP, NP and PP nodes in the sen-
tence;
Signature is the sequence of PoS tags with and
without punctuation;
269
Children of Top-Level Nodes are two features
representing the sequence of syntactic labels
of the children of the root of the parse tree
and the children of the highest S-node;
Branching Factor features express the average
number of children of S, VP, NP and PP
nodes in the parse;
Tree Depth is the average length of the path from
the root to the leaves;
Per-word Entropy is a feature based on Gen-
zel & Charniak?s (2003) observation that
paragraph-initial sentences have lower en-
tropy than non-initial ones;
Sentence Probability according to a language
model computed from the training data;
Character-level n-gram models are built using
the CMU toolkit (Clarkson & Rosenfeld,
1997).
Since the parser we used produces dependency
trees as an output, we could not distinguish be-
tween such features as children of the root of the
tree and children of the top-level S-node. Apart
from this minor change, we reimplemented the al-
gorithm in every detail.
4.3 Our Features
For our algorithm we first selected the features of
Sporleder & Lapata?s (2006) system which per-
formed best on the development set. These are
relative position, the first and the first two words
(relPos, word1, word2). Quote and final punctu-
ation features, which were particularly helpful in
Sporleder & Lapata?s experiments on the German
fiction data, turned out to be superfluous given the
infrequency of quotations and the prevalent use of
the period as sentence delimiter in our data.
We experimented with text cohesion features as-
suming that the paragraph structure crucially de-
pends on cohesion and that paragraph breaks are
likely to occur between sentences where cohesive
links are weak. In order to estimate the degree of
cohesion, we looked at lexical cohesion, pronom-
inalization, discourse cues and information struc-
ture.
4.3.1 Lexical Cohesion
nounOver, verbOver: Similar to Sporleder &
Lapata (2006), we introduced an overlap fea-
ture, but measured the degree of overlap as
a number of common noun and verb lem-
mas between two adjacent sentences. We pre-
ferred lemmas over words in order to match
all possible forms of the same word in Ger-
man.
LCseg: Apart from the overlap, a boolean feature
based on LCseg (Galley et al, 2003) marked
whether the tool suggests that a new topic be-
gins with the current sentence. This feature,
relying on lexical chains, was supposed to
provide more fine-grained information on the
degree of similarity between two sentences.
4.3.2 Pronominalization
As Stark (1988) points out, humans tend to in-
terpret over-reference as a clue for the beginning
of a new paragraph: In a sentence, if a non-
pronominal reference is preferred over a pronom-
inal one where the pronoun would be admissi-
ble, humans are likely to mark this sentence as a
paragraph-initial one. In order to check whether
over-reference indeed correlates with paragraph-
initial sentences, we described the way the bi-
ographee is referred to in the current and the pre-
vious sentences.
prevSPerson, currSPerson: This feature4 with
the values NA, biographee, other indicates
whether there is a reference to the biographee
or some other person in the sentence.
prevSRE, currSRE: This feature describes the
biographee?s referring expression and has
three possible values: NA, name, pronoun.
Although our annotation distinguishes between
first, last and full names, we found out that, for
the PBI task, the distinction is spurious and unify-
ing these three under the same category improves
the results.
REchange: Since our classifiers assume feature
independence and can not infer the informa-
tion on the change in referring expression, we
explicitly encoded that information by merg-
ing the values of the previous feature for the
current and the preceding sentences into one,
which has nine possible values (name-name,
NA-name, pronoun-name, etc.).
4Prefixes prevS-, currS- stand for the previous and the
current sentences respectively.
270
4.3.3 Discourse Cues
The intuition behind these features is that cue
words and phrases are used to signal the relation
between the current sentence and the preceding
sentence or context (Mann & Thompson, 1988).
Such connectives as endlich (finally), abgesehen
davon (apart from that), danach (afterwards) ex-
plicitly mark a certain relation between the sen-
tence they occur in and the preceding context. We
hypothesize that the relations which hold across
paragraph boundaries should differ from those
which hold within paragraphs and that the same is
true for the discourse cues. Absence of a connec-
tive is supposed to be informative as well, being
more typical for paragraph-initial sentences.
Three features describe the connective of the
current sentence. Another three features describe
the one from the preceding sentence.
prevSCue, currSCue: This feature is the con-
nective itself (NA in case of none).
prevSCueClass, currSCueClass: This feature
represents the semantic class of the cue word
or phrase as assigned by the IDS Mannheim.
There are 25 values, including NA in case
of no connective, altogether, with the most
frequent values being temporal, concessive,
conclusive, etc.
prevSProCue, currSProCue: The third binary
feature marks whether the connective is
proadverbial or not (NA if there is no connec-
tive). Being anaphors, proadverbials, such as
deswegen (because of that), daru?ber (about
that) explicitly link a sentence to the preced-
ing one(s).
4.3.4 Information Structure
Information structure, which is in German to a
large extent expressed by word order, provides
additional clues to the degree of connectedness
between two sentences. In respect to the PBI
task, Stark (1988) reports that paragraph-initial
sentences are often theme-marking which means
that the subject of such sentences is not the first
element. Given the lower frequency of paragraph-
initial sentences, this feature can not be considered
reliable, but in combination with others it provides
an additional clue. In German, the first element
best corresponds to the prefield (Vorfeld) ? nor-
mally, the single constituent placed before the fi-
nite verb in the main clause.
currSVF encodes whether the constituent in
the prefield is a NP, PP, ADV, CARD, or
Sub.Clause. Values different from NP un-
ambiguously represent theme-marking sen-
tences, whereas the NP value may stand for
both: theme-marking as well as not theme-
marking sentence.
4.4 Discussion
Note, that we did not exclude text-initial sentences
from the study because the encoding we used does
not make such cases trivial for classification. Al-
though some of the features refer to the previous
sentence, none of them has to be necessarily re-
alized and therefore none of them explicitly indi-
cates the absence of the preceding sentence. For
example, the label NA appears in cases where there
is no discourse cue in the preceding sentence as
well as in cases where there is no preceding sen-
tence. The same holds for all other features pre-
fixed with prevS-.
Another point concerns the use of
pronominalization-based features. Sporleder
& Lapata (2006) waive using such features be-
cause they consider pronominalization dependent
on the paragraph structure and not the other
way round. At the same time they mention
speech and optical character recognition tasks
as possible application domains for the PBI.
There, pronouns are already given and need
not be regenerated, hence for such applications
features which utilize pronouns are absolutely
appropriate. Unlike the recognition tasks, for
multi-document summarization both decisions
have to be made, and the order of the two tasks
is not self-evident. The best decision would
probably be to decide simultaneously on both
using optimization methods (Roth & Yih, 2004;
Marciniak & Strube, 2005). Generating pronouns
before inserting boundaries seems as reasonable
as doing it the other way round.
4.5 Feature Selection
We determine the relevant feature set and evaluate
which features from this set contribute most to the
performance of the system by the following pro-
cedures.
First, we follow an iterative algorithm similar
to the wrapper approach for feature selection (Ko-
havi & John, 1997) using the development data
and TiMBL. The feature subset selection algo-
rithm performs a hill-climbing search along the
271
Feature set F-measure
all 58.85%
?prevSCue 0.78%
?currSCue 0.32%
?currSCueClass 0.38%
?prevSCueClass 0.37%
?prevSProCue 1.02%
best 61.72%
Table 2: Removed features
Feature set F-measure
relPos, word1, word2 48.06%
+currSRE +10.50%
+currSVF +0.49%
+currSPerson +0.57%
+prevSPerson +1.32%
best 60.94%
Table 3: Best features
feature space. We start with a model based on all
available features. Then we train models obtained
by removing one feature at a time. We choose the
worst performing feature, namely the one whose
removal gives the largest improvement based on
the F-measure, and remove it from the model. We
then train classifiers removing each of the remain-
ing features separately from the enhanced model.
The process is iteratively run as long as significant
improvement is observed.
To measure the contribution of the relevant fea-
tures we start with the three best features from
Sporleder & Lapata (2006) (see Section 4.3) and
train TiMBL combining the current feature set
with each feature in turn. We then choose the best
performing feature based on the F-measure and
add it to the model. We iterate the process until
all features are added to the three-feature system.
Thus, we optimize the default setting and obtain
the information on what the paragraph structure
crucially depends.
5 Results
Having trained our algorithm on the development
data, we then determined the optimal feature com-
bination and finally evaluated the performance on
the previously unseen test data.
Table 2 and Table 3 present the ranking of the
least and of the most beneficial features respec-
tively. Somewhat surprising to us, Table 2 shows
that basically all features capturing information on
discourse cues actually worsened the performance
of the classifier. The bad performance of the
prevSCue and currSCue features may be caused
by their extreme sparseness. To test these fea-
tures reasonably, we plan to increase the data set
size by an order of magnitude. Then, at least, it
should be possible to determine which discourse
cues, if any, are correlated with paragraph bound-
aries. The bad performance of the prevSCueClass
and currSCueClass features may be caused by the
categorization provided by the IDS. This question
also requires further investigation, maybe with a
different categorization.
Table 3 also provides interesting insights in the
feature set. First, with only the three features
relPos, word1 and word2 the baseline performs
almost as well as the full feature set used by
Sporleder & Lapata. Then, as expected, currSRE
provides the largest gain in performance, fol-
lowed by currSVF, currSPerson and prevSPerson.
This result confirms our hypothesis that linguisti-
cally motivated features capturing information on
pronominalization and information structure play
an important role in determining paragraph seg-
mentation.
The results of our system and the baselines
for different classifiers (BT stands for BoosTex-
ter and Ti for TiMBL) are summarized in Table
4. Accuracy is calculated by dividing the num-
ber of matches over the total number of test in-
stances. Precision, recall and F-measure are ob-
tained by considering true positives, false positives
and false negatives. The latter metric, WindowDiff
(Pevzner & Hearst, 2002), is supposed to over-
come the disadvantage of the F-measure which pe-
nalizes near misses as harsh as more serious mis-
takes. The value of WindowDiff varies between 0
and 1, where a lesser count corresponds to better
performance.
The significance of our results was computed
using the    test. All results are significantly
better (on the      level or below) than
both baselines and the reimplemented version of
Sporleder & Lapata?s (2006) algorithm whose per-
formance on our data is comparable to what the
authors reported on their corpus of German fic-
tion. Interestingly, TiMBL does much better than
BoosTexter on Sporleder & Lapata?s feature set.
Apparently, Sporleder & Lapata?s presupposition,
that they would rely on many weak hypotheses,
272
Accuracy Precision Recall F-measure WindowDiff
distance 52.16 37.98 31.88 34.66 .426
Galley 56.83 43.04 26.15 32.54 .416
development
Sporleder BT 71.96 80.15 30.46 44.15 .327
Sporleder Ti 62.36 48.65 62.89 54.86 .338
all BT 74.93 72.10 50.67 59.52 .286
all Ti 70.54 59.81 57.91 58.85 .302
best Ti 73.39 64.73 58.97 61.72 .280
test
Sporleder BT 68.76 80.15 28.61 42.16 .341
Sporleder Ti 60.62 50.46 59.67 54.68 .345
all BT 72.12 71.31 50.13 58.88 .286
all Ti 67.13 59.14 56.40 57.74 .303
best Ti 68.00 60.46 56.67 58.50 .302
Table 4: Results for the development and test sets with the two classifiers
does not hold. This is also confirmed by the results
reported in Table 3 where only three of their fea-
tures perform surprisingly strong. In contrast, on
our feature set TiMBL and BoosTexter perform al-
most equally. However, BoosTexter achieves in all
cases a much higher precision which is preferable
over the higher recall provided by TiMBL.
6 Conclusion
In this paper, we proposed a novel approach to
paragraph boundary identification based on lin-
guistic features such as pronominalization, dis-
course cues and information structure. The results
are significantly higher than all baselines and a
reimplementation of Sporleder & Lapata?s (2006)
system and achieve an F-measure of about 59%.
We investigated to what extent the paragraph
structure is determined by each of the three fac-
tors and came to the conclusion that it crucially
depends on the use of pronouns and information
structure. Surprisingly, discourse cues did not turn
out to be useful for this task and even negatively
affected the results which we explain by the ex-
tremely sparseness of the cues in our data.
It turned out that the best results could be
achieved by a combination of surface features (rel-
Pos, word1, word2) and features capturing text
cohesion. This indicates that paragraph bound-
ary identification requires features usually used for
style analysis and ones describing cohesive rela-
tions. Therefore, paragraph boundary identifica-
tion is in fact a task which crosses the borders be-
tween content and style.
An obvious limitation of our study is that we
trained and tested the algorithm on one-genre do-
main where pronouns are used extensively. Ex-
perimenting with different genres should shed
light on whether our features are in fact domain-
dependent. In the future, we also want to ex-
periment with a larger data set for determining
whether discourse cues really do not correlate with
paragraph boundaries. Then, we will move on
towards multi-document summarization, the ap-
plication which motivates the research described
here.
Acknowledments: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
KTF grant (09.009.2004). We would also like
to thank the three anonymous reviewers for their
comments.
References
Bolshakov, Igor A. & Alexander Gelbukh (2001).
Text segmentation into paragraph based on local
text cohesion. In Text, Speech and Dialogue, pp.
158?166.
Brants, Thorsten (2000). TnT ? A statistical Part-
of-Speech tagger. In Proceedings of the 6th
Conference on Applied Natural Language Pro-
cessing, Seattle, Wash., 29 April ? 4 May 2000,
pp. 224?231.
Choi, Freddy Y. Y. (2000). Advances in domain
independent linear text segmentation. In Pro-
273
ceedings of the 1st Conference of the North
American Chapter of the Association for Com-
putational Linguistics, Seattle, Wash., 29 April
? 3 May, 2000, pp. 26?33.
Clarkson, Philip & Roni Rosenfeld (1997). Sta-
tistical language modeling. In Proceedings
of ESCA, EuroSpeech?97. Rhodes, pp. 2707?
2710.
Daelemans, Walter, Jakub Zavrel, Ko van der
Sloot & Antal van den Bosch (2004). TiMBL:
Tilburg Memory Based Learner, version 5.1,
Reference Guide. Technical Report ILK 04-02:
ILK Tilburg.
Foth, Kilian & Wolfgang Menzel (2006). Robust
parsing: More with less. In Proceedings of the
11th Conference of the European Chapter of
the Association for Computational Linguistics,
Trento, Italy, 3?7 April 2006, pp. 25?32.
Galley, Michel, Kathleen R. McKeown, Eric
Fosler-Lussier & Hongyan Jing (2003). Dis-
course segmentation of multi-party conversa-
tion. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguis-
tics, Sapporo, Japan, 7?12 July 2003, pp. 562?
569.
Genzel, Dmitriy (2005). A paragraph bound-
ary detection system. In Proceedings of the
Sixth International Conference on Intelligent
Text Processing and Computational Linguistics,
Mexico City, Mexico.
Genzel, Dmitriy & Eugene Charniak (2003). Vari-
ation of entropy and parse trees of sentences as
a function of the sentence number. In Proceed-
ings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, Sapporo,
Japan, 11?12 July 2003, pp. 65?72.
Hearst, Marti A. (1997). TextTiling: Segment-
ing text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1):33?64.
Irmscher, William F. (1972). The Holt Guide to
English. New-York: Holt, Rinehart Winston.
Kohavi, Ron & George H. John (1997). Wrap-
pers for feature subset selection. Artificial In-
telligence Journal, 97(1-2):273?324.
Mann, William C. & Sandra A. Thompson (1988).
Rhetorical structure theory. Toward a functional
theory of text organization. Text, 8(3):243?281.
Marciniak, Tomacz & Michael Strube (2005). Be-
yond the pipeline: Discrete optimization in
NLP. In Proceedings of the 9th Conference
on Computational Natural Language Learning,
Ann Arbor, Mich., USA, 29?30 June 2005, pp.
136?145.
Pevzner, Lev & Marti Hearst (2002). A critique
and improvement of an evaluation metric for
text segmentation. Computational Linguistics,
28(1):19?36.
Power, Richard, Donia Scott & Nadjet Bouayad-
Agha (2003). Document structure. Computa-
tional Linguistics, 29(2):211?260.
Roth, Dan & Wen-tau Yih (2004). A linear pro-
gramming formulation for global inference in
natural language tasks. In Proceedings of the
8th Conference on Computational Natural Lan-
guage Learning, Boston, Mass., USA, 6?7 May
2004, pp. 1?8.
Schapire, Robert E. & Yoram Singer (2000).
BoosTexter: A boosting-based system for
text categorization. Machine Learning,
39(2/3):135?168.
Schmid, Helmut (1997). Probabilistic part-of-
speech tagging using decision trees. In Daniel
Jones & Harold Somers (Eds.), New Methods
in Language Processing, pp. 154?164. London,
UK: UCL Press.
Sporleder, Caroline & Mirella Lapata (2004). Au-
tomatic paragraph identification: A study across
languages and domains. In Proceedings of the
2004 Conference on Empirical Methods in Nat-
ural Language Processing, Barcelona, Spain,
25?26 July 2004, pp. 72?79.
Sporleder, Caroline & Mirella Lapata (2006).
Broad coverage paragraph segmentation across
languages and domains. ACM Transactions in
Speech and Language Processing. To appear.
Stark, Heather (1988). What do paragraph mark-
ings do? Discourse Processes, (11):275?303.
274
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 322?330,
Beijing, August 2010
Multi-Sentence Compression: Finding Shortest Paths in Word Graphs
Katja Filippova
Google Inc.
katjaf@google.com
Abstract
We consider the task of summarizing a
cluster of related sentences with a short
sentence which we call multi-sentence
compression and present a simple ap-
proach based on shortest paths in word
graphs. The advantage and the novelty of
the proposed method is that it is syntax-
lean and requires little more than a tok-
enizer and a tagger. Despite its simplic-
ity, it is capable of generating grammati-
cal and informative summaries as our ex-
periments with English and Spanish data
demonstrate.
1 Introduction
Sentence compression (henceforth SC) is a task
where the goal is to produce a summary of a sin-
gle sentence which would preserve the important
part of the content and be grammatical. Starting
from the early work of Jing & McKeown (2000),
in the last decade SC has received considerable at-
tention in the NLP community. Ubiquitous use of
mobile devices is an obvious example of where
SC could be applied?a longer text of an email,
news or a Wikipedia article can be compressed
sentence by sentence to fit into a limited display
(Corston-Oliver, 2001). Another reason why SC is
so popular is its potential utility for extractive text
summarization, single or multi-document (Mani,
2001). There, a standard approach is to rank sen-
tences by importance, cluster them by similarity,
and select a sentence from the top ranked clusters.
Selected sentences almost always require revision
and can be reformulated succinctly as it is often
only a part of the sentence which is of interest.
It is this multi-document summarization scenario
which motivates our work.
Given a cluster of similar, or related, sentences,
we aim at summarizing the most salient theme of
it in a short single sentence. We refer to this task
as multi-sentence compression. Defined this way,
it comes close to sentence fusion which was orig-
inally introduced as a text-to-text generation tech-
nique of expressing content common to most of
the input sentences in a single sentence (Barzi-
lay & McKeown, 2005). However, since then the
technique has been extended so that now fusion
also stands for uniting complementary content in
a single concise sentence (Filippova & Strube,
2008b; Krahmer et al, 2008). Since our method
is not designed for the ?union? kind of fusion, we
think it is more appropriate to classify it as a sen-
tence compression technique.
Two challenges of SC as well as text summa-
rization are (i) important content selection and (ii)
its readable presentation. Most existing systems
use syntactic information to generate grammatical
compressions. Incidentally, syntax also provides
clues to what is likely to be important?e.g., the
subject and the verb of the main clause are more
likely to be important than a prepositional phrase
or a verb from a relative clause. Of course, syn-
tax is not the only way to gauge word or phrase
importance. In the case of sentence compression
being used for text summarization, one disposes
of a rich context to identify important words or
phrases. For example, recurring or semantically
322
similar words are likely to be relevant, and this
information has been used in earlier SC systems
(Hori et al, 2003; Clarke & Lapata, 2007, inter
alia). Still, syntactic parsers are assumed to be in-
dispensable tools for both sentence compression
and fusion because syntactic constraints (hand-
crafted or learned from the data) seem to be the
only way to control the grammaticality of the out-
put. In this paper we are going to question this
well-established belief and argue that just like in
some cases syntax helps to find important content
(e.g., when the input is an isolated sentence), in
the multi-sentence case redundancy provides a re-
liable way of generating grammatical sentences.
In particular, the important and novel points of our
work are as follows:
? We present a simple and robust word graph-
based method of generating succinct com-
pressions which requires as little as a part of
speech tagger and a list of stopwords.
? To our knowledge, it is the first method
which requires neither a parser, nor hand-
crafted rules, nor a language model to gen-
erate reasonably grammatical output.
? In an extensive evaluation with native speak-
ers we obtain encouraging results for English
as well as for Spanish.
In the following section we present our approach
to sentence compression (Sec. 2); then we intro-
duce the baseline (Sec. 3) and the data (Sec. 4).
In Section 5 we report about our experiments and
discuss the results. Finally, Section 6 gives an
overview of related work.
2 Multi-sentence Compression
A well-known challenge for extractive multi-
document summarization systems is to produce
non-redundant summaries. There are two stan-
dard ways of avoiding redundancy: either one
adds sentences to the summary one-by-one and
each time checks whether the sentence is signif-
icantly different from what is already there (e.g.,
using MMR), or one clusters related sentences and
selects only one from each cluster. In both cases
a selected sentence may include irrelevant infor-
mation, so one wishes to compress it, usually by
taking syntactic and lexical factors into account.
However, we think this approach is suboptimal in
this case and explore a different way. Instead of
compressing a single sentence, we build a word
graph from all the words of the related sentences
and compress this graph.
A word graph is a directed graph where an edge
from word A to word B represents an adjacency
relation. It also contains the start and end nodes.
Word graphs have been widely used in natural lan-
guage processing for building language models,
paraphrasing, alignment, etc. (see Sec. 6). Com-
pared with dependency graphs, their use for sen-
tence generation has been left largely unexplored,
presumably because it seems that almost all the
grammatical information is missing from this rep-
resentation. Indeed, a link between a finite verb
and an article does not correspond to any gram-
matical relation between the two. However, the
premise for our work is that redundancy should be
sufficient to identify not only important words but
also salient links between words. In this section
we present our approach to word graph compres-
sion. We begin by explaining the graph construc-
tion process and continue with the details of two
compression methods.
2.1 Word Graph Construction
Given a set of related sentences S =
{s1, s2, ...sn}, we build a word graph by it-
eratively adding sentences to it. As an illustration,
consider the four sentences below and the graph
in Figure 1 obtained from them. Edge weights
are omitted and italicized fragments from the
sentences are replaced with dots for clarity.
(1) The wife of a former U.S. president Bill Clin-
ton Hillary Clinton visited China last Mon-
day.
(2) Hillary Clinton wanted to visit China last
month but postponed her plans till Monday
last week.
(3) Hillary Clinton paid a visit to the People Re-
public of China on Monday.
(4) Last week the Secretary of State Ms. Clinton
visited Chinese officials.
323
wanted to
month
on
last
officials
visit
of
Clinton
Chinese
(1)
E
S
week
last
(4)
(2)
(3)
tillthe
Ms
paid
Hillary
Clinton
visited China
Monday
Figure 1: Word graph generated from sentences (1-4) and a possible compression path.
After the first sentence is added the graph is sim-
ply a string of word nodes (punctuation is ex-
cluded) plus the start and the end symbols (S and
E in Fig. 1). A word from the following sentences
is mapped onto a node in the graph provided that
they have the exact same lowercased word form
and the same part of speech1 and that no word
from this sentence has already been mapped onto
this node. Using part of speech information re-
duces chances of merging verbs with nouns (e.g.,
visit) and generating ungrammatical sequences. If
there is no candidate in the graph a new node is
created.
Word mapping/creation is done in three steps
for the following three groups of words: (1) non-
stopwords2 for which no candidate exists in the
graph or for which an unambiguous mapping is
possible; (2) non-stopwords for which there are
either several possible candidates in the graph or
which occur more than once in the sentence; (3)
stopwords.
This procedure is similar to the one used by
Barzilay & Lee (2003) in that we also first iden-
tify ?backbone nodes? (unambiguous alignments)
and then add mappings for which several possi-
bilities exist. However, they build lattices, i.e.,
1We use the OpenNLP package for tagging: http://
opennlp.sourceforge.net.
2We generate a list of about 600 news-specific stopwords
for English (including, e.g., said, seems) and took a publicly
available list of about 180 stopwords for Spanish from www.
ranks.nl/stopwords/spanish.html.
directed acyclic graphs, whereas our graphs may
contain cycles. For the last two groups of words
where mapping is ambiguous we check the imme-
diate context (the preceding and following words
in the sentence and the neighboring nodes in the
graph) and select the candidate which has larger
overlap in the context, or the one with a greater
frequency (i.e., the one which has more words
mapped onto it). For example, in Figure 1 when
sentence (4) is to be added, there are two candi-
date nodes for last. The one pointing to week is
selected as week is the word following last in (4).
Stopwords are mapped only if there is some over-
lap in non-stopword neighbors, otherwise a new
node is created.
Once all the words from the sentence are in
place, we connect words adjacent in the sentence
with directed edges. For newly created nodes,
or nodes which were not connected before, we
add an edge with a default weight of one. Edge
weights between already connected nodes are in-
creased by one. The same is done with the start
and end nodes. Nodes store id?s of the sentences
their words come from as well as all their offset
positions in those sentences.
The described alignment method is fairly sim-
ple and guarantees the following properties of the
word graph: (i) every input sentence corresponds
to a loopless path in the graph; (ii) words refer-
ring to the same entities or actions are likely to
end up in one node; (iii) stopwords are only joined
324
in one node if there is an overlap in context. The
graph may generate a potentially endless amount
of incomprehensible sequences connecting start
and end. It is also likely to contain paths corre-
sponding to good compressions, like the path con-
necting the nodes highlighted with blue in Figure
1. In the following we describe two our methods
of finding the best path, that is, the best compres-
sion for the input sentences.
2.2 Shortest Path as Compression
What properties are characteristic of a good com-
pression? It should neither be too long, nor too
short. It should go through the nodes which rep-
resent important concepts but should not pass the
same node several times. It should correspond to a
likely word sequence. To satisfy these constraints
we invert edge weights, i.e., link frequencies, and
search for the shortest path (i.e., lightest in terms
of the edge weights) from start to end of a pre-
defined minimum length. This path is likely to
mention salient words from the input and put to-
gether words found next to each other in many
sentences. This is the first method we consider.
We set a minimum path length (in words) to eight
which appeared to be a reasonable threshold on a
development set?paths shorter than seven words
were often incomplete sentences.
Furthermore, to produce informative sum-
maries which report about the main event of the
sentence cluster, we filter paths which do not con-
tain a verb node. For example, Ozark?s ?Win-
ter?s Bone? at the 2010 Sundance Film Festival
might be a good title indicating what the article is
about. However, it is not as informative as ?Win-
ter?s Bone? earned the grand jury prize at Sun-
dance which indeed conveys the gist of the event.
Thus, we generate K shortest paths and filter all
those which are shorter than eight words or do not
contain a verb. The path with the minimum total
weight is selected as the summary.
2.3 Improved Scoring and Reranking
The second configuration of our system employs
a more sophisticated weighting function. The pur-
pose of this function is two-fold: (i) to generate a
grammatical compression, it favors strong links,
i.e., links between words which appear signifi-
cantly often in this order; (ii) to generate an in-
formative compression, it promotes paths passing
through salient nodes.
Strong links: Intuitively, we want the compres-
sion path to follow edges between words which
are strongly associated with each other. Inverted
edge frequency is not sufficient for that because
it ignores the overall frequency of the nodes the
edge connects. For example, edge frequency of
three should count more if the edge connects two
nodes with frequency of three rather than if their
frequencies are much higher. Thus, we redefine
edge weight as follows:
w(ei,j) =
freq(i) + freq(j)
freq(ei,j)
(1)
Furthermore, we also promote a connection be-
tween two nodes if there are multiple paths be-
tween them. For example, if some sentences
speak of president Barack Obama or president of
the US Barack Obama, and some sentences are
about president Obama, we want to add some re-
ward to the edge between president and Obama.
However, longer paths between words are weak
signals of word association. Therefore, the weight
of an edge between the nodes i and j is reduced
for every possible path between them but reduced
proportionally to its length:
w?(ei,j) =
freq(i) + freq(j)
P
s?S diff(s, i, j)?1
(2)
where the function diff(s, i, j) refers to the dis-
tance between the offset positions (pos(s, i)) of
words i and j in sentence s and is defined as fol-
lows:
diff(s, i, j) =
(
pos(s, i) ? pos(s, j) if pos(s, i) < pos(s, j)
0 otherwise
(3)
Salient words: The function above only indi-
cates how strong the association between two
words is. It assigns equal weights to edges con-
necting words encountered in a single sentence
and words encountered next to each other in every
sentence. To generate a summary concerning the
most salient events and entities, we force the path
325
to go through most frequent nodes by decreasing
edge weight with respect to the frequency of the
nodes it connects. Thus, we further redefine edge
weight as follows:
w??(ei,j) = w
?(ei,j)
freq(i) ? freq(j) (4)
We implement the K-shortest paths algorithm
to find the fifty shortest paths from start to end
using the weighting function in (4). We filter all
the paths which are shorter than eight words and
which do not pass a verb node. Finally, we rerank
the remaining paths by normalizing the total path
weight over its length. This way we obtain the
path which has the lightest average edge weight.
3 Baseline
As a first baseline we are searching for the most
probable string with respect to the sentence clus-
ter. In particular, we use the Viterbi algorithm to
find the sequence of words of a predefined length
n which maximizes the bigram probability (MLE-
based):
p(w1,n) = p(w1|s)p(w2|w1)...p(e|wn) (5)
Similar to the shortest path implementation, we
specify compression length and set it also here to
eight tokens. However, the compressions obtained
with this method are often unrelated to the main
theme. The reason for that is that a token subse-
quence encountered in a single sentence is likely
to get a high probability?all transition probabili-
ties are equal to one?provided that the probability
of entering this sequence is not too low. To amend
this problem and to promote frequent words (i.e.,
words which are likely to be related to the main
theme) we maximize the following baseline score
which takes into account both the bigram proba-
bilities and the token likelihood, p(wi), which is
also estimated from the sentence cluster:
b(w1,n) = p(w1|s)p(w2|w1)...p(e|wn)
Y
i
p(wi) (6)
4 Data Sources
As data for our experiments we use news arti-
cles presented in clusters on Google News3. The
main reason for why we decided to use this ser-
vice is that it is freely available and does the job
of news classification and clustering with a pro-
duction quality. Apart from that, it is a rich source
of multilingual data.
We collected news clusters in English and
Spanish, 10-30 articles each, 24 articles on aver-
age. To get sets of similar sentences we aggre-
gated first sentences from every article in the clus-
ter, removing duplicates. The article-initial sen-
tence is known to provide a good summary of
the article and has become a standard competi-
tive baseline in summarization4 . Hence, given that
first sentences summarize the articles they belong
to, which are in turn clustered as concerning the
same event, those sentences are likely although
not necessarily need to be similar.
From the total of 150 English clusters we re-
served 70 for development and 80 for testing. For
Spanish we collected 40 clusters, all for testing.
We stripped off bylines and dates from the begin-
ning of every sentence with a handful of regular
expressions before feeding them to the baseline
and our compression methods.
The data we use has two interesting properties:
(i) article-initial sentences are on average longer
than other sentences. In our case average sentence
lengths for English and Spanish (without bylines)
are 28 and 35 tokens, respectively. (ii) such sen-
tence clusters are noisier than what one would ex-
pect in a summarization pipeline. Both properties
make the task realistically hard and pose a chal-
lenge for the robustness of a compression method.
If we show that reasonable compressions can be
generated even from noisy clusters acquired from
a publicly available news service, then we have a
good reason to believe that the method will per-
form at least comparable on more carefully con-
structed clusters of shorter sentences.
3http://news.google.com
4See DUC/TAC competitions: http://www.nist.
gov/tac
326
5 Evaluation
5.1 Experiment Design
The performance of the systems was assessed in
an experiment with human raters, all native speak-
ers. They were presented with a list of snippets of
the articles from one cluster ? first sentence and
title linked to the original document. The raters
were allowed to look up the articles if they need
more background on the matter but this was not
obligatory.
The first question concerned the quality of the
sentence cluster. The raters were asked whether
the cluster contained a single prevailing event, or
whether it was too noisy and no theme stood out.
Given how simple our sentence grouping proce-
dure was, most clusters informed about more than
one event. However, to answer the question posi-
tively it would be enough to identify one prevail-
ing theme.
Below that, a summary and two further ques-
tions concerning its quality were displayed. Simi-
lar to most preceding work, we were interested in
grammaticality and informativity of summaries.
With respect to grammaticality, following Barzi-
lay & McKeown (2005), we asked the raters to
give one of the three possible ratings: perfect if
the summary was a complete grammatical sen-
tence (2 pts); almost if it required a minor edit-
ing, e.g., one mistake in articles or agreement (1
pt); ungrammatical if it was none of above (0 pts).
We explicitly asked the raters to ignore lack or
excess of capitalization or punctuation. Further-
more, based on the feedback from a preliminary
evaluation, we provided an example in which we
made clear that summaries consisting of a few
phrases which cannot be reformulated as a com-
plete sentence (e.g., Early Monday a U.S. Navy
ship.) should not count as grammatical.
The final question, concerning informativity,
had four possible options: n/a if the cluster is too
noisy and unsummarizable in the first place; per-
fect if it conveys the gist of the main event and is
more or less like the summary the person would
produce himself (2 pts); related if it is related to
the the main theme but misses something impor-
tant (1 pt); unrelated if the summary is not related
to the main theme (0 pts).
For each of the 80 sentence clusters (40 for
Spanish) we generated three summaries with the
three systems. Most summaries were rated by four
raters, a few got only three ratings; no rater saw
the same cluster twice.
5.2 Results
We report average grammaticality and informativ-
ity scores in Table 1. However, averaging system
ratings over all clusters and raters is not justified
in our case. It is important to remember that the
score assignments (i.e., 0, 1, 2) are arbitrary and
that the score of one with respect to grammatical-
ity (i.e., a minor mistake) is in fact closer to two
than to zero. One could set the scores differently
but even then, strictly speaking, it is not correct to
average the scores as ratings do not define a metric
space.
System Gram Info
Baseline 0.70 / 0.61 0.62 / 0.53
Shortest path 1.30 / 1.27 1.16 / 0.79
Shortest path++ 1.44 / 1.25 1.30 / 1.25
Table 1: Average ratings for English / Spanish.
Therefore in Table 2 we present distributions
over the three scores for both grammaticality
and informativity together with average summary
lengths in tokens. For both grammaticality and
informativity, for every summary-cluster pair we
did majority voting and resolved ties by assign-
ing the lower score. For example, if a system
got the ratings 1, 1, 2, 2 for a certain cluster, we
counted this as 1. We dismissed cases where the
tie was between the maximum and the minimum
score?this happened with some summaries which
got just three scores (i.e., 0, 1, 2) and accounted
for < 4% of the cases. To obtain the informativ-
ity distribution we considered only clusters which
were classified as containing a single prevailing
event by at least ten raters. For English 75 out
of 80 clusters qualified as such (37 out of 40 for
Spanish). Similar to above, we dismissed about
3% tie cases where the ratings diverged signifi-
cantly (e.g., 0, 1, 2).
327
System Gram-2 Gram-1 Gram-0 Info-2 Info-1 Info-0 Avg. Len.
Baseline (EN) 21% 15% 65% 18% 10% 73% 8
Shortest path (EN) 52% 16% 32% 36% 33% 31% 10
Shortest path++ (EN) 64% 13% 23% 52% 32% 16% 12
Baseline (ES) 12% 15% 74% 9% 19% 72% 8
Shortest path (ES) 58% 21% 21% 23% 26% 51% 10
Shortest path++ (ES) 50% 21% 29% 40% 40% 20% 12
Table 2: Distribution over possible ratings and average length for English and Spanish.
5.3 Discussion
The difference between the baseline and our short-
est path systems is striking. Although more
than 20% of the baseline summaries are perfectly
grammatical, the gap to the improved version of
shortest paths is significant, about 43%. The same
holds for the percentage of informative summaries
(18% vs. 52%). Both numbers are likely to be
understated as we chose to resolve all ties not
in our favor. 84% of the summaries generated
by the improved method are related to the main
theme of the cluster, and more than 60% of those
(52% of the total summaries) convey the very gist
of it without missing any important information.
Comparing the two configurations we have pro-
posed, improved scoring function and reranking
we added on top of the shortest path method were
both rewarding. Interestingly, even the straight-
forward approach of choosing the shortest path of
a minimum length already guarantees a grammat-
ical summary in more than half of the cases.
An interesting difference in the performance
for Spanish and English is that shortest path gen-
erates more grammatical sentences than the im-
proved version of it. However, the price for higher
grammaticality scores is a huge drop in informa-
tivity: half of such summaries are not related to
the main theme at all, whereas 40% of the sum-
maries generated by the improved version got the
highest rating. A possible reason for the poorer
performance for Spanish is that we used a much
smaller list of stopwords which did not include
news-specific words like, e.g., dijo (said) which
resulted in denser graphs. In the future, we would
like to apply the method to more languages and
experiment with longer lists of stopwords.
One may notice that the summaries produced
by the baseline are shorter than those generated
by the shortest paths which might look like a rea-
son for its comparatively poor performance. How-
ever, the main source of errors for the baseline
was its inability to keep track of the words al-
ready present in the summary, so it is unlikely that
longer sequences would be of a much higher qual-
ity. The sentences generated by the baseline were
often repetitive, e.g., The food tax on food tax on
food. This is not an issue with the shortest path
approaches as they never include loops when edge
weights are strictly positive.
The reranking we added to the shortest path
method is the reason for why the summaries gen-
erated by the improved version of the system are
on average slightly longer than those produced
by the simpler version. The average lengths for
both systems are drastically shorter than the aver-
age length of the sentences served as input (10/12
vs. 28 tokens in English or 35 tokens for Span-
ish). This corresponds to the compression rate of
36-43% (29-34% for Spanish) which is compar-
atively ?aggressive? as it usually varies between
50-80% in other systems.
6 Comparison with Related Work
6.1 Sentence Compression
In the last ten years a lot of research has been
devoted to sentence compression. Most studies
share two properties: (1) they rely on syntax, and
(2) they are supervised. The degree of syntax-
dependence varies between methods. Some uti-
lize a parser to identify and later keep certain im-
portant relations but do not require a complete
parse (Clarke & Lapata, 2008), or use a syn-
tactic representation to extract features (McDon-
ald, 2006). For other approaches correct syntac-
328
tic trees are crucial to obtain grammatical com-
pressions (Galley & McKeown, 2007; Filippova
& Strube, 2008a; Cohn & Lapata, 2009). Hand-
crafted rules (Dorr et al, 2003) as well as lan-
guage models also have been utilized to generate
fluent compressions (Hori et al, 2003; Clarke &
Lapata, 2008).
6.2 Sentence Generation
To date the work on sentence fusion is com-
pletely dependency syntax-based. Input sentences
are parsed into trees, from those trees a new de-
pendency structure is generated, and this struc-
ture is finally converted into a sentence (Barzilay
& McKeown, 2005; Filippova & Strube, 2008b;
Wan et al, 2009). Parser quality is of crucial
importance for such methods, and to our knowl-
edge no attempt has been made to generate novel
sentences without adhering to dependency repre-
sentations. In the future, it would be of interest
to compare our method with a syntax-based fu-
sion method. Syntax-lean methods have been ex-
plored for headline generation (Banko et al, 2000;
Dorr et al, 2003; Jin & Hauptmann, 2003). How-
ever, they do not aim at generating complete sen-
tences or informative summaries but rather to in-
dicate what the news is about.
6.3 Word Graphs and Lattices
Perhaps the work of Barzilay & Lee (2003) who
align comparable sentences to generate sentence-
level paraphrases seems closest to ours in that we
both use word graphs for text generation. How-
ever, this is a fairly general similarity, as both
the goal and the implementation are different.
While we search for an optimal weighting func-
tion in noisy graphs to identify readable and in-
formative compressions, they induce paraphrase
patterns from unweighted paths in much smaller
DAGs obtained from highly similar sentences.
Shen et al (2006) is another example of using
word lattices to find paraphrases. Unlike Barzilay
& Lee (2003), they propose to use syntax to obtain
accurate alignments. Numerous examples of the
utility of word lattices come from the field of finite
state automata, language modeling, speech recog-
nition, parsing and machine translation (Mohri,
1997, inter alia).
7 Conclusions
We considered the task of generating a short in-
formative summary for a set of related sentences,
called multi-sentence compression, which arises
naturally in the context of multi-document text
summarization. We presented a simple but ro-
bust method which proceeds by finding shortest
paths in word graphs. The novelty of our work
is that we demonstrated that reasonable compres-
sions can be obtained without any syntactic infor-
mation if a good weighting function is defined.
This distinguishes our work from earlier research
on sentence fusion and compression which re-
lies on syntactic representations and/or language
models. We provided the details of an extensive
evaluation on English and Spanish data and re-
ported high grammaticality as well as informativ-
ity scores. In the future we would like to experi-
ment with other languages and eschew using part-
of-speech information.
Acknowledgements: I am thankful to Keith
Hall for the discussions on this work and the very
helpful feedback on an earlier draft of this paper.
References
Banko, M., V. O. Mittal & M. J. Witbrock (2000).
Headline generation based on statistical trans-
lation. In Proc. of ACL-00, pp. 318?325.
Barzilay, R. & L. Lee (2003). Learning to para-
phrase: An unsupervized approach using multi-
sequence alignment. In Proc. of HLT-NAACL-
03, pp. 16?23.
Barzilay, R. & K. R. McKeown (2005). Sentence
fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?327.
Clarke, J. & M. Lapata (2007). Modelling com-
pression with discourse constraints. In Proc. of
EMNLP-CoNLL-07, pp. 1?11.
Clarke, J. & M. Lapata (2008). Global infer-
ence for sentence compression: An integer lin-
ear programming approach. Journal of Artifi-
cial Intelligence Research, 31:399?429.
329
Cohn, T. & M. Lapata (2009). Sentence compres-
sion as tree transduction. Journal of Artificial
Intelligence Research, 34:637?674.
Corston-Oliver, S. H. (2001). Text compaction for
display on very small screens. In Proceedings
of the NAACL Workshop on Automatic Summa-
rization, Pittsburg, PA, 3 June 2001, pp. 89?98.
Dorr, B., D. Zajic & R. Schwartz (2003). Hedge
trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the Text Sum-
marization Workshop at HLT-NAACL-03, Ed-
monton, Alberta, Canada, 2003, pp. 1?8.
Filippova, K. & M. Strube (2008a). Dependency
tree based sentence compression. In Proc. of
INLG-08, pp. 25?32.
Filippova, K. & M. Strube (2008b). Sentence
fusion via dependency graph compression. In
Proc. of EMNLP-08, pp. 177?185.
Galley, M. & K. R. McKeown (2007). Lexicalized
Markov grammars for sentence compression. In
Proc. of NAACL-HLT-07, pp. 180?187.
Hori, C., S. Furui, R. Malkin, H. Yu & A. Waibel
(2003). A statistical approach to automatic
speech summarization. EURASIP Journal on
Applied Signal Processing, 2:128?139.
Jin, R. & A. G. Hauptmann (2003). Automatic
title generation for spoken broadcast news. In
Proc. of HLT-01, pp. 1?3.
Jing, H. & K. McKeown (2000). Cut and paste
based text summarization. In Proc. of NAACL-
00, pp. 178?185.
Krahmer, E., E. Marsi & P. van Pelt (2008).
Query-based sentence fusion is better defined
and leads to more preferred results than generic
sentence fusion. In Proc. of ACL-HLT-08, pp.
193?196.
Mani, I. (2001). Automatic Summarization. Ams-
terdam, Philadelphia: John Benjamins.
McDonald, R. (2006). Discriminative sentence
compression with soft syntactic evidence. In
Proc. of EACL-06, pp. 297?304.
Mohri, M. (1997). Finite-state transducers in lan-
guage and speech processing. Computational
Linguistics, 23(2):269?311.
Shen, S., D. Radev, A. Patel & G. Erkan (2006).
Adding syntax to dynamic programming for
aligning comparable texts for generation of
paraphrases. In Proc. of COLING-ACL-06, pp.
747?754.
Wan, S., M. Dras, R. Dale & C. Paris (2009). Im-
proving grammaticality in statistical sentence
generation: Introducing a dependency spanning
tree algorithm with an argument satisfaction
model. In Proc. of EACL-09, pp. 852?860.
330
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1478?1488, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
User Demographics and Language in an Implicit Social Network
Katja Filippova
Google Inc.
Brandschenkestr. 110
Zu?rich, 8004 Switzerland
katjaf@google.com
Abstract
We consider the task of predicting the gender
of the YouTube1 users and contrast two infor-
mation sources: the comments they leave and
the social environment induced from the af-
filiation graph of users and videos. We prop-
agate gender information through the videos
and show that a user?s gender can be predicted
from her social environment with the accuracy
above 90%. We also show that the gender can
be predicted from language alone (89%). A
surprising result of our study is that the latter
predictions correlate more strongly with the
gender predominant in the user?s environment
than with the sex of the person as reported in
the profile. We also investigate how the two
views (linguistic and social) can be combined
and analyse how prediction accuracy changes
over different age groups.
1 Introduction
Over the past decade the web has become more and
more social. The number of people having an iden-
tity on one of the Internet social networks (Face-
book2, Google+3, Twitter4, etc.) has been steadily
growing, many users communicate online on a daily
basis. Their interactions open new possibilities for
social sciences, and linguistics is no exception. For
example, with the development and growth of Web
2.0, it has become possible to get access to masses
of text data labeled with respect to different social
1www.youtube.com
2www.facebook.com
3www.plus.google.com
4www.twitter.com
parameters such as country, age, gender, profession
or religion. The study of language varieties between
groups separated by a certain social variable belongs
to the field of sociolinguistics which more generally
investigates the effect of society on how language is
used (Coulmas, 1998). Historically, sociolinguistics
is connected to dialectology whose focus has been
primarily on the phonetic aspect of the regional di-
alects but was later extended to sociolects (Cham-
bers & Trudgill, 1998). A usual study would involve
sampling speakers from a population, interviewing
them and analyzing the linguistic items with respect
to social variables (Hudson, 1980).
The last decade has seen several studies inves-
tigating the relationship between the language and
the demographics of the users of blogs or Twitter
(see Sec. 2 for references). Most of those studies
used social network sites to collect labeled data?
samples of text together with the demographics vari-
able. However, they did not analyse how social en-
vironment affects language, although very similar
questions have been recently posed (but not yet an-
swered) by Ellist (2009). In our work we attempt to
address precisely this issue. In particular, we con-
sider the task of user gender prediction on YouTube
and contrast two information sources: (1) the com-
ments written by the user and (2) her social neigh-
borhood as defined by the bipartite user-video graph.
We use the comments to train a gender classifier on
a variety of linguistic features. We also introduce a
simple gender propagation procedure to predict per-
son?s gender from the user-video graph.
In what follows we will argue that although lan-
guage does provide us with signals indicative of the
1478
user?s gender5 (as reported in the user?s profile), it
is in fact more indicative of a socially defined gen-
der. Leaving aside the debate on the intricate rela-
tionship between language and gender (see Eckert &
McConnell-Ginet (2003) for a thorough discussion
of the subject), we simply demonstrate that a classi-
fier trained to predict the predominant gender in the
user?s social environment, as approximated by the
YouTube graph of users and videos, achieves higher
accuracy for both genders than the one trained to
predict the user?s inborn gender. We also investi-
gate ways of how the language-based and the so-
cial views can be combined to improve prediction
accuracy. Finally, we look at three age groups ?
teenagers, people in their twenties and people over
thirty ? and show that gender identity is more evi-
dent in the language of younger people but also that
there is a higher correlation between their inborn
gender and the predominant gender in their social
environment.
The paper is organized as follows: we first re-
view related work on the language of social media
and user demographics (Sec. 2) and elaborate on the
goals of our research (Sec. 3). Then we describe
our data (Sec. 4), introduce the demographics prop-
agation experiments (Sec. 5) and the experiments on
supervised learning gender from language (Sec. 6).
2 Related work
Previous studies on language and demographics
which looked at online data can be distinguished
with respect to their aims. (1) Studies coming from
the sociolinguistic community aim at empirically
confirming hypotheses, such as that female speakers
use more pronouns, or that males tend to use longer
words. (2) A standard goal of an NLP study is to
build an automatic system which accurately solves
a given task which in the case of demographics is
predicting user age, gender or country of origin. In
this section we start by reviewing the first kind of
studies, which are about data analysis and hypothe-
ses checking. These are relevant for our choice of
features. Then we briefly summarize a selection of
5Although it might be more correct to talk about the user?s
sex in place of gender (Eckert & McConnell-Ginet, 2003), we
stick to the terminology adopted in previous NLP research on
gender prediction.
studies on demographics prediction to better situate
and motivate our approach.
2.1 Language and demographics analysis
Previous sociolinguistic studies mostly checked hy-
potheses formulated before the widespread use of
the Internet, such as that women use hedges more
often (Lakoff, 1973) or that men use more negations
(Mulac et al 2000), or looked at specific words or
word classes. Newman et al(2008) provide a com-
prehensive review of such work and a description of
the non-web corpora used therein. Some of those
hypotheses were confirmed by empirical evidence,
some not.
For example, Herring & Paolillo (2006) analyse
gender- and genre-specific use of language in on-
line communication on a sample of about 130 blog
entries. Looking at a number of stylistic features
which had previously been claimed to be predic-
tive of gender (Argamon et al 2003; Koppel et al
2004), such as personal pronouns, determiners and
other function words, they find no gender effect. Un-
like them, Kapidzic & Herring (2011) analyse re-
cent chat communications and find that they are gen-
dered. Similarly, Huffaker & Calvert (2005) inves-
tigate the question of identity of teenager bloggers
(e.g., age, gender, sexuality) and find language fea-
tures indicative of gender (e.g., use of emoticons by
males). Burger & Henderson (2006) consider the
relationship between different linguistic (e.g., text
length, use of capital and punctuation letters) and
non-linguistic (e.g., interests, mood) features and
blogger?s age and location. They find that many fea-
tures correlate with the age and run an experiment
with the goal of predicting whether the blog author
is over 18.
2.2 Demographics prediction from language
The studies we review here used supervised ma-
chine learning to obtain models for predicting gen-
der or age. Other demographic attributes, like lo-
cation, ethnicity, or educational level, have also
been predicted automatically (Gillick, 2010; Rao &
Yarowsky, 2011, inter alia). Also, generative ap-
proaches have been applied to discover associations
between language and demographics of social media
users (Eisenstein et al 2011, inter alia) but these are
of less direct relevance for the present work. For su-
1479
pervised approaches, major feature sources are the
text the user has written and also her profile which
may list the name, interests, friends, etc. There have
also been studies which did not look at the language
at all but considered the social environment only.
For example, MacKinnon & Warren (2006) aim at
predicting the age and the location of the LiveJour-
nal6 users. What they found is that there is a remark-
able correlation between the age and the location of
the user and those of her friends, although there are
interesting exceptions.
Burger et al(2011) train a gender classifier on
tweets with word and character-based ngram fea-
tures achieving accuracy of 75.5%. Adding the full
name feature alone gives a boost to 89.1%, fur-
ther features like self-written description and screen
name further help to get 92%. Also, a self-training
method exploring unlabeled data is described but its
performance is worse. Other kinds of sociolinguistic
features and a different classifier have been applied
to gender prediction on tweets by Rao & Yarowsky
(2010).
Nowson & Oberlander (2006) achieve 92% accu-
racy on the gender prediction task using ngram fea-
tures only. Their corpus consist of 1,400/450 posts
written by 47 females and 24 males, respectively.
However, the ngram features were preselected based
on whether they occurred with significant relative
frequency in the language of one gender over the
other. Since the complete dataset was used to pre-
select features, the results are inconclusive.
Yan & Yan (2006) train a Naive Bayes classifier
to predict the gender of a blog entry author. In to-
tal they looked at 75,000 individual blog entries au-
thored by 3,000 bloggers, all of them posted their
genders on the profile page. They measure precision
and recall w.r.t. the minority class (males) and get
the best f-measure of 0.64 (precision and recall are
65% and 71%, respectively).
Rosenthal & McKeown (2011) predict the age of
a blogger, most features they use are extracted from
the blog posts, other features include blogger?s inter-
ests, the number of friends, the usual time of post-
ing, etc. Similarly to Schler et al(2006), they run
a classification experiment with three age classes re-
moving intermediate ages and use the majority-class
6www.livejournal.com
baseline for comparison. In their other experiment
they experiment with a binary classifier for age dis-
tinguishing between the pre- and post-social media
generations and using the years from 1975-1988 as a
boundary. The prediction accuracy increases as later
years are taken.
Interestingly, it has been shown that demograph-
ics can be predicted in more restricted genres than
the personal blog or tweets and from text frag-
ments even shorter than tweets (Otterbacher, 2010;
Popescu & Grefenstette, 2010).
3 Motivation for the present study
Similarly to previous NLP studies, our starting goal
is to predict the self-reported user gender. The first
novelty of our research is that in doing so we con-
trast two sources of information: the user?s social
environment and the text she has written. Indeed,
a topic which has not yet been investigated much
in the reviewed studies on language and user demo-
graphics is the relationship between the language of
the user and her social environment. The data analy-
sis studies (Sec. 2.1) verified hypotheses concerning
the dependency between a language trait (e.g., aver-
age sentence length) and a demographic parameter
(e.g., gender). The demographics prediction studies
(Sec. 2.2) mostly relied on language and user pro-
file features and considered users in isolation. An
exception to this is Garera & Yarowsky (2009) who
showed that, for gender prediction in a dialogue, it
helps to know the interlocutor?s gender. However,
we aim at investigating the impact of the social en-
vironment in a much broader sense than the immedi-
ate interlocutors and in a much broader context than
a conversation.
Language is a social phenomenon, and it is this
fact that motivates all the sociolinguistic research.
Many if not most language traits are not hard-wired
or inborn but can be explained by looking at who
the person interacts most with. Since every lan-
guage speaker can be seen as a member of multiple
overlapping communities (e.g., computer scientists,
French, males, runners), the language of the person
may reflect her membership in different communi-
ties to various degrees. Repeated interactions with
other language speakers influence the way the per-
son speaks (Baxter et al 2006; Bybee, 2010), and
1480
the influence is observable on all the levels of the
language representation (Croft, 2000). For exam-
ple, it has been shown that the more a person is in-
tegrated in a certain community and the tighter the
ties of the social network are, the more prominent
are the representative traits of that community in
the language of the person (Milroy & Milroy, 1992;
Labov, 1994). In our study we adopt a similar view
and analyse the implications it has for gender pre-
diction. Given its social nature, does the language
reflect the norms of a community the user belongs
to or the actual value of a demographic variable?
In our study we address this issue with a particular
modeling technique: we assume that the observed
online behavior adequately reflects the offline life of
a user (more on this in Sec. 4 and 5) and based on
this assumption make inferences about the user?s so-
cial environment. We use language-based features
and a supervised approach to gender prediction to
analyse the relationship between the language and
the variable to be predicted. To our knowledge, we
are the first to question whether it is really the in-
born gender that language-based classifiers learn to
predict. More concrete questions we are going to
suggest answers to are as follows:
1. Previous studies which looked at online data re-
lied on self-reported demographics. The pro-
file data are known to be noisy, although it is
hard to estimate the proportion of false profiles
(Burger et al 2011). Concerning the predic-
tion task, how can we make use of what we
know about the user?s social environment to re-
duce the effect of noise? How can we bene-
fit from the language samples from the users
whose gender we do not know at all?
2. When analyzing the language of a user, how
much are its gender-specific traits due to the
user?s inborn gender and to which extent can
they be explained by her social environment?
Using our modeling technique and a language-
based gender classifier, how is its performance
affected by what we know about the online so-
cial environment of the user?
3. Concerning gender predictions across different
age groups, how does classifier performance
change? Judging from the online communica-
tion, do teenagers signal their gender identity
more than older people? In terms of classifier
accuracy, is it easier to predict a teenager?s gen-
der than the gender of an adult?
The final novelty of our study is that we are the
first to demonstrate how YouTube can be used as
a valuable resource for sociolinguistic research. In
the following section we highlight the points which
make YouTube interesting and unique.
4 Data
Most social networks strive to protect user privacy
and by default do not expose profile information or
reveal user activity (e.g., posts, comments, votes,
etc.). To obtain data for our experiments we use
YouTube, a video sharing site. Most of the YouTube
registered users list their gender, age and location on
their profile pages which, like their comments, are
publicly available. YouTube is an interesting domain
for sociolinguistic research for several reasons:
High diversity: it is not restricted to any particular
topic (e.g., like political blogs) but covers a vast va-
riety of topics attracting a very broad audience, from
children interested in cartoons to academics watch-
ing lectures on philosophy7.
Spontaneous speech: the user comments are ar-
guably more spontaneous than blogs which are more
likely to conform to the norms of written language.
At the same time they are less restricted than tweets
written under the length constraint which encour-
ages highly compressed utterances.
Data availability: all the comments are publicly
available, so we have do not get a biased subset of
what a user has written for the public. Moreover,
we observe users? interactions in different environ-
ments because every video targets particular groups
of people who may share origin (e.g., elections in
Greece) or possession (e.g., how to unlock iPhone)
or any other property. Some videos attract a well-
defined group of people (e.g., the family of a new-
born child), whereas some videos appeal to a very
broad audience (e.g., a kitten video).
7For more information and statistics see the official
YouTube demographics on http://www.youtube.com/
yt/advertise/affinities.html.
1481
female male nn
26% 62% 12%
Table 1: Gender distribution for the extracted 6.9M users.
From the users, videos and the comment relation-
ship we build an affiliation graph (Easley & Klein-
berg, 2010): a user and a video are connected if the
user commented on the video (Fig. 1(a)). Our graph
is unweighted although the number of comments
could be used to weight edges. The co-comment
graph is a stricter version of a more popular co-view
graph used in, e.g., video recommendation studies
(Baluja et al 2008, inter alia).
We obtained a random sample of videos by con-
sidering all the videos whose YouTube ID has a spe-
cific prefix8. From those, we collected the profiles of
the users whose commented on the videos. In total,
we extracted about 6.9M profiles of users who have
written at least 20 comments, not more than 30 com-
ments were collected for every user. The threshold
on the minimum number of comments is set in or-
der to reduce the proportion of users who have used
YouTube only a few times and possibly followed
the suggestions of the site in their video choice.
The users? gender distributions is presented in Table
1. Although females, in particular teenagers, have
been reported to be more likely to blog than males
(Herring et al 2004), males are predominant in our
dataset. A random sample from a pool of users with-
out the 20-comments threshold showed that there are
more male commenters overall, although the differ-
ence is less remarkable for teenagers: 58% of the
teenagers with known gender are male as opposed
to 74% and 79% for the age groups 20-29 and 30+.
Teenagers are also more numerous accounting for
about 35% in our data.
Although we did not filter users based on their lo-
cation or mother tongue as many users comment in
multiple languages, the comment set is overwhelm-
ingly English.
8The YouTube API (http://code.google.com/
apis/youtube/getting_started.html) can be used
to retrieve user profiles and video metadata as well as the com-
ments.
5 Gender propagation
We first consider the user?s social environment to see
whether there is any correlation between the gender
of a user and the gender distribution in her vicinity,
independent of the language. We use a simple prop-
agation procedure to reach the closest neighbors of
a user, that is, other users ?affiliated? with the same
videos. Specifically, we perform the following two
steps:
1. We send the gender information (female, male
or unknown) to all the videos the user has com-
mented on. This way for every video we obtain
a multinomial distribution over three classes
(see Fig. 1(b)).
2. We send the gender distributions from every
video back to all the users who commented on
it and average over all the videos the user is
connected with (see Fig. 1(c)). However, in do-
ing so we adjust the distribution for every user
so that her own demographics is excluded. This
way we have a fair setting where the original
gender of the user is never included in what
she gets back from the connected videos. Thus,
the gender of a user contributes to the vicinity
distributions of all the neighbors but not to her
own final gender distribution.
In line with our motivation and modeling tech-
nique, we chose such a simple method (and not,
say, classification) in order to approximate the of-
fline encounters of the user: does she more often
meet women or men? The way we think of the
videos is that they correspond to places (e.g., a cin-
ema, a cosmetic shop, a pub) visited by the user
where she is unintentionally or deliberately exposed
to how other speakers use the language. Similar to
Baxter et al(2006), we assume that these encoun-
ters influence the way the person speaks. Note that
if the user?s gender has no influence on her choice
of videos, then, on average, we would expect every
video to have the same distribution as in our data
overall: 62% male, 26% female and 12% unknown
(Table 1).
To obtain a single gender prediction from the
propagated distribution, for a given user we select
the gender class (female or male) which got more
1482
(a) Color represents gender information:
blue=male, red=female, grey=unknown.
(b) Propagating gender from users to
videos.
(c) Propagating gender distribution from
videos to users.
Figure 1: Affiliation graph of users (circles) and videos (rectangles).
of the distribution mass. The exact procedure is as
follows: given user u connected with videos Vu =
{v1, ..., vm}, there are m gender distributions sent
to u: PV (u) = {p(g|vi) : 1 ? i ? m, g ?
{f,m, n}}. A single distribution is obtained from
PV (u): p?(g|u) =
?
i p(g|vi)/m.
To address the skewness in the data, i.e., the fact
that 70% of our users (62/(26 + 62)) with known
gender are male, we select the female gender if (a) it
got more than zero mass and at least as much mass
as male: p?(f) > 0 ? p?(f) ? p?(m), or (b) it got at
least ? of the mass: p?(f) ? ? . We set ? = 0.26 ini-
tially because it corresponds to the expected propor-
tion of females (26%) but further experimented with
different ? values in the range of 0.25-0.4. We ob-
tained best accuracy and f-measures with the thresh-
old of 0.33, the difference in accuracy from the ini-
tial threshold of 0.26 being less than 2%. The fact
that the optimal ? value is different from the overall
proportion of females (26%) is not surprising given
that we aggregate per video distributions and not raw
user counts.
The predictions obtained with the described prop-
agation method are remarkably accurate, reaching
90% accuracy (Table 2). The baseline of assigning
all the users the majority class (all male) provides us
with the accuracy of 70% ? the proportion of males
among the users with known gender.
Although the purpose of this section is not to
present a gender prediction method, we find it worth
emphasizing that 90% accuracy is remarkable given
that we only look at the immediate user vicinity.
In the following section we are going to investigate
how this social view on demographics can help us in
Acc% P% R% F1
Baseline 70 - - -
all 90 - - -
fem - 84.3 80.8 83
male - 92.2 93.8 93
Table 2: Precision and recall for propagated gender.
predicting gender from language.
6 Supervised learning of gender
In this section we start by describing our first gen-
der prediction experiment and several extensions to
it and then turn to the results.
6.1 Experiments
Similar to previous studies on demographics predic-
tion, we start with a supervised approach and only
look at the text (comments) written by the user. We
do not rely on any information from the social en-
vironment of the user and do not use any features
extracted from the user profile, like name, which
would make the gender prediction task consider-
ably easier (Burger et al 2011). Finally, we do
not extract any features from the videos the user has
commented on because our goal here is to explore
the language as a sole source of information. Here
we simply want to investigate the extent to which
the language of the user is indicative of her gender
which is found in the profile and which, ignoring the
noise, corresponds to the inborn gender.
In our experiments we use a distributed imple-
mentation of the maximum entropy learner (Berger
et al 1996; McDonald et al 2010) which outputs
1483
a distribution over the classes, the final prediction is
the class with the greater probability. We take 80%
of the users for training and generate a training in-
stance for every user who made her gender visible on
the profile page (4.9M). The remaining 20% of the
data are used for testing (1.2M). We use the follow-
ing three groups of features: (1) character-based:
average comment length, ratio of capital letters to
the total number of letters, ratio of punctuation to the
total number of characters; (2) token-based: average
comment length in words, ratio of unique words to
the total tokens, lowercase unigrams with total count
over all the comments (10K most frequent unigrams
were used, the frequencies were computed on a sep-
arate comment set), use of pronouns, determiners,
function words; (3) sentence-based: average com-
ment length in sentences, average sentence length in
words.
Enhancing the training set. The first question we
consider is how the affiliation graph and propagated
gender can be used to enhance our data for the super-
vised experiments. One possibility would be to train
a classifier on a refined set of users by eliminating
all those whose reported gender did not match the
gender predicted by the neighborhood. This would
presumably reduce the amount of noise by discard-
ing the users who intentionally provided false infor-
mation on their profiles. Another possibility would
be to extend the training set with the users who did
not make their gender visible to the public but whose
gender we can predict from their vicinity. The idea
here is similar to co-training where one has two in-
dependent views on the data. In this case a social
graph view would be combined with the language-
based view.
Profile vs. vicinity gender prediction. The next
question posed in the motivation section is as fol-
lows: Does the fact that language is a social phe-
nomenon and that it is being shaped by the social
environment of the speaker impact our gender clas-
sifier? If there are truly gender-specific language
traits and they are reflected in our features, then
we should not observe any significant difference be-
tween the prediction results on the users whose gen-
der matches the gender propagated from the vicinity
and those whose gender does not match. A contrary
hypothesis would be that what the classifier actually
learns to predict is not as much the inborn but a so-
cial gender. In this case, the classifier trained on the
propagated gender labels should be more accurate
than the one trained on the labels extracted from the
profiles.
To address these questions we contrast two classi-
fiers: (1) the one described in the beginning of the
section which is trained on the gender labels col-
lected from the user profiles; (2) a classifier trained
on the vicinity gender, that is the dominating gender
of the environment of a speaker as obtained with the
procedure described in Section 5.
Age groups and gender prediction. Finally, we
look at how gender predictions change with age and
train three age-specific models to predict gender for
teenagers (13-19), people in their twenties (20-29)
and people over thirty (30+), the age is also ex-
tracted from the profiles. These groups are identified
in order to check whether teenagers tend to signal-
ize their gender identity more than older people, a
hypothesis investigated earlier on a sample of blog
posts (Huffaker & Calvert, 2005).
6.2 Results
We report the results of the supervised experiments
for all the settings described above. As an estimate
of the lowest bound we also give the results of the
majority class baseline (all male) which guarantees
70% accuracy. For the supervised classifiers we re-
port accuracy and per-gender precision, recall and f-
measure. Table 3 presents the results for the starting
classifier trained to predict profile gender.
Acc% P% R% F1 Total
Baseline 70 - - - 619K
all 89 - - - 619K
fem - 83 78 80 182K
male - 91 94 93 437K
Table 3: Results on the test set.
In order to investigate the relationship between the
social environment of a person, her gender and the
language, we split the users from the test set into
two groups: those whose profile gender matched the
gender propagated from the vicinity and those for
whom there was a mismatch. Thus Table 4 presents
the same results as Table 3 but separated for these
1484
two groups of users. It also gives user counts w.r.t.
the profile gender.
Acc% P% R% F1 Total
all (same) 94 - - - 557K
fem (same) - 89 87 88 147K
male (same) - 95 96 96 410K
all (diff) 47 - - - 62K
fem (diff) - 54 39 45 35K
male (diff) - 42 56 48 27K
Table 4: Results for users whose profile gender
matches/differs from the vicinity gender.
Enhanced training set. In the next experiment we
refined the training set by removing all the users
whose vicinity gender did not match the gender re-
ported in the profile. The evaluation was done on
the unmodified set (Table 5). The predictions made
by the model trained on a refined set of users turned
out to be slightly less accurate than those made by
the model trained on the full training set (Table 3).
The refined model performed slightly (< 1%) bet-
ter than the starting one on the users whose vicinity
and the profile genders matched but got very poor
results on the users with a gender mismatch, the ac-
curacy being as low as 37%. The accuracy of the
starting model on those users is 47% (Table 4).
Acc% P% R% F1
all 88 - - -
fem - 83 76 79
male - 90 94 92
Table 5: Results of the models trained on the refined
training set.
In another experiment we extended the training
data with the users whose gender was unknown but
was predicted with the propagation method. How-
ever, a larger training set makes a difference only if
there is a substantial performance gain over the in-
creasing size of the training set. We observed only a
minor gain in performance (< 1%) when the train-
ing data size was increased by an order of magni-
tude. Given that, it is not surprising that adding 12%
did not affect the results.
Language, the vicinity and the profile genders.
The gap in accuracies of predictions for the two user
groups in Table 4 is remarkable: 47% vs. 94%. If
we extrapolate what we observe in the affiliation
graph to other online and offline life, then this re-
sult may suggest that gender traits are more promi-
nent in the language of people spending more time
with the people of their gender than in that of the
people who spend more time with the people of the
opposite gender. Given the remarkable difference,
a further question arises whether the classifier actu-
ally learns to predict a kind of socially rather than the
profile gender. To investigate this, we looked at the
results of the model which knew nothing about the
profile gender but was trained to predict the vicinity
gender instead (Table 6). This model relied on the
exact same set of features but both for training and
testing it used the gender labels obtained from the
propagation procedure described in Section 5.
Acc% P% R% F1
all 91 - - -
fem - 86 80 83
male - 92 95 94
Table 6: Results of the models trained and tested on the
propagated gender.
According to all the evaluation metrics, for both gen-
ders the performance of the classifier trained and
tested on the propagated gender is higher (cf. Ta-
ble 3): the differences in f-measure for female and
male are four and two points respectively, both sta-
tistically significant. This indicates that it is the pre-
dominant environment gender that a language-based
classifier is better at learning rather than the inborn
gender.
Predictions across age groups. Finally, to ad-
dress the question of whether gender differences are
more prominent and thus easer to identify in the lan-
guage of younger people, we looked at the accu-
racy of gender predictions across three age groups.
Table 7 summarizes the results and gives the accu-
racy of the all male baseline as well as of the prop-
agation procedure (Prop-acc). Although the over-
all accuracy over the three groups does not degrade
much, from 89% to 87%, both precision and recall
do decrease significantly for females. This is not
1485
directly reflected in the accuracy because the num-
ber of females drops dramatically from 42% among
teenagers to 26% and then 21% in the latter groups.
For a comparison, the accuracy of the propagated
gender (Prop-acc) also decreases from younger to
older age groups although it is slightly higher than
that of language-based predictions. One conclusion
we can make at this point is that a teenager?s gen-
der is easier to predict from the language which is
in line with the hypothesis that younger people sig-
nalize their gender identities more than older people.
Another observation is that, as the person gets older,
we can be less sure about her gender by looking at
her social environment. This in turn might be an
explanation of why there are less gender signals in
the language of a person: the environment becomes
more mixed, and the influence of both genders be-
comes more balanced.
13-19 20-29 30+ Overall
Base-acc% 58 74 79 70
Prop-acc% 91 90 88 90
Accuracy% 89 89 87 89
Fem-P% 87 81 74 83
Fem-R% 87 76 62 78
Fem-F1 87 78 68 80
Male-P% 90 92 90 91
Male-R% 90 94 94 94
Male-F1 90 93 92 93
Table 7: Results across the age groups.
7 Conclusions
In our study we addressed the gender prediction task
from two perspectives: (1) the social one where we
looked at an affiliation graph of users and videos and
propagated gender information between users, and
(2) the language one where we trained a classifier
on features which have been claimed to be indica-
tive of gender. We demonstrated that both perspec-
tives provide us with comparably accurate predic-
tions (around 90%) but that they are far from be-
ing independent. We also investigated a few ways of
how the performance of a language-based classifier
can be enhanced by the social aspect, compared the
accuracy of predictions across different age groups
and found support for hypotheses made in earlier so-
ciolinguistic studies.
We are not the first to predict gender from lan-
guage features with online data. However, to our
knowledge, we are the first to contrast the two views,
social and language-based, using online data and to
question whether there is a clear understanding of
what gender classifiers actually learn to predict from
language. Our results indicate that from the standard
language cues we are better at predicting a social
gender, that is the gender defined by the environment
of a person, rather than the inborn gender.
The theoretical significance of this result is that
it provides support for the usage-based view on lan-
guage (Bybee, 2010), namely that the person?s lan-
guage is largely shaped by the interactions with her
social environment. On the practical side, it may
have implications for targeted advertisement as it en-
riches the understanding of what gender classifiers
predict.
Acknowledgements: I am thankful to Enrique Al-
fonseca, Keith Hall and the EMNLP reviewers for
their feedback.
References
Argamon, S., M. Koppel, J. Fine & A. R. Shimoni
(2003). Gender, genre, and writing style in formal
written texts. Text, 23(3).
Baluja, S., R. Seth, D. Sivakumar, Y. Jing, J. Yag-
nik, S. Kumar, D. Ravichandran & M. Aly (2008).
Video suggestion and discovery for YouTube:
Taking random walks through the view graph. In
Proc. of WWW-08, pp. 895?904.
Baxter, G. J., R. A. Blythe, W. Croft & A. J. McKane
(2006). Utterance selection model of language
change. Physical Review, E73.046118.
Berger, A., S. A. Della Pietra & V. J. Della Pietra
(1996). A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
Burger, J. D., J. Henderson, G. Kim & G. Zarrella
(2011). Discriminating gender on Twitter. In
Proc. of EMNLP-11, pp. 1301?1309.
1486
Burger, J. D. & J. C. Henderson (2006). An ex-
ploration of observable features related to blogger
age. In Proceedings of the AAAI Spring Sympo-
sium on Computational Approaches for Analyzing
Weblogs, Stanford, CA, 27-29 March 2006, pp.
15?20.
Bybee, J. (2010). Language, Usage and Cognition.
Cambridge University Press.
Chambers, J. & P. Trudgill (1998). Dialectology.
Cambridge University Press.
Coulmas, F. (Ed.) (1998). The Handbook of Soci-
olinguistics. Blackwell.
Croft, W. (2000). Explaining Language Change: An
Evolutionary Approach. London: Longman.
Easley, D. & J. Kleinberg (2010). Network, crowds,
and markets: Reasoning about a highly connected
world. Cambridge University Press.
Eckert, P. & S. McConnell-Ginet (2003). Language
and Gender. Cambridge University Press.
Eisenstein, J., N. A. Smith & E. P. Xing (2011). Dis-
covering sociolinguistic associations with struc-
tured sparsity. In Proc. of ACL-11, pp. 1365?
1374.
Ellist, D. (2009). Social (distributed) language mod-
eling, clustering and dialectometry. In Proc. of
TextGraphs at ACL-IJCNLP-09, pp. 1?4.
Garera, N. & D. Yarowsky (2009). Modeling latent
biographic attributes in conversational genres. In
Proc. of ACL-IJCNLP-09, pp. 710?718.
Gillick, D. (2010). Can conversational word usage
be used to predict speaker demographics? In Pro-
ceedings of Interspeech, Makuhari, Japan, 26-30
September 2010.
Herring, S. C. & J. C. Paolillo (2006). Gender and
genre variation in weblogs. Journal of Sociolin-
guistics, 10(4):439?459.
Herring, S. C., L. A. Scheidt, S. Bonus & E. Wright
(2004). Bridging the gap: A genre analysis of
weblogs. In HICSS-04.
Hudson, R. A. (1980). Sociolinguistics. Cambridge
University Press.
Huffaker, D. A. & S. L. Calvert (2005). Gen-
der, identity and language use in teenager blogs.
Journal of Computer-Mediated Communication,
10(2).
Kapidzic, S. & S. C. Herring (2011). Gen-
der, communication, and self-presentation in teen
chatrooms revisited: Have patterns changed?
Journal of Computer-Mediated Communication,
17(1):39?59.
Koppel, M., S. Argamon & A. R. Shimoni (2004).
Automatically categorizing written text by au-
thor gender. Literary and Linguistic Computing,
17(4):401?412.
Labov, W. (1994). Principles of Linguistic Change:
Internal Factors. Blackwell.
Lakoff, R. (1973). Language and woman?s place.
Language in Society, 2(1):45?80.
MacKinnon, I. & R. Warren (2006). Age and geo-
graphic inferences of the LiveJournal social net-
work. In Statistical Network Analysis: Models,
Issues, and New Directions Workshop at ICML-
2006, Pittsburgh, PA, 29 June, 2006.
McDonald, R., K. Hall & G. Mann (2010). Dis-
tributed training strategies for the structured per-
ceptron. In Proc. of NAACL-HLT-10, pp. 456?
464.
Milroy, L. & J. Milroy (1992). Social network and
social class: Toward an integrated sociolinguistic
model. Language in Society, 21:1?26.
Mulac, A., D. R. Seibold & J. R. Farris (2000). Fe-
male and male managers? and professionals? crit-
icism giving: Differences in language use and ef-
fects. Journal of Language and Social Psychol-
ogy, 19(4):389?415.
Newman, M. L., C. J. Groom, L. D. Handelman &
J. W. Pennebaker (2008). Gender differences in
language use: An analysis of 14,000 text samples.
Discourse Processes, 45:211?236.
1487
Nowson, S. & J. Oberlander (2006). The identity of
bloggers: Openness and gender in personal we-
blogs. In Proceedings of the AAAI Spring Sympo-
sium on Computational Approaches for Analyzing
Weblogs, Stanford, CA, 27-29 March 2006, pp.
163?167.
Otterbacher, J. (2010). Inferring gender of movie
reviewers: Exploiting writing style, content and
metadata. In Proc. of CIKM-10.
Popescu, A. & G. Grefenstette (2010). Mining user
home location and gender from Flickr tags. In
Proc. of ICWSM-10, pp. 1873?1876.
Rao, D. & D. Yarowsky (2010). Detecting latent
user properties in social media. In Proc. of the
NIPS MLSN Workshop.
Rao, D. & D. Yarowsky (2011). Typed graph models
for semi-supervised learning of name ethnicity. In
Proc. of ACL-11, pp. 514?518.
Rosenthal, S. & K. McKeown (2011). Age predic-
tion in blogs: A study of style, content, and on-
line behavior in pre- and post-social media gener-
ations. In Proc. of ACL-11, pp. 763?772.
Schler, J., M. Koppel, S. Argamon & J. Pennebaker
(2006). Effects of age and gender on blogging.
In Proceedings of the AAAI Spring Symposium
on Computational Approaches for Analyzing We-
blogs, Stanford, CA, 27-29 March 2006, pp. 199?
205.
Yan, X. & L. Yan (2006). Gender classifica-
tion of weblogs authors. In Proceedings of the
AAAI Spring Symposium on Computational Ap-
proaches for Analyzing Weblogs, Stanford, CA,
27-29 March 2006, pp. 228?230.
1488
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481?1491,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Overcoming the Lack of Parallel Data in Sentence Compression
Katja Filippova and Yasemin Altun
Google
Brandschenkestr. 110
Zu?rich, 8004 Switzerland
katjaf|altun@google.com
Abstract
A major challenge in supervised sentence
compression is making use of rich feature rep-
resentations because of very scarce parallel
data. We address this problem and present
a method to automatically build a compres-
sion corpus with hundreds of thousands of
instances on which deletion-based algorithms
can be trained. In our corpus, the syntactic
trees of the compressions are subtrees of their
uncompressed counterparts, and hence super-
vised systems which require a structural align-
ment between the input and output can be suc-
cessfully trained. We also extend an exist-
ing unsupervised compression method with a
learning module. The new system uses struc-
tured prediction to learn from lexical, syntac-
tic and other features. An evaluation with hu-
man raters shows that the presented data har-
vesting method indeed produces a parallel cor-
pus of high quality. Also, the supervised sys-
tem trained on this corpus gets high scores
both from human raters and in an automatic
evaluation setting, significantly outperforming
a strong baseline.
1 Introduction and related work
Sentence compression is a paraphrasing task where
the goal is to generate sentences shorter than given
while preserving the essential content. A robust
compression system would be useful for mobile de-
vices as well as a module in an extractive sum-
marization system (Mani, 2001). Although a com-
pression may differ lexically and structurally from
the source sentence, to date most systems are ex-
tractive and proceed by deleting words from the
input (Knight & Marcu, 2000; Dorr et al, 2003;
Turner & Charniak, 2005; Clarke & Lapata, 2008;
Berg-Kirkpatrick et al, 2011, inter alia). To de-
cide which words, dependencies or phrases can be
dropped, (i) rule-based approaches (Grefenstette,
1998; Jing & McKeown, 2000; Dorr et al, 2003;
Zajic et al, 2007), (ii) supervised models trained
on parallel data (Knight & Marcu, 2000; Turner &
Charniak, 2005; McDonald, 2006; Gillick & Favre,
2009; Galanis & Androutsopoulos, 2010, inter alia)
and (iii) unsupervised methods which make use of
statistics collected from non-parallel data (Hori &
Furui, 2004; Zajic et al, 2007; Clarke & Lapata,
2008; Filippova & Strube, 2008) have been investi-
gated. Since it is infeasible to manually devise a set
of accurate deletion rules with high coverage, recent
research has been devoted to developing statistical
methods and possibly augmenting them with a few
linguistic rules to improve output readability (Clarke
& Lapata, 2008; Nomoto, 2009).
Supervised models. A major problem for super-
vised deletion-based systems is very limited amount
of parallel data. Many approaches make use of a
small portion of the Ziff-Davis corpus which has
about 1K sentence-compression pairs1. Other main
sources of training data are the two manually crafted
compression corpora from the University of Edin-
burgh (?written? and ?spoken?, each approx. 1.4K
pairs). Galanis & Androutsopoulos (2011) attempt
at getting more parallel data by applying a deletion-
based compressor together with an automatic para-
1The method of Galley & McKeown (2007) could benefit
from a larger number of sentences.
1481
phraser and generating multiple alternative com-
pressions. To our knowledge, this extended data set
has not yet been used for successful training of com-
pression systems.
Scarce parallel data makes it hard to go beyond a
small set of features and explore lexicalization. For
example, Knight & Marcu (2000) only induce non-
lexicalized CFG rules, many of which occurred only
once in the training data. The features of McDon-
ald (2006) are formulated exclusively in terms of
syntactic categories. Berg-Kirkpatrick et al (2011)
have as few as 13 features to decide whether a con-
stituent can be dropped. Galanis & Androutsopou-
los (2010) use many features when deciding which
branches of the input dependency tree can be pruned
but require a reranker to select most fluent com-
pressions from a pool of candidates generated in the
pruning phase, many of which are ungrammatical.
Even further data limitations exist for the algo-
rithms which operate on syntactic trees and refor-
mulate the compression task as a tree pruning one
(Nomoto, 2008; Filippova & Strube, 2008; Cohn &
Lapata, 2009; Galanis & Androutsopoulos, 2010, in-
ter alia). These methods are sensitive to alignment
errors, their performance degrades if the syntactic
structure of the compression is very different from
that of the input. For example, see Nomoto?s 2009
analysis of the poor performance of the T3 system of
Cohn & Lapata (2009) when retrained on a corpus of
loosely similar RSS feeds and news.
Unsupervised models. Few approaches require
no training data at all. The model of Hori & Fu-
rui (2004) combines scores estimated from mono-
lingual corpora to generate compressions of tran-
scribed speech. Adopting an integer linear program-
ming (ILP) framework, Clarke & Lapata (2008) use
hand-crafted syntactic constraints and an ngram lan-
guage model, trained on uncompressed sentences, to
find best compressions. The model of Filippova &
Strube (2008) also uses ILP but the problem is for-
mulated over dependencies and not ngrams. Condi-
tional probabilities and word counts collected from
a large treebank are combined in an ad hoc man-
ner to assess grammatical importance and informa-
tiveness of dependencies. Similarly, Woodsend &
Lapata (2010) formulate an ILP problem to gener-
ate news story highlights using precomputed scores.
Again, an ad hoc combination of the scores learned
independently of the task is used in the objective
function.
Contributions of this paper. Our work is moti-
vated by the obvious need for a large parallel corpus
of sentences and compressions on which extractive
systems can be trained. Furthermore, we want the
compressions in the corpus to be structurally very
close to the input. Ideally, in every pair, the com-
pression should correspond to a subtree of the input.
To this end, our contributions are three-fold:
? We describe an automatic procedure of con-
structing a parallel corpus of 250,000 sentence-
compression pairs such that the dependency
tree of the compression is a subtree of the
source tree. An evaluation with human raters
demonstrates high quality of the parallel data
in terms of readability and informativeness.
? We successfully apply the acquired data to train
a novel supervised compression system which
produces readable and informative compres-
sions without employing a separate reranker.
In particular, we start with the unsupervised
method of Filippova & Strube (2008) and re-
place the ad hoc edge weighting with a lin-
ear function over a rich feature representation.
The parameter vector is learned from our cor-
pus specifically for the compression task us-
ing structured prediction (Collins, 2002). The
new system significantly outperforms the base-
line and hence provides further evidence for the
utility of the parallel data.
? We demonstrate that sparse lexical features are
very useful for sentence compression, and that
a large parallel corpus is a requirement for ap-
plying them successfully.
The compression framework we adopt and the un-
supervised baseline are introduced in Section 2, the
training algorithm for learning edge weights from
parallel data is described in Section 3. In Section
4 we explain how to obtain the data and present an
evaluation of its quality. In Section 5 we compare
the baseline with our system and report the results
of an experiment with humans as well as the results
of an automatic evaluation.
1482
2 Framework and baseline
We adopt the unsupervised compression framework
of Filippova & Strube (2008) as our baseline and ex-
tend it to a supervised structured prediction problem.
In the experiments reported by Filippova & Strube
(2008), the system was evaluated on the Edinburgh
corpora. It achieved an F-score (Riezler et al, 2003)
higher than reported by other systems on the same
data under an aggressive compression rate and thus
presents a competitive baseline.
Tree pruning as optimization. In this framework,
compressions are obtained by deleting edges of the
source dependency structure so that (1) the retained
edges form a valid syntactic tree, and (2) their to-
tal edge weight is maximized. The objective func-
tion is defined over set X = {xe, e ? E} of bi-
nary variables, corresponding to the set E of the
source edges, subject to the structural and length
constraints,
f(X) =
?
e?E
xe ? w(e) (1)
Here, w(e) denotes the weight of edge e. This con-
strained optimization problem is solved under the
tree structure and length constraints using ILP. If xe
is resolved to 1, the respective edge is retained, oth-
erwise it is deleted. The tree structure constraints en-
force at most one parent for every node and structure
connectivity (i.e., no disconnected subtrees). Given
that length(node(e)) denotes the length of the node
to which edge e points and ? is the maximum per-
mitted length for the compression, the length con-
straint is simply
?
e?E
xe ? length(node(e)) ? ? (2)
Word limit is used in the original paper, whereas we
use character length which is more appropriate for
system comparisons (Napoles et al, 2011). If uni-
form weights are used in Eq. (1), the optimal so-
lution would correspond to a subtree covering as
many edges as possible while keeping the compres-
sion length under given limit.
The solution to the surface realization problem
(Belz et al, 2011) is standard: the words in the com-
pression subtree are put in the same order they are
found in the source.
Due to space limitations, we refer the reader to
(Filippova & Strube, 2008) for a detailed descrip-
tion on the method. Essential for the present discus-
sion is that source dependency trees are transformed
to dependency graphs in that (1) auxiliary, deter-
miner, preposition, negation and possessive nodes
are collapsed with their heads; (2) prepositions re-
place labels on the edges to their arguments; (3) the
dummy root node is connected with every inflected
verb. Figures 1(a)-1(b) illustrate most of the trans-
formations. The transformations are deterministic
and reversible, they can be implemented in a single
top-down tree traversal2.
The set E of edges in Eq. (1) is thus the set of
edges of the transformed dependency graph, like in
Fig. 1(b). A benefit of the transformations is that
function words and negation appear in the compres-
sion if and only if their head words are present.
Hence no separate constraints are required to en-
sure that negation or a determiner is preserved. The
dummy root node makes constraint formulation eas-
ier and also allows for the generation of compres-
sions from any finite clause of the source.
The described pruning optimization framework
is used both for the unsupervised baseline and for
our supervised system. The difference between the
baseline and our system is in how edge weights,
w(e)?s in Eq. (1), are instantiated.
Baseline edge weights. The precomputed edge
weights reflect syntactic importance as well as infor-
mativeness of the nodes they point to. Given edge
e from head node h to node n, the edge weight is
the product of the syntactic and the informativeness
weights,
w(e) = wsynt(e)? winfo(e) (3)
The syntactic weight is defined as
wsynt(e) = P (label(e)|lemma(h)) (4)
For example, verb kill may have multiple argu-
ments realized with dependency labels subj, dobj, in,
etc. However, these argument labels are not equally
likely, e.g., P (subj|kill) > P (in|kill). When forced
to prune an edge, the system would prefer to keep
2Some of the transformations are comparable to what is im-
plemented in the Stanford parser (de Marneffe et al, 2006).
1483
Britain ?s Ministry of Defense says a British soldier was killed in a roadside blast in southern Afghanistan
ps
poss
prep pobj
nsubj
amod
det
auxpass
nsubj
ccomp
prep
pobj
amod
det
prep
pobj
amod
root
(a) Source dependency tree
root Britain?s Ministry of Defense says British a soldier was killed roadside in a blast southern in Afghanistan
of
subj
root
root
ccomp
subjamod
in
amod
in
amod
(b) Transformed graph
root British a soldier was killed in a blast in Afghanistan
subj
root
amod in in
(c) Tree of extracted headline A British soldier was killed in a blast in
Afghanistan
A British soldier was killed in a blast in Afghanistan
det
amod
subj
auxpass prep
pobj
det prep pobj
root
(d) Tree of extracted headline with transformations undone
Figure 1: Source, transformed and extracted trees given headline British soldier killed in Afghanistan
the subject edge over the preposition-in edge since it
contributes more weight to the objective function.
The informativeness score is inspired by Wood-
send & Lapata (2012) and is defined as
winfo(e) =
Pheadline(lemma(n))
Particle(lemma(n))
(5)
This weight tells us how likely it is that a word
from an article appears in the headline. For exam-
ple, given two edges one of which points to verb say
and another one to verb kill, the latter would be pre-
ferred over the former because kill is more ?head-
liny? than say. When collecting counts for the syn-
tactic and informativeness scores, we used 9M news
articles crawled from the Internet, much more than
Filippova & Strube (2008). As a result our estimates
are probably more accurate than theirs.
Although both wsynt and winfo have a meaning-
ful interpretation, there is no guarantee that product
is the best way to combine the two when assign-
ing edge weights. Also, it is unclear how to inte-
grate other signals, such as distance to the root, node
length or information about the siblings, which pre-
sumably all play a role in determining the overall
edge importance.
3 Learning edge weights
Our supervised system differs from the unsupervised
baseline in that instead of relying on precomputed
scores, we define edge weight w(e) in Eq. (1) with a
linear function over a feature representation,
w(e) = w ? f(e) (6)
Here f(e) is a vector of binary variables for every
feature from the set of all possible but very infre-
quent features in the training set. f(e) has 1 for every
feature extracted for edge e and zero otherwise.
Table 1 gives an overview of the feature types
we use (edge e points from head h to node n).
Note that syntactic, structural and semantic features
are closed-class. For all the structural features but
char length, seven is used as maximum possible
value; all possible character lengths are bucketed
into six classes. All the features are local ? for a
given edge, contextual information is included about
1484
syntactic label(e); for e* to h, label(e*); pos(h); pos(n)
structural depth(n); #children(n); #children(h); char length(n); #words in(n)
semantic NE tag(h); NE tag(n); is negated(n)
lexical lemma(n); lemma(h)-label(e); for e* to n?s siblings, lemma(h)-label(e*)
Table 1: Types of features extracted for edge e from h to n
the head and the target nodes, and the siblings as
well as the children of the latter. The negation fea-
ture is only applicable to verb nodes which contain
a negative particle, like not, after the tree transfor-
mations. Lexical features which combine lemmas
and syntactic labels are inspired by the unsupervised
baseline and are very sparse.
In what follows, our assumption is that we have a
compression corpus at our disposal where for every
input sentence there is a correct ?oracle? compres-
sion such that its transformed parse tree matches a
subtree of the transformed input graph. Given such
a corpus, we can apply structured prediction meth-
ods to learn the parameter vector w. In our study
we employ an averaged variant of online structured
perceptron (Collins, 2002). In the context of sen-
tence fusion, a similar dependency structure prun-
ing framework and a similar learning approach was
adopted by Elsner & Santhanam (2011).
At every iteration, for every input graph, we find
the optimal solution with ILP under the current pa-
rameter vector w. The maximum permitted com-
pression length is set to be the same as the length
of the oracle compression. Since the oracle com-
pression is a subtree of the input graph, it represents
a feasible solution for ILP. The parameter vector is
updated if there is a mismatch between the predicted
and the oracle sets of edges for all the features with
a non-zero net count. More formally, given an input
graph with the set of edges E, oracle compression
C ? E and compression Ct ? E predicted at itera-
tion t , the parameter update vector at t+ 1 is given
by
wt+1 = wt +
?
e?C\Ct
f(e)?
?
e?Ct\C
f(e) (7)
w is averaged over all the wt?s so that features
whose weight fluctuated a lot during training are pe-
nalized (Freund & Shapire, 1999).
Of course, training a model with a large number
of features, such as a lexicalized model, is only pos-
sible if there is a large compression corpus where
the dependency tree of the compression is a subtree
of the source sentence. In the next section we in-
troduce our method of getting a sufficient amount of
such data.
4 Acquiring parallel data automatically
In this section we explain how we obtained a parallel
corpus of sentences and compressions. The underly-
ing idea is to harvest news articles from the Internet
where the headline appears to be similar to the first
sentence and use it to find an extractive compression
of the sentence.
Collecting headline-sentence pairs. Using a
news crawler, we collected a corpus of news arti-
cles in English from the Internet. Similarly to previ-
ous work (Dolan et al, 2004; Wubben et al, 2009;
Bejan & Harabagiu, 2010, inter alia), the Google
News service3 was used to identify news. From ev-
ery article, the headline and the first sentence, which
are known to be semantically similar (Dorr et al,
2003), were extracted. Predictably, very few head-
lines are extractive compressions of the first sen-
tence, therefore simply looking for pairs where the
headline is a subsequence of the words from the first
sentence would not solve the problem of getting a
large amount of parallel data. Importantly, headlines
are syntactically quite different from ?normal? sen-
tences. For example, they may have no main verb,
omit determiners and appear incomplete, making it
hard for a supervised deletion-based system to learn
useful rules. Moreover, we observed poor parsing
accuracy for headlines which would make syntactic
annotations for headlines hardly useful.
Thus, instead of taking the headline as it is, we use
it to find a proper extractive compression of the sen-
3http://news.google.com, Jan-Dec 2012.
1485
tence by matching lemmas of content words (nouns,
verbs, adjectives, adverbs) and coreference IDs of
entities from the headline with those of the sentence.
The exact procedure is as follows (H, S and T stand
for headline, sentence and transformed graph of the
sentence):
PREPROCESSING H and S are preprocessed in a
standard way: tokenized, lemmatized, PoS and NE
tagged. Additionally, S is parsed with a dependency
parser (Nivre, 2006) and transformed as described in
Section 2 to obtain T. Finally, pronominal anaphora
is resolved in S. Recall that S is the first sentence,
so the antecedent must be located in a preceding,
higher-level clause.
FILTERING To restrict the corpus to grammatical
and informative headlines, we implemented a cas-
cade of filters. Pair (H, S) is discarded if any of the
questions in Table 2 is answered positively.
Is H a question?
Is H or S too short? (less than four word tokens)
Is H about as long as S? (min ratio: 1.5)
Does H lack a verb?
Does H begin with a verb?
Is there a noun, verb, adj, adv lemma from H
not found in S?
Are the noun, verb, adj, adv lemmas from H
found in S in a different order?
Table 2: Filters applied to candidate pair (H, S)
MATCHING Given the content words of H, a sub-
set of nodes in T is selected based on lemma or
coreference identity of the main (head) word in the
nodes. For example, the main word of a collapsed
node in T, which covers two words was killed, is
killed; was is its child attached with label aux in the
untransformed parse tree. This node is marked if H
contains word killed or killing because of the lemma
identity. In some cases there are multiple possible
matches. For example, given S Barack Obama said
he will attend G20 and H mentioning Obama, both
Barack Obama and he nodes are marked in T. Once
all the nodes in T which match content words and
entities from H are identified, a minimum subtree
covering these nodes is found such that every word
or entity from H occurs as many times in T as in
H. So if H mentions Obama only once, then either
Barack Obama or he must be covered by the subtree
but not both. This minimum subtree corresponds to
an extractive headline, H*, which we generate by
ordering the surface forms of all the words in the
subtree nodes by their offsets in S. Finally, the char-
acter length of H* is compared with the length of
H. If H* is much longer than H, the pair (H, S) is
discarded (max ratio 1.5).
As an illustration to the procedure, consider the
example from Figure 1 with the extracted headline
and its tree presented in Figure 1(c). Given the
headline British soldier killed in Afghanistan, the
extracted headline would be A British soldier was
killed in a blast in Afghanistan. The lemmas british,
soldier, kill, afghanistan from the headline match the
nodes British, a soldier, was killed, in Afghanistan
in the transformed graph. The node in a blast is
added because it is on the path from was killed to in
Afghanistan. Of course, it is possible to determinis-
tically undo the transformations in order to obtain a
standard dependency tree. In this case the extracted
headline would still correspond to a subtree of the
input (compare Fig. 1(d) with Fig. 1(a)). Also note
that a similar procedure can be implemented for con-
stituency parses.
The resulting corpus consists of 250K tuples (S,
T, H, H*), Appendix provides more examples of
source sentences, original headlines and extracted
headlines. We did not attempt to tune the values for
minimum/maximum length and ratio ? lower thresh-
olds may have produced comparable results.
Evaluating data quality. The described proce-
dure produces a comparatively large compression
corpus but how good are automatically constructed
compressions? To answer this question, we ran-
domly selected 50 tuples from the corpus and set up
an experiment with human raters to validate and as-
sess data quality in terms of readability4 and infor-
mativeness5 which are standard measures of com-
pression quality (Clarke & Lapata, 2006). Raters
were asked to read a sentence and a compression
(original H or extracted H* headline) and then rate
the compression on two five-point scales. Three rat-
ings were collected for every item. Table 3 gives
4Also called grammaticality and fluency.
5Also called importance and representativeness.
1486
average ratings with standard deviation.
AVG read AVG info
ORIG. HEADLINE 4.36 (0.75) 3.86 (0.79)
EXTR. HEADLINE 4.26 (1.01) 3.70 (1.04)
Table 3: Results for two kinds of headlines
In terms of readability and informativeness the
extracted headlines are comparable with human-
written ones: at 95% confidence there is no statis-
tically significant difference between the two.
Encouraged by the results of the validation exper-
iment we proceeded to our next question: Can a su-
pervised compression system be successfully trained
on this corpus?
5 System evaluation and discussion
From the corpus of 250K tuples we used 100K to
get pairs of extracted headlines and sentences for
training (on the development set we did not observe
much improvement from using more training data),
250 for development and the rest for testing. We
ran the learning algorithm for 20 iterations, checking
the performance on the development set. Features
which applied to less than 20 edges were pruned,
the size of the feature set is about 28K.
5.1 Evaluation with humans
50 pairs of original headlines and sentences (differ-
ent from the data validation set in Sec. 4) were ran-
domly selected for an evaluation with humans from
the test data. As in the data quality validation ex-
periment, we asked raters to assess the readability
and informativeness of proposed compressions for
the unsupervised system, our system and human-
written headlines. The latter provide us with upper
bounds on the evaluation criteria. Three ratings per
item per parameter were collected. To get compara-
ble results, the unsupervised and our systems used
the same compression rate: for both, the requested
maximum length was set to the length of the head-
line. Table 4 summarizes the results.
The results indicate that the trained model signifi-
cantly outperforms the unsupervised system, getting
particularly good marks for readability. The differ-
ence in readability between our system and original
headlines is not statistically significant. Note that
AVG read AVG info
ORIG. HEADLINE 4.66? 4.10??
OUR SYSTEM 4.30? 3.52?
UNSUP. SYSTEM 3.70 2.70
Table 4: Results for the systems and original headline: ?
and ? stand for significantly better than Unsupervised and
Our system at 95% confidence, respectively
the unsupervised baseline is also capable of generat-
ing readable compressions but does a much poorer
job in selecting most important information. Our
trained model successfully learned to optimize both
scores. We refer the reader to Appendix for input
and compression examples. Note that the ratings for
the human-written headlines in this experiment are
slightly different from the ratings in the data valida-
tion experiment because a different data sample was
used.
5.2 Automatic evaluation
Our automatic evaluation had the goal of explic-
itly addressing two relevant questions related to our
claims about (1) the benefits of having a large paral-
lel corpus and (2) employing a supervised approach
with a rich feature representation.
1. Our primary motivation for collecting parallel
data has been that having access to sparse lex-
ical features, which considerably increase the
feature space, would benefit compression sys-
tems. But is it really the case for sentence com-
pression? Can a comparable performance be
achieved with a closed, moderately sized set of
dense, non-lexical features? If yes, then a large
compression corpus is probably not needed.
Furthermore, to demonstrate that a large corpus
is not only sufficient but also necessary to learn
weights for thousands of features, we need to
compare the performance of the system when
trained on the full data set and a small portion
of it.
2. The syntactic and informativeness scores in Eq.
(3) were calculated over millions of news arti-
cles and do provide us with meaninful statis-
tics (see Sec. 2). Is there any benefit in re-
placing those scores with weights learned for
1487
their feature counterparts? Recall that one of
our feature types in Table 1 is the concate-
nation of lemma(h) (parent lemma) and la-
bel(e) which relies on the same information
as wsynt = P (label(e)|lemma(h)). The fea-
ture counterpart of winfo defined in Eq. (5) is
lemma(n)?the lemma of the node to which edge
points. How would the supervised system per-
form against the unsupervised one, if it only ex-
tracted features of these two types?
To answer these questions, we sampled 1,000 tu-
ples from the unused test data and measured F1
score (Riezler et al, 2003) by comparing the trees
of the generated compression and the ?correct?, ex-
tracted headline. The systems we compared are the
unsupervised baseline (UNSUP. SYSTEM) and the
supervised model trained on three kinds of feature
sets: (1) SYNT-INFO FEATURES, corresponding to
the supervised training of the unsupervised base-
line model (i.e., lemma(h)-label(e) and lemma(n));
(2) NON-LEX FEATURES, corresponding to a dense,
non-lexical feature representation (i.e., all the fea-
ture types from Table 1 excluding the three involv-
ing lemmas); (3) ALL FEATURES (same as OUR
SYSTEM). Additionally, we trained the system on
10% of the data?10K as opposed to 100K tuples,
ALL FEATURES (10K)?for 20 iterations ignoring
features which applied to less than three edges6. As
before, the same compression rate was used for all
the systems. The results are summarized in Table 5.
F1 score #features
UNSUP. SYSTEM 52.3 N.A.
SYNT-INFO FEATURES 75.0 12,490
NON-LEX FEATURES 79.6 330
ALL FEATURES 84.3 27,813
ALL FEATURES (10K) 81.4 22,529
Table 5: Results for the unsupervised baseline and the
supervised system trained on three kinds of feature sets
Clearly, having more features, lexicalized and un-
lexicalized, is important: there is a significant im-
6Recall from the beginning of the section that for the full
(100K) training set the threshold was set to 20 with no tuning.
For the 10K training set, we tried values of two, three, five and
varied the number of iterations. The result we report is the high-
est we could get for 10K.
provement in going beyond the closed set of 330
non-lexical features to all, from 79.6 to 84.3 points.
Moreover, successful training requires a large cor-
pus since the performance of the system degrades if
only 10K training instances are used. Note that this
number already exceeds all the existing compression
corpora taken together. Hence, sparse lexical fea-
tures are useful for compression and a large paral-
lel corpus is a requirement for successful supervised
training.
Concerning our second question, learning feature
weights from the data produces significantly better
results than the hand-crafted way of making use of
the same information, even if a much larger data
set is used to collect statistics. We observed a dra-
matic increase from 52.3 to 75.0 points. Thus, we
may conclude that training with dense and sparse
features directly from data definitely improves the
performance of the dependency pruning system.
5.3 Discussion
It is important to note that the data we used is chal-
lenging: first sentences in news articles tend to be
long, in fact longer than other news sentences, which
implies less reliable syntactic analysis and noisier
input to the syntax-based systems. In the test set
we used for the evaluation with humans, the mean
sentence length is 165 characters. The average com-
pression rate in characters is 0.46 ? 0.16 which is
quite aggressive7. Recall that we used the very same
framework for the unsupervised baseline and our
system as well as the same compression rate. All the
preprocessing errors affect both systems equally and
the comparison of the two is fair. Predictably, wrong
syntactic parses significantly increase chances of an
ungrammatical compression, and parser errors seem
to be a major source of readability deficiencies.
A property of the described compression frame-
work is that a desired compression length is ex-
pected to be provided by the user. This can be seen
both as a strength and as a weakness, depending on
the application. In a scenario where mobile devices
with a limited screen size are used, or in a summa-
rization scenario where a total summary length is
provided (see the DUC/TAC guidelines8), being able
7We follow the standard terminology where smaller values
imply shorter compressions.
8http://www.nist.gov/tac/
1488
to specify a length is definitely an advantage. How-
ever, one can also think of other applications where
the user does not have a strict length constraint but
wants the text to be somewhat shorter. In this case,
a reranker which compares compressions generated
for a range of possible lengths can be employed to
find a single compression (e.g., mean edge weight in
the solution or a language model-based score).
6 Conclusions
We have addressed a major problem for supervised
extractive compression models ? the lack of a large
parallel corpus. To this end, we presented a method
to automatically build such a corpus from web doc-
uments available on the Internet. An evaluation
with humans demonstrates that the quality of the
corpus is high ? the compressions are grammati-
cal and informative. We also significantly improved
a competitive unsupervised method achieving high
readability and informativeness scores by incorpo-
rating thousands of features and learning the feature
weights from our corpus. This result further con-
firms the practical utility of the automatically ob-
tained data. We have shown that employing lexi-
cal features is important for sentence compression,
and that our supervised module can successfully
learn their weights from the corpus. To our knowl-
edge, we are the first to empirically demonstrate that
sparse features are useful for compression and that a
large parallel corpus is a requirement for a success-
ful learning of their weights. We believe that other
supervised deletion-based systems can benefit from
our work.
Acknowledgements: The authors are thankful to
the EMNLP reviewers for their feedback and sug-
gestions.
Appendix
The appendix presents examples of source sentences
(S), original headlines (H), extracted headlines (H*),
unsupervised baseline (U) and our system (O) com-
pressions.
References
Bejan, C. & S. Harabagiu (2010). Unsupervised
event coreference resolution with rich linguistic
features. In Proc. of ACL-10, pp. 1412?1422.
Belz, A., M. White, D. Espinosa, E. Kow, D. Hogan
& A. Stent (2011). The first surface realization
shared task: Overview and evaluation results. In
Proc. of ENLG-11, pp. 217?226.
Berg-Kirkpatrick, T., D. Gillick & D. Klein (2011).
Jointly learning to extract and compress. In Proc.
of ACL-11.
Clarke, J. & M. Lapata (2006). Models for sen-
tence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proc. of COLING-ACL-06, pp. 377?385.
Clarke, J. & M. Lapata (2008). Global inference
for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Cohn, T. & M. Lapata (2009). Sentence compres-
sion as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Collins, M. (2002). Discriminative training methods
for Hidden Markov Models: Theory and exper-
iments with perceptron algorithms. In Proc. of
EMNLP-02, pp. 1?8.
de Marneffe, M.-C., B. MacCartney & C. D. Man-
ning (2006). Generating typed dependency parses
from phrase structure parses. In Proc. of LREC-
06, pp. 449?454.
Dolan, B., C. Quirk & C. Brokett (2004). Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzer-
land, 23?27 August 2004, pp. 350?356.
Dorr, B., D. Zajic & R. Schwartz (2003). Hedge
trimmer: A parse-and-trim approach to headline
generation. In Proceedings of the Text Summa-
rization Workshop at HLT-NAACL-03, Edmonton,
Alberta, Canada, 2003, pp. 1?8.
1489
S Country star Sara Evans has married former University of Alabama quarterback Jay Barker.
H Country star Sara Evans marries
H* Country star Sara Evans has married
U Sara Evans has married Jay Barker
O Sara Evans has married Jay Barker
S Intel would be building car batteries, expanding its business beyond its core strength, the company said in a statement
H Intel to build car batteries
H* Intel would be building car batteries
U would be building the company said
O Intel would be building car batteries
S A New Orleans Saints team spokesman says tight end Jeremy Shockey was taken to a hospital but is doing fine.
H Spokesman: Shockey taken to hospital, doing fine
H* spokesman says Jeremy Shockey was taken to a hospital but is doing fine
U A New Orleans Saints team spokesman says Jeremy Shockey was taken
O tight end Jeremy Shockey was taken to a hospital but is doing fine
S President Obama declared a major disaster exists in the State of Florida and ordered Federal aid to supplement
State and local recovery efforts in the area struck by severe storms, flooding, tornadoes, and straight-line winds
beginning on May 17, 2009, and continuing.
H President Obama declares major disaster exists in the State of Florida
H* President Obama declared a major disaster exists in the State of Florida
U President Obama declared a major disaster exists and ordered Federal aid
O President Obama declared a major disaster exists in the State of Florida
S Regulators Friday shut down a small Florida bank, bringing to 119 the number of US bank failures this year amid
mounting loan defaults.
H Regulators shut down small Florida bank
H* Regulators shut down a small Florida bank
U shut down bringing the number of failures
O Regulators shut down a small Florida bank
S Three men were arrested Wednesday night and Dayton police said their arrests are in connection to a west Dayton
bank robbery.
H 3 men arrested in connection with Bank robbery
H* Three men were arrested are in connection to a bank robbery
U were arrested and Dayton police said their arrests are
O Three men were arrested and police said their arrests are
S The government and the social partners will resume the talks on the introduction of the so-called crisis tax,
which will be levied on all salaries, pensions and incomes over HRK 3,000.
H Government, social partners to resume talks on introduction of ?crisis? tax.
H* The government and the social partners will resume the talks on the introduction of the crisis tax
U The government will resume the talks on the introduction of the crisis tax which will be levied
O The government and the social partners will resume the talks on the introduction of the crisis tax
S England star David Beckham may have the chance to return to AC Milan after the Italian club?s coach said
he was open to his move on Sunday.
H Beckham has chance of returning to Milan
H* David Beckham may have the chance to return to AC Milan
U David Beckham may have the chance to return said star was
O David Beckham may have the chance to return to AC Milan
S Eastern Health and its insurance company have accepted liability for some patients involved in the breast cancer
testing scandal, according to a statement released Friday afternoon.
H Eastern Health accepts liability for some patients
H* Eastern Health have accepted liability for some patients
U Health have accepted liability according to a statement
O Eastern Health have accepted liability for some patients
S Frontier Communications Corp., a provider of phone, TV and Internet services, said Thursday
it has started a cash tender offer to purchase up to $700 million of its notes.
H Frontier Communications starts tender offer for up to $700 million of notes
H* Frontier Communications has started a tender offer to purchase $700 million of its notes
U Frontier Communications said Thursday a provider has started a tender offer
O Frontier Communications has started a tender offer to purchase $700 million of its notes
1490
Elsner, M. & D. Santhanam (2011). Learning to fuse
disparate sentences. In Proceedings of the Work-
shop on Monolingual Text-to-text Generation, Prt-
land, OR, June 24 2011, pp. 54?63.
Filippova, K. & M. Strube (2008). Dependency tree
based sentence compression. In Proc. of INLG-
08, pp. 25?32.
Freund, Y. & R. E. Shapire (1999). Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Galanis, D. & I. Androutsopoulos (2010). An ex-
tractive supervised two-stage method for sentence
compression. In Proc. of NAACL-HLT-10, pp.
885?893.
Galanis, D. & I. Androutsopoulos (2011). A new
sentence compression dataset and its use in an ab-
stractive generate-and-rank sentence compressor.
In Proc. of UCNLG+Eval-11, pp. 1?11.
Galley, M. & K. R. McKeown (2007). Lexicalized
Markov grammars for sentence compression. In
Proc. of NAACL-HLT-07, pp. 180?187.
Gillick, D. & B. Favre (2009). A scalable global
model for summarization. In ILP for NLP-09, pp.
10?18.
Grefenstette, G. (1998). Producing intelligent tele-
graphic text reduction to provide an audio scan-
ning service for the blind. In Working Notes of
the Workshop on Intelligent Text Summarization,
Palo Alto, Cal., 23 March 1998, pp. 111?117.
Hori, C. & S. Furui (2004). Speech summariza-
tion: An approach through word extraction and
a method for evaluation. IEEE Transactions on
Information and Systems, E87-D(1):15?25.
Jing, H. & K. McKeown (2000). Cut and paste based
text summarization. In Proc. of NAACL-00, pp.
178?185.
Knight, K. & D. Marcu (2000). Statistics-based
summarization ? step one: Sentence compression.
In Proc. of AAAI-00, pp. 703?711.
Mani, I. (2001). Automatic Summarization. Amster-
dam, Philadelphia: John Benjamins.
McDonald, R. (2006). Discriminative sentence com-
pression with soft syntactic evidence. In Proc. of
EACL-06, pp. 297?304.
Napoles, C., C. Callison-Burch, J. Ganitkevitch &
B. Van Durme (2011). Paraphrastic sentence com-
pression with a character-based metric: Tighten-
ing without deletion. In Proceedings of the Work-
shop on Monolingual Text-to-text Generation, Prt-
land, OR, June 24 2011, pp. 84?90.
Nivre, J. (2006). Inductive Dependency Parsing.
Springer.
Nomoto, T. (2008). A generic sentence trimmer with
CRFs. In Proc. of ACL-HLT-08, pp. 299?307.
Nomoto, T. (2009). A comparison of model free ver-
sus model intensive approaches to sentence com-
pression. In Proc. of EMNLP-09, pp. 391?399.
Riezler, S., T. H. King, R. Crouch & A. Zaenen
(2003). Statistical sentence condensation using
ambiguity packing and stochastic disambiguation
methods for Lexical-Functional Grammar. In
Proc. of HLT-NAACL-03, pp. 118?125.
Turner, J. & E. Charniak (2005). Supervised and
unsupervised learning for sentence compression.
In Proc. of ACL-05, pp. 290?297.
Woodsend, K. & M. Lapata (2010). Automatic gen-
eration of story highlights. In Proc. of ACL-10,
pp. 565?574.
Woodsend, K. & M. Lapata (2012). Multiple as-
pect summarization using Integer Linear Pro-
gramming. In Proc. of EMNLP-12, pp. 233?243.
Wubben, S., A. van den Bosch, E. Krahmer &
E. Marsi (2009). Clustering and matching head-
lines for automatic paraphrase acquisition. In
Proc. of ENLG-09, pp. 122?125.
Zajic, D., B. J. Dorr, J. Lin & R. Schwartz (2007).
Multi-candidate reduction: Sentence compression
as a tool for document summarization tasks. In-
formation Processing & Management, Special Is-
sue on Text Summarization, 43(6):1549?1570.
1491
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54?59,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Pattern Learning for Relation Extraction with a Hierarchical Topic Model
Enrique Alfonseca Katja Filippova Jean-Yves Delort
Google Research
Brandschenkestrasse 110
8002 Zurich, Switzerland
{ealfonseca,katjaf,jydelort}@google.com
Guillermo Garrido?
NLP & IR Group, UNED
Juan del Rosal, 16.
28040 Madrid, Spain
ggarrido@lsi.uned.es
Abstract
We describe the use of a hierarchical topic
model for automatically identifying syntactic
and lexical patterns that explicitly state on-
tological relations. We leverage distant su-
pervision using relations from the knowledge
base FreeBase, but do not require any man-
ual heuristic nor manual seed list selections.
Results show that the learned patterns can be
used to extract new relations with good preci-
sion.
1 Introduction
The detection of relations between entities for the
automatic population of knowledge bases is very
useful for solving tasks such as Entity Disambigua-
tion, Information Retrieval and Question Answer-
ing. The availability of high-coverage, general-
purpose knowledge bases enable the automatic iden-
tification and disambiguation of entities in text
and its applications (Bunescu and Pasca, 2006;
Cucerzan, 2007; McNamee and Dang, 2009; Kwok
et al, 2001; Pasca et al, 2006; Weld et al, 2008;
Pereira et al, 2009; Kasneci et al, 2009).
Most early works in this area were designed
for supervised Information Extraction competitions
such as MUC (Sundheim and Chinchor, 1993) and
ACE (ACE, 2004; Doddington et al, 2004; Li et
al., 2011), which rely on the availability of anno-
tated data. Open Information Extraction (Sekine,
2006; Banko et al, 2007; Bollegala et al, 2010)
started as an effort to approach relation extraction in
?Work done during an internship at Google Zurich.
a completely unsupervised way, by learning regular-
ities and patterns from the web. Two example sys-
tems implementing this paradigm are TEXTRUN-
NER (Yates et al, 2007) and REVERB (Fader et al,
2011). These systems do not need any manual data
or rules, but the relational facts they extract are not
immediately disambiguated to entities and relations
from a knowledge base.
A different family of unsupervised methods for
relation extraction is unsupervised semantic pars-
ing, which aims at clustering entity mentions and
relation surface forms, thus generating a semantic
representation of the texts on which inference may
be used. Some techniques that have been used are
Markov Random Fields (Poon and Domingos, 2009)
and Bayesian generative models (Titov and Klemen-
tiev, 2011). These are quite powerful approaches
but have very high computational requirements (cf.
(Yao et al, 2011)).
A good trade-off between fully supervised and
fully unsupervised approaches is distant supervi-
sion, a semi-supervised procedure consisting of find-
ing sentences that contain two entities whose rela-
tion we know, and using those sentences as train-
ing examples for a supervised classifier (Hoffmann
et al, 2010; Wu and Weld, 2010; Hoffmann et al,
2011; Wang et al, 2011; Yao et al, 2011). A usual
problem is that two related entities may co-occur in
one sentence for many unrelated reasons. For ex-
ample, Barack Obama is the president of the United
States, but not every sentence including the two en-
tities supports and states this relation. Much of the
previous work uses heuristics, e.g. extracting sen-
tences only from encyclopedic entries (Mintz et al,
54
2009; Hoffmann et al, 2011; Wang et al, 2011), or
syntactic restrictions on the sentences and the entity
mentions (Wu and Weld, 2010). These are usually
defined manually and may need to be adapted to dif-
ferent languages and domains. Manually selected
seeds can also be used (Ravichandran and Hovy,
2002; Kozareva and Hovy, 2010).
The main contribution of this work is presenting
a variant of distance supervision for relation extrac-
tion where we do not use heuristics in the selection
of the training data. Instead, we use topic models to
discriminate between the patterns that are expressing
the relation and those that are ambiguous and can be
applied across relations. In this way, high-precision
extraction patterns can be learned without the need
of any manual intervention.
2 Unsupervised relational pattern learning
Similar to other distant supervision methods, our ap-
proach takes as input an existing knowledge base
containing entities and relations, and a textual cor-
pus. In this work it is not necessary for the corpus
to be related to the knowledge base. In what follows
we assume that all the relations studied are binary
and hold between exactly two entities in the knowl-
edge base. We also assume a dependency parser is
available, and that the entities have been automat-
ically disambiguated using the knowledge base as
sense inventory.
One of the most important problems to solve in
distant supervision approaches is to be able to dis-
tinguish which of the textual examples that include
two related entities, ei and ej , are supporting the re-
lation. This section describes a fully unsupervised
solution to this problem, computing the probability
that a pattern supports a given relation, which will
allow us to determine the most likely relation ex-
pressed in any sentence. Specifically, if a sentence
contains two entities, ei and ej , connected through a
pattern w, our model computes the probability that
the pattern is expressing any relation ?P (r|w)? for
any relation r defined in the knowledge base. Note
that we refer to patterns with the symbol w, as they
are the words in our topic models.
Preprocessing As a first step, the textual corpus
is processed and the data is transformed in the fol-
lowing way: (a) the input corpus is parsed and en-
Author-book
(Mark Twain, Adventures of Huckleberry
Finn)
ARG1
poss
,,
ARG2
ARG1
nn
,,
novels
nn
,,
ARG2
ARG1
nsubj
--
released ARG2dobj
qq
ARG2 ARG1
conj
rr
ARG1
nsubj
,,
wrote ARG2dobj
rr
ARG1
poss
,,
ARG2
...
(Jhumpa Lahiri, The Namesake)
ARG1
nn
--
ARG2
ARG2 by
prep
qq
ARG1
nn
uu
ARG1
nn
,,
novel
appos
--
ARG2
ARG2 by
prep
qq
ARG1
nn
uu
ARG2 by
prep
qq
ARG1
nn
uu
ARG1
poss
--
ARG1
...
(...)
Person-parent
(Liza Minneli, Judy Garland)
...
(Achilles, Peleus)
...
(...)
Person-death place
(Napoleon Bonaparte, Saint
Helena)
...
(Johann Christian Bach, Lon-
don)
...
(...)
Person-birth place
(Charles Darwin, Shrewsbury)
...
(Anthony Daniels, Salisbury)
...
(...)
Figure 1: Example of a generated set of document collec-
tions from a news corpus for relation extraction. Larger
boxes are document collections (relations), and inner
boxes are documents (entity pairs). Document contain
dependency patterns, which are words in the topic model.
tities are disambiguated; (b) for each relation r in
the knowledge base, a new (initially empty) docu-
ment collection Cr is created; (c) for each entity pair
(ei, ej) which are related in the knowledge base, a
new (initially empty) document Dij is created; (d)
for each sentence in the input corpus containing one
mention of ei and one mention of ej , a new term is
added to Dij consisting of the context in which the
two entities were seen in the document. This context
may be a complex structure, such as the dependency
path joining the two entities, but it is considered for
our purposes as a single term; (e) for each relation r
relating ei with ej , document Dij is added to collec-
tion Cr. Note that if the two entities are related in
different ways at the same time, an identical copy of
the document Dij will be added to the collection for
all those relations.
Figure 1 shows a set of document collections gen-
55
Figure 2: Plate diagram of the generative model used.
erated for three relations using this procedure. Each
relation r has associated a different document col-
lection, which contains one document associated to
each entity pair from the knowledge base which is
in relation r. The words in each document can be,
for example, all the dependency paths that have been
observed in the input textual corpus between the two
related entities. Each document will contain some
very generic paths (e.g. the two entities consecutive
in the text) and some more specific paths.
Generative model Once these collections are
built, we use the generative model from Figure 2
to learn the probability that a dependency path is
conveying some relation between the entities it con-
nects. This model is very similar to the one used
by Haghighi and Vanderwende (2009) in the con-
text of text summarization. w (the observed vari-
able) represents a pattern between two entities. The
topic model ?G captures general patterns that appear
for all relations. ?D captures patterns that are spe-
cific about a certain entity pair, but which are not
generalizable across all pairs with the same relation.
Finally ?A contains the patterns that are observed
across most pairs related with the same relation. ?A
is the topic model of interest for us.
We use Gibbs sampling to estimate the different
models from the source data. The topic assignments
(for each pattern) that are the output of this process
are used to estimate P (r|w): when we observe pat-
tern w, the probability that it conveys relation r.
3 Experiments and results
Settings We use Freebase as our knowledge base.
It can be freely downloaded1. text corpus used con-
tains 33 million English news articles that we down-
loaded between January 2004 and December 2011.
A random sample of 3M of them is used for building
the document collections on which to train the topic
models, and the remaining 30M is used for testing.
The corpus is preprocessed by identifying Freebase
entity mentions, using an approach similar to (Milne
and Witten, 2008), and parsing it with an inductive
dependency parser (Nivre, 2006).
From the three million training documents, a set
of document collections (one per relation) has been
generated, by considering the sentences that contain
two entities which are related in FreeBase through
any binary relation and restricting to high-frequency
200 relations. Two ways of extracting patterns have
been used: (a) Syntactic, taking the dependency
path between the two entities, and (b) Intertext,
taking the text between the two. In both cases, a
topic model has been trained to learn the probabil-
ity of a relation given a pattern w: p(r|w). For ?
we use symmetric Dirichlet priors ?G = 0.1 and
?D = ?A = 0.001, following the intuition that for
the background the probability mass across patterns
should be more evenly distributed. ? is set as (15,
15, 1), indicating in the prior that we expect more
patterns to belong to the background and entity-pair-
specific distributions due to the very noisy nature of
the input data. These values have not been tuned.
As a baseline, using the same training corpus, we
have calculated p(r|w) using the maximum likeli-
hood estimate: the number of times that a pattern w
has been seen connecting two entities for which r
holds divided by the total frequency of the pattern.
Extractions evaluation The patterns have been
applied to the 30 million documents left for testing.
For each pair of entities disambiguated as FreeBase
entities, if they are connected through a known pat-
tern, they are assigned argmaxr p(r|w). We have
randomly sampled 4,000 such extractions and sent
them to raters. An extraction is to be judged cor-
rect if both it is correct in real life and the sentence
from which it was extracted really supports it. We
1http://wiki.freebase.com/wiki/Data dumps
56
Figure 3: Evaluation of the extractions. X-axis has the threshold for p(r|w), and Y-axis has the precision of the extractions as a percentage.
have collected three ratings per example and taken
the majority decision. There was disagreement for
9.4% of the items on whether the sentence supports
the relation, and for 20% of the items on whether the
relation holds in the real world.
The results for different thresholds of p(r|w) are
shown in Figure 3. As can be seen, the MLE base-
lines (in red with syntactic patterns and green with
intertext) perform consistently worse than the mod-
els learned using the topic models (in pink and blue).
The difference in precision, aggregated across all re-
lations, is statistically significant at 95% confidence
for most of the thresholds.
Extractions aggregation We can take advantage
of redundancy on the web to calculate a support met-
ric for the extractions. In this experiment, for every
extracted relation (r, e1, e2), for every occurrence
of a pattern wi connecting e1 and e2, we add up
p(r|wi). Extractions that are obtained many times
and from high-precision patterns will rank higher.
Table 1 describes the results of this aggregation.
We have considered the top four highest-frequency
relations for people. After aggregating all the ex-
tracted relations and ranking them by support, we
have divided the evaluation set into two parts: (a)
for relations that were not already in FreeBase, we
evaluate the precision; (b) for extractions that were
already in FreeBase, we take the top-confidence sen-
tence identified and evaluate whether the sentence
is providing support to the relation. For each of
these, both syntactic patterns and intermediate-text
patterns have been evaluated.
The results are very interesting: using syntax,
Death place appears easy to extract new relations
and to find support. The patterns obtained are quite
unambiguous, e.g.
ARG1
subj
**
died at
prep
vv
home
pobj
ww
in
prep
uu
ARG2
pobj
ww
Relation Unknown relations Known relations
Correct relation P@50 Sentence support P@50
Syntax Intertext Syntax Intertext
Parent 0.58 0.38 1.00 1.00
Death place 0.90 0.68 0.98 0.94
Birth place 0.38 0.56 0.54 0.98
Nationality 0.86 0.78 0.34 0.40
Table 1: Evaluation on aggregated extractions.
On the other hand, birth place and nationality have
very different results for new relation acquisition
vs. finding sentence support for new relations. The
reason is that these relations are very correlated to
other relations that we did not have in our training
set. In the case of birth place, many relations re-
fer to having an official position in the city, such as
mayor; and for nationality, many of the patterns ex-
tract presidents or ministers. Not having mayor or
president in our initial collection (see Figure 1), the
support for these patterns is incorrectly learned. In
the case of nationality, however, even though the ex-
tracted sentences do not support the relation (P@50
= 0.34 for intertext), the new relations extracted are
mostly correct (P@50 = 0.86) as most presidents and
ministers in the real world have the nationality of the
country where they govern.
4 Conclusions
We have described a new distant supervision model
with which to learn patterns for relation extraction
with no manual intervention. Results are promising,
we could obtain new relations that are not in Free-
Base with a high precision for some relation types. It
is also useful to extract support sentences for known
relations. More work is needed in understanding
which relations are compatible or overlapping and
which ones can partially imply each other (such as
president-country or born in-mayor).
57
References
ACE. 2004. The automatic content extraction projects.
http://projects.ldc.upenn.edu/ace.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI?07.
D.T. Bollegala, Y. Matsuo, and M. Ishizuka. 2010. Rela-
tional duality: Unsupervised extraction of semantic re-
lations between entities on the web. In Proceedings of
the 19th international conference on World wide web,
pages 151?160. ACM.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 2007, pages 708?716.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The automatic
content extraction (ace) program?tasks, data, and eval-
uation. In Proceedings of LREC, volume 4, pages
837?840. Citeseer.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identify-
ing relations for open information extraction. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370. Association for Computational Lin-
guistics.
R. Hoffmann, C. Zhang, and D.S. Weld. 2010. Learning
5000 relational extractors. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 286?295. Association for Computa-
tional Linguistics.
R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and
D.S. Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 541?550. Asso-
ciation for Computational Linguistics.
G. Kasneci, M. Ramanath, F. Suchanek, and G. Weikum.
2009. The yago-naga approach to knowledge discov-
ery. ACM SIGMOD Record, 37(4):41?47.
Z. Kozareva and E. Hovy. 2010. Learning arguments
and supertypes of semantic relations using recursive
patterns. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1482?1491. Association for Computational Lin-
guistics.
C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling
question answering to the web. ACM Transactions on
Information Systems (TOIS), 19(3):242?262.
D. Li, S. Somasundaran, and A. Chakraborty. 2011. A
combination of topic models with max-margin learn-
ing for relation detection.
P. McNamee and H.T. Dang. 2009. Overview of the tac
2009 knowledge base population track. In Text Analy-
sis Conference (TAC).
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM conference
on Information and knowledge management, pages
509?518. ACM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2-Volume 2, pages 1003?
1011. Association for Computational Linguistics.
J. Nivre. 2006. Inductive dependency parsing. In
Text, Speech and Language Technology, volume 34.
Springer Verlag.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts-step one: the one-million fact extraction chal-
lenge. In Proceedings of the National Conference on
Artificial Intelligence, page 1400. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
F. Pereira, A. Rajaraman, S. Sarawagi, W. Tunstall-
Pedoe, G. Weikum, and A. Halevy. 2009. An-
swering web questions using structured data: dream
or reality? Proceedings of the VLDB Endowment,
2(2):1646?1646.
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 1-Volume 1, pages 1?10. Association for
Computational Linguistics.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 41?47. Associa-
tion for Computational Linguistics.
S. Sekine. 2006. On-demand information extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731?738. Association for Com-
putational Linguistics.
Beth M. Sundheim and Nancy A. Chinchor. 1993. Sur-
vey of the message understanding conferences. In
HLT?93.
58
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics.
C. Wang, J. Fan, A. Kalyanpur, and D. Gondek. 2011.
Relation extraction with relation topics. In Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing.
Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amershi,
James Fogarty, Raphael Hoffmann, Kayur Patel, and
Michael Skinner. 2008. Intelligence in wikipedia. In
Proceedings of the 23rd national conference on Artifi-
cial intelligence, pages 1609?1614. AAAI Press.
F. Wu and D.S. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127. Association for Computational
Linguistics.
L. Yao, A. Haghighi, S. Riedel, and A. McCallum. 2011.
Structured relation discovery using generative models.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-
head, and S. Soderland. 2007. Textrunner: Open in-
formation extraction on the web. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
59
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 892?901,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Modelling Events through Memory-based, Open-IE Patterns
for Abstractive Summarization
Daniele Pighin
Google Inc.
biondo@google.com
Marco Cornolti
?
University of Pisa, Italy
cornolti@di.unipi.it
Enrique Alfonseca
Google Inc.
ealfonseca@google.com
Katja Filippova
Google Inc.
katjaf@google.com
Abstract
Abstractive text summarization of news
requires a way of representing events, such
as a collection of pattern clusters in which
every cluster represents an event (e.g.,
marriage) and every pattern in the clus-
ter is a way of expressing the event (e.g.,
X married Y, X and Y tied the knot). We
compare three ways of extracting event
patterns: heuristics-based, compression-
based and memory-based. While the for-
mer has been used previously in multi-
document abstraction, the latter two have
never been used for this task. Compared
with the first two techniques, the memory-
based method allows for generating sig-
nificantly more grammatical and informa-
tive sentences, at the cost of searching a
vast space of hundreds of millions of parse
trees of known grammatical utterances. To
this end, we introduce a data structure and
a search method that make it possible to
efficiently extrapolate from every sentence
the parse sub-trees that match against any
of the stored utterances.
1 Introduction
Text summarization beyond extraction requires a
semantic representation that abstracts away from
words and phrases and from which a summary can
be generated (Mani, 2001; Sp?arck-Jones, 2007).
Following and extending recent work in semantic
parsing, information extraction (IE), paraphrase
generation and summarization (Titov and Klemen-
tiev, 2011; Alfonseca et al, 2013; Zhang and
Weld, 2013; Mehdad et al, 2013), the represen-
tation we consider in this paper is a large collec-
?
Work done during an internship at Google Zurich.
[John Smith] and 
[Mary Brown] wed 
in [Baltimore]...
[Smith] tied the 
knot with [Brown] 
this Monday...
#21: death
#22: divorce
#23: marriage
PER married PER
PER and PER wed
PER tied the knot with PER
PER has married PER
John Smith married Mary Brown
e1: J. Smith (PER)
e2: M. Brown (PER)
e3: Baltimore, MD (LOC)
Figure 1: An example of abstracting from input
sentences to an event representation and genera-
tion from that representation.
tion of clusters of event patterns. An abstractive
summarization system relying on such a represen-
tation proceeds by (1) detecting the most relevant
event cluster for a given sentence or sentence col-
lection, and (2) using the most representative pat-
tern from the cluster to generate a concise sum-
mary sentence. Figure 1 illustrates the summa-
rization architecture we are assuming in this pa-
per. Given input text(s) with resolved and typed
entity mentions, event mentions and the most rele-
vant event cluster are detected (first arrow). Then,
a summary sentence is generated from the event
and entity representations (second arrow).
However, the utility of such a representation for
summarization depends on the quality of pattern
clusters. In particular, event patterns must cor-
respond to grammatically correct sentences. In-
troducing an incomplete or incomprehensible pat-
tern (e.g., PER said PER) may negatively affect
both event detection and sentence generation. Re-
lated work on paraphrase detection and relation
extraction is mostly heuristics-based and has re-
lied on hand-crafted rules to collect such patterns
(see Sec. 2). A standard approach is to focus
on binary relations between entities and extract
892
EYent 
moGel
Pattern 
FlXVterinJ
1ewV 
FlXVterV
Pattern 
e[traFtion
1ewV 
artiFle
Pattern 
e[traFtion
,nIerenFe
$EVtraFtiYe
VXmmar\
Figure 2: A generic pipeline for event-driven ab-
stractive headline generation.
the dependency path between the two entities as
an event representation. An obvious limitation
of this approach is there is no guarantee that the
extracted pattern corresponds to a grammatically
correct sentence, e.g., that an essential preposi-
tional phrase is retained like in file for a divorce.
In this paper we explore two novel, data-driven
methods for event pattern extraction. The first,
compression-based method uses a robust sentence
compressor with an aggressive compression rate
to get to the core of the sentence (Sec. 3). The
second, memory-based method relies on a vast
collection of human-written headlines and sen-
tences to find a substructure which is known to
be grammatically correct (Sec. 4). While the lat-
ter method comes closer to ensuring perfect gram-
maticality, it introduces a problem of efficiently
searching the vast space of known well-formed
patterns. Since standard iterative approaches com-
paring every pattern with every sentence are pro-
hibitive here, we present a search strategy which
scales well to huge collections (hundreds of mil-
lions) of sentences.
In order to evaluate the three methods, we con-
sider an abstractive summarization task where the
goal is to get the gist of single sentences by recog-
nizing the underlying event and generating a short
summary sentence. To the best of our knowledge,
this is the first time that this task has been pro-
posed; it can be considered as abstractive sentence
compression, in contrast to most existing sentence
compression systems which are based on selecting
words from the original sentence or rewriting with
simpler paraphrase tables. An extensive evalua-
tion with human raters demonstrates the utility of
the new pattern extraction techniques. Our analy-
sis highlights advantages and disadvantages of the
three methods.
To better isolate the qualities of the three ex-
traction methodologies, all three methods use the
same training data and share components of the
Algorithm 1 HEURISTICEXTRACTOR(T,E): heuristi-
cally extract relational patterns for the dependency parse T
and the set of entities E.
1: /* Global constants /*
2: global V
p
, V
c
, N
p
, N
c
3: V
c
? {subj, nsubj, nsubjpass, dobj, iobj, xcomp,
4: acomp, expl, neg, aux, attr, prt}
5: V
p
? {xcomp}
6: N
c
? {det, predet, num, ps, poss, nc, conj}
7: N
p
? {ps, poss, subj, nsubj, nsubjpass, dobj, iobj}
8: /* Entry point /*
9: P ? ?
10: for all C ? COMBINATIONS(E) do
11: N ? MENTIONNODES(T,C)
12: N
?
? APPLYHEURISTICS(T, BUILDMST(T,N))
13: P ? P ? {BUILDPATTERN(T,N
?
)}
14: return P
15: /* Procedures /*
16: procedure APPLYHEURISTICS(T,N )
17: N
?
? N
18: while |N
?
| > 0 do
19: N
??
? ?
20: for all n ? N
?
do
21: if n.ISVERB() then
22: N
??
? N
??
? INCLUDECHILDREN(n, V
c
)
23: N
??
? N
??
? INCLUDEPARENT(n, V
p
)
24: else if n.ISNOUN() then
25: N
??
? N
??
? INCLUDECHILDREN(n,N
c
)
26: N
??
? N
??
? INCLUDEPARENT(n,N
p
)
27: N
?
? N
??
\N
?
28: procedure INCLUDECHILDREN(n,L)
29: R? ?
30: for all c ? n.CHILDREN() do
31: if c.PARENTEDGELABEL() ? L then
32: R? R ? {c}
33: return R
34: procedure INCLUDEPARENT(n,L)
35: if n.PARENTEDGELABEL() ? L then
36: return {n}
37: else return ?
very same summarization architecture, as shown
in Figure 2: an event model is constructed by clus-
tering the patterns extracted according to the se-
lected extraction method. Then, the same extrac-
tion method is used to collect patterns from sen-
tences in never-seen-before news articles. Finally,
the patterns are used to query the event model and
generate an abstractive summary. The three differ-
ent pattern extractors are detailed in the next three
sections.
2 Heuristics-based pattern extraction
In order to be able to work in an Open-IE man-
ner, applicable to different domains, most existing
pattern extraction systems are based on linguisti-
cally motivated heuristics. Zhang and Weld (2013)
is based on REVERB (Fader et al, 2011), which
uses a regular expression on part-of-speech tags
to produce the extractions. An alternative system,
893
OLLIE (Schmitz et al, 2012), uses syntactic de-
pendency templates to guide the pattern extraction
process.
The heuristics used in this paper are inspired by
Alfonseca et al (2013), who built well formed re-
lational patterns by extending minimum spanning
trees (MST) which connect entity mentions in a
dependency parse. Algorithm 1 details our re-
implementation of their method and the specific
set of rules that we rely on to enforce pattern gram-
maticality. We use the standard Stanford-style set
of dependency labels (de Marneffe et al, 2006).
The input to the algorithm are a parse tree T and
a set of target entities E. We first generate com-
binations of 1-3 elements of E (line 10), then for
each combination C we identify all the nodes in
T that mention any of the entities in C. We con-
tinue by constructing the MST of these nodes, and
finally apply our heuristics to the nodes in the
MST. The procedure APPLYHEURISTICS (:16) re-
cursively grows a nodeset N
?
by including chil-
dren and parents of noun and verb nodes in N
?
based on dependency labels. For example, we in-
clude all children of verbs in N
?
whose label is
listed in V
c
(:3), e.g., active or passive subjects,
direct or indirect objects, particles and auxiliary
verbs. Similarly, we include the parent of a noun
in N
?
if the dependency relation between the node
and its parent is listed in N
p
.
3 Pattern extraction by sentence
compression
Sentence compression is a summarization tech-
nique that shortens input sentences preserving the
most important content (Grefenstette, 1998; Mc-
Donald, 2006; Clarke and Lapata, 2008, inter
alia). While first attempts at integrating a com-
pression module into an extractive summarization
system were not particularly successful (Daum?e
III and Marcu, 2004, inter alia), recent work
has been very promising (Berg-Kirkpatrick et al,
2011; Wang et al, 2013). It has shown that drop-
ping constituents of secondary importance from
selected sentences ? e.g., temporal modifiers or
relative clauses ? results in readable and more in-
formative summaries. Unlike this related work,
our goal here is to compress sentences to obtain
an event pattern ? the minimal grammatical struc-
ture expressing an event. To our knowledge, this
application of sentence compressors is novel. As
in Section 2, we only consider sentences mention-
ing entities and require the compression (pattern)
to retain at least one such mention.
Sentence compression methods are abundant
but very few can be configured to produce out-
put satisfying certain constraints. For example,
most compression algorithms do not accept com-
pression rate as an argument. In our case, sen-
tence compressors which formulate the compres-
sion task as an optimization problem and solve it
with integer linear programming (ILP) tools un-
der a number of constraints are particularly attrac-
tive (Clarke and Lapata, 2008; Filippova and Al-
tun, 2013). They can be extended relatively easily
with both the length constraint and the constraint
on retaining certain words. The method of Clarke
and Lapata (2008) uses a trigram language model
(LM) to score compressions. Since we are inter-
ested in very short outputs, a LM trained on stan-
dard, uncompressed text would not be suitable. In-
stead, we chose to modify the method of Filippova
and Altun (2013) because it relies on dependency
parse trees and does not use any LM scoring.
Like other syntax-based compressors, the sys-
tem of Filippova and Altun (2013) prunes depen-
dency structures to obtain compression trees and
hence sentences. The objective function to maxi-
mize in an ILP problem (Eq. 1) is formulated over
weighted edges in a transformed dependency tree
and is subject to a number of constraints. Edge
weight is defined as a linear function over a fea-
ture set: w(e) = w ? f(e).
F (X) =
?
e?E
x
e
? w(e) (1)
In our reimplementation we followed the algo-
rithm as described by Filippova and Altun (2013).
The compression tree is obtained in two steps.
First, the input tree is transformed with determin-
istic rules, most of which aim at collapsing indis-
pensable modifiers with their heads (determiners,
auxiliary verbs, negation, multi-word expressions,
etc.). Then a sub-tree maximizing the objective
function is found under a number of constraints.
Apart from the structural constrains from the
original system which ensure that the output is a
valid tree, the constraints we add state that:
1. tree size in edges must be in [3, 6],
2. entity mentions must be retained,
3. subject of the clause must be retained,
4. the sub-tree must be covered by a single
clause ? exactly one finite verb must used.
894
Since we consider compressions with different
lengths as candidates, from this set we select the
one with the maximum averaged edge weight as
the final compression. Figure 3 illustrates the
use of the compressor for obtaining event pat-
terns. Dashed edges are dropped as a result of
constrained compression so that the output is John
Smith married Mary Brown and the event pattern
is PER married PER. Note that the root of a sub-
clause is allowed to be the top-level node in the
extracted compression.
Compared with patterns obtaines with heuris-
tics, compression patterns should retain preposi-
tional verb arguments whose removal would ren-
der the pattern ungrammatical. As an example
consider [C. Zeta-Jones] and [M. Douglas] filed
for divorce. The heuristics-based pattern is PER
and PER filed which is incomplete. Unlike it,
the compression-based method keeps the essential
prepositional phrase for divorce in the pattern be-
cause the average edge weight is greater for the
tree with the prepositional phrase.
4 Memory-based pattern extraction
Neither heuristics-based, nor compression-based
methods provide a guarantee that the extracted
pattern is grammatically correct. In this sec-
tion we introduce an extraction technique which
makes it considerably more likely because it only
extracts patterns which have been observed as
full sentences in a human-written text (Sec. 4.1).
However, this memory-based method also poses
a problem not encountered by the two previous
methods: how to search over the vast space of ob-
served headlines and sentences to extract a pattern
from a given sentence? Our trie-based solution,
which we present in the remainder of this sec-
tion, makes it possible to compare a dependency
graph against millions of observed grammatical
utterances in a fraction of a second.
4.1 A tree-trie to store them all. . .
Our objective is to construct a compact representa-
tion of hundreds of millions of observed sentences
that can fit in the memory of a standard worksta-
tion. This data structure should make it possible
to efficiently identify the sub-trees of a sentence
that match any complete utterance previously ob-
served. To this end, we build a trie of depen-
dency trees (which we call a tree-trie) by scan-
ning all the dependency parses in the news training
Algorithm 2 STORE(T, I): store the dependency tree T
in the tree-trie I .
1: /* Entry point /*
2: L? T.LINEARIZE()
3: STORERECURSION(I.ROOT(), L, 0)
4: return M
5: /* Procedures /*
6: procedure STORERECURSION(n,L, o)
7: if o == L.LENGTH() then
8: n.ADDTREESTRUCTURE(L.STRUCTURE())
9: return
10: if not n.HASCHILD(L.TOKEN(o)) then
11: n.ADDCHILD(L.TOKEN(o))
12: n
?
? n.GETCHILD(L.TOKEN(o))
13: STORERECURSION(n
?
, L, o+ 1)
data, and index each tree in the tree-trie accord-
ing to Algorithm 2. For better clarity, the process
is also described graphically in Figure 4. First,
each dependency tree (a) is linearized, resulting
in a data structure that consists of two aligned se-
quences (b). The first sequence (tokens) encodes
word/parent-relation pairs, while the second se-
quence (structure) encodes the offsets of parent
nodes in the linearized tree. As an example, the
first word ?The? is a determiner (?det?) for the sec-
ond node (offset 1) in the sequence, which is ?cat?.
In turn, ?cat? is the subject (?nsubj?) of the node
in position 2, i.e., ?sleeps?. As described in Algo-
rithm 2, we recursively store the token sequence
in the trie, each word/relation pair being stored in
a node. When the token sequence is completely
consumed, we store in the current trie node the
structure of the linearized tree. Combining struc-
tural information with the sequential information
encoded by each path in the trie makes it possi-
ble to rebuild a complete dependency graph. Fig-
ure 4(c) shows an example trie encoding 4 differ-
ent sentences. We highlighted in bold the path cor-
responding to the linearized form (b) of the exam-
ple parse tree (a).
The figure shows that the tree contains two
kinds of nodes: end-of-sentence (EOS) nodes
(red) and non-terminal nodes (in blue). EOS nodes
do not necessarily coincide with trie leaves, as it
is possible to observe complete sentences embed-
ded in longer ones. EOS nodes differ from non-
terminal nodes in that they store one or more struc-
tural sequences corresponding to different syntac-
tic representations of observed sentences with the
same tokens.
Space-complexity and generalization. Storing
all the observed sentences in a single trie requires
huge amounts of memory. To make it possible to
895
root Our sources report John Smith married Mary Brown in Baltimore yesterday
root
root
subj
subj
obj
in
tmod
Figure 3: Transformed dependency tree with a sub-tree expressing an event pattern.
The cat sleeps under the table
root
nsubj
det
prep
pobj
det
(a)
The
det
cat
nsubj
sleeps
ROOT
under
prep
the
det
table
pobj
1 2 -1 2 5 3
(b)
The
det
dog
nsubj
barked
ROOT
1,2,-1
cat
nsubj
sleeps
ROOT
1,2,-1
soundly
advmod
1,2,-1,2
under
prep
the
det
table
pobj
1,2,-1,2,5,3
(c)
Figure 4: A dependency tree (a), its linearized form (b) and the resulting path in a trie (c), in bold.
store a complete tree-trie in memory, we adopt the
following strategy. We replace the surface form of
entity nodes with the coarse entity type (e.g., PER,
LOC, ORG) of the entity. Similarly, we replace
proper nouns with the placeholder ?[P]?, thus sig-
nificantly reducing lexical sparsity. Then, we en-
code each distinct word/relation pair as a 32-bit
unsigned integer. Assuming a maximum tree size
of 255 nodes, we represent structure sequences as
vectors of type unsigned char (8 bit per element).
Finally, we store trie-node children as sorted vec-
tors instead of hash maps to reduce memory foot-
print. As a result, we are able to load a trie encod-
ing 400M input dependency parses, 170M distinct
nodes and 48M distinct sentence structures in un-
der 10GB of RAM.
4.2 . . . and in the vastness match them
At lookup time, we want to use the tree-trie to
identify all sub-graphs of an input dependency tree
T that match at least a complete observed sen-
tence. To do so, we need to identify all paths in
the trie that match any sub-sequence s of the lin-
earized sequence of T nodes. Whenever we en-
counter an EOS node e, we verify if any of the
structures stored at e matches the sub-tree gener-
ated by s. If so, then we have a positive match.
As a sentence might embed many shorter utter-
ances, each input T will generally yield multiple
matches. For example, querying the tree-trie in
Figure 4(c) with the input tree shown in (a) would
yield two results, as both The cat sleeps and The
cat sleeps under the table are complete utterances
stored in the trie.
Algorithm 3 LOOKUP(T, I): Lookup for matches of sub-
set of tree T in the trie index I .
1: /* Entry point /*
2: L? T.LINEARIZE()
3: M ? ?
4: LOOKUPRECURSIVE(T,L, 0, I.ROOT(), ?,M)
5: return M
6: /* Procedures /*
7: procedure LOOKUPRECURSIVE(T,L, o, n, P,M )
8: for all i ? [o, L.LENGTH()) do
9: if n.HASCHILD(L.TOKEN(i)) then
10: n
?
? n.GETCHILD(L.TOKEN(i))
11: P
?
? P ? {i}
12: for all S ? n
?
.TREESTRUCTURES() do
13: if L.ISCOMPATIBLE(S, P
?
) then
14: M ?M ? {T.GETNODES(P
?
)}
15: LOOKUPRECURSIVE(L, i, o+ 1, n
?
, P
?
,M)
Algorithm 3 describes the lookup process in
more detail. The first step consists in the lineariza-
tion of the input tree T . Then, we recursively tra-
verse the trie calling LOOKUPRECURSIVE. The
inputs of this procedure are: the input tree T , its
linearization L and an offset o (starting at 0), the
trie node currently being traversed n (starting with
the root), the set of offsets in L that constitute a
partial match P (initially empty) and the set of
complete matches found M . We recursively tra-
verse all the nodes in the trie that yield a partial
match with any sub-sequence of the linearized to-
kens of T . At each step, we scan all the tokens
in L in the range [o, L.LENGTH()) looking for to-
kens matching any of the children of n. If a match-
ing node is found, a new partial match P
?
is con-
structed by extending P with the matching token
896
Sheet6
Page 1
0 10 20 30 40 50 60 70 80 901.0E-05
1.0E-04
1.0E-03
1.0E-02
1.0E-01
1.0E+00
1.0E+01
f(x) = 1.9E-07 x^3.3E+00
Tree size (number of nodes)
Time (seconds)
Figure 5: Time complexity of lookup operations
for inputs of different sizes.
offset i (line 11), and the recursion continues from
the matching trie node n
?
and offset i (line 15).
Every time a partial match is found, we verify if
the partial match is compatible with any of the
tree structures stored in the matching node. If that
is the case, we identify the corresponding set of
matching nodes in T and add it to the result M
(lines 12-14). A pattern is generated from each
complete match returned by LOOKUP after apply-
ing a simple heuristic: for each verb node v in the
match, we enforce that negations and auxiliaries in
T depending from x are also included in the pat-
tern.
Time complexity of lookups. Let k be the max-
imum fan-out of trie nodes, d be the depth of
the trie and n be the size of an input tree (num-
ber of nodes). If trie node children are hashed
(which has a negative effect on space complex-
ity), then worst case complexity of LOOKUP() is
O(nk)
d?1
. If they are stored as sorted lists, as in
our memory-efficient implementation, theoretical
complexity becomes O(nk log(k))
d?1
. It should
be noted that worst case complexity can only be
observed under extremely unlikely circumstances,
i.e., that at every step of the recursion all the nodes
in the tail of the linearized tree match a child of
the current node. Also, in the actual trie used in
our experiments the average branching factor k is
very small. We observed that a trie storing 400M
sentences (170M nodes) has an average branching
factor of 1.02. While the root of the trie has unsur-
prisingly many children (210K, all the observed
first sentence words), already at depth 2 the aver-
age fan-out is 13.7, and at level 3 it is 4.9.
For an empirical analysis of lookup complexity,
Figure 5 plots, in black, wall-clock lookup time
as a function of tree size n for a random sample
of 1,600 inputs. As shown by the polynomial re-
gression curve (red), observed lookup complexity
is approximately cubic with a very small constant
factor. In general, we can see that for sentences of
common length (20-50 words) a lookup operation
can be completed in well under one second.
5 Evaluation
5.1 Experimental settings
All the models for the experiments that we present
have been trained using the same corpus of
news crawled from the web between 2008 and
2013. The news have been processed with a to-
kenizer, a sentence splitter (Gillick and Favre,
2009), a part-of-speech tagger and dependency
parser (Nivre, 2006), a co-reference resolution
module (Haghighi and Klein, 2009) and an entity
linker based on Wikipedia and Freebase (Milne
and Witten, 2008). We use Freebase types as fine-
grained named entity types, so we are also able to
label e.g. instances of sports teams as such instead
of the coarser label ORG.
Next, the news have been grouped based on
temporal closeness (Zhang and Weld, 2013) and
cosine similarity (using tf?idf weights). For each
of the three pattern extraction methods we used the
same summarization pipeline (as shown above in
Figure 2):
1. Run pattern extraction on the news.
2. For every news collection Coll and entity set
E, generate a set containing all the extracted
patterns from news in Coll mentioning all
the entities in E. These are patterns that are
likely to be paraphrasing each other.
3. Run a clustering algorithm to group together
patterns that typically co-occur in the sets
generated in the previous step. There are
many choices for clustering algorithms (Al-
fonseca et al, 2013; Zhang and Weld, 2013).
Following Alfonseca et al (2013) we use in
this work a Noisy-OR Bayesian Network be-
cause it has already been applied for abstrac-
tive summarization (albeit multi-document),
it provides an easily interpretable probabilis-
tic clustering, and training can be easily par-
allelized to be able to handle large training
sets. The hidden events in the Bayesian net-
work represent pattern clusters. When train-
ing is done, for each extraction pattern p
j
897
Original sentence Abstractive summary (method)
Two-time defending overall World Cup champion Marcel Hirscher won the
challenging giant slalom on the Gran Risa course with two solid runs Sunday
and attributed his victory to a fixed screw in his equipment setup.
Marcel Hirscher has won the giant
slalom. (C)
Zodiac Aerospace posted a 7.9 percent rise in first-quarter revenue, below mar-
ket expectations, but reaffirmed its full-year financial targets.
Zodiac Aerospace has reported a rise in
profits. (C)
Australian free-agent closer Grant Balfour has agreed to terms with the Balti-
more Orioles on a two-year deal, the Baltimore Sun reported on Tuesday citing
multiple industry sources.
Balfour will join the Baltimore Orioles.
(H)
Paul Rudd is ?Ant-Man?: 5 reasons he needs an ?Agents of SHIELD? appear-
ance.
Paul Rudd to play Ant-Man. (H)
Millwall defender Karleigh Osborne has joined Bristol City on a two-and-a-half
year deal after a successful loan spell.
Bristol City have signed Karleigh Os-
borne. (M)
Simon Hoggart, one of the Spectator?s best-loved columnists, died yesterday
after fighting pancreatic cancer for over three years.
Simon Hoggart passed away yesterday.
(M)
Table 1: Abstraction examples from compression (C), heuristic (H) and memory-based (M) patterns.
Method Extractions Abstractions
HEURISTIC 24,630 956
COMPRESSION 15,687 657
MEMORY-BASED 11,459 967
Table 2: Patterns extracted in each method, before
Noisy-OR inference.
and pattern cluster c
i
, the network provides
p(p
j
|c
i
) ?the probability that c
i
will gener-
ate p
j
? and p(c
i
|p
j
) ?the probability that,
given a pattern p
j
, c
i
was the hidden event
that generated it.
At generation time we proceed in the following
way:
1. Given the title or first sentence of a news ar-
ticle, run the same pattern extraction method
that was used in training and, if possible, ob-
tain a pattern p involving some entities.
2. Find the model clusters that contain this pat-
tern, C
p
= {c
i
such that p(c
i
|p) > 0}.
3. Return a ranked list of model patterns
output = {(p
j
, score(p
j
))}, scored as fol-
lows:
score(p
j
) =
?
c
i
?C
p
p(p
j
|c
i
)p(c
i
|p)
where p was the input pattern.
4. Replace the entity placeholders in the top-
scored patterns p
j
with the entities that were
actually mentioned in the input news article.
In all cases the parameters of the network were
predefined as 20,000 nodes in the hidden layer
(model clusters) and 40 Expectation Maximization
(EM) training iterations. Training was distributed
across 20 machines with 10 GB of memory each.
For testing we used 37,584 news crawled dur-
ing December 2013, which had not been used for
training the models. Table 3 shows one pattern
cluster example from each of the three trained
models. The table shows only the surface form
of the pattern for simplicity.
Pattern cluster (MEMORY-BASED)
organization
1
gets organization
0
nod for drug
organization
1
gets organization
0
nod for tablets
organization
0
approves organization
1
drug
organizations
0
approves organization
1
?s drug
organization
1
gets organization
0
nod for capsules
Pattern cluster (HEURISTIC)
organization
0
to buy organization
1
organization
0
to acquire organization
1
organization
0
buys organization
1
organization
0
acquires organization
1
organization
0
to acquire organizations
1
organization
0
buys organizations
1
organization
0
acquires organizations
1
organization
0
agrees to buy organization
1
organization
0
snaps up organization
1
organization
0
to purchase organizations
1
organization
0
is to acquire organization
1
organization
0
has agreed to buy organization
1
organization
0
announces acquisition of organizations
1
organization
0
may bid for organization
1
organization
1
sold to organization
0
organization
1
acquired by organization
0
Pattern cluster (COMPRESSION)
the sports team
1
have acquired person
0
from the sports team
2
the sports team
1
acquired person
0
from the sports team
2
the sports team
2
have traded person
0
to the sports team
1
sports team
1
acquired the rights to person
0
from sports team
2
sports team
2
acquired from sports team
1
in exchange for person
0
sports team
2
have acquired from the sports team
1
in exchange for person
0
Table 3: Examples of pattern clusters. In each
cluster c
i
, patterns are sorted by p(p
j
|c
i
).
898
5.2 Results
Table 2 shows the number of extracted patterns
from the test set, and the number of abstractive
event descriptions produced.
As expected, the number of extracted patterns
using the memory-based model is smaller than
with the two other models, which are based on
generic rules and are less restricted in what they
can generate. As mentioned, the memory-based
model can only extract previously-seen structures.
Compared to this model, with heuristics we can
obtain patterns for more than twice more news ar-
ticles. At the same time, looking at the number
of summary sentences generated they are com-
parable, meaning that a larger proportion of the
memory-based patterns actually appeared in the
pattern clusters and could be used to produce sum-
maries. This is also consistent with the fact that us-
ing heuristics the space of extracted patterns is ba-
sically unbounded and many new patterns can be
generated that were previously unseen ?and these
cannot generate abstractions. A positive outcome
is that restricting the syntactic structure of the ex-
tracted patterns to what has been observed in past
news does not negatively affect end-to-end cover-
age when generating the abstractive summaries.
Table 1 shows some of the abstractive sum-
maries generated with the different methods. For
manually evaluating their quality, a random sam-
ple of 100 original sentences was selected for each
method. The top ranked summary for each origi-
nal sentence was sent to human raters for evalua-
tion, and received three different ratings. None of
the raters had any involvement in the development
of the work or the writing of the paper, and a con-
straint was added that no rater could rate more than
50 abstractions. Raters were presented with the
original sentence and the compressed abstraction,
and were asked to rate it along two dimensions, in
both cases using a 5-point Likert scale:
? Readability: whether the abstracted com-
pression is grammatically correct.
? Informativeness: whether the abstracted
compression conveys the most important in-
formation from the original sentence.
Inter-judge agreement was measured using the
Intra-Class Correlation (ICC) (Shrout and Fleiss,
1979; Cicchetti, 1994). The ICC for readability
was 0.37 (95% confidence interval [0.32, 0.41]),
Method Readability Informativeness
HEURISTIC 3.95 3.07
COMPRESSION 3.98 2.35
MEMORY-BASED 4.20 3.70
Table 4: Results for the three methods when rating
the top-ranked abstraction.
and for informativeness it was 0.64 (95% confi-
dence interval [0,60, 0.67]), representing fair and
substantial reliability.
Table 4 shows the results when rating the top
ranked abstraction using either of the three dif-
ferent models for pattern extraction. The abstrac-
tions produced with the memory-based method are
more readable than those produced with the other
two methods (statistically significant with 95%
confidence).
Regarding informativeness, the differences be-
tween the methods are bigger, because the first two
methods have a proportionally larger number of
items with a high readability but a low informa-
tiveness score. For each method, we have man-
ually reviewed the 25 items where the difference
between readability and informativeness was the
largest, to understand in which cases grammatical,
yet irrelevant compressions are produced. The re-
sults are shown in Table 5. Be+adjective includes
examples where the pattern is of the form Entity is
Adjective, which the compression-based systems
extracts often represents an incomplete extraction.
Wrong inference contains the cases where patterns
that are related but not equivalent are clustered,
e.g. Person arrived in Country and Person arrived
in Country for talks. Info. missing represents cases
where very relevant information has been dropped
and the summary sentence is not complete. Pos-
sibility contains cases where the original sentence
described a possibility and the compression states
it as a fact, or vice versa. Disambiguation are en-
tity disambiguations errors, and Opposite contains
cases of patterns clustered together that are op-
posite along some dimension, e.g. Person quits
TV Program and Person to return to TV Program.
The method with the largest drop between the
readability and informativeness scores is COM-
PRESSION. As can be seen, many of these mis-
takes are due to relevant information being miss-
ing in the summary sentence. This is also the
largest source of errors for the HEURISTIC system.
For the MEMORY-BASED system, the drop in read-
899
Method Be+adjective Wrong inference Info. missing Possibility Disambiguation Opposite
HEURISTIC 0 7 14 3 1 0
COMPRESSION 3 10 10 0 0 2
MEMORY-BASED 0 17 4 2 0 2
Table 5: Sources of errors for the top 25 items with high readability and low informativeness.
Original sentence Pattern extracted (method) Abstraction
David Moyes is happy to use tough love on Adnan Januzaj
to ensure the Manchester United youngster fulfils his mas-
sive potential.
David Moyes is happy. (C)
Fortune will start to favour
David Moyes.
The Democratic People?s Republic of Korea will ?achieve
nothing by making threats or provocation,? the United
States said Friday.
The United States said Fri-
day. (C, H)
United States officials said
Friday.
EU targets Real and Barca over illegal state aid.
EU targets Real Madrid.
(H)
EU is going after Real
Madrid.
EU warns Israel over settlement construction
EU warns Israel. (M)
EU says Israel needs re-
forms.
Table 6: Examples of compression (C), heuristic (H) and memory-based (M) patterns that led to abstrac-
tions with high readability but a low informativeness score. Both incomplete summary sentences and
wrong inferences can be observed.
ability score is much smaller, so there were less of
these examples. And most of these examples be-
long to the class of wrong inferences (patterns that
are related but not equivalent, so we should not
abstract one of them from the other, but they were
clustered together in the model). Our conclusion
is that the examples with missing information are
not such a big problem with the MEMORY-BASED
system, as using the trie is an additional safeguard
that the generated titles are complete statements,
but the method is not preventing the wrong infer-
ence errors so this class of errors become the dom-
inant class by a large margin.
Some examples with high readability but low
informativeness are shown in Table 6.
6 Conclusions
Most Open-IE systems are based on linguistically-
motivated heuristics for learning patterns that ex-
press relations between entities or events. How-
ever, it is common for these patterns to be incom-
plete or ungrammatical, and therefore they are not
suitable for abstractive summary generation of the
relation or event mentioned in the text.
In this paper, we describe a memory-based ap-
proach in which we use a corpus of past news
to learn valid syntactic sentence structures. We
discuss the theoretical time complexity of look-
ing up extraction patterns in a large corpus of
syntactic structures stored as a trie and demon-
strate empirically that this method is effective in
practice. Finally, the evaluation shows that sum-
mary sentences produced by this method outper-
form heuristics and compression-based ones both
in terms of readability and informativeness. The
problem of generating incomplete summary sen-
tences, which was the main source of informative-
ness errors for the alternative methods, becomes a
minor problem with the memory-based approach.
Yet, there are some cases in which also the mem-
ory based approach extracts correct but misleading
utterances, e.g., a pattern like PER passed away
from the sentence PER passed the ball away. To
solve this class of problems, a possible research
direction would be the inclusion of more complex
linguistic features in the tree-trie, such as verb sub-
categorization frames.
As another direction for future work, more ef-
fort is needed in making sure that no incorrect in-
ferences are made with this model. These happen
when a more specific pattern is clustered together
with a less specific pattern, or when two non-
equivalent patterns often co-occur in news as two
events are somewhat correlated in real life, but it is
generally incorrect to infer one from the other. Im-
provements in the pattern-clustering model, out-
side the scope of this paper, will be required.
900
References
Enrique Alfonseca, Daniele Pighin, and Guillermo
Garrido. 2013. HEADY: News headline abstraction
through event pattern clustering. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, Sofia, Bulgaria, 4?9 August
2013, pages 1243?1253.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
OR, 19?24 June 2011.
Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical Assessment, 6(4):284.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Hal Daum?e III and Daniel Marcu. 2004. A tree-
position kernel for document compression. In Pro-
ceedings of the 2004 Document Understanding Con-
ference held at the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics,, Boston,
Mass., 6?7 May 2004.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006, pages 449?454.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, Edinburgh, UK, 27?29 July 2011, pages 1535?
1545.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, Seattle,
WA, USA, 18?21 October 2013, pages 1481?1491.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the ILP
for NLP Workshop, Boulder, CO, June 4 2009, pages
10?18.
Gregory Grefenstette. 1998. Producing intelligent
telegraphic text reduction to provide an audio scan-
ning service for the blind. In Working Notes of the
Workshop on Intelligent Text Summarization, Palo
Alto, Cal., 23 March 1998, pages 111?117.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, Singapore, 6-7 August 2009, pages 1152?1161.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins, Amsterdam, Philadelphia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Trento, Italy, 3?7 April 2006, pages 297?304.
Yashar Mehdad, Giuseppe Carenini, and Frank W.
Tompa. 2013. Abstractive meeting summariza-
tion with entailment and fusion. In Proceedings
of the 14th European Workshop on Natural Lan-
guage Generation, Sofia, Bulgaria, 8?9 August,
2013, pages 136?146.
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the AAAI
2008 Workshop on Wikipedia and Artificial Intelli-
gence, Chicago, IL, 13-14 July, 2008.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Michael Schmitz, Robert Bart, Stephen Soderland,
Oren Etzioni, et al 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Conference on Empirical Methods in Natu-
ral Language Processing, Jeju Island, Korea, 12?14
July 2012, pages 523?534.
Patrick E Shrout and Joseph L Fleiss. 1979. Intraclass
correlations: uses in assessing rater reliability. Psy-
chological bulletin, 86(2):420.
Karen Sp?arck-Jones. 2007. Automatic summaris-
ing: A review and discussion of the state of the art.
Technical Report UCAM-CL-TR-679, University of
Cambridge, Computer Laboratory, Cambridge, U.K.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
OR, 19?24 June 2011, pages 1445?1455.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics, Sofia, Bulgaria, 4?9 August
2013, pages 1384?1394.
Congle Zhang and Daniel S. Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, WA, USA, 18?21 October 2013,
pages 1776?1786.
901
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1252?1261,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Opinion Mining on YouTube
Aliaksei Severyn
1
, Alessandro Moschitti
3,1
,
Olga Uryupina
1
, Barbara Plank
2
, Katja Filippova
4
1
DISI - University of Trento,
2
CLT - University of Copenhagen,
3
Qatar Computing Research Institute,
4
Google Inc.
severyn@disi.unitn.it, amoschitti@qf.org.qa,
uryupina@gmail.com, bplank@cst.dk, katjaf@google.com
Abstract
This paper defines a systematic approach
to Opinion Mining (OM) on YouTube
comments by (i) modeling classifiers for
predicting the opinion polarity and the
type of comment and (ii) proposing ro-
bust shallow syntactic structures for im-
proving model adaptability. We rely on the
tree kernel technology to automatically ex-
tract and learn features with better gener-
alization power than bag-of-words. An ex-
tensive empirical evaluation on our manu-
ally annotated YouTube comments corpus
shows a high classification accuracy and
highlights the benefits of structural mod-
els in a cross-domain setting.
1 Introduction
Social media such as Twitter, Facebook or
YouTube contain rapidly changing information
generated by millions of users that can dramati-
cally affect the reputation of a person or an orga-
nization. This raises the importance of automatic
extraction of sentiments and opinions expressed in
social media.
YouTube is a unique environment, just like
Twitter, but probably even richer: multi-modal,
with a social graph, and discussions between peo-
ple sharing an interest. Hence, doing sentiment
research in such an environment is highly relevant
for the community. While the linguistic conven-
tions used on Twitter and YouTube indeed show
similarities (Baldwin et al, 2013), focusing on
YouTube allows to exploit context information,
possibly also multi-modal information, not avail-
able in isolated tweets, thus rendering it a valuable
resource for the future research.
Nevertheless, there is almost no work showing
effective OM on YouTube comments. To the best
of our knowledge, the only exception is given by
the classification system of YouTube comments
proposed by Siersdorfer et al (2010).
While previous state-of-the-art models for opin-
ion classification have been successfully applied
to traditional corpora (Pang and Lee, 2008),
YouTube comments pose additional challenges:
(i) polarity words can refer to either video or prod-
uct while expressing contrasting sentiments; (ii)
many comments are unrelated or contain spam;
and (iii) learning supervised models requires train-
ing data for each different YouTube domain, e.g.,
tablets, automobiles, etc. For example, consider a
typical comment on a YouTube review video about
a Motorola Xoom tablet:
this guy really puts a negative spin on
this , and I ?m not sure why , this seems
crazy fast , and I ?m not entirely sure
why his pinch to zoom his laggy all the
other xoom reviews
The comment contains a product name xoom and
some negative expressions, thus, a bag-of-words
model would derive a negative polarity for this
product. In contrast, the opinion towards the prod-
uct is neutral as the negative sentiment is ex-
pressed towards the video. Similarly, the follow-
ing comment:
iPad 2 is better. the superior apps just
destroy the xoom.
contains two positive and one negative word, yet
the sentiment towards the product is negative (the
negative word destroy refers to Xoom). Clearly,
the bag-of-words lacks the structural information
linking the sentiment with the target product.
In this paper, we carry out a systematic study on
OM targeting YouTube comments; its contribution
is three-fold: firstly, to solve the problems outlined
above, we define a classification schema, which
separates spam and not related comments from the
informative ones, which are, in turn, further cate-
gorized into video- or product-related comments
1252
(type classification). At the final stage, differ-
ent classifiers assign polarity (positive, negative or
neutral) to each type of a meaningful comment.
This allows us to filter out irrelevant comments,
providing accurate OM distinguishing comments
about the video and the target product.
The second contribution of the paper is the cre-
ation and annotation (by an expert coder) of a
comment corpus containing 35k manually labeled
comments for two product YouTube domains:
tablets and automobiles.
1
It is the first manu-
ally annotated corpus that enables researchers to
use supervised methods on YouTube for comment
classification and opinion analysis. The comments
from different product domains exhibit different
properties (cf. Sec. 5.2), which give the possibility
to study the domain adaptability of the supervised
models by training on one category and testing on
the other (and vice versa).
The third contribution of the paper is a novel
structural representation, based on shallow syn-
tactic trees enriched with conceptual information,
i.e., tags generalizing the specific topic of the
video, e.g., iPad, Kindle, Toyota Camry. Given the
complexity and the novelty of the task, we exploit
structural kernels to automatically engineer novel
features. In particular, we define an efficient tree
kernel derived from the Partial Tree Kernel, (Mos-
chitti, 2006a), suitable for encoding structural rep-
resentation of comments into Support Vector Ma-
chines (SVMs). Finally, our results show that our
models are adaptable, especially when the struc-
tural information is used. Structural models gen-
erally improve on both tasks ? polarity and type
classification ? yielding up to 30% of relative im-
provement, when little data is available. Hence,
the impractical task of annotating data for each
YouTube category can be mitigated by the use of
models that adapt better across domains.
2 Related work
Most prior work on more general OM has been
carried out on more standardized forms of text,
such as consumer reviews or newswire. The most
commonly used datasets include: the MPQA cor-
pus of news documents (Wilson et al, 2005), web
customer review data (Hu and Liu, 2004), Ama-
zon review data (Blitzer et al, 2007), the JDPA
1
The corpus and the annotation guidelines are pub-
licly available at: http://projects.disi.unitn.
it/iKernels/projects/sentube/
corpus of blogs (Kessler et al, 2010), etc. The
aforementioned corpora are, however, only par-
tially suitable for developing models on social
media, since the informal text poses additional
challenges for Information Extraction and Natu-
ral Language Processing. Similar to Twitter, most
YouTube comments are very short, the language
is informal with numerous accidental and deliber-
ate errors and grammatical inconsistencies, which
makes previous corpora less suitable to train mod-
els for OM on YouTube. A recent study focuses on
sentiment analysis for Twitter (Pak and Paroubek,
2010), however, their corpus was compiled auto-
matically by searching for emoticons expressing
positive and negative sentiment only.
Siersdorfer et al (2010) focus on exploiting user
ratings (counts of ?thumbs up/down? as flagged by
other users) of YouTube video comments to train
classifiers to predict the community acceptance of
new comments. Hence, their goal is different: pre-
dicting comment ratings, rather than predicting the
sentiment expressed in a YouTube comment or its
information content. Exploiting the information
from user ratings is a feature that we have not ex-
ploited thus far, but we believe that it is a valuable
feature to use in future work.
Most of the previous work on supervised senti-
ment analysis use feature vectors to encode doc-
uments. While a few successful attempts have
been made to use more involved linguistic anal-
ysis for opinion mining, such as dependency
trees with latent nodes (T?ackstr?om and McDonald,
2011) and syntactic parse trees with vectorized
nodes (Socher et al, 2011), recently, a comprehen-
sive study by Wang and Manning (2012) showed
that a simple model using bigrams and SVMs per-
forms on par with more complex models.
In contrast, we show that adding structural fea-
tures from syntactic trees is particularly useful for
the cross-domain setting. They help to build a sys-
tem that is more robust across domains. Therefore,
rather than trying to build a specialized system
for every new target domain, as it has been done
in most prior work on domain adaptation (Blitzer
et al, 2007; Daum?e, 2007), the domain adapta-
tion problem boils down to finding a more robust
system (S?gaard and Johannsen, 2012; Plank and
Moschitti, 2013). This is in line with recent ad-
vances in parsing the web (Petrov and McDonald,
2012), where participants where asked to build a
single system able to cope with different yet re-
1253
lated domains.
Our approach relies on robust syntactic struc-
tures to automatically generate patterns that adapt
better. These representations have been inspired
by the semantic models developed for Ques-
tion Answering (Moschitti, 2008; Severyn and
Moschitti, 2012; Severyn and Moschitti, 2013)
and Semantic Textual Similarity (Severyn et al,
2013). Moreover, we introduce additional tags,
e.g., video concepts, polarity and negation words,
to achieve better generalization across different
domains where the word distribution and vocab-
ulary changes.
3 Representations and models
Our approach to OM on YouTube relies on the
design of classifiers to predict comment type and
opinion polarity. Such classifiers are traditionally
based on bag-of-words and more advanced fea-
tures. In the next sections, we define a baseline
feature vector model and a novel structural model
based on kernel methods.
3.1 Feature Set
We enrich the traditional bag-of-word representa-
tion with features from a sentiment lexicon and
features quantifying the negation present in the
comment. Our model (FVEC) encodes each docu-
ment using the following feature groups:
- word n-grams: we compute unigrams and
bigrams over lower-cased word lemmas where
binary values are used to indicate the pres-
ence/absence of a given item.
- lexicon: a sentiment lexicon is a collection of
words associated with a positive or negative senti-
ment. We use two manually constructed sentiment
lexicons that are freely available: the MPQA Lex-
icon (Wilson et al, 2005) and the lexicon of Hu
and Liu (2004). For each of the lexicons, we use
the number of words found in the comment that
have positive and negative sentiment as a feature.
- negation: the count of negation words, e.g.,
{don?t, never, not, etc.}, found in a comment.
2
Our structural representation (defined next) en-
ables a more involved treatment of negation.
- video concept: cosine similarity between a com-
ment and the title/description of the video. Most
of the videos come with a title and a short descrip-
tion, which can be used to encode the topicality of
2
The list of negation words is adopted from
http://sentiment.christopherpotts.net/lingstruc.html
each comment by looking at their overlap.
3.2 Structural model
We go beyond traditional feature vectors by em-
ploying structural models (STRUCT), which en-
code each comment into a shallow syntactic tree.
These trees are input to tree kernel functions
for generating structural features. Our struc-
tures are specifically adapted to the noisy user-
generated texts and encode important aspects of
the comments, e.g., words from the sentiment lexi-
cons, product concepts and negation words, which
specifically targets the sentiment and comment
type classification tasks.
In particular, our shallow tree structure is a
two-level syntactic hierarchy built from word lem-
mas (leaves) and part-of-speech tags that are fur-
ther grouped into chunks (Fig. 1). As full syn-
tactic parsers such as constituency or dependency
tree parsers would significantly degrade in perfor-
mance on noisy texts, e.g., Twitter or YouTube
comments, we opted for shallow structures, which
rely on simpler and more robust components: a
part-of-speech tagger and a chunker. Moreover,
such taggers have been recently updated with
models (Ritter et al, 2011; Gimpel et al, 2011)
trained specifically to process noisy texts show-
ing significant reductions in the error rate on user-
generated texts, e.g., Twitter. Hence, we use the
CMU Twitter pos-tagger (Gimpel et al, 2011;
Owoputi et al, 2013) to obtain the part-of-speech
tags. Our second component ? chunker ? is taken
from (Ritter et al, 2011), which also comes with a
model trained on Twitter data
3
and shown to per-
form better on noisy data such as user comments.
To address the specifics of OM tasks on
YouTube comments, we enrich syntactic trees
with semantic tags to encode: (i) central con-
cepts of the video, (ii) sentiment-bearing words
expressing positive or negative sentiment and (iii)
negation words. To automatically identify con-
cept words of the video we use context words (to-
kens detected as nouns by the part-of-speech tag-
ger) from the video title and video description and
match them in the tree. For the matched words,
we enrich labels of their parent nodes (part-of-
speech and chunk) with the PRODUCT tag. Sim-
ilarly, the nodes associated with words found in
3
The chunker from (Ritter et al, 2011) relies on its own
POS tagger, however, in our structural representations we fa-
vor the POS tags from the CMU Twitter tagger and take only
the chunk tags from the chunker.
1254
Figure 1: Shallow tree representation of the example comment (labeled with product type and
negative sentiment): ?iPad 2 is better. the superior apps just destroy the xoom.? (lemmas are replaced
with words for readability) taken from the video ?Motorola Xoom Review?. We introduce additional tags
in the tree nodes to encode the central concept of the video (motorola xoom) and sentiment-bearing words
(better, superior, destroy) directly in the tree nodes. For the former we add a PRODUCT tag on the chunk
and part-of-speech nodes of the word xoom) and polarity tags (positive and negative) for the latter. Two
sentences are split into separate root nodes S.
the sentiment lexicon are enriched with a polar-
ity tag (either positive or negative), while nega-
tion words are labeled with the NEG tag. It should
be noted that vector-based (FVEC) model relies
only on feature counts whereas the proposed tree
encodes powerful contextual syntactic features in
terms of tree fragments. The latter are automati-
cally generated and learned by SVMs with expres-
sive tree kernels.
For example, the comment in Figure 1 shows
two positive and one negative word from the senti-
ment lexicon. This would strongly bias the FVEC
sentiment classifier to assign a positive label
to the comment. In contrast, the STRUCT model
relies on the fact that the negative word, destroy,
refers to the PRODUCT (xoom) since they form a
verbal phase (VP). In other words, the tree frag-
ment: [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP [PRODUCT-N
[xoom]]]] is a strong feature (induced
by tree kernels) to help the classifier to dis-
criminate such hard cases. Moreover, tree
kernels generate all possible subtrees, thus
producing generalized (back-off) features,
e.g., [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] or [S
[negative-VP [PRODUCT-NP]]]].
3.3 Learning
We perform OM on YouTube using supervised
methods, e.g., SVM. Our goal is to learn a model
to automatically detect the sentiment and type of
each comment. For this purpose, we build a multi-
class classifier using the one-vs-all scheme. A bi-
nary classifier is trained for each of the classes
and the predicted class is obtained by taking a
class from the classifier with a maximum predic-
tion score. Our back-end binary classifier is SVM-
light-TK
4
, which encodes structural kernels in the
SVM-light (Joachims, 2002) solver. We define a
novel and efficient tree kernel function, namely,
Shallow syntactic Tree Kernel (SHTK), which is
as expressive as the Partial Tree Kernel (PTK)
(Moschitti, 2006a) to handle feature engineering
over the structural representations of the STRUCT
model. A polynomial kernel of degree 3 is applied
to feature vectors (FVEC).
Combining structural and vector models. A
typical kernel machine, e.g., SVM, classifies a
test input x using the following prediction func-
tion: h(x) =
?
i
?
i
y
i
K(x,x
i
), where ?
i
are
the model parameters estimated from the training
data, y
i
are target variables, x
i
are support vec-
tors, and K(?, ?) is a kernel function. The latter
computes the similarity between two comments.
The STRUCT model treats each comment as a tu-
ple x = ?T ,v? composed of a shallow syntactic
tree T and a feature vector v . Hence, for each pair
of comments x
1
and x
2
, we define the following
comment similarity kernel:
K(x
1
,x
2
) = K
TK
(T
1
,T
2
) +K
v
(v
1
, v
2
), (1)
where K
TK
computes SHTK (defined next), and
K
v
is a kernel over feature vectors, e.g., linear,
polynomial, Gaussian, etc.
Shallow syntactic tree kernel. Following the
convolution kernel framework, we define the new
4
http://disi.unitn.it/moschitti/Tree-Kernel.htm
1255
SHTK function from Eq. 1 to compute the similar-
ity between tree structures. It counts the number of
common substructures between two trees T
1
and
T
2
without explicitly considering the whole frag-
ment space. The general equations for Convolu-
tion Tree Kernels is:
TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
), (2)
where N
T
1
and N
T
2
are the sets of the T
1
?s and
T
2
?s nodes, respectively and ?(n
1
, n
2
) is equal to
the number of common fragments rooted in the n
1
and n
2
nodes, according to several possible defini-
tion of the atomic fragments.
To improve the speed computation of TK, we
consider pairs of nodes (n
1
, n
2
) belonging to the
same tree level. Thus, given H , the height of the
STRUCT trees, where each level h contains nodes
of the same type, i.e., chunk, POS, and lexical
nodes, we define SHTK as the following
5
:
SHTK(T
1
, T
2
) =
H
?
h=1
?
n
1
?N
h
T
1
?
n
2
?N
h
T
2
?(n
1
, n
2
), (3)
where N
h
T
1
and N
h
T
2
are sets of nodes at height h.
The above equation can be applied with any ?
function. To have a more general and expressive
kernel, we use ? previously defined for PTK.
More formally: if n
1
and n
2
are leaves then
?(n
1
, n
2
) = ??(n
1
, n
2
); else ?(n
1
, n
2
) =
?
(
?
2
+
?
~
I
1
,
~
I
2
,|
~
I
1
|=|
~
I
2
|
?
d(
~
I
1
)+d(
~
I
2
)
|
~
I
1
|
?
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
)
,
where ?, ? ? [0, 1] are decay factors; the large
sum is adopted from a definition of the sub-
sequence kernel (Shawe-Taylor and Cristianini,
2004) to generate children subsets with gaps,
which are then used in a recursive call to ?. Here,
c
n
1
(i) is the i
th
child of the node n
1
;
~
I
1
and
~
I
2
are
two sequences of indexes that enumerate subsets
of children with gaps, i.e.,
~
I = (i
1
, i
2
, .., |I|), with
1 ? i
1
< i
2
< .. < i
|I|
; and d(
~
I
1
) =
~
I
1l(
~
I
1
)
?
~
I
11
+ 1
and d(
~
I
2
) =
~
I
2l(
~
I
2
)
?
~
I
21
+ 1, which penalizes
subsequences with larger gaps.
It should be noted that: firstly, the use of a
subsequence kernel makes it possible to generate
child subsets of the two nodes, i.e., it allows for
gaps, which makes matching of syntactic patterns
5
To have a similarity score between 0 and 1, a normaliza-
tion in the kernel space, i.e.
SHTK(T
1
,T
2
)
?
SHTK(T
1
,T
1
)?SHTK(T
2
,T
2
)
is
applied.
less rigid. Secondly, the resulting SHTK is essen-
tially a special case of PTK (Moschitti, 2006a),
adapted to the shallow structural representation
STRUCT (see Sec. 3.2). When applied to STRUCT
trees, SHTK exactly computes the same feature
space as PTK, but in faster time (on average). In-
deed, SHTK required to be only applied to node
pairs from the same level (see Eq. 3), where the
node labels can match ? chunk, POS or lexicals.
This reduces the time for selecting the matching-
node pairs carried out in PTK (Moschitti, 2006a;
Moschitti, 2006b). The fragment space is obvi-
ously the same, as the node labels of different
levels in STRUCT are different and will not be
matched by PTK either.
Finally, given its recursive definition in Eq. 3
and the use of subsequence (with gaps), SHTK can
derive useful dependencies between its elements.
For example, it will generate the following subtree
fragments: [positive-NP [positive-A
N]], [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] and so on.
4 YouTube comments corpus
To build a corpus of YouTube comments, we fo-
cus on a particular set of videos (technical reviews
and advertisings) featuring commercial products.
In particular, we chose two product categories:
automobiles (AUTO) and tablets (TABLETS). To
collect the videos, we compiled a list of prod-
ucts and queried the YouTube gData API
6
to re-
trieve the videos. We then manually excluded
irrelevant videos. For each video, we extracted
all available comments (limited to maximum 1k
comments per video) and manually annotated each
comment with its type and polarity. We distin-
guish between the following types:
product: discuss the topic product in general or
some features of the product;
video: discuss the video or some of its details;
spam: provide advertising and malicious links; and
off-topic: comments that have almost no content
(?lmao?) or content that is not related to the video
(?Thank you!?).
Regarding the polarity, we distinguish between
{positive, negative, neutral} sentiments with re-
spect to the product and the video. If the comment
contains several statements of different polarities,
it is annotated as both positive and negative: ?Love
the video but waiting for iPad 4?. In total we have
6
https://developers.google.com/youtube/v3/
1256
annotated 208 videos with around 35k comments
(128 videos TABLETS and 80 for AUTO).
To evaluate the quality of the produced labels,
we asked 5 annotators to label a sample set of one
hundred comments and measured the agreement.
The resulting annotator agreement ? value (Krip-
pendorf, 2004; Artstein and Poesio, 2008) scores
are 60.6 (AUTO), 72.1 (TABLETS) for the senti-
ment task and 64.1 (AUTO), 79.3 (TABLETS) for
the type classification task. For the rest of the
comments, we assigned the entire annotation task
to a single coder. Further details on the corpus can
be found in Uryupina et al (2014).
5 Experiments
This section reports: (i) experiments on individ-
ual subtasks of opinion and type classification; (ii)
the full task of predicting type and sentiment; (iii)
study on the adaptability of our system by learn-
ing on one domain and testing on the other; (iv)
learning curves that provide an indication on the
required amount and type of data and the scalabil-
ity to other domains.
5.1 Task description
Sentiment classification. We treat each com-
ment as expressing positive, negative or
neutral sentiment. Hence, the task is a three-
way classification.
Type classification. One of the challenging as-
pects of sentiment analysis of YouTube data is that
the comments may express the sentiment not only
towards the product shown in the video, but
also the video itself, i.e., users may post posi-
tive comments to the video while being generally
negative about the product and vice versa. Hence,
it is of crucial importance to distinguish between
these two types of comments. Additionally, many
comments are irrelevant for both the product and
the video (off-topic) or may even contain
spam. Given that the main goal of sentiment
analysis is to select sentiment-bearing comments
and identify their polarity, distinguishing between
off-topic and spam categories is not critical.
Thus, we merge the spam and off-topic into
a single uninformative category. Similar to
the opinion classification task, comment type clas-
sification is a multi-class classification with three
classes: video, product and uninform.
Full task. While the previously discussed sen-
timent and type identification tasks are useful to
Task class
AUTO TABLETS
TRAIN TEST TRAIN TEST
Sentiment
positive 2005 (36%) 807 (27%) 2393 (27%) 1872 (27%)
neutral 2649 (48%) 1413 (47%) 4683 (53%) 3617 (52%)
negative 878 (16%) 760 (26%) 1698 (19%) 1471 (21%)
total 5532 2980 8774 6960
Type
product 2733 (33%) 1761 (34%) 7180 (59%) 5731 (61%)
video 3008 (36%) 1369 (26%) 2088 (17%) 1674 (18%)
off-topic 2638 (31%) 2045 (39%) 2334 (19%) 1606 (17%)
spam 26 (>1%) 17 (>1%) 658 (5%) 361 (4%)
total 8405 5192 12260 9372
Full
product-pos. 1096 (13%) 517 (10%) 1648 (14%) 1278 (14%)
product-neu. 908 (11%) 729 (14%) 3681 (31%) 2844 (32%)
product-neg. 554 (7%) 370 (7%) 1404 (12%) 1209 (14%)
video-pos. 909 (11%) 290 (6%) 745 (6%) 594 (7%)
video-neu. 1741 (21%) 683 (14%) 1002 (9%) 773 (9%)
video-neg. 324 (4%) 390 (8%) 294 (2%) 262 (3%)
off-topic 2638 (32%) 2045 (41%) 2334 (20%) 1606 (18%)
spam 26 (>1%) 17 (>1%) 658 (6%) 361 (4%)
total 8196 5041 11766 8927
Table 1: Summary of YouTube comments data
used in the sentiment, type and full classification
tasks. The comments come from two product cate-
gories: AUTO and TABLETS. Numbers in paren-
thesis show proportion w.r.t. to the total number of
comments used in a task.
model and study in their own right, our end goal is:
given a stream of comments, to jointly predict both
the type and the sentiment of each comment. We
cast this problem as a single multi-class classifica-
tion task with seven classes: the Cartesian product
between {product, video} type labels and
{positive, neutral, negative} senti-
ment labels plus the uninformative category
(spam and off-topic). Considering a real-life ap-
plication, it is important not only to detect the po-
larity of the comment, but to also identify if it is
expressed towards the product or the video.
7
5.2 Data
We split all the videos 50% between training
set (TRAIN) and test set (TEST), where each
video contains all its comments. This ensures
that all comments from the same video appear
either in TRAIN or in TEST. Since the number
of comments per video varies, the resulting sizes
of each set are different (we use the larger split
for TRAIN). Table 1 shows the data distribution
across the task-specific classes ? sentiment and
type classification. For the sentiment task we ex-
clude off-topic and spam comments as well
as comments with ambiguous sentiment, i.e., an-
7
We exclude comments annotated as both video and
product. This enables the use of a simple flat multi-
classifiers with seven categories for the full task, instead of
a hierarchical multi-label classifiers (i.e., type classification
first and then opinion polarity). The number of comments as-
signed to both product and video is relatively small (8%
for TABLETS and 4% for AUTO).
1257
notated as both positive and negative.
For the sentiment task about 50% of the
comments have neutral polarity, while the
negative class is much less frequent. Inter-
estingly, the ratios between polarities expressed
in comments from AUTO and TABLETS are very
similar across both TRAIN and TEST. Conversely,
for the type task, we observe that comments from
AUTO are uniformly distributed among the three
classes, while for the TABLETS the majority of
comments are product related. It is likely due
to the nature of the TABLETS videos, that are
more geek-oriented, where users are more prone
to share their opinions and enter involved discus-
sions about a product. Additionally, videos from
the AUTO category (both commercials and user
reviews) are more visually captivating and, be-
ing generally oriented towards a larger audience,
generate more video-related comments. Regard-
ing the full setting, where the goal is to have
a joint prediction of the comment sentiment and
type, we observe that video-negative and
video-positive are the most scarce classes,
which makes them the most difficult to predict.
5.3 Results
We start off by presenting the results for the tradi-
tional in-domain setting, where both TRAIN and
TEST come from the same domain, e.g., AUTO or
TABLETS. Next, we show the learning curves to
analyze the behavior of FVEC and STRUCT mod-
els according to the training size. Finally, we per-
form a set of cross-domain experiments that de-
scribe the enhanced adaptability of the patterns
generated by the STRUCT model.
5.3.1 In-domain experiments
We compare FVEC and STRUCT models on three
tasks described in Sec. 5.1: sentiment, type and
full. Table 2 reports the per-class performance
and the overall accuracy of the multi-class clas-
sifier. Firstly, we note that the performance on
TABLETS is much higher than on AUTO across
all tasks. This can be explained by the follow-
ing: (i) TABLETS contains more training data and
(ii) videos from AUTO and TABLETS categories
draw different types of audiences ? well-informed
users and geeks expressing better-motivated opin-
ions about a product for the former vs. more gen-
eral audience for the latter. This results in the
different quality of comments with the AUTO be-
ing more challenging to analyze. Secondly, we
observe that the STRUCT model provides 1-3%
of absolute improvement in accuracy over FVEC
for every task. For individual categories the F1
scores are also improved by the STRUCT model
(except for the negative classes for AUTO, where
we see a small drop). We conjecture that sentiment
prediction for AUTO category is largely driven
by one-shot phrases and statements where it is
hard to improve upon the bag-of-words and senti-
ment lexicon features. In contrast, comments from
TABLETS category tend to be more elaborated
and well-argumented, thus, benefiting from the ex-
pressiveness of the structural representations.
Considering per-class performance, correctly
predicting negative sentiment is most difficult
for both AUTO and TABLETS, which is proba-
bly caused by the smaller proportion of the neg-
ative comments in the training set. For the type
task, video-related class is substantially more dif-
ficult than product-related for both categories. For
the full task, the class video-negative ac-
counts for the largest error. This is confirmed by
the results from the previous sentiment and type
tasks, where we saw that handling negative sen-
timent and detecting video-related comments are
most difficult.
5.3.2 Learning curves
The learning curves depict the behavior of FVEC
and STRUCT models as we increase the size of
the training set. Intuitively, the STRUCT model
relies on more general syntactic patterns and may
overcome the sparseness problems incurred by the
FVEC model when little training data is available.
Nevertheless, as we see in Figure 2, the learning
curves for sentiment and type classification tasks
across both product categories do not confirm this
intuition. The STRUCT model consistently outper-
forms the FVEC across all training sizes, but the
gap in the performance does not increase when we
move to smaller training sets. As we will see next,
this picture changes when we perform the cross-
domain study.
5.3.3 Cross-domain experiments
To understand the performance of our classifiers
on other YouTube domains, we perform a set of
cross-domain experiments by training on the data
from one product category and testing on the other.
Table 3 reports the accuracy for three tasks
when we use all comments (TRAIN + TEST) from
AUTO to predict on the TEST from TABLETS
1258
Task class
AUTO TABLETS
FVEC STRUCT FVEC STRUCT
P R F1 P R F1 P R F1 P R F1
Sent
positive 49.1 72.1 58.4 50.1 73.9 59.0 67.5 70.3 69.9 71.2 71.3 71.3
neutral 68.2 55.0 61.4 70.1 57.6 63.1 81.3 71.4 76.9 81.1 73.1 77.8
negative 42.0 36.9 39.6 41.3 35.8 38.8 48.3 60.0 54.8 50.2 62.6 56.5
Acc 54.7 55.7 68.6 70.5
Type
product 66.8 73.3 69.4 68.8 75.5 71.7 78.2 95.3 86.4 80.1 95.5 87.6
video 45.0 52.8 48.2 47.8 49.9 48.7 83.6 45.7 58.9 83.5 46.7 59.4
uninform 59.3 48.2 53.1 60.6 53.0 56.4 70.2 52.5 60.7 72.9 58.6 65.0
Acc 57.4 59.4 77.2 78.6
Full
product-pos 34.0 49.6 39.2 36.5 51.2 43.0 48.4 56.8 52.0 52.4 59.3 56.4
product-neu 43.4 31.1 36.1 41.4 36.1 38.4 68.0 67.5 68.1 59.7 83.4 70.0
product-neg 26.3 29.5 28.8 26.3 25.3 25.6 43.0 49.9 45.4 44.7 53.7 48.4
video-pos 23.2 47.1 31.9 26.1 54.5 35.5 69.1 60.0 64.7 64.9 68.8 66.4
video-neu 26.1 30.0 29.0 26.5 31.6 28.8 56.4 32.1 40.0 55.1 35.7 43.3
video-neg 21.9 3.7 6.0 17.7 2.3 4.8 39.0 17.5 23.9 39.5 6.1 11.5
uninform 56.5 52.4 54.9 60.0 53.3 56.3 60.0 65.5 62.2 63.3 68.4 66.9
Acc 40.0 41.5 57.6 60.3
Table 2: In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT. The
results are reported for sentiment, type and full classification tasks. The metrics used are precision (P),
recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc).
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVECAccuracy
55
60
65
70
training size1k 2k 3k 4k 5k ALL
(a) Sentiment classification
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVEC
Accurac
y
40
45
50
55
60
65
70
75
80
training size1k 2k 3k 4k 5k ALL
(b) Type classification
Figure 2: In-domain learning curves. ALL refers
to the entire TRAIN set for a given product cate-
gory, i.e., AUTO and TABLETS (see Table 1)
and in the opposite direction (TABLETS?AUTO).
When using AUTO as a source domain, STRUCT
model provides additional 1-3% of absolute im-
Source Target Task FVEC STRUCT
AUTO TABLETS
Sent 66.1 66.6
Type 59.9 64.1
?
Full 35.6 38.3
?
TABLETS AUTO
Sent 60.4 61.9
?
Type 54.2 55.6
?
Full 43.4 44.7
?
Table 3: Cross-domain experiment. Ac-
curacy using FVEC and STRUCT models
when trained/tested in both directions, i.e.
AUTO?TABLETS and TABLETS?AUTO.
?
de-
notes results statistically significant at 95% level
(via pairwise t-test).
provement, except for the sentiment task.
Similar to the in-domain experiments, we stud-
ied the effect of the source domain size on the tar-
get test performance. This is useful to assess the
adaptability of features exploited by the FVEC and
STRUCT models with the change in the number
of labeled examples available for training. Addi-
tionally, we considered a setting including a small
amount of training data from the target data (i.e.,
supervised domain adaptation).
For this purpose, we drew the learning curves of
the FVEC and STRUCT models applied to the sen-
timent and type tasks (Figure 3): AUTO is used
as the source domain to train models, which are
tested on TABLETS.
8
The plot shows that when
8
The results for the other direction (TABLETS?AUTO)
show similar behavior.
1259
STRUCTFVEC
Source +Target
Accurac
y
62
63
64
65
66
67
68
training size1k 2k 3k 4k 5k 8.5k(ALL) 100 500 1k
(a) Sentiment classification
STRUCTFVEC
Source +Target
Accurac
y
30
35
40
45
50
55
60
65
70
training size1k 2k 3k 4k 5k 8.5k(TRAIN) 13k(ALL) 100 500 1k
(b) Type classification
Figure 3: Learning curves for the cross-domain
setting (AUTO?TABLETS). Shaded area refers to
adding a small portion of comments from the same
domain as the target test data to the training.
little training data is available, the features gener-
ated by the STRUCT model exhibit better adapt-
ability (up to 10% of improvement over FVEC).
The bag-of-words model seems to be affected by
the data sparsity problem which becomes a crucial
issue when only a small training set is available.
This difference becomes smaller as we add data
from the same domain. This is an important ad-
vantage of our structural approach, since we can-
not realistically expect to obtain manual annota-
tions for 10k+ comments for each (of many thou-
sands) product domains present on YouTube.
5.4 Discussion
Our STRUCT model is more accurate since it is
able to induce structural patterns of sentiment.
Consider the following comment: optimus pad
is better. this xoom is just to bulky but optimus
pad offers better functionality. The FVEC bag-
of-words model misclassifies it to be positive,
since it contains two positive expressions (better,
better functionality) that outweigh a single nega-
tive expression (bulky). The structural model, in
contrast, is able to identify the product of interest
(xoom) and associate it with the negative expres-
sion through a structural feature and thus correctly
classify the comment as negative.
Some issues remain problematic even for the
structural model. The largest group of errors are
implicit sentiments. Thus, some comments do not
contain any explicit positive or negative opinions,
but provide detailed and well-argumented criti-
cism, for example, this phone is heavy. Such com-
ments might also include irony. To account for
these cases, a deep understanding of the product
domain is necessary.
6 Conclusions and Future Work
We carried out a systematic study on OM from
YouTube comments by training a set of su-
pervised multi-class classifiers distinguishing be-
tween video and product related opinions. We
use standard feature vectors augmented by shallow
syntactic trees enriched with additional conceptual
information.
This paper makes several contributions: (i) it
shows that effective OM can be carried out with
supervised models trained on high quality annota-
tions; (ii) it introduces a novel annotated corpus
of YouTube comments, which we make available
for the research community; (iii) it defines novel
structural models and kernels, which can improve
on feature vectors, e.g., up to 30% of relative im-
provement in type classification, when little data
is available, and demonstrates that the structural
model scales well to other domains.
In the future, we plan to work on a joint model
to classify all the comments of a given video, s.t. it
is possible to exploit latent dependencies between
entities and the sentiments of the comment thread.
Additionally, we plan to experiment with hierar-
chical multi-label classifiers for the full task (in
place of a flat multi-class learner).
Acknowledgments
The authors are supported by a Google Fac-
ulty Award 2011, the Google Europe Fellowship
Award 2013 and the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der the grant #288024: LIMOSINE.
1260
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources? In
IJCNLP.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Hal Daum?e, III. 2007. Frustratingly easy domain
adaptation. ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Alessandro Moschitti. 2006a. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML.
Alessandro Moschitti. 2006b. Making tree kernels
practical for natural language learning. In EACL,
pages 113?120.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In EMNLP.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual simi-
larity with structural representations. In ACL.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,
and Jose San Pedro. 2010. How useful are your
comments?: Analyzing and predicting YouTube
comments and comment ratings. In WWW.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In EMNLP.
Anders S?gaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Oscar T?ackstr?om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In ACL.
Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
SenTube: A corpus for sentiment analysis on
YouTube social media. In LREC.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
1261
Dependency Tree Based Sentence Compression
Katja Filippova and Michael Strube
EML Research gGmbH
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
http://www.eml-research.de/nlp
Abstract
We present a novel unsupervised method for
sentence compression which relies on a de-
pendency tree representation and shortens sen-
tences by removing subtrees. An automatic
evaluation shows that our method obtains re-
sult comparable or superior to the state of the
art. We demonstrate that the choice of the
parser affects the performance of the system.
We also apply the method to German and re-
port the results of an evaluation with humans.
1 Introduction
Within the field of text-to-text generation, the sen-
tence compression task can be defined as follows:
given a sentence S, consisting of words w1w2...wn,
what is a subset of the words of S, such that it
is grammatical and preserves essential information
from S? There are many applications which would
benefit from a robust compression system, such as
subtitle generation, compression for mobile devices
with a limited screen size, or news digests. Given
that to date most text and speech summarization
systems are extractive, sentence compression tech-
niques are a common way to deal with redundancy
in their output.
In recent years, a number of approaches to sen-
tence compression have been developed (Jing, 2001;
Knight & Marcu, 2002; Gagnon & Da Sylva, 2005;
Turner & Charniak, 2005; Clarke & Lapata, 2008,
inter alia). Many explicitly rely on a language
model, usually a trigram model, to produce gram-
matical output (Knight & Marcu, 2002; Hori & Fu-
rui, 2004; Turner & Charniak, 2005; Galley & McK-
eown, 2007). Testing the grammaticality of the out-
put with a language model is justified when work-
ing with a language with rigid word order like En-
glish, and all but one approach mentioned have
been applied to English data. However, compress-
ing sentences in languages with less rigid word or-
der needs a deeper analysis to test grammaticality.
And even for languages with rigid word order the
trigram model ignores the structure of the sentence
and therefore may significantly distort the meaning
of the source sentence. Approaches going beyond
the word level either require a comprehensive lexi-
con (Jing, 2001), or manually devised rules (Gagnon
& Da Sylva, 2005; Clarke & Lapata, 2008) to de-
termine prunable constituents. A lexicon is not al-
ways available, whereas the hand-crafted rules may
not cover all cases and are too general to be univer-
sally applicable (e.g. PPs can be pruned).
In this paper we present a novel unsupervised ap-
proach to sentence compression which is motivated
by the belief that the grammaticality of the output
can be better ensured by compressing trees. In par-
ticular, given a dependency tree, we want to prune
subtrees which are neither obligatory syntactic argu-
ments, nor contribute important information to the
content of the sentence. A tree pruning approach
does not generate new dependencies and is unlikely
to produce a compression with a totally different
meaning. Our approach is unsupervised and adapt-
able to other languages, the main requirement be-
ing that there are a dependency parser and a corpus
available for the languages. We test our approach
on English and German data sets and obtain results
comparable or superior to the state of the art.
25
2 Related Work
Many existing compression systems use a noisy-
channel approach and rely on a language model
to test the grammaticality of the output (Knight &
Marcu, 2002; Turner & Charniak, 2005; Galley &
McKeown, 2007). Other ways to ensure gram-
maticality and to decide whether a constituent is
obligatory or may be pruned are to utilize a sub-
categorization lexicon (Jing, 2001), or to define a
set of generally prunable constituents. Gagnon &
Da Sylva (2005) prune dependency trees by remov-
ing prepositional complements of the verb, subordi-
nate clauses and noun appositions. Apparently, this
does not guarantee grammaticality in all cases. It
may also eliminate important information from the
tree.
Most approaches are supervised and require train-
ing data to learn which words or constituents can
be dropped from a sentence (Riezler et al, 2003;
McDonald, 2006). However, it is difficult to obtain
training data. Still, there are few unsupervised meth-
ods. For example, Hori & Furui (2004) introduce
a scoring function which relies on such informa-
tion sources as word significance score and language
model. A compression of a given length which
maximizes the scoring function is then found with
dynamic programming. Clarke & Lapata (2008)
present another unsupervised approach. They for-
mulate the task as an optimization problem and solve
it with integer linear programming. Two scores con-
tribute to their objective function ? a trigram lan-
guage model score and a word significance score.
Additionally, the grammaticality of the output is en-
sured by a handful of linguistic constraints, stating
e.g. which arguments should be preserved.
In this paper we suggest an alternative to the pop-
ular language model basis for compression systems
? a method which compresses dependency trees and
not strings of words. We will argue that our formu-
lation has the following advantages: firstly, the ap-
proach is unsupervised, the only requirement being
that there is a sufficiently large corpus and a depen-
dency parser available. Secondly, it requires neither
a subcategorization lexicon nor hand-crafted rules to
decide which arguments are obligatory. Thirdly, it
finds a globally optimal compression by taking syn-
tax and word importance into account.
3 Dependency Based Compression
Our method compresses sentences in that it removes
dependency edges from the dependency tree of a
sentence. The aim is to preserve dependencies
which are either required for the output to be gram-
matical or have an important word as the dependent.
The algorithm proceeds in three steps: tree transfor-
mation (Section 3.1), tree compression (Section 3.2)
and tree linearization (Section 3.3).
3.1 Tree Transformation
Before a dependency tree is compressed, i.e. be-
fore some of the dependencies are removed, the tree
is modified. We will demonstrate the effect of the
transformations with the sentence below:
(1) He said that he lived in Paris and Berlin
The first transformation (ROOT) inserts an explicit
rootnode (Fig. 1(a)). The result of the second trans-
formation (VERB) is that every inflected verb in the
tree gets an edge originating from the rootnode (Fig.
1(b)). All edges outgoing from the rootnode bear the
s label. Apart from that we remove auxiliary edges
and memorize such grammatical properties as voice,
tense or negation for verbs.
The purpose of the remaining transformations is
to make relations between open-class words more
explicit. We want to decide on pruning an edge
judging from two considerations: (i) how important
for the head this argument is; (ii) how informative
the dependent word is. As an example, consider a
source sentence given in (2). Here, we want to de-
cide whether one prepositional phrase (or both) can
be pruned without making the resulting sentence un-
grammatical.
(2) After some time, he moved to London.
It would not be very helpful to check whether an ar-
gument attached with the label pp is obligatory for
the verb move. Looking at a particular preposition
(after vs. to) would be more enlightening. This
motivates the PREP transformation which removes
prepositional nodes and places them as labels on the
edge from their head to the respective noun (Fig.
1(c)). We also decompose a chain of conjoined ele-
ments (CONJ) and attach each of them to the head of
the first element in the chain with the label the first
26
Paris
Berlinand
he
live
say
he
in
root
s
(a) The source tree after ROOT.
Paris
Berlinand
he
live
say
he
in
root
s s
(b) After VERB
Paris
Berlinand
he
live
say
he
root
s
s
in
(c) After PREP
live
say
he
root
Paris Berlinhe
s
s
in
in
(d) After CONJ
Figure 1: The dependency structure of He said that he lived in Paris and Berlin after the transformations
element attaches to its head with (Fig. 1(d)). This
way we can retain any of the conjoined elements
in the compression and do not have to preserve the
whole sequence of them if we are interested in only
one. This last transformation is not applied to verbs.
3.2 Tree Compression
We formulate the compression task as an optimiza-
tion problem which we solve using integer linear
programming1. Given a transformed dependency
tree (a graph if new edges have been added), we de-
cide which dependency edges to remove. For each
directed dependency edge from head h to word w we
thus introduce a binary variable xlh,w where l stands
for the edge?s label:
xlh,w =
{
1 if the dependency is preserved
0 otherwise
(1)
The goal is to find a subtree which gets the highest
score of the objective function (2) to which both the
1In our implementation we use lp solve (http://
sourceforge.net/projects/lpsolve).
probability of dependencies (P (l|h)) and the impor-
tance of dependent words (I(w)) contribute:
f(X) =
?
x
xlh,w ? P (l|h) ? I(w) (2)
Intuitively, the conditional probabilities prevent us
from removing obligatory dependencies from the
tree. For example, P (subj|work) is higher than
P (with|work), and therefore the subject will be
preserved whereas the prepositional label and thus
the whole PP can be pruned. This way we do
not have to create an additional constraint for every
obligatory argument (e.g. subject or direct object).
Neither do we require a subcategorization lexicon to
look up which arguments are obligatory for a cer-
tain verb. Verb arguments are preserved because the
dependency edges, with which they are attached to
the head, get high scores. Table 1 presents the prob-
abilities of a number of labels given that the head
is study. Table 2 presents the probabilities for their
German counterparts.
Note that if we would not apply the PREP trans-
formation we would not be able to distinguish be-
27
subj dobj in at after with to
0.16 0.13 0.05 0.04 0.01 0.01 0.01
Table 1: Probabilities of subj, d(irect)obj, in, at, after,
with, to given the verb study
subj obja in an nach mit zu
0.88 0.74 0.44 0.42 0.09 0.02 0.01
Table 2: Probabilities of subj, obja(ccusative), in, at, af-
ter, with, to given the verb studieren
tween different prepositions and could only calcu-
late P (pp|studieren) which would not be very in-
formative. The probabilities for English are lower
than those for German because we calculate the con-
ditional probabilities given word lemma. In English,
the part of speech information cannot be induced
from the lemma and thus the set of possible labels
of a node is on average larger than in German.
There are many ways in which word importance,
I(w) can be defined. Here, we use the formula intro-
duced by Clarke & Lapata (2008) which is a modifi-
cation of the significance score of Hori et al (2003):
I(wi) =
l
N ? fi log
FA
Fi
(3)
wi is the topic word (either noun or verb), fi is the
frequency of wi in the document, Fi is the frequency
of wi in the corpus, and FA is the sum of frequencies
of all topic words in the corpus. l is the number of
clause nodes above w and N is the maximum level
of embedding of the sentence w belongs to.
The objective function is subject to constraints of
two kinds. The constraints of the first kind are stuc-
tural and ensure that the preserved dependencies re-
sult in a tree. (4) ensures that each word has one
head at most. (5) ensures connectivity in the tree.
(6) restricts the size of the resulting tree to ? words.
?w ? W,
?
h,l
xlh,w ? 1 (4)
?w ? W,
?
h,l
xlh,w ?
1
|W |
?
u,l
xlw,u ? 0 (5)
?
x
xlh,w ? ? (6)
? is a function of the length of the source sentence
in open-class words. The function is not linear since
the degree of compression increases with the length
of the sentence. The compression rate of human-
generated sentences is about 70% (Clarke & Lapata,
2008)2. To approximate this value, we set the pro-
portion of deleted words to be 20% for short sen-
tences (5-9 non-stop words), this value increases up
to 50% for long sentences (30+ words).
The constraints of the second type ensure the syn-
tactic validity of the output tree and explicitly state
which edges should be preserved. These constraints
can be general as well as conditional. The former
ensure that an edge is preserved if its source node
is retained in the output. Conditional syntactic con-
straints state that an edge has to be preserved if (and
only if) a certain other edge is preserved. We have
only one syntactic constraint which states that a sub-
ordinate conjunction (sc) should be preserved if and
only if the clause it belongs to functions as a sub-
ordinate clause (sub) in the output. If it is taken as
the main clause, the conjunction should be dropped.
In terms of edges, this can be formulated as follows
(7):
?xscw,u, xsubh,w ? xscw,u = 0 (7)
Due to the constraint (4), the compressed subtree
is always rooted in the node added as a result of the
first transformation. A compression of a sentence to
an embedded clause is not possible unless one pre-
serves the structure above the embedded clause. Of-
ten, however, main clauses are less important than an
embedded clause. For example, given the sentence
He said they have to be held in Beirut it is the em-
bedded clause which is informative and to which the
source sentence should be compressed. The purpose
of the VERB modification is to amend exactly this
problem. Having an edge from the rootnode to ev-
ery inflected verb allows us to compress the source
sentence to any clause.
3.3 Tree Linearization
A very simple but reasonable linearization technique
is to present the words of a compressed sentence in
the order they are found in the source sentence. This
method has been applied before and this is how we
2Higher rates correspond to longer compressions.
28
linearize the trees obtained for the English data. Un-
fortunately, this method cannot be directly applied to
German because of the constraints on word order in
this language. One of the rules of German grammar
states that in the main clause the inflected part of the
verb occupies the second position, the first position
being occupied by exactly one constituent. There-
fore, if the sentence initial position in a source sen-
tence is occupied by a constituent which got pruned
off as a result of compression, the verb becomes
the first element of the sentence which results in an
undesirable output. There are linearization meth-
ods developed for German which find an optimal
word order for a sentence (Ringger et al, 2004;
Filippova & Strube, 2007). We use our recent
method to linearize compressed trees.
4 Corpora and Annotation
We apply our method to sentences from two corpora
in English and German. These are presented below.
English Compression Corpus: The English data
we use is a document-based compression cor-
pus from the British National Corpus and
American News Text Corpus which consists of
82 news stories3. We parsed the corpus with
RASP (Briscoe et al, 2006) and with the Stan-
ford PCFG parser (Klein & Manning, 2003).
The output of the former is a set of dependency
relations whereas the latter provides an option
for converting the output into dependency for-
mat (de Marneffe et al, 2006) which we use.
Tu?Ba-D/Z: The German corpus we use is a col-
lection of 1,000 newspaper articles (Telljohann
et al, 2003)4. Sentence boundaries, morphol-
ogy, dependency structure and anaphoric rela-
tions are manually annotated in this corpus.
RASP has been used by Clarke & Lapata (2008)
whose state of the art results we compare with ours.
We use not only RASP but also the Stanford parser
for several reasons. Apart from being accurate, the
latter has an elaborated set of dependency relations
3The corpus is available from http://homepages.
inf.ed.ac.uk/s0460084/data.
4The corpus is available from http://www.sfs.
uni-tuebingen.de/en_tuebadz.shtml.
(48 vs. 15 of RASP) which is not overly large (com-
pared with the 106 grammatical relations of the Link
Parser). This is important for our system which
relies on syntactic information when making prun-
ing decisions. A comparison between the Stanford
parser and two dependency parsers, MiniPar and
Link Parser, showed a decent performance of the for-
mer (de Marneffe et al, 2006). It is also of interest to
see to what extent the choice of the parser influences
the results.
Apart from the corpora listed above, we use the
Tipster corpus to calculate conditional probabilities
of syntactic labels given head lemmas as well as
word significance scores. The significance score
is calculated from the total number of 128 mil-
lion nouns and verbs. Conditional probabilities are
calculated from a much smaller portion of Tipster
(about 6 million tokens). The latter number is com-
parable to the size of the data set we use to com-
pute the probabilities for German. There, we use
a corpus of about 4,000 articles from the German
Wikipedia to calculate conditional probabilities and
significance scores. The corpus is parsed with the
highly accurate CDG parser (Foth & Menzel, 2006)
and has the same dependency format as Tu?Ba-D/Z
(Versley, 2005).
Although all corpora are annotated with depen-
dency relations, there are considerable differences
between the annotation of the English and German
data sets. The phrase to dependency structure con-
version done by the Stanford parser makes the se-
mantic head of the constituent its syntactic head. For
example, in the sentence He is right it is the adjec-
tive right which is the root of the tree. Unlike that,
sentences from the German corpora always have a
verb as the root. To unify the formats, we write a set
of rules to make the verb the root of the tree in all
cases.
5 Evaluation
We evaluate the results automatically as well as with
human subjects. To assess the performance of the
method on the English data, we calculate the F-
measure on grammatical relations. Following Rie-
zler et al (2003), we calculate average precision and
recall as the amount of grammatical relations shared
between the output of our system and the gold stan-
29
dard variant divided over the total number of rela-
tions in the output and in the human-generated com-
pression respectively. According to Clarke & Lapata
(2006), this measure reliably correlates with human
judgements. The results of our evaluation as well as
the state of the art results reported by Clarke & Lap-
ata (2008) (LM+SIG+CONSTR), whose system uses
language model scoring (LM), word significance
score (SIG), and linguistic constraints (CONSTR),
are presented in Table 3. The F-measure reported
by Clarke & Lapata (2008) is calculated with RASP
which their system builds upon. For our system we
present the results obtained on the data parsed with
RASP as well as with the Stanford parser (SP). In
both cases the F-measure is found with RASP in or-
der to allow for a fair comparison between the three
systems. We recalculate the compression rate for the
gold standard ignoring punctuation. On the whole
corpus the compression rate turns out to be slightly
higher than that reported by Clarke & Lapata (2008)
(70.3%).
F-measure compr.rate
LM+SIG+CONSTR 40.5 72.0%
DEP-BASED (RASP) 40.7 49.6%
DEP-BASED (SP) 49.3 69.3%
GOLD - 72.1%
Table 3: Average results on the English corpus
As there are no human-generated compressions
for German data, we evaluate the performance of the
method in terms of grammaticality and importance
by means of an experiment with native speakers. In
the experiment, humans are presented with a source
sentence and its compression which they are asked
to evaluate on two five-point scales. Higher grades
are given to better sentences. Importance represents
the amount of relevant information from the source
sentence retained in the compression. Since our
method does not generate punctuation, the judges
are asked to ignore errors due to missing commas.
Five participants took part in the experiment and
each rated the total of 25 sentences originating from
a randomly chosen newspaper article. Their ratings
as well as the ratings reported by Clarke & Lapata
(2008) on English corpus are presented in Table 4.
grammar importance
LM+SIG+CONSTR 3.76 3.53
DEP-BASED (DE) 3.62 3.21
Table 4: Average results for the German data
6 Discussion
The results on the English data are comparable with
or superior to the state of the art. These were ob-
tained with a single linguistic constraint (7) and
without any elaborated resources which makes our
system adaptable to other languages. This suggests
that tree compression is a better basis for sentence
compression systems than language model-oriented
word deletion.
In order to explain why the choice of parser sig-
nificantly influences the performance of the method,
we calculate the precision P defined as the number
of dependencies shared by a human-generated com-
pression (depc) and the source sentence (deps) di-
vided over the total number of dependencies found
in the compression:
P = |depc ? deps||depc|
(8)
The intuition is that if a parser does not reach high
precision on gold standard sentences, i.e. if it does
not assign similar dependency structures to a source
sentence and its compression, then it is hopeless
to expect it to produce good compression with our
dependency-based method. However, the precision
does not have to be as high as 100% because of,
e.g., changes within a chain of conjoined elements
or appositions. The precision of the two parsers cal-
culated over the compression corpus is presented in
Table 5.
RASP Stanford parser
precision 79.6% 84.3%
Table 5: Precision of the parsers
The precision of the Stanford parser is about 5%
higher than that of RASP. In our opinion, this partly
explains why the use of the Stanford parser increases
the F-measure by 9 points. Another possible reason
for this improvement is that the Stanford parser iden-
tifies three times more dependency relations than
30
RASP and thus allows for finer distinctions between
the arguments of different types.
Another point concerns the compression rates.
The compressions generated with RASP are consid-
erably shorter than those generated with the Stanford
parser. This is mainly due to the fact that the struc-
ture output by RASP is not necessarily a tree or a
connected graph. In such cases only the first subtree
of the sentence is taken as input and compressed.
The results on the German set are not conclu-
sive since the number of human judges is relatively
small. Still, these preliminary results are compara-
ble to those reported for English and thus give us
some evidence that the method can be adapted to
languages other than English. Interestingly, the im-
portance score depends on the grammaticality of the
sentence. A grammatical sentence can convey unim-
portant information but it was never the case that an
ungrammatical sentence got a high rating on the im-
portance scale. Some of the human judges told us
that they had difficulties assigning the importance
score to ungrammatical sentences.
7 Conclusions
We presented a new compression method which
compresses dependency trees and does not rely on a
language model to test grammaticality. The method
is unsupervised and can be easily adapted to lan-
guages other than English. It does not require a
subcategorization lexicon or elaborated hand-crafted
rules to decide which arguments can be pruned and
finds a globally optimal compression taking syn-
tax and word importance into account. We demon-
strated that the performance of the system depends
on the parser and suggested a way to estimate how
well a parser is suited for the compression task. The
results indicate that the dependency-based approach
is an alternative to the language model-based one
which is worth pursuing.
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would like to thank Yan-
nick Versley who helped us convert Tu?Ba-D/Z in the
CDG format and Elke Teich and the three anony-
mous reviewers for their useful comments.
References
Briscoe, Edward, John Carroll & Rebecca Watson
(2006). The second release of the RASP sys-
tem. In Proceedings of the COLING-ACL In-
teractive Presentation Session, Sydney, Australia,
2006, pp. 77?80.
Clarke, James & Mirella Lapata (2006). Models for
sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, 17?21
July 2006, pp. 377?385.
Clarke, James & Mirella Lapata (2008). Global in-
ference for sentence compression: An integer lin-
ear programming approach. Journal of Artificial
Intelligence Research, 31:399?429.
de Marneffe, Marie-Catherine, Bill MacCartney &
Christopher D. Manning (2006). Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalua-
tion, Genoa, Italy, 22?28 May 2006, pp. 449?454.
Filippova, Katja & Michael Strube (2007). Generat-
ing constituent order in German clauses. In Pro-
ceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, Prague,
Czech Republic, 23?30 June 2007, pp. 320?327.
Foth, Kilian & Wolfgang Menzel (2006). Hybrid
parsing: Using probabilistic models as predictors
for a symbolic parser. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, Sydney, Aus-
tralia, 17?21 July 2006, pp. 321?327.
Gagnon, Michel & Lyne Da Sylva (2005). Text
summarization by sentence extraction and syn-
tactic pruning. In Proceedings of Computational
Linguistics in the North East, Gatineau, Que?bec,
Canada, 26 August 2005.
Galley, Michel & Kathleen R. McKeown (2007).
Lexicalized Markov grammars for sentence com-
31
pression. In Proceedings of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, Rochester, N.Y., 22?27 April
2007, pp. 180?187.
Hori, Chiori & Sadaoki Furui (2004). Speech sum-
marization: An approach through word extraction
and a method for evaluation. IEEE Transactions
on Information and Systems, E87-D(1):15?25.
Hori, Chiori, Sadaoki Furui, Rob Malkin, Hua Yu
& Alex Waibel (2003). A statistical approach to
automatic speech summarization. EURASIP Jour-
nal on Applied Signal Processing, 2:128?139.
Jing, Hongyan (2001). Cut-and-Paste Text Summa-
rization, (Ph.D. thesis). Computer Science De-
partment, Columbia University, New York, N.Y.
Klein, Dan & Christopher D. Manning (2003). Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting of the Association for
Computational Linguistics, Sapporo, Japan, 7?12
July 2003, pp. 423?430.
Knight, Kevin & Daniel Marcu (2002). Summariza-
tion beyond sentence extraction: A probabilistic
approach to sentence compression. Artificial In-
telligence, 139(1):91?107.
McDonald, Ryan (2006). Discriminative sentence
compression with soft syntactic evidence. In
Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics, Trento, Italy, 3?7 April 2006,
pp. 297?304.
Riezler, Stefan, Tracy H. King, Richard Crouch &
Annie Zaenen (2003). Statistical sentence con-
densation using ambiguity packing and stochastic
disambiguation methods for Lexical-Functional
Grammar. In Proceedings of the Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics, Edmonton, Alberta, Canada,
27 May ?1 June 2003, pp. 118?125.
Ringger, Eric, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets & Simon Corston-
Oliver (2004). Linguistically informed statisti-
cal models of constituent structure for ordering
in sentence realization. In Proceedings of the
20th International Conference on Computational
Linguistics, Geneva, Switzerland, 23?27 August
2004, pp. 673?679.
Telljohann, Heike, Erhard W. Hinrichs & Sandra
Ku?bler (2003). Stylebook for the Tu?bingen tree-
bank of written German (Tu?Ba-D/Z). Technical
Report: Seminar fu?r Sprachwissenschaft, Univer-
sita?t Tu?bingen, Tu?bingen, Germany.
Turner, Jenine & Eugene Charniak (2005). Su-
pervised and unsupervised learning for sentence
compression. In Proceedings of the 43rd An-
nual Meeting of the Association for Computa-
tional Linguistics, Ann Arbor, Mich., 25?30 June
2005, pp. 290?297.
Versley, Yannick (2005). Parser evaluation across
text types. In Proceedings of the 4th Workshop
on Treebanks and Linguistic Theories, Barcelona,
Spain, 9-10 December 2005.
32
