Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96?100,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Disambiguation of Preposition Sense Using Linguistically Motivated
Features
Stephen Tratz and Dirk Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{stratz,dirkh}@isi.edu
Abstract
In this paper, we present a supervised classifi-
cation approach for disambiguation of prepo-
sition senses. We use the SemEval 2007
Preposition Sense Disambiguation datasets to
evaluate our system and compare its results to
those of the systems participating in the work-
shop. We derived linguistically motivated fea-
tures from both sides of the preposition. In-
stead of restricting these to a fixed window
size, we utilized the phrase structure. Testing
with five different classifiers, we can report an
increased accuracy that outperforms the best
system in the SemEval task.
1 Introduction
Classifying instances of polysemous words into
their proper sense classes (aka sense disambigua-
tion) is potentially useful to any NLP application
that needs to extract information from text or build
a semantic representation of the textual information.
However, to date, disambiguation between preposi-
tion senses has not been an object of great study. In-
stead, most word sense disambiguation work has fo-
cused upon classifying noun and verb instances into
their appropriate WordNet (Fellbaum, 1998) senses.
Prepositions have mostly been studied in the con-
text of verb complements (Litkowski and Hargraves,
2007). Like instances of other word classes, many
prepositions are ambiguous, carrying different se-
mantic meanings (including notions of instrumental,
accompaniment, location, etc.) as in ?He ran with
determination?, ?He ran with a broken leg?, or ?He
ran with Jane?. As NLP systems take more and more
semantic content into account, disambiguating be-
tween preposition senses becomes increasingly im-
portant for text processing tasks.
In order to disambiguate different senses, most
systems to date use a fixed window size to derive
classification features. These may or may not be
syntactically related to the preposition in question,
resulting?in the worst case?in an arbitrary bag of
words. In our approach, we make use of the phrase
structure to extract words that have a certain syn-
tactic relation with the preposition. From the words
collected that way, we derive higher level features.
In 2007, the SemEval workshop presented par-
ticipants with a formal preposition sense dis-
ambiguation task to encourage the development
of systems for the disambiguation of preposition
senses (Litkowski and Hargraves, 2007). The train-
ing and test data sets used for SemEval have been re-
leased to the general public, and we used these data
to train and test our system. The SemEval work-
shop data consists of instances of 34 prepositions
in natural text that have been tagged with the ap-
propriate sense from the list of the common Eng-
lish preposition senses compiled by The Preposition
Project, cf. Litkowski (2005). The SemEval data
provides a natural method for comparing the per-
formance of preposition sense disambiguation sys-
tems. In our paper, we follow the task requirements
and can thus directly compare our results to the ones
from the study. For evaluation, we compared our re-
sults to those of the three systems that participated
in the task (MELB: Ye and Baldwin (2007); KU:
Yuret (2007); IRST: Popescu et al (2007)). We also
used the ?first sense? and the ?most frequent sense?
96
baselines (see section 3 and table 1). These baselines
are determined by the TPP listing and the frequency
in the training data, respectively. Our system beat
the baselines and outperformed the three participat-
ing systems.
2 Methodology
2.1 Data Preparation
We downloaded the test and training data provided
by the SemEval-2007 website for the preposition
sense disambiguation task. These are 34 separate
XML files?one for each preposition?, comprising
16557 training and 8096 test example sentences,
each sentence containing one example of the respec-
tive preposition.
What are your beliefs
<head>about</head> these emotions ?
The preposition is annotated by a head tag, and the
meaning of the preposition in question is given as
defined by TPP.
Each preposition had between 2 and 25 different
senses (on average 9.76). For the case of ?about?
these would be
1. on the subject of; concerning
2. so as to affect
3. used to indicate movement within a particular
area
4. around
5. used to express location in a particular place
6. used to describe a quality apparent in a person
We parsed the sentences using the Charniak
parser (Charniak, 2000). Note that the Charniak
parser?even though among the best availbale Eng-
lish parsers?occasionally fails to parse a sentence
correctly. This might result in an erroneous extrac-
tion, such as an incorrect or no word. However,
these cases are fairly rare, and we did not manually
correct this, but rather relied on the size of the data
to compensate for such an error.
After this preprocessing step, we were able to ex-
tract the features.
2.2 Feature Extraction
Following O?Hara and Wiebe (2003) and
Alam (2004), we assumed that there is a meaningful
connection between syntactically related words on
both sides of the preposition. We thus focused on
specific words that are syntactically related to the
preposition via the phrase structure. This has the
advantage that it is not limited to a certain window
size; phrases might stretch over dozens of words,
so the extracted word may occur far away from the
actual preposition. These words were chosen based
on a manual analysis of training data. Using Tregex
(Levy and Andrew, 2006), a utility for expressing
?regular expressions over trees?, we created a set
of rules to extract the words in question. Each rule
matched words that exhibited a specific relationship
with the preposition or were within a two word
window to cover collocations. An example rule is
given below.
IN > (PP < (V P < # = x& <
#!AUX))
This particular rule finds the head (denoted by x) of
a verb phrase that governs the prepositional phrase
containing the preposition, unless x is an auxiliary
verb. Tregex rules were used to identify the follow-
ing words for feature generation:
? the head verb/noun that immediately dominates
the preposition along with all of its modifying
determiners, quantifiers, numbers, and adjec-
tives
? the head verb/noun immediately dominated by
the preposition along with all of its modifying
determiners, quantifiers, numbers, and adjec-
tives
? the subject, negator, and object(s) of the imme-
diately dominating verb
? neighboring prepositional phrases dominated
by the same verb/noun (?sister? prepositional
phrases)
? words within 2 positions to the left or right of
the preposition
For each word extracted using these rules, we col-
lected the following items:
97
? the word itself
? lemma
? part-of-speech (both exact and conflated, e.g.
both ?VBD? and ?verb? for ?VBD?)
? all synonyms of the first WordNet sense
? all hypernyms of the first WordNet sense
? boolean indicator for capitalization
Each feature is a combination of the extraction
rule and the extracted item. The values the feature
can take on are binary: present or absent. For some
prepositions, this resulted in several thousand fea-
tures. In order to reduce computation time, we used
the following steps: For each preposition classifier,
we ranked the features using information gain (For-
man, 2003). From the resulting lists,we included at
most 4000 features. Thus not all classifiers used the
same features.
2.3 Classifier Training
We chose maximum entropy (Berger et al, 1996) as
our primary classifier, since it had been successfully
applied by the highest performing systems in both
the SemEval-2007 preposition sense disambiguation
task (Ye and Baldwin, 2007) and the general word
sense disambiguation task (Tratz et al, 2007). We
used the implementation provided by the Mallet ma-
chine learning toolkit (McCallum, 2002). For the
sake of comparison, we also built several other clas-
sifiers, including multinomial na??ve Bayes, SVMs,
kNN, and decision trees (J48) using the WEKA
toolkit (Witten, 1999). We chose the radial basis
function (RBF) kernel for the SVMs and left all
other parameters at their default values.
3 Results
We measured the accuracy of the classifiers over
the test set provided by SemEval-2007 and provided
these results in Table 1. It is notable that our system
produced good results with all classifiers: For three
of the classifiers, the accuracy is higher than MELB,
the winning system of the task. As expected, the
highest accuracy was achieved using the maximum
entropy classifier. Overall, our system outperformed
the winning system by 0.058, an 8 percent improve-
ment. A simple proportion test shows this to be sta-
tistically significant at 0.001. ??????
??????
??????
?? ??????
??????
??????
??????
??????
??????
??????
??????
??????
???????
??????????????
???????????????
?????????????????????
??????????????
???????????????????????????
????????????????
??????????????????????????
????????????????
Table 1: Accuracy results on SemEval data (with 4000
features)
Since our initial cutoff of 4000 features was ar-
bitrary, we reran our Maximum Entropy experiment
multiple times with different cutoffs. Accuracy con-
sistently increased as the feature limit was relaxed,
resulting in 0.764 accuracy at the 10k feature limit.
These results are displayed in Figure 1.
its modifying determiners, quantifiers, 
numbers, and adjectives
? the head verb/noun immediately domi-
nated by the preposition along with all of 
its modifying determiners, quantifiers, 
numbers, and adjectives
? the subject, negator, and object(s) of the 
immediately dominating verb
? neighboring prepositional phrases domi-
nated by the same verb/noun (?sister? 
prepositional phrases)
? words within 2 positions to the left  or right 
of the preposition
For words extracted using these rules, we col-
lected the following features: 
? the word itself
? lemma
? part-of-speech (both exact  and conflated 
(e.g. both 'VBD' and 'verb' for 'VBD'))
? synonyms of the first WordNet sense
? hypernyms of the first WordNet s nse
? boolean indicator for capitalization
This resulted in several thousand features for the 
prepositions. We used information gain (Foreman, 
2003) in order to find the highest ranking features 
of each class and limited our classifiers to the top 
4000 features in order to reduce computation time.
2.3 Classifier Training
We chose maximum entropy (Berger, 1996) as our 
primary classifier because the highest performing 
systems in both the SemEval-2007 preposition 
sense disambiguation task (Ye and Baldwin, 2007) 
and the general word sense dis mbiguation t sk 
(Tratz et al, 2007) used it. We used the implemen-
tation provided by the Mallet machine learning 
toolkit (McCallum, 2002). Then, for the sake of 
comparison, we also built several other classifiers 
including multinomial na?ve Bayes, SVMs, kNN, 
and decision trees (J48) using the WEKA toolkit 
(Witten, 1999). We chose the radial basis function 
(RBF) kernel for the SVMs and left all other pa-
rame ers at their default values.
3 Results
We measured the accuracy of the classifiers over 
the test  et provided by SemEval-2007 and pro-
vided these results in Table 1. It  is notable that  our 
system produced good results with all classifiers: 
For three of the classifiers, the accuracy is higher 
than MELB, the winning system of the task. As 
expected, the highest  accuracy was achieved using 
the maximum entropy classifier.
Overall, our system outperformed the winning 
system by 0.058, an 8 percent improvement. A 
simple proportion test  shows this to be statistically 
significant at 0.001.
System Accuracy
kNN 684
SVM (RBF Kernel) 692
J48 decision trees 712
Multinomial Na?ve Bayes 731
Maximum Entropy 751
Most Frequent Sense 396
IRST (Popescu et al, 2007) 496
KU (Yuret, 2007) 547
MELB (Ye and Baldwin, 2007) 693
Table 1. Accuracy results on SemEval-2007 data.
Since our initial cutoff of 4000 features was arbi-
trary, we reran our Maximum Entropy experiment 
multiple times with different cutoffs. Accuracy 
consistently increased as the feature limit was re-
laxed, resulting in 0.764 accuracy at  the 10k fea-
ture limit. These results re displayed in Figure 1.
Figure 1. Relationship between maximum feature limit 
and accuracy for the Maximum Entropy classifiers.
4 Related Work
The linguistic literature on prepositions and their 
use is copious and diverse. We restrict ourselves to 
the works that deal with preposition sense disam-
biguation in computational linguistics.
O'Hara and Wiebe (2003) make use of Penn 
Treebank (Marcus et  al., 1993) and FrameNet 
(Baker et  al., 1998) to classify prepositions. They 
show that  using high level features from the con-
text, such as semantic roles, significantly aids dis-
Figure 1: Maximum feature limit vs. accuracy for maxi-
mum entropy classifier
4 Related Work
The linguistic literature on prepositions and their use
is copious and div rse. We restrict our lves to the
systems that compet d in the SemEval 2007 Prepo-
sition Sense Disambiguation task. All three of the
systems within the framework of the SemEval task
used supervised learning algorithms, yet they dif-
fered widely in the data collection and model prepa-
ration.
98
Ye and Baldwin (2007) participated in the Sem-
Eval task using a maximum entropy classifier and
achieved the highest accuracy of the participating
systems. The features they extracted were similar
to the ones we used, including POS and WordNet
features, but they used a substantially larger word
window, taking seven words from each side of the
preposition. While they included many higher level
features, they state that the direct lexical context
(i.e., bag-of-words) features were the most effective
and account for the majority of features, while syn-
tactic and semantic features had relatively little im-
pact.
Yuret (2007) used a n-gram model based on word
substitution by synonyms or antonyms. While this
proved to be quite successful with content words, it
had considerable problems with prepositions, since
the number of synonyms and/or antonyms is fairly
limited.
Popescu et al (2007) take an interesting approach
which they call Chain Clarifying Relationship. They
are using a supervised algorithm to learn a regu-
lar language. They used the Charniak parser and
FrameNet information on the head, yet the features
they extract are generally not linguistically moti-
vated.
5 Discussion
Using the phrase structure allows for more freedom
in the choice of words for feature selection, yet still
guarantees to find words for which some syntactic
relation with the preposition holds. Extracting se-
mantic features from these words (hypernyms, syn-
onyms, etc.) allows for a certain degree of abstrac-
tion, and thus a high level comparison. O?Hara and
Wiebe (2003) also make use of high level features,
in their case the Penn Treebank (Marcus et al, 1993)
and FrameNet (Baker et al, 1998) to classify prepo-
sitions. They show that using high level features?
such as semantic roles?of words in the context sub-
stantially aids disambiguation efforts. They cau-
tion, however, that indiscriminately using colloca-
tions and neighboring words may yield high accu-
racy, but has the risk of overfitting. In order to mit-
igate this, they classify the features by their part of
speech. While we made use of collocation features,
we also took into account higher order aspects of the
context, such as the governing phrase, part of speech
type, and semantic class according to WordNet. All
other things being equal, this seems to increase per-
formance substantially.
As for the classifiers used, our results seem to
confirm that Maximum Entropy classifiers are very
well suited for disambiguation tasks. Other than
na??ve Bayes, they do not presuppose a conditional
independence between the features, which clearly
not always holds (quite contrary, the underlying syn-
tactic structure creates strong interdependencies be-
tween words and features). This, however, does not
satisfactory explain the ranking of the other classi-
fiers. One possible explanation could be the sensi-
tivity of for example decision trees to random noise.
Though we made use of information gain before
classification, there still seems to be a certain ten-
dency to split on features that are not optimal.
6 Conclusion
We showed that using a number of simple linguis-
tically motivated features can improve the accu-
racy of preposition sense disambiguation. Utilizing
widely used and freely available standard tools for
language processing and a set of simple rules, we
were able to extract these features easily and with
very limited preprocessing. Instead of taking a ?bag
of words? approach that focuses primarily upon the
words within a fixed window size, we focused on el-
ements that are related via the phrase structure. We
also included semantic information gathered from
WordNet about the extracted words. We compared
five different classifiers and demonstrated that they
all perform very well, using our selected feature set.
Several of them even outperformed the top system
at SemEval. Our best result was obtained using a
maximum entropy classifier, just as the best partici-
pating system, leading us to believe that our primary
advantage was our feature set. While the contribu-
tion of the direct context (+/-7 words) might have
a stronger effect than higher level features (Ye and
Baldwin, 2007), we conclude from our findings that
higher level features do make an important contribu-
tion. These results are very encouraging on several
levels, and demonstrate the close interaction of syn-
tax and semantics. Leveraging these types of fea-
tures effectively is a promising prospect for future
99
machine learning research in preposition sense dis-
ambiguation.
Acknowledgements
The authors would like to thank Eduard Hovy and
Gully Burns for invaluable comments and helpful
discussions.
References
Y.S. Alam. 2004. Decision Trees for Sense Disambigua-
tion of Prepositions: Case of Over. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 52?59.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In ACM International Conference Proceeding Series,
volume 4, pages 132?139.
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press USA.
G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. The Jour-
nal of Machine Learning Research, 3:1289?1305.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In LREC 2006.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Ken Litkowski. 2005. The preposition project.
http://www.clres.com/prepositions.html.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of Eng-
lish: the Penn TreeBank. Computational Linguistics,
19(2):313?330.
A.K. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. 2002. http://mallet. cs. umass.
edu.
T. O?Hara and J. Wiebe. 2003. Preposition semantic
classification via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79?86.
Octavian Popescu, Sara Tonelli, and Emanuele Pianta.
2007. IRST-BP: Preposition Disambiguation based on
Chain Clarifying Relationships Contexts. In MELB-
YB: Preposition Sense Disambiguation Using Rich Se-
mantic Features, Prague, Czech Republic.
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007).
I.H. Witten. 1999. Weka: Practical Machine Learn-
ing Tools and Techniques with Java Implementations.
Dept. of Computer Science, University of Waikato,
University of Waikato, Dept. of Computer Science.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
100
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 141?144,
New York, June 2006. c?2006 Association for Computational Linguistics
Word Domain Disambiguation via Word Sense Disambiguation 
 
Antonio Sanfilippo, Stephen Tratz, Michelle Gregory 
Pacific Northwest National Laboratory 
Richland, WA 99352 
{Antonio.Sanfilippo, Stephen.Tratz, Michelle.Gregory}@pnl.gov 
 
 
  
 
Abstract 
Word subject domains have been 
widely used to improve the perform-
ance of word sense disambiguation al-
gorithms. However, comparatively little 
effort has been devoted so far to the 
disambiguation of word subject do-
mains. The few existing approaches 
have focused on the development of al-
gorithms specific to word domain dis-
ambiguation. In this paper we explore 
an alternative approach where word 
domain disambiguation is achieved via 
word sense disambiguation. Our study 
shows that this approach yields very 
strong results, suggesting that word 
domain disambiguation can be ad-
dressed in terms of word sense disam-
biguation with no need for special 
purpose algorithms.  
1 Introduction 
Word subject domains have been ubiquitously 
used in dictionaries to help human readers pin-
point the specific sense of a word by specifying 
technical usage, e.g. see ?subject field codes? in 
Procter (1978). In computational linguistics, 
word subject domains have been widely used to 
improve the performance of machine translation 
systems. For example, in a review of commonly 
used features in automated translation, Mowatt 
(1999) reports that most of the machine transla-
tion systems surveyed made use of word subject 
domains. Word subject domains have also been 
used in information systems. For example, San-
filippo (1998) describes a summarization system 
where subject domains provide users with useful 
conceptual parameters to tailor summary re-
quests to a user?s interest.  
Successful usage of word domains in applica-
tions such as machine translation and summari-
zation is strongly dependent on the ability to 
assign the appropriate subject domain to a word 
in its context. Such an assignment requires a 
process of Word Domain Disambiguation 
(WDD) because the same word can often be as-
signed different subject domains out of context 
(e.g. the word partner can potentially be re-
lated to FINANCE or MARRIAGE).  
Interestingly enough, word subject domains 
have been widely used to improve the perform-
ance of Word Sense Disambiguation (WSD) 
algorithms (Wilks and Stevenson 1998, Magnini 
et al 2001; Gliozzo et al 2004). However, com-
paratively little effort has been devoted so far to 
the word domain disambiguation itself. The 
most notable exceptions are the work of Magnini 
and Strapparava (2000) and Suarez & Palomar 
(2002). Both studies propose algorithms specific 
to the WDD task and have focused on the dis-
ambiguation of noun domains.  
In this paper we explore an alternative ap-
proach where word domain disambiguation is 
achieved via word sense disambiguation. More-
over, we extend the treatment of WDD to verbs 
and adjectives. Initial results show that this ap-
proach yield very strong results, suggesting that 
WDD can be addressed in terms of word sense 
disambiguation with no need of special purpose 
algorithms.  
141
  
Figure 1: Senses and domains for the word bank in WordNet Domains, with number of occurrences in SemCor, 
adapted from Magnini et al (2002). 
2 WDD via WSD 
Our approach relies on the use of WordNet Do-
mains (Bagnini and Cavagli? 2000) and can be 
outlined in the following two steps:  
1. use a WordNet-based WSD algorithm to 
assign a sense to each word in the input 
text, e.g. doctor  doctor#n#1 
2. use WordNet Domains to map disam-
biguated words into the subject domain 
associated with the word, e.g. doc-
tor#n#1doctor#n#1#MEDICINE. 
2.1 WordNet Domains 
WordNet Domains is an extension of WordNet 
(http://wordnet.princeton.edu/) where synonym 
sets have been annotated with one or more sub-
ject domain labels, as shown in Figure 1. Subject 
domains provide an interesting and useful classi-
fication which cuts across part of speech and 
WordNet sub-hierarchies. For example, doc-
tor#n#1 and operate#n#1 both have sub-
ject domain MEDICINE, and SPORT includes both 
athlete#n#1 with top hypernym life-
form#n#1 and sport#n#1 with  top hy-
pernym act#n#2.  
2.2 Word Sense Disambiguation 
To assign a sense to each word in the input text, 
we used the WSD algorithm presented in San-
filippo et al (2006). This WSD algorithm is 
based on a supervised classification approach 
that uses SemCor1 as training corpus. The algo-
rithm employs the OpenNLP MaxEnt imple-
mentation of the maximum entropy 
classification algorithm (Berger et al 1996) to 
develop word sense recognition signatures for 
each lemma which predicts the most likely sense 
for the lemma according to the context in which 
the lemma occurs. 
Following Dang & Palmer (2005) and Ko-
homban & Lee (2005), Sanfilippo et al (2006) 
use contextual, syntactic and semantic informa-
tion to inform our verb class disambiguation 
system.  
? Contextual information includes the verb 
under analysis plus three tokens found on 
each side of the verb, within sentence 
boundaries. Tokens included word as well 
as punctuation. 
? Syntactic information includes grammatical 
dependencies (e.g. subject, object) and mor-
pho-syntactic features such as part of 
speech, case, number and tense.  
? Semantic information includes named entity 
types (e.g. person, location, organization) 
and hypernyms. 
We chose this WSD algorithm as it provides 
some of the best published results to date, as the 
comparison with top performing WSD systems 
in Senseval3 presented in Table 1 shows---see 
http://www.senseval.org and Snyder & Palmer 
(2004) for terms of reference on Senseval3. 
                                                          
1
 http://www.cs.unt.edu/~rada/downloads.html. 
142
System Precision Fraction of 
Recall 
Sanfilippo et al  2006 61% 22% 
GAMBL 59.0% 21.3% 
SenseLearner 56.1% 20.2% 
Baseline 52.9% 19.1% 
Table 1: Results for verb sense disambiguation on 
Senseval3 data, adapted from Sanfilippo et al (2006). 
3 Evaluation 
To evaluate our WDD approach, we used both 
the SemCor and Senseval3 data sets. Both cor-
pora were stripped of their sense annotations and 
processed with an extension of the WSD algo-
rithm of Sanfilippo et al (2006) to assign a 
WordNet sense to each noun, verb and adjective. 
The extension consisted in extending the train-
ing data set so as to include a selection of 
WordNet examples (full sentences containing a 
main verb) and the Open Mind Word Expert 
corpus (Chklovski and Mihalcea 2002).  
The original hand-coded word sense annota-
tions of the SemCor and Senseval3 corpora and 
the word sense annotations assigned by the 
WSD algorithm used in this study were mapped 
into subject domain annotations using WordNet 
Domains, as described in the opening paragraph 
of section 2 above. The version of the SemCor 
and Senseval3 corpora where subject domain 
annotations were generated from hand-coded 
word senses served as gold standard.  A baseline 
for both corpora was obtained by assigning to 
each lemma the subject domain corresponding to 
sense 1 of the lemma.  
WDD results of a tenfold cross-validation for 
the SemCor data set are given in Table 2. Accu-
racy is high across nouns, verbs and adjectives.2 
To verify the statistical significance of these re-
sults against the baseline, we used a standard 
proportions comparison test (see Fleiss 1981, p. 
30). According to this test, the accuracy of our 
system is significantly better than the baseline.  
The high accuracy of our WDD algorithm is 
corroborated by the results for the Senseval3 
data set in Table 3. Such corroboration is impor-
tant as the Senseval3 corpus was not part of the 
data set used to train the WSD algorithm which 
provided the basis for subject domain assign-
                                                          
2
 We have not worked on adverbs yet, but we expect com-
parable results. 
ment. The standard comparison test for the Sen-
seval3 is not as conclusive as with SemCor. This 
is probably due to the comparatively smaller size 
of the Senseval3 corpus. 
 
 Nouns Verbs Adj.s Overall 
Accuracy 0.874 0.933 0.942 0.912 
Baseline 0.848 0.927 0.932 0.897 
p-value 4.6e-54 1.4e-07 5.5e-08 1.4e-58 
Table 2: SemCor WDD results. 
 
 Nouns Verbs Adj.s Overall 
Accuracy 0.797 0.908 0.888 0.848 
Baseline 0.783 0.893 0.862 0.829 
p-value 0.227 0.169 0.151 0.048 
Table 3: Senseval3 WDD results. 
4 Comparison with Previous WDD 
Work 
Our WDD algorithm compares favorably with 
the approach explored in Bagnini and Strap-
parava (2000), who report 0.82 p/r in the WDD 
tasks for a subset of nouns in SemCor.  
Suarez and Palomar (2002) report WDD re-
sults of 78.7% accuracy for nouns against a 
baseline of 68.7% accuracy for the same data 
set. As in the present study, Suarez and Palomar 
derive the baseline by assigning to each lemma 
the subject domain corresponding to sense 1 of 
the lemma. Unfortunately, a meaningful com-
parison with Suarez and Palomar (2002) is not 
possible as they use a different data set, the DSO 
corpus.3 We are currently working on repeating 
our study with the DSO corpus and will include 
the results of this evaluation in the final version 
of the paper to achieve commensurability with 
the results reported by Suarez and Palomar. 
5 Conclusions and Further Work 
Current approaches to WDD have assumed that 
special purpose algorithms are needed to model 
the WDD task. We have shown that very com-
petitive and perhaps unrivaled results (pending 
on evaluation of our WDD algorithm with the 
DSO corpus) can be obtained using WSD as the 
basis for subject domain assignment. This im-
provement in WDD performance can be used to 
                                                          
3
 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?cata 
logId=LDC97T12.  
143
obtain further gains in WSD accuracy, following 
Wilks and Stevenson (1998), Magnini et al 
(2001) and Gliozzo et al (2004). A more accu-
rate WSD model will in turn yield yet better 
WDD results, as demonstrated in this paper. 
Consequently, further improvements in accuracy 
for both WSD and WDD can be expected 
through a bootstrapping cycle where WDD re-
sults are fed as input to the WSD process, and 
the resulting improved WSD model is then used 
to achieve better WDD results. We intend to 
explore this possibility in future extensions of 
this work. 
Acknowledgements 
We would like to thank Paul Whitney for help 
with the evaluation of the results presented in 
Section 3.  
References  
Berger, A., S. Della Pietra and V. Della Pietra (1996) 
A Maximum Entropy Approach to Natural Lan-
guage Processing. Computational Linguistics, vol-
ume 22, number 1, pages 39-71. 
Chklovski, T. and R. Mihalcea (2002) Building a 
Sense Tagged Corpus with Open Mind Word Ex-
pert. Proceedings of the ACL 2002 Workshop on 
"Word Sense Disambiguation: Recent Successes 
and Future Directions, Philadelphia, July 2002, pp. 
116-122. 
Dang, H. T. and M. Palmer (2005) The Role of Se-
mantic Roles in Disambiguating Verb Senses. In 
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Ar-
bor MI, June 26-28, 2005.  
Fleiss, J. L. (1981) Statistical Methods for Rates and 
Proportions. 2nd edition. New York: John Wiley 
& Sons. 
Gliozzo, A., C. Strapparava, I. Dagan (2004) Unsu-
pervised and Supervised Exploitation of Semantic 
Domains in Lexical Disambiguation. Computer 
Speech and Language,18(3), Pages 275-299. 
Kohomban, U. and  W. Lee (2005) Learning seman-
tic classes for word sense disambiguation. In Pro-
ceedings of the 43rd Annual meeting of the 
Association for Computational Linguistics, Ann 
Arbor, MI.   
Magnini, B., Cavagli?, G. (2000) Integrating Subject 
Field Codes into WordNet. Proceedings of LREC-
2000, Second International Conference on Lan-
guage Resources and Evaluation, Athens, Greece, 
31 MAY- 2 JUNE 2000, pp. 1413-1418. 
Magnini, B., Strapparava C. (2000) Experiments in 
Word Domain Disambiguation for Parallel Texts. 
Proceedings of the ACL Workshop on Word 
Senses and Multilinguality, Hong-Kong, October 
7, 2000, pp. 27-33 
Magnini, B., C. Strapparava, G. Pezzulo and A. 
Gliozzo (2001) Using Domain Information for 
Word Sense Disambiguation. In Proceeding of 
SENSEVAL-2: Second International Workshop on 
Evaluating Word Sense Disambiguation Systems, 
pp. 111-114, 5-6 July 2001, Toulouse, France. 
Magnini, B., C. Strapparava, G. Pezzulo and A. 
Gliozzo (2002) The Role of Domain Information 
in Word Sense Disambiguation. Natural Language 
Engineering, 8(4):359?373. 
Mowatt, D. (1999) Types of Semantic Information 
Necessary in a Machine Translation Lexicon. Con-
f?rence TALN, Carg?se, pp. 12-17. 
Procter, Paul (Ed.) (1978) Longman Dictionary o 
Contemporary English. Longman Group Ltd., Es-
sex, UK. 
Sanfilippo, A. (1998) Ranking Text Units According 
to Textual Saliency, Connectivity and Topic Apt-
ness. COLING-ACL 1998: 1157-1163. 
Sanfilippo, A., S. Tratz, M. Gregory, A.Chappell, P. 
Whitney, C. Posse, P. Paulson, B. Baddeley, R. 
Hohimer, A. White. (2006) Automating Ontologi-
cal Annotation with WordNet. Proceedings of the 
3rd Global WordNet Conference, Jeju Island, 
South Korea, Jan 19-26 2006.  
Snyder, B.  and M. Palmer. 2004. The English all-
words task. SENSEVAL-3: Third International 
Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text. Barcelona, Spain.  
Su?rez, A., Palomar, M. (2002) Word sense vs. word 
domain disambiguation: a maximum entropy ap-
proach. In Sojka P., Kopecek I., Pala K., eds.: 
Text, Speech and Dialogue (TSD 2002). Volume 
2448 of Lecture Notes in Artificial Intelligence, 
Springer, (2002) 131?138. 
Wilks, Y. and Stevenson, M. (1998) Word sense dis-
ambiguation using optimised combinations of 
knowledge sources. Proceedings of the 17th inter-
national conference on Computational Linguistics, 
pp. 1398?1402. 
144
Proceedings of NAACL HLT 2007, Companion Volume, pages 169?172,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A High Accuracy Method for Semi-supervised Information Extraction 
 
 
Stephen Tratz Antonio Sanfilippo 
Pacific Northwest National Laboratory Pacific Northwest National Laboratory 
Richland, WA 99352 Richland, WA 99352 
stephen.tratz@pnl.gov antonio.sanfilippo@pnl.gov 
 
 
 
 
Abstract 
Customization to specific domains of dis-
course and/or user requirements is one of 
the greatest challenges for today?s Infor-
mation Extraction (IE) systems. While 
demonstrably effective, both rule-based 
and supervised machine learning ap-
proaches to IE customization pose too 
high a burden on the user. Semi-
supervised learning approaches may in 
principle offer a more resource effective 
solution but are still insufficiently 
accurate to grant realistic application. We 
demonstrate that this limitation can be 
overcome by integrating fully-supervised 
learning techniques within a semi-
supervised IE approach, without 
increasing resource requirements. 
1 Introduction 
Customization to specific discourse domains 
and/or user requirements is one of the greatest 
challenges for today?s Information Extraction (IE) 
systems. While demonstrably effective, both rule-
based and supervised machine learning approaches 
to IE customization require a substantial 
development effort. For example, Aone and 
Ramos-Santacruz (2000) present a rule-based IE 
system which handles 100 types of relations and 
events. Building such a system requires the manual 
construction of numerous extraction patterns 
supported by customized ontologies. Soderland 
(1999) uses supervised learning to induce a set of 
rules from hand-tagged training examples. While 
Sonderland suggests that the human effort can be 
reduced by interleaving learning and manual 
annotation activities, the creation of training data 
remains an onerous task. 
To reduce the knowledge engineering burden on 
the user in constructing and porting an IE system, 
unsupervised learning has been utilized, e.g. Riloff 
(1996), Yangarber et al (2000), and Sekine (2006). 
Banko et al (2007) present a self-supervised 
system that aims to avoid the manual IE 
customization problem by extracting all possible 
relations of interest from text. Stevenson and 
Greenwood (2005) propose a weakly supervised 
approach to sentence filtering that uses semantic 
similarity and bootstrapping to acquire IE patterns.  
Stevenson?s and Greenwood?s approach provides 
some of the best available results in weakly 
supervised IE to date, with 0.58 F-measure. While 
very good, an F-measure of 0.58 does not provide 
sufficient reliability to grant use in a production 
system.  
In this paper, we show that it is possible to 
provide a significant improvement over 
Stevenson?s and Greenwood?s results, without 
increasing resource requirements, by integrating 
fully-supervised learning techniques within a 
weakly supervised IE approach. 
1.1 Learning Algorithm 
Our method is modeled on the approach developed 
by Stevenson and Greenwood (2005) but uses a 
different technique for ranking candidate patterns. 
Stevenson?s and Greenwood?s algorithm takes as 
data inputs a small set of initial seed patterns and a 
corpus of documents, and uses any of several 
semantic similarity measures (Resnik, 1995; Jiang 
and Conrath, 1997; Patwardhan et al, 2003) to 
iteratively identify patterns in the document corpus 
169
that bear a strong resemblance to the seed patterns. 
After each iteration, the top-ranking candidate 
patterns are added to the seed patterns and 
removed from the corpus. Our approach differs 
from that of Stevenson and Greenwood in that we 
use a supervised classifier to rank candidate 
patterns.  This grants our system greater robustness 
and flexibility because the weight of classification 
features can be automatically determined within a 
supervised classification approach. 
In building supervised classifiers to rank 
candidate patterns at each iteration, we use both 
positive and negative training examples. Instead of 
creating manually annotated training examples, we 
follow an active learning approach where training 
examples are automatically chosen by ranking 
candidate patterns in terms of cosine similarity 
with the seed patterns. More specifically, we  
select patterns that have the lowest similarity with 
seed patterns to be the negative training examples. 
We hypothesized that these negative examples 
would contain many of the uninformative features 
occurring throughout the corpus and that using 
these examples would enable the classifier to 
determine that these features would not be useful.  
The pattern learning approach we propose 
includes the following steps. 
1. An unannotated corpus is required as input. 
For each sentence, a set of features is 
extracted. This information becomes Scand, the 
set of all candidate patterns. 
2. The user defines a set of seed patterns, Sseed. 
These patterns contain features expected to be 
found in a relevant sentence.  
3. The cosine measure is used to determine the 
distance between the patterns in Sseed and Scand. 
The patterns in Scand are then ordered by their 
lowest distance to a member of Sseed. 
4. The ? highest ranked patterns in Scand are 
added to Spos, the set of positive training 
examples. 
5. Sseed and Sacc are added to Spos. Sneg, the set of 
negative training examples is constructed from 
?+iteration*? of the lowest ranked patterns in 
Scand. Then, a maximum entropy classifier is 
built using Spos and Sneg as training data.  
6. The classifier is used to score each pattern in 
Scand. Scand is then sorted by these scores. 
7. The top ? patterns in Scand are added to Sacc and 
removed from Scand. 
8. If a suitable stopping point has been reached, 
the process ends. Otherwise, Spos and Sneg are 
emptied and the process continues at step 6. 
We set ? to 5, ? to 20, ? to 15, ? to 5, and used the 
following linguistic processing tools: (1) the 
OpenNLP library (opennlp.sourceforge.net) for 
sentence splitting and named-entity recognition,  
and (2) Connexor for syntactic parsing 
(Tapanainen and J?rvinen, 1997). For the 
classifier, we used the OpenNLP MaxEnt 
implementation (maxent.sourceforge.net) of the 
maximum entropy classification algorithm (Berger 
et al 1996).  We used the MUC-6 data set as the 
testing ground for our proposed approach. 
1.2 Description of Features Used 
Stevenson and Greenwood (2005) use subject-
verb-object triples for their features. We use a 
richer feature set. Our system can easily 
accommodate more features because we let the 
maximum entropy classifier determine the weight 
for the features. Stevenson?s and Greenwood?s 
approach determines weights using semantic 
similarity and would require significant changes to 
take into account various other features, especially 
those for which a WordNet (Fellbaum, 1998) 
similarity score is not available. 
We use single tokens, token combinations, and 
semantic information to inform our IE pattern 
extraction system. Lexical items marked by the 
named-entity recognition system as PERSON or 
ORGANIZATION are replaced with ?person? and 
?organization?, respectively. Number tokens are 
replaced with ?numeric?. Single Token Features 
include: 
? All words in the sentence and all hypernyms of 
the first sense of the word with attached part-
of-speech 
? All words in the sentence with attached 
dependency 
? The verb base of each nominalization and the 
verb?s first sense hypernyms are included. 
Token Combinations include: 
? All bigrams from the sentence 
? All subject-object pairs 
? All parent-child pairs from the parse tree 
170
? A specially marked copy of the parent-child 
pairs where the main verb is the parent. 
We also added semantic features indicating if a 
PERSON or ORGANIZATION was detected 
within the sentence boundaries. Table1 provides an 
example where a simple sentence is mapped into 
the set of features we have just described. 
 
Alan G. Spoon, 42, will succeed 
Mr. Graham as president of the 
company. 
 
 
Single Token Features 
With attached dependencies: 
attr:person, subj:person, mod:numeric, v-ch:will, 
main:succeed, obj:person, copred:as, pcomp:president, 
mod:of, det:the, pcomp:company 
With part-of-speech tags: 
n:person, v:succeed, v:will, dt:the, n:company, 
n:institution, n:social_group, n:group, n:organization, 
n:person, n:president, n:executive, n:corporat-
e_executive, n:administrator, n:head, n:leader, n:orga-
nism, n:living_thing, n:object, n:entity, num:numeric, 
abbr:person, prp:as, prp:of, v:control, v:declare, 
v:decree, v:express, v:ordain, v:preside, v:state 
Token Combinations 
Bigrams: 
person+comma, comma+numeric, numeric+comma, 
comma+will, will+succeed, succeed+person, person+as, 
as+president, president+of, of+the, the+company 
Subject Object Pairs: 
sop:person+person 
Parent-Child Pairs: 
pc:person+person, pc:person+numeric, pc:will+person, 
pc:succeed+will, pc:succeed+person, pc:succeed+as, 
pc:as+president, pc:president+of, pc:of+company, 
pc:company+the 
Main Verb Parent-Child Pairs: 
mvpc:succeed+person, mvpc:succeed+will, mv-
pc:succeed+as 
Semantic Features 
hasOrganization, hasPerson 
Table 1: Feature representation of a simple sentence. 
 
The seeds we used are adapted from the seed 
patterns employed by Stevenson and Greenwood. 
As shown in Table 2, only a subset of the features 
described above is used in the seed patterns. 
2 Evaluation 
We used the document collection which was 
initially developed for the Sixth Message 
Understanding Conference (MUC-6) as ground 
truth data set to evaluate our approach. The MUC-
6 corpus (www.ldc.upenn.edu) is composed of 100 
Wall Street Journal documents written during 1993 
and 1994.  Our task was to detect sentences which 
included management succession patterns, such as 
those shown in Table 2.  
 
1: subj:organization, main:appoint, obj:person, hasPers-
on, hasOrganization 
2: subj:organization, main:elect, obj:person, hasOrgani-
zation, hasPerson 
3: subj:organization, main:promote, obj:person, hasOrg-
anization, hasPerson 
4: subj:organization, main:name, obj:person, hasOrgani-
zation, hasPerson 
5: subj:person, main:resign, hasPerson 
6: subj:person, main:depart, hasPerson 
7: subj:person, main:quit, hasPerson 
Table 2: Feature representation of seed patterns. 
 
     The version of the MUC-6 corpus produced by 
Soderland (1999) provided us with a specification 
of succession patterns at the sentence level, but as 
shown in Table 3 did not include the source text.  
We reconstructed the original text by  
automatically aligning the succession patterns in 
the sentence structures in Soderland?s version of 
the MUC-6 corpus with the sentences in the 
original MUC-6 corpus. This alignment produced a 
set of 1581 sentences, of which 134 contained 
succession patterns.  
 
@S[ 
  {SUBJ  @CN[ FOX ]CN } 
  {VB  NAMED @NAM } 
  {OBJ  @PN[ LUCILLE S. SALHANY ]PN , @PS[ 
CHAIRMAN ]PS OF @CN[ FOX INC. ]CN 'S 
TELEVISION PRODUCTION ARM , } 
  {REL_V  TO SUCCEED @SUCCEED HIM . } 
]@S 9301060123-5 
@@TAGS Succession {PersonIn @PN[ LUCILLE S. 
SALHANY ]PN}+ {Post @PS[ CHAIRMAN ]PS}+ 
{Org @CN[ FOX INC. ]CN}_  @@COVERED_BY 
@@ENDTAGS 
Table 3: Data sample from Soderland test set. 
 
As shown in Figure 1, our best score of 0.688 F-
measure was obtained on the 36th iteration; at the 
end of this iteration, our algorithm selected 180 
sentences including 108 of the sentences that 
contained succession patterns. This is a significant 
improvement over the 0.58 F-measure score 
171
reported by Stevenson and Greenwood (2005) for 
the same task. The use of a supervised 
classification approach to the ranking of candidate 
patterns with a richer feature set were the two 
determinant factors in achieving such 
improvement. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70 80
Iteration
F-
m
ea
su
re
 
Figure 1: Evaluation results with MUC-6 data. 
3 Conclusions 
Our results show a substantial improvement over 
previous efforts in weakly supervised IE methods, 
suggesting that weakly supervised methods can be 
made to rival rule-based or fully supervised 
approaches both in resource effectiveness and 
accuracy. We plan to verify the strength of our 
approach evaluating against other ground truth data 
sets. We also plan to detail how the various 
features in our classification model contribute to 
ranking of candidate patterns.  An additional area 
of envisioned improvement regards the use of a 
random sub selection of negative candidate 
patterns as training samples to counteract the 
presence of sentence fragments among low-
ranking candidate patterns. Finally, we intend to 
evaluate the benefit of having a human in the loop 
in the first few iterations to filter out patterns 
chosen by the system. 
References  
C. Aone and M. Ramos-Santacruz. 2000. REES: A 
Large-Scale Relation and Event Extraction System, 
pages 76-83, In Proceedings of the 6th Applied 
Natural Language Processing Conference (ANLP 
2000), Seattle. 
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proceedings of the 20th 
International Joint Conference on Artificial 
Intelligence (IJCAI 2007). Hyderabad, India. 
A. Berger, S. Della Pietra and V. Della Pietra (1996) A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, volume 22, 
number 1, pages 39-71. 
C. Fellbaum, editor. 1998. WordNet: An Electronic 
Lexical Database and some of its Applications. MIT 
Press, Cambridge, MA. 
J. Jiang and D. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. In 
Proceedings of the 10th International Conference on 
Research in Computational Linguistics, Taiwan. 
S. Patwardhan, S. Banerjee, and T. Pederson. 2003. 
Using measures of semantic relatedness for word 
sense disambiguation. In Proceedings of the 4th 
International Conferences on Intelligent Text 
Processing and Computational Linguistics, pages 
241-257, Mexico City. 
P. Resnik. 1995. Using Information Content to evaluate 
Semantic Similarity in a Taxonomy. In Proceedings 
of the 14th International Joint Conference on 
Artificial Intelligence (IJCAI-95), pages 448-452, 
Montreal, Canada. 
E. Riloff and R. Jones. 1999. Learning Dictionaries for 
Information Extraction by Multi-level Bootstrapping.  
In Proceedings of the 16th National Conference on 
Artificial Intelligence. Orlando, Florida. 
S. Sekine. 2006. On-Demand Information Extraction. In 
Proceedings of the COLING/ACL 2006 Main 
Conference Poster Sessions. Sydney, Australia. 
S. Soderland. 1999. Learning Information Extraction 
Rules for Semi-structured and free text. Machine 
Learning, 31(1-3):233-272. 
M. Stevenson and M. A. Greenwood. 2005. A Semantic 
Approach to IE Pattern Induction. In Proceedings of 
the 43rd Annual Meeting of the ACL (ACL 05), Ann 
Arbor, Michigan.    
P. Tapanainen and Timo J?rvinen. 1997. A non-
projective dependency parser. In Proceedings of the 
5th Conference on Applied Natural Language 
Processing, pages 64?71, Washington D.C. 
Association for Computational Linguistics. 
R. Yangarber, R. Grishman, P. Tapanainen, and S. 
Huttunen. 2000. Automatic acquisition of domain 
knowledge for information extraction. In 
Proceedings of the 18th International Conference of 
Computational Linguistics (COLING 2002), Taipei. 
172
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 25?32,
New York City, June 2006. c?2006 Association for Computational Linguistics
 
Integrating Ontological Knowledge and Textual Evidence in Estimating 
Gene and Gene Product Similarity 
 
Antonio Sanfilippo, Christian Posse, Banu Gopalan, Stephen Tratz, Michelle Gregory 
Pacific Northwest National Laboratory 
Richland, WA 99352 
{Antonio.Sanfilippo, Christian.Posse, Banu.Gopalan, Stephen.Tratz, 
Michelle.Gregory}@pnl.gov  
 
 
  
 
Abstract 
With the rising influence of the Gene On-
tology, new approaches have emerged 
where the similarity between genes or 
gene products is obtained by comparing 
Gene Ontology code annotations associ-
ated with them. So far, these approaches 
have solely relied on the knowledge en-
coded in the Gene Ontology and the gene 
annotations associated with the Gene On-
tology database. The goal of this paper is 
to demonstrate that improvements to these 
approaches can be obtained by integrating 
textual evidence extracted from relevant 
biomedical literature. 
1 Introduction 
The establishment of similarity between genes and 
gene products through homology searches has be-
come an important discovery procedure that biolo-
gists use to infer structural and functional 
properties of genes and gene products?see Chang 
et al (2001) and references therein. With the rising 
influence of the Gene Ontology1 (GO), new ap-
proaches have emerged where the similarity be-
tween genes or gene products is obtained by 
comparing GO code annotations associated with 
them. The Gene Ontology provides three orthogo-
nal networks of functional genomic concepts struc-
                                                          
1 http://www.geneontology.org. 
tured in terms of semantic relationships such as 
inheritance and meronymy, which encode biologi-
cal process (BP), molecular function (MF) and cel-
lular component (CC) properties of genes and gene 
products. GO code annotations explicitly relate 
genes and gene products in terms of participation 
in the same/similar biological processes, presence 
in the same/similar cellular components and ex-
pression of the same/similar molecular functions. 
Therefore, the use of GO code annotations in es-
tablishing gene and gene product similarity pro-
vides significant added functionality to methods 
such as BLAST (Altschul et al 1997) and FASTA 
(Pearson and Lipman 1988) where gene and gene 
product similarity is calculated using string-based 
heuristics to select maximal segment pair align-
ments across gene and gene product sequences to 
approximate the Smith-Waterman algorithm 
(Smith and Waterman 1981). 
Three main GO-based approaches have emerged 
so far to compute gene and gene product similarity. 
One approach assesses GO code similarity in terms 
of shared hierarchical relations within each gene 
ontology (BP, MF, or CC) (Lord et al 2002, 2003; 
Couto et al 2003; Azuaje et al 2005).  For exam-
ple, the relative semantic closeness of two biologi-
cal processes would be determined by the 
informational specificity of the most immediate 
parent that the two biological processes share in 
the BP ontology. The second approach establishes 
GO code similarity by leveraging associative rela-
tions across the three gene ontologies (Bodenreider 
et al 2005). Such associative relations make pre-
dictions such as which cellular component is most 
likely to be the location of a given biological proc-
25
ess and which molecular function is most likely to 
be involved in a given biological process. The third 
approach computes GO code similarity by combin-
ing hierarchical and associative relations (Posse et 
al. 2006). 
Several studies within the last few years 
(Andrade et al 1997, Andrade 1999, MacCallum et 
al. 2000, Chang at al. 2001) have shown that the 
inclusion of evidence from relevant scientific lit-
erature improves homology search. It is therefore 
highly plausible that literature evidence can also 
help improve GO-based approaches to gene and 
gene product similarity. Sanfilippo et al (2004) 
propose a method for integrating literature evi-
dence within an early version of the GO-based 
similarity algorithm presented in Posse et al 
(2006). However, no effort has been made so far in 
evaluating the potential contribution of textual evi-
dence extracted from relevant biomedical literature 
for GO-based approaches to the computation of 
gene and gene product similarity. The goal of this 
paper is to address this gap with specific reference 
to the assessment of protein similarity. 
2 Background 
GO-based similarity methods that focus on meas-
uring intra-ontological relations have adopted the 
information theoretic treatment of semantic simi-
larity developed in Natural Language Process-
ing?see Budanitsky (1999) for an extensive 
survey. An example of such a treatment is given by 
Resnik (1995), who defines semantic similarity 
between two concept nodes c1 c2 in a graph as the 
information content of the least common su-
perordinate (lcs) of c1 and c2, as shown in (1). The 
information content of a concept node c, IC(c), is 
computed as -log p(c) where p(c) indicates the 
probability of encountering instances of c in a spe-
cific corpus. 
(1)     
)),c p(lcs(c
)),c IC(lcs(c) ,csim(c
21log
2121
?=
==
Jiang and Conrath (1997) provide a refinement of 
Resnik?s measure by factoring in the distance from 
each concept to the least common superordinate, as 
shown in (2).2
                                                          
2 Jiang and Conrath (1997) actually define the distance be-
tween two concepts nodes c1 c2, e.g.     
 )), c IC(lcs(c ) -  IC(c)  IC(c) , cdist(c 2122121 ?+=
(2)
)),cIC(lcs(c) -IC(c)IC(c ) ,csim(c 21221
121 ?+=  
Lin (1998) provides a slight variant of Jiang?s and 
Conrath?s measure, as indicated in (3).  
(3) 
)  IC(c) IC(c
)), c IC(lcs(c 
 ) ,csim(c
21
212
21 +
?=  
The information theoretic approach is very well 
suited to assess GO code similarity since each gene 
subontology is formalized as a directed acyclic 
graph. In addition, the GO database3 includes nu-
merous curated GO annotations which can be used 
to calculate the information content of each GO 
code with high reliability. Evaluations of this 
methodology have yielded promising results. For 
example, Lord et al (2002, 2003) demonstrate that 
there is strong correlation between GO-based simi-
larity judgments for human proteins and similarity 
judgments obtained through BLAST searches for 
the same proteins. Azuaje et al (2005) show that 
there is a strong connection between the degree of 
GO-based similarity and the expression correlation 
of gene products. 
As Bodenreider et al (2005) remark, the main 
problem with the information theoretic approach to 
GO code similarity is that it does not take into ac-
count associative relations across the gene ontolo-
gies. For example, the two GO codes 0050909 
(sensory perception of taste) and 0008527 (taste 
receptor activity) belong to different gene ontolo-
gies (BP and MF), but they are undeniably very 
closely related. The information theoretic approach 
would simply miss associations of this kind as it is 
not designed to capture inter-ontological relations.  
Bodenreider et al (2005) propose to recover as-
sociative relations across the gene ontologies using 
a variety of statistical techniques which estimate 
the similarity of two GO codes inter-ontologically 
in terms of the distribution of the gene product an-
notations associated with the two GO codes in the 
GO database. One such technique is an adaptation 
of the vector space model frequently used in In-
formation Retrieval (Salton et al 1975), where 
                                                                                           
For ease of exposition, we have converted Jiang?s and Con-
rath?s semantic distance measure to semantic similarity by 
taking its inverse, following Pedersen et al (2005). 
3 http://www.godatabase.org/dev/database.  
26
each GO code is represented as a vector of gene-
based features weighted according to their distribu-
tion in the GO annotation database, and the simi-
larity between two GO codes is computed as the 
cosine of the vectors for the two codes. 
The ability to measure associative relations 
across the gene ontologies can significantly aug-
ment the functionality of the information theoretic 
approach so as to provide a more comprehensive 
assessment of gene and gene product similarity. 
However, in spite of their complementarities, the 
two GO code similarity measures are not easily 
integrated. This is because the two measures are 
obtained through different methods, express dis-
tinct senses of similarity (i.e. intra- and inter-
ontological) and are thus incomparable.  
Posse et al (2006) develop a GO-based similar-
ity algorithm?XOA, short for Cross-Ontological 
Analytics?capable of combining intra- and inter-
ontological relations by ?translating? each associa-
tive relation across the gene ontologies into a hier-
archical relation within a single ontology. More 
precisely, let c1 denote a GO code in the gene on-
tology O1 and c2 a GO code in the gene ontology 
O2. The XOA similarity between c1 and c2 is de-
fined as shown in (4), where4
? cos(ci,cj) denotes the cosine associative meas-
ure proposed by Bodenreider et al (2005) 
? sim(ci,cj) denotes any of the three intra-
ontological semantic similarities described 
above, see (1)-(3) 
? maxci in Oj {f(ci)} denotes the maximum of the 
function f() over all GO codes ci in the gene 
ontology Oj.  
The major innovation of the XOA approach is to 
allow the comparison of two nodes c1, c2 across 
distinct ontologies O1, O2 by mapping c1 into its 
closest node c4 in O2 and c2 into its closest node 
c3 in O1. The inter-ontological semantic similarity 
between c1 and c2 can be then estimated from the 
intra-ontological semantic similarities between c1-
                                                          
4 If c1 and c2 are in the same ontology, i.e. O1=O2, then 
xoa(c1,c2) is still computed as in (4). In most cases, the 
maximum in (4) would be obtained with c3 = c2 and c4 = c1 
so that  XOA(c1,c2) would simply be computed as sim(c1,c2). 
However, there are situations where there exists a GO code c3 
(c4) in the same ontology which 
? is highly associated with c1 (c2),  
?  is semantically close to c2 (c1), and  
?  leads to a value for  sim(c1,c3) x cos(c2,c3)  ((sim(c2,c4) 
x cos(c1,c4)) that is higher than sim(c1,c2). 
c3 and c2-c4, using multiplication with the associa-
tive relations between c2-c3 and c1-c4 as a score 
enrichment device. 
 
(4)  
??
?
?
??
?
?
?
??
?
?
??
?
?
?
?
?
??
??
?
??
??
?
??
??
?
??
??
?
=
), c(c
), c(c
), c(c
), c(c
Oinc
Oinc
,
), c(c
41cos
42sim
32cos
31sim
XOA
24
13
max
max
max21  
 
Posse et al (2006) show that the XOA similarity 
measure provides substantial advantages. For ex-
ample, a comparative evaluation of protein similar-
ity, following the benchmark study of Lord et al 
(2002, 2003), reveals that XOA provides the basis 
for a better correlation with protein sequence simi-
larities as measured by BLAST bit score than any 
intra-ontological semantic similarity measure. The 
XOA similarity between genes/gene products de-
rives from the XOA similarity between GO codes. 
Let GP1 and GP2 be two genes/gene products. Let 
c11,c12,?, c1n denote the set of GO codes associ-
ated with GP1 and c21, c22,?., c2m the set of GO 
codes associated with GP2. The XOA similarity 
between GP1 and GP2 is defined as in (5), where 
i=1,?,n and j=1,?,m.  
 
(5) XOA(GP1,GP2) = max {XOA(c1i, c2j)} 
 
The results of the study by Posse et al (2006) are 
shown in Table 1. Note that the correlation be-
tween protein similarities based on intra-
ontological similarity measures and BLAST bit 
scores in Table 1 is given for each choice of gene 
ontology (MF, BP, CC). This is because intra-
ontological similarity methods only take into ac-
count GO codes that are in the same ontology and 
can therefore only assess protein similarity from a 
single ontology viewpoint. By contrast, the XOA-
based protein similarity measure makes use of GO 
codes that can belong to any of the three gene on-
tologies and needs not be broken down by single 
ontologies, although the contribution of each gene 
ontology or even single GO codes can still be 
fleshed out, if so desired. 
Is it possible to improve on these XOA results 
by factoring in textual evidence? We will address 
this question in the remaining part of the paper. 
27
 
Semantic Similarity 
Measures 
Resnik Lin Jiang &  
Conrath 
Intra-ontological    
Molecular Function 0.307 0.301 0.296 
Biological Process 0.195 0.202 0.203 
Cellular Component 0.229 0.234 0.233 
XOA 0.405 0.393 0.368 
Table 1: Spearman rank order correlation coeffi-
cients between BLAST bit score and semantic 
similarities, calculated using a set of 255,502 pro-
tein pairs?adapted from Posse et al (2006). 
3 Textual Evidence Selection 
Our first step in integrating textual evidence into 
the XOA algorithm is to select salient information 
from biomedical literature germane to the problem. 
Several approaches can be used to carry out this 
prerequisite. For example, one possibility is to col-
lect documents relevant to the task at hand, e.g. 
through PubMed queries, and use feature weight-
ing and selection techniques from the Information 
Retrieval literature?e.g. tf*idf (Buckley 1985) and 
Information Gain (e.g. Yang and Pedersen 
1997)?to distill the most relevant information. An-
other possibility is to use Information Extraction 
algorithms tailored to the biomedical domain such 
as Medstract (http://www.medstract.org, Puste-
jovsky et al 2002) to extract entity-relationship 
structures of relevance. Yet another possibility is to 
use specialized tools such as GoPubMed (Doms 
and Schroeder 2005) where traditional keyword-
based capabilities are coupled with term extraction 
and ontological annotation techniques.  
In our study, we opted for the latter solution, us-
ing generic Information Retrieval techniques to 
normalize and weigh the textual evidence ex-
tracted. The main advantage of this choice is that 
tools such as GoPubMed provide very high quality 
term extraction at no cost. Less appealing is the 
fact that the textual evidence provided is GO-based 
and therefore does not offer information which is 
orthogonal to the gene ontology. It is reasonable to 
expect better results than those reported in this pa-
per if more GO-independent textual evidence were 
brought to bear. We are currently working on using 
Medstract as a source of additional textual evi-
dence. 
GoPubMed is a web server which allows users 
to explore PubMed search results using the Gene 
Ontology for categorization and navigation pur-
poses (available at http://www.gopubmed.org). As 
shown in Figure 1 below, the system offers the 
following functionality: 
? It provides an overview of PubMed search re-
sults by categorizing abstracts according to the 
Gene Ontology 
? It verifies its classification by providing an 
accuracy percentage for each 
? It shows definitions of Gene Ontology terms 
? It allows users to navigate PubMed search re-
sults by GO categories 
? It automatically shows GO terms related to the 
original query for each result  
? It shows query terms (e.g. ?Rab5? in the mid-
dle windowpane of Figure 1) 
? It automatically extracts terms from search 
results which map to GO categories (e.g. high-
lighted terms other than ?Rab5?  in the middle 
windowpane of Figure 1). 
In integrating textual evidence with the XOA al-
gorithm, we utilized the last functionality (auto-
matic extraction of terms) as an Information 
Extraction capability. Details about the term ex-
traction algorithm used in GoPubMed are given in 
Delfs et al (2004). In short, the GoPubMed term 
extraction algorithm uses word alignment strate-
gies in combination with stemming to match word 
sequences from PubMed abstracts with GO terms. 
In doing so, partial and discontinuous matches are 
allowed. Partial and discontinuous matches are 
weighted according to closeness of fit. This is indi-
cated by the accuracy percentages associated with 
GO in Figure 1 (right side). In this study we did 
not make use of these accuracy percentages, but 
plan to do so in the future. 
28
 
Figure 1: GoPubMed sample query for the ?rab5? protein. The abstracts shown are automatically proposed by the 
system after the user issues the protein query and then selects the GO term ?late endosome? (bottom left) as the 
discriminating parameter. 
  
Our data set consists of 2360 human protein 
pairs containing 1783 distinct human proteins. This 
data set was obtained as a 1% random sample of 
the human proteins used in the benchmark study of 
Posse et al (2006)?see Table 1.5 For each of the 
1783 human proteins, we made a GoPubMed query 
and retrieved up to 100 abstracts. We then col-
lected all the terms extracted by GoPubMed for 
each protein across the abstracts retrieved. Table 2 
provides an example of the output of this process. 
 
nutrient, uptake, carbohydrate, metabolism, affect-
ing, cathepsin, activity, protein, lipid, growth, rate, 
habitually, signal, transduction, fat, protein, cad-
herin, chromosomal, responses, exogenous, lactat-
ing, exchanges, affects, mammary, gland, ?. 
Table 2: Sample output of the GoPubMed term extrac-
tion process for the Cadherin-related tumor suppressor 
protein. 
                                                          
5 We chose such a small sample to facilitate the collection of 
evidence from GoPubMed, which is not yet fully automated. 
Our XOA approach is very scalable, and we do not anticipate 
any problem running the full protein data set of 255,502 pairs, 
once we fully automate the GoPubMed extraction process. 
4 Integrating Textual Evidence in XOA 
Using the output of the GoPubMed term extraction 
process, we created vector-based signatures for 
each of the 1783 proteins, where  
? features are obtained by stemming the terms 
provided by GoPubMed  
? the value for each feature is derived as the 
tf*idf  for the feature. 
We then calculated the similarity between each of 
the 2360 protein pairs as the cosine value of the 
two vector-based signatures associated with the 
protein pair. 
We tried two different strategies to augment the 
XOA score for protein similarity using the protein 
similarity values obtained as the cosine of the 
GoPubMed term-based signatures. The first strat-
egy adopts a fusion approach in which the two 
similarity measures are first normalized to be 
commensurable and then combined to provide an 
interpretable integrated model. A simple normali-
zation is obtained by observing that the Resnik?s 
information content measure is commensurable to 
29
the log of the text based cosine (LC). This leads us 
to the fusion model shown in (5) for XOA, based 
on Resnik?s semantic similarity measure (XOAR). 
(5)      Fusion(Resnik) = XOAR + LC 
We then observe that the XOA measures based on 
Resnik, Lin (XOAL) and Jiang & Conrath (XOAJC) 
are highly correlated (correlations exceed 0.95 on 
the large benchmarking dataset discussed in sec-
tion 2, see Table 1). This suggests the fusion model 
shown in (6), where the averages of the XOA 
scores are computed from the benchmarking data 
set. 
(6)      Fusion(Lin) =  
                XOAL + LC*Ave( XOAL)/Ave(XOAR) 
          Fusion(Jiang & Conrath) =  
               XOAJC + LC*Ave(XOAJC)/Ave(XOAR) 
The second strategy consists in building a predic-
tion model for BLAST bit score (BBS) using the 
XOA score and the log-cosine LC as predictors 
without the constraint of remaining interpretable. 
As in the previous strategy, a different model was 
sought for each of the three XOA variants. In each 
case, we restrict ourselves to cubic polynomial re-
gression models as such models are quite efficient 
at capturing complex nonlinear relationships be-
tween target and predictors (e.g. Weisberg 2005). 
More precisely, for each of the semantic similarity 
measures, we fit the regression model to BBS 
shown in (7), where the subscript x denotes either 
R, L or JC, and the coefficients a to h are found by 
maximizing the Spearman rank order correlations 
between BBS and the regression model. This 
maximization is automatically carried out by using 
a random walk optimization approach (Romeijn 
1992). The coefficients used in this study for each 
semantic similarity measure are shown in Table 3. 
 
(7)    a*XOAx + b*XOAx2 + c*XOAx +  d*LC 
        + e*LC2 + f*LC3 +  g*XOAx*LC 
5 Evaluation 
Table 4 summarizes the results for both strategies, 
comparing Spearman rank correlations between 
BBS and the models from the fusion and regres-
sion approaches with Spearman rank correlations 
between BBS and XOA alone. Note that the latter 
correlations are lower than the one reported in Ta-
ble 2 due to the small size of our sample (1% of the 
original data set, as pointed out above). P-values 
associated with the changes in the correlation val-
ues are also reported, enclosed in parentheses.  
 
 Resnik Lin Jiang & Conrath 
a -10684.43 2.83453e-05 0.2025174 
b 1.786986 -31318.0 -1.93974 
c 503.3746 45388.66 0.08461453 
d -3.952441 208.5917 4.939535e-06 
e 0.0034074 1.55518e-04 0.0033902 
f 1.4036e-05 9.972911e-05 -0.000838812 
g 713.769 -1.10477e-06 2.461781 
Table 3: Coefficients of the regression model maximiz-
ing Spearman rank correlation between BBS and the 
regression model for each of the three semantic similar-
ity measures. 
 
 XOA Fusion Regression 
Resnik 0.295 0.325 (>0.20) 0.388 (0.0008) 
Lin 0.274 0.301 (>0.20) 0.372 (0.0005) 
Jiang & 
Conrath 0.273 0.285 (>0.20) 0.348 (0.008) 
Table 4: Spearman rank order correlation coefficients 
between BLAST bit score BBS and XOA, BBS and the 
fusion model, and BBS and the regression model. P-
values for the differences between the augmented mod-
els and XOA alone are given in parentheses. 
 
An important finding from Table 4 is that inte-
grating text-based evidence in the semantic simi-
larity measures systematically improves the 
relationships between BLAST and XOA. Not sur-
prisingly, the fusion models yield smaller im-
provements. However, these improvements in the 
order of 3% for the Resnik and Lin variants are 
very encouraging, even though they are not statis-
tically significant. The regression models, on the 
other hand, provide larger and statistically signifi-
cant improvements, reinforcing our hypothesis that 
textual evidence complements the GO-based simi-
larity measures. We expect that a more sophisti-
cated NLP treatment of textual evidence will yield 
significant improvements even for the more inter-
pretable fusion models.   
Conclusions and Further Work 
Our early results show that literature evidence pro-
vides a significant contribution, even using very 
simple Information Extraction and integration 
methods such as those described in this paper. The 
employment of more sophisticated Information 
30
Extraction tools and integration techniques is 
therefore likely to bring higher gains.  
Further work using GoPubMed involves factor-
ing in the accuracy percentage which related ex-
tracted terms to their induced GO categories and 
capturing complex phrases (e.g. signal transduc-
tion, fat protein). We also intend to compare the 
advantages provided by the GoPubMed term ex-
traction process with Information Extraction tools 
created for the biomedical domain such as Med-
stract (Pustejovsky et al 2002), and develop a 
methodology for integrating a variety of Informa-
tion Extraction processes into XOA. 
References  
Altschul, S.F., T. L. Madden, A. A. Schaffer, J. Zhang, 
Z. Anang, W. Miller and D.J. Lipman (1997) Gapped 
BLAST and PSI-BLST: a new generation of protein 
database search programs.  Nucl. Acids Res. 25:3389-
3402. 
Andrade, M.A. (1999) Position-specific annotation of 
protein function based on multiple homologs. ISMB 
28-33. 
Andrade, M.A. and A. Valencia (1997) Automatic an-
notation for biological sequences by extraction of 
keywords from MEDLINE abstracts. Development 
of a prototype system. ISMB 25-32. 
Azuaje F., H. Wang and O. Bodenreider (2005) Ontol-
ogy-driven similarity approaches to supporting gene 
functional assessment. In Proceedings of the 
ISMB'2005 SIG meeting on Bio-ontologies 2005, 
pages 9-10.  
Bodenreider, O., M. Aubry and A. Burgun (2005) Non-
lexical approaches to identifying associative relations 
in the Gene Ontology. In Proceedings of Pacific 
Symposium on Biocomputing, pages 104-115. 
Buckley, C. (1985) Implementation of the SMART in-
formation retrieval system. Technical Report 85-686, 
Cornell University. 
Budanitsky, A. (1999) Lexical semantic relatedness and 
its application in natural language processing. Tech-
nical report CSRG-390, Department of Computer 
Science, University of Toronto. 
Chang, J.T., S. Raychaudhuri, and R.B. Altman (2001) 
Including biological literature improves homology 
search. In Proc. Pacific Symposium on Biocomput-
ing, pages 374?383. 
Couto, F. M., M. J. Silva and P. Coutinho (2003) Im-
plementation of a functional semantic similarity 
measure between gene-products. Technical Report, 
Department of Informatics, University of Lisbon, 
http://www.di.fc.ul.pt/tech-reports/03-29.pdf.  
Delfs, R., A. Doms, A. Kozlenkov, and M. Schroeder. 
(2004) GoPubMed: ontology based literature search 
applied to Gene Ontology and PubMed. In Proc. of 
German Bioinformatics Conference, Bielefeld, Ger-
many. LNBI Springer. 
Doms, A. and M. Schroeder (2005) GoPubMed: Explor-
ing PubMed with the GeneOntology. Nucleic Acids 
Research. 33: W783-W786; doi:10.1093/nar/gki470. 
Jiang J. and D. Conrath (1997) Semantic similarity 
based on corpus statistics and lexical taxonomy. In 
Proceedings of International Conference on Re-
search in Computational Linguistics, Taiwan. 
Romeijn, E.H. (1992) Global Optimization by Random 
Walk Sampling Methods. Tinbergen Institute Re-
search Series, Volume 32. Thesis Publishers, Am-
sterdam. 
Lin, D. (1998) An information-theoretic definition of 
similarity. In Proceedings of the 15th International 
Conference on Machine Learning, Madison, WI. 
Lord P.W., R.D. Stevens, A. Brass, and C.A. Goble 
(2002) Investigating semantic similarity measures 
across the Gene Ontology: the relationship between 
sequence and annotation. Bioinformatics 
19(10):1275-1283. 
Lord P.W., R.D. Stevens, A. Brass, and C.A. Goble 
(2003) Semantic similarity measures as tools for ex-
ploring the Gene Ontology. In Proceedings of Pacific 
Symposium on Biocomputing, pages 601-612. 
MacCallum, R. M., L. A. Kelley and Sternberg, M. J. 
(2000) SAWTED: structure assignment with text de-
scription--enhanced detection of remote homologues 
with automated SWISS-PROT annotation compari-
sons. Bioinformatics 16, 125-9. 
Pearson, W. R. and D. J. Lipman (1988) Improved tools 
for biological sequence analysis. In Proceedings of 
the National Academy of Sciences 85:2444-2448.  
Pedersen, T., S. Banerjee and S. Patwardhan (2005) 
Maximizing Semantic Relatedness to Perform Word 
Sense Disambiguation. University of Minnesota Su-
percomputing Institute Research Report UMSI 
2005/25, March. Available at http://www.msi.umn. 
edu/general/Reports/rptfiles/2005-25.pdf.  
Posse, C., A. Sanfilippo, B. Gopalan, R. Riensche, N. 
Beagley, and B. Baddeley (2006) Cross-Ontological 
Analytics: Combining associative and hierarchical re-
lations in the Gene Ontologies to assess gene product 
similarity. To appear in Proceedings of International 
31
Workshop on Bioinformatics Research and Applica-
tions. Reading, U.K. 
Pustejovsky, J., J. Casta?o, R. Saur?, A. Rumshisky, J. 
Zhang, W. Luo (2002) Medstract: Creating large-
scale information servers for biomedical libraries. 
ACL 2002 Workshop on Natural Language Process-
ing in the Biomedical Domain. Philadelphia, PA. 
Resnik, P. (1995) Using information content to evaluate 
semantic similarity. In Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence, 
pages 448?453, Montreal. 
Sanfilippo A., C. Posse and B. Gopalan (2004) Aligning 
the Gene Ontologies. In Proceedings of the Stan-
dards and Ontologies for Functional Genomics Con-
ference 2, Philadelphia, PA, http://www.sofg.org/ 
meetings/sofg2004/Sanfilippo.ppt.  
Salton, G., A. Wong and C. S. Yang (1975) A Vector 
space model for automatic indexing, CACM 
18(11):613-620. 
Smith, T. and M. S. Waterman (1981) Identification of 
common molecular subsequences. J. Mol. Biol. 
147:195-197. 
Weisberg, S. (2005) Applied linear regression. Wiley, 
New York. 
Yang, Y. and J.O. Pedersen (1997) A comparative 
Study on feature selection in text categorization. In 
Proceedings of the 14th International Conference on 
Machine Learning (ICML), pages 412-420, Nash-
ville.  
 
32
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 264?267,
Prague, June 2007. c?2007 Association for Computational Linguistics
PNNL: A Supervised Maximum Entropy Approach to Word Sense 
Disambiguation
Stephen Tratz, Antonio Sanfilippo, Michelle Gregory, Alan Chappell, Christian 
Posse, Paul Whitney
Pacific Northwest National Laboratory
902 Battelle Blvd, PO Box 999
Richland, WA 99352, USA
{stephen.tratz, antonio.sanfilippo, michelle, alan.chap-
pell, christian.posse, paul.whitney}@pnl.gov
Abstract
In  this  paper,  we  described  the  PNNL 
Word Sense Disambiguation system as ap-
plied  to  the  English  all-word  task  in  Se-
mEval 2007. We use a supervised learning 
approach,  employing  a  large  number  of 
features and using Information Gain for di-
mension  reduction.  The  rich  feature  set 
combined with a Maximum Entropy classi-
fier  produces results  that  are significantly 
better than baseline and are the highest F-
score  for  the  fined-grained  English  all-
words subtask of SemEval.
1 Introduction
Accurate  word  sense  disambiguation  (WSD)  can 
support  many  natural  language  processing  and 
knowledge management  tasks.  The  main goal  of 
the  PNNL  WSD system  is  to  support  Semantic 
Web applications, such as semantic-driven search 
and  navigation,  through  a  reliable  mapping  of 
words  in  naturally  occurring  text  to  ontological 
classes.  As described  in  Sanfilippo et  al.  (2006), 
this goal is achieved by defining a WordNet-based 
(Fellbaum,  1998)  ontology that  offers  a manage-
able set of concept classes, provides an extensive 
characterization of concept class in terms of lexical 
instances, and integrates an automated class recog-
nition algorithm. We found that the same features 
that are useful for predicting word classes are also 
useful in distinguishing individual word senses. 
Our main objective in this paper is to predict in-
dividual word senses using a large combination of 
features  including contextual,  semantic,  and syn-
tactic information. In our earlier paper (Sanfilippo 
et al, 2006), we reported that the PNNL WSD sys-
tem exceeded the performance of the best perform-
ers  for  verbs  in  the  SENSEVAL-3  English  all-
words task dataset. SemEval 2007 is our first op-
portunity  to  enter  a  word  sense  disambiguation 
competition.
2 Approach
While many unsupervised word sense disambigua-
tion systems have been created, supervised systems 
have generally produced superior  results  (Snyder 
and Palmer, 2004; Mihalcea et al, 2004). Our sys-
tem is based on a supervised WSD approach that 
uses  a  Maximum  Entropy  classifier  to  predict 
WordNet senses.
We  use  SemCor1,  OMWE 1.0  (Chklovski  and 
Mihalcea, 2002), and example sentences in Word-
Net  as  the  training  corpus.  We  utilize  the 
OpenNLP MaxEnt  implementation2 of  the  maxi-
mum  entropy  classification  algorithm  (Berger  et 
al.,  1996)  to  train  classification  models  for  each 
lemma and part-of-speech combination in the train-
ing  corpus.  These  models  are  used  to  predict 
WordNet  senses  for  words found in natural  text. 
For  lemma  and  part-of-speech  combinations  that 
are not present in the training corpus, the PNNL 
WSD system defaults to the most frequent Word-
Net sense.
2.1 Features
We use a rich set of features to predict individual 
word senses.  A large number of  features are ex-
tracted for each word sense instance in the training 
data.  Following  Dang & Palmer  (2005)  and Ko-
homban & Lee (2005), we use contextual, syntac-
tic and semantic  information to inform our word 
1
 http://www.cs.unt.edu/~rada/downloads.html. 
2
 http://maxent.sourceforge.net/.
264
sense disambiguation system. However,  there are 
significant  differences  between the specific  types 
of contextual,  syntactic  and semantic information 
we use in our system and those proposed by Dang 
& Palmer  (2005)  and Kohomban  & Lee (2005). 
More specifically,  we employ novel  features  and 
feature combinations, as described below. 
? Contextual information. The contextual infor-
mation we use includes the word under analy-
sis plus the three tokens found on each side of 
the word, within sentence boundaries. Tokens 
include both words and punctuation.
? Syntactic information. We include grammatical 
dependencies  (e.g.  subject,  object)  and  mor-
pho-syntactic  features such as part of speech, 
case, number and tense. We use the Connexor 
parser3 (Tapanainen and J?rvinen, 1997) to ex-
tract lemma information, parts of speech, syn-
tactic  dependencies,  tense,  case,  and  number 
information.  A sample output  of  a  Connexor 
parse is given in Table 1. Features are extract-
ed  for  all  tokens  that  are  related  through no 
more than 3 levels of dependency to the word 
to be disambiguated. 
? Semantic  information.  The semantic  informa-
tion  we  incorporate  includes  named  entity 
types (e.g. PERSON, LOCATION, ORGANI-
ZATION) and hypernyms. We use OpenNLP4 
and  LingPipe5 to  identify  named  entities,  re-
placing the strings identified as named entities 
(e.g., Joe Smith) with the corresponding entity 
type  (PERSON).  We also  substitute  personal 
pronouns  that  unambiguously  denote  people 
with the entity type PERSON. Numbers in the 
text  are  replaced  with  type  label  NUMBER. 
Hypernyms  are  retrieved  from WordNet  and 
added to the feature set for all noun tokens se-
lected by the contextual and syntactic rules. In 
contrast to Dang & Palmer (2005), we only in-
clude  the  hypernyms  of  the  most  frequent 
sense,  and  we  include  the  entire  hypernym 
chain (e.g. motor, machine, device, instrumen-
tality, artifact, object, whole, entity).
To address feature extraction processes specific 
to  noun and verbs,  we add the  following  condi-
tions.
3
 http://www.connexor.com/.
4
 http://opennlp.sourceforge.nt/.
5
 http://www.alias-i.com/lingpipe/.
? Syntactic  information  for  verbs.  If  the  verb 
does not have a subject, the subject of the clos-
est ancestor verb in the syntax tree is used in-
stead.
? Syntactic information for nouns. The first verb 
ancestor in the syntax tree is also used to gen-
erate features. 
? Semantic information for nouns. A feature in-
dicating whether a token is capitalized for each 
of the tokens used to generate features.
A sample of the resulting feature vectors that are 
used by the PNNL word sense disambiguation sys-
tem is presented in Table 2.
ID Word Lemma Grammatical 
Dependen-
cies
Morphosyntactic 
Features
1
2
3
4
5
6
the
engine
throbbe
d
into
life
.
the
engine
throb
into
life
.
det:>2
subj:>3
main:>0
goa:>3
pcomp:>4
@DN> %>N DET
@SUBJ %NH N NOM SG
@+FMAINV %VA V PAST
@ADVL %EH PREP
@<P %NH N NOM SG
Table 1. Connexor sample output for the sentence 
?The engine throbbed into life?.
the pre:2:the, pre:2:pos:DET, det:the, det:pos:DET, 
hassubj:det:
engine pre:1:instrumentality, pre:1:object, pre:1:artifact,
 pre:1:device, pre:1:engine, pre:1:motor, pre:1:whole, 
pre:1:entity, pre:1:machine, pre:1:pos:N, 
pre:1:case:NOM, 
pre:1:num:SG,subj:instrumentality,subj:object, subj:arti-
fact, subj:device, subj:engine, subj:motor, subj:whole, 
subj:entity, subj:machine, subj:pos:N, hassubj:, 
subj:case:NOM, subj:num:SG,
throbbed haspre:1:,haspre:2:,haspost:1:, haspost:2:, haspost:3:,
self:throb, self:pos:V, main:,throbbed, self:tense:PAST
into post:1:into, post:1:pos:PREP, goa:into, goa:pos:PREP, 
life post:2:life, post:2:state, post:2:being, post:2:pos:N, 
post:2:case:NOM, post:2:num:SG, hasgoa:, pcomp:life, 
pcomp:state, pcomp:being, pcomp:pos:N, 
hasgoa:pcomp:, goa:pcomp:case:NOM, 
goa:pcomp:num:SG
. post:3:.
Table  2. Feature  vector  for  throbbed in the sen-
tence ?The engine throbbed into life?.
As the example in Table 2 indicates, the combi-
nation of contextual, syntactic, and semantic infor-
mation types results in a large number of features. 
Inspection  of  the  training data  reveals  that  some 
features may be more important than others in es-
tablishing word sense assignment for each choice 
of word lemma. We use a feature selection proce-
265
dure to reduce the full set of features to the feature 
subset that is most relevant to word sense assign-
ment for each lemma. This practice improves the 
efficiency of our word sense disambiguation algo-
rithm. The feature selection procedure we adopted 
consists of scoring each potential feature according 
to  a  particular  feature  selection  metric,  and  then 
taking the best k features.
We choose Information Gain as our feature se-
lection metric. Information Gain measures the de-
crease in entropy when the feature is given versus 
when it is absent. Yang and Pederson (1997) report 
that  Information Gain outperformed other feature 
selection  approaches  in  their  multi-class  bench-
marks,  and  Foreman  (2003)  showed  that  it  per-
formed amongst the best for his 2-class problems. 
3 Evaluation
To evaluate our approach and feature set, we ran 
our  model  on  the  SENSEVAL-3  English  all-words 
task test data. Using data provided by the SENSE-
VAL website6, we were able to compare our results 
for  verbs  to  the  top  performers  on  verbs  alone. 
Upali S. Kohomban and Wee Sun Lee provided us 
with  the  results  file  for  the  Simil-Prime  system 
(Kohomban and Lee, 2005). As reported in Sanfil-
ippo et al (2006) and shown in table 3, our results 
for verbs rival those of top performers. We had a 
significant  improvement  (p-value<0.05)  over  the 
baseline of  52.9%, a marginal  improvement  over 
the second best performer (SenseLearner) (Mihal-
cea and Faruque, 2004), and we were as good as 
the top performer (GAMBL) (Decadt et al, 2004).7
System Precision Fraction of 
Recall
Our system 61% 22%
GAMBL 59.0% 21.3%
SenseLearner 56.1% 20.2%
Baseline 52.9% 19.1%
Table 3. Results for verb sense disambiguation on 
SENSEVAL-3 data, adapted from Sanfilippo et al 
(2006).
Since then, we have expanded our evaluation to 
all parts of speech. Table 4 provides the evaluation 
6
 http://www.senseval.org/.
7
 The 2% improvement in precision which our system 
showed as  compared to GAMBL was not statistically 
significant (p=0.21).
of our system as compared  to  the three top per-
formers on the SENSEVAL-3 data and the baseline. 
The baseline of 0.631 F-score8 was computed us-
ing the most frequent WordNet sense. The PNNL 
WSD system performs significantly better than the 
baseline (p-value<0.05) and rivals the top perform-
ers.  The performance of the PNNL WSD system 
relative to the other three systems and the baseline 
remains unchanged when the unknown sense an-
swers  (denoted  by a  ?U?)  are  excluded  from the 
evaluation.
System Precision Recall
PNNL 0.670 0.670
Simil-Prime 0.661 0.663
GAMBL 0.652 0.652
SenseLearner 0.646 0.646
Baseline 0.631 0.631
Table 4. SENSEVAL-3 English all-words.
System Recall Precision 
PNNL 0.669 0.671
GAMBL 0.651 0.651
Simil-Prime 0.644 0.657
SenseLearner 0.642 0.651
Baseline 0.631 0.631
Table 5. SENSEVAL-3 English all-words, No ?U?.
4 Experimental  results  on  SemEval  all-
words subtask
This was our first opportunity to test our model in 
a WSD competition. For this competition, we fo-
cused our efforts  on the fine-grained English all-
words task because our system was set up to per-
form fine-grained WordNet  sense  prediction.  We 
are  pleased that  our  system achieved the  highest 
score for this subtask. Our results for the SemEval 
dataset as compared to baseline are reported in Ta-
ble 6. The PNNL WSD system did not assign the 
unknown sense, ?U?, to any word instances in the 
SemEval dataset.
8
 This baseline is slightly higher than that reported by 
others (Snyder and Palmer 2004).
266
System F-score
PNNL 0.591
Baseline 0.514
p-value <0.01
Table 6. SemEval Results.
5 Discussion
Although these results are promising, there is still 
much work to be done. For example, we need to 
investigate the contribution of each feature to the 
overall performance of the system in terms of pre-
cision and recall. Such a feature sensitivity analysis 
will provide us with a better understanding of how 
the algorithm can be further improved and/or made 
more efficient by leaving out features whose con-
tribution is negligible. 
Another important point to make is that, while 
our system shows the best precision/recall results 
overall,  we  can  only  claim  statistical  relevance 
with  reference  to  the  baseline  and  results  worse 
than  baseline.  The  size  of  the  SemEval  data  set 
(N=465) is too small to establish whether the dif-
ference in precision/recall results with the other top 
systems is statistically significant. 
Acknowledgements
We would like to thank Upali  S. Kohomban and 
Wee Sun Lee for  providing us with their  SENSE-
VAL-3 English all-words task results file for Simil-
Prime. Many thanks also to Patrick Paulson, Bob 
Baddeley, Ryan Hohimer, and Amanda White for 
their  help  in  developing  the  word  class  disam-
biguation system on which the work presented in 
this paper is based.
References
Berger, A., S. Della Pietra and V. Della Pietra (1996) A 
Maximum  Entropy  Approach  to  Natural  Language 
Processing.  Computational  Linguistics,  volume  22, 
number 1, pages 39-71.
Chklovski, T. and R. Mihalcea (2002) Building a sense 
tagged corpus with open mind word expert. In  Pro-
ceedings of the ACL-02 workshop on Word sense dis-
ambiguation: recent successes and future directions.
Dang, H. T. and M. Palmer (2005) The Role of Semant-
ic Roles in Disambiguating Verb Senses. In Proceed-
ings of the 43rd Annual Meeting of the Association  
for Computational Linguistics,  Ann Arbor MI, June 
26-28, 2005. 
Decadt,  B., V. Hoste,  W. Daelemans and A. Van den 
Bosch (2004) GAMBL, genetic  algorithm optimiza-
tion of memory-based WSD. SENSEVAL-3: Third In-
ternational Workshop on the Evaluation of Systems  
for the Semantic Analysis of Text. Barcelona, Spain. 
Fellbaum,  C.,  editor.  (1998)  WordNet:  An  Electronic 
Lexical Database. MIT Press, Cambridge, MA.
Foreman, G. (2003) An Extensive Empirical Study of 
Feature  Selection  Metrics  for  Text  Classification. 
Journal  of  Machine  Learning  Research,  3,  pages 
1289-1305. 
Kohomban, U. and  W. Lee (2005) Learning semantic 
classes  for  word sense disambiguation.  In Proceed-
ings of the 43rd Annual meeting of the Association for  
Computational Linguistics, Ann Arbor, MI.
Mihalcea,  R.,  T.  Chklovski,  and  A.  Kilgarriff  (2004) 
The  SENSEVAL-3  English   Lexical  Sample  Task, 
SENSEVAL-3: Third International Workshop on the  
Evaluation of  Systems for the Semantic Analysis of  
Text. Barcelonna, Span.
Mihalcea,  R.  and  E.  Faruque   (2004)  SenseLearner: 
Minimally supervised word sense disambiguation for 
all words in open text.  SENSEVAL-3: Third Interna-
tional Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text. Barcelona, Spain.
Sanfilippo, A.,  S.  Tratz,  M. Gregory, A.  Chappell,  P. 
Whitney, C. Posse, P. Paulson, B. Baddeley, R. Hohi-
mer,  A.  White  (2006)  Automating  Ontological  An-
notation with WordNet. Proceedings to the Third In-
ternational WordNet Conference, Jan 22-26, Jeju Is-
land, Korea.
Snyder,  B.   and  M.  Palmer.  2004.  The  English  All-
Words  Task.  SENSEVAL-3:  Third  International 
Workshop on the Evaluation of  Systems for the Se-
mantic Analysis of Text. Barcelona, Spain. 
Tapanainen, P. and Timo J?rvinen (1997) A nonproject-
ive  dependency  parser.  In  Proceedings  of  the  5th 
Conference on Applied Natural Language Processing, 
pages 64?71, Washington D.C. Association for Com-
putational Linguistics.
Yang,  Y.  and  J.  O.  Pedersen  (1997)  A  Comparative 
Study on Feature Selection in Text Categorization. In 
Proceedings of the 14th International Conference on  
Machine Learning (ICML), pages 412-420, 1997.
267
Coling 2010: Poster Volume, pages 454?462,
Beijing, August 2010
What?s in a Preposition?
Dimensions of Sense Disambiguation for an Interesting Word Class
Dirk Hovy, Stephen Tratz, and Eduard Hovy
Information Sciences Institute
University of Southern California
{dirkh, stratz, hovy}@isi.edu
Abstract
Choosing the right parameters for a word
sense disambiguation task is critical to
the success of the experiments. We ex-
plore this idea for prepositions, an of-
ten overlooked word class. We examine
the parameters that must be considered in
preposition disambiguation, namely con-
text, features, and granularity. Doing
so delivers an increased performance that
significantly improves over two state-of-
the-art systems, and shows potential for
improving other word sense disambigua-
tion tasks. We report accuracies of 91.8%
and 84.8% for coarse and fine-grained
preposition sense disambiguation, respec-
tively.
1 Introduction
Ambiguity is one of the central topics in NLP. A
substantial amount of work has been devoted to
disambiguating prepositional attachment, words,
and names. Prepositions, as with most other word
types, are ambiguous. For example, the word in
can assume both temporal (?in May?) and spatial
(?in the US?) meanings, as well as others, less
easily classifiable (?in that vein?). Prepositions
typically have more senses than nouns or verbs
(Litkowski and Hargraves, 2005), making them
difficult to disambiguate.
Preposition sense disambiguation (PSD) has
many potential uses. For example, due to the
relational nature of prepositions, disambiguating
their senses can help with all-word sense disam-
biguation. In machine translation, different senses
of the same English preposition often correspond
to different translations in the foreign language.
Thus, disambiguating prepositions correctly may
help improve translation quality.1 Coarse-grained
PSD can also be valuable for information extrac-
tion, where the sense acts as a label. In a recent
study, Hwang et al (2010) identified preposition
related features, among them the coarse-grained
PP labels used here, as the most informative fea-
ture in identifying caused-motion constructions.
Understanding the constraints that hold for prepo-
sitional constructions could help improve PP at-
tachment in parsing, one of the most frequent
sources of parse errors.
Several papers have successfully addressed
PSD with a variety of different approaches (Rudz-
icz and Mokhov, 2003; O?Hara and Wiebe, 2003;
Ye and Baldwin, 2007; O?Hara and Wiebe, 2009;
Tratz and Hovy, 2009). However, while it is often
possible to increase accuracy by using a differ-
ent classifier and/or more features, adding more
features creates two problems: a) it can lead to
overfitting, and b) while possibly improving ac-
curacy, it is not always clear where this improve-
ment comes from and which features are actually
informative. While parameter studies exist for
general word sense disambiguation (WSD) tasks
(Yarowsky and Florian, 2002), and PSD accuracy
has been steadily increasing, there has been no
exploration of the parameters of prepositions to
guide engineering decisions.
We go beyond simply improving accuracy to
analyze various parameters in order to determine
which ones are actually informative. We explore
the different options for context and feature se-
1See (Chan et al, 2007) for the relevance of word sense
disambiguation and (Chiang et al, 2009) for the role of
prepositions in MT.
454
lection, the influence of different preprocessing
methods, and different levels of sense granular-
ity. Using the resulting parameters in a Maximum
Entropy classifier, we are able to improve signif-
icantly over existing results. The general outline
we present can potentially be extended to other
word classes and improve WSD in general.
2 Related Work
Rudzicz and Mokhov (2003) use syntactic and
lexical features from the governor and the preposi-
tion itself in coarse-grained PP classification with
decision heuristics. They reach an average F-
measure of 89% for four classes. This shows that
using a very small context can be effective. How-
ever, they did not include the object of the prepo-
sition and used only lexical features for classifi-
cation. Their results vary widely for the different
classes.
O?Hara and Wiebe (2003) made use of a win-
dow size of five words and features from the
Penn Treebank (PTB) (Marcus et al, 1993) and
FrameNet (Baker et al, 1998) to classify prepo-
sitions. They show that using high level fea-
tures, such as semantic roles, significantly aid dis-
ambiguation. They caution that using colloca-
tions and neighboring words indiscriminately may
yield high accuracy, but has the risk of overfit-
ting. O?Hara and Wiebe (2009) show compar-
isons of various semantic repositories as labels for
PSD approaches. They also provide some results
for PTB-based coarse-grained senses, using a five-
word window for lexical and hypernym features in
a decision tree classifier.
SemEval 2007 (Litkowski and Hargraves,
2007) included a task for fine-grained PSD (more
than 290 senses). The best participating system,
that of Ye and Baldwin (2007), extracted part-of-
speech and WordNet (Fellbaum, 1998) features
using a word window of seven words in a Max-
imum Entropy classifier. Tratz and Hovy (2009)
present a higher-performing system using a set of
20 positions that are syntactically related to the
preposition instead of a fixed window size.
Though using a variety of different extraction
methods, contexts, and feature words, none of
these approaches explores the optimal configura-
tions for PSD.
3 Theoretical Background
The following parameters are applicable to other
word classes as well. We will demonstrate their
effectiveness for prepositions.
Analyzing the syntactic elements of preposi-
tional phrases, one discovers three recurring ele-
ments that exhibit syntactic dependencies and de-
fine a prepositional phrase. The first one is the
governing word (usually a noun, verb, or adjec-
tive)2, the preposition itself, and the object of the
preposition.
Prepositional phrases can be fronted (?In May,
prices dropped by 5%?), so that the governor (in
this case the verb ?drop?) occurs later in the sen-
tence. Similarly, the object can be fronted (con-
sider ?a dessert to die for?).
In the simplest version, we can do classification
based only on the preposition and the governor or
object alone.3 Furthermore, directly neighboring
words can influence the preposition, mostly two-
word prepositions such as ?out of? or ?because
of?.
To extract the words discussed above, one can
either employ a fixed window size, (which has
to be large enough to capture the words), or se-
lect them based on heuristics or parsing informa-
tion. The governor and object can be hard to ex-
tract if they are fronted, since they do not occur in
their unusual positions relative to the preposition.
While syntactically related words improve over
fixed-window-size approaches (Tratz and Hovy,
2009), it is not clear which words contribute most.
There should be an optimal context, i.e., the small-
est set of words that achieves the best accuracy. It
has to be large enough to capture all relevant infor-
mation, but small enough to avoid noise words.4
We surmise that earlier approaches were not uti-
lizing that optimal context, but rather include a lot
of noise.
Depending on the task, different levels of sense
granularity may be used. Fewer senses increase
the likelihood of correct classification, but may in-
2We will refer to the governing word, irrespective of
class, as governor.
3Basing classification on the preposition alone is not fea-
sible, because of the very polysemy we try to resolve.
4It is not obvious how much information a sister-PP can
provide, or the subject of the superordinate clause.
455
correctly conflate prepositions. A finer granular-
ity can help distinguish nuances and better fit the
different contexts. However, it might suffer from
sparse data.
4 Experimental Setup
We explore the different context types (fixed win-
dow size vs. selective), the influence of the words
in that context, and the preprocessing method
(heuristics vs. parsing) on both coarse and fine-
grained disambiguation. We use a most-frequent-
sense baseline. In addition, we compare to the
state-of-the-art systems for both types of granu-
larity (O?Hara and Wiebe, 2009; Tratz and Hovy,
2009). Their results show what has been achieved
so far in terms of accuracy, and serve as a second
measure for comparison beyond the baseline.
4.1 Model
We use the MALLET implementation (McCal-
lum, 2002) of a Maximum Entropy classifier
(Berger et al, 1996) to construct our models. This
classifier was also used by two state-of-the-art
systems (Ye and Baldwin, 2007; Tratz and Hovy,
2009). For fine-grained PSD, we train a separate
model for each preposition due to the high num-
ber of possible classes for each individual prepo-
sition. For coarse-grained PSD, we use a single
model for all prepositions, because they all share
the same classes.
4.2 Data
We use two different data sets from existing re-
sources for coarse and fine-grained PSD to make
our results as comparable to previous work as pos-
sible.
For the coarse-grained disambiguation, we use
data from the POS tagged version of the Wall
Street Journal (WSJ) section of the Penn Tree-
Bank. A subset of the prepositional phrases in
this corpus is labelled with a set of seven classes:
beneficial (BNF), direction (DIR), extent (EXT),
location (LOC), manner (MNR), purpose (PRP),
and temporal (TMP). We extract only those prepo-
sitions that head a PP labelled with such a class
(N = 35, 917). The distribution of classes is
highly skewed (cf. Figure 1). We compare the
PTB class distrib
Page 1
LOC TMP DIR MNR PRP EXT BNF
0
2000
4000
6000
8000
10000
12000
14000
16000
18000 16995
10332
5414
1781 1071 280 44
classes
fre
qu
en
cy
Figure 1: Distribution of Class Labels in the WSJ
Section of the Penn TreeBank.
results of this task to the findings of O?Hara and
Wiebe (2009).
For the fine-grained task, we use data from
the SemEval 2007 workshop (Litkowski and Har-
graves, 2007), separate XML files for the 34 most
frequent English prepositions, comprising 16, 557
training and 8096 test sentences, each instance
containing one example of the respective prepo-
sition. Each preposition has between two and 25
senses (9.76 on average) as defined by The Prepo-
sition Project (Litkowski and Hargraves, 2005).
We compare our results directly to the findings
from Tratz and Hovy (2009). As in the original
workshop task, we train and test on separate sets.
5 Results
In this section we show experimental results for
the influence of word extraction method (parsing
vs. POS-based heuristics), context, and feature se-
lection on accuracy. Each section compares the
results for both coarse and fine-grained granular-
ity. Accuracy for the coarse-grained task is in all
experiments higher than for the fine-grained one.
5.1 Word Extraction
In order to analyze the impact of the extraction
method, we compare parsing versus POS-based
heuristics for word extraction.
Both O?Hara and Wiebe (2009) and Tratz and
Hovy (2009) use constituency parsers to prepro-
cess the data. However, parsing accuracy varies,
456
and the problem of PP attachment ambiguity in-
creases the likelihood of wrong extractions. This
is especially troublesome in the present case,
where we focus on prepositions.5 We use the
MALT parser (Nivre et al, 2007), a state-of-the-
art dependency parser, to extract the governor and
object.
The alternative is a POS-based heuristics ap-
proach. The only preprocessing step needed is
POS tagging of the data, for which we used the
system of Shen et al (2007). We then use simple
heuristics to locate the prepositions and their re-
lated words. In order to determine the governor
in the absence of constituent phrases, we consider
the possible governing noun, verb, and adjective.
The object of the preposition is extracted as first
noun phrase head to the right. This approach is
faster than parsing, but has problems with long-
range dependencies and fronting of the PP (e.g.,
the PP appearing earlier in the sentence than its
governor). word selection
Page 1
MALT 84.4 94.0
84.8 90.9
84.8 91.8
extraction method fine coarse
Heuristics
MALT + Heuristics
Table 1: Accuracies (%) for Word-Extraction Us-
ing MALT Parser or Heuristics.
Interestingly, the extraction method does not
significantly affect the final score for fine-grained
PSD (see Table 1). The high score achieved when
using the MALT parse for coarse-grained PSD
can be explained by the fact that the parser was
originally trained on that data set. The good re-
sults we see when using heuristics-based extrac-
tion only, however, means we can achieve high-
accuracy PSD even without parsing.
5.2 Context
We compare the effects of fixed window size ver-
sus syntactically related words as context. Table 2
shows the results for the different types and sizes
of contexts.6
5Rudzicz and Mokhov (2003) actually motivate their
work as a means to achieve better PP attachment resolution.
6See also (Yarowsky and Florian, 2002) for experiments
on the effect of varying window size for WSD.
context
Page 1
91.6 80.4
92.0 81.4
91.6 79.8
91.0 78.7
80.7 78.9
94.2 56.9
94.0 84.8
Context coarse fine
2-word window
3-word window
4-word window
5-word window
Governor, prep
Prep, object
Governor, prep, object
Table 2: Accuracies (%) for Different Context
Types and Sizes
The results show that the approach using both
governor and object is the most accurate one. Of
the fixed-window-size approaches, three words to
either side works best. This does not necessarily
reflect a general property of that window size, but
can be explained by the fact that most governors
and objects occur within this window size.7 This
dista ce can vary from corpus to corpus, so win-
dow size would have to be determined individu-
ally for each task. The difference between using
governor and preposition versus preposition and
object between coarse and fine-grained classifica-
tion might reflect the annotation process: while
Litkowski and Hargraves (2007) selected exam-
ples based on a search for governors8, most anno-
tators in the PTB may have based their decision
of the PP label on the object that occurs in it. We
conclude that syntactically related words present a
better context for classification than fixed window
sizes.
5.3 Features
Having established the context we want to use, we
now turn to the details of extracting the feature
words from that context.9 Using higher-level fea-
tures instead of lexical ones helps accounting for
sparse training data (given an infinite amount of
data, we would not need to take any higher-level
7Based on such statistics, O?Hara and Wiebe (2003) ac-
tually set their window size to 5.
8Personal communication.
9As one reviewer pointed out, these two dimensions are
highly interrelated and influence each other. To examine the
effects, we keep one dimension constant while varying the
other.
457
features into account, since every case would be
covered). Compare O?Hara and Wiebe (2009).
Following the prepocessing, we use a set of
rules to select the feature words, and then gen-
erate feature values from them using a variety
of feature-generating functions.10 The word-
selection rules are listed below.
Word-Selection Rules
? Governor from the MALT parse
? Object from the MALT parse
? Heuristically determined object of the prepo-
sition
? First verb to the left of the preposition
? First verb/noun/adjective to the left of the
preposition
? Union of (First verb to the left, First
verb/noun/adjective to the left)
? First word to the left
The feature-generating functions, many of
which utilize WordNet (Fellbaum, 1998), are
listed below. To conserve space, curly braces are
used to represent multiple functions in a single
line. The name of each feature is the combination
of the word-selection rule and the output from the
feature-generating function.
WordNet-based Features
? {Hypernyms, Synonyms} for {1st, all}
sense(s) of the word
? All terms in the definitions (?glosses?) of the
word
? Lexicographer file names for the word
? Lists of all link types (e.g., meronym links)
associated with the word
? Part-of-speech indicators for the existence of
NN/VB/JJ/RB entries for the word
? All sentence frames for the word
? All {part, member, substance}-of holonyms
for the word
? All sentence frames for the word
Other Features
? Indicator that the word-finding rule found a
word
10Some words may be selected by multiple word-selection
rules. For example, the governor of the preposition may
be identified by the Governor from MALT parse rule, first
noun/verb/adjective to left, and the first word to the left rule.
? Capitalization indicator
? {Lemma, surface form} of the word
? Part-of-speech tag for the word
? General POS tag for the word (e.g. NNS?
NN, VBZ? VB)
? The {first, last} {two, three} letters of each
word
? Indicators for suffix types (e.g., de-
adjectival, de-nominal [non]agentive,
de-verbal [non]agentive)
? Indicators for a wide variety of other affixes
including those related to degree, number, or-
der, etc. (e.g., ultra-, poly-, post-)
? Roget?s Thesaurus divisions for the word
To establish the impact of each feature word on
the outcome, we use leave-one-out and only-one
evaluation.11 The results can be found in Table 3.
A word that does not perform well as the only at-
tribute may still be important in conjunction with
others. Conversely, leaving out a word may not
hurt performance, despite being a good single at-
tribute. word selection
Page 1
Word LOO LOO
92.1 80.1 84.3 78.9
93.4 94.2 84.9 56.3
92.0 77.9 85.0 62.1
92.1 78.7 84.3 78.5
92.1 78.4 84.5 81.0
92.0 78.8 84.4 77.2
91.9 93.0 84.9 56.8
91.8 ? 84.8 ?
coarse fine
Only Only
MALT governor
MALT object
Heuristics VB to left
Heur. NN/VB/ADJ to left
Heur. Governor Union
Heuristics word to left
Heuristics object
none
Table 3: Accuracies (%) for Leave-One-
Out (LOO) and Only-One Word-Extraction-Rule
Evaluation. none includes all words and serves for
comparison. Important words reduce accuracy for
LOO, but rank high when used as only rule.
Independent of the extraction method (MALT
parser or POS-based heuristics), the governor is
the most informative word. Combining several
heuristics to locate the governor is the best sin-
gle feature for fine-grained classification. The rule
looking only for a governing verb fails to account
11Since the feature words are not independent of one an-
other, neither of the two measures is decisive on its own.
458
full both
Page 1
Total Total Total Total
? ? 6 100.0 125 90.4 53 47.2
364 94.0 5 80.0 ? ? 74 93.2
23 69.6 78 65.4 ? ? 1 0.0
151 96.7 87 79.3 ? ? 7 71.4
53 79.2 841 92.5 of 1478 87.9 71 64.8
92 92.4 16 43.8 76 84.2 28 75.0
173 96.0 45 71.1 441 81.4 2287 90.8
? ? 5 80.0 58 91.4 15 53.3
? ? 58 70.7 out ? ? 90 68.9
50 80.0 358 93.9 ? ? 62 90.3
? ? 1 0.0 98 79.6 417 89.4
155 69.0 107 86.0 ? ? 6 83.3
84 100.0 232 84.5 per ? ? 3 100.0
? ? 2 50.0 82 65.9 ? ?
367 86.4 3078 92.0 ? ? 449 94.4
? ? 5 100.0 ? ? 2 0.0
? ? 420 91.7 208 48.1 364 69.0
20 90.0 384 83.3 ? ? 62 93.5
68 77.9 65 87.7 ? ? 3 100.0
? ? 94 71.3 to 572 89.7 3166 97.5
28 78.6 11 72.7 ? ? 55 65.5
29 100.0 4 100.0 102 97.1 2 100.0
? ? 1 0.0 ? ? 604 91.4
102 94.1 98 84.7 ? ? 2 50.0
? ? 45 64.4 ? ? 208 94.2
248 88.3 1341 87.5 up ? ? 20 75.0
down 153 81.7 16 56.2 ? ? 23 73.9
39 87.2 547 92.1 via ? ? 22 40.9
? ? 1 0.0 ? ? 1 100.0
478 82.4 1455 84.5 ? ? 3 33.3
578 85.5 1712 90.5 578 84.4 272 69.5
in 688 77.0 15706 95.0 ? ? 213 96.2
38 73.7 24 91.7 ? ? 69 63.8
297 86.2 415 80.0
Overall 8096 84.8 35917 91.8
fine coarse fine coarse
Prep Acc Acc Prep Acc Acc
aboard like
about near
above nearest
across next
after
against off
along on
alongside onto
amid
among outside
amongst over
around past
as
astride round
at since
atop than
because through
before throughout
behind till
below
beneath toward
beside towards
besides under
between underneath
beyond until
by
upon
during
except whether
for while
from with
within
inside without
into
Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by
preposition.
for noun governors, which consequently leads to
a slight improvement when left out.
Curiously, the word directly to the left is a bet-
ter single feature than the object (for fine-grained
classification). Leaving either of them out in-
creases accuracy, which implies that their infor-
mation can be covered by other words.
459
coarse both 2009
Page 1
Most Frequent Sense
f1 f1 f1
LOC 71.8 97.4 82.6 90.8 93.2 92.0 94.7 96.4 95.6
TMP 77.5 39.4 52.3 84.5 85.2 84.8 94.6 94.6 94.6
DIR 91.6 94.2 92.8 95.6 96.5 96.1 94.6 94.5 94.5
MNR 69.9 43.2 53.4 82.6 55.8 66.1 83.3 75.0 78.9
PRP 78.2 48.8 60.1 79.3 70.1 74.4 90.6 83.8 87.1
EXT 0.0 0.0 0.0 81.7 84.6 82.9 87.5 82.1 84.7
BNF 0.0 0.0 0.0 ? ? ? 75.0 34.1 46.9
O'Hara/Wiebe 2009 10-fold CV
Class prec rec prec rec prec rec
Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification. Comparison to O?Hara
and Wiebe (2009). Classes ordered by frequency
5.4 Comparison with Related Work
To situate our experimental results within the
body of work on PSD, we compare them to both
a most-frequent-sense baseline and existing work
for both granularities (see Table 6). The results
use a syntactically selective context of preposi-
tion, governor, object, and word to the left as
determined by combined extraction information
(POS tagging and parsing).
accuracies
Page 1
75.8 39.6  
89.3* 78.3**
93.9 84.8  
coarse fine
Baseline
Related Work
Our system
Table 6: Accuracies (%) for Different Classifi-
cations. Comparison with O?Hara and Wiebe
(2009)*, and Tratz and Hovy (2009)**.
Our system easily exceeds the baseline for both
coarse and fine-grained PSD (see Table 6). Com-
parison with related work shows that we achieve
an improvement of 6.5% over Tratz and Hovy
(2009), which is significant at p < .0001, and
of 4.5% over O?Hara and Wiebe (2009), which is
significant at p < .0001.
A detailed overview over all prepositions for
frequencies and accuracies of both coarse and
fine-grained PSD can be found in Table 4.
In addition to overall accuracy, O?Hara and
Wiebe (2009) also measure precision, recall and
F-measure for the different classes. They omitted
BNF because it is so infrequent. Due to different
training data and models, the two systems are not
strictly comparable, yet they provide a sense of
the general task difficulty. See Table 5. We note
that both systems perform better than the most-
frequent-sense baseline. DIR is reliably classified
using the baseline, while EXT and BNF are never
selected for any preposition. Our method adds
considerably to the scores for most classes. The
low score for BNF is mainly due to the low num-
ber of instances in the data, which is why it was
excluded by O?Hara and Wiebe (2009).
6 Conclusion
To get maximal accuracy in disambiguating
prepositions?and also other word classes?one
needs to consider context, features, and granular-
ity. We presented an evaluation of these parame-
ters for preposition sense disambiguation (PSD).
We find that selective context is better than
fixed window size. Within the context for prepo-
sitions, the governor (head of the NP or VP gov-
erning the preposition), the object of the prepo-
sition (i.e., head of the NP to the right), and the
word directly to the left of the preposition have
the highest influence.12 This corroborates the lin-
guistic intuition that close mutual constraints hold
between the elements of the PP. Each word syn-
tactically and semantically restricts the choice of
the other elements. Combining different extrac-
tion methods (POS-based heuristics and depen-
dency parsing) works better than either one in iso-
lation, though high accuracy can be achieved just
using heuristics. The impact of context and fea-
tures varies somewhat for different granularities.
12These will likely differ for other word classes.
460
Not surprisingly, we see higher scores for coarser
granularity than for the more fine-grained one.
We measured success in accuracy, precision, re-
call, and F-measure, and compared our results to
a most-frequent-sense baseline and existing work.
We were able to improve over state-of-the-art sys-
tems in both coarse and fine-grained PSD, achiev-
ing accuracies of 91.8% and 84.8% respectively.
Acknowledgements
The authors would like to thank Steve DeNeefe,
Victoria Fossum, and Zornitsa Kozareva for com-
ments and suggestions. StephenTratz is supported
by a National Defense Science and Engineering
fellowship.
References
Baker, C.F., C.J. Fillmore, and J.B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
Berger, A.L., V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
Chan, Y.S., H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In Annual Meeting ? Association For Com-
putational Linguistics, volume 45, pages 33?40.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 218?226, Boulder, Colorado, June.
Association for Computational Linguistics.
Fellbaum, C. 1998. WordNet: an electronic lexical
database. MIT Press USA.
Hwang, J. D., R. D. Nielsen, and M. Palmer. 2010.
Towards a domain independent semantics: Enhanc-
ing semantic representation with construction gram-
mar. In Proceedings of the NAACL HLT Workshop
on Extracting and Using Constructions in Computa-
tional Linguistics, pages 1?8, Los Angeles, Califor-
nia, June. Association for Computational Linguis-
tics.
Litkowski, K. and O. Hargraves. 2005. The preposi-
tion project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Litkowski, K. and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation of Preposi-
tions. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Marcus, M.P., M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn TreeBank. Computational Linguis-
tics, 19(2):313?330.
McCallum, A.K. 2002. MALLET: A Machine Learn-
ing for Language Toolkit. 2002. http://mallet. cs.
umass. edu.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
O?Hara, T. and J. Wiebe. 2003. Preposition semantic
classification via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79?86.
O?Hara, T. and J. Wiebe. 2009. Exploiting seman-
tic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
Rudzicz, F. and S. A. Mokhov. 2003. To-
wards a heuristic categorization of prepo-
sitional phrases in english with word-
net. Technical report, Cornell University,
arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Shen, L., G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, volume 45, pages
760?767.
Tratz, S. and D. Hovy. 2009. Disambiguation of
preposition sense using linguistically motivated fea-
tures. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Student Re-
search Workshop and Doctoral Consortium, pages
96?100, Boulder, Colorado, June. Association for
Computational Linguistics.
Yarowsky, D. and R. Florian. 2002. Evaluating sense
disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
461
Ye, P. and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Fea-
tures. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
462
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257?1268,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Fast, Accurate, Non-Projective, Semantically-Enriched Parser
Stephen Tratz and Eduard Hovy
Information Sciences Institute
University of Southern California
Marina del Rey, California 90292
{stratz,hovy}@isi.edu
Abstract
Dependency parsers are critical components
within many NLP systems. However, cur-
rently available dependency parsers each ex-
hibit at least one of several weaknesses, in-
cluding high running time, limited accuracy,
vague dependency labels, and lack of non-
projectivity support. Furthermore, no com-
monly used parser provides additional shal-
low semantic interpretation, such as prepo-
sition sense disambiguation and noun com-
pound interpretation. In this paper, we present
a new dependency-tree conversion of the Penn
Treebank along with its associated fine-grain
dependency labels and a fast, accurate parser
trained on it. We explain how a non-projective
extension to shift-reduce parsing can be in-
corporated into non-directional easy-first pars-
ing. The parser performs well when evalu-
ated on the standard test section of the Penn
Treebank, outperforming several popular open
source dependency parsers; it is, to the best
of our knowledge, the first dependency parser
capable of parsing more than 75 sentences per
second at over 93% accuracy.
1 Introduction
Parsers are critical components within many natu-
ral language processing (NLP) systems, including
systems for information extraction, question answer-
ing, machine translation, recognition of textual en-
tailment, summarization, and many others. Unfortu-
nately, currently available dependency parsers suf-
fer from at least one of several weaknesses includ-
ing high running time, limited accuracy, vague de-
pendency labels, and lack of non-projectivity sup-
port. Furthermore, few parsers include any sort of
additional semantic interpretation, such as interpre-
tations for prepositions, possessives, or noun com-
pounds.
In this paper, we describe 1) a new dependency
conversion (Section 3) of the Penn Treebank (Mar-
cus, et al, 1993) along with the associated de-
pendency label scheme, which is based upon the
Stanford parser?s popular scheme (de Marneffe and
Manning, 2008), and a fast, accurate dependency
parser with non-projectivity support (Section 4) and
additional integrated semantic annotation modules
for automatic preposition sense disambiguation and
noun compound interpretation (Section 5). We show
how Nivre?s (2009) swap-based reordering tech-
nique for non-projective shift-reduce-style parsing
can be integrated into the non-directional easy-first
framework of Goldberg and Elhadad (2010) to sup-
port non-projectivity, and we report the results of our
parsing experiments on the standard test section of
the PTB, providing comparisons with several freely
available parsers, including Goldberg and Elhadad?s
(2010) implementation, MALTPARSER (Nivre et al,
2006), MSTPARSER (McDonald et al, 2005; Mc-
Donald and Pereira, 2006), the Charniak (2000)
parser, and the Berkeley parser (Petrov et al, 2006;
Petrov and Klein, 2007).
The experimental results show that the parser is
substantially more accurate than Goldberg and El-
hadad?s original implementation, with fairly simi-
lar overall speed. Furthermore, the results prove
that Stanford-granularity dependency labels can be
learned by modern dependency parsing systems
when using our Treebank conversion, unlike the
Stanford conversion, for which Cer et al (2010)
show that this isn?t the case.
The optional semantic annotation modules also
1257
perform well, with the preposition sense disam-
biguation module exceeding the accuracy of the pre-
vious best reported result for fine-grained preposi-
tion sense disambiguation (85.7% vs Hovy et al?s
(2010) 84.8%), the possessives interpretation sys-
tem achieving over 85% accuracy, and the noun
compound interpretation system performing simi-
larly to an earlier version described by Tratz and
Hovy (2010) at just over 79% accuracy.
2 Background
The NLP community has recently seen a surge of
interest in dependency parsing, with several CoNLL
shared tasks focusing on it (Buchholz and Marsi,
2006; Nivre et al, 2007). One of the main advan-
tages of dependency parsing is the relative ease with
which it can handle non-projectivity1. Additionally,
since each word is linked directly to its head via a
link that, ideally, indicates the syntactic dependency
type, there is no difficulty in determining either the
syntactic head of a particular word or the syntactic
relation type, whereas these issues often arise when
dealing with constituent parses2.
Unfortunately, most currently available depen-
dency parsers produce relatively vague labels or, in
many cases, produce no labels at all. While the
Stanford fine-grain dependency scheme (de Marn-
effe and Manning, 2008) has proven to be popular,
recent experiments by Cer et al (2010) using the
Stanford conversion of the Penn Treebank indicate
that it is difficult for current dependency parsers to
learn. Indeed, the highest scoring parsers trained us-
ing the MSTPARSER (McDonald and Pereira, 2006)
and MALTPARSER (Nivre et al, 2006) parsing suites
achieved only 78.8 and 81.1 labeled attachment
F1, respectively. This contrasted with the much
higher performance obtained using a constituent-to-
dependency conversion approach with accurate, but
much slower, constituency parsers such as the Char-
niak and Johnson (2005) and Berkeley (Petrov et
al., 2006; Petrov and Klein, 2007) parsers, which
achieved 89.1 and 87.9 labeled F1 scores, respec-
tively.
1A tree is non-projective if the sequence of words visited in
a left-to-right, depth-first traversal of the sentence?s parse tree is
different than the actual word order of the sentence.
2These latter two issues are not problems for constituent
parses with binarized output and functional tags.
Though there are many syntactic parsers than can
reconstruct the grammatical structure of a text, there
are few, if any, accurate and widely accepted sys-
tems that also produce shallow semantic analysis of
the text. For example, a parser may indicate that,
in the case of ?ice statue?, ?ice? modifies ?statue? but
will not indicate that ?ice? is the substance of the
statue. Similarly, a parser will indicate which words
a preposition connects but will not give any seman-
tic interpretation (e.g., ?the boy with the pirate hat?
? wearing or carrying, ?wash with cold water? ?
means, ?shave with the grain? ? in the same direc-
tion as). While, in some cases, it may be possible to
use the output from a separate system for this pur-
pose, doing so is often difficult in practice due to a
wide variety of complications, including program-
ming language differences, alternative data formats,
and, sometimes, other parsers.
3 Dependency Conversion
3.1 Relations and Structure
Most recent English dependency parsers produce
one of three sets of dependency types: unlabeled,
some variant of the coarse labels used by the
CoNLL dependency parsing shared-tasks (Buchholz
and Marsi, 2006; Nivre et al, 2007) (e.g., ADV,
NMOD, PMOD), or Stanford?s dependency labels
(de Marneffe and Manning, 2008). Unlabeled de-
pendencies are clearly too impoverished for many
tasks. Similarly, the coarse labels of the CoNLL
tasks are not very specific; for example, the same re-
lation, NMOD, is used for determiners, adjectives,
nouns, participle modifiers, relative clauses, etc. that
modify nouns. In contrast, the Stanford relations
provide a more reasonable level of granularity.
Our dependency relation scheme is similar to
Stanford?s basic scheme but has several differ-
ences. It introduces several new relations including
ccinit ?initial coordinating conjunction?, cleft ?cleft
clause?, combo ?combined term?, extr ?extraposed
element?, infmark ?infinitive marker ?to? ?, objcomp
?object complement?, postloc ?post-modifying lo-
cation?, sccomp ?clausal complement of ?so? ?, vch
?verbal chain? and whadvmod ?wh- adverbial mod-
ifier?. The nsubjpass, csubjpass, and auxpass rela-
tions of Stanford?s are left out because adding them
up front makes learning more difficult and the fact
1258
abbrev abbreviation csubjpass clausal subject (passive) pobj prepositional object
acomp adjectival complement det determiner poss possessive
advcl adverbial clause dobj direct object possessive possessive marker
advmod adverbial modifier extr extraposed element postloc post-modifying location
agent ?by? agent expl ?there? expletive preconj pre conjunct
amod adjectival modifier infmark infinitive marker (?to?) predet predeterminer
appos appositive infmod infinite modifier prep preposition
attr attributive iobj indirect object prt particle
aux auxillary mark subordinate clause marker punct punctuation
auxpass auxillary (passive) measure measure modifier purpcl purpose clause
cleft cleft clause neg negative quantmod quantifier modifier
cc coordination nn noun compound rcmod relative clause
ccinit initial CC nsubj nominal subject rel relative
ccomp clausal complement nsubjpass nominal subject (passive) sccomp clausal complement of ?so?
combo combination term num numeric modifier tmod temporal modifier
compl complementizer number compound number vch verbal chain
conj conjunction objcomp object complement whadvmod wh- adverbial
cop copula complement parataxis parataxis xcomp clausal complement w/o subj
csubj clausal subject partmod participle modifier
Table 1: Dependency scheme with differences versus basic Stanford dependencies highlighted. Bold indicates the
relation does not exist in the Stanford scheme. Italics indicate the relation appears in Stanford?s scheme but not ours.
that a nsubj, csubj, or aux is passive can easily be de-
termined from the final tree. Stanford?s aux depen-
dencies are replaced using verbal chain (vch) links;
conversion of these to Stanford-style aux dependen-
cies is also trivial as a post-processing step.3 The attr
dependency is excluded because it is redundant with
the cop relation due to different handling of copula,
and the dependency scheme does not have an abbrev
label because this information is not provided by the
Penn Treebank. The dependency scheme with dif-
ferences with Stanford highlighted is presented in
Table 1.
In addition to using a slightly different set of de-
pendency names, a handful of relations, notably cop,
conj, and cc, are treated in a different manner. These
differences are illustrated by Figure 1. The Stan-
ford scheme?s treatment of copula may be one rea-
son why dependency parsers have trouble learning
and applying it. Normally, the head of the clause
is a verb, but, under Stanford?s scheme, if the verb
happens to be a copula, the complement of the cop-
ula (cop) is treated as the head of the clause instead.
3The parsing system includes an optional script that can con-
vert vch arcs into aux and auxpass and the subject relations into
csubjpass and nsubjpass.
Figure 1: Example comparing Stanford?s (top) handling
of copula and coordinating conjunctions with ours (bot-
tom).
3.2 Conversion Process
A three-step process is used to convert the Penn
Treebank (Marcus, et al, 1993) from constituent
parses into dependency trees labeled according to
the dependency scheme presented in the prior sec-
tion. The first step is to apply the noun phrase
structure patch created by Vadas and Curran (2007),
which adds structure to the otherwise flat noun
phrases (NPs) of the Penn Treebank (e.g., ?(metal
soup pot cover)? would become ?(metal (soup pot)
cover)?). The second step is to apply a version
of Johansson and Nugues? (2007) constituent-to-
dependency converter with some head-finding rule
modifications; these rules, with changes highlighted
1259
(WH)?NP|NX|NML|NAC FW|NML|NN* JJR $|# CD|FW QP JJ|NAC JJS PRP ADJP RB[SR] VBG|DT|WP
RB NP- S|SBAR|UCP|PP SINV|SBARQ|SQ UH VP|NP VB|VBP
ADJP|JJP NNS QP NN $|# JJ VBN VBG (AD|J)JP ADVP JJR NP|NML JJS DT FW RBR RBS SBAR RB
ADVP RB|RBR|JJ|JJR RBS FW ADVP TO CD IN NP|NML JJS NN
PRN S* VP NN*|NX|NML NP W* PP|IN ADJP|JJ ADVP RB NAC VP INTJ
QP $|# NNS NN CD JJ RB DT NCD QP IN CC JJR JJS
SBARQ SQ S SBARQ SINV FRAG
SQ VBZ VBD VBP VB MD *-PRD SQ VP FRAG X
UCP [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X
VP VBD|AUX VBN MD VBZ VB VBG VBP VP POS *-PRD ADJP JJ NN NNS NP|NML
WHADJP CC JJ WRB ADJP
WHADVP CC WRB|RB
X [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X|CONJP
LST LS : DT|NN|SYM
Figure 2: Modified head-finding rules. Underline indicates that the search is performed in a left-to-right fashion instead
of the default right-to-left order. NML and JJP are both products of Vadas and Curran?s (2007) patch. Bold indicates
an added or moved element; for the original rules, see the paper by Johansson and Nugues (2007).
in bold, are provided in Figure 2. Finally, an addi-
tional script makes additional changes and converts
the intermediate output into the dependency scheme.
This dependency conversion has several advan-
tages to it. Using the modified head-finding rules for
Johansson and Nugues? (2007) converter results in
fewer buggy trees than were present in the CoNLL
shared tasks, including fewer trees in which words
are headed by punctuation marks. For sections 2?
21, there are far fewer generic dep/DEP relations
(2,765) than with the Stanford conversion (34,134)
or the CoNLL 2008 shared task conversion (23,811).
Also, the additional conversion script contains vari-
ous rules for correcting part-of-speech (POS) errors
using the syntactic structure as well as additional
rules for some specific word forms, mostly common
words with inconsistent taggings. Many of these
changes cover part-of-speech problems discussed by
Manning (2011), including VBD/VBN, VBZ/NNS,
NNP/NNPS, and IN/WDT/DT issues. In total, the
script changes over 9,500 part-of-speech tags, with
the most common change being to change preposi-
tion tags (IN) into adverb tags (RB) for cases where
there is no prepositional complement/object. The
top fifteen of these changes are presented in Table
2. The conversion script contains a variety of ad-
ditional rules for modifying the parse structure and
fixing erroneous trees as well, including cases where
one or more POS tags were incorrect and, as such,
the initial dependency parse was flawed. Quick
manual inspections of the changes suggested that the
vast majority are accurate.
In the final output from the conversion, the num-
ber of sentences with one or more words dependent
on non-projective arcs in sections 2?21 is 3,245?
about 8.1% of the dataset. About 1.3% of this, or
556 of sentences, is due to the secondary conver-
sion script, with sentences containing approximate
currency amounts (e.g., about $ 10) comprising the
bulk of difference. For these, the quantifying text
(e.g., about, over, nearly), is linked to the number
following the currency symbol instead of to the cur-
rency symbol as it was in the CoNLL 2008 task.
Original New # of changes
IN RB 1128
JJ NN 787
VBD VBN 601
RB IN 462
VBN VBD 441
NN JJ 409
NNPS NNP 405
IN WDT 388
VBG NN 223
DT IN 220
RB JJ 214
VB VBP 184
NN NNS 169
RB NN 157
NNS VBZ 148
Table 2: Top 15 part-of-speech tag changes performed by
the conversion script.
1260
4 Parser
4.1 Algorithm
The parsing approach is based upon the non-
directional easy-first algorithm recently presented
by Goldberg and Elhadad (2010). Their original al-
gorithm behaves as follows. For a sentence of length
n, the algorithm performs a total of n steps. In each
step, one of the unattached tokens is added as a child
to one of its current neighbors and is then removed
from the list of unprocessed tokens. When only one
token remains unprocessed, it is designated as the
root. Provided that only a constant number of po-
tential attachments need to be re-evaluated after each
step, which is the case if one restricts the context for
feature generation to a constant number of neigh-
boring tokens, the algorithm can be implemented to
run in O(n log n). However, since only O(n) dot
products must be calculated by the parser and these
have a large constant associated with them, the run-
ning time will rival O(n) parsers for any reasonable
n, and, thus, a naive O(n2) implementation will be
nearly as fast as a priority queue implementation in
practice.4
The algorithm has a couple potential advantages
over standard shift-reduce style parsing algorithms.
The first advantage is that performing easy ac-
tions first may make the originally difficult deci-
sions easier. The second advantage is that perform-
ing parse actions in a more flexible order than left-
to-right/right-to-left shift-reduce parsing reduces the
chance of error propagation.
Unfortunately, the original algorithm does not
support non-projective trees. To extend the algo-
rithm to support non-projective trees, we introduce
move-right and move-left operations similar to the
stack-to-buffer swaps proposed by Nivre (2009) for
shift-reduce style parsing. Thus, instead of attaching
a token to one of its neighbors at each step, the algo-
rithm may instead decide to move a token past one
of its neighbors. Provided that no node is allowed
to be moved past a token in such a way that a previ-
ous move operation is undone, there can be at most
O(n2) moves and the overall worst-case complexity
becomes O(n2 log n). While theoretically slower,
this has a limited impact upon actual parsing times
4See Goldberg and Elhadad (2010) for more explanation.
in practice, especially for languages with relatively
fixed word order such as English.5 Though Gold-
berg and Elhadad?s (2010) original implementation
only supports unlabeled dependencies, the algorithm
itself is in no way limited in this regard, and it is
simple enough to add labeled dependency support
by treating each dependency label as a specific type
of attach operation (e.g., attach_as_nsubj), which
is the method used by this implementation. Pseu-
docode for the non-directional easy-first algorithm
with non-projective support is given in Algorithm 1.
input : w1 ... wn, #the sentence
m, #the model
k, #the context width
actions, #the list of parse actions
?, #the feature generator
output: tree #a collection of dependency arcs
words = copyOf(s);
stale = copyOf (s);
cache; #cache of action scores
while |words| > 1 do
for w ? stale do
for act ? actions do
cache[w,act] = score(act, ?(w,...),
m);
stale.remove(w);
best = argmax
a?actions&valid(a),w?words
cache[w, a]
if isMove(best) then
i =
words.index(getTokenToMove(best));
words.move (i, isMoveLeft(best) ? -1
: 1);
else
arc = createArc(best);
tree.add(arc);
i = words.index(getChild(arc));
words.remove(i);
for x ? -k,...,k do
stale.add(words.get(index+x));return tree
Algorithm 1: Modified version of Goldberg and
Elhadad?s (2010) Easy-First Algorithm with non-
projective support.
5See Nivre (2009) for more information on the effect of re-
ordering operations on parse time.
1261
4.2 Features
One of the key aspects of the parser is the complex
set of features used. The feature set is based off
the features used by Goldberg and Elhadad (2010)
but has a significant number of extensions. Various
feature templates are specifically designed to pro-
duce features that help with several syntactic issues
including preposition attachment, coordination, ad-
verbial clauses, clausal complements, and relative
clauses. Unfortunately, there is insufficient space in
this paper to describe them all here. However, a list
of feature templates will be provided with the parser
download.
Several of the feature templates use unsupervised
word clusters created with the Brown et al (1992)
hierarchical clustering algorithm. The use of this al-
gorithm was inspired by Koo et al (2008), who used
the top branches of the cluster hierarchy as features.
However, unlike Koo et al?s (2008) parser, the fine-
grained cluster identifiers are used instead of just
the top 4-6 branches of the cluster hierarchy. The
175 word clusters utilized by the parser were created
from the New York Times corpus (Sandhaus, 2008).
Some examples from the clusters are presented in
Figure 3. The ideal number of such clusters was not
thoroughly investigated.
while where when although despite unless unlike ...
why what whom whatever whoever whomever whence ...
based died involved runs ended lived charged born ...
them him me us himself themselves herself myself ...
really just almost nearly simply quite fully virtually ...
know think thought feel believe knew felt hope mean ...
into through on onto atop astride Saturday/Early thru ...
Ms. Mr. Dr. Mrs. Judge Miss Professor Officer Colonel ...
John President David J. St. Robert Michael James George ...
wife own husband brother sister grandfather beloved ...
often now once recently sometimes clearly apparently ...
everyone it everybody somebody anybody nobody hers ...
around over under among near behind outside across ...
Clinton Bush Johnson Smith Brown Williams King ...
children companies women people men things students ...
Figure 3: High frequency examples from 15 of the Brown
clusters.
4.3 Training
The parsing model is trained using a variant of the
structured perceptron training algorithm used in the
original Goldberg and Elhadad (2010) implementa-
tion. The general idea of the algorithm is to iterate
over the sentences and, whenever the model predicts
an incorrect action, update the model weights. Fol-
lowing Goldberg and Elhadad, parameter averaging
is used to reduce overfitting.
Our implementation varies slightly from that of
Goldberg and Elhadad (2010). The difference is
that, at any particular step for a given sentence, the
algorithm continues to update the weight vector as
long as any invalid action is scored higher than any
valid action, not just the highest scoring valid ac-
tion; unfortunately, this change significantly slowed
down the training process. In early experiments, this
change produced a slight improvement in accuracy
though it also slowed training significantly. In later
experiments using additional feature templates, this
change ceased to have any notable impact on the
overall accuracy, but it was kept anyway. 6
The oracle used to determine whether a move op-
eration should be considered legal during the train-
ing phase is similar to Nivre et al?s (2009) improved
oracle based upon maximal projective subcompo-
nents. As an additional restriction, during training,
move actions were only considered valid either if no
other action was valid or if the token to be moved
already had all its children attached and moving it
caused it to be adjacent to its parent. This fits with
Nivre et al?s (2009) intuition that it is best to delay
word reordering as long as possible.
4.4 Speed Enhancements
To enhance the speed for practical use, the parser
uses constraints based upon the part-of-speech tags
of the adjacent word pairs to eliminate invalid de-
pendencies from even being evaluated. A rela-
tion is only considered between a pair of words if
such a relation was observed in the training data
between a pair of words with the same parts-of-
speech (with the exception of the generic dep de-
pendency, which is permitted between any POS tag
pair). Early experiments utilizing similar constraints
showed an improvement in parsing speed of about
16% with no significant impact on accuracy, regard-
less of whether the constraints were enforced during
training.
6See Goldberg and Elhadad (2010) for more description of
the general training procedure.
1262
System Arc Accuracy Perfect Sentences Non-Proj ArcsLabeled Unlabeled Labeled Unlabeled Labeled Unlabeled
THIS WORK 92.1 (93.3) 93.7 (94.3) 38.4 (42.5) 46.2 (48.5) 66.5 (69.7) 69.3 (71.7)
THIS WORKno clusters 91.8 (93.1) 93.4 (94.1) 38.2 (42.3) 45.5 (47.3) 67.3 (70.9) 69.3 (72.5)
THIS WORKmoves disabled 91.7 (92.9) 93.3 (93.9) 37.1 (40.8) 44.2 (46.2) 21.1 (21.1) 22.7 (21.9)
NON-DIR EASY FIRST * 91.2 (92.0) * 37.8 (39.4) * 15.1 (16.3)
EISNER?MST 90.9 (92.2) 92.8 (93.5) 32.1 (35.6) 40.6 (42.3) 62.5 (65.3) 63.7 (66.9)
CHU-LIU-EDMONDSMST 90.0 (91.2) 91.8 (92.5) 28.4 (31.3) 35.0 (36.4) 62.9 (65.3) 64.1 (66.5)
ARC-EAGERMalt 89.8 (91.1) 91.3 (92.1) 31.6 (34.2) 37.4 (38.5) 19.5 (19.5) 20.3 (19.9)
ARC-STANDARDMalt 88.3 (89.5) 89.7 (90.4) 31.4 (34.1) 36.1 (37.3) 13.1 (12.0) 13.9 (12.7)
STACK-EAGERMalt 90.0 (91.2) 91.5 (92.3) 34.5 (37.5) 40.4 (41.9) 51.8 (53.8) 53.8 (55.4)
STACK-LAZYMalt 90.4 (91.7) 91.9 (92.8) 34.8 (37.7) 40.6 (42.5) 61.8 (63.3) 63.3 (65.3)
CHARNIAK? * 93.2 * 43.5 * 32.3
BERKELEY? * 93.3 * 43.6 * 34.3
Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were
produced using gold POS tags. ?Eisner (1996) algorithm with non-projective rewriting and second order features.
?Results not directly comparable; see text. ?Labeled dependencies not available/comparable.
4.5 Evaluation
The following split of the Penn Treebank (Marcus,
et al, 1993) was used for the experiments: sections
2?21 for training, 22 for development, and 23 for
testing.
For part-of-speech (POS) tagging, we used an in-
house SVM-based POS tagger modeled after the
work of Gim?nez and M?rquez (2004) 7. The train-
ing data was tagged in a 10-fold fashion; each fold
was tagged using a tagger trained from the nine re-
maining folds. The development and test sections
were tagged by an instance of the tagger trained us-
ing the entire training set. The full details of the
POS tagger are outside the scope of this paper; it is
included with the parser download.
The final parser was trained for 31 iterations,
which is the point at which its performance on the
development set peaked. One test run was per-
formed with non-projectivity support disabled in or-
der to get some idea of the impact of the move opera-
tions on the parser?s overall performance; also, since
the parsers used for comparison had no access to the
unsupervised word clusters, an additional instance
of the parser was trained with every word treated
as belonging to the same cluster so as to facilitate
a more fair comparison.
Seven different dependency parsing models were
797.42% accuracy on traditional POS evaluation (Penn Tree-
bank WSJ sections 22-24).
trained for comparison using the following open
source parsing packages: Goldberg and Elhadad?s
(2010)?s non-directional easy-first parser, MALT-
PARSER (Nivre et al, 2006), and MSTPARSER
(McDonald and Pereira, 2006)8. The model trained
using Goldberg and Elhadad?s (2010) easy-first
parser serves as something of a baseline. The
four MALTPARSER parsing models used the arc-
eager, arc-standard, stack-eager, and stack-lazy al-
gorithms. One of the MSTPARSER models used
the Chu-Liu-Edmonds maximum spanning tree ap-
proach, and the other used the Eisner (1996) al-
gorithm with second order features and a non-
projective rewriting post-processing step.
Unfortunately, it is not possible to directly com-
pare the parser?s accuracy with most popular con-
stituent parsers such as the Charniak (2000) and
Berkeley (Petrov et al, 2006; Petrov and Klein,
2007) parsers9 both because they do not pro-
duce functional tags for subjects, direct objects,
etc., which are required for the final script of the
constituent-to-dependency conversion routine, and
because they determine part-of-speech tags in con-
junction with the parsing. However, it is possible to
compute approximate unlabeled accuracy scores by
training the constituent parsers on the NP-patched
(Vadas and Curran, 2007) version of the data and
then running the test output through just the first
conversion script?that is, the modified version of
Johansson and Nugues? (2007) converter.
1263
The results of the experiment are given in Ta-
ble 3, including accuracy for individual arcs, non-
projective arcs only, and full sentence match. Punc-
tuation is excluded in all the result computations. To
determine whether an arc is non-projective, the fol-
lowing heuristic was used. Traverse the sentence in
a depth-first search, starting from the imaginary root
node and pursuing child arcs in order of increasing
absolute distance from their parent. Whenever an
arc being traversed is found to cross a previously tra-
versed arc, mark it as non-projective and continue.
To evaluate the impact of part-of-speech tagging er-
ror, results for parsing using the gold standard part-
of-speech tags are also included.
We also measured the speed of the parser on the
various sentences in the test collection. For reason-
able sentence lengths, the parser scales quite well.
The scatterplot depicting the relation between sen-
tence length and parsing time is presented in Figure
5.
Figure 4: Parse times for Penn Treebank section 23 for
the parsers on a PC with a 2.4Ghz Q6600 processor and
8GB RAM. MALTPARSER ran substantially slower than
the others, perhaps due to its use of polynomial kernels,
and isn?t shown. (C-L-E - Chu-Liu-Edmonds, G&E -
Goldberg and Elhadad (2010)).
4.5.1 Results Discussion
The parser achieves 92.1% labeled and 93.7% un-
labeled accuracy on the evaluation, a solid result and
about 2.5% higher than the original easy-first imple-
mentation of Goldberg and Elhadad (2010). Further-
more, the parser processed the entire test section in
8Versions 1.4.1, 0.4.3b, and 0.2, respectively
9Versions 1.1 and 05Aug16, respectively
just over 30 seconds?a rate of over 75 sentences per
second, substantially faster than most of the other
parsers.
Not surprisingly, the results for non-projective
arcs are substantially lower than the results for all
arcs, and the systems that are designed to handle
them outperformed the strictly projective parsers in
this regard.
The negative effect of part-of-speech tagging er-
ror appears to impact the different parsers about the
same amount, with a loss of .6% to .8% in unlabeled
accuracy and 1.1% to 1.3% in labeled accuracy.
The 93.2% and 93.3% accuracy scores achieved
by the Charniak and Berkeley parsers are not too
different from the 93.7% result, but, of course, it is
important to remember that these scores are not di-
rectly comparable.
Figure 5: Sentence length versus parse time. Median
times for five runs over section 23.
5 Shallow Semantic Annotation
To create a more informative parse, the parser in-
cludes four optional modules, a preposition sense
disambiguation (PSD) system, a work-in-progress
?s-possessive interpretation system, a noun com-
pound interpretation system, and a PropBank-based
semantic role labeling system10. Taken together,
these integrated modules enable the parsing sys-
tem to produce substantially more informative out-
put than a traditional parser.
Preposition Sense Disambiguation The PSD
system is a newer version of the system described
10Lack of space prohibits a sufficiently thorough discussion
of these individual components and their evaluations, but addi-
tional information will be available with the system download.
1264
by Tratz and Hovy (2009) and Hovy et al (2010); it
achieves 85.7% accuracy on the SemEval-2007 fine-
grain PSD task (Litkowski and Hargraves, 2007),
which is a statistically significant (p<=0.05; upper-
tailed z test) increase over the previous best reported
result for this dataset, Hovy et al?s (2010) 84.8%.
Noun Compound Interpretation The noun com-
pound interpretation system is a newer version of
the system described by Tratz and Hovy (2010) with
similar accuracy (79.6% vs 79.3% using 10-fold
cross-validation11).
Possessives Interpretation The possessive inter-
pretation system assigns interpretations to ?s pos-
sessives (e.g., John?s arm ? PART-OF, Mowgli?s
capture ? PATIENT/THEME). The current system
achieves over 85.0% accuracy, but it is important to
note that the annotation scheme, automatic classifier,
and dataset are all still under active development.
PropBank SRL The PropBank-based semantic
role labeling system achieves 86.8 combined F1
measure for automatically-generated parse trees cal-
culated over both predicate disambiguation and ar-
gument/adjunct classification (89.5 F1 on predicate
disambiguation, 85.6 F1 on argument and adjuncts
corresponding to dependency links, and 86.8 F1);
this score is not directly comparable to any previ-
ous work due to some differences, including differ-
ences in both the parse tree conversion and the Prop-
Bank conversion. The most similar work is that of
the CoNLL shared task work (Surdeanu et al, 2008;
Hajic? et al, 2009).
6 Related Work
Non-projectivity. There are two main approaches
used in recent NLP literature for handling non-
projectivity in parse trees. The first is to use an al-
gorithm, like the one presented in this paper, that
has inherent support for non-projective trees. Ex-
amples of this include the Chu-Liu-Edmonds? ap-
proach for maximum spanning tree (MST) parsing
(McDonald et al, 2005) and Nivre?s (2009) swap-
based reordering method for shift-reduce parsing.
The second approach is to create an initial projec-
tive parse and then apply transformations to intro-
11These accuracy figures are higher than what should be ex-
pected for unseen datasets; see Tratz and Hovy (2010) for more
detail.
duce non-projectivity into it. Examples of this in-
clude McDonald and Pereira?s (2006) rewriting of
projective trees produced by the Eisner (1996) al-
gorithm, and Nivre and Nilsson?s (2005) pseudo-
projective approach that creates projective trees with
specially marked arcs that are later transformed into
non-projective dependencies.
Descriptive dependency labels. While most re-
cent dependency parsing research has used either
vague labels, such as those of the CoNLL shared
tasks, or no labels at all, some descriptive depen-
dency label schemes exist. By far the most promi-
nent of these is the Stanford typed dependency
scheme (de Marneffe and Manning, 2008). An-
other descriptive scheme that exists, but which is
less widely used in the NLP community, is the one
used by Tapanainen and J?rvinen?s parser (1997).
Unfortunately, the Stanford dependency conversion
of the Penn Treebank has proven difficult to learn for
current dependency parsers (Cer et al, 2010), and
there is no publicly available dependency conversion
according to Tapanainen and J?rvinen?s scheme.
Faster parsing. While the fastest reasonable
parsing algorithms are the O(n) shift-reduce algo-
rithms, such as Nivre?s (2003) algorithm and an ex-
pected linear time dynamic programming approach
presented by Huang and Sagae (2010), a few other
fast alternatives exist. Goldberg and Elhadad?s
(2010) easy-first algorithm is one such example. An-
other example, is Roark and Hollingshead?s (2009)
work that uses chart constraints to achieve linear
time complexity for constituency parsing.
Effective features for parsing. A variety of work
has investigated the use of more informative fea-
tures for parsing. This includes work that inte-
grates second and even third order features (McDon-
ald et al, 2006; Carreras, 2007; Koo and Collins,
2010). Also, some work has incorporated unsuper-
vised word clusters as features, including that of Koo
et al (2008) and Suzuki et al (2009), who utilized
unsupervised word clusters created using the Brown
et al (1992) hierarchical clustering algorithm.
Semantically-enriched output. The 2008 and
2009 CoNLL shared tasks (Surdeanu et al, 2008;
Hajic? et al, 2009), which required participants to
build systems capable of both syntactic parsing and
Semantic Role Labeling (SRL) (Gildea and Juraf-
sky, 2002), are the most notable attempts to encour-
1265
age the development of parsers with additional se-
mantic annotation. These tasks relied upon Prop-
Bank (2005) and NomBank (2004) for the seman-
tic roles. A variety of other systems have focused
on FrameNet-based (1998) SRL instead, including
those that participated in the SemEval-2007 Task 19
(Baker et al, 2007) and work by Das et al (2010).
7 Conclusion
In this paper, we have described a new high-quality
dependency tree conversion of the Penn Treebank
(Marcus, et al, 1993) along with its labeled depen-
dency scheme and presented a parser that is fast, ac-
curate, supports non-projective trees and provides
rich output, including not only informative depen-
dency labels similar to Stanford?s but also additional
semantic annotation for prepositions, possessives,
and noun compound relations. We showed how the
easy-first algorithm of Goldberg and Elhadad (Gold-
berg and Elhadad, 2010) can be extended to support
non-projective trees by adding move actions similar
to Nivre?s (2009) swap-based reordering for shift-
reduce parsing and evaluated our parser on the stan-
dard test section of the Penn Treebank, comparing
with several other freely available parsers.
The Penn Treebank conversion process fixes a
number of buggy trees and part-of-speech tags and
produces dependency trees with a relatively small
percentage of generic dep dependencies. The ex-
perimental results show that dependency parsers can
generally produce Stanford-granularity labels with
high accuracy when using the new dependency con-
version of the Penn Treebank, something which, ac-
cording to the findings of Cer et al (2010), does
not appear to be the case when training and testing
dependency parsers on the Stanford conversion.
The parser achieves high labeled and unlabeled
accuracy in the evaluation, 92.1% and 93.7%, re-
spectively. The 93.7% result represents a 2.5% in-
crease over the accuracy of Goldberg and Elhadad?s
(2010) implementation. Also, the parser proves to
be quite fast, processing section 23 of the Penn Tree-
bank in just over 30 seconds (a rate of over 75 sen-
tences per second).
The parsing system is capable of not only produc-
ing fine-grained dependency relations, but can also
produce shallow semantic annotations for preposi-
tions, possessives, and noun compounds by using
several optional integrated modules. The preposi-
tion sense disambiguation (PSD) module achieves
85.7% accuracy on the SemEval-2007 PSD task, ex-
ceeding the previous best published result of 84.8%
by a statistically significant margin, the possessives
module is over 85% accurate, the noun compound
interpretation module achieves 79.6% accuracy on
Tratz and Hovy?s (2010) dataset. The PropBank
SRL module achieves 89.5 F1 on predicate disam-
biguation and 85.6 F1 on argument and adjuncts cor-
responding to dependency links, for an overall F1 of
86.8. Combined with the core parser, these modules
allow the system to produce a substantially more in-
formative textual analysis than a standard parser.
8 Future Work
There are a variety of ways to extend and improve
upon this work. We would like to change our han-
dling of coordinating conjunctions to treat the co-
ordinating conjunction as the head because this has
fewer ambiguities than the current approach and also
add the ability to produce traces for WH- words. It
would also be interesting to examine the impact on
final parsing accuracy of the various differences be-
tween our dependency conversion and Stanford?s.
To aid future NLP research work, the code,
including the treebank converter, part-of-speech
tagger, parser, and semantic annotation add-ons,
will be made publicly available for download via
http://www.isi.edu.
Acknowledgements
We would like to thank Richard Johansson for
providing us with the code for the pennconverter
consituent-to-dependency converter. We would also
like to thank Dirk Hovy and Anselmo Pe?as for
many valuable dicussions and suggestions.
References
Collin Baker, and Michael Ellsworth and Katrin Erk.
2007. SemEval?07 task 19: Frame Semantic Structure
Extraction. In Proc. of the 4th International Workshop
on Semantic Evaluations
Collin Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proc. of the
1266
17th international conference on Computational lin-
guistics
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Lin-
guistics 22(1):39?71
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
Based n-gram Models of Natural Language. Compu-
tational Linguistics 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL 2006.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford Dependencies: Trade-offs between speed and
accuracy. In Proc. of LREC 2010.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of NAACL 2000.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
find-grained n-best parsing and discriminative rerank-
ing. In Proc. of ACL 2005.
Michael A. Covington. 2001. A Fundamental Algorithm
for Dependency Parsing. In Proc. of the 39th Annual
ACM Southeast Conference.
Dipanjan Das, Nathan Schneider, Desai Chen, and Noah
A. Smith. 2010. Probabilistic Frame-Semantic Pars-
ing. In Proc. of HLT-NAACL 2010.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In COLING Workshop on Cross-framework
and Cross-domain Parser Evaluation.
Jason Eisner. 1996. Three New Probabilistic Models
for Dependency Parsing: An Exploration. In Proc. of
COLING 1996.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics.
28(3):245?288.
Jes?s Gim?nez and Llu?s M?rquez 2004. SVMTool: A
General POS Tagger Generator Based on Support Vec-
tor Machines. In Proc. of LREC 2004.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the ACL.
Yoav Goldberg and Michael Elhadad. 2009. The
CoNLL-2009 Shared Task: Syntactic and Semantic
Dependencies in Multiple Languages. In Proc. of
the Thirteenth Conference on Computational Natural
Language Learning: Shared Task.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition??Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Proc. of
COLING 2010.
Liang Huang and Kenji Sagae. 2010. Dynamic Program-
ming for Linear-Time Shift-Reduce Parsing. In Proc.
of ACL 2010.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc. of NODALIDA.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In
Proc. of ACL 2008.
Terry Koo and Michael Collins. 2010. Efficient Third-
order Dependency Parsers. In Proc. of ACL 2010.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proc. of the 4th International Workshop on
Semantic Evaluations.
Christopher D. Manning. 2011. Part-of-Speech Tagging
from 97% to 100%: Is It Time for Some Linguistics?
In Proc. of the 12th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing 2011).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-Projective Dependency Parsing
Using Spanning Tree Algorithms. In Proc. of HLT-
EMNLP 2005.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In Proc. of EACL 2006.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young and Ralph
Grishman. 2004. The NomBank Project: An Interim
Report. In Proc. of the NAACL/HLT Workshop on
Frontiers in Corpus Annotation.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proc. of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP.
Joakim Nivre. 2003. An Efficient Algorithm for Projec-
tive Dependency Parsing. In Proc. of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
Joakim Nivre, Marco Kuhlmann, and Johan Hall. 2009.
An Improved Oracle for Dependency Parsing with On-
line Reordering. In Proc. of the 11th International
Conference on Parsing Technologies (IWPT).
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of EMNLP-CoNLL 2007.
1267
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A Data-Driven Parser-Generator for Depen-
dency Parsing. In Proc. of LREC 2006.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. of ACL-2005.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. In Computational Linguistics.
31(1):71?106.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. of HLT-NAACL
2007.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of COLING-ACL
2006.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proc. of HLT-NAACL.
Evan Sandhaus. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu?s M?rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proc. of the Twelfth Confer-
ence on Computational Natural Language Learning.
Jun Suzuki, Hideki Isozaki, Xavier Carrerras, and
Michael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for Depen-
dency Parsing. In Proc. of EMNLP.
Pasi Tapanainen and Timo J?rvinen. 1997. A non-
projective dependency parser. In Proc. of the fifth con-
ference on applied natural language processing.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense using Linguistically Motivated Fea-
tures. In Proc. of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium.
Stephen Tratz and Eduard Hovy. 2010. A Taxonomy,
Dataset, and Classifier for Automatic Noun Com-
pound Interpretation. In Proc. of ACL 2010.
David Vadas and James R. Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proc. of
ACL 2007.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis With Support Vector Ma-
chines. In Proc. of 8th International Workshop on
Parsing Technologies (IWPT).
1268
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678?687,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Taxonomy, Dataset, and Classifier for Automatic Noun Compound
Interpretation
Stephen Tratz and Eduard Hovy
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
{stratz,hovy}@isi.edu
Abstract
The automatic interpretation of noun-noun
compounds is an important subproblem
within many natural language processing
applications and is an area of increasing
interest. The problem is difficult, with dis-
agreement regarding the number and na-
ture of the relations, low inter-annotator
agreement, and limited annotated data. In
this paper, we present a novel taxonomy
of relations that integrates previous rela-
tions, the largest publicly-available anno-
tated dataset, and a supervised classifica-
tion method for automatic noun compound
interpretation.
1 Introduction
Noun compounds (e.g., ?maple leaf?) occur very
frequently in text, and their interpretation?
determining the relationships between adjacent
nouns as well as the hierarchical dependency
structure of the NP in which they occur?is an
important problem within a wide variety of nat-
ural language processing (NLP) applications, in-
cluding machine translation (Baldwin and Tanaka,
2004) and question answering (Ahn et al, 2005).
The interpretation of noun compounds is a difficult
problem for various reasons (Sp?rck Jones, 1983).
Among them is the fact that no set of relations pro-
posed to date has been accepted as complete and
appropriate for general-purpose text. Regardless,
automatic noun compound interpretation is the fo-
cus of an upcoming SEMEVAL task (Butnariu et
al., 2009).
Leaving aside the problem of determining the
dependency structure among strings of three or
more nouns?a problem we do not address in this
paper?automatic noun compound interpretation
requires a taxonomy of noun-noun relations, an
automatic method for accurately assigning the re-
lations to noun compounds, and, in the case of su-
pervised classification, a sufficiently large dataset
for training.
Earlier work has often suffered from using tax-
onomies with coarse-grained, highly ambiguous
predicates, such as prepositions, as various labels
(Lauer, 1995) and/or unimpressive inter-annotator
agreement among human judges (Kim and Bald-
win, 2005). In addition, the datasets annotated ac-
cording to these various schemes have often been
too small to provide wide coverage of the noun
compounds likely to occur in general text.
In this paper, we present a large, fine-grained
taxonomy of 43 noun compound relations, a
dataset annotated according to this taxonomy, and
a supervised, automatic classification method for
determining the relation between the head and
modifier words in a noun compound. We com-
pare and map our relations to those in other tax-
onomies and report the promising results of an
inter-annotator agreement study as well as an au-
tomatic classification experiment. We examine the
various features used for classification and iden-
tify one very useful, novel family of features. Our
dataset is, to the best of our knowledge, the largest
noun compound dataset yet produced. We will
make it available via http://www.isi.edu.
2 Related Work
2.1 Taxonomies
The relations between the component nouns in
noun compounds have been the subject of various
linguistic studies performed throughout the years,
including early work by Jespersen (1949). The
taxonomies they created are varied. Lees created
an early taxonomy based primarily upon grammar
(Lees, 1960). Levi?s influential work postulated
that complex nominals (Levi?s name for noun com-
pounds that also permits certain adjectival modi-
fiers) are all derived either via nominalization or
678
by deleting one of nine predicates (i.e., CAUSE,
HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT)
from an underlying sentence construction (Levi,
1978). Of the taxonomies presented by purely
linguistic studies, our categories are most similar
to those proposed by Warren (1978), whose cat-
egories (e.g., MATERIAL+ARTEFACT, OBJ+PART)
are generally less ambiguous than Levi?s.
In contrast to studies that claim the existence of
a relatively small number of semantic relations,
Downing (1977) presents a strong case for the
existence of an unbounded number of relations.
While we agree with Downing?s belief that the
number of relations is unbounded, we contend that
the vast majority of noun compounds fits within a
relatively small set of categories.
The relations used in computational linguistics
vary much along the same lines as those proposed
earlier by linguists. Several lines of work (Finin,
1980; Butnariu and Veale, 2008; Nakov, 2008) as-
sume the existence of an unbounded number of re-
lations. Others use categories similar to Levi?s,
such as Lauer?s (1995) set of prepositional para-
phrases (i.e., OF, FOR, IN, ON, AT, FROM, WITH,
ABOUT) to analyze noun compounds. Some work
(e.g., Barker and Szpakowicz, 1998; Nastase and
Szpakowicz, 2003; Girju et al, 2005; Kim and
Baldwin, 2005) use sets of categories that are
somewhat more similar to those proposed by War-
ren (1978). While most of the noun compound re-
search to date is not domain specific, Rosario and
Hearst (2001) create and experiment with a taxon-
omy tailored to biomedical text.
2.2 Classification
The approaches used for automatic classification
are also varied. Vanderwende (1994) presents one
of the first systems for automatic classification,
which extracted information from online sources
and used a series of rules to rank a set of most
likely interpretations. Lauer (1995) uses corpus
statistics to select a prepositional paraphrase. Sev-
eral lines of work, including that of Barker and
Szpakowicz (1998), use memory-based methods.
Kim and Baldwin (2005) and Turney (2006) use
nearest neighbor approaches based upon WordNet
(Fellbaum, 1998) and Turney?s Latent Relational
Analysis, respectively. Rosario and Hearst (2001)
utilize neural networks to classify compounds ac-
cording to their domain-specific relation taxon-
omy. Moldovan et al (2004) use SVMs as well as
a novel algorithm (i.e., semantic scattering). Nas-
tase et al (2006) experiment with a variety of clas-
sification methods including memory-based meth-
ods, SVMs, and decision trees. ? S?aghdha and
Copestake (2009) use SVMs and experiment with
kernel methods on a dataset labeled using a rela-
tively small taxonomy. Girju (2009) uses cross-
linguistic information from parallel corpora to aid
classification.
3 Taxonomy
3.1 Creation
Given the heterogeneity of past work, we decided
to start fresh and build a new taxonomy of re-
lations using naturally occurring noun pairs, and
then compare the result to earlier relation sets.
We collected 17509 noun pairs and over a period
of 10 months assigned one or more relations to
each, gradually building and refining our taxon-
omy. More details regarding the dataset are pro-
vided in Section 4.
The relations we produced were then compared
to those present in other taxonomies (e.g., Levi,
1978; Warren, 1978; Barker and Szpakowicz,
1998; Girju et al, 2005), and they were found to
be fairly similar. We present a detailed comparison
in Section 3.4.
We tested the relation set with an initial
inter-annotator agreement study (our latest inter-
annotator agreement study results are presented in
Section 6). However, the mediocre results indi-
cated that the categories and/or their definitions
needed refinement. We then embarked on a se-
ries of changes, testing each generation by anno-
tation using Amazon?s Mechanical Turk service, a
relatively quick and inexpensive online platform
where requesters may publish tasks for anony-
mous online workers (Turkers) to perform. Me-
chanical Turk has been previously used in a va-
riety of NLP research, including recent work on
noun compounds by Nakov (2008) to collect short
phrases for linking the nouns within noun com-
pounds.
For the Mechanical Turk annotation tests, we
created five sets of 100 noun compounds from
noun compounds automatically extracted from a
random subset of New York Times articles written
between 1987 and 2007 (Sandhaus, 2008). Each
of these sets was used in a separate annotation
round. For each round, a set of 100 noun com-
pounds was uploaded along with category defini-
679
Category Name % Example Approximate Mappings
Causal Group
COMMUNICATOR OF COMMUNICATION 0.77 court order ?BGN:Agent, ?L:Acta+Producta, ?V:Subj
PERFORMER OF ACT/ACTIVITY 2.07 police abuse ?BGN:Agent, ?L:Acta+Producta, ?V:Subj
CREATOR/PROVIDER/CAUSE OF 2.55 ad revenue ?BGV:Cause(d-by), ?L:Cause2, ?N:Effect
Purpose/Activity Group
PERFORM/ENGAGE_IN 13.24 cooking pot ?BGV:Purpose, ?L:For, ?N:Purpose, ?W:Activity?Purpose
CREATE/PROVIDE/SELL 8.94 nicotine patch?BV:Purpose, ?BG:Result,?G:Make-Produce, ?GNV:Cause(s),
?L:Cause1?Make1?For, ?N:Product, ?W:Activity?Purpose
OBTAIN/ACCESS/SEEK 1.50 shrimp boat ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
MODIFY/PROCESS/CHANGE 1.50 eye surgery ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
MITIGATE/OPPOSE/DESTROY 2.34 flak jacket ?BGV:Purpose, ?L:For, ?N:Detraction, ?W:Activity?Purpose
ORGANIZE/SUPERVISE/AUTHORITY 4.82 ethics board ?BGNV:Purpose/Topic, ?L:For/Abouta, ?W:Activity
PROPEL 0.16 water gun ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
PROTECT/CONSERVE 0.25 screen saver ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
TRANSPORT/TRANSFER/TRADE 1.92 freight train ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
TRAVERSE/VISIT 0.11 tree traversal ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
Ownership, Experience, Employment, and Use
POSSESSOR + OWNED/POSSESSED 2.11 family estate ?BGNVW:Possess*, ?L:Have2
EXPERIENCER + COGINITION/MENTAL 0.45 voter concern ?BNVW:Possess*, ?G:Experiencer, ?L:Have2
EMPLOYER + EMPLOYEE/VOLUNTEER 2.72 team doctor ?BGNVW:Possess*, ?L:For/Have2, ?BGN:Beneficiary
CONSUMER + CONSUMED 0.09 cat food ?BGNVW:Purpose, ?L:For, ?BGN:Beneficiary
USER/RECIPIENT + USED/RECEIVED 1.02 voter guide ?BNVW:Purpose, ?G:Recipient, ?L:For, ?BGN:Beneficiary
OWNED/POSSESSED + POSSESSION 1.20 store owner ?G:Possession, ?L:Have1, ?W:Belonging-Possessor
EXPERIENCE + EXPERIENCER 0.27 fire victim ?G:Experiencer,?L:Have1
THING CONSUMED + CONSUMER 0.41 fruit fly ?W:Obj-SingleBeing
THING/MEANS USED + USER 1.96 faith healer ?BNV:Instrument, ?G:Means?Instrument, ?L:Use,
?W:MotivePower-Obj
Temporal Group
TIME [SPAN] + X 2.35 night work ?BNV:Time(At), ?G:Temporal, ?L:Inc, ?W:Time-Obj
X + TIME [SPAN] 0.50 birth date ?G:Temporal, ?W:Obj-Time
Location and Whole+Part/Member of
LOCATION/GEOGRAPHIC SCOPE OF X 4.99 hillside home ?BGV:Locat(ion/ive), ?L:Ina?Fromb, B:Source,
?N:Location(At/From), ?W:Place-Obj?PlaceOfOrigin
WHOLE + PART/MEMBER OF 1.75 robot arm ?B:Possess*, ?G:Part-Whole, ?L:Have2, ?N:Part,
?V:Whole-Part, ?W:Obj-Part?Group-Member
Composition and Containment Group
SUBSTANCE/MATERIAL/INGREDIENT + WHOLE 2.42 plastic bag ?BNVW:Material*,?GN:Source,?L:Froma, ?L:Have1,
?L:Make2b,?N:Content
PART/MEMBER + COLLECTION/CONFIG/SERIES 1.78 truck convoy ?L:Make2ac, ?N:Whole, ?V:Part-Whole, ?W:Parts-Whole
X + SPATIAL CONTAINER/LOCATION/BOUNDS 1.39 shoe box ?B:Content?Located, ?L:For, ?L:Have1, ?N:Location,
?W:Obj-Place
Topic Group
TOPIC OF COMMUNICATION/IMAGERY/INFO 8.37 travel story ?BGNV:Topic, ?L:Aboutab, ?W:SubjectMatter, ?G:Depiction
TOPIC OF PLAN/DEAL/ARRANGEMENT/RULES 4.11 loan terms ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF OBSERVATION/STUDY/EVALUATION 1.71 job survey ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF COGNITION/EMOTION 0.58 jazz fan ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF EXPERT 0.57 policy wonk ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF SITUATION 1.64 oil glut ?BGNV:Topic, ?L:Aboutc
TOPIC OF EVENT/PROCESS 1.09 lava flow ?G:Theme, ?V:Subj
Attribute Group
TOPIC/THING + ATTRIB 4.13 street name ?BNV:Possess*, ?G:Property, ?L:Have2, ?W:Obj-Quality
TOPIC/THING + ATTRIB VALUE CHARAC OF 0.31 earth tone
Attributive and Coreferential
COREFERENTIAL 4.51 fighter plane ?BV:Equative, ?G:Type?IS-A, ?L:BEbcd, ?N:Type?Equality,
?W:Copula
PARTIAL ATTRIBUTE TRANSFER 0.69 skeleton crew ?W:Resemblance, ?G:Type
MEASURE + WHOLE 4.37 hour meeting ?G:Measure, ?N:TimeThrough?Measure, ?W:Size-Whole
Other
HIGHLY LEXICALIZED / FIXED PAIR 0.65 pig iron
OTHER 1.67 contact lens
Table 1: The semantic relations, their frequency in the dataset, examples, and approximate relation
mappings to previous relation sets. ?-approximately equivalent; ?/?-super/sub set; ?-some overlap;
?-union; initials BGLNVW refer respectively to the works of (Barker and Szpakowicz, 1998; Girju et
al., 2005; Girju, 2007; Levi, 1978; Nastase and Szpakowicz, 2003; Vanderwende, 1994; Warren, 1978).
680
tions and examples. Turkers were asked to select
one or, if they deemed it appropriate, two cate-
gories for each noun pair. After all annotations for
the round were completed, they were examined,
and any taxonomic changes deemed appropriate
(e.g., the creation, deletion, and/or modification of
categories) were incorporated into the taxonomy
before the next set of 100 was uploaded. The cate-
gories were substantially modified during this pro-
cess. They are shown in Table 1 along with exam-
ples and an approximate mapping to several other
taxonomies.
3.2 Category Descriptions
Our categories are defined with sentences. For
example, the SUBSTANCE category has the
definition n1 is one of the primary physi-
cal substances/materials/ingredients that n2 is
made/composed out of/from. Our LOCATION cat-
egory?s definition reads n1 is the location / geo-
graphic scope where n2 is at, near, from, gener-
ally found, or occurs. Defining the categories with
sentences is advantageous because it is possible to
create straightforward, explicit defintions that hu-
mans can easily test examples against.
3.3 Taxonomy Groupings
In addition to influencing the category defini-
tions, some taxonomy groupings were altered with
the hope that this would improve inter-annotator
agreement for cases where Turker disagreement
was systematic. For example, LOCATION and
WHOLE + PART/MEMBER OF were commonly dis-
agreed upon by Turkers so they were placed within
their own taxonomic subgroup. The ambiguity
between these categories has previously been ob-
served by Girju (2009).
Turkers also tended to disagree between the
categories related to composition and contain-
ment. Due this apparent similarity they were also
grouped together in the taxonomy.
The ATTRIBUTE categories are positioned near
the TOPIC group because some Turkers chose a
TOPIC category when an ATTRIBUTE category was
deemed more appropriate. This may be because
attributes are relatively abstract concepts that are
often somewhat descriptive of whatever possesses
them. A prime example of this is street name.
3.4 Contrast with other Taxonomies
In order to ensure completeness, we mapped into
our taxonomy the relations proposed in most pre-
vious work including those of Barker and Sz-
pakowicz (1998) and Girju et al (2005). The
results, shown in Table 1, demonstrate that our
taxonomy is similar to several taxonomies used
in other work. However, there are three main
differences and several less important ones. The
first major difference is the absence of a signif-
icant THEME or OBJECT category. The second
main difference is that our taxonomy does not in-
clude a PURPOSE category and, instead, has sev-
eral smaller categories. Finally, instead of pos-
sessing a single TOPIC category, our taxonomy has
several, finer-grained TOPIC categories. These dif-
ferences are significant because THEME/OBJECT,
PURPOSE, and TOPIC are typically among the
most frequent categories.
THEME/OBJECT is typically the category to
which other researchers assign noun compounds
whose head noun is a nominalized verb and whose
modifier noun is the THEME/OBJECT of the verb.
This is typically done with the justification that the
relation/predicate (the root verb of the nominaliza-
tion) is overtly expressed.
While including a THEME/OBJECT category has
the advantage of simplicity, its disadvantages are
significant. This category leads to a significant
ambiguity in examples because many compounds
fitting the THEME/OBJECT category also match
some other category as well. Warren (1978) gives
the examples of soup pot and soup container
to illustrate this issue, and Girju (2009) notes a
substantial overlap between THEME and MAKE-
PRODUCE. Our results from Mechanical Turk
showed significant overlap between PURPOSE and
OBJECT categories (present in an earlier version of
the taxonomy). For this reason, we do not include
a separate THEME/OBJECT category. If it is im-
portant to know whether the modifier also holds a
THEME/OBJECT relationship, we suggest treating
this as a separate classification task.
The absence of a single PURPOSE category
is another distinguishing characteristic of our
taxonomy. Instead, the taxonomy includes a
number of finer-grained categories (e.g., PER-
FORM/ENGAGE_IN), which can be conflated to
create a PURPOSE category if necessary. During
our Mechanical Turk-based refinement process,
our now-defunct PURPOSE category was found
to be ambiguous with many other categories as
well as difficult to define. This problem has been
noted by others. For example, Warren (1978)
681
points out that tea in tea cup qualifies as both the
content and the purpose of the cup. Similarly,
while WHOLE+PART/MEMBER was selected by
most Turkers for bike tire, one individual chose
PURPOSE. Our investigation identified five main
purpose-like relations that most of our PURPOSE
examples can be divided into, including activity
performance (PERFORM/ENGAGE_IN), cre-
ation/provision (CREATE/PROVIDE/CAUSE OF),
obtainment/access (OBTAIN/ACCESS/SEEK),
supervision/management (ORGA-
NIZE/SUPERVISE/AUTHORITY), and opposition
(MITIGATE/OPPOSE/DESTROY).
The third major distinguishing different be-
tween our taxonomy and others is the absence of a
single TOPIC/ABOUT relation. Instead, our taxon-
omy has several finer-grained categories that can
be conflated into a TOPIC category. Unlike the
previous two distinguishing characteristics, which
were motivated primarily by Turker annotations,
this separation was largely motivated by author
dissatisfaction with a single TOPIC category.
Two differentiating characteristics of less im-
portance are the absence of BENEFICIARY or
SOURCE categories (Barker and Szpakowicz,
1998; Nastase and Szpakowicz, 2003; Girju et
al., 2005). Our EMPLOYER, CONSUMER, and
USER/RECIPIENT categories combined more or
less cover BENEFICIARY. Since SOURCE is am-
biguous in multiple ways including causation
(tsunami injury), provision (government grant),
ingredients (rice wine), and locations (north
wind), we chose to exclude it.
4 Dataset
Our noun compound dataset was created from
two principal sources: an in-house collection of
terms extracted from a large corpus using part-
of-speech tagging and mutual information and the
Wall Street Journal section of the Penn Treebank.
Compounds including one or more proper nouns
were ignored. In total, the dataset contains 17509
unique, out-of-context examples, making it by far
the largest hand-annotated compound noun dataset
in existence that we are aware of. Proper nouns
were not included.
The next largest available datasets have a vari-
ety of drawbacks for noun compound interpreta-
tion in general text. Kim and Baldwin?s (2005)
dataset is the second largest available dataset, but
inter-annotator agreement was only 52.3%, and
the annotations had an usually lopsided distribu-
tion; 42% of the data has TOPIC labels. Most
(73.23%) of Girju?s (2007) dataset consists of
noun-preposition-noun constructions. Rosario and
Heart?s (2001) dataset is specific to the biomed-
ical domain, while ? S?aghdha and Copestake?s
(2009) data is labeled with only 5 extremely
coarse-grained categories. The remaining datasets
are too small to provide wide coverage. See Table
2 below for size comparison with other publicly
available, semantically annotated datasets.
Size Work
17509 Tratz and Hovy, 2010
2169 Kim and Baldwin, 2005
2031 Girju, 2007
1660 Rosario and Hearst, 2001
1443 ? S?aghdha and Copestake, 2007
505 Barker and Szpakowicz, 1998
600 Nastase and Szpakowicz, 2003
395 Vanderwende, 1994
385 Lauer, 1995
Table 2: Size of various available noun compound
datasets labeled with relation annotations. Ital-
ics indicate that the dataset contains n-prep-n con-
structions and/or non-nouns.
5 Automated Classification
We use a Maximum Entropy (Berger et al, 1996)
classifier with a large number of boolean features,
some of which are novel (e.g., the inclusion of
words from WordNet definitions). Maximum En-
tropy classifiers have been effective on a variety of
NLP problems including preposition sense disam-
biguation (Ye and Baldwin, 2007), which is some-
what similar to noun compound interpretation. We
use the implementation provided in the MALLET
machine learning toolkit (McCallum, 2002).
5.1 Features Used
WordNet-based Features
? {Synonyms, Hypernyms} for all NN and VB
entries for each word
? Intersection of the words? hypernyms
? All terms from the ?gloss? for each word
? Intersection of the words? ?gloss? terms
? Lexicographer file names for each word?s NN
and VB entries (e.g., n1:substance)
682
? Logical AND of lexicographer file names
for the two words (e.g., n1:substance ?
n2:artifact)
? Lists of all link types (e.g., meronym links)
associated with each word
? Logical AND of the link types (e.g.,
n1:hasMeronym(s) ? n2:hasHolonym(s))
? Part-of-speech (POS) indicators for the exis-
tence of VB, ADJ, and ADV entries for each
of the nouns
? Logical AND of the POS indicators for the
two words
? ?Lexicalized? indicator for the existence of an
entry for the compound as a single term
? Indicators if either word is a part of the other
word according to Part-Of links
? Indicators if either word is a hypernym of the
other
? Indicators if either word is in the definition of
the other
Roget?s Thesaurus-based Features
? Roget?s divisions for all noun (and verb) en-
tries for each word
? Roget?s divisions shared by the two words
Surface-level Features
? Indicators for the suffix types (e.g., de-
adjectival, de-nominal [non]agentive, de-
verbal [non]agentive)
? Indicators for degree, number, order, or loca-
tive prefixes (e.g., ultra-, poly-, post-, and
inter-, respectively)
? Indicators for whether or not a preposition
occurs within either term (e.g., ?down? in
?breakdown?)
? The last {two, three} letters of each word
Web 1T N-gram Features
To provide information related to term usage to
the classifier, we extracted trigram and 4-gram fea-
tures from the Web 1T Corpus (Brants and Franz,
2006), a large collection of n-grams and their
counts created from approximately one trillion
words of Web text. Only n-grams containing low-
ercase words were used. 5-grams were not used
due to memory limitations. Only n-grams con-
taining both terms (including plural forms) were
extracted. Table 3 describes the extracted n-gram
features.
5.2 Cross Validation Experiments
We performed 10-fold cross validation on our
dataset, and, for the purpose of comparison,
we also performed 5-fold cross validation on ?
S?aghdha?s (2007) dataset using his folds. Our
classification accuracy results are 79.3% on our
data and 63.6% on the ? S?aghdha data. We
used the ?2 measure to limit our experiments
to the most useful 35000 features, which is the
point where we obtain the highest results on ?
S?aghdha?s data. The 63.6% figure is similar to the
best previously reported accuracy for this dataset
of 63.1%, which was obtained by ? S?aghdha and
Copestake (2009) using kernel methods.
For comparison with SVMs, we used Thorsten
Joachims? SVMmulticlass, which implements an
optimization solution to Cramer and Singer?s
(2001) multiclass SVM formulation. The best re-
sults were similar, with 79.4% on our dataset and
63.1% on ? S?aghdha?s. SVMmulticlass was, how-
ever, observed to be very sensitive to the tuning
of the C parameter, which determines the tradeoff
between training error and margin width. The best
results for the datasets were produced with C set
to 5000 and 375 respectively.
Trigram Feature Extraction Patterns
text <n1> <n2>
<*> <n1> <n2>
<n1> <n2> text
<n1> <n2> <*>
<n1> text <n2>
<n2> text <n1>
<n1> <*> <n2>
<n2> <*> <n1>
4-Gram Feature Extraction Patterns
<n1> <n2> text text
<n1> <n2> <*> text
text <n1> <n2> text
text text <n1> <n2>
text <*> <n1> <n2>
<n1> text text <n2>
<n1> text <*> <n2>
<n1> <*> text <n2>
<n1> <*> <*> <n2>
<n2> text text <n1>
<n2> text <*> <n1>
<n2> <*> text <n1>
<n2> <*> <*> <n1>
Table 3: Patterns for extracting trigram and 4-
Gram features from the Web 1T Corpus for a given
noun compound (n1 n2).
To assess the impact of the various features, we
ran the cross validation experiments for each fea-
ture type, alternating between including only one
683
feature type and including all feature types except
that one. The results for these runs using the Max-
imum Entropy classifier are presented in Table 4.
There are several points of interest in these re-
sults. The WordNet gloss terms had a surpris-
ingly strong influence. In fact, by themselves they
proved roughly as useful as the hypernym features,
and their removal had the single strongest negative
impact on accuracy for our dataset. As far as we
know, this is the first time that WordNet definition
words have been used as features for noun com-
pound interpretation. In the future, it may be valu-
able to add definition words from other machine-
readable dictionaries. The influence of the Web 1T
n-gram features was somewhat mixed. They had a
positive impact on the ? S?aghdha data, but their
affect upon our dataset was limited and mixed,
with the removal of the 4-gram features actually
improving performance slightly.
Our Data ? S?aghdha Data
1 M-1 1 M-1
WordNet-based
synonyms 0.674 0.793 0.469 0.626
hypernyms 0.753 0.787 0.539 0.626
hypernyms? 0.250 0.791 0.357 0.624
gloss terms 0.741 0.785 0.510 0.613
gloss terms? 0.226 0.793 0.275 0.632
lexfnames 0.583 0.792 0.505 0.629
lexfnames? 0.480 0.790 0.440 0.629
linktypes 0.328 0.793 0.365 0.631
linktypes? 0.277 0.792 0.346 0.626
pos 0.146 0.793 0.239 0.633
pos? 0.146 0.793 0.235 0.632
part-of terms 0.372 0.793 0.368 0.635
lexicalized 0.132 0.793 0.213 0.637
part of other 0.132 0.793 0.216 0.636
gloss of other 0.133 0.793 0.214 0.635
hypernym of other 0.132 0.793 0.227 0.627
Roget?s Thesaurus-based
div info 0.679 0.789 0.471 0.629
div info? 0.173 0.793 0.283 0.633
Surface level
affixes 0.200 0.793 0.274 0.637
affixes? 0.201 0.792 0.272 0.635
last letters 0.481 0.792 0.396 0.634
prepositions 0.136 0.793 0.222 0.635
Web 1T-based
trigrams 0.571 0.790 0.437 0.615
4-grams 0.558 0.797 0.442 0.604
Table 4: Impact of features; cross validation ac-
curacy for only one feature type and all but one
feature type experiments, denoted by 1 and M-1
respectively. ??features shared by both n1 and n2;
??n1 and n2 features conjoined by logical AND
(e.g., n1 is a ?substance? ? n2 is a ?artifact?)
6 Evaluation
6.1 Evaluation Data
To assess the quality of our taxonomy and classi-
fication method, we performed an inter-annotator
agreement study using 150 noun compounds ex-
tracted from a random subset of articles taken
from New York Times articles dating back to 1987
(Sandhaus, 2008). The terms were selected based
upon their frequency (i.e., a compound occurring
twice as often as another is twice as likely to be
selected) to label for testing purposes. Using a
heuristic similar to that used by Lauer (1995), we
only extracted binary noun compounds not part of
a larger sequence. Before reaching the 150 mark,
we discarded 94 of the drawn examples because
they were included in the training set. Thus, our
training set covers roughly 38.5% of the binary
noun compound instances in recent New York
Times articles.
6.2 Annotators
Due to the relatively high speed and low cost of
Amazon?s Mechanical Turk service, we chose to
use Mechanical Turkers as our annotators.
Using Mechanical Turk to obtain inter-
annotator agreement figures has several draw-
backs. The first and most significant drawback is
that it is impossible to force each Turker to label
every data point without putting all the terms onto
a single web page, which is highly impractical
for a large taxonomy. Some Turkers may label
every compound, but most do not. Second,
while we requested that Turkers only work on
our task if English was their first language, we
had no method of enforcing this. Third, Turker
annotation quality varies considerably.
6.3 Combining Annotators
To overcome the shortfalls of using Turkers for an
inter-annotator agreement study, we chose to re-
quest ten annotations per noun compound and then
combine the annotations into a single set of selec-
tions using a weighted voting scheme. To com-
bine the results, we calculated a ?quality? score for
each Turker based upon how often he/she agreed
with the others. This score was computed as the
average percentage of other Turkers who agreed
with his/her annotations. The score for each label
for a particular compound was then computed as
the sum of the Turker quality scores of the Turkers
684
who annotated the compound. Finally, the label
with the highest rating was selected.
6.4 Inter-annotator Agreement Results
The raw agreement scores along with Cohen?s ?
(Cohen, 1960), a measure of inter-annotator agree-
ment that discounts random chance, were calcu-
lated against the authors? labeling of the data for
each Turker, the weighted-voting annotation set,
and the automatic classification output. These
statistics are reported in Table 5 along with the
individual Turker ?quality? scores. The 54 Turk-
ers who made fewer than 3 annotations were ex-
cluded from the calculations under the assumption
that they were not dedicated to the task, leaving a
total of 49 Turkers. Due to space limitations, only
results for Turkers who annotated 15 or more in-
stances are included in Table 5.
We recomputed the ? statistics after conflating
the category groups in two different ways. The
first variation involved conflating all the TOPIC
categories into a single topic category, resulting in
a total of 37 categories (denoted by ?* in Table
5). For the second variation, in addition to con-
flating the TOPIC categories, we conflated the AT-
TRIBUTE categories into a single category and the
PURPOSE/ACTIVITY categories into a single cate-
gory, for a total of 27 categories (denoted by ?**
in Table 5).
6.5 Results Discussion
The .57-.67 ? figures achieved by the Voted an-
notations compare well with previously reported
inter-annotator agreement figures for noun com-
pounds using fine-grained taxonomies. Kim and
Baldwin (2005) report an agreement of 52.31%
(not ?) for their dataset using Barker and Sz-
pakowicz?s (1998) 20 semantic relations. Girju
et al (2005) report .58 ? using a set of 35 se-
mantic relations, only 21 of which were used, and
a .80 ? score using Lauer?s 8 prepositional para-
phrases. Girju (2007) reports .61 ? agreement
using a similar set of 22 semantic relations for
noun compound annotation in which the annota-
tors are shown translations of the compound in for-
eign languages. ? S?aghdha (2007) reports a .68
? for a relatively small set of relations (BE, HAVE,
IN, INST, ACTOR, ABOUT) after removing com-
pounds with non-specific associations or high lex-
icalization. The correlation between our automatic
?quality? scores for the Turkers who performed at
Id N Weight Agree ? ?* ?**
1 23 0.45 0.70 0.67 0.67 0.74
2 34 0.46 0.68 0.65 0.65 0.72
3 35 0.34 0.63 0.60 0.61 0.61
4 24 0.46 0.63 0.59 0.68 0.76
5 16 0.58 0.63 0.59 0.59 0.54
Voted 150 NA 0.59 0.57 0.61 0.67
6 52 0.45 0.58 0.54 0.60 0.60
7 38 0.35 0.55 0.52 0.54 0.56
8 149 0.36 0.52 0.49 0.53 0.58
Auto 150 NA 0.51 0.47 0.47 0.45
9 88 0.38 0.48 0.45 0.49 0.59
10 36 0.42 0.47 0.43 0.48 0.52
11 104 0.29 0.46 0.43 0.48 0.52
12 38 0.33 0.45 0.40 0.46 0.47
13 66 0.31 0.42 0.39 0.39 0.49
14 15 0.27 0.40 0.34 0.31 0.29
15 62 0.23 0.34 0.29 0.35 0.38
16 150 0.23 0.30 0.26 0.26 0.30
17 19 0.24 0.26 0.21 0.17 0.14
18 144 0.21 0.25 0.20 0.22 0.22
19 29 0.18 0.21 0.14 0.17 0.31
20 22 0.18 0.18 0.12 0.10 0.16
21 51 0.19 0.18 0.13 0.20 0.26
22 41 0.02 0.02 0.00 0.00 0.01
Table 5: Annotation results. Id ? annotator id; N
? number of annotations; Weight ? voting weight;
Agree ? raw agreement versus the author?s annota-
tions; ? ? Cohen?s ? agreement; ?* and ?** ? Co-
hen?s ? results after conflating certain categories.
Voted ? combined annotation set using weighted
voting; Auto ? automatic classification output.
least three annotations and their simple agreement
with our annotations was very strong at 0.88.
The .51 automatic classification figure is re-
spectable given the larger number of categories in
the taxonomy. It is also important to remember
that the training set covers a large portion of the
two-word noun compound instances in recent New
York Times articles, so substantially higher accu-
racy can be expected on many texts. Interestingly,
conflating categories only improved the ? statis-
tics for the Turkers, not the automatic classifier.
7 Conclusion
In this paper, we present a novel, fine-grained tax-
onomy of 43 noun-noun semantic relations, the
largest annotated noun compound dataset yet cre-
ated, and a supervised classification method for
automatic noun compound interpretation.
We describe our taxonomy and provide map-
pings to taxonomies used by others. Our inter-
annotator agreement study, which utilized non-
experts, shows good inter-annotator agreement
685
given the difficulty of the task, indicating that our
category definitions are relatively straightforward.
Our taxonomy provides wide coverage, with only
2.32% of our dataset marked as other/lexicalized
and 2.67% of our 150 inter-annotator agreement
data marked as such by the combined Turker
(Voted) annotation set.
We demonstrated the effectiveness of a straight-
forward, supervised classification approach to
noun compound interpretation that uses a large va-
riety of boolean features. We also examined the
importance of the different features, noting a novel
and very useful set of features?the words com-
prising the definitions of the individual words.
8 Future Work
In the future, we plan to focus on the interpretation
of noun compounds with 3 or more nouns, a prob-
lem that includes bracketing noun compounds into
their dependency structures in addition to noun-
noun semantic relation interpretation. Further-
more, we would like to build a system that can
handle longer noun phrases, including preposi-
tions and possessives.
We would like to experiment with including fea-
tures from various other lexical resources to deter-
mine their usefulness for this problem.
Eventually, we would like to expand our data
set and relations to cover proper nouns as well.
We are hopeful that our current dataset and re-
lation definitions, which will be made available
via http://www.isi.edu will be helpful to other re-
searchers doing work regarding text semantics.
Acknowledgements
Stephen Tratz is supported by a National Defense
Science and Engineering Graduate Fellowship.
References
Ahn, K., J. Bos, J. R. Curran, D. Kor, M. Nissim, and
B. Webber. 2005. Question Answering with QED
at TREC-2005. In Proc. of TREC-2005.
Baldwin, T. & T. Tanaka 2004. Translation by machine
of compound nominals: Getting it right. In Proc. of
the ACL 2004 Workshop on Multiword Expressions:
Integrating Processing.
Barker, K. and S. Szpakowicz. 1998. Semi-Automatic
Recognition of Noun Modifier Relationships. In
Proc. of the 17th International Conference on Com-
putational Linguistics.
Berger, A., S. A. Della Pietra, and V. J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics
22:39-71.
Brants, T. and A. Franz. 2006. Web 1T 5-gram Corpus
Version 1.1. Linguistic Data Consortium.
Butnariu, C. and T. Veale. 2008. A concept-centered
approach to noun-compound interpretation. In Proc.
of 22nd International Conference on Computational
Linguistics (COLING 2008).
Butnariu, C., S.N. Kim, P. Nakov, D. ? S?aghdha, S.
Szpakowicz, and T. Veale. 2009. SemEval Task 9:
The Interpretation of Noun Compounds Using Para-
phrasing Verbs and Prepositions. In Proc. of the
NAACL HLT Workshop on Semantic Evaluations:
Recent Achievements and Future Directions.
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Measure-
ment. 20:1.
Crammer, K. and Y. Singer. On the Algorithmic Imple-
mentation of Multi-class SVMs In Journal of Ma-
chine Learning Research.
Downing, P. 1977. On the Creation and Use of English
Compound Nouns. Language. 53:4.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Finin, T. 1980. The Semantic Interpretation of Com-
pound Nominals. Ph.D dissertation University of
Illinois, Urbana, Illinois.
Girju, R., D. Moldovan, M. Tatu and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19.
Girju, R. 2007. Improving the interpretation of noun
phrases with cross-linguistic information. In Proc.
of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL 2007).
Girju, R. 2009. The Syntax and Semantics of
Prepositions in the Task of Automatic Interpreta-
tion of Nominal Phrases and Compounds: a Cross-
linguistic Study. In Computational Linguistics 35(2)
- Special Issue on Prepositions in Application.
Jespersen, O. 1949. A Modern English Grammar on
Historical Principles. Ejnar Munksgaard. Copen-
hagen.
Kim, S.N. and T. Baldwin. 2007. Interpreting Noun
Compounds using Bootstrapping and Sense Collo-
cation. In Proc. of the 10th Conf. of the Pacific As-
sociation for Computational Linguistics.
Kim, S.N. and T. Baldwin. 2005. Automatic
Interpretation of Compound Nouns using Word-
Net::Similarity. In Proc. of 2nd International Joint
Conf. on Natural Language Processing.
686
Lauer, M. 1995. Corpus statistics meet the compound
noun. In Proc. of the 33rd Meeting of the Associa-
tion for Computational Linguistics.
Lees, R.B. 1960. The Grammar of English Nominal-
izations. Indiana University. Bloomington, IN.
Levi, J.N. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press. New York.
McCallum, A. K. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu. 2002.
Moldovan, D., A. Badulescu, M. Tatu, D. Antohe, and
R. Girju. 2004. Models for the semantic classifi-
cation of noun phrases. In Proc. of Computational
Lexical Semantics Workshop at HLT-NAACL 2004.
Nakov, P. and M. Hearst. 2005. Search Engine Statis-
tics Beyond the n-gram: Application to Noun Com-
pound Bracketing. In Proc. the Ninth Conference on
Computational Natural Language Learning.
Nakov, P. 2008. Noun Compound Interpretation
Using Paraphrasing Verbs: Feasibility Study. In
Proc. the 13th International Conference on Artifi-
cial Intelligence: Methodology, Systems, Applica-
tions (AIMSA?08).
Nastase V. and S. Szpakowicz. 2003. Exploring noun-
modifier semantic relations. In Proc. the 5th Inter-
national Workshop on Computational Semantics.
Nastase, V., J. S. Shirabad, M. Sokolova, and S. Sz-
pakowicz 2006. Learning noun-modifier semantic
relations with corpus-based and Wordnet-based fea-
tures. In Proc. of the 21st National Conference on
Artificial Intelligence (AAAI-06).
? S?aghdha, D. and A. Copestake. 2009. Using lexi-
cal and relational similarity to classify semantic re-
lations. In Proc. of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2009).
? S?aghdha, D. 2007. Annotating and Learning Com-
pound Noun Semantics. In Proc. of the ACL 2007
Student Research Workshop.
Rosario, B. and M. Hearst. 2001. Classifying the Se-
mantic Relations in Noun Compounds via Domain-
Specific Lexical Hierarchy. In Proc. of 2001 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-01).
Sandhaus, E. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Sp?rck Jones, K. 1983. Compound Noun Interpreta-
tion Problems. Computer Speech Processing, eds.
F. Fallside and W A. Woods, Prentice-Hall, NJ.
Turney, P. D. 2006. Similarity of semantic relations.
Computation Linguistics, 32(3):379-416
Vanderwende, L. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proc. of
COLING-94.
Warren, B. 1978. Semantic Patterns of Noun-Noun
Compounds. Acta Universitatis Gothobugensis.
Ye, P. and T. Baldwin. 2007. MELB-YB: Prepo-
sition Sense Disambiguation Using Rich Semantic
Features. In Proc. of the 4th International Workshop
on Semantic Evaluations (SemEval-2007).
687
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 323?328,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Models and Training for Unsupervised Preposition Sense Disambiguation
Dirk Hovy and Ashish Vaswani and Stephen Tratz and
David Chiang and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh,avaswani,stratz,chiang,hovy}@isi.edu
Abstract
We present a preliminary study on unsu-
pervised preposition sense disambiguation
(PSD), comparing different models and train-
ing techniques (EM, MAP-EM with L0 norm,
Bayesian inference using Gibbs sampling). To
our knowledge, this is the first attempt at un-
supervised preposition sense disambiguation.
Our best accuracy reaches 56%, a significant
improvement (at p <.001) of 16% over the
most-frequent-sense baseline.
1 Introduction
Reliable disambiguation of words plays an impor-
tant role in many NLP applications. Prepositions
are ubiquitous?they account for more than 10% of
the 1.16m words in the Brown corpus?and highly
ambiguous. The Preposition Project (Litkowski and
Hargraves, 2005) lists an average of 9.76 senses
for each of the 34 most frequent English preposi-
tions, while nouns usually have around two (Word-
Net nouns average about 1.2 senses, 2.7 if monose-
mous nouns are excluded (Fellbaum, 1998)). Dis-
ambiguating prepositions is thus a challenging and
interesting task in itself (as exemplified by the Sem-
Eval 2007 task, (Litkowski and Hargraves, 2007)),
and holds promise for NLP applications such as
Information Extraction or Machine Translation.1
Given a sentence such as the following:
In the morning, he shopped in Rome
we ultimately want to be able to annotate it as
1See (Chan et al, 2007) for how using WSD can help MT.
in/TEMPORAL the morning/TIME he/PERSON
shopped/SOCIAL in/LOCATIVE
Rome/LOCATION
Here, the preposition in has two distinct meanings,
namely a temporal and a locative one. These mean-
ings are context-dependent. Ultimately, we want
to disambiguate prepositions not by and for them-
selves, but in the context of sequential semantic la-
beling. This should also improve disambiguation of
the words linked by the prepositions (here, morn-
ing, shopped, and Rome). We propose using un-
supervised methods in order to leverage unlabeled
data, since, to our knowledge, there are no annotated
data sets that include both preposition and argument
senses. In this paper, we present our unsupervised
framework and show results for preposition disam-
biguation. We hope to present results for the joint
disambiguation of preposition and arguments in a
future paper.
The results from this work can be incorporated
into a number of NLP problems, such as seman-
tic tagging, which tries to assign not only syntac-
tic, but also semantic categories to unlabeled text.
Knowledge about semantic constraints of preposi-
tional constructions would not only provide better
label accuracy, but also aid in resolving preposi-
tional attachment problems. Learning by Reading
approaches (Mulkar-Mehta et al, 2010) also cru-
cially depend on unsupervised techniques as the
ones described here for textual enrichment.
Our contributions are:
? we present the first unsupervised preposition
sense disambiguation (PSD) system
323
? we compare the effectiveness of various models
and unsupervised training methods
? we present ways to extend this work to prepo-
sitional arguments
2 Preliminaries
A preposition p acts as a link between two words, h
and o. The head word h (a noun, adjective, or verb)
governs the preposition. In our example above, the
head word is shopped. The object of the preposi-
tional phrase (usually a noun) is denoted o, in our
example morning and Rome. We will refer to h and
o collectively as the prepositional arguments. The
triple h, p, o forms a syntactically and semantically
constrained structure. This structure is reflected in
dependency parses as a common construction. In
our example sentence above, the respective struc-
tures would be shopped in morning and shopped in
Rome. The senses of each element are denoted by a
barred letter, i.e., p? denotes the preposition sense, h?
denotes the sense of the head word, and o? the sense
of the object.
3 Data
We use the data set for the SemEval 2007 PSD
task, which consists of a training (16k) and a test
set (8k) of sentences with sense-annotated preposi-
tions following the sense inventory of The Preposi-
tion Project, TPP (Litkowski and Hargraves, 2005).
It defines senses for each of the 34 most frequent
prepositions. There are on average 9.76 senses per
preposition. This corpus was chosen as a starting
point for our study since it allows a comparison with
the original SemEval task. We plan to use larger
amounts of additional training data.
We used an in-house dependency parser to extract
the prepositional constructions from the data (e.g.,
?shop/VB in/IN Rome/NNP?). Pronouns and num-
bers are collapsed into ?PRO? and ?NUM?, respec-
tively.
In order to constrain the argument senses, we con-
struct a dictionary that lists for each word all the
possible lexicographer senses according to Word-
Net. The set of lexicographer senses (45) is a higher
level abstraction which is sufficiently coarse to allow
for a good generalization. Unknown words are as-
sumed to have all possible senses applicable to their
respective word class (i.e. all noun senses for words
labeled as nouns, etc).
4 Graphical Model
ph o
p?h? o?
h o
p?h? o?
h o
p?h? o?
a)
b)
c)
Figure 1: Graphical Models. a) 1st order HMM. b)
variant used in experiments (one model/preposition,
thus no conditioning on p). c) incorporates further
constraints on variables
As shown by Hovy et al (2010), preposition
senses can be accurately disambiguated using only
the head word and object of the PP. We exploit this
property of prepositional constructions to represent
the constraints between h, p, and o in a graphical
model. We define a good model as one that reason-
ably constrains the choices, but is still tractable in
terms of the number of parameters being estimated.
As a starting point, we choose the standard first-
order Hidden Markov Model as depicted in Figure
1a. Since we train a separate model for each preposi-
tion, we can omit all arcs to p. This results in model
1b. The joint distribution over the network can thus
be written as
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (1)
P (p?|h?) ? P (o?|p?) ? P (o|o?)
We want to incorporate as much information as
possible into the model to constrain the choices. In
Figure 1c, we condition p? on both h? and o?, to reflect
the fact that prepositions act as links and determine
324
their sense mainly through context. In order to con-
strain the object sense o?, we condition on h?, similar
to a second-order HMM. The actual object o is con-
ditioned on both p? and o?. The joint distribution is
equal to
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (2)
P (o?|h?) ? P (p?|h?, o?) ? P (o|o?, p?)
Though we would like to also condition the prepo-
sition sense p? on the head word h (i.e., an arc be-
tween them in 1c) in order to capture idioms and
fixed phrases, this would increase the number of pa-
rameters prohibitively.
5 Training
The training method largely determines how well the
resulting model explains the data. Ideally, the sense
distribution found by the model matches the real
one. Since most linguistic distributions are Zipfian,
we want a training method that encourages sparsity
in the model.
We briefly introduce different unsupervised train-
ing methods and discuss their respective advantages
and disadvantages. Unless specified otherwise, we
initialized all models uniformly, and trained until the
perplexity rate stopped increasing or a predefined
number of iterations was reached. Note that MAP-
EM and Bayesian Inference require tuning of some
hyper-parameters on held-out data, and are thus not
fully unsupervised.
5.1 EM
We use the EM algorithm (Dempster et al, 1977) as
a baseline. It is relatively easy to implement with ex-
isting toolkits like Carmel (Graehl, 1997). However,
EM has a tendency to assume equal importance for
each parameter. It thus prefers ?general? solutions,
assigning part of the probability mass to unlikely
states (Johnson, 2007). We ran EM on each model
for 100 iterations, or until the perplexity stopped de-
creasing below a threshold of 10?6.
5.2 EM with Smoothing and Restarts
In addition to the baseline, we ran 100 restarts with
random initialization and smoothed the fractional
counts by adding 0.1 before normalizing (Eisner,
2002). Smoothing helps to prevent overfitting. Re-
peated random restarts help escape unfavorable ini-
tializations that lead to local maxima. Carmel pro-
vides options for both smoothing and restarts.
5.3 MAP-EM with L0 Norm
Since we want to encourage sparsity in our mod-
els, we use the MDL-inspired technique intro-
duced by Vaswani et al (2010). Here, the goal
is to increase the data likelihood while keeping
the number of parameters small. The authors use
a smoothed L0 prior, which encourages probabil-
ities to go down to 0. The prior involves hyper-
parameters ?, which rewards sparsity, and ?, which
controls how close the approximation is to the true
L0 norm.2 We perform a grid search to tune the
hyper-parameters of the smoothed L0 prior for ac-
curacy on the preposition against, since it has a
medium number of senses and instances. For HMM,
we set ?trans =100.0, ?trans =0.005, ?emit =1.0,
?emit =0.75. The subscripts trans and emit de-
note the transition and emission parameters. For
our model, we set ?trans =70.0, ?trans =0.05,
?emit =110.0, ?emit =0.0025. The latter resulted
in the best accuracy we achieved.
5.4 Bayesian Inference
Instead of EM, we can use Bayesian inference with
Gibbs sampling and Dirichlet priors (also known as
the Chinese Restaurant Process, CRP). We follow
the approach of Chiang et al (2010), running Gibbs
sampling for 10,000 iterations, with a burn-in pe-
riod of 5,000, and carry out automatic run selec-
tion over 10 random restarts.3 Again, we tuned the
hyper-parameters of our Dirichlet priors for accu-
racy via a grid search over the model for the prepo-
sition against. For both models, we set the concen-
tration parameter ?trans to 0.001, and ?emit to 0.1.
This encourages sparsity in the model and allows for
a more nuanced explanation of the data by shifting
probability mass to the few prominent classes.
2For more details, the reader is referred to Vaswani et al
(2010).
3Due to time and space constraints, we did not run the 1000
restarts used in Chiang et al (2010).
325
result table
Page 1
HMM
0.40 (0.40)
0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53)
0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49)
baseline Vanilla EM
EM, smoothed, 
100 random 
restarts
MAP-EM + 
smoothed L0 
norm
CRP, 10 random 
restarts
our model
Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAP-
EM+smoothed L0 norm on our model. Italics denote significant improvement over baseline at p <.001.
Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters)
6 Results
Given a sequence h, p, o, we want to find the se-
quence of senses h?, p?, o? that maximizes the joint
probability. Since unsupervised methods use the
provided labels indiscriminately, we have to map the
resulting predictions to the gold labels. The pre-
dicted label sequence h?, p?, o? generated by the model
via Viterbi decoding can then be compared to the
true key. We use many-to-1 mapping as described
by Johnson (2007) and used in other unsupervised
tasks (Berg-Kirkpatrick et al, 2010), where each
predicted sense is mapped to the gold label it most
frequently occurs with in the test data. Success is
measured by the percentage of accurate predictions.
Here, we only evaluate p?.
The results presented in Table 1 were obtained
on the SemEval test set. We report results both
with and without against, since we tuned the hyper-
parameters of two training methods on this preposi-
tion. To test for significance, we use a two-tailed
t-test, comparing the number of correctly labeled
prepositions. As a baseline, we simply label all word
types with the same sense, i.e., each preposition to-
ken is labeled with its respective name. When using
many-to-1 accuracy, this technique is equivalent to a
most-frequent-sense baseline.
Vanilla EM does not improve significantly over
the baseline with either model, all other methods
do. Adding smoothing and random restarts increases
the gain considerably, illustrating how important
these techniques are for unsupervised training. We
note that EM performs better with the less complex
HMM.
CRP is somewhat surprisingly roughly equivalent
to EM with smoothing and random restarts. Accu-
racy might improve with more restarts.
MAP-EM with L0 normalization produces the
best result (56%), significantly outperforming the
baseline at p < .001. With more parameters (9.7k
vs. 3.7k), which allow for a better modeling of
the data, L0 normalization helps by zeroing out in-
frequent ones. However, the difference between
our complex model and the best HMM (EM with
smoothing and random restarts, 55%) is not signifi-
cant.
The best (supervised) system in the SemEval task
(Ye and Baldwin, 2007) reached 69% accuracy. The
best current supervised system we are aware of
(Hovy et al, 2010) reaches 84.8%.
7 Related Work
The semantics of prepositions were topic of a special
issue of Computational Linguistics (Baldwin et al,
2009). Preposition sense disambiguation was one of
the SemEval 2007 tasks (Litkowski and Hargraves,
2007), and was subsequently explored in a number
of papers using supervised approaches: O?Hara and
Wiebe (2009) present a supervised preposition sense
disambiguation approach which explores different
settings; Tratz and Hovy (2009), Hovy et al (2010)
make explicit use of the arguments for preposition
sense disambiguation, using various features. We
differ from these approaches by using unsupervised
methods and including argument labeling.
The constraints of prepositional constructions
have been explored by Rudzicz and Mokhov (2003)
and O?Hara and Wiebe (2003) to annotate the se-
mantic role of complete PPs with FrameNet and
Penn Treebank categories. Ye and Baldwin (2006)
explore the constraints of prepositional phrases for
326
semantic role labeling. We plan to use the con-
straints for argument disambiguation.
8 Conclusion and Future Work
We evaluate the influence of two different models (to
represent constraints) and three unsupervised train-
ing methods (to achieve sparse sense distributions)
on PSD. Using MAP-EM with L0 norm on our
model, we achieve an accuracy of 56%. This is a
significant improvement (at p <.001) over the base-
line and vanilla EM. We hope to shorten the gap to
supervised systems with more unlabeled data. We
also plan on training our models with EM with fea-
tures (Berg-Kirkpatrick et al, 2010).
The advantage of our approach is that the models
can be used to infer the senses of the prepositional
arguments as well as the preposition. We are cur-
rently annotating the data to produce a test set with
Amazon?s Mechanical Turk, in order to measure la-
bel accuracy for the preposition arguments.
Acknowledgements
We would like to thank Steve DeNeefe, Jonathan
Graehl, Victoria Fossum, and Kevin Knight, as well
as the anonymous reviewers for helpful comments
on how to improve the paper. We would also like
to thank Morgan from Curious Palate for letting us
write there. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program and by DARPA under con-
tract DOI-NBC N10AP20031.
References
Tim Baldwin, Valia Kordoni, and Aline Villavicencio.
2009. Prepositions in applications: A survey and in-
troduction to the special issue. Computational Lin-
guistics, 35(2):119?149.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Unsu-
pervised Learning with Features. In North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Annual Meeting ? Association
For Computational Linguistics, volume 45, pages 33?
40.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for Finite-State transducers. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 447?455. Association for
Computational Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Jonathan Graehl. 1997. Carmel Finite-state Toolkit.
ISI/USC.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454?462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Ken Litkowski and Orin Hargraves. 2005. The prepo-
sition project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Tom O?Hara and Janyce Wiebe. 2003. Preposi-
tion semantic classification via Penn Treebank and
FrameNet. In Proceedings of CoNLL, pages 79?86.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting se-
mantic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
327
Frank Rudzicz and Serguei A. Mokhov. 2003. Towards
a heuristic categorization of prepositional phrases in
english with wordnet. Technical report, Cornell
University, arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96?100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214. Association for Computational
Linguistics.
Patrick Ye and Tim Baldwin. 2006. Semantic role la-
beling of prepositional phrases. ACM Transactions
on Asian Language Information Processing (TALIP),
5(3):228?244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
328
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 372?381,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatic Interpretation of the English Possessive
Stephen Tratz ?
Army Research Laboratory
Adelphi Laboratory Center
2800 Powder Mill Road
Adelphi, MD 20783
stephen.c.tratz.civ@mail.mil
Eduard Hovy ?
Carnegie Mellon University
Language Technologies Institute
5000 Forbes Avenue
Pittsburgh, PA 15213
hovy@cmu.edu
Abstract
The English ?s possessive construction oc-
curs frequently in text and can encode
several different semantic relations; how-
ever, it has received limited attention from
the computational linguistics community.
This paper describes the creation of a se-
mantic relation inventory covering the use
of ?s, an inter-annotator agreement study
to calculate how well humans can agree
on the relations, a large collection of pos-
sessives annotated according to the rela-
tions, and an accurate automatic annota-
tion system for labeling new examples.
Our 21,938 example dataset is by far the
largest annotated possessives dataset we
are aware of, and both our automatic clas-
sification system, which achieves 87.4%
accuracy in our classification experiment,
and our annotation data are publicly avail-
able.
1 Introduction
The English ?s possessive construction occurs fre-
quently in text?approximately 1.8 times for every
100 hundred words in the Penn Treebank1(Marcus
et al, 1993)?and can encode a number of
different semantic relations including ownership
(John?s car), part-of-whole (John?s arm), extent (6
hours? drive), and location (America?s rivers). Ac-
curate automatic possessive interpretation could
aid many natural language processing (NLP) ap-
plications, especially those that build semantic
representations for text understanding, text gener-
ation, question answering, or information extrac-
tion. These interpretations could be valuable for
machine translation to or from languages that al-
low different semantic relations to be encoded by
?The authors were affiliated with the USC Information
Sciences Institute at the time this work was performed.
the possessive/genitive.
This paper presents an inventory of 17 semantic
relations expressed by the English ?s-construction,
a large dataset annotated according to the this in-
ventory, and an accurate automatic classification
system. The final inter-annotator agreement study
achieved a strong level of agreement, 0.78 Fleiss?
Kappa (Fleiss, 1971) and the dataset is easily the
largest manually annotated dataset of possessive
constructions created to date. We show that our
automatic classication system is highly accurate,
achieving 87.4% accuracy on a held-out test set.
2 Background
Although the linguistics field has devoted signif-
icant effort to the English possessive (?6.1), the
computational linguistics community has given it
limited attention. By far the most notable excep-
tion to this is the line of work by Moldovan and
Badulescu (Moldovan and Badulescu, 2005; Bad-
ulescu and Moldovan, 2009), who define a tax-
onomy of relations, annotate data, calculate inter-
annotator agreement, and perform automatic clas-
sification experiments. Badulescu and Moldovan
(2009) investigate both ?s-constructions and of
constructions in the same context using a list of 36
semantic relations (including OTHER). They take
their examples from a collection of 20,000 ran-
domly selected sentences from Los Angeles Times
news articles used in TREC-9. For the 960 ex-
tracted ?s-possessive examples, only 20 of their se-
mantic relations are observed, including OTHER,
with 8 of the observed relations occurring fewer
than 10 times. They report a 0.82 Kappa agree-
ment (Siegel and Castellan, 1988) for the two
computational semantics graduates who annotate
the data, stating that this strong result ?can be ex-
plained by the instructions the annotators received
1Possessive pronouns such as his and their are treated as
?s constructions in this work.
372
prior to annotation and by their expertise in Lexi-
cal Semantics.?
Moldovan and Badulescu experiment with sev-
eral different classification techniques. They find
that their semantic scattering technique signifi-
cantly outperforms their comparison systems with
its F-measure score of 78.75. Their SVM system
performs the worst with only 23.25% accuracy?
suprisingly low, especially considering that 220 of
the 960 ?s examples have the same label.
Unfortunately, Badulescu and Moldovan (2009)
have not publicly released their data2. Also, it is
sometimes difficult to understand the meaning of
the semantic relations, partly because most rela-
tions are only described by a single example and,
to a lesser extent, because the bulk of the given
examples are of -constructions. For example, why
President of Bolivia warrants a SOURCE/FROM re-
lation but University of Texas is assigned to LOCA-
TION/SPACE is unclear. Their relations and pro-
vided examples are presented below in Table 1.
Relation Examples
POSSESSION Mary?s book
KINSHIP Mary?s brother
PROPERTY John?s coldness
AGENT investigation of the crew
TEMPORAL last year?s exhibition
DEPICTION-DEPICTED a picture of my niece
PART-WHOLE the girl?s mouth
CAUSE death of cancer
MAKE/PRODUCE maker of computer
LOCATION/SPACE Univerity of Texas
SOURCE/FROM President of Bolivia
TOPIC museum of art
ACCOMPANIMENT solution of the problem
EXPERIENCER victim of lung disease
RECIPIENT Josephine?s reward
ASSOCIATED WITH contractors of shipyard
MEASURE hundred (sp?) of dollars
THEME acquisition of the holding
RESULT result of the review
OTHER state of emergency
Table 1: The 20 (out of an original 36) seman-
tic relations observed by Badulescu and Moldovan
(2009) along with their examples.
3 Dataset Creation
We created the dataset used in this work from
three different sources, each representing a distinct
genre?newswire, non-fiction, and fiction. Of the
2Email requests asking for relation definitions and the
data were not answered, and, thus, we are unable to provide
an informative comparison with their work.
21,938 total examples, 15,330 come from sections
2?21 of the Penn Treebank (Marcus et al, 1993).
Another 5,266 examples are from The History of
the Decline and Fall of the Roman Empire (Gib-
bon, 1776), a non-fiction work, and 1,342 are from
The Jungle Book (Kipling, 1894), a collection of
fictional short stories. For the Penn Treebank, we
extracted the examples using the provided gold
standard parse trees, whereas, for the latter cases,
we used the output of an open source parser (Tratz
and Hovy, 2011).
4 Semantic Relation Inventory
The initial semantic relation inventory for pos-
sessives was created by first examining some of
the relevant literature on possessives, including
work by Badulescu and Moldovan (2009), Barker
(1995), Quirk et al (1985), Rosenbach (2002), and
Taylor (1996), and then manually annotating the
large dataset of examples. Similar examples were
grouped together to form initial categories, and
groups that were considered more difficult were
later reexamined in greater detail. Once all the
examples were assigned to initial categories, the
process of refining the definitions and annotations
began.
In total, 17 relations were created, not including
OTHER. They are shown in Table 3 along with ap-
proximate (best guess) mappings to relations de-
fined by others, specifically those of Quirk et al
(1985), whose relations are presented in Table 2,
as well as Badulescu and Moldovan?s (2009) rela-
tions.
Relation Examples
POSSESSIVE my wife?s father
SUBJECTIVE boy?s application
OBJECTIVE the family?s support
ORIGIN the general?s letter
DESCRIPTIVE a women?s college
MEASURE ten days? absense
ATTRIBUTE the victim?s courage
PARTITIVE the baby?s eye
APPOSITION (marginal) Dublin?s fair city
Table 2: The semantic relations proposed by Quirk
et al (1985) for ?s along with some of their exam-
ples.
4.1 Refinement and Inter-annotator
Agreement
The semantic relation inventory was refined us-
ing an iterative process, with each iteration involv-
373
Relation Example HDFRE JB PTB Mappings
SUBJECTIVE Dora?s travels 1083 89 3169 Q:SUBJECTIVE, B:AGENT
PRODUCER?S PRODUCT Ford?s Taurus 47 44 1183 Q:ORIGIN, B:MAKE/PRODUCE
B:RESULT
OBJECTIVE Mowgli?s capture 380 7 624 Q:OBJECTIVE, B:THEME
CONTROLLER/OWNER/USER my apartment 882 157 3940 QB:POSSESSIVE
MENTAL EXPERIENCER Sam?s fury 277 22 232 Q:POSSESSIVE, B:EXPERIENCER
RECIPIENT their bonuses 12 6 382 Q:POSSESSIVE, B:RECIPIENT
MEMBER?S COLLECTION John?s family 144 31 230 QB:POSSESSIVE
PARTITIVE John?s arm 253 582 451 Q:PARTITIVE, B:PART-WHOLE
LOCATION Libya?s people 24 0 955 Q:POSSESSIVE, B:SOURCE/FROM
B:LOCATION/SPACE
TEMPORAL today?s rates 0 1 623 Q:POSSESSIVE, B:TEMPORAL
EXTENT 6 hours? drive 8 10 5 QB:MEASURE
KINSHIP Mary?s kid 324 156 264 Q:POSSESSIVE, B:KINSHIP
ATTRIBUTE picture?s vividness 1013 34 1017 Q:ATTRIBUTE, B:PROPERTY
TIME IN STATE his years in Ohio 145 32 237 QB:POSSESSIVE
POSSESSIVE COMPOUND the [men?s room] 0 0 67 Q:DESCRIPTIVE
ADJECTIVE DETERMINED his fellow Brit 12 0 33
OTHER RELATIONAL NOUN his friend 629 112 1772 QB:POSSESSIVE
OTHER your Lordship 33 59 146 B:OTHER
Table 3: Possessive semantic relations along with examples, counts, and approximate mappings to other
inventories. Q and B represent Quirk et al (1985) and Badulescu and Moldovan (2009), respectively.
HDFRE, JB, PTB: The History of the Decline and Fall of the Roman Empire, The Jungle Book, and the
Penn Treebank, respectively.
ing the annotation of a random set of 50 exam-
ples. Each set of examples was extracted such
that no two examples had an identical possessee
word. For a given example, annotators were in-
structed to select the most appropriate option but
could also record a second-best choice to provide
additional feedback. Figure 1 presents a screen-
shot of the HTML-based annotation interface. Af-
ter the annotation was complete for a given round,
agreement and entropy figures were calculated and
changes were made to the relation definitions and
dataset. The number of refinement rounds was ar-
bitrarily limited to five. To measure agreement,
in addition to calculating simple percentage agree-
ment, we computed Fleiss? Kappa (Fleiss, 1971),
a measure of agreement that incorporates a cor-
rection for agreement due to chance, similar to
Cohen?s Kappa (Cohen, 1960), but which can be
used to measure agreement involving more than
two annotations per item. The agreement and en-
tropy figures for these five intermediate annotation
rounds are given in Table 4. In all the possessive
annotation tables, Annotator A refers to the pri-
mary author and the labels B and C refer to two
additional annotators.
To calculate a final measure of inter-annotator
agreement, we randomly drew 150 examples from
the dataset not used in the previous refinement it-
erations, with 50 examples coming from each of
Figure 1: Screenshot of the HTML template page
used for annotation.
the three original data sources. All three annota-
tors initially agreed on 82 of the 150 examples,
leaving 68 examples with at least some disagree-
ment, including 17 for which all three annotators
disagreed.
Annotators then engaged in a new task in which
they re-annotated these 68 examples, in each case
being able to select only from the definitions pre-
viously chosen for each example by at least one
annotator. No indication of who or how many
people had previously selected the definitions was
374
Figure 2: Semantic relation distribution for the dataset presented in this work. HDFRE: History of the
Decline and Fall of the Roman Empire; JB: Jungle Book; PTB: Sections 2?21 of the Wall Street Journal
portion of the Penn Treebank.
given3. Annotators were instructed not to choose
a definition simply because they thought they had
chosen it before or because they thought some-
one else had chosen it. After the revision pro-
cess, all three annotators agreed in 109 cases and
all three disagreed in only 6 cases. During the re-
vision process, Annotator A made 8 changes, B
made 20 changes, and C made 33 changes. Anno-
tator A likely made the fewest changes because he,
as the primary author, spent a significant amount
of time thinking about, writing, and re-writing the
definitions used for the various iterations. Anno-
tator C?s annotation work tended to be less consis-
tent in general than Annotator B?s throughout this
work as well as in a different task not discussed
within this paper, which probably why Annotator
C made more changes than Annotator B. Prior to
this revision process, the three-way Fleiss? Kappa
score was 0.60 but, afterwards, it was at 0.78. The
inter-annotator agreement and entropy figures for
before and after this revision process, including
pairwise scores between individual annotators, are
presented in Tables 5 and 6.
4.2 Distribution of Relations
The distribution of semantic relations varies some-
what by the data source. The Jungle Book?s dis-
tribution is significantly different from the oth-
3Of course, if three definitions were present, it could be
inferred that all three annotators had initially disagreed.
ers, with a much larger percentage of PARTITIVE
and KINSHIP relations. The Penn Treebank and
The History of the Decline and Fall of the Ro-
man Empire were substantially more similar, al-
though there are notable differences. For instance,
the LOCATION and TEMPORAL relations almost
never occur in The History of the Decline and Fall
of the Roman Empire. Whether these differences
are due to variations in genre, time period, and/or
other factors would be an interesting topic for fu-
ture study. The distribution of relations for each
data source is presented in Figure 2.
Though it is harder to compare across datasets
using different annotation schemes, there are
at least a couple notable differences between
the distribution of relations for Badulescu and
Moldovan?s (2009) dataset and the distribution of
relations used in this work. One such difference is
the much higher percentage of examples labeled as
TEMPORAL?11.35% vs only 2.84% in our data.
Another difference is a higher incidence of the
KINSHIP relation (6.31% vs 3.39%), although it
is far less frequent than it is in The Jungle Book
(11.62%).
4.3 Encountered Ambiguities
One of the problems with creating a list of rela-
tions expressed by ?s-constructions is that some
examples can potentially fit into multiple cate-
gories. For example, Joe?s resentment encodes
375
both SUBJECTIVE relation and MENTAL EXPE-
RIENCER relations and UK?s cities encodes both
PARTITIVE and LOCATION relations. A represen-
tative list of these types of issues along with ex-
amples designed to illustrate them is presented in
Table 7.
5 Experiments
For the automatic classification experiments, we
set aside 10% of the data for test purposes, and
used the the remaining 90% for training. We used
5-fold cross-validation performed using the train-
ing data to tweak the included feature templates
and optimize training parameters.
5.1 Learning Approach
The LIBLINEAR (Fan et al, 2008) package was
used to train linear Support Vector Machine
(SVMs) for all the experiments in the one-against-
the-rest style. All training parameters took their
default values with the exception of the C pa-
rameter, which controls the tradeoff between mar-
gin width and training error and which was set to
0.02, the point of highest performance in the cross-
validation tuning.
5.2 Feature Generation
For feature generation, we conflated the pos-
sessive pronouns ?his?, ?her?, ?my?, and ?your?
to ?person.? Similarly, every term match-
ing the case-insensitive regular expression
(corp|co|plc|inc|ag|ltd|llc)\\.?) was replaced with
the word ?corporation.?
All the features used are functions of the follow-
ing five words.
? The possessor word
? The possessee word
? The syntactic governor of the possessee word
? The set of words between the possessor and
possessee word (e.g., first in John?s first kiss)
? The word to the right of the possessee
The following feature templates are used to gener-
ate features from the above words. Many of these
templates utilize information from WordNet (Fell-
baum, 1998).
? WordNet link types (link type list) (e.g., at-
tribute, hypernym, entailment)
? Lexicographer filenames (lexnames)?top
level categories used in WordNet (e.g.,
noun.body, verb.cognition)
? Set of words from the WordNet definitions
(gloss terms)
? The list of words connected via WordNet
part-of links (part words)
? The word?s text (the word itself)
? A collection of affix features (e.g., -ion, -er,
-ity, -ness, -ism)
? The last {2,3} letters of the word
? List of all possible parts-of-speech in Word-
Net for the word
? The part-of-speech assigned by the part-of-
speech tagger
? WordNet hypernyms
? WordNet synonyms
? Dependent words (all words linked as chil-
dren in the parse tree)
? Dependency relation to the word?s syntactic
governor
5.3 Results
The system predicted correct labels for 1,962 of
the 2,247 test examples, or 87.4%. The accuracy
figures for the test instances from the Penn Tree-
bank, The Jungle Book, and The History of the De-
cline and Fall of the Roman Empire were 88.8%,
84.7%, and 80.6%, respectively. The fact that the
score for The Jungle Book was the lowest is some-
what surprising considering it contains a high per-
centage of body part and kinship terms, which tend
to be straightforward, but this may be because the
other sources comprise approximately 94% of the
training examples.
Given that human agreement typically repre-
sents an upper bound on machine performance
in classification tasks, the 87.4% accuracy figure
may be somewhat surprising. One explanation is
that the examples pulled out for the inter-annotator
agreement study each had a unique possessee
word. For example, ?expectations?, as in ?ana-
lyst?s expectations?, occurs 26 times as the pos-
sessee in the dataset, but, for the inter-annotator
agreement study, at most one of these examples
could be included. More importantly, when the
initial relations were being defined, the data were
first sorted based upon the possessee and then the
possessor in order to create blocks of similar ex-
amples. Doing this allowed multiple examples to
be assigned to a category more quickly because
one can decide upon a category for the whole lot
at once and then just extract the few, if any, that be-
long to other categories. This is likely to be both
faster and more consistent than examining each
376
Agreement (%) Fleiss? ? Entropy
Iteration A vs B A vs C B vs C A vs B A vs C B vs C All A B C
1 0.60 0.68 0.54 0.53 0.62 0.46 0.54 3.02 2.98 3.24
2 0.64 0.44 0.50 0.59 0.37 0.45 0.47 3.13 3.40 3.63
3 0.66 0.66 0.72 0.57 0.58 0.66 0.60 2.44 2.47 2.70
4 0.64 0.30 0.38 0.57 0.16 0.28 0.34 2.80 3.29 2.87
5 0.72 0.66 0.60 0.67 0.61 0.54 0.61 3.21 3.12 3.36
Table 4: Intermediate results for the possessives refinement work.
Agreement (%) Fleiss? ? Entropy
Portion A vs B A vs C B vs C A vs B A vs C B vs C All A B C
PTB 0.62 0.62 0.54 0.56 0.56 0.46 0.53 3.22 3.17 3.13
HDFRE 0.82 0.78 0.72 0.77 0.71 0.64 0.71 2.73 2.75 2.73
JB 0.74 0.56 0.54 0.70 0.50 0.48 0.56 3.17 3.11 3.17
All 0.73 0.65 0.60 0.69 0.61 0.55 0.62 3.43 3.35 3.51
Table 5: Final possessives annotation agreement figures before revisions.
Agreement (%) Fleiss? ? Entropy
Source A vs B A vs C B vs C A vs B A vs C B vs C All A B C
PTB 0.78 0.74 0.74 0.75 0.70 0.70 0.72 3.30 3.11 3.35
HDFRE 0.78 0.76 0.76 0.74 0.72 0.72 0.73 3.03 2.98 3.17
JB 0.92 0.90 0.86 0.90 0.87 0.82 0.86 2.73 2.71 2.65
All 0.83 0.80 0.79 0.80 0.77 0.76 0.78 3.37 3.30 3.48
Table 6: Final possessives annotation agreement figures after revisions.
First Relation Second Relation Example
PARTITIVE CONTROLLER/... BoA?s Mr. Davis
PARTITIVE LOCATION UK?s cities
PARTITIVE OBJECTIVE BoA?s adviser
PARTITIVE OTHER RELATIONAL NOUN BoA?s chairman
PARTITIVE PRODUCER?S PRODUCT the lamb?s wool
CONTROLLER/... PRODUCER?S PRODUCT the bird?s nest
CONTROLLER/... OBJECTIVE his assistant
CONTROLLER/... LOCATION Libya?s oil company
CONTROLLER/... ATTRIBUTE Joe?s strength
CONTROLLER/... MEMBER?S COLLECTION the colonel?s unit
CONTROLLER/... RECIPIENT Joe?s trophy
RECIPIENT OBJECTIVE Joe?s reward
SUBJECTIVE PRODUCER?S PRODUCT Joe?s announcement
SUBJECTIVE OBJECTIVE its change
SUBJECTIVE CONTROLLER/... Joe?s employee
SUBJECTIVE LOCATION Libya?s devolution
SUBJECTIVE MENTAL EXPERIENCER Joe?s resentment
OBJECTIVE MENTAL EXPERIENCER Joe?s concern
OBJECTIVE LOCATION the town?s inhabitants
KINSHIP OTHER RELATIONAL NOUN his fiancee
Table 7: Ambiguous/multiclass possessive examples.
example in isolation. This advantage did not ex-
ist in the inter-annotator agreement study.
5.4 Feature Ablation Experiments
To evaluate the importance of the different types
of features, the same experiment was re-run multi-
ple times, each time including or excluding exactly
one feature template. Before each variation, the C
parameter was retuned using 5-fold cross valida-
tion on the training data. The results for these runs
are shown in Table 8.
Based upon the leave-one-out and only-one fea-
ture evaluation experiment results, it appears that
the possessee word is more important to classifica-
tion than the possessor word. The possessor word
is still valuable though, with it likely being more
377
valuable for certain categories (e.g., TEMPORAL
and LOCATION) than others (e.g., KINSHIP). Hy-
pernym and gloss term features proved to be about
equally valuable. Curiously, although hypernyms
are commonly used as features in NLP classifica-
tion tasks, gloss terms, which are rarely used for
these tasks, are approximately as useful, at least in
this particular context. This would be an interest-
ing result to examine in greater detail.
6 Related Work
6.1 Linguistics
Semantic relation inventories for the English ?s-
construction have been around for some time; Tay-
lor (1996) mentions a set of 6 relations enumerated
by Poutsma (1914?1916). Curiously, there is not
a single dominant semantic relation inventory for
possessives. A representative example of semantic
relation inventories for ?s-constructions is the one
given by Quirk et al (1985) (presented earlier in
Section 2).
Interestingly, the set of relations expressed by
possessives varies by language. For example,
Classical Greek permits a standard of comparison
relation (e.g., ?better than Plato?) (Nikiforidou,
1991), and, in Japanese, some relations are ex-
pressed in the opposite direction (e.g., ?blue eye?s
doll?) while others are not (e.g., ?Tanaka?s face?)
(Nishiguchi, 2009).
To explain how and why such seemingly dif-
ferent relations as whole+part and cause+effect
are expressed by the same linguistic phenomenon,
Nikiforidou (1991) pursues an approach of
metaphorical structuring in line with the work
of Lakoff and Johnson (1980) and Lakoff (1987).
She thus proposes a variety of such metaphors as
THINGS THAT HAPPEN (TO US) ARE (OUR) POS-
SESSIONS and CAUSES ARE ORIGINS to explain
how the different relations expressed by posses-
sives extend from one another.
Certainly, not all, or even most, of the linguis-
tics literature on English possessives focuses on
creating lists of semantic relations. Much of the
work covering the semantics of the ?s construc-
tion in English, such as Barker?s (1995) work,
dwells on the split between cases of relational
nouns, such as sister, that, by their very definition,
hold a specific relation to other real or conceptual
things, and non-relational, or sortal nouns (L?b-
ner, 1985), such as car.
Vikner and Jensen?s (2002) approach for han-
dling these disparate cases is based upon Puste-
jovsky?s (1995) generative lexicon framework.
They coerce sortal nouns (e.g., car) into being
relational, purporting to create a uniform analy-
sis. They split lexical possession into four types:
inherent, part-whole, agentive, and control, with
agentive and control encompassing many, if not
most, of the cases involving sortal nouns.
A variety of other issues related to possessives
considered by the linguistics literature include ad-
jectival modifiers that significantly alter interpre-
tation (e.g., favorite and former), double geni-
tives (e.g., book of John?s), bare possessives (i.e.,
cases where the possessee is omitted, as in ?Eat
at Joe?s?), possessive compounds (e.g., driver?s
license), the syntactic structure of possessives,
definitiveness, changes over the course of his-
tory, and differences between languages in terms
of which relations may be expressed by the geni-
tive. Representative work includes that by Barker
(1995), Taylor (1996), Heine (1997), Partee and
Borschev (1998), Rosenbach (2002), and Vikner
and Jensen (2002).
6.2 Computational Linguistics
Though the relation between nominals in the
English possessive construction has received lit-
tle attention from the NLP community, there is
a large body of work that focuses on similar
problems involving noun-noun relation interpreta-
tion/paraphrasing, including interpreting the rela-
tions between the components of noun compounds
(Butnariu et al, 2010), disambiguating preposition
senses (Litkowski and Hargraves, 2007), or anno-
tating the relation between nominals in more arbi-
trary constructions within the same sentence (Hen-
drickx et al, 2009).
Whereas some of these lines of work use fixed
inventories of semantic relations (Lauer, 1995;
Nastase and Szpakowicz, 2003; Kim and Bald-
win, 2005; Girju, 2009; ? S?aghdha and Copes-
take, 2009; Tratz and Hovy, 2010), other work al-
lows for a nearly infinite number of interpretations
(Butnariu and Veale, 2008; Nakov, 2008). Recent
SemEval tasks (Butnariu et al, 2009; Hendrickx et
al., 2013) pursue this more open-ended strategy. In
these tasks, participating systems recover the im-
plicit predicate between the nouns in noun com-
pounds by creating potentially unique paraphrases
for each example. For instance, a system might
generate the paraphrase made of for the noun com-
378
Feature Type Word(s) Results
L R C G B N LOO OO
Gloss Terms  0.867 (0.04) 0.762 (0.08)
Hypernyms  0.870 (0.04) 0.760 (0.16)
Synonyms  0.873 (0.04) 0.757 (0.32)
Word Itself  0.871 (0.04) 0.745 (0.08)
Lexnames  0.871 (0.04) 0.514 (0.32)
Last Letters  0.870 (0.04) 0.495 (0.64)
Lexnames  0.872 (0.04) 0.424 (0.08)
Link types  0.874 (0.02) 0.398 (0.64)
Link types  0.870 (0.04) 0.338 (0.32)
Word Itself  0.870 (0.04) 0.316 (0.16)
Last Letters  0.872 (0.02) 0.303 (0.16)
Gloss Terms  0.872 (0.02) 0.271 (0.04)
Hypernyms  0.875 (0.02) 0.269 (0.08)
Word Itself  0.874 (0.02) 0.261 (0.08)
Synonyms  0.874 (0.02) 0.260 (0.04)
Lexnames  0.874 (0.02) 0.247 (0.04)
Part-of-speech List  0.873 (0.02) 0.245 (0.16)
Part-of-speech List  0.874 (0.02) 0.243 (0.16)
Dependency  0.872 (0.02) 0.241 (0.16)
Part-of-speech List  0.874 (0.02) 0.236 (0.32)
Link Types  0.874 (0.02) 0.236 (0.64)
Word Itself  0.870 (0.02) 0.234 (0.32)
Assigned Part-of-Speech  0.874 (0.02) 0.228 (0.08)
Affixes  0.873 (0.02) 0.227 (0.16)
Assigned Part-of-Speech  0.873 (0.02) 0.194 (0.16)
Hypernyms  0.873 (0.02) 0.186 (0.04)
Lexnames  0.870 (0.04) 0.170 (0.64)
Text of Dependents  0.874 (0.02) 0.156 (0.08)
Parts List  0.873 (0.02) 0.141 (0.16)
Affixes  0.870 (0.04) 0.114 (0.32)
Affixes  0.873 (0.02) 0.105 (0.04)
Parts List  0.874 (0.02) 0.103 (0.16)
Table 8: Results for leave-one-out and only-one feature template ablation experiment results for all
feature templates sorted by the only-one case. L, R, C, G, B, and N stand for left word (possessor), right
word (possessee), pairwise combination of outputs for possessor and possessee, syntactic governor of
possessee, all tokens between possessor and possessee, and the word next to the possessee (on the right),
respectively. The C parameter value used to train the SVMs is shown in parentheses.
pound pepperoni pizza. Computer-generated re-
sults are scored against a list of human-generated
options in order to rank the participating systems.
This approach could be applied to possessives in-
terpretation as well.
Concurrent with the lack of NLP research on
the subject is the absence of available annotated
datasets for training, evaluation, and analysis. The
NomBank project (Meyers et al, 2004) provides
coarse annotations for some of the possessive con-
structions in the Penn Treebank, but only those
that meet their criteria.
7 Conclusion
In this paper, we present a semantic relation in-
ventory for ?s possessives consisting of 17 rela-
tions expressed by the English ?s construction, the
largest available manually-annotated collection of
possessives, and an effective method for automat-
ically assigning the relations to unseen examples.
We explain our methodology for building this in-
ventory and dataset and report a strong level of
inter-annotator agreement, reaching 0.78 Kappa
overall. The resulting dataset is quite large, at
21,938 instances, and crosses multiple domains,
including news, fiction, and historical non-fiction.
It is the only large fully-annotated publicly-
available collection of possessive examples that
we are aware of. The straightforward SVM-
based automatic classification system achieves
87.4% accuracy?the highest automatic posses-
sive interpretation accuracy figured reported to
date. These high results suggest that SVMs are
a good choice for automatic possessive interpre-
379
tation systems, in contrast to Moldovan and Bad-
ulescu (2005) findings. The data and software
presented in this paper are available for down-
load at http://www.isi.edu/publications/licensed-
sw/fanseparser/index.html.
8 Future Work
Going forward, we would like to examine the var-
ious ambiguities of possessives described in Sec-
tion 4.3. Instead of trying to find the one-best
interpretation for a given possessive example, we
would like to produce a list of all appropriate in-
tepretations.
Another avenue for future research is to study
variation in possessive use across genres, includ-
ing scientific and technical genres. Similarly, one
could automatically process large volumes of text
from various time periods to investigate changes
in the use of the possessive over time.
Acknowledgments
We would like to thank Charles Zheng and Sarah
Benzel for all their annotation work and valuable
feedback.
References
Adriana Badulescu and Dan Moldovan. 2009. A Se-
mantic Scattering Model for the Automatic Interpre-
tation of English Genitives. Natural Language En-
gineering, 15:215?239.
Chris Barker. 1995. Possessive Descriptions. CSLI
Publications, Stanford, CA, USA.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics, pages 81?88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid ? S?aghdha, Stan Szpakowicz, and Tony
Veale. 2009. SemEval-2010 Task 9: The Inter-
pretation of Noun Compounds Using Paraphrasing
Verbs and Prepositions. In DEW ?09: Proceedings
of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 100?
105.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid. ? S?aghdha, Stan Szpakowicz, and Tony
Veale. 2010. SemEval-2010 Task 9: The Interpreta-
tion of Noun Compounds Using Paraphrasing Verbs
and Prepositions. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
39?44.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press, Cambridge, MA.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Edward Gibbon. 1776. The History of the Decline
and Fall of the Roman Empire, volume I of The His-
tory of the Decline and Fall of the Roman Empire.
Printed for W. Strahan and T. Cadell.
Roxanna Girju. 2009. The Syntax and Seman-
tics of Prepositions in the Task of Automatic In-
terpretation of Nominal Phrases and Compounds:
A Cross-linguistic Study. Computational Linguis-
tics - Special Issue on Prepositions in Application,
35(2):185?228.
Bernd Heine. 1997. Possession: Cognitive Sources,
Forces, and Grammaticalization. Cambridge Uni-
versity Press, United Kingdom.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ? S?aghdha, Sebastian
Pad?, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task
8: Multi-Way Classification of Semantic Relations
between Pairs of Nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 94?99.
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov,
Diarmuid ? S?aghdha, Stan Szpakowicz, and
Tony Veale. 2013. Task Description: SemEval-
2013 Task 4: Free Paraphrases of Noun Com-
pounds. http://www.cs.york.ac.uk/
semeval-2013/task4/. [Online; accessed
1-May-2013].
Su Nam Kim and Timothy Baldwin. 2005. Automatic
Interpretation of Noun Compounds using Word-
Net::Similarity. Natural Language Processing?
IJCNLP 2005, pages 945?956.
Rudyard Kipling. 1894. The Jungle Book. Macmillan,
London, UK.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live by. The University of Chicago Press,
Chicago, USA.
George Lakoff. 1987. Women, Fire, and Dangerous
Things: What Categories Reveal about the Mind.
The University of Chicago Press, Chicago, USA.
Mark Lauer. 1995. Corpus Statistics Meet the Noun
Compound: Some Empirical Results. In Proceed-
ings of the 33rd Annual Meeting on Association for
Computational Linguistics, pages 47?54.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of
Prepositions. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages
24?29.
380
Sebastian L?bner. 1985. Definites. Journal of Seman-
tics, 4(4):279.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):330.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank
Project: An Interim Report. In Proceedings of the
NAACL/HLT Workshop on Frontiers in Corpus An-
notation.
Dan Moldovan and Adriana Badulescu. 2005. A Se-
mantic Scattering Model for the Automatic Interpre-
tation of Genitives. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 891?898.
Preslav Nakov. 2008. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study. In
Proceedings of the 13th International Conference on
Artificial Intelligence: Methodology, Systems, and
Applications, pages 103?117.
Vivi Nastase and Stan Szpakowicz. 2003. Explor-
ing Noun-Modifier Semantic Relations. In Fifth In-
ternational Workshop on Computational Semantics
(IWCS-5), pages 285?301.
Kiki Nikiforidou. 1991. The Meanings of the Gen-
itive: A Case Study in the Semantic Structure and
Semantic Change. Cognitive Linguistics, 2(2):149?
206.
Sumiyo Nishiguchi. 2009. Qualia-Based Lexical
Knowledge for the Disambiguation of the Japanese
Postposition No. In Proceedings of the Eighth Inter-
national Conference on Computational Semantics.
Diarmuid ? S?aghdha and Ann Copestake. 2009. Us-
ing Lexical and Relational Similarity to Classify Se-
mantic Relations. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 621?629.
Barbara H. Partee and Vladimir Borschev. 1998. In-
tegrating Lexical and Formal Semantics: Genitives,
Relational Nouns, and Type-Shifting. In Proceed-
ings of the Second Tbilisi Symposium on Language,
Logic, and Computation, pages 229?241.
James Pustejovsky. 1995. The Generative Lexicon.
MIT Press, Cambridge, MA, USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman Inc., New
York.
Anette Rosenbach. 2002. Genitive Variation in En-
glish: Conceptual Factors in Synchronic and Di-
achronic Studies. Topics in English linguistics.
Mouton de Gruyter.
Sidney Siegel and N. John Castellan. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill.
John R. Taylor. 1996. Possessives in English. Oxford
University Press, New York.
Stephen Tratz and Eduard Hovy. 2010. A Taxonomy,
Dataset, and Classifier for Automatic Noun Com-
pound Interpretation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 678?687.
Stephen Tratz and Eduard Hovy. 2011. A Fast, Accu-
rate, Non-Projective, Semantically-Enriched Parser.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1257?1268.
Carl Vikner and Per Anker Jensen. 2002. A Seman-
tic Analysis of the English Genitive. Interation of
Lexical and Formal Semantics. Studia Linguistica,
56(2):191?226.
381
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 222?225,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
ISI: Automatic Classification of Relations Between Nominals Using a
Maximum Entropy Classifier
Stephen Tratz and Eduard Hovy
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
{stratz,hovy}@isi.edu
Abstract
The automatic interpretation of semantic
relations between nominals is an impor-
tant subproblem within natural language
understanding applications and is an area
of increasing interest. In this paper, we
present the system we used to participate
in the SEMEVAL 2010 Task 8 Multi-Way
Classification of Semantic Relations be-
tween Pairs of Nominals. Our system,
based upon a Maximum Entropy classifier
trained using a large number of boolean
features, received the third highest score.
1 Introduction
Semantic interpretation of the relations between
nominals in text is an area of growing interest
within natural language processing (NLP). It has
potential uses for a variety of tasks including ma-
chine translation (Baldwin and Tanaka, 2004) and
question answering (Ahn et al, 2005). The related
and more narrowly-focused problem of automatic
interpretation of noun compounds is the focus of
another SEMEVAL task (Butnariu et al, 2009).
In this paper, we discuss the overall setup of
SEMEVAL 2010 Task 8 (Hendrickx et al, 2010),
present the system we used to participate, and
discuss our system?s performance. Our system,
which consists of a Maximum Entropy classifier
trained using a large variety of boolean features,
received the third highest official score of all the
entries.
2 Related Work
The groundwork for SEMEVAL 2010 Task 8 was
laid by an earlier SEMEVAL task (Girju et al,
2007). For SEMEVAL 2007 Task 4, participants
provided yes or no answers as to whether a partic-
ular relation held for each test example. For SE-
MEVAL 2010, instead of providing a binary out-
put for a single class, participants were required to
perform multi-way classification, that is, select the
most appropriate relation from a set of 10 relations
including the OTHER relation.
The selection of a semantic relation for a pair
of nominals within a sentence is somewhat sim-
ilar to the task of noun compound interpretation,
which is a more restricted problem focused only
upon the nouns within noun compounds. Some
of the recent work on this problem includes that
of Butnariu et al (2009), Girju (2007), Girju
et al (2005), Kim and Baldwin (2005), Nakov
(2008), Nastase et al (2006), Turney (2006), and
? S?aghdha and Copestake (2009).
3 Task Overview
The task is, given a pair of nominals within their
sentence context, select the most appropriate se-
mantic relation from the set of available relations
and indicate the direction of the relation. Though
the final score was based upon the output of the
system trained using the whole training dataset,
participants were also required to submit three ad-
ditional label sets using the first 12.5%, 25%, and
50% of the training data.
3.1 Relation Scheme
The relations were taken from earlier work on
noun compounds by Nastase and Szpakowicz
(2003).
A total of 10 relations were used includ-
ing CAUSE-EFFECT, COMPONENT-WHOLE,
CONTENT-CONTAINER, ENTITY-ORIGIN,
ENTITY-DESTINATION, INSTRUMENT-AGENCY,
MEMBER-COLLECTION, MESSAGE-TOPIC,
OTHER, and PRODUCT-PRODUCER. Since each
relation except the OTHER relation must have its
direction specified, there are a total of 19 possible
labels.
222
3.2 Data
The training and testing datasets consist of 8000
and 2717 examples respectively. Each example
consists of a single sentence with two of its nomi-
nals marked as being the nominals of interest. The
training data also provides the correct relation for
each example.
4 Method
4.1 Classifier
We use a Maximum Entropy (Berger et al, 1996)
classifier trained using a large number of boolean
features. Maximum Entropy classifiers have
proven effective for a variety of NLP problems in-
cluding word sense disambiguation (Tratz et al,
2007; Ye and Baldwin, 2007). We use the imple-
mentation provided in the MALLET machine learn-
ing toolkit (McCallum, 2002). We used the default
Gaussian prior parameter value of 1.0.
4.2 Features Used
We generate features from individual words, in-
cluding both the nominals and their context, and
from combinations of the nominals.
To generate the features for individual words,
we first use a set of word selection rules to se-
lect the words of interest and then run these words
of interest through a variety of feature-generating
functions. Some words may be selected by multi-
ple word selection rules. For example, the word to
the right of the first nominal will be identified by
the word 1 to the right of the 1st nominal rule, the
words that are 3 or less to the right of the 1st nom-
inal rule, and the all words between the nominals
rule. In these cases, the actual feature is the com-
bination of an identifier for the word selection rule
and the output from the feature-generating func-
tion. The 19 word-selection rules are listed below:
Word-Selection Rules
? The {1st, 2nd} nominal (2 rules)
? Word {1, 2, 3} to the {left, right} of the {1st,
2nd} nominal (12 rules)
? Words that are 3 or less to the {left, right} of
the {1st, 2nd} nominal (4 rules)
? All words between the two nominals (1 rule)
The features generated from the individual
words come from a variety of sources includ-
ing word orthography, simple gazetteers, pattern
matching, WordNet (Fellbaum, 1998), and Ro-
get?s Thesaurus.
Orthographic Features
? Capitalization indicator
? The {first, last} {two, three} letters of each
word
? Indicator if the first letter of the word is a/A.
? Indicator for the overall form of the word
(e.g. jump -> a, Mr. -> Aa., SemEval2 ->
AaAa0)
? Indicators for the suffix types (e.g., de-
adjectival, de-nominal [non]agentive, de-
verbal [non]agentive)
? Indicators for a wide variety of affixes includ-
ing those related to degree, number, order,
etc. (e.g., ultra-, poly-, post-)
? Indicators for whether or not a preposition
occurs within either term (e.g., ?down? in
?breakdown?)
Gazetteer and Pattern Features
? Indicators if the word is one of a number of
closed classes (e.g. articles, prepositions)
? Indicator if the word is listed in the U.S. Cen-
sus 2000?s most common surnames list
? Indicator if the word is listed in the U.S. Cen-
sus 2000?s most common first names list
? Indicator if the word is a name or location
based upon some simple regular expressions
WordNet-based Features
? Lemmatized version of the word
? Synonyms for all NN and VB entries for the
word
? Hypernyms for all NN and VB entries for the
word
? All terms in the definitions (?gloss?) for the
word
? Lexicographer file names for the word
? Lists of all link types (e.g., meronym links)
associated with the word
? Part-of-speech indicators for the existence of
NN/VB/JJ/RB entries for the word
? All sentence frames for the word
? All part, member, substance-of holonyms for
the word
Roget?s Thesaurus-based Features
? Roget?s divisions for all noun (and verb) en-
tries for the word
223
Some additional features were extracted using
combinations of the nominals. These include fea-
tures generated using The Web 1T corpus (Brants
and Franz, 2006), and the output of a noun com-
pound interpretation system.
Web 1T N-gram Features
To provide information related to term usage
to the classifier, we extracted trigram and 4-gram
features from the Web 1T Corpus (Brants and
Franz, 2006). Only n-grams containing lowercase
words were used. The nominals were converted
to lowercase if needed. Only n-grams contain-
ing both terms (including plural forms) were ex-
tracted. We included the n-gram, with the nomi-
nals replaced with N1 and N2 respectively, as in-
dividual boolean features. We also included ver-
sions of the n-gram features with the words re-
placed with wild cards. For example, if the nomi-
nals were ?food? and ?basket? and the extracted n-
gram was ?put_N1_in_the_N2?, we also included
?*_N1_in_the_N2?, ?*_N1_*_the_N2?, etc. as
features.
Noun Compound System Features
We also ran the nominals through an in-house
noun compound interpretation system and took its
output as features. We will not be discussing the
noun compound interpretation system in detail in
this paper. It uses a similar approach to that de-
scribed in this paper including a Maximum En-
tropy classifier trained with similar features that
outputs a ranked list of a fixed set of semantic re-
lations. The relations ranked within the top 5 and
bottom 5 were included as features. For example,
if ?Topic of Communication? was the third high-
est relation, both ?top:3:Topic of Communication?
and ?top:*:Topic of Communication? would be in-
cluded as features.
4.3 Feature Filtering
The aforementioned feature generation process
creates a very large number of features. To deter-
mine the final feature set, we first ranked the fea-
tures according to the Chi-Squared metric. Then,
by holding out one tenth of the training data
and trying different thresholds, we concluded that
100,000 features was roughly optimal. For the
cases where we used 12.5%, 25%, and 50%, we
tested on the remaining training data and came up
different cutoffs: 25,000, 40,000, and 60,000, re-
spectively.
5 Results
Each participating site was allowed to submit mul-
tiple runs based upon different systems or config-
urations thereof. The results for the best perform-
ing submissions from each team are presented in
Table 1. The official metric for the task was F1
macroaveraged across the different relations. We
are pleased to see that our system received the
third highest score.
Our results by the different relation types are
shown in Table 2. We note that the performance
on the OTHER relation is relatively low.
Top Results
System Macroaveraged F1
12.5% 25% 50% 100%
UTD 73.08 77.02 79.93 82.19
FBK_IRST 63.61 70.20 73.40 77.62
ISI 66.68 71.01 75.51 77.57
ECNU 49.32 50.70 72.63 75.43
TUD 58.35 62.45 66.86 69.23
ISTI 50.49 55.80 61.14 68.42
FBK_NK 55.71 64.06 67.80 68.02
SEKA 51.81 56.34 61.10 66.33
JU 41.62 44.98 47.81 52.16
UNITN 16.57 18.56 22.45 26.67
Table 1: Final results (macroaveraged F1) for the
highest ranking (based upon result for training
with the complete training set) submissions for
each site. 12.5%, 25%, 50%, and 100% indicate
the amount of training data used.
Results by Relation
Relation P R F1
Cause-Effect 87.77 87.50 87.63
Component-Whole 73.21 75.32 74.25
Content-Container 82.74 84.90 83.80
Entity-Destination 81.51 81.51 81.51
Entity-Origin 81.86 75.19 78.38
Instrument-Agency 64.34 58.97 61.54
Member-Collection 84.62 84.98 84.80
Message-Topic 75.91 79.69 77.76
Product-Producer 70.83 66.23 68.46
Other 43.28 45.37 44.30
Table 2: Precision, recall, and F1 results for our
system by semantic relation.
224
6 Conclusion
We explain the system we used to participate in
the SEMEVAL 2010 Task 8: Multi-Way Classi-
fication of Semantic Relations Between Pairs of
Nominals and present its results. The overall ap-
proach is straight forward, consisting of a single
Maximum Entropy classifier using a large number
of boolean features, and proves effective, with our
system receiving the third highest score of all the
submissions.
7 Future Work
In the future, we are interested in utilizing pars-
ing and part-of-speech tagging to enrich the fea-
ture set. We also want to investigate the relatively
low performance for the OTHER category and see
if we could develop a method to improve this.
Acknowledgements
Stephen Tratz is supported by a National De-
fense Science and Engineering Graduate Fellow-
ship. We would like to thank the organizers of
this task for their hard work in putting this task
together.
References
Ahn, K., J. Bos, J. R. Curran, D. Kor, M. Nissim, and
B. Webber. 2005. Question Answering with QED
at TREC-2005. In Proc. of TREC-2005.
Baldwin, T. & T. Tanaka 2004. Translation by machine
of compound nominals: Getting it right. In Proc. of
the ACL 2004 Workshop on Multiword Expressions:
Integrating Processing.
Berger, A., S. A. Della Pietra, and V. J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22.
Brants, T. and A. Franz. 2006. Web 1T 5-gram Corpus
Version 1.1. Linguistic Data Consortium.
Butnariu, C. and T. Veale. 2008. A concept-centered
approach to noun-compound interpretation. In Proc.
of 22nd International Conference on Computational
Linguistics (COLING 2008).
Butnariu, C., S.N. Kim, P. Nakov, D. ? S?aghdha, S.
Szpakowicz, and T. Veale. 2009. SemEval Task 9:
The Interpretation of Noun Compounds Using Para-
phrasing Verbs and Prepositions. In Proc. of the
NAACL HLT Workshop on Semantic Evaluations:
Recent Achievements and Future Directions.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Girju, R., D. Moldovan, M. Tatu and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19.
Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2007. SemEval-2007 Task 04:
Classification of Semantic Relations between Nom-
inals In Proc. of the 4th Semantic Evaluation Work-
shop (SemEval-2007).
Hendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov, D.
? S?aghdha, Sebastian Pad?, M. Pennacchiotti, L.
Romano, and S. Szpakowicz. 2010. Improving the
interpretation of noun phrases with cross-linguistic
information. In Proc. of the 5th SIGLEX Workshop
on Semantic Evaluation.
Girju, R. 2007. Improving the interpretation of noun
phrases with cross-linguistic information. In Proc.
of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL 2007).
Kim, S.N. and T. Baldwin. 2005. Automatic
Interpretation of Compound Nouns using Word-
Net::Similarity. In Proc. of 2nd International Joint
Conf. on Natural Language Processing.
McCallum, A. K. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu. 2002.
Nakov, P. 2008. Noun Compound Interpretation
Using Paraphrasing Verbs: Feasibility Study. In
Proc. the 13th International Conference on Artifi-
cial Intelligence: Methodology, Systems, Applica-
tions (AIMSA?08).
Nastase V. and S. Szpakowicz. 2003. Exploring noun-
modifier semantic relations. In Proc. the 5th Inter-
national Workshop on Computational Semantics.
Nastase, V., J. S. Shirabad, M. Sokolova, and S. Sz-
pakowicz 2006. Learning noun-modifier semantic
relations with corpus-based and Wordnet-based fea-
tures. In Proc. of the 21st National Conference on
Artificial Intelligence (AAAI-06).
? S?aghdha, D. and A. Copestake. 2009. Using lexi-
cal and relational similarity to classify semantic re-
lations. In Proc. of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2009).
Tratz, S., A. Sanfilippo, M. Gregory, A. Chappell, C.
Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation In Proc. of the 4th International Workshop
on Semantic Evaluations (SemEval-2007).
Turney, P. D. 2006. Similarity of semantic relations.
Computation Linguistics, 32(3):379-416
Ye, P. and T. Baldwin. 2007. MELB-YB: Prepo-
sition Sense Disambiguation Using Rich Semantic
Features. In Proc. of the 4th International Workshop
on Semantic Evaluations (SemEval-2007).
225
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 135?139,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Tweet Conversation Annotation Tool
with a Focus on an Arabic Dialect, Moroccan Darija
Stephen Tratz?, Douglas Briesch?, Jamal Laoudi?, and Clare Voss?
?Army Research Laboratory, Adelphi, MD 20783
?ArtisTech, Inc., Fairfax, VA 22030
{stephen.c.tratz.civ,douglas.m.briesch.civ,jamal.laoudi.ctr,clare.r.voss.civ}@mail.mil
Abstract
This paper presents the DATOOL, a graph-
ical tool for annotating conversations con-
sisting of short messages (i.e., tweets), and
the results we obtain in using it to annotate
tweets for Darija, an historically unwritten
Arabic dialect spoken by millions but not
taught in schools and lacking standardiza-
tion and linguistic resources.
With the DATOOL, a native-Darija
speaker annotated hundreds of mixed-
language and mixed-script conversations
at approximately 250 tweets per hour. The
resulting corpus was used in developing
and evaluating Arabic dialect classifiers
described briefly herein.
The DATOOL supports downstream dis-
course analysis of tweeted ?conversations?
by mapping extracted relations such as,
who tweets to whom in which language,
into graph markup formats for analysis in
network visualization tools.
1 Overview
For historically unwritten languages, few textual
resources exist for developing NLP applications
such as machine translation engines. Even when
audio resources are available, difficulties arise
when converting sound to text (Robinson and
Gadelii, 2003). Increasingly, however, with the
widespread use of mobile phones, these languages
are being written in social media such as Twitter.
Not only can these languages be written in multi-
ple scripts, but conversations, and even individual
messages, often involve multiple languages. To
build useful textual resources for documenting and
translating these languages (e.g., bilingual dictio-
naries), tools are needed to assist in language an-
notation for this noisy, multiscript, multilingual
form of communication.
This paper presents the Dialect Annotation Tool
(DATOOL), a graphical tool for annotating conver-
sations consisting of short messages (i.e., tweets),
and the results we obtain in using it to annotate
tweets for Darija, an historically unwritten North
African Arabic dialect spoken by millions but not
taught in schools and lacking in standardardiza-
tion and linguistic resources. The DATOOL can
retrieve the conversation for each tweet on a user?s
timeline or via Apollo (Le et al, 2011) and display
the discourse, enabling annotators to make more
informed decisions. It has integrated classifiers for
automatically annotating data so a user can either
verify or alter the automatically-generated annota-
tions rather than start from scratch. The tool can
also export annotated data to GEPHI (Bastian et
al., 2009), an open source network visualization
tool with many layout algorithms, which will fa-
cilitate future ?code-switching? research.
2 Tool Description
2.1 Version 1.0
The first version of the tool is depicted in Figure
1. It is capable of loading a collection of tweets
and extracting the full conversations they belong
to. Each conversation is displayed within its own
block in the conversation display table. An anno-
tator can mark multiple tweets as Darija (or other
language) by selecting multiple checkboxes in the
lefthand side of the table. Also, if a tweet is writ-
ten in multiple languages, the annotator can anno-
tate the different sections using the Message text
box below the conversation display table.
The tool also calculates user and collection level
summary statistics, which it displays below the
main annotation section.
We worked with a Darija-speaking annotator
during the tool?s development, who provided
valuable feedback, helping to shape the overall
design of the tool and improve its functionality.
135
Figure 1: The Dialect Annotation Tool (DATOOL) displaying a possible Twitter conversation.
Data Annotation Using version 1.0, the annotator
marked up 3013 tweets from 3 users for the pres-
ence of the Darija (approximately 1,000 per user),
averaging about 250 tweets per hour. Of the 1,400
tweets with Arabic script, 1,013 contained Darija.
This annotated data is used to evaluate the Arabic
dialect classifier discussed in Section 3.
2.2 Version 2.0
The second version of the tool contains the ad-
ditional ability to invoke pre-trained classification
models to automatically annotate tweets. The tool
displays the classifier?s judgment confidence next
to each tweet, and the user can set a minimal con-
fidence threshold, below which automatic annota-
tions are hidden. Figure 2 illustrates the new clas-
sification functionality.
2.3 XML Output
The DATOOL stores data in an XML-based for-
mat that can be reloaded for continuing or re-
vising annotation. It can also export four differ-
ent views of the data in Graph Exchange XML
Format (GEXF), a format that can be read by
GEPHI. In the social network view, users are
represented by nodes, and tweets are represented
as directed edges between the nodes. The in-
formation network view displays tweets as nodes
with directed edges between time-ordered tweets
within a conversation. In the social-information
network view, both users and tweets are repre-
sented by nodes, and there are directed edges both
from tweet senders to their tweets and from tweets
to recipients. The social-information network plus
view provides all the information of both the so-
cial network and the information network.
3 Classifier
For the second version of the DATOOL, we inte-
grated an Arabic dialect classifier capable of dis-
tinguishing among Darija, Egyptian, Gulf, Lev-
antine and MSA with the goal of improving the
speed and consistency of the annotation process.
Though language classification is sometimes
viewed as a solved problem (McNamee, 2005),
with some experiments achieving over 99% ac-
curacy (Cavnar and Trenkle, 1994), it is signifi-
cantly more difficult when distinguishing closely-
related languages or short texts (Vatanen et al,
2010; da Silva and Lopes, 2006). The only lan-
guage classification work for distinguishing be-
tween these closely-related Arabic dialects that
we are aware of was performed by Zaidan and
Callison-Burch (2013). They collected web com-
mentary data written in MSA, Egyptian, Levan-
tine, and Gulf and performed dialect identifica-
tion experiments, their strongest classifier achiev-
136
Figure 2: Screenshot showcasing the automatic classification output, including confidence values.
ing 81.0% accuracy.
3.1 Training Data
Since Zaidan and Callison-Burch?s dataset in-
cludes no Darija, we collected Darija exam-
ples from the following sources to augment their
dataset: Moroccan jokes from noktazwina.
com, web pages collected using Darija-specific
query terms with a popular search engine, and
37,538 Arabic script commentary entries from
hespress.com (a Moroccan news website).
Nearly all the joke (N=399) and query term
(N=874) data contained Darija. By contrast, the
commentary data was mostly MSA. To extract
a subset of the commentary entries most likely
to contain Darija, we applied an iterative, semi-
supervised approach similar to that described by
Tratz and Sanfilippo (2007), in which the joke and
query term data were treated as initial seeds and,
in each iteration, a small portion of commentary
data with the highest Darija scores were added to
the training set. After having run this process to
its completion, we examined 131 examples at in-
tervals of 45 from the resulting ranked list of com-
mentary. The 62nd example was the first of these
to have been incorrectly classified as containing
Darija. We thus elected to assume all examples up
to the 61st of the 131 contain Darija, for a total of
2,745 examples (61*45=2,745). As an additional
check, we examined two more commentary entries
from each of the 61 blocks, finding that 118 of 122
contain Darija.
3.2 Initial Classifier
The integrated dialect classifier is a Maximum En-
tropy model (Berger et al, 1996) that we train us-
ing the LIBLINEAR (Fan et al, 2008) toolkit.
In preprocessing, Arabic diacritics are removed,
all non-alphabetic and non-Arabic script charac-
ters are converted to whitespace, and sequences of
any repeating character are collapsed to a single
character. The following set of feature templates
are applied to each of the resulting whitespace-
separated tokens:
? The full token
? ?Shape? of the token?all consonants are replaced by
the letter C, alefs by A, and waws and yehs by W
? First character plus the last character (if length ? 2)
? Character unigrams, bigrams, and trigrams
? The last character of the token plus the first character
of the next token
? Prefixes of length 1, 2, and 3
? Indicators that token starts with mA and
? ends with $
? the next token ends with $
? is length 5 or greater
3.3 LDA Model
As an exploratory effort, we investigated using La-
tent Direchlet Allocation (LDA) (Blei et al, 2003)
as a method of language identification. Unfor-
tunately, using the aforementioned feature tem-
plates, LDA produced topics that corresponded
poorly with the training data labels. But, after
several iterations of feature engineering, the topics
began to reflect the dialect distinctions. Our final
LDA model feature templates are listed below.
? The full token
? Indicators that the token contains
? theh; thal; zah; theh, thal, or zah
? Indicators the token is of length 5+ and starts with
? hah plus yeh, teh, noon, or alef
? seen plus yeh, teh, noon, or alef
? beh plus yeh, teh, noon, or alef
? ghain plus yeh, teh, or noon
? or kaf plus yeh, teh, or noon
? Indicators that token starts with mA and
? ends with $
? the next token ends with $
? is length 5 or greater
The following features produced using the LDA
model for each document are given to the Maxi-
mum Entropy classifier: 1) indicator of the most-
likely cluster, 2) product of scores for each pair of
clusters.
3.4 Classifier Evaluation
We evaluated the versions of the classifier by ap-
plying them to the annotated data discussed in
137
Section 2.1. The initial classifier without the
LDA-derived features achieved 96.9% precision
and 24.1% recall. The version with LDA-derived
features achieved 97.2% precision and 44.1% re-
call, a substantial improvement. Upon review, we
concluded that most cases where the classifier ?in-
correctly? selected the Darija label were due to er-
rors in the gold standard.
4 Analysis of Annotated Conversations
Visualization of Darija in Conversations
The DATOOL may recover the conversation in
which a tweet occurs, providing the annotator with
the tweet?s full, potentially-multilingual context.
To visualize the distribution of Darija1 by script
in ?1K tweets from each user?s conversations, the
DATOOL transforms and exports annotated data
into a GEXF information network (cf. Figure 3),
which can be displayed in GEPHI.2 Currently,
GEPHI displays at most one edge between any two
nodes?GEPHI automatically augments the edge?s
weight for each additional copy of the edge.
The Darija in this user?s conversations, unlike
our two other users, is predominantly Romanized.
With more data, we plan to assess the impact of
one user?s script and language choice on others.
Figure 3: Information network visualization.
Red?contains Romanized Darija; green?
contains Arabic-script Darija; blue?no Darija.
Code-Switching
The alternation of Darija with non-Darija in the
1In our initial annotation work, words and tweets in lan-
guages other than Darija received no markup.
2GEPHI?s Force Atlas layout automatically positions sub-
graphs by size, with larger ones further away from the center.
information network (red and green nodes vs.
blue nodes) within conversations is consistent with
well-known code-switching among Arabic speak-
ers, extending spoken discourse into informal
writing (Bentahila and Davies, 1983; Redouane,
2005). Code-switching also appears within our
tweet corpus where Romanized Darija frequently
alternates with French. Given the prevalence of
code-switching within tweets, future work will en-
tail training a Roman-script classifier at the to-
ken level.3 Since our DATOOL already supports
token-level as well as multi-token, tweet-internal
annotation in the mid-screen Message box, our
current corpus provides a seed set for this effort.
5 Conclusion and Future Work
The DATOOL now supports semi-automated an-
notation of tweet conversations for Darija. As
we scale the process of building low-resource lan-
guage corpora, we will document its impact on an-
notation time when few native speakers are avail-
able, a condition also relevant and critical to pre-
serving endangered languages. We have begun ex-
tending the classifier to support additional Arabic
script languages (e.g., Farsi, Urdu), leveraging re-
sources from others (Bergsma et al, 2012).
Many other open questions remain regarding
the annotation process, the visualizations, and the
human expert. Which classified examples should
the language expert review? When should an an-
notator adjust the confidence threshold in the DA-
TOOL? For deeper linguistic analysis and code-
switching prediction, would seeing participants
and tweets, turn by turn, in network diagrams such
as Figure 4 help experts understand new patterns
emerging in tweet conversations?
Figure 4: Social-Information Network Plus.
3As described in Section 3, our current classifier works at
the tweet level and only on Arabic-script tweets.
138
Acknowledgments
We would like to thank Tarek Abdelzaher for all
his feedback regarding our work and guidance in
using Apollo. We would also like to thank our re-
viewers for their valuable comments and sugges-
tions.
References
Mathieu Bastian, Sebastien Heymann, and Mathieu Ja-
comy. 2009. Gephi: An Open Source Software for
Exploring and Manipulating Networks. In Interna-
tional AAAI Conference on Weblogs and Social Me-
dia.
Abdelali Bentahila and Eirlys E Davies. 1983. The
Syntax of Arabic-French Code-Switching. Lingua,
59(4):301?330.
Adam L. Berger, Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A Maximum En-
tropy Approach to Natural Language Processing.
Computational Linguistics, 22(1):39?71.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
Identification for Creating Language-Specific Twit-
ter Collections. In Proceedings of the 2012 Work-
shop on Language in Social Media (LSM 2012),
pages 65?74.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. The Journal of Machine
Learning Research, 3:993?1022.
William B Cavnar and John M Trenkle. 1994. N-
gram-based text categorization. Ann Arbor MI,
48113(2):161?175.
Joaquim Ferreira da Silva and Gabriel Pereira Lopes.
2006. Identification of document language is not yet
a completely solved problem. In Computational In-
telligence for Modelling, Control and Automation,
2006 and International Conference on Intelligent
Agents, Web Technologies and Internet Commerce,
International Conference on, pages 212?212. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Hieu Khac Le, Jeff Pasternack, Hossein Ahmadi,
M. Gupta, Y. Sun, Tarek F. Abdelzaher, Jiawei Han,
Dan Roth, Boleslaw K. Szymanski, and Sibel Adali.
2011. Apollo: Towards factfinding in participatory
sensing. In IPSN, pages 129?130.
Paul McNamee. 2005. Language identification: A
solved problem suitable for undergraduate instruc-
tion. Journal of Computing Sciences in Colleges,
20(3):94?101.
Rabia Redouane. 2005. Linguistic constraints on
codeswitching and codemixing of bilingual Moroc-
can Arabic-French speakers in Canada. In ISB4:
Proceedings of the 4th International Symposium on
Bilingualism, pages 1921?1933.
Clinton Robinson and Karl Gadelii. 2003. Writing
Unwritten Languages, A Guide to the Process.
http://portal.unesco.org/education/en/ev.php-URL
ID=28300&URL DO=DO TOPIC&URL SECTIO
N=201.html, UNESCO, Paris, France. December.
Stephen Tratz and Antonio Sanfilippo. 2007. A
High Accuracy Method for Semi-supervised Infor-
mation Extraction. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
169?172.
Tommi Vatanen, Jaakko J Va?yrynen, and Sami Virpi-
oja. 2010. Language identification of short text seg-
ments with n-gram models. In Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation LREC?10.
Omar Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguistics
(To Appear).
139
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 34?45,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
A Cross-Task Flexible Transition Model for Arabic Tokenization, Affix
Detection, Affix Labeling, POS Tagging, and Dependency Parsing
Stephen Tratz
Army Research Laboratory
Adelphi Laboratory Center
2800 Powder Mill Road
Adelphi, MD 20783
stephen.c.tratz.civ@mail.mil
Abstract
This paper describes cross-task flexible tran-
sition models (CTF-TMs) and demonstrates
their effectiveness for Arabic natural language
processing (NLP). NLP pipelines often suffer
from error propagation, as errors committed
in lower-level tasks cascade through the re-
mainder of the processing pipeline. By al-
lowing a flexible order of operations across
and within multiple NLP tasks, a CTF-TM can
mitigate both cross-task and within-task error
propagation. Our Arabic CTF-TM models to-
kenization, affix detection, affix labeling, part-
of-speech tagging, and dependency parsing,
achieving state-of-the-art results. We present
the details of our general framework, our Ara-
bic CTF-TM, and the setup and results of our
experiments.
1 Introduction
Natural Language Processing (NLP) systems often
consist of a series of NLP components, each trained
to perform a specific task such as parsing. These
pipelines tend to suffer from error propagation?
errors introduced by early components cascade
through the remainder of the pipeline causing subse-
quent components to commit additional errors. Par-
tial solutions from higher-level tasks (e.g., parsing)
can aid in resolving the difficult decisions that must
be made in solving lower-level tasks, as with part-
of-speech tagging the classic ?garden path? sentence
example ?The horse raced past the barn fell.? To
this end, this paper presents cross-task flexible tran-
sition models (CTF-TMs), which model multiple
tasks and solve these tasks in a more flexible or-
der than pipeline approaches. We implement and
experiment with a CTF-TM for Arabic1 language
processing and report experimental results for it on
Arabic tokenization (i.e., clitic separation), affix de-
tection, affix labeling, part-of-speech tagging, and
dependency parsing.
In addition to error propagation between mod-
ules within a parsing pipeline, errors may propa-
gate within the parsing process itself due to the
fixed order of operations of the parser. This is com-
mon for standard transition-based dependency pars-
ing models (McDonald and Nivre, 2007), such as
shift-reduce parsers, which incrementally construct
a parse by processing the input in a fixed left-to-right
or right-to-left fashion. However, using a transition
model that allows a more flexible order of opera-
tions, such as Goldberg and Elhadad?s (2010) parser,
allows difficult decisions to be postponed until later,
when more of the solution has been constructed.
CTF-TMs extend this approach by modeling mul-
tiple tasks and providing this flexibility across tasks
so that no one task needs to be complete before an-
other can be partially solved.
As a morphologically rich language, Arabic re-
quires a significant number of processing steps. Ara-
bic uses a variety of affixes to inflect for case, gen-
der, number (including dual), and mood, has clitics
that attach to other words, permits both VSO and
SVO constructions, and rarely includes short vow-
els in written form. The presence of clitics and the
absence of written short vowels are particularly sig-
nificant sources of ambiguity. As Tsarfaty (2006)
argues for Modern Hebrew, a Semitic language that
shares these characteristics, we contend that mor-
1This paper focuses on Modern Standard Arabic rather than
any of the dialects.
34
phological analysis and parsing should be done in
a unified framework, such as a CTF-TM, rather than
by separate components.
In this paper, we describe CTF-TMs, which can
be used for a wide variety of NLP tasks, and present
our Arabic CTF-TM for Arabic tokenization, af-
fix detection, affix labeling, part-of-speech tagging,
and dependency parsing as well as the results ob-
tained in applying it to our dependency conversion
of the Penn Arabic Treebank (ATB) (Maamouri et
al., 2004; Maamouri and Bies, 2004). We find
that our Arabic CTF-TM for tokenization, affix de-
tection, affix labeling, POS tagging, and parsing
achieves slightly better results than a similar CTF-
TM that performs all the tasks except parsing. The
CTF-TM that supports parsing appears to be more
accurate at distinguishing between passive and ac-
tive verbs as well as between nouns and adjectives?
cases where the context is crucial for proper inter-
pretation due to Arabic?s ambiguities. Our system
achieves tokenization accuracy similar to Kulick?s
(2011) state-of-the-art system for a standard split of
the ATB part 3, and, in our experiments using ATB
parts 1?3, our system achieves the highest labeled
attachment, unlabeled attachment, and clitic separa-
tion figures (including pronomial clitics) for Arabic
yet reported (although no other work can be com-
pared directly).
2 Relevant Arabic Linguistics
Arabic has rich morphology, with a wide array of af-
fixes and clitics and inflecting for case, number, gen-
der, and, occasionally, mood. Coordinating conjunc-
tions, pronouns, and most true prepositions, along
with some other particles and the definite article,
usually occur as clitics in Arabic. Thus, a space-
delimited2 sequence of Arabic characters may con-
sist of multiple words, and identifying the bound-
aries between these must be done in order to produce
syntactic parses. These boundaries can?t be detected
perfectly using simple deterministic rules. Signifi-
cantly, short vowels, which are expressed using dia-
critics, are not typically written in Arabic, resulting
in pervasive ambiguity. For example, active and pas-
sive forms of verbs vary only in their diacritics, and
nouns and adjectives are both derived from Arabic
2Technically, space-and-punctuation-deliminated.
roots using the same templates and, thus, look sim-
ilar. A single Arabic token may permit a variety of
different analyses, as the example in Table 1 illus-
trates.
??@? wAlY ?ruler?
?


+??@+? w+AlY+y ?and to me?
?


?

@+? w+<ly ?and I follow?
?


+?

@+? w+|l+y ?and my clan?
?


?

@+? w+|ly ?and automatic?
Table 1: Possible interpretations for the text wAlY
(Habash and Rambow, 2005).
3 CTF-TM Framework
Error propagation is not simply a problem that oc-
curs between components in a pipeline but one that
often occurs within a single component?s process-
ing. Since transition systems can use the partially
built solution for feature generation, incorrect ac-
tions taken early on result not only in an invalid
final solution, but the invalid partial solution may
dissuade the system from making correct decisions
with respect to other parts of the solution. If a
transition system can postpone decisions it is not
confident of until later, the partial solution created
by performing other actions may provide more or
better information that enables the system to prop-
erly resolve more difficult decisions. This ?easy-
first? strategy is adopted by Goldberg and Elhadad?s
(2010) parsing system, which starts with an ordered
list of unattached words and, in each iteration, cre-
ates a new arc between any of the adjacent pairs
of words in the list and removes the daughter node
(word) from the list.
This strategy is much more flexible than shift-
reduce style parsing because the system has more
options available to it at any one step for building
up the solution. However, simply having flexibil-
ity within a single component does not reduce er-
ror propagation to or from other components in a
pipeline and, to mitigate the potential for this, one
may use a cross-task flexible transition model (CTF-
TM) that does not have to wait for lower level tasks
to be 100% complete before starting work on higher
level tasks.
35
McDonald and Nivre (2007) define a transition
system as follows:
1. a set C of parse configurations, each of which
defines a (partially built) dependency graph G
2. a set T of transitions, each a function t : C ?
C
3. for every sentence x = w0, w1, ..., wn
(a) a unique initial configuration cx
(b) a set Cx of terminal configurations
These systems start at the initial configuration and
use a scoring function s : C ? T ? R to repeatedly
select and follow the locally optimal transition, stop-
ping when a terminal configuration is reached.
We make a few changes to McDonald and Nivre?s
transition system definition in order to explain our
framework. First, to support modeling of multiple
tasks, instead of referring to parse configurations,
we simply use the term configuration, defining it to
represent a partially built solution rather than a de-
pendency graph. Second, we specify that there ex-
ists a routine for enumerating a set of anchors for
any given configuration. Anchors are an organiza-
tional concept for dealing with arbitrary data struc-
tures; each anchor acts as a hook into some por-
tion of the configuration that may be changed. Fi-
nally, there exist routines for enumerating legal ac-
tions that can be performed in relation to any anchor
and, for training, a routine for verifying that per-
forming a given action will lead to a configuration
consistent with the final solution. The performance
of an action constitutes a transition between config-
urations.3 It is quite straightfoward to adapt Gold-
berg and Elhadad?s (2010) parsing approach to any
configuration that is indexable by anchors, and in so
doing we are able to create cross-task flexible tran-
sition models.
3For example, in a fixed order, one-word-at-a-time POS tag-
ging system, there would be only one anchor?the word cur-
rently being labeled?but, for a one-at-a-time POS tagger capa-
ble of tagging words in any order, the anchor set would contain
the entire list of still-unlabeled words. The POS labeling ac-
tions for the anchors in each of these cases constitute transitions
to new configurations.
4 Our Arabic CTF-TM
4.1 Tasks
Our Arabic CTF-TM system performs the follow-
ing tasks: split a series of space-delimited Arabic to-
kens into words (tokenization), identify the bounds
of affixes within the words (affix detection), label
the affixes (affix labeling), label the words with their
parts of speech (POS tagging), and construct a la-
beled dependency tree (dependency parsing). Tok-
enization, part-of-speech tagging, and dependency
parsing are frequent topics in NLP literature. Affix
identification and labeling are parts of morphologi-
cal analysis that are sometimes completely ignored
or are performed using an external morphological
analyzer. Identifying affixes and labeling them can
help the overall system contend with lexical sparsity
issues as well as utilize the information encoded by
the affixes (e.g., person).
4.2 Anchors and Actions
The configurations that the system deals with have
anchors of two types, token anchors and affix an-
chors. The initial configuration consists of an or-
dered list of neighboring token anchors (neighbor-
hood), each of which corresponds to one of the orig-
inal space-delimited tokens. As processing contin-
ues, new token anchors may be created by splitting
off clitics, new affix anchors may be created by iden-
tifying substrings of tokens as affixes, and token an-
chors will be removed from the ordered list to be-
come daughter nodes of their neighbors, attached via
labeled dependency arcs. The complete list of ac-
tions that can be performed on the anchors, which, as
described earlier, constitute the transitions between
configurations, are as follows:
Tokenization
1. Separate a proclitic of length l from a token anchor, cre-
ating a new token anchor for the clitic and reducing the
width of the original token
2. Separate an enclitic of length l from a token anchor, cre-
ating a new token anchor for the clitic and reducing the
width of the original token
Affix Detection
3. Create an affix (prefix) anchor from the first l characters
of a token anchor that are not labeled as part of an affix
(If the affix is the definite determiner Al, which we treat
as an affix for consistency with the ATB?s tokenization
scheme, it is automatically labeled as DET and removed
from further processing for the sake of efficiency.)
36
4. Create an affix (suffix) anchor from the last l characters
of a token anchor that are not labeled as part of an affix
POS and affix labeling
5. Assign a label l to the anchor (Affixes are automatically
removed from further processing after labeling)
Dependency parsing
6. Create a dependency arc with label d between a token an-
chor and the preceding unattached neighbor token anchor
and remove the attached anchor from the neighborhood
7. Create a dependency arc with label d between a token
anchor and the following unattached neighbor token an-
chor with label l and remove the attached anchor from the
neighborhood
8. Swap the position of two neighboring token anchors (this
adds Nivre-style (2009) non-projectivity support as de-
scribed by Tratz and Hovy (2011))
General
9. Mark an anchor as fully processed and remove it from
further processing
The dependency labels, POS labels, clitic lengths,
and affix lengths used to define the actions are all
collected automatically from the training data. 4
The actions are subject to the following con-
straints/preconditions:
1. Labeling is only valid if the anchor has not been labeled
2. Tokens may only be labeled with token labels, prefixes
with prefix labels, and suffixes with suffix labels (as de-
termined by the training data)
3. Affix strings observed in the training data may not be la-
beled with any label not used with them in the training
data
4. Token anchors may not be assigned labels that do not co-
occur with the labels of any already-labeled affixes and
vice versa
5. A prefix creation action may only be applied to a token
anchor that doesn?t yet have a prefix
6. Proclitics may not be created and detached if the token
already has a prefix, and enclitics are similarly restricted
by the presence of a suffix
7. Clitics may not be detached from a token that has already
been attached to another token via a dependency arc
8. A dependency arc with label x may not be created be-
tween token anchors T1 and T2 if 1) one or both are la-
beled and 2) no arc between similarly POS tagged an-
chors exists in the training data
9. Swap actions may not undo previous swaps
10. Marking a token anchor as fully processed may only oc-
cur if it has already been labeled, and it must either be 1)
the last unattached token or 2) already attached
4Training examples with clitics that are invalid (i.e., too
long) are discarded at the beginning of training.
4.3 Scoring Function
For our scoring function, like Goldberg and El-
hadad, we use the structured perceptron algorithm
(Collins, 2002) with parameter averaging. This
has previously been shown to produce strong re-
sults when modeling multiple NLP tasks (Zhang and
Clark, 2008).
4.4 Features
For a given anchor5, the system extracts features
from the partially built solution (e.g., the text, af-
fixes, POS tags, and syntactic dependencies of the
anchor and nearby anchors). The same feature tem-
plates are used for all action types except the affix la-
beling actions?affix labeling is applied to affix an-
chors instead of word-level anchors, and, since all
templates are defined relative to an anchor, the tem-
plates must be different. The system uses no external
resources (e.g., lexicons, morphological analyzers).
We leave out a more exhaustive listing and descrip-
tion of the features due to space limitations6, the fact
that the focus of this paper is not on the value of any
particular feature template but rather on our overall
approach and experimental results, and because we
plan to release our code, which will be more helpful
for reproducibility efforts.
4.5 Data Preparation
For our experiments, we use the original writ-
ten form of the data from the latest versions of
the first three parts of the Penn Arabic Treebank
(ATB) (Maamouri et al, 2004; Maamouri and Bies,
2004) as well as the new broadcast news collection
(Maamouri et al, 2012).7 We convert the constituent
trees into dependency trees and adjust the part-of-
speech tags.
5?A given action? may be more correct technically, but our
implementation is set up to share the same set of string-based
features across all actions associated with a given anchor.
6Simply listing the feature templates in a normal font size
with minimal (insufficient) explanation would require well over
a page. The set of feature templates is based upon the tem-
plates used by Tratz and Hovy?s (2011) English parser, which
are given in Tratz?s (2011) thesis.
7We use version 4.1 of ATB part 1, 3.1 of part 2, 3.2 of part
3, and 1.0 of the broadcast news transcriptions.
37
4.5.1 Dependency Conversion
The two main Modern Standard Arabic de-
pendency treebanks currently available are the
Columbia Arabic Treebank (CATiB) (Habash and
Roth, 2009) and the Prague Arabic Dependency
Treebank (PADT) (Hajic? et al, 2004). CATiB has
over 228,000 manually annotated words as well as
an automatic ATB conversion. It uses only 8 de-
pendency relations (subject, object, predicate, topic,
idafa, tamyiz, modifier, and flat) and 6 part-of-
speech tags, and it has not yet been publicly released
by the LDC. The PADT, which was used in the
CoNLL 2006 and 2007 shared tasks (Buchholz and
Marsi, 2006; Nivre et al, 2007), is much smaller,
with only about 148,000 annotated tokens. Since we
want a large annotated corpus with fine-grained la-
bels, we create our own ATB conversion.
4.5.2 Transformations
In addition to converting the ATB?s constituent
parses to dependency trees, we make a handful
of other changes. Following Green and Manning
(2010) and others, sentences headed by X nodes
are deleted because the treebank annotators con-
sidered them unbracketable or somehow erroneous.
Following Rambow et al (2005), Treebank sen-
tences headed by TOP elements containing multiple
S daughters are split into separate sentences.8 Addi-
tionally, if the dependency converter concludes that
an S node without treebank functional tags is depen-
dent upon another S node and is separated from it
via sentence-final punctuation (e.g., an exclamation
point), these S nodes are separated into distinct sen-
tences as well. For the broadcast news data, we re-
move all subtrees headed by EDITED tags to make
it more closely resemble newswire text.9
Since we adhere to the tokenization scheme used
by the ATB, and we do not split off the determiner Al
as its own tree token. Instead, we treat it as a prefix.
The words referred to as inna and her sisters are
annotated using two different part-of-speech cate-
gories and syntactic structures in the ATB. In our
conversion, both ATB structures are converted to
8The ATB often has multiple sentences, or even entire para-
graphs, annotated under a single TOP element.
9The EDITED tag ?is used to show the repetition and restart-
ing of constituents that are repaired by subsequent speech?
(Maamouri et al, 2012).
the same dependency structure headed by the INNA
word, similar to CATiB (Habash and Roth, 2009).
We treat the focus particle AmmA like a preposi-
tion in our dependency structure, following CATiB.
4.5.3 Dependency Label Scheme
Our dependency scheme consists of a total of
35 labels. Many of these are similar to those of
Stanford?s basic dependency scheme for English (de
Marneffe and Manning, 2008), although they are
somewhat closer to a similar scheme used by (Tratz
and Hovy, 2011). The list of relations is presented
in Table 2.
Most of the relations are self-explanatory or cor-
respond to similar labels in either Tratz and Hovy?s
(2011) scheme for English or CATiB?s (Habash and
Roth, 2009) scheme for Arabic. A few are new
or significantly different from their similarly named
counterparts in other schemes and are described in
greater detail below.
? adjnom ? connects the head of an NP to that of a sister
NP (occurs with apposition and preposition-like nouns)
? advcl ? connects verbal nouns to their syntactic governor
in what resemble English?s adverbial participle clauses
? advnp ? connects NPs with treebank adverbial function
tags (e.g., -LOC, -TMP, -DIR), which are often headed by
preposition-like nouns, to what they modify
? fidafa ? for false idafa (idafa-like structures that are
headed by adjectives instead of nouns)
? kccmp ? connects a clausal complement that is part of a
past progressive or habitual construction to the head verb
kana
? lakinna ? similar to cc but used with the sister of inna
lakinna instead of coordinating conjunctions
? part ? particle modifier; connects particles (other than
FOCUS PART) to their governors
? rcmod ? connects a bare relative clause to its head
? reladv ? connects an adverbial WH- clause to its gover-
nor
? relmod ? connects the head of a WH- node to the rela-
tivized word
? ripcmp ? connects a clause to the relative or interrogative
pronoun that heads it
4.5.4 Part-of-Speech Tag Scheme
The Penn Arabic Treebank uses complex part
of speech tags for the entire tree token such as
DET+NOUN+NSUFF FEM SG+CASE DEF GEN.
Across the treebank data used in our experiments,
there are a total of 579 such tags, which are
composed of 179 different parts separated by plus
signs. Each part corresponds to a substring of the
38
adjnom adjunct nominal intj interjection prep preposition modifier
advcl adverbial clause iobj indirect object punct punctuation modifier
advmod adverbial modifier idafa idafa rcmod (bare) relative clause modifier
advnp adverbial noun phrase fidafa false idafa reladv relative pronoun adverbial
cc coordinating conjunction flat flat structure relmod relative pronoun modifier
ccinit initial coordinating conjunction kccmp kana clausal complement ripcmp relative/interrogative pronoun complement
ccomp clausal complement lakinna see text sc subordinating conjunction modifier
combo combination term neg negation subj subject
conj conjunction obj object tmz tamyiz
cop copula complement objcomp object complement tpc topicalized element
dep unspecified dependency part particle modifier voc vocative
det determiner pcomp preposition complement
Table 2: Syntactic dependency scheme used in this work. Labels that aren?t self-explanatory or similar to the labels
used by Tratz and Hovy (2011) for English or CATiB for Arabic (Habash and Roth, 2009) are in bold (for completely
new relations) or italics (for similarly named but semantically different relations)
vowelized version of the word.10 Due at least in part
to the enormity of this label set, simpler schemes
are often preferred, such as the ?Bies? labels (Bikel,
2004; Kulick et al, 2006), Diab?s (2007) labels,
Kulick?s (2011) labels, and CATiB?s labels (Habash
and Roth, 2009). Marton et al (2010) find that
using simpler schemes allow them to get better
parsing results when using predicted POS tags due
to the relatively poor performance of taggers trained
using the full ATB scheme.
The part-of-speech tag scheme we use is quite
similar to that of the original ATB but has several
simplifications. These changes are listed below.
1. Possessive and direct object pronoun clitics are all given
the same label (PRON OPP) (50 fewer tags; mapping back
to the originals is trival in almost all cases)
2. .VN forms of NOUN and ADJ are merged with their re-
spective more generic categories
3. Interrogative and relative adverbial and pronoun labels
are merged together into RI ADV and RI PRON
4. Noun suffix labels (e.g., NSUFF MASC PL GEN,
NSUFF MASC PL ACC) with genitive or accusative case
distinctions are merged because there is no distinction in
unvowelized form
5. Labels for dual masculine noun suffixes are merged with
their plural counterparts (no distinction in the unvow-
elized forms)
6. Demonstrative pronoun labels are collapsed to
DEM PRON (person and number information is easily
recovered)
7. The words called inna and her sisters are labeled INNA
instead of PSEUDO VERB or SUB CONJ
10Since we use the original written form of the data and the
internal segmentation of the words are only provided for the
vowelized versions, we project the segmentation into the orig-
inal written forms, discarding any parts that weren?t actually
written (e.g., case labels associated with unwritten diacritics).
Since our system splits off clitics and identifies
the affixes, the tagging is performed at the individual
morpheme level instead of producing a single all-
encompassing tag for the entire token.
Some of the part-of-speech tags (mostly in-
stances of DIALECT, TYPO, TRANSERR, and
NOT IN LEXICON tags) are automatically cor-
rected/improved during the dependency conversion
based upon the original constituent parse.
4.6 Filtering
Sentences containing invalid clitics are not used in
training both because they are erroneous and be-
cause including them would require allowing the
system to perform actions that should not occur (i.e.,
splitting off a clitic of length 8); similarly, train-
ing examples with more than 20% of their tokens
tagged as DIALECT, TRANSERR, LATIN, PARTIAL,
GRAMMAR PROBLEM, and/or TYPO are ignored on
the assumption that including them would harm the
model. This filtering process is not applied in test-
ing.
4.7 Data Split
We train and test models using three different splits
of the data. The first split is based upon the split used
by Zitouni et al (2006) in their diacritization work
and is the same as that used by Marton et al (2013)
in their parsing work and by Kulick (2011) in his to-
kenization and part-of-speech tagging work, in order
to facilitate better comparison. However, Marton et
al. use the CATiB conversion of a slightly earlier
version of the data (3.1, not 3.2), and, thus, the re-
sults are not directly comparable. This split places
39
Part Use Files Sent Toks Tree Toks Affixes
1 train 514 4090 101629 116892 49057
dev 110 909 22932 26261 11074
test 110 823 20825 24127 10032
2 train 351 3011 102795 120605 56273
dev 75 559 20869 24619 11245
test 75 630 20518 24078 11078
3 train 509 11350 287945 341033 145621
dev 45 1029 26347 31200 13828
test 45 992 25299 29938 12220
BN train 68 5504 82388 98040 48190
dev 26 1801 29873 35676 17890
test 26 2082 34361 41192 20366
Table 3: Counts of the number of files, sentences (Sent),
original space-delimited tokens (Tok), ATB tree tokens
(Tree Toks), and affixes in the experimental data.
the first (in name and chronological order) 85% of
the documents in ATB part 3 in training, the next
7.5% in development, and the final 7.5% in test.
In the second split, we use data from the first
three parts of the ATB, each of which consists
of documents coming from a different newswire
source. Parts 1 and 2 are split 70%/15%/15% train-
ing/dev/test, and we reuse the split of part 3 just
mentioned. Under this setup, we train two different
CTF-TMs, one that performs all of the tasks and one
that performs all of the tasks except parsing. This
enables us to test whether modeling parsing task im-
proves performance on the lower level tasks.
In the final split, we use the splits for parts 1?3
plus the data in LDC?s annotated broadcast news
transcripts (Maamouri et al, 2012). Unlike parts
1?3, the broadcast news data are drawn from a va-
riety of sources. Files from sources with three or
more files are split across training, development, and
test, with the latest documents being placed in test.
11 This experiment illustrates how the system per-
forms when additional, out-of-domain data are in-
cluded.
Statistics for the data are given in Table 3.
4.8 Evaluation Measures
Dependency parsing quality is measured in terms of
labeled and unlabeled attachment scores (LAS and
UAS), which indicate the percentage of words at-
tached to their correct parent and, in the case of
LAS, whose attachment is labeled with the correct
11We will make the exact list of files used in the training,
development, and test sets available.
dependency. Since a given space-delimited token
may not be tokenized into words correctly, the de-
pendency arcs are only counted as correct if they
occur between the correct words (spans of charac-
ter indices). We measure part-of-speech tagging in
terms of F-score (F1) and require that the tree token
have the correct bounds (was tokenized correctly)
and have the correct label.
Normally, we would choose LAS on the develop-
ment set as the measure for determining the version
of the model to keep for testing because it measures
performance on the highest-level task (labeled de-
pendency parsing). However, since one of the CTF-
TMs does not perform parsing, we instead use POS
tagging F1. In general, we observe that the scores
are highly correlated, making the point moot. For
the ATB part 3 experiment, POS tagging F1 peaks
on iteration 437.12 For the second experiment, POS
tagging F1 peaks at iteration 301 for the CTF-TM
with parsing and iteration 278 for the one without.
For the third experiment, the highest score occurs
on iteration 431.
4.9 Results and Discussion
The results for the various experimental setups are
presented in Table 4.
ATB 3 Experiment When using the same split
of ATB part 3 as Kulick (2011) and Marton et al
(2013), the system correctly tokenizes 99.3% of the
space-delimited tokens, similar to Kulick?s (2011)
accuracy (99.3%) and slightly higher than the 99.0%
figure Kulick calculates for MADA. Though these
results are obtained using our dependency conver-
sion of the ATB rather than the original, we use the
same tokenization scheme. The POS labeling F1
score of 95.8 can?t be compared well with any other
work due to differences in tag schemes, which vary
greatly, as well as use of gold tokenization and other
differences. Our system obtains 84.9 UAS and 82.0
LAS, which are higher than Marton et al?s best re-
sults of 84.0 UAS and 81.0 LAS, but they were using
a different conversion (CATiB) of a different version
of the data (3.1, not 3.2) as well as gold tokenization,
so the results are not directly comparable.
Framework Internal Experiment The CTF-TM
12We run 500 iterations for each experiment, which can take
as long as a week using a quad-core machine. However, little
improvement is seen after the first 100 iterations.
40
Train Eval Data Tok Acc POS F1 Affix Bounds F1 Affix Label F1 UAS LAS
3 3 Dev 99.5 96.6 98.7 98.4 86.3 83.8
3 3 Test 99.3 95.8 98.4 97.9 84.9 82.0
1,2,3 1,2,3 Dev 99.6 97.1 99.1 98.9 88.3 86.0
1,2,3 1,2,3 Test 99.6 96.8 99.0 98.7 87.4 84.8
1,2,3,BN 1,2,3 Dev 99.6 97.1 99.1 98.9 88.5 86.2
1,2,3,BN 1,2,3 Test 99.6 96.8 99.0 98.8 87.5 85.0
1,2,3,BN 1,2,3,BN Dev 99.5 96.0 98.8 98.5 87.4 84.6
1,2,3,BN 1,2,3,BN Test 99.3 95.7 98.7 98.4 86.6 83.8
Without Parsing
1,2,3 1,2,3 Dev 99.6 96.9 99.1 98.9 NA NA
1,2,3 1,2,3 Test 99.5 96.5 98.9 98.6 NA NA
Table 4: Results for the various experiments (Exp) for both the development and test portions of the data, including per-
token clitic separation (tokenization) accuracy, part-of-speech tagging F1, affix boundary detection F1, affix labeling
F1, and both unlabeled and labeled attachment scores.
that does parsing and the CTF-TM that doesn?t
achieve similar overall results for the different tasks
(other than parsing, of course). However, when
looking deeper at the individual POS tagging mis-
takes that one system made more often by one sys-
tem than the other, (see Tables 5 and 6), we ob-
serve that the parsing CTF-TM does a better job
with labeling some parts-of-speech. For instance,
the non-parsing system mismarks passive verbs as
active more than 29% more often than the other. In
Arabic, passive and active forms of verbs are only
distinguished by their short vowels, which are typi-
cally unwritten, and, thus, the context is of particular
importance in distinguishing between the two. The
non-parsing system also has more trouble with the
distinction between nouns and adjectives, which is
likely because adjectives are derived using the same
templatic structures as nouns (Attia et al, 2010) and,
thus, context is, once again, of great importance.
Broadcast News Experiment The scores ob-
tained in the experiment with the broadcast news
data are slightly lower than in the second exper-
iment. However, this appears to be because the
broadcast news portions of the development and test
sections are more difficult to parse than the remain-
der. If we apply the model to the development and
test sections of parts 1, 2, and 3, we observe that
the results, which are given in Table 4, are higher
than those of the model trained without the broad-
cast news data.
Gold Prediction Errors Diff
-parse +parse
NOUN ADJ 297 238 -59
ADJ NOUN 328 298 -30
VB IV PASS VB IV 109 80 -29
VB PV PASS VB PV 86 68 -18
VB PV NOUN 104 88 -16
VB IV VB PV 12 22 +10
INNA SUB CONJ 9 2 -7
VB PV VB IV 19 13 -6
NOUN NOUN PROP 140 134 -6
ADJ NOUN PROP 32 27 -5
Table 5: Top 10 POS mistakes made more often by either
the CTF-TM with parsing or the CTF-TM without on the
ATB part 1, 2, and 3 development set.
Tag #Gold Tag #Gold
NOUN 26195 INNA 1456
ADJ 7491 SUB CONJ 641
NOUN PROP 5913 VB PV PASS 231
VB PV 3478 VB IV PASS 207
VB IV 2682
Table 6: Counts for the POS tags mentioned in Table 5.
5 Related Work
5.1 Semitic Language Parsing
Much of the Arabic parsing research to date uses the
pipeline approach, either running a tokenizer prior to
parsing or simply assuming the existence of gold to-
kenization (Bikel, 2004; Buchholz and Marsi, 2006;
Kulick et al, 2006; Nivre et al, 2007; Marton et al,
2010; Marton et al, 2011; Marton et al, 2013). Of
course, using gold tokenization results in optimistic
41
evaluation figures.13
Other methods exist however. For example, to
parse Modern Hebrew, Cohen and Smith (2007)
combine a morphological model with a syntactic
model using a product of experts. Another alterna-
tive is lattice parsing, which can be used to jointly
model both tokenization and parsing (Chappelier et
al., 1999). Curiously, while researchers of Mod-
ern Hebrew parsing find lattice parsers outperform-
ing their pipeline systems (Goldberg and Tsarfaty,
2008; Goldberg and Elhadad, 2011; Goldberg and
Elhadad, 2013), Green and Manning (2010) obtain
the opposite result in their Arabic parsing experi-
ments, with the lattice parser underperforming the
pipeline system by over 3 points (76.01 F1 vs 79.17
F1). Why lattice parsing may help in some cases but
not others is not clear.
Some Arabic parsing work focuses on the useful-
ness of various features and part-of-speech tagsets.
Marton et al (2013) examine various morphologi-
cal features and part-of-speech tagsets, employing
MADA (Habash and Rambow, 2005; Habash et al,
2009) to predict form-based morphological features
and an in-house system (Alkuhlani and Habash,
2012) to predict functional morphological features.
Dehdari et al (2011) investigate the best set of fea-
tures for Arabic constituent parsing and try several
approaches for selecting an optimal feature set, find-
ing that the best-first with backtracking algorithm is
the most effective in their experiments.
5.2 Other Languages
There has been a flurry of recent research involv-
ing the joint modeling of dependency parsing and
lower-level tasks14 for a variety of languages, with
most of the attention focused on Chinese. While
lacking Arabic?s morphological richness, Chinese
has its own challenges, such as word segmentation
and part-of-speech ambiguities, which have led re-
searchers to develop new unified approaches for pro-
cessing it. Qian and Liu (2012) train independent
models for word segmentation, POS tagging, and
13Green and Manning (2010) find that using automatic tok-
enization provided by MADA (Habash et al, 2009) instead of
gold tokenization results in a 1.92% F score drop in their con-
stituent parsing work.
14Systems that jointly model POS tagging and constituent
parsing have existed for some time.
parsing but then incorporate them together during
decoding. Li et al (2011), Li and Zhou (2012), Ha-
tori et al (2011), and Ma et al (2012) present sys-
tems that jointly model Chinese POS tagging and
dependency parsing. Li et al (2011) use a dy-
namic programming approach similar to Koo and
Collins (2010), Li and Zhou (2012) present a shift-
reduce style system that uses structured perceptron
and beam search, Hatori et al (2011) implement
a shift-reduce style algorithm that utilizes dynamic
programming and beam search in the manner of
Huang and Sagae (2010), and Ma et al (2012) ex-
tend Goldberg and Elhadad?s (2010) easy-first ap-
proach to support both dependency parsing and POS
tagging and is thus similar to our work. Hatori et al
(2012) extend their previous system to tackle word
segmentation, and Ma et al (2013) build upon ear-
lier work by implementing beam search to get bet-
ter results. Li and Zhou (2012) side step some of
the issues of Chinese word segmentation by pars-
ing structures of words, phrases, and sentences in a
unified framework using a structured perceptron and
beam search.
Some researchers focus their work on other lan-
guages. Lee et al (2011) present a graphical model
for morphological disambiguation and dependency
parsing that they apply to Latin, Ancient Greek,
Hungarian, and Czech. Bohnet and Nivre (2012)
present a shift-reduce style system similar to Li
and Zhou?s (2012) system that jointly models POS
tagging and labeled dependency parsing, achieving
state-of-the-art accuracy on Czech, German, Chi-
nese, and English.
6 Conclusion
In this paper, we described cross-task flexible transi-
tion models (CTF-TMs) and demonstrated their via-
bility for Arabic tokenization, affix detection, affix
labeling, part-of-speech labeling, and dependency
parsing, obtaining very strong results in each tasks.
We plan to release our software in the near future,
including the software for converting the ATB to de-
pendency parses, and would like to release our de-
pendency conversion of the Penn Arabic Treebank
via the LDC.
42
7 Future Work
In the future, we plan to integrate beam search into
the training and decoding. We want to add support
for the recovery of diacritics, roots, and derivation
templates, and we would like to apply modified ver-
sions of our system to other languages.
Our choice of anchors, operations, and constraints
represent one possible design for an Arabic CTF-
TM. Other options, such as creating unlabeled de-
pendencies and adding labels in subsequent opera-
tions, restricting clitic separation to a hand-crafted
list of clitics, utilizing information from a dictionary
or morphological analyzer, or following some sort
of coarse-to-fine labeling scheme, are also possible,
and we hope to investigate more of these options.
References
Sarah Alkuhlani and Nizar Habash. 2012. Identifying
broken plurals, irregular gender, and rationality in ara-
bic text. In Proceedings of EACL 2012, pages 675?
685.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef Van Gen-
abith. 2010. Handling Unknown Words in Statis-
tical Latent-Variable Parsing Models for Arabic, En-
glish and French. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67?75.
Dan M Bikel. 2004. On the parameter space of gen-
erative lexicalized statistical parsing models. Ph.D.
thesis, University of Pennsylvania.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1455?1465.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on Multilingual Dependency Parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning, pages 149?164.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
Parsing for Speech Recognition. In Proc. of 6e`me
confe?rence sur le Traitement Automatique du Langage
Naturel (TALN 99), pages 95?104.
Shay B Cohen and Noah A Smith. 2007. Joint Morpho-
logical and Syntactic Disambiguation. In Proceedings
of the EMNLP-CoNLL 2007.
Michael J. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and experi-
ments with Perceptron Algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In COLING 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation.
Jon Dehdari, Lamia Tounsi, and Josef van Gen-
abith. 2011. Morphological Features for Parsing
Morphologically-Rich Languages: A Case of Arabic.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages.
Mona Diab. 2007. Toward an Optimal POS Tag Set for
Modern Standard Arabic Processing. In Proceedings
of Recent Advances in Natural Language Processing.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 742?750.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew Segmentation and Parsing Using a PCFG-LA
Lattice Parser. In Proceedings of ACL 2011.
Yoav Goldberg and Michael Elhadad. 2013. Word Seg-
mentation, Unknown-word Resolution, and Morpho-
logical Agreement in a Hebrew Parsing System. Com-
putational Linguistics, 39(1):121?160.
Yoav Goldberg and Reut Tsarfaty. 2008. A Single Gen-
erative Model for Joint Morphological Segmentation
and Syntactic Parsing. Proceedings of ACL-08: HLT.
Spence Green and Christopher Manning. 2010. Better
Arabic Parsing: Baselines, Evaluations, and Analysis.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 394?402.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of Association for Computa-
tional Linguistics, pages 573?580.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A Toolkit for Arabic Tokenization,
Diacritization, Morphological Disambiguation, POS
Tagging, Stemming and Lemmatization. In Proceed-
ings of the 2nd International Conference on Arabic
Language Resources and Tools (MEDAR).
Jan Hajic?, Otakar Smrz, Petr Zema?nek, Jan S?naidauf, and
Emanuel Bes?ka. 2004. Prague Arabic Dependency
43
Treebank: Development in Data and Tools. In Pro-
ceedings of the NEMLAR International Conference on
Arabic Language Resources and Tools.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In IJCNLP, pages
1216?1224.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach to
word segmentation, pos tagging, and dependency pars-
ing in chinese. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 1045?1053.
Liang Huang and Kenji Sagae. 2010. Dynamic Program-
ming for Linear-Time Shift-Reduce Parsing. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1077?1086.
Terry Koo and Michael Collins. 2010. Efficient Third-
order Dependency Parsers. In Proceedings of ACL
2010, pages 1?11.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42.
Seth Kulick. 2011. Exploiting Separation of Closed-
Class Categories for Arabic Tokenization and Part-of-
Speech Tagging. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 10(1):4.
John Lee, Jason Naradowsky, and David A Smith. 2011.
A discriminative model for joint morphological dis-
ambiguation and dependency parsing. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies-Volume 1, pages 885?894.
Zhongguo Li and Guodong Zhou. 2012. Unified depen-
dency parsing of chinese morphological and syntactic
structures. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12, pages 1445?1454.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1180?1191.
Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren. 2012.
Easy-First Chinese POS Tagging and Dependency
Parsing. In Proceedings of COLING 2012, pages
1731?1746, Mumbai, India.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 110?114, Sofia, Bul-
garia. Association for Computational Linguistics.
Mohamed Maamouri and Ann Bies. 2004. Develop-
ing an Arabic Treebank: Methods, Guidelines, Pro-
cedures, and Tools. In Proceedings of the Workshop
on Computational Approaches to Arabic Script-based
languages, pages 2?9.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2012.
Expanding Arabic Treebank to Speech: Results from
Broadcast News. In Proceedings of LREC 2012.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic Dependency Parsing with Lexical
and Inflectional Morphological Features. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages.
Yuval Marton, Nizar Habash, and Owen Rambow. 2011.
Improving Arabic Dependency Parsing with Form-
based and Functional Morphological Features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 1586?1596.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013.
Dependency Parsing of Modern Standard Arabic with
Lexical and Inflectional Features. Computational Lin-
guistics, 39(1):161?194.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proceedings of the EMNLP-CoNLL 2007,
pages 122?131.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Dependency
Parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP.
Xian Qian and Yang Liu. 2012. Joint Chinese Word Seg-
mentation, POS tagging and Parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 501?511.
Owen Rambow, David Chiang, Mona Diab, Nizar
Habash, Rebecca Hwa, Khalil Simaan, Vincent Lacey,
Roger Levy, Carol Nichols, and Safiullah Shareef.
2005. Parsing arabic dialects. In Final Report, JHU
Summer Workshop.
44
Stephen Tratz and Eduard Hovy. 2011. A Fast, Ac-
curate, Non-Projective, Semantically-Enriched Parser.
In Proceedings of EMNLP 2011.
Stephen Tratz. 2011. Semantically-Enriched Parsing for
Natural Language Understanding. Ph.D. thesis, Uni-
versity of Southern California.
Reut Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics: Student
Research Workshop, pages 49?54.
Yue Zhang and Stephen Clark. 2008. Joint Word Seg-
mentation and POS Tagging Using a Single Percep-
tron. In Proceedings of ACL 2008, pages 888?896.
Imed Zitouni, Jeffrey S Sorensen, and Ruhi Sarikaya.
2006. Maximum entropy based restoration of Ara-
bic diacritics. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 577?584.
45
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 42?47,
Gothenburg, Sweden, April 27, 2014.
c
?2014 Association for Computational Linguistics
Resumptive Pronoun Detection
for Modern Standard Arabic to English MT
Stephen Tratz
?
Clare Voss
?
Jamal Laoudi
?
?
Army Research Laboratory, Adelphi, MD 20783
?
Advanced Resource Technologies, Inc. Alexandria, VA 22314
{stephen.c.tratz.civ,clare.r.voss.civ,jamal.laoudi.ctr}@mail.mil
Abstract
Many languages, including Modern Stan-
dard Arabic (MSA), insert resumptive pro-
nouns in relative clauses, whereas many
others, such as English, do not, using
empty categories instead. This discrep-
ancy is a source of difficulty when trans-
lating between these languages because
there are words in one language that cor-
respond to empty categories in the other,
and these words must either be inserted
or deleted?depending on translation di-
rection. In this paper, we first examine
challenges presented by resumptive pro-
nouns in MSA-English translations and re-
view resumptive pronoun translations gen-
erated by a popular online MSA-English
MT engine. We then present what is, to
the best of our knowledge, the first system
for automatic identification of resumptive
pronouns. The system achieves 91.9 F1
and 77.8 F1 on Arabic Treebank data when
using gold standard parses and automatic
parses, respectively.
1 Introduction
One of the challenges for modern machine trans-
lation (MT) is the need to systematically insert
or delete information that is overtly expressed
in only one of the languages in order to main-
tain intelligibility and/or fluency. For example,
word alignment between pro-drop and non-pro-
drop languages can be negatively impacted by the
systematic dropping of pronouns in only one of the
languages (Xiang et al., 2013). A similar type of
linguistic phenomenon of great interest to linguists
that has not yet received significant attention in
MT research is the mismatch between languages
in their usage of resumptive pronouns. Some lan-
guages, such as Modern Standard Arabic (MSA),
require the insertion of resumptive pronouns in
many relative clauses, whereas other languages,
including English, rarely permit them. An exam-
ple of an MSA sentence is given below, with its
English gloss showing the resumptive pronoun in
bold, its reference translation (RT), and an MT
system output where the roles of patient and doc-
tor are incorrectly reversed:

?J
.
fi


J
.
??@ ?

K
	
Y

?
	
K

@ ?


	
Y?@
	
?fl


Q?
?
@

IK



@P
Gloss: I.saw the.patient who rescued.him the.doctor.
RT: I saw the patient whom the doctor rescued.
MT: I saw a patient who rescued the doctor.
In this paper, we examine translations pro-
duced by a popular online translation system for
MSA resumptive pronouns occurring in several
different syntactic positions to gain insight into
the types of errors generated by current MT en-
gines. In a test suite of 300 MSA sentences with
resumptive pronouns, over 30% of the relative
clauses with resumptive pronouns were translated
inaccurately. We then present an automatic classi-
fier that we built for identifying MSA resumptive
pronouns and the results obtained from using it in
experiments with the Arabic Treebank (Maamouri
et al., 2004; Maamouri and Bies, 2004). The
system achieves 91.9 F1 and 77.8 F1 on Arabic
Treebank data when using gold standard parses
and automatic parses, respectively. To the best
of our knowledge, this is the first attempt to
automatically identify resumptive pronouns in any
language.
2 Relevant MSA Linguistics
MSA and English relative clauses differ in struc-
ture, with one of the most prominent differences
being in regard to resumptive pronouns. Resump-
tive pronouns are required in many MSA rela-
tive clauses but are almost never grammatical in
English. In MSA, like English, if the external
42
Arabic (. . .
	
?Q?

@) Gloss (I know...) English RT (I know...) MT Output (I know...)
1a @
Q
ffi



J? ???

Jfi
.

K ?



?? @

?YJ


??@ the+lady who
i

i
smiles a lot the lady who
i

i
smiles a lot the lady who smiles a lot
1b @
Q
ffi



J? ???

Jfi
.

K

?YJ


? lady ?
i
smiles 
i
a lot a lady who
i

i
smiles a lot a lot lady smiling
1c @
Q
ffi



J? ???

Jfi
.
K


	
?? who
i
smiles 
i
a lot who
i

i
smiles a lot a lot of smiles
2a ?g
.
Q?@ A???? ?



?? @

??Q?

?? @ the+company that
i
financed+it
i
the+man the company that
i
the man financed 
i
the company that financed the man
2b ?g
.
Q?@ A????

??Q?

? company ?
i
financed+it
i
the+man a company ?
i
the man financed 
i
a company funded by the man
2c ?g
.
Q?@ ???? A? what
i
financed+it
i
the+man what
i
the man financed 
i
what the man-funded
3a ???

?A

J
	
?? @

I???

K ?


	
Y?@ Y???@ the+boy whom
i
talked the+girl with+him
i
the boy whom
i
the girl talked with 
i
the boy who spoke with the girl
3b ???

?A

J
	
?? @

I???

K @Y?? boy ?
i
talked the+girl with+him
i
a boy ?
i
the girl talked with 
i
the girl was born I spoke with him
3c

?A

J
	
?? @

I???

K
	
?? ?? [with whom]
i
talked the+girl 
i
[with whom]
i
the girl talked 
i
from speaking with the girl
4a ??
	
Q
	
ffi? PA?
	
E @ ?


	
Y?@ ?g
.
Q?@ the+man who
i
collapsed house+his
i
the man [whose house]
i

i
collapsed a man who collapsed home
4b ??
	
Q
	
ffi? PA?
	
E @ Cg
.
P man ?
i
collapsed house+his
i
a man [whose house]
i

i
collapsed a man of his house collapsed
4c ??
	
Q
	
ffi? PA?
	
E @
	
?? who
i
collapsed house+his
i
[whose house]
i

i
collapsed of his house collapsed
5 ?



??
	
J? ?? A? what
i
it
i
logical what
i

i
is logical what is logical
Table 1: A list of MSA sentences starting with relative clauses
	
?Q?

@ (translation: I know) along with their
English glosses, English reference translation (RT), and the output of MT system X. Empty categories
are indicated with  and empty WH nodes are indicated with ?. Subscripts indicate coreference. To
avoid clutter, the glosses do not explicitly indicate person, number, or gender.
antecedent plays the role of the subject, no re-
sumptive pronoun is inserted
1
; instead, MSA in-
flects the verb to agree with the subject in number
and gender by attaching an affix
2
. A second sig-
nificant difference between the two languages is
that, in MSA, relative pronouns are required for
relative clauses modifying definite noun phrases
but are prohibited when modifying indefinite noun
phrases; in English, definitiveness neither prevents
nor necessitates the inclusion of a relative pro-
noun. A third significant difference is that, for free
relative clauses?that is, relative clauses that are
not attached to an external antecedent?MSA has
a different set of relative pronouns for introducing
the clause
3
. A fourth challenge is that MSA has no
equivalent word for the English word ?whose? and,
to convey a similar meaning, employs resumptive
pronouns as possessive modifiers. Examples illus-
trating these differences are provided in Table 1.
For further background on MSA relative clauses
and MSA grammar, we refer readers to books by
Ryding (2005) and Badawi et al. (2004).
1
A notable exception to this rule is for equational sen-
tences. MSA lacks an overt copula corresponding to the En-
glish word ?is? and, to convey a similar meaning, resumptive
subject pronouns must be inserted in these contexts.
2
In standard VSO and VOS constructions, the verbs in-
flect as singular regardless of the number of the subject.
3
These pronouns are also employed to introduce ques-
tions.
3 Data
In our research, we rely on the conversion of con-
stituent into dependency structures and the train-
ing/dev/test splits of the Arabic Treebank (ATB)
parts 1, 2, & 3 (Maamouri et al., 2004; Maamouri
and Bies, 2004) as presented by Tratz (2013).
We extract features from labeled dependency trees
(rather than constituent trees) generated by Tratz?s
(2013) Arabic NLP system, which separates cli-
tics, labels parts-of-speech, produces dependency
parses, and identifies and labels affixes.
The original ATB dependency conversion does
not mark pronouns for resumptiveness, so we
modify the conversion process to obtain this infor-
mation. The original ATB constituent trees mark
this by labeling WHNP nodes and NP nodes with
identical indices. If the NP node corresponds to a
null subject and the head of the S under the SBAR
is a verb, we mark the inflectional affix on the
verb, which agrees with the subject in gender and
number, as resumptive. These inflectional affixes
are included as their own category within our anal-
yses since their presence precludes the appearance
of another resumptive pronoun within the relative
clause (e.g., as a direct object).
The total number of resumptive pronouns and
?resumptive? inflectional affixes in the training,
dev, and test sections are presented in Table 2. In
43
Training Dev Test
Pronouns 5775 794 796
Inflectional affixes 6161 807 845
Table 2: Number of resumptive pronouns and ?re-
sumptive? inflectional affixes by data section.
the training data, the four most likely positions
4
for the resumptive pronouns are:
i) direct object of relative clause?s main verb (33.9%)
ii) object of a preposition attached to the verb (20.8%)
iii) possessive modifier of the subject of the verb (5.4%)
iv) subject pronoun in an equational sentence (4.2%).
4 Translation Error Analysis
As an exploratory exercise to gain insight into the
types of errors generated by current MT engines
when translating from a language that inserts re-
sumptive pronouns (i.e., MSA) to one that doesn?t
(i.e., English), we worked with a native Arabic
speaker to produce a list of Arabic sentences that
vary in terms of definitiveness (and existence, as
with free relatives) of the external antecedent, and
the syntactic position of the resumptive pronoun,
along with English glosses and reference transla-
tions for these sentences. This set was then pro-
cessed using a popular online translation system,
which we refer to as system X. The sentences,
their glosses, reference translations, and automatic
translations are presented in Table 1.
Although system X did not typically produce
English pronouns corresponding to the resumptive
pronouns in the source, most of the translations
proved problematic, with many of the issues be-
ing related to reordering. Thus, while system X
appears to be good at not translating resumptive
pronouns, its performance on the relative clauses
that contain them has ample room for improve-
ment. Our working hypothesis is that system X?s
English language model is effective in discount-
ing candidate translations that keep the resumptive
pronoun.
As a second exploratory exercise, we automat-
ically extracted all the resumptive pronoun exam-
ples in the training section of the data described
in Section 3 and grouped them based upon the se-
quence of dependency arc labels from the resump-
tive pronoun up to the head of the relative clause
4
Examples of these frequent configurations are in Table 1.
and the first letter of the POS tag of the interven-
ing words (e.g., ?N? for noun, ?A? for adjective).
For each of the thirty most common configura-
tions, we took ten examples (for a total of 300), ran
them through system X?s Arabic-English model
and gave both the translation and the source text
to our native Arabic expert. Our expert examined
whether 1) the translation engine generated a pro-
noun corresponding to the source side resumptive
pronoun and 2) whether the translation was correct
locally within the relative clause (whether the pro-
noun was retained or not)
5
. The results for these
two judgments are presented in Table 3.
Corresponding Pronoun?
Yes No
Correct?
Yes 17 189
No 20 74
Table 3: Expert judgments
Our expert concluded that a corresponding En-
glish pronoun was produced in only 37 of the
300 examples (12.3%). Seventeen of these were
judged correct, although in many of these cases a
significant portion of the relative clause was trans-
lated incorrectly even though a small portion in-
cluding the pronoun was translated properly, mak-
ing judgment difficult. Our expert noted that many
of the correct translations involved switching the
voice of the verb in the relative clause from ac-
tive to passive voice using a past participle. Of
the 189 that had no corresponding pronoun and
were judged correct, 46 (24.3%) involved switch-
ing to passive voice. In general, it appears that
system X does a good job at not generating En-
glish pronouns corresponding to MSA resumptive
pronouns, although it makes numerous mistakes
with the data we presented to it.
5 System Description
Our MSA resumptive pronoun identification sys-
tem processes one sentence at a time and relies
upon the (averaged) structured perceptron algo-
rithm (Collins, 2002) to rank the feasible actions.
When processing a sentence containing n pro-
nouns and affixes, a total of n iterations are per-
formed. During each processing iteration, the
system considers two actions for every unlabeled
5
This latter task was challenging, but permitted, as in-
tended, lenient judgment of the MT output.
44
Function Definitions:
path(x) ? returns a list of dependency arcs from x up through the first ?ripcmp?, ?rcmod?, or ?ROOT? arc (link from affix to the
core word is also treated as an arc)
rDescendants(x) ? returns a list of paths (dependency arc lists) from x to each descendant already marked as resumptive
pDescendants(x) ? returns a list of paths (dependency arc lists) from x to each pronoun / verbal inflectional affix, not following
?cc?, ?ripcmp?, or ?rcmod? arcs
hasDepArc(x,y) ? returns a Boolean value indicating if an arc with label y descends from x
pathToString(x) ? concatenates the labels of the arcs in a list to create a string
last(x) ? returns the last element in the list x
split(x, y) ? splits a string x apart wherever it contains substring y, returning these pieces
deps(x), parent(x) ? return dependency arc(s) of which x is the {head, child}
head(x), child(x) ? returns the {head, child} of arc x
pro(x) ? if x is an affix, the word attached to it is returned, otherwise x is returned
l(x) ? return the label/part-of-speech for a dependency arc, affix, or word
T(x), t(x), suffixes(x) ? return the {type (?affix? or ?pro?), written text, suffixes} for x
n(x,y) ? returns the word node that is y words after pro(x)
Given: p ? pronoun or inflectional affix
Pseudocode:
?0:?+T(p), ?1:?+t(p), ?2:?+l(p), ?3:?+l(parent(p)), for(s in split(l(p),? ?)) { ?4:?+s }
if(T(p)=?affix?) { for(a in deps(pro(p))) { ?5:?+l(a) }, if(T(p)=?pro? or not(hasDepArc(pro(p), ?subj?))) { ?6? }
for(i in {-3,-2,-1,0,+1,+2,+3,+4}) { ?7:?+i+t(n(pro(p),i)), ?8:?+i+l(n(pro(p),i)), ?9:?+i+l(parent(n(pro(p),i))) }
?10:?+pathToString(path(p)), end := last(path(p)), resumptives := rDescendants(child(end))
if(l(end) != ?ROOT?) {
if(size(resumptives) > 0) {?11a? } else {?11b?+(size(pDescendants(child(end))) > 0)}
for(s in split(l(head(end)), ? ?)) ?12:?+s, for(arc in path(p)) { ?13?+l(arc) }
?14:?+t(head(end)), ?15:?+l(head(end)), ?16:?+l(parent(head(end)))
?17:?+t(child(end)), ?18:?+l(child(end)), ?19:?+l(parent(child(end)))
if(l(child(end)) = ?VB PV? and size(suffixes(child(end)))=0) { ?20? }
for(suff in suffixes(head(end))) { for(s in split(l(suff), ? ?)) { ?21:?+suff }} }
Figure 1: Pseudocode for feature production. Statements in bold font produce strings that are used to
identify features. The feature set consists of all pairwise combinations of these strings.
personal pronoun and inflectional verbal affix
6
within a given sentence, these actions being label-
as-?resumptive? and label-as-?not-resumptive?.
The highest scored action is performed and the
newly-labeled pronoun or affix is removed from
further processing.
The system scores each action by computing the
dot product between the feature vector derived for
the pronoun/inflectional affix and the weight vec-
tor. The feature vectors consist entirely of Boolean
values, each of which indicates the presence or ab-
sence of a particular feature. Each feature is iden-
tified by a unique string and these strings are gen-
erated using the pseudocode presented in Figure
1. (All pairwise combinations of the strings gen-
erated by the pseudocode are included as features.)
For space reasons, we omit a review of the train-
ing procedure for the structured perceptron and re-
fer the interested reader to work by Goldberg and
Elhadad (2010).
6
Occasionally an imperfect verb will have both a written
inflectional prefix and a written inflectional suffix. For these
cases, the system only considers the prefix as there is no need
to make two separate judgments.
6 Experiments
We trained our system on the training data us-
ing the gold standard clitic segmentation, parse,
and part-of-speech information and optimized it
for overall F1 (pronouns and inflectional affixes
combined) on the development data. Performance
peaked on training iteration 8, and we applied the
resulting model to two treatments of the test data,
once using the gold standard annotation and once
using the Tratz (2013) Arabic NLP system to au-
tomatically pre-process the data.
6.1 Results and Discussion
The scores for the development and test sections,
both for gold and automatic annotation, are pre-
sented in Table 4.
The system performs well when given input
with gold standard clitic segmentation, POS tags,
and dependency parses, achieving 91.9 F1 for re-
sumptive pronouns on the test set and 95.4 F1 for
the affixes. Performance however degrades sub-
stantially when automatic pre-processing of the
source is input instead. Some of this drop can
be explained by the use of gold standard markup
in training?more weight was likely assigned to
45
Pronoun Inflectional Affix
P R F1 P R F1
Dev
Gold 92.5 92.8 92.6 96.7 96.4 96.5
Auto 88.0 81.0 84.4 86.1 77.3 81.5
Test
Gold 92.1 91.7 91.9 95.0 95.9 95.4
Auto 83.6 72.8 77.8 86.6 76.0 81.0
Table 4: Precision, recall, and F1 results for the
?is-resumptive? label on the development and test
sets for gold standard clitic separation/POS tag-
ging/parsing and automatic preprocessing.
parse and POS tag-related features than would
have if automatic pre-processing of the source had
been used in training.
Having examined the classification system er-
rors on the development data, we conclude that
the main source of this drop is due to poor iden-
tification and attachment of bare relatives
7
by the
Tratz (2013) NLP system. While the NLP system
achieves 88.5 UAS and 86.1 LAS on the develop-
ment section,
8
its performance on identifying bare
relatives is comparatively low, with 70.0 precision
and 60.5 recall. For the test section, the NLP sys-
tem performance on bare relatives is even lower at
69.6 precision and 52.7 recall. This helps to ex-
plain why our resumptive pronoun classifier per-
forms worse on the test data than on the devel-
opment data when using automatic pre-processing
but not when using gold standard markup.
7 Related Work
The computational linguistics research most rele-
vant to ours is the work on identifying empty cat-
egories for several languages, including English,
Chinese, Korean, and Hindi. Empty categories
are nodes in a parse tree that do not correspond
to any written morpheme; these are used to han-
dle several linguistic phenomena, including pro-
drop. Recent research demonstrates that recovery
of empty categories can lead to improved transla-
tion quality for some language pairs (Chung and
Gildea, 2010; Xiang et al., 2013). For more in-
formation on the recovery of empty categories, we
refer the interested reader to work by Kukkadapu
and Mannem (2013), Cai et al. (2011), Yang and
Xue (2010), Gabbard et al. (2006), Schmid (2006),
Dienes and Dubey (2003), and Johnson (2002).
7
Relative clauses lacking a relative pronoun. As explained
in Section 2, MSA lacks relative pronouns for relative clauses
modifying indefinite noun phrases.
8
UAS and LAS stand for unlabeled and labeled attach-
ment scores.
8 Conclusion
In this paper, we present the challenge of translat-
ing MSA relative clauses, which often contain re-
sumptive pronouns, into English, which relies on
(inferred) empty categories instead. We examine
errors made by a popular online translation service
on MSA relative clauses and present an automatic
system for identifying MSA resumptive pronouns.
The online translation service occasionally gen-
erates English pronouns corresponding to MSA
resumptive pronouns, producing resumptive pro-
nouns for only 37 of 300 examples that cover a
variety of frequent MSA relative clause structures.
Our MSA resumptive pronoun identification
system achieves high levels of precision (92.1)
and recall (91.7) on resumptive pronoun identifi-
cation when using gold standard markup. Perfor-
mance drops significantly when using automatic
pre-processing, with precision and recall falling to
83.6 and 72.8, respectively. One of the sources
of the drop appears to be the weak performance
of the Tratz (2013) Arabic NLP system in identi-
fying and attaching bare relative clauses?that is,
relative clauses that lack a relative pronoun.
This work is the first attempt we are aware of to
automatically identify resumptive pronouns in any
language, and it presents a baseline for compari-
son for future research efforts.
9 Future Work
Going forward, we plan to experiment with apply-
ing our resumptive pronoun identifier to enhance
MT performance, likely by deleting all resumptive
pronouns during alignment and, again, at transla-
tion time. Another natural next step is to train the
system using automatically generated parse, part-
of-speech tag, and clitic segmentation information
instead of gold standard annotation to see if this
produces a similar drop in performance. We also
plan to investigate the use of frame information of
Arabic VerbNet (Mousser, 2010) as features, and
we would like to focus in greater detail on the dif-
ficulties in generating resumptive pronouns when
translating from English into MSA.
References
Elsaid Badawi, Michael G. Carter, and Adrian Gully.
2004. Modern Wrtitten Arabic: A Comprehensive
Grammar. Psychology Press.
46
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty ele-
ments. In ACL (Short Papers), pages 212?216.
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636?
645.
Michael J. Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory and
experiments with Perceptron Algorithms. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing.
P?eter Dienes and Amit Dubey. 2003. Antecedent re-
covery: Experiments with a trace tagger. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 33?40.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the penn treebank. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 184?191.
Y. Goldberg and M. Elhadad. 2010. An efficient
algorithm for easy-first non-directional dependency
parsing. In HLT-NAACL 2010.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 136?143.
Puneeth Kukkadapu and Prashanth Mannem. 2013. A
statistical approach to prediction of empty categories
in hindi dependency treebank. In Fourth Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages, page 91.
Mohamed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, Guidelines, Proce-
dures, and Tools. In Proceedings of the Workshop on
Computational Approaches to Arabic Script-based
languages, pages 2?9.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109.
Jaouad Mousser. 2010. A Large Coverage Verb Taxon-
omy for Arabic. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Eval-
uation.
Karin C. Ryding. 2005. A Reference Grammar of
Modern Standard Arabic. Cambridge University
Press.
Helmut Schmid. 2006. Trace prediction and recov-
ery with unlexicalized pcfgs and slash features. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 177?184.
Stephen Tratz. 2013. A cross-task flexible transition
model for arabic tokenization, affix detection, affix
labeling, pos tagging, and dependency parsing. In
Fourth Workshop on Statistical Parsing of Morpho-
logically Rich Languages.
Bing Xiang, Xiaoqiang Luo, and Bowen Zhou. 2013.
Enlisting the Ghost: Modeling Empty Categories for
Machine Translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistic.
Yaqin Yang and Nianwen Xue. 2010. Chasing the
ghost: recovering empty categories in the chinese
treebank. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 1382?1390.
47
