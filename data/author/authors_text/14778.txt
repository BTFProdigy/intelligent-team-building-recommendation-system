2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597?601,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Context-Enhanced Citation Sentiment Detection
Awais Athar
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge, CB3 0FD, U.K.
awais.athar@cl.cam.ac.uk
Simone Teufel
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge, CB3 0FD, U.K.
simone.teufel@cl.cam.ac.uk
Abstract
Sentiment analysis of citations in scientific pa-
pers and articles is a new and interesting prob-
lem which can open up many exciting new ap-
plications in bibliographic search and biblio-
metrics. Current work on citation sentiment
detection focuses on only the citation sen-
tence. In this paper, we address the problem
of context-enhanced citation sentiment detec-
tion. We present a new citation sentiment cor-
pus which has been annotated to take the dom-
inant sentiment in the entire citation context
into account. We believe that this gold stan-
dard is closer to the truth than annotation that
looks only at the citation sentence itself. We
then explore the effect of context windows of
different lengths on the performance of a state-
of-the-art citation sentiment detection system
when using this context-enhanced gold stan-
dard definition.
1 Introduction
Sentiment analysis of citations in scientific papers
and articles is a new and interesting problem. It can
open up many exciting new applications in biblio-
graphic search and in bibliometrics, i.e., the auto-
matic evaluation of the influence and impact of in-
dividuals and journals via citations. Automatic de-
tection of citation sentiment can also be used as a
first step to scientific summarisation (Abu-Jbara and
Radev, 2011). Alternatively, it can help researchers
during search, e.g., by identifying problems with a
particular approach, or by helping to recognise un-
addressed issues and possible gaps in the current re-
search.
However, there is a problem with the expression
of sentiment in scientific text. Conventionally, the
writing style in scientific writing is meant to be ob-
jective. Any personal bias by authors has to be
hedged (Hyland, 1995). Negative sentiment is po-
litically particularly dangerous (Ziman, 1968), and
some authors have documented the strategy of pref-
acing the intended criticism by slightly disingenuous
praise (MacRoberts and MacRoberts, 1984). This
makes the problem of identifying such opinions par-
ticularly challenging. This non-local expression of
sentiment has been observed in other genres as well
(Wilson et al, 2009; Polanyi and Zaenen, 2006).
Figure 1: Example of anaphora in citations
A typical case is illustrated in Figure 1. While the
first sentence praises some aspects of the cited pa-
per, the remaining sentences list its shortcomings. It
is clear that criticism is the intended sentiment, but
597
if we define our gold standard only by looking at
the citation sentence, we lose a significant amount
of sentiment hidden in the text. Given that most ci-
tations are neutral (Spiegel-Rosing, 1977; Teufel et
al., 2006), this makes it ever more important to re-
cover what explicit sentiment there is from the con-
text of the citation.
However, the dominant assumption in current ci-
tation identification methods (Ritchie et al, 2008;
Radev et al, 2009) is that the sentiment present in
the citation sentence represents the true sentiment
of the author towards the cited paper. This is due
to the difficulty of determining the relevant context,
whereas it is substantially easier to identify the cita-
tion sentence. In our example above, however, such
an approach would lead to the wrong prediction of
praise or neutral sentiment.
In this paper, we address the problem of context-
enhanced citation sentiment detection. We present
a new citation sentiment corpus where each citation
has been annotated according to the dominant sen-
timent in the corresponding citation context. We
claim that this corpus is closer to the truth than an-
notation that considers only the citation sentence it-
self. We show that it increases citation sentiment
coverage, particularly for negative sentiment. Using
this gold standard, we explore the effect of assum-
ing context windows of different but fixed lengths
on the performance of a state-of-the-art citation sen-
timent detection system where the sentiment of ci-
tation is considered in the entire context of the ci-
tation and more than one single sentiment can be
assigned. Previous approaches neither detect cita-
tion sentiment and context simultaneously nor use
as large a corpus as we do.
2 Corpus Construction
We chose the dataset used by Athar (2011) compris-
ing 310 papers taken from the ACL Anthology (Bird
et al, 2008). The citation summary data from the
ACL Anthology Network1 (Radev et al, 2009) was
used. This dataset is rather large (8736 citations) and
since manual annotation of context for each citation
is a time consuming task, a subset of 20 papers were
selected corresponding to approximately 20% of the
original dataset.
1http://www.aclweb.org
We selected a four-class scheme for annotation.
Every sentence that is in a window of 4 sentences
of the citation and does not contain any direct or in-
direct mention of the citation was labelled as being
excluded (x). The window length was motivated by
recent research (Qazvinian and Radev, 2010) which
shows the best score for a four-sentence boundary
when detecting non-explicit citation. The rest of the
sentences were marked either positive (p), negative
(n) or objective/neutral (o).
A total of 1,741 citations were annotated. Al-
though this annotation was performed by the first
author only, we know from previous work that simi-
lar styles of annotation can achieve acceptable inter-
annotator agreement (Teufel et al, 2006). An exam-
ple annotation for Smadja (1993) is given in Figure
2, where the first column shows the line number and
the second one shows the class label.
Figure 2: Example annotation of a citation context.
To compare our work with Athar (2011), we also
applied a three-class annotation scheme. In this
method of annotation, we merge the citation context
into a single sentence. Since the context introduces
more than one sentiment per citation, we marked the
citation sentiment with the last sentiment mentioned
in the context window as this is pragmatically most
likely to be the real intention (MacRoberts and Mac-
Roberts, 1984).
As is evident from Table 1, including the 4 sen-
tence window around the citation more than dou-
bles the instances of subjective sentiment, and in the
case of negative sentiment, this proportion rises to 3.
In light of the overall sparsity of detectable citation
sentiment in a paper, and of the envisaged applica-
598
tions, this is a very positive result. The reason for
this effect is most likely ?sweetened criticism? ? au-
thors? strategic behaviour of softening the effect of
criticism among their peers (Hornsey et al, 2008).
Without Context With Context
o 87% 73%
n 5% 17%
p 8% 11%
Table 1: Distribution of classes.
3 Experiments and Results
We represent each citation as a feature set in a Sup-
port Vector Machine (SVM) (Cortes and Vapnik,
1995) framework and use n-grams of length 1 to 3
as well as dependency triplets as features. The de-
pendency triplets are constructed by merging the re-
lation, governor and dependent in a single string, for
instance, the relation nsubj(failed, method) is rep-
resented as nsubj failed method . This setup
has been shown to produce good results earlier as
well (Pang et al, 2002; Athar, 2011).
The first set of experiments focuses on simulta-
neous detection of sentiment and context sentences.
For this purpose, we use the four-class annotated
corpus described earlier. While the original anno-
tations were performed for a window of length 4,
we also experiment with asymmetrical windows of l
sentences preceding the citation and r sentences suc-
ceeding it. The detailed results are given in Table 2.
l r x o n p Fmacro Fmicro
0 0 - 1509 86 146 0.768 0.932
1 1 2823 1982 216 200 0.737 0.820
2 2 5984 2214 273 218 0.709 0.851
3 3 9170 2425 318 234 0.672 0.875
4 4 12385 2605 352 252 0.680 0.892
0 4 5963 2171 322 215 0.712 0.853
0 3 4380 2070 293 201 0.702 0.832
0 2 2817 1945 258 193 0.701 0.801
0 1 1280 1812 206 182 0.717 0.777
Table 2: Results for joint context and sentiment de-
tection.
Because of the skewed class distribution, we use
both the Fmacro and Fmicro scores with 10-fold
cross-validation. The baseline score, shown in bold,
is obtained with no context window and is compara-
ble to the results reported by Athar (2011). However,
we can observe that the F scores decrease as more
context is introduced. This may be attributed to the
increase in the vocabulary size of the n-grams and a
consequent reduction in the discriminating power of
the decision boundaries. These results show that the
task of jointly detecting sentiment and context is a
hard problem.
For our second set of experiments, we use the
three-class annotation scheme. We merge the text
of the sentences in the context windows as well as
their dependency triplets to obtain the features. The
results are reported in Table 3 with best results in
bold. Although these results are not better than the
context-less baseline, the reason might be data spar-
sity since existing work on citation sentiment analy-
sis uses more data (Athar, 2011).
l r Fmacro Fmicro
1 1 0.638 0.827
2 2 0.620 0.793
3 3 0.629 0.786
4 4 0.628 0.771
0 4 0.643 0.796
0 3 0.658 0.816
0 2 0.642 0.824
0 1 0.731 0.871
Table 3: Results using different context windows.
4 Related Work
While different schemes have been proposed for
annotating citations according to their function
(Spiegel-Rosing, 1977; Nanba and Okumura, 1999;
Garzone and Mercer, 2000), the only recent work on
citation sentiment detection using a relatively large
corpus is by Athar (2011). However, this work does
not handle citation context. Piao et al (2007) pro-
posed a system to attach sentiment information to
the citation links between biomedical papers by us-
ing existing semantic lexical resources.
A common approach for sentiment detection is to
use a labelled lexicon to score sentences (Hatzivas-
siloglou and McKeown, 1997; Turney, 2002; Yu and
Hatzivassiloglou, 2003). However, such approaches
599
have been found to be highly topic dependent (En-
gstro?m, 2004; Gamon and Aue, 2005; Blitzer et al,
2007).
Teufel et al (2006) worked on a 2,829 sentence ci-
tation corpus using a 12-class classification scheme.
Although they used context in their annotation, their
focus was on determining the author?s reason for cit-
ing a given paper. This task differs from citation sen-
timent, which is in a sense a ?lower level? of analy-
sis.
For implicit citation extraction, Kaplan et al
(2009) explore co-reference chains for citation ex-
traction using a combination of co-reference reso-
lution techniques. However, their corpus consists
of only 94 sentences of citations to 4 papers which
is likely to be too small to be representative. The
most relevant work is by Qazvinian and Radev
(2010) who extract only the non-explicit citations
for a given paper. They model each sentence as a
node in a graph and experiment with various win-
dow boundaries to create edges between neighbour-
ing nodes. However, their dataset consists of only 10
papers and their annotation scheme differs from our
four-class annotation as they do not deal with any
sentiment.
5 Conclusion
In this paper, we focus on automatic detection of
citation sentiment using the citation context. We
present a new corpus and show that ignoring the cita-
tion context would result in loss of a lot of sentiment,
specially criticism towards the cited paper. We also
report the results of the state-of-the-art citation sen-
timent detection systems on this corpus when using
this context-enhanced gold standard definition.
Future work directions may include improving
the detection algorithms by filtering the context sen-
tences more intelligently. For this purpose, exist-
ing work on coreference resolution (Lee et al, 2011)
may prove to be useful. Context features may also
be used for first filtering citations which have been
mentioned only in passing, and then applying con-
text based sentiment classification to the remaining
significant citations.
References
A. Abu-Jbara and D. Radev. 2011. Coherent citation-
based summarization of scientific papers. In Proc. of
ACL.
A. Athar. 2011. Sentiment analysis of citations using
sentence structure-based features. In Proc of ACL,
page 81.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The acl anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proc. of LREC.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In Proc. of ACL,
number 1.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
C. Engstro?m. 2004. Topic dependence in sentiment clas-
sification. University of Cambridge.
M. Gamon and A. Aue. 2005. Automatic identification
of sentiment vocabulary: exploiting low association
with known sentiment terms. In Proc. of the ACL.
M. Garzone and R. Mercer. 2000. Towards an automated
citation classifier. Advances in Artificial Intelligence.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proc. of
ACL, page 181.
M.J. Hornsey, E. Robson, J. Smith, S. Esposo, and R.M.
Sutton. 2008. Sugaring the pill: Assessing rhetori-
cal strategies designed to minimize defensive reactions
to group criticism. Human Communication Research,
34(1):70?98.
K. Hyland. 1995. The Author in the Text: Hedging Sci-
entific Writing. Hong Kong papers in linguistics and
language teaching, 18:11.
D. Kaplan, R. Iida, and T. Tokunaga. 2009. Automatic
extraction of citation contexts for research paper sum-
marization: A coreference-chain based approach. In
Proc. of the 2009 Workshop on Text and Citation Anal-
ysis for Scholarly Digital Libraries.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-pass
sieve coreference resolution system at the conll-2011
shared task. ACL HLT 2011.
M.H. MacRoberts and B.R. MacRoberts. 1984. The
negational reference: Or the art of dissembling. So-
cial Studies of Science, 14(1):91?94.
H. Nanba and M. Okumura. 1999. Towards multi-paper
summarization using reference information. In IJCAI,
volume 16, pages 926?931. Citeseer.
600
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proc. of EMNLP.
S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-
Naught. 2007. Mining opinion polarity relations of ci-
tations. In International Workshop on Computational
Semantics (IWCS). Citeseer.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: Theory
and applications, pages 1?10.
V. Qazvinian and D.R. Radev. 2010. Identifying non-
explicit citing sentences for citation-based summariza-
tion. In Proc. of ACL.
D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis of
the field of Computational Linguistics. Journal of the
American Soc. for Info. Sci. and Tech.
A. Ritchie, S. Robertson, and S. Teufel. 2008. Com-
paring citation contexts for information retrieval. In
Proc. of ACM conference on Information and knowl-
edge management, pages 213?222. ACM.
I. Spiegel-Rosing. 1977. Science studies: Bibliometric
and content analysis. Social Studies of Science.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proc. of
EMNLP, pages 103?110.
P.D. Turney. 2002. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of ACL.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: an exploration of fea-
tures for phrase-level sentiment analysis. Comp. Ling.,
35(3):399?433.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proc.
of EMNLP, page 136.
J.M. Ziman. 1968. Public Knowledge: An essay con-
cerning the social dimension of science. Cambridge
Univ. Press, College Station, Texas.
601
Proceedings of the ACL-HLT 2011 Student Session, pages 81?87,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Sentiment Analysis of Citations using Sentence Structure-Based Features
Awais Athar
University of Cambridge
Computer Laboratory
15 JJ Thompson Avenue
Cambridge, CB3 0FD, U.K.
awais.athar@cl.cam.ac.uk
Abstract
Sentiment analysis of citations in scientific pa-
pers and articles is a new and interesting prob-
lem due to the many linguistic differences be-
tween scientific texts and other genres. In
this paper, we focus on the problem of auto-
matic identification of positive and negative
sentiment polarity in citations to scientific pa-
pers. Using a newly constructed annotated ci-
tation sentiment corpus, we explore the effec-
tiveness of existing and novel features, includ-
ing n-grams, specialised science-specific lex-
ical features, dependency relations, sentence
splitting and negation features. Our results
show that 3-grams and dependencies perform
best in this task; they outperform the sentence
splitting, science lexicon and negation based
features.
1 Introduction
Sentiment analysis is the task of identifying positive
and negative opinions, sentiments, emotions and at-
titudes expressed in text. Although there has been in
the past few years a growing interest in this field for
different text genres such as newspaper text, reviews
and narrative text, relatively less emphasis has been
placed on extraction of opinions from scientific liter-
ature, more specifically, citations. Analysis of cita-
tion sentiment would open up many exciting new ap-
plications in bibliographic search and in bibliomet-
rics, i.e., the automatic evaluation the influence and
impact of individuals and journals via citations.
Existing bibliometric measures like H-Index
(Hirsch, 2005) and adapted graph ranking algo-
rithms like PageRank (Radev et al, 2009) treat all ci-
tations as equal. However, Bonzi (1982) argued that
if a cited work is criticised, it should consequently
carry lower or even negative weight for bibliometric
measures. Automatic citation sentiment detection is
a prerequisite for such a treatment.
Moreover, citation sentiment detection can also
help researchers during search, by detecting prob-
lems with a particular approach. It can be used as
a first step to scientific summarisation, enable users
to recognise unaddressed issues and possible gaps
in the current research, and thus help them set their
research directions.
For other genres a rich literature on sentiment de-
tection exists and researchers have used a number
of features such as n-grams, presence of adjectives,
adverbs and other parts-of-speech (POS), negation,
grammatical and dependency relations as well as
specialised lexicons in order to detect sentiments
from phrases, words, sentences and documents.
State-of-the-art systems report around 85-90% ac-
curacy for different genres of text (Nakagawa et al,
2010; Yessenalina et al, 2010; Ta?ckstro?m and Mc-
Donald, 2011).
Given such good results, one might think that a
sentence-based sentiment detection system trained
on a different genre could be used equally well to
classify citations. We argue that this might not be
the case; our citation sentiment recogniser uses spe-
cialised training data and tests the performance of
specialised features against current state-of-the-art
features. The reasons for this are based on the fol-
lowing observations:
? Sentiment in citations is often hidden. This might
81
be because of the general strategy to avoid overt
criticism due to the sociological aspect of cit-
ing (MacRoberts and MacRoberts, 1984; Thomp-
son and Yiyun, 1991). Ziman (1968) states that
many works are cited out of ?politeness, policy or
piety?. Negative sentiment, while still present and
detectable for humans, is expressed in subtle ways
and might be hedged, especially when it cannot be
quantitatively justified (Hyland, 1995).
While SCL has been successfully applied to POS tag-
ging and Sentiment Analysis (Blitzer et al, 2006), its
effectiveness for parsing was rather unexplored.
? Citation sentences are often neutral with respect
to sentiment, either because they describe an al-
gorithm, approach or methodology objectively, or
because they are used to support a fact or state-
ment.
There are five different IBM translation models (Brown
et al , 1993).
This gives rise to a far higher proportion of objec-
tive sentences than in other genres.
? Negative polarity is often expressed in contrastive
terms, e.g. in evaluation sections. Although the
sentiment is indirect in these cases, its negativity
is implied by the fact that the authors? own work
is clearly evaluated positively in comparison.
This method was shown to outperform the class based
model proposed in (Brown et al, 1992) . . .
? There is also much variation between scientific
texts and other genres concerning the lexical
items chosen to convey sentiment. Sentiment car-
rying science-specific terms exist and are rela-
tively frequent, which motivates the use of a sen-
timent lexicon specialised to science.
Similarity-based smoothing (Dagan, Lee, and Pereira
1999) provides an intuitively appealing approach to
language modeling.
? Technical terms play a large role overall in scien-
tific text (Justeson and Katz, 1995). Some of these
carry sentiment as well.
Current state of the art machine translation systems
(Och, 2003) use phrasal (n-gram) features . . .
For this reason, using higher order n-grams might
prove to be useful in sentiment detection.
? The scope of influence of citations varies widely
from a single clause (as in the example below) to
several paragraphs:
As reported in Table 3, small increases in METEOR
(Banerjee and Lavie, 2005), BLEU (Papineni et al,
2002) and NIST scores (Doddington, 2002) suggest
that . . .
This affects lexical features directly since there
could be ?sentiment overlap? associated with
neighbouring citations. Ritchie et al (2008)
showed that assuming larger citation scopes has
a positive effect in retrieval. We will test the op-
posite direction here, i.e., we assume short scopes
and use a parser to split sentences, so that the fea-
tures associated with the clauses not directly con-
nected to the citation are disregarded.
We created a new sentiment-annotated corpus of
scientific text in the form of a sentence-based col-
lection of over 8700 citations. Our experiments
use a supervised classifier with the state-of-the-art
features from the literature, as well as new fea-
tures based on the observations above. Our results
show that the most successful feature combination
includes dependency features and n-grams longer
than for other genres (n = 3), but the assumption
of a smaller scope (sentence splitting) decreased re-
sults.
2 Training and Test Corpus
We manually annotated 8736 citations from 310 re-
search papers taken from the ACL Anthology (Bird
et al, 2008). The citation summary data from the
ACL Anthology Network1 (Radev et al, 2009) was
used. We identified the actual text of the citations
by regular expressions and replaced it with a special
token <CIT> in order to remove any lexical bias
associated with proper names of researchers. We la-
belled each sentence as positive, negative or objec-
tive, and separated 1472 citations for development
and training. The rest were used as the test set con-
taining 244 negative, 743 positive and 6277 objec-
tive citations. Thus our dataset is heavily skewed,
with subjective citations accounting for only around
14% of the corpus.
1http://www.aclweb.org
82
3 Features
We represent each citation as a feature set in a Sup-
port Vector Machine (SVM) (Cortes and Vapnik,
1995) framework which has been shown to produce
good results for sentiment classification (Pang et
al., 2002). The corpus is processed using WEKA
(Hall et al, 2008) and the Weka LibSVM library
(EL-Manzalawy and Honavar, 2005; Chang and Lin,
2001) with the following features.
3.1 Word Level Features
In accordance with Pang et al (2002), we use uni-
grams and bigrams as features and also add 3-grams
as new features to capture longer technical terms.
POS tags are also included using two approaches:
attaching the tag to the word by a delimiter, and ap-
pending all tags at the end of the sentence. This may
help in distinguishing between homonyms with dif-
ferent POS tags and signalling the presence of ad-
jectives (e.g., JJ) respectively. Name of the primary
author of the cited paper is also used as a feature.
A science-specific sentiment lexicon is also added
to the feature set. This lexicon consists of 83 polar
phrases which have been manually extracted from
the development set of 736 citations. Some of the
most frequently occurring polar phrases in this set
consists of adjectives such as efficient, popular, suc-
cessful, state-of-the-art and effective.
3.2 Contextual Polarity Features
Features previously found to be useful for detect-
ing phrase-level contextual polarity (Wilson et al,
2009) are also included. Since the task at hand is
sentence-based, we use only the sentence-based fea-
tures from the literature e.g., presence of subjectiv-
ity clues which have been compiled from several
sources2 along with the number of adjectives, ad-
verbs, pronouns, modals and cardinals.
To handle negation, we include the count of nega-
tion phrases found within the citation sentence. Sim-
ilarly, the number of valance shifters (Polanyi and
Zaenen, 2006) in the sentence are also used. The
polarity shifter and negation phrase lists have been
taken from the OpinionFinder system (Wilson et al,
2005).
2Available for download at http://www.cs.pitt.edu/mpqa/
3.3 Sentence Structure Based Features
We explore three different feature sets which focus
on the lexical and grammatical structure of a sen-
tence and have not been explored previously for the
task of sentiment analysis of scientific text.
3.3.1 Dependency Structures
The first set of these features include typed depen-
dency structures (de Marneffe and Manning, 2008)
which describe the grammatical relationships be-
tween words. We aim to capture the long distance
relationships between words. For instance in the
sentence below, the relationship between results and
competitive will be missed by trigrams but the de-
pendency representation captures it in a single fea-
ture nsubj competitive results.
<CIT> showed that the results for French-English
were competitive to state-of-the-art alignment systems.
A variation we experimented with, but gave up
on as it did not show any improvements, concerns
backing-off the dependent and governor to their POS
tags (Joshi and Penstein-Rose?, 2009).
3.3.2 Sentence Splitting
Removing irrelevant polar phrases around a ci-
tation might improve results. For this purpose, we
split each sentence by trimming its parse tree. Walk-
ing from the citation node (<CIT>) towards the
root, we select the subtree rooted at the first sentence
node (S) and ignore the rest. For example, in Figure
1, the cited paper is not included in the scope of the
discarded polar phrase significant improvements.
Figure 1: An example of parse tree trimming
83
3.3.3 Negation
Dependencies and parse trees attach negation
nodes, such as not, to the clause subtree and this
shows no interaction with other nodes with respect
to valence shifting. To handle this effect, we take
a simple window-based inversion approach. All
words inside a k-word window of any negation term
are suffixed with a token neg to distinguish them
from their non-polar versions. For example, a 2-
word negation window inverts the polarity of the
positive phrase work well in the sentence below.
Turney?s method did not work neg well neg although
they reported 80% accuracy in <CIT>.
The negation term list has been taken from the
OpinionFinder system. Khan (2007) has shown that
this approach produces results comparable to gram-
matical relations based negation models.
4 Results
Because of our skewed dataset, we report both
the macro-F and the micro-F scores using 10-fold
cross-validation (Lewis, 1991). The bold values in
Table 1 show the best results.
Features macro-F micro-F
1 grams 0.581 0.863
1-2 grams 0.592 0.864
1-3 grams 0.597 0.862
?? + POS 0.535 0.859
?? + POS (tokenised) 0.596 0.859
?? + scilex 0.597 0.860
?? + wlev 0.535 0.859
?? + cpol 0.418 0.859
?? + dep 0.760 0.897
?? + dep + split + neg 0.683 0.872
?? + dep + split 0.642 0.866
?? + dep + neg 0.764 0.898
Table 1: Results using science lexicon (scilex), contex-
tual polarity (cpol), dependencies (dep), negation (neg),
sentence splitting (split) and word-level (wlev) features.
The selection of the features is on the basis of im-
provements over a baseline of 1-3 grams i.e. if a
feature (e.g. scilex) did not shown any improvement,
it is has been excluded from the subsequent experi-
ments.
The results show that contextual polarity features
do not work well on citation text. Adding a science-
specific lexicon does not help either. This may indi-
cate that n-grams are sufficient to capture discrim-
inating lexical structures. We find that word level
and contextual polarity features are surpassed by de-
pendency features. Sentence splitting does not help,
possibly due to longer citation scope. Adding a
negation window (k=15) improves the performance
but the improvement was not found to be statistically
significant. This might be due to skewed class dis-
tribution and a larger dataset may prove to be useful.
5 Related Work
While different schemes have been proposed for
annotating citations according to their function
(Spiegel-Ro?sing, 1977; Nanba and Okumura, 1999;
Garzone and Mercer, 2000), there have been no at-
tempts on citation sentiment detection in a large cor-
pus.
Teufel et al (2006) worked on a 2829 sentence ci-
tation corpus using a 12-class classification scheme.
However, this corpus has been annotated for the task
of determining the author?s reason for citing a given
paper and is thus built on top of sentiment of cita-
tion. It considers usage, modification and similar-
ity with a cited paper as positive even when there is
no sentiment attributed to it. Moreover, contrast be-
tween two cited methods (CoCoXY) is categorized
as objective in the annotation scheme even if the text
indicates that one method performs better than the
other. For example, the sentence below talks about
a positive attribute but is marked as neutral in the
scheme.
Lexical transducers are more efficient for analysis and
generation than the classical two-level systems (Kosken-
niemi,1983) because . . .
Using this corpus is thus more likely to lead to
inconsistent representation of sentiment in any sys-
tem which relies on lexical features. Teufel et al
(2006) group the 12 categories into 3 in an at-
tempt to perform a rough approximation of senti-
ment analysis over the classifications and report a
0.710 macro-F score. Unfortunately, we have ac-
84
cess to only a subset3 of this citation function cor-
pus. We have extracted 1-3 grams, dependencies and
negation features from the reduced citation function
dataset and used them in our system with 10-fold
cross-validation. This results in an improved macro-
F score of 0.797 for the subset. This shows that
our system is comparable to Teufel et al (2006).
When this subset is used to test the system trained on
our newly annotated corpus, a low macro-F score of
0.484 is achieved. This indicates that there is a mis-
match in the annotated class labels. Therefore, we
can infer that citation sentiment classification is dif-
ferent from citation function classification.
Other approaches to citation annotation and clas-
sification include Wilbur et al (2006) who annotated
a small 101 sentence corpus on focus, polarity, cer-
tainty, evidence and directionality. Piao et al (2007)
proposed a system to attach sentiment information
to the citation links between biomedical papers.
Different dependency relations have been ex-
plored by Dave et al (2003), Wilson et al (2004)
and Ng et al (2006) for sentiment detection. Nak-
agawa et al (2010) report that using dependencies
on conditional random fields with lexicon based po-
larity reversal results in improvements over n-grams
for news and reviews corpora.
A common approach is to use a sentiment la-
belled lexicon to score sentences (Hatzivassiloglou
and McKeown, 1997; Turney, 2002; Yu and Hatzi-
vassiloglou, 2003). Research suggests that creating
a general sentiment classifier is a difficult task and
existing approaches are highly topic dependent (En-
gstro?m, 2004; Gamon and Aue, 2005; Blitzer et al,
2007).
6 Conclusion
In this paper, we focus on automatic identification
of sentiment polarity in citations. Using a newly
constructed annotated citation sentiment corpus, we
examine the effectiveness of existing and novel fea-
tures, including n-grams, scientific lexicon, depen-
dency relations and sentence splitting. Our results
show that 3-grams and dependencies perform best
in this task; they outperform the scientific lexicon
and the sentence splitting features. Future direc-
3This subset contains 591 positive, 59 negative and 1259
objective citations.
tions include trying to improve the performance by
modelling negations using a more sophisticated ap-
proach. New techniques for detection of the nega-
tion scope such as the one proposed by Councill et
al. (2010) might also be helpful in citations. Explor-
ing longer citation scopes by including citation con-
texts might also improve citation sentiment detec-
tion.
References
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph,
M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and Y.F.
Tan. 2008. The acl anthology reference corpus: A
reference dataset for bibliographic research in compu-
tational linguistics. In Proc. of the 6th International
Conference on Language Resources and Evaluation
Conference (LREC08), pages 1755?1759. Citeseer.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL, volume 45,
page 440.
S. Bonzi. 1982. Characteristics of a literature as pre-
dictors of relatedness between cited and citing works.
Journal of the American Society for Information Sci-
ence, 33(4):208?216.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a li-
brary for support vector machines, 2001. Software
available at http://www.csie.ntu.edu.tw/
cjlin/libsvm.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
I.G. Councill, R. McDonald, and L. Velikovich. 2010.
What?s great and what?s not: learning to classify the
scope of negation for improved sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 51?59.
Association for Computational Linguistics.
K. Dave, S. Lawrence, and D.M. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic
classification of product reviews. In Proceedings of
the 12th international conference on World Wide Web,
pages 519?528. ACM.
M.C. de Marneffe and C.D. Manning. 2008. The Stan-
ford typed dependencies representation. In COLING,
pages 1?8. Association for Computational Linguistics.
Y. EL-Manzalawy and V. Honavar, 2005. WLSVM:
Integrating LibSVM into Weka Environment. Soft-
ware available at http://www.cs.iastate.
edu/?yasser/wlsvm.
C. Engstro?m. 2004. Topic dependence in sentiment clas-
sification. Unpublished MPhil Dissertation. Univer-
sity of Cambridge.
85
M. Gamon and A. Aue. 2005. Automatic identification
of sentiment vocabulary: exploiting low association
with known sentiment terms. In Proceedings of the
ACL Workshop on Feature Engineering for Machine
Learning in Natural Language Processing, pages 57?
64. Association for Computational Linguistics.
M. Garzone and R. Mercer. 2000. Towards an automated
citation classifier. Advances in Artificial Intelligence,
pages 337?346.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Studying
the history of ideas using topic models. In EMNLP,
pages 363?371.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of EACL, pages 174?181. Association for Com-
putational Linguistics.
J.E. Hirsch. 2005. An index to quantify an individual?s
scientific research output. Proceedings of the National
Academy of Sciences of the United States of America,
102(46):16569.
K. Hyland. 1995. The Author in the Text: Hedging Sci-
entific Writing. Hong Kong papers in linguistics and
language teaching, 18:11.
M. Joshi and C. Penstein-Rose?. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313?316. Association for Computational
Linguistics.
J.S. Justeson and S.M. Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural language engineering,
1(01):9?27.
S. Khan. 2007. Negation and Antonymy in Sentiment
Classification. Ph.D. thesis, Computer Lab, Univer-
sity of Cambridge.
D.D. Lewis. 1991. Evaluating text categorization. In
Proceedings of Speech and Natural Language Work-
shop, pages 312?318.
M.H. MacRoberts and B.R. MacRoberts. 1984. The
negational reference: Or the art of dissembling. So-
cial Studies of Science, 14(1):91?94.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL HLT, pages 786?
794. Association for Computational Linguistics.
H. Nanba and M. Okumura. 1999. Towards multi-paper
summarization using reference information. In IJCAI,
volume 16, pages 926?931. Citeseer.
V. Ng, S. Dasgupta, and SM Arifin. 2006. Examining
the role of linguistic knowledge sources in the auto-
matic identification and classification of reviews. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 611?618. Association for Com-
putational Linguistics.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In EMNLP, pages 79?86. Association for
Computational Linguistics.
S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-
Naught. 2007. Mining opinion polarity relations of ci-
tations. In International Workshop on Computational
Semantics (IWCS), pages 366?371. Citeseer.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: Theory
and applications, pages 1?10.
D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis of
the field of Computational Linguistics. Journal of the
American Society for Information Science and Tech-
nology, 1001:48109?1092.
A. Ritchie, S. Robertson, and S. Teufel. 2008. Compar-
ing citation contexts for information retrieval. In Pro-
ceeding of the 17th ACM Conference on Information
and Knowledge Management, pages 213?222. ACM.
I. Spiegel-Ro?sing. 1977. Science studies: Bibliomet-
ric and content analysis. Social Studies of Science,
7(1):97?113.
O. Ta?ckstro?m and R. McDonald. 2011. Discovering
fine-grained sentiment with latent variable structured
prediction models. In Proceedings of the ECIR.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In EMNLP,
pages 103?110. Association for Computational Lin-
guistics.
G. Thompson and Y. Yiyun. 1991. Evaluation in the
reporting verbs used in academic papers. Applied lin-
guistics, 12(4):365.
P.D. Turney. 2002. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classification of
reviews. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
417?424. Association for Computational Linguistics.
W.J. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC bioinfor-
matics, 7(1):356.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the National Conference on Artificial
Intelligence, pages 761?769. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Rec-
ognizing contextual polarity in phrase-level sentiment
analysis. In EMNLP, pages 347?354. Association for
Computational Linguistics.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
86
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399?433.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In Proceedings of EMNLP, pages 1046?
1056, Cambridge, MA, October. Association for Com-
putational Linguistics.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP, pages 129?136. Association
for Computational Linguistics.
J.M. Ziman. 1968. Public Knowledge: An essay con-
cerning the social dimension of science. Cambridge
Univ. Press, College Station, Texas.
87
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 18?26,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Detection of Implicit Citations for Sentiment Detection
Awais Athar Simone Teufel
Computer Laboratory, University of Cambridge
15 JJ Thomson Avenue, Cambridge CB3 0FD, UK
{awais.athar,simone.teufel}@cl.cam.ac.uk
Abstract
Sentiment analysis of citations in scientific pa-
pers is a new and interesting problem which
can open up many exciting new applications in
bibliometrics. Current research assumes that
using just the citation sentence is enough for
detecting sentiment. In this paper, we show
that this approach misses much of the exist-
ing sentiment. We present a new corpus in
which all mentions of a cited paper have been
annotated. We explore methods to automat-
ically identify these mentions and show that
the inclusion of implicit citations in citation
sentiment analysis improves the quality of the
overall sentiment assignment.
1 Introduction
The idea of using citations as a source of information
has been explored extensively in the field of biblio-
metrics, and more recently in the field of compu-
tational linguistics. State-of-the-art citations iden-
tification mechanisms focus either on detecting ex-
plicit citations i.e. those that consist of either the
author names and the year of publication or brack-
eted numbers only, or include a small sentence win-
dow around the explicit citation as input text (Coun-
cill et al, 2008; Radev et al, 2009; Ritchie et al,
2008). The assumption behind this approach is that
all related mentions of the paper would be concen-
trated in the immediate vicinity of the anchor text.
However, this assumption does not generally hold
true (Teufel, 2010; Sugiyama et al, 2010). The phe-
nomenon of trying to determine a citations?s cita-
tion context has a long tradition in library sciences
(O?Connor, 1982), and its connection with corefer-
ence has been duely noted (Kim et al, 2006; Kaplan
et al, 2009). Consider Figure 1, which illustrates a
typical case.
Figure 1: Example of the use of anaphora
While the first sentence cites the target paper ex-
plicitly using the name of the primary author along
with the year of publication of the paper, the re-
maining sentences mentioning the same paper ap-
pear after a gap and contain an indirect and implicit
reference to that paper. These mentions occur two
sentences after the formal citation in the form of
anaphoric it and the lexical hook METEOR. Most
current techniques, with the exception of Qazvinian
and Radev (2010), are not able to detect linguistic
mentions of citations in such forms. Ignoring such
mentions and examining only the sentences contain-
18
ing an explicit citation results in loss of information
about the cited paper. While this phenomenon is
problematic for applications like scientific summari-
sation (Abu-Jbara and Radev, 2011), it has a particu-
lar relevance for citation sentiment detection (Athar,
2011).
Citation sentiment detection is an attractive task.
Availability of citation polarity information can help
researchers in understanding the evolution of a field
on the basis of research papers and their critiques.
It can also help expert researchers who are in the
process of preparing opinion based summaries for
survey papers by providing them with motivations
behind as well as positive and negative comments
about different approaches (Qazvinian and Radev,
2008).
Current work on citation sentiment detection
works under the assumption that the sentiment
present in the citation sentence represents the true
sentiment of the author towards the cited paper
(Athar, 2011; Piao et al, 2007; Pham and Hoffmann,
2004). This assumption is so dominant because
current citation identification methods (Councill et
al., 2008; Ritchie et al, 2008; Radev et al, 2009)
can readily identify the citation sentence, whereas
it is much harder to determine the relevant context.
However, this assumption most certainly does not
hold true when the citation context spans more than
one sentence.
Concerning the sentiment aspect of the citation
context from Figure 1, we see that the citation sen-
tence does not contain any sentiment towards the
cited paper, whereas the following sentences act as
a critique and list its shortcomings. It is clear that
criticism is the intended sentiment, but if the gold
standard is defined by looking at the citation sen-
tence in isolation, a significant amount of sentiment
expressed in the text is lost. Given that overall most
citations in a text are neutral with respect to sen-
timent (Spiegel-Rosing, 1977; Teufel et al, 2006),
this makes it even more important to recover what
explicit sentiment there is in the article, wherever it
is to be found.
In this paper, we examine methods to extract all
opinionated sentences from research papers which
mention a given paper in as many forms as we can
identify, not just as explicit citations. We present
a new corpus in which all mentions of a cited paper
have been manually annotated, and show that our an-
notation treatment increases citation sentiment cov-
erage, particularly for negative sentiment. We then
explore methods to automatically identify all men-
tions of a paper in a supervised manner. In par-
ticular, we consider the recognition of named ap-
proaches and acronyms. Our overall system then
classifies explicit and implicit mentions according to
sentiment. The results support the claim that includ-
ing implicit citations in citation sentiment analysis
improves the quality of the overall sentiment assign-
ment.
2 Corpus Construction
We use the dataset from Athar (2011) as our starting
point, which consists of 8,736 citations in the ACL
Anthology (Bird et al, 2008) that cite a target set of
310 ACL Anthology papers. The citation summary
data from the ACL Anthology Network1 (Radev et
al., 2009) is used. This dataset is rather large, and
since manual annotation of context for each citation
is a time consuming task, a subset of 20 target pa-
pers (i.e., all citations to these) has been selected
for annotation. These 20 papers correspond to ap-
proximately 20% of incoming citations in the orig-
inal dataset. They contain a total of 1,555 citations
from 854 citing papers.
2.1 Annotation
We use a four-class scheme for annotation. Every
sentence which does not contain any direct or indi-
rect mention of the citation is labelled as being ex-
cluded (x) from the context. The rest of the sen-
tences are marked either positive (p), negative (n)
or objective/neutral (o). To speed up the annotation
process, we developed a customised annotation tool.
A total of 203,803 sentences have been annotated
from 1,034 paper?reference pairs. Although this an-
notation been performed by the first author only,
we know from previous work that similar styles
of annotation can achieve acceptable inter-annotator
agreement (Teufel et al, 2006). An example anno-
tation is given in Figure 2, where the first column
shows the line number and the second one shows
the class label for the citation to Smadja (1993). It
should be noted that since annotation is always per-
1http://www.aclweb.org
19
formed for a specific citation only, sentences such as
the one at line 32, which carry sentiment but refer to
a different citation, are marked as excluded from the
context.
If there are multiple sentiments in the same sen-
tence, the sentence has been labelled with the class
of the last sentiment mentioned. In this way, a total
of 3,760 citation sentences have been found in the
whole corpus, i.e. sentences belonging to class o, n
or p, and the rest have been labelled as x. Table 1
compares the number of sentences with only the ex-
plicit citations with all explicit and implicit mentions
of those citations. We can see that including the
citation context increases the subjective sentiment
by almost 185%. The resulting negative sentiment
also increases by more than 325%. This may be at-
tributed to the strategic behaviour of the authors of
?sweetening? the criticism in order to soften its ef-
fects among their peers (Hornsey et al, 2008).
Figure 2: Example annotation of a citation context.
Explicit mentions All mentions
o 1, 509 3, 100
n 86 368
p 146 292
Table 1: Distribution of classes.
Another view of the annotated data is available in
Figure 3a. This is in the form of interactive HTML
where each HTML page represents all the incoming
links to a paper. Each row represents the citing pa-
per and each column square represents a sentence.
The rows are sorted by increasing publication date.
Black squares are citations with the author name and
year of publication mentioned in the text. The red,
green and gray squares show negative, positive and
neutral sentiment respectively. Pointing the mouse
cursor at any square gives the text content of the cor-
responding sentence, as shown in the Figure 3a.
The ACL Id, paper title and authors? names are
also given at the top of the page. Similar data for the
corresponding citing paper is made available when
the mouse cursor is positioned on one of the orange
squares at the start of each row, as shown in the Fig-
ure 3b. Clicking on the checkboxes at the top hides
or shows the corresponding type of squares. There is
also an option to hide/show a grid so that the squares
are separated and rows are easier to trace. For ex-
ample, Figure 3b shows the grid with the neutral or
objective citations hidden.
In the next section, we describe the features set we
use to detect implicit citations from this annotated
corpus and discuss the results.
3 Experiments and Results
For the task of detecting all mentions of a citation,
we merge the class labels of sentences mentioning a
citation in any form (o n p). To make sure that the
easily detectable explicit citations do not influence
the results, we change the class label of all those
sentences to x which contain the first author?s name
within a 4-word window of the year of publication.
Our dataset is skewed as there are many more ob-
jective sentences than subjective ones. In such sce-
narios, average micro-F scores tend to be slightly
higher as they are a weighted measure. To avoid
this bias, we also report the macro-F scores. Fur-
thermore, to ensure there is enough data for training
each class, we use 10-fold cross-validation (Lewis,
1991) in all our experiments.
We represent each citation as a feature set in a
Support Vector Machine (SVM) (Cortes and Vapnik,
1995) framework. The corpus is processed using
WEKA (Hall et al, 2008) and the Weka LibSVM
library (EL-Manzalawy and Honavar, 2005; Chang
and Lin, 2001). For each ith sentence Si, we use the
following binary features.
? Si?1 contains the last name of the primary au-
thor, followed by the year of publication within
a four-word window.
20
(a) Sentence Text (b) Paper metadata
Figure 3: Different views of an annotated paper.
This feature is meant to capture the fact that
the sentence immediately after an explicit cita-
tion is more likely to continue talking about the
same work.
? Si contains the last name of the primary au-
thor followed by the year of publication within
a four-word window.
This feature should help in identifying sen-
tences containing explicit citations. Since such
sentences are easier to identify, including them
in the evaluation metric would result in a false
boost in the final score. We have thus excluded
all such sentences in our annotation and this
feature should indicate a negative instance to
the classifier.
? Si contains the last name of the primary au-
thor.
This feature captures sentences which contain
a reference to tools and algorithms which have
been named after their inventors, such as,
?One possible direction for future work is to
compare the search-based approach of Collins
and Roark with our DP-based approach.?
It should also capture the mentions of methods
and techniques used in the cited paper e.g.,
?We show that our approach outperforms Tur-
ney?s approach.?
? Si contains an acronym used in an explicit ci-
tation.
Acronyms are taken to be capitalised words
which are extracted from the vicinity of the
cited author?s last name using regular expres-
sions. For example, METEOR in Figure 1 is an
acronym which is used in place of a formal ci-
tation to refer to the original paper in the rest of
the citing paper.
? Si contains a determiner followed by a work
noun.
We use the following determiners D = {the,
this, that, those, these, his, her, their, such, pre-
vious, other}. The list of work nouns (tech-
nique, method, etc.) has been taken from Teufel
(2010). This feature extracts a pattern which
has been found to be useful for extracting cita-
tions in previous work (Qazvinian and Radev,
2010). Such phrases usually signal a continua-
tion of the topics related to citations in earlier
sentences. For example:
?Church et al(1989), Wettler & Rapp (1989)
and Church & Hanks (1990) describe algo-
rithms which do this. However, the validity of
these algorithms has not been tested by system-
atic comparisons with associations of human
subjects.?
? Si starts with a third person pronoun.
The feature also tries to capture the topic con-
tinuation after a citation. Sentences starting
with a pronoun (e.g. they, their, he, she, etc.)
are more likely to describe the subject citation
of the previous sentence in detail. For example:
21
?Because Daume III (2007) views the adapta-
tion as merely augmenting the feature space,
each of his features has the same prior mean
and variance, regardless of whether it is do-
main specific or independent. He could have
set these parameters differently, but he did not.?
? Si starts with a connector.
This feature also focuses on detecting the topic
continuity. Connectors have been shown to
be effective in other context related works as
well (Hatzivassiloglou and McKeown, 1997;
Polanyi and Zaenen, 2006). A list of 23 con-
nectors (e.g. however, although, moreover, etc.)
has been compiled by examining the high fre-
quency connectors from a separate set of papers
from the same domain. An example is:
?An additional consistent edge of a linear-
chain conditional random field (CRF) explicitly
models the dependencies between distant oc-
currences of similar words (Sutton and McCal-
lum, 2004; Finkel et al , 2005). However, this
approach requires additional time complexity
in inference/learning time and it is only suit-
able for representing constraints by enforcing
label consistency.?
? Si starts with a (sub)section heading.
? Si?1 starts with a (sub)section heading.
? Si+1 starts with a (sub)section heading.
The three features above are a consequence of
missing information about the paragraph and
section boundaries in the used corpus. Since
the text extraction has been done automatically,
the section headings are usually found to be
merged with the text of the succeeding sen-
tence. For example, the text below merges the
heading of section 4.2 with the next sentence.
?4.2 METEOR vs. SIA SIA is designed to take
the advantage of loose sequence-based metrics
without losing word-level information.?
Start and end of such section boundaries can
give us important information about the scope
of a citation. In order to exploit this informa-
tion, we use regular expressions to detect if the
sentences under review contains these merged
section titles and headings.
? Si contains a citation other than the one under
review.
It is more probable for the context of a citation
to end when other citations are mentioned in
a sentence, which is the motivation behind us-
ing this feature, which might contribute to the
discriminating power of the classifier in con-
junction with the presence of a citation in the
previous sentence. For example, in the extract
below, the scope of the first citation is limited
to the first sentence only.
?Blitzer et al(2006) proposed a structural
correspondence learning method for domain
adaptation and applied it to part-of-speech tag-
ging. Daume III (2007) proposed a simple fea-
ture augmentation method to achieve domain
adaptation.?
? Si contains a lexical hook.
The lexical hooks feature identifies lexical sub-
stitutes for the citations. We obtain these hooks
by examining all explicit citation sentences to
the cited paper and selecting the most frequent
capitalized phrase in the vicinity of the author?s
last name. The explicit citations come from all
citing papers and not just the paper for which
the features are being determined. For exam-
ple, the sentences below have been taken from
two different papers and cite the same target pa-
per (Cutting et al, 1992). While the acronym
HMM will be captured by the feature stated ear-
lier, the word Xerox will be missed.
E95-1014: ?This text was part-of-speech
tagged using the Xerox HMM tagger (Cutting
et al , 1992).?
J97-3003: ?The Xerox tagger (Cutting et al
1992) comes with a set of rules that assign an
unknown word a set of possible pos-tags (i.e. ,
POS-class) on the basis of its ending segment.?
This ?domain level? feature makes it possible
to extract the commonly used name for a tech-
nique which may have been missed by the
acronym feature due to long term dependen-
cies. We also extrapolate the acronym for such
22
phrases, e.g., in the example below, SCL would
also be checked along with Structural Corre-
spondence Learning.
?The paper compares Structural Correspon-
dence Learning (Blitzer et al, 2006) with (var-
ious instances of) self-training (Abney, 2007;
McClosky et al, 2006) for the adaptation of a
parse selection model to Wikipedia domains?
We also add n-grams of length 1 to 3 to this lexi-
cal feature set and compare the results obtained with
an n-gram only baseline in Table 2. N-grams have
been shown to perform consistently well in various
NLP tasks (Bergsma et al, 2010).
Class Baseline Our System
x 0.995 0.996
o n p 0.358 0.513
Avg. 0.990 0.992
Avg.(macro) 0.677 0.754
Table 2: Comparison of F -scores for non-explicit
citation detection.
By adding the new features listed above, the per-
formance of our system increases by almost 8% over
the n-gram baseline for the task of detecting citation
mentions. Using the pairwise Wilcoxon rank-sum
test at 0.05 significance level, we found that the dif-
ference between the baseline and our system is sta-
tistically significant2. While the micro-F score ob-
tained is quite high, the individual class scores show
that the task is hard and a better solution may require
a deeper analysis of the context.
4 Impact on Citation Sentiment Detection
We explore the effect of this context on citation sen-
timent detection. For a baseline, we use features of
the state-of-the-art system proposed in our earlier
work (Athar, 2011). While there we used n-gram
and dependency feature on sentences containing ex-
plicit citations only, our annotation is not restricted
to such citations and we may have more than one
2While this test may not be adequate as the data is highly
skewed, we are reporting the results since there is no obvious
alternative for discrete skewed data. In future, we plan to use
the continuous probability estimates produced by the classifier
for testing significance.
sentiment per each explicit citation. For example,
in Figure 2, our 2011 system will be restricted to
analysing sentence 33 only. However, it is clear
from our annotation that there is more sentiment
present in the succeeding sentences which belongs
to this explicit citation. While sentence 34 in Fig-
ure 2 is positive towards the cited paper, the next
sentence criticises it. Thus for this explicit citation,
there are three sentences with sentiment and all of
them are related to the same explicit citation. Treat-
ing these sentences separately will result in an artifi-
cial increase in the amount of data because they par-
ticipate in the same discourse. It would also make
it impossible to compare the sentiment annotated in
the previous work with our annotation.
To make sure the annotations are comparable,
we mark the true citation sentiment to be the last
sentiment mentioned in a 4-sentence context win-
dow, as this is pragmatically most likely to be the
real intention (MacRoberts and MacRoberts, 1984).
The window length is motivated by recent research
(Qazvinian and Radev, 2010) which favours a four-
sentence boundary for detecting non-explicit cita-
tions. Analysis of our data shows that more than
60% of the subjective citations lie in this window.
We include the implicit citations predicted by the
method described in the previous section in the con-
text. The results of the single-sentence baseline sys-
tem are compared with this context enhanced system
in Table 3.
Class Baseline Our System
o 0.861 0.887
n 0.138 0.621
p 0.396 0.554
Avg. 0.689 0.807
Avg.(macro) 0.465 0.687
Table 3: F -scores for citation sentiment detection.
The results show that our system outperforms the
baseline in all evaluation criteria. Performing the
pairwise Wilcoxon rank-sum testat 0.05 significance
level, we found that the improvement is statistically
significant. The baseline system does not use any
context and thus misses out on all the sentiment
information contained within. While this window-
based representation does not capture all the senti-
23
ment towards a citation perfectly, it is closer to the
truth than a system based on single sentence analysis
and is able to detect more sentiment.
5 Related Work
While different schemes have been proposed for
annotating citations according to their function
(Spiegel-Rosing, 1977; Nanba and Okumura, 1999;
Garzone and Mercer, 2000), the only recent work on
citation sentiment detection using a relatively large
corpus is by Athar (2011). However, this work does
not handle citation context. Other approaches to ci-
tation classification include work by Wilbur et al
(2006), who annotated a 101 sentence corpus on
focus, polarity, certainty, evidence and directional-
ity. Piao et al (2007) proposed a system to attach
sentiment information to the citation links between
biomedical papers by using existing semantic lexical
resources and NLP tools.
A common approach for sentiment detection is to
use a labelled lexicon to score sentences (Hatzivas-
siloglou and McKeown, 1997; Turney, 2002; Yu and
Hatzivassiloglou, 2003). However, such approaches
have been found to be highly topic dependent (En-
gstro?m, 2004; Gamon and Aue, 2005; Blitzer et al,
2007), which makes the creation of a general senti-
ment classifier a difficult task.
Teufel et al (2006) worked on a 2,829 sentence ci-
tation corpus using a 12-class classification scheme.
While the authors did make use of the context in
their annotation, their focus was on the task of deter-
mining the author?s reason for citing a given paper.
This task differs from citation sentiment detection,
which is in a sense a ?lower level? of analysis.
Some other recent work has focused on the prob-
lem of implicit citation extraction (Kaplan et al,
2009; Qazvinian and Radev, 2010). Kaplan et al
(2009) explore co-reference chains for citation ex-
traction using a combination of co-reference resolu-
tion techniques (Soon et al, 2001; Ng and Cardie,
2002). However, the corpus that they use consists of
only 94 citations to 4 papers and is likely to be too
small to be representative.
For citation extraction, the most relevant work is
by Qazvinian and Radev (2010) who proposed a
framework of Markov Random Fields to extract only
the non-explicit citations for a given paper. They
model each sentence as a node in a graph and ex-
periment with various window boundaries to cre-
ate edges between neighbouring nodes weighted by
lexical similarity between nodes. However, their
dataset consists of only 569 citations from 10 pa-
pers and their annotation scheme deals with neither
acronyms nor sentiment.
6 Discussion
What is the role of citation contexts in the overall
structure of scientific context? We assume a hier-
archical, rhetorical structure not unlike RST (Mann
and Thompson, 1987), but much flatter, where the
atomic units are textual blocks which carry a cer-
tain functional role in the overall scientific argument
for publication (Teufel, 2010; Hyland, 2000). Under
such a general model, citation blocks are certainly
a functional unit, and their recognition is a reward-
ing task in their own right. If citation blocks can be
recognised along with their sentiment, this is even
more useful, as it restricts the possibilities for which
rhetorical function the segment plays. For instance,
in the motivation section of a paper, before the pa-
per contribution is introduced, we often find nega-
tive sentiment assigned to citations, as any indica-
tion can serve as a justification for the current paper.
In contrast, positive sentiment is more likely to be
restricted to the description of an approach which
the authors include in their solution, or further de-
velop.
Another aspect concerns which features might
help in detecting coherent citation blocks. We have
here addressed coherence of citation contexts via
certain referring expressions, lexical hooks and also
coherence-indicating conjunctions (amongst oth-
ers). The reintroduction of citation contexts was
addressed via lexical hooks. Much more could be
done to explore this very interesting question. A
more fine-grained model of coherence might include
proper anaphora resolution (Lee et al, 2011), which
is still an unsolved task for scientific texts, and also
include models of lexical coherence such as lexical
chains (Barzilay and Elhadad, 1997) and entity co-
herence (Barzilay and Lapata, 2008).
24
7 Conclusion
In this paper, we focus on automatic detection of ci-
tation sentiment using citation context. We annotate
a new large corpus and show that ignoring the cita-
tion context would result in loss of a lot of sentiment,
specially criticism. We also report the results of the
state-of-the-art citation sentiment detection systems
on this corpus and when using this context-enhanced
gold standard definition.
References
A. Abu-Jbara and D. Radev. 2011. Coherent citation-
based summarization of scientific papers. In Proc. of
ACL.
A. Athar. 2011. Sentiment analysis of citations using
sentence structure-based features. In Proc of ACL,
page 81.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Inderjeet
Mani and Mark T. Maybury, editors, Proceedings of
the ACL/EACL-97 Workshop on Intelligent Scalable
Text Summarization.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, (1):1?34.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 865?874, Uppsala, Sweden, July. Association
for Computational Linguistics.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The acl anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proc. of LREC.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In Proc. of ACL,
number 1.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a li-
brary for support vector machines, 2001. Software
available at http://www.csie.ntu.edu.tw/
cjlin/libsvm.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
I.G. Councill, C.L. Giles, and M.Y. Kan. 2008. Parscit:
An open-source crf reference string parsing package.
In Proc. of LREC, volume 2008. Citeseer.
Y. EL-Manzalawy and V. Honavar, 2005. WLSVM:
Integrating LibSVM into Weka Environment. Soft-
ware available at http://www.cs.iastate.
edu/?yasser/wlsvm.
C. Engstro?m. 2004. Topic dependence in sentiment clas-
sification. Unpublished MPhil Dissertation. Univer-
sity of Cambridge.
M. Gamon and A. Aue. 2005. Automatic identifica-
tion of sentiment vocabulary: exploiting low associa-
tion with known sentiment terms. In Proc. of the ACL,
pages 57?64.
M. Garzone and R. Mercer. 2000. Towards an automated
citation classifier. Advances in Artificial Intelligence.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Studying
the history of ideas using topic models. In EMNLP,
pages 363?371.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proc. of
ACL, page 181.
M.J. Hornsey, E. Robson, J. Smith, S. Esposo, and R.M.
Sutton. 2008. Sugaring the pill: Assessing rhetori-
cal strategies designed to minimize defensive reactions
to group criticism. Human Communication Research,
34(1):70?98.
Ken Hyland. 2000. Disciplinary Discourses; Social In-
teraction in Academic Writing. Longman, Harlow.
D. Kaplan, R. Iida, and T. Tokunaga. 2009. Automatic
extraction of citation contexts for research paper sum-
marization: A coreference-chain based approach. In
Proc. of the 2009 Workshop on Text and Citation Anal-
ysis for Scholarly Digital Libraries.
D. Kim, P. Webber, et al 2006. Implicit references to
citations: A study of astronomy papers.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-pass
sieve coreference resolution system at the conll-2011
shared task. ACL HLT 2011.
D.D. Lewis. 1991. Evaluating text categorization. In
Proc. of Speech and Natural Language Workshop,
pages 312?318.
M.H. MacRoberts and B.R. MacRoberts. 1984. The
negational reference: Or the art of dissembling. So-
cial Studies of Science, 14(1):91?94.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text organ-
isation. ISI/RS-87-190. Technical report, Information
Sciences Institute, University of Southern California,
Marina del Rey, CA.
H. Nanba and M. Okumura. 1999. Towards multi-paper
summarization using reference information. In IJCAI,
volume 16, pages 926?931. Citeseer.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proc. of ACL,
pages 104?111.
25
J. O?Connor. 1982. Citing statements: Computer recog-
nition and use to improve retrieval. Information Pro-
cessing & Management, 18(3):125?131.
S.B. Pham and A. Hoffmann. 2004. Extracting positive
attributions from scientific papers. In Discovery Sci-
ence, pages 39?45. Springer.
S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-
Naught. 2007. Mining opinion polarity relations of ci-
tations. In International Workshop on Computational
Semantics (IWCS). Citeseer.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: Theory
and applications, pages 1?10.
V. Qazvinian and D.R. Radev. 2008. Scientific paper
summarization using citation summary networks. In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 689?696.
Association for Computational Linguistics.
V. Qazvinian and D.R. Radev. 2010. Identifying non-
explicit citing sentences for citation-based summariza-
tion. In Proc. of ACL.
D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis of
the field of Computational Linguistics. Journal of the
American Soc. for Info. Sci. and Tech.
A. Ritchie, S. Robertson, and S. Teufel. 2008. Com-
paring citation contexts for information retrieval. In
Proc. of ACM conference on Information and knowl-
edge management, pages 213?222. ACM.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Comp. Ling., 27(4):521?544.
I. Spiegel-Rosing. 1977. Science studies: Bibliometric
and content analysis. Social Studies of Science.
K. Sugiyama, T. Kumar, M.Y. Kan, and R.C. Tripathi.
2010. Identifying citing sentences in research papers
using supervised learning. In Information Retrieval &
Knowledge Management,(CAMP), 2010 International
Conference on, pages 67?72. IEEE.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proc. of
EMNLP, pages 103?110.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Citation Indexing and Summa-
rization. Stanford: CSLI Publications.
P.D. Turney. 2002. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of ACL.
W.J. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC bioinfor-
matics, 7(1):356.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proc.
of EMNLP, page 136.
26
