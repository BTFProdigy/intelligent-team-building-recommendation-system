Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 64?69,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hedge Detection Using the RelHunter Approach?
Eraldo R. Fernandes? and Carlos E. M. Crestana? and Ruy L. Milidiu??
Departamento de Informa?tica, PUC-Rio
Rio de Janeiro, Brazil
{efernandes, ccrestana, milidiu}@inf.puc-rio.br
Abstract
RelHunter is a Machine Learning based
method for the extraction of structured in-
formation from text. Here, we apply Rel-
Hunter to the Hedge Detection task, pro-
posed as the CoNLL-2010 Shared Task1.
RelHunter?s key design idea is to model
the target structures as a relation over enti-
ties. The method decomposes the original
task into three subtasks: (i) Entity Iden-
tification; (ii) Candidate Relation Gener-
ation; and (iii) Relation Recognition. In
the Hedge Detection task, we define three
types of entities: cue chunk, start scope
token and end scope token. Hence, the
Entity Identification subtask is further de-
composed into three token classification
subtasks, one for each entity type. In
the Candidate Relation Generation sub-
task, we apply a simple procedure to gen-
erate a ternary candidate relation. Each in-
stance in this relation represents a hedge
candidate composed by a cue chunk, a
start scope token and an end scope to-
ken. For the Relation Recognition sub-
task, we use a binary classifier to discrim-
inate between true and false candidates.
The four classifiers are trained with the
Entropy Guided Transformation Learning
algorithm. When compared to the other
hedge detection systems of the CoNLL
shared task, our scheme shows a competi-
tive performance. The F -score of our sys-
tem is 54.05 on the evaluation corpus.
? This work is partially funded by CNPq and FAPERJ
grants 557.128/2009-9 and E-26/170028/2008.
? Holds a CNPq doctoral fellowship and has financial
support from IFG, Brazil.
?Holds a CAPES doctoral fellowship.
?Holds a CNPq research fellowship.
1Closed Task 2: detection of hedge cues and their scopes.
1 Introduction
Hedges are linguistic devices that indicate un-
certain or unreliable information within a text.
The detection of hedge structures is important for
many applications that extract facts from textual
data. The CoNLL-2010 Shared Task (Farkas et
al., 2010) is dedicated to hedge detection.
A hedge structure consists of a cue and a scope.
In Figure 1, we present a sentence with two hedge
instances. The hedge cues are highlighted and
their scopes are delimited by brackets. The hedge
cue comprises one or more keywords that indi-
cate uncertainty. The hedge scope is the uncertain
statement which is hedged by the cue. The scope
always includes the corresponding cue.
[ They indicate that [ the demonstration
is possible in this context ] and there is a
correlation ]
Figure 1: Sentence with two hedge instances.
Over the last two decades, several Computa-
tional Linguistic problems have been successfully
modeled as local token classification tasks (Brill,
1995; Milidiu? et al, 2009). Nevertheless, the
harder problems consist in identifying complex
structures within a text. These structures comprise
many tokens and show non local token dependen-
cies.
Phrase chunking (Sang and Buchholz, 2000) is
a task that involves structure recognition. Pun-
yakanok and Roth decompose this task into
four subtasks, that are sequentially solved (Pun-
yakanok and Roth, 2001). They use Hidden
Markov Models for the first three subtasks. They
find out that task decomposition improves the
overall token classification modeling.
Clause identification (Sang and De?jean, 2001)
is another task that requires structure recognition.
As clauses may embed other clauses, these struc-
64
tures involve stronger dependencies than phrase
chunks. Carreras et al propose an approach that
extends Punyakanok and Roth?s previous work
(Carreras et al, 2002). Their system comprises
complex methods for training and extraction, in
order to exploit the specific dependency aspects of
clause structures.
Phrase Recognition is a general type of task that
includes both phrase chunking and clause iden-
tification. Carreras et al propose the Filtering-
Ranking Perceptron (FRP) system for this general
task (Carreras et al, 2005). The FRP task model-
ing is strongly related to previous proposals (Pun-
yakanok and Roth, 2001; Carreras et al, 2002).
However, it simultaneously learns to solve three
subtasks. FRP is very effective, although compu-
tationally expensive at both training and prediction
time. Currently, FRP provides the best performing
clause identification system.
In Morante and Daelemans (2009), the hedge
detection task is solved as two consecutive classi-
fication tasks. The first one consists of classify-
ing the tokens of a sentence as hedge cues using
the IOB tagging style. The second task consists of
classifying tokens of a sentence as being the start
of a hedge scope, the end of one, or neither. The
result of those two tasks is combined using a set of
six rules to solve the hedge detection task.
Here, we describe RelHunter, a new method
for the extraction of structured information from
text. Additionally, we apply it to the Hedge Detec-
tion task. RelHunter extends the modeling strat-
egy used both in Carreras et al (2005) and Pun-
yakanok et al (2001). Other applications of this
method are presented in Fernandes at al. (2009b;
2010).
The remainder of this text is organized as fol-
lows. In Section 2, we present an overview of the
RelHunter method. The modeling approach for
the Hedge Detection task is presented in Sections
3 and 4. The experimental findings are depicted
and discussed in Section 5. Finally, in Section 6,
we present our final remarks.
2 RelHunter Overview
The central idea of RelHunter is to model the tar-
get structures as a relation over entities. To learn
how to extract this relation from text, RelHunter
uses two additional schemes: task decomposition
and interdependent classification.
We decompose the original task into three sub-
tasks: (i) Entity Identification; (ii) Candidate Re-
lation Generation; and (iii) Relation Recognition.
In Figure 2, we illustrate the application of Rel-
Hunter to hedge detection. We use the sentence
introduced by Figure 1.
Entity Identification is a local subtask, in which
simple entities are detected without any concern
about the structures they belong to. The outcome
of this subtask is the entity set. For instance, for
hedge detection, we identify three types of enti-
ties: hedge cues, tokens that start a scope and to-
kens that end a scope.
The second subtask is performed by a simple
procedure that generates the candidate relation
over the entity set. This relation includes true and
false candidates. This procedure considers do-
main specific knowledge to avoid the generation
of all possible candidates. In the hedge detection
task, we define the candidate relation as the set
of entity triples that comprise a hedge cue, a start
scope token and an end scope token, such that the
start token does not occur after the end token and
the hedge cue occurs between the start and the end
tokens.
The Relation Recognition subtask is a binary
classification problem. In this subtask, we dis-
criminate between true and false candidates. The
output relation produced in this subtask contains
the identified hedge instances.
3 Hedge Detection using RelHunter
In this section, we detail the RelHunter method
and describe its application to hedge detection.
3.1 Entity Identification
We consider three specific entity types: cue chunk,
start scope token, and end scope token. We divide
entity identification into three token classification
tasks, one for each entity type. Thus, we use the
original corpus to train three classifiers.
The cue chunk subtask is approached as a to-
ken classification problem by using the IOB tag-
ging style. The token tag is defined as follows: I,
when it is inside a hedge cue; O, when it is outside
a hedge cue; and B, when it begins a hedge cue
immediately after a distinct cue. As the baseline
classifier, we use the Cue Dictionary proposed in
Morante and Daelemans (2009), classifying each
occurrence of those words as a cue.
The start scope and end scope subtasks are
modeled as binary token classification problems.
65
Figure 2: Diagram of the RelHunter method.
As the baseline classifier for the start scope sub-
task, we assign the first token of each hedge cue as
the start of a scope.
We have two baseline classifiers for the end
scope subtask: END and END-X. The END sys-
tem classifies as an end token the second to the
last token of each sentence that contains a cue.
Due to the frequent occurrence of parenthesized
clauses at the end of sentences in full articles, the
END-X system extends the END system with an
additional operation. It reassigns an end scope tag,
from a close parentheses token, to the token before
its corresponding open parentheses.
3.2 Candidate Relation Generation
We define as the candidate hedge relation the set
of entity triples that comprise a hedge cue, a start
scope token and an end scope token, such that the
start token does not occur after the end token and
the hedge cue occurs between the start and the end
tokens.
3.3 Relation Recognition
We train a binary classifier to discriminate be-
tween positive and negative candidates within the
candidate relation. This classifier is trained on the
relation dataset, which is built by a general pro-
cedure. This dataset contains an entry for each
candidate. For each candidate, we generate two
feature sets: local features and global features.
The local features include local information
about each candidate entity, namely: cue chunk,
start scope token and end scope token. These fea-
tures are retrieved from the original corpus. For
the start and end tokens, we use all their features in
the original corpus. For the cue chunk, we use the
features of the rightmost token within the chunk.
The global features follow Carreras et al
(2002). These features are generated by consid-
ering the whole sentence where the candidate lies
in. They inform about the occurrence of relevant
elements within sentence fragments. We consider
as relevant elements the three entity types and ver-
bal chunks.
For each candidate entity, we consider three
fragments. The first one contains all the tokens be-
fore the entity. The second, all the entity tokens,
and the third all the tokens after the entity. Simi-
larly, for the whole candidate, we have three more
fragments: one containing all the tokens before the
candidate, another containing all the candidate to-
kens, and the third one containing all the tokens
after the candidate. Thus, there are 12 fragments
for each candidate, three for each entity plus three
for the whole candidate.
For each relevant element and fragment, we
generate two global features in the relation dataset:
a flag indicating the occurrence of the element
within the fragment and a counter showing its fre-
quency.
The relation dataset has km local features and
6r(k + 1) global features, where k is the relation
cardinality (number of entities), m is the number
of features in the original corpus, and r is the num-
ber of relevant elements.
Our current RelHunter implementation uses the
Entropy Guided Transformation Learning (ETL)
as its learning engine (Milidiu? et al, 2008; dos
Santos and Milidiu?, 2009). For instance, we train
four ETL based classifiers: one for each Entity
Identification subtask and one for the Relation
Recognition subtask. In the next section, we de-
scribe an important issue explored by the ETL al-
gorithm.
66
4 Interdependent Classification
The input to the Relation Recognition subtask is
the candidate relation, i.e., a set of hedge candi-
dates. The corresponding classifier must discrim-
inate positive from negative candidates. However,
identifying one candidate as positive implies that
some other candidates must be negatives. This in-
volves a special modeling issue: interdependent
classification. The learning engine may explore
these dependencies, when building the classifier
for this subtask.
Interdependent classification is usually assumed
for neighboring examples. When the learning
model adopts a Markovian Property, then the
neighborhood is given by a context window. This
is the case for Markovian Fields such as Hidden
Markov Models. Another model that also explores
interdependent examples is ETL.
ETL is a very attractive modeling tool and has
been applied to several classification tasks (Mi-
lidiu? et al, 2008; dos Santos and Milidiu?, 2009;
Fernandes et al, 2009a; Fernandes et al, 2010).
ETL uses an annotated corpus, where the corre-
sponding class is attached to each example. The
corpus is partitioned into segments. Each segment
is a sequence of examples. Examples within the
same segment are considered dependent. Con-
versely, examples within different segments are
considered independent. Moreover, an example
classification depends only on the features of the
examples from its corresponding context window.
Hence, to apply ETL we need to provide three
modeling ingredients: segment definition, exam-
ple ordering within a segment and the context win-
dow size. Given that, classification dependencies
are explored by the ETL classifier. Hence, Rel-
Hunter uses ETL as its learning engine.
We include in the same segment the hedge can-
didates that have the same cue and start scope to-
kens. Within a segment, we order the candidates
by the order of the end token in the original cor-
pus. We use a context window of 7 candidates,
i.e., three candidates before the current, the current
candidate and three candidates after the current.
5 Experimental Results
We use the corpus provided in the CoNLL-2010
Shared Task to train and evaluate our hedge de-
tection system. We add the following annota-
tion to the corpus: word stems, part-of-speech
tags, phrase chunks, and clause annotations. Word
stems have been generated by the Porter stemmer
(Porter, 1980). The additional annotation has been
generated by ETL based systems (dos Santos and
Milidiu?, 2009; Fernandes et al, 2009b; Milidiu? et
al., 2008).
The CoNLL corpus is based on the BioScope
corpus (Vincze et al, 2008). Since it contains doc-
uments of two different kinds ? paper abstracts and
full papers ? we split it into two subcorpora. The
first subcorpus is called ABST and contains all the
paper abstracts. The second is called FULL and
contains all the full papers.
We have two experimental setups: Development
and Evaluation. In the Development Setup, we use
ABST as the training corpus and FULL as the de-
velopment corpus. This is a conservative decision
since the CoNLL Evaluation Corpus is comprised
only of full articles. In the Evaluation Setup, we
use the union of ABST and FULL as the train-
ing corpus and report the performance over the
CoNLL Evaluation Corpus.
5.1 Development
Here, we report the development setup experimen-
tal findings. In Table 1, we show the performance
of the three baseline classifiers. The start and end
classifiers are evaluated with golden standard cue
chunks. All results are obtained with the END-X
baseline system, except when explicitly stated.
Task Precision Recall F-score
Cue 51.96 51.65 51.80
Start scope 72.01 72.22 72.11
End scope 65.90 58.97 62.24
Table 1: Development performance of the three
Baseline Classifiers.
In Table 2, we report the performance of the
three entity identification ETL classifiers. Again,
the start and end classifiers are evaluated with
golden standard cue chunks. These results indi-
cate that the end scope subtask is the hardest one.
Indeed, our ETL classifier is not able to improve
the baseline classifier performance. The last ta-
ble line shows the performance of the RelHunter
method on the target task ? hedge detection.
5.2 Evaluation
Here, we report the evaluation setup findings. In
Table 3, we show the performance of the three
67
Task Precision Recall F-score
Cue 81.23 73.20 77.01
Start scope 91.81 72.37 80.94
End scope 65.90 58.97 62.24
Hedge 53.49 34.43 41.89
Table 2: Development performance of the three
entity identification ETL classifiers and the Rel-
Hunter method to hedge detection.
baseline classifiers. The start and end classifiers
are evaluated with golden standard cue chunks.
Task Precision Recall F-score
Cue 45.12 60.02 51.52
Start scope 75.51 75.73 75.62
End scope 81.01 72.56 76.55
Table 3: Evaluation performance of the three
Baseline Classifiers.
In Table 4, we report the performance of the
three entity identification ETL classifiers. Again,
the start and end classifiers are evaluated with
golden standard cue chunks. The last table line
shows the performance of the RelHunter method
on the target task ? hedge detection.
Task Precision Recall F-score
Cue 78.73 77.05 77.88
Start scope 89.21 77.86 83.15
End scope 81.01 72.56 76.55
Hedge 57.84 50.73 54.05
Table 4: Evaluation performance of the three
entity identification ETL classifiers and the Rel-
Hunter method to hedge detection.
In Table 5, we report the Hedge Detection per-
formances when using END and END-X, as the
baseline classifier for the end scope subtask. The
use of END-X improves the overall system F -
score by more than ten twelve.
In Table 6, we report the Final Results of the
CoNLL-2010 Shared Task ? Closed Task 2. For
the sake of comparison, we also include the per-
formance of the RelHunter system with END-X,
that has been developed and tested after the com-
End scope Precision Recall F-score
END 45.96 38.04 41.63
END-X 57.84 50.73 54.05
Table 5: Evaluation performance of the RelHunter
system when using END and END-X.
petition end. The version with the END baseline
holds rank 7 at the competition.
Official System P R FRank
1 Morante 59.62 55.18 57.32
2 Rei 56.74 54.60 55.65
3 Velldal 56.71 54.02 55.33
- RelHunter 57.84 50.73 54.05
4 Li 57.42 47.92 52.24
5 Zhou 45.32 43.56 44.42
6 Zhang 45.94 42.69 44.25
7 Fernandes 45.96 38.04 41.63
8 Vlachos 41.18 35.91 38.37
9 Zhao 34.78 41.05 37.66
10 Tang 34.49 31.85 33.12
11 Ji 21.87 17.23 19.27
12 Ta?ckstro?m 02.27 02.03 02.15
Table 6: Evaluation performance of the CoNLL-
2010 systems and the RelHunter method with the
END-X end scope classifier.
6 Conclusion
We propose RelHunter, a new machine learning
based method for the extraction of structured in-
formation from text. RelHunter consists in model-
ing the target structures as a relation over entities.
To learn how to extract this relation from text, Rel-
Hunter uses two main schemes: task decomposi-
tion and interdependent classification.
RelHunter decomposes the identification of en-
tities into several but simple token classification
subtasks. Additionally, the method generates a
candidate relation over the identified entities and
discriminates between true and false candidates
within this relation.
RelHunter uses the Entropy Guided Transfor-
mation Learning algorithm as its learning engine.
As Hidden Markov Models, ETL is able to con-
sider interdependent examples. RelHunter ex-
68
ploits this powerful feature in order to tackle de-
pendencies among the hedge candidates.
RelHunter is easily applied to many complex
Computational Linguistic problems. We show its
effectiveness by applying it to hedge detection.
Other successful applications of this method are
presented in Fernandes et al (2009b; 2010).
RelHunter explores the dependency among lin-
guistic structures by using a powerful feature of
the ETL algorithm. Nevertheless, this feature
is restricted to sequentially organized examples,
since ETL has been initially proposed for token
classification problems. Linguistic structures in-
volve topologies that are frequently more complex
than that. The ETL algorithm may be extended to
consider more complex topologies. We conjecture
that it is possible to consider quite general topolo-
gies. This would contribute to the construction of
better solutions to many Computational Linguistic
tasks.
Acknowledgments
The authors thank Evelin Amorim and Eduardo
Motta for coding dataset normalization procedures
that are very handy for Hedge Detection.
References
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Xavier Carreras, Llu??s Ma`rquez, Vasin Punyakanok,
and Dan Roth. 2002. Learning and inference for
clause identification. In Proceedings of the Thir-
teenth European Conference on Machine Learning,
pages 35?47.
Xavier Carreras, Llu??s Ma`rquez, and Jorge Castro.
2005. Filtering-ranking perceptron learning for par-
tial parsing. Machine Learning, 60(1?3):41?71.
C??cero N. dos Santos and Ruy L. Milidiu?, 2009. Foun-
dations of Computational Intelligence, Volume 1:
Learning and Approximation, volume 201 of Stud-
ies in Computational Intelligence, chapter Entropy
Guided Transformation Learning, pages 159?184.
Springer.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Eraldo R. Fernandes, C??cero N. dos Santos, and Ruy L.
Milidiu?. 2009a. Portuguese language processing
service. In Proceedings of the Web in Ibero-America
Alternate Track of the 18th World Wide Web Confer-
ence (WWW?2009), Madrid.
Eraldo R. Fernandes, Bernardo A. Pires, C??cero N. dos
Santos, and Ruy L. Milidiu?. 2009b. Clause identifi-
cation using entropy guided transformation learning.
In Proceedings of the 7th Brazilian Symposium in In-
formation and Human Language Technology (STIL),
Sa?o Carlos, Brazil.
Eraldo R. Fernandes, Bernardo A. Pires, C??cero N.
dos Santos, and Ruy L. Milidiu?. 2010. A ma-
chine learning approach to Portuguese clause iden-
tification. In Proceedings of the Nineth Interna-
tional Conference on Computational Processing of
the Portuguese Language (PROPOR), volume 6001
of Lecture Notes in Artificial Intelligence, pages 55?
64, Porto Alegre, Brazil. Springer.
Ruy L. Milidiu?, C??cero N. dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy
guided transformation learning. In Proceedings of
ACL-08: HLT, pages 647?655, Columbus, USA.
Association for Computational Linguistics.
Ruy L. Milidiu?, C??cero N. dos Santos, and Carlos
E. M. Crestana. 2009. A token classification ap-
proach to dependency parsing. In Proceedings of
the 7th Brazilian Symposium in Information and Hu-
man Language Technology (STIL?2009), Sa?o Carlos,
Brazil.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, USA, June. Association for Com-
putational Linguistics.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130?137.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Proceedings of
the Conference on Advances in Neural Information
Processing Systems (NIPS), pages 995?1001. MIT
Press.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of CoNLL-2000 and
LLL-2000, Lisbon, Portugal.
Erik F. T. K. Sang and Herve? De?jean. 2001. Introduc-
tion to the CoNLL-2001 shared task: Clause identifi-
cation. In Proceedings of Fifth Conference on Com-
putational Natural Language Learning, Toulouse,
France.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 (Suppl 11):S9.
69
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 41?48,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Latent Structure Perceptron with Feature Induction
for Unrestricted Coreference Resolution
Eraldo Rezende Fernandes
Departamento de Informa?tica
PUC-Rio
Rio de Janeiro, Brazil
efernandes@inf.puc-rio.br
C??cero Nogueira dos Santos
Brazilian Research Lab
IBM Research
Rio de Janeiro, Brazil
cicerons@br.ibm.com
Ruy Luiz Milidiu?
Departamento de Informa?tica
PUC-Rio
Rio de Janeiro, Brazil
milidiu@inf.puc-rio.br
Abstract
We describe a machine learning system based
on large margin structure perceptron for unre-
stricted coreference resolution that introduces
two key modeling techniques: latent corefer-
ence trees and entropy guided feature induc-
tion. The proposed latent tree modeling turns
the learning problem computationally feasi-
ble. Additionally, using an automatic feature
induction method, we are able to efficiently
build nonlinear models and, hence, achieve
high performances with a linear learning algo-
rithm. Our system is evaluated on the CoNLL-
2012 Shared Task closed track, which com-
prises three languages: Arabic, Chinese and
English. We apply the same system to all lan-
guages, except for minor adaptations on some
language dependent features, like static lists
of pronouns. Our system achieves an offi-
cial score of 58.69, the best one among all the
competitors.
1 Introduction
The CoNLL-2012 Shared Task (Pradhan et al,
2012) is dedicated to the modeling of coreference
resolution for multiple languages. The participants
are provided with corpora for three languages: Ara-
bic, Chinese and English. These corpora are pro-
vided by the OntoNotes project and, besides accu-
rate anaphoric coreference information, contain var-
ious annotation layers such as part-of-speech (POS)
tagging, syntax parsing, named entities (NE) and se-
mantic role labeling (SRL). The shared task consists
in the automatic identification of coreferring men-
tions of entities and events, given predicted infor-
mation on other OntoNotes layers.
We propose a machine learning system for coref-
erence resolution that is based on the large margin
structure perceptron algorithm (Collins, 2002; Fer-
nandes and Milidiu?, 2012). Our system learns a pre-
dictor that takes as input a set of candidate men-
tions in a document and directly outputs the clus-
ters of coreferring mentions. This predictor com-
prises an optimization problem whose objective is a
function of the clustering features. To embed clas-
sic cluster metrics in this objective function is prac-
tically infeasible since most of such metrics lead to
NP-hard optimization problems. Thus, we introduce
coreference trees in order to represent a cluster by
a directed tree over its mentions. In that way, the
prediction problem optimizes over trees instead of
clusters, which makes our approach computationally
feasible. Since coreference trees are not given in the
training data, we assume that these structures are la-
tent and use the latent structure perceptron (Fernan-
des and Brefeld, 2011; Yu and Joachims, 2009) as
the learning algorithm.
To provide high predicting power features to
our model, we use entropy guided feature induc-
tion (Fernandes and Milidiu?, 2012). By using this
technique, we automatically generate several fea-
ture templates that capture coreference specific lo-
cal context knowledge. Furthermore, this feature in-
duction technique extends the structure perceptron
framework by providing an efficient general method
to build strong nonlinear classifiers.
Our system is evaluated on the CoNLL-2012
Shared Task closed track and achieves the scores
41
54.22, 58.49 and 63.37 on Arabic, Chinese and En-
glish test sets, respectively. The official score ? the
mean over the three languages ? is 58.69, which is
the best score achieved in the shared task.
The remainder of this paper is organized as fol-
lows. In Section 2, we present our machine learning
modeling for the unrestricted coreference resolution
task. In Section 3, we present the corpus preprocess-
ing steps. The experimental findings are depicted in
Section 4 and, in Section 5, we present our final re-
marks.
2 Task Modeling
Coreference resolution consists in identifying men-
tion clusters in a document. We split this task into
two subtasks: mention detection and mention clus-
tering. For the first subtask, we apply the strategy
proposed in (dos Santos and Carvalho, 2011). The
second subtask requires a complex output. Hence,
we use a structure learning approach that has been
successfully applied to many similar structure find-
ing NLP tasks (Collins, 2002; Tsochantaridis et
al., 2005; McDonald et al, 2006; Fernandes and
Brefeld, 2011; Fernandes and Milidiu?, 2012).
2.1 Mention Detection
For each text document, we generate a list of candi-
date mentions using the strategy of (dos Santos and
Carvalho, 2011). The basic idea is to use all noun
phrases, and, additionally, pronouns and named en-
tities, even if they are inside larger noun phrases. We
do not include verbs as mentions.
2.2 Mention Clustering
In the mention clustering subtask, a training in-
stance (x,y) consists of a set of mentions x from
a document and the correct coreferring clusters y.
The structure perceptron algorithm learns a predic-
tor from a given training set D = {(x,y)} of cor-
rect input-output pairs. More specifically, it learns
the weight vector w of the parameterized predictor
given by
F (x) = arg max
y??Y(x)
s(y?;w),
where Y(x) is the set of clusterings over mentions
x and s is a w-parameterized scoring function over
clusterings.
We use the large margin structure perceptron
(Fernandes and Milidiu?, 2012) that, during training,
embeds a loss function in the prediction problem.
Hence, it uses a loss-augmented predictor given by
F `(x) = arg max
y??Y(x)
s(y?;w) + `(y,y?),
where ` is a non-negative loss function that mea-
sures how a candidate clustering y? differs from the
ground truth y. The training algorithm makes in-
tense use of the predictor, hence the prediction prob-
lem must be efficiently solved. Letting s be a classic
clustering metric is infeasible, since most of such
metrics lead to NP-hard optimization problems.
2.2.1 Coreference Trees
In order to reduce the complexity of the prediction
problem, we introduce coreference trees to represent
clusters of coreferring mentions. A coreference tree
is a directed tree whose nodes are the coreferring
mentions and arcs represent some coreference rela-
tion between mentions. In Figure 1, we present a
document with seven highlighted mentions compris-
ing two clusters. One plausible coreference tree for
the cluster {a1,a2,a3,a4} is presented in Figure 2.
North Koreaa1 opened itsa2 doors to the U.S. today,
welcoming Secretary of State Madeleine Albrightb1 .
Sheb2 says herb3 visit is a good start. The U.S. remains
concerned about North Korea?sa3 missile development
program and itsa4 exports of missiles to Iran.
Figure 1: Exemplary document with seven highlighted
mentions comprising two clusters: {a1,a2,a3,a4} and
{b1,b2,b3}. The letter in the mention subscript indicates
its cluster and the number uniquely identifies the mention
within the cluster.
[North Korea]a
[its]a [North Korea's]a
[its]a
1
2 3
4
Figure 2: Coreference tree for the cluster a in Figure 1.
We are not concerned about the semantics under-
lying coreference trees, since they are just auxiliary
42
structures for the clustering task. However, we ar-
gue that this concept is linguistically plausible, since
there is a dependency relation between coreferring
mentions. Observing the aforementioned example,
one may agree that mention a3 (North Korea?s) is
indeed more likely to be associated with mention a1
(North Korea) than with mention a2 (its), even con-
sidering that a2 is closer than a1 in the text.
For a given document, we have a forest of coref-
erence trees, one tree for each coreferring cluster.
However, for the sake of simplicity, we link the root
node of every coreference tree to an artificial root
node, obtaining the document tree. In Figure 3, we
depict a document tree for the text in Figure 1.
Figure 3: Document tree with two coreference trees for
the text in Figure 1. Dashed lines indicate artificial arcs.
2.2.2 Latent Structure Learning
Coreference trees are not given in the training
data. Thus, we assume that these structures are la-
tent and make use of the latent structure perceptron
(Fernandes and Brefeld, 2011; Yu and Joachims,
2009) to train our models. We decompose the origi-
nal predictor into two predictors, that is
F (x) ? Fy(Fh(x)),
where the latent predictor Fh(x) is defined as
argmaxh?H(x)?w,?(x,h)?,H(x) is the set of fea-
sible document trees for x and ?(x,h) is the joint
feature vector representation of mentions x and doc-
ument tree h. Hence, the latent predictor finds a
maximum scoring rooted tree over the given men-
tions x, where a tree score is given by a linear func-
tion over its features. Fy(h) is a straightforward
procedure that creates a cluster for each subtree con-
nected to the artificial root node in the document tree
h.
In Figure 4, we depict the proposed latent struc-
ture perceptron algorithm for the mention cluster-
ing task. Like its univariate counterpart (Rosenblatt,
w0 ? 0
t? 0
while no convergence
for each (x,y) ? D
h?? argmaxh?H(x,y)?wt,?(x,h)?
h?? argmaxh?H(x)?wt,?(x,h)?+ `r(h, h?)
wt+1 ? wt + ?(x, h?)??(x, h?)
t? t+ 1
w ? 1t
?t
i=1 wi
Figure 4: Latent structure perceptron algorithm.
1957), the structure perceptron is an online algo-
rithm that iterates through the training set. For each
training instance, it performs two major steps: (i)
a prediction for the given input using the current
model; and (ii) a model update based on the dif-
ference between the predicted and the ground truth
outputs. The latent structure perceptron performs an
additional step to predict the latent ground truth h?
using a specialization of the latent predictor and the
current model. This algorithm learns to predict doc-
ument trees that help to solve the clustering task.
Thereafter, for an unseen document x, the predic-
tor Fh(x) and the learned model w are employed to
produce a predicted document tree h which, in turn,
is fed to Fy(h) to give the predicted clusters.
Golden coreference trees are not available. How-
ever, during training, for a given input x, we have
the golden clustering y. Thus, we predict the con-
strained document tree h? for the training instance
(x,y) using a specialization of the latent predictor
? the constrained latent predictor ? that makes use
of y. The constrained predictor finds the maximum
scoring document tree among all rooted trees of x
that follow the correct clustering y, that is, rooted
trees that only include arcs between mentions that
are coreferent according to y, plus one arc from the
artificial node to each cluster. In that way, the con-
strained predictor optimizes over a subset H(x,y)
contained in H(x) and, moreover, it guarantees that
43
Fy(h?) = y, for any w. The constrained tree is
used as the ground truth on each iteration. There-
fore, the model update is determined by the differ-
ence between the constrained document tree and the
document tree predicted by the ordinary predictor.
The loss function measures the impurity in the
predicted document tree. In our modeling, we use a
simple loss function that just counts how many pre-
dicted edges are not present in the constrained docu-
ment tree. For the arcs from the artificial root node,
we use a different loss value. We set that through the
parameter r, which we call the root loss value.
We decompose the joint feature vector ?(x,h)
along tree edges, that is, pairs of candidate corefer-
ring mentions. This approach is similar to previous
structure learning modelings for dependency pars-
ing (McDonald et al, 2005; Fernandes and Milidiu?,
2012). Thus, the prediction problem reduces to a
maximum branching problem, which is efficiently
solved by the Chu-Liu-Edmonds algorithm (Chu and
Liu, 1965; Edmonds, 1967). We also use the aver-
aged structure perceptron as suggested by (Collins,
2002), since it provides a more robust model.
3 Data Preparation
It is necessary to perform some corpus processing
steps in order to prepare training and test data. In
this section, we detail the methodology we use to
generate coreference arcs and the features that de-
scribe them.
3.1 Coreference Arcs Generation
The input for the prediction problem is a graph
whose nodes are the mentions in a document. Ide-
ally, we could consider the complete graph for each
document, thus every mention pair would be an op-
tion for building the document tree. However, since
the total number of mentions is huge and a big por-
tion of arcs can be easily identified as incorrect, we
filter the arcs and, thus, include only candidate men-
tion pairs that are more likely to be coreferent.
We filter arcs by simply adapting the sieves
method proposed in (Lee et al, 2011). However, in
our filtering strategy, precision is not a concern and
the application order of filters is not important. The
objective here is to build a small set of candidate arcs
that shows good recall.
Given a mention pair (mi,mj), where mi appears
before mj in the text, we create a directed arc from
mi to mj if at least one of the following conditions
holds: (1) the number of mentions between mi and
mj is not greater than a given parameter; (2) mj is
an alias of mi; (3) there is a match of both mentions
strings up to their head words; (4) the head word
of mi matches the head word of mj ; (5) test shallow
discourse attributes match for both mentions; (6) mj
is a pronoun and mi has the same gender, number,
speaker and animacy of mj ; (7) mj is a pronoun and
mi is a compatible pronoun or proper name.
Sieves 2 to 7 are obtained from (Lee et al, 2011).
We only introduce sieve 1 to lift recall without using
other strongly language-dependent sieves.
3.2 Basic Features
We use a set of 70 basic features to describe each
pair of mentions (mi, mj). The feature set includes
lexical, syntactic, semantic, and positional informa-
tion. Our feature set is very similar to the one used
by (dos Santos and Carvalho, 2011). However, here
we do not use the semantic features derived from
WordNet. In the following, we briefly describe some
of these basic features.
Lexical: head word of mi/j ; String matching of
(head word of) mi and mj (y/n); Both are pro-
nouns and their strings match (y/n); Previous/Next
two words of mi/j ; Length of mi/j ; Edit distance of
head words; mi/j is a definitive NP (y/n); mi/j is a
demonstrative NP (y/n); Both are proper names and
their strings match (y/n).
Syntactic: POS tag of the mi/j head word; Previ-
ous/Next two POS tags of mi/j ; mi and mj are both
pronouns / proper names (y/n); Previous/Next pred-
icate of mi/j ; Compatible pronouns, which checks
whether two pronouns agree in number, gender and
person (y/n); NP embedding level; Number of em-
bedded NPs in mi/j .
Semantic: the result of a baseline system; sense
of the mi/j head word; Named entity type of mi/j ;
mi and mj have the same named entity; Semantic
role of mi/j for the prev/next predicate; Concatena-
tion of semantic roles of mi and mj for the same
predicate (if they are in the same sentence); Same
speaker (y/n); mj is an alias of mi (y/n).
Distance and Position: Distance between mi and
mj in sentences; Distance in number of mentions;
44
Distance in number of person names (applies only
for the cases where mi and mj are both pronouns or
one of them is a person name); One mention is in
apposition to the other (y/n).
3.3 Language Specifics
Our system can be easily adapted to different lan-
guages. In our experiments, only small changes are
needed in order to train and apply the system to three
different languages. The adaptations are due to: lack
of input features for some languages; different POS
tagsets are used in the corpora; and creation of static
list of language specific pronouns.
Some input features, that are available for the En-
glish corpus, are not available in Arabic and Chinese
corpora. Namely, the Arabic corpus does not contain
NE, SRL and speaker features. Therefore, for this
language we do not derive basic features that make
use of these input features. For Chinese, we do not
use features derived from NE data, since this data is
not provided. Additionally, the Chinese corpus uses
a different POS tagset. Hence, some few mappings
are needed during the basic feature derivation stage.
The lack of input features for Arabic and Chinese
also impact the sieve-based arcs generation. For
Chinese, we do not use sieve 6, and, for Arabic, we
only use sieves 1, 3, 4 and 7. Sieve 7 is not used
for the English corpus, since it is a specialization of
sieve 6. The first sieve parameter is 4 for Arabic and
Chinese, and 8 for English.
In the arcs generation and basic feature derivation
steps, our system makes use of static lists of lan-
guage specific pronouns. In our experiments, we use
the POS tagging information and the golden coref-
erence chains to automatically extract these pronoun
lists from training corpora.
3.4 Entropy Guided Feature Induction
In order to improve the predictive power of our sys-
tem, we add complex features that are combinations
of the basic features described in the previous sec-
tion. We use feature templates to generate such com-
plex features. However, we automatically generate
templates using the entropy guided feature induction
approach (Fernandes and Milidiu?, 2012; Milidiu? et
al., 2008). These automatically generated templates
capture complex contextual information and are dif-
ficult to be handcrafted by humans. Furthermore,
this feature induction mechanism extends the struc-
ture perceptron framework by providing an efficient
general method to build strong nonlinear predictors.
We experiment with different template sets for
each language. The main difference between these
sets is basically the training data used to induce
them. We obtain better results when merging dif-
ferent template sets. For the English language, it is
better to use a template set of 196 templates, which
merges two different sets: (a) a set induced using
training data that contains mention pairs produced
by filters 2 to 6; and (b) another set induced using
training data that contains mention pairs produced
by all filters. For Chinese and Arabic, it is better to
use template sets induced specifically for these lan-
guages merged with the template set (a) generated
for the English language. The final set for the Chi-
nese language has 197 templates, while the final set
for Arabic has 223.
4 Empirical Results
We train our system on the corpora provided in the
CoNLL-2012 Shared Task. There are corpora avail-
able on three languages: Arabic, Chinese and En-
glish. For each language, results are reported using
three metrics: MUC, B3 and CEAFe. We also re-
port the mean of the F-scores on these three met-
rics, which gives a unique score for each language.
Additionally, the official score on the CoNLL-2012
shared task is reported, that is the mean of the scores
obtained on the three languages.
We report our system results on development and
test sets. The development results are obtained with
systems trained only on the training sets. However,
test set results are obtained by training on a larger
dataset ? the one obtained by concatenating train-
ing and development sets. During training, we use
the gold standard input features, which produce bet-
ter performing models than using the provided au-
tomatic values. That is usually the case on NLP
tasks, since golden values eliminate the additional
noise introduced by automatic features. On the other
hand, during evaluation, we use the automatic values
provided in the CoNLL shared task corpora.
In Table 1, we present our system performances
on the CoNLL-2012 development sets for the three
languages. Given the size of the Arabic training cor-
45
Language
MUC B3 CEAFe Mean
R P F1 R P F1 R P F1
Arabic 43.00 47.87 45.30 61.41 70.38 65.59 49.42 44.19 46.66 52.52
Chinese 54.40 68.19 60.52 64.17 78.84 70.76 51.42 38.96 44.33 58.54
English 64.88 74.74 69.46 66.53 78.28 71.93 54.93 43.68 48.66 63.35
Official Score 58.14
Table 1: Results on the development sets.
Language
MUC B3 CEAFe Mean
R P F1 R P F1 R P F1
Arabic 34.18 58.85 43.25 50.61 82.13 62.63 57.37 33.75 42.49 49.45
Chinese 49.17 76.03 59.72 58.16 86.33 69.50 57.56 34.38 43.05 57.42
English 62.75 77.41 69.31 63.88 81.34 71.56 57.46 41.08 47.91 62.92
Official Score 56.59
Table 2: Results on the development sets without root loss value.
Language
MUC B3 CEAFe Mean
R P F1 R P F1 R P F1
Arabic 43.63 49.69 46.46 62.70 72.19 67.11 52.49 46.09 49.08 54.22
Chinese 52.69 70.58 60.34 62.99 80.57 70.70 53.75 37.88 44.44 58.49
English 65.83 75.91 70.51 65.79 77.69 71.24 55.00 43.17 48.37 63.37
Official Score 58.69
Table 3: Official results on the test sets.
Language Parse / Mentions
MUC B3 CEAFe Mean
R P F1 R P F1 R P F1
Arabic
Auto / GB 45.18 47.39 46.26 64.56 69.44 66.91 49.73 47.39 48.53 53.90
Auto / GM 57.25 76.48 65.48 60.27 79.81 68.68 72.61 46.00 56.32 63.49
Golden / Auto 46.38 51.78 48.93 63.53 72.37 67.66 52.57 46.88 49.56 55.38
Golden / GB 46.38 51.78 48.93 63.53 72.37 67.66 52.57 46.88 49.56 55.38
Golden / GM 56.89 76.27 65.17 60.07 80.02 68.62 72.24 45.58 55.90 63.23
Chinese
Auto / GB 58.76 71.46 64.49 66.62 79.88 72.65 54.09 42.02 47.29 61.48
Auto / GM 61.64 90.81 73.43 63.55 89.43 74.30 72.78 39.68 51.36 66.36
Golden / Auto 59.35 74.49 66.07 66.31 81.43 73.10 55.97 41.50 47.66 62.28
Golden / GB 59.35 74.49 66.07 66.31 81.43 73.10 55.97 41.50 47.66 62.28
Golden / GM 61.70 91.45 73.69 63.57 89.76 74.43 72.84 39.49 51.21 66.44
English
Auto / GB 64.92 77.53 70.67 64.25 78.95 70.85 56.48 41.69 47.97 63.16
Auto / GM 70.69 91.21 79.65 65.46 85.61 74.19 74.71 42.55 54.22 69.35
Golden / Auto 67.73 77.25 72.18 66.42 78.01 71.75 56.16 44.51 49.66 64.53
Golden / GB 65.65 78.26 71.40 64.36 79.09 70.97 57.36 42.23 48.65 63.67
Golden / GM 71.18 91.24 79.97 65.81 85.51 74.38 74.93 43.09 54.72 69.69
Table 4: Supplementary results on the test sets alternating parse quality and mention candidates. Parse quality can be
automatic or golden; and mention candidates can be automatically identified (Auto), golden mention boundaries (GB)
or golden mentions (GM).
pus and the feature limitations for Arabic and Chi-
nese, the performance variations among the three
languages are no more than expected. One impor-
tant parameter that we introduce in this work is the
root loss value, a different loss function value on arcs
from the artificial root node. The effect of this pa-
rameter is to diminish the creation of clusters, thus
stimulating bigger clusters and adjusting the balance
between precision and recall. Using the develop-
ment sets for tuning, we set the value of the root loss
value parameter to 6, 2 and 1.5 for Arabic, Chinese
and English, respectively. In Table 2, we present our
system performances on the development sets when
we set this parameter to 1 for all languages, that is
46
equivalent to not use this parameter at all. We can
observe, by comparing these results with the ones
in Table 1, that this parameter really causes a better
balancing between precision and recall, and conse-
quently increases the F1 scores. Its effect is accen-
tuated on Arabic and Chinese, since the unbalancing
issue is worse on these languages.
The official results on the test sets are depicted
in Table 3. For Chinese and English, these perfor-
mances are virtually identical to the performances
on the development sets. On the other hand, the offi-
cial performance for the Arabic language is signifi-
cantly higher than the development set performance.
This difference is likely due to the fact that the Ara-
bic training set is much smaller than the Chinese and
English counterparts. Thus, by including the devel-
opment set in the training of the final Arabic system,
we significantly improve the official performance.
We report in Table 4 the supplementary results
provided by the shared task organizers on the test
sets. These additional experiments investigate two
key aspects of any coreference resolution system:
the parse feature and the mention candidates that
are given to the clustering procedure. We alter-
nate the parse feature between the official automatic
parse and the golden parse from OntoNotes. Re-
garding mention candidates, we use three differ-
ent strategies: automatic mentions (Auto, in Ta-
ble 4), golden mention boundaries (GB) and golden
mentions (GM). Automatic mentions are completely
detected by our system, as described in Section
2.1. Golden mention boundaries comprise all noun
phrases in the golden parse tree, even when the au-
tomatic parse is used as input feature. Golden men-
tions are all non-singleton mentions, i.e., all men-
tions that take part in some entity cluster. It is im-
portant to notice that golden mention information is
much stronger than golden boundaries.
By observing Table 4, it is clear that the most ben-
eficial information is golden mentions (compare the
Auto/GM results in Table 4 with the results in Table
3). The mean F-score over all languages when us-
ing golden mentions is almost 8 points higher than
the official score. These results are not surprising
since to identify non-singleton mentions greatly re-
duces the final task complexity. Golden mention
boundaries (Auto/GB) increase the mean F-score for
Chinese by almost 3 points. Conversely, for the
other two languages, the results are decreased when
this information is given. This is probably due to
parameter tuning, since any additional information
potentially changes the learning problem and, nev-
ertheless, we use exactly the same three models ?
one per language ? to produce all the results on Ta-
bles 3 and 4. One can observe, for instance, that
the recall/precision balance greatly varies among the
different configurations in these experiments. The
golden parse feature (Golden/Auto) causes big im-
provements on the mean F-scores for all languages,
specially for Chinese.
5 Conclusion
In this paper, we describe a machine learning system
based on large margin latent structure perceptron for
unrestricted coreference resolution. We introduce
two modeling approaches that have direct impact
on the final system performance: latent coreference
trees and entropy guided feature induction.
According to our experiments, latent coreference
trees are powerful enough to model the complex-
ity of coreference structures in a document, while
turning the learning problem computationally feasi-
ble. Our empirical findings also show that entropy
guided feature induction enables learning of effec-
tive nonlinear classifiers.
Our system is evaluated on the CoNLL-2012
Shared Task closed track, which consists on model-
ing coreference resolution for three languages: Ara-
bic, Chinese and English. In order to cope with this
multi-language task, our system needs only minor
adaptations on some language dependent features.
As future work, we plan to include second order
features and cluster sensitive features.
Acknowledgments
This work was partially funded by Conselho Na-
cional de Desenvolvimento Cient??fico e Tecnolo?gico
(CNPq), Fundac?a?o de Amparo a` Pesquisa do Es-
tado do Rio de Janeiro and Fundac?a?o Cearense de
Apoio ao Desenvolvimento Cient??fico e Tecnolo?gico
through grants 557.128/2009-9, E-26/170028/2008
and 0011-00147.01.00/09, respectively. The first au-
thor was also supported by a CNPq doctoral fel-
lowship and by the Instituto Federal de Educac?a?o,
Cie?ncia e Tecnologia de Goia?s.
47
References
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1?8.
Cicero Nogueira dos Santos and Davi Lopes Carvalho.
2011. Rule and tree ensembles for unrestricted
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 51?55, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
Eraldo R. Fernandes and Ulf Brefeld. 2011. Learning
from partially annotated sequences. In Proceedings of
the European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in
Databases (ECML-PKDD), Athens, Greece.
Eraldo R. Fernandes and Ruy L. Milidiu?. 2012. Entropy-
guided feature generation for structured learning of
Portuguese dependency parsing. In Proceedings of
the Conference on Computational Processing of the
Portuguese Language (PROPOR), volume 7243 of
Lecture Notes in Computer Science, pages 146?156.
Springer Berlin / Heidelberg.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natu-
ral Language Learning: Shared Task, CoNLL Shared
Task 2011, pages 28?34, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL?05,
pages 91?98.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL), pages 216?220.
Ruy L. Milidiu?, C??cero N. dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings of ACL2008,
Columbus, Ohio.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Frank Rosenblatt. 1957. The Perceptron ? a perceiving
and recognizing automaton. Technical report, Cornell
Aeronautical Laboratory. Report 85-460-1.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
Chun-Nam Yu and Thorsten Joachims. 2009. Learning
structural SVMs with latent variables. In Proceedings
of the International Conference on Machine Learning
(ICML).
48
