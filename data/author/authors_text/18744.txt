Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 677?687, Dublin, Ireland, August 23-29 2014.
Joint Opinion Relation Detection Using One-Class Deep Neural Network
Liheng Xu, Kang Liu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, jzhao}@nlpr.ia.ac.cn
Abstract
Detecting opinion relation is a crucial step for fine-gained opinion summarization. A valid opin-
ion relation has three requirements: a correct opinion word, a correct opinion target and the
linking relation between them. Previous works prone to only verifying two of these requirements
for opinion extraction, while leave the other requirement unverified. This could inevitably intro-
duce noise terms. To tackle this problem, this paper proposes a joint approach, where all three
requirements are simultaneously verified by a deep neural network in a classification scenario.
Some seeds are provided as positive labeled data for the classifier. However, negative labeled
data are hard to acquire for this task. We consequently introduce one-class classification problem
and develop a One-Class Deep Neural Network. Experimental results show that the proposed
joint approach significantly outperforms state-of-the-art weakly supervised methods.
1 Introduction
Opinion summarization aims to extract and summarize customers? opinions from reviews on products or
services (Hu and Liu, 2004; Cardie et al., 2004). With the rapid expansion of e-commerce, the number of
online reviews is growing at a high speed, which makes it impractical for customers to read throughout
large amounts of reviews to choose better products. Therefore, it is imperative to automatically gener-
ate opinion summarization to help customers make more informed purchase decisions, where detecting
opinion relation is a crucial step for opinion summarization.
Before going further, we first introduce some notions. An opinion relation, is a triple o = (s, t, r),
where three factors are involved: s is an opinion word which refers to those words indicating sentiment
polarities; t is an opinion target, which can be any entity or aspect of an entity about which an opinion has
been expressed; r refers to the linking relation between s and t. As in Example 1, s={clear}, t={sceen},
and there is a linking relation between the two words because clear is used to modify screen.
Example 1. This mp3 has a clear screen.
For a valid opinion relation, there are three requirements corresponding to the three factors: (i) the
opinion word indicates sentiment polarity; (ii) the opinion target is related to current domain; (iii) the
opinion word modifies the opinion target. Previous weakly supervised methods often expand a seed set
and identify opinion relation either by co-occurrence statistics (Hu and Liu, 2004; Hai et al., 2012) or
syntactic dependencies (Popescu and Etzioni, 2005; Qiu et al., 2009) following the assumption below.
Assumption 1. Terms that are likely to have linking relation with the seed terms are believed to be
opinion words or opinion targets.
For example, if one has an opinion word seed clear (which satisfies requirement i), and one finds that
it modifies the word screen in Example 1 (which satisfies requirement iii). Then one infers that screen
is an opinion target according to Assumption 1 (whether screen is correct is not checked). However, in
Example 2(a), we can see that good is an opinion word and it modifies thing, but thing is not related to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
677
mp3 domain. If one follows Assumption 1, thing will be mistaken as an opinion target. Similarly, in
Example 2(b), if one uses mp3 to extract another as an opinion word, he may get an objective word.
Example 2. (a) This mp3 has many good things. (b) Just another mp3 I bought.
The reason for the errors above is that Assumption 1 only verifies two requirements for an opinion
relation. Unfortunately, this issue occurs frequently in online reviews. As a result, previous methods
often suffer from these noise terms. To produce more precise opinion summary, it is argued that we shall
follow a more restricted assumption as follows.
Assumption 2. The three requirements: the opinion word, the opinion target and the linking relation
between them, shall be all verified during opinion relation detection.
To make accordance with Assumption 2, this paper proposes a novel joint opinion relation detection
method, where opinion words, opinion targets and linking relations are simultaneously considered in a
classification scenario. Following previous works, we provide a small set of seeds (i.e. opinion words
or targets) for supervision, which are regarded as positive labeled examples for classification. However,
negative labeled examples (i.e. noise terms) are hard to acquire, because we do not know which term
is not an opinion word or target. This leads to One-Class Classification (OCC) problem (Moya et al.,
1993). The key to OCC is semantic similarity measuring between terms, and Deep Neural Network
(DNN) with word embeddings is a powerful tool for handling this problem. We consequently inte-
grate DNN into a OCC classifier and develop a One-Class Deep Neural Network (OCDNN). Concretely,
opinion words/targets/relations are first represented by embedding vectors and then jointly classified.
Experimental results show that the proposed joint method which follows Assumption 2 significantly
outperforms state-of-the-art weakly supervised methods which are based on Assumption 1.
2 Related Work
In opinion relation detection task, previous works often used co-occurrence statistics or syntax informa-
tion to identify opinion relations. For co-occurrence statistical methods, Hu and Liu (2004) proposed a
pioneer research for opinion summarization based on association rules. Popescu and Etzioni (2005) de-
fined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features.
Hai et al. (2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests
(LRT) (Dunning, 1993) as the co-occurrence statistical measure. For syntax-based approaches, Riloff
and Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions. Zhuang
et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to su-
pervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for
useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double
Propagation which introduced eight heuristic syntactic rules to detect opinion relations.
However, none of the above methods could verify opinion words/targets/relations simultaneously dur-
ing opinion relation detection. To perform joint extraction, various models had been proposed, most of
which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM
(Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, op-
timal models such as Integer Linear Programming (ILP) were also employed to perform joint inference
for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013).
Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless,
most existing joint models rely on full supervision, which have the difficulty of obtaining annotated
training data in practical applications. Also, supervised models that are trained on one domain often fail
to give satisfactory results when shifted to another domain. Our method does not require annotated data.
3 The Proposed Method
To detect opinion relations, previous methods often leverage some seed terms, such as opinion word
seeds (Hu and Liu, 2004; Baccianella et al., 2010) and opinion target seeds (Jijkoun et al., 2010; Hai
et al., 2012). These seeds can be used as positive labeled examples to train a classifier. However, it is
hard to get negative labeled examples for this task. Because opinion words or targets are often domain
678
dependent and words that do not bear any sentiment polarity in one domain may be used to express
opinion in another domain. It is also very hard to specify in what case there is no linking relation
between two words.
To deal with this problem, we employ one-class classification, and develop a One-Class Deep Neural
Network (OCDNN) for opinion relation detection. The architecture of OCDNN is shown in Figure 1,
which consists of two levels. The lower level learns feature representations unsupervisedly for opinion
words/targets/relations, where the left component uses word embedding learning to represent opinion
words/targets, and the right component maps linking relations to embedding vectors by a recursive au-
toencoder. Then the upper level uses the learnt features to perform one-class classification.
 
Figure 1: The architecture of OCDNN.
 
Figure 2: An example of recursive autoencoder.
3.1 Opinion Seed Generation
To obtain training data for OCDNN, we shall first get some seed terms as follows.
Opinion Word Seeds. We manually pick 186 domain independent opinion words from SentiWordNet
(Baccianella et al., 2010) as the opinion word seed set SS.
Opinion Target Seeds. Likelihood Ratio Tests (LRT) (Dunning, 1993) used in (Hai et al., 2012) is
employed to generate opinion target seeds. LRT aims to measure how greatly two terms T
i
and T
j
are
associated with each other by sentence-level corpus statistics which is defined as follows,
LRT = 2[logL(p
1
, k
1
, n
1
) + logL(p
2
, k
2
, n
2
)? logL(p, k
1
, n
1
)? logL(p, k
2
, n
2
)] (1)
where k
1
= tf(T
i
, T
j
), k
2
= tf(T
i
,
?
T
j
), k
3
= tf(
?
T
i
, T
j
), k
4
= tf(
?
T
i
,
?
T
j
), tf(?) denotes term frequency;
L(p, k, n) = p
k
(1 ? p)
n?k
, n
1
= k
1
+ k
3
, n
2
= k
2
+ k
4
, p
1
= k
1
/n
1
, p
2
= k
2
/n
2
and p =
(k
1
+ k
2
)/(n
1
+ n
2
). We measure LRT between a domain name (e.g. mp3, hotel, etc.) and all opinion
target candidates. Then N terms with highest LRT scores are added into the opinion target seed set TS.
Linking Relation Seeds. Linking relation can be naturally captured by syntactic dependency, because
it directly models the modification relation between opinion word and opinion target. We employ an
automatic syntactic opinion pattern learning method called Sentiment Graph Walking (Xu et al., 2013)
and get 12 opinion patterns with highest confidence as the linking relation seed set RS.
After seed generation, every opinion relation s
o
= (s
s
, s
t
, s
r
) in review corpus that satisfies s
s
? SS,
s
t
? TS and s
r
? RS is taken as a positive labeled training instance.
3.2 Opinion Relation Candidate Generation
The opinion term candidate set is denoted by C = {SC, TC}, where SC/TC represents opinion
word/target candidate. Following previous works (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu
et al., 2009), we take adjectives or verbs as opinion word candidates, and take nouns or noun phrases as
opinion target candidates. A statistic-based method in Zhu et al. (2009) is used to detect noun phrases.
An opinion relation candidate is denoted by c
o
= (c
s
, c
t
, c
r
), where c
s
? SC, c
t
? TC, and c
r
is a
potential linking relation. To get c
r
, we first get dependency tree of a sentence using Stanford Parser (de
679
Marneffe et al., 2006). Then, the shortest dependency path between a c
s
and a c
t
is taken as a c
r
. To
avoid introducing too many noise candidates, we constrain that there are at most four terms in a c
r
.
3.3 Word Representation by Word Embedding Learning
Word embedding, a.k.a word representation, is a mathematical object associated with each word, which
is often used in a vector form, where each dimension?s value corresponds to a feature and might even
have a semantic or grammatical interpretation (Turian et al., 2010). By word embedding learning, words
are embedded into a hyperspace, where two words that are more semantically similar to each other are
located closer. This characteristic is precisely what we want, because the key to one-class classification
is semantic similarity measuring (illustrated in Section 3.5).
For word representation, we use a matrix LT ? R
n?|V
w
|
, where i-th column represents the embedding
vector for term t
i
, n is the size of embedding vector and V
w
is the vocabulary of LT . Therefore, we
can denote t
i
by a binary vector b
i
? R
|V
w
|
and get its embedding vector by x
i
= LTb
i
. The training
criterion for word embeddings is,
?
? = argmin
?
?
c?C
?
v?V
w
max{0, 1? s
?
(c) + s
?
(v)} (2)
where ? is the parameters of neural network used for training. See Collobert et al. (2011) for the detailed
implementation.
3.4 Linking Relation Representation by Using Recursive Autoencoder
The goal of this section is to represent the linking relation between an opinion word and an opinion target
by a n-element vector as we do during word representation. Specifically, we combine embedding vectors
of words in a linking relation by a recursive autoencoder (Socher et al., 2011) according to syntactic
dependency structure. In this way, linking relations are no longer limited to the initial seeds during
classification, because linking relations that are similar to the seed relations will have similar vector
representations.
Figure 2 shows a linking relation representation process by an example: too loud to listen to the player.
First, we get its dependency path between the opinion word c
s
:loud and the opinion target c
t
:player.
Then c
s
and c
t
are replaced by wildcards [SC] and [TC] because they are not concerned in the linking
relation. The dash line box in Figure 2 shows a standard autoencoder, which is a three-layer neural
network, where the number of nodes in input layer is equal to that of output layer. It takes two n-element
vectors as input and compresses semantics of the two vectors into one n-element vector in hidden layer
by,
y = f(W
(dep)
[x
1
;x
2
] + b), W
(dep)
=
1
2
[I
1
; I
2
; I
b
] +  (3)
where [x
1
;x
2
] is the concatenation of the two input vectors and f is the sigmoid function; W
(dep)
is a
parameter matrix that is chosen according to the dependency relation between x
1
and x
2
(In the case of
y
1
, W
(dep)
= W
(xcomp)
), which is initialized by I
i
, where I
i
is a n ? n unit matrix, I
b
is a n-element
null vector, and  is sampled from a uniform distribution U [?0.001, 0.001] (Socher et al., 2013). Then
W
(dep)
are updated during training. The training criterion of autoencoder is to minimize Euclidean
distance between the original input and its output,
E
rae
= ||[x
1
;x
2
]? [x
?
1
;x
?
2
]||
2
(4)
where [x
?
1
;x
?
2
] = W
(out)
y and W
(out)
is initialized by W
(dep)
T
.
We always start the combination process from [SC] and it is repeated along the dependency path. For
example, the result vector y
1
of the first combination is used as the input vector when computing y
2
.
Finally, the linking relation is represented by a n-element vector (the green vector in Figure 2).
680
3.5 One-Class Classification for Opinion Relation Detection
We represent an opinion relation candidate c
o
= (c
s
, c
t
, c
r
) by a vector v
o
= [v
s
; v
t
; v
r
], which is
a concatenation of the opinion word embedding v
s
, the opinion target embedding v
t
and the linking
relation embedding v
r
. Then v
o
is feed to the upper level autoencoder in Figure 1.
To perform one-class classification, the number of nodes in the hidden layer of the upper level autoen-
coder is constrained to be smaller than that of the input layer. By using such a ?bottleneck? network
structure, characteristics of the input are first compressed into the hidden layer and then reconstructed
by the output layer (Japkowicz et al., 1995). Concretely, characteristics of positive labeled opinion rela-
tions are first compressed into the hidden layer, and then the autoencoder should be able to adequately
reconstruct positive instances in the output layer, but should fail to reconstruct negative instances which
present different characteristics from positive instances. Therefore, the detection of opinion relation is
equivalent to assessing how well a candidate is reconstructed by the autoencoder. As the input vector
v
o
consists of representations for opinion words/targets/relations, characteristics of the three factors are
jointly compressed by one hidden layer. Either false opinion word/target/relation will lead to failure of
reconstruction. Consequently, our approach follows Assumption 2.
For opinion relation detection, candidates with reconstruction error scores that are smaller than a
threshold ? are classified as positive. Determining the exact value of ? is very difficult. Inspired by other
one-class approaches (Liu et al., 2002; Manevitz and Yousef, 2007), we introduce some negative opinion
terms to help to estimate ?.
1
Although negative instances are hard to acquire, Xu et al. (2013) show that
a set of general nouns (such as thing, one, etc., we denote them by GN ) seldom appear to be opinion
targets. One the other hand, we create a 50-opinion-word validation set SV from SentiWordNet.
To estimate ?, we first introduce a positive proportion (pp) score,
pp(t) = tf
+
(t)/tf(t), t ? PE, PE = {c
o
|E
r
(c
o
) < ?} (5)
where PE denotes the opinion relations that are classified as positive, E
r
(?) is the reconstruction error
of OCDNN and tf
+
(?) is the frequency of term in PE. Then an error function E
?
is minimized, which
balances between the proportion of non-target terms (GN ) in PE (which shall be as small as possible)
and the proportion of opinion words in validation set (SV ) in PE (which shall be as large as possible).
E
?
=
?
t?GN?PE
[pp(t)? 0]
2
+
?
s?SV ?PE
[pp(s)? 1]
2
(6)
3.6 Opinion Target Expansion
We apply bootstrapping to iteratively expand opinion target seeds. It is because the vocabulary of seed
set is limited, which cannot fully represent the distribution of opinion targets. So we expand opinion
target seeds in a self-training manner to alleviate this issue. After training OCDNN, all opinion relation
candidates are classified, and opinion targets are ranked in descent order by,
s(t) = log tf(t)? pp(t). (7)
Then, top M candidates are added into the target seed set TS for the next training iteration.
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets. Three real world datasets are selected for evaluation. The first one is called Customer Review
Dataset (CRD)
2
which contains reviews on five products (denoted by D1 to D5). The second is a bench-
mark dataset (Wang et al., 2011) on MP3 and Hotel
3
. The last one is crawled from www.amazon.com,
which involves Mattress and Phone. Two annotating criteria are applied.
1
This is not in contradiction with OCC problem, because these negative examples are NOT used during training.
2
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
3
http://timan.cs.uiuc.edu/downloads.html
681
Annotation 1 is used to evaluate opinion words/targets extraction. Firstly, 10,000 sentences are ran-
domly selected from reviews and all possible terms are extracted along with their contexts. Then, anno-
tators are required to judge whether each term is an opinion word or an opinion target.
Annotation 2 is used to evaluate intra-sentence opinion relation detection. Annotators are required to
carefully read through each sentence and find out every opinion relation, which consists of an opinion
word, an opinion target, as well as the linking relation between them. The annotation is very labor-
intensive, so only 5,000 sentences are annotated for MP3 and Hotel.
Two annotators were required to annotate following the criteria above. When conflicts happened, a
third annotator would make the final judgment. Note that Annotation 1 and Annotation 2 were annotated
by two different groups. Detailed information of the annotated datasets are shown in Table 1. Further-
more, the kappa values between Annotation 1 and Annotation 2 are 0.88 for opinion words and 0.84 for
opinion targets, showing highly substantial agreement.
Domain #OW #OT Kappa OW Kappa OT
Hotel 434 1,015 0.72 0.67
MP3 559 1,158 0.69 0.65
Mattress 366 523 0.67 0.62
Phone 391 862 0.68 0.64
(a) Annotation 1
Domain #LR #OW #OT Kappa LR
Hotel 2,196 317 735 0.62
MP3 2,328 342 791 0.61
(b) Annotation 2
Table 1: The detailed information of Annotations. OW/OT/LR stands for opinion words/opinion tar-
gets/linking relations. The Kappa-values are calculated by using exact matching metric for Annotation 1
and overlap matching metric for Annotation 2.
Evaluation Metrics. We perform evaluation in terms of Precision(P), Recall(R) and F-measure(F)
according to exact and overlap matching metrics (Wiebe et al., 2005). The exact metric is used to
evaluate opinion word/target extraction, which requires exact string match. And the overlap metric is
used to evaluate opinion relation detection, where an extracted opinion relation is regarded as correct
when both the opinion word and the opinion target in it overlap with the gold standard.
4
Evaluation Settings. Four state-of-the-art weakly supervised approaches are selected as competi-
tors. Two are co-occurrence statistical methods and two are syntax-based methods, all of which follow
Assumption 1.
AdjRule extracts opinion words/targets by using adjacency rules (Hu and Liu, 2004).
LRTBOOT is a bootstrapping algorithm which employs Likelihood Ratio Tests (Dunning, 1993) as
the co-occurrence statistical measure (Hai et al., 2012).
DP denotes the Double Propagation algorithm (Qiu et al., 2009).
DP-HITS is an enhanced version of DP proposed by Zhang et al. (2010), which ranks terms by
s(t) = log tf(t)? importance(t) (8)
where importance(t) is estimated by the HITS algorithm (Kleinberg, 1999).
OCDNN is the proposed method. The target seed size N = 40, the opinion targets expanded in each
iteration M = 20, and the max bootstrapping iteration number is X = 10. The representation learning
in lower level of OCDNN is trained on the whole corpus, while the test data are the same for all settings.
All results of OCDNN are taken by average performance over five runs with randomized parameters.
4.2 OCDNN vs. the State-of-the-art
We compare OCDNN with state-of-the-art methods for opinion words/targets extraction. In OCDNN,
Eq. 7 is used to rank opinion words/targets. The results on CRD and the four domains are shown in
Table 2 and Table 3. DP-HITS does not extract opinion words so their results for opinion words are not
taken into account.
4
Determining the exact boundaries of opinion terms is hard even for human (Wiebe et al., 2005), so we use this relaxation.
682
Opinion Targets
Method
D1 D2 D3 D4 D5
Avg.
P R F P R F P R F P R F P R F F
AdjRule 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
DP-HITS 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
LRTBOOT 0.77 0.87 0.82 0.74 0.90 0.81 0.79 0.89 0.84 0.72 0.88 0.79 0.74 0.88 0.80 0.81
OCDNN 0.83 0.82 0.82 0.86 0.85 0.85 0.86 0.87 0.86 0.78 0.84 0.81 0.89 0.85 0.87 0.84
Opinion Words
AdjRule 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
LRTBOOT 0.60 0.79 0.68 0.52 0.82 0.64 0.60 0.76 0.67 0.56 0.70 0.62 0.66 0.71 0.68 0.66
OCDNN 0.64 0.77 0.70 0.63 0.79 0.70 0.66 0.73 0.69 0.68 0.70 0.69 0.70 0.69 0.69 0.70
Table 2: Results of opinion terms extraction on Customer Review Dataset.
Opinion Targets
Method
MP3 Hotel Mattress Phone
Avg.
P R F P R F P R F P R F F
AdjRule 0.53 0.55 0.54 0.55 0.57 0.56 0.50 0.60 0.55 0.52 0.51 0.51 0.54
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.55 0.60 0.57 0.60 0.53 0.56 0.59
DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.55 0.67 0.60 0.62 0.64 0.63 0.63
LRTBOOT 0.60 0.77 0.67 0.59 0.78 0.67 0.55 0.78 0.65 0.57 0.76 0.65 0.66
OCDNN 0.70 0.68 0.69 0.71 0.70 0.70 0.63 0.69 0.66 0.69 0.68 0.68 0.68
Opinion Words
AdjRule 0.48 0.65 0.55 0.51 0.68 0.58 0.51 0.68 0.58 0.48 0.61 0.54 0.56
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.54 0.68 0.60 0.55 0.59 0.57 0.60
LRTBOOT 0.52 0.69 0.59 0.54 0.74 0.62 0.51 0.73 0.60 0.50 0.68 0.58 0.60
OCDNN 0.68 0.65 0.66 0.70 0.68 0.69 0.59 0.70 0.64 0.63 0.59 0.61 0.65
Table 3: Results of opinion terms extraction on the four domains.
From Table 2, we can see that our method outperforms co-occurrence-based methods AdjRule and
LRTBOOT, but achieves comparable or a little worse results than syntax-based methods DP and DP-
HITS. This is because CRD is quite small, which only contains several hundred sentences for each prod-
uct review set. In this case, methods based on careful-designed syntax rules have superiority over those
based on statistics (Liu et al., 2013). For results on larger datasets shown in Table 3, our method out-
performs all of the competitors. Comparing OCDNN with DP-HITS, the two approaches use similar
term ranking metrics (Eq. 7 and Eq. 8), but OCDNN significantly outperforms DP-HITS. Therefore, the
positive proportion score estimated by OCDNN is more effective than the importance score in DP-HITS.
Comparing OCDNN with LRTBOOT, we find that LRTBOOT achieves better recall but lower precision.
This is because LRTBOOT follows Assumption 1 during bootstrapping, which suffers a lot from error
propagation, while our joint classification approach effectively alleviates this issue. We will discuss the
impact of error propagation in detail later.
4.3 Assumption 1 vs. Assumption 2
This section evaluates intra-sentence opinion relation detection, which is more useful for practical appli-
cations. It also reflects the impacts of Assumption 1 and Assumption 2. The results are shown in Table
4 and Table 5, where OCDNN significantly outperforms all competitors. The average improvement of
F-measure over the best competitor is 6% on CRD and 9% on Hotel and MP3.
As Assumption 1 only verifies two of the requirements in an opinion relation, it would inevitably
introduce noise terms during extraction. For syntax-based method DP, it extracts many false opinion
relations such as good thing and nice one (where thing and one are false opinion targets) or objective
expressions like another mp3 and every mp3 (which contain false opinion words another and every). For
co-occurrence statistical methods AdjRule and LRTBOOT, it is very hard to deal with ambiguous linking
relations. For example, in phrase this mp3 is very good except the size, co-occurrence statistical methods
could hardly tell which opinion target does good modify (mp3 or size). Our method follows Assumption
683
Method
D1 D2 D3 D4 D5
Avg.
P R F P R F P R F P R F P R F F
AdjRule 0.51 0.66 0.58 0.53 0.63 0.58 0.50 0.61 0.55 0.48 0.60 0.53 0.50 0.61 0.55 0.56
DP 0.66 0.63 0.64 0.68 0.60 0.64 0.69 0.62 0.65 0.66 0.57 0.61 0.67 0.60 0.63 0.64
LRTBOOT 0.53 0.70 0.60 0.57 0.72 0.64 0.55 0.69 0.61 0.52 0.70 0.60 0.55 0.68 0.61 0.61
OCDNN 0.76 0.66 0.71 0.74 0.67 0.70 0.77 0.67 0.72 0.70 0.65 0.67 0.77 0.66 0.71 0.70
Table 4: Results of opinion relation detection on Customer Review Dataset.
Method
MP3 Hotel
Avg.
P R F P R F F
AdjRule 0.49 0.55 0.52 0.45 0.53 0.49 0.50
DP 0.63 0.51 0.56 0.59 0.50 0.54 0.55
LRTBOOT 0.54 0.63 0.58 0.50 0.60 0.55 0.56
OCDNN 0.73 0.60 0.66 0.70 0.59 0.64 0.65
Table 5: Results of opinion relation detection on the two domains.
2, which verifies all three requirements for opinion word/target/relation in an opinion relation, so the
above errors are greatly reduced. Therefore, Assumption 2 is more reasonable than Assumption 1.
4.4 The Effect of Joint Classification
We evaluate the three bootstrapping methods (DP, LRTBOOT and OCDNN) for opinion target expansion.
The precision of each iteration is shown in Figure 3. We can see that DP and LRTBOOT gradually suffer
from error propagation and the precision drops quickly along with the number of iteration increases. For
OCDNN, although error propagation is inevitable, the precision curve retains at a high level. Therefore,
the joint approach produces more precise results.
For more detailed analysis, we give a variation of the proposed method named 3NN, which uses
3 individual autoencoders to classify opinion words/targets/relations separately. An opinion relation
candidate is classified as positive only when the three factors are all classified as positive. Then opinion
relations are ranked by the sum of reconstruction scores of the three factors. In the results of opinion
relation detection, when the recall is fixed at 0.6, the precisions of 3NN are 0.67 for MP3 and 0.65
for Hotel, while the precisions of OCDNN are 0.73 for MP3 and 0.70 for Hotel. Therefore, OCDNN
achieves much better performance than 3NN.
An example may explain the reason of why 3NN gets worse performance. In our experiment on Hotel,
a false opinion relation happy day is misclassified as positive by 3NN. It is because the word day has
a small reconstruction score in 3NN. At the same time, happy is a correct opinion word, so the whole
expression happy day also has a small reconstruction score and then be misclassified. In contrast, the
reconstruction score of happy day from OCDNN is quite large so the phrase is dropped. The reason
is that the joint approach captures the semantic of a whole phrase rather than its single components.
Therefore, it is more reasonable.
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(a) MP3
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(b) Hotel
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(c) Mattress
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(d) Phone
Figure 3: Precision (y-axis) of opinion target seed expansion at each bootstrapping iteration (x-axis).
684
5 Conclusion and Future Work
This paper proposes One-Class Deep Neural Network for joint opinion relation detection in one-class
classification scenario, where opinion words/targets/relations are simultaneously verified during classifi-
cation. Experimental results show the proposed method significantly outperforms state-of-the-art weakly
supervised methods that only verify two factors in an opinion relation.
In future work, we plan to adapt our method and make it be capable of capturing implicit opinion
relations.
Acknowledgement
This work was sponsored by the National Natural Science Foundation of China (No. 61202329 and No.
61333018) and CCF-Tencent Open Research Fund.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical re-
source for sentiment analysis and opinion mining. Seventh conference on International Language Resources
and Evaluation, pages 2200?2204.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings
of the 20th international joint conference on Artifical intelligence, IJCAI?07, pages 2683?2688, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Claire Cardie, Janyce Wiebe, Theresa Wilson, and Diane Litman. 2004. Low-level annotations and summary
representations of opinions for multi-perspective question answering.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,
pages 431?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537,
November.
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language
Technology. The Stanford Natural Language Processing Group.
Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Comput. Linguist., 19(1):61?
74, March.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012. One seed to find them all: mining opinion features via association.
In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM
?12, pages 255?264, New York, NY, USA. ACM.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 168?177.
Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single- and cross-domain setting with
conditional random fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 1035?1045, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nathalie Japkowicz, Catherine Myers, and Mark Gluck. 1995. A novelty detection approach to classification. In
Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI?95, pages
518?523, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Valentin Jijkoun, Maarten de Rijke, and Wouter Weerkamp. 2010. Generating focused topic-specific sentiment
lexicons. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL
?10, pages 585?594, Stroudsburg, PA, USA. Association for Computational Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized hmm-based learning framework for web opinion mining. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ?09, pages 465?472.
685
Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604?632, September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1065?1074,
Prague, Czech Republic, June. Association for Computational Linguistics.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware
review mining and summarization. In Proceedings of the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 653?661, Stroudsburg, PA, USA. Association for Computational Linguistics.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li. 2002. Partially supervised classification of text documents.
In Proceedings of the Nineteenth International Conference on Machine Learning, ICML ?02, pages 387?394,
San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion tar-
gets from online reviews. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1754?1763, August.
Larry Manevitz and Malik Yousef. 2007. One-class document classification via neural networks. Neurocomput-
ing, 70(7C9):1466?1481.
Mary M. Moya, Mark W. Koch, and Larry D. Hostetler. 1993. One-class classifier networks for target recognition
applications. In Proceedings world congress on neural networks, pages 797?801.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceed-
ings of the conference on Human Language Technology and Empirical Methods in Natural Language Process-
ing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI?09, pages
1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings
of the 2003 conference on Empirical methods in natural language processing, EMNLP ?03, pages 105?112,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing, EMNLP ?11, pages 151?161, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 455?465, Sofia, Bulgaria, August. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 384?394, Stroudsburg, PA, USA. Association for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword
supervision. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 618?626.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-3):165?210.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 -
Volume 3, EMNLP ?09, pages 1533?1541, Stroudsburg, PA, USA. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
686
Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1640?1649, Sofia, Bulgaria, August. Association for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O?Brien-Strain. 2010. Extracting and ranking product features
in opinion documents. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, COLING ?10, pages 1462?1470, Stroudsburg, PA, USA. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the
15th ACM international conference on Information and knowledge management, CIKM ?06, pages 43?50, New
York, NY, USA. ACM.
687
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2107?2116, Dublin, Ireland, August 23-29 2014.
Exploring Fine-grained Entity Type Constraints for Distantly Supervised
Relation Extraction
Yang Liu Kang Liu Liheng Xu Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
Zhongguancun East Road #95, Beijing 100190, China
{yang.liu, kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Distantly supervised relation extraction, which can automatically generate training data by align-
ing facts in the existing knowledge bases to text, has gained much attention. Previous work used
conjunction features with coarse entity types consisting of only four types to train their model-
s. Entity types are important indicators for a specific relation, for example, if the types of two
entities are ?PERSON? and ?FILM? respectively, then there is more likely a ?DirectorOf? rela-
tion between the two entities. However, the coarse entity types are not sufficient to capture the
constraints of a relation between entities. In this paper, we propose a novel method to explore
fine-grained entity type constraints, and we study a series of methods to integrate the constraints
with the relation extracting model. Experimental results show that our methods achieve bet-
ter precision/recall curves in sentential extraction with smoother curves in aggregated extraction
which mean more stable models.
1 Introduction
Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences
containing them. It can potentially benefit many applications, such as knowledge base construction,
question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Tra-
ditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to
manually label training data, which is expensive and limits the ability to scale up. Due to the shortcom-
ing of supervised approaches mentioned above, recently, a more promising approach named distantly
supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has be-
come popular. Instead of manual labeling, it automatically generates training data by aligning facts in
existing knowledge bases to text.
However, the paradigm of distant supervision also causes new problems of noisy training data both in
positive training instances and negative training instances. To overcome the false positive problem caused
by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Sur-
deanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they
assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al.
(Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail
when there was only one sentence containing both entities. They proposed a method to learn and filter
noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu
et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative
training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudo-
relevance feedback method trying to find out the false negative instances and add them into positive
training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training in-
stances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni,
2013) used hidden variables to model the missing data in databases based on a graphical model. The
training data generation process for all the above work is under the framework of (Mintz et al., 2009),
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2107
one important step of which is to recognize entity mentions from text and assign them entity types which
are used to compose features for training the model. The entity types they used are very coarse only con-
sisting of four categories (PERSON, ORGANIZATION, LOCATION, NONE). We argue that the coarse
entity types are not sufficient to indicate relations.
A specific relation constrains the entity types of its two entities. For instance, the SingerOf relation
limits the entity type of its first entity as PERSON or more fine-grained ARTIST, and the entity type of
its second entity as ART or more fine-grained MUSIC. Therefore, when extracting a relation instance,
the entity types of its two entities are important indicators for a specific relation. Previous work used
conjunction features (Details in Section 3.3) by combining the coarse entity types of entity mentions
with its contextual lexical and syntactic features. However, the conjunction features may fail to dis-
tinguish the relations. For example, the following two sentences contain two relation instances, one is
DirectorOf(Ang Lee, Life of Pi), and the other is AuthorOf(George R.R. Martin, A Song of Ice and Fire).
1. Ang Lee?s Life of Pi surprised many by scoring a leading four Oscars on Sunday night...
2. Westeros is the premiere fansite for George R.R. Martin?s A Song of Ice and Fire.
Only using the above conjunction features, we cannot tell the difference between the two entity pairs,
and are probable to incorrectly classify them as the same relation. By contrast, if we can assign each
entity with fine-grained entity types, for example, Ang Lee as the entity type ARTIST and George R.R.
Martin as AUTHOR, we may succeed in classifying the two entity pairs correctly.
To achieve the goal mentioned above, there are mainly three challenges: (1) how to define the fine-
grained type set; (2) how to assign the types to entity mentions; (3) how to integrate the fine-grained
entity type constraints with the relation extracting model. To address these challenges, in this paper,
we propose a novel approach to explore the fine-grained entity type constraints for distantly supervised
relation extraction. First, we use the types defined in (Ling and Weld, 2012) stemmed from Freebase
1
as the fine-grained entity type set (introduced in Section 3.1). Second, we leverage Web knowledge
to train a fine-grained entity type classifier and predict entity types for each entity mention. Third, we
study several methods to integrate the type constraints with an existing system MULTIR, a multi-instance
multi-label model in (Hoffmann et al., 2011), to train the extractor.
In summary, the contribution of this paper can be concluded as follows.
(a) We explore the effect of fine-grained entity type constraints on distantly supervised relation extrac-
tion. A novel method is proposed to leverage Web knowledge to automatically train a fine-grained
entity type classifier, which is used to predict the fine-grained types of each entity mention.
(b) We study a series of methods for integrating the fine-grained entity type constraints with the extract-
ing model and compare their performance with different parameter settings.
(c) We conduct experiments to demonstrate the effects of the newly exploited fine-grained entity type
constraints. It shows that our method achieves a much better precision/recall curves over the base-
line system in sentential extraction, and improves the performance with a smoother precision/recall
curve in aggregated extraction, which means a more stable model.
2 Distant Supervision for Relation Extraction
We define a relation instance (or a fact), which means a binary relation, as r(e
1
, e
2
). r is the relation, and
e
1
and e
2
mean the two entities in the relation instance, for example, BornIn(Y ao Ming, Shanghai).
Distant supervision supplies a method to automatically generate training data. In this part, we will
introduce the general steps in distant supervision for relation extraction. First, we define the notations
we use. ? denotes sentences comprising the corpus, E denotes entity mentions in the corpus which are
consecutive words with the same named entity tags assigned by an NER system, ? denotes the facts (or
relation instances) in the existing knowledge base. R denotes the relations in ?.
1
http://www.freebase.com/
2108
 Figure 1: Fine-grained entity type set.
Figure 2: Framework of fine-grained entity type classifier.
To generate training data, we align pairs of entity mentions in the same sentence with ?. The aligned
entity mentions E
train
and their sentences ?
train
along with R
train
are used as training data. Features
are extracted from them to train the relation extracting model.
To predict the unknown data for extracting new relation instances, we input pairs of entity mentions
E
predict
and the sentences containing them ?
preidct
into the trained extracting model for extracting new
relation instances.
3 Fine-grained Entity Type Constraints
Entity mentions in sentences are considered consecutive words with the same entity types (Section 2).
The entity types are part of the lexical and syntactic features(Mintz et al., 2009), and the feature setting
is followed by other related work. Their entity types are assigned by an NER system and consist of
four categories (PERSON, ORGANIZATION, LOCATION, NONE). The types of entity mentions in
a relation are important indicators for the very type of relation. However, the coarse (only four types)
entity types may not capture sufficient constraints to distinguish a relation. In this section, we explore
fine-grained entity type constraints and study different methods to integrate them with the extracting
model.
This section first introduces the fine-grained entity type set(Section 3.1), and then describes our method
which leverages Web knowledge to train the fine grained entity type classifier and assign entity mentions
with the fine-grained entity types (Section 3.2). At last, we illustrate methods to integrate fine-grained
entity type constraints with the relation extracting model.
2109
Entity pair [Hank Ratner], [Cablevision]
Sentence
Cablevision?s $600 million offer came in the form of a letter to Peter S.Kalikow,
chairman of the M.T.A., from the Garden?s vice chairman, Hank Ratner.
Conjunction Reverse Left NE1 Middle NE2 Right
Feature examples
False PER ORG
False Hank[NMOD] PER [NMOD]chairman ... offer[SBJ] ORG
True B -1 ORG POS $ ... NN NN, PER .B 1
Table 1: Examples of conjunction features.
3.1 Fine-grained Entity Types
Figure 1 is the type set we use. It was introduced in (Ling and Weld, 2012) and was derived from
Freebase types. The bold types in each small box of Figure 1 are upper-class types for others in that
small box. For example, /actor is a lower-class type of /person which is denoted as /person/actor.
And /person and /person/actor coexist in the type set.
3.2 Fine-grained Entity Type Classifier
In this section, we describe our method that leverages Web knowledge to train a fine-grained entity type
classifier and predict entity types of each entity mention. Its architecture is shown in Figure 2.
3.2.1 Training
The training data are obtained from Wikipedia. Because the defined fine-grained types are tailored based
on Freebase types, we can find the mappings between the two type sets, for example, /person/doctor
maps to two Freebase types /medicine/physician and /medicine/surgeon. And Freebase WEX
2
supplies a mapping between Freebase types to Wikipedia articles. As a result, we can map Wikipedia
articles to defined fine-grained types.
Based on the mappings, we obtain Wikipedia articles for each type as training data and negative
training examples are sampled from articles not contained in the mappings. We preprocess the articles
by: stop words filtering, stemming, and term frequency filtering and use a maxent model to train the
classifier.
3.2.2 Predicting
To predict types of each entity mention, we first use search engines to expand entity mentions. Specif-
ically, each entity mention is used as a query sent to the search engine
3
. Titles and descriptions of top
k returned snippets are selected (We keep the top 20 in the experiments). The obtained text are pre-
processed with the same method as training examples. Then we use the trained fine-grained entity type
classifier to predict the types of each entity mention.
After predicting, we obtain a ranked list of types for each entity mention, which are ranked by the
predicting scores.
3.3 Integrating Fine-grained Entity Type Constraints into the Extracting Model
This section introduces our methods to integrate the fine-grained entity type constraints with the ex-
tracting model. First of all, we briefly review the features used in previous models which derived from
(Mintz et al., 2009) and (Riedel et al., 2010). Their features mainly comprise two types: lexical features
(POS tags, words and entity types) and syntactic features (dependency parsing tags, words and entity
types). Each feature is a conjunction with several parts: entity types of two entity mentions, the left
context window of the first entity mention, the right context window of the second entity mention and
the part between them (the window contains none or one or two words ). Table 1 shows an example of
the conjunction features.
2
http://wiki.freebase.com/wiki/WEX
3
We use Bing search API. http://datamarket.azure.com/dataset/bing/search
2110
To integrate the exploited fine-grained entity type constraints with the extracting model, we proposed
three methods (substitution, augment and selection) to make the type constraints take effects.
3.3.1 Substitution Method
In this method, we substitute coarse entity types of the features with the entity mentions? fine-grained
types, and use the new features to train the model. Instead of substituting directly, an entity mention
is first represented by its fine-grained types and the upper-class of the fine-grained type, for example,
/person/politician derives two types /person and /person/politician itself. The reason is that the
extracting model can benefit from the related types like the upper-class types. And then we use the
obtained entity types to substitute the old coarse entity types as new features greedily, which mean-
s that all the possible combinations of types between the entity pair are considered. For example,
?Barack Obama? has the fine-grained type /person/politician and his birth place ?Hawaii? has
the type /location/island, then there are 4 combinations between the two entities, they are (/person,
/location), (/person, /location/island), (/person/politician, /location) and (/person/politician,
/location).
3.3.2 Augment Method
In this method, we generate new features by substituting the coarse entity types with predicted fine-
grained types, and expand the old features with new features. Different from the substitution method, we
do not add the upper-class types, for that we think the coarse types in old features have the same effect.
In this method, we use the fine-grained constraints as a complementary.
3.3.3 Selection Method
The selection method is similar to the augment method. The difference is that we do not expand all
old features with new features. We select some of them to expand. The reason is that some of the
conjunction features are of high-precision themselves, it can clearly indicate the relations with its left,
middle and right parts, even without the entity types (informative ones). If we expand these features,
it may cause more noisy features. So we expect to only expand the ones that lack of the indicating
abilities (non-informative ones). In this paper, we employ a simple method to distinguish between the
informative ones and non-informative ones by the length of the features, which means that the longer is
more informative than the shorter. In our experiments, the length threshold is set as 20.
In the predicting phase (Section 3.2), we obtain a ranked type list for each entity mention. The top list
types are considered in our methods. Experiments in Section 4.3 are conducted on top k {k ? 1, 2, 3}
type/types in the obtained ranked list. And they are combined with a greedy method similar to that in the
substitution method explained above.
4 Experiments
4.1 Settings
We use the same data sets as (Riedel et al., 2010) and (Hoffmann et al., 2011), where NYTimes sentences
in the years 2005-2006 are used as training corpus ?
train
for distant supervision and sentences in 2007
are used as testing corpus ?
predict
. The data was first tagged with an NER system (Finkel et al., 2005)
and consecutive words with the same tag are extracted as entity mentions. And then, entity mentions
E
train
in training corpus are aligned to facts ? in Freebase as training examples to train the models.
We integrate our fine-grained entity type constraint with MULTIR, an existing multi-instance multi-
label extracting model in (Hoffmann et al., 2011). Following their setttings, we conduct experiments on
aggregated extraction and sentential extraction to show the effect of fine-grained entity type constraints.
? Aggregated extraction: Aggregated extraction is corpus-level extraction. When given an entity
pair, it predicts its relation types based on the whole corpus. After extraction, the precision and
recall are computed by comparing the results with facts in Freebase. The evaluation underestimates
the accuracy because there may be correct facts in the extracted results but not existing in Freebase,
these facts are labeled as incorrect by mistake here. Because aggregated extraction is an automatic
evaluation, it is used to tune parameters like held-out evaluation in (Mintz et al., 2009).
2111
(a) PR curves of the substitution method (b) PR curves of the augment method
(c) PR curves of the selection method (d) Comparison with other methods
Figure 3: Precision-recall (PR) curves of the aggregated extraction.
? Sentential extraction: Sentential extraction predicts an entity pair only based on a specified sen-
tence containing the pair of entities. We use manually labeled data in (Hoffmann et al., 2011) as
benchmark. The data consist of 1,000 sentences and are sampled from the results their system out-
puts and sentences aligned with facts in Freebase. As they stated in their paper, these results provide
a good approximation to the true precision but can overestimate the actual recall.
4.2 Experimental Results
In aggregated extraction, we first evaluate the three type-constraint integration methods (substitution,
augment and selection) with the top k {k ? 1, 2, 3} type/types (Section 3.3). And then, we compare the
best parameter setting methods with previous work. In sentential extraction, we compare methods tuned
in aggregated extraction with MULTIR.
4.2.1 Aggregated Extraction
Figure 3 shows the precision-recall (PR) curves of the aggregated extraction. In it, Sub topk {k ?
1, 2, 3} means using the substitution method (Section 3.3) with top k fine-grained entities types re-
turned by the type classifier in Section 3.2. Correspondingly, Aug topk is for the augment method
and Select topk is for the selection method.
Figure 3(a) shows that Sub top3 outperforms the other two settings of k in the substitution method,
it seems that more fine-grained types produce better curves. In Figure 3(b), Aug top1 and Aug top2
achieve similar performances. However, when adding one more type with k = 3, we obtain a lower
curve, which contradicts the trend showed in the curves of the substitution method (Figure 3(a)). Fig-
ure 3(c) shows the PR curves of three selection methods, Select top1 has a better performance at the
beginning. Then Select top2 exceeds it a bit consistently.
In Figure 3(d), we demonstrate the comparison of best tuned methods above with previous work.
They are Sub top3, Aug top1 and Select top2. From Figure 3(d), it shows that, among the three of
our methods, Aug top1 achieves better precisions along the PR curves, and Select top2 reaches the best
2112
Figure 4: Comparison with MULTIR
recall at the highest recall point. Comparing to other methods, the PR curve of Aug top1 reaches a higher
recall with 29.3% at the highest recall point than MULTIR (24.5%). Select top2 achieves 29.3% at the
highest recall point, best among all methods. And by integrating the fine-grained entity type constraints,
they improve the PR curve of MULTIR with a more smoother curve without most of the depressions seen
in MULTIR. As stated in (Hoffmann et al., 2011), the smoother curve indicated a more stable model.
4.2.2 Sentential Extraction
Figure 4 shows the precision-recall (PR) curves of the sentential extraction. In the evaluation, we com-
pare the three best integration methods tuned in aggregated extraction with original MULTIR. Among our
three method, Aug top1 outperforms in precision and achieves a better curve in general among the three
methods, however, Select top2 gains a better recall at the end. Sub top3 has the worst recall. In gen-
eral, our methods have much better precisions than MULTIR. Aug top1 and Select top2 achieve better
curves than MULTIR. Since the evaluation of sentential extraction is a good approximation of precision,
it implies that the proposed methods are effective.
4.2.3 Analysis
On one hand, among the three proposed integration methods, generally, the augment method and selec-
tion method get better performance. The reason is that substitution method uses predicted fine-grained
entity types to replace the old coarse features in the conjunction features completely, and the conjunction
features are sensitive to entity types for different entity types indicate different conjunction features, as
a result, if we can not promise a good accuracy in the type classification which is hard to achieve in
classifying hundreds of fine-grained types, the performance will be badly influenced. Different from the
substitution method, augment method and selection method keep the old features with coarse features,
they use the features with fine-grained entity type constraints as extra information to help the extraction
and achieve better results.
On the other hand, comparing to other methods, by integration the exploited fine-grained entity type
constraints, our methods achieve improvements in both aggregated and sentential extraction. It proves
that the fine-grained entity type constraints we exploit are effective, and our proposed integration meth-
ods succeed in integrating the constraints into the extracting model. Our augment method outperforms
MULTIR in precision along the PR curves in sentential extraction and improve it performance with a
more smoother PR curve in aggregated extraction, which indicates a more stable model. Moreover, the
method gets a better recall. And our selection method consistently outperforms MULTIR in sentential
2113
k=1 k=2 k=3
Recall@k 0.596 0.740 0.806
Table 2: Evaluation of the fine-grained type classifier.
extraction. In aggregated extraction, it also achieves a smoother curve and an impressive promotion at
the highest recall point. Since the evaluation of aggregated extraction only considers the facts existing
in Freebase which may incorrectly label the right extracting results and underestimate the true precision,
and based on its better performance of precision in sentential extraction, we consider it is a more promis-
ing method. This paper only employs very naive method to select the non-informative features by its
length (Section 3.3.3), a more effective selecting method may lead further improvements.
4.3 Performance of Entity Type Classifier
We evaluate the performance of the fine-grained entity type classifier (Section 3.2). In section 3.2, we
sample the training examples from a collection of Wikipeida articles mapped with the fine-grained types.
To generate test entity mentions, we first remove the sampled training articles from the collection, and
then sample the articles from it, where the titles of sampled articles are used as the test entity mentions
(we sample 12,000 test entity mentions) and their mapped fine-grained types are used as benchmark.
After that, the predicting method in Section 3.2.2 is used to expand mentions and predict the types of
each test entity mention. After predicting, we obtain a ranked list of types for each test entity mention.
To evaluate, we define a notation of Hit@k, which equals 1 if the true type of an entity mention is
hit in the top k predicted types, otherwise equals 0. And then we evaluate it by the Recall@k defined
bellow.
Recall@k =
?
12000
i=1
Hit@k
i
12000
(1)
In equation (1), i means the ith test entity mention. Table 2 shows the results for the top 3 predicted
types.
5 Related Work
Distant supervision (also known as weak supervision or self supervision) is used to a broad class of meth-
ods in information extraction which aims to automatically generate labeled data by aligning with data
in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast
Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum
(Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN
system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and
trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUN-
NER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic
labeled data from Penn Treebank and Wikipedia infoboxes respectively.
Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation
extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and
trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a
method to generate training data automatically, however it also bring the problem of noisy labeling. After
their work, a variety of methods focused to solve this problem. Riedel (Riedel et al., 2010) proposed a
multi-instance model to model the false positive noise in training data with the assumption that at least
one of the labeled sentences truly expressed their relation. After their work, Hoffmann (Hoffmann et
al., 2011) and Surdeanu (Surdeanu et al., 2012) tried to not only model the noisy training data, but also
overcame the problem of multi-label where two entities may exist more than one relation, they proposed
graphic models as kinds of multi-instance multi-label learning methods and made improvements over
previous work. The at-least-one assumption would fail when encountering entity pairs with only one
aligned sentence. Takamatsu (Takamatsu et al., 2012) employed an alternative approach without the
mentioned assumptions. Their work predicted negative patterns using a generative model and remove
labeled data containing negative patterns to reducing noise in labeled data.
2114
Besides the problem of false positive training examples caused by distant supervision. There were a
bunch of researches trying to solve the problem of false negative training examples caused by incomplete
knowledge bases. Zhang (Zhang et al., 2013) made heuristic rules to filter the false negative training
examples. And Xu (Xu et al., 2013) tried to overcom this problem by pseudo-relevance feedback. Min
(Min et al., 2013) improved MIML in (Surdeanu et al., 2012) by adding a new layer in their 3-layer
graphic model to model the incomplete knowledge base. Ritter (Ritter and Etzioni, 2013) employed
similar intuition with (Xu et al., 2013) that they thought rear entities missing in the database would
be often mentioned in the text. They proposed a latent-variable approach to model it and showed its
improvement over aggregate and sentential extraction.
6 Conclusion
In this paper, we propose a novel approach to explore the fine-grained entity type constraints for distantly
supervised relation extraction. We leverage Web knowledge to automatically train a fine-grained entity
type classifier and predict entity types of each entity mention. And we study a series of methods to inte-
grate the type constraints with a relation extraction model. At last, thorough experiments are conducted.
The experimental results imply our methods are effective with better precision/recall curves in senten-
tial extraction and smoother precision/recall curves in aggregated extraction, which indicate more stable
models.
In the future we hope to explore more details of integration methods that integrates fine-grained entity
type constraints with relation extraction models, especially the selection integration method. We consider
that a more effective method to distinguish between the informative and non-informative features will
lead more improvements.
Acknowledgements
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61202329). This work was supported in part by
Noahs Ark Lab of Huawei Tech. Co. Ltd.
References
Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration on the Web.
Mark Craven, Johan Kumlien, et al. 1999. Constructing biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular
Biology, pages 77?86. Heidelberg, Germany.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 363?370. Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 541?
550.
Xiao Ling and DS Weld. 2012. Fine-Grained Entity Recognition. In AAAI.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In Proceedings of NAACL-HLT, pages 777?782.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages
1003?1011. Association for Computational Linguistics.
2115
Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47. Associa-
tion for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148?163. Springer.
Alan Ritter and Oren Etzioni. 2013. Modeling Missing Data in Distant Supervision for Information Extraction.
Transactions of the Association for Computational Linguistics, 1:367?378.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 455?465. Association for
Computational Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaventura Coppola, et al. 2005. Scaling Web-based aquisition of
entailment relations. Ph.D. thesis, Tel Aviv University.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 721?729. Association for Computational Linguistics.
Fei Wu and Daniel S Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the sixteenth ACM
conference on Conference on information and knowledge management, pages 41?50. ACM.
Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pages 118?127. Association for Computational
Linguistics.
W Xu, RH Le Zhao, and R Grishman. 2013. Filling Knowledge Base Gaps for Distant Supervision of Relation
Extraction. Proceedings of Association for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland.
2007. Textrunner: open information extraction on the web. In Proceedings of Human Language Technolo-
gies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:
Demonstrations, pages 25?26. Association for Computational Linguistics.
Xingxing Zhang, jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zuifang Sui. 2013. Towards Accurate
Distant Supervision for Relational Facts Extraction. In Proceedings of Association for Computational Linguis-
tics.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
Association for Computational Linguistics.
GuoDong Zhou, Min Zhang, Dong Hong Ji, and Qiaoming Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning.
2116
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1346?1356, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Opinion Target Extraction Using Word-Based Translation Model 
 
Kang Liu, Liheng Xu, Jun Zhao 
 
National Laboratory of Pattern Recognition,  
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China 
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn  
 
 
Abstract 
This paper proposes a novel approach to 
extract opinion targets based on word-
based translation model (WTM). At first, 
we apply WTM in a monolingual scenario 
to mine the associations between opinion 
targets and opinion words. Then, a graph-
based algorithm is exploited to extract 
opinion targets, where candidate opinion 
relevance estimated from the mined 
associations, is incorporated with candidate 
importance to generate a global measure. 
By using WTM, our method can capture 
opinion relations more precisely, especially 
for long-span relations. In particular, 
compared with previous syntax-based 
methods, our method can effectively avoid 
noises from parsing errors when dealing 
with informal texts in large Web corpora. 
By using graph-based algorithm, opinion 
targets are extracted in a global process, 
which can effectively alleviate the problem 
of error propagation in traditional 
bootstrap-based methods, such as Double 
Propagation. The experimental results on 
three real world datasets in different sizes 
and languages show that our approach is 
more effective and robust than state-of-art 
methods. 
1 Introduction 
With the rapid development of e-commerce, most 
customers express their opinions on various kinds 
of entities, such as products and services. These 
reviews not only provide customers with useful 
information for reference, but also are valuable for 
merchants to get the feedback from customers and 
enhance the qualities of their products or services. 
Therefore, mining opinions from these vast 
amounts of reviews becomes urgent, and has 
attracted a lot of attentions from many researchers.  
In opinion mining, one fundamental problem is 
opinion target extraction. This task is to extract 
items which opinions are expressed on. In reviews, 
opinion targets are usually nouns/noun phrases. 
For example, in the sentence of ?The phone has a 
colorful and even amazing screen?, ?screen? is an 
opinion target. In online product reviews, opinion 
targets often are products or product features, so 
this task is also named as product feature 
extraction in previous work (Hu et al 2004; Ding 
et al 2008; Liu et al 2005; Popescu et al 2005; 
Wu et al 2005; Su et al 2008).  
To extract opinion targets, many studies 
regarded opinion words as strong indicators (Hu et 
al., 2004; Popescu et al 2005; Liu et al 2005; 
Qiu et al 2011; Zhang et al 2010), which is 
based on the observation that opinion words are 
usually located around opinion targets, and there 
are associations between them. Therefore, most 
pervious methods iteratively extracted opinion 
targets depending upon the associations between 
opinion words and opinion targets (Qiu et al 2011; 
Zhang et al 2010). For example, ?colorful? and 
?amazing? is usually used to modify ?screen? in 
reviews about cell phone, so there are strong 
associations between them. If ?colorful? and 
?amazing? had been known to be opinion words, 
?screen? is likely to be an opinion target in this 
domain. In addition, the extracted opinion targets 
can be used to expand more opinion words 
according to their associations. It?s a mutual 
reinforcement procedure. 
Therefore, mining associations between opinion 
targets and opinion words is a key for opinion 
1346
target extraction (Wu et al 2009). To this end, 
most previous methods (Hu et al 2004; Ding et al 
2004; Wang et al 2008), named as adjacent 
methods, employed the adjacent rule, where an 
opinion target was regarded to have opinion 
relations with the surrounding opinion words in a 
given window. However, because of the limitation 
of window size, opinion relations cannot be 
captured precisely, especially for long-span 
relations, which would hurt estimating associations 
between opinion targets and opinion words. To 
resolve this problem, several studies exploited 
syntactic information such as dependency trees 
(Popescu et al 2005; Qiu et al 2009; Qiu et al 
2011; Wu et al 2009; Zhang et al 2010). If the 
syntactic relation between an opinion word and an 
opinion target satisfied a designed pattern, then 
there was an opinion relation between them. 
Experiments consistently reported that syntax-
based methods could yield better performance than 
adjacent methods for small or medium corpora 
(Zhang et al 2010). The performance of syntax-
based methods heavily depends on the parsing 
performance. However, online reviews are often 
informal texts (including grammar mistakes, typos, 
improper punctuations etc.). As a result, parsing 
may generate many mistakes. Thus, for large 
corpora from Web including a great deal of 
informal texts, these syntax-based methods may 
suffer from parsing errors and introduce many 
noises. Furthermore, this problem maybe more 
serious on non-English language reviews, such as 
Chinese reviews, because that the performances of 
parsing on these languages are often worse than 
that on English. 
To overcome the weakness of the two kinds of 
methods mentioned above, we propose a novel 
unsupervised approach to extract opinion targets 
by using word-based translation model (WTM). 
We formulate identifying opinion relations 
between opinion targets and opinion words as a 
word alignment task. We argue that an opinion 
target can find its corresponding modifier through 
monolingual word alignment. For example in 
Figure 1, the opinion words ?colorful? and 
?amazing? are aligned with the target ?screen? 
through word alignment. To this end, we use WTM 
to perform monolingual word alignment for mining 
associations between opinion targets and opinion 
words. In this process, several factors, such as 
word co-occurrence frequencies, word positions 
etc., can be considered globally. Compared with 
adjacent methods, WTM doesn?t identify opinion 
relations between words in a given window, so 
long-span relations can be effectively captured 
(Liu et al 2009). Compared with syntax-based 
methods, without using parsing, WTM can 
effectively avoid errors from parsing informal texts. 
So it will be more robust. In addition, by using 
WTM, our method can capture the ?one-to-many? 
or ?many-to-one? relations (?one-to-many? means 
that, in a sentence one opinion word modifies 
several opinion targets, and ?many-to-one? means 
several opinion words modify one opinion target). 
Thus, it?s reasonable to expect that WTM is likely 
to yield better performance than traditional 
methods for mining associations between opinion 
targets and opinion words.  
Based on the mined associations, we extract 
opinion targets in a ranking framework. All 
nouns/noun phrases are regarded as opinion target 
candidates. Then a graph-based algorithm is 
exploited to assign confidences to each candidate, 
in which candidate opinion relevance and 
importance are incorporated to generate a global 
measure. At last, the candidates with higher ranks 
are extracted as opinion targets. Compared with 
most traditional methods (Hu et al2004; Liu et al 
2005; Qiu et al 2011), we don?t extract opinion 
targets iteratively based on the bootstrapping 
strategy, such as Double Propagation (Qiu et al 
2011), instead all candidates are dynamically 
ranked in a global process. Therefore, error 
propagation can be effectively avoided and the 
performance can be improved.  
 
 Figure 1: Word-based translation model for 
opinion relation identification 
The main contributions of this paper are as 
follows. 
1) We formulate the opinion relation 
identification between opinion targets and 
opinion words as a word alignment task. To 
our best knowledge, none of previous methods 
deal with this task using monolingual word 
alignment model (in Section 3.1). 
Translation 
The phone has a colorful and even amazing screen 
The phone has a colorful and even amazing screen 
1347
2) We propose a graph-based algorithm for 
opinion target extraction in which candidate 
opinion relevance and importance are 
incorporated into a unified graph to estimate 
candidate confidence. Then the candidates 
with higher confidence scores are extracted as 
opinion targets (in Section 3.2). 
3) We have performed experiments on three 
datasets in different sizes and languages. The 
experimental results show that our approach 
can achieve performance improvement over 
the traditional methods. (in Section 4). 
The rest of the paper is organized as follows. In 
the next section, we will review related work in 
brief. Section 3 describes our approach in detail. 
Then experimental results will be given in Section 
4. At the same time, we will give some analysis 
about the results. Finally, we give the conclusion 
and the future work. 
2 Related Work 
Many studies have focused on the task of opinion 
target extraction, such as (Hu et al 2004; Ding et 
al., 2008; Liu et al 2006; Popescu et al 2005; 
Wu et al 2005; Wang et al 2008; Li et al 2010; 
Su et al 2008; Li et al 2006). In general, the 
existing approaches can be divided into two main 
categories: supervised and unsupervised methods. 
In supervised approaches, the opinion target 
extraction task was usually regarded as a sequence 
labeling task (Jin et al2009; Li et al2010; Wu et 
al., 2009; Ma et al2010; Zhang et al 2009). Jin et 
al. (2009) proposed a lexicalized HMM model to 
perform opinion mining. Li et al(2010) proposed 
a Skip-Tree CRF model for opinion target 
extraction. Their methods exploited three 
structures including linear-chain structure, 
syntactic structure, and conjunction structure. In 
addition, Wu et al(2009) utilized a SVM classifier 
to identify relations between opinion targets and 
opinion expressions by leveraging phrase 
dependency parsing. The main limitation of these 
supervised methods is that labeling training data 
for each domain is impracticable because of the 
diversity of the review domains.  
In unsupervised methods, most approaches 
regarded opinion words as the important indicators 
for opinion targets (Hu et al 2004; Popsecu et al 
2005; Wang et al 2008; Qiu et al 2011; Zhang et 
al., 2010). The basic idea was that reviewers often 
use the same opinion words when they comment 
on the similar opinion targets. The extraction 
procedure was often a bootstrapping process which 
extracted opinion words and opinion targets 
iteratively, depending upon their associations. 
Popsecu et al(2005) used syntactic patterns to 
extract opinion target candidates. After that they 
computed the point-wise mutual information (PMI) 
score between a candidate and a product category 
to refine the extracted results. Hu et al(2004) 
exploited an association rule mining algorithm and 
frequency information to extract frequent explicit 
product features. The adjective nearest to the 
frequent explicit feature was extracted as an 
opinion word. Then the extracted opinion words 
were used to extract infrequent opinion targets. 
Wang et al(2008) adopted the similar idea, but 
their method needed a few seeds to weakly 
supervise the extraction process. Qiu et al(2009, 
2011) proposed a Double Propagation method to 
expand a domain sentiment lexicon and an opinion 
target set iteratively. They exploited direct 
dependency relations between words to extract 
opinion targets and opinion words iteratively. The 
main limitation of Qiu?s method is that the patterns 
based on dependency parsing tree may introduce 
many noises for the large corpora (Zhang et al 
2010). Meanwhile, Double Propagation is a 
bootstrapping strategy which is a greedy process 
and has the problem of error propagation. Zhang et 
al. (2010) extended Qiu?s method. Besides the 
patterns used in Qiu?s method, they adopted some 
other patterns, such as phrase patterns, sentence 
patterns and ?no? pattern, to increase recall. In 
addition they used the HITS (Klernberg et al 1999) 
algorithm to compute the feature relevance scores, 
which were simply multiplied by the log of feature 
frequencies to rank the extracted opinion targets. In 
this way, the precision of result can be improved.  
3 Opinion Target Extraction Using 
Word-Based Translation Model 
3.1 Method Framework 
As mentioned in the first section, our approach for 
opinion target extraction is composed of the 
following two main components:  
1) Mining associations between opinion targets 
and opinion words: Given a collection of 
reviews, we adopt a word-based translation 
1348
model to identify potential opinion relations in 
all sentences, and then the associations 
between opinion targets and opinion words are 
estimated.  
2) Candidate confidence estimation: Based on 
these associations, we exploit a graph-based 
algorithm to compute the confidence of each 
opinion target candidate. Then the candidates 
with higher confidence scores are extracted as 
opinion targets.  
3.2 Mining associations between opinion 
targets and opinion words using Word-
based Translation Model 
This component is to identify potential opinion 
relations in sentences and estimate associations 
between opinion targets and opinion words. We 
assume opinion targets and opinion words 
respectively to be nouns/noun phrases and 
adjectives, which have been widely adopted in 
previous work (Hu et al 2004; Ding et al 2008; 
Wang et al 2008; Qiu et al 2011). Thus, our aim 
is to find potential opinion relations between 
nouns/noun phrases and adjectives in sentences, 
and calculate the associations between them. As 
mentioned in the first section, we formulate 
opinion relation identification as a word alignment 
task. We employ the word-based translation model 
(Brown et al1993) to perform monolingual word 
alignment, which has been widely used in many 
tasks, such as collocation extraction (Liu et al 
2009), question retrieval (Zhou et al 2011) and so 
on. In our method, every sentence is replicated to 
generate a parallel corpus, and we apply the 
bilingual word alignment algorithm to the 
monolingual scenario to align a noun/noun phase 
with its modifier. 
Given a sentence with n words 
1 2{ , ,..., }nS w w w? , the word alignment 
{( , ) | [1, ]}iA i a i n? ? can be obtained by 
maximizing the word alignment probability of the 
sentence as follows. 
?=arg max ( | )
A
A P A S
                   (1) 
where ( , )ii a  means that a noun/noun phrase at 
positioni  is aligned with an adjective at position ia . 
If we directly use this alignment model to our task, 
a noun/noun phrase may align with the irrelevant 
words other than adjectives, like prepositions or 
conjunctions and so on. Thus, in the alignment 
procedure, we introduce some constrains: 1) 
nouns/noun phrases (adjectives) must be aligned 
with adjectives (nouns/noun phrases) or null words; 
2) other words can only align with themselves. 
Totally, we employ the following 3 WTMs (IBM 
1~3) to identify opinion relations. 
1
1
( | ) ( | )j
n
IBM j a
j
P A S t w w?
?
??
 
2
1
( | ) ( | ) ( | , )j
n
IBM j a j
j
P A S t w w d j a n?
?
??
 
3
1 1
( | ) ( | ) ( | ) ( | , )j
n n
IBM i i j a j
i j
P A S n w t w w d j a n??
? ?
?? ?
(2) 
There are three main factors: ( | )jj at w w
, 
( | , )jd j a n
and ( | )i in w? , which respectively 
models different information.  
1) ( | )jj at w w
models the co-occurrence 
information of two words in corpora. If an 
adjective co-occurs with a noun/noun phrase 
frequently in the reviews, this adjective has high 
association with this noun/noun phrase. For 
example, in reviews of cell phone, ?big? often co-
occurs with ?phone?s size?, so ?big? has high 
association with ?phone?s size?. 
2) ( | , )jd j a l
 models word position information, 
which describes the probability of a word in 
position 
ja aligned with a word in position j .  
3) ( | )i in w? models the fertility of words, which 
describe the ability of a word for ?one-to-many? 
alignment. 
i? denotes the number of words that are 
aligned with 
iw . For example, ?Iphone4 has 
amazing screen and software?. In this sentence, 
?amazing? is used to modify two words: ?screen? 
and ?software?. So? equals to 2 for ?amazing?.  
Therefore, in Eq. (2), 
1( | )IBMP A S?  only models 
word co-occurrence information. 
2 ( | )IBMP A S?  
additionally employs word position information. 
Besides these two information, 
3( | )IBMP A S?  
considers the ability of a word for ?one-to-many? 
alignment. In the following experiments section, 
we will discuss the performance difference among 
these models in detail. Moreover, these models 
1349
may capture ?one-to-many? or ?many-to-one? 
opinion relations (mentioned in the first section). 
In our knowledge, it isn?t specifically considered 
by previous methods including adjacent methods 
and syntax-based methods. Meanwhile ? the 
alignment results may contain empty-word 
alignments, which means a noun/noun phrase has 
no modifier or an adjective modify nothing in the 
sentence. 
After gathering all word pairs from the review 
sentences, we can estimate the translation 
probabilities between nouns/noun phrases and 
adjectives as follows. 
( , )( | ) ( )
N A
N A
A
Count w wp w w Count w?
           (3) 
where ( | )N Ap w w means the translation 
probabilities from adjectives to nouns/noun 
phrases. Similarly, we can obtain translation 
probability ( | )A Np w w . Therefore, similar to (Liu 
et al2009), the association between a noun/noun 
phrase and an adjective is estimated as follows. 
1| |
( , )
( ( ) (1 ) ( ))
N A
N NA A
Association w w
t p w w t p w w ?? ? ?
    (4) 
where t is the harmonic factor to combine these 
two translation probabilities. In this paper, we set 
0.5t ? . For demonstration, we give some 
examples in Table 1. We can see that our method 
using WTM can successfully capture associations 
between opinion targets and opinion words. 
 battery life sound software 
wonderful 0.000 0.042 0.000 
poor 0.032 0.000 0.026 
long 0.025 0.000 0.000 
Table 1: Examples of associations between opinion 
targets and opinion words. 
3.3 Candidate Confidence Estimation 
In this component, we compute the confidence of 
each opinion target candidate and rank them. The 
candidates with higher confidence are regarded as 
the opinion targets. We argue that the confidence 
of a candidate is determined by two factors: 1) 
Opinion Relevance; 2) Candidate Importance. 
Opinion Relevance reflects the degree that a 
candidate is associated to opinion words. If an 
adjective has higher confidence to be an opinion 
word, the noun/noun phrase it modifies will have 
higher confidence to be an opinion target. 
Similarly, if a noun/noun phrase has higher 
confidence to be an opinion target, the adjective 
which modifies it will be highly possible to be an 
opinion word. It?s an iterative reinforcement 
process, which indicates that existing graph-based 
algorithms are applicable.  
Candidate Importance reflects the salience of a 
candidate in the corpus. We assign an importance 
score to an opinion target candidate f according to 
its -tf idf score, which is further normalized by the 
sum of -tf idf scores of all candidates. 
- ( )( )
- ( )
c
tf idf cImportance c
tf idf c
??
              (5) 
where c represents a candidate, tf is the term 
frequency in the dataset, and df is computed by 
using the Google n-gram corpus1. 
To model these two factors, a bipartite graph is 
constructed, the vertices of which include all 
nouns/noun phrases and adjectives. As shown in 
Figure 2, the white vertices represent nouns/noun 
phrases and the gray vertices represent adjectives. 
An edge between a noun/noun phrase and an 
adjective represents that there is an opinion 
relation between them. The weight on the edges 
represents the association between them, which are 
estimated by using WTM, as shown in Eq. (4).  
To estimate the confidence of each candidate on 
this bipartite graph, we exploit a graph-based 
algorithm, where we use C to represent candidate 
confidence vector, a 1n? vector. We set the 
candidate initial confidence with candidate 
importance score, i.e. 0C S? , where S is the 
candidate initial confidence vector and each item 
in S is computed using Eq. (5). 
 
 
Figure 2: Bipartite graph for modeling relations 
between opinion targets and opinion words 
                                                          
1 http://books.google.com/ngrams/datasets 
..... 
..... 
Opinion Word Candidates (adjectives) 
Opinion Target Candidates (nouns/noun phrases) 
1350
Then we compute the candidate confidence by 
using the following iterative formula. 
1t T tC M M C? ? ? ?                  (6) 
where tC is the candidate confidence vector at 
time t , and 1tC ?  is the candidate confidence 
vector at time 1t ? . M is an opinion relevance 
matrix, a m n? matrix, where ,i jM is the 
associated weight between a noun/noun phrase 
i and an adjective j . 
To consider the candidate importance scores, we 
introduce a reallocate condition: combining the 
candidate opinion relevance with the candidate 
importance at each step. Thus we can get the final 
recursive form of the candidate confidence as 
follows. 
1 (1 )t T tC M M C S? ?? ? ? ? ? ? ? ?       (7) 
where [0,1]?? is the proportion of candidate 
importance in the candidate confidence. When 
1? ? , the candidate confidence is completely 
determined by the candidate importance; and when 
0? ? , the candidate confidence is determined by 
the candidate opinion relevance. We will discuss 
its effect in the section of experiments.  
To solve Eq. (7), we rewrite it as the following 
form. 
1( (1 ) )TC I M M S? ? ?? ? ? ? ? ? ?        (8) 
where I is an identity matrix. To handle the 
inverse of the matrix, we expand the Eq. (8) as a 
power series as following. 
[ ]kC I B B S?? ? ? ? ? ?              (9) 
where (1 ) TB M M?? ? ? ?and [0, )k? ? is an 
approximate factor. In experiments, we set 
100k ? . Using this equation, we estimate 
confidences for opinion target candidates. The 
candidates with higher confidence scores than the 
threshold will be extracted as the opinion targets.  
4 Experiments 
4.1 Datasets and Evaluation Metrics 
In our experiments, we select three real world 
datasets to evaluate our approach. The first dataset 
is COAE2008 dataset22, which contains Chinese 
reviews of four different products. The detailed 
                                                          
2 http://ir-china.org.cn/coae2008.html 
information can be seen in Table 2. Moreover, to 
evaluate our method comprehensively, we collect a 
larger collection named by Large, which includes 
three corpora from three different domains and 
different languages. The detailed statistical 
information of this dataset is also shown in Table 2. 
Restaurant is crawled from the Chinese Web site: 
www.dianping.com. The Hotel and MP3 3  were 
used in (Wang et al 2011), which are respectively 
clawed from www.tripadvisor.com and 
www.amazon.com. For each collection, we 
perform random sampling to generate testing 
dataset, which include 6,000 sentences for each 
domain. Then the opinion targets in Large were 
manually annotated as the gold standard for 
evaluations. Three annotators are involved in the 
annotation process as follows. First, every 
noun/noun phrase and its contexts in review 
sentences are extracted. Then two annotators were 
required to judge whether every noun/noun phrase 
is opinion target or not. If a conflict happens, a 
third annotator will make judgment for finial 
results. The inter-agreement was 0.72. In total, we 
respectively obtain 1,112, 1,241 and 1,850 opinion 
targets in Hotel, MP3 and Restaurant. The third 
dataset is Customer Review Datasets 4  (English 
reviews of five products), which was also used in 
(Hu et al 2004; Qiu et al 2011). They have 
labeled opinion targets. The detailed information 
can be found in (Hu et al 2004).  
 
Domain Language #Sentence #Reviews 
Camera Chinese 2075 137 
Car Chinese 4783 157 
Laptop Chinese 1034 56 
Phone Chinese 2644 123 
(a) COAE2008 dataset2 
Domain Language #Sentence #Reviews 
Hotel English 1,855,351 185,829 
MP3 English 289,931 30,837 
Restaurant Chinese 1,683,129 395,124 
(b) Large 
Table 2: Experimental Data Sets, # denotes the size 
of the reviews/sentences 
In experiments, each review is segmented into 
sentences according to punctuations. Then 
sentences are tokenized and the part-of-speech of 
                                                          
3 http://sifaka.cs.uiuc.edu/~wang296/Data/index.html 
4 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
1351
Methods 
Camera Car Laptop Phone 
P R F P R F P R F P R F 
Hu 0.63 0.65 0.64 0.62 0.58 0.60 0.51 0.67 0.58 0.69 0.60 0.64 
DP 0.71 0.70 0.70 0.72 0.65 0.68 0.58 0.69 0.63 0.78 0.66 0.72 
Zhang 0.71 0.78 0.74 0.69 0.68 0.68 0.57 0.80 0.67 0.80 0.71 0.75 
Ours 0.75 0.81 0.78 0.71 0.71 0.71 0.61 0.85 0.71 0.83 0.74 0.78 
Table 3: Experiments on COAE2008 dataset2 
Methods 
Hotel MP3 Restaurant 
P R F P R F P R F 
Hu 0.60  0.65  0.62  0.61  0.68  0.64  0.64  0.69  0.66  
DP 0.67  0.69  0.68  0.69  0.70  0.69  0.74  0.72  0.73  
Zhang 0.67  0.76  0.71  0.67  0.77  0.72  0.75  0.79  0.77  
Ours 0.71  0.80  0.75  0.70  0.82  0.76  0.80  0.84  0.82  
Table 4: Experiments on Large 
Methods 
D1 D2 D3 D4 D5 
P R F P R F P R F P R F P R F 
Hu 0.75  0.82  0.78  0.71  0.79  0.75  0.72  0.76  0.74  0.69  0.82  0.75  0.74  0.80  0.77  
DP 0.87  0.81  0.84  0.90  0.81  0.85  0.90  0.86  0.88  0.81  0.84  0.82  0.92  0.86  0.89  
Zhang 0.83  0.84  0.83  0.86  0.85  0.85  0.86  0.88  0.87  0.80  0.85  0.83  0.86  0.86  0.86  
Ours 0.84  0.85  0.84  0.87  0.85  0.86  0.88  0.89  0.88  0.81  0.85  0.83  0.89  0.87  0.88  
Table 5: Experiments on Customer Review Dataset 
each word is assigned. Stanford NLP tool5 is used 
to perform POS-tagging and dependency parsing. 
The method in (Zhu et al 2009) is used to identify 
noun phrases. We select precision, recall and F-
measure as the evaluation metrics. We also 
perform a significant test, i.e., a t-test with a 
default significant level of 0.05. 
4.2 Our Methods vs. State-of-art Methods 
To prove the effectiveness of our method, we 
select the following state-of-art unsupervised 
methods as baselines for comparison. 
1) Hu is the method described in (Hu et al 2004), 
which extracted opinion targets by using adjacent 
rule.  
2) DP is the method described in (Qiu et al 2011), 
which used Double Propagation algorithm to 
extract opinion targets depending on syntactic 
relations between words.  
3) Zhang is the method described in (Zhang et al 
2010), which is an extension of DP. They extracted 
opinion targets candidates using syntactic patterns 
and other specific patterns. Then HITS (Kleinberg 
1999) algorithm combined with candidate 
frequency is employed to rank the results for 
opinion target extraction.  
Hu is selected to represent adjacent methods for 
opinion target extraction. And DP and Zhang are 
                                                          
5 http://nlp.stanford.edu/software/tagger.shtml 
selected to represent syntax-based methods. The 
parameter settings in these three baselines are the 
same as the original papers. In special, for DP and 
Zhang, we used the same patterns for different 
language reviews. The overall performance results 
are shown in Table 3, 4 and 5, respectively, where 
?P? denotes precision, ?R? denotes recall and ?F? 
denotes F-measure. Ours denotes full model of our 
method, in which we use IBM-3 model for 
identifying opinion relations between words. 
Moreover, we set
max 2? ? in Eq. (2) and 0.3? ? in 
Eq. (7). From results, we can make the following 
observations. 
1) Ours achieves performance improvement over 
other methods. This indicates that our method 
based on word-based translation model is 
effective for opinion target extraction.  
2) The graph-based methods (Ours and Zhang) 
outperform the methods using Double 
Propagation (DP). Similar observations have 
been made by Zhang et al(2010). The reason 
is that graph-based methods extract opinion 
targets in a global framework and they can 
effectively avoid the error propagation made 
by traditional methods based on Double 
Propagation. Moreover, Ours outperforms 
Zhang. We believe the reason is that Ours 
consider the opinion relevance and the 
candidate importance in a unified graph-based 
framework. By contrast, Zhang only simply 
1352
plus opinion relevance with frequency to 
determine the candidate confidence. 
3) In Table 4, the improvement made by Ours on 
Restaurant (Chinese reviews) is larger than 
that on Hotel and MP3 (English reviews). The 
same phenomenon can be found when we 
compare the improvement made by Ours in 
Table 3 (Chinese reviews) with that in Table 5 
(English reviews). We believe that reason is 
that syntactic patterns used in DP and Zhang 
were exploited based on English grammar, 
which may not be suitable to Chinese language. 
Moreover, another reason is that the 
performance of parsing on Chinese texts is not 
better than that on English texts, which will 
hurt the performance of syntax-based methods 
(DP and Zhang).  
4) Compared the results in Table 3 with the 
results in Table 4, we can observe that Ours 
obtains larger improvements with the increase 
of the data size. This indicates that our method 
is more effective for opinion target extraction 
than state-of-art methods, especially for large 
corpora. When the data size increase, the 
methods based on syntactic patterns will 
introduce more noises due to the parsing errors 
on informal texts. On the other side, Ours uses 
WTM other than parsing to identify opinion 
relations between words, and the noises made 
by inaccurate parsing can be avoided. Thus, 
Ours can outperform baselines. 
5) In Table 5, Ours makes comparable results 
with baselines in Customer Review Datasets, 
although there is a little loss in precision in 
some domains. We believe the reason is that 
the size of Customer Review Datasets is too 
small. As a result, WTM may suffer from data 
sparseness for association estimation. 
Nevertheless, the average recall is improved. 
An Example In Table 6, we show top 10 opinion 
targets extracted by Hu, DP, Zhang and Ours in 
MP3 of Large. In Hu and DP, since they didn?t 
rank the results, their results are ranked according 
to frequency in this experiment. The errors are 
marked in bold face. From these examples, we can 
see Ours extracts more correct opinion targets than 
others. In special, Ours outperforms Zhang. It 
indicates the effectiveness of our graph-based 
method for candidate confidence estimation. 
Moreover, Ours considers candidate importance 
besides opinion relevance, so some specific 
opinion targets are ranked to the fore, such as 
?voice recorder?, ?fm radio? and ?lcd screen?.  
4.3 Effect of Word-based Translation Model 
In this subsection, we aim to prove the 
effectiveness of our WTM for estimating 
associations between opinion targets and opinion 
words. For comparison, we select two baselines for 
comparison, named as Adjacent and Syntax. These 
baselines respectively use adjacent rule (Hu et al
2004; Wang et al 2008) and syntactic patterns 
(Qiu et al 2009) to identify opinion relations in 
sentences. Then the same method (Eq.3 and Eq.4) 
is used to estimate associations between opinion 
targets and opinion words. At last the same graph-
based method (in Section 3.3) is used to extract 
opinion targets. Due to the limitation of the space, 
the experimental results only on COAE2008 
dataset2 and Large are shown in Figure 3. 
 
 
Figure 3: Experimental comparison among 
different relation identification methods 
 
Hu quality, thing, drive, feature, battery, sound, 
time, music, price 
DP quality, battery, software, device, screen, file, 
thing, feature, battery life 
Zhang quality, size, battery life, hour, version, function, 
upgrade, number, music 
Ours quality, battery life, voice recorder, video, fm 
radio, battery, file system, screen, lcd screen 
Table 6: Top 10 opinion targets extracted by 
different methods. 
In Figure 3, we observe that Ours using WTM 
makes significant improvements compared with 
1353
two baselines, both on precision and recall. It 
indicates that WTM is effective for identifying 
opinion relations, which makes the estimation of 
the associations be more precise. 
4.4 Effect of Our Graph-based Method 
In this subsection, we aim to prove the 
effectiveness of our graph-based method for 
opinion target extraction. We design two baselines, 
named as WTM_DP and WTM_HITS. Both 
WTM_DP and WTM_HITS use WTM to mine 
associations between opinion targets and opinion 
words. Then, WTM_DP uses Double Propagation 
adapted in (Wang et al2008; Qiu et al2009) to 
extract opinion targets, which only consider the 
candidate opinion relevance. WTM_HITS uses a 
graph-based method of Zhang et al(2010) to 
extract opinion targets, which consider both 
candidate opinion relevance and frequency. Figure 
4 gives the experimental results on COAE2008 
dataset2 and Large. In Figure 4, we can observe 
that our graph-based algorithm outperforms not 
only the method based on Double Propagation, but 
also the previous graph-based approach.  
 
 
Figure 4: Experimental Comparison between 
different ranking algorithms 
4.5 Parameter Influences 
4.5.1 Effect of Different WTMs 
In section 3, we use three different WTMs in Eq. 
(2) to identify opinion relations. In this subsection, 
we make comparison among them. Experimental 
results on COAE2008 dataset2 and Large are 
shown in Figure 5. Ours_1, Ours_2 and Ours_3 
respectively denote our method using different 
WTMs (IBM 1~3). From the results in Figure 5, 
we observe that Ours_2 outperforms Ours_1, 
which indicates that word position is useful for 
identifying opinion relations. Furthermore, Ours_3 
outperforms other models, which indicates that 
considering the fertility of a word can produce 
better performance. 
4.5.2 Effect of ?  
In our method, when we employ Eq. (7) to assign 
confidence score to each candidate, 
[0,1]?? decides the proportion of candidate 
importance in our method. Due to the limitation of 
space, we only show the F-measure of Ours on 
COAE2008 dataset2 and Large when varying ? in 
Figure 6.  
In Figure 6, curves increase firstly, and decrease 
with the increase of ? . The best performance is 
obtained when ? is around 0.3. It indicates that 
candidate importance and candidate opinion 
relevance are both important for candidate 
confidence estimation. The performance of opinion 
target extraction benefits from their combination. 
 
 
 
Figure 5. Experimental results by using different 
word-based translation model. 
 
 
Figure 6. Experimental results when varying ?  
1354
5 Conclusions and Future Work 
This paper proposes a novel graph-based approach 
to extract opinion targets using WTM. Compared 
with previous adjacent methods and syntax-based 
methods, by using WTM, our method can capture 
opinion relations more precisely and therefore be 
more effective for opinion target extraction, 
especially for large informal Web corpora.  
In future work, we plan to use other word 
alignment methods, such as discriminative model 
(Liu et al 2010) for this task. Meanwhile, we will 
add some syntactic information into WTM to 
constrain the word alignment process, in order to 
identify opinion relations between words more 
precisely. Moreover, we believe that there are 
some verbs or nouns can be opinion words and 
they may be helpful for opinion target extraction. 
And we think that it?s useful to add some prior 
knowledge of opinion words (sentiment lexicon) in 
our model for estimating candidate opinion 
relevance. 
Acknowledgements 
The work is supported by the National Natural 
Science Foundation of China (Grant No. 
61070106), the National Basic Research Program 
of China (Grant No. 2012CB316300), Tsinghua 
National Laboratory for Information Science and 
Technology (TNList) Cross-discipline Foundation 
and the Opening Project of Beijing Key Laboratory 
of Internet Culture and Digital Dissemination 
Research (Grant No. 5026035403). We thank the 
anonymous reviewers for their insightful 
comments. 
 
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311.  
Xiaowen Ding, Bing Liu and Philip S. Yu. 2008. A 
Holistic Lexicon-Based Approach to Opinion Mining. 
In Proceedings of WSDM 2008. 
Xiaowen Ding and Bing Liu. 2010. Resolving Object 
and Attribute Reference in Opinion Mining. In 
Proceedings of COLING 2010. 
Mingqin Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. In Proceedings of 
KDD 2004 
Minqing Hu and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proceedings of 
AAAI-2004, San Jose, USA, July 2004. 
Wei Jin and Huang Hay Ho. A Novel Lexicalized 
HMM-based Learning Framework for Web Opinion 
Mining. In Proceedings of ICML 2009. 
Jon Klernberg. 1999. Authoritative Sources in 
Hyperlinked Environment. Journal of the ACM 46(5): 
604-632 
Zhuang Li, Feng Jing, Xiao-yan Zhu. 2006. Movie 
Review Mining and Summarization. In Proceedings 
of CIKM 2006 
Fangtao Li, Chao Han, Minlie Huang and Xiaoyan Zhu. 
2010. Structure-Aware Review Mining and 
Summarization. In Proceedings of COLING 2010. 
Zhichao Li, Min Zhang, Shaoping Ma, Bo Zhou, Yu 
Sun. Automatic Extraction for Product Feature 
Words from Comments on the Web. In Proceedings 
of AIRS 2009.  
Bing Liu, Hu Mingqing and Cheng Junsheng. 2005. 
Opinion Observer: Analyzing and Comparing 
Opinions on the Web. In Proceedings of WWW 2005 
Bing Liu. 2006. Web Data Mining: Exploring 
Hyperlinks, contents and usage data. Springer, 2006 
Bing Liu. 2010. Sentiment analysis and subjectivity. 
Handbook of Natural Language Processing, second 
edition, 2010. 
Yang Liu, Qun Liu, and Shouxun Lin. 2010. 
Discriminative word alignment by linear modeling. 
Computational Linguistics, 36(3):303?339. 
Zhanyi Liu, Haifeng Wang, Hua Wu and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Model. In Proceedings of EMNLP 
2009.  
Tengfei Ma and Xiaojun Wan. 2010. Opinion Target 
Extraction in Chinese News Comments. In 
Proceedings of COLING 2010. 
Popescu, Ana-Maria and Oren, Etzioni. 2005. 
Extracting produt fedatures and opinions from 
reviews. In Proceedings of EMNLP 2005 
Guang Qiu, Bing Liu., Jiajun Bu and Chun Che. 2009. 
Expanding Domain Sentiment Lexicon through 
Double Popagation. In Proceedings of IJCAI 2009 
Guang Qiu, Bing Liu, Jiajun Bu and Chun Chen. 2011. 
Opinion Word Expansion and Target Extraction 
1355
through Double Propagation. Computational 
Linguistics, March 2011, Vol. 37, No. 1: 9.27 
Qi Su, Xinying Xu., Honglei Guo, Zhili Guo, Xian Wu, 
Xiaoxun Zhang, Bin Swen and Zhong Su. 2008. 
Hidden Sentiment Association in Chinese Web 
Opinion Mining. In Proceedings of WWW 2008 
Bo Wang, Houfeng Wang. Bootstrapping both Product 
Features and Opinion Words from Chinese Customer 
Reviews with Cross-Inducing. In Proceedings of 
IJCNLP 2008. 
Hongning Wang, Yue Lu and Chengxiang Zhai. 2011. 
Latent Aspect Rating Analysis without Aspect 
Keyword Supervision. In Proceedings of KDD 2011. 
Yuanbin Wu, Qi Zhang, Xuangjing Huang and Lide 
Wu, 2009, Phrase Dependency Parsing For Opinion 
Mining, In Proceedings of EMNLP 2009 
Lei Zhang, Bing Liu, Suk Hwan Lim and Eamonn 
O?Brien-Strain. 2010. Extracting and Ranking 
Product Features in Opinion Documents. In 
Proceedings of COLING 2010. 
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara, 
Joseph Johnson, Xuanjing Huang. 2009. Mining 
Product Reviews Based on Shallow Dependency 
Parsing, In Proceedings of SIGIR 2009.  
Guangyou Zhou, Li Cai, Jun Zhao and Kang Liu. 2011. 
Phrase-based Translation Model for Question 
Retrieval in Community Question Answer Archives. 
In Proceedings of ACL 2011. 
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou and 
Muhua Zhu. 2009. Multi-aspect Opinion Polling 
from Textual Reviews. In Proceedings of CIKM 
2009. 
1356
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1092?1103,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Question Answering over Linked Data Using First-order Logic
?
Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{shizhu.he, kliu, yzzhang, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Question Answering over Linked Data
(QALD) aims to evaluate a question an-
swering system over structured data, the
key objective of which is to translate
questions posed using natural language
into structured queries. This technique
can help common users to directly ac-
cess open-structured knowledge on the
Web and, accordingly, has attracted much
attention. To this end, we propose a
novel method using first-order logic. We
formulate the knowledge for resolving
the ambiguities in the main three steps
of QALD (phrase detection, phrase-to-
semantic-item mapping and semantic item
grouping) as first-order logic clauses in a
Markov Logic Network. All clauses can
then produce interacted effects in a unified
framework and can jointly resolve all am-
biguities. Moreover, our method adopts a
pattern-learning strategy for semantic item
grouping. In this way, our method can
cover more text expressions and answer
more questions than previous methods us-
ing manually designed patterns. The ex-
perimental results using open benchmarks
demonstrate the effectiveness of the pro-
posed method.
1 Introduction
With the rapid development of the Web of Data,
many RDF datasets have been published as Linked
Data (Bizer et al., 2009), such as DBpedia (Auer
et al., 2007), Freebase (Bollacker et al., 2008)
and YAGO (Suchanek et al., 2007). The grow-
ing amount of Linked Data contains a wealth of
knowledge, including entities, classes and rela-
tions. Moreover, these linked data usually have
?
Shizhu He and Kang Liu have equal contribution to this
work.
complex structures and are highly heterogeneous.
As a result, there are gaps for users regarding ac-
cess. Although a few experts can write queries us-
ing structured languages (such as SPARQL) based
on their needs, this skill cannot be easily utilized
by common users (Christina and Freitas, 2014).
Thus, providing user-friendly, simple interfaces
to access these linked data becomes increasingly
more urgent.
Because of this, question answering over linked
data (QALD) (Walter et al., 2012) has recently
received much interest, and most studies on this
topic have focused on translating natural lan-
guage questions into structured queries (Freitas
and Curry, 2014; Yahya et al., 2012; Unger et al.,
2012; Shekarpour et al., 2013; Yahya et al., 2013;
Bao et al., 2014; Zou et al., 2014). For example,
with respect to the question
?Which software has been developed by organi-
zations founded in California, USA??,
the aim is to automatically convert this utterance
into an SPARQL query that contains the follow-
ing subject-property-object (SPO) triple format:
??url rdf:type dbo:Software, ?url dbo:developer ?x1,
?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace
dbr:California?
1
.
To fulfill this objective, existing systems (Lopez
et al., 2006; Unger et al., 2012; Yahya et al., 2012;
Zou et al., 2014) usually adopt a pipeline frame-
work that contains four major steps: 1) decompos-
ing the question and detecting phrases, 2) map-
ping the detected phrases into semantic items of
Linked Data, 3) grouping the mapped semantic
items into semantic triples, and 4) generating the
correct SPARQL query.
However, completing these four steps and con-
structing such a structured query is not easy. The
first three steps mentioned above are subject to the
1
The prefixes in semantic items indicate the source of
their vocabularies.
1092
problem of ambiguity, which is the major chal-
lenge in QALD. Using the question mentioned
above as an example, we can choose Califor-
nia or California, USA when detecting phrases,
the phrase California can be mapped to the en-
tity California State or California Film, and the class
Software (mapped from the phrase software) can
be matched with the first argument of the rela-
tion producer or developer (these two relations can
be mapped from the phrase developed). Previ-
ous methods (Lopez et al., 2006; Lehmann et
al., 2012; Freitas and Curry, 2014) have usu-
ally performed disambiguation at each step only,
and the subsequent step was performed based on
the disambiguation results in the previous step(s).
However, we argue that the three steps men-
tioned above have mutual effects. In the previ-
ous example, the phrase founded in (verb) can
be mapped to the entities (Founding of Rome and
Founder (company)), classes (Company and Depart-
ment) or relations (foundedBy and foundationPlace).
If we know that the phrase California can refer
to the entity California State, and which can be the
second argument of the relation foundationPlace,
together with a verb phrase being more likely
to be mapped to Relation, we should map the
phrase founded in to foundationPlace in this ques-
tion. Thus, we aim to determine if joint disam-
biguation is better than individual disambigua-
tion. (Question One)
In addition, previous systems usually employed
manually designed patterns to extract predicate-
argument structures that are used to guide the dis-
ambiguation process in the three steps mentioned
above (Yahya et al., 2012; Unger et al., 2012; Zou
et al., 2014). For example, (Yahya et al., 2012)
used only three dependency patterns to group the
mapped semantic items into semantic triples. Nev-
ertheless, these three manually designed patterns
miss many cases because of the diversity of the
question expressions. We gathered statistics on
144 questions and found that the macro-average
F1 and micro-average F1 of the three patterns
2
used in (Yahya et al., 2012) are only 62.8 and
66.2%, respectively. Furthermore, these specially
designed patterns may not be valid with variations
in domains or languages. Therefore, another im-
portant question arises: can we automatically
learn rules or patterns to achieve the same ob-
2
They are 1) verbs and their arguments, 2) adjectives and
their arguments and 3) propositionally modified tokens and
objects of prepositions.
jective? (Question Two)
Focusing on the two problems mentioned
above, this paper proposes a novel algorithm based
on a learning framework, Markov Logic Networks
(MLNs) (Richardson and Domingos, 2006), to
learn a joint model for constructing structured
queries from natural language utterances. MLN
is a statistical relational learning framework that
combines first-order logic and Markov networks.
The appealing property of MLN is that it is read-
ily interpretable by humans and that it is a natural
framework for performing joint learning. We for-
mulate the knowledge for resolving the ambigui-
ties in the main three steps of QALD (phrase de-
tection, phrase-to-semantic-item mapping and se-
mantic item grouping) as first-order logic clauses
in an MLN. In the framework of MLN, all clauses
will produce interacted effects that jointly resolve
all problems into a unified process. In this way,
the result in each step can be globally optimized.
Moreover, in contrast to previous methods, we
adopt a learning strategy to automatically learn
the patterns for semantic item grouping. We de-
sign several meta patterns as opposed to the spe-
cific patterns. In addition, these meta patterns are
formulated as the first-order logic formulas in the
MLN. The specific patterns can be generated by
these meta patterns based on the training data. The
model will learn the weights of each clause to de-
termine the most effective patterns for semantic
triple construction. In this way, with little effort,
our approach can cover more semantic expressions
and answer more questions than previous meth-
ods, which depend on manually designed patterns.
We evaluate the proposed method using several
benchmarks (QALD-1, QALD-3, QALD-4). The
experimental results demonstrate the advantage of
the joint disambiguation process mentioned above.
They also prove that our approach, employing
MLN to automatically learn the patterns of seman-
tic triple grouping, is effective. Our system can
answer more questions and obtain better perfor-
mance than the traditional methods based on man-
ually designed heuristic rules.
2 Background
2.1 Linked Data Sources
Linked Data consist of many relational data,
which are usually inter-linked as subject-property-
object (SPO) triple statements (such as using the
owl:sameAs relation). In this paper, we mainly use
1093
Subject(Arg1) Relation(Property) Object(Arg2) 
ProgrammingLanguage subClassOf Software 
Java_(programming_language) type Software 
Java_(programming_language) developer Oracle_Corporation 
Oracle_Corporation foundationPlace California_(State) 
foundationPlace domain Organisation 
California_(State) label ?California? 
California_(1977_film) label ?California? 
Oracle_Corporation numEmployees 118119(xsd:integer) 
 
Figure 1: Sample knowledge base facts.
DBpedia
3
and some classes from Yago
4
. These
knowledge bases (KBs) are composed of many on-
tological and instance statements, and all state-
ments are expressed by SPO triple facts. Figure
1 shows some triple fact samples from DBpedia.
Each fact is composed of three semantic items. A
semantic item can be an entity (California (State),
Oracle Corporation, etc.), a class (Software, Organ-
isation, etc.) or a relation (called a property
or predicate in some occasions). Some entities
are literals including strings, numbers and dates
(118119(xsd:integer), etc.). Relations contain stan-
dard Semantic Web relations (subClassOf, type, do-
main and label) and ontological relations (developer,
foundationPlace and numEmployees).
2.2 Task Statement
Given a knowledge base (KB), our objective is to
translate a natural language question q
NL
into a
formal language query q
FL
that targets the seman-
tic vocabularies given by the KB, and the query
q
FL
should capture the user information needs ex-
pressed by q
NL
.
Following (Yahya et al., 2012), we focus on the
factoid questions, and the answers to such ques-
tions are an entity or a set of entities. We ignore
the questions that need the aggregation
5
(max/min,
etc.) and negation operations. That is, we generate
queries that consist of a plentiful number of triple
patterns, which are multiple conjunctions of SPO
search conditions.
3 Framework
Figure 2 shows the entire framework of our system
for translating a question into a formal SPARQL
query. The first three steps address the input ques-
tion through 1) Phrase Detection (detecting pos-
sible phrases), 2) Phrase Mapping (mapping all
3
http://dbpedia.org/
4
http://www.mpi-inf.mpg.de/yago-naga/yago/
5
We can address the count query questions, which will
be explained in Section 3.
phrase candidates to the corresponding seman-
tic items), and 3) Feature Extraction (extracting
the linguistic features and semantic item features
from the question and the Linked Data, respec-
tively). As a result, a space of candidates is con-
structed, including possible phrases, mapped se-
mantic items and the possible argument match re-
lations among them. Next, the fourth step (In-
ference) formulates the joint disambiguation as a
generalized inference task. We employ rich fea-
tures and constraints (including hard and soft con-
straints) to infer a joint decision through an MLN.
Finally, with the inference results, we can con-
struct a semantic item query graph and generate
an executable SPARQL query. In the following
subsection, we demonstrate each step in detail.
1) Phrase detection. In this step, we detect
phrases (sequences of tokens) that probably indi-
cate semantic items in the KB. We do not use a
named entity recognizer (NER) because of its low
coverage. We perform testing on two commonly
used question corpora, QALD-3 and free917
6
, us-
ing the Stanford NER tool
7
. The results demon-
strate that only 51.5 and 23.8% of the NEs are
correctly recognized, respectively. To avoid miss-
ing useful phrases, we retain all n-grams as phrase
candidates, and then use some rules to filter them.
The rules include the following: the span length
must be less than 4 (accepting that all contiguous
tokens are capitalizations), the POS tag of the start
token must be jj, nn, rb and vb, all contiguous
capitalization tokens must not be split, etc. For
instance, software, developed by, organizations,
founded in and California are detected in the ex-
ample of the first section.
2) Phrase mapping. After the phrases are de-
tected, each phrase can be mapped to the corre-
sponding semantic item in KB (entity, class and
relation). For example, software is mapped to
dbo:Software, dbo:developer, etc., and California is
mapped to dbr:California, dbr:California (wine), etc.
For different types of semantic items, we use dif-
ferent techniques. For mapping phrases to en-
tities, considering that the entities in DBpedia
and Wikipedia are consistent, we employ anchor,
redirection and disambiguation information from
Wikipedia. For mapping phrases to classes, con-
sidering that classes have lexical variation, espe-
cially synonyms, e.g., dbo:Film can be mapped
6
http://www.cis.temple.edu/?yates/open-sem-
parsing/index.html
7
http://nlp.stanford.edu/software/CRF-NER.shtml
1094
Which software has been developed by
organizations founded in California, USA? software, developed, developed by, organizations,
founded, founded in, California, USA
software
developed by
...
...
...
California
phraseIndex
phrasePosTag
resourceType
priorMatchScore
hasMeanWord
phraseDepTag
hasRelatedness
...
isTypeCompatible
hasPhrase hasResource
hasRelation
Figure 2: Framework of our system.
from film, movie and show, we compute the simi-
larity between the phrase and the class in the KB
with the word2vec tool
8
. The word2vec tool com-
putes fixed-length vector representations of words
with a recurrent-neural-network based language
model (Mikolov et al., 2010). The similarity scor-
ing methods are introduced in Section 4.2. Then,
the top-N most similar classes for each phrase are
returned. For mapping phrases to relations, we
employ the resources from PATTY (Nakashole et
al., 2012) and ReVerb (Fader et al., 2011). Specif-
ically, we first compute the associations between
the ontological relations in DBpedia and the re-
lation patterns in PATTY and ReVerb through in-
stance alignments as in (Berant et al., 2013). Next,
if a detected phrase is matched to some relation
pattern, the corresponding ontological relations in
DBpedia will be returned as a candidate. This step
only generates candidates for every possible map-
ping, and the decision of the best selection will be
performed in the next step.
3) Feature extraction and joint inference.
There exist ambiguities in phrase detection and in
mapping phrases to semantic items. This step fo-
cuses on addressing these ambiguities and deter-
8
https://code.google.com/p/word2vec/
mining the argument match relations among the
mapped semantic items. This is the core compo-
nent of our system, and it performs disambigua-
tion in a unified manner. First, feature extraction
is performed to prepare a rich number of features
from the input question and from the KB. Next,
the disambiguation is performed in a joint fashion
with a Markov Logic Network. Detailed informa-
tion will be presented in Section 4.
4) Semantic item query graph construction.
Based on the inference results, we construct a
query graph. The vertices contain the following:
the detected phrase, the token span indexes of
the phrases, the mapped semantic items and their
types. The edge indicates the argument match re-
lation between two semantic items. For example,
we use 1 2 to indicate that the first argument of
an item matches the second argument of another
item
9
. The right bottom in Figure 2 shows an ex-
ample of this.
5) Query generation. The SPARQL queries
require the grouped triples of semantic items.
Thus, in this step, we convert a query graph
into multiple joined semantic triples. Three in-
terconnected semantic items, whereby it must
9
The other marks will be introduced in Section 4.2.
1095
be ensured that the middle item is a rela-
tion, are converted into a semantic triple (mul-
tiple joined facts containing variables). For
example, the query graph Vdbo:Book[Class] 1 2
??
dbo:author[Relation] 1 1
??
dbr:Danielle Steel[Entity]W is
converted into ??x rdf:type dbo:Book, dbr:Danielle
dbo:author ?x?, and Vdbo:populationTotal[Relation]
1 2
??
dbo:capital[Relation] 1 1
??
dbr:Australia[Entity]W
10
is
converted into ??x1 dbo:populationTotal ?answer, ?x1
dbo:capital dbr:Australia?. If the query graph only
contains one vertex that indicates a class ClassURI,
we generate ??x rdf:type ClassURI?. If the query
graph only contains two connected vertexes, we
append a variable to bind the missing match argu-
ment of the semantic item.
The final SPARQL query is constructed by join-
ing the semantic item triples based on the cor-
responding SPARQL template. We divide the
questions into three types: Yes/No, Normal and
Number. Yes/No questions use the ASK WHERE
template. Normal questions use the SELECT ?url
WHERE template. Number questions first use the
normal question template, and if they cannot ob-
tain a correct answer (a valid numeric value), we
use the SELECT COUNT(?url) WHERE template to
generate a query again. For instance, we construct
the SPARQL query SELECT(?url) WHERE{ ?url
rdf:type dbo:Software. ?url dbo:developer ?x1. ?x1 rdf:type
dbo:Company. ?x1 dbo:foundationPlace dbr:California.}
for this example.
4 Joint Disambiguation with MLN
In this section, we present our method for ques-
tion answering over linked data using a Markov
Logic Network (MLN). In the following subsec-
tions, we first briefly describe the MLN. Then, we
present the predicates and the first-order logic for-
mulas used in the model.
4.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic in a probabilistic framework
(Richardson and Domingos, 2006). An MLNM
consists of several weighted formulas {(?
i
, w
i
)}
i
,
where ?
i
is a first order formula and w
i
is the
penalty (the formula?s weight). In contrast to
the first-order logic, whereby a formula repre-
sents a hard constraint, these logic formulas are
relaxed and can be violated with penalties in the
10
This corresponds to the question ?How many people live
in the capital of Australia??
MLN. Each formula ?
i
consists of a set of first-
order predicates, logical connectors and variables.
These weighted formulas define a probability dis-
tribution over a possible world. Let y denote a pos-
sible world. Then p(y) is defined as follows:
p(y) =
1
Z
exp
?
?
?
(?
i
,w
i
)?M
w
i
?
c?C
n
?
i
f
?
i
c
(y)
?
?
,
where each c is a binding of the free variables in
?
i
to constants; f
?
i
c
is a binary feature function
that returns 1 if the ground formula that we ob-
tain through replacing the free variables in ?
i
with
the constants in c under the given possible world
y is true and is 0 otherwise; and C
n
?
i
is the set of
all possible bindings for the free variables in ?
i
.
Z is a normalized constant. The Markov network
corresponds to this distribution, where nodes rep-
resent ground atoms and factors represent ground
formulas.
4.2 Predicates
In the MLN, we design several predicates to re-
solve the ambiguities in phrase detection, map-
ping phrases to semantic items and semantic item
grouping. Specifically, we design a hidden pred-
icate hasPhrase(i) to indicate that the i-th candi-
date phrase has been chosen. The predicate hasRe-
source(i,j) indicates that the i-th phrase is mapped
to the j-th semantic item. The predicate hasRe-
lation(j,k,rr) indicates that the j-th semantic item
and the k-th semantic item should be grouped to-
gether with the argument-match-type rr. Note that
we define four argument match types between two
semantic items: 1 1, 1 2, 2 1 and 2 2. Here, the
argument match type t s denotes that the t-th argu-
ment of the first semantic item corresponds to the
s-th argument of the second semantic item
11
. The
detailed illustration is shown in Table 1.
Type Example Question
1 1 dbo:height 1 1 dbr:Michael Jordan How tall is Michael Jor-
dan?
1 2 dbo:River 1 2 dbo:crosses Which river does the
Brooklyn Bridge cross?
2 1 dbo:creator 2 1 dbr:Walt Disney Which television shows
were created by Walt
Disney?
2 2 dbo:birthPlace 2 2 dbo:capital Which actors were born in
the capital of American?
Table 1: Examples of the argument match types.
11
The 2-nd argument is corresponding to the object argu-
ment of the relation, and the 1-st argument is corresponding
with the subject argument of the relation and the entity (in-
cluding the class) itself.
1096
Describing the attributes of phrases and relation between two phrases
phraseIndex(p, i, j) The start and end position of phrase p in question.
phrasePosTag(p, pt) The POS tag of the head word in phrase p.
phraseDepTag(p, q, dt) The dependency path tags between phrase p and q.
phraseDepOne (p, q) If there is only one tag in the dependency path, the predicate is true.
hasMeanWord (p, q) If there is any one meaning word in the dependency path of two phrases, the predicate is true.
Describing the attributes of semantic item and the mappings between phrases and semantic items
resourceType(r, rt) The type of semantic item r. Types of semantic items include Entity, Class and Relation
priorMatchScore(p, r, s) The prior score of phrase p mapping to semantic item r.
Describing the attributes of relation between two semantic items in a knowledge base
hasRelatedness(p, q, s) The semantic coherence of semantic items.
isTypeCompatible(p, q, rr) If the semantic items p are type-compatible with the semantic items q, the predicate is true.
hasQueryResult(s, p, o, rr1, rr2) If the triple pattern consisting of semantic items s, p, o and argument-match-types rr1 and rr2 have query
results, the predicate is true.
Table 2: Descriptions of observed predicates.
Moreover, we define a set of observed predi-
cates to describe the properties of phrases, seman-
tic items, relations between phrases and relations
between semantic items. The observed predicates
and descriptions are shown in Table 2.
Previous methods usually designed some
heuristic patterns to group semantic items, which
usually employed a human-designed syntactic
path between two phrases to determine their re-
lations. In contrast, we collect all the tokens in
the dependency path between two phrases as pos-
sible patterns. The predicates phraseDepTag and
hasMeanWord are designed to indicate the possi-
ble patterns. Note that if these tokens only contain
POS tags dt|in|wdt|to|cc|ex|pos|wp or stop words,
the value of the predicate hasMeanWord is false;
otherwise, it is true. In this way, our system is ex-
pected to cover more question expressions. More-
over, the SPARQL endpoint is used to verify the
type compatibility of two semantic items and if
one triple pattern can obtain query results.
The predicate hasRelatedness needs to compute
the coherence score between two semantic items.
Following (Yahya et al., 2012), we use the Jaccard
coefficient (Jaccard, 1908) based on the inlinks be-
tween two semantic items.
The predicate priorMatchScore assigns a prior
score when mapping a phrase to a semantic item.
We use different methods to compute this score
according to different semantic item types. For
entities, we use a normalized score based on the
frequencies of a phrase referring to an entity.
For classes and relations, we use different meth-
ods. We first define the following three similar-
ity metrics: a) s
1
: The Levenshtein distance score
(Navarro, 2001) between the labels of the seman-
tic item and the phrase; b) s
2
: The word embed-
ding (Mikolov et al., 2010) score, which measures
the similarity between two phrases and is the max-
imum cosine value of the words? word embed-
dings between two phrases; and c) s
3
: the instance
overlap score, which is computed using the Jac-
card coefficient of the instance overlap. All scores
are normalized to produce a comparable scores
in the interval of (0, 1). The final prior scores
for mapping phrases to classes and relations are
?s
1
+ (1? ?)s
2
and ?s
1
+ ?s
2
+ (1? ?? ?)s
3
,
respectively. The parameters are set to empirical
values
12
.
4.3 Formulas
According to these predicates, we design several
first-order logic formulas for joint disambiguation.
As mentioned in the first section, these formulas
represent the meta patterns. The concrete pat-
terns can be generated through these meta pat-
terns with training data. Specifically, we use two
types of formulas for the joint decisions: Boolean
and Weighted formulas. Boolean formulas are
hard constraints, which must be satisfied by all
of the ground atoms in the final inference results.
Weighted formulas are soft constraints, which can
be violated with some penalties.
4.3.1 Boolean Formulas (Hard Constraints)
Table 3 lists the Boolean formulas used in this
work. The ? ? notation in the formulas indicates
an arbitrary constant. The ?|f |? notation expresses
the number of true grounded atoms in the formula
f . These formulas express the following con-
straints:
hf1: If a phrase is chosen, then it must have a
mapped semantic item;
hf2: If a semantic item is chosen, then its mapped
phrase must be chosen;
hf3: A phrase can be mapped to at most one se-
mantic item;
hf4: If the phrase is not chosen, then its mapped
12
Set ? to 0.6 for Class and set ? and ? to 0.3 and 0.3 for
Relation, respectively.
1097
hf1 hasPhrase(p)? hasResource(p, )
hf2 hasResource(p, )? hasPhrase(p)
hf3 |hasResource(p, )| ? 1
hf4 !hasPhrase(p)?!hasResource(p, r)
hf5 hasResource( , r)? hasRelation(r, , ) ? hasRelation( , r, )
hf6 |hasRelation(r1, r2, )| ? 1
hf7 hasRelation(r1, r2, )? hasResource( , r1) ? hasResource( , r2)
hf8 phraseIndex(p1, s1, e1) ? phraseIndex(p2, s2, e2) ? overlap(s1, e1, s2, e2) ? hasPhrase(p1)?!hasPhrase(p2)
hf9 resourceType(r, ?Entity?)?!hasRelation(r, , ?2 1?) ? !hasRelation(r, , ?2 2?)
hf10 resourceType(r, ?Entity?)?!hasRelation( , r, ?2 1?) ? !hasRelation(r, , ?2 2?)
hf11 resourceType(r, ?Class?)?!hasRelation(r, , ?2 1?) ? !hasRelation(r, , ?2 2?)
hf12 resourceType(r, ?Class?)?!hasRelation( , r, ?2 1?) ? !hasRelation(r, , ?2 2?)
hf13 !isTypeCompatible(r1, r2, rr)?!hasRelation(r1, r2, rr)
Table 3: Descriptions of Boolean formulas.
sf1 priorMatchScore(p, r, s)? hasPhrase(p)
sf2 priorMatchScore(p, r, s)? hasResource(p)
sf3 phrasePosTag(p, pt+) ? resourceType(r, rt+)? hasResource(p, r)
sf4 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2)? hasRelation(r1, r2, rr+)
sf5 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2)?!hasMeanWord(p1, p2) ?
hasRelation(r1, r2, rr+)
sf6 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2) ? phraseDepOne(p1, p2) ?
hasRelation(r1, r2, rr+)
sf7 hasRelatedness(r1, r2, s) ? hasResource( , r1) ? hasResource( , r2)? hasRelation(r1, r2, )
sf8 hasQueryResult(r1, r2, r3, rr1, rr2)? hasRelation(r1, r2, rr1) ? hasRelation(r2, r3, rr2)
Table 4: Descriptions of weighted formulas.
semantic item should not be chosen;
hf5: If a semantic item is chosen, then it should
have at least one argument match relation with
other semantic items;
hf6: Two semantic items have at most one argu-
ment match relation;
hf7: If an argument match relation for two seman-
tic items is chosen, then they must be chosen;
hf8: Each of two chosen phrases must not overlap;
hf9, hf10, hf11, hf12: The semantic item with
type Entity and Class should not have a second ar-
gument that matches with others;
hf13: The chosen argument match relation for two
sematic items must be type compatible.
4.3.2 Weighted Formulas (Soft Constraints)
Table 4 lists the weighted formulas used in this
work. The ?+? notation in the formulas indicates
that each constant of the logic variable should be
weighted separately. Those formulas express the
following properties in joint decisions:
sf1, sf2: The larger the score of the phrase map-
ping to a semantic item, the more likely the cor-
responding phrase and semantic item should been
chosen;
sf3: There are some associations between the POS
tags of phase and the types of mapped semantic
items;
sf4, sf5, sf6: There are some associations be-
tween the dependency tags in the dependency pat-
tern path of two phases and the types of argument
match relations of two mapped semantic items;
sh7: The larger the relatedness of two seman-
tic items, the more likely they have an argument
match relation;
sf8: If the triple pattern has query results, these se-
mantic items should have corresponding argument
match relations.
5 Experiments
5.1 Dataset & Evaluation Metrics
We use the following three collections of questions
from the QALD
13
task for question answering
over linked data: QALD-1, QALD-3 and QALD-
4. The generated SPARQL queries are evaluated
on Linked Data from DBpedia and YAGO using
a Virtuoso engine
14
. A typical example question
from the QALD benchmark is ?Which books writ-
ten by Kerouac were published by Viking Press??.
As mentioned in Section 2.2, our system is not de-
signed to answer questions that contain numbers,
date comparisons and aggregation operations such
as group by or order by. Therefore, we remove
these types of questions and retain 110 questions
from the QALD-4 training set for generating the
specific formulas and for training their weights in
MLN. We test our system using 37, 75 and 26
questions from the training set of QALD-1
15
, and
the testing set of QALD-3 and QALD-4 respec-
tively. We use #T, #Q and #A to indicate the total
13
www.sc.cit-ec.uni-bielefeld.de/qald/
14
https://github.com/openlink/virtuoso-opensource
15
We use the training set because we try to make a fair
comparison with (Yahya et al., 2012).
1098
number of questions in the testing set, the num-
ber of questions we could address and the number
of questions answered correct, respectively. We
select Precision (P =
#A
#Q
), Recall (R =
#A
#T
),
and F1-score (F1 =
2?P ?R
P+R
) as the evaluation met-
rics. To assess the effectiveness of the disambigua-
tion process in the MLN, we computed the overall
quality measures by precision and recall with the
manually obtained results.
5.2 Experimental Configurations
The Stanford dependency parser (De Marneffe et
al., 2006) is used for extracting features from the
dependency parse trees. We use the toolkit the-
beast
16
to learn the weights of the formulas and
to perform the MAP inference. The inference al-
gorithm uses a cutting plane approach. In addi-
tion, for the parameter learning, we set all ini-
tial weights to zero and use an online learning
algorithm with MIRA update rules to update the
weights of the formulas. The number of iterations
for the training and testing are set to 10 and 200,
respectively.
5.3 Results and Discussion
5.3.1 The Effect of Joint Learning
To demonstrate the advantages of our joint learn-
ing, we design a pipeline system for compari-
son, which independently performs phrase detec-
tion, phrase mapping, and semantic item grouping
by removing the unrelated formulas in MLN. For
example, the formulas
17
related to the predicates
hasResource and hasRelation are removed when
detecting phrases in questions.
Table 5 shows the results, where Joint de-
notes the proposed method with joint inference
and Pipeline denotes the compared method per-
forming each step independently. We perform a
comparison with the question answering results of
QALD (QA), and comparisons at each of the fol-
lowing steps: PD (phrase detection), PM (phrase
mapping) and MG (mapped semantic items group-
ing). From the results, we observe that our method
answers over half of the questions. Moreover, our
joint model based on MLN can obtain better per-
formance in question answering compared to the
pipeline system. We also observe that Joint ex-
hibits better performance than Pipeline in most
steps, except for MG in QALD-3. We believe this
16
http://code.google.com/p/thebeast
17
including entire formulas, excluding hf8 and sf1
is because the three tasks (phrase detection, phrase
mapping, and semantic item grouping) are con-
nected with each other. Each step can provide use-
ful information for the other two tasks. Therefore,
performing joint inference can effectively improve
the performance. Finally, we observe that the for-
mer task usually produces better results than the
subsequent tasks (phrase detection exhibits a bet-
ter performance than phrase mapping, and phrase
mapping exhibits a better performance than se-
mantic item grouping). The main reason is that
the latter subtask is more complex than the former
task. The decisions of the latter subtask strongly
rely on the former results even though they have
interacted effects.
5.3.2 The Effect of Pattern Learning
Table 6 shows a comparison of our system with
DEANNA (Yahya et al., 2012), which is based
on a joint disambiguation model but which em-
ploys hand-written patterns in its system. Because
DEANNA only reports its results of the QALD-1
dataset, we do not show the results for QALD-3
and QALD-4 for equity. From the results, we can
see that our system solved more questions and ex-
hibited a better performance than did DEANNA.
One of the greatest strengths of our system is that
the learning system can address more questions
than hand-written pattern rules.
System #T #Q #A P R F1
DEANNA (Yahya et al., 2012) 50 27 13 0.48 0.26 0.33
Ours 50 37 20 0.54 0.4 0.46
Table 6: Comparisons with DEANNA using the
QALD-1 test questions.
Compared to the ILP (Integer Linear Program-
ming) used in (Yahya et al., 2012) for joint disam-
biguation, we argue that there are two major dif-
ferences to our method. 1) Our method is a data-
driven approach that can learn effective patterns
or rules for the task. Therefore, it exhibits more
robustness and adaptability for various KBs. 2)
We design several meta rules in MLN as opposed
to specific ones. The specific rules can be gen-
erated by these meta rules based on the training
data. By contrast, the traditional approach using
ILP needs to set specific rules in advance, which
requires more intensive labor than our approach.
To further illustrate the effectiveness of our
pattern-learning strategy, we show the weights of
the learned patterns corresponding to formula sf3
in the MLN, as shown in Table 7. From the table,
1099
Benchmark
PD PM MG QA
P R F1 P R F1 P R F1 #T #Q #A P R F1
QALD-1(Joint) 0.93 0.981 0.955 0.895 0.944 0.919 0.703 0.813 0.754 50 37 20 0.54 0.4 0.46
QALD-1(Pipeine) 0.921 0.972 0.946 0.868 0.917 0.892 0.585 0.859 0.696 50 34 17 0.5 0.34 0.41
QALD-3(Joint) 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 99 75 45 0.6 0.46 0.52
QALD-3(Pipeline) 0.912 0.912 0.912 0.829 0.867 0.848 0.677 0.789 0.729 99 75 42 0.56 0.42 0.48
QALD-4(Joint) 0.947 0.978 0.963 0.937 0.967 0.952 0.776 0.865 0.817 50 26 15 0.58 0.3 0.4
QALD-4(Pipeline) 0.937 0.967 0.952 0.905 0.935 0.920 0.683 0.827 0.748 50 24 13 0.54 0.26 0.35
Table 5: The performance of joint learning on three benchmark datasets.
we can see that nn
18
is more likely mapped to En-
tity
19
than to Class and Relation, and vb is most
likely mapped to Relation. This proves that our
model can learn effective and reasonable patterns
for QALD.
POS tag of Phrase type of mapped Item Weight
nn Entity 2.11
nn Class 0.243
nn Relation 0.335
vb Relation 0.517
wp Class 0.143
wr Class 0.025
Table 7: Sample weights of formulas, correspond-
ing with formula sf3.
5.3.3 Comparison to the state of the art
To illustrate the effectiveness of the proposed
method, we perform comparisons to the state-of-
the-art methods. Table 8 shows the results using
QALD-3 and QALD-4. These systems are the
participants in the QALD evaluation campaigns.
From the results, we can see that our system out-
performs most systems at a competitive perfor-
mance. They further prove the effectiveness of the
proposed method.
Test set System #T #Q #A P R F1
QALD-3
CASIA (He et al.,
2013)
99 52 29 0.56 0.3 0.38
Scalewelis (Joris
and Ferr?e, 2013)
99 70 32 0.46 0.32 0.38
RTV (Cristina et
al., 2013)
99 55 30 0.55 0.3 0.39
Intui2 (Corina,
2013)
99 99 28 0.28 28 0.28
SWIP (Pradel et al.,
2013)
99 21 15 0.71 0.15 0.25
Ours 99 75 45 0.6 0.46 0.52
QALD-4
20
gAnswer 50 25 16 0.64 0.32 0.43
Intui3 50 33 10 0.30 0.2 0.24
ISOFT 50 50 10 0.2 0.2 0.2
RO FII 50 50 6 0.12 0.12 0.12
Ours 50 26 15 0.58 0.3 0.4
Table 8: Comparisons with state-of-the-art sys-
tems using the QALD benchmark.
18
The POS tag of the head word in the phrase
19
The type of semantic item
20
Because the QALD-4 conference does not start un-
til after submission, we have no citation for the state-of-
5.3.4 The Effect of Different Formulas
To determine which formulas are more useful for
QALD, we evaluate the performance of the pro-
posed method with different predicate sets. We
subtract one weighted formula from the original
sets at a time, except retaining the first two for-
mulas sf1 and sf2 for basic inference. Because of
space limitations, only the results using QALD-3
testing set are shown in Table 9.
From the results, we can observe that remov-
ing some formulas can boost the performance on
some single tasks, but employing all formulas can
produce the best performance. This illustrates that
solely resolving the steps in QALD (phrase detec-
tion, phrase mapping, semantic items grouping)
can obtain local results, and that making joint in-
ference is necessary and useful.
6 Related Work
Our proposed method is related to two lines of
work: Question Answering over Knowledge bases
and Markov Logic Networks.
Question answering over knowledge bases
has attracted a substantial amount of interest over
a long period of time. The initial attempts in-
cluded BaseBall (Green Jr et al., 1961) and Lu-
nar (Woods, 1977). However, these systems were
mostly limited to closed domains due to a lack of
knowledge resources. With the rapid development
of structured data, such as DBpedia, Freebase and
Yago, the need for providing user-friendly inter-
face to these data has become increasingly urgent.
Keyword (Elbassuoni and Blanco, 2011) and se-
mantic (Pound et al., 2010) searches are limited
to their ability to specify the relations among the
different keywords.
The open topic progress has also been pushed
by the QALD evaluation campaigns (Walter et al.,
2012). Lopez et al. (2011) gave a comprehensive
survey in this research area. The authors devel-
oped the PowerAqua system (Lopez et al., 2006) to
the-art systems in QALD-4. The results can be found at
http://greententacle.techfak.uni-bielefeld.de/ cunger/qald.
1100
Formulas
PD PM MG Avg
P R F1 P R F1 P R F1 P R F1
All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869
-sf3 0.931 0.927 0.929 0.877 0.913 0.895 0.637 0.816 0.715 0.834 0.897 0.864
-sf4 0.926 0.917 0.922 0.852 0.883 0.867 0.63 0.763 0.69 0.824 0.87 0.846
-sf5 0.931 0.927 0.929 0.873 0.908 0.89 0.633 0.816 0.713 0.831 0.895 0.862
-sf6 0.922 0.922 0.922 0.844 0.883 0.863 0.702 0.746 0.723 0.842 0.868 0.855
-sf7 0.931 0.917 0.924 0.881 0.908 0.894 0.621 0.763 0.685 0.833 0.88 0.856
-sf8 0.927 0.927 0.927 0.868 0.908 0.888 0.639 0.807 0.713 0.83 0.893 0.861
Table 9: Performance comparisons of different weighted formulas evaluated using the QALD-3 question
set.
answer questions on large, heterogeneous datasets.
For questions containing quantifiers, comparatives
or superlatives, Unger et al. (2012) translated
NL to FL using several SPARQL templates and
using a set of heuristic rules mapping phrases
to semantic items. The system most similar to
ours is DEANNA (Yahya et al., 2012). However,
DEANNA extracts predicate-argument structures
from the questions using three hand-written pat-
terns. Our system jointly learns these mappings
and extractions completely from scratch.
Recently, the Semantic Parsing (SP) community
targeted this problem from limited domains (Tang
and Mooney, 2001; Liang et al., 2013) to open do-
mains (Cai and Yates, 2013; Berant et al., 2013).
The methods in semantic parsing answer questions
by first converting natural language utterances into
meaningful representations (e.g., the lambda cal-
culus) and subsequently executing the formal log-
ical forms over KBs. Compared to deriving the
complete logical representation, our method aims
to parse a question into a limited logic form with
the semantic item query, which we believe is more
appropriate for answering factoid questions.
Markov Logic Networks have been widely
used in NLP tasks. Huang (2012) applied MLN
to compress sentences by formulating the task as a
word/phrase deletion problem. Fahrni and Strube
(2012) jointly disambiguated and clustered con-
cepts using MLN. MLN has also been used in
coreference resolution (Song et al., 2012). For
the task of identifying subjective text segments
and of extracting their corresponding explanations
from product reviews, Zhang et al. (2013) mod-
eled these segments with MLN. To discover log-
ical knowledge for deep question answering, Liu
(2012) used MLN to resolve the inconsistencies
of multiple knowledge bases.
Meza-Ruiz and Riedel (2009) employed MLN
for Semantic Role Labeling (SRL). They jointly
performed the following tasks for a sentence:
predicate identification, frame disambiguation, ar-
gument identification and argument classification.
The semantic analysis of SRL solely rested on
the lexical level, but our analysis focuses on the
knowledge-base level and aims to obtain an exe-
cutable query and to support natural language in-
ference.
7 Conclusions and Future Work
For the task of QALD, we present a joint learn-
ing framework for phrase detection, phrase map-
ping and semantic item grouping. The novelty of
our method lies in the fact that we perform joint
inference and pattern learning for all subtasks in
QALD using first-order logic. Our experimental
results demonstrate the effectiveness of the pro-
posed method.
In the future, we plan to address the follow-
ing limitations that still exist in the current sys-
tem: a) numerous hand-labeled data are required
for training the MLN, and we could use a la-
tent form of semantic item query graphs (Liang et
al., 2013); b) more robust solutions can be devel-
oped to find the implicit relations in questions; c)
our system can be scaled up to large-scale open-
domain knowledge bases (Fader et al., 2013; Yao
and Van Durme, 2014); and d) the learning system
has the advantage of being easily adapted to new
settings, and we plan to extend it to other domains
and languages (Liang and Potts, 2014).
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. This
work was sponsored by the National Basic Re-
search Program of China (No. 2014CB340503)
and the National Natural Science Foundation of
China (No. 61202329, 61272332), CCF-Tencent
Open Fund. This work was also supported in part
by Noahs Ark Lab of Huawei Tech. Ltm.
1101
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. In ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009. Linked data-the story so far. International
journal on semantic web and information systems,
5(3):1?22.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL.
Unger Christina and Andr Freitas. 2014. Question an-
swering over linked data: Challenges, approaches,
trends. In ESWC.
Dima Corina. 2013. Intui2: A prototype system
for question answering over linked data. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Giannone Cristina, Bellomaria Valentina, and Basili
Roberto. 2013. A hmm-based approach to question
answering against linked data. In Work. Multilin-
gual Question Answering over Linked Data (QALD-
3).
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In LREC.
Shady Elbassuoni and Roi Blanco. 2011. Keyword
search over rdf graphs. In CIKM.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with markov logic. In COLING.
Andre Freitas and Edward Curry. 2014. Natural
language queries over heterogeneous linked data
graphs: A distributional-compositional semantics
approach. In IUI.
Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference, pages 219?224. ACM.
Shizhu He, Shulin Liu, Yubo Chen, Guangyou Zhou,
Kang Liu, and Jun Zhao. 2013. Casia@qald-3:
A question answering system over linked data. In
Work. Multilingual Question Answering over Linked
Data (QALD-3).
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Paul. Jaccard. 1908. Nouvelles recherches sur la dis-
tribution florale. Bulletin de la Soci`ete Vaudense des
Sciences Naturelles, 44:223?270.
Guyonvarc?H Joris and S?ebastien Ferr?e. 2013.
Scalewelis: a scalable query-based faceted search
system on top of sparql endpoints. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Jens Lehmann, Tim Furche, Giovanni Grasso, Axel-
Cyrille Ngonga Ngomo, Christian Schallhart, An-
drew Sellers, Christina Unger, Lorenz B?uhmann,
Daniel Gerber, Konrad H?offner, et al. 2012. Deqa:
deep web extraction for question answering. In
ISWC.
Percy Liang and Christopher Potts. 2014. Bringing
machine learning and compositional semantics to-
gether. Annual Reviews of Linguistics (to appear).
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Zhao Liu, Xipeng Qiu, Ling Cao, and Xuanjing Huang.
2012. Discovering logical knowledge for deep ques-
tion answering. In CIKM.
Vanessa Lopez, Enrico Motta, and Victoria Uren.
2006. Poweraqua: Fishing the semantic web. In
The Semantic Web: research and applications, pages
393?410. Springer.
Vanessa Lopez, Victoria Uren, Marta Sabou, and En-
rico Motta. 2011. Is question answering fit for the
semantic web?: a survey. Semantic Web, 2(2):125?
155.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In NAACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In EMNLP.
1102
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88.
Jeffrey Pound, Ihab F Ilyas, and Grant Weddell. 2010.
Expressive and flexible access to web-extracted
data: a keyword-based structured query language.
In SIGMOD.
C Pradel, G Peyet, O Haemmerl?e, and N Hernandez.
2013. Swip at qald-3: results, criticisms and les-
son learned (working notes). In Work. Multilingual
Question Answering over Linked Data (QALD-3).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine learning, 62(1-
2):107?136.
Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,
and S?oren Auer. 2013. Question answering on in-
terlinked data. In WWW.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li,
and Houfeng Wang. 2012. Joint learning for coref-
erence resolution with markov logic. In EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of the 12th European Conference on Machine Learn-
ing, pages 466?477.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over rdf data. In WWW.
Sebastian Walter, Christina Unger, Philipp Cimiano,
and Daniel B?ar. 2012. Evaluation of a layered
approach to question answering over linked data.
In The Semantic Web?ISWC 2012, pages 362?374.
Springer.
William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. In Linguistic structures processing, pages
521?569.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In EMNLP.
Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,
and Gerhard Weikum. 2013. Robust question an-
swering over the web of linked data. In CIKM.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In ACL.
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xu-
anjing Huang. 2013. Discourse level explanatory
relation extraction from product reviews using first-
order logic. In ACL.
Lei Zou, Ruizhe Huang, Haixun WangZou, Jeffrey Xu
Yu, Wenqiang He, and Dongyan Zhao. 2014. Natu-
ral language question answering over rdf ? a graph
data driven approach. In SIGMOD.
1103
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1754?1763,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Syntactic Patterns versus Word Alignment: Extracting Opinion Targets
from Online Reviews
Kang Liu, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Mining opinion targets is a fundamen-
tal and important task for opinion min-
ing from online reviews. To this end,
there are usually two kinds of methods:
syntax based and alignment based meth-
ods. Syntax based methods usually ex-
ploited syntactic patterns to extract opin-
ion targets, which were however prone to
suffer from parsing errors when dealing
with online informal texts. In contrast,
alignment based methods used word align-
ment model to fulfill this task, which could
avoid parsing errors without using pars-
ing. However, there is no research fo-
cusing on which kind of method is more
better when given a certain amount of re-
views. To fill this gap, this paper empiri-
cally studies how the performance of these
two kinds of methods vary when chang-
ing the size, domain and language of the
corpus. We further combine syntactic pat-
terns with alignment model by using a par-
tially supervised framework and investi-
gate whether this combination is useful or
not. In our experiments, we verify that
our combination is effective on the corpus
with small and medium size.
1 Introduction
With the rapid development of Web 2.0, huge
amount of user reviews are springing up on the
Web. Mining opinions from these reviews be-
come more and more urgent since that customers
expect to obtain fine-grained information of prod-
ucts and manufacturers need to obtain immediate
feedbacks from customers. In opinion mining, ex-
tracting opinion targets is a basic subtask. It is
to extract a list of the objects which users express
their opinions on and can provide the prior infor-
mation of targets for opinion mining. So this task
has attracted many attentions. To extract opin-
ion targets, pervious approaches usually relied on
opinion words which are the words used to ex-
press the opinions (Hu and Liu, 2004a; Popescu
and Etzioni, 2005; Liu et al, 2005; Wang and
Wang, 2008; Qiu et al, 2011; Liu et al, 2012). In-
tuitively, opinion words often appear around and
modify opinion targets, and there are opinion re-
lations and associations between them. If we have
known some words to be opinion words, the words
which those opinion words modify will have high
probability to be opinion targets.
Therefore, identifying the aforementioned opin-
ion relations between words is important for ex-
tracting opinion targets from reviews. To fulfill
this aim, previous methods exploited the words
co-occurrence information to indicate them (Hu
and Liu, 2004a; Hu and Liu, 2004b). Obviously,
these methods cannot obtain precise extraction be-
cause of the diverse expressions by reviewers, like
long-span modified relations between words, etc.
To handle this problem, several methods exploited
syntactic information, where several heuristic pat-
terns based on syntactic parsing were designed
(Popescu and Etzioni, 2005; Qiu et al, 2009; Qiu
et al, 2011). However, the sentences in online
reviews usually have informal writing styles in-
cluding grammar mistakes, typos, improper punc-
tuation etc., which make parsing prone to gener-
ate mistakes. As a result, the syntax-based meth-
ods which heavily depended on the parsing per-
formance would suffer from parsing errors (Zhang
et al, 2010). To improve the extraction perfor-
mance, we can only employ some exquisite high-
precision patterns. But this strategy is likely to
miss many opinion targets and has lower recall
with the increase of corpus size. To resolve these
problems, Liu et al (2012) formulated identifying
opinion relations between words as an monolin-
gual alignment process. A word can find its cor-
responding modifiers by using a word alignment
1754
Figure 1: Mining Opinion Relations between Words using Partially Supervised Alignment Model
model (WAM). Without using syntactic parsing,
the noises from parsing errors can be effectively
avoided. Nevertheless, we notice that the align-
ment model is a statistical model which needs suf-
ficient data to estimate parameters. When the data
is insufficient, it would suffer from data sparseness
and may make the performance decline.
Thus, from the above analysis, we can observe
that the size of the corpus has impacts on these
two kinds of methods, which arises some impor-
tant questions: how can we make selection be-
tween syntax based methods and alignment based
method for opinion target extraction when given
a certain amount of reviews? And which kind of
methods can obtain better extraction performance
with the variation of the size of the dataset? Al-
though (Liu et al, 2012) had proved the effective-
ness of WAM, they mainly performed experiments
on the dataset with medium size. We are still curi-
ous about that when the size of dataset is larger
or smaller, can we obtain the same conclusion?
To our best knowledge, these problems have not
been studied before. Moreover, opinions may be
expressed in different ways with the variation of
the domain and language of the corpus. When the
domain or language of the corpus is changed, what
conclusions can we obtain? To answer these ques-
tions, in this paper, we adopt a unified framework
to extract opinion targets from reviews, in the key
component of which we vary the methods between
syntactic patterns and alignment model. Then we
run the whole framework on the corpus with dif-
ferent size (from #500 to #1, 000, 000), domain
(three domains) and language (Chinese and En-
glish) to empirically assess the performance varia-
tions and discuss which method is more effective.
Furthermore, this paper naturally addresses an-
other question: is it useful for opinion targets ex-
traction when we combine syntactic patterns and
word alignment model into a unified model? To
this end, we employ a partially supervised align-
ment model (PSWAM) like (Gao et al, 2010; Liu
et al, 2013). Based on the exquisitely designed
high-precision syntactic patterns, we can obtain
some precisely modified relations between words
in sentences, which provide a portion of links of
the full alignments. Then, these partial alignment
links can be regarded as the constrains for a stan-
dard unsupervised word alignment model. And
each target candidate would find its modifier un-
der the partial supervision. In this way, the er-
rors generated in standard unsupervised WAM can
be corrected. For example in Figure 1, ?kindly?
and ?courteous? are incorrectly regarded as the
modifiers for ?foods? if the WAM is performed
in an whole unsupervised framework. However,
by using some high-precision syntactic patterns,
we can assert ?courteous? should be aligned to
?services?, and ?delicious? should be aligned to
?foods?. Through combination under partial su-
pervision, we can see ?kindly? and ?courteous?
are correctly linked to ?services?. Thus, it?s rea-
sonable to expect to yield better performance than
traditional methods. As mentioned in (Liu et al,
2013), using PSWAM can not only inherit the
advantages of WAM: effectively avoiding noises
from syntactic parsing errors when dealing with
informal texts, but also can improve the mining
performance by using partial supervision. How-
ever, is this kind of combination always useful for
opinion target extraction? To access this problem,
we also make comparison between PSWAM based
method and the aforementioned methods in the
same corpora with different size, language and do-
main. The experimental results show the combina-
tion by using PSWAM can be effective on dataset
with small and medium size.
1755
2 Related Work
Opinion target extraction isn?t a new task for opin-
ion mining. There are much work focusing on
this task, such as (Hu and Liu, 2004b; Ding et al,
2008; Li et al, 2010; Popescu and Etzioni, 2005;
Wu et al, 2009). Totally, previous studies can be
divided into two main categories: supervised and
unsupervised methods.
In supervised approaches, the opinion target ex-
traction task was usually regarded as a sequence
labeling problem (Jin and Huang, 2009; Li et al,
2010; Ma and Wan, 2010; Wu et al, 2009; Zhang
et al, 2009). It?s not only to extract a lexicon or list
of opinion targets, but also to find out each opin-
ion target mentions in reviews. Thus, the contex-
tual words are usually selected as the features to
indicate opinion targets in sentences. And classi-
cal sequence labeling models are used to train the
extractor, such as CRFs (Li et al, 2010), HMM
(Jin and Huang, 2009) etc.. Jin et al (2009) pro-
posed a lexicalized HMM model to perform opin-
ion mining. Both Li et al (2010) and Ma et al
(2010) used CRFs model to extract opinion tar-
gets in reviews. Specially, Li et al proposed a
Skip-Tree CRF model for opinion target extrac-
tion, which exploited three structures including
linear-chain structure, syntactic structure, and con-
junction structure. However, the main limitation
of these supervised methods is the need of labeled
training data. If the labeled training data is insuf-
ficient, the trained model would have unsatisfied
extraction performance. Labeling sufficient train-
ing data is time and labor consuming. And for dif-
ferent domains, we need label data independently,
which is obviously impracticable.
Thus, many researches focused on unsupervised
methods, which are mainly to extract a list of opin-
ion targets from reviews. Similar to ours, most ap-
proaches regarded opinion words as the indicator
for opinion targets. (Hu and Liu, 2004a) regarded
the nearest adjective to an noun/noun phrase as
its modifier. Then it exploited an association
rule mining algorithm to mine the associations be-
tween them. Finally, the frequent explicit prod-
uct features can be extracted in a bootstrapping
process by further combining item?s frequency in
dataset. Only using nearest neighbor rule to mine
the modifier for each candidate cannot obtain pre-
cise results. Thus, (Popescu and Etzioni, 2005)
used syntax information to extract opinion targets,
which designed some syntactic patterns to capture
the modified relations between words. The experi-
mental results showed that their method had better
performance than (Hu and Liu, 2004a). Moreover,
(Qiu et al, 2011) proposed a Double Propagation
method to expand sentiment words and opinion
targets iteratively, where they also exploited syn-
tactic relations between words. Specially, (Qiu
et al, 2011) didn?t only design syntactic patterns
for capturing modified relations, but also designed
patterns for capturing relations among opinion tar-
gets and relations among opinion words. How-
ever, the main limitation of Qiu?s method is that
the patterns based on dependency parsing tree may
miss many targets for the large corpora. There-
fore, Zhang et al (2010) extended Qiu?s method.
Besides the patterns used in Qiu?s method, they
adopted some other special designed patterns to
increase recall. In addition they used the HITS
(Kleinberg, 1999) algorithm to compute opinion
target confidences to improve the precision. (Liu
et al, 2012) formulated identifying opinion re-
lations between words as an alignment process.
They used a completely unsupervised WAM to
capture opinion relations in sentences. Then the
opinion targets were extracted in a standard ran-
dom walk framework where two factors were con-
sidered: opinion relevance and target importance.
Their experimental results have shown that WAM
was more effective than traditional syntax-based
methods for this task. (Liu et al, 2013) extend
Liu?s method, which is similar to our method and
also used a partially supervised alignment model
to extract opinion targets from reviews. We notice
these two methods ((Liu et al, 2012) and (Liu et
al., 2013)) only performed experiments on the cor-
pora with a medium size. Although both of them
proved that WAM model is better than the meth-
ods based on syntactic patterns, they didn?t dis-
cuss the performance variation when dealing with
the corpora with different sizes, especially when
the size of the corpus is less than 1,000 and more
than 10,000. Based on their conclusions, we still
don?t know which kind of methods should be se-
lected for opinion target extraction when given a
certain amount of reviews.
3 Opinion Target Extraction
Methodology
To extract opinion targets from reviews, we adopt
the framework proposed by (Liu et al, 2012),
which is a graph-based extraction framework and
1756
has two main components as follows.
1) The first component is to capture opinion
relations in sentences and estimate associations
between opinion target candidates and potential
opinion words. In this paper, we assume opinion
targets to be nouns or noun phrases, and opinion
words may be adjectives or verbs, which are usu-
ally adopted by (Hu and Liu, 2004a; Qiu et al,
2011; Wang and Wang, 2008; Liu et al, 2012).
And a potential opinion relation is comprised of
an opinion target candidate and its corresponding
modified word.
2) The second component is to estimate the
confidence of each candidate. The candidates with
higher confidence scores than a threshold will be
extracted as opinion targets. In this procedure, we
formulate the associations between opinion target
candidates and potential opinion words in a bipar-
tite graph. A random walk based algorithm is em-
ployed on this graph to estimate the confidence of
each target candidate.
In this paper, we fix the method in the sec-
ond component and vary the algorithms in the
first component. In the first component, we re-
spectively use syntactic patterns and unsupervised
word alignment model (WAM) to capture opinion
relations. In addition, we employ a partially super-
vised word alignment model (PSWAM) to incor-
porate syntactic information into WAM. In exper-
iments, we run the whole framework on the differ-
ent corpora to discuss which method is more effec-
tive. In the following subsections, we will present
them in detail.
3.1 The First Component: Capturing
Opinion Relations and Estimating
Associations between Words
3.1.1 Syntactic Patterns
To capture opinion relations in sentences by using
syntactic patterns, we employ the manual designed
syntactic patterns proposed by (Qiu et al, 2011).
Similar to Qiu, only the syntactic patterns based
on the direct dependency are employed to guar-
antee the extraction qualities. The direct depen-
dency has two types. The first type indicates that
one word depends on the other word without any
additional words in their dependency path. The
second type denotes that two words both depend
on a third word directly. Specifically, we employ
Minipar1 to parse sentences. To further make syn-
1http://webdocs.cs.ualberta.ca/lindek/minipar.htm
tactic patterns precisely, we only use a few depen-
dency relation labels outputted by Minipar, such
as mod, pnmod, subj, desc etc. To make a clear
explanation, we give out some syntactic pattern
examples in Table 1. In these patterns, OC is a
potential opinion word which is an adjective or a
verb. TC is an opinion target candidate which is
a noun or noun phrase. The item on the arrows
means the dependency relation type. The item in
parenthesis denotes the part-of-speech of the other
word. In these examples, the first three patterns
are based on the first direct dependency type and
the last two patterns are based on the second direct
dependency type.
Pattern#1: <OC> mod????<TC>
Example: This phone has an amazing design
Pattern#2: <TC> obj???<OC>
Example: I like this phone very much
Pattern#3: <OC> pnmod?????<TC>
Example: the buttons easier to use
Pattern#4: <OC> mod????(NN) subj????<TC>
Example: IPhone is a revolutionary smart phone
Pattern#5: <OC> pred????(VBE) subj????<TC>
Example: The quality of LCD is good
Table 1: Some Examples of Used Syntactic Pat-
terns
3.1.2 Unsupervised Word Alignment Model
In this subsection, we present our method for cap-
turing opinion relations using unsupervised word
alignment model. Similar to (Liu et al, 2012),
every sentence in reviews is replicated to gener-
ate a parallel sentence pair, and the word align-
ment algorithm is applied to the monolingual sce-
nario to align a noun/noun phase with its modi-
fiers. We select IBM-3 model (Brown et al, 1993)
as the alignment model. Formally, given a sen-
tence S = {w1, w2, ..., wn}, we have
Pibm3(A|S)
?
N?
i=1
n(?i|wi)
N?
j=1
t(wj |waj )d(j|aj , N)
(1)
where t(wj |waj ) models the co-occurrence infor-
mation of two words in dataset. d(j|aj , n) mod-
els word position information, which describes the
probability of a word in position aj aligned with a
word in position j. And n(?i|wi) describes the
ability of a word for modifying (being modified
by) several words. ?i denotes the number of words
1757
that are aligned with wi. In our experiments, we
set ?i = 2.
Since we only have interests on capturing opin-
ion relations between words, we only pay at-
tentions on the alignments between opinion tar-
get candidates (nouns/noun phrases) and potential
opinion words (adjectives/verbs). If we directly
use the alignment model, a noun (noun phrase)
may align with other unrelated words, like prepo-
sitions or conjunctions and so on. Thus, we set
constrains on the model: 1) Alignment links must
be assigned among nouns/noun phrases, adjec-
tives/verbs and null words. Aligning to null words
means that this word has no modifier or modifies
nothing; 2) Other unrelated words can only align
with themselves.
3.1.3 Combining Syntax-based Method with
Alignment-based Method
In this subsection, we try to combine syntactic in-
formation with word alignment model. As men-
tioned in the first section, we adopt a partially
supervised alignment model to make this com-
bination. Here, the opinion relations obtained
through the high-precision syntactic patterns (Sec-
tion 3.1.1) are regarded as the ground truth and
can only provide a part of full alignments in sen-
tences. They are treated as the constrains for the
word alignment model. Given some partial align-
ment links A? = {(k, ak)|k ? [1, n], ak ? [1, n]},
the optimal word alignment A? = {(i, ai)|i ?
[1, n], ai ? [1, n]} can be obtained as A? =
argmax
A
P (A|S, A?), where (i, ai) means that a
noun (noun phrase) at position i is aligned with
its modifier at position ai.
Since the labeled data provided by syntactic pat-
terns is not a full alignment, we adopt a EM-based
algorithm, named as constrained hill-climbing al-
gorithm(Gao et al, 2010), to estimate the parame-
ters in the model. In the training process, the con-
strained hill-climbing algorithm can ensure that
the final model is marginalized on the partial align-
ment links. Particularly, in the E step, their method
aims to find out the alignments which are consis-
tent to the alignment links provided by syntactic
patterns, where there are main two steps involved.
1) Optimize towards the constraints. This step
aims to generate an initial alignments for align-
ment model (IBM-3 model in our method), which
can be close to the constraints. First, a simple
alignment model (IBM-1, IBM-2, HMM etc.) is
trained. Then, the evidence being inconsistent
to the partial alignment links will be got rid of
by using the move operator operator mi,j which
changes aj = i and the swap operator sj1,j2 which
exchanges aj1 and aj2 . The alignment is updated
iteratively until no additional inconsistent links
can be removed.
2) Towards the optimal alignment under the
constraints. This step aims to optimize towards
the optimal alignment under the constraints which
starts from the aforementioned initial alignments.
Gao et.al. (2010) set the corresponding cost value
of the invalid move or swap operation in M and
S to be negative, where M and S are respec-
tively called Moving Matrix and Swapping Ma-
trix, which record all possible move and swap
costs between two different alignments. In this
way, the invalid operators will never be picked
which can guarantee that the final alignment links
to have high probability to be consistent with the
partial alignment links provided by high-precision
syntactic patterns.
Then in M-step, evidences from the neighbor of
final alignments are collected so that we can pro-
duce the estimation of parameters for the next iter-
ation. In the process, those statistics which come
from inconsistent alignment links aren?t be picked
up. Thus, we have
P (wi|wai , A?)
=
{ ?, otherwise
P (wi|wai) + ?, inconsistent with A?(2)
where ? means that we make soft constraints on
the alignment model. As a result, we expect some
errors generated through high-precision patterns
(Section 3.1.1) may be revised in the alignment
process.
3.2 Estimating Associations between Words
After capturing opinion relations in sentences, we
can obtain a lot of word pairs, each of which is
comprised of an opinion target candidate and its
corresponding modified word. Then the condi-
tional probabilities between potential opinion tar-
get wt and potential opinion word wo can be es-
timated by using maximum likelihood estimation.
Thus, we have P (wt|wo) = Count(wt,wo)Count(wo) , where
Count(?) means the item?s frequency informa-
tion. P (wt|wo) means the conditional probabili-
ties between two words. At the same time, we can
obtain conditional probability P (wo|wt). Then,
1758
similar to (Liu et al, 2012), the association be-
tween an opinion target candidate and its modifier
is estimated as follows. Association(wt, wo) =
(?? P (wt|wo) + (1? ?)? P (wo|wt))?1, where
? is the harmonic factor. We set ? = 0.5 in our
experiments.
3.3 The Second Component: Estimating
Candidate Confidence
In the second component, we adopt a graph-based
algorithm used in (Liu et al, 2012) to compute
the confidence of each opinion target candidate,
and the candidates with higher confidence than the
threshold will be extracted as the opinion targets.
Here, opinion words are regarded as the impor-
tant indicators. We assume that two target candi-
dates are likely to belong to the similar category, if
they are modified by similar opinion words. Thus,
we can propagate the opinion target confidences
through opinion words.
To model the mined associations between
words, a bipartite graph is constructed, which
is defined as a weighted undirected graph G =
(V,E,W ). It contains two kinds of vertex: opin-
ion target candidates and potential opinion words,
respectively denoted as vt ? V and vo ? V .
As shown in Figure 2, the white vertices repre-
sent opinion target candidates and the gray ver-
tices represent potential opinion words. An edge
evt,vo ? E between vertices represents that there is
an opinion relation, and the weight w on the edge
represents the association between two words.
Figure 2: Modeling Opinion Relations between
Words in a Bipartite Graph
To estimate the confidence of each opinion tar-
get candidate, we employ a random walk algo-
rithm on our graph, which iteratively computes
the weighted average of opinion target confidences
from neighboring vertices. Thus we have
Ci+1 = (1? ?)?M ?MT ? Ci + ? ? I (3)
where Ci+1 and Ci respectively represent the
opinion target confidence vector in the (i + 1)th
and ith iteration. M is the matrix of word asso-
ciations, where Mi,j denotes the association be-
tween the opinion target candidate i and the po-
tential opinion word j. And I is defined as the
prior confidence of each candidate for opinion tar-
get. Similar to (Liu et al, 2012), we set each item
in Iv = tf(v)idf(v)?
v tf(v)idf(v)
, where tf(v) is the term fre-
quency of v in the corpus, and df(v) is computed
by using the Google n-gram corpus2. ? ? [0, 1]
represents the impact of candidate prior knowl-
edge on the final estimation results. In experi-
ments, we set ? = 0.4. The algorithm run un-
til convergence which is achieved when the confi-
dence on each node ceases to change in a tolerance
value.
4 Experiments
4.1 Datasets and Evaluation Metrics
In this section, to answer the questions men-
tioned in the first section, we collect a large
collection named as LARGE, which includes re-
views from three different domains and differ-
ent languages. This collection was also used
in (Liu et al, 2012). In the experiments, re-
views are first segmented into sentences accord-
ing to punctuation. The detailed statistical in-
formation of the used collection is shown in Ta-
ble 2, where Restaurant is crawled from the Chi-
nese Web site: www.dianping.com. The Hotel and
MP3 are used in (Wang et al, 2011), which are re-
spectively crawled from www.tripadvisor.com and
www.amazon.com. For each dataset, we perform
random sampling to generate testing set with dif-
ferent sizes, where we use sampled subsets with
#sentences = 5? 102, 103, 5? 103, 104, 5?
104, 105 and 106 sentences respectively. Each
Domain Language Sentence Reviews
Restaurant Chinese 1,683,129 395,124
Hotel English 1,855,351 185,829
MP3 English 289,931 30,837
Table 2: Experimental Dataset
sentence is tokenized, part-of-speech tagged by
using Stanford NLP tool3, and parsed by using
Minipar toolkit. And the method of (Zhu et al,
2009) is used to identify noun phrases.
2http://books.google.com/ngrams/datasets
3http://nlp.stanford.edu/software/tagger.shtml
1759
We select precision and recall as the metrics.
Specifically, to obtain the ground truth, we man-
ually label all opinion targets for each subset. In
this process, three annotators are involved. First,
every noun/noun phrase and its contexts in review
sentences are extracted. Then two annotators were
required to judge whether every noun/noun phrase
is opinion target or not. If a conflict happens, a
third annotator will make judgment for final re-
sults. The average inter-agreements is 0.74. We
also perform a significant test, i.e., a t-test with a
default significant level of 0.05.
4.2 Compared Methods
We select three methods for comparison as fol-
lows.
? Syntax: It uses syntactic patterns mentioned
in Section 3.1.1 in the first component to
capture opinion relations in reviews. Then
the associations between words are estimated
and the graph based algorithm proposed in
the second component (Section 3.3) is per-
formed to extract opinion targets.
? WAM: It is similar to Syntax, where the only
difference is that WAM uses unsupervised
WAM (Section 3.1.2) to capture opinion re-
lations.
? PSWAM is similar to Syntax and WAM,
where the difference is that PSWAM uses the
method mentioned in Section 3.1.3 to capture
opinion relations, which incorporates syntac-
tic information into word alignment model by
using partially supervised framework.
The experimental results on different domains are
respectively shown in Figure 3, 4 and 5.
4.3 Syntax based Methods vs. Alignment
based Methods
Comparing Syntax with WAM and PSWAM, we
can obtain the following observations:
Figure 3: Experimental results on Restaurant
Figure 4: Experimental results on Hotel
Figure 5: Experimental results on MP3
1) When the size of the corpus is small, Syntax
has better precision than alignment based meth-
ods (WAM and PSWAM). We believe the reason
is that the high-precision syntactic patterns em-
ployed in Syntax can effectively capture opinion
relations in a small amount of texts. In contrast,
the methods based on word alignment model may
suffer from data sparseness for parameter estima-
tion, so the precision is lower.
2) However, when the size of the corpus in-
creases, the precision of Syntax decreases, even
worse than alignment based methods. We believe
it?s because more noises were introduced from
parsing errors with the increase of the size of the
corpus , which will have more negative impacts on
extraction results. In contrast, for estimating the
parameters of alignment based methods, the data
is more sufficient, so the precision is better com-
pared with syntax based method.
3) We also observe that recall of Syntax is
worse than other two methods. It?s because the
human expressions of opinions are diverse and the
manual designed syntactic patterns are limited to
capture all opinion relations in sentences, which
may miss an amount of correct opinion targets.
4) It?s interesting that the performance gap be-
tween these three methods is smaller with the in-
crease of the size of the corpus (more than 50,000).
We guess the reason is that when the data is suffi-
cient enough, we can obtain sufficient statistics for
each opinion target. In such situation, the graph-
based ranking algorithm in the second component
will be apt to be affected by the frequency infor-
mation, so the final performance could not be sen-
sitive to the performance of opinion relations iden-
1760
tification in the first component. Thus, in this situ-
ation, we can get conclusion that there is no obvi-
ously difference on performance between syntax-
based approach and alignment-based approach.
5) From the results on dataset with different lan-
guages and different domains, we can obtain the
similar observations. It indicates that choosing ei-
ther syntactic patterns or word alignment model
for extracting opinion targets can take a few con-
sideration on the language and domain of the cor-
pus.
Thus, based on the above observations, we can
draw the following conclusions: making chooses
between different methods is only related to the
size of the corpus. The method based on syn-
tactic patterns is more suitable for small cor-
pus (#sentences < 5 ? 103 shown in our
experiments). And word alignment model is
more suitable for medium corpus (5 ? 103 <
#sentences < 5 ? 104). Moreover, when the
size of the corpus is big enough, the performance
of two kinds of methods tend to become the same
(#sentences ? 105 shown in our experiments).
4.4 Is It Useful Combining Syntactic Patterns
with Word Alignment Model
In this subsection, we try to see whether combin-
ing syntactic information with alignment model by
using PSWAM is effective or not for opinion tar-
get extraction. From the results in Figure 3, 4 and
5, we can see that PSWAM has the similar recall
compared with WAM in all datasets. PSWAM
outperforms WAM on precision in all dataset. But
the precision gap between PSWAM and WAM
decreases when the size of the corpus increases.
When the size is larger than 5 ? 104, the perfor-
mance of these two methods is almost the same.
We guess the reason is that more noises from pars-
ing errors will be introduced by syntactic patterns
with the increase of the size of corpus , which have
negative impacts on alignment performance. At
the same time, as mentioned above, a great deal of
reviews will bring sufficient statistics for estimat-
ing parameters in alignment model, so the roles
of partial supervision from syntactic information
will be covered by frequency information used in
our graph based ranking algorithm.
Compared with State-of-the-art Methods.
However, it?s not say that this combination is
not useful. From the results, we still see that
PSWAM outperforms WAM in all datasets on
precision when size of corpus is smaller than
5 ? 104. To further prove the effectiveness of
our combination, we compare PSWAM with some
state-of-the-art methods, including Hu (Hu and
Liu, 2004a), which extracted frequent opinion tar-
get words based on association mining rules, DP
(Qiu et al, 2011), which extracted opinion tar-
gets through syntactic patterns, and LIU (Liu et
al., 2012), which fulfilled this task by using un-
supervised WAM. The parameter settings in these
baselines are the same as the settings in the orig-
inal papers. Because of the space limitation, we
only show the results on Restaurant and Hotel, as
shown in Figure 6 and 7.
Figure 6: Compared with the State-of-the-art
Methods on Restaurant
Figure 7: Compared with the State-of-the-art
Methods on Hotel
From the experimental results, we can obtain
the following observations. PSWAM outperforms
other methods in most datasets. This indicates
that our method based on PSWAM is effective
for opinion target extraction. Especially compared
PSWAM with LIU, both of which are based on
word alignment model, we can see PSWAM iden-
tifies opinion relations by performing WAM under
partial supervision, which can effectively improve
the precision when dealing with small and medium
corpus. However, these improvements are limited
when the size of the corpus increases, which has
the similar observations obtained above.
The Impact of Syntactic Information on
Word Alignment Model. Although we have
prove the effectiveness of PSWAM in the corpus
with small and medium size, we are still curious
about how the performance varies when we incor-
1761
porate different amount of syntactic information
into WAM. In this experiment, we rank the used
syntactic patterns mentioned in Section 3.1.1 ac-
cording to the quantities of the extracted alignment
links by these patterns. Then, to capture opin-
ion relations, we respectively use top N syntactic
patterns according to frequency mentioned above
to generate partial alignment links for PSWAM in
section 3.1.3. We respectively define N=[1,7]. The
larger is N , the more syntactic information is in-
corporated. Because of the space limitation, only
the average performance of all dataset is shown in
Figure 8.
Figure 8: The Impacts of Different Syntactic In-
formation on Word Alignment Model
In Figure 8, we can observe that the syntactic in-
formation mainly have effect on precision. When
the size of the corpus is small, the opinion rela-
tions mined by high-precision syntactic patterns
are usually correct, so incorporating more syntac-
tic information can improve the precision of word
alignment model more. However, when the size of
the corpus increases, incorporating more syntactic
information has little impact on precision.
5 Conclusions and Future Work
This paper discusses the performance variation of
syntax based methods and alignment based meth-
ods on opinion target extraction task for the dataset
with different sizes, different languages and dif-
ferent domains. Through experimental results, we
can see that choosing which method is not related
with corpus domain and language, but strongly
associated with the size of the corpus . We can
conclude that syntax-based method is likely to be
more effective when the size of the corpus is small,
and alignment-based methods are more useful for
the medium size corpus. We further verify that in-
corporating syntactic information into word align-
ment model by using PSWAM is effective when
dealing with the corpora with small or medium
size. When the size of the corpus is larger and
larger, the performance gap between syntax based,
WAM and PSWAM will decrease.
In future work, we will extract opinion targets
based on not only opinion relations. Other seman-
tic relations, such as the topical associations be-
tween opinion targets (or opinion words) should
also be employed. We believe that considering
multiple semantic associations will help to im-
prove the performance. In this way, how to model
heterogenous relations in a unified model for opin-
ion targets extraction is worthy to be studied.
Acknowledgement
This work was supported by the National Natu-
ral Science Foundation of China (No. 61070106,
No. 61272332 and No. 61202329), the Na-
tional High Technology Development 863 Pro-
gram of China (No. 2012AA011102), the Na-
tional Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the Conference on Web Search and
Web Data Mining (WSDM).
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 1?10, Uppsala, Sweden,
July. Association for Computational Linguistics.
1762
Mingqin Hu and Bing Liu. 2004a. Mining opinion fea-
tures in customer reviews. In Proceedings of Con-
ference on Artificial Intelligence (AAAI).
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Wei Jin and Hay Ho Huang. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of International Confer-
ence on Machine Learning (ICML).
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Chu-Ren Huang and Dan Jurafsky, editors, COL-
ING, pages 653?661. Tsinghua University Press.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Allan Ellis and Tatsuya Hagino,
editors, WWW, pages 342?351. ACM.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013.
Opinion target extraction using partially supervised
word alignment model.
Tengfei Ma and Xiaojun Wan. 2010. Opinion tar-
get extraction in chinese news comments. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 782?790. Chinese Information Pro-
cessing Society of China.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 339?346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation.
Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Bo Wang and Houfeng Wang. 2008. Bootstrapping
both product features and opinion words from chi-
nese customer reviews with cross-inducing.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Chid Apt, Joydeep Ghosh,
and Padhraic Smyth, editors, KDD, pages 618?626.
ACM.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ?09, pages
726?727, New York, NY, USA. ACM.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking
product features in opinion documents. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 1462?1470. Chinese Information
Processing Society of China.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In David Wai-Lok Cheung,
Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and
Jimmy J. Lin, editors, CIKM, pages 1799?1802.
ACM.
1763
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1764?1773,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Opinion Words and Opinion Targets in a Two-Stage Framework
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, ybchen, jzhao}@nlpr.ia.ac.cn
Abstract
This paper proposes a novel two-stage
method for mining opinion words and
opinion targets. In the first stage, we
propose a Sentiment Graph Walking algo-
rithm, which naturally incorporates syn-
tactic patterns in a Sentiment Graph to ex-
tract opinion word/target candidates. Then
random walking is employed to estimate
confidence of candidates, which improves
extraction accuracy by considering confi-
dence of patterns. In the second stage, we
adopt a self-learning strategy to refine the
results from the first stage, especially for
filtering out high-frequency noise terms
and capturing the long-tail terms, which
are not investigated by previous meth-
ods. The experimental results on three real
world datasets demonstrate the effective-
ness of our approach compared with state-
of-the-art unsupervised methods.
1 Introduction
Opinion mining not only assists users to make in-
formed purchase decisions, but also helps busi-
ness organizations understand and act upon cus-
tomer feedbacks on their products or services in
real-time. Extracting opinion words and opinion
targets are two key tasks in opinion mining. Opin-
ion words refer to those terms indicating positive
or negative sentiment. Opinion targets represent
aspects or attributes of objects toward which opin-
ions are expressed. Mining these terms from re-
views of a specific domain allows a more thorough
understanding of customers? opinions.
Opinion words and opinion targets often co-
occur in reviews and there exist modified relations
(called opinion relation in this paper) between
them. For example, in the sentence ?It has a clear
screen?, ?clear? is an opinion word and ?screen? is
an opinion target, and there is an opinion relation
between the two words. It is natural to identify
such opinion relations through common syntactic
patterns (also called opinion patterns in this pa-
per) between opinion words and targets. For ex-
ample, we can extract ?clear? and ?screen? by us-
ing a syntactic pattern ?Adj-{mod}-Noun?, which
captures the opinion relation between them. Al-
though previous works have shown the effective-
ness of syntactic patterns for this task (Qiu et al,
2009; Zhang et al, 2010), they still have some lim-
itations as follows.
False Opinion Relations: As an example, the
phrase ?everyday at school? can be matched by
a pattern ?Adj-{mod}-(Prep)-{pcomp-n}-Noun?,
but it doesn?t bear any sentiment orientation. We
call such relations that match opinion patterns but
express no opinion false opinion relations. Pre-
vious pattern learning algorithms (Zhuang et al,
2006; Kessler and Nicolov, 2009; Jijkoun et al,
2010) often extract opinion patterns by frequency.
However, some high-frequency syntactic patterns
can have very poor precision (Kessler and Nicolov,
2009).
False Opinion Targets: In another case, the
phrase ?wonderful time? can be matched by
an opinion pattern ?Adj-{mod}-Noun?, which is
widely used in previous works (Popescu and Et-
zioni, 2005; Qiu et al, 2009). As can be seen, this
phrase does express a positive opinion but unfortu-
nately ?time? is not a valid opinion target for most
domains such as MP3. Thus, false opinion targets
are extracted. Due to the lack of ground-truth
knowledge for opinion targets, non-target terms
introduced in this way can be hardly filtered out.
Long-tail Opinion Targets: We further no-
tice that previous works prone to extract opinion
targets with high frequency (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Qiu et al, 2009; Zhu
et al, 2009), and they often have difficulty in iden-
tifying the infrequent or long-tail opinion targets.
1764
To address the problems stated above, this pa-
per proposes a two-stage framework for mining
opinion words and opinion targets. The under-
lying motivation is analogous to the novel idea
?Mine the Easy, Classify the Hard? (Dasgupta and
Ng, 2009). In our first stage, we propose a Senti-
ment Graph Walking algorithm to cope with the
false opinion relation problem, which mines easy
cases of opinion words/targets. We speculate that
it may be helpful to introduce a confidence score
for each pattern. Concretely, we create a Sen-
timent Graph to model opinion relations among
opinion word/target/pattern candidates and apply
random walking to estimate confidence of them.
Thus, confidence of pattern is considered in a uni-
fied process. Patterns that often extract false opin-
ion relations will have low confidence, and terms
introduced by low-confidence patterns will also
have low confidence accordingly. This could po-
tentially improve the extraction accuracy.
In the second stage, we identify the hard cases,
which aims to filter out false opinion targets and
extract long-tail opinion targets. Previous super-
vised methods have been shown to achieve state-
of-the-art results for this task (Wu et al, 2009; Jin
and Ho, 2009; Li et al, 2010). However, the big
challenge for fully supervised method is the lack
of annotated training data. Therefore, we adopt a
self-learning strategy. Specifically, we employ a
semi-supervised classifier to refine the target re-
sults from the first stage, which uses some highly
confident target candidates as the initial labeled
examples. Then opinion words are also refined.
Our main contributions are as follows:
? We propose a Sentiment Graph Walking al-
gorithm to mine opinion words and opinion
targets from reviews, which naturally incor-
porates confidence of syntactic pattern in a
graph to improve extraction performance. To
our best knowledge, the incorporation of pat-
tern confidence in such a Sentiment Graph
has never been studied before for opinion
words/targets mining task (Section 3).
? We adopt a self-learning method for refining
opinion words/targets generated by Sentiment
Graph Walking. Specifically, it can remove
high-frequency noise terms and capture long-
tail opinion targets in corpora (Section 4).
? We perform experiments on three real world
datasets, which demonstrate the effectiveness
of our method compared with state-of-the-art
unsupervised methods (Section 5).
2 Related Work
In opinion words/targets mining task, most unsu-
pervised methods rely on identifying opinion rela-
tions between opinion words and opinion targets.
Hu and Liu (2004) proposed an association mining
technique to extract opinion words/targets. The
simple heuristic rules they used may potentially
introduce many false opinion words/targets. To
identify opinion relations more precisely, subse-
quent research work exploited syntax information.
Popescu and Etzioni (2005) used manually com-
plied syntactic patterns and Pointwise Mutual In-
formation (PMI) to extract opinion words/targets.
Qiu et al (2009) proposed a bootstrapping frame-
work called Double Propagation which intro-
duced eight heuristic syntactic rules. While man-
ually defining syntactic patterns could be time-
consuming and error-prone, we learn syntactic
patterns automatically from data.
There have been extensive works on mining
opinion words and opinion targets by syntac-
tic pattern learning. Riloff and Wiebe (2003)
performed pattern learning through bootstrapping
while extracting subjective expressions. Zhuang
et al (2006) obtained various dependency re-
lationship templates from an annotated movie
corpus and applied them to supervised opinion
words/targets extraction. Kobayashi et al (2007)
adopted a supervised learning technique to search
for useful syntactic patterns as contextual clues.
Our approach is similar to (Wiebe and Riloff,
2005) and (Xu et al, 2013), all of which apply
syntactic pattern learning and adopt self-learning
strategy. However, the task of (Wiebe and Riloff,
2005) was to classify sentiment orientations in
sentence level, while ours needs to extract more
detailed information in term level. In addition,
our method extends (Xu et al, 2013), and we
give a more complete and in-depth analysis on
the aforementioned problems in the first section.
There were also many works employed graph-
based method (Li et al, 2012; Zhang et al, 2010;
Hassan and Radev, 2010; Liu et al, 2012), but
none of previous works considered confidence of
patterns in the graph.
In supervised approaches, various kinds of
models were applied, such as HMM (Jin and Ho,
2009), SVM (Wu et al, 2009) and CRFs (Li et al,
2010). The downside of supervised methods was
the difficulty of obtaining annotated training data
in practical applications. Also, classifiers trained
1765
on one domain often fail to give satisfactory re-
sults when shifted to another domain. Our method
does not rely on annotated training data.
3 The First Stage: Sentiment Graph
Walking Algorithm
In the first stage, we propose a graph-based al-
gorithm called Sentiment Graph Walking to mine
opinion words and opinion targets from reviews.
3.1 Opinion Pattern Learning for Candidates
Generation
For a given sentence, we first obtain its depen-
dency tree. Following (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Qiu et al, 2009), we regard all
adjectives as opinion word candidates (OC) and
all nouns or noun phrases as opinion target can-
didates (TC). A statistic-based method in (Zhu et
al., 2009) is used to detect noun phrases. Then
candidates are replaced by wildcards ?<OC>? or
?<TC>?. Figure 1 gives a dependency tree exam-
ple generated by Minipar (Lin, 1998).
p red s det
m od
gor geous<OC>
is(VBE)
style<TC>
the(Det)
of(P r ep) scr een<TC>pcom p-n
the(Det)
det
Figure 1: The dependency tree of the sentence
?The style of the screen is gorgeous?.
We extract two kinds of opinion patterns: ?OC-
TC? pattern and ?TC-TC? pattern. The ?OC-
TC? pattern is the shortest path between an OC
wildcard and a TC wildcard in dependency tree,
which captures opinion relation between an opin-
ion word candidate and an opinion target can-
didate. Similarly, the ?TC-TC? pattern cap-
tures opinion relation between two opinion tar-
get candidates.1 Words in opinion patterns are
replaced by their POS tags, and we constrain
that there are at most two words other than
wildcards in each pattern. In Figure 1, there
are two opinion patterns marked out by dash
lines: ?<OC>{pred}(VBE){s}<TC>? for the
?OC-TC? type and ?<TC>{mod}(Prep){pcomp-
n}<TC>? for the ?TC-TC? type. After all pat-
1We do not identify the opinion relation ?OC-OC? be-
cause this relation is often unreliable.
terns are generated, we drop those patterns with
frequency lower than a threshold F .
3.2 Sentiment Graph Construction
To model the opinion relations among opinion
words/targets and opinion patterns, a graph named
as Sentiment Graph is constructed, which is a
weighted, directed graph G = (V,E,W ), where
? V = {Voc ? Vtc ? Vp} is the set of vertices in
G, where Voc, Vtc and Vp represent the set of
opinion word candidates, opinion target can-
didates and opinion patterns, respectively.
? E = {Epo?Ept} ? {Vp?Voc}?{Vp?Vtc}
is the weighted, bi-directional edge set in G,
where Epo and Ept are mutually exclusive
sets of edges connecting opinion word/target
vertices to opinion pattern vertices. Note that
there are no edges between Voc and Vtc.
? W : E ? R+ is the weight function which
assigns non-negative weight to each edge.
For each (e : va ? vb) ? E, where
va, vb ? V , the weight function w(va, vb) =
freq(va, vb)/freq(va), where freq(?) is the
frequency of a candidate extracted by opinion
patterns or co-occurrence frequency between
two candidates.
Figure 2 shows an example of Sentiment Graph.
n icelarge
screen display
<OC>{mod}<TC> <OC>{mod}<TC>{con j}<TC>
1
0.8
0.7
0.2
0.3
0.4
0.2
0.33
0.33
0.33
0.6
0.4
0.2 0.2
Figure 2: An example of Sentiment Graph.
3.3 Confidence Estimation by Random
Walking with Restart
We believe that considering confidence of patterns
can potentially improve the extraction accuracy.
Our intuitive idea is: (i) If an opinion word/target
is with higher confidence, the syntactic patterns
containing this term are more likely to be used to
express customers? opinion. (ii) If an opinion pat-
tern has higher confidence, terms extracted by this
pattern are more likely to be correct. It?s a rein-
forcement process.
1766
We use Random Walking with Restart (RWR)
algorithm to implement our idea described above.
Let Moc p denotes the transition matrix from Voc
to Vp, for vo ? Voc, vp ? Vp, Moc p(vo, vp) =
w(vo, vp). Similarly, we have Mtc p, Mp oc,
Mp tc. Let c denotes confidence vector of candi-
dates so ctoc, cttc and ctp are confidence vectors for
opinion word/target/pattern candidates after walk-
ing t steps. Initially c0oc is uniformly distributed
on a few domain-independent opinion word seeds,
then the following formula are updated iteratively
until cttc and ctoc converge:
ct+1p = MToc p ? ctoc +MTtc p ? cttc (1)
ct+1oc = (1? ?)MTp oc ? ctp + ?c0oc (2)
ct+1tc = MTp tc ? ctp (3)
where MT is the transpose of matrix M and ? is
a small probability of teleporting back to the seed
vertices which prevents us from walking too far
away from the seeds. In the experiments below, ?
is set 0.1 empirically.
4 The Second Stage: Refining Extracted
Results Using Self-Learning
At the end of the first stage, we obtain a ranked
list of opinion words and opinion targets, in which
higher ranked terms are more likely to be correct.
Nevertheless, there are still some issues needed to
be addressed:
1) In the target candidate list, some high-
frequency frivolous general nouns such as
?thing? and ?people? are also highly ranked.
This is because there exist many opinion ex-
pressions containing non-target terms such as
?good thing?, ?nice people?, etc. in reviews.
Due to the lack of ground-truth knowledge
for opinion targets, the false opinion target
problem still remains unsolved.
2) In another aspect, long-tail opinion targets
may have low degree in Sentiment Graph.
Hence their confidence will be low although
they may be extracted by some high qual-
ity patterns. Therefore, the first stage is in-
capable of dealing with the long-tail opinion
target problem.
3) Furthermore, the first stage also extracts
some high-frequency false opinion words
such as ?every?, ?many?, etc. Many terms
of this kind are introduced by high-frequency
false opinion targets, for there are large
amounts of phrases like ?every time? and
?many people?. So this issue is a side effect
of the false opinion target problem.
To address these issues, we exploit a self-
learning strategy. For opinion targets, we use a
semi-supervised binary classifier called target re-
fining classifier to refine target candidates. For
opinion words, we use the classified list of opin-
ion targets to further refine the extracted opinion
word candidates.
4.1 Opinion Targets Refinement
There are two keys for opinion target refinement:
(i) How to generate the initial labeled data for tar-
get refining classifier. (ii) How to properly repre-
sent a long-tail opinion target candidate other than
comparing frequency between different targets.
For the first key, it is clearly improper to select
high-confidence targets as positive examples and
choose low-confidence targets as negative exam-
ples2, for there are noise with high confidence and
long-tail targets with low confidence. Fortunately,
a large proportion of general noun noises are the
most frequent words in common texts. Therefore,
we can generate a small domain-independent gen-
eral noun (GN) corpus from large web corpora to
cover some most frequently used general noun ex-
amples. Then labeled examples can be drawn from
the target candidate list and the GN corpus.
For the second key, we utilize opinion words
and opinion patterns with their confidence scores
to represent an opinion target. By this means, a
long-tail opinion target can be determined by its
own contexts, whose weights are learnt from con-
texts of frequent opinion targets. Thus, if a long-
tail opinion target candidate has high contextual
support, it will have higher probability to be found
out in despite of its low frequency.
Creation of General Noun Corpora. 1000
most frequent nouns in Google-1-gram3 were se-
lected as general noun candidates. On the other
hand, we added all nouns in the top three levels of
hyponyms in four WordNet (Miller, 1995) synsets
?object?, ?person?, ?group? and ?measure? into
the GN corpus. Our idea was based on the fact that
a term is more general when it sits in higher level
in the WordNet hierarchy. Then inapplicable can-
didates were discarded and a 3071-word English
2Note that the ?positive? and ?negative? here denote opin-
ion targets and non-target terms respectively and they do not
indicate sentiment polarities.
3http://books.google.com/ngrams.
1767
GN corpus was created. Another Chinese GN cor-
pus with 3493 words was generated in the similar
way from HowNet (Gan and Wong, 2000).
Generation of Labeled Examples. Let T =
{Y+1,Y?1} denotes the initial labeled set, where
N most highly confident target candidates but not
in our GN corpora are regarded as the positive ex-
ample set Y+1, other N terms from GN corpora
which are also top ranked in the target list are se-
lected as the negative example set Y?1. The re-
minder unlabeled candidates are denoted by T ?.
Feature Representation for Classifier. Given
T and T ? in the form of {(xi, yi)}. For a target
candidate ti, xi = (o1, . . . , on, p1, . . . , pm)T rep-
resents its feature vector, where oj is the opinion
word feature and pk is the opinion pattern feature.
The value of feature is defined as follows,
x(oj) = conf(oj)?
?
pk freq(ti, oj , pk)
freq(oj)
(4)
x(pk) = conf(pk)?
?
oj freq(ti, oj , pk)
freq(pk)
(5)
where conf(?) denotes confidence score estimated
by RWR, freq(?) has the same meaning as in Sec-
tion 3.2. Particularly, freq(ti, oj , pk) represents
the frequency of pattern pk extracting opinion tar-
get ti and opinion word oj .
Target Refinement Classifier: We use support
vector machine as the binary classifier. Hence, the
classification problem can be formulated as to find
a hyperplane < w, b > that separates both labeled
set T and unlabeled set T ? with maximum mar-
gin. The optimization goal is to minimize over
(T ,T ?,w, b, ?1, ..., ?n, ??1 , ..., ??k):
1
2 ||w||
2 + C
n?
i=0
?i + C?
k?
j=0
??j
subject to : ?ni=1 : yi[w ? xi + b] ? 1? ?i
?kj=1 : y?j [w ? x?j + b] ? 1? ??j
?ni=1 : ?i > 0
?kj=1 : ??j > 0
where yi, y?j ? {+1,?1}, xi and x?j represent
feature vectors, C and C? are parameters set by
user. This optimization problem can be imple-
mented by a typical Transductive Support Vector
Machine (TSVM) (Joachims, 1999).
4.2 Opinion Words Refinement
We use the classified opinion target results to re-
fine opinion words by the following equation,
s(oj) =
?
ti?T
?
pk
s(ti)conf(pk)freq(ti, oj , pk)
freq(ti)
where T is the opinion target set in which each el-
ement is classified as positive during opinion tar-
get refinement, s(ti) denotes confidence score ex-
ported by the target refining classifier. Particularly,
freq(ti) =
?
oj
?
pk freq(ti, oj , pk). A higher
score of s(oj) means that candidate oj is more
likely to be an opinion word.
5 Experiments
5.1 Datasets and Evaluation Metrics
Datasets: We select three real world datasets to
evaluate our approach. The first one is called
Customer Review Dataset (CRD) (Hu and Liu,
2004) which contains reviews on five different
products (represented by D1 to D5) in English.
The second dataset is pre-annotated and published
in COAE084, where two domains of Chinese re-
views are selected. At last, we employ a bench-
mark dataset in (Wang et al, 2011) and named it
as Large. We manually annotated opinion words
and opinion targets as the gold standard. Three
annotators were involved. Firstly, two annotators
were required to annotate out opinion words and
opinion targets in sentences. When conflicts hap-
pened, the third annotator would make the final
judgment. The average Kappa-values of the two
domains were 0.71 for opinion words and 0.66
for opinion targets. Detailed information of our
datasets is shown in Table 1.
Dataset Domain #Sentences #OW #OT
Large
(English)
Hotel 10,000 434 1,015
MP3 10,000 559 1,158
COAE08(Chinese)
Camera 2,075 351 892
Car 4,783 622 1,179
Table 1: The detailed information of datasets. OW
stands for opinion words and OT stands for targets.
Pre-processing: Firstly, HTML tags are re-
moved from texts. Then Minipar (Lin, 1998)
is used to parse English corpora, and Standford
Parser (Chang et al, 2009) is used for Chinese
4http://ir-china.org.cn/coae2008.html
1768
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
Ours-Stage1 0.79 0.85 0.82 0.82 0.87 0.84 0.83 0.87 0.85 0.78 0.88 0.83 0.82 0.88 0.85 0.84
Ours-Full 0.86 0.82 0.84 0.88 0.83 0.85 0.89 0.86 0.87 0.83 0.86 0.84 0.89 0.85 0.87 0.86
Table 2: Results of opinion target extraction on the Customer Review Dataset.
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
Ours-Stage1 0.61 0.75 0.67 0.55 0.80 0.65 0.63 0.75 0.68 0.60 0.69 0.64 0.68 0.70 0.69 0.67
Ours-Full 0.64 0.74 0.69 0.59 0.79 0.68 0.66 0.71 0.68 0.65 0.67 0.66 0.72 0.67 0.69 0.68
Table 3: Results of opinion word extraction on the Customer Review Dataset.
corpora. Stemming and fuzzy matching are also
performed following previous work (Hu and Liu,
2004).
Evaluation Metrics: We evaluate our method
by precision(P), recall(R) and F-measure(F).
5.2 Our Method vs. the State-of-the-art
Three state-of-the-art unsupervised methods are
used as competitors to compare with our method.
Hu extracts opinion words/targets by using ad-
jacency rules (Hu and Liu, 2004).
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009).
Zhang is an enhanced version of DP and em-
ploys HITS algorithm (Kleinberg, 1999) to rank
opinion targets (Zhang et al, 2010).
Ours-Full is the full implementation of our
method. We employ SVMlight (Joachims, 1999)
as the target refining classifier. Default parameters
are used except the bias item is set 0.
Ours-Stage1 only uses Sentiment Graph Walk-
ing algorithm which does?t have opinion word and
opinion target refinement.
All of the above approaches use same five
common opinion word seeds. The choice of opin-
ion seeds seems reasonable, as most people can
easily come up with 5 opinion words such as
?good?, ?bad?, etc. The performance on five prod-
ucts of CRD dataset is shown in Table 2 and Ta-
ble 3. Zhang does not extract opinion words so
their results for opinion words are not taken into
account. We can see that Ours-Stage1 achieves
superior recall but has some loss in precision com-
pared with DP and Zhang. This may be because
the CRD dataset is too small and our statistic-
based method may suffer from data sparseness.
In spite of this, Ours-Full achieves comparable F-
measure with DP, which is a well-designed rule-
based method.
The results on two larger datasets are shown
in Table 4 and Table 5, from which we can have
the following observation: (i) All syntax-based-
methods outperform Hu, showing the importance
of syntactic information in opinion relation identi-
fication. (ii) Ours-Full outperforms the three com-
petitors on all domains provided. (iii) Ours-Stage1
outperforms Zhang, especially in terms of recall.
We believe it benefits from our automatical pattern
learning algorithm. Moreover, Ours-Stage1 do
not loss much in precision compared with Zhang,
which indicates the applicability to estimate pat-
tern confidence in Sentiment Graph. (iv) Ours-
Full achieves 4-9% improvement in precision over
the most accurate method, which shows the effec-
tiveness of our second stage.
5.3 Detailed Discussions
This section gives several variants of our method
to have a more detailed analysis.
Ours-Bigraph constructs a bi-graph between
opinion words and targets, so opinion patterns
are not included in the graph. Then RWR algo-
rithm is used to only assign confidence to opinion
word/target candidates.
Ours-Stage2 only contains the second stage,
which doesn?t apply Sentiment Graph Walking al-
gorithm. Hence the confidence score conf(?) in
Equations (4) and (5) have no values and they are
set to 1. The initial labeled examples are exactly
the same as Ours-Full. Due to the limitation of
space, we only give analysis on opinion target ex-
traction results in Figure 3.
1769
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.53 0.55 0.54 0.55 0.57 0.56 0.63 0.65 0.64 0.62 0.58 0.60 0.58
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
Zhang 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
Ours-Stage1 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
Ours-Full 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
Table 4: Results of opinion targets extraction on Large and COAE08.
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.48 0.65 0.55 0.51 0.68 0.58 0.72 0.74 0.73 0.70 0.71 0.70 0.64
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.80 0.73 0.76 0.79 0.71 0.75 0.68
Ours-Stage1 0.59 0.69 0.64 0.61 0.71 0.66 0.79 0.78 0.78 0.77 0.77 0.77 0.71
Ours-Full 0.64 0.67 0.65 0.67 0.69 0.68 0.82 0.78 0.80 0.80 0.76 0.78 0.73
Table 5: Results of opinion words extraction on Large and COAE08.
Figure 3: Opinion target extraction results.
5.3.1 The Effect of Sentiment Graph Walking
We can see that our graph-based methods (Ours-
Bigraph and Ours-Stage1) achieve higher recall
than Zhang. By learning patterns automatically,
our method captures opinion relations more ef-
ficiently. Also, Ours-Stage1 outperforms Ours-
Bigraph, especially in precision. We believe it is
because Ours-Stage1 estimated confidence of pat-
terns so false opinion relations are reduced. There-
fore, the consideration of pattern confidence is
beneficial as expected, which alleviates the false
opinion relation problem. On another hand, we
find that Ours-Stage2 has much worse perfor-
mance than Ours-Full. This shows the effective-
ness of Sentiment Graph Walking algorithm since
the confidence scores estimated in the first stage
are indispensable and indeed key to the learning
of the second stage.
5.3.2 The Effect of Self-Learning
Figure 4 shows the average Precision@N curve of
four domains on opinion target extraction. Ours-
GN-Only is implemented by only removing 50
initial negative examples found by our GN cor-
pora. We can see that the GN corpora work quite
well, which find out most top-ranked false opin-
ion targets. At the same time, Ours-Full has much
better performance than Ours-GN-Only which in-
dicates that Ours-Full can filter out more noises
other than the initial negative examples. There-
fore, our self-learning strategy alleviates the short-
coming of false opinion target problem. More-
over, Table 5 shows that the performance of opin-
ion word extraction is also improved based on the
classified results of opinion targets.
Figure 4: The average precision@N curve of the
four domains on opinion target extraction.
1770
ID Pattern Example #Ext. Conf. PrO PrT
#1 <OC>{mod}<TC> it has a clear screen 7344 0.3938 0.59 0.66
#2 <TC>{subj}<OC> the sound quality is excellent 2791 0.0689 0.62 0.70
#3 <TC>{conj}<TC> the size and weight make it convenient 3620 0.0208 N/A 0.67
#4 <TC>{subj}<TC> the button layout is a simplistic plus 1615 0.0096 N/A 0.67
#5 <OC>{pnmod}<TC> the buttons easier to use 128 0.0014 0.61 0.34
#6 <TC>{subj}(V){s}(VBE){subj}<OC> software provided is simple 189 0.0015 0.54 0.33
#7 <OC>{mod}(Prep){pcomp-c}(V){obj}<TC> great for playing audible books 211 0.0013 0.43 0.48
Table 6: Examples of English patterns. #Ext. represent number of terms extracted, Conf. denotes confi-
dence score estimated by RWR and PrO/PrT stand for precisions of extraction on opinion words/targets
of a pattern respectively. Opinion words in examples are in bold and opinion targets are in italic.
Figure 5 gives the recall of long-tail opinion
targets5 extracted, where Ours-Full is shown to
have much better performance than Ours-Stage1
and the three competitors. This observation proves
that our method can improve the limitation of
long-tail opinion target problem.
Figure 5: The recall of long-tail opinion targets.
5.3.3 Analysis on Opinion Patterns
Table 6 shows some examples of opinion pattern
and their extraction accuracy on MP3 reviews in
the first stage. Pattern #1 and #2 are the two
most high-confidence opinion patterns of ?OC-
TC? type, and Pattern #3 and #4 demonstrate two
typical ?TC-TC? patterns. As these patterns ex-
tract too many terms, the overall precision is very
low. We give Precision@400 of them, which is
more meaningful because only top listed terms
in the extracted results are regarded as opinion
targets. Pattern #5 and #6 have high precision
on opinion words but low precision on opinion
targets. This observation demonstrates the false
opinion target problem. Pattern #7 is a pattern ex-
ample that extracts many false opinion relations
and it has low precision for both opinion words
and opinion targets. We can see that Pattern #7 has
5Since there is no explicit definition for the notion ?long-
tail?, we conservatively regard 60% opinion targets with the
lowest frequency as the ?long-tail? terms.
a lower confidence compared with Pattern #5 and
#6 although it extracts more words. It?s because
it has a low probability of walking from opinion
seeds to this pattern. This further proves that our
method can reduce the confidence of low-quality
patterns.
5.3.4 Sensitivity of Parameters
Finally, we study the sensitivity of parameters
when recall is fixed at 0.70. Figure 6 shows the
precision curves at different N initial training ex-
amples and F filtering frequency. We can see that
the performance saturates when N is set to 50 and
it does not vary much under different F , showing
the robustness of our method. We thus set N to
50, and F to 3 for CRD, 5 for COAE08 and 10 for
Large accordingly.
Figure 6: Influence of parameters.
1771
6 Conclusion and Future Work
This paper proposes a novel two-stage framework
for mining opinion words and opinion targets. In
the first stage, we propose a Sentiment Graph
Walking algorithm, which incorporates syntactic
patterns in a Sentiment Graph to improve the ex-
traction performance. In the second stage, we pro-
pose a self-learning method to refine the result of
first stage. The experimental results show that our
method achieves superior performance over state-
of-the-art unsupervised methods.
We further notice that opinion words are not
limited to adjectives but can also be other type of
word such as verbs or nouns. Identifying all kinds
of opinion words is a more challenging task. We
plan to study this problem in our future work.
Acknowledgement
Thanks to Prof. Yulan He for her insightful
advices. This work was supported by the Na-
tional Natural Science Foundation of China (No.
61070106, No. 61272332 and No. 61202329),
the National High Technology Development 863
Program of China (No. 2012AA011102), the
National Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, SSST
?09, pages 51?59.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: a semi-supervised approach to au-
tomatic sentiment classification. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 701?709.
Kok Wee Gan and Ping Wai Wong. 2000. Anno-
tating information structures in chinese texts using
hownet. In Proceedings of the second workshop on
Chinese language processing: held in conjunction
with the 38th Annual Meeting of the Association for
Computational Linguistics - Volume 12, CLPW ?00,
pages 85?92, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 395?
403, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 585?594,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning, ICML
?09, pages 465?472.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pages 200?209.
Jason Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking
of linguistic configurations. In Proceedings of the
Third International AAAI Conference on Weblogs
and Social Media.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-
of relations in opinion mining. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1065?1074, June.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
653?661, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
410?419, July.
1772
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on Evaluation of Parsing Sys-
tems at ICLRE.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL ?12, pages 1346?1356,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, EMNLP ?03,
pages 105?112, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th international
conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing?05, pages 486?497.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Walk and learn: A two-stage approach
for opinion words and opinion targets co-extraction.
In Proceedings of the 22nd International World Wide
Web Conference, WWW ?13.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM conference on Information and knowledge
management, CIKM ?09, pages 1799?1802.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management, CIKM
?06, pages 43?50.
1773
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 314?324,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Extracting Opinion Targets and Opinion Words from Online Reviews
with Graph Co-ranking
Kang Liu, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Extracting opinion targets and opinion
words from online reviews are two fun-
damental tasks in opinion mining. This
paper proposes a novel approach to col-
lectively extract them with graph co-
ranking. First, compared to previous
methods which solely employed opinion
relations among words, our method con-
structs a heterogeneous graph to model
two types of relations, including seman-
tic relations and opinion relations. Next,
a co-ranking algorithm is proposed to es-
timate the confidence of each candidate,
and the candidates with higher confidence
will be extracted as opinion targets/words.
In this way, different relations make coop-
erative effects on candidates? confidence
estimation. Moreover, word preference
is captured and incorporated into our co-
ranking algorithm. In this way, our co-
ranking is personalized and each candi-
date?s confidence is only determined by its
preferred collocations. It helps to improve
the extraction precision. The experimen-
tal results on three data sets with differ-
ent sizes and languages show that our ap-
proach achieves better performance than
state-of-the-art methods.
1 Introduction
In opinion mining, extracting opinion targets and
opinion words are two fundamental subtasks.
Opinion targets are objects about which users?
opinions are expressed, and opinion words are
words which indicate opinions? polarities. Ex-
tracting them can provide essential information
for obtaining fine-grained analysis on customers?
opinions. Thus, it has attracted a lot of attentions
(Hu and Liu, 2004b; Liu et al, 2012; Moghaddam
and Ester, 2011; Mukherjee and Liu, 2012).
To this end, previous work usually employed a
collective extraction strategy (Qiu et al, 2009; Hu
and Liu, 2004b; Liu et al, 2013b). Their intuition
is: opinion words usually co-occur with opinion
targets in sentences, and there are strong modifi-
cation relationship between them (called opinion
relation in (Liu et al, 2012)). If a word is an
opinion word, other words with which that word
having opinion relations will have highly proba-
bility to be opinion targets, and vice versa. In this
way, extraction is alternatively performed and mu-
tual reinforced between opinion targets and opin-
ion words. Although this strategy has been widely
employed by previous approaches, it still has sev-
eral limitations.
1) Only considering opinion relations is in-
sufficient. Previous methods mainly focused on
employing opinion relations among words for
opinion target/word co-extraction. They have in-
vestigated a series of techniques to enhance opin-
ion relations identification performance, such as
nearest neighbor rules (Liu et al, 2005), syntactic
patterns (Zhang et al, 2010; Popescu and Etzioni,
2005), word alignment models (Liu et al, 2012;
Liu et al, 2013b; Liu et al, 2013a), etc. How-
ever, we are curious that whether merely employ-
ing opinion relations among words is enough for
opinion target/word extraction? We note that there
are additional types of relations among words. For
example, ?LCD? and ?LED? both denote the same
aspect ?screen? in TV set domain, and they are
topical related. We call such relations between
homogeneous words as semantic relations. If we
have known ?LCD? to be an opinion target, ?LED?
is naturally to be an opinion target. Intuitively,
besides opinion relations, semantic relations may
provide additional rich clues for indicating opin-
ion targets/words. Which kind of relations is more
effective for opinion targets/words extraction? Is it
beneficial to consider these two types of relations
together for the extraction? To our best knowl-
314
edge, these problems have seldom been studied
before (see Section 2).
2) Ignoring word preference. When employ-
ing opinion relations to perform mutual reinforc-
ing extraction between opinion targets and opin-
ion words, previous methods depended on opin-
ion associations among words, but seldom consid-
ered word preference. Word preference denotes
a word?s preferred collocations. Intuitively, the
confidence of a candidate being an opinion tar-
get (opinion word) should mostly be determined
by its word preferences rather than all words hav-
ing opinion relations with it. For example
?This camera?s price is expensive for me.?
?It?s price is good.?
?Canon 40D has a good price.?
In these three sentences, ?price? is modified by
?good? more times than ?expensive?. In tradi-
tional extraction strategy, opinion associations are
usually computed based on the co-occurrence fre-
quency. Thus, ?good? has more strong opinion
association with ?price? than ?expensive?, and it
would have more contributions on determining
?price? to be an opinion target or not. It?s un-
reasonable. ?Expensive? actually has more re-
latedness with ?price? than ?good?, and ?expen-
sive? is likely to be a word preference for ?price?.
The confidence of ?price? being an opinion target
should be influenced by ?expensive? in greater ex-
tent than ?good?. In this way, we argue that the
extraction will be more precise.
????4 ????6 ????5 ????1 ????3 ????2 
????2 ????4 ????3 ????5 ????6 ????1 
?????? 
?????? 
?????? 
Figure 1: Heterogeneous Graph: OC means opin-
ion word candidates. TC means opinion target
candidates. Solid curves and dotted lines respec-
tively mean semantic relations and opinion rela-
tions between two candidates.
Thus, to resolve these two problems, we present
a novel approach with graph co-ranking. The col-
lective extraction of opinion targets/words is per-
formed in a co-ranking process. First, we oper-
ate over a heterogeneous graph to model seman-
tic relations and opinion relations into a unified
model. Specifically, our heterogeneous graph is
composed of three subgraphs which model differ-
ent relation types and candidates, as shown in Fig-
ure 1. The first subgraph G
tt
represents semantic
relations among opinion target candidates, and the
second subgraph G
oo
models semantic relations
among opinion word candidates. The third part
is a bipartite subgraph G
to
, which models opinion
relations among different candidate types and con-
nects the above two subgraphs together. Then we
perform a random walk algorithm onG
tt
, G
oo
and
G
to
separately, to estimate all candidates? confi-
dence, and the entries with higher confidence than
a threshold are correspondingly extracted as opin-
ion targets/words. The results could reflect which
type of relation is more useful for the extraction.
Second, a co-ranking algorithm, which incor-
porates three separate random walks on G
tt
, G
oo
and G
to
into a unified process, is proposed to
perform candidate confidence estimation. Differ-
ent relations may cooperatively affect candidate
confidence estimation and generate more global
ranking results. Moreover, we discover each can-
didate?s preferences through topics. Such word
preference will be different for different candi-
dates. We add word preference information into
our algorithm and make our co-ranking algorithm
be personalized. A candidate?s confidence would
mainly absorb the contributions from its word
preferences rather than its all neighbors with opin-
ion relations, which may be beneficial for improv-
ing extraction precision.
We perform experiments on real-world datasets
from different languages and different domains.
Results show that our approach effectively im-
proves extraction performance compared to the
state-of-the-art approaches.
2 Related Work
There are many significant research efforts on
opinion targets/words extraction (sentence level
and corpus level). In sentence level extraction,
previous methods (Wu et al, 2009; Ma and Wan,
2010; Li et al, 2010; Yang and Cardie, 2013)
mainly aimed to identify all opinion target/word
mentions in sentences. They regarded it as a se-
quence labeling task, where several classical mod-
els were used, such as CRFs (Li et al, 2010) and
SVM (Wu et al, 2009).
This paper belongs to corpus level extraction,
and aims to generate a sentiment lexicon and a
target list rather than to identify mentions in sen-
315
tences. Most of previous corpus-level methods
adopted a co-extraction framework, where opin-
ion targets and opinion words reinforce each other
according to their opinion relations. Thus, how
to improve opinion relations identification perfor-
mance was their main focus. (Hu and Liu, 2004a)
exploited nearest neighbor rules to mine opinion
relations among words. (Popescu and Etzioni,
2005) and (Qiu et al, 2011) designed syntactic
patterns to perform this task. (Zhang et al, 2010)
promoted Qiu?s method. They adopted some spe-
cial designed patterns to increase recall. (Liu et
al., 2012; Liu et al, 2013a; Liu et al, 2013b) em-
ployed word alignment model to capture opinion
relations rather than syntactic parsing. The exper-
imental results showed that these alignment-based
methods are more effective than syntax-based ap-
proaches for online informal texts. However, all
aforementioned methods only employed opinion
relations for the extraction, but ignore consider-
ing semantic relations among homogeneous can-
didates. Moreover, they all ignored word prefer-
ence in the extraction process.
In terms of considering semantic relations
among words, our method is related with sev-
eral approaches based on topic model (Zhao et
al., 2010; Moghaddam and Ester, 2011; Moghad-
dam and Ester, 2012a; Moghaddam and Ester,
2012b; Mukherjee and Liu, 2012). The main
goals of these methods weren?t to extract opin-
ion targets/words, but to categorize all given as-
pect terms and sentiment words. Although these
models could be used for our task according to the
associations between candidates and topics, solely
employing semantic relations is still one-sided and
insufficient to obtain expected performance.
Furthermore, there is little work which consid-
ered these two types of relations globally (Su et
al., 2008; Hai et al, 2012; Bross and Ehrig, 2013).
They usually captured different relations using co-
occurrence information. That was too coarse to
obtain expected results (Liu et al, 2012). In ad-
dition, (Hai et al, 2012) extracted opinion tar-
gets/words in a bootstrapping process, which had
an error propagation problem. In contrast, we per-
form extraction with a global graph co-ranking
process, where error propagation can be effec-
tively alleviated. (Su et al, 2008) used heteroge-
neous relations to find implicit sentiment associ-
ations among words. Their aim was only to per-
form aspect terms categorization but not to extract
opinion targets/words. They extracted opinion tar-
gets/words in advanced through simple phrase de-
tection. Thus, the extraction performance is far
from expectation.
3 The Proposed Method
In this section, we propose our method in detail.
We formulate opinion targets/words extraction as
a co-ranking task. All nouns/noun phrases are re-
garded as opinion target candidates, and all ad-
jectives/verbs are regarded as opinion word candi-
dates, which are widely adopted by pervious meth-
ods (Hu and Liu, 2004a; Qiu et al, 2011; Wang
and Wang, 2008; Liu et al, 2012). Then each can-
didate will be assigned a confidence and ranked,
and the candidates with higher confidence than a
threshold will be extracted as the results.
Different from traditional methods, besides
opinion relations among words, we additionally
capture semantic relations among homogeneous
candidates. To this end, a heterogeneous undi-
rected graph G = (V,E) is constructed. V =
V
t
? V
o
denotes the vertex set, which includes
opinion target candidates v
t
? V
t
and opinion
word candidates v
o
? V
o
. E denotes the edge
set, where e
ij
? E means that there is a relation
between two vertices. E
tt
? E represents the se-
mantic relations between two opinion target candi-
dates. E
oo
? E represents the semantic relations
between two opinion word candidates. E
to
? E
represents the opinion relations between opinion
target candidates and opinion word candidates.
Based on different relation types, we used three
matrices M
tt
? R
|V
t
|?|V
t
|
, M
oo
? R
|V
o
|?|V
o
|
and M
to
? R
|V
t
|?|V
o
|
to record the association
weights between any two vertices, respectively.
Section 3.4 will illustrate how to construct them.
3.1 Only Considering Opinion Relations
To estimate the confidence of each candidate, we
use a random walk algorithm on our graph to per-
form co-ranking. Most previous methods (Hu and
Liu, 2004a; Qiu et al, 2011; Wang and Wang,
2008; Liu et al, 2012) only considered opinion
relations among words. Their basic assumption is
as follows.
Assumption 1: If a word is likely to
be an opinion word, the words which
it has opinion relation with will have
higher confidence to be opinion targets,
and vice versa.
316
In this way, candidates? confidences (v
t
or v
o
) are
collectively determined by each other iteratively.
It equals to making random walk on subgraph
G
to
= (V,E
to
) of G. Thus we have
C
t
= (1? ?)?M
to
? C
o
+ ?? I
t
C
o
= (1? ?)?M
T
to
? C
t
+ ?? I
o
(1)
where C
t
and C
o
respectively represent confi-
dences of opinion targets and opinion words.
m
to
i,j
?M
to
means the association weight between
the ith opinion target and the jth opinion word ac-
cording to their opinion relations.
It?s worthy noting that I
t
and I
o
respectively de-
note prior confidences of opinion target candidates
and opinion word candidates. We argue that opin-
ion targets are usually domain-specific, and there
are remarkably distribution difference of them on
different domains (in-domain D
in
vs. out-domain
D
out
). If a candidate is salient inD
in
but common
in D
out
, it?s likely to be an opinion target in D
in
.
Thus, we use a domain relevance measure (DR)
(Hai et al, 2013) to compute I
t
.
DR(t) =
R(t,D
in
)
R(t,D
out
)
(2)
where R(t,D) =
w?
t
s
t
?
?
N
j=1
(w
tj
?
1
W
j
?
?
W
j
k=1
w
kj
) represents candidate relevance with
domain D. w
tj
= (1 + logTF
tj
) ? log
N
DF
t
is a TF-IDF-like weight of candidate t in doc-
ument j. TF
tj
is the frequency of the candi-
date t in the jth document, and DF
t
is docu-
ment frequency. N means the document num-
ber in domain D. R(t,D) includes two mea-
sures to reflect the salient of a candidate in D. 1)
w
tj
?
1
W
j
?
?
W
j
k=1
w
kj
reflects how frequently a
term is mentioned in a particular document. W
j
denotes the word number in document j. 2)
w?
t
s
t
quantifies how significantly a term is mentioned
across all documents in D. w?
t
=
1
N
?
?
N
k=1
w
tk
denotes average weight across all documents for
t. s
t
=
?
1
N
?
?
N
j=1
(w
tj
? w?
j
)
2
denotes the
standard variance of term t. We use the given
reviews as in-domain collection D
in
and Google
n-gram corpus
1
as out-domain collection D
out
.
Finally, each entry in I
t
is a normalized DR(t)
score. In contrast, opinion words are usually
domain-independent. Users may use same words
to express theirs opinions, like ?good?, ?bad?, etc.
But there are still some domain-dependent opinion
1
http://books.google.com/ngrams/datasets
words, like ?delicious? in the restaurant domain,
?powerful? in the car domain. It?s difficult to dis-
criminate them from other words by using statisti-
cal information. So we simply set al entries in I
o
to be 1. ? ? [0, 1] in Eq.1 determines the impact
of the prior confidence on results.
3.2 Only Considering Semantic Relations
To estimate candidates? confidences by only con-
sidering semantic relations among words, we
make two separately random walks on the sub-
graphs of G, G
tt
= (V,E
tt
) and G
oo
= (V,E
oo
).
The basic assumption is as follows:
Assumption 2: If a word is likely to
be an opinion target (opinion word), the
words which it has strong semantic rela-
tion with will have higher confidence to
be opinion targets (opinion words).
In this way, the confidence of the candidate is
determined only by its homogeneous neighbours.
There is no mutual reinforcement between opinion
targets and opinion words. Thus we have
C
t
= (1? ?)?M
tt
? C
t
+ ? ? I
t
C
o
= (1? ?)?M
oo
? C
o
+ ? ? I
o
(3)
where ? has the same role as ? in Eq.1.
3.3 Considering Semantic Relations and
Opinion Relations Together
To jointly model semantic relations and opinion
relations for opinion targets/words extraction, we
couple two random walking algorithms mentioned
above together. Here, Assumption 1 and As-
sumption 2 are both satisfied. Thus, an opinion
target/word candidate?s confidence is collectively
determined by its neighbours according to differ-
ent relation types. Meanwhile, each item may
make influence on it?s neighbours. It?s an iterative
reinforcement process. Thus, we have
C
t
= (1? ?? ?)?M
to
? C
o
+ ??M
tt
? C
t
+ ?? I
t
C
o
= (1? ?? ?)?M
T
to
? C
t
+ ??M
oo
? C
o
+ ?? I
o
(4)
where ? ? [0, 1] determines which type of rela-
tions dominates candidate confidence estimation.
? = 0 means that each candidate?s confidence
is estimated by only considering opinion relations
among words, which equals to Eq.1. Otherwise,
when ? = 1, candidate confidence estimation only
317
considers semantic relations among words, which
equals to Eq.3. ?, I
o
and I
t
have the same meaning
in Eq.1. Our algorithm will run iteratively until it
converges or in a fixed iteration number Iter. In
experiments, we set Iter = 200.
Obtaining Word Preference. The co-ranking
algorithm in Eq.4 is based on a standard random
walking algorithm, which randomly selects a link
according to the association matrix M
to
, M
tt
and
M
oo
, or jumps to a random node with prior confi-
dence value. However, it generates a global rank-
ing over all candidates without taking the node
preference (word preference) into account. As
mentioned in the first section, each opinion tar-
get/word has its preferred collocations, it?s reason-
able that the confidence of an opinion target (opin-
ion word) candidate should be preferentially de-
termined by its preferences, rather than all of its
neighbors with opinion relations.
To obtain the word preference, we resort to top-
ics. We believe that if an opinion word v
i
o
is
topical related with a target word v
j
t
, v
i
o
can be
regarded as a word preference for v
j
t
, and vice
versa. For example, ?price? and ?expensive? are
topically related in phone?s domain, so they are a
word preference for each other.
Specifically, we use a vector P
T
i
=
[P
T
i
1
, ..., P
T
i
k
, ..., P
T
i
|V
o
|
]
1?|V
o
|
to represent word
preference of the ith opinion target candidate.
P
T
i
k
means the preferred probability of the ith
potential opinion target for the kth potential
opinion words. To compute P
T
i
k
, we first use
Kullback-Leibler divergence to measure the
semantic distance between any two candidates on
the bridge of topics. Thus, we have
D(v
i
, v
j
) =
1
2
?
z
(KL
z
(v
i
||v
j
) +KL
z
(v
j
||v
i
))
whereKL
z
(v
i
||v
j
) = p(z|v
i
)log
p(z|v
i
)
p(z|v
j
)
means the
KL-divergence from candidate v
i
to v
j
based on
topic z. p(z|v) = p(v|z)
p(z)
p(v)
, where p(v|z) is the
probability of the candidate v to topic z (see Sec-
tion 3.4). p(z) is the probability that topic z in
reviews. p(v) is the probability that a candidate
occurs in reviews. Then, a logistic function is used
to map D(v
i
, v
j
) into [0, 1].
SA(v
i
, v
j
) =
1
1 + e
D(v
i
,v
j
)
(5)
Then, we calculate P
T
i
k
by normalize SA(v
i
, v
j
)
score, i.e. P
T
i
k
=
SA(v
t
i
,v
o
k
)
?
|V
o
|
p=1
SA(v
t
i
,v
o
p
)
. For demon-
stration, we give some examples in Table 1, where
each entry denotes a SA(v
i
, v
j
) score between two
candidates. We can see that using topics can suc-
cessfully capture the preference information for
each opinion target/word.
expensive good long colorful
price 0.265 0.043 0.003 0.000
LED 0.002 0.035 0.007 0.098
battery 0.000 0.015 0.159 0.001
Table 1: Examples of Calculated Word Preference
And we use a vector P
O
j
=
[P
O
j
1
, ..., P
O
j
q
, ..., P
O
j
|V
t
|
]
1?|V
t
|
to represent
the preference information of the jth opin-
ion word candidate. Similarly, we have
P
O
j
q
=
SA(v
t
q
,v
o
j
)
?
|V
t
|
k=1
SA(v
t
k
,v
o
j
)
.
Incorporating Word Preference into Co-
ranking. To consider such word preference in
our co-ranking algorithm, we incorporate it into
the random walking on G
to
. Intuitively, prefer-
ence vectors will be different for different can-
didates. Thus, the co-ranking algorithm would
be personalized. It allows that the candidate
confidence propagates to other candidates only
in its preference cluster. Specifically, we make
modification on original transition matrix M
to
=
(M
to
1
,M
to
2
, ...,M
to
|V
t
|
) and add each candidate?s
preference in it. Let
?
M
to
= (
?
M
to
1
,
?
M
to
2
, ...,
?
M
to
|V
t
|
)
be the modified transition matrix, which records
the associations between opinion target candi-
dates and opinion word candidates. Here M
to
k
?
R
1?|V
o
|
and
?
M
to
k
? R
1?|V
o
|
denotes the kth col-
umn vector in M
to
and
?
M
to
, respectively. And
let Diag(P
T
k
) denote a diagonal matrix whose
eigenvalue is vector P
T
k
, we have
?
M
to
k
= M
to
k
Diag(P
T
k
)
Similarly, let U
to
k
? R
1?|V
t
|
and
?
U
to
k
? R
1?|V
t
|
denotes the kth row vector in M
T
to
and
?
M
T
to
, re-
spectively. Diag(P
O
k
) denote a diagonal matrix
whose eigenvalue is vector P
O
k
. Then we have
?
U
to
k
= U
to
k
Diag(P
O
k
)
In this way, each candidate?s preference is in-
corporated into original associations based on
opinion relation M
to
through Diag(P
O
k
) and
Diag(P
T
k
). And candidates? confidences will
mainly come from the contributions of its prefer-
ences. Thus, C
t
and C
o
in Eq.4 become:
318
Ct
= (1? ?? ?)?
?
M
to
? C
o
+ ??M
tt
? C
t
+ ?? I
t
C
o
= (1? ?? ?)?
?
M
T
to
? C
t
+ ??M
oo
? C
o
+ ?? I
o
(6)
3.4 Capturing Semantic and Opinion
Relations
In this section, we explain how to capture seman-
tic relations and opinion relations for constructing
transition matrices M
tt
, M
oo
and M
to
.
Capturing Semantic Relations: For captur-
ing semantic relations among homogenous candi-
dates, we employ topics. We believe that if two
candidates share similar topics in the corpus, there
is a strong semantic relation between them. Thus,
we employ a LDA variation (Mukherjee and Liu,
2012), an extension of (Zhao et al, 2010), to dis-
cover topic distribution on words, which sampled
all words into two separated observations: opinion
targets and opinion words. It?s because that we are
only interested in topic distribution of opinion tar-
gets/words, regardless of other useless words, in-
cluding conjunctions, prepositions etc. This model
has been proven to be better than the standard
LDA model and other LDA variations for opinion
mining (Mukherjee and Liu, 2012).
After topic modeling, we obtain the proba-
bility of the candidates (v
t
and v
o
) to topic z,
i.e. p(z|v
t
) and p(z|v
o
), and topic distribution
p(z). Then, a symmetric Kullback-Leibler diver-
gence as same as Eq.5 is used to calculate the se-
mantical associations between any two homoge-
nous candidates. Thus, we obtain SA(v
t
, v
t
) and
SA(v
o
, v
o
), which correspond to the entries in
M
tt
and M
oo
, respectively.
Capturing Opinion Relations: To capture
opinion relations among words and construct the
transition matrix M
to
, we used an alignment-
based method proposed in (Liu et al, 2013b).
This approach models capturing opinion relations
as a monolingual word alignment process. Each
opinion target can find its corresponding mod-
ifiers in sentences through alignment, in which
multiple factors are considered globally, such as
co-occurrence information, word position in sen-
tence, etc. Moreover, this model adopted a par-
tially supervised framework to combine syntac-
tic information with alignment results, which has
been proven to be more precise than the state-of-
the-art approaches for opinion relations identifica-
tion (Liu et al, 2013b).
After performing word alignment, we obtain
a set of word pairs composed of a noun (noun
phrase) and its corresponding modified word.
Then, we simply employ Pointwise Mutual Infor-
mation (PMI) to calculate the opinion associations
among words as the entries in M
to
. OA(v
t
, v
o
) =
log
p(v
t
,v
o
)
p(v
t
)p(v
o
)
, where v
t
and v
o
denote an opinion
target candidate and an opinion word candidate,
respectively. p(v
t
, v
o
) is the co-occurrence prob-
ability of v
t
and v
o
based on the opinion relation
identification results. p(v
t
) and p(v
o
) give the in-
dependent occurrence probability of of v
t
and v
o
,
respectively
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets: To evaluate the proposed method, we
used three datasets. The first one is Customer
Review Datasets (CRD), used in (Hu and Liu,
2004a), which contains reviews about five prod-
ucts. The second one is COAE2008 dataset2
2
,
which contains Chinese reviews about four prod-
ucts. The third one is Large, also used in (Wang
et al, 2011; Liu et al, 2012; Liu et al, 2013a),
where two domains are selected (Mp3 and Hotel).
As mentioned in (Liu et al, 2012), Large con-
tains 6,000 sentences for each domain. Opinion
targets/words are manually annotated, where three
annotators were involved. Two annotators were
required to annotate out opinion words/targets in
reviews. When conflicts occur, the third annota-
tor make final judgement. In total, we respectively
obtain 1,112, 1,241 opinion targets and 334, 407
opinion words in Hotel, MP3.
Pre-processing: All sentences are tagged to
obtain words? part-of-speech tags using Stanford
NLP tool
3
. And noun phrases are identified using
the method in (Zhu et al, 2009) before extraction.
Evaluation Metrics: We select precision(P),
recall(R) and f-measure(F) as metrics. And a sig-
nificant test is performed, i.e., a t-test with a de-
fault significant level of 0.05.
4.2 Our Method vs. The State-of-the-art
Methods
To prove the effectiveness of the proposed method,
we select some state-of-the-art methods for com-
parison as follows:
2
http://ir-china.org.cn/coae2008.html
3
http://nlp.stanford.edu/software/tagger.shtml
319
Methods
D1 D2 D3 D4 D5 Avg.
P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.758
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.856
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.846
SAS 0.80 0.79 0.79 0.82 0.76 0.79 0.79 0.74 0.76 0.77 0.78 0.77 0.80 0.76 0.78 0.778
Liu 0.84 0.85 0.84 0.87 0.85 0.86 0.88 0.89 0.88 0.81 0.85 0.83 0.89 0.87 0.88 0.858
Hai 0.77 0.87 0.83 0.79 0.86 0.82 0.79 0.89 0.84 0.72 0.88 0.79 0.74 0.88 0.81 0.818
CR 0.84 0.86 0.85 0.87 0.85 0.86 0.87 0.90 0.88 0.81 0.87 0.83 0.89 0.88 0.89 0.862
CR WP 0.86 0.86 0.86 0.88 0.86 0.87 0.89 0.90 0.89 0.81 0.87 0.83 0.91 0.89 0.90 0.870
Table 2: Results of Opinion Targets Extraction on Customer Review Dataset
Methods
Camera Car Laptop Phone Mp3 Hotel Avg.
P R F P R F P R F P R F P R F P R F F
Hu 0.63 0.65 0.64 0.62 0.58 0.60 0.51 0.67 0.58 0.69 0.60 0.64 0.61 0.68 0.64 0.60 0.65 0.62 0.587
DP 0.71 0.70 0.70 0.72 0.65 0.68 0.58 0.69 0.63 0.78 0.66 0.72 0.69 0.70 0.69 0.67 0.69 0.68 0.683
Zhang 0.71 0.78 0.74 0.69 0.68 0.68 0.57 0.80 0.67 0.80 0.71 0.75 0.67 0.77 0.72 0.67 0.76 0.71 0.712
SAS 0.72 0.72 0.72 0.71 0.64 0.67 0.59 0.72 0.65 0.78 0.69 0.73 0.69 0.75 0.72 0.69 0.74 0.71 0.700
Liu 0.75 0.81 0.78 0.71 0.71 0.71 0.61 0.85 0.71 0.83 0.74 0.78 0.70 0.82 0.76 0.71 0.80 0.75 0.749
Hai 0.68 0.84 0.76 0.69 0.75 0.72 0.58 0.86 0.72 0.75 0.76 0.76 0.65 0.83 0.74 0.62 0.82 0.75 0.742
CR 0.75 0.83 0.79 0.72 0.74 0.73 0.60 0.85 0.70 0.83 0.77 0.80 0.70 0.84 0.76 0.71 0.83 0.77 0.758
CR WP 0.78 0.84 0.81 0.74 0.75 0.74 0.64 0.85 0.73 0.84 0.76 0.80 0.74 0.84 0.79 0.74 0.82 0.78 0.773
Table 3: Results of Opinion Targets Extraction on COAE 2008 and Large
Hu extracted opinion targets/words using asso-
ciation mining rules (Hu and Liu, 2004a).
DP used syntax-based patterns to capture opin-
ion relations in sentences, and then used a boot-
strapping process to extract opinion targets/words
(Qiu et al, 2011),.
Zhang is proposed by (Zhang et al, 2010).
They also used syntactic patterns to capture opin-
ion relations between words. Then a HITS (Klein-
berg, 1999) algorithm is employed to extract opin-
ion targets.
Liu is proposed by (Liu et al, 2013a), an ex-
tension of (Liu et al, 2012). They employed a
word alignment model to capture opinion relations
among words, and then used a random walking al-
gorithm to extract opinion targets.
Hai is proposed by (Hai et al, 2012), which is
similar to our method. They employed both of se-
mantic relations and opinion relations to extract
opinion words/targets in a bootstrapping frame-
work. But they captured relations only using co-
occurrence statistics. Moreover, word preference
was not considered.
SAS is proposed by (Mukherjee and Liu, 2012),
an extended lda-based model of (Zhao et al,
2010). The top K items for each aspect are ex-
tracted as opinion targets/words. It means that
only semantic relations among words are consid-
ered in SAS. And we set aspects number to be 9 as
same as (Mukherjee and Liu, 2012).
CR: is the proposed method in this paper by us-
ing co-ranking, referring to Eq.4. CR doesn?t con-
sider word preference.
CR WP: is the full implementation of our
method, referring to Eq.6.
Hu, DP, Zhang and Liu are the methods which
only consider opinion relations among words.
SAS is the methods which only consider seman-
tic relations among words. Hai, CR and CR WP
consider these two types of relations together. The
parameter settings of state-of-the-art methods are
same as their original paper. In CR and CR WP,
we set ? = 0.4 and ? = 0.1. The experimental
results are shown in Table 2, 3, 4 and 5, where the
last column presents the average F-measure scores
for multiple domains. Since Liu and Zhang aren?t
designed for opinion words extraction, we don?t
present their results in Table 4 and 5. From exper-
imental results, we can see.
1) Our methods (CR and CR WP) outperform
other methods not only on opinion targets extrac-
tion but on opinion words extraction in most do-
mains. It proves the effectiveness of the proposed
method.
2) CR and CR WP have much better perfor-
mance than Liu and Zhang, especially on Recall.
Liu and Zhang also use a ranking framework like
ours, but they only employ opinion relations for
extraction. In contrast, besides opinion relations,
CR and CR WP further take semantic relations
into account. Thus, more opinion targets/words
can be extracted. Furthermore, we observe that
CR and CR WP outperform SAS. SAS only ex-
ploits semantic relations, but ignores opinion re-
lations among words. Its extraction is performed
separately and neglects the reinforcement between
opinion targets and opinion words. Thus, SAS has
worse performance than our methods. It demon-
strates the usefulness of considering multiple rela-
tion types.
320
Methods
D1 D2 D3 D4 D5 Avg.
P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.624
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.666
SAS 0.64 0.68 0.66 0.55 0.70 0.62 0.62 0.65 0.63 0.60 0.61 0.60 0.68 0.63 0.65 0.632
Hai 0.62 0.77 0.69 0.52 0.80 0.64 0.60 0.74 0.67 0.56 0.69 0.62 0.66 0.70 0.68 0.660
CR 0.62 0.75 0.68 0.57 0.79 0.67 0.64 0.75 0.69 0.63 0.69 0.66 0.68 0.69 0.69 0.678
CR WP 0.65 0.75 0.70 0.59 0.80 0.68 0.65 0.74 0.70 0.66 0.68 0.67 0.71 0.70 0.70 0.690
Table 4: Results of Opinion Words Extraction on Customer Review Dataset
Methods
Camera Car Laptop Phone Mp3 Hotel Avg.
P R F P R F P R F P R F P R F P R F F
Hu 0.72 0.74 0.73 0.70 0.71 0.70 0.66 0.70 0.68 0.70 0.70 0.70 0.48 0.67 0.56 0.52 0.69 0.59 0.660
DP 0.80 0.73 0.76 0.79 0.71 0.75 0.75 0.69 0.72 0.78 0.68 0.73 0.60 0.65 0.62 0.61 0.66 0.63 0.702
SAS 0.73 0.70 0.71 0.75 0.68 0.71 0.72 0.68 0.69 0.71 0.66 0.68 0.64 0.62 0.63 0.66 0.61 0.63 0.675
Hai 0.76 0.74 0.75 0.72 0.74 0.73 0.69 0.72 0.70 0.72 0.70 0.71 0.61 0.69 0.64 0.59 0.68 0.64 0.690
CR 0.80 0.75 0.77 0.77 0.74 0.75 0.73 0.71 0.72 0.75 0.71 0.73 0.63 0.69 0.64 0.63 0.68 0.66 0.710
CR WP 0.80 0.75 0.77 0.80 0.74 0.77 0.77 0.71 0.74 0.78 0.72 0.75 0.66 0.68 0.67 0.67 0.69 0.68 0.730
Table 5: Results of Opinion Words Extraction on COAE 2008 and Large
3) CR and CR WP both outperform Hai. We
believe the reasons are as follows. First, CR and
CR WP considers multiple relations in a unified
process by using graph co-ranking. In contrast,
Hai adopts a bootstrapping framework which per-
forms extraction step by step and may have the
problem of error propagation. It demonstrates
that our graph co-ranking is more suitable for this
task than bootstrapping-based strategy. Second,
our method captures semantic relations using topic
modeling and captures opinion relations through
word alignments, which are more precise than Hai
which merely uses co-occurrence information to
indicate such relations among words. In addition,
word preference is not handled in Hai, but pro-
cessed in CR WP. The results show the usefulness
of word preference for opinion targets/words ex-
traction.
4) CR WP outperforms CR, especially on pre-
cision. The only difference between them is that
CR WP considers word preference when perform-
ing graph ranking for candidate confidence esti-
mation, but CR does not. Each candidate confi-
dence estimation in CR WP gives more weights
for this candidate?s preferred words than CR.
Thus, the precision can be improved.
4.3 Semantic Relation vs. Opinion Relation
In this section, we discuss which relation type
is more effective for this task. For comparison,
we design two baselines, called OnlySA and On-
lyOA. OnlyOA only employs opinion relations
among words, which equals to Eq.1. OnlySA only
employs semantic relations among words, which
equals to Eq.3. Moreover, Combine is our method
which considers both of opinion relations and se-
mantic relations together, referring to Eq.4 with
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.65
.70
.75
.80
.85
.90
.95
OnlySA
OnlyOA
Combine
(a) Opinion Target Extraction Results
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.60
.65
.70
.75
.80
OnlySA
OnlyOA
Combine
(b) Opinion Word Extraction Results
Figure 2: Semantic Relations vs. Opinion Rela-
tions
? = 0.5. Figure 2 presents experimental results.
The left graph presents opinion targets extraction
results and the right graph presents opinion words
extraction results. Because of space limitation, we
only shown the results of four domains (MP3, Ho-
tel, Laptop and Phone).
From results, we observe that OnlyOA outper-
forms OnlySA in all domains. It demonstrates
that employing opinion relations are more useful
than semantic relations for co-extracting opinion
targets/words. And it is necessary to utilize the
mutual reinforcement relationship between opin-
ion words and opinion targets. Moreover, Com-
bine outperforms OnlySA and OnlyOA in all do-
mains. It indicates that combining different rela-
tions among words together is effective.
4.4 The Effectiveness of Considering Word
Preference
In this section, we try to prove the necessity of
considering word preference in Eq.6. Besides the
comparison between CR and CR WP performed
321
in the main experiment in Section 4.2, we fur-
ther incorporate word preference in aforemen-
tioned OnlyOA, named as OnlyOA WP, which
only employs opinion relations among words and
equals to Eq.6 with ? = 0. Experimental results
are shown in Figure 3. Because of space limita-
tion, we only show the results of the same domains
in section 4.3,
Form results, we observe that CR WP out-
performs CR, and OnlyOA WP outperforms On-
lyOA in all domains, especially on precision.
These observations demonstrate that considering
word preference is very important for opinion tar-
gets/words extraction. We believe the reason is
that exploiting word preference can provide more
fine information for opinion target/word candi-
dates? confidence estimation. Thus the perfor-
mance can be improved.
 
MP3 Hotel Laptop Phone
P
re
c
i
si
o
n
.60
.65
.70
.75
.80
.85
.90
OnlyOA
OnlyOA_WP
CR
CR_WP
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.70
.75
.80
.85
.90
.95
OnlyOA
OnlyOA_WP
CR
CR_WP
(a) Opinion Target Extraction Results
 
MP3 Hotel Laptop Phone
P
re
c
i
si
o
n
.60
.65
.70
.75
.80
OnlyOA
OnlyOA_WP
CR
CR_WP
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.60
.65
7
.75
.80
OnlyOA
OnlyOA_WP
CR
CR_WP
(b) Opinion Word Extraction Results
Figure 3: Experimental results when considering
word preference
4.5 Parameter Sensitivity
In this subsection, we discuss the variation of ex-
traction performance when changing ? and ? in
Eq.6. Due to space limitation, we only show the
F-measure of CR WP on four domains. Experi-
mental results are shown in Figure 4 and Figure
5. The left graphs in Figure 4 and 5 present the
performance variation of CR WP with varying ?
from 0 to 0.9 and fixing ? = 0.1. The right graphs
in Figure 4 and 5 present the performance varia-
tion of CR WP with varying ? from 0 to 0.6 and
fixing ? = 0.4.
In the left graphs in Figure 4 and 5, we observe
the best performance is obtained when ? = 0.4.
It indicates that opinion relations and semantic re-
lations are both useful for extracting opinion tar-
gets/words. The extraction performance is benefi-
cial from their combination. In the right graphs in
Figure 4 and 5, the best performance is obtained
when ? = 0.1. It indicates prior knowledge is
useful for extraction. When ? increases, perfor-
mance, however, decreases. It demonstrates that
incorporating more prior knowledge into our al-
gorithm would restrain other useful clues on esti-
mating candidate confidence, and hurt the perfor-
mance.
 
0.0 .1 .2 .3 .4 .5 .6 .7 .8 .9
F-M
e
a
sure
.60
.65
.70
.75
.80
.85
MP3
Hotel 
Laptop
Phone
 
0.0 .1 .2 .3 .4 .5 .6
F-M
e
a
sure
.65
.70
.75
.80
.85
MP3
Hotel 
Laptop
Phone
Figure 4: Opinion targets extraction results
 
0.0 .1 .2 .3 .4 .5 .6 .7 .8 .9
F-M
e
a
sure
.55
.60
.65
.70
.75
.80
MP3
Hotel 
Laptop
Phone
 
0.0 .1 .2 .3 .4 .5 .6
F-M
e
a
sure
.50
.55
.60
.65
.70
.75
.80
MP3
Hotel 
Laptop
Phone
Figure 5: Opinion words extraction results
5 Conclusions
This paper presents a novel method with graph co-
ranking to co-extract opinion targets/words. We
model extracting opinion targets/words as a co-
ranking process, where multiple heterogenous re-
lations are modeled in a unified model to make co-
operative effects on the extraction. In addition, we
especially consider word preference in co-ranking
process to perform more precise extraction. Com-
pared to the state-of-the-art methods, experimental
results prove the effectiveness of our method.
Acknowledgement
This work was sponsored by the National
Basic Research Program of China (No.
2014CB340500), the National Natural Sci-
ence Foundation of China (No. 61272332
and No. 61202329), the National High Tech-
nology Development 863 Program of China
(No. 2012AA011102), and CCF-Tencent Open
Research Fund.
References
Juergen Bross and Heiko Ehrig. 2013. Automatic con-
struction of domain and aspect specific sentiment
322
lexicons for customer review mining. In Proceed-
ings of the 22nd ACM international conference on
Conference on information &#38; knowledge man-
agement, CIKM ?13, pages 1077?1086, New York,
NY, USA. ACM.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012. One
seed to find them all: mining opinion features via
association. In CIKM, pages 255?264.
Zhen Hai, Kuiyu Chang, Jung-Jae Kim, and Christo-
pher C. Yang. 2013. Identifying features in opinion
mining via intrinsic and extrinsic domain relevance.
IEEE Transactions on Knowledge and Data Engi-
neering, 99(PrePrints):1.
Mingqin Hu and Bing Liu. 2004a. Mining opinion fea-
tures in customer reviews. In Proceedings of Con-
ference on Artificial Intelligence (AAAI).
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Chu-Ren Huang and Dan Jurafsky, editors, COL-
ING, pages 653?661. Tsinghua University Press.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Allan Ellis and Tatsuya Hagino,
editors, WWW, pages 342?351. ACM.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013a.
Opinion target extraction using partially supervised
word alignment model.
Kang Liu, Liheng Xu, and Jun Zhao. 2013b. Syntactic
patterns versus word alignment: Extracting opinion
targets from online reviews.
Tengfei Ma and Xiaojun Wan. 2010. Opinion tar-
get extraction in chinese news comments. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 782?790. Chinese Information Pro-
cessing Society of China.
Samaneh Moghaddam and Martin Ester. 2011. Ilda:
Interdependent lda model for learning latent aspects
and their ratings from online product reviews. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ?11, pages 665?674, New
York, NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2012a.
Aspect-based opinion mining from product reviews.
In Proceedings of the 35th International ACM SIGIR
Conference on Research and Development in In-
formation Retrieval, SIGIR ?12, pages 1184?1184,
New York, NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2012b. On
the design of lda models for aspect-based opinion
mining. In CIKM, pages 803?812.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 339?346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation.
Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian
Wu, Xiaoxun Zhang, Bin Swen, and Zhong Su.
2008. Hidden sentiment association in chinese web
opinion mining. In Jinpeng Huai, Robin Chen,
Hsiao-Wuen Hon, Yunhao Liu, Wei-Ying Ma, An-
drew Tomkins, and Xiaodong Zhang 0001, editors,
WWW, pages 959?968. ACM.
Bo Wang and Houfeng Wang. 2008. Bootstrapping
both product features and opinion words from chi-
nese customer reviews with cross-inducing.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Chid Apt, Joydeep Ghosh,
and Padhraic Smyth, editors, KDD, pages 618?626.
ACM.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
323
Papers), pages 1640?1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking
product features in opinion documents. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 1462?1470. Chinese Information
Processing Society of China.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 56?
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In David Wai-Lok Cheung,
Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and
Jimmy J. Lin, editors, CIKM, pages 1799?1802.
ACM.
324
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 336?346,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Product Feature Mining: Semantic Clues versus Syntactic Constituents
Liheng Xu, Kang Liu, Siwei Lai and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, jzhao}@nlpr.ia.ac.cn
Abstract
Product feature mining is a key subtask
in fine-grained opinion mining. Previ-
ous works often use syntax constituents in
this task. However, syntax-based methods
can only use discrete contextual informa-
tion, which may suffer from data sparsity.
This paper proposes a novel product fea-
ture mining method which leverages lexi-
cal and contextual semantic clues. Lexical
semantic clue verifies whether a candidate
term is related to the target product, and
contextual semantic clue serves as a soft
pattern miner to find candidates, which ex-
ploits semantics of each word in context
so as to alleviate the data sparsity prob-
lem. We build a semantic similarity graph
to encode lexical semantic clue, and em-
ploy a convolutional neural model to cap-
ture contextual semantic clue. Then Label
Propagation is applied to combine both se-
mantic clues. Experimental results show
that our semantics-based method signif-
icantly outperforms conventional syntax-
based approaches, which not only mines
product features more accurately, but also
extracts more infrequent product features.
1 Introduction
In recent years, opinion mining has helped cus-
tomers a lot to make informed purchase decisions.
However, with the rapid growth of e-commerce,
customers are no longer satisfied with the over-
all opinion ratings provided by traditional senti-
ment analysis systems. The detailed functions or
attributes of products, which are called product
features, receive more attention. Nevertheless, a
product may have thousands of features, which
makes it impractical for a customer to investigate
them all. Therefore, mining product features au-
tomatically from online reviews is shown to be a
key step for opinion summarization (Hu and Liu,
2004; Qiu et al, 2009) and fine-grained sentiment
analysis (Jiang et al, 2011; Li et al, 2012).
Previous works often mine product features via
syntactic constituent matching (Popescu and Et-
zioni, 2005; Qiu et al, 2009; Zhang et al, 2010).
The basic idea is that reviewers tend to comment
on product features in similar syntactic structures.
Therefore, it is natural to mine product features by
using syntactic patterns. For example, in Figure 1,
the upper box shows a dependency tree produced
by Stanford Parser (de Marneffe et al, 2006), and
the lower box shows a common syntactic pattern
from (Zhang et al, 2010), where <feature/NN>
is a wildcard to be fit in reviews and NN denotes
the required POS tag of the wildcard. Usually, the
product name mp3 is specified, and when screen
matches the wildcard, it is likely to be a product
feature of mp3.
 
Figure 1: An example of syntax-based prod-
uct feature mining procedure. The word screen
matches the wildcard <feature/NN>. Therefore,
screen is likely to be a product feature of mp3.
Generally, such syntactic patterns extract prod-
uct features well but they still have some limita-
tions. For example, the product-have-feature pat-
tern may fail to find the fm tuner in a very similar
case in Example 1(a), where the product is men-
tioned by using player instead of mp3. Similarly,
it may also fail on Example 1(b), just with have re-
placed by support. In essence, syntactic pattern is
336
a kind of one-hot representation for encoding the
contexts, which can only use partial and discrete
features, such as some key words (e.g., have) or
shallow information (e.g., POS tags). Therefore,
such a representation often suffers from the data
sparsity problem (Turian et al, 2010).
One possible solution for this problem is us-
ing a more general pattern such as NP-VB-feature,
where NP represents a noun or noun phrase and
VB stands for any verb. However, this pattern be-
comes too general that it may find many irrelevant
cases such as the one in Example 1(c), which is not
talking about the product. Consequently, it is very
difficult for a pattern designer to balance between
precision and generalization.
Example 1:
(a) This player has an
::
fm
:::::
tuner.
(b) This mp3 supports
::::
wma
:::
file.
(c) This review has helped
:::::
people a lot.
(d) This mp3 has some
:::::
flaws.
To solve the problems stated above, it is ar-
gued that deeper semantics of contexts shall be ex-
ploited. For example, we can try to automatically
discover that the verb have indicates a part-whole
relation (Zhang et al, 2010) and support indicates
a product-function relation, so that both sth. have
and sth. support suggest that terms following them
are product features, where sth. can be replaced
by any terms that refer to the target product (e.g.,
mp3, player, etc.). This is called contextual se-
mantic clue. Nevertheless, only using contexts is
not sufficient enough. As in Example 1(d), we can
see that the word flaws follows mp3 have, but it
is not a product feature. Thus, a noise term may
be extracted even with high contextual support.
Therefore, we shall also verify whether a candi-
date is really related to the target product. We call
it lexical semantic clue.
This paper proposes a novel bootstrapping ap-
proach for product feature mining, which lever-
ages both semantic clues discussed above. Firstly,
some reliable product feature seeds are automat-
ically extracted. Then, based on the assumption
that terms that are more semantically similar to
the seeds are more likely to be product features,
a graph which measures semantic similarities be-
tween terms is built to capture lexical semantic
clue. At the same time, a semi-supervised con-
volutional neural model (Collobert et al, 2011) is
employed to encode contextual semantic clue. Fi-
nally, the two kinds of semantic clues are com-
bined by a Label Propagation algorithm.
In the proposed method, words are represented
by continuous vectors, which capture latent se-
mantic factors of the words (Turian et al, 2010).
The vectors can be unsupervisedly trained on large
scale corpora, and words with similar semantics
will have similar vectors. This enables our method
to be less sensitive to lexicon change, so that the
data sparsity problem can be alleviated . The con-
tributions of this paper include:
? It uses semantics of words to encode contextual
clues, which exploits deeper level information
than syntactic constituents. As a result, it mines
product features more accurately than syntax-
based methods.
? It exploits semantic similarity between words
to capture lexical clues, which is shown to be
more effective than co-occurrence relation be-
tween words and syntactic patterns. In addition,
experiments show that the semantic similarity
has the advantage of mining infrequent product
features, which is crucial for this task. For ex-
ample, one may say ?This hotel has low water
pressure?, where low water pressure is seldom
mentioned, but fatal to someone?s taste.
? We compare the proposed semantics-based ap-
proach with three state-of-the-art syntax-based
methods. Experiments show that our method
achieves significantly better results.
The rest of this paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed method in details. Section 4 gives the
experimental results. Lastly, we conclude this pa-
per in Section 5.
2 Related Work
In product feature mining task, Hu and Liu (2004)
proposed a pioneer research. However, the asso-
ciation rules they used may potentially introduce
many noise terms. Based on the observation that
product features are often commented on by simi-
lar syntactic structures, it is natural to use patterns
to capture common syntactic constituents around
product features.
Popescu and Etzioni (2005) designed some syn-
tactic patterns to search for product feature candi-
dates and then used Pointwise Mutual Information
(PMI) to remove noise terms. Qiu et al (2009)
proposed eight heuristic syntactic rules to jointly
extract product features and sentiment lexicons,
where a bootstrapping algorithm named Double
337
Propagation was applied to expand a given seed
set. Zhang et al (2010) improved Qiu?s work
by adding more feasible syntactic patterns, and the
HITS algorithm (Kleinberg, 1999) was employed
to rank candidates. Moghaddam and Ester (2010)
extracted product features by automatical opinion
pattern mining. Zhuang et al (2006) used various
syntactic templates from an annotated movie cor-
pus and applied them to supervised movie feature
extraction. Wu et al (2009) proposed a phrase
level dependency parsing for mining aspects and
features of products.
As discussed in the first section, syntactic pat-
terns often suffer from data sparsity. Further-
more, most pattern-based methods rely on term
frequency, which have the limitation of finding
infrequent but important product features. A re-
cent research (Xu et al, 2013) extracted infrequent
product features by a semi-supervised classifier,
which used word-syntactic pattern co-occurrence
statistics as features for the classifier. However,
this kind of feature is still sparse for infrequent
candidates. Our method adopts a semantic word
representation model, which can train dense fea-
tures unsupervisedly on a very large corpus. Thus,
the data sparsity problem can be alleviated.
3 The Proposed Method
We propose a semantics-based bootstrapping
method for product feature mining. Firstly, some
product feature seeds are automatically extracted.
Then, a semantic similarity graph is created to
capture lexical semantic clue, and a Convolutional
Neural Network (CNN) (Collobert et al, 2011) is
trained in each bootstrapping iteration to encode
contextual semantic clue. Finally we use Label
Propagation to find some reliable new seeds for
the training of the next bootstrapping iteration.
3.1 Automatic Seed Generation
The seed set consists of positive labeled examples
(i.e. product features) and negative labeled exam-
ples (i.e. noise terms). Intuitively, popular product
features are frequently mentioned in reviews, so
they can be extracted by simply mining frequently
occurring nouns (Hu and Liu, 2004). However,
this strategy will also find many noise terms (e.g.,
commonly used nouns like thing, one, etc.). To
produce high quality seeds, we employ a Domain
Relevance Measure (DRM) (Jiang and Tan, 2010),
which combines term frequency with a domain-
specific measuring metric called Likelihood Ratio
Test (LRT) (Dunning, 1993). Let ?(t) denotes the
LRT score of a product feature candidate t,
?(t) =
p
k
1
(1? p)
n
1
?k
1
p
k
2
(1? p)
n
2
?k
2
p
k
1
1
(1? p
1
)
n
1
?k
1
p
k
2
2
(1? p
2
)
n
2
?k
2
(1)
where k
1
and k
2
are the frequencies of t in the
review corpus R and a background corpus
1
B, n
1
and n
2
are the total number of terms in R and B,
p = (k
1
+ k
2
)/(n
1
+ n
2
), p
1
= k
1
/n
1
and p
2
=
k
2
/n
2
. Then a modified DRM
2
is proposed,
DRM(t) =
tf(t)
max[tf(?)]
?
1
log df(t)
?
| log ?(t)| ?min| log ?(?)|
max| log ?(?)| ?min| log ?(?)|
(2)
where tf(t) is the frequency of t inR and df(t) is
the frequency of t in B.
All nouns in R are ranked by DRM(t) in de-
scent order, where top N nouns are taken as the
positive example set V
+
s
. On the other hand, Xu
et al (2013) show that a set of general nouns sel-
dom appear to be product features. Therefore, we
employ their General Noun Corpus to create the
negative example set V
?
s
, where N most frequent
terms are selected. Besides, it is guaranteed that
V
+
s
? V
?
s
= ?, i.e., conflicting terms are taken as
negative examples.
3.2 Capturing Lexical Semantic Clue in a
Semantic Similarity Graph
To capture lexical semantic clue, each word is first
converted into word embedding, which is a con-
tinuous vector with each dimension?s value corre-
sponds to a semantic or grammatical interpretation
(Turian et al, 2010). Learning large-scale word
embeddings is very time-consuming (Collobert et
al., 2011), we thus employ a faster method named
Skip-gram model (Mikolov et al, 2013).
3.2.1 Learning Word Embedding for
Semantic Representation
Given a sequence of training words W =
{w
1
, w
2
, ..., w
m
}, the goal of the Skip-gram
model is to learn a continuous vector space EB =
{e
1
, e
2
, ..., e
m
}, where e
i
is the word embedding
of w
i
. The training objective is to maximize the
1
Google-n-Gram (http://books.google.com/ngrams) is
used as the background corpus.
2
The df(t) part of the original DRM is slightly modified
because we want a tf ? idf -like scheme (Liu et al, 2012).
338
average log probability of using word w
t
to pre-
dict a surrounding word w
t+j
,
?
EB = argmax
e
t
?EB
1
m
m
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
; e
t
)
(3)
where c is the size of the training window. Basi-
cally, p(w
t+j
|w
t
; e
t
) is defined as,
p(w
t+j
|w
t
; e
t
) =
exp(e
?
T
t+j
e
t
)
?
m
w=1
exp(e
?
T
w
e
t
)
(4)
where e
?
i
is an additional training vector associ-
ated with e
i
. This basic formulation is impracti-
cal because it is proportional to m. A hierarchical
softmax approximation can be applied to reduce
the computational cost to log
2
(m), see (Morin and
Bengio, 2005) for details.
To alleviate the data sparsity problem, EB is
first trained on a very large corpus
3
(denoted by
C), and then fine-tuned on the target review cor-
pusR. Particularly, for phrasal product features, a
statistic-based method in (Zhu et al, 2009) is used
to detect noun phrases in R. Then, an Unfold-
ing Recursive Autoencoder (Socher et al, 2011) is
trained on C to obtain embedding vectors for noun
phrases. In this way, semantics of infrequent terms
in R can be well captured. Finally, the phrase-
based Skip-gram model in (Mikolov et al, 2013)
is applied onR.
3.2.2 Building the Semantic Similarity Graph
Lexical semantic clue is captured by measuring se-
mantic similarity between terms. The underlying
motivation is that if we have known some product
feature seeds, then terms that are more semanti-
cally similar to these seeds are more likely to be
product features. For example, if screen is known
to be a product feature of mp3, and lcd is of high
semantic similarity with screen, we can infer that
lcd is also a product feature. Analogously, terms
that are semantically similar to negative labeled
seeds are not product features.
Word embedding naturally meets the demand
above: words that are more semantically similar
to each other are located closer in the embedding
space (Collobert et al, 2011). Therefore, we can
use cosine distance between two embedding vec-
tors as the semantic distance measuring metric.
Thus, our method does not rely on term frequency
3
Wikipedia(http://www.wikipedia.org) is used in practice.
to rank candidates. This could potentially improve
the ability of mining infrequent product features.
Formally, we create a semantic similarity graph
G = (V,E,W ), where V = {V
s
? V
c
} is the
vertex set, which contains the labeled seed set V
s
and the unlabeled candidate set V
c
; E is the edge
set which connects every vertex pair (u, v), where
u, v ? V ; W = {w
uv
: cos(EB
u
, EB
v
)} is a
function which associates a weight to each edge.
3.3 Encoding Contextual Semantic Clue
Using Convolutional Neural Network
The CNN is trained on each occurrence of seeds
that is found in review texts. Then for a candidate
term t, the CNN classifies all of its occurrences.
Since seed terms tend to have high frequency in
review texts, only a few seeds will be enough to
provide plenty of occurrences for the training.
3.3.1 The architecture of the Convolutional
Neural Network
The architecture of the Convolutional Neural Net-
work is shown in Figure 2. For a product feature
candidate t in sentence s, every consecutive sub-
sequence q
i
of s that containing t with a window
of length l is fed to the CNN. For example, as
in Figure 2, if t = {screen}, and l = 3, there
are three inputs: q
1
= [the, ipod, screen], q
2
=
[ipod, screen, is], q
3
= [screen, is, impressive].
Partially, t is replaced by a token ?*PF*? to re-
move its lexicon influence
4
.
 
Figure 2: The architecture of the Convolutional
Neural Network.
To get the output score, q
i
is first converted into
a concatenated vector x
i
= [e
1
; e
2
; ...; e
l
], where
e
j
is the word embedding of the j-th word. In
this way, the CNN serves as a soft pattern miner:
4
Otherwise, the CNN will quickly get overfitting on t, be-
cause very few seed lexicons are used for the training.
339
since words that have similar semantics have sim-
ilar low-dimension embedding vectors, the CNN
is less sensitive to lexicon change. The network is
computed by,
y
(1)
i
= tanh(W
(1)
x
i
+ b
(1)
) (5)
y
(2)
= max(y
(1)
i
) (6)
y
(3)
= W
(3)
y
(2)
+ b
(3)
(7)
where y
(i)
is the output score of the i-th layer, and
b
(i)
is the bias of the i-th layer; W
(1)
? R
h?(nl)
and W
(3)
? R
2?h
are parameter matrixes, where
n is the dimension of word embedding, and h is
the size of nodes in the hidden layer.
In conventional neural models, the candidate
term t is placed in the center of the window. How-
ever, from Example 2, when l = 5, we can see that
the best windows should be the bracketed texts
(Because, intuitively, the windows should contain
mp3, which is a strong evidence for finding the
product feature), where t = {screen} is at the
boundary. Therefore, we use Equ. 6 to formulate
a max-convolutional layer, which is aimed to en-
able the CNN to find more evidences in contexts
than conventional neural models.
Example 2:
(a) The [screen of this mp3 is] great.
(b) This [mp3 has a great screen].
3.3.2 Training
Let ? = {EB,W
(?)
, b
(?)
} denotes all the trainable
parameters. The softmax function is used to con-
vert the output score of the CNN to a probability,
p(t|X; ?) =
exp(y
(3)
)
?
|C|
j=1
exp(y
(3)
j
)
(8)
whereX is the input set for term t, andC = {0, 1}
is the label set representing product feature and
non-product feature, respectively.
To train the CNN, we first use V
s
to collect each
occurrence of the seeds in R to form a training
set T
s
. Then, the training criterion is to minimize
cross-entropy over T
s
,
?
? = argmin
?
|T
s
|
?
i=1
? log ?
i
p(t
i
|X
i
; ?) (9)
where ?
i
is the binomial target label distribution
for one entry. Backpropagation algorithm with
mini-batch stochastic gradient descent is used to
solve this optimization problem. In addition, some
useful tricks can be applied during the training.
The weight matrixes W
(?)
are initialized by nor-
malized initialization (Glorot and Bengio, 2010).
W
(1)
is pre-trained by an autoencoder (Hinton,
1989) to capture semantic compositionality. To
speed up the learning, a momentum method is ap-
plied (Sutskever et al, 2013).
3.4 Combining Lexical and Contextual
Semantic Clues by Label Propagation
We propose a Label Propagation algorithm to
combine both semantic clues in a unified process.
Each term t ? V is assumed to have a label dis-
tribution L
t
= (p
+
t
, p
?
t
), where p
+
t
denotes the
probability of the candidate being a product fea-
ture, and on the contrary, p
?
t
= 1? p
+
t
. The clas-
sified results of the CNN which encode contextual
semantic clue serve as the prior knowledge,
I
t
=
?
?
?
(1, 0), if t ? V
+
s
(0, 1), if t ? V
?
s
(r
+
t
, r
?
t
), if t ? V
c
(10)
where (r
+
t
, r
?
t
) is estimated by,
r
+
t
=
count
+
(t)
count
+
(t) + count
?
(t)
(11)
where count
+
(t) is the number of occurrences of
term t that are classified as positive by the CNN,
and count
?
(t) represents the negative count.
Label Propagation is applied to propagate the
prior knowledge distribution I to the product fea-
ture distribution L via semantic similarity graph
G, so that a product feature candidate is deter-
mined by exploring its semantic relations to all of
the seeds and other candidates globally. We pro-
pose an adapted version on the random walking
view of the Adsorption algorithm (Baluja et al,
2008) by updating the following formula until L
converges,
L
i+1
= (1? ?)M
T
L
i
+ ?DI (12)
where M is the semantic transition matrix built
from G; D = Diag[log tf(t)] is a diagonal ma-
trix of log frequencies, which is designed to as-
sign higher ?confidence? scores to more frequent
seeds; and ? is a balancing parameter. Particu-
larly, when ? = 0, we can set the prior knowledge
I without V
c
to L
0
so that only lexical semantic
clue is used; otherwise if ? = 1, only contextual
semantic clue is used.
340
3.5 The Bootstrapping Framework
We summarize the bootstrapping framework of the
proposed method in Algorithm 1. During boot-
strapping, the CNN is enhanced by Label Propaga-
tion which finds more labeled examples for train-
ing, and then the performance of Label Propaga-
tion is also improved because the CNN outputs a
more accurate prior distribution. After running for
several iterations, the algorithm gets enough seeds,
and a final Label Propagation is conducted to pro-
duce the results.
Algorithm 1: Bootstrapping using semantic clues
Input: The review corpusR, a large corpus C
Output: The mined product feature list P
Initialization: Train word embedding set EB first on
C, and then onR
Step 1: Generate product feature seeds V
s
(Section 3.1)
Step 2: Build semantic similarity graph G (Section 3.2)
while iter < MAX ITER do
Step 3: Use V
s
to collect occurrence set T
s
fromR
for training
Step 4: Train a CNNN on T
s
(Section 3.3)
Apply mini-batch SGD on Equ. 9;
Step 5: Run Label Propagation (Section 3.4)
Classify candidates usingN to setup I;
L
0
? I;
repeat
L
i+1
? (1? ?)M
T
L
i
+ ?DI;
until ||L
i+1
? L
i
||
2
< ?;
Step 6: Expand product feature seeds
Move top T terms from V
c
to V
s
;
iter++
end
Step 7: Run Label Propagation for a final result L
f
Rank terms by L
+
f
to get P , where L
+
f
> L
?
f
;
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets: We select two real world datasets to
evaluate the proposed method. The first one
is a benchmark dataset in Wang et al (2011),
which contains English review sets on two do-
mains (MP3 and Hotel)
5
. The second dataset is
proposed by Chinese Opinion Analysis Evalua-
tion 2008 (COAE 2008)
6
, where two review sets
(Camera and Car) are selected. Xu et al (2013)
had manually annotated product features on these
four domains, so we directly employ their annota-
tion as the gold standard. The detailed information
can be found in their original paper.
5
http://timan.cs.uiuc.edu/downloads.html
6
http://ir-china.org.cn/coae2008.html
Evaluation Metrics: We evaluate the proposed
method in terms of precision(P), recall(R) and F-
measure(F). The English results are evaluated by
exact string match. And for Chinese results, we
use an overlap matching metric, because deter-
mining the exact boundaries is hard even for hu-
man (Wiebe et al, 2005).
4.2 Experimental Settings
For English corpora, the pre-processing are the
same as that in (Qiu et al, 2009), and for Chinese
corpora, the Stanford Word Segmenter (Chang
et al, 2008) is used to perform word segmenta-
tion. We select three state-of-the-art syntax-based
methods to be compared with our method:
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009), which is
a conventional syntax-based method.
DP-HITS is an enhanced version of DP pro-
posed by Zhang et al (2010), which ranks product
feature candidates by
s(t) = log tf(t) ? importance(t) (13)
where importance(t) is estimated by the HITS al-
gorithm (Kleinberg, 1999).
SGW is the Sentiment Graph Walking algo-
rithm proposed in (Xu et al, 2013), which first
extracts syntactic patterns and then uses random
walking to rank candidates. Afterwards, word-
syntactic pattern co-occurrence statistic is used
as feature for a semi-supervised classifier TSVM
(Joachims, 1999) to further refine the results. This
two-stage method is denoted as SGW-TSVM.
LEX only uses lexical semantic clue. Label
Propagation is applied alone in a self-training
manner. The dimension of word embedding n =
100, the convergence threshold ? = 10
?7
, and the
number of expanded seeds T = 40. The size of
the seed set N is 40. To output product features,
it ranks candidates in descent order by using the
positive score L
+
f
(t).
CONT only uses contextual semantic clue,
which only contains the CNN. The window size
l is 5. The CNN is trained with a mini-batch size
of 50. The hidden layer size h = 250. Finally,
importance(t) in Equ. 13 is replaced with r
+
t
in
Equ. 11 to rank candidates.
LEX&CONT leverages both semantic clues.
341
Method
MP3 Hotel Camera Car Avg.
P R F P R F P R F P R F F
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
SGW 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
LEX 0.64 0.74 0.69 0.65 0.75 0.70 0.69 0.84 0.76 0.68 0.78 0.73 0.72
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72 0.71
SGW-TSVM 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
LEX&CONT 0.74 0.75 0.74 0.75 0.77 0.76 0.80 0.84 0.82 0.79 0.79 0.79 0.78
Table 1: Experimental results of product feature mining. The precision or recall of CONT is the average
performance over five runs with different random initialization of parameters of the CNN. Avg. stands
for the average score.
4.3 The Semantics-based Methods vs.
State-of-the-art Syntax-based Methods
The experimental results are shown in Table 1,
from which we have the following observations:
(i) Our method achieves the best performance
among all of the compared methods. We
also equally split the dataset into five sub-
sets, and perform one-tailed t-test (p ? 0.05),
which shows that the proposed semantics-
based method (LEX&CONT) significantly out-
performs the three syntax-based strong com-
petitors (DP, DP-HITS and SGW-TSVM).
(ii) LEX&CONT which leverages both lexical and
contextual semantic clues outperforms ap-
proaches that only use one kind of semantic
clue (LEX and CONT), showing that the com-
bination of the semantic clues is helpful.
(iii) Our methods which use only one kind of
semantic clue (LEX and CONT) outperform
syntax-based methods (DP, DP-HITS and
SGW). Comparing DP-HITS with LEX and
CONT, the difference between them is that
DP-HITS uses a syntax-pattern-based algo-
rithm to estimate importance(t) in Equ. 13,
while our methods use lexical or contextual se-
mantic clue instead. We believe the reason that
LEX or CONT is better is that syntactic pat-
terns only use discrete and local information.
In contrast, CONT exploits latent semantics of
each word in context, and LEX takes advantage
of word embedding, which is induced from
global word co-occurrence statistic. Further-
more, comparing SGW and LEX, both methods
are base on random surfer model, but LEX gets
better results than SGW. Therefore, the word-
word semantic similarity relation used in LEX
is more reliable than the word-syntactic pattern
relation used in SGW.
(iv) LEX&CONT achieves the highest recall
among all of the evaluated methods. Since
DP and DP-HITS rely on frequency for rank-
ing product features, infrequent candidates are
ranked low in their extracted list. As for SGW-
TSVM, the features they used for the TSVM
suffer from the data sparsity problem for in-
frequent terms. In contrast, LEX&CONT is
frequency-independent to the review corpus.
Further discussions on this observation are
given in the next section.
4.4 The Results on Extracting Infrequent
Product Features
We conservatively regard 30% product features
with the highest frequencies in R as frequent fea-
tures, so the remaining terms in the gold standard
are infrequent features. In product feature mining
task, frequent features are relatively easy to find.
Table 2 shows the recall of all the four approaches
for mining frequent product features. We can see
that the performance are very close among differ-
ent methods. Therefore, the recall mainly depends
on mining the infrequent features.
Method MP3 Hotel Camera Car
DP 0.89 0.92 0.86 0.84
DP-HITS 0.89 0.91 0.86 0.85
SGW-TSVM 0.87 0.92 0.88 0.87
LEX&CONT 0.89 0.91 0.89 0.87
Table 2: The recall of frequent product features.
Figure 3 gives the recall of infrequent prod-
uct features, where LEX&CONT achieves the best
performance. So our method is less influenced
by term frequency. Furthermore, LEX gets better
recall than CONT and all syntax-based methods,
which indicates that lexical semantic clue does aid
to mine more infrequent features as expected.
342
 1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(a) MP3
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(b) Hotel
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(c) Camera
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(d) Car
Figure 4: Accuracy (y-axis) of product feature seed expansion at each bootstrapping iteration (x-axis).
The error bar shows the standard deviation over five runs.
Method
MP3 Hotel Camera Car
P R F P R F P R F P R F
FW-5 0.62 0.63 0.62 0.64 0.64 0.64 0.68 0.73 0.70 0.67 0.66 0.66
FW-9 0.64 0.65 0.64 0.66 0.68 0.67 0.70 0.76 0.73 0.71 0.70 0.70
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72
Table 3: The results of convolutional method vs. the results of non-convolutional methods.
MP3 Hotel Camera Car
Reca
ll
.4
.5
.6
.7
.8
.9 DPDP-HITSSGW-TSVMCONTLEXLEX&CONT
Figure 3: The recall of infrequent features. The
error bar shows the standard deviation over five
different runs.
4.5 Lexical Semantic Clue vs. Contextual
Semantic Clue
This section studies the effects of lexical seman-
tic clue and contextual semantic clue during seed
expansion (Step 6 in Algorithm 1), which is con-
trolled by ?. When ? = 1, we get the CONT; and
if ? is set 0, we get the LEX. To take into account
the correctly expanded terms for both positive and
negative seeds, we use Accuracy as the evaluation
metric,
Accuracy =
#TP + #TN
# Extracted Seeds
where TP denotes the true positive seeds, and TN
denotes the true negative seeds.
Figure 4 shows the performance of seed ex-
pansion during bootstrapping, in which the accu-
racy is computed on 40 seeds (20 being positive
and 20 being negative) expanded in each itera-
tion. We can see that the accuracies of CONT and
LEX&CONT retain at a high level, which shows
that they can find reliable new product feature
seeds. However, the performance of LEX oscil-
lates sharply and it is very low for some points,
which indicates that using lexical semantic clue
alone is infeasible. On another hand, comparing
CONT with LEX in Table 1, we can see that LEX
performs generally better than CONT. Although
LEX is not so accurate as CONT during seed ex-
pansion, its final performance surpasses CONT.
Consequently, we can draw conclusion that CONT
is more suitable for the seed expansion, and LEX
is more robust for the final result production.
To combine advantages of the two kinds of se-
mantic clues, we set ? = 0.7 in Step 5 of Algo-
rithm 1, so that contextual semantic clue plays a
key role to find new seeds accurately. For Step 7,
we set ? = 0.3. Thus, lexical semantic clue is
emphasized for producing the final results.
4.6 The Effect of Convolutional Layer
Two non-convolutional variations of the proposed
method are used to be compared with the convo-
lutional method in CONT. FW-5 uses a traditional
neural network with a fixed window size of 5 to
replace the CNN in CONT, and the candidate term
to be classified is placed in the center of the win-
dow. Similarly, FW-9 uses a fixed window size
of 9. Note that CONT uses a 5-term dynamic
window containing the candidate term, so the ex-
ploited number of words in the context is equiva-
lent to FW-9.
343
Table 3 shows the experimental results. We can
see that the performance of FW-5 is much worse
than CONT. The reason is that FW-5 only exploits
half of the context as that of CONT, which is not
sufficient enough. Meanwhile, although FW-9 ex-
ploits equivalent range of context as that of CONT,
it gets lower precisions. It is because FW-9 has
approximately two times parameters in the param-
eter matrix W
(1)
than that in Equ. 5 of CONT,
which makes it more difficult to be trained with
the same amount of data. Also, lengths of many
sentences in the review corpora are shorter than 9.
Therefore, the convolutional approach in CONT is
the most effective way among these settings.
4.7 Parameter Study
We investigate two key parameters of the proposed
method: the initial number of seeds N , and the
size of the window l used by the CNN.
Figure 5 shows the performance under differ-
ent N , where the F-Measure saturates when N
equates to 40 and beyond. Hence, very few seeds
are needed for starting our algorithm.
 
N
10 20 30 40 50 60
F-Measure
.65
.70
.75
.80
.85
MP3
Hote l
Came ra
Car
Figure 5: F-Measure vs. N for the final results.
Figure 6 shows F-Measure under different win-
dow size l. We can see that the performance is
improved little when l is larger than 5. Therefore,
l = 5 is a proper window size for these datasets.
 
l
2 3 4 5 6 7
F-Measure
.5
.6
.7
.8
.9
MP3
Hote l
Came ra
Car
Figure 6: F-Measure vs. l for the final results.
5 Conclusion and Future Work
This paper proposes a product feature mining
method by leveraging contextual and lexical se-
mantic clues. A semantic similarity graph is built
to capture lexical semantic clue, and a convo-
lutional neural network is used to encode con-
textual semantic clue. Then, a Label Propaga-
tion algorithm is applied to combine both seman-
tic clues. Experimental results prove the effec-
tiveness of the proposed method, which not only
mines product features more accurately than con-
ventional syntax-based method, but also extracts
more infrequent product features.
In future work, we plan to extend the proposed
method to jointly mine product features along with
customers? opinions on them. The learnt seman-
tic representations of words may also be utilized
to predict fine-grained sentiment distributions over
product features.
Acknowledgement
This work was sponsored by the National
Basic Research Program of China (No.
2012CB316300), the National Natural Sci-
ence Foundation of China (No. 61272332 and
No. 61202329), the National High Technol-
ogy Development 863 Program of China (No.
2012AA011102), and CCF-Tencent Open Re-
search Fund. This work was also supported in
part by Noahs Ark Lab of Huawei Tech. Ltm.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak
Ravichandran, and Mohamed Aly. 2008. Video
suggestion and discovery for youtube: Taking ran-
dom walks through the view graph. In Proceedings
of the 17th International Conference on World Wide
Web, WWW ?08, pages 895?904, New York, NY,
USA. ACM.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, StatMT ?08, pages 224?232.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
344
dependency parses from phrase structure parses. In
Proceedings of the IEEE / ACL?06 Workshop on
Spoken Language Technology.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of the International Con-
ference on Artificial Intelligence and Statistics.
Geoffrey E. Hinton. 1989. Connectionist learning pro-
cedures. Artificial Intelligence, 40(1C3):185 ? 234.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Xing Jiang and Ah-Hwee Tan. 2010. Crctol: A
semantic-based domain ontology learning system.
Journal of the American Society for Information Sci-
ence and Technology, 61(1):150?168.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the 16th International Conference on
Machine Learning, pages 200?209.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 410?419, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: An unsupervised opinion miner from
unstructured product reviews. In Proceedings of
the 19th ACM International Conference on Informa-
tion and Knowledge Management, CIKM ?10, pages
1825?1828, New York, NY, USA. ACM.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on arti-
ficial intelligence and statistics, AISTATS05, pages
246?252.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS?2011, vol-
ume 24, pages 801?809.
Ilya Sutskever, James Martens, George Dahl, and Ge-
offrey Hinton. 2013. Distributed representations of
words and phrases and their compositionality. In
Proceedings of the 30 th International Conference
on Machine Learning.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, EMNLP ?09, pages 1533?
1541, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
345
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Mining opinion words and opinion tar-
gets in a two-stage framework. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1764?1773, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
1462?1470, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM Conference on Information and Knowledge
Management, CIKM ?09, pages 1799?1802, New
York, NY, USA. ACM.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM International Conference
on Information and Knowledge Management, CIKM
?06, pages 43?50, New York, NY, USA. ACM.
346
