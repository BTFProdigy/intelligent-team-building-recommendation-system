c? 2004 Association for Computational Linguistics
Verb Class Disambiguation Using
Informative Priors
Mirella Lapata? Chris Brew?
University of Sheffield Ohio State University
Levin?s (1993) study of verb classes is a widely used resource for lexical semantics. In her frame-
work, some verbs, such as give, exhibit no class ambiguity. But other verbs, such as write, have
several alternative classes. We extend Levin?s inventory to a simple statistical model of verb class
ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the
use of a disambiguated corpus. We additionally show that these preferences are useful as priors
for a verb sense disambiguator.
1. Introduction
Much research in lexical semantics has concentrated on the relation between verbs and
their arguments. Many scholars hypothesize that the behavior of a verb, particularly
with respect to the expression and interpretation of its arguments, is to a large extent
determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993;
Pinker 1989; Green 1974; Gropen et al 1989; Fillmore 1965). The correspondence be-
tween verbal meaning and syntax has been extensively studied in Levin (1993), which
argues that verbs which display the same diathesis alternations?alternations in the
realization of their argument structure?can be assumed to share certain meaning
components and to form a semantically coherent class.
The converse of this assumption is that verb behavior (i.e., participation in diathe-
sis alternations) can be used to provide clues about aspects of meaning, which in turn
can be exploited to characterize verb senses (referred to as classes in Levin?s [1993] ter-
minology). A major advantage of this approach is that criteria for assigning senses can
be more concrete than is traditionally assumed in lexicographic work (e.g., WordNet or
machine-readable dictionaries) concerned with sense distinctions (Palmer 2000). As an
example consider sentences (1)?(4), taken from Levin. Examples (1) and (2) illustrate
the dative and benefactive alternations, respectively. Dative verbs alternate between
the prepositional frame ?NP1 V NP2 to NP3? (see (1a)) and the double-object frame
?NP1 V NP2 NP3? (see (1b)), whereas benefactive verbs alternate between the double-
object frame (see (2a)) and the prepositional frame ?NP1 V NP2 for NP3? (see (2b)).
To decide whether a verb is benefactive or dative it suffices to test the acceptability
of the for and to frames. Verbs undergoing the conative alternation can be attested
either as transitive or as intransitive with a prepositional phrase headed by the word
at.1 The role filled by the object of the transitive variant is shared by the noun phrase
complement of at in the intransitive variant (see (3)). This example makes explicit that
class assignment depends not only on syntactic facts but also on judgments about
? Department of Computer Science, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK. E-mail:
mlap@dcs.shef.ac.uk.
? Department of Linguistics, Oxley Hall,1712 Neil Avenue, Columbus, OH. E-mail: cbrew@ling.ohio-
state.edu.
1 At is the most likely choice, but for some conative verbs the preposition is instead on or onto.
46
Computational Linguistics Volume 30, Number 1
semantic roles. Similarly, the possessor object alternation involves a possessor and a
possessed attribute that can be manifested either as the verbal object or as the object
of a prepositional phrase headed by for (see (4)).
(1) a. Bill sold a car to Tom.
b. Bill sold Tom a car.
(2) a. Martha carved the baby a toy.
b. Martha carved a toy for the baby.
(3) a. Paula hit the fence.
b. Paula hit at the fence.
(4) a. I admired his honesty.
b. I admired him for his honesty.
Observation of the semantic and syntactic behavior of pay and give reveals that
they pattern with sell in licensing the dative alternation. These verbs are all members
of the Give class. Verbs like make and build behave similarly to carve in licensing the
benefactive alternation and are members of the class of Build verbs. The verbs beat,
kick, and hit undergo the conative alternation; they are all members of the Hit verb
class. By grouping together verbs that pattern together with respect to diathesis alter-
nations, Levin (1993) defines approximately 200 verb classes, which she argues reflect
important semantic regularities. These analyses (and many similar ones by Levin and
her successors) rely primarily on straightforward syntactic and syntactico-semantic cri-
teria. To adopt this approach is to accept some limitations on the reach of our analyses,
since not all semantically interesting differences will have the appropriate reflexes in
syntax. Nevertheless, the emphasis on concretely available observables makes Levin?s
methodology a good candidate for automation (Palmer 2000).
Therefore, Levin?s (1993) classification has formed the basis for many efforts that
aim to acquire lexical semantic information from corpora. These exploit syntactic cues,
or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte
im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin?s classifi-
cation (in conjunction with other lexical resources) to create dictionaries that express
the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosen-
zweig, and Palmer 1997; Dorr and Jones 1996). Levin?s inventory of verbs and classes
has been also useful for applications such as machine translation (Dorr 1997; Palmer
and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin
2000), and document classification (Klavans and Kan 1998).
Although the classification provides a general framework for describing verbal
meaning, it says only which verb meanings are possible, staying silent on the relative
likelihoods of the different meanings. The inventory captures systematic regularities
in the meaning of words and phrases but falls short of providing a probabilistic model
of these regularities. Such a model would be useful in applications that need to resolve
ambiguity in the presence of multiple and conflicting probabilistic constraints.
More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists the
semantic classes and diathesis alternations. The mapping between verbs and classes
is not one to one. Of the 3,024 verbs which she covers, 784 are listed as having more
than one class. Even though Levin?s monosemous verbs outnumber her polysemous
verbs by a factor of nearly four to one, the total frequency of the former (4,252,715)
47
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 1
Polysemous verbs according to Levin.
Classes Verbs BNC frequency
1 2,239 4,252,715
2 536 2,325,982
3 173 738,854
4 43 395,212
5 23 222,747
6 7 272,669
7 2 26,123
10 1 4,427
Figure 1
Relation between number of classes and alternations.
is comparable to the total frequency of the latter (3,986,014). This means that close
to half of the cases processed by a semantic tagger would manifest some degree of
ambiguity. The frequencies are detailed in Table 1 and were compiled from a lemma-
tized version of the British National Corpus (BNC) (Burnard 1995). Furthermore, as
shown in Figure 1, the level of ambiguity increases in tandem with the number of
alternations licensed by a given verb. Consider, for example, verbs participating in
one alternation only: Of these, 90.4% have one semantic class, 8.6% have two classes,
0.7% have three classes, and 0.3% have four classes. In contrast, of the verbs licensing
six different alternations, 14% have one class, 17% have two classes, 12.4% have three
classes, 53.6% have four classes, 2% have six classes, and 1% have seven classes. As
ambiguity increases, so does the availability and potential utility of information about
diathesis alternations.
Palmer (2000) and Dang et al (1998) argue that syntactic frames and verb classes
are useful for developing principled classifications of verbs. We go beyond this, show-
ing that they can also be of assistance in disambiguation. Consider, for instance, the
verb serve, which is a member of four Levin classes: Give, Fit, Masquerade, and
Fulfilling. Each of these classes can in turn license four distinct syntactic frames.
48
Computational Linguistics Volume 30, Number 1
As shown in the examples2 below, in (5a) serve appears ditransitively and belongs to
the semantic class of Give verbs, in (5b) it occurs transitively and is a member of the
class of Fit verbs, in (5c) it takes the predicative complement as minister of the interior
and is a member of the class of Masquerade verbs. Finally, in sentence (5d) serve is
a Fulfilling verb and takes two complements, a noun phrase (an apprenticeship) and
a prepositional phrase headed by to (to a still-life photographer). In the case of verbs like
serve, we can guess their semantic class solely on the basis of the frame with which
they appear.
(5) a. I?m desperately trying to find a venue for the reception which can
serve our guests an authentic Italian meal. NP1 V NP2 NP3
b. The airline serves 164 destinations in over 75 countries. NP1 V NP2
c. Jean-Antoine Chaptal was a brilliant chemist and technocrat who
served Napoleon as minister of the interior from 1800 to 1805. NP1 V NP2
as NP3
d. Before her brief exposure to pop stardom, she served an
apprenticeship to a still-life photographer. NP1 V NP2 to NP3
But sometimes we do not have the syntactic information that would provide cues
for semantic disambiguation. Consider example (6). The verb write is a member of three
Levin classes, two of which (Message Transfer, Performance) take the double-
object frame. In this case, we have the choice between theMessage Transfer reading
(see (6a)) and the Performance reading (see (6b)). The same situation arises with the
verb toast, which is listed as a Prepare verb and a Judgment verb; both these classes
license the prepositional frame ?NP1 V NP2 for NP3.? In sentence (7a) the preferred
reading is that of Prepare rather than that of Judgment (see sentence (7b)). The verb
study is ambiguous among three classes when attested in the transitive frame: Learn
(see example (8a)), Sight (see example (8b)), and Assessment (see example (8c)). The
verb convey, when attested in the prepositional frame ?NP1 V NP2 to NP3,? can be
ambiguous between the Say class (see example (9a)) and the Send class (see exam-
ple (9b)). In order to correctly decide the semantic class for a given ambiguous verb,
we would need not only detailed semantic information about the verb?s arguments,
but also a considerable amount of world knowledge. Admittedly, selectional restric-
tions are sufficient for distinguishing (7a) from (7b) (one normally heats up inanimate
entities and salutes animate ones), but selectional restrictions alone are probably not
enough to disambiguate (6a) from (6b), since both letter and screenplay are likely to be
described as written material. Rather, we need fine-grained world knowledge: Both
scripts and letters can be written for someone: only letters can be written to someone.
(6) a. A solicitor wrote him a letter at the airport.
b. I want you to write me a screenplay called ?The Trip.?
(7) a. He sat by the fire and toasted a piece of bread for himself.
b. We all toasted Nigel for his recovery.
2 Unless otherwise stated, our example sentences were taken (possibly in simplified form) from the BNC.
49
Lapata and Brew Verb Class Disambiguation Using Informative Priors
(8) a. Chapman studied medicine at Cambridge.
b. Romanov studied the old man carefully, looking for some sign that he
knew exactly what had been awaiting him at the bank.
c. The alliance will also study the possibility of providing service to other
high-volume products, such as IBM and multi-vendor workstations.
(9) a. By conveying the news to her sister, she would convey by implication
something of her own anxiety.
b. The judge signed the committal warrant and the police conveyed Mr.
Butler to prison, giving the warrant to the governor.
This need for world knowledge (or at least a convenient way of approximating
this knowledge) is not an isolated phenomenon but manifests itself across a variety
of classes and frames (e.g., double object, transitive, prepositional frame: see exam-
ples (6)?(9)). We have argued that the concreteness of Levin-style verb classes is an
advantage, but this advantage would be compromised if we tried to fold too much
world knowledge into the classification. We do not do this. Instead, Section 5 of the
current article describes disambiguation experiments in which our probabilistic Levin
classes are used in tandem with proxies for appropriate world knowledge.
It is important to point out that Levin?s (1993) classification is not intended as an
exhaustive description of English verbs, their meanings, and their likelihood. Many
other classifications could have been built using the same principles. A different group-
ing might, for example, have occurred if finer or coarser semantic distinctions were
taken into account (see Merlo and Stevenson [2001] and Dang, Rosenzweig, and Palmer
[1997] for alternative classifications) or if the containment of ambiguity was one of the
classification objectives. As pointed out by Kipper, Dang, and Palmer (2000), Levin
classes exhibit inconsistencies, and verbs are listed in multiple classes, some of which
have conflicting sets of syntactic frames. This means that some ambiguities may also
arise as a result of accidental errors or inconsistencies. The classification was created
not with computational uses in mind, but for human readers, so it has not been nec-
essary to remedy all the errors and omissions that might cause trouble for machines.
Similar issues arise in almost all efforts to make use of preexisting lexical resources for
computational purposes (Briscoe and Carroll 1997), so none of the above comments
should be taken as criticisms of Levin?s achievement.
The objective of this article is to show how to train and use a probabilistic version
of Levin?s classification in verb sense disambiguation. We treat errors and inconsis-
tencies in the classification as noise. Although all our tests have used Levin?s classes
and the British National Corpus, the method itself depends neither on the details of
Levin?s classification nor on parochial facts about the English language. Our future
work will include tests on other languages, other classifications, and other corpora.
The model developed in this article takes as input a partially parsed corpus and
generates, for each combination of a verb and its syntactic frame, a probability distri-
bution over the available verb classes. The corpus itself does not have to be labeled
with classes. This makes it feasible to use large corpora. Our model is not immediately
useful for disambiguation, because it cannot discriminate among different occurrences
of the same verb and frame, but it can (as we show in Section 5) be used as a prior
in a full disambiguation system that does take appropriate account of context. The
model relies on several gross simplifications; it does not take selectional restrictions,
discourse, or pragmatic information into account but is demonstrably superior to sim-
pler priors that make no use of subcategorization.
50
Computational Linguistics Volume 30, Number 1
The remainder of this article is organized as follows. In Section 2 we describe the
probabilistic model and the estimation of the various model parameters. In Sections 3
and 4 we report on the results of two experiments that use the model to derive the
dominant class for polysemous verbs. Sections 5 and 6 discuss our verb class disam-
biguation experiments. We base our results on the BNC, a 100-million-word collection
of samples of written and spoken language from a wide range of sources designed
to represent a wide cross-section of current British English, both spoken and writ-
ten (Burnard 1995). We discuss our results in Section 7 and review related work in
Section 8.
2. The Prior Model
Consider again the sentences in (6). Assuming that we more often write something to
someone rather than for someone, we would like to derive Message Transfer as
the prevalent class for write rather than Performance. We view the choice of a class
for a polysemous verb in a given frame as maximizing the joint probability P(c, f , v),
where v is a verb subcategorizing for the frame f and inhabiting more than one Levin
class c:
P(c, f , v) = P(v) ? P(f |v) ? P(c|v, f ) (10)
Although the terms P(v) and P(f |v) can be estimated from the BNC (P(v) reduces
to the number of times a verb is attested in the corpus, and P(f |v) can be obtained
through parsing), the estimation of P(c|v, f ) is somewhat problematic, since it relies
on the frequency F(c, v, f ). The latter could be obtained straightforwardly if we had
access to a parsed corpus annotated with subcategorization and semantic-class infor-
mation. Lacking such a corpus we will assume that the semantic class determines the
subcategorization patterns of its members independently of their identity (see (11)):
P(c|v, f ) ? P(c|f ) (11)
The independence assumption is a simplification of Levin?s (1993) hypothesis that the
argument structure of a given verb is a direct reflection of its meaning. The rationale
behind the approximation in (11) is that since class formation is determined on the
basis of diathesis alternations, it is the differences in subcategorization structure, rather
than the identity of the individual verbs, that determine class likelihood. For example,
if we know that some verb subcategorizes for the double object and the prepositional
?NP1 V NP2 to NP3? frames, we can guess that it is a member of the Give class or
the Message Transfer class without knowing whether this verb is give, write, or tell.
Note that the approximation in (11) assumes that verbs of the same class uniformly
subcategorize (or not) for a given frame. This is evidently not true for all classes of
verbs. For example, all Give verbs undergo the dative diathesis alternation, and there-
fore we would expect them to be attested in both the double-object and prepositional
frame, but only a subset of Create verbs undergo the benefactive alternation. For
example, the verb invent is a Create verb and can be attested only in the benefactive
prepositional frame (I will invent a tool for you versus ?I will invent you a tool; see Levin
[1993] for details). By applying Bayes? law we write P(c|f ) as
P(c|f ) = P(f |c) ? P(c)
P(f )
(12)
By substituting (12) into (10), we can write P(c, f , v) as
P(c, f , v) =
P(v) ? P(f |v) ? P(f |c) ? P(c)
P(f )
(13)
51
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 2
Estimation of model parameters.
(a) P?(v) =
F(v)
?
i
F(vi)
(b) P?(f |v) = F(f , v)
F(v)
(c) P?(f |c) = F(f , c)
F(c)
(d) P?(c) =
F(c)
?
i
F(ci)
(e) P?(f ) =
F(f )
?
i
F(fi)
(f) F(f , c) =
?
i
F(c, f , vi)
(g) F(c) =
?
i
F(vi, c) (h) F(v, c) = F(v) ? P(c|v)
It is easy to obtain P(v) from the lemmatized BNC (see (a) in Table 2). In order to es-
timate the probability P(f |v), we need to know how many times a verb is attested with
a given frame. We acquired Levin-compatible subcategorization frames from the BNC
after performing a coarse-grained mapping between Levin?s frame descriptions and
surface syntactic patterns without preserving detailed semantic information about ar-
gument structure and thematic roles. This resulted in 80 frame types that were grossly
compatible with Levin. We used Gsearch (Corley et al 2001), a tool that facilitates the
search of arbitrary part-of-speech-tagged corpora for shallow syntactic patterns based
on a user-specified context-free grammar and a syntactic query. We specified a chunk
grammar for recognizing the verbal complex, NPs, and PPs and used Gsearch to ex-
tract tokens matching the frames specified in Levin. We discarded all frames with a
frequency smaller than five, as they were likely to be unreliable given our heuristic
approach. The frame probability P(f ) (see the denominator in (13) and equation (e)
in Table 2) was also estimated on the basis of the Levin-compatible subcategorization
frames that were acquired from the BNC.
We cannot read off P(f |c) in (13) directly from the corpus, because the corpus
is not annotated with verb classes. Nevertheless Levin?s (1993) classification records
the syntactic frames that are licensed by a given verb class (for example, Give verbs
license the double object and the ?NP1 V NP2 to NP3? frame) and also the number and
type of classes a given verb exhibits (e.g., write inhabits two classes, Performance
and Message Transfer). Furthermore, we know how many times a given verb is
attested with a certain frame in the corpus, as we have acquired Levin-compatible
frames from the BNC (see (b) in Table 2). We first explain how we obtain F(f , c), which
we rewrite as the sum of all occurrences of verbs v that are members of class c and
are attested in the corpus with frame f (see (c) and (f) in Table 2).
For monosemous verbs the count F(c, f , v) reduces to the number of times these
verbs have been attested in the corpus with a certain frame. For polysemous verbs,
we additionally need to know the class in which they were attested in the corpus.
Note that we don?t necessarily need an annotated corpus for class-ambiguous verbs
whose classes license distinct frames (see example (5)), provided that we have extracted
verb frames relatively accurately. For genuinely ambiguous verbs (i.e., verbs licensed
by classes that take the same frame), given that we don?t have access to a corpus
annotated with verb class information, we distribute the frequency of the verb and its
frame evenly across its semantic classes:
F(c, f , v) =
F(f , v)
|classes(v, f )| (14)
Here F(f , v) is the co-occurrence frequency of a verb and its frame and |classes(v, f )|
is the number of classes verb v is a member of when found with frame f . The joint
52
Computational Linguistics Volume 30, Number 1
Table 3
Estimation of F(c, f , v) and F(v, c).
Give F(Give, NPVNPNP, v) F(Give, NPVNPtoNP, v) F(v,Give)
feed
98
2
40
2
3, 263
4
give 25, 705 7, 502 126, 894
lend 343 648 2, 650
rent
6
2
10
1, 060
2
pass
181
3
256
3
19, 459
4
serve 85
58
2
15, 457
4
frequency of a class and its frame F(f , c) is then the sum of all verbs that are mem-
bers of the class c and are attested with frame f in the corpus (see (f) in Table 2).
Table 3 shows the estimation of the frequency F(c, f , v) for six verbs that are mem-
bers of the Give class. Consider for example feed, which is a member of four classes:
Give, Gorge, Feeding, and Fit. Of these classes, only Feeding and Give license
the double-object and prepositional frames. This is why the co-occurrence frequency
of feed with these frames is divided by two. The verb serve inhabits four classes. The
double-object frame is licensed by the Give class, whereas the prepositional frame
is additionally licensed by the Fulfilling class, and therefore the co-occurrence fre-
quency F(NPVNPtoNP, serve) is equally distributed between these two classes. This
is clearly a simplification, since one would expect F(c, f , v) to vary for different verb
classes. However, note that according to this estimation, F(f , c) will vary across frames
reflecting differences in the likelihood of a class being attested with a certain frame.
Both terms P(f |c) and P(c) in (13) rely on the class frequency F(c) (see (c) and (d)
in Table 2). We rewrite F(c) as the sum of all verbs attested in the corpus with class c
(see (g) in Table 2). For monosemous verbs the estimate of F(v, c) reduces to the count
of the verb in the corpus. Once again we cannot estimate F(v, c) for polysemous verbs
directly. The task would be straightforward if we had a corpus of verbs, each labeled
explicitly with class information. All we have is the overall frequency of a given verb
in the BNC and the number of classes it is a member of according to Levin (1993).
Since polysemous verbs can generally be the realization of more than one semantic
class, counts of semantic classes can be constructed by dividing the contribution from
the verb by the number of classes it belongs to (Resnik 1993; Lauer 1995). We rewrite
the frequency F(v, c) as shown in (h) in Table 2 and approximate P(c|v), the true
distribution of the verb and its classes, as follows:
P(c|v) ? F(v)|classes(v)| (15)
Here, F(v) is the number of times the verb v was observed in the corpus and |classes(v)|
is the number of classes c it belongs to. For example, in order to estimate the frequency
of the class Give, we consider all verbs that are listed as members of this class in Levin
(1993). The class contains thirteen verbs, among which six are polysemous. We will
obtain F(Give) by taking into account the verb frequency of the monosemous verbs
(|classes(v)| is one in this case) as well as distributing the frequency of the polyse-
mous verbs among their classes. For example, feed inhabits the classes Give, Gorge,
53
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Feeding, and Fit and occurs in the corpus 3,263 times. We will increment the count
of F(Give) by 3,2634 . Table 3 illustrates the estimation of F(v, c) for six members of the
Give class. The total frequency of the class is obtained by summing over the individual
values of F(v, c) (see equation (g) in Table 2).
The approach in (15) relies on the simplifying assumption that the frequency of
a verb is distributed evenly across its semantic classes. This is clearly not true for all
verbs. Consider, for example, the verb rent, which inhabits classes Give (Frank rented
Peter his room) and Get (I rented my flat for my sister). Intuitively speaking, the Give
sense of rent is more frequent than the Get sense, however, this is not taken into
account in (15), primarily because we do not know the true distribution of the classes
for rent. An alternative to (15) is to distribute the verb frequency unequally among verb
classes. Even though we don?t know how likely classes are in relation to a particular
verb, we can approximate how likely classes are in general on the basis of their size
(i.e., number of verbs that are members of each class). So then we can distribute a
verb?s frequency unequally, according to class size. This time we approximate P(c|v)
(see (h) in Table 2) by P(c|amb class), the probability of class c given the ambiguity
class3 amb class. The latter represents the set of classes a verb might inhabit:
F(v, c) ? F(v) ? P(c|amb class) (16)
We collapse verbs into ambiguity classes in order to reduce the number of parameters
that must be estimated; we certainly lose information, but the approximation makes it
easier to get reliable estimates from limited data. We simply approximate P(c|amb class)
using a heuristic based on class size:
P(c|amb class) ? |c|?
c ? amb class
|c| (17)
For each class we recorded the number of its members after discarding verbs
whose frequency was less than one per million in the BNC. This gave us a first ap-
proximation of the size of each class. We then computed, for each polysemous verb,
the total size of the classes of which it was a member. We calculated P(c|amb class) by
dividing the former by the latter (see equation (17)). We obtained the class frequency
F(c) by multiplying P(c|amb class) by the observed frequency of the verb in the BNC
(see equation (16)). As an example, consider again F(Give), which is calculated by
summing over all verbs that are members of this class (see (g) in Table 2). In order
to add the contribution of the verb feed, we need to distribute its corpus frequency
among the classes Give, Gorge, Feed, and Fit. The respective P(c|amb class) values
for these classes are 1538 ,
8
38 ,
3
38 , and
12
38 . By multiplying these by the frequency of feed in
the BNC (3,263), we obtain the values of F(v, c) given in Table 4. Only the frequency
F(feed,Give) is relevant for F(Give).
The estimation process just described involves at least one gross simplification,
since P(c|amb class) is calculated without reference to the identity of the verb in ques-
tion. For any two verbs that fall into the same set of classes, P(c|amb class) will be the
same, even though one or both may be atypical in its distribution across the classes.
Furthermore, the estimation tends to favor large classes, again irrespectively of the
identity of the verb in question. For example, the verb carry has three classes, Carry,
3 Our use of ambiguity classes is inspired by a similar use in hidden Markov model?based
part-of-speech tagging (Kupiec 1992).
54
Computational Linguistics Volume 30, Number 1
Table 4
Estimation of F(v, c) for the verb feed.
c |c| P(c|amb class) F(v, c)
Give 15 .39 1,272.57
Gorge 8 .21 685.23
Feed 3 .08 261.04
Fit 12 .32 1,044.16
Table 5
Ten most frequent classes using equal distribution of verb frequencies.
c F(c)
Characterize 601,647.4
Get 514,308.0
Say 450,444.6
Conjecture 390,618.4
Future Having 369,229.3
Declare 264,923.6
Amuse 258,857.9
Directed Motion 252,775.6
Message Transfer 248,238.7
Give 208,884.1
Table 6
Ten most frequent classes using unequal distribution of verb frequencies.
c F(c)
Get 453,843.6
Say 447,044.2
Characterize 404,734.2
Conjecture 382,193.8
Future Having 370,717.7
Declare 285,431.7
Directed Motion 255,821.6
Pocket 247,392.7
Amuse 205,729.4
Give 197,828.8
Fit, and Cost. Intuitively speaking, the Carry class is the most frequent (e.g., Smok-
ing can impair the blood which carries oxygen to the brain; I carry sugar lumps around with me).
However, since the Fit class (e.g., Thameslink presently carries 20,000 passengers daily)
is larger than the Carry class, it will be given a higher probability (.45 versus .4).
Our estimation scheme is clearly a simplification, but it is an empirical question how
much it matters. Tables 5 and 6 show the ten most frequent classes as estimated us-
ing (15) and (16). We explore the contribution of the two estimation schemes for P(c)
in Experiments 1 and 2.
The probabilities P(f |c) and P(f |v) will be unreliable when the frequencies F(f , v)
and F(f , c) are small and will be undefined when the frequencies are zero. Following
Hindle and Rooth (1993), we smooth the observed frequencies as shown in Table 7.
When F(f , v) is zero, the estimate used is proportional to the average F(f ,V)F(V) across
55
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 7
Smoothed estimates.
(a) P(f |v) ?
F(f , v) + F(f ,V)F(V)
F(v) + 1
(b) F(f , V) =
?
i
F( f , vi)
(c) P(f |c) ?
F(f , c) + Ff ,C)F(C)
F(c) + 1
(d) F(C) =
?
i
F( f , ci)
all verbs. Similarly, when F(f , c) is zero, our estimate is proportional to the average
F(f ,C)
F(C) across all classes. We do not claim that this scheme is perfect, but any deficien-
cies it may have are almost certainly masked by the effects of approximations and
simplifications elsewhere in the system.
We evaluated the performance of the model on all verbs listed in Levin (1993) that
are polysemous (i.e., members of more than one class) and take frames characteristic
of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and
Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less
well-known conative and possessor object alternations (see examples (1)?(4)). All four
alternations seem fairly productive; that is, a large number of verbs undergo these
alternations, according to Levin. A large number of classes license the frames that
are relevant for these alternations and the verbs that inhabit these classes are likely
to exhibit class ambiguity: 20 classes license the double object frame, 22 license the
prepositional frame ?NP1 V NP2 to NP3,? 17 classes license the benefactive ?NP1 V
NP2 for NP3? frame, 118 (out of 200) classes license the transitive frame, and 15 classes
license the conative ?NP1 V at NP2? frame.
In Experiment 1 we use the model to test the hypothesis that subcategorization
information can be used to disambiguate polysemous verbs. In particular, we concen-
trate on verbs like serve (see example (5)) that can be disambiguated solely on the
basis of their frame. In Experiment 2 we focus on verbs that are genuinely ambiguous;
that is, they inhabit a single frame and yet can be members of more than one seman-
tic class (e.g., write, study, see examples (6)?(9)). In this case, we use the probabilistic
model to assign a probability to each class the verb inhabits. The class with the highest
probability represents the dominant meaning for a given verb.
3. Experiment 1: Using Subcategorization to Resolve Verb Class Ambiguity
3.1 Method
In this experiment we focused solely on verbs whose meaning can be potentially
disambiguated by taking into account their subcategorization frame. A model that
performs badly on this task cannot be expected to produce any meaningful results for
genuinely ambiguous verbs.
We considered 128 verbs with the double-object frame (2.72 average class ambi-
guity), 101 verbs with the prepositional frame ?NP1 V NP2 to NP3? (2.59 average
class ambiguity), 113 verbs with the frame ?NP1 V NP2 for NP3? (2.63 average class
ambiguity), 42 verbs with the frame ?NP1 V at NP3? (3.05 average class ambiguity),
and 39 verbs with the transitive frame (2.28 average class ambiguity). The task was the
following: Given that we know the frame of a given verb, can we predict its semantic
class? In other words by varying the class c in the term P(c, f , v), we are trying to see
whether the class that maximizes it is the one predicted by the lexical semantics and
56
Computational Linguistics Volume 30, Number 1
Table 8
Model accuracy using equal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 60.9% 93.8%
NP1 V NP to NP3 63.3% 95.0%
NP1 V NP for NP3 63.6% 98.2%
NP1 V at NP2 2.4% 83.3%
NP1 V NP2 43.6% 87.2%
Combined 55.8% 93.9%
Table 9
Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 62.5% 93.8%
NP1 V NP to NP3 67.3% 95.0%
NP1 V NP for NP3 66.4% 98.2%
NP1 V at NP2 2.4% 85.7%
NP1 V NP2 41.0% 84.6%
Combined 56.7% 93.9%
the argument structure of the verb in question. The model?s responses were evaluated
against Levin?s (1993) classification. The model?s performance was considered correct
if it agreed with Levin in assigning a verb to an appropriate class given a particular
frame. Recall from Section 2 that we proposed two approaches for the estimation of
the class probability P(c). We explore the influence of P(c) by obtaining two sets of
results corresponding to the two estimation schemes.
3.2 Results
The model?s accuracy is shown in Tables 8 and 9. The results in Table 8 were ob-
tained using the estimation scheme for P(c) that relies on the even distribution of
the frequency of a verb across its semantic classes (see equation (15)). The results
in Table 9 were obtained using an alternative scheme that distributes verb frequency
unequally among verb classes by taking class size into account (see equation (16)).
As mentioned in Section 3.1, the results were based on comparison of the model?s
performance against Levin?s (1993) classification. We also compared the results to the
baseline of choosing the most likely class P(c) (without taking subcategorization in-
formation into account). The latter was determined on the basis of the approximations
described in Section 2 (see equation (9) in Table 2, as well as equations (15), (16),
and (17)).
The model achieved an accuracy of 93.9% using either type of estimation for P(c).
It also outperformed the baseline by 38.1% (see Table 8) and 37.2% (see Table 9). One
might expect an accuracy of 100%, since these verbs can be disambiguated solely on the
basis of their frame. However, the performance of our model achieves a lesser accuracy,
mainly because of the way we estimate the terms P(c) and P(f |c): We overemphasize
the importance of class information without taking into account how individual verbs
distribute across classes. Furthermore, we rely on frame frequencies acquired from the
BNC, using shallow syntactic analysis, which means that the correspondence between
57
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Levin?s (1993) frames and our acquired frames is not one to one. Except for the fact
that our frames do not preserve much of the linguistic information detailed Levin,
the number of frames acquired for a given verb can be a subset or superset of the
frames available in Levin. Note that the two estimation schemes yield comparable
performances. This is a positive result given the importance of P(c) in the estimation
of P(c, f , v).
A more demanding task for our probabilistic model will be with genuinely am-
biguous verbs (i.e., verbs for which the mapping between meaning and subcatego-
rization is not one to one). Although native speakers may have intuitions about the
dominant interpretation for a given verb, this information is entirely absent from Levin
(1993) and from the corpus on which our model is trained. In Experiment 2 we show
how our model can be used to recover this information.
4. Experiment 2: Using Corpus Distributions to Derive Verb Class Preferences
4.1 Method
We evaluated the performance of our model on 67 genuinely ambiguous verbs, that
is, verbs that inhabit a single frame and can be members of more than one seman-
tic class (e.g., write). These verbs are listed in Levin (1993) and undergo the dative,
benefactive, conative, and possessor object alternations. As in Experiment 1, we con-
sidered verbs with the double-object frame (3.27 average class ambiguity), verbs with
the frame ?NP1 V NP2 to NP3? (2.94 average class ambiguity), verbs with the frame
?NP1 V NP2 for NP3? (2.42 average class ambiguity), verbs with the frame ?NP1 V
at NP3? (2.71 average class ambiguity), and transitive verbs (2.77 average class am-
biguity). The model?s predictions were compared against manually annotated data
that was used only for testing purposes. The model was trained without access to a
disambiguated corpus. More specifically, corpus tokens characteristic of the verb and
frame in question were randomly sampled from the BNC and annotated with class
information so as to derive the true distribution of the verb?s classes in a particular
frame. We describe the verb selection procedure as follows.
Given the restriction that these verbs be semantically ambiguous in a specific
syntactic frame, we could not simply sample from the entire BNC, since this would
decrease the chances of finding the verb in the frame we are interested in. Instead,
a stratified sample was used: For all class-ambiguous verbs, tokens were randomly
sampled from the parsed data used for the acquisition of verb frame frequencies. The
model was evaluated on verbs for which a reliable sample could be obtained. This
meant that verbs had to have a frame frequency larger than 50. For verbs exceeding
this threshold 100 tokens were randomly selected and annotated with verb class infor-
mation. For verbs with frame frequency less than 100 and more than 50, no sampling
took place; the entire set of tokens was manually annotated. This selection procedure
resulted in 14 verbs with the double-object frame, 16 verbs with the frame ?NP1 V
NP2 to NP3,? 2 verbs with the frame ?NP1 V NP2 for NP3,? 1 verb with the frame
?NP1 V at NP3,? and 80 verbs with the transitive frame. From the transitive verbs
we further randomly selected 34 verbs; these were manually annotated and used for
evaluating the model?s performance.4
The selected tokens were annotated with class information by two judges, both
linguistics graduate students. The classes were taken from Levin (1993) and augmented
4 Although the model can yield predictions for any number of verbs, evaluation could not be performed
for all 80 verbs, as to perform such evaluation, our judges would have had to annotate 8,000 corpus
tokens.
58
Computational Linguistics Volume 30, Number 1
with the class Other, which was reserved for either corpus tokens that had the wrong
frame or those for which the classes in question were not applicable. The judges were
given annotation guidelines (for each verb) but no prior training (for details on the
annotation study see Lapata [2001]). The annotation provided a gold standard for
evaluating the model?s performance and enabled us to test whether humans agree
on the class annotation task. We measured the judges? agreement on the annotation
task using the kappa coefficient (Cohen 1960). In general, the agreement on the class
annotation task was good, with kappa values ranging from .66 to 1.00 (the mean kappa
was .80, SD = .09).
4.2 Results
We counted the performance of our model as correct if it agreed with the ?most pre-
ferred,? that is, the most frequent, verb class, as determined in the manually annotated
corpus sample by taking the average of the responses of both judges. As an example,
consider the verb feed, which in the double-object frame is ambiguous between the
classes Feed and Give. According to the model, Feed is the most likely class for feed.
Out of 100 instances of the verb feed in the double-object frame, 61 were manually
assigned the Feed class, 32 were assigned the Give class, and 6 were parsing mis-
takes (and therefore assigned the class Other). In this case the model?s outcome is
considered correct given that the corpus tokens also reveal a preference for the Feed
(i.e., the Feed instances outnumber the Give ones).
As in Experiment 1, we explored the influence of the parameter P(c) on the model?s
performance by obtaining two sets of results corresponding to the two estimation
schemes discussed in Section 2. The model?s accuracy is shown in Tables 10 and 11.
The results in Table 10 were obtained using the estimation scheme for P(c) that
relies on the even distribution of a verb?s frequency across its semantic classes (see
Table 10
Model accuracy using equal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 50.0% 78.6%
NP1 V NP to NP3 43.8% 68.8%
NP1 V NP for NP3 00.0% 100.0%
NP1 V at NP2 100.0% 100.0%
NP1 V NP2 47.1% 73.5%
Combined 46.2% 74.6%
Table 11
Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 50.0% 78.6%
NP1 V NP to NP3 43.8% 75.0%
NP1 V NP for NP3 00.0% 100.0%
NP1 V at NP2 100.0% 100.0%
NP1 V NP2 47.1% 67.6%
Combined 46.2% 73.1%
59
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Table 12
Semantic preferences for verbs with the double-object frame.
Verb Class K
call
?
Dub Get Other
93 -7.59 3 -8.12 4 .82
cook Build Prepare Other
28 -11.68 33 -11.50 1 1.00
declare
?
Declare Ref. Appear. Other
35 -10.51 18 -12.18 5 .89
feed
?
Feed Give Other
61 -10.63 32 -12.16 6 .73
find
?
Declare Get Other
36 -7.69 47 -7.43 17 .70
leave
?
Get Fulfill F. Have Other
6 -7.91 14 -10.40 56 ?7.66 23 .67
make
?
Build Dub Other
21 -7.25 66 -6.13 13 .79
pass
?
Give Send Throw Other
81 -8.84 0 -8.96 0 ?9.98 19 .93
save
?
Bill Get Other
24 -9.74 62 -9.59 14 .74
shoot Throw Get Other
91 -10.94 0 -9.99 5 1.00
take Bring-Take Perform Other
15 -7.02 40 -7.38 45 .77
write
?
Msg. Trans. Perform Other
54 -8.79 19 -9.05 18 .85
equation (15)). The results in Table 11 were obtained using a scheme that distributes
verb frequency unequally among verb classes by taking class size into account (see
equation (16)). As in Experiment 1, the results were compared to a simple baseline that
defaults to the most likely class without taking verb frame information into account
(see equation (g) in Table 2 as well as equations (15), (16), and (17)).
The model achieved an accuracy of 74.6% using the estimation scheme of equal
distribution and a accuracy of 73.1% using the estimation scheme of unequal distribu-
tion. The difference between the two estimation schemes is not statistically significant
( ?2(67) = 2.17, p = .84). Table 12 gives the distribution of classes for 12 polysemous
verbs taking the double-object frame as obtained from the manual annotation of corpus
tokens together with interannotator agreement (?). We also give the (log-transformed)
probabilities of these classes as derived by the model.5 The presence of the symbol
?
indicates that the model?s class preference for a given verb agrees with its distribution
in the corpus. The absence of
?
indicates disagreement. For the comparison shown in
Table 12, model class preferences were derived using the equal-distribution estimation
scheme for P(c) (see equation (15)).
As shown in Table 12, the model?s predictions are generally borne out in the corpus
data. Misclassifications are due mainly to the fact that the model does not take verb
class dependencies into account. Consider, for example, the verb cook. According to the
model the most likely class for cook is Build. Although it may generally be the case
that Build verbs (e.g., make, assemble, build) are more frequent than Prepare verbs
5 No probabilities are given for the Other class; this is not a Levin class, however, it was used by the
annotators, mainly to indicate parsing errors.
60
Computational Linguistics Volume 30, Number 1
(e.g., bake, roast, boil), the situation is reversed for cook. The same is true for the verb
shoot, which when attested in the double-object frame is more likely to be a Throw
verb (Jamie shot Mary a glance) rather than a Get verb (I will shoot you two birds).
Notice that our model is not context sensitive; that is, it does not derive class
rankings tailored to specific verbs, primarily because this information is not readily
available in the corpus, as explained in Section 2. However, we have effectively built a
prior model of the joint distribution of verbs, their classes, and their syntactic frames
that can be useful for disambiguating polysemous verbs in context. We describe our
class disambiguation experiments as follows.
5. Class Disambiguation
In the previous sections we focused on deriving a model of the distribution of Levin
classes without relying on annotated data and showed that this model infers the right
class for genuinely ambiguous verbs 74.6% of the time without taking the local context
of their occurrence into account. An obvious question is whether this information is
useful for disambiguating tokens rather than types. In the following we report on a
disambiguation experiment that takes advantage of this prior information.
Word sense disambiguation is often cast as a problem in supervised learning,
where a disambiguator is induced from a corpus of manually sense-tagged text. The
context within which the ambiguous word occurs is typically represented by a set
of linguistically motivated features from which a learning algorithm induces a repre-
sentative model that performs the disambiguation. A variety of classifiers have been
employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews),
the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classi-
fiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and
Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for
our experiments, as it is a very convenient framework for incorporating prior knowl-
edge and studying its influence on the classification task. In Section 5.1 we describe
a basic naive Bayesian classifier and show how it can be extended with informative
priors. In Section 5.2 we discuss the types of contextual features we use. We report on
our experimental results in Section 6.
5.1 Naive Bayes Classification
A naive Bayesian classifier assumes that all the feature variables representing a prob-
lem are conditionally independent, given the value of the classification variable. In
word sense disambiguation, the features (a1, a2, . . . , an) represent the context surround-
ing the ambiguous word, and the classification variable c is the sense (Levin class in
our case) of the ambiguous word in this particular context. Within a naive Bayes
approach, the probability of the class c given its context can be expressed as
P(c|ai) =
P(c)
n
?
i=1
P(ai|c)
P(ai)
(18)
where P(ai|c) is the probability that a test example is of class c given the contextual
features ai. Since the denominator P(ai) is constant for all classes c, the problem reduces
to finding the class c with the maximum value for the numerator:
P(c|ai) ? P(c)
n
?
i=1
P(ai|c) (19)
61
Lapata and Brew Verb Class Disambiguation Using Informative Priors
If we choose the prior P(c) to be uniform (P(c) = 1|c| for all c ? C), (19) can be
further simplified to
P(c|a) ?
n
?
i=1
P(ai|c) (20)
Assuming a uniform prior, a basic naive Bayesian classifier is as follows:
? c =
n
?
i=1
P(ai|c) (21)
Note, however, that we developed in the previous section two types of non-
uniform prior models. The first model derives P(c) heuristically from the BNC, ig-
noring the identity of the polysemous verb and its subcategorization profile, and the
second model estimates the class distribution P(c, v, f ) by taking the frame distribu-
tion into account. So, the naive Bayesian classifier in (21) can be extended with a
nonuniform prior:
? c = P(c)
n
?
i=1
P(ai|c) (22)
? c = P(c, v, f )
n
?
i=1
P(ai|c, f , v) (23)
where P(c) is estimated as shown in (d)?(g) in Table 2 and P(c, v, f ), the prior for
each class c corresponding to verb v in frame f , is estimated as explained in Sec-
tion 2 (see (13)). As before, ai are the contextual features. The probabilities P(ai|c)
and P(ai|c, f , v) can be estimated from the training data simply by counting the co-
occurrence of feature ai with class c (for (22)) or the co-occurrence of ai with class
c, verb v, and frame f (for (23)). For features that have zero counts, we use add-k
smoothing (Johnson 1932), where k is a number less than one.
5.2 Feature Space
As is common in word sense disambiguation studies, we experimented with two types
of context representations, collocations and co-occurrences. Co-occurrences simply in-
dicate whether a given word occurs within some number of words to the left or right
of an ambiguous word. In this case the contextual features are binary and represent
the presence or absence of a particular word in the current or preceding sentence. We
used four types of context in our experiments: left context (i.e., words occurring to
the left of the ambiguous word), right context (i.e., words occurring to the right of the
ambiguous word), the current sentence (i.e., words surrounding the ambiguous word),
and the current sentence together with its immediately preceding sentence. Punctua-
tion and capitalization were removed from the windows of context; noncontent words
were included. The context words were represented as lemmas or parts of speech.
Collocations are words that are frequently adjacent to the word to be disam-
biguated. We considered 12 types of collocations. Examples of collocations for the
verb write are illustrated in Table 13. The L columns in the table indicate the number
of words to the left of the ambiguous words, and the R columns, the number of words
to the right. So for example, the collocation 1L3R represents one word to the left and
three words to the right of the ambiguous word. Collocations again were represented
as lemmas (see Table 13) or parts of speech.
62
Computational Linguistics Volume 30, Number 1
Table 13
Features for collocations.
L R Example L R Example
0 1 write you 1 1 can write you
1 0 can write 1 2 can write you a
0 2 write you a 2 1 I can write you
2 0 I can write 1 3 can write you a story
0 3 write you a story 3 1 perhaps I can write you
3 0 perhaps I can write 2 4 I can write you a story sunshine
6. Experiment 3: Disambiguating Polysemous Verbs
6.1 Method
We tested the performance of our naive Bayesian classifiers on the 67 genuinely am-
biguous verbs on which the prior models were tested. Recall that these models were
trained without access to a disambiguated corpus, which was used only to determine
for a given verb and its frame its most likely meaning overall (i.e., across the corpus)
instead of focusing on the meaning of individual corpus tokens. The same corpus
was used for the disambiguation of individual tokens, excluding tokens assigned the
class Other. The naive Bayes classifiers were trained and tested using 10-fold cross-
validation on a set of 5,002 examples. These were representative of the frames ?NP1
V NP2,? ?NP1 V NP2 NP3,? ?NP1 V NP2 to NP3,? and ?NP1 V NP2 for NP3.? The
frame ?NP1 V at NP2? was excluded from our disambiguation experiments as it was
represented solely by the verb kick (50 instances).
In this study we compare a naive Bayesian classifier that relies on a uniform
prior (see (20)) against two classifiers that make use of nonuniform prior models:
The classifier in (22) effectively uses as prior the baseline model P(c) from Section 2,
whereas the classifier in (23) relies on the more informative model P(c, f , v). As a
baseline for the disambiguation task, we simply assign the most common class in the
training data to every instance in the test data, ignoring context and any form of prior
information (Pedersen 2001; Gale, Church, and Yarowsky 1992a). We also report an
upper bound on disambiguation performance by measuring how well human judges
agree with one another (percentage agreement) on the class assignment task. Recall
from Section 4.1 that our corpus was annotated by two judges with Levin-compatible
verb classes.
6.2 Results
The results of our class disambiguation experiments are summarized in Figures 2?5.
In order to investigate differences among different frames, we show how the naive
Bayesian classifiers perform for each frame individually. Figures 2?5 (x-axis) also re-
veal the influence of collocational features of different sizes (see Table 13) on the
classification task. Panel (b) in the figures presents the classifiers? accuracy when the
collocational features are encoded as lemmas; in panel (c) of the figures, the context
is represented as parts of speech, whereas in panel (a) of the figures, the context is
represented by both lemmas and parts of speech.
As can be seen in the figures, the naive Bayesian classifier with our informa-
tive prior (P(c, f , v), IPrior in Figures 2?5) generally outperforms the baseline prior
(P(c), BPrior in Figures 2?5), the uniform prior (UPrior in Figures 2?5), and the base-
line (Baseline in Figures 2?5) for all frames. Good performances are attained with
63
Lapata and Brew Verb Class Disambiguation Using Informative Priors
(a) (b) (c)
Figure 2
Word sense disambiguation accuracy for ?NP1 V NP2? frame.
(a) (b) (c)
Figure 3
Word sense disambiguation accuracy for ?NP1 V NP2 NP3? frame.
(a) (b) (c)
Figure 4
Word sense disambiguation accuracy for ?NP1 V NP2 to NP3? frame.
lemmas, parts of speech, and combination of the two. The naive Bayesian classifier
(IPrior) reaches the upper bound (UpBound in Figures 2?5) for the ditransitive frames
?NP1 V NP2 NP3,? ?NP1 V NP2 to NP3,? and ?NP1 V NP2 for NP3.?
The best accuracy (87.8%) for the transitive frame is achieved with the collocational
features 0L2R, 1L2R, and 1L3R (see Figures 2(a)?(c)). For the double-object frame, the
highest accuracy (90.8%) is obtained with features 0LR1 and 0L3R (see Figures 3(b)
and 3(c)). Similarly, for the ditransitive ?NP1 V NP2 to NP3? frame, the features 0L3R
and 0L1R yield the best accuracies (88.8%, see Figures 4(a)?(c)). Finally, for the ?NP1 V
NP2 for NP3? frame, accuracy (94.4%) is generally good for most features when an
informative prior is used. In fact, neither the uniform prior nor the baseline P(c)
outperforms the baseline for this frame.
64
Computational Linguistics Volume 30, Number 1
(a) (b) (c)
Figure 5
Word sense disambiguation accuracy for ?NP1 V NP2 for NP3? frame.
The context encoding (lemmas versus parts of speech) does not seem to have a
great influence on the disambiguation performance. Good accuracies are obtained with
either parts of speech or lemmas; combination of the two does not yield better results.
The classifier with the informative prior P(c, f , v) outperforms the baseline prior
P(c) and the uniform prior also when co-occurrences are used. However, the co-
occurrences never outperform the collocational features, for all four types of context.
The classifiers (regardless of the type of prior being used) never beat the baseline for
the frames ?NP1 V NP2? and ?NP1 V NP2 to NP3?. Accuracies above the baseline
are achieved for the frames ?NP1 V NP2 NP3? and ?NP1 V NP2 for NP3? when
an informative prior is used. Detailed results are summarized in the Appendix. Co-
occurrences and windows of large sizes traditionally work well for topical distinctions
(Gale, Church, and Yarowsky 1992b). Levin classes, however, typically capture differ-
ences in argument structure, that is, the types of objects or subjects that verbs select
for. Argument structure is approximated by our collocational features. For example, a
verb often taking a reflexive pronoun as its object is more likely to be a Reflexive
Verb of Appearance than a verb that never subcategorizes for a reflexive object.
There is not enough variability among the wider contexts surrounding a polysemous
verb to inform the class-disambiguation task, as the Levin classes often do not cross
topical boundaries.
7. Discussion
In this article, we have presented a probabilistic model of verb class ambiguity based
on Levin?s (1993) semantic classification. Our results show that subcategorization in-
formation acquired automatically from corpora provides important cues for verb class
disambiguation (Experiment 1). In the absence of subcategorization cues, corpus-based
distributions and quantitative approximations of linguistic concepts can be used to de-
rive a preference ordering on the set of verbal meanings (Experiment 2). The semantic
preferences that we have generated can be thought of as default semantic knowledge,
to be used in the absence of any explicit contextual or lexical semantic information to
the contrary (see Table 12). We have also shown that these preferences are useful for
disambiguating polysemous verbs within their local contexts of occurrence (Experi-
ment 3).
The approach is promising in that it achieves satisfactory results with a simple
model that has a straightforward interpretation in a Bayesian framework and does not
65
Lapata and Brew Verb Class Disambiguation Using Informative Priors
rely on the availability of annotated data. The model?s parameters are estimated using
simple distributions that can be extracted easily from corpora. Our model achieved an
accuracy of 93.9% (over a baseline of 56.7%) on the class disambiguation task (Exper-
iment 1) and an accuracy of 74.6% (over a baseline of 46.2%) on the task of deriving
dominant verb classes (Experiment 2). Our disambiguation experiments reveal that
this default semantic knowledge, when incorporated as a prior in a naive Bayes classi-
fier, outperforms the uniform prior and the baseline of always defaulting to the most
frequent class (Experiment 3). In fact, for three out of the four frames under study,
our classifier with the informative prior achieved upper-bound performance.
Although our results are promising, it remains to be shown that they generalize
across frames and alternations. Four types of alternations were investigated in this
study. However, Levin lists 79 alternations and approximately 200 classes. Although
distributions for different class/frame combinations can easily be derived automati-
cally, it remains to be shown that these distributions are useful for all verbs, frames,
and classes. Also note that the models described in the previous sections crucially rely
on the acquisition of relatively accurate frames from the corpus. It is a matter of future
work to examine how the quality of the acquired frames influences the disambiguation
task. Also, the assumption that the semantic class determines the subcategorization
patterns of its class members independently of their identity may not be harmless for
all classes and frames.
Although our original aim was to develop a probabilistic framework that exploits
Levin?s (1993) linguistic classification and the systematic correspondence between syn-
tax and semantics, a limitation of the model is that it cannot infer class information for
verbs not listed in Levin. For these verbs, P(c), and hence P(c, f , v), will be zero. Recent
work in computational linguistics (e.g., Schu?tze 1998) and cognitive psychology (e.g.,
Landauer and Dumais 1997) has shown that large corpora implicitly contain semantic
information, which can be extracted and manipulated in the form of co-occurrence
vectors. One possible approach would be to compute the centroid (geometric mean)
of the vectors of all members of a semantic class. Given an unknown verb (i.e., a verb
not listed in Levin), we can decide its semantic class by comparing its semantic vector
to the centroids of all semantic classes. For example, we could determine class mem-
bership on the basis of the distance to the closest centroid representing a semantic
class (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit). Another
approach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles
1991) to find similarities (via synonymy) between unknown verbs and verbs listed in
Levin. Once we have chosen a class for an unknown verb, we are entitled to assume
that it will share the broad syntactic and semantic properties of that class.
8. Related Work
Levin?s (1993) seminal study on diathesis alternations and verb semantic classes has
recently influenced work in dictionary creation (Dorr 1997; Dang et al 1998; Dorr
and Jones 1996) and notably lexicon acquisition on the basis of the assumption that
verbal meaning can be gleaned from corpora using cues pertaining to syntactic struc-
ture (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000).
Previous work in word sense disambiguation has not tackled explicitly the ambiguity
problems arising from Levin?s classification, although methods for deriving informa-
tive priors in an unsupervised manner have been proposed by Ciaramita and Johnson
(2000) and Chao and Dyer (2000) within the context of noun and adjective sense dis-
ambiguation, respectively. In this section we review related work on classification and
lexicon acquisition and compare it to our own work.
66
Computational Linguistics Volume 30, Number 1
Dang et al (1998) observe that verbs in Levin?s (1993) database are listed in more
than one class. The precise meaning of this ambiguity is left open to interpretation
in Levin, as it may indicate that the verb has more than one sense or that one sense
(i.e., class) is primary and the alternations for this class should take precedence over
the alternations for the other classes for which the verb is listed. Dang et al augment
Levin?s semantic classes with a set of ?intersective? classes that are created by grouping
together sets of existing classes that share a minimum of three overlapping members.
Intersective classes are more fine-grained than the original Levin classes and exhibit
more-coherent sets of syntactic frames and associated semantic components. Dang et
al. further argue that intersective classes are more compatible with WordNet than the
broader Levin classes and thus make it possible to attribute the semantic components
and associated sets of syntactic frames to specific WordNet senses as well, thereby
enriching the WordNet representation and providing explicit criteria for word sense
disambiguation.
Most statistical approaches, including ours, treat verbal-meaning assignment as a
semantic classification task. The underlying question is the following: How can corpus
information be exploited in deriving the semantic class for a given verb? Despite the
unifying theme of using corpora and corpus distributions for the acquisition task, the
approaches differ in the inventory of classes they employ, in the methodology used
for inferring semantic classes, and in the specific assumptions concerning the verbs to
be classified (e.g., can they be polysemous or not).
Merlo and Stevenson (2001) use grammatical features (acquired from corpora) to
classify verbs into three semantic classes: unergative, unaccusative, and object drop.
These classes are abstractions of Levin?s (1993) classes and as a result yield a coarser
classification. For example, object-drop verbs comprise a variety of Levin classes such
as Gesture verbs, Caring verbs, Load verbs, Push-Pull verbs, Meet verbs, So-
cial Interaction verbs, andAmuse verbs. Unergative, unaccusative, and object-drop
verbs have identical subcategorization patterns (i.e., they alternate between the tran-
sitive and intransitive frame), yet distinct argument structures, and therefore differ in
the thematic roles they assign to their arguments. For example, when attested in the
intransitive frame, the subject of an object-drop verb is an agent, whereas the subject
of an unaccusative verb is a theme. Under the assumption that differences in thematic
role assignment uniquely identify semantic classes, numeric approximations of argu-
ment structure are derived from corpora and used in a machine-learning paradigm to
place verbs in their semantic classes. The approach is evaluated on 59 verbs manually
selected from Levin (20 unergatives, 20 object drops, and 19 unaccusatives). It is as-
sumed that these verbs are monosemous, that is, they can be ergative, unergative, or
object drop. A decision-tree learner achieves an accuracy of 69.8% on the classification
task over a chance baseline of 34%.
Schulte im Walde (2000) uses subcategorization information and selectional re-
strictions to cluster verbs into Levin (1993)?compatible semantic classes. Subcatego-
rization frames are induced from the BNC using a robust statistical parser (Carroll and
Rooth 1998). The selectional restrictions are acquired using Resnik?s (1993) information-
theoretic measure of selectional association, which combines distributional and taxo-
nomic information (e.g., WordNet) to formalize how well a predicate associates with
a given argument. Two sets of experiments are run to evaluate the contribution of se-
lectional restrictions using two types of clustering algorithms: iterative clustering and
latent-class clustering (see Schulte im Walde [2000] for details). The approach is evalu-
ated on 153 verbs taken from Levin, 53 of which are polysemous (i.e., belong to more
than one class). The size of the derived clusters is restricted to four verbs and compared
to Levin: Verbs are classified correctly if they are members of a nonsingleton cluster
67
Lapata and Brew Verb Class Disambiguation Using Informative Priors
that is a subset of a Levin class. Polysemous verbs can be assigned to distinct clusters
only using the latent-class clustering method. The best results achieve a recall of 36%
and a precision of 61% (over a baseline of 5%, calculated as the number of randomly
created clusters that are subsets of a Levin class) using subcategorization information
only and iterative clustering. Inclusion of information about selectional restrictions
yields a lower accuracy of 38% (with a recall of 20%), again using iterative clustering.
Dorr and Jones (1996) use Levin?s (1993) classification to show that there is a pre-
dictable relationship between verbal meaning and syntactic behavior. They create a
database of Levin verb classes and the sentences exemplifying them (including both
positive and negative examples, i.e., examples marked with asterisks). A parser is used
to extract basic syntactic patterns for each semantic class. These patterns form the syn-
tactic signature of the class. Dorr and Jones show that 97.9% of the semantic classes can
be identified uniquely by their syntactic signatures. Grouping verbs (instead of classes)
with identical signatures to form a semantic class yields a 6.3% overlap with Levin
classes. Dorr and Jones?s results are somewhat difficult to interpret, since in practice
information about a verb and its syntactic signature is not available, and it is pre-
cisely this information that is crucial for classifying verbs into Levin classes. Schulte
im Walde?s study and our own study show that acquisition of syntactic signatures
(i.e., subcategorization frames) from corpora is feasible; however, these acquired sig-
natures are not necessarily compatible with Levin and in most cases will depart from
those derived by Dorr and Jones, as negative examples are not available in real corpora.
Ciaramita and Johnson (2000) propose an unsupervised Bayesian model for dis-
ambiguating verbal objects that uses WordNet?s inventory of senses. For each verb
the model creates a Bayesian network whose architecture is determined by WordNet?s
hierarchy and whose parameters are estimated from a list of verb-object pairs found in
a corpus. A common problem for unsupervised models trained on verb-object tuples
is that the objects can belong to more than one semantic class. The class ambiguity
problem is commonly resolved by considering each observation of an object as evi-
dence for each of the classes the word belongs to. The formalization of the problem in
terms of Bayesian networks allows the contribution of different senses to be weighted
via explaining away (Pearl 1988): If A is a hyponym of B and C is a hyponym of B,
and B is true, then finding that C is true makes A less likely.
Prior knowledge about the likelihoods of concepts is hand coded in the network
according to the following principles: (1) It is unlikely that any given class will be
a priori selected for; (2) if a class is selected, then its hyponyms are also likely to be
selected; (3) a word is likely as the object of a verb, if at least one of its classes is selected
for. Likely and unlikely here correspond to numbers that sum up to to one. Ciaramita
and Johnson show that their model outperforms other word sense disambiguation
approaches that do not make use of prior knowledge.
Chao and Dyer (2000) propose a method for the disambiguation of polysemous
adjectives in adjective-noun combinations that also uses Bayesian networks and Word-
Net?s taxonomic information. Prior knowledge about the likelihood of different senses
or semantic classes is derived heuristically by submitting queries (e.g., great hurricane)
to the AltaVista search engine and extrapolating from the number of returned doc-
uments the frequency of the adjective-noun pair (see Mihalcea and Moldovan [1998]
for details of this technique). For each polysemous adjective-noun combination, the
synonyms representative of each sense are retrieved from WordNet (e.g., {great, large,
big} vs. {great, neat, good}). Queries are submitted to AltaVista for each synonym-noun
pair; the number of documents returned is used then as an estimate of how likely
the different adjective senses are. Chao and Dyer obtain better results when prior
knowledge is factored into their Bayesian network.
68
Computational Linguistics Volume 30, Number 1
Our work focuses on the ambiguity inherently present in Levin?s (1993) classifi-
cation. The problem is ignored by Merlo and Stevenson (2001), who focus only on
monosemous verbs. Polysemous verbs are included in Schulte im Walde?s (2000) ex-
periments: The clustering approach can go so far as to identify more than one class
for a given verb without, however, providing information about its dominant class.
We recast Levin?s classification in a statistical framework and show in agreement with
Merlo and Stevenson and Schulte im Walde that corpus-based distributions provide
important information for semantic classification, especially in the case of polysemous
verbs whose meaning cannot be easily inferred from the immediate surrounding con-
text (i.e., subcategorization). We additionally show that the derived model is useful
not only for determining the most likely overall class for a given verb (i.e., across the
corpus), but also for disambiguating polysemous verb tokens in context.
Like Schulte im Walde (2000), our approach relies on subcategorization frames
extracted from the BNC (although using a different methodology). We employ Levin?s
inventory of semantic classes, arriving at a finer-grained classification than Merlo and
Stevenson (2001). In contrast to Schulte im Walde, we do not attempt to discover Levin
classes from corpora; instead, we exploit Levin?s classification and corpus frequencies
in order to derive a distribution of verbs, classes, and their frames that is not known a
priori but is approximated using simplifications. Our approach is not particularly tied
to Levin?s exact classification. We have presented in this article a general framework
that could be extended to related classifications such as the semantic hierarchy pro-
posed by Dang et al (1998). In fact the latter may be more appropriate than Levin?s
original classification for our disambiguation experiments, as it is based on a tighter
correspondence between syntactic frames and semantic components and contains links
to the WordNet taxonomy.
Prior knowledge with regard to the likelihood of polysemous verb classes is ac-
quired automatically in an unsupervised manner by combining corpus frequencies
estimated from the BNC and information inherent in Levin. The models proposed by
Chao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicable
to Levin?s classification, as the latter is not a hierarchy (and therefore not a DAG) and
cannot be straightforwardly mapped into a Bayesian network. However, in agreement
with Chao and Dyer and Ciaramita and Johnson, we show that prior knowledge about
class preferences improves word sense disambiguation performance.
Unlike Schulte im Walde (2000) and Merlo and Stevenson (2001), we ignore infor-
mation about the arguments of a given verb in the form of either selectional restric-
tions or argument structure while building our prior models. The latter information is,
however, indirectly taken into account in our disambiguation experiments: The verbs?
arguments are features for our naive Bayesian classifiers. Such information can be also
incorporated into the prior model in the form of conditional probabilities, where the
verb is, for example, conditioned on the thematic role of its arguments if this is known
(see Gildea and Jurafsky [2000] for a method that automatically labels thematic roles).
Unlike Stevenson and Merlo, Schulte im Walde, and Dorr and Jones (1996), we pro-
vide a general probabilistic model that assigns a probability to each class of a given
verb by calculating the probability of a complex expression in terms of the probability
of simpler expressions that compose it. We further show that this model is useful for
disambiguating polysemous verbs in context.
Appendix: Disambiguation Results with Co-occurrences
Figures 6?9 show the performances of our naive Bayesian classifier when co-occurrences
are used as features. We experimented with four types of context: left context (Left),
69
Lapata and Brew Verb Class Disambiguation Using Informative Priors
right context (Right), sentential context (Sentence), and the sentence within which
the ambiguous verb is found together with its immediately preceding sentence
(PSentence). The context was encoded as lemmas or parts of speech.
(a) (b)
Figure 6
Word sense disambiguation accuracy for ?NP1 V NP2? frame.
(a) (b)
Figure 7
Word sense disambiguation accuracy for ?NP1 V NP2 NP3? frame.
70
Computational Linguistics Volume 30, Number 1
(a) (b)
Figure 8
Word sense disambiguation accuracy for ?NP1 V to NP2 NP3? frame.
(a) (b)
Figure 9
Word sense disambiguation accuracy for ?NP1 V for NP2 NP3? frame.
71
Lapata and Brew Verb Class Disambiguation Using Informative Priors
Acknowledgments
Mirella Lapata was supported by ESRC
grant number R000237772. Thanks to Frank
Keller, Alex Lascarides, Katja Markert, Paola
Merlo, Sabine Schulte im Walde, Stacey
Bailey, Markus Dickinson, Anna Feldman,
Anton Rytting, and two anonymous
reviewers for valuable comments.
References
Boguraev, Branimir K. and Ted Briscoe. 1989.
Utilising the LDOCE grammar codes. In
Ted Briscoe and Branimir K. Boguraev,
editors, Computational Lexicography for
Natural Language Processing. Longman,
London, pages 85?116.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC.
Briscoe, Ted and Ann Copestake. 1999.
Lexical rules in constraint-based grammar.
Computational Linguistics, 25(4):487?526.
Burnard, Lou, 1995. The Users Reference Guide
for the British National Corpus. British
National Corpus Consortium, Oxford
University Computing Service.
Carroll, Glenn and Mats Rooth. 1998.
Valence induction with a head-lexicalized
PCFG. In Nancy Ide and Atro Voutilainen,
editors, Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 36?45, Granada, Spain.
Chao, Gerald and Michael G. Dyer. 2000.
Word sense disambiguation of adjectives
using probabilistic networks. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 152?158, Saarbru?cken, Germany.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional restrictions with
Bayesian networks. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 187?193,
Saarbru?cken, Germany.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:37?46.
Corley, Steffan, Martin Corley, Frank Keller,
Matthew W. Crocker, and Shari Trewin.
2001. Finding syntactic structure in
unparsed corpora: The Gsearch corpus
query system. Computers and the
Humanities, 35(2):81?94.
Cucerzan, Silviu and David Yarowsky. 2002.
Augmented mixture models for lexical
disambiguation. In Jan Hajic? and Yuji
Matsumoto, editors, Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 33?40,
Philadelphia, PA.
Dang, Hoa Trang, Karin Kipper, Martha
Palmer, and Joseph Rosenzweig. 1998.
Investigating regular sense extensions
based on intersective Levin classes. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 293?299,
Montre?al, Que?bec, Canada.
Dang, Hoa Trang, Joseph Rosenzweig, and
Martha Palmer. 1997. Associating
semantic components with intersective
Levin classes. In Proceedings of the First
AMTA SIG-IL Workshop on Interlinguas,
pages 1?8, San Diego, CA.
Dorr, Bonnie J. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation.
Machine Translation, 12(4):371?322.
Dorr, Bonnie J. and Doug Jones. 1996. Role
of word sense disambiguation in lexical
acquisition: Predicting semantics from
syntactic cues. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 322?327, Copenhagen,
Denmark.
Duda, Richard O. and Peter E. Hart. 1973.
Pattern Classification and Scene Analysis.
Wiley, New York.
Fillmore, Charles. 1965. Indirect Object
Constructions and the Ordering of
Transformations. Mouton, The Hague.
Gale, William, Kenneth W. Church, and
David Yarowsky. 1992a. Estimating upper
and lower bounds on the performance of
word-sense disambiguation programs. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics,
pages 249?256, Columbus, OH.
Gale, William A., Kenneth W. Church, and
David Yarowsky. 1992b. A method for
disambiguating word senses in a large
corpus. Computers and the Humanities,
26(5?6):415?439.
Gildea, Daniel and Daniel Jurafsky. 2000.
Automatic labelling of semantic roles. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
Hong Kong.
Goldberg, Adele. 1995. Constructions.
University of Chicago Press, Chicago.
Green, Georgia. 1974. Semantics and Syntactic
Regularity. Indiana University Press,
Bloomington.
Gropen, Jess, Steven Pinker,
Michelle Hollander, Richard M. Goldberg,
and Ronald Wilson. 1989. The learnability
and acquisition of the dative alternation.
72
Computational Linguistics Volume 30, Number 1
Language, 65(2):203?257.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Ide, Nancy and Jean Ve?ronis. 1998.
Introduction to the special issue on word
sense disambiguation: The state of the art.
Computational Linguistics, 24(1):1?40.
Jackendoff, Ray. 1983. Semantics and
Cognition. MIT Press, Cambridge, MA.
Johnson, William E. 1932. Probability: The
deductive and inductive problems. Mind,
49:409?423.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction of a
verb lexicon. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 691?696, Austin, TX.
Klavans, Judith and Min-Yen Kan. 1998. Role
of verbs in document analysis. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 680?688,
Montre?al, Que?bec, Canada.
Kupiec, Julian. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language, 6(3):225?242.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato?s problem: The
latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211?240.
Lapata, Maria. 1999. Acquiring lexical
generalizations from corpora: A case study
for diathesis alternations. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 397?404,
College Park, MD.
Lapata, Maria. 2001. The Acquisition and
Modeling of Lexical Knowledge: A
Corpus-Based Investigation of Systematic
Polysemy. Ph.D. thesis, University of
Edinburgh.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on Compound
Nouns. Ph.D. thesis, Macquarie University,
Sydney, Australia.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levow, Gina-Anne, Bonnie Dorr, and
Dekang Lin. 2000. Construction of
Chinese-English semantic hierarchy for
information retrieval. Technical report,
University of Maryland, College Park.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In
Proceedings of the First North American
Annual Meeting of the Association for
Computational Linguistics, pages 256?263,
Seattle, WA.
Merlo, Paola and Susanne Stevenson. 2001.
Automatic verb classification based on
statistical distribution of argument
structure. Computational Linguistics,
27(3):373?408.
Mihalcea, Rada and Dan Moldovan. 1998.
Word sense disambiguation based on
semantic density. In Sanda Harabagiu,
editor, Proceedings of COLING/ACL
Workshop on Usage of WordNet in Natural
Language Processing, pages 16?22,
Montre?al, Que?bec, Canada.
Miller, George A. and William G. Charles.
1991. Contextual correlates of semantic
similarity. Language and Cognitive Processes,
6(1):1?28.
Mooney, Raymond J. 1996. Comparative
experiments on disambiguating word
senses: An illustration of the role of bias
in machine learning. In Eric Brill and
Kenneth Church, editors, Proceedings of the
First Conference on Empirical Methods in
Natural Language Processing, pages 82?91,
Philadelphia, PA.
Ng, Hwee Tou. 1997. Exemplar-based word
sense disambiguation: Some recent
improvements. In Claire Cardie and
Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods
in Natural Language Processing, pages
208?216, Providence, RI.
Palmer, Martha. 2000. Consistent criteria for
sense distinctions. Computers and the
Humanities, 34(1?2):217?222.
Palmer, Martha and Zhibiao Wu. 1995. Verb
semantics for English-Chinese translation.
Machine Translation, 10:59?92.
Patel, Malti, John A. Bullinaria, and
Joseph P. Levy. 1998. Extracting semantic
representations from large text corpora. In
John A. Bullinaria, D. W. Glasspool, and
G. Houghton, editors, Proceedings of the
Fourth Workshop on Neural Computation and
Psychology, pages 199?212. Springer,
Berlin.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems. Morgan Kaufmann,
San Mateo, CA.
Pedersen, Ted. 2000. A simple approach to
building ensembles of naive Bayesian
classifiers for word sense disambiguation.
In Proceedings of the First North American
Annual Meeting of the Association for
Computational Linguistics, pages 63?69,
Seattle, WA.
Pedersen, Ted. 2001. A decision tree of
bigrams is an accurate predictor of word
sense. In Proceedings of the Second North
73
Lapata and Brew Verb Class Disambiguation Using Informative Priors
American Annual Meeting of the Association
for Computational Linguistics, pages 63?69,
Pittsburgh, PA.
Pedersen, Ted and Rebecca Bruce. 1998.
Knowledge lean word-sense
disambiguation. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 800?805, Madison, WI.
Pinker, Steven. 1989. Learnability and
Cognition: The Acquisition of Argument
Structure. MIT Press, Cambridge, MA.
Resnik, Philip Stuart. 1993. Selection and
Information: A Class-Based Approach to Lexical
Relationships. Ph.D. thesis, University of
Pennsylvania.
Schulte im Walde, Sabine. 2000. Clustering
verbs semantically according to their
alternation behaviour. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 747?753,
Saarbru?cken, Germany.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
Stede, Manfred. 1998. A generative
perspective on verb alternations.
Computational Linguistics, 24(3):401?430.
Talmy, Leonard. 1985. Lexicalisation
patterns: Semantic structure in lexical
forms. In Timothy Shopen, editor,
Language Typology and Syntactic Description,
III: Grammatical Categories and the Lexicon.
Cambrige University Press, Cambridge,
pages 57?149.
Yarowsky, David. 1994. Decision lists for
lexical ambiuguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics, pages 88?95, Las Cruces, NM.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 189?196, Cambridge,
MA.
Inducing German Semantic Verb Classes
from Purely Syntactic Subcategorisation Information
Sabine Schulte im Walde
Institut f?r Maschinelle Sprachverarbeitung
Universit?t Stuttgart
Azenbergstra?e 12, 70174 Stuttgart, Germany
schulte@ims.uni-stuttgart.de
Chris Brew
Department of Linguistics
The Ohio State University
Columbus, USA, OH 43210-1298
cbrew@ling.ohio-state.edu
Abstract
The paper describes the application of k-
Means, a standard clustering technique,
to the task of inducing semantic classes
for German verbs. Using probability
distributions over verb subcategorisation
frames, we obtained an intuitively plausi-
ble clustering of 57 verbs into 14 classes.
The automatic clustering was evaluated
against independently motivated, hand-
constructed semantic verb classes. A
series of post-hoc cluster analyses ex-
plored the influence of specific frames and
frame groups on the coherence of the verb
classes, and supported the tight connec-
tion between the syntactic behaviour of
the verbs and their lexical meaning com-
ponents.
1 Introduction
A long-standing linguistic hypothesis asserts a tight
connection between the meaning components of a
verb and its syntactic behaviour: To a certain ex-
tent, the lexical meaning of a verb determines its be-
haviour, particularly with respect to the choice of its
arguments. The theoretical foundation has been es-
tablished in extensive work on semantic verb classes
such as (Levin, 1993) for English and (V?zquez
et al, 2000) for Spanish: each verb class contains
verbs which are similar in their meaning and in their
syntactic properties.
From a practical point of view, a verb classifi-
cation supports Natural Language Processing tasks,
since it provides a principled basis for filling gaps in
available lexical knowledge. For example, the En-
glish verb classification has been used for applica-
tions such as machine translation (Dorr, 1997), word
sense disambiguation (Dorr and Jones, 1996), and
document classification (Klavans and Kan, 1998).
Various attempts have been made to infer conve-
niently observable morpho-syntactic and semantic
properties for English verb classes (Dorr and Jones,
1996; Lapata, 1999; Stevenson and Merlo, 1999;
Schulte im Walde, 2000; McCarthy, 2001).
To our knowledge this is the first work to ob-
tain German verb classes automatically. We used
a robust statistical parser (Schmid, 2000) to ac-
quire purely syntactic subcategorisation information
for verbs. The information was provided in form
of probability distributions over verb frames for
each verb. There were two conditions: the first
with relatively coarse syntactic verb subcategorisa-
tion frames, the second a more delicate classifica-
tion subdividing the verb frames of the first con-
dition using prepositional phrase information (case
plus preposition). In both conditions verbs were
clustered using k-Means, an iterative, unsupervised,
hard clustering method with well-known properties,
cf. (Kaufman and Rousseeuw, 1990). The goal of a
series of cluster analyses was (i) to find good values
for the parameters of the clustering process, and (ii)
to explore the role of the syntactic frame descrip-
tions in verb classification, to demonstrate the im-
plicit induction of lexical meaning components from
syntactic properties, and to suggest ways in which
the syntactic information might further be refined.
Our long term goal is to support the development of
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 223-230.
                         Proceedings of the 40th Annual Meeting of the Association for
high-quality and large-scale lexical resources.
2 Syntactic Descriptors for Verb Frames
The syntactic subcategorisation frames for German
verbs were obtained by unsupervised learning in a
statistical grammar framework (Schulte im Walde et
al., 2001): a German context-free grammar contain-
ing frame-predicting grammar rules and information
about lexical heads was trained on 25 million words
of a large German newspaper corpus. The lexi-
calised version of the probabilistic grammar served
as source for syntactic descriptors for verb frames
(Schulte im Walde, 2002b).
The verb frame types contain at most three
arguments. Possible arguments in the frames
are nominative (n), dative (d) and accusative (a)
noun phrases, reflexive pronouns (r), prepositional
phrases (p), expletive es (x), non-finite clauses (i),
finite clauses (s-2 for verb second clauses, s-dass for
dass-clauses, s-ob for ob-clauses, s-w for indirect
wh-questions), and copula constructions (k). For
example, subcategorising a direct (accusative case)
object and a non-finite clause would be represented
by nai. We defined a total of 38 subcategorisation
frame types, according to the verb subcategorisa-
tion potential in the German grammar (Helbig and
Buscha, 1998), with few further restrictions on ar-
gument combination.
We extracted verb-frame distributions from the
trained lexicalised grammar. Table 1 shows an
example distribution for the verb glauben ?to
think/believe? (for probability values   1%).
Frame Prob
ns-dass 0.27945
ns-2 0.27358
np 0.09951
n 0.08811
na 0.08046
ni 0.05015
nd 0.03392
nad 0.02325
nds-2 0.01011
Table 1: Probability distribution for glauben
We also created a more delicate version of subcate-
gorisation frames that discriminates between differ-
ent kinds of pp-arguments. This was done by dis-
tributing the frequency mass of prepositional phrase
frame types (np, nap, ndp, npr, xp) over the prepo-
sitional phrases, according to their frequencies in
the corpus. Prepositional phrases are referred to by
case and preposition, such as ?Dat.mit?, ?Akk.f?r?.
The resulting lexical subcategorisation for reden and
the frame type np whose total joint probability is
0.35820, is displayed in Table 2 (for probability val-
ues
  1%).
Refined Frame Prob
np:Akk.?ber acc / ?about? 0.11981
np:Dat.von dat / ?about? 0.11568
np:Dat.mit dat / ?with? 0.06983
np:Dat.in dat / ?in? 0.02031
Table 2: Refined np distribution for reden
The subcategorisation frame descriptions were for-
mally evaluated by comparing the automatically
generated verb frames against manual definitions in
the German dictionary Duden ? Das Stilw?rterbuch
(Dudenredaktion, 2001). The F-score was 65.30%
with and 72.05% without prepositional phrase in-
formation: the automatically generated data is both
easy to produce in large quantities and reliable
enough to serve as proxy for human judgement
(Schulte im Walde, 2002a).
3 German Semantic Verb Classes
Semantic verb classes have been defined for sev-
eral languages, with dominant examples concern-
ing English (Levin, 1993) and Spanish (V?zquez et
al., 2000). The basic linguistic hypothesis underly-
ing the construction of the semantic classes is that
verbs in the same class share both meaning compo-
nents and syntactic behaviour, since the meaning of
a verb is supposed to influence its behaviour in the
sentence, especially with regard to the choice of its
arguments.
We hand-constructed a concise classification with
14 semantic verb classes for 57 German verbs before
we initiated any clustering experiments. We have on
hand a larger set of verbs and a more elaborate clas-
sification, but choose to work on the smaller set for
the moment, since an important component of our
research program is an informative post-hoc analysis
which becomes infeasible with larger datasets. The
semantic aspects and majority of verbs are closely
related to Levin?s English classes. They are consis-
tent with the German verb classification in (Schu-
macher, 1986) as far as the relevant verbs appear in
his less extensive semantic ?fields?.
1. Aspect: anfangen, aufh?ren, beenden, begin-
nen, enden
2. Propositional Attitude: ahnen, denken,
glauben, vermuten, wissen
3. Transfer of Possession (Obtaining): bekom-
men, erhalten, erlangen, kriegen
4. Transfer of Possession (Supply): bringen,
liefern, schicken, vermitteln, zustellen
5. Manner of Motion: fahren, fliegen, rudern,
segeln
6. Emotion: ?rgern, freuen
7. Announcement: ank?ndigen, bekanntgeben,
er?ffnen, verk?nden
8. Description: beschreiben, charakterisieren,
darstellen, interpretieren
9. Insistence: beharren, bestehen, insistieren,
pochen
10. Position: liegen, sitzen, stehen
11. Support: dienen, folgen, helfen, unterst?tzen
12. Opening: ?ffnen, schlie?en
13. Consumption: essen, konsumieren, lesen,
saufen, trinken
14. Weather: blitzen, donnern, d?mmern, nieseln,
regnen, schneien
The class size is between 2 and 6, no verb ap-
pears in more than one class. For some verbs this is
something of an oversimplification; for example, the
verb bestehen is assigned to verbs of insistence, but
it also has a salient sense more related to existence.
Similarly, schlie?en is recorded under open/close, in
spite of the fact it also has a meaning related to infer-
ence and the formation of conclusions. The classes
include both high and low frequency verbs, because
we wanted to make sure that our clustering technol-
ogy was exercised in both data-rich and data-poor
situations. The corpus frequencies range from 8 to
31,710.
Our target classification is based on semantic in-
tuitions, not on our knowledge of the syntactic be-
haviour. As an extreme example, the semantic class
Support contains the verb unterst?tzen, which syn-
tactically requires a direct object, together with the
three verbs dienen, folgen, helfen which dominantly
subcategorise an indirect object. In what follows we
will show that the semantic classification is largely
recoverable from the patterns of verb-frame occur-
rence.
4 Clustering Methodology
Clustering is a standard procedure in multivariate
data analysis. It is designed to uncover an inher-
ent natural structure of the data objects, and the
equivalence classes induced by the clusters provide
a means for generalising over these objects. In our
case, clustering is realised on verbs: the data objects
are represented by verbs, and the data features for
describing the objects are realised by a probability
distribution over syntactic verb frame descriptions.
Clustering is applicable to a variety of areas in
Natural Language Processing, e.g. by utilising
class type descriptions such as in machine transla-
tion (Dorr, 1997), word sense disambiguation (Dorr
and Jones, 1996), and document classification (Kla-
vans and Kan, 1998), or by applying clusters for
smoothing such as in machine translation (Prescher
et al, 2000), or probabilistic grammars (Riezler et
al., 2000).
We performed clustering by the k-Means algo-
rithm as proposed by (Forgy, 1965), which is an un-
supervised hard clustering method assigning   data
objects to exactly  clusters. Initial verb clusters are
iteratively re-organised by assigning each verb to its
closest cluster (centroid) and re-calculating cluster
centroids until no further changes take place.
One parameter of the clustering process is the
distance measure used. Standard choices include
the cosine, Euclidean distance, Manhattan metric,
and variants of the Kullback-Leibler (KL) diver-
gence. We concentrated on two variants of KL in
Equation (1): information radius, cf. Equation (2),
and skew divergence, recently shown as an effective
measure for distributional similarity (Lee, 2001), cf.
Equation (3).

	








Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 49?56,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Finite-State Model of Human Sentence Processing
Jihyun Park and Chris Brew
Department of Linguisitcs
The Ohio State University
Columbus, OH, USA
{park|cbrew}@ling.ohio-state.edu
Abstract
It has previously been assumed in the
psycholinguistic literature that finite-state
models of language are crucially limited
in their explanatory power by the local-
ity of the probability distribution and the
narrow scope of information used by the
model. We show that a simple computa-
tional model (a bigram part-of-speech tag-
ger based on the design used by Corley
and Crocker (2000)) makes correct predic-
tions on processing difficulty observed in a
wide range of empirical sentence process-
ing data. We use two modes of evaluation:
one that relies on comparison with a con-
trol sentence, paralleling practice in hu-
man studies; another that measures prob-
ability drop in the disambiguating region
of the sentence. Both are surprisingly
good indicators of the processing difficulty
of garden-path sentences. The sentences
tested are drawn from published sources
and systematically explore five different
types of ambiguity: previous studies have
been narrower in scope and smaller in
scale. We do not deny the limitations of
finite-state models, but argue that our re-
sults show that their usefulness has been
underestimated.
1 Introduction
The main purpose of the current study is to inves-
tigate the extent to which a probabilistic part-of-
speech (POS) tagger can correctly model human
sentence processing data. Syntactically ambigu-
ous sentences have been studied in great depth in
psycholinguistics because the pattern of ambigu-
ity resolution provides a window onto the human
sentence processing mechanism (HSPM). Prima
facie it seems unlikely that such a tagger will be
adequate, because almost all previous researchers
have assumed, following standard linguistic the-
ory, that a formally adequate account of recur-
sive syntactic structure is an essential component
of any model of the behaviour. In this study, we
tested a bigram POS tagger on different types of
structural ambiguities and (as a sanity check) to
the well-known asymmetry of subject and object
relative clause processing.
Theoretically, the garden-path effect is defined
as processing difficulty caused by reanalysis. Em-
pirically, it is attested as comparatively slower
reading time or longer eye fixation at a disam-
biguating region in an ambiguous sentence com-
pared to its control sentences (Frazier and Rayner,
1982; Trueswell, 1996). That is, the garden-path
effect detected in many human studies, in fact, is
measured through a ?comparative? method.
This characteristic of the sentence processing
research design is reconstructed in the current
study using a probabilistic POS tagging system.
Under the assumption that larger probability de-
crease indicates slower reading time, the test re-
sults suggest that the probabilistic POS tagging
system can predict reading time penalties at the
disambiguating region of garden-path sentences
compared to that of non-garden-path sentences
(i.e. control sentences).
2 Previous Work
Corley and Crocker (2000) present a probabilistic
model of lexical category disambiguation based on
a bigram statistical POS tagger. Kim et al (2002)
suggest the feasibility of modeling human syntac-
tic processing as lexical ambiguity resolution us-
ing a syntactic tagging system called Super-Tagger
49
(Joshi and Srinivas, 1994; Bangalore and Joshi,
1999). Probabilistic parsing techniques also have
been used for sentence processing modeling (Ju-
rafsky, 1996; Narayanan and Jurafsky, 2002; Hale,
2001; Crocker and Brants, 2000). Jurafsky (1996)
proposed a probabilistic model of HSPM using
a parallel beam-search parsing technique based
on the stochastic context-free grammar (SCFG)
and subcategorization probabilities. Crocker and
Brants (2000) used broad coverage statistical pars-
ing techniques in their modeling of human syn-
tactic parsing. Hale (2001) reported that a proba-
bilistic Earley parser can make correct predictions
of garden-path effects and the subject/object rela-
tive asymmetry. These previous studies have used
small numbers of examples of, for example, the
Reduced-relative clause ambiguity and the Direct-
Object/Sentential-Complement ambiguity.
The current study is closest in spirit to a pre-
vious attempt to use the technology of part-
of-speech tagging (Corley and Crocker, 2000).
Among the computational models of the HSPM
mentioned above, theirs is the simplest. They
tested a statistical bigram POS tagger on lexi-
cally ambiguous sentences to investigate whether
the POS tagger correctly predicted reading-time
penalty. When a previously preferred POS se-
quence is less favored later, the tagger makes a re-
pair. They claimed that the tagger?s reanalysis can
model the processing difficulty in human?s disam-
biguating lexical categories when there exists a
discrepancy between lexical bias and resolution.
3 Experiments
In the current study, Corley and Crocker?s model
is further tested on a wider range of so-called
structural ambiguity types. A Hidden Markov
Model POS tagger based on bigrams was used.
We made our own implementation to be sure of
getting as close as possible to the design of Cor-
ley and Crocker (2000). Given a word string,
w0, w1, ? ? ? , wn, the tagger calculates the proba-
bility of every possible tag path, t0, ? ? ? , tn. Un-
der the Markov assumption, the joint probability
of the given word sequence and each possible POS
sequence can be approximated as a product of con-
ditional probability and transition probability as
shown in (1).
(1) P (w0, w1, ? ? ? , wn, t0, t1, ? ? ? , tn)
? ?ni=1P (wi|ti) ? P (ti|ti?1), where n ? 1.
Using the Viterbi algorithm (Viterbi, 1967), the
tagger finds the most likely POS sequence for a
given word string as shown in (2).
(2) argmaxP (t0, t1, ? ? ? , tn|w0, w1, ? ? ? , wn, ?).
This is known technology, see Manning and
Schu?tze (1999), but the particular use we make
of it is unusual. The tagger takes a word string
as an input, outputs the most likely POS sequence
and the final probability. Additionally, it presents
accumulated probability at each word break and
probability re-ranking, if any. Note that the run-
ning probability at the beginning of a sentence will
be 1, and will keep decreasing at each word break
since it is a product of conditional probabilities.
We tested the predictability of the model on em-
pirical reading data with the probability decrease
and the presence or absence of probability re-
ranking. Adopting the standard experimental de-
sign used in human sentence processing studies,
where word-by-word reading time or eye-fixation
time is compared between an experimental sen-
tence and its control sentence, this study compares
probability at each word break between a pair of
sentences. Comparatively faster or larger drop of
probability is expected to be a good indicator of
comparative processing difficulty. Probability re-
ranking, which is a simplified model of the reanal-
ysis process assumed in many human studies, is
also tested as another indicator of garden-path ef-
fect. Given a word string, all the possible POS
sequences compete with each other based on their
probability. Probability re-ranking occurs when an
initially dispreferred POS sub-sequence becomes
the preferred candidate later in the parse, because
it fits in better with later words.
The model parameters, P (wi|ti) and
P (ti|ti?1), are estimated from a small sec-
tion (970,995 tokens,47,831 distinct words) of
the British National Corpus (BNC), which is a
100 million-word collection of British English,
both written and spoken, developed by Oxford
University Press (Burnard, 1995). The BNC was
chosen for training the model because it is a
POS-annotated corpus, which allows supervised
training. In the implementation we use log
probabilities to avoid underflow, and we report
log probabilities in the sequel.
3.1 Hypotheses
If the HSPM is affected by frequency information,
we can assume that it will be easier to process
50
events with higher frequency or probability com-
pared to those with lower frequency or probability.
Under this general assumption, the overall diffi-
culty of a sentence is expected to be measured or
predicted by the mean size of probability decrease.
That is, probability will drop faster in garden-path
sentences than in control sentences (e.g. unam-
biguous sentences or ambiguous but non-garden-
path sentences).
More importantly, the probability decrease pat-
tern at disambiguating regions will predict the
trends in the reading time data. All other things be-
ing equal, we might expect a reading time penalty
when the size of the probability decrease at the
disambiguating region in garden-path sentences is
greater compared to the control sentences. This is
a simple and intuitive assumption that can be eas-
ily tested. We could have formed the sum over
all possible POS sequences in association with the
word strings, but for the present study we simply
used the Viterbi path: justifying this because this
is the best single-path approximation to the joint
probability.
Lastly, re-ranking of POS sequences is expected
to predict reanalysis of lexical categories. This is
because re-ranking in the tagger is parallel to re-
analysis in human subjects, which is known to be
cognitively costly.
3.2 Materials
In this study, five different types of ambiguity were
tested including Lexical Category ambiguity, Re-
duced Relative ambiguity (RR ambiguity), Prepo-
sitional Phrase Attachment ambiguity (PP ambi-
guity), Direct-Object/Sentential-Complement am-
biguity (DO/SC ambiguity), and Clausal Bound-
ary ambiguity. The following are example sen-
tences for each ambiguity type, shown with the
ambiguous region italicized and the disambiguat-
ing region bolded. All of the example sentences
are garden-path sentneces.
(3) Lexical Category ambiguity
The foreman knows that the warehouse
prices the beer very modestly.
(4) RR ambiguity
The horse raced past the barn fell.
(5) PP ambiguity
Katie laid the dress on the floor onto the bed.
(6) DO/SC ambiguity
He forgot Pam needed a ride with him.
(7) Clausal Boundary ambiguity
Though George kept on reading the story re-
ally bothered him.
There are two types of control sentences: unam-
biguous sentences and ambiguous but non-garden-
path sentences as shown in the examples below.
Again, the ambiguous region is italicized and the
disambiguating region is bolded.
(8) Garden-Path Sentence
The horse raced past the barn fell.
(9) Ambiguous but Non-Garden-Path Control
The horse raced past the barn and fell.
(10) Unambiguous Control
The horse that was raced past the barn fell.
Note that the garden-path sentence (8) and its
ambiguous control sentence (9) share exactly the
same word sequence except for the disambiguat-
ing region. This allows direct comparison of prob-
ability at the critical region (i.e. disambiguating
region) between the two sentences. Test materi-
als used in experimental studies are constructed in
this way in order to control extraneous variables
such as word frequency. We use these sentences
in the same form as the experimentalists so we in-
herit their careful design.
In this study, a total of 76 sentences were tested:
10 for lexical category ambiguity, 12 for RR am-
biguity, 20 for PP ambiguity, 16 for DO/SC am-
biguity, and 18 for clausal boundary ambiguity.
This set of materials is, to our knowledge, the
most comprehensive yet subjected to this type of
study. The sentences are directly adopted from
various psycholinguistic studies (Frazier, 1978;
Trueswell, 1996; Frazier and Clifton, 1996; Fer-
reira and Clifton, 1986; Ferreira and Henderson,
1986).
As a baseline test case of the tagger, the
well-established asymmetry between subject- and
object-relative clauses was tested as shown in (11).
(11) a. The editor who kicked the writer fired
the entire staff. (Subject-relative)
b. The editor who the writer kicked fired
the entire staff. (Object-relative)
The reading time advantage of subject-relative
clauses over object-relative clauses is robust in En-
glish (Traxler et al, 2002) as well as other lan-
guages (Mak et al, 2002; Homes et al, 1981). For
this test, materials from Traxler et al (2002) (96
sentences) are used.
51
4 Results
4.1 The Probability Decrease per Word
Unambiguous sentences are usually longer than
garden-path sentences. To compare sentences of
different lengths, the joint probability of the whole
sentence and tags was divided by the number of
words in the sentence. The result showed that
the average probability decrease was greater in
garden-path sentences compared to their unam-
biguous control sentences. This indicates that
garden-path sentences are more difficult than un-
ambiguous sentences, which is consistent with
empirical findings.
Probability decreased faster in object-relative
sentences than in subject relatives as predicted.
In the psycholinguistics literature, the comparative
difficulty of object-relative clauses has been ex-
plained in terms of verbal working memory (King
and Just, 1991), distance between the gap and the
filler (Bever and McElree, 1988), or perspective
shifting (MacWhinney, 1982). However, the test
results in this study provide a simpler account for
the effect. That is, the comparative difficulty of
an object-relative clause might be attributed to its
less frequent POS sequence. This account is par-
ticularly convincing since each pair of sentences in
the experiment share the exactly same set of words
except their order.
4.2 Probability Decrease at the
Disambiguating Region
A total of 30 pairs of a garden-path sentence
and its ambiguous, non-garden-path control were
tested for a comparison of the probability decrease
at the disambiguating region. In 80% of the cases,
the probability drops more sharply in garden-path
sentences than in control sentences at the critical
word. The test results are presented in (12) with
the number of test sets for each ambiguous type
and the number of cases where the model correctly
predicted reading-time penalty of garden-path sen-
tences.
(12) Ambiguity Type (Correct Predictions/Test
Sets)
a. Lexical Category Ambiguity (4/4)
b. PP Ambiguity (10/10)
c. RR Ambiguity (3/4)
d. DO/SC Ambiguity (4/6)
e. Clausal Boundary Ambiguity (3/6)
?60
?55
?50
?45
?40
?35
L
o
g
 P
ro
b
ab
il
it
y
(a)     PP  Attachment  Ambiguity                    
Katie put the dress on the floor  and / onto  the ...
?35
?30
?25
?20
?15
L
o
g
 P
ro
b
ab
il
it
y
(b)  DO / SC  Ambiguity  (DO Bias)   
He forgot Susan  but / remembered ...
the
and
the
floor
theonto
Susan
but
remembered
forgot
Figure 1: Probability Transition (Garden-Path vs.
Non Garden-Path)
(a) ? ? ? : Non-Garden-Path (Adjunct PP), ? ? ? : Garden
-Path (Complement PP)
(b) ? ? ? : Non-Garden-Path (DO-Biased, DO-Resolved),
? ? ? : Garden-Path (DO-Biased, SC-Resolved)
The two graphs in Figure 1 illustrate the com-
parison of probability decrease between a pair of
sentence. The y-axis of both graphs in Figure 1
is log probability. The first graph compares the
probability drop for the prepositional phrase (PP)
attachment ambiguity (Katie put the dress on the
floor and/onto the bed....) The empirical result
for this type of ambiguity shows that reading time
penalty is observed when the second PP, onto the
bed, is introduced, and there is no such effect for
the other sentence. Indeed, the sharper probability
drop indicates that the additional PP is less likely,
which makes a prediction of a comparative pro-
cessing difficulty. The second graph exhibits the
probability comparison for the DO/SC ambiguity.
The verb forget is a DO-biased verb and thus pro-
cessing difficulty is observed when it has a senten-
tial complement. Again, this effect was replicated
here.
The results showed that the disambiguating
word given the previous context is more difficult
in garden-path sentences compared to control sen-
tences. There are two possible explanations for
the processing difficulty. One is that the POS se-
quence of a garden-path sentence is less probable
than that of its control sentence. The other account
is that the disambiguating word in a garden-path
52
sentence is a lower frequency word compared to
that of its control sentence.
For example, slower reading time was observed
in (13a) and (14a) compared to (13b) and (14b) at
the disambiguating region that is bolded.
(13) Different POS at the Disambiguating Region
a. Katie laid the dress on the floor onto
(?57.80) the bed.
b. Katie laid the dress on the floor after
(?55.77) her mother yelled at her.
(14) Same POS at the Disambiguating Region
a. The umpire helped the child on (?42.77)
third base.
b. The umpire helped the child to (?42.23)
third base.
The log probability for each disambiguating word
is given at the end of each sentence. As ex-
pected, the probability at the disambiguating re-
gion in (13a) and (14a) is lower than in (13b) and
(14b) respectively. The disambiguating words in
(13) have different POS?s; Preposition in (13a) and
Conjunction (13b). This suggests that the prob-
abilities of different POS sequences can account
for different reading time at the region. In (14),
however, both disambiguating words are the same
POS (i.e. Preposition) and the POS sequences
for both sentences are identical. Instead, ?on?
and ?to?, have different frequencies and this in-
formation is reflected in the conditional probabil-
ity P (wordi|state). Therefore, the slower read-
ing time in (14b) might be attributable to the lower
frequency of the disambiguating word, ?to? com-
pared to ?on?.
4.3 Probability Re-ranking
The probability re-ranking reported in Corley and
Crocker (2000) was replicated. The tagger suc-
cessfully resolved the ambiguity by reanalysis
when the ambiguous word was immediately fol-
lowed by the disambiguating word (e.g. With-
out her he was lost.). If the disambiguating word
did not immediately follow the ambiguous region,
(e.g. Without her contributions would be very in-
adequate.) the ambiguity is sometimes incorrectly
resolved.
When revision occurred, probability dropped
more sharply at the revision point and at the dis-
ambiguation region compared to the control sen-
?41
?36
?31
?26
?21
(b)  " The woman told the joke did not ... "
?30
?25
?20
?15
?10
?5
(a)    " The woman chased by ... "
the 
woman 
chased (MV) 
chased (PP) 
by 
the 
told 
the 
joke
did 
but 
Figure 2: Probability Transition in the RR Ambi-
guity
(a) ? ? ? : Non-Garden-Path (Past Tense Verb), ? ? ? :
Garden-Path (Past Participle)
(b) ? ? ? : Non-Garden-Path (Past Tense Verb), ? ? ? :
Garden-Path, (Past Participle)
tences. When the ambiguity was not correctly re-
solved, the probability comparison correctly mod-
eled the comparative difficulty of the garden-path
sentences
Of particular interest in this study is RR ambi-
guity resolution. The tagger predicted the process-
ing difficulty of the RR ambiguity with probabil-
ity re-ranking. That is, the tagger initially favors
the main-verb interpretation for the ambiguous -ed
form, and later it makes a repair when the ambigu-
ity is resolved as a past-participle.
In the first graph of Figure 2, ?chased? is re-
solved as a past participle also with a revision
since the disambiguating word ?by? is immedi-
ately following. When revision occurred, proba-
bility dropped more sharply at the revision point
and at the disambiguation region compared to the
control sentences. When the disambiguating word
is not immediately followed by the ambiguous
word as in the second graph of Figure 2, the ambi-
guity was not resolved correctly, but the probaba-
biltiy decrease at the disambiguating regions cor-
rectly predict that the garden-path sentence would
be harder.
The RR ambiguity is often categorized as a syn-
tactic ambiguity, but the results suggest that the
ambiguity can be resolved locally and its pro-
cessing difficulty can be detected by a finite state
model. This suggests that we should be cautious
53
in assuming that a structural explanation is needed
for the RR ambiguity resolution, and it could be
that similar cautions are in order for other ambi-
guities usually seen as syntactic.
Although the probability re-ranking reported in
the previous studies (Corley and Crocker, 2000;
Frazier, 1978) is correctly replicated, the tagger
sometimes made undesired revisions. For exam-
ple, the tagger did not make a repair for the sen-
tence The friend accepted by the man was very im-
pressed (Trueswell, 1996) because accepted is bi-
ased as a past participle. This result is compatible
with the findings of Trueswell (1996). However,
the bias towards past-participle produces a repair
in the control sentence, which is unexpected. For
the sentence, The friend accepted the man who
was very impressed, the tagger showed a repair
since it initially preferred a past-participle analy-
sis for accepted and later it had to reanalyze. This
is a limitation of our model, and does not match
any previous empirical finding.
5 Discussion
The current study explores Corley and Crocker?s
model(2000) further on the model?s account of hu-
man sentence processing data seen in empirical
studies. Although there have been studies on a
POS tagger evaluating it as a potential cognitive
module of lexical category disambiguation, there
has been little work that tests it as a modeling tool
of syntactically ambiguous sentence processing.
The findings here suggest that a statistical POS
tagging system is more informative than Crocker
and Corley demonstrated. It has a predictive
power of processing delay not only for lexi-
cally ambiguous sentences but also for structurally
garden-pathed sentences. This model is attractive
since it is computationally simpler and requires
few statistical parameters. More importantly, it is
clearly defined what predictions can be and can-
not be made by this model. This allows system-
atic testability and refutability of the model un-
like some other probabilistic frameworks. Also,
the model training and testing is transparent and
observable, and true probability rather than trans-
formed weights are used, all of which makes it
easy to understand the mechanism of the proposed
model.
Although the model we used in the current
study is not a novelty, the current work largely dif-
fers from the previous study in its scope of data
used and the interpretation of the model for human
sentence processing. Corley and Crocker clearly
state that their model is strictly limited to lexical
ambiguity resolution, and their test of the model
was bounded to the noun-verb ambiguity. How-
ever, the findings in the current study play out dif-
ferently. The experiments conducted in this study
are parallel to empirical studies with regard to the
design of experimental method and the test mate-
rial. The garden-path sentences used in this study
are authentic, most of them are selected from the
cited literature, not conveniently coined by the
authors. The word-by-word probability compar-
ison between garden-path sentences and their con-
trols is parallel to the experimental design widely
adopted in empirical studies in the form of region-
by-region reading or eye-gaze time comparison.
In the word-by-word probability comparison, the
model is tested whether or not it correctly pre-
dicts the comparative processing difficulty at the
garden-path region. Contrary to the major claim
made in previous empirical studies, which is that
the garden-path phenomena are either modeled by
syntactic principles or by structural frequency, the
findings here show that the same phenomena can
be predicted without such structural information.
Therefore, the work is neither a mere extended
application of Corley and Crocker?s work to a
broader range of data, nor does it simply con-
firm earlier observations that finite state machines
might accurately account for psycholinguistic re-
sults to some degree. The current study provides
more concrete answers to what finite state machine
is relevant to what kinds of processing difficulty
and to what extent.
6 Future Work
Even though comparative analysis is a widely
adopted research design in experimental studies,
a sound scientific model should be independent
of this comparative nature and should be able to
make systematic predictions. Currently, proba-
bility re-ranking is one way to make systematic
module-internal predictions about the garden-path
effect. This brings up the issue of encoding more
information in lexical entries and increasing am-
biguity so that other ambiguity types also can be
disambiguated in a similar way via lexical cate-
gory disambiguation. This idea has been explored
as one of the lexicalist approaches to sentence pro-
cessing (Kim et al, 2002; Bangalore and Joshi,
54
1999).
Kim et al (2002) suggest the feasibility of mod-
eling structural analysis as lexical ambiguity res-
olution. They developed a connectionist neural
network model of word recognition, which takes
orthographic information, semantic information,
and the previous two words as its input and out-
puts a SuperTag for the current word. A Su-
perTag is an elementary syntactic tree, or sim-
ply a structural description composed of features
like POS, the number of complements, category
of each complement, and the position of comple-
ments. In their view, structural disambiguation
is simply another type of lexical category disam-
biguation, i.e. SuperTag disambiguation. When
applied to DO/SC ambiguous fragments, such as
?The economist decided ...?, their model showed
a general bias toward the NP-complement struc-
ture. This NP-complement bias was overcome by
lexical information from high-frequency S-biased
verbs, meaning that if the S-biased verb was a high
frequency word, it was correctly tagged, but if the
verb had low frequency, then it was more likely to
be tagged as NP-complement verb. This result is
also reported in other constraint-based model stud-
ies (e.g. Juliano and Tanenhaus (1994)), but the
difference between the previous constraint-based
studies and Kim et. al is that the result of the
latter is based on training of the model on nois-
ier data (sentences that were not tailored to the
specific research purpose). The implementation of
SuperTag advances the formal specification of the
constraint-based lexicalist theory. However, the
scope of their sentence processing model is lim-
ited to the DO/SC ambiguity, and the description
of their model is not clear. In addition, their model
is far beyond a simple statistical model: the in-
teraction of different sources of information is not
transparent. Nevertheless, Kim et al (2002) pro-
vides a future direction for the current study and
a starting point for considering what information
should be included in the lexicon.
The fundamental goal of the current research is
to explore a model that takes the most restrictive
position on the size of parameters until additional
parameters are demanded by data. Equally impor-
tant, the quality of architectural simplicity should
be maintained. Among the different sources of
information manipulated by Kim et. al., the so-
called elementary structural information is consid-
ered as a reasonable and ideal parameter for ad-
dition to the current model. The implementation
and the evaluation of the model will be exactly the
same as a statistical POS tagger provided with a
large parsed corpus from which elementary trees
can be extracted.
7 Conclusion
Our studies show that, at least for the sample of
test materials that we culled from the standard lit-
erature, a statistical POS tagging system can pre-
dict processing difficulty in structurally ambigu-
ous garden-path sentences. The statistical POS
tagger was surprisingly effective in modeling sen-
tence processing data, given the locality of the
probability distribution. The findings in this study
provide an alternative account for the garden-path
effect observed in empirical studies, specifically,
that the slower processing times associated with
garden-path sentences are due in part to their rela-
tively unlikely POS sequences in comparison with
those of non-garden-path sentences and in part to
differences in the emission probabilities that the
tagger learns. One attractive future direction is to
carry out simulations that compare the evolution
of probabilities in the tagger with that in a theo-
retically more powerful model trained on the same
data, such as an incremental statistical parser (Kim
et al, 2002; Roark, 2001). In so doing we can
find the places where the prediction problem faced
both by the HSPM and the machines that aspire
to emulate it actually warrants the greater power
of structurally sensitive models, using this knowl-
edge to mine large corpora for future experiments
with human subjects.
We have not necessarily cast doubt on the hy-
pothesis that the HSPM makes crucial use of struc-
tural information, but we have demonstrated that
much of the relevant behavior can be captured in
a simple model. The ?structural? regularities that
we observe are reasonably well encoded into this
model. For purposes of initial real-time process-
ing it could be that the HSPM is using a similar
encoding of structural regularities into convenient
probabilistic or neural form. It is as yet unclear
what the final form of a cognitively accurate model
along these lines would be, but it is clear from our
study that it is worthwhile, for the sake of clarity
and explicit testability, to consider models that are
simpler and more precisely specified than those
assumed by dominant theories of human sentence
processing.
55
Acknowledgments
This project was supported by the Cognitive Sci-
ence Summer 2004 Research Award at the Ohio
State University. We acknowledge support from
NSF grant IIS 0347799.
References
S. Bangalore and A. K. Joshi. Supertagging: an
approach to almost parsing. Computational Lin-
guistics, 25(2):237?266, 1999.
T. G. Bever and B. McElree. Empty categories
access their antecedents during comprehension.
Linguistic Inquiry, 19:35?43, 1988.
L Burnard. Users Guide for the British National
Corpus. British National Corpus Consortium,
Oxford University Computing Service, 1995.
S. Corley and M. W Crocker. The Modular Sta-
tistical Hypothesis: Exploring Lexical Category
Ambiguity. Architectures and Mechanisms for
Language Processing, M. Crocker, M. Picker-
ing. and C. Charles (Eds.) Cambridge Univer-
sity Press, 2000.
W. C. Crocker and T. Brants. Wide-coverage prob-
abilistic sentence processing, 2000.
F. Ferreira and C. Clifton. The independence of
syntactic processing. Journal of Memory and
Language, 25:348?368, 1986.
F. Ferreira and J. Henderson. Use of verb infor-
mation in syntactic parsing: Evidence from eye
movements and word-by-word self-paced read-
ing. Journal of Experimental Psychology, 16:
555?568, 1986.
L. Frazier. On comprehending sentences: Syntac-
tic parsing strategies. Ph.D. dissertation, Uni-
versity of Massachusetts, Amherst, MA, 1978.
L. Frazier and C. Clifton. Construal. Cambridge,
MA: MIT Press, 1996.
L. Frazier and K. Rayner. Making and correct-
ing errors during sentence comprehension: Eye
movements in the analysis of structurally am-
biguous sentences. Cognitive Psychology, 14:
178?210, 1982.
J. Hale. A probabilistic earley parser as a psy-
cholinguistic model. Proceedings of NAACL-
2001, 2001.
V. M. Homes, J. O?Regan, and K.G. Evensen. Eye
fixation patterns during the reading of relative
clause sentences. Journal of Verbal Learning
and Verbal Behavior, 20:417?430, 1981.
A. K. Joshi and B. Srinivas. Disambiguation of
super parts of speech (or supertags): almost
parsing. The Proceedings of the 15th Inter-
national Confer-ence on Computational Lin-
gusitics (COLING ?94), pages 154?160, 1994.
C. Juliano and M.K. Tanenhaus. A constraint-
based lexicalist account of the subject-object at-
tachment preference. Journal of Psycholinguis-
tic Research, 23:459?471, 1994.
D Jurafsky. A probabilistic model of lexical and
syntactic access and disambiguation. Cognitive
Science, 20:137?194, 1996.
A. E. Kim, Bangalore S., and J. Trueswell. A com-
putational model of the grammatical aspects of
word recognition as supertagging. paola merlo
and suzanne stevenson (eds.). The Lexical Basis
of Sentence Processing: Formal, computational
and experimental issues, University of Geneva
University of Toronto:109?135, 2002.
J. King and M. A. Just. Individual differences in
syntactic processing: The role of working mem-
ory. Journal of Memory and Language, 30:580?
602, 1991.
B. MacWhinney. Basic syntactic processes. Lan-
guage acquisition; Syntax and semantics, S.
Kuczaj (Ed.), 1:73?136, 1982.
W. M. Mak, Vonk W., and H. Schriefers. The influ-
ence of animacy on relative clause processing.
Journal of Memory and Language,, 47:50?68,
2002.
C.D. Manning and H. Schu?tze. Foundations of
Statistical Natural Language Processing. The
MIT Press, Cambridge, Massachusetts, 1999.
S. Narayanan and D Jurafsky. A bayesian model
predicts human parse preference and reading
times in sentence processing. Proceedings
of Advances in Neural Information Processing
Systems, 2002.
B. Roark. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27
(2):249?276, 2001.
M. J. Traxler, R. K. Morris, and R. E. Seely. Pro-
cessing subject and object relative clauses: evi-
dence from eye movements. Journal of Memory
and Language, 47:69?90, 2002.
J. C. Trueswell. The role of lexical frequency
in syntactic ambiguity resolution. Journal of
Memory and Language, 35:556?585, 1996.
A. Viterbi. Error bounds for convolution codes and
an asymptotically optimal decoding algorithm.
IEEE Transactions of Information Theory, 13:
260?269, 1967.
56
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 515?522,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Parsing and Subcategorization Data
Jianguo Li and Chris Brew
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{jianguo|cbrew}@ling.ohio-state.edu
Abstract
In this paper, we compare the per-
formance of a state-of-the-art statistical
parser (Bikel, 2004) in parsing written and
spoken language and in generating sub-
categorization cues from written and spo-
ken language. Although Bikel?s parser
achieves a higher accuracy for parsing
written language, it achieves a higher ac-
curacy when extracting subcategorization
cues from spoken language. Our exper-
iments also show that current technology
for extracting subcategorization frames
initially designed for written texts works
equally well for spoken language. Addi-
tionally, we explore the utility of punctu-
ation in helping parsing and extraction of
subcategorization cues. Our experiments
show that punctuation is of little help in
parsing spoken language and extracting
subcategorization cues from spoken lan-
guage. This indicates that there is no need
to add punctuation in transcribing spoken
corpora simply in order to help parsers.
1 Introduction
Robust statistical syntactic parsers, made possi-
ble by new statistical techniques (Collins, 1999;
Charniak, 2000; Bikel, 2004) and by the avail-
ability of large, hand-annotated training corpora
such as WSJ (Marcus et al, 1993) and Switch-
board (Godefrey et al, 1992), have had a major
impact on the field of natural language process-
ing. There are many ways to make use of parsers?
output. One particular form of data that can be ex-
tracted from parses is information about subcate-
gorization. Subcategorization data comes in two
forms: subcategorization frame (SCF) and sub-
categorization cue (SCC). SCFs differ from SCCs
in that SCFs contain only arguments while SCCs
contain both arguments and adjuncts. Both SCFs
and SCCs have been crucial to NLP tasks. For ex-
ample, SCFs have been used for verb disambigua-
tion and classification (Schulte im Walde, 2000;
Merlo and Stevenson, 2001; Lapata and Brew,
2004; Merlo et al, 2005) and SCCs for semantic
role labeling (Xue and Palmer, 2004; Punyakanok
et al, 2005).
Current technology for automatically acquiring
subcategorization data from corpora usually relies
on statistical parsers to generate SCCs. While
great efforts have been made in parsing written
texts and extracting subcategorization data from
written texts, spoken corpora have received little
attention. This is understandable given that spoken
language poses several challenges that are absent
in written texts, including disfluency, uncertainty
about utterance segmentation and lack of punctu-
ation. Roland and Jurafsky (1998) have suggested
that there are substantial subcategorization differ-
ences between written corpora and spoken cor-
pora. For example, while written corpora show a
much higher percentage of passive structures, spo-
ken corpora usually have a higher percentage of
zero-anaphora constructions. We believe that sub-
categorization data derived from spoken language,
if of acceptable quality, would be of more value to
NLP tasks involving a syntactic analysis of spoken
language. We do not show this here.
The goals of this study are as follows:
1. Test the performance of Bikel?s parser in
parsing written and spoken language.
2. Compare the accuracy level of SCCs gen-
erated from parsed written and spoken lan-
515
guage. We hope that such a comparison will
shed some light on the feasibility of acquiring
subcategorization data from spoken language
using the current SCF acquisition technology
initially designed for written language.
3. Apply our SCF extraction system (Li and
Brew, 2005) to spoken and written lan-
guage separately and compare the accuracy
achieved for the acquired SCFs from spoken
and written language.
4. Explore the utility of punctuation1 in pars-
ing and extraction of SCCs. It is gen-
erally recognized that punctuation helps in
parsing written texts. For example, Roark
(2001) finds that removing punctuation from
both training and test data (WSJ) decreases
his parser?s accuracy from 86.4%/86.8%
(LR/LP) to 83.4%/84.1%. However, spo-
ken language does not come with punctua-
tion. Even when punctuation is added in the
process of transcription, its utility in help-
ing parsing is slight. Both Roark (2001)
and Engel et al (2002) report that removing
punctuation from both training and test data
(Switchboard) results in only 1% decrease in
their parser?s accuracy.
2 Experiment Design
Three models will be investigated for parsing and
extracting SCCs from the parser?s output:
1. punc: leaving punctuation in both training
and test data.
2. no-punc: removing punctuation from both
training and test data.
3. punc-no-punc: removing punctuation from
only the test data.
Following the convention in the parsing com-
munity, for written language, we selected sections
02-21 of WSJ as training data and section 23 as
test data (Collins, 1999). For spoken language, we
designated section 2 and 3 of Switchboard as train-
ing data and files of sw4004 to sw4135 of section 4
as test data (Roark, 2001). Since we are also inter-
ested in extracting SCCs from the parser?s output,
1We use punctuation to refer to sentence-internal punctu-
ation unless otherwise specified.
label clause type desired SCCs
gerundive (NP)-GERUND
S small clause NP-NP, (NP)-ADJP
control (NP)-INF-to
control (NP)-INF-wh-to
SBAR with a complementizer (NP)-S-wh, (NP)-S-that
without a complementizer (NP)-S-that
Table 1: SCCs for different clauses
we eliminated from the two test corpora all sen-
tences that do not contain verbs. Our experiments
proceed in the following three steps:
1. Tag test data using the POS-tagger described
in Ratnaparkhi (1996).
2. Parse the POS-tagged data using Bikel?s
parser.
3. Extract SCCs from the parser?s output. The
extractor we built first locates each verb in the
parser?s output and then identifies the syntac-
tic categories of all its sisters and combines
them into an SCC. However, there are cases
where the extractor has more work to do.
? Finite and Infinite Clauses: In the Penn
Treebank, S and SBAR are used to label
different types of clauses, obscuring too
much detail about the internal structure
of each clause. Our extractor is designed
to identify the internal structure of dif-
ferent types of clause, as shown in Table
1.
? Passive Structures: As noted above,
Roland and Jurafsky (Roland and Juraf-
sky, 1998) have noticed that written lan-
guage tends to have a much higher per-
centage of passive structures than spo-
ken language. Our extractor is also
designed to identify passive structures
from the parser?s output.
3 Experiment Results
3.1 Parsing and SCCs
We used EVALB measures Labeled Recall (LR)
and Labeled Precision (LP) to compare the pars-
ing performance of different models. To compare
the accuracy of SCCs proposed from the parser?s
output, we calculated SCC Recall (SR) and SCC
Precision (SP). SR and SP are defined as follows:
SR = number of correct cues from the parser?s output
number of cues from treebank parse (1)
516
WSJ
model LR/LP SR/SP
punc 87.92%/88.29% 76.93%/77.70%
no-punc 86.25%/86.91% 76.96%/76.47%
punc-no-punc 82.31%/83.70% 74.62%/74.88%
Switchboard
model LR/LP SR/SP
punc 83.14%/83.80% 79.04%/78.62%
no-punc 82.42%/83.74% 78.81%/78.37%
punc-no-punc 78.62%/80.68% 75.51%/75.02%
Table 2: Results of parsing and extraction of SCCs
SP = number of correct cues from the parser?s output
number of cues from the parser?s output (2)
SCC Balanced F-measure = 2 ? SR ? SPSR + SP (3)
The results for parsing WSJ and Switchboard
and extracting SCCs are summarized in Table 2.
The LR/LP figures show the following trends:
1. Roark (2001) showed LR/LP of
86.4%/86.8% for punctuated written
language, 83.4%/84.1% for unpunctuated
written language. We achieve a higher
accuracy in both punctuated and unpunctu-
ated written language, and the decrease if
punctuation is removed is less
2. For spoken language, Roark (2001) showed
LR/LP of 85.2%/85.6% for punctuated spo-
ken language, 84.0%/84.6% for unpunctu-
ated spoken language. We achieve a lower
accuracy in both punctuated and unpunctu-
ated spoken language, and the decrease if
punctuation is removed is less. The trends in
(1) and (2) may be due to parser differences,
or to the removal of sentences lacking verbs.
3. Unsurprisingly, if the test data is unpunctu-
ated, but the models have been trained on
punctuated language, performance decreases
sharply.
In terms of the accuracy of extraction of SCCs,
the results follow a similar pattern. However, the
utility of punctuation turns out to be even smaller.
Removing punctuation from both the training and
test data results in a 0.8% drop in the accuracy of
SCC extraction for written language and a 0.3%
drop for spoken language.
Figure 1 exhibits the relation between the ac-
curacy of parsing and that of extracting SCCs.
If we consider WSJ and Switchboard individu-
ally, there seems to exist a positive correlation be-
tween the accuracy of parsing and that of extract-
ing SCCs. In other words, higher LR/LP indicates
punc no?punc punc?no?punc
74
76
78
80
82
84
86
88
90
Models
F?
me
as
ur
e(%
)
WSJ parsing
Switchboard parsing
WSJ SCC
Switchboard SCC
Figure 1: F-measure for parsing and extraction of
SCCs
higher SR/SP. However, Figure 1 also shows that
although the parser achieves a higher F-measure
value for paring WSJ, it achieves a higher F-
measure value for generating SCCs from Switch-
board.
The fact that the parser achieves a higher ac-
curacy of extracting SCCs from Switchboard than
WSJ merits further discussion. Intuitively, it
seems to be true that the shorter an SCC is, the
more likely that the parser is to get it right. This
intuition is confirmed by the data shown in Fig-
ure 2. Figure 2 plots the accuracy level of extract-
ing SCCs by SCC?s length. It is clear from Fig-
ure 2 that as SCCs get longer, the F-measure value
drops progressively for both WSJ and Switch-
board. Again, Roland and Jurafsky (1998) have
suggested that one major subcategorization differ-
ence between written and spoken corpora is that
spoken corpora have a much higher percentage of
the zero-anaphora construction. We then exam-
ined the distribution of SCCs of different length in
WSJ and Switchboard. Figure 3 shows that SCCs
of length 02 account for a much higher percentage
in Switchboard than WSJ, but it is always the other
way around for SCCs of non-zero length. This
observation led us to believe that the better per-
formance that Bikel?s parser achieves in extracting
SCCs from Switchboard may be attributed to the
following two factors:
1. Switchboard has a much higher percentage of
SCCs of length 0.
2. The parser is very accurate in extracting
shorter SCCs.
2Verbs have a length-0 SCC if they are intransitive and
have no modifiers.
517
0 1 2 3 4
10
20
30
40
50
60
70
80
90
Length of SCC
F?
me
as
ur
e(%
)
WSJ
Switchboard
Figure 2: F-measure for SCCs of different length
0 1 2 3 4
0
10
20
30
40
50
60
Length of SCCs
Pe
rc
en
ta
ge
(%
)
WSJ
Switchboard
Figure 3: Distribution of SCCs by length
3.2 Extraction of Dependents
In order to estimate the effects of SCCs of length
0, we examined the parser?s performance in re-
trieving dependents of verbs. Every constituent
(whether an argument or adjunct) in an SCC gen-
erated by the parser is considered a dependent of
that verb. SCCs of length 0 will be discounted be-
cause verbs that do not take any arguments or ad-
juncts have no dependents3 . In addition, this way
of evaluating the extraction of SCCs also matches
the practice in some NLP tasks such as semantic
role labeling (Xue and Palmer, 2004). For the task
of semantic role labeling, the total number of de-
pendents correctly retrieved from the parser?s out-
put affects the accuracy level of the task.
To do this, we calculated the number of depen-
dents shared by between each SCC proposed from
the parser?s output and its corresponding SCC pro-
3We are aware that subjects are typically also consid-
ered dependents, but we did not include subjects in our
experiments
shared-dependents[i.j] = MAX(
shared-dependents[i-1,j],
shared-dependents[i-1,j-1]+1 if target[i] = source[j],
shared-dependents[i-1,j-1] if target[i] != source[j],
shared-dependents[i,j-1])
Table 3: The algorithm for computing shared de-
pendents
INF #5 1 1 2 3
ADVP #4 1 1 2 2
PP-in #3 1 1 2 2
NP #2 1 1 1 1
NP #1 1 1 1 1
#0 #1 #2 #3 #4
NP S-that PP-in INF
Table 4: An example of computing the number of
shared dependents
posed from Penn Treebank. We based our cal-
culation on a modified version of Minimum Edit
Distance Algorithm. Our algorithm works by cre-
ating a shared-dependents matrix with one col-
umn for each constituent in the target sequence
(SCCs proposed from Penn Treebank) and one
row for each constituent in the source sequence
(SCCs proposed from the parser?s output). Each
cell shared-dependent[i,j] contains the number of
constituents shared between the first i constituents
of the target sequence and the first j constituents of
the source sequence. Each cell can then be com-
puted as a simple function of the three possible
paths through the matrix that arrive there. The al-
gorithm is illustrated in Table 3.
Table 4 shows an example of how the algo-
rithm works with NP-S-that-PP-in-INF as the tar-
get sequence and NP-NP-PP-in-ADVP-INF as the
source sequence. The algorithm returns 3 as the
number of dependents shared by two SCCs.
We compared the performance of Bikel?s parser
in retrieving dependents from written and spo-
ken language over all three models using De-
pendency Recall (DR) and Dependency Precision
(DP). These metrics are defined as follows:
DR = number of correct dependents from parser?s output
number of dependents from treebank parse
(4)
DP = number of correct dependents from parser?s output
number of dependents from parser?s output
(5)
Dependency F-measure = 2 ?DR ?DPDR +DP (6)
518
punc no?punc punc?no?punc
78
80
82
84
86
Models
F?
me
as
ur
e(%
)
WSJ
Switchboard
Figure 4: F-measure for extracting dependents
The results of Bikel?s parser in retrieving depen-
dents are summarized in Figure 4. Overall, the
parser achieves a better performance for WSJ over
all three models, just the opposite of what have
been observed for SCC extraction. Interestingly,
removing punctuation from both the training and
test data actually slightly improves the F-measure.
This holds true for both WSJ and Switchboard.
This Dependency F-measure differs in detail from
similar measures in Xue and Palmer (2004). For
present purposes all that matters is the relative
value for WSJ and Switchboard.
4 Extraction of SCFs from Spoken
Language
Our experiments indicate that the SCCs generated
by the parser from spoken language are as accurate
as those generated from written texts. Hence, we
would expect that the current technology for ex-
tracting SCFs, initially designed for written texts,
should work equally well for spoken language.
We previously built a system for automatically ex-
tracting SCFs from spoken BNC, and reported ac-
curacy comparable to previous systems that work
with only written texts (Li and Brew, 2005). How-
ever, Korhonen (2002) has shown that a direct
comparison of different systems is very difficult to
interpret because of the variations in the number
of targeted SCFs, test verbs, gold standards and in
the size of the test data. For this reason, we apply
our SCF acquisition system separately to a written
and spoken corpus of similar size from BNC and
compare the accuracy of acquired SCF sets.
4.1 Overview
As noted above, previous studies on automatic ex-
traction of SCFs from corpora usually proceed in
two steps and we adopt this approach.
1. Hypothesis Generation: Identify all SCCs
from the corpus data.
2. Hypothesis Selection: Determine which SCC
is a valid SCF for a particular verb.
4.2 SCF Extraction System
We briefly outline our SCF extraction system
for automatically extracting SCFs from corpora,
which was based on the design proposed in
Briscoe and Carroll (1997).
1. A Statistical Parser: Bikel?s parser is used
to parse input sentences.
2. An SCF Extractor: An extractor is use to
extract SCCs from the parser?s output.
3. An English Lemmatizer: MORPHA (Min-
nen et al, 2000) is used to lemmatize each
verb.
4. An SCF Evaluator: An evaluator is used
to filter out false SCCs based on their like-
lihood.
An SCC generated by the parser and extractor
may be a correct SCC, or it may contain an ad-
junct, or it may simply be wrong due to tagging or
parsing errors. We therefore need an SCF evalua-
tor capable of filtering out false cues. Our evalu-
ator has two parts: the Binomial Hypothesis Test
(Brent, 1993) and a back-off algorithm (Sarkar and
Zeman, 2000).
1. The Binomial Hypothesis Test (BHT): Let
p be the probability that an scfi occurs with
verbj that is not supposed to take scfi. If a
verb occurs n times and m of those times it
co-occurs with scfi, then the scfi cues are
false cues is estimated by the summation of
the binomial distribution for m ? k ? n:
P (m+, n, p) =
n
X
k=m
n!
k!(n? k)!p
k(1? p)(n?k) (7)
If the value of P (m+, n, p) is less than or
equal to a small threshold value, then the null
hypothesis that verbj does not take scfi is ex-
tremely unlikely to be true. Hence, scfi is
very likely to be a valid SCF for verbj . The
519
SCCs SCFs
NP-PP-before
NP-S-when NP
NP-PP-at-S-before
NP-PP-to-S-when
NP-PP-to-PP-at NP-PP-to
NP-PP-to-S-because-ADVP
Table 5: SCCs and correct SCFs for introduce
corpus WC SC
number of verb tokens 115,524 109,678
number of verb types 5,234 4,789
verb types seen more than 10 times 1,102 998
number of acquired SCFs 2,688 1,984
average number of SCFs per verb 2.43 1.99
Table 6: Training data for WC and SC
value of m and n can be directly computed
from the extractor?s output, but the value of
p is not easy to obtain. Following Manning
(1993), we empirically determined the value
of p. It was between 0.005 to 0.4 depend-
ing on the likelihood of an SCC being a valid
SCF.
2. Back-off Algorithm: Many SCCs generated
by the parser and extractor tend to contain
some adjuncts. However, for many SCCs,
one of its subsets is likely to be the correct
SCF. Table 5 shows some SCCs generated by
the extractor and the corresponding SCFs.
The Back-off Algorithm always starts with
the longest SCC for each verb. Assume that
this SCC fails the BHT. The evaluator then
eliminates the last constituent from the re-
jected cue, transfers its frequency to its suc-
cessor and submits the successor to the BHT
again. In this way, frequency can accumulate
and more valid frames survive the BHT.
4.3 Results and Discussion
We evaluated our SCF extraction system on writ-
ten and spoken BNC. We chose one million word
written corpus (WC) and a comparable spoken
corpus (SC) from BNC. Table 6 provides relevant
information on the two corpora. We only keep the
verbs that occur at least 10 times in our training
data.
To compare the performance of our system on
WC and SC, we calculated the type precision, type
gold standard COMLEX Manually Constructed
corpus WC SC WC SC
type precision 93.1% 92.9% 93.1% 92.9%
type recall 49.2% 47.7% 56.5% 57.6%
F-measure 64.4% 63.1% 70.3% 71.1%
Table 7: Type precision and recall and F-measure
recall and F-measure. Type precision is the per-
centage of SCF types that our system proposes
which are correct according some gold standard
and type recall is the percentage of correct SCF
types proposed by our system that are listed in the
gold standard. We used the 14 verbs 4 selected
by Briscoe and Carroll (1997) and evaluated our
results of these verbs against the SCF entries in
two gold standards: COMLEX (Grishman et al,
1994) and a manually constructed SCF set from
the training data. It makes sense to use a manually
constructed SCF set while calculating type preci-
sion and recall because some of the SCFs in a syn-
tax dictionary such as COMLEX might not occur
in the training data at all. We constructed separate
SCF sets for the written and spoken BNC.
The results are summarized in Table 7. As
shown in Table 7, the accuracy achieved for WC
and SC are very comparable: Our system achieves
a slightly better result for WC when using COM-
LEX as the gold standard and for SC when using
manually constructed SCF set as gold standard,
suggesting that it is feasible to apply the current
technology for automatically extracting SCFs to
spoken language.
5 Conclusions and Future Work
5.1 Use of Parser?s Output
In this paper, we have shown that it is not nec-
essarily true that statistical parsers always per-
form worse when dealing with spoken language.
The conventional accuracy metrics for parsing
(LR/LP) should not be taken as the only metrics
in determining the feasibility of applying statisti-
cal parsers to spoken language. It is necessary to
consider what information we want to extract out
of parsers? output and make use of.
1. Extraction of SCFs from Corpora: This task
takes SCCs generated by the parser and ex-
tractor as input. Our experiments show that
4The 14 verbs used in Briscoe and Carroll (1997) are ask,
begin, believe, cause, expect, find, give, help, like, move, pro-
duce, provide, seem and sway. We replaced sway with show
because sway occurs less than 10 times in our training data.
520
the SCCs generated for spoken language are
as accurate as those generated for written lan-
guage. We have also shown that it is feasible
to apply the current SCF extraction technol-
ogy to spoken language.
2. Semantic Role Labeling: This task usually
operates on parsers? output and the number
of dependents of each verb that are correctly
retrieved by the parser clearly affects the ac-
curacy of the task. Our experiments show
that the parser achieves a much lower accu-
racy in retrieving dependents from the spoken
language than written language. This seems
to suggest that a lower accuracy is likely to
be achieved for a semantic role labeling task
performed on spoken language. We are not
aware that this has yet been tried.
5.2 Punctuation and Speech Transcription
Practice
Both our experiments and Roark?s experiments
show that parsing accuracy measured by LR/LP
experiences a sharper decrease for WSJ than
Switchboard after we removed punctuation from
training and test data. In spoken language, com-
mas are largely used to delimit disfluency ele-
ments. As noted in Engel et al (2002), statis-
tical parsers usually condition the probability of
a constituent on the types of its neighboring con-
stituents. The way that commas are used in speech
transcription seems to have the effect of increasing
the range of neighboring constituents, thus frag-
menting the data and making it less reliable. On
the other hand, in written texts, commas serve as
more reliable cues for parsers to identify phrasal
and clausal boundaries.
In addition, our experiment demonstrates that
punctuation does not help much with extraction of
SCCs from spoken language. Removing punctu-
ation from both the training and test data results
in rougly a 0.3% decrease in SR/SP. Furthermore,
removing punctuation from both training and test
data actually slightly improves the performance
of Bikel?s parser in retrieving dependents from
spoken language. All these results seem to sug-
gest that adding punctuation in speech transcrip-
tion is of little help to statistical parsers includ-
ing at least three state-of-the-art statistical parsers
(Collins, 1999; Charniak, 2000; Bikel, 2004). As a
result, there may be other good reasons why some-
one who wants to build a Switchboard-like corpus
should choose to provide punctuation, but there is
no need to do so simply in order to help parsers.
However, segmenting utterances into individual
units is necessary because statistical parsers re-
quire sentence boundaries to be clearly delimited.
Current statistical parsers are unable to handle an
input string consisting of two sentences. For ex-
ample, when presented with an input string as in
(1) and (2), if the two sentences are separated by a
period (1), Bikel?s parser wrongly treats the sec-
ond sentence as a sentential complement of the
main verb like in the first sentence. As a result, the
extractor generates an SCC NP-S for like, which is
incorrect. The parser returns the same parse after
we removed the period (2) and let the parser parse
it again.
(1) I like the long hair. It was back in high
school.
(2) I like the long hair It was back in high school.
Hence, while adding punctuation in transcribing
a Switchboard-like corpus is not of much help to
statistical parsers, segmenting utterances into in-
dividual units is crucial for statistical parsers. In
future work, we plan to develop a system capa-
ble of automatically segmenting speech utterances
into individual units.
6 Acknowledgments
This study was supported by NSF grant 0347799.
Our thanks go to Eric Fosler-Lussier, Mike White
and three anonymous reviewers for their valuable
comments.
References
D. Bikel. 2004. Intricacies of Collin?s parsing models.
Computational Linguistics, 30(2):479?511.
M. Brent. 1993. From grammar to lexicon: Unsu-
pervised learning of lexical syntax. Computational
Linguistics, 19(3):243?262.
T. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In Proceedings
of the 5th ACL Conference on Applied Natural Lan-
guage Processing, pages 356?363.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 2000 Conference of
the North American Chapter of the Association for
Computation Linguistics, pages 132?139.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University
of Pennsylvania.
521
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. In Proceedings of 2002
Conference on Empirical Methods of Natural Lan-
guage Processing, pages 49?54.
J. Godefrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICASSP-92, pages 517?520.
R. Grishman, C. Macleod, and A. Meryers. 1994.
Comlex syntax: Building a computational lexicon.
In Proceedings of the 1994 International Conference
of Computational Linguistics, pages 268?272.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, Cambridge University.
M. Lapata and C. Brew. 2004. Verb class disambigua-
tion using informative priors. Computational Lin-
guistics, 30(1):45?73.
J. Li and C. Brew. 2005. Automatic extraction of sub-
categorization frames from spoken corpora. In Pro-
ceedings of the Interdisciplinary Workshop on the
Identification and Representation of Verb Features
and Verb Classes, Saarbracken, Germany.
C. Manning. 1993. Automatic extraction of a large
subcategorization dictionary from corpora. In Pro-
ceedings of 31st Annual Meeting of the Association
for Computational Linguistics, pages 235?242.
M. Marcus, G. Kim, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics,
19(2):313?330.
P. Merlo and S. Stevenson. 2001. Automatic
verb classification based on statistical distribution
of argument structure. Computational Linguistics,
27(3):373?408.
P. Merlo, E. Joanis, and J. Henderson. 2005. Unsuper-
vised verb class disambiguation based on diathesis
alternations. manuscripts.
G. Minnen, J. Carroll, and D. Pearce. 2000. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling.
In Proceedings of the 2nd Midwest Computational
Linguistics Colloquium, pages 15?22.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods of Natural Language
Processing, pages 133?142.
B. Roark. 2001. Robust Probabilistic Predictive
Processing: Motivation, Models, and Applications.
Ph.D. thesis, Brown University.
D. Roland and D. Jurafsky. 1998. How verb sub-
categorization frequency is affected by the corpus
choice. In Proceedings of the 17th International
Conference on Computational Linguistics, pages
1122?1128.
A. Sarkar and D. Zeman. 2000. Automatic extraction
of subcategorization frames for Czech. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, pages 691?697.
S. Schulte im Walde. 2000. Clustering verbs semanti-
cally according to alternation behavior. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 747?753.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 88?94.
522
Proceedings of ACL-08: HLT, pages 434?442,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Which Are the Best Features for Automatic Verb Classification
Jianguo Li
Department of Linguistics
The Ohio State University
Columbus Ohio, USA
jianguo@ling.ohio-state.edu
Chris Brew
Department of Linguistics
The Ohio State University
Columbus Ohio, USA
cbrew@ling.ohio-state.edu
Abstract
In this work, we develop and evaluate a wide
range of feature spaces for deriving Levin-
style verb classifications (Levin, 1993). We
perform the classification experiments using
Bayesian Multinomial Regression (an effi-
cient log-linear modeling framework which
we found to outperform SVMs for this task)
with the proposed feature spaces. Our exper-
iments suggest that subcategorization frames
are not the most effective features for auto-
matic verb classification. A mixture of syntac-
tic information and lexical information works
best for this task.
1 Introduction
Much research in lexical acquisition of verbs has
concentrated on the relation between verbs and their
argument frames. Many scholars hypothesize that
the behavior of a verb, particularly with respect to
the expression of arguments and the assignment of
semantic roles is to a large extent driven by deep
semantic regularities (Dowty, 1991; Green, 1974;
Goldberg, 1995; Levin, 1993). Thus measurements
of verb frame patterns can perhaps be used to probe
for linguistically relevant aspects of verb meanings.
The correspondence between meaning regularities
and syntax has been extensively studied in Levin
(1993) (hereafter Levin). Levin?s verb classes are
based on the ability of a verb to occur or not occur
in pairs of syntactic frames that are in some sense
meaning preserving (diathesis alternation). The fo-
cus is on verbs for which distribution of syntactic
frames is a useful indicator of class membership,
and, correspondingly, on classes which are relevant
for such verbs. By using Levin?s classification, we
obtain a window on some (but not all) of the poten-
tially useful semantic properties of verbs.
Levin?s verb classification, like others, helps re-
duce redundancy in verb descriptions and enables
generalizations across semantically similar verbs
with respect to their usage. When the information
about a verb type is not available or sufficient for us
to draw firm conclusions about its usage, the infor-
mation about the class to which the verb type be-
longs can compensate for it, addressing the perva-
sive problem of data sparsity in a wide range of NLP
tasks, such as automatic extraction of subcategoriza-
tion frames (Korhonen, 2002), semantic role label-
ing (Swier and Stevenson, 2004; Gildea and Juraf-
sky, 2002), natural language generation for machine
translation (Habash et al, 2003), and deriving pre-
dominant verb senses from unlabeled data (Lapata
and Brew, 2004).
Although there exist several manually-created
verb lexicons or ontologies, including Levin?s verb
taxonomy, VerbNet, and FrameNet, automatic verb
classification (AVC) is still necessary for extend-
ing existing lexicons (Korhonen and Briscoe, 2004),
building and tuning lexical information specific to
different domains (Korhonen et al, 2006), and boot-
strapping verb lexicons for new languages (Tsang
et al, 2002).
AVC helps avoid the expensive hand-coding of
such information, but appropriate features must be
identified and demonstrated to be effective. In this
work, our primary goal is not necessarily to obtain
the optimal classification, but rather to investigate
434
the linguistic conditions which are crucial for lex-
ical semantic classification of verbs. We develop
feature sets that combine syntactic and lexical infor-
mation, which are in principle useful for any Levin-
style verb classification. We test the general ap-
plicability and scalability of each feature set to the
distinctions among 48 verb classes involving 1,300
verbs, which is, to our knowledge, the largest in-
vestigation on English verb classification by far. To
preview our results, a feature set that combines both
syntactic information and lexical information works
much better than either of them used alone. In ad-
dition, mixed feature sets also show potential for
scaling well when dealing with larger number of
verbs and verb classes. In contrast, subcategoriza-
tion frames, at least on their own, are largely inef-
fective for AVC, despite their evident effectiveness
in supporting Levin?s initial intuitions.
2 Related Work
Earlier work on verb classification has generally
adopted one of the two approaches for devising sta-
tistical, corpus-based features.
Subcategorization frame (SCF): Subcategoriza-
tion frames are obviously relevant to alternation
behaviors. It is therefore unsurprising that much
work on verb classification has adopted them as fea-
tures (Schulte im Walde, 2000; Brew and Schulte im
Walde, 2002; Korhonen et al, 2003). However, rely-
ing solely on subcategorization frames also leads to
the loss of semantic distinctions. Consider the frame
NP-V-PPwith. The semantic interpretation of this
frame depends to a large extent on the NP argument
selected by the preposition with. In (1), the same
surface form NP-V-PPwith corresponds to three dif-
ferent underlying meanings. However, such seman-
tic distinctions are totally lost if lexical information
is disregarded.
(1) a. I ate with a fork. [INSTRUMENT]
b. I left with a friend. [ACCOMPANIMENT]
c. I sang with confidence. [MANNER]
This deficiency of unlexicalized subcategoriza-
tion frames leads researchers to make attempts to
incorporate lexical information into the feature rep-
resentation. One possible improvement over subcat-
egorization frames is to enrich them with lexical in-
formation. Lexicalized frames are usually obtained
by augmenting each syntactic slot with its head noun
(2).
(2) a. NP(I)-V-PP(with:fork)
b. NP(I)-V-PP(with:friend)
c. NP(I)-V-PP(with:confidence)
With the potentially improved discriminatory
power also comes increased exposure to sparse data
problems. Trying to overcome the problem of data
sparsity, Schulte im Walde (2000) explores the ad-
ditional use of selectional preference features by
augmenting each syntactic slot with the concept to
which its head noun belongs in an ontology (e.g.
WordNet). Although the problem of data sparsity
is alleviated to certain extent (3), these features
do not generally improve classification performance
(Schulte im Walde, 2000; Joanis, 2002).
(3) a. NP(PERSON)-V-PP(with:ARTIFACT)
b. NP(PERSON)-V-PP(with:PERSON)
c. NP(PERSON)-V-PP(with:FEELING)
JOANIS07: Incorporating lexical information di-
rectly into subcategorization frames has proved in-
adequate for AVC. Other methods for combining
syntactic information with lexical information have
also been attempted (Merlo and Stevenson, 2001;
Joanis et al, 2007). These studies use a small col-
lection of features that require some degree of expert
linguistic analysis to devise. The deeper linguistic
analysis allows their feature set to cover a variety of
indicators of verb semantics, beyond that of frame
information. Joanis et al (2007) reports an experi-
ment that involves 15 Levin verb classes. They de-
fine a general feature space that is supposed to be
applicable to all Levin classes. The features they
use fall into four different groups: syntactic slots,
slot overlaps, tense, voice and aspect, and animacy
of NPs.
? Syntactic slots: They encode the frequency of
the syntactic positions (e.g. SUBJECT, OB-
JECT, PPat). They are considered approxima-
tion to subcategorization frames.
? Slot overlaps: They are supposed to capture
the properties of alternation by identifying if
a given noun can occur in different syntactic
positions relative to a particular verb. For in-
stance, in the alternation The ice melted and
435
The sun melted the ice, ice occurs in the sub-
ject position in the first sentence but in the ob-
ject position in the second sentence. An over-
lap feature records that there is a subject-object
alternation for melt.
? Tense, voice and aspect: Verb meaning and al-
ternations also interact in interesting ways with
tense, voice, and aspect. For example, mid-
dle construction is usually used in present tense
(e.g. The bread cuts easily).
? Animacy of NPs: The animacy of the seman-
tic role corresponding to the head noun in each
syntactic slot can also distinguish classes of
verbs.
Joanis et al (2007) demonstrates that the gen-
eral feature space they devise achieves a rate of
error reduction ranging from 48% to 88% over a
chance baseline accuracy, across classification tasks
of varying difficulty. However, they also show that
their general feature space does not generally im-
prove the classification accuracy over subcategoriza-
tion frames (see table 1).
Experimental Task All Features SCF
Average 2-way 83.2 80.4
Average 3-way 69.6 69.4
Average (? 6)-way 61.1 62.8
Table 1: Results from Joanis et al (2007) (%)
3 Integration of Syntactic and Lexical
Information
In this study, we explore a wider range of features
for AVC, focusing particularly on various ways to
mix syntactic with lexical information.
Dependency relation (DR): Our way to over-
come data sparsity is to break lexicalized frames into
lexicalized slots (a.k.a. dependency relations). De-
pendency relations contain both syntactic and lexical
information (4).
(4) a. SUBJ(I), PP(with:fork)
b. SUBJ(I), PP(with:friend)
c. SUBJ(I), PP(with:confidence)
However, augmenting PP with nouns selected by
the preposition (e.g. PP(with:fork)) still gives rise
to data sparsity. We therefore decide to break it
into two individual dependency relations: PP(with),
PP-fork. Although dependency relations have been
widely used in automatic acquisition of lexical infor-
mation, such as detection of polysemy (Lin, 1998)
and WSD (McCarthy et al, 2004), their utility in
AVC still remains untested.
Co-occurrence (CO): CO features mostly convey
lexical information only and are generally consid-
ered not particularly sensitive to argument structures
(Rohde et al, 2004). Nevertheless, it is worthwhile
testing whether the meaning components that are
brought out by syntactic alternations are also cor-
related to the neighboring words. In other words,
Levin verbs may be distinguished on the dimension
of neighboring words, in addition to argument struc-
tures. A test on this claim can help answer the ques-
tion of whether verbs in the same Levin class also
tend to share their neighboring words.
Adapted co-occurrence (ACO): Conventional
CO features generally adopt a stop list to filter out
function words. However, some of the functions
words, prepositions in particular, are known to carry
great amount of syntactic information that is related
to lexical meanings of verbs (Schulte im Walde,
2003; Brew and Schulte im Walde, 2002; Joanis
et al, 2007). In addition, whereas most verbs tend to
put a strong selectional preference on their nominal
arguments, they do not care much about the iden-
tity of the verbs in their verbal arguments. Based on
these observations, we propose to adapt the conven-
tional CO features by (1) keeping all prepositions
(2) replacing all verbs in the neighboring contexts of
each target verb with their part-of-speech tags. ACO
features integrate at least some degree of syntactic
information into the feature space.
SCF+CO: Another way to mix syntactic informa-
tion with lexical information is to use subcategoriza-
tion frames and co-occurrences together in hope that
they are complementary to each other, and therefore
yield better results for AVC.
4 Experiment Setup
4.1 Corpus
To collect each type of features, we use the Giga-
word Corpus, which consists of samples of recent
newswire text data collected from four distinct in-
436
ternational sources of English newswire.
4.2 Feature Extraction
We evaluate six different feature sets for their effec-
tiveness in AVC: SCF, DR, CO, ACO, SCF+CO,
and JOANIS07. SCF contains mainly syntactic in-
formation, whereas CO lexical information. The
other four feature sets include both syntactic and lex-
ical information.
SCF and DR: These more linguistically informed
features are constructed based on the grammatical
relations generated by the C&C CCG parser (Clark
and Curran, 2007). Take He broke the door with a
hammer as an example. The grammatical relations
generated are given in table 2.
he broke the door with a hammer.
(det door 3 the 2)
(dobj broke 1 door 3)
(det hammer 6 a 5)
(dobj with 4 hammer 6)
(iobj broke 1 with 4)
(ncsubj broke 1 He 0 )
Table 2: grammatical relations generated by the parser
We first build a lexicalized frame for the verb
break: NP1(he)-V-NP2(door)-PP(with:hammer).
This is done by matching each grammatical label
onto one of the traditional syntactic constituents.
The set of syntactic constituents we use is summa-
rized in table 3.
constituent remark
NP1 subject of the verb
NP2 object of the verb
NP3 indirect object of the verb
PPp prepositional phrase
TO infinitival clause
GER gerund
THAT sentential complement headed by that
WH sentential complement headed by a wh-word
ADJP adjective phrase
ADVP adverb phrase
Table 3: Syntactic constituents used for building SCFs
Based on the lexicalized frame, we construct
an SCF NP1-NP2-PPwith for break. The set of
DRs generated for break is [SUBJ(he), OBJ(door),
PP(with), PP-hammer].
CO: These features are collected using a flat 4-
word window, meaning that the 4 words to the
left/right of each target verb are considered poten-
tial CO features. However, we eliminate any CO
features that are in a stopword list, which con-
sists of about 200 closed class words including
mainly prepositions, determiners, complementizers
and punctuation. We also lemmatize each word us-
ing the English lemmatizer as described in Minnen
et al (2000), and use lemmas as features instead of
words.
ACO: As mentioned before, we adapt the conven-
tional CO features by (1) keeping all prepositions
(2) replacing all verbs in the neighboring contexts of
each target verb with their part-of-speech tags. (3)
keeping words in the left window only if they are
tagged as a nominal.
SCF+CO: We combine the SCF and CO features.
JOANIS07: We use the feature set proposed in
Joanis et al (2007), which consists of 224 features.
We extract features on the basis of the output gener-
ated by the C&C CCG parser.
4.3 Verb Classes
Our experiments involve two separate sets of verb
classes:
Joanis15: Joanis et al (2007) manually selects
pairs, or triples of classes to represent a range of
distinctions that exist among the 15 classes they in-
vestigate. For example, some of the pairs/triples are
syntactically dissimilar, while others show little syn-
tactic distinction across the classes.
Levin48: Earlier work has focused only on a
small set of verbs or a small number of verb classes.
For example, Schulte im Walde (2000) uses 153
verbs in 30 classes, and Joanis et al (2007) takes
on 835 verbs and 15 verb classes. Since one of our
primary goals is to identify a general feature space
that is not specific to any class distinctions, it is of
great importance to understand how the classifica-
tion accuracy is affected when attempting to classify
more verbs into a larger number of classes. In our
automatic verb classification, we aim for a larger
scale experiment. We select our experimental verb
classes and verbs as follows: We start with all Levin
197 verb classes. We first remove all verbs that be-
long to at least two Levin classes. Next, we remove
any verb that does not occur at least 100 times in
the English Gigaword Corpus. All classes that are
left with at least 10 verbs are chosen for our experi-
437
ment. This process yields 48 classes involving about
1,300 verbs. In our automatic verb classification ex-
periment, we test the applicability of each feature
set to distinctions among up to 48 classes 1. To our
knowledge, this is, by far, the largest investigation
on English verb classification.
5 Machine Learning Method
5.1 Preprocessing Data
We represent the semantic space for verbs as a ma-
trix of frequencies, where each row corresponds to
a Levin verb and each column represents a given
feature. We construct a semantic space with each
feature set. Except for JONAIS07 which only con-
tains 224 features, all the other feature sets lead to a
very high-dimensional space. For instance, the se-
mantic space with CO features contains over one
million columns, which is too huge and cumber-
some. One way to avoid these high-dimensional
spaces is to assume that most of the features are irrel-
evant, an assumption adopted by many of the previ-
ous studies working with high-dimensional seman-
tic spaces (Burgess and Lund, 1997; Pado and La-
pata, 2007; Rohde et al, 2004). Burgess and Lund
(1997) suggests that the semantic space can be re-
duced by keeping only the k columns (features) with
the highest variance. However, Rohde et al (2004)
have found it is simpler and more effective to dis-
card columns on the basis of feature frequency, with
little degradation in performance, and often some
improvement. Columns representing low-frequency
features tend to be noisier because they only involve
few examples. We therefore apply a simple fre-
quency cutoff for feature selection. We only use fea-
tures that occur with a frequency over some thresh-
old in our data.
In order to reduce undue influence of outlier fea-
tures, we employ the four normalization strategies in
table 4, which help reduce the range of extreme val-
ues while having little effect on others (Rohde et al,
2004). The raw frequency (wv,f ) of a verb v oc-
curring with a feature f is replaced with the normal-
1In our experiment, we only use monosemous verbs from
these 48 verb classes. Due to the space limit, we do not list the
48 verb classes. The size of the most classes falls in the range
between 10 to 30, with a couple of classes having a size over
100.
ized value (w?v,f ), according to each normalization
method. Our experiments show that using correla-
tion for normalization generally renders the best re-
sults. The results reported below are obtained from
using correlation for normalization.
w?v,f =
row
wv,fP
j wv,j
column
wv,fP
i wi,f
length
wv,f
P
j w
2
v,j
1/2
correlation
Twv,f?
P
j wv,j
P
i wi,f
(
P
j wv,j(T?
P
j wv,j)
P
i wi,f (T?
P
i wi,f ))
1/2
T =
P
i
P
j wi,j
Table 4: Normalization techniques
To preprocess data, we first apply a frequency cut-
off to our data set, and then normalize it using the
correlation method. To find the optimal threshold
for frequency cut, we consider each value between 0
and 10,000 at an interval of 500. In our experiments,
results on training data show that performance de-
clines more noticeably when the threshold is lower
than 500 or higher than 10,000. For each task and
feature set, we select the frequency cut that offers
the best accuracy on the preprocessed training set
according to k-fold stratified cross validation 2.
5.2 Classifier
For all of our experiments, we use the software that
implements the Bayesian multinomial logistic re-
gression (a.k.a BMR). The software performs the so-
called 1-of-k classification (Madigan et al, 2005).
BMR is similar to Maximum Entropy. It has been
shown to be very efficient with handling large num-
bers of features and extremely sparsely populated
matrices, which characterize the data we have for
AVC 3. To begin, let x = [x1, ..., xj , ..., xd]T be a
vector of feature values characterizing a verb to be
classified. We encode the fact that a verb belongs
to a class k ? 1, ...,K by a K-dimensional 0/1 val-
ued vector y = (y1, ..., yK)T , where yk = 1 and all
other coordinates are 0. Multinomial logistic regres-
210-fold for Joanis15 and 9-fold for Levin48. We use a bal-
anced training set, which contains 20 verbs from each class in
Joanis15, but only 9 verbs from each class in Levin48.
3We also tried Chang and Lin (2001)?s LIBSVM library for
Support Vector Machines (SVMs), however, BMR generally
outperforms SVMs.
438
sion is a conditional probability model of the form,
parameterized by the matrix ? = [?1, ..., ?K ]. Each
column of ? is a parameter vector corresponding to
one of the classes: ?k = [?k1, ..., ?kd]T .
P (yk = 1|?k, x) = exp(?
T
k x)/
X
ki
exp(?Tkix)
6 Results and Discussion
6.1 Evaluation Metrics
Following Joanis et al (2007), we adopt a single
evaluation measure - macro-averaged recall - for all
of our classification tasks. As discussed below, since
we always use balanced training sets for each indi-
vidual task, it makes sense for our accuracy metric to
give equal weight to each class. Macro-averaged re-
call treats each verb class equally, so that the size of
a class does not affect macro-averaged recall. It usu-
ally gives a better sense of the quality of classifica-
tion across all classes. To calculate macro-averaged
recall, the recall value for each individual verb class
has to be computed first.
recall =
no. of test verbs in class c correctly labeled
no. of test verbs in class c
With a recall value computed for each verb class,
the macro-averaged recall can be defined by:
macro-averaged recall =
1
|C|
X
c?C
recall for class c
C : a set of verb classes
c : an individual verb class
|C| : the number of verb classes
6.2 Joanis15
With those manually-selected 15 classes, Joanis
et al (2007) conducts 11 classification tasks includ-
ing six 2-way classifications, two 3-way classifica-
tions, one 6-way classification, one 8-way classifi-
cation, and one 14-way classification. In our exper-
iments, we replicate these 11 classification tasks us-
ing the proposed six different feature sets. For each
classification task in this task set, we randomly se-
lect 20 verbs from each class as the training set. We
repeat this process 10 times for each task. The re-
sults reported for each task is obtained by averaging
the results of the 10 trials. Note that for each trial,
each feature set is trained and tested on the same
training/test split.
The results for the 11 classification tasks are sum-
marized in table 5. We provide a chance baseline
and the accuracy reported in Joanis et al (2007) 4 for
comparison of our results. A few points are worth
noting:
? Although widely used for AVC, SCF, at least
when used alone, is not the most effective fea-
ture set. Our experiments show that the per-
formance achieved by using SCF is generally
worse than using the feature sets that mix syn-
tactic and lexical information. As a matter of
fact, it even loses to the simplest feature setCO
on 4 tasks, including the 14-way task.
? The two feature sets (DR, SCF+CO) we pro-
pose that combine syntactic and lexical infor-
mation generally perform better than those fea-
ture sets (SCF, CO) that only include syntactic
or lexical information. Although there is not a
clear winner, DR and SCF+CO generally out-
perform other feature sets, indicating that they
are effective ways for combining syntactic and
lexical information. In particular, these two
feature sets perform comparatively well on the
tasks that involve more classes (e.g. 14-way),
exhibiting the tendency to scale well with larger
number of verb classes and verbs. Another fea-
ture set that combines syntactic and lexical in-
formation, ACO, which keeps function words
in the feature space to preserve syntactic infor-
mation, outperforms the conventional CO on
the majority of tasks. All these observations
suggest that how to mix syntactic and lexical
information is one of keys to an improved verb
classification.
? Although JOANIS07 also combines syntactic
and lexical information, its performance is not
comparable to that of other feature sets that mix
syntactic and lexical information. In fact, SCF
4Joanis et al (2007) is different from our experiments in that
they use a chunker for feature extraction and the Support Vector
Machine for classification.
439
Experimental Task
Random As Reported in Feature Set
Baseline Joanis et al (2007) SCF DR CO ACO SCF+CO JOANIS07
1) Benefactive/Recipient 50 86.4 88.6 88.4 88.2 89.1 90.7 88.9
2) Admire/Amuse 50 93.9 96.7 97.5 92.1 90.5 96.4 96.6
3) Run/Sound 50 86.8 85.4 89.6 91.8 90.2 90.5 87.1
4) Light/Sound 50 75.0 74.8 90.8 86.9 89.7 88.8 82.1
5) Cheat/Steal 50 76.5 77.6 80.6 72.1 75.5 77.8 76.4
6) Wipe/Steal 50 80.4 84.8 80.6 79.0 79.4 84.4 83.9
7) Spray/Fill/Putting 33.3 65.6 73.0 72.8 59.6 66.6 73.8 69.6
8) Run/State Change/Object drop 33.3 74.2 74.8 77.2 76.9 77.6 80.5 75.5
9) Cheat/Steal/Wipe/Spray/Fill/Putting 16.7 64.3 64.9 65.1 54.8 59.1 65.0 64.3
10) 9)/Run/Sound 12.5 61.7 62.3 65.8 55.7 60.8 66.9 63.1
11) 14-way (all except Benefactive) 7.1 58.4 56.4 65.7 57.5 59.6 66.3 57.2
Table 5: Experimental results for Joanis15 (%)
and JOANIS07 yield similar accuracy in our
experiments, which agrees with the findings in
Joanis et al (2007) (compare table 1 and 5).
6.3 Levin48
Recall that one of our primary goals is to identify
the feature set that is generally applicable and scales
well while we attempt to classify more verbs into a
larger number of classes. If we could exhaust all the
possible n-way (2 ? n ? 48) classification tasks
with the 48 Levin classes we will investigate, it will
allow us to draw a firmer conclusion about the gen-
eral applicability and scalability of a particular fea-
ture set. However, the number of classification tasks
grows really huge when n takes on certain value (e.g.
n = 20). For our experiments, we set n to be 2, 5,
10, 20, 30, 40, or 48. For the 2-way classification,
we perform all the possible 1,028 tasks. For the 48-
way classification, there is only one possible task.
We randomly select 100 n-way tasks each for n =
5, 10, 20, 30, 40. We believe that this series of tasks
will give us a reasonably good idea of whether a par-
ticular feature set is generally applicable and scales
well.
The smallest classes in Levin48 have only 10
verbs. We therefore reduce the number of training
verbs to 9 for each class. For each n = 2, 5, 10, 20,
30, 40, 48, we will perform certain number of n-way
classification tasks. For each n-way task, we ran-
domly select 9 verbs from each class as training data,
and repeat this process 10 times. The accuracy for
each n-way task is then computed by averaging the
results from these 10 trials. The accuracy reported
for the overall n-way classification for each selected
n, is obtained by averaging the results from each in-
dividual n-way task for that particular n. Again, for
each trial, each feature set is trained and tested on
the same training/test split.
The results for Levin48 are presented in table 6,
which clearly reveals the general applicability and
scalability of each feature set.
? Results from Levin48 reconfirm our finding
that SCF is not the most effective feature set for
AVC. Although it achieves the highest accuracy
on the 2-way classification, its accuracy drops
drastically as n gets bigger, indicating that SCF
does not scale as well as other feature sets when
dealing with larger number of verb classes. On
the other hand, the co-occurrence feature (CO),
which is believed to convey only lexical infor-
mation, outperforms SCF on every n-way clas-
sification when n ? 10, suggesting that verbs
in the same Levin classes tend to share their
neighboring words.
? The three feature sets we propose that com-
bine syntactic and lexical information generally
scale well. Again, DR and SCF+CO gener-
ally outperform all other feature sets on all n-
way classifications, except the 2-way classifica-
tion. In addition, ACO achieves a better perfor-
mance on every n-way classification than CO.
Although SCF and CO are not very effective
when used individually, they tend to yield the
best performance when combined together.
? Again, JOANIS07 does not match the perfor-
mance of other feature sets that combine both
syntactic and lexical information, but yields
similar accuracy as SCF.
440
Experimental Task No of Tasks Random Baseline
Feature Set
SCF DR CO ACO SCF+CO JOANIS07
2-way 1,028 50 84.0 83.4 77.8 80.9 82.9 82.4
5-way 100 20 71.9 76.4 70.4 73.0 77.3 72.2
10-way 100 10 65.8 73.7 68.8 71.2 72.8 65.9
20-way 100 5 51.4 65.1 58.8 60.1 65.8 50.7
30-way 100 3.3 46.7 56.9 48.6 51.8 57.8 47.1
40-way 100 2.5 43.6 54.8 47.3 49.9 55.1 44.2
48-way 1 2.2 39.1 51.6 42.4 46.8 52.8 38.9
Table 6: Experimental results for Levin48 (%)
6.4 Further Discussion
Previous studies on AVC have focused on using
SCFs. Our experiments reveal that SCFs, at least
when used alone, compare poorly to the feature sets
that mix syntactic and lexical information. One ex-
planation for the poor performance could be that we
use all the frames generated by the CCG parser in
our experiment. A better way of doing this would
be to use some expert-selected SCF set. Levin clas-
sifies English verbs on the basis of 78 SCFs, which
should, at least in principle, be good at separating
verb classes. To see if Levin-selected SCFs are
more effective for AVC, we match each SCF gen-
erated by the C&C CCG parser (CCG-SCF) to one
of 78 Levin-defined SCFs, and refer to the resulting
SCF set as unfiltered-Levin-SCF. Following stud-
ies on automatic SCF extraction (Brent, 1993), we
apply a statistical test (Binomial Hypothesis Test) to
the unfiltered-Levin-SCF to filter out noisy SCFs,
and denote the resulting SCF set as filtered-Levin-
SCF. We then perform the 48-way task (one of
Levin48) with these two different SCF sets. Recall
that using CCG-SCF gives us a macro-averaged re-
call of 39.1% on the 48-way task. Our experiments
show that using unfiltered-Levin-SCF and filtered-
Levin-SCF raises the accuracy to 39.7% and 40.3%
respectively. Although a little performance gain has
been obtained by using expert-defined SCFs, the ac-
curacy level is still far below that achieved by using
a feature set that combines syntactic and semantic
information. In fact, even the simple co-occurrence
feature (CO) yields a better performance (42.4%)
than these Levin-selected SCF sets.
7 Conclusion and Future Work
We have performed a wide range of experiments
to identify which features are most informative in
AVC. Our conclusion is that both syntactic and lex-
ical information are useful for verb classification.
Although neither SCF nor CO performs well on its
own, a combination of them proves to be the most in-
formative feature for this task. Other ways of mixing
syntactic and lexical information, such as DR, and
ACO, work relatively well too. What makes these
mixed feature sets even more appealing is that they
tend to scale well in comparison to SCF and CO. In
addition, these feature sets are devised on a general
level without relying on any knowledge about spe-
cific classes, thus potentially applicable to a wider
range of class distinctions. Assuming that Levin?s
analysis is generally applicable across languages in
terms of the linking of semantic arguments to their
syntactic expressions, these mixed feature sets are
potentially useful for building verb classifications
for other languages.
For our future work, we aim to test whether an
automatically created verb classification can be ben-
eficial to other NLP tasks. One potential applica-
tion of our verb classification is parsing. Lexicalized
PCFGs (where head words annotate phrasal nodes)
have proved a key tool for high performance PCFG
parsing, however its performance is hampered by
the sparse lexical dependency exhibited in the Penn
Treebank. Our experiments on verb classification
have offered a class-based approach to alleviate data
sparsity problem in parsing. It is our goal to test
whether this class-based approach will lead to an im-
proved parsing performance.
8 Acknowledgments
This study was supported by NSF grant 0347799.
We are grateful to Eric Fosler-Lussier, Detmar
Meurers, Mike White and Kirk Baker for their valu-
able comments.
441
References
Brent, M. (1993). From grammar to lexicon: Unsuper-
vised learning of lexical syntax. Computational Lin-
guistics, 19(3):243?262.
Brew, C. and Schulte im Walde, S. (2002). Spectral clus-
tering for German verbs. In Proccedings of the 2002
Conference on EMNLP, pages 117?124.
Burgess, C. and Lund, K. (1997). Modelling parsing
constraints with high-dimentional context space. Lan-
guage and Cognitive Processes, 12(3):177?210.
Chang, C. and Lin, C. (2001). LIBSVM:
A library for support vector machines.
http://www.csie.ntu.edu.tw. cjlin/libsvm.
Clark, S. and Curran, J. (2007). Formalism-independent
parser evaluation with CCG and Depbank. In Proceed-
ings of the 45th Annual Meeting of ACL, pages 248?
255.
Dowty, D. (1991). Thematic proto-roles and argument
selection. Language, 67:547?619.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of
semantic role. Computational Linguistics, 28(3):245?
288.
Goldberg, A. (1995). Constructions. University of
Chicago Press, Chicago, 1st edition.
Green, G. (1974). Semantics and Syntactic Regularity.
Indiana University Press, Bloomington.
Habash, N., Dorr, B., and Traum, D. (2003). Hybrid natu-
ral language generation from lexical conceptual struc-
tures. Machine Translation, 18(2):81?128.
Joanis, E. (2002). Automatic verb classification using a
general feature space. Master?s thesis, University of
Toronto.
Joanis, E., Stevenson, S., and James, D. (2007). A general
feature space for automatic verb classification. Natural
Language Engineering, 1:1?31.
Korhonen, A. (2002). Subcategorization Acquisition.
PhD thesis, Cambridge University.
Korhonen, A. and Briscoe, T. (2004). Extended lexical-
semantic classification of english verbs. In Proceed-
ings of the 2004 HLT/NAACL Workshop on Computa-
tional Lexical Semantics, pages 38?45, Boston, MA.
Korhonen, A., Krymolowski, Y., and Collier, N. (2006).
Automatic classification of verbs in biomedical texts.
In Proceedings of the 21st International Conference
on COLING and 44th Annual Meeting of ACL, pages
345?352, Sydney, Australia.
Korhonen, A., Krymolowski, Y., and Marx, Z. (2003).
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting of ACL, pages 48?55, Sapparo, Japan.
Lapata, M. and Brew, C. (2004). Verb class disambigua-
tion using informative priors. Computational Linguis-
tics, 30(1):45?73.
Levin, B. (1993). English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago, 1st edition.
Lin, D. (1998). Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th Internation Con-
ference on COLING and 36th Annual Meeting of ACL.
Madigan, D., Genkin, A., Lewis, D., and Fradkin, D.
(2005). Bayesian Multinomial Logistic Regression for
Author Identification. DIMACS Technical Report.
McCarthy, D., Koeling, R., Weeds, J., and Carroll, J.
(2004). Finding predominant senses in untagged text.
In Proceedings of the 42nd Annual Meeting of ACL,
pages 280?287.
Merlo, P. and Stevenson, S. (2001). Automatic verb clas-
sification based on statistical distribution of argument
structure. Computational Linguistics, 27(3):373?408.
Minnen, G., Carroll, J., and Pearce, D. (2000). Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
Pado, S. and Lapata, M. (2007). Dependency-based con-
struction of semantic space models. Computional Lin-
guistics, 33(2):161?199.
Rohde, D., Gonnerman, L., and Plaut, D. (2004). An im-
proved method for deriving word meaning from lexical
co-occurrence. http://dlt4.mit.edu/ dr/COALS.
Schulte im Walde, S. (2000). Clustering verbs seman-
tically according to alternation behavior. In Proceed-
ings of the 18th International Conference on COLING,
pages 747?753.
Schulte im Walde, S. (2003). Experiments on the choice
of features for learning verb classes. In Proceedings of
the 10th Conference of EACL, pages 315?322.
Swier, R. and Stevenson, S. (2004). Unsupervised se-
mantic role labelling. In Proceedings of the 2004 Con-
ference on EMNLP, pages 95?102.
Tsang, V., Stevenson, S., and Merlo, P. (2002). Crosslin-
guistic transfer in automatic verb classification. In
Proceedings of the 19th International Conference on
COLING, pages 1023?1029, Taiwan, China.
442
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 37?45,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Brutus: A Semantic Role Labeling System Incorporating CCG, CFG, and
Dependency Features
Stephen A. Boxwell, Dennis Mehay, and Chris Brew
Department of Linguistics
The Ohio State University
{boxwe11,mehay,cbrew}@1ing.ohio-state.edu
Abstract
We describe a semantic role labeling system
that makes primary use of CCG-based fea-
tures. Most previously developed systems
are CFG-based and make extensive use of a
treepath feature, which suffers from data spar-
sity due to its use of explicit tree configura-
tions. CCG affords ways to augment treepath-
based features to overcome these data sparsity
issues. By adding features over CCG word-
word dependencies and lexicalized verbal sub-
categorization frames (?supertags?), we can
obtain an F-score that is substantially better
than a previous CCG-based SRL system and
competitive with the current state of the art. A
manual error analysis reveals that parser errors
account for many of the errors of our system.
This analysis also suggests that simultaneous
incremental parsing and semantic role labeling
may lead to performance gains in both tasks.
1 Introduction
Semantic Role Labeling (SRL) is the process of assign-
ing semantic roles to strings of words in a sentence ac-
cording to their relationship to the semantic predicates
expressed in the sentence. The task is difficult because
the relationship between syntactic relations like ?sub-
ject? and ?object? do not always correspond to seman-
tic relations like ?agent? and ?patient?. An effective
semantic role labeling system must recognize the dif-
ferences between different configurations:
(a) [The man]Arg0 opened [the door]Arg1 [for
him]Arg3 [today]ArgM?TMP .
(b) [The door]Arg1 opened.
(c) [The door]Arg1 was opened by [a man]Arg0.
We use Propbank (Palmer et al, 2005), a corpus of
newswire text annotated with verb predicate semantic
role information that is widely used in the SRL litera-
ture (Ma`rquez et al, 2008). Rather than describe se-
mantic roles in terms of ?agent? or ?patient?, Propbank
defines semantic roles on a verb-by-verb basis. For ex-
ample, the verb open encodes the OPENER as Arg0, the
OPENEE as Arg1, and the beneficiary of the OPENING
action as Arg3. Propbank also defines a set of adjunct
roles, denoted by the letter M instead of a number. For
example, ArgM-TMP denotes a temporal role, like ?to-
day?. By using verb-specific roles, Propbank avoids
specific claims about parallels between the roles of dif-
ferent verbs.
We follow the approach in (Punyakanok et al, 2008)
in framing the SRL problem as a two-stage pipeline:
identification followed by labeling. During identifica-
tion, every word in the sentence is labeled either as
bearing some (as yet undetermined) semantic role or
not . This is done for each verb. Next, during label-
ing, the precise verb-specific roles for each word are
determined. In contrast to the approach in (Punyakanok
et al, 2008), which tags constituents directly, we tag
headwords and then associate them with a constituent,
as in a previous CCG-based approach (Gildea and
Hockenmaier, 2003). Another difference is our choice
of parsers. Brutus uses the CCG parser of (Clark and
Curran, 2007, henceforth the C&C parser), Charniak?s
parser (Charniak, 2001) for additional CFG-based fea-
tures, and MALT parser (Nivre et al, 2007) for de-
pendency features, while (Punyakanok et al, 2008)
use results from an ensemble of parses from Char-
niak?s Parser and a Collins parser (Collins, 2003; Bikel,
2004). Finally, the system described in (Punyakanok et
al., 2008) uses a joint inference model to resolve dis-
crepancies between multiple automatic parses. We do
not employ a similar strategy due to the differing no-
tions of constituency represented in our parsers (CCG
having a much more fluid notion of constituency and
the MALT parser using a different approach entirely).
For the identification and labeling steps, we train
a maximum entropy classifier (Berger et al, 1996)
over sections 02-21 of a version of the CCGbank cor-
pus (Hockenmaier and Steedman, 2007) that has been
augmented by projecting the Propbank semantic anno-
tations (Boxwell and White, 2008). We evaluate our
SRL system?s argument predictions at the word string
level, making our results directly comparable for each
argument labeling.1
In the following, we briefly introduce the CCG
grammatical formalism and motivate its use in SRL
(Sections 2?3). Our main contribution is to demon-
strate that CCG ? arguably a more expressive and lin-
1This is guaranteed by our string-to-string mapping from
the original Propbank to the CCGbank.
37
guistically appealing syntactic framework than vanilla
CFGs ? is a viable basis for the SRL task. This is sup-
ported by our experimental results, the setup and details
of which we give in Sections 4?10. In particular, using
CCG enables us to map semantic roles directly onto
verbal categories, an innovation of our approach that
leads to performance gains (Section 7). We conclude
with an error analysis (Section 11), which motivates
our discussion of future research for computational se-
mantics with CCG (Section 12).
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (Steedman, 2000)
is a grammatical framework that describes syntactic
structure in terms of the combinatory potential of the
lexical (word-level) items. Rather than using standard
part-of-speech tags and grammatical rules, CCG en-
codes much of the combinatory potential of each word
by assigning a syntactically informative category. For
example, the verb loves has the category (s\np)/np,
which could be read ?the kind of word that would be
a sentence if it could combine with a noun phrase on
the right and a noun phrase on the left?. Further, CCG
has the advantage of a transparent interface between the
way the words combine and their dependencies with
other words. Word-word dependencies in the CCG-
bank are encoded using predicate-argument (PARG)
relations. PARG relations are defined by the functor
word, the argument word, the category of the functor
word and which argument slot of the functor category
is being filled. For example, in the sentence John loves
Mary (figure 1), there are two slots on the verbal cat-
egory to be filled by NP arguments. The first argu-
ment (the subject) fills slot 1. This can be encoded
as <loves,john,(s\np)/np,1>, indicating the head of
the functor, the head of the argument, the functor cat-
egory and the argument slot. The second argument
(the direct object) fills slot 2. This can be encoded as
<loves,mary,(s\np)/np,2>. One of the potential ad-
vantages to using CCGbank-style PARG relations is
that they uniformly encode both local and long-range
dependencies ? e.g., the noun phrase the Mary that
John loves expresses the same set of two dependencies.
We will show this to be a valuable tool for semantic
role prediction.
3 Potential Advantages to using CCG
There are many potential advantages to using the CCG
formalism in SRL. One is the uniformity with which
CCG can express equivalence classes of local and long-
range (including unbounded) dependencies. CFG-
based approaches often rely on examining potentially
long sequences of categories (or treepaths) between the
verb and the target word. Because there are a number of
different treepaths that correspond to a single relation
(figure 2), this approach can suffer from data sparsity.
CCG, however, can encode all treepath-distinct expres-
sions of a single grammatical relation into a single
predicate-argument relationship (figure 3). This fea-
ture has been shown (Gildea and Hockenmaier, 2003)
to be an effective substitute for treepath-based features.
But while predicate-argument-based features are very
effective, they are still vulnerable both to parser er-
rors and to cases where the semantics of a sentence
do not correspond directly to syntactic dependencies.
To counteract this, we use both kinds of features with
the expectation that the treepath feature will provide
low-level detail to compensate for missed, incorrect or
syntactically impossible dependencies.
Another advantage of a CCG-based approach (and
lexicalist approaches in general) is the ability to en-
code verb-specific argument mappings. An argument
mapping is a link between the CCG category and the
semantic roles that are likely to go with each of its ar-
guments. The projection of argument mappings onto
CCG verbal categories is explored in (Boxwell and
White, 2008). We describe this feature in more detail
in section 7.
4 Identification and Labeling Models
As in previous approaches to SRL, Brutus uses a two-
stage pipeline of maximum entropy classifiers. In ad-
dition, we train an argument mapping classifier (de-
scribed in more detail below) whose predictions are
used as features for the labeling model. The same
features are extracted for both treebank and automatic
parses. Automatic parses were generated using the
C&C CCG parser (Clark and Curran, 2007) with its
derivation output format converted to resemble that of
the CCGbank. This involved following the derivational
bracketings of the C&C parser?s output and recon-
structing the backpointers to the lexical heads using an
in-house implementation of the basic CCG combina-
tory operations. All classifiers were trained to 500 iter-
ations of L-BFGS training ? a quasi-Newton method
from the numerical optimization literature (Liu and No-
cedal, 1989) ? using Zhang Le?s maxent toolkit.2 To
prevent overfitting we used Gaussian priors with global
variances of 1 and 5 for the identifier and labeler, re-
spectively.3 The Gaussian priors were determined em-
pirically by testing on the development set.
Both the identifier and the labeler use the following
features:
(1) Words. Words drawn from a 3 word window
around the target word,4 with each word asso-
ciated with a binary indicator feature.
(2) Part of Speech. Part of Speech tags drawn
from a 3 word window around the target word,
2Available for download at http://homepages.
inf.ed.ac.uk/s0450736/maxent_toolkit.
html.
3Gaussian priors achieve a smoothing effect (to prevent
overfitting) by penalizing very large feature weights.
4The size of the window was determined experimentally
on the development set ? we use the same window sizes
throughout.
38
John loves Mary
np (s[dcl]\np)/np np
>
s[dcl]\np
<
s[dcl]
Figure 1: This sentence has two depen-
dencies: <loves,mary,(s\np)/np,2> and
<loves,john,(s\np)/np,1>
Saaa!!!
NP
Robin
VPbb""
V
fixed
NP
@ 
Det
the
N
car
NPHHH
Det
the
NHHH
N
car
RCHHH
Rel
that
S
ZZSpectral Clustering for German Verbs
Chris Brew
Department of Linguistics
The Ohio State University
Columbus, Ohio, USA
cbrew@ling.osu.edu
Sabine Schulte im Walde
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
Stuttgart, Germany
schulte@ims.uni-stuttgart.de
Abstract
We describe and evaluate the application of a
spectral clustering technique (Ng et al, 2002)
to the unsupervised clustering of German verbs.
Our previous work has shown that standard
clustering techniques succeed in inducing Levin-
style semantic classes from verb subcategorisa-
tion information. But clustering in the very
high dimensional spaces that we use is fraught
with technical and conceptual difficulties. Spec-
tral clustering performs a dimensionality reduc-
tion on the verb frame patterns, and provides a
robustness and efficiency that standard cluster-
ing methods do not display in direct use. The
clustering results are evaluated according to the
alignment (Christianini et al, 2002) between
the Gram matrix defined by the cluster output
and the corresponding matrix defined by a gold
standard.
1 Introduction
Standard multivariate clustering technology
(such as k-Means) can be applied to the problem
of inferring verb classes from information about
the estimated prevalence of verb frame patterns
(Schulte im Walde and Brew, 2002). But one
of the problems with multivariate clustering is
that it is something of a black art when applied
to high-dimensional natural language data. The
search space is very large, and the available
techniques for searching this large space do not
offer guarantees of global optimality.
In response to this insight, the present work
applies a spectral clustering technique (Ng et
al., 2002) to the verb frame patterns. At the
heart of this approach is a transformation of
the original input into a set of orthogonal eigen-
vectors. We work in the space defined by the
first few eigenvectors, using standard clustering
techniques in the reduced space. The spectral
clustering technique has been shown to han-
dle difficult clustering problems in image pro-
cessing, offers principled methods for initializ-
ing cluster centers, and (in the version that we
use) has no random component.
The clustering results are evaluated accord-
ing to their alignment with a gold standard.
Alignment is Pearson correlation between corre-
sponding elements of the Gram matrices, which
has been suggested as a measure of agreement
between a clustering and a distance measure
(Christianini et al, 2002). We are also able to
use this measure to quantify the fit between a
clustering result and the distance matrix that
serves as input to clustering. The evidence is
that the spectral technique is more effective
than the methods that have previously been
tried.
2 Verb valency description
The data in question come from a subcate-
gorization lexicon induced from a large Ger-
man newspaper corpus (Schulte im Walde,
2002). The verb valency information is pro-
vided in form of probability distributions over
verb frames for each verb. There are two condi-
tions: the first with 38 relatively coarse syntac-
tic verb subcategorisation frames, the second a
more delicate classification subdividing the verb
frames of the first condition using prepositional
phrase information (case plus preposition), re-
sulting in 171 possible frame types.
The verb frame types contain at most three
arguments. Possible arguments in the frames
are nominative (n), dative (d) and accusative
(a) noun phrases, reflexive pronouns (r), prepo-
sitional phrases (p), expletive es (x), non-finite
clauses (i), finite clauses (s-2 for verb second
clauses, s-dass for dass-clauses, s-ob for ob-
clauses, s-w for indirect wh-questions), and cop-
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 117-124.
                         Proceedings of the Conference on Empirical Methods in Natural
ula constructions (k). For example, subcate-
gorising a direct (accusative case) object and
a non-finite clause would be represented by
nai. Table 1 shows an example distribution
for the verb glauben ?to think/believe?. The
more delicate version of subcategorisation frame
was done by distributing the frequency mass of
prepositional phrase frame types (np, nap, ndp,
npr, xp) over the prepositional phrases, accord-
ing to their frequencies in the corpus. Preposi-
tional phrases are referred to by case and prepo-
sition, such as ?Dat.mit?, ?Akk.fu?r?. The present
work uses the latter, more delicate, verb valency
descriptions.
Frame Prob Frame Prob
ns-dass 0.27945 npr 0.00261
ns-2 0.27358 nds-dass 0.00253
np 0.09951 ndi 0.00161
n 0.08811 nrs-dass 0.00029
na 0.08046 ndr 0.00029
ni 0.05015 nrs-w 0.00029
nd 0.03392 nir 0.00027
nad 0.02325 nds-w 0.00024
nds-2 0.01011 xd 0.00017
nai 0.00894 ns-ob 0.00014
ns-w 0.00859 nas-ob 0.00014
nas-w 0.00681 nds-ob 0.00000
nap 0.00594 nrs-ob 0.00000
nr 0.00455 x 0.00000
nar 0.00436 xa 0.00000
nrs-2 0.00391 xp 0.00000
ndp 0.00356 xr 0.00000
nas-dass 0.00342 xs-dass 0.00000
nas-2 0.00281 k 0.00000
Table 1: Probability distribution for glauben
3 Problems with standard clustering
Our previous work on the valency data ap-
plied k-Means (a standard technique) to the
task of inducing semantic classes for German
verbs (Schulte im Walde and Brew, 2002). We
compared the results of k-Means clustering with
a gold standard set prepared according to the
principles of verb classification advocated by
(Levin, 1993), and reported on the sensitivity
of the classes to linguistically motivated ?lesion-
ing? of the input verb frame. The verb classes
we used are listed in Table 2.
The verb classes are closely related to Levin?s
English classes. They also agree with the Ger-
man verb classification in (Schumacher, 1986)
as far as the relevant verbs appear in his less
extensive semantic ?fields?. The rough glosses
and the references to Levin?s classes in the table
are primarily to aid the intuition of non-native
speakers of German.
Clustering can be thought of as a process
that finds a discrete approximation to a distance
measure. For any data set of n items over which
a distance measure is defined, the Gram matrix
is the symmetric n-by-n matrix whose elements
Mij are the distances between items i and j.
The diagonal elements Mii of this matrix will all
be 0. Every clustering corresponds to a block-
diagonal Gram matrix. Clustering n items into
k classes corresponds to the choice of an order-
ing for the labels on the axes of the Gram matrix
and the choice of k ? 1 change points marking
the boundaries between the blocks. Thus, the
search space of clusters is very large. The avail-
able techniques for searching this large space do
not (and probably cannot) offer guarantees of
global optimality. Standard nostrums include
transformations of the underlying data, and the
deployment of different strategies for initializ-
ing the cluster centers. These may produce in-
tuitively attractive clusters, but when we apply
these ideas to our verb frame data many ques-
tions remain, including the following:
? When the solutions found by clustering dif-
fer from our intuitions, is this because of
failures in the features used, the clustering
techniques, or the intuitions?
? How close are the local optima found by the
clustering techniques to the best solutions
in the space defined by the data?
? Is it even appropriate to use frequency in-
formation for this problem? Or would it
suffice to characterize verb classes by the
pattern of frames that their members can
inhabit, without regard to frequency?
? Does the data support some clusters more
strongly than others? Are all the distinc-
tions made in classifications such as Levin?s
of equal validity?
In response to these questions, the present
paper describes an application to the verb data
of a particular spectral clustering technique (Ng
et al, 2002). At the heart of this approach is a
transformation of the original verb frame data
into a set of orthogonal eigenvectors. We work
Aspect 55.1 anfangen, aufho?ren, beenden, beginnen, enden
start, stop, bring to an end, begin, end
Propositional Attitude 29.5 ahnen, denken, glauben, vermuten, wissen
sense, think, think, guess, know
Transfer of Possession 29.5 bekommen, erhalten, erlangen, kriegen
(Obtaining) receive, obtain, acquire, get
Transfer of Possession 11.1/3 bringen, liefern, schicken, vermitteln, zustellen
(Supply) bring, deliver, send, procure, deliver
Manner of Motion 51.4.2 fahren, fliegen, rudern, segeln
drive, fly, row, sail
Emotion 31.1 a?rgern, freuen
irritate, delight
Announcement 37.7 anku?ndigen, bekanntgeben, ero?ffnen, verku?nden
announce, make known, disclose, proclaim
Description 29.2 beschreiben, charakterisieren, darstellen, interpretieren
describe, characterise, present, interpret
Insistence - beharren, bestehen, insistieren, pochen
all mean insist
Position 50 liegen, sitzen, stehen
lie, sit, stand
Support - dienen, folgen, helfen, unterstu?tzen
serve, follow, help, support
Opening 45.4 o?ffnen, schlie?en
open, close
Consumption 39.4 essen, konsumieren, lesen, saufen, trinken
eat, consume, read, drink (esp. animals or drunkards), drink
Weather 57 blitzen, donnern, da?mmern, nieseln, regnen, schneien
flash, thunder, dawn/grow dusky, drizzle, rain, snow
Table 2: Levin-style verb classes for German
in the space defined by the first few eigenvec-
tors, using standard clustering techniques in the
transformed space.
4 The spectral clustering algorithm
The spectral clustering algorithm takes as in-
put a matrix formed from a pairwise similarity
function over a set of data points. In image
segmentation two pixels might be declared sim-
ilar if they have similar intensity, similar hue or
similar location, or if a local edge-detection al-
gorithm does not place an edge between them.
The technique is generic, and as (Longuet-
Higgins and Scott, 1990) point out, originated
not in computer science or AI but in molecu-
lar physics. Most of the authors nevertheless
?adopt the terminology of image segmentation
(i.e. the data points are pixels and the set of pix-
els is the image), keeping in mind that all the
results are also valid for similarity-based clus-
tering? (Meila? and Shi, 2001). Our natural lan-
guage application of the technique uses straight-
forward similarity measures based on verb frame
statistics, but nothing in the algorithm hinges
on this, and we plan in future work to elabo-
rate our similarity measures. Although there
are several roughly analogous spectral cluster-
ing techniques in the recent literature (Meila?
and Shi, 2001; Longuet-Higgins and Scott, 1990;
Weiss, 1999), we use the algorithm from (Ng et
al., 2002) because it is simple to implement and
understand.
Here are the key steps of that algorithm:
Given a set of points S = {s1, . . . , sn} in a high
dimensional space.
1. Form a distance matrix D ? R2. For
(Ng et al, 2002) this distance measure is
Euclidean, but other measures also make
sense.
2. Transform the distance matrix to an affin-
ity matrix by Aij = exp(?
D2ij
?2 ) if i 6= j,
0 if i = j. The free parameter ?2 controls
the rate at which affinity drops off with dis-
tance.
3. Form the diagonal matrix D whose (i,i) el-
ement is the sum of A?s ith row, and create
the matrix L = D?1/2AD?1/2.
4. Obtain the eigenvectors and eigenvalues of
L.
5. Form a new matrix from the vectors associ-
ated with the k largest eigenvalues. Choose
k either by stipulation or by picking suffi-
cient eigenvectors to cover 95% of the vari-
ance1.
6. Each item now has a vector of k co-
ordinates in the transformed space. Nor-
malize these vectors to unit length.
7. Cluster in k-dimensional space. Following
(Ng et al, 2002) we use k-Means for this
purpose, but any other algorithm that pro-
duces tight clusters could fill the same role.
In (Ng et al, 2002) an analysis demon-
strates that there are likely to be k well-
separated clusters.
We carry out the whole procedure for a range
of values of ?. In our experiments ? is searched
in steps of 0.001 from 0.01 to 0.059, since that
always sufficed to find the best aligned set of
clusters. If ? is set too low no useful eigenvec-
tors are returned, but this situation is easy to
detect. We take the solution with the best align-
ment (see definition below) to the (original) dis-
tance measure. This is how (Christianini et al,
2002) choose the best solution, while (Ng et al,
2002) explain that they choose the solution with
the tightest clusters, without being specific on
how this is done.
In general it matters how initialization of
cluster centers is done for algorithms like k-
Means. (Ng et al, 2002) provide a neat ini-
tialization strategy, based on the expectation
that the clusters in their space will be orthog-
onal. They select the first cluster center to be
a randomly chosen data point, then search the
remaining data points for the one most orthog-
onal to that. For the third data point they look
for one that is most orthogonal to the previ-
ous two, and so on until sufficient have been
obtained. We modify this strategy slightly, re-
moving the random component by initializing n
times, starting out at each data point in turn.
This is fairly costly, but improves results, and
is less expensive than the random initializations
and multiple runs often used with k-Means.
1Srini Parthasarathy suggested this dodge for allow-
ing the eigenvalues to select the appropriate number of
clusters.
5 Experiments and evaluation
We clustered the verb frames data using our ver-
sion of the algorithm in (Ng et al, 2002). To cal-
culate the distance d between two verbs v1 and
v2 we used a range of measures: the cosine of the
angle between the two vectors of frame proba-
bilities, a flattened version of the cosine mea-
sure in which all non-zero counts are replaced
by 1.0 (labelled bcos, for binarized cosine, in Ta-
ble 3), and skew divergence, recently shown as
an effective measure for distributional similar-
ity (Lee, 2001). This last is defined in terms of
KL-divergence, and includes a free weight pa-
rameter w, which we set to 0.9, following(Lee,
2001), Skew-divergence is asymmetric in its ar-
guments, but our technique needs a symmet-
ric measure,so we calculate it in both directions
and use the larger value.
Table 3 contains four results for each of three
distance measures (cos,bcos and skew). The first
line of each set gives the results when the spec-
tral algorithm is provided with the prior knowl-
edge that k = 14. The second line gives the
results when the standard k-Means algorithm is
used, again with k = 14. In the third line of
each set, the value of k is determined from the
eigenvalues, as described above. For cos 12 clus-
ters are chosen, for bcos the chosen value is 17,
and for skew it is 16. The final line of each set
gives the results when the standard algorithm is
used, but k is set to the value selected for that
distance measure by the spectral method.
For standard k-Means, the initialization
strategy from (Ng et al, 2002) does not ap-
ply (and does not work well in any case), so
we used 100 random replications of the initial-
ization, each time initializing the cluster centers
with k randomly chosen data points. We report
the result that had the highest alignment with
the distance measure (cf. Section 5.1).
(Meila? and Shi, 2001) provide analysis in-
dicating that their MNcut algorithm (another
spectral clustering technique) will be exact
when the eigenvectors used for clustering are
piecewise constant. Figure 1 shows the top 16
eigenvectors of a distance matrix based on skew
divergence, with the items sorted by the first
eigenvector. Most of the eigenvectors appear to
be piecewise constant, suggesting that the con-
ditions for good performance in clustering are
indeed present in the language data. Many of
evidence performance
Algorithm k Support Confidence Quality Precision Recall F-Measure
Cos (Ng) 14 0.80 0.83 0.81 0.30 0.43 0.35
Cos (Direct) 14 - 0.78 0.74 0.21 0.44 0.28
Cos (Ng) (12) - 0.79 0.81 0.26 0.40 0.32
Cos (Direct) 12 - 0.72 0.79 0.20 0.45 0.28
BCos (Ng) 14 0.86 0.86 0.86 0.21 0.23 0.22
BCos (Direct) 14 - 0.81 0.78 0.16 0.21 0.18
BCos (Ng) (17) - 0.86 0.83 0.28 0.23 0.25
BCos (Direct) 17 - 0.87 0.80 0.13 0.11 0.12
Skew (Ng) 14 0.84 0.85 0.85 0.37 0.47 0.41
Skew (Direct) 14 - 0.84 0.78 0.22 0.34 0.27
Skew (Ng) (16) - 0.86 0.88 0.49 0.47 0.48
Skew (Direct) 16 - 0.84 0.84 0.35 0.41 0.37
Table 3: Performance of the clustering algorithms
the eigenvectors appear to correspond to a par-
tition of the data into a small number of tight
clusters. Taken as a whole they induce the clus-
terings reported in Table 3.
5.1 Alignment as an evaluation tool
Pearson correlation between corresponding ele-
ments of the Gram matrices has been suggested
as a measure of agreement between a cluster-
ing and a distance measure (Christianini et al,
2002). Since we can convert a clustering into a
distance measure, alignment can be used in a
number of ways, including comparison of clus-
terings against each other.
For evaluation, three alignment-based mea-
sures are particularly relevant:
? The alignment between the gold standard
and the distance measure reflects the pres-
ence or absence in the distance measure
of evidential support for the relationships
that the clustering algorithm is supposed
to infer. This is the column labelled ?Sup-
port? in Table 3.
? The alignment between the clusters in-
ferred by the algorithm and the distance
measure reflects the confidence that the al-
gorithm has in the relationships that it has
chosen. This is the column labelled ?Con-
fidence? in Table 3.
? The alignment between the gold standard
and the inferred clusters reflects the quality
of the result. This is the column labelled
?Quality? in Table 3.
We hope that when the algorithms are confi-
dent they will also be right, and that when the
data strongly supports a distinction the algo-
rithms will find it.
5.2 Results
Table 3 contains our data. The columns based
on various forms of alignment have been dis-
cussed above. Clusterings are also sets of pairs,
so, when the Gram matrices are discrete, we
can also provide the standard measures of pre-
cision, recall and F-measure. Usually it is ir-
relevant whether we choose alignment or the
standard measures, but the latter can yield un-
expected results for extreme clusterings (many
small clusters or few very big clusters). The
remaining columns provide these conventional
performance measures.
For all the evaluation methods and all the
distance measures that we have tried, the algo-
rithm from (Ng et al, 2002) does better than di-
rect clustering, usually finding a clustering that
aligns better with the distance measure than
does the gold standard. Deficiencies in the re-
sult are due to weaknesses in the distance mea-
sures or the original count data, rather than
search errors committed by the clustering al-
gorithm. Skew divergence is the best distance
measure, cosine is less good and cosine on bina-
rized data the worst.
5.3 Which verbs and clusters are hard?
All three alignment measures can be applied to
a clustering as whole, as above, or restricted to
a subset of the Gram matrix. These can tell
us how well each verb and each cluster matches
the distance measure (or indeed the gold stan-
dard). To compute alignment for a verb we cal-
?1
.
0
?
0
.
2
0
.
0
0
.
6
?
1
.
0
0
.
0
?
1
.
0
0
.
0
?
1
.
0
0
.
0
?
0
.
8
0
.
0
?
0
.
2
0
.
8
?
0
.
8
0
.
2
?
1
.
0
?
0
.
2
0
.
0
0
.
8
0
.
0
0
.
8
?
0
.
4
0
.
6
?
0
.
8
0
.
2
?
0
.
8
0
.
2
?
0
.
4
0
.
6
?
0
.
2
0
.
8
Figure 1: The top 16 eigenvectors of the distance matrix
culate Spearman correlation over its row of the
Gram matrix. For a cluster we do the same, but
over all the rows corresponding to the cluster
members. The second column of Table 4, la-
belled ?Support?, gives the contribution of that
verb to the alignment between the gold stan-
dard clustering and the skew-divergence dis-
tance measure (that is, the empirical support
that the distance measure gives to the human-
preferred placement of the verb). The third col-
umn, labelled ?Confidence? contains the contri-
bution of the verb to the alignment between the
skew-divergence and the clustering inferred by
our algorithm (this is the measure of the confi-
dence that the clustering algorithm has in the
correctness of its placement of the verb, and
is what is maximized by Ng?s algorithm as we
vary ?). The fourth column, labelled ?Correct-
ness?, measures the contribution of the verb to
the alignment between the inferred cluster and
the gold standard (this is the measure of how
correctly the verb was placed). To get a feel
for performance at the cluster level we mea-
sured the alignment with the gold standard.
We merged and ranked the lists proposed by
skew divergence and binary cosine. The figure
of merit, labelled ?Score? is the geometric mean
of the alignments for the members of the clus-
ter. The second column, labelled ?Method?,
indicates which distance measure or measures
produced this cluster. Table 5 shows this rank-
ing. Two highly ranked clusters (Emotion and
a large subset of Weather) are selected by both
distance measures. The highest ranked clus-
ter proposed only by binary cosine is a sub-
Verb Support Confidence Correctness
freuen 0.97 0.97 1.00
a?rgern 0.95 0.95 1.00
stehen 0.93 0.93 1.00
sitzen 0.93 0.93 1.00
liegen 0.92 0.92 1.00
glauben 0.90 0.96 0.89
dienen 0.90 0.94 0.96
pochen 0.89 0.91 0.96
beharren 0.89 0.91 0.96
segeln 0.89 0.87 0.89
. . .
nieseln 0.87 0.92 0.93
. . .
da?mmern 0.82 0.87 0.93
. . .
donnern 0.76 0.86 0.68
unterstu?tzen 0.71 0.79 0.68
beenden 0.68 0.80 0.65
Table 4: Empirical support, confidence and
alignment for skew-divergence
set of Position, but this is dominated by skew-
divergence?s correct identification of the whole
class (see Table 2 for a reminder of the defini-
tions of these classes). The systematic superi-
ority of the probabilistic measure suggests that
there is after all useful information about verb
classes in the non-categorical part of our verb
frame data.
6 Related work
Levin?s (Levin, 1993) classification has pro-
voked several studies that aim to acquire lex-
ical semantic information from corpora using
cues pertaining to mainly syntactic structure
Score Method Cluster
1.0 both freuen a?rgern
1.0 skew liegen sitzen stehen
0.96 skew dienen folgen helfen
0.96 skew beschreiben charakterisieren
interpretieren
0.96 skew beharren insistieren, pochen
0.96 bcos liegen stehen
0.93 skew liefern vermitteln zustellen
0.93 both da?mmern nieseln regnen schneien
0.93 skew ahnen vermuten wissen
Table 5: Cluster quality by origin
(Merlo and Stevenson, 2001; Schulte im Walde,
2000; Lapata, 1999; McCarthy, 2000; Lapata
and Brew, 1999). Other work has used Levin?s
list of verbs (in conjunction with related lexical
resources) for the creation of dictionaries that
exploit the systematic correspondence between
syntax and meaning (Dorr, 1997; Dang et al,
1997; Dorr and Jones, 1996).
Most statistical approaches, including ours,
treat verbal meaning assignment as a semantic
clustering or classification task. The underly-
ing question is the following: how can corpus
information be exploited in deriving the seman-
tic class for a given verb? Despite the unify-
ing theme of using corpora and corpus distri-
butions for the acquisition task, the approaches
differ in the inventory of classes they employ,
in the methodology used for inferring semantic
classes and the specific assumptions concerning
the verbs to be classified (i.e., can they be pol-
ysemous or not).
(Merlo and Stevenson, 2001) use grammati-
cal features (acquired from corpora) to classify
verbs into three semantic classes: unergative,
unaccusative, and object-drop. These classes
are abstractions of Levin?s (Levin, 1993) classes
and as a result yield a coarser classification.
The classifier used is a decision tree learner.
(Schulte im Walde, 2000) uses subcategoriza-
tion information and selectional restrictions to
cluster verbs into (Levin, 1993) compatible se-
mantic classes. Subcategorization frames are in-
duced from the BNC using a robust statistical
parser (Carroll and Rooth, 1998). The selec-
tional restrictions are acquired using Resnik?s
(Resnik, 1993) information-theoretic measure of
selectional association which combines distribu-
tional and taxonomic information in order to
formalise how well a predicate associates with a
given argument.
7 Conclusions
We have described the application to natural
language data of a spectral clustering technique
(Ng et al, 2002) closely related to kernel PCA
(Christianini et al, 2002). We have presented
evidence that the dimensionality reduction in-
volved in the clustering technique can give k-
Means a robustness that it does not display in
direct use. The solutions found by the spec-
tral clustering are always at least as well-aligned
with the distance measure as is the gold stan-
dard measure produced by human intuition, but
this does not hold when k-Means is used directly
on the untransformed data.
Since we work in a transformed space of low
dimensionality, we gain efficiency, and we no
longer have to sum and average data points
in the original space associated with the verb
frame data. In principle, this gives us the
freedom to use, as is standardly done with
SVMs (Christianini and Shawe-Taylor, 2000),
extremely high dimensional representations for
which it would not be convenient to use k-Means
directly. We could for instance use features
which are derived not from the counts of a single
frame but of two or more. This is linguistically
desirable, since Levin?s verb classes are defined
primarily in terms of alternations rather than in
terms of single frames. We plan to explore this
possibility in future work.
It is also clearly against the spirit of (Levin,
1993) to insist that verbs should belong to only
one cluster, since, for example, both the Ger-
man ?da?mmern? and the English ?dawn? are
clearly related both to verbs associated with
weather and natural phenomena (because of
?Day dawns.?) and to verbs of cognition (be-
cause of ?It dawned on Kim that . . . ?). In or-
der to accommodate this, we are exploring the
consequences of replacing the k-Means step of
our algorithm with an appropriate soft cluster-
ing technique.
References
Glenn Carroll and Mats Rooth. 1998. Valence
induction with a head-lexicalized PCFG. In
Nancy Ide and Atro Voutilainen, editors, Pro-
ceedings of the 3rd Conference on Empiri-
cal Methods in Natural Language Processing,
pages 36?45, Granada, Spain.
Nello Christianini and John Shawe-Taylor.
2000. An Introduction to Support Vector
Machines and other Kernel-based Learning
Methods. Cambridge University Press.
Nello Christianini, John Shawe-Taylor, and Jaz
Kandola. 2002. Spectral kernel methods for
clustering. In T. G. Dietterich, S. Becker, and
Z. Ghahramani, editors, Advances in Neu-
ral Information Processing Systems 14, Cam-
bridge, MA. MIT Press.
Hoa Trang Dang, Joseph Rosenzweig, and
Martha Palmer. 1997. Associating semantic
components with intersective Levin classes.
In Proceedings of the 1st AMTA SIG-IL
Workshop on Interlinguas, pages 1?8, San
Diego, CA.
Bonnie J. Dorr and Doug Jones. 1996. Role
of word sense disambiguation in lexical ac-
quisition: Predicting semantics from syntac-
tic cues. In Proceedings of the 16th Interna-
tional Conference on Computational Linguis-
tics, pages 322?327, Copenhagen, Denmark.
Bonnie J. Dorr. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation. Ma-
chine Translation, 12(4):371?322.
Maria Lapata and Chris Brew. 1999. Using
subcategorization to resolve verb class am-
biguity. In Pascal Fung and Joe Zhou, edi-
tors, Joint SIGDAT Conference on Empiri-
cal Methods in NLP and Very Large Corpora,
College Park, Maryland.
Maria Lapata. 1999. Acquiring lexical gener-
alizations from corpora: A case study for
diathesis alternations. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 397?404,
College Park, MD.
Lillian Lee. 2001. On the effectiveness of the
skew divergence for statistical language anal-
ysis. In Artificial Intelligence and Statistics
2001, pages 65?72.
Beth Levin. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
H. Christopher Longuet-Higgins and Guy L.
Scott. 1990. Feature grouping by ?relocalisa-
tion? of eigenvectors of the proximity matrix.
In Proceedings of the British Machine Vision
Conference., pages 103?8, Oxford, UK.
Diana McCarthy. 2000. Using semantic pref-
erences to identify verbal participation in
role switching alternations. In Proceedings of
the 1st North American Annual Meeting of
the Association for Computational Linguis-
tics, pages 256?263, Seattle, WA.
Marina Meila? and Jianbo Shi. 2001. A random
walks view of spectral segmentation. In Arti-
ficial Intelligence and Statistics 2001.
Paola Merlo and Susanne Stevenson. 2001. Au-
tomatic verb classification based on statistical
distribution of argument structure. Compu-
tational Linguistics, 27(3):373?408.
Andrew. Y. Ng, Michael. I. Jordan, and Yair
Weiss. 2002. On spectral clustering: Anal-
ysis and an algorithm. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Ad-
vances in Neural Information Processing Sys-
tems 14, Cambridge, MA. MIT Press.
Philip Stuart Resnik. 1993. Selection and In-
formation: A Class-Based Approach to Lexi-
cal Relationships. Ph.D. thesis, University of
Pennsylvania.
Sabine Schulte im Walde and Chris Brew. 2002.
Inducing german semantic verb classes from
purely syntactic subcategorization informa-
tion. In Association for Computational Lin-
guistics,40th Anniversary Meeting, Philadel-
phia,Pa. To appear.
Sabine Schulte im Walde. 2000. Clustering
verbs semantically according to their alter-
nation behaviour. In Proceedings of the 18th
International Conference on Computational
Linguistics, pages 747?753, Saarbru?cken,
Germany.
Sabine Schulte im Walde. 2002. A subcategori-
sation lexicon for german verbs induced from
a lexicalised PCFG. In Proceedings of the 3rd
Conference on Language Resources and Eval-
uation, Las Palmas, Spain. To appear.
Helmut Schumacher. 1986. Verben in Feldern.
de Gruyter, Berlin.
Yair Weiss. 1999. Segmentation using eigenvec-
tors: A unifying view. In ICCV (2), pages
975?982.
Using the Segmentation Corpus to define an inventory of 
concatenative units for Cantonese speech synthesis 
 
Wai Yi Peggy WONG 
Chris BREW 
Mary E. BECKMAN 
Linguistics Dept., Ohio State University 
222 Oxley Hall, 1712 Neil Ave. 
Columbus, OH, 43210-1298  USA 
{pwong, cbrew, mbeckman}@ling.osu.edu 
Shui-duen CHAN 
Chinese Language Centre, 
Hong Kong Polytechnic University 
Yuk Choi Road, Hung Hom, Kowloon, 
HONG KONG 
chsdchan@polyu.edu.hk 
 
Abstract  
The problem of word segmentation affects 
all aspects of Chinese language processing, 
including the development of text-to-speech 
synthesis systems. In synthesizing a Hong 
Kong Cantonese text, for example, words 
must be identified in order to model fusion 
of coda [p] with initial [h], and other similar 
effects that differentiate word-internal 
syllable boundaries from syllable edges that 
begin or end words. Accurate segmentation 
is necessary also for developing any list of 
words large enough to identify the word-
internal cross-syllable sequences that must 
be recorded to model such effects using 
concatenated synthesis units. This paper 
describes our use of the Segmentation 
Corpus to constrain such units. 
Introduction 
What are the best units to use in building a fixed 
inventory of concatenative units for an unlimited 
vocabulary text-to-speech (TTS) synthesis 
system for a language? Given a particular choice 
of unit type, how large is the inventory of such 
units for the language, and what is the best way 
to design materials to cover all or most of these 
units in one recording session? Are there effects 
such as prosodically conditioned allophony that 
cannot be modeled well by the basic unit type? 
These are questions that can only be answered 
language by language, and answering them for 
Cantonese1 poses several interesting challenges.   
                                                     
                                                                               
1We use ?Cantonese? to mean the newer Hong Kong 
One major challenge involves the definition 
of the ?word? in Cantonese. As in other varieties 
of Chinese, morphemes in Cantonese are 
typically monosyllabic and syllable structure is 
extremely simple, which might suggest the 
demi-syllable or even the syllable (Chu & 
Ching, 1997) as an obvious basic unit. At the 
same time, however, there are segmental 
?sandhi? effects that conjoin syllables within a 
word. For example, when the morpheme ? 
zaap6 2  stands as a word alone (meaning ?to 
collect?), the [p] is a glottalized and unreleased 
coda stop, but when the morpheme occurs in the 
longer word ?? zaap6hap6 (?to assemble?), 
the coda [p] often resyllabifies and fuses with 
the following [h] to make an initial aspirated 
stop. Accurate word segmentation at the text 
analysis level is essential for identifying the 
domain of such sandhi effects in any full-fledged 
TTS system, whatever method is used for 
generating the waveform from the specified 
pronunciation of the word. A further challenge is 
to find a way to capture such sandhi effects in 
systems that use concatenative methods for 
waveform generation.  
This paper reports on research aimed at 
defining an inventory of concatenative units for 
Cantonese using the Segmentation Corpus, a 
lexicon of 33k words extracted from a large 
corpus of Cantonese newspaper texts. The 
corpus is described further in Section 2 after an 
excursus (in Section 1) on the problems posed 
standard, and not the older Canton City one. 
2 We use the Jyutping romanization developed by the 
Linguistics Society of Hong Kong in 1993. See 
http://www.cpct92.cityu.edu.hk/lshk.  
by the Cantonese writing system. Section 3 
outlines facts about Cantonese phonology 
relevant to choosing the concatenative unit, and 
Section 4 calculates the number of units that 
would be necessary to cover all theoretically 
possible syllables and sequences of syllables. 
The calculation is done for three models: (1) 
syllables, as in Chu & Ching (1997), (2) Law & 
Lee?s (2000) mixed model of onsets, rhymes, 
and cross-syllabic rhyme-onset units, and (3) a 
positionally sensitive diphone model. This 
section closes by reporting how the number of 
units in the last model is reduced by exploiting 
the sporadic and systematic phonotactic gaps 
discovered by looking for words exemplifying 
each possible unit in the Segmentation Corpus.  
1 The Cantonese writing system 
The Cantonese writing system poses unique 
challenges for developing online lexicons, not 
all of which are related to the ?foremost 
problem? of word segmentation. These problems 
stem from the long and rich socio-political 
history of the dialect, which makes the writing 
system even less regular than the Mandarin one, 
even though Cantonese is written primarily with 
the same logographs (?Chinese characters?).  
The main problem is that each character has 
several readings, and the reading cannot always 
be determined based on the immediate context 
of the other characters in a multisyllabic word. 
For some orthographic forms, the variation is 
stylistic. For example, the word?? ?support? 
can be pronounced zi1jyun4 or zi1wun4. But 
for other orthographic forms, the variation in 
pronunciation corresponds to different words, 
with different meanings.  For example, the 
character sequence?? writes both the function 
word zing3dong1 ?while? and the content word 
zing3dong3 ?proper?. Moreover, some words, 
such as ko1 ?to page, telephone?, ge3 (genitive 
particle), and laa3 (aspect marker), have no 
standard written form. In colloquial styles of 
writing, these forms may be rendered in non-
standard ways, such as using the English source 
word call to write ko1, or writing the particles 
with special characters unique to Cantonese. In 
more formal writing, however, such forms must 
be left to the reader to interpolate from a 
character ?borrowed? from some other 
morpheme. For example, ge3 (genitive particle) 
might be written ? , a character which more 
typically writes the morpheme dik1 in 
muk6dik1?? ?aim?, but which suggests ge3 
because it also writes a genitive particle in 
Mandarin (de in the Pinyin romanization). Thus, 
?  has a reading dik1 that is etymologically 
related to the Mandarin morpheme, but it also 
has the etymologically independent reading ge3 
because Cantonese readers can read texts written 
in Mandarin as if they were Cantonese. Such 
ambiguities of reading make the task of 
developing online wordlists from text corpora 
doubly difficult, since word segmentation is 
only half the task.  
2 The Segmentation Corpus 
The Segmentation Corpus is an electronic 
database of around 33,000 Cantonese word types 
extracted from a 1.7 million character corpus of 
Hong Kong newspapers, along with a tokenized 
record of the text. It is described in more detail 
in Chan & Tang (1999). The Cantonese corpus 
is part of a larger database of segmented Chinese 
texts, including Mandarin newspapers from both 
the PRC and Taiwan.  The three databases were 
created using word-segmentation criteria 
developed by researchers at the Chinese 
Language Centre and Department of Chinese 
and Bilingual Studies, Hong Kong Polytechnic 
University. Since these criteria were intended to 
be applicable to texts in all three varieties, they 
do not refer to the phonology.  
For our current purpose, the most useful part 
of the Segmentation Corpus is the wordlist 
proper, a file containing a separate entry for each 
word type identified by the segmentation criteria. 
Each entry has three fields: the orthographic 
form(s), the pronunciation(s) in Jyutping, and 
the token frequency in the segmented newspaper 
corpus. In the original corpus, the first field 
could have multiple entries. For example, there 
are two character strings, ?? and??, in the 
entry for the word faan6laam6 ?to flood?. 
However, the two readings of ?? were not 
listed separately in the pronunciation field for 
that orthographic form (and there was only one 
entry for the two words written with ??).  
Before we could use the wordlist, therefore, 
we had to check the pronunciation field for each 
entry. The first author, a native speaker of Hong 
Kong Cantonese, examined each entry in order 
to add variant readings not originally listed (as 
in the entry for ?? ?support?) and to correct 
readings that did not correspond to the modern 
Hong Kong pronunciation (as in the entry for? 
?box?). In addition, when the added variant 
pronunciation corresponded to an identifiably 
different word (as in zing3dong3 ?proper? 
versus zing3dong1 ?while? for ?? ), the 
entry was split in two, and all tokens of that 
character string in the larger corpus were 
examined, in order to allocate the total token 
count for the orthographic form to the two 
separate frequencies for the two different words. 
Approximately 90 original entries were split into 
separate entries by this processing. In this way, 
the 32,840 entries in the original word list 
became 33,037 entries. Once this task was 
completed, we could use the wordlist to count 
all of the distinct units that would be needed to 
synthesize all of the words in the Segmentation 
Corpus. To explain what these units are, we 
need to describe the phonology of Cantonese 
words and longer utterances. 
3 Cantonese phonology 
The smallest possible word is a nasal ([m] or 
[]) or vowel as a bare tone-bearing syllable 
nucleus, as in ? ng5 ?five? and? aa2 ?dumb?. 
A syllable with a vowel as nucleus can also have 
an onset consonant, and it can have a more 
complex rhyme which combines the vowel with 
any of the three coda nasals [m], [n], and [], the 
two glides [j] and [w], or the three coda stops 
[p], [t], and [k], as in ? zoeng1 ?will?, ? wai6 
?stomach?, and ? leot2 ?rate?. Tables 1 and 2 
show the 11 vowels and 19 consonants of 
Cantonese, and Figure 1 plots representative F0 
contours for each of the tones that contrast on a 
syllable with an all-sonorant rhyme.  
If there were no phonotactic restrictions on 
combinations of vowels and consonants, there 
would be 101 distinct rhymes in each of the six 
tones. However, not all combinations are 
possible. For example, tone 5 does not occur on 
syllables with coda stops (and tone 4 on such 
checked syllables is limited to onomatopoeia). 
Also, the mid short vowel [e] does not occur in 
open syllables, and in closed syllables it occurs 
only before [k], [], and [j], whereas [i:] occurs 
in closed syllables only before [p], [t], [m], [n], 
and [w]. The Jyutping transliteration system 
takes advantage of this kind of complementary 
distribution to limit the number of vowel 
symbols. Thus ?i? is used to transcribe both the 
short mid vowel [e] in the rhymes [ek] and [e], 
and the high vowel [i:] in the rhymes [i:], [i:t], 
[i:p], [i:m], [i:n], and [i:w]. Ignoring tone, there 
are 54 rhyme types in standard usage of 
Jyutping. Canonical forms of longer words can 
be described to a very rough first approximation 
as strings of syllables. However, fluent synthesis 
cannot be achieved simply by stringing syllables 
together without taking into account the effects 
of position in the word or phrase.  
 
front central back  
 round  round  
 i: y:  u: high 
e  

o mid (short) 
: :  : mid (long) 
  
 a: low vowels
Table 1. Vowels of Cantonese. 
 
labial dental palatal velar labio
velar 
 
ph th, tsh  kh khw  
p t, ts  k kw  
f s    h 
m n, l 
  w  
Table 2. Consonants of Cantonese. 
Time (s)
0 0.7
100
300
 
tone 1
tone 3
tone 6
tone 4
tone 2 
tone 5 
Figure 1. F0 contours for six words [wj] with 
different tones. Numbers to the right identify the 
endpoints of the two rising tones (in grey) and 
numbers to the left identify starting points of the 
other four tones (in black). The discontinuities in 
wai4 are where the speaker breaks into creak. 
o5 jyun4 loi4 hai6 wai3
223 221 221 22 333 HL%
laa221+22
Time (s)
0 1.56807
100
250
 
F
re
qu
en
cy
 (
H
z)
 
Figure 2. Spectrogram with F0 track superimposed for an utterance of the sentence o5 jyun4loi4 hai6 
wai3 ?Oh, I get it.  It was the character?!? (The context is a dictation task.) The labelling window above the 
signal view shows a partial transcription in the annotation conventions proposed by Wong, Chan & Beckman 
(in press), with a syllable-by-syllable Jyutping transliteration (top tier), a transcription of the (canonical) 
lexical tones and boundary tone, and a phonetic transcription of fused forms (lowest tier). The HL% 
boundary tone is a pragmatic morpheme, which we have translated with the ?Oh, I get it.? phrase.   
 
 ? ? ? ? basic units 
Jyutping ging1 zai3 hok6 gaa1 (added units) 
Chu & Ching 	
   :	 	:# 1042 (1042) 
Law & Lee #	 
$ $ :	$	 :# 1801 
diphones #	
 
 $  j $: :	 	: :# 1097 
Table 3. The string of basic units and exceptional units (underlined) that would be needed to synthesize an 
utterance of the word ?economist? in each of the three models.   
 
One of these effects is the fusion of coda [p] 
with initial [h] in words such as ?? zaap6 
hap6. In fluent connected speech, such effects 
can be extreme. Consonants can be reduced or 
deleted, with the abutting vowels fused together, 
as in the pronunciation [jy:n21la:212] for the 
phrase ??? jyun4loi4 hai6 ?was? (with 
the verb cliticized onto the preceding tense 
adverb) as in Figure 2. Eventually, we plan to 
use the larger Segmentation Corpus to look for 
likely targets of fusion. For now, however, we 
focus on positional effects that can be modeled 
just by recording word lists. Figure 2 illustrates 
one such effect. The final syllable in this 
utterance is sustained, to be longer than the other 
four syllables combined. It also bears two extra 
tone targets for the HL% boundary tone. (See 
Wong, Chan & Beckman, in press, for a 
discussion of these utterance-level effects.) 
Phrase-final lengthening is not usually so 
extreme in read speech, and the inventory of 
boundary tones is more limited. However, there 
will be sufficient final lengthening to warrant the 
recording of separate units for (the rhymes of) 
final syllables. These two positional effects 
increase the inventory of units, albeit in different 
ways depending on the choice of ?basic? unit.  
4 Counting different unit types 
Table 3 illustrates three synthesis models using 
the word ????  ging1zai3hok6gaa1 
?economist?. The last column in Table 3 lists the 
theoretically possible number of basic units. The 
first model concatenates syllables. If each onset 
could be combined with each rhyme, there 
would be 1042 syllable types. A second set of 
syllables can be recorded to capture final 
lengthening. However, there is no comparably 
obvious way to capture the cross-syllabic effects 
with this concatenative unit. The second model 
uses cross-syllabic units which combine the 
rhyme of one syllable with the following initial. 
This automatically captures the sandhi effects. 
The model also captures final lengthening, 
because separate units are recorded for onsets 
with no preceding rhyme and rhymes with no 
following onset. With 54 final rhymes, 1728 
combinations of rhyme followed by medial 
onset, and 19 initial onsets, there are 1801 
theoretically possible units. The last model is 
our diphone model, which differentiates codas 
from onset consonants. That is, the rhyme aak$ 
is distinct from the cross-syllabic diphone aa$k. 
This model has the advantage over Law & Lee?s 
cross-syllable final-initial combination model in 
that spectral continuity between the initial and 
rhyme is captured in the CV diphones (such as 
#gi and zai). Similarly, the diphones capture 
the dependency between the quality of the [h] 
and that of the following vowel (i.e., one records 
separate cross-syllable diphones for i$ho, i$hi, 
i$haa, and so on). However, the number of 
theoretically possible units is smaller, because 
we do not record consonant sequences that abut 
silence with silence ? e.g., aak$ can be 
combined directly with $ka or $ta, so no cross-
syllabic units need be recorded for k$k and k$t. 
Note that none of these counts takes tone 
into consideration. However, since every 
syllable bears a (full) tone, and since tones are 
rarely deleted in running speech, recording 
different units for rhymes with different tones 
should improve naturalness, particularly for 
cases where voice quality is part of the tonal 
specification (as suggested by the contour for 
tone 4 in Figure 1). Naturalness may also require 
different cross-syllabic units for different tone 
sequences when sonorant segments abut at 
syllable edges (so as to insure tonal continuity).  
Of course, when tone specification is taken 
into account, the number of necessary units 
grows substantially. For example, there are 
12,120 distinct syllables, and even more units in 
the other two models. However, when we count 
only those types that are attested in the words of 
the Segmentation Corpus, there are many fewer 
units. For example, the total number of attested 
units taking tone into account in the diphone 
model is 2292. If each diphone were recorded in 
a disyllabic carrier word, a Cantonese speaker 
could speak all of the words to make a new 
voice in a single recording session. (For 
comparison, the number of attested diphones 
ignoring tone is 634.) 
Conclusion 
We have shown one way of using a segmented 
database to inform the design of a unit inventory 
for TTS. We augmented the Segmentation 
Corpus with transliterations that would let us 
predict more accurately the pronunciation that a 
Cantonese speaker adopting a careful speaking 
style would be likely to produce for a character 
sequence. Judgements about the phonology of 
Cantonese, in combination with the augmented 
wordlist, and the associated word frequency 
data, can be used to assess the costs and likely 
benefits of different strategies for unit selection 
in Cantonese TTS. In particular, we present data 
indicating the feasibility of a new diphone 
selection strategy that finesses some of the 
problems in modelling the interactions between 
tone and segmental identity. It remains to be 
demonstrated that this strategy can actually 
deliver the results which it appears to promise. 
This is our future work. 
Acknowledgements 
This work was supported by a grant from the 
University Grants Committee of Hong Kong to 
Y. S. Cheung and an SBC/Ameritech Faculty 
Research Award to C. Brew and M. Beckman.  
References  
Chan S. D. and Tang Z. X. (1999) Quantitative 
Analysis of Lexical Distribution in Different 
Chinese Communities in the 1990?s. Yuyan Wenzi 
Yingyong [Applied Linguistics], No.3, 10-18. 
Chu M. and Ching P. C. (1997) A Cantonese 
synthesizer based on TD-PSOLA method. 
Proceedings of the 1997 International Symposium 
on Multimedia Information Processing. Academia 
Sinica, Taipei, Taiwan, Dec. 1997.  
Law K. M. and Lee Tan (2000) Using cross-syllable 
units for Cantonese speech synthesis. Proceedings 
of the 2000 International Conference on Spoken 
Language Processing, Beijing, China, Oct. 2000.  
Wong W. Y. P., Chan M. K-M., and Beckman M. E. 
(in press)  An Autosegmental-Metrical analysis and 
prosodic conventions for Cantonese. To appear in 
S-A. Jun, ed. Prosodic Models and Transcription: 
Towards Prosodic Typology. Oxford University 
Press.  
A Resource-light Approach to Russian Morphology: Tagging Russian using
Czech resources
Jiri Hana and Anna Feldman and Chris Brew
Department of Linguistics
Ohio State University
Columbus, OH 43210
Abstract
In this paper, we describe a resource-light system
for the automatic morphological analysis and tag-
ging of Russian. We eschew the use of extensive
resources (particularly, large annotated corpora and
lexicons), exploiting instead (i) pre-existing anno-
tated corpora of Czech; (ii) an unannotated corpus
of Russian. We show that our approach has benefits,
and present what we believe to be one of the first full
evaluations of a Russian tagger in the openly avail-
able literature.
1 Introduction
Morphological processing and part-of-speech tag-
ging are essential for many NLP tasks, including
machine translation, information retrieval and pars-
ing. In this paper, we describe a resource-light ap-
proach to the tagging of Russian. Because Russian
is a highly inflected language with a high degree
of morpheme homonymy (cf. Table 11) the tags in-
volved are more numerous and elaborate than those
typically used for English. This complicates the tag-
ging task, although as has been previously noted
(Elworthy, 1995), the increased complexity of the
tags does not necessarily translate into a more de-
manding tagging task. Because no large annotated
corpora of Russian are available to us, we instead
chose to use an annotated corpus of Czech. Czech
is sufficiently similar to Russian that it is reasonable
to suppose that information about Czech will be rel-
evant in some way to the tagging of Russian.
The languages share many linguistic properties (free
word order and a rich morphology which plays
a considerable role in determining agreement and
argument relationships). We created a morpho-
logical analyzer for Russian, combined the results
with information derived from Czech and used the
TnT (Brants, 2000) tagger in a number of differ-
1All Russian examples in this paper are transcribed in the
Roman alphabet. Our system is able to analyze Russian texts
in both Cyrillic and various transcriptions.
krasiv-a beautiful (short adjective, feminine)
muz?-a husband (noun, masc., sing., genitive)
husband (noun, masc., sing., accusative)
okn-a window (noun, neuter, sing., genitive)
window (noun, neuter, pl., nominative)
window (noun, neuter, pl., accusative)
knig-a book (noun, fem., sing., nominative)
dom-a house (noun, masc., sing., genitive)
house (noun, masc., pl., nominative)
house (noun, masc., pl., accusative)
skazal-a say (verb, fem., sing., past tense)
dv-a two (numeral, masc., nominative)
Table 1: Homonymy of the a ending
ent ways, including a a committee-based approach,
which turned out to give the best results. To eval-
uate the results, we morphologically annotated (by
hand) a small corpus of Russian: part of the transla-
tion of Orwell?s ?1984? from the MULTEXT-EAST
project (Ve?ronis, 1996).
2 Why TnT?
Readers may wonder why we chose to use TnT,
which was not designed for Slavic languages. The
short answer is that it is convenient and successful,
but the following two sections address the issue in
rather more detail.
2.1 The encoding of lexical information in TnT
TnT records some lexical information in the emis-
sion probabilities of its second order Markov
Model. Since Russian and Czech do not use the
same words we cannot use this information (at least
not directly) to tag Russian. Given this, the move
from Czech to Russian involves a loss of detailed
lexical information. Therefore we implemented a
morphological analyzer for Russian, the output of
which we use to provide surrogate emission proba-
bilities for the TnT tagger (Brants, 2000). The de-
tails are described below in section 4.2.
2.2 The modelling of word order in TnT
Both Russian and Czech have relatively free word
order, so it may seem an odd choice to use a Markov
model (MM) tagger. Why should second order
MM be able to capture useful facts about such lan-
guages? Firstly, even if a language has the poten-
tial for free word order, it may still turn out that
there are recurring patterns in the progressions of
parts-of-speech attested in a training corpus. Sec-
ondly, n-gram models including MM have indeed
been shown to be successful for various Slavic lan-
guages, e.g., Czech (Hajic? et al, 2001) or Slovene
(Dz?eroski et al, 2000); although not as much as
for English. This shows that the transitional in-
formation captured by the second-order MM from
a Czech or Slovene corpus is useful for Czech or
Slovene.2 The present paper shows that transitional
information acquired from Czech is also useful for
Russian.
3 Russian versus Czech
A deep comparative analysis of Czech and Russian
is far beyond the scope of this paper. However, we
would like to mention just a number of the most im-
portant facts. Both languages are Slavic (Czech is
West Slavonic, Russian is East Slavonic). Both have
extensive morphology whose role is important in
determining the grammatical functions of phrases.
In both languages, the main verb agrees in person
and number with subject; adjectives agree in gen-
der, number and case with nouns. Both languages
are free constituent order languages. The word or-
der in a sentence is determined mainly by discourse.
It turns out that the word order in Czech and Russian
is very similar. For instance, old information mostly
precedes new information. The ?neutral? order in
the two languages is Subject-Verb-Object. Here is a
parallel Czech-Russian example from our develop-
ment corpus:
(1) a. [Czech]
Byl
wasMasc.Past
jasny?,
brightMasc.Sg.Nom
studeny?
coldMasc.Sg.Nom
dubnovy?
AprilMasc.Sg.Nom
den
dayMasc.Sg.Nom
i
and
hodiny
clocksFem.P l.Nom
odb??jely
strokeFem.P l.Past
tr?ina?ctou.
thirteenthFem.Sg.Acc
b. [Russian]
2Respectively, and if the techniques in the present paper
generalize, probably also irrespectively.
Byl
wasMasc.Past
jasnyj,
brightMasc.Sg.Nom
xolodnyj
coldMasc.Sg.Nom
aprel?skij
AprilMasc.Sg.Nom
den?
dayMasc.Sg.Nom
i
and
c?asy
clocksPl.Nom
probili
strokePl.Past
trinadtsat?.
thirteenAcc
?It was a bright cold day in April, and the
clocks were striking thirteen.? [from Orwell?s
?1984?]
Of course, not all utterances are so similar. Sec-
tion 5.4 briefly mentions how to improve the utility
of the corpus by eradicating some of the systematic
differences.
4 Realization
4.1 The tag system
We adopted the Czech tag system (Hajic?, 2000) for
Russian. Every tag is represented as a string of 15
symbols each corresponding to one morphological
category. For example, the word vidjela is assigned
the tag VpFS- - -XR-AA- - -, because it is a verb (V),
past participle (p), feminine (F), singular (S), does
not distinguish case (-), possessive gender (-), pos-
sessive number (-), can be any person (X), is past
tense (R), is not gradable (-), affirmative (A), active
voice (A), and does not have any stylistic variants
(the final hyphen).
No. Description Abbr. No. of values
Cz Ru
1 POS P 12 12
2 SubPOS ? detailed POS S 75 32
3 Gender g 11 5
4 Number n 6 4
5 Case c 9 8
6 Possessor?s Gender G 5 4
7 Possessor?s Number N 3 3
8 Person p 5 5
9 Tense t 5 5
10 Degree of comparison d 4 4
11 Negation a 3 3
12 Voice v 3 3
13 Unused 1 1
14 Unused 1 1
15 Variant, Style V 10 2
Table 2: Overview and comparison of the tagsets
The tagset used for Czech (4290+ tags) is larger
than the tagset we use for Russian (about 900 tags).
There is a good theoretical reason for this choice
? Russian morphological categories usually have
fewer values (e.g., 6 cases in Russian vs. 7 in Czech;
Czech often has formal and colloquial variants of
the same morpheme); but there is also an immedi-
ate practical reason ? the Czech tag system is very
elaborate and specifically devised to serve multiple
needs, while our tagset is designed solely to capture
the core of Russian morphology, as we need it for
our primary purpose of demonstrating the portabil-
ity and feasibility of our technique. Still, our tagset
is much larger than the Penn Treebank tagset, which
uses only 36 non-punctuation tags (Marcus et al,
1993).
4.2 Morphological analysis
In this section we describe our approach to a
resource-light encoding of salient facts about the
Russian lexicon. Our techniques are not as rad-
ical as previously explored unsupervised methods
(Goldsmith, 2001; Yarowsky and Wicentowski,
2000), but are designed to be feasible for languages
for which serious morphological expertise is un-
available to us. We use a paradigm-based morphol-
ogy that avoids the need to explicitly create a large
lexicon. The price that we pay for this is overgener-
ation. Most of these analyses look very implausible
to a Russian speaker, but significantly increasing the
precision would be at the cost of greater develop-
ment time than our resource-light approach is able
to commit. We wish our work to be portable at least
to other Slavic languages, for which we assume that
elaborate morphological analyzers will not be avail-
able. We do use two simple pre-processing methods
to decrease the ambiguity of the results handed to
the tagger ? longest ending filtering and an automat-
ically acquired lexicon of stems. These were easy to
implement and surprisingly effective.
Our analyzer captures just a few textbook facts
about the Russian morphology (Wade, 1992), ex-
cluding the majority of exceptions and including in-
formation about 4 declension classes of nouns, 3
conjugation classes of verbs. In total our database
contains 80 paradigms. A paradigm is a set of end-
ings and POS tags that can go with a particular set
of stems. Thus, for example, the paradigm in Table
3 is a set of inflections that go with the masculine
stems ending on the ?hard? consonants, e.g., slon
?elephant?, stol ?table?.
Unlike the traditional notions of stem and ending,
for us a stem is the part of the word that does not
change within its paradigm, and the ending is the
part of the word that follows such a stem. For ex-
ample, the forms of the verb moc?? ?can.INF?: mogu
?1sg?, moz?es?? ?2sg?, moz?et ?3sg?, etc. are analyzed as
0 NNMS1 - - - - - - - - - - y NNMP1 - - - - - - - - - -
a NNMS2 - - - - - - - - - - ov NNMP2 - - - - - - - - - -
u NNMS3 - - - - - - - - - - am NNMP3 - - - - - - - - - -
a NNMS4 - - - - - - - - - - ov NNMP4 - - - - - - - - - -
u NNMS4 - - - - - - - - - 1
e NNMS6 - - - - - - - - - - ax NNMP6 - - - - - - - - - -
u NNMS6 - - - - - - - - - 1
om NNMS7 - - - - - - - - - - ami NNMP7 - - - - - - - - - -
Table 3: A paradigm for ?hard? consonant mascu-
line nouns
the stem mo followed by the endings gu, z?es??, z?et. A
more linguistically oriented analysis would involve
the endings u, es??, et and phonological alternations
in the stem. All stem internal variations are treated
as suppletion.3
Unlike the morphological analyzers that exist for
Russian (Segalovich and Titov, 2000; Segalovich,
2003; Segalovich and Maslov, 1989; Kovalev, 2002;
Mikheev and Liubushkina, 1995; Yablonsky, 1999;
Segalovich, 2003; Kovalev, 2002, among others)
(Segalovich, 2003; Kovalev, 2002; Mikheev and Li-
ubushkina, 1995; Yablonsky, 1999, among others),
our analyzer does not rely on a substantial manu-
ally created lexicon. This is in keeping with our aim
of being resource-light. When analyzing a word,
the system first checks a list of monomorphemic
closed-class words and then segments the word into
all possible prefix-stem-ending triples.4 The result
has quite good coverage (95.4%), but the average
ambiguity is very high (10.9 tags/token), and even
higher for open class words. We therefore have two
strategies for reducing ambiguity.
4.2.1 Longest ending filtering (LEF)
The first approach to ambiguity reduction is based
on a simple heuristic ? the correct ending is usually
one of the longest candidate endings. In English, it
would mean that if a word is analyzed either as hav-
ing a zero ending or an -ing ending, we would con-
sider only the latter; obviously, in the vast majority
of cases that would be the correct analysis. In addi-
tion, we specify that a few long but very rare end-
ings should not be included in the maximum length
calculation (e.g., 2nd person pl. imperative).
3We do in fact have a very similar analysis, the analyzer?s
run-time representation of the paradigms is automatically pro-
duced from a more compact and linguistically attractive spec-
ification of the paradigms. It is possible to specify the ba-
sic paradigms and then specify the subparadigms, exceptions
and paradigms involving phonological changes by referring to
them.
4Currently, we consider only two inflectional prefixes ? neg-
ative ne and superlative nai.
4.2.2 Deriving a lexicon
The second approach uses a large raw corpus5 to
generate an open class lexicon of possible stems
with their paradigms. In this paper, we can only
sketch the method, for more details see (Hana and
Feldman, to appear). It is based on the idea that
open-class lemmata are likely to occur in more than
one form. First, we run the morphological analyzer
on the text (without any filtering), then we add to
the lexicon those entries that occurred with at least a
certain number of distinct forms and cover the high-
est number of forms. If we encounter the word talk-
ing, using the information about paradigms, we can
assume that it is either the -ing form of the lemma
talk or that it is a monomorphemic word (such as
sibling). Based on this single form we cannot really
say more. However, if we also encounter the forms
talk, talks and talked, the former analysis seems
more probable; and therefore, it seems reasonable
to include the lemma talk as a verb into the lexi-
con. If we encountered also talkings, talkinged and
talkinging, we would include both lemmata talk and
talking as verbs.
Obviously, morphological analysis based on such
a lexicon overgenerates, but it overgenerates much
less than if based on the endings alone. For ex-
ample, for the word form partii of the lemma par-
tija ?party?, our analysis gives 8 possibilities ? the
5 correct ones (noun fem sg gen/dat/loc sg and pl
nom/acc) and 3 incorrect ones (noun masc sg loc,
pl nom, and noun neut pl acc; note that only gen-
der is incorrect). Analysis based on endings alone
would allow 20 possibilities ? 15 of them incorrect
(including adjectives and an imperative).
4.3 Tagging
We use the TnT tagger (Brants, 2000), an imple-
mentation of the Viterbi algorithm for second order
Markov models. We train the transition probabili-
ties on Czech (1.5M tokens of the Prague Depen-
dency Treebank (Be?mova? et al, 1999)). We ob-
tain surrogate emission probabilities by running our
morphological analyzer, then assuming a uniform
distribution over the resulting emissions.
5 Experiments
5.1 Corpora
For evaluation purposes, we selected and morpho-
logically annotated (by hand) a small portion from
5We used The Uppsala Russian Corpus (1M tokens), which
is freely available from Uppsala University at http://www.
slaviska.uu.se/ryska/corpus.html.
the Russian translation of Orwell?s ?1984?. This cor-
pus contains 4011 tokens and 1858 types. For devel-
opment, we used another part of ?1984?. Since we
want to work with minimal language resources, the
development corpus is intentionally small ? 1788 to-
kens. We used it to test our hypotheses and tune the
parameters of our tools.
In the following sections, we discuss our experi-
ments and report the results. Note that we do not
report the results for tag position 13 and 14, since
these positions are unused; and therefore, always
trivially correct.
5.2 Morphological analysis
As can be seen from Table 4, morphological anal-
ysis without any filters gives good recall (although
on a non-fiction text it would probably be lower),
but also very high average ambiguity. Both fil-
ters (the longest-ending filter and automatically ac-
quired lexicon) reduce the ambiguity significantly;
the former producing a considerable drop of recall,
the latter retaining high recall. However, we do best
if we first attempt lexical lookup, then apply LEF
to the words not found. This keeps recall reason-
ably high at the same time as decreasing ambiguity.
As expected, performance increases with the size of
the unannotated Russian corpus used to generate the
lexicon. All subsequent experimental results were
obtained using this best filter combination, i.e., the
combination of the lexicon based on the 1Mword
corpus and LEF.
LEF no no no yes yes yes
Lexicon based on 0 100K 1M 0 100K 1M
recall 95.4 94 93.1 84.4 88.3 90.4
avg ambig (tag/word) 10.9 7.0 4.7 4.1 3.5 3.1
Tagging ? accuracy 50.7 62.1 67.5 62.1 66.8 69.4
Table 4: Morph. analysis with various parameters
5.3 Tagging
Table 7 summarizes the results of our taggers on test
data. Our baseline is produced by the morphologi-
cal analyzer without any filters followed by a tagger
randomly selecting a tag among the tags offered by
the morphological analyzer. The direct-full tag col-
umn shows the result of the TNT tagger with transi-
tion probabilities obtained directly from the Czech
corpus and the emission symbols based on the mor-
phological analyzer with the best filters.
To further improve the results, we used two tech-
niques: (i) we modified the training corpus to re-
move some systematic differences between Czech
and Russian (5.4); (ii) we trained batteries of tag-
gers on subtags to address the data sparsity problem
(5.5 and 5.6).
5.4 Russification
We experimented with ?russified? models. We
trained the TnT tagger on the Czech corpus with
modifications that made the structure of training
data look more like Russian. For example, plural
adjectives and participles in Russian, unlike Czech,
do not distinguish gender.
(2) a. Nadan??
Giftedmasc.pl
muz?i
men
soutez?ili.
competedmasc.pl
?Gifted sportsmen were competing.? [Cz]
b. Nadane?
Giftedfem.pl
z?eny
women
soutez?ily.
competedfem.pl
?Gifted women were competing.? [Cz]
c. Nadana?
Giftedneut.pl
de?vc?ata
girlsneut
soute?z?ila.
competingneut.pl
?Gifted girls were competing.? [Cz]
d. Talantlivye
Giftedpl
muz?c?iny/z?ens?c?iny
men/women
sorevnovalis?.
competedpl
?Gifted men/women were competing.?[Ru]
Negation in Czech is in the majority of cases is ex-
pressed by the prefix ne-, whereas in Russian it is
very common to see a separate particle (ne) instead:
(3) a. Nic
nothing
ner?ekl.
not-said
?He didn?t say anything.? [Cz]
b. On
he
nic?ego
nothing
ne
not
skazal.
said
?He didn?t say anything.? [Ru]
In addition, reflexive verbs in Czech are formed by a
verb followed by a reflexive clitic, whereas in Rus-
sian, the reflexivization is the affixation process:
(4) a. Filip
Filip
se
REFL-CL
jes?te?
still
nehol??.
not-shaves
?Filip doesn?t shave yet.? [Cz]
b. Filip
Filip
esc?e
still
ne
not
breet+sja.
shaves+REFL.SUFFIX
?Filip doesn?t shave yet.? [Ru]
Even though auxiliaries and the copula are the forms
of the same verb byt? ?to be?, both in Russian and in
Czech, the use of this verb is different in the two
languages. For example, Russian does not use an
auxiliary to form past tense:
(5) a. Ja?
I
jsem
aux1sg
psal.
wrote
?I was writing/I wrote.? [Cz]
b. Ja
I
pisal.
wrote
?I was writing/I wrote.? [Ru]
It also does not use the present tense copula, except
for emphasis; but it uses forms of the verb byt? in
some other constructions like past passive.
We implemented a number of simple ?russifica-
tions?. The combination of random omission of the
verb byt?, omission of the reflexive clitics, and nega-
tion transformation gave us the best results on the
development corpus. Their combination improves
the overall result from 68.0% to 69.4%. We admit
we expected a larger improvement.
5.5 Sub-taggers
One of the problems when tagging with a large
tagset is data sparsity; with 1000 tags there are
10003 potential trigrams. It is very unlikely that a
naturally occurring corpus will contain all the ac-
ceptable tag combinations with sufficient frequency
to reliably distinguish them from the unacceptable
combinations. However, not all morphological at-
tributes are useful for predicting the attributes of the
succeeding word (e.g., tense is not really useful for
case). We therefore tried to train the tagger on indi-
vidual components of the full tag, in the hope that
each sub-tagger would be able to learn what it needs
for prediction. This move has the additional bene-
fit of making the tag set of each such tagger smaller
and reducing data sparsity. We focused on the first 5
positions ? POS (P), SubPOS (S), gender (g), num-
ber (n), case (c) and person (p). The selection of
the slots is based on our linguistic intuition ? for
example it is reasonable to assume that the infor-
mation about part-of-speech and the agreement fea-
tures (gnc) of previous words should help in pre-
diction of the same slots of the current word; or
information about part-of-speech, case and person
should assist in determining person. On the other
hand, the combination of tense and case is prima fa-
cie unlikely to be much use for prediction. Indeed,
most of our expectations have been met. The perfor-
mance of some of the models on the development
corpus is summarized in Table 5. The bold num-
bers indicate that the tagger outperforms the full-tag
tagger. As can be seen, the taggers trained on indi-
vidual positions are worse than the full-tag tagger
on these positions. This proves that a smaller tagset
does not necessarily imply that tagging is easier ?
see (Elworthy, 1995) for more discussion of this in-
teresting relation. Similarly, there is no improve-
ment from the combination of unrelated slots ? case
and tense (ct) or gender and negation (ga). How-
ever, the combinations of (detailed) part-of-speech
with various agreement features (e.g., Snc) outper-
form the full-tag tagger on at least some of the slots.
full-tag P S g n c
1 (P) 89.0 87.2 ? ? ? ?
2 (S) 86.6 ? 84.5 ? ? ?
3 (g) 81.4 ? ? 78.8 ? ?
4 (n) 92.4 ? ? ? 91.2 ?
5 (c) 80.9 ? ? ? ? 78.4
full-tag Pc gc ga nc cp ct
1 (P) 89.0 87.5 ? ? ? ? ?
2 (S) 86.6 ? ? ? ? ? ?
3 (g) 81.4 ? 80.4 78.7 ? ? ?
4 (n) 92.4 ? ? ? 91.8 ? ?
5 (c) 80.9 80.6 81.1 ? 81.5 79.3 79.5
8 (p) 98.3 ? ? ? ? 96.9 ?
9 (t) 97.0 ? ? ? ? ? 96.1
11 (a) 97.0 ? ? 95.4 ? ? ?
full-tag Pgc Pnc Sgc Snc Sgnc
1 (P) 89.0 87.9 87.5 ? ? ?
2 (S) 86.6 ? ? 86.1 86.4 87.1
3 (g) 81.4 80.3 ? 81.4 ? 82.7
4 (n) 92.4 ? 92.4 ? 93.0 92.8
5 (c) 80.9 81.8 81.4 80.9 82.9 82.3
Table 5: Performance of the TnT tagger trained on
various subtags (development data)
5.6 Combining Sub-taggers
We now need to put the sub-tags back together to
produce estimates of the correct full tags. We can-
not simply combine the values offered by the best
taggers for each slot, because that could yield ille-
gal tags (e.g., nouns in past tense). Instead we select
the best tag from those offered by our morphologi-
cal analyzer using the following formula:
(6) bestTag = argmaxt?TMAval(t)
TMA ? the set of tags offered by MA
val(t) =?14k=0 Nk(t)/Nk
Nk(t) ? # of taggers voting for k-th slot of t
Nk ? the total # of taggers on slot k
That means, that the best tag is the tag that received
the highest average percentage of votes for each of
full-tag all best 1 best 3
overall 69.5 70.3 70.7 71.1
1 (P) 89.0 88.9 89.1 89.2
2 (S) 86.6 86.5 86.9 86.9
3 (g) 81.4 81.8 83.0 83.2
4 (n) 92.4 92.6 93.1 93.2
5 (c) 80.9 82.1 83.0 83.2
6 (G) 98.5 98.5 98.7 98.7
7 (N) 99.6 99.7 99.8 99.8
8 (p) 98.3 98.2 98.4 98.3
9 (t) 97.0 97.0 97.0 97.0
10 (G) 96.0 96.0 96.0 96.0
11 (a) 97.0 97.0 96.9 97.0
12 (v) 97.4 97.3 97.5 97.4
15 (V) 99.1 99.1 99.0 99.0
Table 6: Combining sub-taggers (development data)
Baseline Direct Russified Russified
Tagger random full-tag full-tag voting
Accuracy
Tags 33.6 69.4 72.6 73.5
1 (POS) 63.2 88.5 90.1 90.4
2 (SubPOS) 57.0 86.8 88.1 88.6
3 (Gender) 59.2 82.5 84.5 85.0
4 (Number) 75.9 91.2 92.6 93.4
5 (Case) 47.3 80.4 84.1 85.3
6 (PossGen) 83.4 98.4 98.8 99.0
7 (PossNr) 99.6 99.6 99.6 99.8
8 (Person) 97.1 99.3 98.9 98.9
9 (Tense) 86.6 96.5 97.6 97.6
10 (Grade) 90.1 95.9 96.6 96.6
11 (Neg) 81.4 95.3 95.5 95.5
12 (Voice) 86.4 97.2 97.9 97.9
15 (Variant) 97.0 99.1 99.5 99.5
Table 7: Tagging with various parameters (test data)
its slots. If we cared about certain slots more than
about others we could weight the slots in the val
function.
We ran several experiments, the results of three of
them are summarized in Table 6. All of them work
better than the full-tag tagger. One (?all?) uses all
available subtaggers, other (?best 1?) uses the best
tagger for each slot (therefore voting in Formula 6
reduces to finding a closest legal tag). The best re-
sult is obtained by the third tagger (?best 3?) which
uses the three best taggers for each of the Pgcp slots
and the best tagger for the rest. We selected this tag-
ger to tag the test corpus, for which the results are
summarized in Table 7.
Russian Gloss Correct Xerox Ours
?Clen member noun nom gen
partii party noun gen obl
po prep prep obl acc
vozmoz?nosti possibility noun obl acc
staralsja tried vfin
nje not ptcl
govorit? to-speak vinf
ni nor ptcl
o about prep obl
Bratstvje Brotherhood noun obl
, cm
ni nor ptcl
o about prep obl
knigje book noun obl
Errors 3 1
?Neither the Brotherhood nor the book was a subject
that any ordinary Party member would mention if
there was a way of avoiding it.? [Orwell: ?1984?]
Table 8: Tagging with Xerox & our tagger
5.7 Comparison with Xerox tagger
A tagger for Russian is part of the Xerox language
tools. We could not perform a detailed evaluation
since the tool is not freely available. We used the
online demo version of Xerox?s Disambiguator6 to
tag a few sentences and compared the results with
the results of our tagger. The Xerox tagset is much
smaller than ours, it uses 63 tags, collapsing some
cases, not distinguishing gender, number, person,
tense etc. (However, it uses different tags for dif-
ferent punctuation, while we have one tag for all
punctuation). For the comparison, we translated our
tagset to theirs. On 201 tokens of the testing cor-
pus, the Xerox tagger achieved an accuracy of 82%,
while our tagger obtained 88%; i.e., a 33% reduc-
tion in error rate. A sample analysis is in Table 8.
5.8 Comparison with Czech taggers
The numbers we obtain are significantly worse than
the numbers reported for Czech (Hajic? et al, 2001)
(95.16% accuracy); however, they use an extensive
manually created morphological lexicon (200K+
entries) which gives 100.0% recall on their testing
data. Moreover, they train and test their taggers on
the same language.
6 Ongoing Research
We are currently working on improving both the
morphological analysis and tagging. We would like
6http://www.xrce.xerox.com/
competencies/content-analysis/demos/
russian
to improve the recall of filters following morpholog-
ical analysis, e.g., using n maximal values instead
of 1, using some basic knowledge of derivational
morphology, etc. We are incorporating phonological
conditions on stems into the guesser module as well
as trying to deal with different morphological phe-
nomena specific to Russian, e.g., verb reflexiviza-
tion. However, we try to stay language independent
(at least within Slavic languages) as much as possi-
ble and limit the language dependent components to
a minimum.
Currently, we are working on more sophisticated
russifications that would be still easily portable to
other languages. For example, instead of omitting
auxiliaries randomly, we want to use the syntac-
tic information present in Prague Dependency Tree-
bank to omit only the ?right? ones.
If possible, we would like to avoid entirely throw-
ing away the Czech emission probabilities, because
our intuition tells us that there are useful lexical
similarities between Russian and Czech, and that
some suitable process of cognate detection will al-
low us to transfer information from the Czech to
the Russian emission probabilities. Just as a knowl-
edge of English words is sometimes helpful (mod-
ulo sound changes) when reading German, a knowl-
edge of the Czech lexicon should be helpful (mod-
ulo character set issues) when reading Russian. We
are seeking the right way to operationalize this in-
tuition in our system, bearing in mind that we want
a sufficiently general algorithm to make the method
portable to other languages, for which we assume
we have neither the time nor the expertise to under-
take knowledge-intensive work. A potentially suit-
able cognate algorithm is described by (Kondrak,
2001).
Finally, we would like to extend our work to Slavic
languages for which there are even fewer available
resources than Russian, such as Belarusian, since
this was the original motivation for undertaking the
work in the first place.
Acknowledgements
We thank Erhard Hinrichs and Eric Fosler-Lussier
for giving us feedback on previous versions of the
paper and providing useful suggestions for subtag-
gers and voting; Jan Hajic? for the help with the
Czech tag system and the morphological analyzer;
to the Clippers discussion group for allowing us to
interview ourselves in front of them, and for ensuing
discussion, and to two anonymous EMNLP review-
ers for extremely constructive feedback.
References
Alena Be?mova?, Jan Hajic?, Barbora Hladka?, and
Jarmila Panevova?. 1999. Morphological and syn-
tactic tagging of the prague dependency treebank.
In Proceedings of ATALA Workshop, pages 21?29.
Paris, France.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP-NAACL,
pages 224?231.
Sas?o Dz?eroski, Tomaz? Erjavec, and Jakub
Zavrel. 2000. Morphosyntactic Tagging of
Slovene:Evaluating Taggers and Tagsets. In Pro-
ceedings of the Second International Conference
on Language Resources and Evaluation, pages
1099?1104.
David Elworthy. 1995. Tagset design and inflected
languages. In EACL SIGDAT workshop ?From
Texts to Tags: Issues in Multilingual Language
Analysis?, pages 1?10, Dublin, April.
John Goldsmith. 2001. Unsupervised Learning of
the Morphology of a Natural Language. Computa-
tional Linguistics, 27(2):153?198.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva,
and Vladim??r Petkevic?. 2001. Serial Combination
of Rules and Statistics: A Case Study in Czech Tag-
ging. In Proceedings of ACL Conference, Toulouse,
France.
Jan Hajic?. 2000. Morphological Tagging: Data
vs. Dictionaries. In Proceedings of ANLP-NAACL
Conference, pages 94?101, Seattle, Washington,
USA.
Jiri Hana and Anna Feldman. to appear. Portable
Language Technology: The case of Czech and Rus-
sian. In Proceedings from the Midwest Computa-
tional Linguistics Colloquium, June 25-26, 2004,
Bloomington, Indiana.
Greg Kondrak. 2001. Identifying cognates by pho-
netic and semantic similarity. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-2001), pages 103?110, June.
Andrey Kovalev. 2002. A Probabilistic Mor-
phological Analyzer for Russian and Ukranian.
http://linguist.nm.ru/stemka/stemka.html.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Andrei Mikheev and Liubov Liubushkina. 1995.
Russian Morphology: An Engineering Approach.
Natural Language Engineering, 3(1):235?260.
Ilya Segalovich and Michail Maslov. 1989.
Dictionary-based Russian morphological analy-
sis and synthesis with generation of morpho-
logical models of unknown words (in Russian).
http://company.yandex.ru/articles/article1.html.
Ilya Segalovich and Vitaly Titov. 2000. Au-
tomatic morphological annotation MYSTEM.
http://corpora.narod.ru/article.html.
Ilya Segalovich. 2003. A fast morpholog-
ical algorithm with unknown word guessing
induced by a dictionary for a web search
engine. http://company.yandex.ru/articles/iseg-las-
vegas.html.
Jean Ve?ronis. 1996. MULTEXT-
EAST (Copernicus 106).
http://www.lpl.univaix.fr/projects/multext-east.
Terence Wade. 1992. A Comprehensive Russian
Grammar. Blackwell. 582 pp.
Serge A. Yablonsky. 1999. Russian Morphological
Analysis. In Proceedings VEXTAL.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Meeting of the Association for Computational Lin-
guistics, pages 207?216.
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 15?22,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
?Language and Computers?
Creating an Introduction for a General Undergraduate Audience
Chris Brew
Department of Linguistics
The Ohio State University
cbrew@ling.osu.edu
Markus Dickinson
Department of Linguistics
The Ohio State University
dickinso@ling.osu.edu
W. Detmar Meurers
Department of Linguistics
The Ohio State University
dm@ling.osu.edu
Abstract
This paper describes the creation
of Language and Computers, a new
course at the Ohio State University de-
signed to be a broad overview of topics
in computational linguistics, focusing
on applications which have the most
immediate relevance to students. This
course satisfies the mathematical and
logical analysis requirement at Ohio
State by using natural language sys-
tems to motivate students to exercise
and develop a range of basic skills in
formal and computational analysis. In
this paper we discuss the design of the
course, focusing on the success we have
had in offering it, as well as some of the
difficulties we have faced.
1 Introduction
In the autumn of 2003, we created Language
and Computers (Linguistics 384), a new course
at the Ohio State University that is designed to
be a broad overview of topics in computational
linguistics, focusing on applications which have
the most immediate relevance to students. Lan-
guage and Computers is a general enrollment
course designed to meet the Mathematical and
Logical Analysis requirement that is mandated
for all undergraduates at the Ohio State Uni-
versity (OSU), one of the largest universities in
the US. We are committed to serving the av-
erage undergraduate student at OSU, including
those for whom this is the first and last Lin-
guistics course. Some of the students take the
course because it is an alternative to calculus,
others because of curiosity about the subject
matter. The course was first taught in Win-
ter 2004, drawing a wide range of majors, and
has since expanded to three sections of up to 35
students each. In this paper we will discuss the
design of the course, focusing on the success we
have had in offering it, as well as some of the
difficulties we have faced.
2 General Context
The Linguistics Department at OSU is the home
of a leading graduate program in which 17 grad-
uate students are currently specializing in com-
putational linguistics. From the perspective of
the graduate program, the goal of the new course
development was to create more appropriate
teaching opportunities for the graduate students
specializing in computational linguistics. Much
of the undergraduate teaching load in Linguis-
tics at OSU is borne by graduate teaching assis-
tants (GTAs) who receive stipends directly from
the department. After a training course in the
first year, most such GTAs act as instructors on
the Department?s ?Introduction to Language,?
which is taught in multiple small sections. In-
structors are given considerable responsibility
for all aspects of course design, preparation, de-
livery, and grading. This works very well and
produces many superb instructors, but by 2003
it was apparent that increasing competition was
reducing the pool of undergraduates who want
to take this general overview course.
The Ohio State University has a distribution
requirement, the General Education Curricu-
15
lum (GEC), that is designed to ensure adequate
breadth in undergraduate education. The twin
demands of the student?s major and the distri-
bution requirement are sufficient to take up the
vast majority of the credit hours required for
graduation. In practice this means that students
tend to make course selections motivated pri-
marily by the goal of completing the necessary
requirements as quickly and efficiently as they
can, possibly at the expense of curiosity-driven
exploration. Linguistics, as an interdisciplnary
subject, can create courses that satisfy both cu-
riosity and GEC requirements.
To fill this interdisciplinary niche, the OSU
Department of Linguistics has created a range
of new courses such as Language and Gender,
Language and the Mind, Language and the Law,
and the Language and Computers course dis-
cussed in this paper. In addition to filling a dis-
tribution requirement niche for undergraduates,
the courses also allow the linguistics GTAs to
teach courses on topics that are related to their
area of specialization, which can be beneficial
both to the instructors and to those instructed.
Prior to creation of the new Language and Com-
puters course, there were virtually no opportu-
nities for student members of the computational
linguistics group to teach material close to their
focus.
3 Course overview
The mission statement for our course reads:
In the past decade, the widening use
of computers has had a profound influ-
ence on the way ordinary people com-
municate, search and store informa-
tion. For the overwhelming majority
of people and situations, the natural
vehicle for such information is natu-
ral language. Text and to a lesser ex-
tent speech are crucial encoding for-
mats for the information revolution.
This course will give students insight
into the fundamentals of how comput-
ers are used to represent, process and
organize textual and spoken informa-
tion, as well as providing tips on how
to effectively integrate this knowledge
into their working practice. The course
will cover the theory and practice of
human language technology.
The course was designed to meet the Math-
ematical and Logical Analysis (MLA) require-
ment for students at the Ohio State University,
which is characterized in the following way:
A student in a B.A. program must take
one course that focuses on argument in
a context that emphasizes natural lan-
guage, mathematics, computer science
or quantitative applications not pri-
marily involving data. Courses which
emphasize the nature of correct argu-
mentation either in natural languages
or in symbolic form would satisfy this
requirement, as would many mathe-
matics or computer science courses.
. . . The courses themselves should em-
phasize the logical processes involved
in mathematics, inductive or deductive
reasoning, or computing and the the-
ory of algorithms.
Linguistics 384 responds to this specification
by using natural language systems to motivate
students to exercise and develop a range of ba-
sic skills in formal and computational analysis.
The course combines lectures with group work
and in-class discussions, resulting in a seminar-
like environment. We enrol no more than 35
students per section, often significantly fewer at
unpopular times of day.
The course philosophy is to ground abstract
concepts in real world examples. We intro-
duce strings, regular expressions, finite-state
and context-free grammars, as well as algo-
rithms defined over these structures and tech-
niques for probing and evaluating systems that
rely on these algorithms. This meets the MLA
objective to emphasize the nature of correct ar-
gumentation in symbolic form as well as the logi-
cal processes involved in computing and the the-
ory of algorithms. These abstract ideas are em-
bedded in practical applications: web searching,
16
spelling correction, machine translation and di-
alogue systems. By covering the technologies
behind these applications, the course addresses
the requirement to sharpen a student?s ability
to reason critically, construct valid arguments,
think creatively, analyze objectively, assess ev-
idence, perceive tacit assumptions, and weigh
evidence.
Students have impressions about the quality
of such systems, but the course goes beyond
merely subjective evaluation of systems and em-
phasizes the use of formal reasoning to draw and
argue for valid conclusions about the design, ca-
pabilities and behavior of natural language sys-
tems.
In ten weeks, we cover eight topics, using a
data projector in class, with copies of the slides
being handed out to the student before each
class. There is no textbook, and there are rel-
atively few assigned readings, as we have been
unable to locate materials appropriate for an av-
erage student without required background who
may never take another (computational) linguis-
tics class. The topics covered are the following,
in this order:
? Text and speech encoding
? (Web-)Searching
? Spam filtering (and other classification
tasks, such as language identification)
? Writers? aids (Spelling and grammar correc-
tion)
? Machine translation (2 weeks)
? Dialogue systems (2 weeks)
? Computer-aided language learning
? Social context of language technology use
In contrast to the courses of which we are
aware that offer computational linguistics to un-
dergraduates, our Language and Computers is
supposed to be accessible without prerequisites
to students from every major (a requirement for
GEC courses). For example, we cannot assume
any linguistic background or language aware-
ness. Like Lillian Lee?s Cornell course (Lee,
2002), the course cannot presume programming
ability. But the GEC regulations additionally
prohibit us from requiring anything beyond high
school level abilities in algebraic manipulation.
We initially hoped that this meant that we
would be able to rely on the kind of math knowl-
edge that we ourselves acquired in secondary
school, but soon found that this was not real-
istic. The sample questions from Lee?s course
seem to us to be designed for students who ac-
tively enjoy math. Our goal is different: we
want to exercise and extend the math skills of
the general student population, ensuring that
the course is as accessible to the well-motivated
dance major as it is to the geekier people with
whom we are somewhat more familiar. This is
hard, but worthwhile.
The primary emphasis is on discrete math-
ematics, especially with regard to strings and
grammars. In addition, the text classification
and spam-filtering component exercise the abil-
ity to reason clearly using probabilities. All of
this can be achieved for students with no colle-
giate background in mathematics.
Specifically, Linguistics 384 uses non-trivial
mathematics at a level at or just beyond algebra
1 in the following contexts:
? Reasoning about finite-state automata and
regular expressions (in the contexts of web
searching and of information management).
Students reason about relationships be-
tween specific and general search terms.
? Reasoning about more elaborate syntactic
representations (such as context-free gram-
mars) and semantic representations (such
as predicate calculus), in order to better
understand grammar checking and machine
translation errors.
? Reasoning about the interaction between
components of natural language systems (in
the contexts of machine translation and of
dialog systems).
? Understanding the basics of dynamic pro-
gramming via spelling correction (edit dis-
tance) and applying algebraic thinking to
algorithm design.
17
? Simple probabilistic reasoning (in the con-
text of text classification).
There is also an Honors version of the course,
which is draws on a somewhat different pool
of students. In 2004 the participants in Hon-
ors 384 were equally split between Linguistics
majors looking for a challenging course, people
with a computer background and some interest
in language and people for whom the course was
a good way of meeting the math requirement at
Honors level. Most were seniors, so there was lit-
tle feed-through to further Linguistics courses.
The Honors course, which used to be
called Language Processing Technology, pre-
dates Language and Computers, and includes
more hands-on material. Originally the first half
of this course was an introduction to phonetics
and speech acoustics through Praat, while the
second was a Prolog-based introduction to sym-
bolic NLP. We took the opportunity to redesign
this course when we created the non-honors ver-
sion. In the current regime, the hands-on aspect
is less important than the opportunities offered
by the extra motivation and ability of these stu-
dents. Two reading assignments in the honors
version were Malcolm Gladwell?s book review on
the Social Life of Paper (Gladwell, 2001) and
Turing?s famous paper on the Imitation Game
(Turing, 1950). We wondered whether the ec-
centricity and dated language of the latter would
be a problem, but it was not.
Practical assignments in the laboratory are
possible in the honors course, because the class
size can be limited. One such assignment was
a straightforward run-through of the clock tu-
torial from the Festival speech synthesis system
and another a little machine translation system
between digits and number expressions. Having
established that they can make a system that
turns 24 into ?twenty four?, and so on, the stu-
dents are challenged to adapt it to speak ?Fairy
Tale English?: that is, to make it translate 24
into ?four and twenty?, and vice-versa.
1
1For a complete overview of the course materials,
there are several course webpages to check out. The web-
page for the first section of the course (Winter 2004)
4 General themes of the course
Across the eight different topics that are taught,
we try to maintain a cohesive feel by emphasiz-
ing and repeating different themes in computa-
tional linguistics. Each theme allows the stu-
dents to see that certain abstract ideas are quite
powerful and can inform different concrete tasks.
The themes which have been emphasized to this
point are as follows:
? There are both statistical and rule-based
methods for approaching a problem in nat-
ural language processing. We show this
most clearly in the spam filtering unit and
the machine translation unit with different
types of systems.
? There is a tension between developing tech-
nology in linguistically-informed ways and
developing technology so that a product is
effective. In the context of dialogue sys-
tems, for example, the lack of any linguistic
knowledge in ELIZA makes it fail quickly,
but an ELIZA with a larger database and
still no true linguistic knowledge could have
more success.
? Certain general techniques, such as n-gram
analysis, can be applied to different compu-
tational linguistic applications.
? Effective technology does not have to solve
every problem; focusing on a limited do-
main is typically more practical for the ap-
plications we look at. In machine transla-
tion, this means that a machine translation
system translating the weather (e.g., the
METEO system) will perform better than
a general-purpose system.
? Intelligent things are being done to improve
natural language technology, but the task is
a very difficult one, due to the complexities
of language. Part of each unit is devoted to
is at http://ling.osu.edu/~dickinso/384/wi04/. A
more recent section (Winter 2005) can be found at http:
//ling.osu.edu/~dm/05/winter/384/. For the honors
course, the most recent version is located at http:
//ling.osu.edu/~cbrew/2005/spring/H384/. A list of
weblinks to demos, software, and on-line tutorials cur-
rently used in connection with the course can be found
at http://ling.osu.edu/~xflu/384/384links.html
18
showing that the problem the technology is
addressing is a complex one.
5 Aspects of the course that work
The course has been a positive experience, and
students overall seemed pleased with it. This
is based on the official student evaluation of
instruction, anonymous, class specific question-
naires we handed out at the end of the class,
personal feedback, and new students enrolling
based on recommendations from students who
took the course. We attribute the positive re-
sponse to several different aspects of the course.
5.1 Topics they could relate to
Students seem to most enjoy those topics which
were most relevant to their everyday life. On the
technological end, this means that the units on
spam filtering, web searching, and spell check-
ing are generally the most well-received. The
more practical the focus, the more they seem
to appreciate it; for web searching, for instance,
they tend to express interest in becoming better
users of the web. On the linguistic end, discus-
sions of how dialogue works and how language
learning takes place, as part of the units on di-
alogue systems and CALL, respectively, tend to
resonate with many students. These topics are
only sketched out insofar as they were relevant
to the NLP technology in question, but this has
the advantage of not being too repetitive for the
few students who have had an introductory lin-
guistics class before.
5.2 Math they can understand
Students also seem to take pride in being able
to solve what originally appear to be difficult
mathematical concepts. To many, the concept
and look of a binary number is alien, but they
consistently find this to be fairly simple. The
basics of finite-state automata and boolean ex-
pressions (even quite complicated expressions)
provide opportunities for students to understand
that they are capable of learning concepts of log-
ical thinking. Students with more interest and
more of an enjoyment for math are encouraged
to go beyond the material and, e.g., figure out
the nature of more complicated finite-state au-
tomata. In this way, more advanced students are
able to stay interested without losing the other
students.
More difficult topics, such as calculating the
minimum edit distance between a word and its
misspelling via dynamic programming, can be
frustrating, but they just as often are a source
of a greater feeling of success for students. After
some in-class exercises, when it becomes appar-
ent that the material is learnable and that there
is a clear, well-motivated point to it, students
generally seem pleased in conquering somewhat
more difficult mathematical concepts.
5.3 Interactive demos
In-class demos of particular software are also
usually well-received, in particular when they
present applications that students themselves
can use. These demos often focus on the end
result of a product, such as simply listening to
the output of several text-to-speech synthesiz-
ers, but they can also be used for understanding
how the applications works. For example, some
sections attempt to figure out as a class where
a spelling checker fails and why. Likewise, an
in-class discussion with ELIZA has been fairly
popular, and students are able to deduce many
of the internal properties of ELIZA.
5.4 Fun materials
In many ways, we have tried to keep the tone
of the course fairly light. Even though we
are teaching mathematical and logical concepts,
these concepts are still connected to the real
world, and as such, there is much opportunity
to present the material in a fun and engaging
manner.
Group work One such way to make the learn-
ing process more enjoyable was to use group
work. In the past few quarters, we have been
refining these exercises. Because of the nature
of the topics, some topics are easier to derive
group exercises for than others. The more math-
ematical topics, such as regular expressions, suit
themselves well for straightforward group work
on problem sets in class; others can be more
19
creative. The group exercises usually serve as a
way for students to think about issues they al-
ready know something about, often as a way to
introduce the topic.
For example, on the first day, they are given
a sheet and asked to evaluate sets of opposing
claims, giving arguments for both sides, such as
the following:
1. A person will have better-quality papers if
they use a spell checker.
A person will have worse-quality papers if
they use a spell checker.
2. An English-German dictionary is the main
component needed to automatically trans-
late from English to German.
An English-German dictionary is not the
main component needed to automatically
translate from English to German.
3. Computers can make you sound like a na-
tive speaker of another language.
Computers cannot make you sound like a
native speaker of another language.
To take another example, to get students
thinking about the social aspects of the use of
language technology, they are asked in groups to
consider some of the implications of a particu-
lar technology. The following is an excerpt from
one such handout.
You work for a large software company
and are in charge of a team of com-
putational linguists. One day, you are
told: ?We?d like you and your team to
develop a spell checker for us. Do you
have any questions?? What questions
do you have for your boss?
...
Somehow or another, the details of
your spell checker have been leaked to
the public. This wouldn?t be too bad,
except that it?s really ticked some lin-
guists off. ?It?s just a big dictionary!?
they yell. ?It?s like you didn?t know
anything about morphology or syntax
or any of that good stuff.? There?s
a rumor that they might sue you for
defamation of linguistics. What do you
do?
Although the premise is somewhat ridiculous,
with such group work, students are able to con-
sider important topics in a relaxed setting. In
this case, they have to first consider the speci-
fications needed for a technology to work (who
will be using it, what the expectations are, etc.)
and, secondly, what the relationship is between
the study of language and designing a product
which is functional.
Fun homework questions In the home-
works, students are often instructed to use a
technology on the internet, or in some way to
take the material presented in class a step far-
ther. Additionally, most homework assignments
had at least one lighter question which allowed
students to be more creative in their responses
while at the same time reinforcing the material.
For example, instructors have asked students
to send them spam, and the most spam-worthy
message won a prize. Other homework ques-
tions have included sketching out what it would
take to convert an ELIZA system into a hostage
negotiator?and what the potential dangers are
in such a use. Although some students put down
minimal answers, many students offer pages of
detailed suggestions to answer such a question.
This gives students a taste of the creativity in-
volved in designing new technology without hav-
ing to deal with the technicalities.
6 Challenges for the course
Despite the positive response, there are several
aspects to the course which have needed im-
provement and continue to do so. Teaching
to a diverse audience of interests and capabili-
ties presents obstacles which are not easily over-
come. To that end, here we will review aspects
of the course which students did not generally
enjoy and which we are in the process of adapt-
ing to better suit our purposes and our students?
needs.
20
6.1 Topics they do not relate to
For such a range of students, there is the diffi-
culty of presenting abstract concepts. Although
we try to relate everything to something which
students actually use or could readily use, we
sometimes include topics from computational
linguistics that make one better able to think
logically in general and which we feel will be
of future use for our students. One such topic
is that of regular expressions, in the context of
searching for text in a document or corpus. As
most students only experience searching as part
of what they do on the web, and no web search
engine (to the best of our knowledge) currently
supports regular expression searching, students
often wonder what the point of the topic is. In
making most topics applicable to everyday life,
we had raised expect. In this particular case,
students seemed to accept regular expressions
more once it they saw that Microsoft Word has
something roughly analogous.
Another difficulty that presented itself for a
subset of the students was that of using for-
eign language text to assist in teaching ma-
chine translation and computer-aided language
learning. Every example was provided with an
English word-by-word gloss, as well as a para-
phrase, yet the examples can still be difficult to
understand without a basic appreciation for the
relevant languages. If the students know Span-
ish, the example is in Spanish and the instruc-
tor has a decent Spanish accent, things can go
well. But students tend to blame difficulties in
the machine translation homework on not know-
ing the languages used in the examples. Under-
standing the distinction between different kinds
of machine translation systems requires some
ability to grasp how languages can differ, so we
certainly must (unless we use proxies like fairy-
tale English) present some foreign material, but
we are in dire need of means to do this as gently
as possible
6.2 Math they do not understand
While some of the more difficult mathemati-
cal concepts were eventually understood, oth-
ers continued to frustrate students. The al-
ready mentioned regular expressions, for exam-
ple, caused trouble. Firstly, even if you do
understand them, they are not necessarily life-
enhancing, unless you are geeky enough to write
your papers in a text editor that properly sup-
ports them. Secondly, and more importantly,
many students saw them as unnecessarily ab-
stract and complex. For instance, some stu-
dents were simply unable to understand the no-
tion that the Kleene star is to be interpreted as
an operator rather than as a special character
occurring in place of any string.
Even though we thought we had calibrated
our expectations to respect the fact that our
students knew no math beyond high school, the
amount that they had retained from high school
was often less than we expected. For exam-
ple, many students behaved exactly as if they
had never seen Venn diagrams before, so time
had to be taken away from the main material
in order to explain them. Likewise, figuring
out how to calculate probabilities for a bag of
words model of statistical machine translation
required a step-by-step explanation of where
each number comes from. A midterm ques-
tion on Bayesian spam filtering needed the same
treatment, revealing that even good students
may have significant difficulties in deploying the
high school math knowledge they almost cer-
tainly possess.
6.3 Technology which did not work
Most assignments required students to use the
internet or the phone in some capacity, usu-
ally to try out a demo. With such tasks, there
is always the danger that the technology will
not work. For example, during the first quar-
ter the course was taught, students were asked
to call the CMU Communicator system and in-
teract with it, to get a feel for what it is like
to interact with a computer. As it turns out,
halfway through the week the assignment was
due, the system was down, and thus some stu-
dents could not finish the exercise. Follow-
ing this episode, homework questions now come
with alternate questions. In this case, if the sys-
tem is down, the first alternate is to listen to a
pre-recorded conversation to see how the Com-
21
municator works. Since some students are un-
able to listen to sounds in the campus computer
labs, the second alternate is to read a transcript.
Likewise, students were instructed to view the
page source code for ELIZA. However, some
campus computer labs at OSU do not allow stu-
dents to view the source of a webpage. In re-
sponse to this, current versions of the assign-
ment have a separate webpage with the source
code written out as plain text, so all students
can view it.
One final note is that students have often com-
plained of weblinks failing to work, but this ?fail-
ure? is most often due to students mistyping
the link provided in the homework. Providing
links directly on the course webpage or including
them in the web- or pdf-versions of the home-
work sheets is the simplest solution for this prob-
lem.
7 Summary and Outlook
We have described the course Language and
Computers (Linguistics 384), a general introduc-
tion to computational linguistics currently being
taught at OSU. While there are clear lessons
to be learned for developing similar courses at
other universities, there are also more general
points to be made. In courses which assume
some CS background, for instance, it is still
likely the case that students will want to see
some practical use of what they are doing and
learning.
There are several ways in which this course
can continue to be improved. The most pressing
priority is to develop a course packet and pos-
sibly a textbook. Right now, students rely only
on the instructor?s handouts, and we would like
to provide a more in-depth and cohesive source
of material. Along with this, we want to de-
velop a wider range of readings for students (e.g.
Dickinson, to appear) to provide students with
a wider variety of perspectives and explanations
for difficult concepts.
To address the wide range of interests and ca-
pabilities of the students taking this course as a
general education requirement, it would be good
to tailor some of the sections to audiences with
specific backgrounds?but given the lack of a
dedicated free time slot for all students of a par-
ticular major, etc., it is unclear whether this is
feasible in practice.
We are doing reasonably well in integrating
mathematical thinking into the course, but we
would like to give students more experience of
thinking about algorithms. Introducing a ba-
sic form of pseudocode might go some way to-
wards achieving this, provided we can find a mo-
tivating linguistic example that is both simple
enough to grasp and complex enough to justify
the overhead of introducing a new topic. Fur-
ther developments might assist us in developing
a course between Linguistics 384 and Linguistics
684, our graduate-level computational linguis-
tics course, as we currently have few options for
advanced undergraduates.
Acknowledgements We would like to thank
the instructors of Language and Computers for
their discussions and insights into making it a
better course: Stacey Bailey, Anna Feldman, Xi-
aofei Lu, Crystal Nakatsu, and Jihyun Park. We
are also grateful to the two ACL-TNLP review-
ers for their detailed and helpful comments.
References
Markus Dickinson, to appear. Writers? Aids. In
Keith Brown (ed.), Encyclopedia of Language
and Linguistics. Second Edition, Elsevier, Ox-
ford.
Malcolm Gladwell, 2001. The Social Life
of Paper. New Yorker . available from
http://www.gladwell.com/archive.html.
Lillian Lee, 2002. A non-programming introduc-
tion to computer science via NLP, IR, and
AI. In ACL Workshop on Effective Tools
and Methodologies for Teaching Natural Lan-
guage Processing and Computational Linguis-
tics. pp. 32?37.
A.M. Turing, 1950. Computing Machinery and
Intelligence. Mind , 59(236):433?460.
22
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 204?205,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Robust Extraction of Subcategorization Data from Spoken Language 
 
Jianguo Li & Chris Brew Eric Fosler-Lussier 
Department of Linguistics Department of Computer Science & Engineering 
The Ohio State University, USA The Ohio State University, USA 
{jianguo|cbrew}@ling.ohio-state.edu fosler@cse.ohio-state.edu 
 
 
 
 
1 Introduction 
Subcategorization data has been crucial for various 
NLP tasks. Current method for automatic SCF ac-
quisition usually proceeds in two steps: first, gen-
erate all SCF cues from a corpus using a parser, 
and then filter out spurious SCF cues with statisti-
cal tests. Previous studies on SCF acquisition have 
worked mainly with written texts; spoken corpora 
have received little attention. Transcripts of spoken 
language pose two challenges absent in written 
texts: uncertainty about utterance segmentation and 
disfluency. 
     Roland & Jurafsky (1998) suggest that there are 
substantial subcategorization differences between 
spoken and written corpora. For example, spoken 
corpora tend to have fewer passive sentences but 
many more zero-anaphora structures than written 
corpora. In light of such subcategorization differ-
ences, we believe that an SCF set built from spo-
ken language may, if of acceptable quality, be of 
particular value to NLP tasks involving syntactic 
analysis of spoken language.  
2 SCF Acquisition System  
Following the design proposed by Briscoe and 
Carroll (1997), we built an SCF acquisition system 
consisting of the following four components: 
Charniak?s parser (Charniak, 2000); an SCF ex-
tractor; a lemmatizer; and an SCF evaluator. The 
first three components are responsible for generat-
ing SCF cues from the training corpora and the last 
component, consisting of the Binomial Hypothesis 
Test (Brent, 1993) and a back-off algorithm 
(Sarkar & Zeman, 2000), is used to filter SCF cues 
on the basis of their reliability and likelihood.  
We evaluated our system on a million word 
written corpus and a comparable spoken corpus 
from BNC.  For type precision and recall, we used 
14 verbs selected by Briscoe & Carroll (1997) and 
evaluated our results against SCF entries in 
COMLEX (Grishman et al, 1994). We also calcu-
lated token recall and the results are summarized in 
the following table. 
Corpus Written Spoken 
type precision 93.1% 91.2% 
type recall 48.2% 46.4% 
token recall 82.3% 80% 
Table 1: Type precision, recall and token recall 
3 Detecting Incorrect SCF Cues 
We examined the way segmentation errors and 
disfluency affects our acquisition system ? the sta-
tistical parser and the extractor in particular ? in 
proposing SCF cues and explored ways to detect 
incorrect SCF cues. We extracted 500 SCF cues 
from the ViC corpus (Pitt, et al 2005) and identi-
fied four major reasons that seem to have caused 
the extractor to propose incorrect SCF cues: multi-
ple utterances; missing punctuation; disfluency; 
parsing errors.  
Error analysis reveals that segmentation errors 
and disfluencies cause the parser and the extractor 
to tend to make systematic errors in proposing SCF 
cues ? incorrect SCF cues are likely to have an 
extra complement. We therefore proposed the fol-
lowing two sets of linguistic heuristics for auto-
matically detecting incorrect SCF cues: 
Linguistic Heuristic Set 1: The following SCF 
cues are extremely unlikely whatever the verb. Re-
ject an SCF cue as incorrect if it contains the fol-
lowing patterns: 
? [(NP) PP NP]: We reach out [to your friends] [your 
neighbor]. 
? [NP PP-to S]: Would I want them to say [that][to 
me] [would I want them to do that to me]. 
? [NP NP S]: They just beat [Indiana in basketball] 
[the- Saturday] [I think it was um-hum]. 
204
? [PP-p PP-p]: He starts living [with the] [with the 
guys]. 
Linguistic Heuristic Set 2: The following SCF 
cues are all possibly valid SCFs: for SCF cues of 
the following type, check if the given verb takes it 
in COMLEX. If not, reject it: 
? [(NP) S]: When he was dying [what did he say]. 
? [PP-to S]: The same thing happened [to him] [uh 
he had a scholarship]. 
? [(NP) NP]: OU had a heck of time beating [them] 
[uh-hum]. 
? [(NP) INF]: You take [the plate] from the table 
[rinse them off] and put them by the sink. 
Given the utilization of a gold standard in the 
heuristics, it would be improper to build an end-to-
end system and evaluate against COMLEX. In-
stead, we evaluate by seeing how often our heuris-
tics succeed producing results agreeable to a 
human judge. 
To evaluate the robustness of our linguistic heu-
ristics, we conducted a cross-corpora and cross-
parser comparison. We used 1,169 verb tokens 
from the ViC corpus and another 1,169 from the 
Switchboard corpus. 
Cross-corpus Comparison: The purpose of the 
cross-corpus comparison is to show that our lin-
guistic heuristics based on the data from one spo-
ken corpus can be applied to other spoken corpora. 
Therefore, we applied our heuristics to the ViC and 
the Switchboard corpus parsed by Charniak?s 
parser. We calculated the percentage of incorrect 
SCF cues before and after applying our linguistic 
heuristics. The results are shown in Table 2.  
Charniak?s parser ViC Switchboard 
before heuristics 18.8% 9.5% 
after heuristics 6.4% 4.6% 
Table 2: Incorrect SCF cue rate before and after heuristics 
 
Table 2 shows that the incorrect SCF cue rate 
has been reduced to roughly the same level for the 
two spoken corpora after applying our linguistic 
heuristics. 
Cross-parser Comparison: The purpose of the 
cross-parser comparison is to show that our lin-
guistic heuristics based on the data parsed by one 
parser can be applied to other parsers as well. To 
this end, we applied our heuristics to the 
Switchboard corpus parsed by both Charniak?s 
parser and Bikel?s parsing engine (Bikel, 2004). 
Again, we calculated the percentage of incorrect 
SCF cues before and after applying our heuristics. 
The results are displayed in Table 3. 
Although our linguistic heuristics works slightly 
better for data parsed by Charniak? parser, the in-
correct SCF cue rate after applying heuristics re-
mains at about the same level for the two different 
parsers we used. 
Switchboard Charniak Bikel 
before heuristics 9.5% 9.2% 
after heuristics 4.6% 5.4% 
Table 3: Incorrect SCF cue rate before and after heuristics 
4 Conclusion 
We showed that it should not be assumed that stan-
dard statistical parsers will fail on language that is 
very different from what they are trained on. Spe-
cifically, the results of Experiment 1 showed that it 
is feasible to apply current SCF extraction 
technology to spoken language. Experiment 2 
showed that incorrect SCF cues due to segmenta-
tion errors and disfluency can be recognized by our 
linguistic heuristics. We have shown that our SCF 
acquisition system as a whole will work for the 
different demands of spoken language. 
5 Acknowledgements 
This work was supported by NSF grant 0347799 to 
the second author, and by a summer fellowship 
from the Ohio State Center for Cognitive Science 
to the first author. 
References  
Biekl, D. 2004. Intricacies of Collins? Parsing Model. Computational 
Linguistics, 30(4): 470-511 
Brent, M. 1993. From Grammar to Lexicon: Unsupervised Learning 
of Lexical Syntax. Computational Lingusitics: 19(3): 243-262 
Briscoe, E. & Carroll, G. 1997. Automatic Extraction of Subcategori-
zation from Corpora. In Proceedings of the 5th ACL Conference on 
Applied Natural Language Processing, Washington, DC. 356-363 
Chaniak, E. 2000. A Maximum-Entropy-Inspired Parser. In Proceed-
ings of the 2000 Conference of the North American Chapter of 
ACL. 132-139 
Grishman, R., Macleod, C. & Meyers, A. 1994. COMLEX Syntax: 
Building a Computational Lexicon. In Proceedings of the Interna-
tional Conference on Computational Lingusitics, COLING-94, 
Kyoto, Japan. 268-272 
Pitt, M., Johnson, K., Hume, E., Kiesling, S., Raymond, W. 2005. 
They Buckeye Corpus of Conversational Speech: Labeling Con-
ventions and a Test of Transcriber Reliability. Speech Communica-
tion, 45: 89-95 
Roland, D. & Jurafsky, D. 1998. How Verb Subcategorization Fre-
quency Affected by the Corpus Choice. In Proceedings of 17th In-
ternational Conference on Computational Lingusitics, 2: 1122-
1128 
Sarkar, A. & Zeman, D. 2000. Automatic Extraction of Subcategoriza-
tion Frames for Czech. In Proceedings of the 19th International 
Conference on Computational Lingusitics. 691-697 
205
Tagging Portuguese with a Spanish Tagger Using Cognates
Jirka Hana
Department of Linguistics
The Ohio State University
hana.1@osu.edu
Anna Feldman
Department of Linguistics
The Ohio State University
afeldman@ling.osu.edu
Luiz Amaral
Department of Spanish and Portuguese
The Ohio State University
amaral.1@osu.edu
Chris Brew
Department of Linguistics
The Ohio State University
cbrew@acm.org
Abstract
We describe a knowledge and resource
light system for an automatic morpholog-
ical analysis and tagging of Brazilian Por-
tuguese.1 We avoid the use of labor in-
tensive resources; particularly, large anno-
tated corpora and lexicons. Instead, we
use (i) an annotated corpus of Peninsular
Spanish, a language related to Portuguese,
(ii) an unannotated corpus of Portuguese,
(iii) a description of Portuguese morphol-
ogy on the level of a basic grammar book.
We extend the similar work that we have
done (Hana et al, 2004; Feldman et al,
2006) by proposing an alternative algo-
rithm for cognate transfer that effectively
projects the Spanish emission probabili-
ties into Portuguese. Our experiments use
minimal new human effort and show 21%
error reduction over even emissions on a
fine-grained tagset.
1 Introduction
Part of speech (POS) tagging is an important step
in natural language processing. Corpora that have
been POS-tagged are very useful both for linguis-
tic research, e.g. finding instances or frequencies
of particular constructions (Meurers, 2004) and for
further computational processing, such as syntac-
tic parsing, speech recognition, stemming, word-
sense disambiguation. Morphological tagging is
the process of assigning POS, case, number, gen-
der and other morphological information to each
word in a corpus. Despite the importance of mor-
phological tagging, there are many languages that
1We thank the anonymous reviewers for their constructive
comments on an earlier version of the paper.
lack annotated resources of this kind, mainly due
to the lack of training corpora which are usually
required for applying standard statistical taggers.
Applications of taggers include syntactic pars-
ing, stemming, text-to-speech synthesis, word-
sense disambiguation, information extraction. For
some of these getting all the tags right is inessen-
tial, e.g. the input to noun phrase chunking does
not necessarily require high accuracy fine-grained
tag resolution.
Cross-language information transfer is not new;
however, most of the existing work relies on par-
allel corpora (e.g. Hwa et al, 2004; Yarowsky
and Ngai, 2001) which are difficult to find, es-
pecially for lesser studied languages. In this pa-
per, we describe a cross-language method that re-
quires neither training data of the target language
nor bilingual lexicons or parallel corpora. We re-
port the results of the experiments done on Brazil-
ian Portuguese and Peninsular Spanish, however,
our system is not tied to these particular languages.
The method is easily portable to other (inflected)
languages. Our method assumes that an anno-
tated corpus exists for the source language (here,
Spanish) and that a text book with basic linguis-
tic facts about the source language is available
(here, Portuguese). We want to test the generality
and specificity of the method. Can the systematic
commonalities and differences between two ge-
netically related languages be exploited for cross-
language applications? Is the processing of Por-
tuguese via Spanish different from the processing
of Russian via Czech (Hana et al, 2004; Feldman
et al, 2006)?
33
Spanish Portuguese
1. sg. canto canto
2. sg. cantas cantas
3. sg. canta canta
1. pl. catamos cantamos
2. pl. cantais cantais
3. pl. cantan cantam
Table 1: Verb conjugation present indicative: -ar
regular verb: cantar ?to sing?
2 Brazilian Portuguese (BP)
vs. Peninsular Spanish (PS)
Portuguese and Spanish are both Romance lan-
guages from the Iberian Peninsula, and share
many morpho-syntactic characteristics. Both lan-
guages have a similar verb system with three main
conjugations (-ar, -er, -ir), nouns and adjectives
may vary in number and gender, and adverbs are
invariable. Both are pro-drop languages, they have
a similar pronominal system, and certain phenom-
ena, such as clitic climbing, are prevalent in both
languages. They also allow rather free constituent
order; and in both cases there is considerable de-
bate in the literature about the appropriate char-
acterization of their predominant word order (the
candidates being SVO and VSO).
Sometimes the languages exhibit near-complete
parallelism in their morphological patterns, as
shown in Table 1.
The languages are also similar in their lexicon and
syntactic word order:
(1) Os
Los
The
estudantes
estudiantes
students
ja?
ya
already
comparam
compraron
bought
os
los
the
livros.
libros.
books
[BP]
[PS]
?The students have already bought the
books.?
One of the main differences is the fact that
Brazilian Portuguese (BP) accepts object drop-
ping, while Peninsular Spanish (PS) doesn?t. In
addition, subjects in BP tend to be overt while in
PS they tend to be omitted.
(2) a. A: O que
What
voce?
you
fez
did
com
with
o
the
livro?
book?
[BP]
A: ?What did you do with the book??
B: Eu
I
dei
gave
para
to
Maria.
Mary
B: ?I gave it to Mary.?
b. A: ?Que?
What
hiciste
did
con
with
el
the
libro?
book?
[PS]
A: ?What did you do with the book??
B: Se
Her.dat
lo
it.acc
di
gave
a
to
Mar??a.
Mary.
B: ?I gave it to Mary.?
Notice also that in the Spanish example (2b) the
dative pronoun se ?her? is obligatory even when
the prepositional phrase a Mar??a ?to Mary? is
present.
3 Resources
3.1 Tagset
For both Spanish and Portuguese, we used posi-
tional tagsets developed on the basis of Spanish
CLiC-TALP tagset (Torruella, 2002). Every tag is
a string of 11 symbols each corresponding to one
morphological category. For example, the Por-
tuguese word partires ?you leave? is assigned the
tag VM0S---2PI-, because it is a verb (V), main
(M), gender is not applicable to this verb form (0),
singular (S), case, possesor?s number and form are
not applicable to this category(-), 2nd person (2),
present (P), indicative (I) and participle type is not
applicable (-).
A comparison of the two tagsets is in Table 2.2
When possible the Spanish and Portuguese tagsets
use the same values, however some differences are
unavoidable. For instance, the pluperfect is a com-
pound verb tense in Spanish, but a separate word
that needs a tag of its own in Portuguese. In ad-
dition, we added a tag for ?treatment? Portuguese
pronouns.
The Spanish tagset has 282 tags, while that for
Portuguese has 259 tags.
3.2 Training corpora
Spanish training corpus. The Spanish corpus
we use for training the transition probabilities as
well as for obtaining Spanish-Portuguese cognate
pairs is a fragment (106,124 tokens, 18,629 types)
of the Spanish section of CLiC-TALP (Torruella,
2Notice that we have 6 possible values for the gender po-
sition: M (masc.), F (fem.), N (neutr., for certain pronouns), C
(common, either M or F), 0 (unspecified for this form within
the category), - (the category does not distinguish gender)
34
No. Description No. of values
Sp Po
1 POS 14 11
2 SubPOS ? detailed POS 30 29
3 Gender 6 6
4 Number 5 5
5 Case 6 6
6 Possessor?s Number 4 4
7 Form 3 3
8 Person 5 5
9 Tense 7 9
10 Mood 8 9
11 Participle 3 3
Table 2: Overview and comparison of the tagsets
2002). CLiC-TALP is a balanced corpus, contain-
ing texts of various genres and styles. We automat-
ically translated the CLiC-TALP tagset into our
system (see Sect. 3.1) for easier detailed evalua-
tion and for comparison with our previous work
that used a similar approach for tagging (Hana
et al, 2004; Feldman et al, 2006).
Raw Portuguese corpus. For automatic lexi-
con acquisition, we use NILC corpus,3 containing
1.2M tokens.
3.3 Evaluation corpus
For evaluation purposes, we selected and manually
annotated a small portion (1,800 tokens) of NILC
corpus.
4 Morphological Analysis
Our morphological analyzer (Hana, 2005) is an
open and modular system. It allows us to com-
bine modules with different levels of manual in-
put ? from a module using a small manually pro-
vided lexicon, through a module using a large lex-
icon automatically acquired from a raw corpus, to
a guesser using a list of paradigms, as the only
resource provided manually. The general strat-
egy is to run modules that make fewer errors and
less overgenerate before modules that make more
errors and overgenerate more. This, for exam-
ple, means that modules with manually created
resources are used before modules with resources
3Nu?cleo Interdisciplinar de Lingu???stica Computacional;
available at http://nilc.icmc.sc.usp.br/nilc/,
we used the version with POS tags assigned by PALAVRAS.
We ignored the POS tags.
automatically acquired. In the experiments below,
we used the following modules ? lookup in a list
of (mainly) closed-class words, a paradigm-based
guesser and an automatically acquired lexicon.
4.1 Portuguese closed class words
We created a list of the most common preposi-
tions, conjunctions, and pronouns, and a number
of the most common irregular verbs. The list con-
tains about 460 items and it required about 6 hours
of work. In general, the closed class words can be
derived either from a reference grammar book, or
can be elicited from a native speaker. This does
not require native-speaker expertise or intensive
linguistic training. The reason why the creation
of such a list took 6 hours is that the words were
annotated with detailed morphological tags used
by our system.
4.2 Portuguese paradigms
We also created a list of morphological paradigms.
Our database contains 38 paradigms. We just en-
coded basic facts about the Portuguese morphol-
ogy from a standard grammar textbook (Cunha
and Cintra, 2001). The paradigms include all three
regular verb conjugations (-ar, -er, -ir), the most
common adjective and nouns paradigms and a rule
for adverbs of manner that end with -mente (anal-
ogous to the English -ly). We ignore majority of
exceptions. The creation of the paradigms took
about 8 h of work.
4.3 Lexicon Acquisition
The morphological analyzer supports a module or
modules employing a lexicon containing informa-
tion about lemmas, stems and paradigms. There is
always the possibility to provide this information
manually. That, however, is very costly. Instead,
we created such a lexicon automatically.
Usually, automatically acquired lexicons and
similar systems are used as a backup for large
high-precision high-cost manually created lexi-
cons (e.g. Mikheev, 1997; Hlava?c?ova?, 2001). Such
systems extrapolate the information about the
words known by the lexicon (e.g. distributional
properties of endings) to unknown words. Since
our approach is resource light, we do not have any
such large lexicon to extrapolate from.
The general idea of our system is very sim-
ple. The paradigm-based Guesser, provides all the
possible analyses of a word consistent with Por-
tuguese paradigms. Obviously, this approach mas-
35
sively overgenerates. Part of the ambiguity is usu-
ally real but most of it is spurious. We use a large
corpus to weed the spurious analyses out of the
real ones. In such corpus, open-class lemmas are
likely to occur in more than one form. Therefore,
if a lemma+paradigm candidate suggested by the
Guesser occurs in other forms in other parts of the
corpus, it increases the likelihood that the candi-
date is real and vice versa. If we encounter the
word cantamos ?we sing? in a Portuguese corpus,
using the information about the paradigms we can
analyze it in two ways, either as being a noun in
the plural with the ending -s, or as being a verb in
the 1st person plural with the ending -amos. Based
on this single form we cannot say more. However
if we also encounter the forms canto, canta, can-
tam the verb analysis becomes much more prob-
able; and therefore, it will be chosen for the lex-
icon. If the only forms that we encounter in our
Portuguese corpus were cantamos and (the non-
existing) cantamo (such as the existing word ramo
and ramos) then we would analyze it as a noun and
not as a verb.
With such an approach, and assuming that the
corpus contains the forms of the verb matar ?to
kill?, mato1sg matas2sg, mata3sg, etc., we would
not discover that there is also a noun mata ?forest?
with a plural form matas ? the set of the 2 noun
forms is a proper subset of the verb forms. A sim-
ple solution is to consider not the number of form
types covered in a corpus, but the coverage of the
possible forms of the particular paradigm. How-
ever this brings other problems (e.g. it penalizes
paradigms with large number of forms, paradigms
with some obsolete forms, etc.). We combine both
of these measures in Hana (2005).
Lexicon Acquisition consists of three steps:
1. A large raw corpus is analyzed with a
lexicon-less MA (an MA using a list of
mainly closed-class words and a paradigm
based guesser);
2. All possible hypothetical lexical entries over
these analyses are created.
3. Hypothetical entries are filtered with aim to
discard as many nonexisting entries as possi-
ble, without discarding real entries.
Obviously, morphological analysis based on
such a lexicon still overgenerates, but it overgener-
ates much less than if based on the endings alone.
Lexicon no yes
recall 99.0 98.1
avg ambig (tag/word) 4.3 3.5
Tagging (cognates) ? accuracy 79.1 82.1
Table 3: Evaluation of Morphological analysis
Consider for example, the form func?o?es ?func-
tions? of the feminine noun func?a?o. The analyzer
without a lexicon provides 11 analyses (6 lemmas,
each with 1 to 3 tags); only one of them is cor-
rect. In contrast, the analyzer with an automati-
cally acquired lexicon provides only two analyses:
the correct one (noun fem. pl.) and an incorrect
one (noun masc. pl., note that POS and number
are still correct). Of course, not all cases are so
persuasive.
The evaluation of the system is in Table 3. The
98.1% recall is equivalent to the upper bound for
the task. It is calculated assuming an oracle-
Portuguese tagger that is always able to select the
correct POS tag if it is in the set of options given
by the morphological analyzer. Notice also that
for the tagging accuracy, the drop of recall is less
important than the drop of ambiguity.
5 Tagging
We used the TnT tagger (Brants, 2000), an im-
plementation of the Viterbi algorithm for second-
order Markov model. In the traditional approach,
we would train the tagger?s transitional and emis-
sion probabilities on a large annotated corpus of
Portuguese. However, our resource-light approach
means that such corpus is not available to us and
we need to use different ways to obtain this infor-
mation.
We assume that syntactic properties of Spanish
and Portuguese are similar enough to be able to
use the transitional probabilities trained on Span-
ish (after a simple tagset mapping).
The situation with the lexical properties as cap-
tured by emission probabilities is more complex.
Below we present three different ways how to ob-
tains emissions, assuming:
1. they are the same: we use the Spanish emis-
sions directly (?5.1).
2. they are different: we ignore the Spanish
emissions and instead uniformly distribute
36
the results of our morphological analyzer.
(?5.2)
3. they are similar: we map the Spanish emis-
sions onto the result of morphological analy-
sis using automatically acquired cognates.
(?5.3)
5.1 Tagging ? Baseline
Our lowerbound measurement consists of training
the TnT tagger on the Spanish corpus and apply-
ing this model directly to Portuguese.4 The overall
performance of such a tagger is 56.8% (see the the
min column in Table 4). That means that half of
the information needed for tagging of Portuguese
is already provided by the Spanish model. This
tagger has seen no Portuguese whatsoever, and is
still much better than nothing.
5.2 Tagging ? Approximating Emissions I
The opposite extreme to the baseline, is to assume
that Spanish emissions are useless for tagging Por-
tuguese. Instead we use the morphological an-
alyzer to limit the number of possibilities, treat-
ing them all equally ? The emission probabilities
would then form a uniform distribution of the tags
given by the analyzer. The results are summarized
in Table 4 (the e-even column) ? accuracy 77.2%
on full tags, or 47% relative error reduction against
the baseline.
5.3 Tagging ? Approximating Emissions II
Although it is true that forms and distributions of
Portuguese and Spanish words are not the same,
they are also not completely unrelated. As any
Spanish speaker would agree, the knowledge of
Spanish words is useful when trying to understand
a text in Portuguese.
Many of the corresponding Portuguese and
Spanish words are cognates, i.e. historically they
descend from the same ancestor root or they are
mere translations. We assume two things: (i) cog-
nate pairs have usually similar morphological and
distributional properties, (ii) cognate words are
similar in form.
Obviously both of these assumptions are ap-
proximations:
1. Cognates could have departed in their mean-
ings, and thus probably also have dif-
4Before training, we translated the Spanish tagset into the
Portuguese one.
ferent distributions. For example, Span-
ish embarazada ?pregnant? vs. Portuguese
embarac?ada ?embarrassed?.
2. Cognates could have departed in their mor-
phological properties. For example, Span-
ish cerca ?near?.adverb vs. Portuguese cerca
?fence?.noun (from Latin circa, circus ?cir-
cle?).
3. There are false cognates ? unrelated,
but similar or even identical words. For
example, Spanish salada ?salty?.adj vs. Por-
tuguese salada ?salad?.noun, Spanish doce
?twelve?.numeral vs. Portuguese doce
?candy?.noun
Nevertheless, we believe that these examples
are true exceptions from the rule and that in major-
ity of cases, the cognates would look and behave
similarly. The borrowings, counter-borrowings
and parallel developments of the various Romance
languages have of course been extensively studied,
and we have no space for a detailed discussion.
Identifying cognates. For the present work,
however, we do not assume access to philologi-
cal erudition, or to accurate Spanish-Portuguese
translations or even a sentence-aligned corpus. All
of these are resources that we could not expect to
obtain in a resource poor setting. In the absence
of this knowledge, we automatically identify cog-
nates, using the edit distance measure (normalized
by word length).
Unlike in the standard edit distance, the cost of
operations is dependent on the arguments. Simi-
larly as Yarowsky and Wicentowski (2000), we as-
sume that, in any language, vowels are more muta-
ble in inflection than consonants, thus for example
replacing a for i is cheaper that replacing s by r.
In addition, costs are refined based on some well
known and common phonetic-orthographic regu-
larities, e.g. replacing a q with c is less costly than
replacing m with, say s. However, we do not want
to do a detailed contrastive morpho-phonological
analysis, since we want our system to be portable
to other languages. So, some facts from a simple
grammar reference book should be enough.
Using cognates. Having a list of Spanish-
Portuguese cognate pairs, we can use these to
map the emission probabilities acquired on Span-
ish corpus to Portuguese.
37
Let?s assume Spanish word ws and Portuguese
word wp are cognates. Let Ts denote the tags that
ws occurs within the Spanish corpus, and let ps(t)
be the emission probability of a tag t (t 6? Ts ?
ps(t) = 0). Let Tp denote tags assigned to the
Portuguese word wp by our morphological ana-
lyzer, and the pp(t) is the even emission proba-
bility: pp(t) = 1|Tp| . Then we can assign the new
emission probability p?p(t) to every tag t ? Tp in
the following way (followed by normalization):
p?p(t) =
ps(t) + pp(t)
2
(1)
Results. This method provides the best results.
The full-tag accuracy is 82.1%, compared to
56.9% for baseline (58% error rate reduction) and
77.2% for even-emissions (21% reduction). The
accuracy for POS is 87.6%. Detailed results are in
column e-cognates of Table 4.
6 Evaluation & Comparison
The best way to evaluate our results would be to
compare it against the TnT tagger used the usual
way ? trained on Portuguese and applied on Por-
tuguese. We do not have access to a large Por-
tuguese corpus annotated with detailed tags. How-
ever, we believe that Spanish and Portuguese are
similar enough (see Sect. 2) to justify our assump-
tion that the TnT tagger would be equally success-
ful (or unsuccessful) on them. The accuracy of
TnT trained on 90K tokens of the CLiC-TALP cor-
pus is 94.2% (tested on 16K tokens). The accuracy
of our best tagger is 82.1%. Thus the error-rate is
more than 3 times bigger (17.9% vs. 5.4%).
Branco and Silva (2003) report 97.2% tagging
accuracy on 23K testing corpus. This is clearly
better than our results, on the other hand they
needed a large Portuguese corpus of 207K tokens.
The details of the tagset used in the experiments
are not provided, so precise comparison with our
results is difficult.
7 Related work
Previous research in resource-light language
learning has defined resource-light in different
ways. Some have assumed only partially tagged
training corpora (Merialdo, 1994); some have be-
gun with small tagged seed wordlists (Cucerzan
and Yarowsky, 1999) for named-entity tagging,
while others have exploited the automatic trans-
fer of an already existing annotated resource in a
min e-even e-cognates
Tag: 56.9 77.2 82.1
POS: 65.3 84.2 87.6
SubPOS: 61.7 83.3 86.9
gender: 70.4 87.3 90.2
number: 78.3 95.3 96.0
case: 93.8 96.8 97.2
possessor?s num: 85.4 96.7 97.0
form: 92.9 99.2 99.2
person: 74.5 91.2 92.7
tense: 90.7 95.1 96.1
mood: 91.5 95.0 96.0
participle: 99.9 100.0 100.0
Table 4: Tagging Brazilian Portuguese
different genres or a different language (e.g. cross-
language projection of morphological and syn-
tactic information in (Yarowsky et al, 2001;
Yarowsky and Ngai, 2001), requiring no direct su-
pervision in the target language).
Ngai and Yarowsky (2000) observe that the to-
tal weighted human and resource costs is the most
practical measure of the degree of supervision.
Cucerzan and Yarowsky (2002) observe that an-
other useful measure of minimal supervision is the
additional cost of obtaining a desired functional-
ity from existing commonly available knowledge
sources. They note that for a remarkably wide
range of languages, there exist a plenty of refer-
ence grammar books and dictionaries which is an
invaluable linguistic resource.
7.1 Resource-light approaches to Romance
languages
Cucerzan and Yarowsky (2002) present a method
for bootstrapping a fine-grained, broad coverage
POS tagger in a new language using only one
person-day of data acquisition effort. Similarly
to us, they use a basic library reference gram-
mar book, and access to an existing monolingual
text corpus in the language, but they also use a
medium-sized bilingual dictionary.
In our work, we use a paradigm-based mor-
phology, including only the basic paradigms from
a standard grammar textbook. Cucerzan and
Yarowsky (2002) create a dictionary of regular in-
flectional affix changes and their associated POS
and on the basis of it, generate hypothesized in-
flected forms following the regular paradigms.
38
Clearly, these hypothesized forms are inaccurate
and overgenerated. Therefore, the authors perform
a probabilistic match from all lexical tokens actu-
ally observed in a monolingual corpus and the hy-
pothesized forms. They combine these two mod-
els, a model created on the basis of dictionary in-
formation and the one produced by the morpho-
logical analysis. This approach relies heavily on
two assumptions: (i) words of the same POS tend
to have similar tag sequence behavior; and (ii)
there are sufficient instances of each POS tag la-
beled by either the morphology models or closed-
class entries. For richly inflectional languages,
however, there is no guarantee that the latter as-
sumption would always hold.
The accuracy of their model is comparable to
ours. On a fine-grained (up to 5-feature) POS
space, they achieve 86.5% for Spanish and 75.5%
for Romanian. With a tagset of a similar size (11
features) we obtain the accuracy of 82.1% for Por-
tuguese.
Carreras et al (2003) present work on develop-
ing low-cost Named Entity recognizers (NER) for
a language with no available annotated resources,
using as a starting point existing resources for a
similar language. They devise and evaluate several
strategies to build a Catalan NER system using
only annotated Spanish data and unlabeled Cata-
lan text, and compare their approach with a classi-
cal bootstrapping setting where a small initial cor-
pus in the target language is hand tagged. It turns
out that the hand translation of a Spanish model is
better than a model directly learned from a small
hand annotated training corpus of Catalan. The
best result is achieved using cross-linguistic fea-
tures. Solorio and Lo?pez (2005) follow their ap-
proach; however, they apply the NER system for
Spanish directly to Portuguese and train a classi-
fier using the output and the real classes.
7.2 Cognates
Mann and Yarowsky (2001) present a method for
inducing translation lexicons based on trasduction
modules of cognate pairs via bridge languages.
Bilingual lexicons within language families are in-
duced using probabilistic string edit distance mod-
els. Translation lexicons for abitrary distant lan-
guage pairs are then generated by a combination
of these intra-family translation models and one
or more cross-family online dictionaries. Simi-
larly to Mann and Yarowsky (2001), we show that
languages are often close enough to others within
their language family so that cognate pairs be-
tween the two are common, and significant por-
tions of the translation lexicon can be induced with
high accuracy where no bilingual dictionary or
parallel corpora may exist.
8 Conclusion
We have shown that a tagging system with a small
amount of manually created resources can be suc-
cessful. We have previously shown that this ap-
proach can work for Czech and Russian (Hana
et al, 2004; Feldman et al, 2006). Here we have
shown its applicability to a new language pair.
This can be done in a fraction of the time needed
for systems with extensive manually created re-
sources: days instead of years. Three resources
are required: (i) a reference grammar (for infor-
mation about paradigms and closed class words);
(ii) a large amount of text (for learning a lexicon;
e.g. newspapers from the internet); (iii) a limited
access to a native speaker ? reference grammars
are often too vague and a quick glance at results
can provide feedback leading to a significant in-
crease of accuracy; however both of these require
only limited linguistic knowledge.
In this paper we proposed an algorithm for cog-
nate transfer that effectively projects the source
language emission probabilities into the target lan-
guage. Our experiments use minimal new human
effort and show 21% error reduction over even
emissions on a fine-grained tagset.
In the near future, we plan to compare the ef-
fectiveness (time and price) of our approach with
that of the standard resource-intensive approach to
annotating a medium-size corpus (on a corpus of
around 100K tokens). A resource-intensive sys-
tem will be more accurate in the labels which it of-
fers to the annotator, so annotator can work faster
(there are fewer choices to make, fewer keystrokes
required). On the other hand, creation of the in-
frastructure for such a system is very time con-
suming and may not be justified by the intended
application.
The experiments that we are running right now
are supposed to answer the question of whether
training the system on a small corpus of a closely
related language is better than training on a larger
corpus of a less related language. Some prelim-
inary results (Feldman et al, 2006) suggest that
using cross-linguistic features leads to higher pre-
39
cision, especially for the source languages which
have target-like properties complementary to each
other.
9 Acknowledgments
We would like to thank Maria das Grac?as
Volpe Nunes, Sandra Maria Alu??sio, and Ricardo
Hasegawa for giving us access to the NILC cor-
pus annotated with PALAVRAS and to Carlos
Rodr??guez Penagos for letting us use the Spanish
part of the CLiC-TALP corpus.
References
Branco, A. and J. Silva (2003). Portuguese-
specific Issues in the Rapid Development of
State-of-the-art Taggers. In Workshop on Tag-
ging and Shallow Processing of Portuguese:
TASHA?2000.
Brants, T. (2000). TnT ? A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-
NAACL, pp. 224?231.
Carreras, X., L. Ma`rquez, and L. Padro? (2003).
Named Entity Recognition for Catalan Using
Only Spanish Resources and Unlabelled Data.
In Proceedings of EACL-2003.
Cucerzan, S. and D. Yarowsky (1999). Lan-
guage Independent Named Entity Recognition
Combining Morphological and Contextual Ev-
idence. In Proceedings of the 1999 Joint SIG-
DAT Conference on EMNLP and VLC, pp. 90?
99.
Cucerzan, S. and D. Yarowsky (2002). Boot-
strapping a Multilingual Part-of-speech Tagger
in One Person-day. In Proceedings of CoNLL
2002, pp. 132?138.
Cunha, C. and L. F. L. Cintra (2001). Nova
Grama?tica do Portugue?s Contempora?neo. Rio
de Janeiro, Brazil: Nova Fronteira.
Feldman, A., J. Hana, and C. Brew (2006). Experi-
ments in Morphological Annotation Transfer. In
Proceedings of Computational Linguistics and
Intelligent Text Processing (CICLing).
Hana, J. (2005). Knowledge and labor light mor-
phological analysis. Unpublished manuscript.
Hana, J., A. Feldman, and C. Brew (2004). A
Resource-light Approach to Russian Morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of EMNLP 2004, Barcelona,
Spain.
Hlava?c?ova?, J. (2001). Morphological Guesser
or Czech Words. In V. Matous?ek (Ed.), Text,
Speech and Dialogue, Lecture Notes in Com-
puter Science, pp. 70?75. Berlin: Springer-
Verlag.
Hwa, R., P. Resnik, A. Weinberg, C. Cabezas,
and O. Kolak (2004). Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Nat-
ural Language Engineering 1(1), 1?15.
Mann, G. S. and D. Yarowsky (2001). Multipath
Translation Lexicon via Bridge Languages. In
Proceedings of NAACL 2001.
Merialdo, B. (1994). Tagging English Text with
a Probabilistic Model. Computational Linguis-
tics 20(2), 155?172.
Meurers, D. (2004). On the Use of Electronic Cor-
pora for Theoretical Linguistics. Case Studies
from the Syntax of German. Lingua.
Mikheev, A. (1997). Automatic Rule Induction
for Unknown Word Guessing. Computational
Linguistics 23(3), 405?423.
Ngai, G. and D. Yarowsky (2000). Rule Writing or
Annotation: Cost-efficient Resource Usage for
Base Noun Phrase Chunking. In Proceedings of
the 38th Meeting of ACL, pp. 117?125.
Solorio, T. and A. L. Lo?pez (2005). Learning
named entity recognition in Portuguese from
Spanish. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing (CI-
CLing).
Torruella, M. (2002). Gu??a para la anotacio?n mor-
folo?gica del corpus CLiC-TALP (Versio?n 3).
Technical Report WP-00/06, X-Tract Working
Paper.
Yarowsky, D. and G. Ngai (2001). Inducing Mul-
tilingual POS Taggers and NP Bracketers via
Robust Projection Across Aligned Corpora. In
Proceedings of NAACL-2001, pp. 200?207.
Yarowsky, D., G. Ngai, and R. Wicentowski
(2001). Inducing Multilingual Text Analy-
sis Tools via Robust Projection across Aligned
Corpora. In Proceedings of HLT 2001, First
International Conference on Human Language
Technology Research.
Yarowsky, D. and R. Wicentowski (2000). Min-
imally supervised morphological analysis by
multimodal alignment. In Proceedings of the
38th Meeting of the Association for Computa-
tional Linguistics, pp. 207?216.
40
BioNLP 2007: Biological, translational, and clinical language processing, pages 97?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Shared Task Involving Multi-label Classification of Clinical Free Text
John P. Pestian1, Christopher Brew2, Pawe? Matykiewicz1,4,
DJ Hovermale2, Neil Johnson1, K. Bretonnel Cohen3,
W?odzis?aw Duch4
1Cincinnati Children?s Hospital Medical Center, University of Cincinnati,
2Ohio State University, Department of Linguistics,
3University of Colorado School of Medicine,
4Nicolaus Copernicus University, Torun?, Poland.
Abstract
This paper reports on a shared task involving
the assignment of ICD-9-CM codes to radi-
ology reports. Two features distinguished
this task from previous shared tasks in the
biomedical domain. One is that it resulted in
the first freely distributable corpus of fully
anonymized clinical text. This resource is
permanently available and will (we hope) fa-
cilitate future research. The other key fea-
ture of the task is that it required catego-
rization with respect to a large and commer-
cially significant set of labels. The number
of participants was larger than in any pre-
vious biomedical challenge task. We de-
scribe the data production process and the
evaluation measures, and give a preliminary
analysis of the results. Many systems per-
formed at levels approaching the inter-coder
agreement, suggesting that human-like per-
formance on this task is within the reach of
currently available technologies.
1 Introduction
Clinical free text (primary data about patients, as op-
posed to journal articles) poses significant technical
challenges for natural language processing (NLP).
In addition, there are ethical and social demands
when working with such data, which is intended for
use by trained medical practitioners who appreciate
the constraints that patient confidentiality imposes.
State-of-the-art NLP systems handle carefully edited
text better than fragmentary notes, and clinical lan-
guage is known to exhibit unique sublanguage char-
acteristics (Hirschman and Sager, 1982; Friedman
et al, 2002; Stetson et al, 2002) (e.g. verbless
sentences, domain-specific punctuation semantics,
and unusual metonomies) that may limit the perfor-
mance of general NLP tools. More importantly, the
confidentiality requirements take time and effort to
address, so it is not surprising that much work in
the biomedical domain has focused on edited jour-
nal articles (and the genomics domain) rather than
clinical free text in medical records. The fact re-
mains, however, that the automation of healthcare
workflows can bring important benefits to treatment
(Hurtado et al, 2001) and reduce administrative bur-
den, and that free text is a critical component of
these workflows. There are economic motivations
for the task, as well. The cost of adding labels like
ICD-9-CM to clinical free text and the cost of re-
pairing associated errors is approximately $25 bil-
lion per year in the US (Lang, 2007). For these
(and many other) reasons, there have been consis-
tent attempts to overcome the obstacles which hin-
der the processing of clinical text (Uzuner et al,
2006). This paper discusses one such attempt?
The 2007 Computational Medicine Challenge, here-
after referred to as ?the Challenge?. There were two
main reasons for conducting the Challenge. One
is to facilitate advances in mining clinical free text;
shared tasks in other biomedical domains have been
shown to drive progress in the field in multiple ways
(see (Hirschman and Blaschke, 2006; Hersh et al,
2005; Uzuner et al, 2006; Hersh et al, 2006) for a
comprehensive review of biomedical challenge tasks
and their contributions). The other is a ground-
97
breaking distribution of useful, reusable, carefully
anonymized clinical data to the research commu-
nity, whose data use agreement is simply to cite the
source. The remaining sections of this paper de-
scribe how the data were prepared, the methods for
scoring, preliminary results [to be updated if sub-
mission is accepted?results are currently still under
analysis], and some lessons learned.
2 Corpus collection and coding process
Supervised methods for machine learning require
training data. Yet, due to confidentiality require-
ments, spotty electronic availability, and variance in
recording standards, the requisite clinical training
data are difficult to obtain. One goal of the chal-
lenge was to create a publicly available ?gold stan-
dard? that could serve as the seed for a larger, open-
source clinical corpus. For this we used the follow-
ing guiding principles: individual identity must be
expunged to meet United States HIPAA standards,
(U.S. Health, 2002) and approved for release by the
local Institutional Review Board (IRB); the sample
must represent problems that medical records coders
actually face; the sample must have enough data for
machine-learning-based systems to do well; and the
sample must include proportionate representations
of very low-frequency classes.
Data for the corpus were collected from the
Cincinnati Children?s Hospital Medical Center?s
(CCHMC) Department of Radiology. CCHMC?s
Institutional Review Board approved release of the
data. Sampling of all outpatient chest x-ray and re-
nal procedures for a one-year period was done us-
ing a bootstrap method (Walters, 2004). These data
are among those most commonly used, and are de-
signed to provide enough codes to cover a substan-
tial proportion of pediatric radiology activity. Ex-
punging patient identity to meet HIPAA standards
included three steps: disambiguation, anonymiza-
tion, and data scrubbing (Pestian et al, 2005).
Ambiguity and Anonymization. Not surprisingly,
some degree of disambiguation is needed to carry
out effective anonymization (Uzuner et al, 2006;
Sibanda and Uzuner, 2006). The reason is that clini-
cal text is dense with medical jargon, abbreviations,
and acronyms, many of which turn out to be ambigu-
ous between a sense that needs anonymization and a
different sense that does not. For example, in a clin-
ical setting, FT can be an abbreviation for full-term,
fort (as in Fort Bragg), feet, foot, field test, full-time
or family therapy. Fort Bragg, being a place name,
and a possible component of an address, could indi-
rectly lead to identification of the patient. Until such
occurrences are disambiguated, it is not possible to
be certain that other steps to anonymize data are ad-
equate. To resolve the relevant ambiguities found in
this free text, we relied on previous efforts that used
expert input to develop clinical disambiguation rules
(Pestian et al, 2004).
Anonymization. To assure patient privacy, clin-
ical text that is used for non-clinical reasons must
be anonymized. However, to be maximally useful
for machine-learning, this must be done in a par-
ticular way. Replacing personal names with some
unspecific value such as ?*? would lose potentially
useful information. Our goal is to replace the sensi-
tive fields with like values that obscure the identity
of the individual (Cho et al, 2002). We found that
the amount of sensitive information routinely found
in unstructured free text data is limited. In our case,
these data included patient and physician names and
sometimes dates or geographic locations, but little or
no other sensitive information turned up in the rele-
vant database fields. Using our internally developed
encryption broker software, we replaced all female
names with ?Jane?, all male names with ?John?, and
all surnames with ?Johnson?. Dates were randomly
shifted.
Manual Inspection. Once the data were disam-
biguated and anonymized, they were manually re-
viewed for the presence of any Protected Health In-
formation (PHI). If a specific token was perceived to
potentially violate PHI regulations, the entire record
was deleted from the dataset. In some case, how-
ever, a general geographic area was changed and
not deleted. For example if the data read ?patient
lived near Mr. Roger?s neighborhood? it would be
deleted, because it may be traceable. On the other
hand, if the data read ?patient was from Cincinnati?
it may have been changed to read ?patient was from
the Covington? After this process, a corpus of 2,216
records was obtained (See Table 2 for details).
ICD-9-CM Assignment. A radiology report has
multiple components. Two parts in particular are
essential for the assignment of ICD-9-CM codes:
98
clinical history?provided by an ordering physician
before a radiological procedure, and impression?
reported by a radiologist after the procedure. In the
case of radiology reports, ICD-9-CM codes serve as
justification to have a certain procedure performed.
There are official guidelines for radiology ICD-9-
CM coding (Moisio, 2000). These guidelines note
that every disease code requires a minimum num-
ber of digits before reimbursement will occur; that
a definite diagnosis should always be coded when
possible; that an uncertain diagnosis should never
be coded; and that symptoms must never be coded
when a definite diagnosis is available. Particular
hospitals and insurance companies typically aug-
ment these principles with more specific internal
guidelines and practices for coding. For these rea-
sons of policy, and because of natural variation in
human judgment, it is not uncommon for multiple
annotators to assign different codes to the same text.
Understanding the sources of this variation is impor-
tant; so too is the need to create a definite gold stan-
dard for use in the challenge. To do so, data were
annotated by the coding staff of CCHMC and two
independent coding companies: COMPANY Y and
COMPANY Z.
Majority annotation. A single gold standard was
created from these three sets of annotations. There
was no reason to adopt any a priori preference for
one annotator over another, so the democratic princi-
ple of assigning a majority annotation was used. The
majority annotation consists of those codes assigned
to the document by two or more of the annotators.
There are, however, several possible problems with
this approach. For example, it could be that the ma-
jority annotation will be empty. This will be rare
(126 records out of 2,216 in our case), because it
only happens when the codes assigned by the three
annotators form disjoint sets. In most hospital sys-
tems, including our own, the coders are required to
indicate a primary code. By convention, the primary
code is listed as the record?s first code, and has an
especially strong impact on the billing process. For
simplicity?s sake, the majority annotation process ig-
nores the distinction between primary and secondary
codes. There is space for a better solution here, but
we have not seriously explored it. We have, how-
ever, conducted an analysis of agreement statistics
(not further discussed here) that suggests that the
overall effect of the majority method is to create a
coding that shares many statistical properties with
the originals, except that it reduces the effect of the
annotators? individual idiosyncrasies. The majority
annotation is illustrated in Table 1.
Our evaluation strategy makes the simplistic as-
sumption that the majority annotation is a true gold
standard and a worthwhile target for emulation. This
is debatable, and is discussed below, but for the sake
of definiteness we simply stipulate that submissions
will be compared against the majority annotation,
and that the best possible performance is to exactly
replicate said majority annotation.
3 Evaluation
Micro- and macro-averaging. Although we rank
systems for purposes of determining the top three
performers on the basis of micro-averaged F1, we
report a variety of performance data, including the
micro-average, macro-average, and a cost-sensitive
measure of loss. Jackson and Moulinier comment
(for general text classification) that: ?No agree-
ment has been reached...on whether one should pre-
fer micro- or macro-averages in reporting results.
Macro-averaging may be preferred if a classification
system is required to perform consistently across all
classes regardless of how densely populated these
are. On the other hand, micro-averaging may be
preferred if the density of a class reflects its impor-
tance in the end-user system? (Jackson and Moulin-
ier, 2002):160-161. For the present medical ap-
plication, we are more interested in the number of
patients whose cases are correctly documented and
billed than in ensuring good coverage over the full
range of diagnostic codes. We therefore emphasize
the micro-average.
A cost-sensitive accuracy measure. While F-
measure is well-established as a method for ranking,
there are reasons for wanting to augment this with
a cost-sensitive measure. An approach that allows
penalties for over-coding (a false positive) and
under-coding (a false negative) to be manipulated
has important implications. The penalty for under-
coding is simple?the hospital loses the amount of
revenue that it would have earned if it had assigned
the code. The regulations under which coding is
done enforce an automatic over-coding penalty of
99
Table 1: Majority Annotation
Hospital Company Y Company Z Majority
Document 1 AB BC AB AB
Document 2 BC ABD CDE BCD
Document 3 EF EF E EF
Document 4 ABEF ACEF CDEF ACEF
three times what is earned from the erroneous code,
with the additional risk of possible prosecution
for fraud. This motivates a generalized version of
Jaccard?s similarity metric (Gower and Legendre,
1986), which was introduced by Boutell, Shen, Luo
and Brown (Boutell et al, 2003).
Suppose that Yx is the set of correct labels for a test
set and Px is the set of labels predicted by some
participating system. Define Fx = Px ? Yx and
Mx = Yx ? Px , i.e. Fx is the set of false positives,
and Mx is the set of missed labels or false negatives.
The score is given by
score(Px) =
(
1?
?|Mx|+ ?|Fx|
|Yx ? Px|
)?
(1)
As noted in (Boutell et al, 2003), if ? = ? = 1 this
formula reduces to the simpler case of
score(Px) =
(
1?
|Yx ? Px|
|Yx ? Px|
)?
(2)
The discussion in (Boutell et al, 2003) points out
that constraints are necessary on ? and ? to ensure
that the inner term of the expression is non-negative.
We do not understand the way that they formulate
these constraints, but note that non-negativity will be
ensured if 0 ? ? ? 1 and 0 ? ? ? 1 . Since over-
coding is three times as bad as undercoding, we use
? = 1.0 , ? = 0.33 . Varying the value of ? would
affect the range of the scores, but does not alter the
rankings of individual systems. We therefore used
? = 1 . This measure does not represent the pos-
sibility of prosecution for fraud, because the costs
involved are incommensurate with the ones that are
represented. With these parameter settings, the cost-
sensitive measure produces rankings that differ con-
siderably from those produced by macro-averaged
balanced F-measure. For example, we shall see that
the system ranked third in the competition by macro-
averaged F-measure assigns a total of 1167 labels,
where the second-ranked assigns 1232, and the cost-
sensitive measure rewards this conservatism in as-
signing labels by reversing the ranking of the two
systems. In either case, the difference between the
systems is small (0.86% difference in F-measure,
0.53% difference in the cost-sensitive measure).
4 The Data
We selected for the challenge a subset of the com-
prehensive data set described above. The subset was
created by stratified sampling, such that it contains
20% of the documents in each category. Thus, the
proportion of categories in the sample is the same as
the proportion of categories in the full data set. We
included in the initial sample only those categories
to which 100 or more documents from the compre-
hensive data set were assigned. After the process
summarized in Table 2, the data were divided into
two partitions: a training set with 978 documents,
and a testing set with 976. Forty-five ICD-9-CM
labels (e.g 780.6) are observed in these data sets.
These labels form 94 distinct combinations (e.g. the
combination 780.6, 786.2). We required that any
combination have at least two exemplars in the data,
and we split each combination between the train-
ing and the test sets. So, there may be labels and
combinations of labels that occur only one time in
the training data, but participants can be sure that
no combination will occur in the test data that has
not previously occurred at least once in the train-
ing data. Our policy here has the unintended con-
sequence that any combination that appears exactly
once in the training data is highly likely to appear
exactly once in the test data. This gives unnecessary
information to the participants. In future challenges
we will drop the requirement for two occurrences in
the data, but ensure that single-occurrence combina-
tions are allocated to the training set rather than the
100
test set. This maintains the guarantee that there will
be no unseen combinations in the test data. The full
data set may be downloaded from the official chal-
lenge web-site.
5 Results
Notice of the Challenge was distributed using elec-
tronic mailing lists supplied by the Association of
Computational Linguistics, IEEE Computer Intelli-
gence and Data Mining, and American Medical In-
formatics Association?s Natural Language Process-
ing special interest group. Interested participants
were asked to register at the official challenge web-
site. Registration began February 1, 2007 and ended
February 28, 2007. Approximately 150 individu-
als registered from 22 countries and six continents.
Upon completing registration, an automated e-mail
was sent with the location of the training data. On
March 1, 2007 participants received notice of the
location of the testing data. Participants were en-
couraged to use the data for other purposes as long
as it was non-commercial and the appropriate cita-
tion was made. There were no other data use re-
strictions. Participants had until March 18, 2007
to submit their results and an explanation of their
model. Approximately 33% (50) of the partici-
pants submitted results. During the course of the
Challenge participants asked a range of questions.
These were posted to the official challenge web-site
- www.computationalmedicine.org/challenge.
The figure below is a scatterplot relating micro-
averaged F1 to the cost-sensitive measure described
above. Each point represents a system. The top-
performing systems achieved 0.8908, the minimum
was 0.1541, and the mean was 0.7670, with a SD
of 0.1340. There are 21 systems with a micro-
averaged F1 between 0.81 and 0.90. Another 14
have F1 > 0.70 . It is noticeable that the systems
are not ranked identically by the cost-sensitive and
the micro-averaged measure, but the differences are
small in each case.
A preliminary screening using a two-factor ANOVA
with system identity and diagnostic code as predic-
tive factors for balanced F-measure revealed a sig-
nificant main effect of both system and code. Pair-
wise t-tests using Holm?s correction for multiple
comparisons revealed no statistically significant dif-
Figure 1: Scatter plot of evaluation measures
ferences between the systems performing at F=0.70
or higher. Differences between the top system and a
system with a microaveraged F-measure of 0.66 do
come out significant on this measure.
We have also calculated (Table 3) the agreement
figures for the three individual annotations that
went into the majority gold standard. We see
that CCHMC outranks COMPANY Y on the cost-
sensitive measure, but the reverse is true for micro-
and macro-averaged F1, with the agreement be-
tween the hospital and the gold standard being espe-
cially low for the macro-averaged version. To under-
stand these figures, it is necessary to recall that the
gold standard is a majority annotation that is formed
from the the three component annotations. It appears
that for rare codes, which have a disproportionate
effect on the macro-averaged F, the majority anno-
tation is dominated by cases where company Y and
company Z assign the same code, one that CCHMC
did not assign.
The agreement figures are comparable to those of
the best automatic systems. If submitted to the
competition, the components of the majority anno-
tation would not have outranked the best systems,
even though the components contributed to the ma-
jority opinion. It is tempting to conclude that the
automated systems are close to human-level perfor-
mance. Recall, however, that while the hospital and
the companies did not have the luxury of exposure
to the majority annotation, the systems did have that
access, which allowed them to explicitly model the
properties of that majority annotation. A more mod-
erate conclusion is that the hospital and the compa-
nies might be able to improve (or at least adjust)
their annotation practices by studying the majority
101
Table 2: Characteristics of the data set through the development process.
Step Removed Total documents
One-year collection of documents 20,275
20 percent sample of one-year collection 4,055
Manual inspection for anonymization problems 1,839 2,216
Removal of records with no majority code 126 2,090
Removal of records with a code occurring only once 136 1,954
Table 3: Comparison of human annotators against majority.
Annotator Cost-sensitive Micro-averaged F1 Macro-averaged F1
HOSPITAL 0.9056 0.8264 0.6124
COMPANY Y 0.8997 0.8963 0.8973
COMPANY Z 0.8621 0.8454 0.8829
annotation and adapting as appropriate.
6 Discussion
Compared to other recent text classification shared
tasks in the biomedical domain (Uzuner et al, 2006;
Hersh et al, 2004; Hersh et al, 2005), this task re-
quired categorization with respect to a set of labels
more than an order of magnitude larger than previ-
ous evaluations. This increase in the size of the set
of labels is an important step forward for the field?
systems that perform well on smaller sets of cate-
gories do not necessarily perform well with larger
sets of categories (Jackson and Moulinier, 2002), so
the data set will allow for more thorough text cat-
egorization system evaluations than have been pos-
sible in the past. Another important contribution of
the work reported here may be the distribution of
the data?the first fully distributable, freely usable
data set of clinical text. The high number of partici-
pants and final submissions was a pleasant surprise;
we attribute this, among other things, to the fact that
this was an applied challenge, that real data were
supplied, and that participants were free to use these
data in other venues.
Participants utilized a diverse range of approaches.
These system descriptions are based on brief com-
ments entered into the submission box, and are ob-
viously subject to revision. The three highest scor-
ers all mentioned ?negation,? all seemed to be us-
ing the structure of UMLS in a serious way. The
better systems frequently mentioned ?hypernyms?
or ?synonyms,? and were apparently doing signifi-
cant amounts of symbolic processing. Two of the
top three had machine-learning components, while
one of the top three used purely symbolic methods.
The most common approach seems to be thought-
ful and medically-informed feature engineering fol-
lowed by some variety of machine learning. The
top-performing system used C4.5, suggesting that
use of the latest algorithms is not a pre-requisite for
success. SVMs and related large-margin approaches
to machine learning were strongly represented, but
did not seem to be reliably predictive of high rank-
ing.
6.1 Observations on running the task and the
evaluation
The most frequently viewed question of the FAQ
was related to a script to calculate the evaluation
score. This was supplied both as a downloadable
script and as an interactive web-page with a form for
submission. In retrospect, we realize that we had not
fully thought through what would happen as people
began to use this script. If we run a similar contest
in the future, we will be better prepared for the con-
fusion that this can cause.
A novel aspect of this task was that although we only
scored a single run on the test data, we allowed par-
ticipants to submit their ?final? run up to 10 times,
and to see their score each time. Note that although
102
participants could see how their score varied on suc-
cessive submissions, they did not have access to the
actual test data or to the correct answers, and so there
were no opportunities for special-purpose hacks to
handle special cases in the test data. The average
participant tried 5.27 (SD 3.17) submissions against
the test data. About halfway through the submis-
sion period we began to realize that in a competi-
tive situation, there are risks in providing the type
of feedback given on the submission form. In fu-
ture challenges, we will be judicious in selecting the
number of attempts allowed and the provision of any
type of feedback. As far as we can tell our general
assumption that the scientific integrity of the partic-
ipants was greater than the need to game the system
is true. It is good policy for those administering the
contest, however, to keep temptations to a minimum.
Our current preference would be to provide only the
web-page interface with no more than five attempts,
and to tell participants only whether their submis-
sion had been accepted, and if so, how many items
and how many codes were recognized.
We provided an XML schema as a precise and pub-
licly visible description of the submission format.
Although we should not have been, we were sur-
prised when changes to the schema were required
in order to accommodate small but unexpected vari-
ations in participant submissions. An even simpler
submission format would have been good. The ad-
vantage of the approach that we took was that XML
validation gave us a degree of sanity-checking at lit-
tle cost. The disadvantage was that some of the nec-
essary sanity-checking went beyond what we could
see how to do in a schema.
The fact that numerous participants generated sys-
tems with high performance indicates that the task
was reasonable, and that sufficient information
about the coding task was either provided by us or
inferred by the participants to allow them to do their
work. Since this is a first attempt, it is not yet clear
what the upper limits on performance are for this
task, but preliminary indications are that automated
systems are or will soon be viable as a component of
deployed systems for this kind of application.
7 Acknowledgements
The authors thank Aaron Cohen of the Oregon
Health and Science University for observations on
the inter-rater agreement between the three sources
and its relationship to the majority assignments, and
also for his input on testing for statistically signif-
icant differences between systems. We also thank
PERSON of ORGANIZATION for helpful com-
ments on the manuscript. Most importantly we
thank all the participants for their on-going commit-
ment, professional feedback and scientific integrity.
References
[Boutell et al, 2003] Boutell M., Shen X., Luo J. and
Brown C. 2003. Multi-label Semantic Scene Clas-
sification, Technical Report 813. Department of Com-
puter Science, University of Rochester September.
[Cho et al, 2002] Cho P. S., Taira R. K., and Kangarloo
H. 2002 Text boundary detection of medical reports.
Proceedings of the Annual Symposium of the American
Medical Informatics Association, 998.
[Friedman et al, 2002] Friedman C., Kra P., and Rzhetsky
A. 2002. Two biomedical sublanguages: a descrip-
tion based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35:222?235.
[Gower and Legendre, 1986] Gower J. C. and Legendre P.
1986. Metric and euclidean properties of dissimilarity
coefficient. Journal of Classification, 3:5?48.
[Hersh et al, 2004] Hersh W., Bhupatiraju R. T., Ross L.,
Roberts P., Cohen A. M., and Kraemer D. F. 2004.
TREC 2004 Genomics track overview. Proceedings of
the 13th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hersh et al, 2006] Hersh W., Cohen A. M., Roberts P.,
and Rekapalli H. K. 2006. TREC 2006 Genomics
track overview. Proceedings of the 15th Annual Text
Retrieval Conference National Institute of Standards
and Technology.
[Hersh et al, 2005] Hersh W., Cohen A. M., Yang J.,
Bhupatiraju R. T., Roberts P., and Hearst M. 2005.
TREC 2005 Genomics track overview. Proceedings of
the 14th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hirschman and Blaschke, 2006] Hirschman L. and
Blaschke C. 2006. Evaluation of text mining in
biology. Text mining for biology and biomedicine,
Chapter 9. Ananiadou S. and McNaught J., editors.
Artech House.
103
[Hirschman and Sager, 1982] Hirschman L. and Sager S.
1982. Automatic information formatting of a medi-
cal sublanguage. Sublanguage: studies of language in
restricted semantic domains, Chapter 2. Kittredge R.
and Lehrberger J., editors. Walter de Gruyter.
[Hurtado et al, 2001] Hurtado M. P, Swift E. K., and Cor-
rigan J. M. 2001. Crossing the Quality Chasm: A
New Health System for the 21st Century. Institute of
Medicine, National Academy of Sciences.
[Jackson and Moulinier, 2002] Jackson P. and Moulinier
I. 2002. Natural language processing for online appli-
cations: text retrieval, extraction, and categorization.
John Benjamins Publishing Co.
[Lang, 2007] Lang, D. 2007. CONSULTANT REPORT
- Natural Language Processing in the Health Care In-
dustry. Cincinnati Children?s Hospital Medical Cen-
ter, Winter 2007.
[Moisio, 2000] Moisio M. 2000. A Guide to Health Care
Insurance Billing. Thomson Delmar Learning, Clifton
Park.
[Pestian et al, 2005] Pestian J. P., Itert L., Andersen C. L.,
and Duch W. 2005. Preparing Clinical Text for Use in
Biomedical Research. Journal of Database Manage-
ment, 17(2):1-12.
[Pestian et al, 2004] Pestian J. P., Itert L., and Duch W.
2004. Development of a Pediatric Text-Corpus for
Part-of-Speech Tagging. Intelligent Information Pro-
cessing and Web Mining, Advances in Soft Computing,
219?226 New York, Springer Verlag.
[Sammuelsson and Wiren, 2000] Sammuelsson C. and
Wiren M. 2000. Parsing Techniques. Handbook of
Natural Language Processing, 59?93. Dale R., Moisl
H., Somers H., editors. New York, Marcel Deker.
[Sibanda and Uzuner, 2006] Sibanda T. and Uzuner O.
2006. Role of local context in automatic deidentifica-
tion of ungrammatical, fragmented text. Proceedings
of the Human Language Technology conference of the
North American chapter of the Association for Com-
putational Linguistics, 65?73.
[Stetson et al, 2002] Stetson P. D., Johnson S. B., Scotch
M., and Hripcsak G. 2002. The sublanguage of cross-
coverage. Proceedings of the Annual Symposium of
the American Medical Informatics Association, 742?
746.
[U.S. Health, 2002] U.S. Heath & Human Services.
2002. 45 CFR Parts 160 and 164 Standards for Privacy
of Individually Identifiable Health Information Final
Rule Federal Register, 67(157):53181?53273.
[Uzuner et al, 2006] Uzuner O., Szolovits P., and Kohane
I. 2006. i2b2 workshop on natural language process-
ing challenges for clinical records. Proceedings of the
Fall Symposium of the American Medical Informatics
Association.
[Walters, 2004] Walters S. J. 2004. Sample size and
power estimation for studies with health related quality
of life outcomes: a comparison of four methods using
the SF-36 Health and Quality of Life Outcomes, 2:26.
104
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 28?31,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Using the Wiktionary Graph Structure for Synonym Detection
Timothy Weale, Chris Brew, Eric Fosler-Lussier
Department of Computer Science and Engineering
The Ohio State University
{weale,cbrew,fosler}@cse.ohio-state.edu
Abstract
This paper presents our work on using
the graph structure of Wiktionary for syn-
onym detection. We implement seman-
tic relatedness metrics using both a direct
measure of information flow on the graph
and a comparison of the list of vertices
found to be ?close? to a given vertex. Our
algorithms, evaluated on ESL 50, TOEFL
80 and RDWP 300 data sets, perform bet-
ter than or comparable to existing seman-
tic relatedness measures.
1 Introduction
The recent creation of large-scale, collabora-
tively constructed semantic resources provides re-
searchers with cheap, easily accessible informa-
tion. Previous metrics used for synonym detec-
tion had to be built using co-occurrence statistics
of collected corpora (Higgins, 2004) or expensive,
expert-created resources such as WordNet or Ro-
get?s Thesaurus (Jarmasz and Szpakowicz, 2003).
Here, we evaluate the effectiveness of Wiktionary,
a collaboratively constructed resource, as a source
of semantic relatedness information for the syn-
onym detection problem.
Researching these metrics is important because
they have been empirically shown to improve per-
formance in a variety of NLP applications, includ-
ing word sense disambiguation (Turdakov and Ve-
likhov, 2008), real-world spelling errors (Budan-
itsky and Hirst, 2006) and coreference resolution
(Strube and Ponzetto, 2006).
Synonym detection is a recognized testbed
for comparing semantic relatedness metrics (e.g
(Zesch et al, 2008)). In this task, a target word
or phrase is presented to the system, which is then
presented with four alternative words or phrases.
The goal of the system is to pick the alternative
most related to the target. Example questions can
be found in Figure 1.
Through the Wikimedia Foundation,
1
volun-
teers have created two large-scale, collaborative
resources that have been used in previous related-
ness research ? Wikipedia (an encyclopedia) and
Wiktionary (a dictionary). These sources have
been used for synonym detection and replicating
human relatedness evaluations using the category
structure (Strube and Ponzetto, 2006), local link
structure (Milne and Witten, 2008) and (Turdakov
and Velikhov, 2008) and global features (Zesch et
al., 2008). They contain related information but
focus on different information needs; which infor-
mation source provides better results depends on
the needs of the task. We use Wiktionary which,
due to its role as a dictionary, focuses on common
words and definitions ? the type of information
found in our synonym detection problems.
Both Wikipedia and Wiktionary are organized
around a basic ?page? unit, containing informa-
tion about an individual word, phrase or entity
in the world ? definitions, thesaurus entries, pro-
nunciation guides and translations in Wiktionary
and general biographical, organizational or philo-
sophical information in Wikipedia. In both data
sets, pages are linked to each each other and to
a user-created category structure ? a graph struc-
ture where pages are vertices of the graph and page
links are the graph edges. We will leverage this
graph for determining relatedness.
1
http://www.wikimedia.org/
Source Word Alternative Words
make earn, print, trade, borrow
flawed imperfect, tiny, lustrous, crude
solitary alone, alert, restless, fearless
Figure 1: Example TOEFL Questions
28
2 Extracting Relatedness Measures
We define relatedness based on information flow
through the entire Wiktionary graph, rather than
by any local in-bound or out-bound link structure.
This provides a global measurement of vertex im-
portance, as we do not limit the approach to com-
paring immediate neighbors.
To do this, we first run the PageRank algorithm
(Brin and Page, 1998) iteratively over the graph
until convergence to measure the a priori impor-
tance of each vertex in graph:
~
PR
t+1
= ??
(
~
PR
t
? E
)
+ (1 ? ?) ?
~
J (1)
In this, E contains the edge transition probabilities,
set to a uniform out-bound probability.
~
PR holds
the PageRank value for each vertex and
~
J is uni-
form vector used to randomly transition between
vertices. Traditionally, ? = 0.85 and is used to
tradeoff between a strict transition model and the
random-walk model.
We then adopt the extensions proposed in (Ol-
livier and Senellart, 2007) (OS) to determine re-
latedness given a source vertex:
~
R
t+1
= ??
(
~
R
t
? E + (
~
S ?
~
PR)
)
+(1??)?
~
J
(2)
~
S is a vector that contains zeros except for a one
at our source vertex, and
~
PR removes an overall
value of 1 based on the a priori PageRank value of
the vertex. In this way, vertices close to the source
are rewarded with weight and vertices that have a
high a priori importance are penalized. When
~
R
converges, it contains measures of importance for
vertices based on the source vertex.
Final relatedness values are then calculated
from the vector generated by Equation 2 and the
a priori importance of the vector based on the
PageRank from Equation 1:
rel
OS
(w, a) =
~
R
w
[a] ? log
(
1
PR[a]
)
(3)
w is the vertex for the source word and a is the
alternative word vertex. ThePR[a] penalty is used
to further ensure that our alternative vertex is not
highly valued simply because it is well-connected.
Applying Equation 3 provides comparable se-
mantic relatedness performance (see Tables 1 and
2). However, cases exist where a single data value
is insufficient to make an adequate determination
of word relatedness because of small differences
for candidate words. We can incorporate addi-
tional relatedness information about our vertices
by leveraging information about the set of vertices
deemed ?most related? to our current vertex.
2.1 Integrating N-Best Neighbors
We add information by looking at the similarity
between the n-best related words for each vertex.
Intuitively, given a source word w and candidate
alternatives a
1
and a
2
,
2
we look at the set of words
that are semantically related to each of the can-
didates (represented as vectors W , A
1
and A
2
).
If the overlap between elements of W and A
1
is
greater thanW andA
2
, A
1
is more likely to be the
synonym of W .
Highly-ranked shared elements are good indi-
cators of relatedness and should contribute more
than low-ranked related words. Lists with many
low-ranked words could be an artifact of the data
set and should not be ranked higher than ones con-
taining a few high-ranked words.
Our ranked-list comparison metric (NB) is a se-
lective mean reciprocal ranking function:
rel
NB
(
~
W,
~
A, n) =
n
?
r=1
1
r
? ?(W
r
?
~
A) (4)
~
W is the n-best list based on the source vertex
and
~
A is the n-best list based on the alternative
vertex. Values are added to our relatedness metric
based on the position of a vertex in the target list
and the traditional Dirac ?-function, which has a
value of one if the target vertex appears anywhere
in our candidate list and a zero in all other cases.
Each metric (OS and NB) will have different
ranges. We therefore normalize the reported value
by scaling each based on the maximum value for
that portion in order to achieve a uniform scale.
Our final metric (OS+NB) is created by aver-
aging the two normalized scores. In this work,
both scores are given equal weighting. Deriving
weightings for combining the two scores will be
part of our future work.
rel
OS+NB
(w
i
,
j
) =
OS(c
i
, c
j
) + NB(c
i
, c
j
, n)
2
(5)
In this, OS() returns the normalized rel
OS
()
value and NB() returns the normalized rel
NB
value. The maximum rel
P+N
() value of 1.0 is
achieved if c
j
has the highest PageRank-based
value and the highest N-Best value.
2
See Figure 1
29
Source
ESL TOEFL
Acc. (%) Acc. (%)
JPL 82 78.8
LC-IR 78 81.3
OS 86 88.8
NB 80 88.8
OS+NB 88 93.8
Table 1: ESL and TOEFL Performance
3 Evaluation
We present performance results on three data sets.
The first, ESL, uses 50 questions from the English
as a Second Language test (Turney, 2001). Next,
an 80 question data set from the Test of English
as a Foreign Language (TOEFL) is used (Lan-
dauer and Dumais, 1997). Finally, we evaluate
on the Reader?s Digest WordPower (RDWP) data
set (Jarmasz and Szpakowicz, 2003). This is a set
of 300 synonym detection problems gathered from
the Word Power game of the Canadian edition of
Reader?s Digest Word from 2000 ? 2001.
We use the Feb. 03, 2009 version of the English
Wiktionary data set
3
for extracting graph structure
and relatedness information.
Table 1 presents the performance of our algo-
rithm on the ESL and TOEFL test sets. Our results
are compared to Jarmasz and Szpakowicz (2003),
which uses a path-based cost on the structure
of Roget?s Thesaurus (JPL) and a cooccurence-
based metric, LC-IR (Higgins, 2004), which con-
strained context to only consider adjacent words in
structured web queries.
Information about our algorithm?s performance
on the RDWP test set is found in Table 2. Our re-
sults are compared to the previously mentioned al-
gorithms and also the work of Zesch et al (2008).
Their first metric (ZPL) uses the path length be-
tween two graph vertices for relatedness determi-
nation. The second, (ZCV), creates concept vec-
tors based on a distribution of pages that contain a
particular word.
RDWP is not only larger then the previous two,
but also more complictated. TOEFL and ESL
average 1.0 and 1.008 number of words in each
source and alternative, respectively. For RDWP
each entry averages 1.4 words.
We map words and phrases to graph vertices by
first matching against the page title. If there is no
3
http://download.wikimedia.org
match, we follow the approach outlined in (Zesch
et al, 2008). Common words are removed from
the phrase
4
and for every remaining word in the
phrase, we determine the page mapping for that
individual word. The relatedness of the phrase
is then set to be the maximum relatedness value
attributed to any of the individual words in the
phrase.
Random guessing by an algorithm could in-
crease algorithm performance through random
chance. Therefore, we present both a overall
percentage and also a precision-based percentage.
The first (Raw) is defined as the correct number of
guesses over all questions. The second (Prec) is
defined as the correct number of guesses divided
by only those questions that were attempted.
3.1 Discussion
For NB and OS+NB, we set n = 3000 based on
TOEFL data set training.
5
Testing was then per-
formed on the ESL and RDWP data set.
As shown in Table 1, the OS algorithm per-
forms better on the task than the comparison sys-
tems. On its own, NB relatedness performs well ?
at or slightly worse than OS. Combining the two
measures increases performance on both data sets.
While our TOEFL results are below the reported
performance of (Turney et al, 2003) (97.5%), we
do not use any task-dependent learning for our re-
sults and our algorithms have better performance
than any individual module in their system.
Combining OS with NB mitigates the influence
of OS when it is not confident. OS correctly picks
?pinnacle? as a synonym of ?zenith? with a relat-
edness value 126,000 times larger than its next
competitor. For ?consumed?, OS is wrong, giving
?bred? a higher score than ?eaten? ? but only by
a value 1.2 times that of ?eaten?. The latter case
is overcome by the addition of n-best information
while the former is unaffected.
Table 2 demonstrates that we have results com-
parable to existing state-of-the-art measures. Our
choice of n resulted in reduced scores on this task
when compared to using the OS metric by itself.
But, our algorithm still outperforms both the ZPL
and ZCV metrics for our data set in raw scores and
in three out of the four precision measures. Fur-
ther refinement of the RDWP data set mapping or
changing our metric score to a weighted sum of
4
Defined here as: {and, or, to, be, the, a, an, of, on, in, for,
with, by, into, is, no}
5
Out of 1.1 million vertices
30
Metric Source Attempted Score # Ties Raw Prec
JPL Roget?s 300 223 0 .74 .74
LC-IR Web 300 224.33 - .75 .75
ZPL
Wikipedia
226 88.33 96 .29 .39
ZCV 288 165.83 2 .55 .58
ZPL
Wiktionary
201 103.7 55 .35 .52
ZCV 174 147.3 3 .49 .85
OS
Wiktionary
300 234 0 .78 .78
NB 300 212 0 .71 .71
OS+NB 300 227 0 .76 .76
Table 2: Reader?s Digest WordPower 300 Overall Performance
sorts (rather than a raw maximum) could result in
increased performance.
Wiktionary?s coverage enables all words in the
first two tasks to be found (with the exception of
?bipartisanly?). Enough of the words in the RDWP
task are found to enable the algorithm to attempt
all synonym detection questions.
4 Conclusion and Future Work
In this paper, we have demonstrated the effective-
ness of Wiktionary as a source of relatedness in-
formation when coupled with metrics based on
information flow using synonym detection as our
evaluation testbed.
Our immediate work will be in learning weights
for the combination measure, using (Turney et al,
2003) as our guideline. Additional work will be in
automatically determining an effective value for n
across all data sets.
Long-term work will be in modifying the page
transition values to achieve non-uniform transition
values. Links are of differing quality, and the tran-
sition probabilities should reflect that.
References
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search En-
gine. Computer Networks and ISDN Systems, 30(1?
7):107?117.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Derrick Higgins. 2004. Which Statistics Reflect Se-
mantics? Rethinking Synonymy and Word Similar-
ity. In Proceedings of the International Conference
on Linguistic Evidence.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
Thesaurus and Semantic Similarity. In Proceedings
of Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2003).
Thomas K. Landauer and Susan T. Dumais. 1997. A
Solution to Plato?s Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction, and Rep-
resentation of Knowledge. Psychological Review.
David Milne and Ian H. Witten. 2008. An Effec-
tive, Low-Cost Measure of Semantic Relatedness
Obtained from Wikipedia Links. In Proceedings of
AAAI 2008.
Yann Ollivier and Pierre Senellart. 2007. Finding Re-
lated Pages Using Green Measures: An Illustration
with Wikipedia. In Proceedings of AAAI 2007.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! Computing Semantic Relatedness Us-
ing Wikipedia. In AAAI.
Denis Turdakov and Pavel Velikhov. 2008. Semantic
relatedness metric for Wikipedia concepts based on
link analysis and its application to word sense dis-
ambiguation. In Proceedings of CEUR.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining Indepen-
dent Modules in Lexical Multiple-Choice Problems.
In Recent Advances in Natural Language Processing
III: Selected Papers from RANLP 2003.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491?502, Freidburg,
Germany.
Torsten Zesch, Christof Muller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of AAAI 2008.
31
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 736?744,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
What a Parser can Learn from a Semantic Role Labeler and Vice Versa
Stephen A. Boxwell, Dennis N. Mehay, Chris Brew
The Ohio State University
{boxwell, mehay, cbrew}@ling.ohio-state.edu
Abstract
In many NLP systems, there is a unidirectional flow
of information in which a parser supplies input to a
semantic role labeler. In this paper, we build a sys-
tem that allows information to flow in both direc-
tions. We make use of semantic role predictions in
choosing a single-best parse. This process relies on
an averaged perceptron model to distinguish likely
semantic roles from erroneous ones. Our system pe-
nalizes parses that give rise to low-scoring semantic
roles. To explore the consequences of this we per-
form two experiments. First, we use a baseline gen-
erative model to produce n-best parses, which are
then re-ordered by our semantic model. Second, we
use a modified version of our semantic role labeler
to predict semantic roles at parse time. The perfor-
mance of this modified labeler is weaker than that
of our best full SRL, because it is restricted to fea-
tures that can be computed directly from the parser?s
packed chart. For both experiments, the resulting se-
mantic predictions are then used to select parses. Fi-
nally, we feed the selected parses produced by each
experiment to the full version of our semantic role
labeler. We find that SRL performance can be im-
proved over this baseline by selecting parses with
likely semantic roles.
1 Introduction
In the semantic role labeling task, words or groups of
words are described in terms of their relations to a pred-
icate. For example, the sentence Robin admires Leslie
has two semantic role-bearing words: Robin is the agent
or experiencer of the admire predicate, and Leslie is
the patient. These semantic relations are distinct from
syntactic relations like subject and object ? the proper
nouns in the sentence Leslie is admired by Robin have
the same semantic relationships as Robin admires Leslie,
even though the syntax differs.
Although syntax and semantics do not always align with
each other, they are correlated. Almost all automatic se-
mantic role labeling systems take a syntactic representa-
tion of a sentence (taken from an automatic parser or a
human annotator), and use the syntactic information to
predict semantic roles. When a semantic role labeler pre-
dicts an incorrect role, it is often due to an error in the
parse tree. Consider the erroneously annotated sentence
from the Penn Treebank corpus shown in Figure 1. If a
semantic role labeling system relies heavily upon syntac-
tic attachment decisions, then it will likely predict that
in 1956 describes the time that asbestos was used, rather
than when it ceased to be used.
Errors of this kind are common in treebanks and in au-
tomatic parses. It is telling, though, that while the hand-
annotated Penn Treebank (Marcus et al, 1993), the Char-
niak parser (Charniak, 2001), and the C&C parser (Clark
and Curran, 2004) all produce the erroneous parse from
Figure 1, the hand-annotated Propbank corpus of verbal
semantic roles (Palmer et al, 2005) correctly identifies in
1956 as a temporal modifier of stopped, rather than using.
This demonstrates that while syntactic attachment deci-
sions like these are difficult for humans and for automatic
parsers, a human reader has little difficulty identifying the
correct semantic relationship between the temporal mod-
ifier and the verbs. This is likely due to the fact that the
meaning suggested by the parse in Figure 1 is unlikely ?
the reader instinctively feels that a temporal modifier fits
better with the verb stop than with the verb use.
In this paper, we will use the idea that semantic roles
predicted by correct parses are more natural than seman-
tic roles predicted by erroneous parses. By modifying a
state-of-the-art CCG semantic role labeler to predict se-
mantic roles at parse time, or by using it to select from
an n-best list, we can prefer analyses that yield likely se-
mantic roles. Syntactic analysis is treated not as an au-
tonomous task, but rather as a contributor to the final goal
of semantic role labeling.
2 Related Work
There has been a great deal of work in joint parsing and
semantic role labeling in recent years. Two notable ef-
forts have been the CoNLL 2008 and 2009 shared tasks
736
S


HH
HH
H
NP
the company
VP



HH
H
H
VB
stopped
VP



HH
H
HH
VB
using
NP
asbestos
PP
in 1956
Figure 1: A parse tree based on the treebank parse of wsj 0003.3. Notice that the temporal adjunct is erroneously attached low. In
a syntax-based SRL system, this will likely lead to a role prediction error.
(Surdeanu et al, 2008; Hajic? et al, 2009). Many of these
systems perform joint syntactic and semantic analysis by
generating an n-best list of syntactic parses, labeling se-
mantic roles on all of them, then re-ranking these parses
by some means. Our approach differs from this strategy
by abandoning the preliminary ranking and predicting se-
mantic roles at parse time. By doing this, we effectively
open semantic roles in the entire parse forest to exami-
nation by the ranking model, rather than restricting the
model to an n-best list generated by a baseline parser. The
spirit of this work more closely resembles that of Finkel
and Manning (2009) , which improves both parsing and
named entity recognition by combining the two tasks.
3 Why Predicting Semantic Roles in a
Packed Chart is Difficult
Predicting semantic roles in the environment of a packed
chart is difficult when using an atomic CFG. In order to
achieve the polynomial efficiency appropriate for wide-
coverage parsing, it is necessary to ?pack? the chart ?
that is, to combine distinct analyses of a given span of
words that produce the same category. The only other
widely used option for wide-coverage parsing is to use
beam search with a narrow beam, which runs the risk
of search errors. On methodological grounds we pre-
fer an exhaustive search, since systems that rely heav-
ily on heuristics for their efficiency are difficult to un-
derstand, debug or improve. It is straightforward to read
off the highest scoring parse from a packed chart, and
similarly routine to generate an n-best list containing a
highly-ranked subset of the parses. However, a packed
chart built on an atomic CFG does not make available
all of the features that are important to many CFG-based
SRL systems. In particular, the very useful treepath fea-
ture, which lists the categories touched by walking the
tree from the predicate to the target word, only makes
sense when you have a complete tree, so cannot easily
be computed from the chart (Figure 2). Chart edges can
S




HH
H
HH
H
NPpeople



PP
PP
P
More intelligent people
VPsaw



PP
PP
P
saw kids with telescopes
Figure 2: In the context of a packed chart, it is meaningless to
speak of a treepath between saw and people because multiple
analyses are ?packed? under a single category.
be lexicalized with their headwords, and this information
would be useful in role labeling ? but even this misses
vital subcategorization information that would be avail-
able in the complete parse. An ideal formalism for our
purpose would condense into the category label a wide
range of information about combinatory potential, heads,
and syntactic dependencies. At the same time it should
allow the creation of a packed chart, come with labeled
training data, and have a high-quality parser and semantic
role labeler already available. Fortunately, Combinatory
Categorial Grammar offers these desiderata, so this is our
formalism of choice.
4 Combinatory Categorial Grammar
Combinatory Categorial Grammar (Steedman, 2000) is
a grammar formalism that describes words in terms of
their combinatory potential. For example, determiners
belong to the category np/n, or ?the category of words
that become noun phrases when combined with a noun
to the right?. The rightmost category indicates the argu-
ment that the category is seeking, the leftmost category
indicates the result of combining this category with its
argument, and the slash (/ or \) indicates the direction of
combination. Categories can be nested within each other:
a transitive verb like devoured belongs to the category
737
The man devoured the steak
np/n n (s\np)/npx npx/nx nx
> >np npx
>
s\np
<s
Figure 3: A simple CCG derivation.
The steak that the man devoured
np (npx\npx)/(s/npx) np (s\np)/npx
>T
s/(s\np)
>B
s/npx
>
npx\npx
<npx
Figure 4: An example of CCG?s treatment of relative clauses.
The syntactic dependency between devoured and steak is the
same as it was in figure 3. Co-indexations (the ?xs?) have been
added here and above to aid the eye in following the relevant
[devoured-steak] dependency.
(s\np)/np, or ?the category that would become a sentence
if it could combine with a noun phrase to the right and
another noun phrase to the left?. An example of how cat-
egories combine to make sentences is shown in Figure 3.
CCG has many capabilities that go beyond that of a typ-
ical context-free grammar. First, it has a sophisticated
internal system of managing syntactic heads and depen-
dencies1. These dependencies are used to great effect in
CCG-based semantic role labeling systems (Gildea and
Hockenmaier, 2003; Boxwell et al, 2009), as they do
not suffer the same data-sparsity effects encounted with
treepath features in CFG-based SRL systems. Secondly,
CCG permits these dependencies to be passed through in-
termediary categories in grammatical structures like rel-
ative clauses. In Figure 4, the steak is still in the object
relation to devoured, even though the verb is inside a rel-
ative clause. Finally and most importantly, these depen-
dencies are represented directly on the CCG categories
themselves. This is what makes CCG resistant to the
problem described in Section 3 ? because the dependency
is formed when the two heads combine, it is available to
be used as a local feature by the semantic role labeler.
1A complete explanation of CCG predicate-argument dependencies
can be found in the CCGbank user manual (Hockenmaier and Steed-
man, 2005)
5 Semantic Role Labeling
We use a modified version of the Brutus semantic role
labeling system (Boxwell et al, 2009)2. The original ver-
sion of this system takes complete CCG derivations as in-
put, and predicts semantic roles over them. For our pur-
poses, however, it is necessary to modify the system to
make semantic predictions at parse time, inside a packed
chart, before the complete derivation is available. For
this reason, it is necessary to remove the global features
from the system (that is, features that rely on the com-
plete parse), leaving only local features (features that are
known at the moment that the predicate is attached to the
argument). Crucially, dependency features count as ?lo-
cal? features, even though they have the potential to con-
nect words that are very far apart in the sentence.
Brutus is arranged in a two-stage pipeline. First, a max-
imum entropy classifier3 predicts, for each predicate in
turn, which words in the sentence are likely headwords of
semantic roles. Then, a second maximum entropy classi-
fier assigns role labels to each of these words. The fea-
tures used in the identification model of the local-only
version of Brutus are as follows:
? Words. A three-word window surrounding the can-
didate word. For example, if we were considering
the word steak in Figure 3, the three features would
be represented as word -1=the, word 0=steak, and
word 1=#, with the last feature representing an out-
of-bounds index.
? Predicate. The predicate whose semantic roles the
system is looking for. For example, the sentence in
figure 3 contains one predicate: devour.
? Syntactic Dependency. As with a previous ap-
proach in CCG semantic role labeling (Gildea and
Hockenmaier, 2003), this feature shows the ex-
act nature of the syntactic dependency between the
predicate and the word we are considering, if any
such dependency exists. This feature is represented
by the category of the predicate, the argument slot
that this word fits into, and whether or not the predi-
cate is the head of the resultant category, represented
with a left or right arrow. In the example from fig-
ure 3, the relationship between devoured and steak
would be represented as (s\np)/np.2.?.
The second maximum entropy classifier uses all of the
features from the identifier, plus several more:
2Found at http://www.ling.ohio-state.edu/
?boxwell/software/brutus.html
3Brutus uses Zhang Le?s maxent toolkit, available at
http://homepages.inf.ed.ac.uk/s0450736/maxent_
toolkit.html.
738
Model P R F
Local 89.8% 80.8% 85.1%
Global 89.8% 84.3% 87.0%
Table 1: SRL results for treebank parses, using the local model
described in Section 5 and the full global model.
? Before / After. A binary indicator feature indicat-
ing whether the candidate word is before or after the
predicate.
? Result Category Detail. This indicates the feature
on the result category of the predicate. Possible
values include dcl (for declarative sentences), pss
(for passive sentences), ng (for present-progressive
phrases like ?running the race?), etc. These are read
trivially off of the verbal category.
? Argument Mapping. An argument mapping is a
prediction of a likely set of semantic roles for a
given CCG predicate category. For example, a likely
argument mapping for devoured:(s[dcl]\np)/np is
[Arg0,Arg1]. These are predicted from string-level
features, and are useful for bringing together oth-
erwise independent classification decisions for in-
dividual roles. Boxwell et al (2009) describe this
feature in detail.
The Maximum-Entropy models were trained to 500 it-
erations. To prevent overfitting, we used Gaussian pri-
ors with global variances of 1 and 5 for the identifier
and the labeler, respectively. Table 1 shows SRL perfor-
mance for the local model described above, and the full
global CCG-system described by Boxwell et al (2009).
We use the method for calculating the accuracy of Prop-
bank verbal semantic roles described in the CoNLL-2008
shared task on semantic role labeling (Surdeanu et al,
2008). Because the Brutus SRL system is not designed
to accommodate Nombank roles (Meyers et al, 2004),
we restrict ourselves to predicting Propbank roles in the
present work.
The local system has the same precision as the global
one, but trails it on recall and F-measure. Note that this
performance is achieved with gold standard parses.
6 Performing Semantic Role Predictions at
Parse Time
Recall that the reasoning for using a substantially pared
down version of the Brutus SRL system is to allow it to
predict semantic roles in the context of a packed chart.
Because we predict semantic roles for each constituent
immediately after the constituent is formed and before it
is added to the chart, we can use semantic roles to inform
parsing. We use a CKY parsing algorithm, though this
approach could be easily adapted to other parse strate-
gies.
Whenever two constituents are combined, the SRL sys-
tem checks to see if either of the constituents contains
predicates. The system then attempts to identify seman-
tic roles in the other constituent related to this predicate.
This process repeats at every step, creating a combined
syntax-semantics parse forest. Crucially, this allows us
to use features derived from the semantic roles to rank
parses inside the packed chart. This could result in an
improvement over ranking completed parses, because re-
ranking completed parses requires first generating an n-
best list of parse candidates, potentially preventing the
re-ranker from examining high value parses falling out-
side the n-best list.
In order to train our parse model, it is necessary to first
employ a baseline parse model over the training set. The
baseline model is a PCFG model, where the products of
the probabilities of individual rule applications are used
to rank candidate parses. We use a cross-fold validation
technique to parse the training set (train on sections 02-
20 to parse section 21, train on sections 02-19 and 21 to
parse section 20, and so on). As we parse these sentences,
we use the local SRL model described in Section 5 to
predict semantic roles inside the packed chart. We then
iterate over the packed chart and extract features based
on the semantic roles in it, effectively learning from ev-
ery possible semantic role in the parse forest. Notice that
this does not require enumerating every parse in the for-
est (which would be prohibitively expensive) ? the roles
are labeled at parse time and can therefore be read di-
rectly from the packed chart. For each role in the packed
chart, we label it as a ?good? semantic role if it appears in
the human-judged Propbank annotation for that sentence,
and a ?bad? semantic role if it does not.
The features extracted from the packed chart are as fol-
lows:
? Role. The semantic role itself, concatenated with
the predicate. For example, play.Arg1. This will
represent the intuition described in Section 1 that
certain roles are more semantically appealing than
others.
? Role and Headword. The semantic role concate-
nated with the predicate and the headword of the se-
mantic role. This reflects the idea that certain words
fit with particular roles better than others.
These features are used to train an averaged percep-
tron model to distinguish between likely and unlikely se-
mantic roles. We incorporate the perceptron directly with
the parser using a packed feature forest implementation,
following an approach used by the current state-of-the-
art CCG parser (Clark and Curran, 2004). By prefer-
739
ring sentences with good semantic roles, we hope to pro-
duce parses that give better overall semantic role predic-
tions. The parser prefers spans with better semantic roles,
and breaks ties that would have arisen using the base-
line model alone. Similarly the baseline model can break
ties between equivalent semantic roles; this has the added
benefit of encouraging normal-form derivations in cases
of spurious ambiguity. The result is a single-best com-
plete parse with semantic roles already predicted. Once
the single-best parse is selected, we allow the global SRL
model to predict any additional roles over the parse, to
catch those roles that are difficult to predict from local
features alone.
7 Experiment 1: Choosing a Single-Best
Derivation from an N-best List
Our first experiment demonstrates our model?s perfor-
mance in a ranking task. In this task, a list of candidate
parses are generated by our baseline model. This base-
line model treats rule applications as a PCFG ? each rule
application (say, np + s\np = s) is given a probability in
the standard way. The rule probabilities are unsmoothed
maximum likelihood estimates derived from rule counts
in the training portion of CCGbank. After n-best deriva-
tions are produced by the baseline model, we use the Bru-
tus semantic role labeler to assign roles to each candi-
date derivation. We vary the size of the n-best list from
1 to 10 (note that an n-best list of size 1 is equivalent to
the single-best baseline parse). We then use the seman-
tic model to re-rank the candidate parses and produce a
single-best parse. The outcomes are shown in Table 2.
n P R F
1 85.1 71.7 77.8
2 85.9 74.8 79.9
5 84.5 76.8 80.5
10 83.7 76.8 80.1
C&C 83.6 76.8 80.0
Table 2: SRL performance on the development set (section 00)
for various values of n. The final row indicates SRL perfor-
mance on section 00 parses from the Clark and Curran CCG
parser.
The availability of even two candidate parses yields
a 2.1% boost to the balanced F-measure. This is be-
cause the semantic role labeler is very sensitive to syn-
tactic attachment decisions, and in many cases the set of
rule applications used in the derivation are very similar or
even the same. Consider the simplified version of a phe-
nomenon found in wsj 0001.1 shown in Figures 5 and 6.
The only difference in rule applications in these deriva-
tions is whether the temporal adjunct attaches to s[b]\np
or s[dcl]\np. Because the s[dcl]\np case is slightly more
He will join Nov. 27th
np (s[dcl]\np)/(s[b]\np) s[b]\np (s\np)\(s\np)
>
s[dcl]\np
>
s[dcl]\np
<
s[dcl]
Figure 5: The single-best analysis for He will join Nov 27th
according to the baseline model. Notice that the temporal ad-
junct is attached high, leading the semantic role labeler to fail
to identify ArgM-TMP.
He will join Nov. 27th
np (s[dcl]\np)/(s[b]\np) s[b]\np (s\np)\(s\np)
<
s[b]\np
>
s[dcl]\np
<
s[dcl]
Figure 6: The second-best analysis of He will join Nov 27th.
This analysis correctly predicts Nov 27th as the ArgM-TMP of
join, and the semantic model correctly re-ranks this analysis to
the single-best position.
common in the treebank, the baseline model identifies it
as the single-best parse, and identifies the derivation in
figure 6 as the second-best parse. The semantic model,
however, correctly recognizes that the semantic roles pre-
dicted by the derivation in Figure 6 are superior to those
predicted by the derivation in figure 5. This demonstrates
how a second or third-best parse according to the baseline
model can be greatly superior to the single-best in terms
of semantics.
8 Experiment 2: Choosing a Single-Best
Derivation Directly from the Packed
Chart
One potential weakness with the n-best list approach de-
scribed in Section 7 is choosing the size of the n-best list.
As the length of the sentence grows, the number of can-
didate analyses grows. Because sentences in the treebank
and in real-world applications are of varying length and
complexity, restricting ourselves to an n-best list of a par-
ticular size opens us to considering some badly mangled
derivations on short, simple sentences, and not enough
derivations on long, complicated ones. One possible so-
lution to this is to simply choose a single best derivation
directly from the packed chart using the semantic model,
eschewing the baseline model entirely except for break-
ing ties. In this approach, we use the local SRL model
described in section 6 to predict semantic roles at parse
time, inside the packed chart. This frees us from the
740
need to have a complete derivation (as in the n-best list
approach in Section 7). We use the semantic model to
choose a single-best parse from the packed chart, then we
pass this complete parse through the global SRL model to
give it all the benefits afforded to the parses in the n-best
approach. The results for the semantic model compared
to the baseline model are shown in table 3. Interestingly,
Model P R F
Baseline 85.1 71.7 77.8
Semantic 82.7 70.5 76.1
Table 3: A comparison of the performance of the baseline model
and the semantic model on semantic role labeling. The seman-
tic model, when unrestrained by the baseline model, performs
substantially worse.
the semantic model performs considerably worse than the
baseline model. To understand why, it is necessary to re-
member that the semantic model uses only semantic fea-
tures ? probabilities of rule applications are not consid-
ered. Therefore, the semantic model is perfectly happy to
predict derivations with sequences of highly unlikely rule
applications so long as they predict a role that the model
has been trained to prefer.
Apparently, the reckless pursuit of appealing semantic
roles can ultimately harm semantic role labeling accuracy
as well as parse accuracy. Consider the analysis shown
in Figure 7. Because the averaged perceptron semantic
model is not sensitive to the relationships between differ-
ent semantic roles, and because Arg1 of name is a ?good?
semantic role, the semantic model predicts as many of
them as it can. The very common np-appositive construc-
tion is particularly vulnerable to this kind of error, as it
can be easily mistaken for a three-way coordination (like
carrots, peas and watermelon). Many of the precision
errors generated by the local model are of this nature,
and the global model is unlikely to remove them, given
the presence of strong dependencies between each of the
?subjects? and the predicate.
Coordination errors are also common when dealing with
relative clause attachment. Consider the analysis in Fig-
ure 8. To a PCFG model, there is little difference be-
tween attaching the relative clause to the researchers or
Lorillard nor the researchers. The semantic model, how-
ever, would rather predict two semantic roles than just
one (because study:Arg0 is a highly appealing semantic
role). Once again, the pursuit of appealing semantic roles
has led the system astray.
We have shown in Section 7 that the semantic model
can improve SRL performance when it is constrained to
the most likely PCFG derivations, but enumerating n-best
lists is costly and cumbersome. We can, however, com-
bine the semantic model with the baseline PCFG. Our
method for doing this is designed to avoid the kinds of er-
ror described above. We first identify the highest-scoring
parse according to the PCFG model. This parse will be
used in later processing unless we are able to identify an-
other parse that satisfies the following criteria:
1. It must be closely related to the parse that has the
best score according to the semantic model. To iden-
tify such parses, we ask the chart unpacking algo-
rithm to generate all the parses that can be reached
by making up to five attachment changes to this se-
mantically preferred parse ? no more.
2. It must have a PCFG score that is not much less than
that of the single-best PCFG parse. We do this by
requiring that it has a score that is within a factor of
? of the best available. That is, the single-best parse
from the semantic model must satisfy
logP (sem) > logP (baseline) + log(?)
where the ? value is tuned on the development set.
If no semantically preferred parse meets the above cri-
teria, the single-best PCFG parse is used. We find that
the PCFG-preferred parse is used about 35% of the time
and an alternative used instead about 65% of the time.
The SRL performance for this regime, using a range of
cut-off factors, is shown in table 4. On this basis we se-
lect a cut-off of 0.5 as suitable for use for final testing.
On the development set this method gives the best pre-
cision in extracting dependencies, but is slightly inferior
to the method using a 2-best list on recall and balanced
F-measure.
Factor (?) P R F
0.5 86.3 71.9 78.5
0.1 85.4 72.0 78.1
0.05 85.2 72.0 78.0
0.005 84.3 71.3 77.3
Table 4: SRL accuracy when the semantic model is constrained
by the baseline model
9 Results and Discussion
We use the method for calculating SRL performance de-
scribed in the CoNNL 2008 and 2009 shared tasks. How-
ever, because the semantic role labeler we use was not de-
signed to work with Nombank (and it is difficult to sepa-
rate Nombank and Propbank predicates from the publicly
released shared task output), it is not feasible to compare
results with the candidate systems described there. We
can, however, compare our two experimental models with
our baseline parser and the current state-of-the-art CCG
741
Arg1 Arg1 Arg1 mod rel Arg2
Rudolph Agnew, 61 and the former chairman, was named a nonexecutive director
np np conj np/n n/n n (s\np)/(s\np) (s\np)/np np/n n/n n
> >
n/n n/n
> >np np
<?> >
np s\np
<?> >
np s\np
<s
Figure 7: A parse produced by the unrestricted semantic model. Notice that Rudolph Agnew, 61 and the former chairman is
erroneously treated as a three-way conjunction, assigning semantic roles to all three heads.
Arg0 Arg0 rel Arg1
Neither Lorillard nor the researchers who studied the workers were aware
np/np np conj np (np\np)/(s\np) (s\np)/np np (s\np)/(s\np) s\np
<?> > >
np s\np s\np
>
np\np
<np
>np
>s
Figure 8: Relative clause attachment poses problems when preceded by a conjunction ? the system generally prefers attaching
relative clauses high. In this case, the relative clause should be attached low.
parser (Clark and Curran, 2004). The results on the test
set (WSJ Section 23, <40 words) are shown in Table 5.
There are many areas for potential improvement for the
system. The test set scores of both of our experimental
models are lower than their development set scores,where
the n-best model outperforms even the Clark and Curran
parser in the SRL task. This may be due to vocabulary
issues (we are of course unable to evaluate if the vocab-
ulary of the training set more closely resembles the de-
velopment set or the test set). If there are vocabulary is-
sues, they could be alleviated by experimenting with POS
based lexical features, or perhaps even generalizing a la-
tent semantics over heads of semantic roles (essentially
identifying broad categories of words that appear with
particular semantic roles, rather than counting on having
encountered that particular word in training). Alternately,
this drop in performance could be caused by a mismatch
in the average length of sentences, which would cause our
? factor and the size of our n-best lists (which were tuned
on the development set) to be suboptimal. We anticipate
the opportunity to further explore better ways of deter-
mining n-best list size. We also anticipate the possibility
of integrating the semantic model with a state-of-the-art
CCG parser, potentially freeing the ranker from the limi-
tations of a simple PCFG baseline.
It is also worth noting that the chart-based model seems
heavily skewed towards precision. Because the parser can
dig deeply into the chart, it is capable of choosing a parse
that predicts only semantic roles that it is highly confi-
dent about. By choosing these parses (and not parses with
less attractive semantic roles), the model can maximize
the average score of the semantic roles it predicts. This
tendency towards identifying only the most certain roles
is consistent with high-precision low-recall results. The
n-best parser has a much more restricted set of semantic
roles from parses more closely resembling the single-best
parse, and therefore is less likely to be presented with the
opportunity to choose parses that do away with less likely
(but still reasonable) roles.
10 Conclusions and Future Work
In this paper, we discuss the procedure for identifying se-
mantic roles at parse time, and using these roles to guide
the parse. We demonstrate that using semantic roles to
guide parsing can improve overall SRL performance, but
that these same benefits can be realized by re-ranking an
n-best list with the same model. Regardless, there are
several reasons why it is useful to have the ability to pre-
dict semantic roles inside the chart.
Predicting semantic roles inside the chart could be used
to perform SRL on very long or unstructured passages.
742
SRL Labeled Deps
Model P R F P R F
Baseline 84.7 70.7 77.0 80.0 79.8 79.9
Rank n=5 82.0 73.7 77.7 80.1 80.0 80.0
Chart 90.0 68.4 77.7 82.3 80.2 81.2
C&C 83.3 77.6 80.4 84.9 84.6 84.7
Char 77.1 75.5 76.5 - - -
Table 5: The full system results on the test set of the WSJ
corpus (Section 23). Included are the baseline parser, the n-
best reranking model from Section 7, the single-best chart-
unpacking model from Section 8, and the state-of-the-art C&C
parser. The final row shows the SRL performance obtained by
Punyakanok et al (2008) using the Charniak parser. Unfor-
tunately, their results are evaluated based on spans of words
(rather than headword labels), which interferes with direct com-
parison. The Charniak parser is a CFG-style parser, making la-
beled dependency non-applicable.
Most parsing research on the Penn Treebank (the present
work included) focuses on sentences of 40 words or less,
because parsing longer sentences requires an unaccept-
ably large amount of computing resources. In practice,
however, semantic roles are rarely very distant from their
predicates ? generally they are only a few words away;
often they are adjacent. In long sentences, the prediction
of an entire parse may be unnecessary for the purposes of
SRL.
The CKY parsing algorithm works by first predicting all
constituents spanning two words, then all constituents
spanning three words, then four, and so on until it pre-
dicts constituents covering the whole sentence. By setting
a maximum constituent size (say, ten or fifteen), we could
abandon the goal of completing a spanning analysis in fa-
vor of identifying semantic roles in the neighborhood of
their predicates, eliminating the need to unpack the chart
at all. This could be used to efficiently perform SRL on
poorly structured text or even spoken language transcrip-
tions that are not organized into discrete sentences. Doing
so would also eliminate the potentially noisy step of au-
tomatically separating out individual sentences in a larger
text. Alternately, roles predicted in the chart could even
be incorporated into a low-precision-high-recall informa-
tion retrieval system seeking a particular semantic rela-
tionship by scanning the chart for a particular semantic
role.
Another use for the packed forest of semantic roles could
be to predict complete sets of roles for a given sentence
using a constraint based method like integer linear pro-
gramming. Integer linear programming takes a large
number of candidate results (like semantic roles), and ap-
plies a set of constraints over them (like ?roles may not
overlap? or ?no more than one of each role is allowed in
each sentence?) to find the optimal set. Doing so could
eliminate the need to unpack the chart at all, effectively
producing semantic roles without committing to a single
syntactic analysis.
11 Acknowledgements
We would like to thank Mike White, William Schuler,
Eric Fosler-Lussier, and Matthew Honnibal for their help-
ful feedback.
References
Stephen A. Boxwell, Dennis N. Mehay, and Chris Brew. 2009.
Brutus: A semantic role labeling system incorporating CCG,
CFG, and Dependency features. In Proc. ACL-09.
E. Charniak. 2001. Immediate-head parsing for language mod-
els. In Proc. ACL-01, volume 39, pages 116?123.
Stephen Clark and James R. Curran. 2004. Parsing the WSJ
using CCG and Log-Linear Models. In Proc. ACL-04.
J.R. Finkel and C.D. Manning. 2009. Joint parsing and named
entity recognition. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational Lin-
guistics, pages 326?334. Association for Computational Lin-
guistics.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying se-
mantic roles using Combinatory Categorial Grammar. In
Proc. EMNLP-03.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A. Mart??,
L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?, J. S?te?pa?nek, et al
2009. The CoNLL-2009 shared task: Syntactic and seman-
tic dependencies in multiple languages. In Proceedings of
the Thirteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Association for
Computational Linguistics.
J. Hockenmaier and M. Steedman. 2005. CCGbank manual.
Technical report, MS-CIS-05-09, University of Pennsylva-
nia.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,
B. Young, and R. Grishman. 2004. The nombank project:
An interim report. In A. Meyers, editor, HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic Roles.
Computational Linguistics, 31(1):71?106.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008. The
Importance of Syntactic Parsing and Inference in Semantic
Role Labeling. Computational Linguistics, 34(2):257?287.
Mark Steedman. 2000. The Syntactic Process. MIT Press.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In Proceedings
743
of the Twelfth Conference on Computational Natural Lan-
guage Learning, pages 159?177. Association for Computa-
tional Linguistics.
744
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200?210,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Towards Effective Tutorial Feedback for Explanation Questions:
A Dataset and Baselines
Myroslava O. Dzikovska? and Rodney D. Nielsen? and Chris Brew?
?School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, UK
?Computational Language & Education Research Center
University of Colorado at Boulder, Boulder, CO 80309-0594, USA
?Educational Testing Service, Princeton, NJ 08451, USA
m.dzikovska@ed.ac.uk, rodney.nielsen@colorado.edu, cbrew@ets.org
Abstract
We propose a new shared task on grading stu-
dent answers with the goal of enabling well-
targeted and flexible feedback in a tutorial di-
alogue setting. We provide an annotated cor-
pus designed for the purpose, a precise speci-
fication for a prediction task and an associated
evaluation methodology. The task is feasible
but non-trivial, which is demonstrated by cre-
ating and comparing three alternative baseline
systems. We believe that this corpus will be
of interest to the researchers working in tex-
tual entailment and will stimulate new devel-
opments both in natural language processing
in tutorial dialogue systems and textual entail-
ment, contradiction detection and other tech-
niques of interest for a variety of computa-
tional linguistics tasks.
1 Introduction
In human-human tutoring, it is an effective strategy
to ask students to explain instructional material in
their own words. Self-explanation (Chi et al, 1994)
and contentful talk focused on the domain are cor-
related with better learning outcomes (Litman et al,
2009; Chi et al, 1994). There has therefore been
much interest in developing automated tutorial dia-
logue systems that ask students open-ended expla-
nation questions (Graesser et al, 1999; Aleven et
al., 2001; Jordan et al, 2006; VanLehn et al, 2007;
Nielsen et al, 2009; Dzikovska et al, 2010a). In
order to do this well, it is not enough to simply
ask the initiating question, because students need
the experience of engaging in meaningful dialogue
about the instructional content. Thus, systems must
respond appropriately to student explanations, and
must provide detailed, flexible and appropriate feed-
back (Aleven et al, 2002; Jordan et al, 2004).
In simple domains, we can adopt a knowledge en-
gineering approach and build a domain model and a
diagnoser, together with a natural language parser to
produce detailed semantic representations of student
input (Glass, 2000; Aleven et al, 2002; Pon-Barry
et al, 2004; Callaway et al, 2006; Dzikovska et al,
2010a). The advantage of this approach is that it
allows for flexible adaptation of feedback to a va-
riety of factors such as student performance. For
example, it is easy for the system to know if the
student made the same error before, and adjust its
feedback to reflect it. Moreover, this approach al-
lows for easy addition of new exercises : as long as
an exercise relies on the concepts covered by the do-
main model, the system can apply standard instruc-
tional strategies to each new question automatically.
However, this approach is significantly limited by
the requirement that the domain be small enough to
allow comprehensive knowledge engineering, and it
is very labor-intensive even for small domains.
Alternatively, we can adopt a data-driven ap-
proach, asking human tutors to anticipate in ad-
vance a range of possible correct and incorrect an-
swers, and associating each answer with an appro-
priate remediation (Graesser et al, 1999; Jordan et
al., 2004; VanLehn et al, 2007). The advantage
of this approach is that it allows more complex and
interesting domains and provides a good framework
for eliciting the necessary information from the hu-
man experts. A weakness of this approach, which
200
also arises in content-scoring applications such as
ETS?s c-rater (Leacock and Chodorow, 2003), is that
human experts find it extremely difficult to predict
with any certainty what the full range of student re-
sponses will be. This leads to a lack of adaptivity
and generality ? if the system designers have failed
to predict the full range of possibilities, students will
often receive the default feedback. It is frustrating
and confusing for students to repeatedly receive the
same feedback, regardless of their past performance
or dialogue context (Jordan, 2004).
Our goal is to address the weaknesses of the data-
driven approach by creating a framework for sup-
porting more flexible and systematic feedback. Our
approach identifies general classes of error, such as
omissions, incorrect statements and off-topic state-
ments, then aims to develop general remediation
strategies for each error type. This has the potential
to free system designers from the need to pre-author
separate remediations for each individual question.
A precondition for the success of this approach is
that the system be able to identify error types based
on the student response and the model answers.
A contribution of this paper is to provide a new
dataset that will enable researchers to develop clas-
sifiers specifically for this purpose. The hope is that
with an appropriate dataset the data-driven approach
will be flexible and responsive enough to maintain
student engagement. We provide a corpus that is la-
beled for a set of five student response types, develop
a precise definition of the corresponding supervised
classification task, and report results for a variety of
simple baseline classifiers. This will provide a ba-
sis for the development, comparison and evaluation
of alternative approaches to the error classification
task. We believe that the natural language capabil-
ities needed for this task will be directly applicable
to a far wider range of tasks in educational assess-
ment, information extraction and computational se-
mantics. This dataset is publicly available and will
be used in a community-wide shared task.
2 Corpus
The data set we developed draws on two established
sources ? a data set collected and annotated during
an evaluation of the BEETLE II tutorial dialogue sys-
tem (Dzikovska et al, 2010a) (henceforth, BEETLE
corpus) and a set of student answers to questions
from 16 science modules in the Assessing Science
Knowledge (ASK) assessment inventory (Lawrence
Hall of Science, 2006) (henceforth, the Science En-
tailments Bank or SCIENTSBANK).
In both corpora, each question was associated
with one or more reference answers provided by
the experts. Student answers were evaluated against
these reference answers and, using corpus-specific
annotation schemes, assigned labels for correct-
ness. In order to reconcile the two different schemes
and to cast the task in terms of standard supervised
machine classification at the sentence level, we de-
rived a new set of annotations, using the annotation
scheme shown in Figure 1.
Our label set has some similarity to the RTE5 3-
way task (Bentivogli et al, 2009), which used ?en-
tailment?, ?contradiction? and ?unknown? labels.
The additional distinctions in our labels reflect typi-
cal distinctions made by tutorial dialogue systems.
They match our human tutors? intuitions about
the general error types observed in student answers
and corresponding teaching tactics. For example,
a likely response to ?partially correct incomplete?
would be to tell the student that what they said so far
was correct but it had some gaps, and to encourage
them to fill in those gaps. In contrast, the response
to ?contradictory? would emphasize that there is a
mistake and the student needs to change their an-
swer rather than just expand it. Finally, the response
to ?irrelevant? may encourage the student to address
relevant concepts. The ?non domain? content could
be an indicator that the student is frustrated or con-
fused, and may require special attention.
The annotations in the source corpora make some
more fine-grained distinctions based on the needs of
the corresponding systems. In principle, it is possi-
ble to have answers that have both correct and con-
tradictory parts, and acknowledge correct parts be-
fore pointing out mistakes. There are also distinct
classes of ?non domain? utterances, e.g., social and
metacognitive statements, to which an ITS may want
to react differently (described in Section 2.1). How-
ever, these situations were rare in our corpora, and
we decided to use a single class for all contradictory
answers and a single non-domain class. This may be
expanded in the future as more data becomes avail-
able for new versions of this challenge task.
201
Label Definition
non domain does not contain domain content, e.g., a help request or ?I don?t know?
correct the student answer is correct
partially correct incomplete the answer does not contradict the reference answer and includes some
correct nuggets, but parts are missing
contradictory an answer that contradicts some part of the reference answer
irrelevant contains domain content, but does not answer the question
Figure 1: The set of answer labels used in our task
We further discuss the relationship with the task
of recognizing textual entailment in Section 5. In
the rest of this section, we describe our corpora and
discuss how we obtained these labels from the raw
data available in our datasets.
2.1 BEETLE II data
The BEETLE corpus consists of the interactions be-
tween students and the BEETLE II tutorial dialogue
system (Dzikovska et al, 2010b). The BEETLE II
system is an intelligent tutoring system that teaches
students with no knowledge of high-school physics
concepts in basic electricity and electronics. In the
first system evaluation, students spend 3-5 hours go-
ing through prepared reading material, building and
observing circuits in the simulator and interacting
with a dialogue-based tutor. The interaction was
by keyboard, with the computer tutor asking ques-
tions, receiving replies and providing feedback via a
text-based chat interface. The data from 73 under-
graduate volunteer participants at southeastern US
university were recorded and annotated to form the
BEETLE human-computer dialogue corpus.
The BEETLE II lesson material contains two types
of questions. Factual questions require them to name
a set of objects or a simple property, e.g., ?Which
components in circuit 1 are in a closed path?? or
?Are bulbs A and B wired in series or in parallel?.
Explanation and definition questions require longer
answers that consist of 1-2 sentences, e.g., ?Why
was bulb A on when switch Z was open?? (expected
answer ?Because it was still in a closed path with the
battery?) or ?What is voltage?? (expected answer
?Voltage is the difference in states between two ter-
minals?). From the full BEETLE evaluation corpus,
we automatically extracted only the students? an-
swers to explanation and definition questions, since
reacting to them appropriately requires processing
more complex input than factual questions.
The extracted answers were filtered to remove du-
plicates. In the BEETLE II lesson material there
are a number of similar questions and the tutor ef-
fectively had a template answer such as ?Terminal
X is connected to the negative/positive battery ter-
minal?. A number of students picked up on this
and used the same pattern in their responses (Stein-
hauser et al, 2011). This resulted in a number of an-
swers to certain questions that came from different
speakers but which were exact copies of each other.
We removed such answers from the data set, since
they were likely to be in both the training and test
set, thus inflating our results. Note that only exact
matches were removed: for example, answers that
were nearly identical but contained spelling errors
were retained, since they would need to be handled
in a practical system.
Student utterances were manually labeled using a
simplified version of the DEMAND coding scheme
(Campbell et al, 2009) shown in Figure 2. The utter-
ances were first classified as related to domain con-
tent, student?s metacognitive state, or social inter-
action. Utterances addressing domain content were
further classified with respect to their correctness as
described in the table. The Kappa value for this
annotation effort was ? = 0.69.
This annotation maps straightforwardly into our
set of labels. The social and metacognitive state-
ments are mapped to the ?non domain? label;
?pc some error?, ?pc? and ?incorrect? are mapped
to the ?contradictory? label; and the other classes
have a one-to-one correspondence with our task la-
bels.
2.2 SCIENTSBANK data
The SCIENTSBANK corpus (Nielsen et al, 2008)
consists of student responses to science assessment
202
Category Subcategory Description
Metacognitive positive
negative
content-free expressions describing student knowledge, e.g., ?I don?t
know?
Social positive
negative
neutral
expressions describing student?s attitudes towards themselves and
the computer (mostly negative in this data, e.g., ?You are stupid?)
Content the utterance addresses domain content.
correct the student answer is fully correct
pc some missing the student said something correct, but incomplete
incorrect the student?s answer is completely incorrect
pc some error the student?s answer contains correct parts, but some errors as well
pc the answer contains a mixture of correct, incorrect and missing parts
irrelevant the answer may be correct or incorrect, but it is not answering the
question.
Figure 2: Annotation scheme used in the BEETLE corpus
questions. Specifically, around 16k answers were
collected spanning 16 distinct science subject ar-
eas within physical sciences, life sciences, earth
sciences, space sciences, scientific reasoning and
technology. The tests were part of the Berke-
ley Lawrence Hall of Science Assessing Science
Knowledge (ASK) standardized assessments cover-
ing material from their Full Option Science System
(FOSS) (Lawrence Hall of Science, 2011). The an-
swers came from students in grades 3-6 in schools
across North America.
The tests included a variety of questions includ-
ing ?fill in the blank? and multiple choice, but the
SCIENTSBANK corpus only used a subset that re-
quired students to explain their beliefs about top-
ics, typically in one to two sentences. We reviewed
the questions and a sample of the responses and
decided to filter the following types of questions
from the corpus, because they did not mesh with
our goals. First, we removed questions whose ex-
pected answer was more than two full sentences
(typically multi-step procedures), which were be-
yond the scope of our task. Second, we removed
questions where the expected answer was ill-defined
or very open-ended. Finally, the most frequent rea-
son for removing questions was an extreme imbal-
ance in the answer classifications (e.g., for many
questions, almost all of the answers were labeled
?partially correct incomplete?). Specifically, we re-
moved questions where more than 80% of the an-
swers had the same label and questions with fewer
than three correct answers, since these questions
were unlikely to be useful in differentiating between
the quality of assessment systems.
The SCIENTSBANK corpus was developed for the
purpose of assessing student responses at a very fine-
grained level. The reference answers were broken
down into several facets, which consisted roughly
of two key terms and the relation connecting them.
Nielsen et al annotated student responses to indicate
for each reference answer facet whether the response
1) implied the student understood the facet, 2) im-
plied they held a contradictory belief, 3) included a
related, non-contradicting facet, or 4) left the facet
unaddressed. Reported agreement was 86.2% with
a kappa statistic (Cohen, 1960) of 0.728, which is in
the range of substantial agreement.1
Because our task focuses on answer classifica-
tion rather than facet classification, we developed a
set of rules indicating which combinations of facets
constituted a correct answer. We were then able
to compute an answer label from the gold-standard
facet annotations, as follows. First, if any facet
was annotated as contradictory, the answer was also
labeled ?contradictory?. Second, if all of the ex-
pected facets for any valid answer were annotated
as being understood, the answer was labeled ?cor-
1These statistics were actually based on five labels, but we
chose to combine the fifth, a self-contradiction, with other con-
tradictions for the purposes of our task.
203
rect?. Third, the remaining answers that included
some but not all of the expected facets were la-
beled ?partially correct incomplete?. Fourth, if an
answer matched none of the expected facets, and
had not been previously labeled as ?contradictory? it
was given the label ?irrelevant?. Finally, all ?irrele-
vant? answers were reviewed manually to determine
whether they should be relabeled as ?non domain?.
However, since Nielsen et al had already removed
most of the responses that originally fell into this
category, we only found 24 ?non domain? answers.
3 Baselines
We established three baselines for our data set ? a
straightforward majority class baseline, an existing
system baseline (BEETLE II system performance,
which we report only for the BEETLE portion of the
dataset), and the performance of a simple classifier
based on lexical similarity, which we report in order
to offer a substantial example of applying the same
classifier to both portions of the dataset.
3.1 BEETLE II system baseline
The interpretation component of the BEETLE II
system uses a syntactic parser and a set of hand-
authored rules to extract the domain-specific se-
mantic representations of student utterances from
the text. These representations were then matched
against the semantic representations of expected cor-
rect answers supplied by tutors. The resulting sys-
tem output was automatically mapped into our target
labels as discussed in (Dzikovska et al, 2012).
3.2 Lexical similarity baseline
To provide a higher baseline that is compara-
ble across both subsets of the data, we built
a simple decision tree classifier using the Weka
3.6.2 implementation of C4.5 pruned decision trees
(weka.classifiers.trees.J48 class), with default pa-
rameters. As features, we used lexical similar-
ity scores computed by the Text::Similarity
package with default parameters2. The code com-
putes four similarity metrics ? the raw number of
overlapping words, F1 score, Lesk score and cosine
score. We compared the learner response to the ex-
pected answer(s) and the question, resulting in eight
2http://search.cpan.org/dist/Text-Similarity/
total features (the four values indicated above for
the comparison with the question and the highest of
each value from the comparisons with each possible
expected answer).
This baseline is based on the lexical overlap base-
line used in RTE tasks (Bentivogli et al, 2009).
However, we measured overlap with the question
text in addition to the overlap with the expected
answers. Students often repeat parts of the ques-
tion in their answer and this needs to be taken
into account to differentiate, for example, ?par-
tially correct incomplete? and ?correct? answers.
4 Results
4.1 Experimental Setup
We held back part of the data set for use as standard
test data in the future challenge tasks. For BEETLE,
this consisted of all student answers to 9 out of 56
explanation questions asked by the system, plus ap-
proximately 15% of the student answers to the re-
maining 47 questions, sampling so that the distribu-
tion of labels in test data was similar to the training
data. For SCIENTSBANK, we used a previous train-
test split (Nielsen et al, 2009). For both data sets,
the data was split so that in the future we can test
how well the different systems generalize: i.e., how
well they perform on answers to questions for which
they have some sample student answers vs. how
well they perform on answers to questions that were
not in the training data (e.g., newly created questions
in a deployed system). We discuss this in more detail
in Section 5.
In this paper, we report baseline performance on
the training set to demonstrate that the task is suf-
ficiently challenging to be interesting and that sys-
tems can be compared using our evaluation met-
rics. We preserve the true test data for use in the
planned large-scale system comparisons in a com-
munity shared task.
For the lexical similarity baseline, we use 10-fold
cross-validation.3 For the BEETLE II system base-
line, the language understanding module was de-
3We did not take the student id into account explicitly during
cross-validation. While there is some risk that the classifiers
will learn features specific to the student, we concluded (based
on our understanding of data collection specifics for both data
sets) that there is little enough overlap in cross-validation on the
training data that this should not have a big effect on the results.
204
veloped based on eight transcripts, each taken from
the interaction of a different student with an earlier
version of the system. These sessions were com-
pleted prior to the beginning of the experiment dur-
ing which the BEETLE corpus was collected, and are
not included in the corpus presented here. Thus, the
dataset used in the paper constitutes unseen data for
the BEETLE II system.
We process the two corpora separately because
the additional system baseline is available for bee-
tle, and because the corpora may be different enough
that it will be helpful for shared task participants to
devise processing strategies that are sensitive to the
provenance of the data.
4.2 Evaluation Metrics
Table 1 shows the distribution of codes in the anno-
tated data. The distribution is unbalanced, and there-
fore in our evaluation results we report per-class pre-
cision, recall and F1 scores, plus the averaged scores
using two different ways to average over per-class
evaluation scores, micro- and macro- averaging.
For a set of classes C, each represented with Nc
instances in the test set, the macro-averaged recall is
defined as
Rmacro =
1
|C|
?
cC
R(c)
and the micro-averaged recall as
Rmicro =
?
cC
1
Nc
R(c)
Micro- and macro-averaged precision and F1 are de-
fined similarly.
Micro-averaging takes class sizes into account, so
a system that performs well on the most common
classes will have a high micro-average score. This is
the most commonly used classifier evaluation met-
ric. Note that, in particular, overall classification
accuracy (defined as the number of correctly clas-
sified instances out of all instances) is mathemat-
ically equivalent to micro-averaged recall (Abuda-
wood and Flach, 2011). However, macro-averaging
better reflects performance on small classes, and is
commonly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)). We report both val-
ues in our results.
BEETLE SCIENTSBANK
Label Count Freq. Count Freq.
correct 1157 0.42 2095 0.40
partially correct
incomplete
626 0.23 1431 0.27
contradictory 656 0.24 526 0.10
irrelevant 86 0.03 1175 0.22
non domain 204 0.07 24 0.005
total 2729 5251
Table 1: Distribution of annotated labels in the data
In addition, we report the system scores on the bi-
nary decision of whether or not the corrective feed-
back should be issued (denoted ?corrective feed-
back? in the results table). It assumes that a tutoring
system using a classifier will give corrective feed-
back if the classifiers returns any label other than
?correct?. Thus, every instance classified as ?par-
tially correct incomplete?, ?contradictory?, ?irrele-
vant? or ?non domain? is counted as true positive
if the hand-annotated label also belongs to this set
(even if the classifier disagrees with the annotation);
and as false positive if the hand-annotated label is
?correct?. This reflects the idea that students are
likely to be frustrated if the system gives corrective
feedback when their answer is in fact a fully accurate
paraphrase of a correct answer.
4.3 BEETLE baseline performance
The detailed evaluation results for all baselines are
presented in Table 2.
The majority class baseline is to assign ?correct?
to every test instance. It achieves 42% overall ac-
curacy. However, this is obviously at the expense
of serious errors; for example, such a system would
tell the students that they are correct if they are say-
ing something contradictory. This is reflected in a
much lower macro-averaged F1 score.
The BEETLE II system performs only slightly bet-
ter than the baseline on the overall accuracy (0.44
vs. 0.42 micro-averaged recall). However, the
macro-averaged F1 score of the BEETLE II system
is substantially higher (0.46 vs. 0.12). The micro-
averaged results show a similar pattern, although the
majority-class baseline performs slightly better than
in the macro-averaged case, as expected.
Comparing the BEETLE II parser to our lexical
205
similarity baseline, BEETLE II has lower overall ac-
curacy, but performs similarly on micro- and macro-
averaged scores. BEETLE II precision is higher than
that of the classifier in all cases except for the binary
decision as to whether corrective feedback should
be issued. This is not unexpected given how the sys-
tem was designed ? since misunderstandings caused
dialogue breakdown in pilot tests, the parser was
built to prefer rejecting utterances as uninterpretable
rather than assigning them an incorrect class, lead-
ing to a considerably lower recall. Around 31% of
utterances could not be interpreted.
Our recent analysis shows that both incorrect
interpretations (in particular, confusions between
?partially correct incomplete? and ?contradictory?)
and rejections have significant negative effects on
learning gain (Dzikovska et al, 2012). Classifiers
can be tuned to reject examples where classification
confidence falls below a given threshold, resulting
in precision-recall trade-offs. Our baseline classifier
classified all answer instances; exploring the possi-
bilities for rejecting some low-confidence answers is
planned for future work.
4.4 SCIENTSBANK baseline performance
The accuracy of the majority class baseline (which
assumes all answers are ?correct?) is 40% for SCI-
ENTSBANK, about the same as it was for BEE-
TLE. The evaluation results, based on 10-fold cross-
validation, for our simple lexical similarity classi-
fier are presented in Table 3. The lexical similar-
ity based classifier outperforms the majority class
baseline by 0.18 and 3% on the macro-averaged
F1-measure and accuracy, respectively. The F1-
measure for the two-way classification detecting an-
swers which need corrective feedback is 0.66.
The scores on SCIENTSBANK are noticeably
lower than those for BEETLE. The SCIENTSBANK
includes questions from 12 distinct science subject
areas, rather than a single area as in BEETLE. This
decision tree classifier learns a function from the
eight text similarity features to the desired answer
label. Because the features do not mention particular
words, the model can be applied to items other than
the ones on which it was trained, and even to items
from different subject areas. However, the correct
weighting of the textual similarity features depends
on the extent and nature of the expected textual over-
Predictn correct pc inc contra irrlvnt nondom
correct 1213 553 209 392 2
pc inc 432 497 128 241 2
contra 115 109 58 74 3
irrlvnt 335 272 131 468 17
nondom 0 0 0 0 0
Figure 4: Confusion matrix for lexical classifier on SCI-
ENTSBANK. Predictions in rows, gold labels in columns
lap, which does vary from subject-area to subject-
area. We suspect that the differences between sub-
ject areas made it hard for the decision-tree classi-
fier to find a single, globally appropriate strategy.
Nielsen (2009) reported the best results for classify-
ing facets when training separate question-specific
or even facet-specific classifiers. Although separate
training for each item reduces the amount of relevant
training data for each classifier, it allows each clas-
sifier to learn the specifics of how that item works.
A comparison using this style of training would be a
reasonable next step,
5 Discussion and Future Work
The results presented satisfy two critical require-
ments for a challenge task. First, we have shown that
it is feasible to develop a system that performs sig-
nificantly better than the majority class baseline. On
the macro-averaged F1-measure, our lexical clas-
sifier outperformed the majority-class baseline by
0.33 (on BEETLE) and 0.18 (on SCIENTSBANK)
and by 13% and 3% on accuracy. Second, we have
also shown, as is desired for a challenge task, that
the task is not trivial. With a system specifically
designed to parse the BEETLE corpus answers, the
macro-averaged F1-measure was just 0.46 and on
the binary decision regarding whether the response
needed corrective feedback, it achieved just 0.63.
One contribution of this work was to define a gen-
eral classification scheme for student responses that
allows more specific learner feedback. Another key
contribution was to unify two, previously incom-
patible, large student response corpora under this
common annotation scheme. The resultant corpus
will enable researchers to train learning algorithms
to classify student responses. These classifications
can then be used by a dialogue manager to generate
targeted learner feedback. The corpus is available
206
Classifier: majority lexical similarity BEETLE II
Predicted label prec. recall F1 prec. recall F1 prec. recall F1
correct 0.42 1.00 0.60 0.68 0.75 0.72 0.93 0.53 0.68
partially correct incomplete 0.00 0.00 0.00 0.41 0.38 0.39 0.43 0.53 0.47
contradictory 0.00 0.00 0.00 0.39 0.34 0.36 0.58 0.23 0.33
irrelevant 0.00 0.00 0.00 0.05 0.02 0.03 0.23 0.17 0.20
non domain 0.00 0.00 0.00 0.66 0.82 0.73 0.92 0.46 0.61
macroaverage 0.09 0.20 0.12 0.44 0.46 0.45 0.62 0.39 0.46
microaverage 0.18 0.42 0.25 0.53 0.55 0.54 0.71 0.44 0.53
corrective feedback 0.00 0.00 0.00 0.80 0.74 0.77 0.73 0.56 0.63
Table 2: Evaluation results for BEETLE corpus
Classifier: lexical similarity BEETLE II
Predicted label corrct pc inc contra irrlvnt nondom corrct pc inc contra irrlvnt nondom
correct 870 187 199 20 2 617 20 23 0 3
part corr incmp 138 239 178 24 11 249 332 146 29 20
contradictory 139 153 221 33 22 68 38 149 3 0
irrelevant 3 20 12 2 1 4 22 23 15 1
non domain 7 27 46 7 168 3 3 1 1 94
uninterpretable n/a n/a n/a n/a n/a 216 211 314 38 86
Figure 3: Confusion matrix for BEETLE corpus. Predictions in rows, gold labels in columns
Classifier: baseline lexical similarity
Predicted label prec. recall F1 prec. recall F1
correct 0.40 1.00 0.57 0.51 0.58 0.54
partially correct incomplete 0.00 0.00 0.00 0.38 0.35 0.36
contradictory 0.00 0.00 0.00 0.16 0.11 0.13
irrelevant 0.00 0.00 0.00 0.38 0.40 0.39
non domain 0.00 0.00 0.00 0.00 0.00 0.00
macroaverage 0.08 0.20 0.11 0.29 0.29 0.29
microaverage 0.16 0.40 0.23 0.41 0.43 0.42
corrective feedback 0.00 0.00 0.00 0.69 0.63 0.66
Table 3: Evaluation results for SCIENTSBANK baselines
207
for general research purposes and forms the basis
of SEMEVAL-2013 shared task ?Textual entailment
and paraphrasing for student input assessment?.4
A third contribution of this work was to provide
basic evaluation benchmark metrics and the corre-
sponding evaluation scripts (downloadable from the
site above) for other researchers, including shared
task participants. This will facilitate the comparison
and, hence, the progress, of research.
The work reported here is based on approximately
8000 student responses to questions covering 12 dis-
tinct science subjects and coming from a wide range
of student ages. These responses comprise the train-
ing data for our task. The vast majority of prior
work, including BEETLE II, which was included as
a benchmark here, has been designed to provide ITS
feedback for relatively small, well-defined domains.
The corpus presented in this paper is intended to en-
courage research into more generalizable, domain-
independent techniques. Following Nielsen (2009),
from whom the SCIENTSBANK corpus was adapted,
our shared task evaluation corpus will be composed
of three types of data: additional student responses
for all of the questions in the training data (Un-
seen Answers), student responses to questions that
were not seen in the training data, but that are from
the same subject areas (Unseen Questions), and re-
sponses to questions from three entirely different
subject areas (Unseen Domains), though in this case
the questions are still from the same general domain
? science. Unseen Answers is the typical scenario
for the vast majority of prior work ? training and
testing on responses to the same questions. Unseen
Questions and Unseen Domains allow researchers to
evaluate how well their systems generalize to near
and far domains, respectively.
The primary target application for this work is in-
telligent tutoring systems, where the classification of
responses is intended to facilitate specific pedagogic
feedback. Beneath the surface, the baseline systems
reported here are more similar to grading systems
that use the approach of (Leacock and Chodorow,
2003), which uses classifier technology to detect ex-
pressions of facet-like concepts, then converts the
result to a numerical score, than to grading systems
like (Mohler et al, 2011), which directly produces a
4See http://www.cs.york.ac.uk/semeval-2013/task4/
numerical score, using support vector regression and
similar techniques. Either approach is reasonable,
but we think that feedback is the more challeng-
ing test of a system?s ultimate abilities, and there-
fore a better candidate for the shared task. The cor-
pora from those systems, alongside with new cor-
pora currently being collected in BEETLE and SCI-
ENTSBANK domains, can serve as sources of data
for future tasks extensions.
Future systems developed for this task can benefit
from the large amount of existing work on recog-
nizing textual entailment (Giampiccolo et al, 2007;
Giampiccolo et al, 2008; Bentivogli et al, 2009)
and on detecting contradiction (Ritter et al, 2008;
De Marneffe et al, 2008). However, there are sub-
stantial challenges in applying the RTE tools directly
to this data set. Our set of labels is more fine-grained
than RTE labels to reflect the needs of intelligent tu-
toring systems (see Section 2). In addition, the top-
performing systems in RTE5 3-way task, as well as
contradiction detection methods, rely on NLP tools
such as dependency parsers and semantic role la-
belers; these do not perform well on specialized
terminology and language constructs coming from
(typed) dialogue context. We chose to use lexical
similarity as a baseline specifically because a simi-
lar measure was used as a standard baseline in RTE
tasks, and we expect that adapting the more complex
RTE approaches for purposes of this task will result
in both improved results on our data set and new de-
velopments in computational linguistics algorithms
used for RTE and related tasks.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation. The research
reported here was supported by the US ONR award
N000141010085 and by the Institute of Education
Sciences, U.S. Department of Education, through
Grant R305A110811 to Boulder Language Tech-
nologies Inc. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education.
208
References
Tarek Abudawood and Peter Flach. 2011. Learn-
ing multi-class theories in ilp. In The 20th Interna-
tional Conference on Inductive Logic Programming
(ILP?10). Springer, June.
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Vincent Aleven, Octav Popescu, and Koedinger
Koedinger. 2002. Pilot-testing a tutorial dialogue
system that supports self-explanation. Lecture Notes
in Computer Science, 2363:344?354.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Charles Callaway, Myroslava Dzikovska, Colin Mathe-
son, Johanna Moore, and Claus Zinn. 2006. Using
dialogue to learn math in the LeActiveMath project.
In Proceedings of the ECAI Workshop on Language-
Enhanced Educational Technology, pages 1?8, Au-
gust.
Gwendolyn C. Campbell, Natalie B. Steinhauser, My-
roslava O. Dzikovska, Johanna D. Moore, Charles B.
Callaway, and Elaine Farrow. 2009. The DeMAND
coding scheme: A ?common language? for represent-
ing and analyzing student discourse. In Proceedings
of 14th International Conference on Artificial Intelli-
gence in Education (AIED), poster session, Brighton,
UK, July.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):3746.
M.C. De Marneffe, A.N. Rafferty, and C.D. Manning.
2008. Finding contradictions in text. Proceedings of
ACL-08: HLT, pages 1039?1047.
Myroslava Dzikovska, Diana Bental, Johanna D. Moore,
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Elaine Farrow, and Charles B. Callaway. 2010a. In-
telligent tutoring with natural language support in the
Beetle II system. In Sustaining TEL: From Innovation
to Learning and Practice - 5th European Conference
on Technology Enhanced Learning, (EC-TEL 2010),
Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-
2010) demo session, Uppsala, Sweden, July.
Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-
hanna D. Moore. 2012. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
computational Linguistics, Avignon, France, April.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9, Prague, June.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pro-
ceedings of the AAAI Fall Symposium on Building Di-
alogue Systems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, K. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language un-
derstanding approaches in an intelligent tutoring sys-
tem. In James C. Lester, Rosa Maria Vicari, and
Fa?bio Paraguac?u, editors, Intelligent Tutoring Systems,
volume 3220 of Lecture Notes in Computer Science,
pages 346?357. Springer.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Pamela W. Jordan. 2004. Using student explanations
as models for adapting tutorial dialogue. In Valerie
Barr and Zdravko Markov, editors, FLAIRS Confer-
ence. AAAI Press.
Lawrence Hall of Science. 2006. Assessing Science
Knowledge (ask). University of California at Berke-
ley, NSF-0242510.
Lawrence Hall of Science. 2011. Full option science
system.
209
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
David D. Lewis. 1991. Evaluating text categorization. In
Proceedings of the workshop on Speech and Natural
Language, HLT ?91, pages 312?318, Stroudsburg, PA,
USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proceedings of 14th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Brighton, UK, July.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. The Journal of Natural Language Engineer-
ing, 15:479?501.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proceedings of ITS-
2004, pages 390?400.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction?no, it?s not: a case study
using functional relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 11?20.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proceed-
ings of the 15th International Conference on Artificial
Intelligence in Education (AIED-2011).
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technol-
ogy in Education, Farmington, PA, October.
210
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 210?221,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
CCG Syntactic Reordering Models for Phrase-based Machine Translation
Dennis N. Mehay
The Ohio State University
Columbus, OH, USA
mehay@ling.ohio-state.edu
Chris Brew
Educational Testing Service
Princeton, NJ, USA
cbrew@ets.org
Abstract
Statistical phrase-based machine translation
requires no linguistic information beyond
word-aligned parallel corpora (Zens et al,
2002; Koehn et al, 2003). Unfortunately,
this linguistic agnosticism often produces un-
grammatical translations. Syntax, or sentence
structure, could provide guidance to phrase-
based systems, but the ?non-constituent? word
strings that phrase-based decoders manipu-
late complicate the use of most recursive syn-
tactic tools. We address these issues by
using Combinatory Categorial Grammar, or
CCG, (Steedman, 2000), which has a much
more flexible notion of constituency, thereby
providing more labels for putative non-
constituent multiword translation phrases. Us-
ing CCG parse charts, we train a syntactic
analogue of a lexicalized reordering model by
labelling phrase table entries with multiword
labels and demonstrate significant improve-
ments in translating between Urdu and En-
glish, two language pairs with divergent sen-
tence structure.
1 Introduction
Statistical phrase-based machine translation (PMT)
is attractive, as it requires no linguistic informa-
tion beyond word-aligned parallel corpora (Zens et
al., 2002; Koehn et al, 2003). Unfortunately, this
linguistic agnosticism leaves phrase-based systems
with no precise characterization of the word order
relationships between languages, often leading to
ungrammatical translations. Syntax could provide
guidance to phrase-based systems, by steering them
towards reorderings that reflect the structural rela-
tionships between languages, but using syntax to
guide a phrase-based system is problematic. Phrase-
based systems build the result incrementally from
the beginning of the target string to the end, and
the intermediate strings need not constitute complete
traditional syntactic constituents. It is difficult to
reconcile traditional recursive syntactic processing
with this regime, because not all intermediate strings
considered by the decoder would even have a syntac-
tic category to assess. As a result, most phrase-based
decoders control reordering using simple distance-
based distortion models, which penalize all reorder-
ing equally, and lexicalized reordering models (Till-
mann, 2004; Axelrod et al, 2005), which probabilis-
tically score various reordering configurations con-
ditioned on specific lexical translations. While un-
doubtedly better than nothing, these models perform
poorly when languages diverge considerably in sen-
tence structure. Distance-based distortion models
are too coarse-grained to distinguish correct from
incorrect reordering, while lexical reordering mod-
els suffer from data sparsity and fail to capture more
general patterns. We argue that finding a way to
label translation phrases with syntactic labels will
abstract over the observed reordering configurations
thereby address both all three deficiencies of granu-
larity, data sparsity and lack of generality.
The present work presents a novel syntactic ana-
logue of the lexicalized reordering model that uses
multiword syntactic labels to capture the general re-
ordering patterns between two languages with very
different word order. We accomplish this by using
Combinatory Categorial Grammar, or CCG (Steed-
210
man, 2000), a word-centered syntax that allows a
great deal of flexibility in how sentence analyses
are formed. Syntactic derivations in CCG are mas-
sively spuriously ambiguous, i.e., there are many
ways to derive the same semantic analysis of a sen-
tence, similar to how a mathematical equation can
be reduced by canceling out variables in different
orders. Despite its name, spurious ambiguity is a
benefit to us, as it provides many different labelled
bracketings for the same dependency graph of the
same sentence, thereby increasing the chance that
any substring of that sentence will have a syntactic
label. Our approach exploits this property of CCG
to derive multiword CCG syntactic labels for target
translation strings in a phrase table, thus providing a
firmer basis on which to collect syntactic reordering
statistics. In particular:
? We show how CCG can derive constituent la-
bels for target-side phrase-table entries that
are often lamented as ?non-constituents? or as
?crossing a phrase boundary?.
? Our CCG categories are not limited to single-
word supertags. Rather, as these labels are
drawn from CCG parse charts, they can span
multiple words. Further, the labels are tailored
specifically to each translation constituent?s
boundaries (Section 2.1). As a consequence,
?70% of phrase table entries receive a single
syntactic label (Section 5), largely removing
the terminological inconsistency of calling lex-
ical translation constituents ?phrases?. Now,
more of them actually are syntactic phrases.
? We use these labels to train a target-language
bidirectional reordering model over CCG syn-
tactic sequences (Section 3), which, when
added to the baseline system, is found to be su-
perior to systems that use both lexicalized re-
ordering models and supertag reordering mod-
els (Section 5).
With only minor modifications, we incorporate these
enhancements into a state-of-the-art PMT decoder
(Koehn et al, 2007), achieving significant improve-
ments over two competitive baselines in an Urdu-
English translation task (Sections 5). This language
pair was chosen to highlight the promise of this ap-
proach for languages with considerable, but syntac-
tically governed, word-order differences to one an-
other. Finally, in a small discussion we provide qual-
itative evidence that the improvements in automatic
metric scores correspond to real gains in target lan-
guage fluency.
2 Syntax, Constituency and Phrase-based
MT
Consider the following German-English PMT
phrase pair that we have extracted from a parallel
European parliamentary transcript:1
Ich hoffe, da? ? I hope that
Neither word string is a well-formed constituent in
traditional theories of syntax. But tradition is at odds
with the intuition that that such ?non-constituent?
sequences are still well-formed substrings, governed
by rules of how they can be combined with other
word strings ? e.g., declarative sentence translation
rules like es mo?glich sein wird ? it will be possible
can grammatically extend each, but a noun phrase
rule cannot.
As Figure 1 illustrates, putative non-constituent
word sequences abound in phrase-based MT. Here a
translation ?phrase? is simply any contiguous word
string that is consistent with a word alignment (a
relation between source and target words), usually
produced by a language-independent alignment pro-
cedure (Zens et al, 2002). The figure also high-
lights the need for linguistic syntax in controlling
how translations are assembled; the successful trans-
lation is merely one among many possible reorder-
ings, many of which (despite their ungrammatical-
ity) might score well on a word n-gram model. But
rather than changing the word alignments or PMT
?phrase? boundaries to fit a syntactic theory, we
choose to use a flexible syntax which can produce a
wider range of bracketings to accommodate the re-
sults of alignment-derived translations. To this end,
we use Combinatory Categorial Grammar, or CCG,
(Steedman, 2000). To understand how CCG allows
this, we illustrate its use with some simple examples.
1Throughout this paper, the term ?PMT phrase? refers to an
unbroken sequence of words used by a PMT system, whereas
?phrase? (without context) refers to a syntactic constituent.
211
Wiederaufnahme der Situngsperiode
Resumption of the session
Ich hoffe , da? es mo?glich sein wird
I hope that it will be possible
Ich hoffe, da? das den Weg fu?r eine baldige Wiederaufnahme der Debatte ebnen wird
I hope that this will pave the way for an early resumption of the debate
Figure 1: Two phrase-based MT word groups are extracted from aligned words (the dashed outlines) and then used to form a new
translation (bottom). [Adapted from parallel sentences in the Europarl German-English corpus, v6.]
2.1 CCG, Spurious Ambiguity and PMT:
Turning ?Phrases? into Phrases
CCG is a derivational syntax, where words are as-
signed a lexical category2 and sentence structures
are then recursively built using a small set of de-
ductive rule schemata known as combinators (Steed-
man, 2000). Lexical syntactic categories can be
richly structured in CCG, indicating how words can
combine. A syntactic category of the form X/Y,
e.g., states that a category of type X can be formed if
combined with a Y to its right ? i.e., a function from
rightward Ys to X. This can be accomplished with
the forward function application combinator (>),3
which is written in derivational form as follows:4
X/Y Y
>
X
This derivation of the symbol X is known as the
normal-form derivation (Steedman, 2000), since it
uses function application whenever possible. But
CCG has the ability to construct the same result
by using a different, non-normal-form sequence of
combinatory inferences. For example, by using the
backward type-raising combinator (T<) and then
backward function application (<), we can arrive at
the same result:
2When represented by a strings, lexical categories are called
supertags.
3CCG actually respects the rule-to-rule hypothesis (Bach,
l976), where, for every syntactic term built, there is a corre-
sponding semantic term, but, for simplicity of exposition, we
focus only on syntax here.
4The reader will notice that CCG derivations are in fact
trees, but that they ?grow? in the direction opposite to how parse
trees are often depicted in NLP.
X/Y Y
T<
X\(X/Y)
<
X
This derivation shows how the argument Y to the
functional type X/Y5 can ?raise? its type to be-
come a function that consumes that functional type,
X\(X/Y), only to produce same result as before,
namely X. This property of CCG is often referred
to as ?spurious ambiguity?, because there are many
ways of reaching the same result as the canonical,
normal-form derivation.
Despite the name, this property is useful for our
purposes. Considering the target translation in Fig-
ure 1, we then observe in Figure 2 how CCG can
derive not only a bracketing similar to a more tra-
ditional Penn Treebank-style parse, but also a non-
normal-form variant that gives us a single category
for the English translation string I hope that ?
namely the category S[dcl]/S[dcl] (a declarative sen-
tence lacking a declarative sentence complement to
its right).
We use this fact about CCG to label a wider
range of PMT phrases with genuine syntactic con-
stituent labels. First we parse the English sen-
tences in our training data with the C&C parser, a
state-of-the-art, treebank-trained CCG parser (Clark
and Curran, 2007), producing normal-form CCG
derivations. We then enumerate all non-normal-
form derivations that result in the same top-level
symbol, packing all derivations (normal-form and
non-normal-form) into a parse chart (see Figure 4).
5Also referred to as a functor.
212
SNP
I
VP
VBP
hope
SBAR
WNP
WDT
that
S
it will...
I hope that it will ...
NP (S[dcl]\NP)/S[em] S[em]/S[dcl] S[dcl]
>
S[em]
>
S[dcl]\NP
<
S[dcl]
I hope that it will ...
NP (S[dcl]\NP)/S[em] S[em]/S[dcl] S[dcl]
T>
S[dcl]/(S[dcl]\NP)
B>
S[dcl]/S[em]
B>
S[dcl]/S[dcl]
>
S[dcl]
Figure 2: Left: a traditional syntactic derivation; top right: a normal-form CCG derivation with the same subject+predicate
bracketing; bottom right: one of many non-normal-form variants. Combinator symbol key: >=forward function application,
<=backward function application, T>=forward type-raising, B>=forward composition. Note: the CCG dependencies that are
discharged in different orders are indicated by color-coding (if available in your medium) and underlining the appropriate categories
(type-raising discharges no dependencies). Both CCG derivations lead to the same symbol (S[dcl]), and dependencies.
UR.-EN.
SINGLE-LABEL COVERAGE 69%
AVE. EN. PHRASE LEN. 2.8 wds
AVE. CCG LABEL SPAN 2.3 wds
AVE. CCG LABS/ENTRY 1.4
Table 1: Training data statistics (top to bottom): (1) % of sin-
gle CCG labels spanning entire English translation phrases, (2)
average length of English translation phrase, (3) average CCG
label span and (4) average CCG labels per English translation
phrase. (Maximum translation phrase length is 7 words.)
For the English string of each phrase table entry, we
inspect the chart for the English-side sentence that
it came from and extract a list of labels as in Fig-
ure 3. For each span, this procedure either (lines
5?9) finds the topmost single label, only using type-
raised categories when no others exist,6 or (lines 10?
19) recursively and greedily finds the longest span-
ning labels from left to right, if no single label ex-
ists. The degenerate case is the single-word level
(supertags). In this way we find single labels for
69% of the English-side phrase training instances.
Table 1 gives more details.
6Type-raisings are almost always possible, and will always
be closer to the top-level symbol. Many type-raisings, however,
are superfluous ? i.e., produce no novel bracketings. Therefore
we only use type-raised symbols to derive a label for a span of
words when necessary.
GETLABELS(C,s)
1 B C: a packed chart of derivations of E
2 B s = (el, er): a span in target sentence E
3 B RETURN: a list of labels covering all words
4 B from E in span s
5 if EXISTSSINGLESPANNINGLABEL(C,S)
6 then B Get the topmost label
7 B non-type-raised, if possible
8 lb ? GETTOPMOSTLABEL(C,s)
9 return [ lb ]
10 else B Get the longest label starting at el
11 for i? (er ? 1) to (el + 1)
12 do lbs ? GETLABELS(C,(el, i))
13 if LENGTH(lbs)=1
14 then el? ? i+ 1
15 lb ? HEAD(lbs)
16 BREAK
17 else CONTINUE
18 return
19 CONS(lb,GETLABELS(C,(el? , er)))
Figure 3: Algorithm for labeling English sides of phrase
table instances.
213
0 I 1 hope 2 that 3 it 4 will 5 rain 6
0 Ich 1 hoffe 2 , 3 da? 4 es 5 regnen 6 wird 7
NP
S/(S\NP)
S[dcl]/S[em]
S[dcl]/S[dcl]
(S[dcl]\NP)/S[em]
S[em]/S[dcl]
NP
S/(S\NP)
S[em]/(S[dcl]\NP)
S[dcl]/(S[b]\NP)
S[dcl]
S[em]
S[dcl]\NP
S[em]/(S[b]\NP)
(S[dcl]\NP)/(S[b]\NP) S[b]\NP
(S[dcl]\NP)/(S[b]\NP)
S[dcl]\NP
S[dcl]
Figure 4: A packed CCG parse chart with multiple semantically equivalent derivations and two word-aligned strings. (Not all
derivations are depicted.)
3 Reordering Models: from Words to
Supertags to Parses
In phrase-based MT systems, the standard reorder-
ing model that controls the order in which the
source string is translated is the lexicalized reorder-
ing model (Tillmann, 2004; Axelrod et al, 2005). In
its simplest form, a lexicalized reordering model es-
timates, for each translation phrase pair (fi...j , ek...l)
(where the indices sit ?in-between? words, as in Fig-
ure 4), the probability of p(O | fi...j , ek...l), where
O ? {MONO, SWAP,DISCONTINUOUS} (abbrevi-
ated M, S and D) is the orientation of the phrase pair
(fi...j , ek...l) w.r.t. the previously translated source
phrase fu...v. If v = i, then O = M; if u = j, then
O = S; otherwise O = D. This model, known as
a unidirectional MSD lexicalized reordering model,
can also be enriched with statistics over orientations
to the next source phrase translated (i.e., it can be
a bidirectional model), as well as with more fine-
grained distinctions in the third class D (i.e., whether
it is DLEFT or DRIGHT). All models in the present
work are bidirectional MSD models.
During decoding, orientations are predicted based
on previously translated (or following) phrases in
the decoder?s search state, but, when extracting ori-
entation statistics, there are many different possi-
ble phrasal segmentations of both strings. A sim-
ple solution, known as word-based extraction, is to
look for neighboring alignment points that support
the various orientations. In Figure 4, e.g., a word-
based extraction regime would count the phrase
hoffe ? hope as being in orientation D w.r.t. to
what follows, because its rightmost index, 2, is dis-
contiguous with the next aligned source point, (3,4).
Another approach, known as phrase-based extrac-
tion aims to remedy this situation by conditioning
the extraction of orientations on translation phrases
consistent with the alignment. In Figure 4 there is a
translation phrase that follows the phrase in question
? viz., , da? ? that ? and an orientation of M
is therefore tallied.
Regardless of the method of extraction, lexi-
calized reordering model statistics rely on exact
word-string pairs, (f, e), which can lead problems
with data sparsity. Moreover, even given ample
data, cross-phrasal reordering generalizations will
be missed. E.g., the fact that regnen ? rain has
orientation S w.r.t. the previous phrase pair does not
support the fact that other infinitival German verbs
should also behave similarly in relative clausal envi-
ronments.
To remedy this we might substitute abstract sym-
bols for each word in e, and train a syntactic bidirec-
tional MSD reordering model. For this we use CCG
supertags (cf. the single-word labels in the parse
214
chart in Figure 4), which are richly structured parts
of speech that describe their potential to combine
with other words (cf. Section 2.1). Given the same
phrase from Figure 4, we can estimate the proba-
bility of orientation S, given regnen ? S[b]\NP .
A further level of abstraction is to use CCG parse
charts packed with all derivations. The phrase
da? es ? that it can therefore be abstracted to
da? es ? S[em]/(S[dcl]\NP) (a ?that? clause
lacking a verb phrase to the right).
Except in cases of high ambiguity, the source
phrase effectively encodes the target phrase, mean-
ing that these extensions will suffer from data spar-
sity similarly to the baseline lexicalized model. We
therefore omit the source phrase in our syntactic
reordering models, estimating probability distribu-
tions p(O|LAB(e)) where LAB(e) is the syntactic la-
bel sequence derived from the chart (or supertagged
string, as the case may be) using the algorithm in
Figure 3.7 Orientations are determined using the
phrase-based extraction regime described in (Till-
mann, 2004), but statistics are tallied only for the
syntactic label sequence of the target string. More
precisely, for phrase pair (fi...j , ek...l), if a phrase
(fa...i, eb...k) exists in the alignment grid, an orien-
tation of M is assigned to LAB(ek...l) . Otherwise,
if a phrase (fj...p, el...m) exists in the alignment grid,
an orientation of S is assigned. In all other cases, an
orientation of D is assigned.
Using these statistics, we deploy target-side re-
ordering models, as described below.
4 Related Work
As noted, lexicalized reordering models can be
trained and configured in many different ways. In
addition to the standard word-based extraction (Ax-
elrod et al, 2005) and phrase-based extraction (Till-
mann, 2004) cases, more recent work has explored
using dynamic programming to extract and later
score orientations based on hierarchical configura-
tions of phrases consistent with an alignment (Gal-
ley and Manning, 2008). This means that the re-
ordering model can be conditioned on an unbounded
amount of context and can capture the fact that
7Note that a tagged string can be viewed as a very impover-
ished parse chart, and so the algorithm defined in Figure 3 can
be applied to the supertagging case as well.
many translations are monotonic w.r.t. the previ-
ously translated block, but are mistakenly identified
as having orientation S or D.
Su and colleagues (2010) observe that the space
of phrase pairs consistent with an alignment can
be viewed in its entirety, as a graph of phrases,
thereby collecting reordering statistics w.r.t. the en-
tire space of surrounding phrases. Ling and col-
leagues (2011) extend this approach by weighting
orientation counts with multiple scored alignments.
All of these more sophisticated reordering extrac-
tion approaches are compatible with the current ap-
proach, and could be straightforwardly applied to
our labelled target-side word strings.
Syntax-driven reordering approaches in phrase-
based MT abound, but, perhaps due to the incom-
patibility of phrase table entries and traditional syn-
tactic constituency, most research has avoided using
recursive target-side syntax during decoding. Till-
mann (2008) presents an algorithm that reorders us-
ing part-of-speech based permutation patterns dur-
ing the decoding process. Others have side-stepped
the issue by restructuring the source language be-
fore decoding to resemble the target language using
syntactic rules, either automatically extracted (Xia
and McCord, 2004), or hand-crafted (Collins et al,
2005; Wang et al, 2007; Xu and Seneff, 2008).
The flexibility of CCG syntax is also gaining
recognition as a useful tool for constraining statis-
tical MT decoders. Hassan (2009) describes an in-
cremental CCG parsing language model, although
his model does not beat a supertag factored PMT
approach. Almaghout and colleagues (2010) also
use a CCG chart to improve translation, augment-
ing SCFG rules by consulting the multiple deriva-
tions in the parse chart of Clark and Curran?s (2007)
CCG parser. We note two key differences to our
use of spurious ambiguity. First, they use a chart
packed with multiple dependency analyses, unlike
our spuriously ambiguous reworkings of the parser?s
single-best analysis. Second, the C&C parser re-
strains type-raising to a small number of possi-
bilities, thereby blocking many non-normal-form
derivations that we do not.
Two SCFG approaches that employ catego-
rial syntax that resembles CCG are the syntax-
augmented MT (SAMT) system described in (Venu-
gopal et al, 2007), and the target dependency lan-
215
guage model of of (Shen et al, 2008). (Venu-
gopal et al, 2007) uses a Penn Treebank-trained
CFG parser to label target strings and then re-
works the CFG parse trees, if needed,x to ac-
count for non-traditional constituents. This on-
demand reworking process, however, is bounded by
tree depth, and sometimes produces conjoined cat-
egories, rather than consistently produce the func-
tional ?slash? categories that a full CCG would ?
e.g., a subject + transitive verb string might some-
times be labelled NP+ V and other times S/NP .
The approach in (Shen et al, 2010) uses a simple
categorial grammar with only a single atomic sym-
bol ? i.e., every functional category has the form
C\X or C/X, where X is either C or another slash
category C\X or C/X. In contrast to these two ap-
proaches, the CCG parser we use is trained on a
CCG treebank that is the result of a carefully engi-
neered Penn Treebank-to-CCG conversion (Hocken-
maier and Steedman, 2007) and we impose no limits
on deriving categorial functional categories (X/Y).
We view our reworking of CCG charts as a poten-
tially useful extension to such approaches.
5 Experimental Results
We empirically validate our technique by translat-
ing from Urdu into English. Urdu has a canoni-
cal word order of SOV ? subject, object(s), verb
? whereas English has SVO, leading to indefinitely
long distances between corresponding verbs and ob-
jects. This language pair is therefore a strong test
case for a reordering model.
For decoding we use Moses (Koehn et al, 2007),
a state-of-the-art PMT decoder, with IRST LM (Fed-
erico and Cettolo, 2007) for language model infer-
ence. For Urdu-English parallel data, we use the
OpenMT 2008 training set which consists of 88
thousand sentence-level translations and a transla-
tion dictionary of ?114 thousand word and phrase
translations. We use half of the OpenMT 2008 Urdu-
English evaluation data for development and per-
form development testing on the other half. Both
halves are ?900 sentences long and were balanced
to contain approximately the same number of to-
kens. Our blind test set is the entire OpenMT 2009
Urdu-English evaluation set. All evaluation sets had
4 reference translations for each tuning or testing in-
stance. All system component weights were tuned
using minimum error-rate training (Och, 2003), with
three tuning runs for each condition. The data was
normalized, tokenized and the English sentences
were lowercased,8
As a baseline, we train a standard phrase-based
system with a bidirectional MSD lexicalized re-
ordering model using word-based extraction. Our
CCG-augmented reordering system has all of the
model components of the baseline, as well as a bidi-
rectional orientation reordering model over target-
side multiword syntactic labels. To directly test the
effect of using CCG parse charts ? as opposed to
simply using a CCG supertagger ? we also added a
CCG supertag bidirectional MSD reordering model
to the baseline set-up. All systems were tuned and
tested with distortion limit of 15 words, and test
runs were performed with and without 200-best min-
imum Bayes? risk (MBR) hypothesis selection (Ku-
mar and Byrne, 2004).
To acquire CCG labels for our English parallel
data, we use the C&C CCG toolkit of Clark and
Curran (2007). We build CCG parse charts by re-
working the normal-form derivations from the C&C
parser in all spuriously ambiguous ways, as de-
scribed in Section 2.1. For supertags, we tag with
the C&C supertagger. Rather than training sepa-
rate phrase tables for our CCG systems, however,
we instead decorate the baseline phrase tables with
CCG multiword labels or supertags. To smooth over
parsing and tagging errors, we only use those la-
bels whose relative frequency (rf) is sufficiently high
w.r.t. the most frequent label for that phrase pair
LAB*[f?e]. More precisely, for each phrase pair, we
use the set of labels:9
{LAB[f?e]|rf(LAB[f?e]) ? ? ? rf(LAB*[f?e])}
This is reminiscent of the ?-best tagging approach
of (Clark and Curran, 2004), but performed in a
batch process when creating the syntactic phrase ta-
bles (both supertag and CCG chart-derived). We set
8N.B. We use Penn Treebank III-compatible tokenization for
English and a specially designed tokenization script for Urdu,
cf. (Baker et al, 2010), Appendix C
9Recalling that ?31% of the time, a phrase pair might have
a list of labels, rather than a single label, the word ?label? here
refers to a single token that can be the concatenation of multiple
symbols.
216
DEVTEST (NIST-08) (MBR/NON-MBR) NIST-09 TEST (MBR/NON-MBR)
BLEU-4 METEOR TER LENGTH BLEU-4 METEOR TER LENGTH
LR 25.3/24.7 28.3/28.2 64.2/64.4 98.2/97.6 29.1/28.8 30.0/28.8 60.0/60.1 98.2/97.8
NO-LR 22.5/22.1 27.5/27.3 66.3/66.3 97.6/97.1 26.2/25.8 29.2/29.1 61.9/62.0 97.1/96.6
ST+LR 24.5/24.2 28.4/28.3 64.6/64.5 97.9/97.3 28.5/28.2 30.0/30.0 60.3/60.2 97.9/97.3
CCG+LR 25.6/25.2 28.7/28.5 64.3/64.5 98.7/98.1 29.1/29.2 30.1/30.2 59.5/59.8 97.4/97.9
Table 2: Case-insensitive BLEU-4, METEOR, TER and hypothesis/reference length ratio (LENGTH) for a lexicalized reordering
baseline (LR), a system with only a distance-based distortion model (NO-LR), a system with an additional CCG supertag reordering
model (ST+LR) and our system with an additional CCG chart-derived reordering model (CCG+LR). Systems were run with (left
of slash) and without (right of slash) 200-best-list MBR hypothesis selection. All boldfaced results were found to be significantly
better than the baseline at ? the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning
runs for each system. Non-boldfaced numbers are statistically indistinguishable from (or worse than) the baseline.
? = 0.5 in all of our CCG experiments.
To minimize disruption to the Moses decoder
(which only supports single-word labels in phrase-
based mode), we project multiword labels across the
words they label as single-word factors with book-
keeping characters, similar to the ?microtag? anno-
tations of asynchronous factored translation mod-
els (Cettolo et al, 2008). We modified to the de-
coder to reassemble the multiple single-word fac-
tors into a single label before querying the reorder-
ing model. As an example, we might have the phrase
pair le ve?lo rouge ? the|NP( red|NP+ bike|NP) .
Before querying the reordering model, the fac-
tor sequence NP( NP+ NP) is collapsed into the
single, multiword label ?NP? by the rule schema
X( . . . X+ . . . X) ? X.
We train a language model using all of the WMT
2011 NEWSCRAWL, NEWSCOMENTARY and EU-
ROPARL monolingual data,10 tokenized and lower-
cased as above, but de-duplicated to address the re-
dundancy of the Web-crawled portion of that data
set. We also train a separate language model on the
English portion of the Urdu-English parallel corpus
(minus the dictionary entries), and interpolate the
two models by optimizing perplexity on our tuning
set.
Table 2 lists our results, where we see significant
improvement over both of our baselines, lexicalized
reordering (LR) and supertag reordering plus lexi-
calized reordering (ST+LR). To test the effects of
the lexicalized reordering model itself, we also eval-
uate a system with no lexicalized reordering model
10http://www.statmt.org/wmt11/
translation-task.html
(only a distance-based distortion model). This last
system (a system which almost always prefers not
to reorder) is considerably worse than all other sys-
tems, demonstrating the need for non-monotonic
reordering configurations when accounting for the
Urdu-English data.
6 Analysis and Discussion
Our CCG system (CCG+LR) outperforms both
baseline systems (LR and ST+LR) in a majority of
metrics in both MBR and non-MBR conditions. We
see that, even though MBR decoding closes the per-
formance gap somewhat, our system continues to
match or outperform (if sometimes insignificantly)
in all areas. Note that the CCG+LR non-MBR
configuration outperforms both LR and ST+LR in
MBR and non-MBR decoding conditions in its ME-
TEOR score on the NIST-09 test set. We note also
that, in the NIST-09 test case, the CCG+LR sys-
tem?s poorer performance is perhaps due to a mis-
match in hypothesis length, which could be harming
its scores, particularly the BLEU brevity penalty.
6.1 Poor Performance of CCG Supertag Model
We have no firm explanation for the poor per-
formance of the CCG supertag model (ST-
LR), but it is important to note that the su-
pertag reordering model does not unify statis-
tics across phrases of different lengths, as the
CCG chart-derived model does. E.g., the
phrase pair den Weg fu?r eine ? the way for an
will query the CCG chart-derived reordering
model with the same symbol as the phrase pair
den Weg fu?r eine baldige ? the way for an early
217
twenty-seven year old abdullah britain blasts in hatcheries planning accused of is .
CCG+LR: twenty-seven year old abdullah is accused of planning hatcheries blasts in britain .
LR: twenty-seven years on charges of planning bombings hatcheries , abdullah in britain .
Reference 1: 27 years old abdullah is accused of planning explosions in britain .
Reference 2: twenty-seven years old abdullah is blamed for planning attacks in britain .
Reference 3: abdullah , 27 , has been blamed for planning the blasts in britain .
Reference 4: abdullah , 27 , has been blamed for planning the blasts in britain .
now musharraf resignation give should .
CCG+LR: now musharraf should give resignation .
LR: now musharraf resignation should be given .
Reference 1: now musharraf should resign .
Reference 2: now , musharraf should resign .
Reference 3: now , musharraf should resign .
Reference 4: musharraf should resign .
Figure 5: Sample devtest (NIST-08) translations of the median-performing tuned CCG syntactic reordering model
(CCG+LR) compared to the median-performing baseline lexicalized reordering model (LR).
? viz., NP/N. The CCG supertag model, how-
ever, will have two distinct label sequences for these
phrases ? viz., NP/N N (NP\NP)/NP NP/N and
NP/N N (NP\NP)/NP NP/N N/N, resp. ? both
of which could be reduced to the single label, NP/N,
using CCG?s syntactic combinators. The supertag
system does not have the means of relating the
reordering patterns of strings of symbols such as
this.11 Such data fragmentation may be leading to
decreased performance, which would indicate the
use of recursive CCG syntax.
6.2 Qualitative Improvements
In addition to improved metric scores, we noted real
qualitative improvements in some examples, as Fig-
ure 5 shows. These examples demonstrate the abil-
ity of the reordering model to navigate the massive,
structure-governed reorderings needed to approxi-
mate the correct answer with the phrase inventory
it is given.
11Its reordering table has more than twice as many entries as
that of the chart-derived model.
6.3 Comparison to the State of the Art
To our knowledge, the state of the art in Urdu-
English translation using the OpenMT data is
listed in the NIST OpenMT 2009 evaluation re-
sults (http://www.itl.nist.gov/iad/
mig/tests/mt/2009/ResultsRelease/
currentUrdu.html). This evaluation accepted
only single system outputs, and used cased refer-
ences. Therefore we had to choose a single system
output and recase its text.
For system selection, we picked the tuned sys-
tem that performed best on the development test
set. For recasing, we trained a lowercased-to-cased
monolingual phrase-based ?translation model? with
no reordering and a cased language model, similar to
what is described in (Baker et al, 2010). The train-
ing text is simply the non-dictionary portion of the
Urdu-English parallel corpus, with its lowercased
version as the source and the original cased text as
the target, both halves tokenized as above. We tuned
on a similar version of the English half of our tuning
218
references. The lowercased output of our system is
fed to this model and the first token of each casing
?translation? is capitalized (if not already).
The official metric of the NIST 2009 evaluation
is BLEU (as implemented in the NIST-distributed
mteval-v13a.pl script).12 The best-performing
system in the constrained data evaluation scored
0.312 w.r.t. the cased references, with the second
and third place systems scoring 0.2395 and 0.2322,
respectively.13 Our best performing MERT-tuned
system (as determined on the devtest data) scores
0.2734 on the test set, putting it between the top two
systems. For comparison, our devtest-best baseline
LR system scores 0.2683 on the test set.
While is generally not useful to test experimental
manipulations based on a single tuning run (Clark et
al., 2011) and with different monolingual language
modelling data, we note these figures simply to situ-
ate our results within the state of the art.
7 Conclusion
We have argued for the use of CCG in phrase-
based translation, due to its flexibility in providing
a wealth of different bracketings that better accom-
modate lexical translation strings. We have also pre-
sented a novel method for using CCG constituent la-
bels in a syntactic reordering model where the syn-
tactic labels span multiple words, do not cross trans-
lation constituent boundaries and are tailored specif-
ically to each translation constituent. The result is a
significant improvement in Urdu-English (SOV ?
SVO) translation scores over two baselines: a tra-
ditional phrase-based baseline with a lexicalized re-
ordering model and a phrase-based baseline with an
additional supertag reordering model. Moreover, we
have provided qualitative examples that confirm the
improvements in automatic metrics.
In future work we would like explore whether
further improvements can be gained by using more
sophisticated reordering models, such as reordering
graphs (Su et al, 2010) and hierarchical reordering
models (Galley and Manning, 2008) both for our
word-based and syntactic reordering models. Fur-
ther, as in prior work (Zollmann et al, 2006; Shen
12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz.
13We exclude combination entries that are combinations of
multiple systems with different algorithmic approaches.
et al, 2010; Almaghout et al, 2010), our categorial
labels could also be used to derive CCG-augmented
SCFG rules, both lexicalized and unlexicalized, cf.
(Zhao and Al-onaizan, 2008) ? the latter being the
SCFG analogue of our current model.
Acknowledgments
The authors would like to thank Chong Min Lee,
Aoife Cahill and Nitin Madnani at ETS for taking
the time to read earlier drafts of this (and closely re-
lated) work. Their comments and suggestions made
this a better paper. We would also like to thank
the anonymous reviewers for their very helpful feed-
back. The views expressed in this paper do not nec-
essarily reflect those of The Ohio State University or
of Educational Testing Service.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2010.
CCG Augmented Hierarchical Phrase-based Machine
Translation. In Proceedings of the 7th International
Workshop on Spoken Language Translation, Paris,
France.
Amittai Axelrod, Ra Birch Mayne, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005. Edin-
burgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT-05), Pittsburgh, PA, USA.
Emmon Bach. l976. An Extension of Classical Transfor-
mational Grammar. In Proceedings of the 1976 Con-
ference on Problems of Linguistic Metatheory, pages
183?224, East Lansing, MI, USA.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim
Mayeld, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2010. Semantically informed machine translation.
Technical Report 002, Johns Hopkins University, Bal-
timore, MD, Human Language Technology Center of
Excellence.
Mauro Cettolo, Marcello Federico, Daniele Pighin, and
Nicola Bertoldi. 2008. Shallow-syntax Phrase-based
Translation: Joint versus Factored String-to-chunk
Models. In Proceedings of AMTA 2008, Honolulu, HI,
USA.
Stephen Clark and James R. Curran. 2004. The Impor-
tance of Supertagging for Wide-Coverage CCG Pars-
ing. In Proceedings of the 20th International Con-
219
ference on Computational Linguistics (COLING-04),
Geneva, Switzerland.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better Hypothesis Testing for Ma-
chine Translation: Controlling for Optimizer Insta-
bility. In Proceedings of the Meeting of the Associ-
ation for Computational Linguistics (ACL-11), Port-
land, OR, USA.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the Association for
Computational Linguistics (ACL-05), Ann Arbor, MI,
USA.
Marcello Federico and Mauro Cettolo. 2007. Efficient
Handling of n-gram Language Models for Statistical
Machine Translation. In Proceedings of Association
for Computational Linguistics, Prague, The Czech Re-
public.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of EMNLP-08.
Hany Hassan. 2009. Lexical Syntax for Statistical Ma-
chine Translation. Ph.D. thesis, Dublin City Univer-
sity, Dublin, Ireland.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of NAACL-HLT, pages 48?54, Edmonton, Al-
berta, CA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, Companion Volume Proceedings of the Demo
and Poster Sessions, Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Transla-
tion. In Proceedings of HLT-NAACL.
Wang Ling, Jo ao Grac?a, David Martins de Matos, Is-
abel Trancoso, and Alan Black. 2011. Discrimi-
native Phrase-based Lexicalized Reordering Models
using Weighted Reordering Graphs. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing.
Franz Joseph Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-dependency Machine Translation Algo-
rithm with a Target Dependency Language Model. In
Proceedings of the Joint Meeting of the Association
for Computational Linguistics and Human Language
Technologies (ACL-08:HLT), Columbus, OH, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Translation.
Computational Linguistics, 36(4):649?671.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Jinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, and Qun
Liu. 2010. Learning Lexicalized Reordering Models
from Reordering Graphs. In Proceedings of the ACL
2010; Short Papers.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04.
Christoph Tillmann. 2008. A Rule-Driven Dynamic Pro-
gramming Decoder for Statistical MT. In Proceedings
of the Second Workshop on Syntax and Structure in
Statistical Translation (SSST-08).
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An Efficient Two-pass Approach to
Synchronous-CFG Driven Statistical MT. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL-07), Rochester, NY.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese Syntactic Reordering for Statistical Machine
Translation. In Proceedings of EMNLP/CoNLL-07,
Prague, The Czech Republic.
Fei Xia and Michael McCord. 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of International Conference
on Computational Linguistics (COLING-04), Geneva,
Switzerland.
Yushi Xu and Stephanie Seneff. 2008. Two-stage Trans-
lation: A Combined Linguistic and Statistical Machine
Translation Framework. In Proceedings of the 8th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-08), Waikiki, Honolulu,
HI, USA.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-Based Statistical Machine Translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI-
2002: Advances in Artificial Intelligence, Proceedings
of the 25th Annual German Conference on AI, (KI-
2002), pages 18?32. Springer Verlag, Aachen, Ger-
many.
220
Bing Zhao and Yaser Al-onaizan. 2008. Generalizing
Local and Non-Local Word-Reordering Patterns for
Syntax-Based Machine Translation. In Proceedings
of The Conference on Empirical Methods in Natural
Language Processig (EMNLP-08).
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-UKA Syntax Aug-
mented Machine Translation System for IWSLT-06.
In Proceedings of International Workshop on Spoken
Language Translation (IWSLT-06), Kyoto, Japan.
221
