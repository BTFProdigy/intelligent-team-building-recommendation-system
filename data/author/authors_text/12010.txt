Proceedings of NAACL HLT 2009: Short Papers, pages 281?284,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tightly coupling Speech Recognition and Search
Taniya Mishra
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
taniya@research.att.com
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Abstract
In this paper, we discuss the benefits of tightly
coupling speech recognition and search com-
ponents in the context of a speech-driven
search application. We demonstrate that by in-
corporating constraints from the information
repository that is being searched not only im-
proves the speech recognition accuracy but
also results in higher search accuracy.
1 Introduction
With the exponential growth in the use of mobile de-
vices in recent years, the need for speech-driven in-
terfaces is becoming apparent. The limited screen
space and soft keyboards of mobile devices make it
cumbersome to type in text input. Furthermore, by
the mobile nature of these devices, users often would
like to use them in hands-busy environments, ruling
out the possibility of typing text.
In this paper, we focus on the problem of speech-
driven search to access information repositories us-
ing mobile devices. Such an application typically
uses a speech recognizer (ASR) for transforming the
user?s speech input to text and a search component
that uses the resulting text as a query to retrieve
the relevant documents from the information reposi-
tory. For the purposes of this paper, we use the busi-
ness listings containing the name, address and phone
number of businesses as the information repository.
Most of the literature on speech-driven search ap-
plications that are available in the consumer mar-
ket (Acero et al, 2008; Bacchiani et al, 2008;
VLingo FIND, 2009) have quite rightly emphasized
the importance of the robustness of the ASR lan-
guage model and the data needed to build such a ro-
bust language model. We acknowledge that this is a
significant issue for building such systems, and we
provide our approach to creating a language model.
However, in contrast to most of these systems that
treat speech-driven search to be largely an ASR
problem followed by a Search problem, in this pa-
per, we show the benefits of tightly coupling ASR
and Search tasks and illustrate techniques to im-
prove the accuracy of both components by exploit-
ing the co-constraints between the two components.
The outline of the paper is as follows. In Sec-
tion 2, we discuss the set up of our speech-driven
application. In Section 3, we discuss our method to
integrating the speech and search components. We
present the results of the experiments in Section 4
and conclude in Section 5.
2 Speech-driven Search
We describe the speech-driven search application in
this section. The user of this application provides
a speech utterance to a mobile device intending to
search for the address and phone number of a busi-
ness. The speech utterance typically contains a busi-
ness name, optionally followed by a city and state
to indicate the location of the business (e.g. pizza
hut near urbana illinois.). User input with a busi-
ness category (laundromats in madison) and without
location information (hospitals) are some variants
supported by this application. The result of ASR is
used to search a business listing database of over 10
million entries to retrieve the entries pertinent to the
user query.
The ASR used to recognize these utterances in-
corporates an acoustic model adapted to speech col-
lected from mobile devices and a trigram language
model that is built from over 10 million text query
logs obtained from the web-based text-driven ver-
sion of this application. The 1-best speech recogni-
tion output is used to retrieve the relevant business
listing entries.
281
3 Tightly coupling ASR and Search
As mentioned earlier, most of the speech-driven
search systems use the the 1-best output from the
ASR as the query for the search component. Given
that ASR 1-best output is likely to be erroneous,
this serialization of the ASR and search components
might result in sub-optimal search accuracy. As will
be shown in our experiments, the oracle word/phrase
accuracy using n-best hypotheses is far greater than
the 1-best output. However, using each of the n-
best hypothesis as a query to the search compo-
nent is computationally sub-optimal since the strings
in the n-best hypotheses usually share large subse-
quences with each other. A lattice representation
of the ASR output, in particular, a word-confusion
network (WCN) transformation of the lattice, com-
pactly encodes the n-best hypothesis with the flexi-
bility of pruning alternatives at each word position.
An example of a WCN is shown in Figure 1. In or-
der to obtain a measure of the ambiguity per word
position in the WCN, we define the (average) arc
density of a WCN as the ratio of the total number
of arcs to the number of states in the WCN. As can
be seen, with very small increase in arc density, the
number of paths that are encoded in the WCN can
be increased exponentially. In Figure 2, we show
the improvement in oracle-path word and phrase ac-
curacies as a function of the arc density for our data
set. Oracle-path is a path in the WCN that has the
least edit-distance (Levenshtein, 1966) to the refer-
ence string. It is interesting to note that the oracle
accuracies can be improved by almost 10% absolute
over the 1-best accuracy with small increase in the
arc density.
0
1
ball
ys/0
.
317
aud
i/2.1
26
ball
ew
/4.7
04
ball
y/3.
625
ellie
s/4.
037
ellio
t/4.3
72
ellio
tt/4.
513
2/1
aut
om
obil
es/6
.
735
Figure 1: A sample word confusion network
3.1 Representing Search Index as an FST
In order to exploit WCNs for Search, we have im-
plemented our own search engine instead of using an
1 1.17 1.26 1.37 1.53 1.72 1.935456
5860
6264
6668
7072
74
Arc Densities
Accurac
y (in %)
 
 Word accuracyPhrase accuracy
Figure 2: Oracle accuracy graph for the WCNs at differ-
ent arc densities
0
audi
:aud
i_rep
air/c
1
audi
:aud
i_au
tomo
bile_
deale
rs/c2
auto
mob
ile:a
utom
obile
_salv
age/c
3
auto
mob
ile:a
udi_
auto
mob
ile_d
ealer
s/c4
bally
s:bal
lys_h
otel/
c5
bally
s:bal
lys_f
itnes
s/c6
bally
s:bal
lys_f
itnes
s/c6
Figure 3: An example of an FST representing the search
index
off-the-shelf search engine such as Lucene (Hatcher
and Gospodnetic., 2004). We index each business
listing (d) in our data that we intend to search using
the words (wd) in that listing. The pair (wd, d) is
assigned a weight (c(wd,d)) using different metrics,including the standard tf ? idf , as explained below.
This index is represented as a weighted finite-state
transducer (SearchFST) as shown in Figure 3 where
wd is the input symbol, d is the output symbol and
c(wd,d) is the weight of that arc.
3.2 Relevance Metrics
In this section, we describe six different weighting
metrics used to determine the relevance of a docu-
ment for a given query word that we have experi-
mented with in this paper.
idfw: idfw refers to the inverse document fre-
quency of the word, w, which is computed as
ln(D/dw), where D refers to the total number
of documents in the collection, and dw refers to
the total number of documents in the collection
that contain the word, w (Robertson and Jones,
1997; Robertson, 2004).
atfw: atfw refers to average term frequency, which
is computed as cfw/dw (Pirkola et al, 2002).
cfw ? idfw: Here cfw refers to the collection fre-
quency, which is simply the total number of oc-
currences of the word, w in the collection.
282
atfw ? idfw: (Each term as described above).
? fw,d
|dw| ? idfw: Here fw,d refers to the frequency of
the word, w, in the document, d, whereas |dw|
is the length of the document, d, in which the
word, w, occurs.
cfw?
|dw| ? idfw: (Each term as described above).
3.3 Search
By composing a query (Qfst) (either a 1-best
string represented as a finite-state acceptor, or a
WCN), with the SearchFST, we obtain all the arcs
(wq, dwq , c(wq ,dwq )) where wq is a query word, dwqis a listing with the query word and, c(wq ,dwq ) is theweight associated with that pair. Using this informa-
tion, we aggregate the weight for a listing (dq) across
all query words and rank the retrieved listings in the
descending order of this aggregated weight. We se-
lect the top N listings from this ranked list. The
query composition, listing weight aggregation and
selection of top N listings are computed with finite-
state transducer operations.
In Figure 4, we illustrate the result of reranking
the WCN shown in Figure 1 using the search rele-
vance weights of each word in the WCN. It must be
noted that the least cost path1 for the WCN in Fig-
ure 1 is ballys automobiles while the reranked 1-best
output in Figure 4 is audi automobiles. Given that
the user voice query was audi automobiles, the list-
ings retrieved from the 1-best output after reranking
are much more relevant than those retrieved before
reranking, as shown in Table 1.
0
1
aud
i/2.1
00
ball
ys/2
.
276
2/4
aut
om
obil
es/0
.
251
Figure 4: A WCN rescored using word-level search rele-
vance weights.
4 Experiments and Results
We took 852 speech queries collected from users us-
ing a mobile device based speech search application.
We ran the speech recognizer on these queries us-
ing the language model described in Section 2 and
created word-confusion networks such as those il-
lustrated in Figure 1. These 852 utterances were
divided into 300 utterances for the development set
and 552 for the test set.
1We transform the scores into costs and search for minimum
cost paths.
Before rescoring After rescoring
ballys intl auburn audi repair
los angeles ca auburn wa
ballys las vegas audi bellevue repair
las vegas nv bellevue wa
ballys las health spa university audi seattle wa
las vegas nv
ballys cleaners beverly hills audi
palm desert ca los angeles ca
ballys brothers audi independent repairs
yorba linda ca by eurotech livermore ca
Table 1: Listings retrieved for query audi automobiles
before and after ASR WCNs were rescored using search
relevance weights.
4.1 ASR Experiments
The baseline ASR word and sentence (complete
string) accuracies on the development set are 63.1%
and 57.0% while those on the test set are 65.1% and
55.3% respectively.
Metric Word Sent. Scaling AD
Acc. Acc. Factor
idfw 63.1 57.0 10?3 all
cfw ? idfw 63.5 58.3 15 ? 10?4 1.37
atfw 63.6 57.3 1 all
atfw ? idf 63.1 57.0 10?3 all? fw,d
|dfw| ? idf 63.9 58.3 15 ? 10?4 1.25
cfw?
|dfw|
? idfw 63.5 57.3 1 all
Table 2: Performance of the metrics used for rescoring
the WCNs output by ASR. (AD refers to arc density.)
In Table 2, we summarize the improvements ob-
tained by rescoring the ASRWCNs based on the dif-
ferent metrics used for computing the word scores
according to the search criteria. The largest im-
provement in word and sentence accuracies is ob-
tained by using the rescoring metric: ? fw,d|dfw| ? idf .The word-level accuracy improved from the baseline
accuracy of 63.1% to 63.9% after rescoring while
the sentence-level accuracy improved from 57.0%
to 58.3%. Thus, this rescoring metric, and the cor-
responding pruning AD and the scaling factor was
used to rerank the 552 WCNs in the test set. After
rescoring, on the test set, the word-level accuracy
improved from 65.1% to 65.9% and sentence-level
accuracy improved from 55.3% to 56.2%.
283
Number of Scores Baseline Rerankeddocuments
All
Precision 0.708 0.728
Documents
Recall 0.728 0.742
F-Score 0.718 0.735
Table 3: Table showing the relevancy of the search results
obtained by the baseline ASR output compared to those
obtained by the reranked ASR output.
4.2 Search Experiments
To analyze the Search accuracy of the baseline ASR
output in comparison to the ASR output, reranked
using the ? fw,d|dfw| ? idf reranking metric, we usedeach of the two sets of ASR outputs (i.e., base-
line and reranked) as queries to our search engine,
SearchFST (described in Section 3). For the search
results produced by each set of queries, we com-
puted the precision, recall, and F-score values of the
listings retrieved with respect to the listings retrieved
by the set of human transcribed queries (Reference).
The precision, recall, and F-scores for the baseline
ASR output and the reranked ASR output, averaged
across each set, is presented in Table 3. For the pur-
poses of this experiment, we assume that the set re-
turned by our SearchFST for the human transcribed
set of queries is the reference search set. This is
however an approximation for a human annotated
search set.
In Table 3, by comparing the search accuracy
scores corresponding to the baseline ASR output to
those corresponding to the reranked ASR output, we
see that reranking the ASR output using the informa-
tion repository produces a substantial improvement
in the accuracy of the search results.
It is interesting to note that even though the
reranking of the ASR as shown in Table 2 is of the
order of 1%, the improvement in Search accuracy is
substantially higher. This indicates to the fact that
exploiting constraints from both components results
in improving the recognition accuracy of that subset
of words that are more relevant for Search.
5 Conclusion
In this paper, we have presented techniques for
tightly coupling ASR and Search. The central idea
behind these techniques is to rerank the ASR out-
put using the constraints (encoded as relevance met-
rics) from the Search task. The relevance metric that
best improved accuracy is ? fw,d|dw| ? idfw, as deter-
mined on our development set. Using this metric
to rerank the ASR output of our test set, we im-
proved ASR accuracy from 65.1% to 65.9% at the
word-level and from 55.3% to 56.2% at the phrase
level. This reranking also improved the F-score of
the search component from 0.718 to 0.735. These
results bear out our expectation that tightly coupling
ASR and Search can improve the accuracy of both
components.
Encouraged by the results of our experiments, we
plan to explore other relevance metrics that can en-
code more sophisticated constraints such as the rel-
ative coherence of the terms within a query.
Acknowledgments
The data used in this work is partly derived from the
Speak4It voice search prototype. We wish to thank
every member of that team for having deployed that
voice search system.
References
A. Acero, N. Bernstein, R.Chambers, Y. Ju, X. Li,
J. Odell, O. Scholtz P. Nguyen, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proceedings of ICASSP 2008, Las Ve-
gas.
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster,
and B. Strope. 2008. Deploying GOOG-411: Early
lesstons in data, measurement and testing. In Proceed-
ings of ICASSP 2008, Las Vegas.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Action
(In Action series). Manning Publications Co., Green-
wich, CT, USA.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertion and reversals. Soviet Physics
Doklady, 10:707?710.
A. Pirkola, E. Lepaa?nen, and K. Ja?rvelin. 2002. The
?ratf? formula (kwok?s formula): exploiting average
term frequency in cross-language retrieval. Informa-
tion Research, 7(2).
S. E. Robertson and K. Sparck Jones. 1997. Simple
proven approaches to text retrieval. Technical report,
Cambridge University.
Stephen Robertson. 2004. Understanding inverse doc-
ument frequency: On theoretical arguments for idf.
Journal of Documentation, 60.
VLingo FIND, 2009.
http://www.vlingomobile.com/downloads.html.
284
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55?63,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Qme! : A Speech-based Question-Answering system on Mobile Devices
Taniya Mishra
AT&T Labs-Research
180 Park Ave
Florham Park, NJ
taniya@research.att.com
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ
srini@research.att.com
Abstract
Mobile devices are becoming the dominant
mode of information access despite being
cumbersome to input text using small key-
boards and browsing web pages on small
screens. We present Qme!, a speech-based
question-answering system that allows for
spoken queries and retrieves answers to the
questions instead of web pages. We present
bootstrap methods to distinguish dynamic
questions from static questions and we show
the benefits of tight coupling of speech recog-
nition and retrieval components of the system.
1 Introduction
Access to information has moved from desktop and
laptop computers in office and home environments
to be an any place, any time activity due to mo-
bile devices. Although mobile devices have small
keyboards that make typing text input cumbersome
compared to conventional desktop and laptops, the
ability to access unlimited amount of information,
almost everywhere, through the Internet, using these
devices have made them pervasive.
Even so, information access using text input on
mobile devices with small screens and soft/small
keyboards is tedious and unnatural. In addition, by
the mobile nature of these devices, users often like
to use them in hands-busy environments, ruling out
the possibility of typing text. We address this issue
by allowing the user to query an information repos-
itory using speech. We expect that spoken language
queries to be a more natural and less cumbersome
way of information access using mobile devices.
A second issue we address is related to directly
and precisely answering the user?s query beyond
serving web pages. This is in contrast to the current
approach where a user types in a query using key-
words to a search engine, browses the returned re-
sults on the small screen to select a potentially rele-
vant document, suitably magnifies the screen to view
the document and searches for the answer to her
question in the document. By providing a method
for the user to pose her query in natural language and
presenting the relevant answer(s) to her question, we
expect the user?s information need to be fulfilled in
a shorter period of time.
We present a speech-driven question answering
system, Qme!, as a solution toward addressing these
two issues. The system provides a natural input
modality ? spoken language input ? for the users
to pose their information need and presents a col-
lection of answers that potentially address the infor-
mation need directly. For a subclass of questions
that we term static questions, the system retrieves
the answers from an archive of human generated an-
swers to questions. This ensures higher accuracy
for the answers retrieved (if found in the archive)
and also allows us to retrieve related questions on
the user?s topic of interest. For a second subclass of
questions that we term dynamic questions, the sys-
tem retrieves the answer from information databases
accessible over the Internet using web forms.
The layout of the paper is as follows. In Section 2,
we review the related literature. In Section 3, we
illustrate the system for speech-driven question an-
swering. We present the retrieval methods we used
to implement the system in Section 4. In Section 5,
we discuss and evaluate our approach to tight cou-
pling of speech recognition and search components.
In Section 6, we present bootstrap techniques to dis-
tinguish dynamic questions from static questions,
and evaluate the efficacy of these techniques on a
test corpus. We conclude in Section 7.
2 Related Work
Early question-answering (QA) systems, such as
Baseball (Green et al, 1961) and Lunar (Woods,
1973) were carefully hand-crafted to answer ques-
tions in a limited domain, similar to the QA
components of ELIZA (Weizenbaum, 1966) and
SHRDLU (Winograd, 1972). However, there has
been a resurgence of QA systems following the
TREC conferences with an emphasis on answering
factoid questions. This work on text-based question-
answering which is comprehensively summarized
55
in (Maybury, 2004), range widely in terms of lin-
guistic sophistication. At one end of the spectrum,
There are linguistically motivated systems (Katz,
1997; Waldinger et al, 2004) that analyze the user?s
question and attempt to synthesize a coherent an-
swer by aggregating the relevant facts. At the other
end of the spectrum, there are data intensive sys-
tems (Dumais et al, 2002) that attempt to use the
redundancy of the web to arrive at an answer for
factoid style questions. There are also variants of
such QA techniques that involve an interaction and
use context to resolve ambiguity (Yang et al, 2006).
In contrast to these approaches, our method matches
the user?s query against the questions in a large cor-
pus of question-answer pairs and retrieves the asso-
ciated answer.
In the information retrieval community, QA sys-
tems attempt to retrieve precise segments of a doc-
ument instead of the entire document. In (To-
muro and Lytinen, 2004), the authors match the
user?s query against a frequently-asked-questions
(FAQ) database and select the answer whose ques-
tion matches most closely to the user?s question.
An extension of this idea is explored in (Xue et al,
2008; Jeon et al, 2005), where the authors match the
user?s query to a community collected QA archive
such as (Yahoo!, 2009; MSN-QnA, 2009). Our ap-
proach is similar to both these lines of work in spirit,
although the user?s query for our system originates
as a spoken query, in contrast to the text queries in
previous work. We also address the issue of noisy
speech recognition and assess the value of tight in-
tegration of speech recognition and search in terms
of improving the overall performance of the system.
A novelty in this paper is our method to address dy-
namic questions as a seamless extension to answer-
ing static questions.
Also related is the literature on voice-search ap-
plications (Microsoft, 2009; Google, 2009; Yellow-
Pages, 2009; vlingo.com, 2009) that provide a spo-
ken language interface to business directories and
return phone numbers, addresses and web sites of
businesses. User input is typically not a free flowing
natural language query and is limited to expressions
with a business name and a location. In our system,
users can avail of the full range of natural language
expressions to express their information need.
And finally, our method of retrieving answers to
dynamic questions has relevance to the database and
meta search community. There is growing interest
in this community to mine the ?hidden? web ? infor-
mation repositories that are behind web forms ? and
provide a unified meta-interface to such informa-
tion sources, for example, web sites related travel,
or car dealerships. Dynamic questions can be seen
as providing a natural language interface (NLI) to
such web forms, similar to early work on NLI to
databases (Androutsopoulos, 1995).
3 Speech-driven Question Retrieval
System
We describe the speech-driven query retrieval appli-
cation in this section. The user of this application
provides a spoken language query to a mobile device
intending to find an answer to the question. Some
example users? inputs are1 what is the fastest ani-
mal in water, how do I fix a leaky dishwasher, why
is the sky blue. The result of the speech recognizer
is used to search a large corpus of question-answer
pairs to retrieve the answers pertinent to the user?s
static questions. For the dynamic questions, the an-
swers are retrieved by querying a web form from
the appropriate web site (e.g www.fandango.com for
movie information). The result from the speech rec-
ognizer can be a single-best string or a weighted
word lattice.2 The retrieved results are ranked using
different metrics discussed in the next section. In
Figure 2, we illustrate the answers that Qme!returns
for static and dynamic quesitons.
Lattice1?best
Q&A corpus
ASRSpeech
Dynamic
Classify
from WebRetrieve
Rank
Search
Ranked ResultsMatch
Figure 1: The architecture of the speech-driven question-
answering system
4 Methods of Retrieval
We formulate the problem of answering static
questions as follows. Given a question-answer
archive QA = {(q1, a1), (q2, a2), . . . , (qN , aN )}
1The query is not constrained to be of any specific question
type (for example, what, where, when, how).
2For this paper, the ASR used to recognize these utterances
incorporates an acoustic model adapted to speech collected
from mobile devices and a four-gram language model that is
built from the corpus of questions.
56
Figure 2: Retrieval results for static and dynamic ques-
tions using Qme!
of N question-answer pairs, and a user?s ques-
tion qu, the task is to retrieve a subset QAr =
{(qr1, a
r
1), (q
r
2, a
r
2), . . . , (q
r
M , a
r
M )} M << N us-
ing a selection function Select and rank the mem-
bers of QAr using a scoring function Score such
that Score(qu, (qri , a
r
i )) > Score(qu, (q
r
i+1, a
r
i+1)).
Here, we assume
Score(qu, (qri , a
r
i )) = Score(qu, q
r
i ).
The Select function is intended to select the
matching questions that have high ?semantic? simi-
larity to the user?s question. However, given there is
no objective function that measures semantic simi-
larity, we approximate it using different metrics dis-
cussed below.
Ranking of the members of the retrieved set can
be based on the scores computed during the selec-
tion step or can be independently computed based
on other criteria such as popularity of the question,
credibility of the source, temporal recency of the an-
swer, geographical proximity to the answer origin.
4.1 Question Retrieval Metrics
We retrieve QA pairs from the data repository based
on the similarity of match between the user?s query
and each of the set of questions (d) in the repos-
itory. To measure the similarity, we have experi-
mented with the following metrics.
1. TF-IDF metric: The user input query and the
document (in our case, questions in the repos-
itory) are represented as bag-of-n-grams (aka
terms). The term weights are computed using a
combination of term frequency (tf ) and inverse
document frequency (idf ) (Robertson, 2004).
If Q = q1, q2, . . . , qn is a user query, then the
aggregated score for a document d using a un-
igram model of the query and the document is
given as in Equation 1. For a given query, the
documents with the highest total term weight
are presented as retrieved results. Terms can
also be defined as n-gram sequences of a query
and a document. In our experiments, we have
used up to 4-grams as terms to retrieve and rank
documents.
Score(d) =
?
w?Q
tfw,d ? idfw (1)
2. String Comparison Metrics: Since the length
of the user query and the query to be retrieved
are similar in length, we use string compar-
ison methods such as Levenshtein edit dis-
tance (Levenshtein, 1966) and n-gram overlap
(BLEU-score) (Papineni et al, 2002) as simi-
larity metrics.
We compare the search effectiveness of these sim-
ilarity metrics in Section 5.3.
5 Tightly coupling ASR and Search
Most of the speech-driven search systems use the
1-best output from the ASR as the query for the
search component. Given that ASR 1-best output
is likely to be erroneous, this serialization of the
ASR and search components might result in sub-
optimal search accuracy. A lattice representation
of the ASR output, in particular, a word-confusion
network (WCN) transformation of the lattice, com-
pactly encodes the n-best hypothesis with the flexi-
bility of pruning alternatives at each word position.
An example of a WCN is shown in Figure 3. The
weights on the arcs are to be interpreted as costs and
the best path in the WCN is the lowest cost path
from the start state (0) to the final state (4). Note
that the 1-best path is how old is mama, while the
input speech was how old is obama which also is in
the WCN, but at a higher cost.
0 1how/0.001
who/6.292
2
old/0.006
does/12.63
late/14.14
was/14.43
_epsilon/5.010
3
is/0.000
a/12.60
_epsilon/8.369
4/1
obama/7.796
lil/7.796
obamas/13.35
mama/0.000
bottle/12.60
Figure 3: A sample word confusion network with arc
costs as negative logarithm of the posterior probabilities.
57
0how:qa25/c1
old:qa25/c2
is:qa25/c3
obama:qa25/c4
old:qa150/c5
how:qa12/c6
obama:qa450/c7
is:qa1450/c8
Figure 4: Example of an FST representing the search in-
dex.
5.1 Representing Search Index as an FST
Lucene (Hatcher and Gospodnetic., 2004) is an off-
the-shelf search engine that implements the TF-IDF
metric. But, we have implemented our own search
engine using finite-state transducers (FST) for this
reason. The oracle word/phrase accuracy using n-
best hypotheses of an ASR is usually far greater than
the 1-best output. However, using each of the n-best
(n > 1) hypothesis as a separate query to the search
component is computationally sub-optimal since the
strings in the n-best hypotheses usually share large
subsequences with each other. The FST representa-
tion of the search index allows us to efficiently con-
sider lattices/WCNs as input queries.
The FST search index is built as follows. We in-
dex each question-answer (QA) pair from our repos-
itory ((qi, ai), qai for short) using the words (wqi) in
question qi. This index is represented as a weighted
finite-state transducer (SearchFST) as shown in Fig-
ure 4. Here a word wqi (e.g old) is the input symbol
for a set of arcs whose output symbol is the index
of the QA pairs where old appears in the question.
The weight of the arc c(wqi ,qi) is one of the simi-
larity based weights discussed in Section 4.1. As
can be seen from Figure 4, the words how, old, is
and obama contribute a score to the question-answer
pair qa25; while other pairs, qa150, qa12, qa450 are
scored by only one of these words.
5.2 Search Process using FSTs
A user?s speech query, after speech recognition, is
represented as an FSA (either 1-best or WCN), a
QueryFSA. The QueryFSA (denoted as q) is then
transformed into another FSA (NgramFSA(q)) that
represents the set of n-grams of the QueryFSA.
Due to the arc costs from WCNs, the NgramFSA
for a WCN is a weighted FSA. The NgramFSA is
composed with the SearchFST and we obtain all
the arcs (wq, qawq , c(wq ,qawq )) where wq is a query
term, qawq is a QA index with the query term and,
c(wq ,qawq ) is the weight associated with that pair. Us-
ing this information, we aggregate the weight for a
QA pair (qaq) across all query words and rank the
retrieved QAs in the descending order of this aggre-
gated weight. We select the top N QA pairs from
this ranked list. The query composition, QA weight
aggregation and selection of top N QA pairs are
computed with finite-state transducer operations as
shown in Equations 2 to 5.3
D1 = pi2(NgramFSA(q) ? SearchFST ) (2)
R1 = fsmbestpath(D1, 1) (3)
D2 = pi2(NgramFSA(R1) ? SearchFST ) (4)
TopN = fsmbestpath(fsmdeterminize(D2), N)
(5)
The process of retrieving documents using the
Levenshtein-based string similarity metric can also
be encoded as a composition of FSTs.
5.3 Experiments and Results
We have a fairly large data set consisting of over a
million question-answer pairs collected by harvest-
ing the web. In order to evaluate the retrieval meth-
ods discussed earlier, we use two test sets of QA
pairs: a Seen set of 450 QA pairs and an Unseen set
of 645 QA pairs. The queries in the Seen set have
an exact match with some question in the database,
while the queries in the Unseen set may not match
any question in the database exactly. 4 The questions
in theUnseen set, however, like those in the Seen set,
also have a human generated answer that is used in
our evaluations.
For each query, we retrieve the twenty most rel-
evant QA pairs, ranked in descending order of the
value of the particular metric under consideration.
However, depending on whether the user query is a
seen or an unseen query, the evaluation of the rele-
vance of the retrieved question-answer pairs is dif-
ferent as discussed below.5
3We have dropped the need to convert the weights into the
real semiring for aggregation, to simplify the discussion.
4There may however be semantically matching questions.
5The reason it is not a recall and precision curve is that, for
the ?seen? query set, the retrieval for the questions is a zero/one
boolean accuracy. For the ?unseen? query set there is no perfect
match with the input question in the query database, and so we
determine the closeness of the questions based on the closeness
of the answers. Coherence attempts to capture the homogen-
ity of the questions retrieved, with the assumption that the user
might want to see similar questions as the returned results.
58
5.3.1 Evaluation Metrics
For the set of Seen queries, we evaluate the rele-
vance of the retrieved top-20 question-answer pairs
in two ways:
1. Retrieval Accuracy of Top-N results: We eval-
uate whether the question that matches the user
query exactly is located in the top-1, top-5,
top-10, top-20 or not in top-20 of the retrieved
questions.
2. Coherence metric: We compute the coherence
of the retrieved set as the mean of the BLEU-
score between the input query and the set of
top-5 retrieved questions. The intuition is that
we do not want the top-5 retrieved QA pairs
to distract the user by not being relevant to the
user?s query.
For the set of Unseen queries, since there are no
questions in the database that exactly match the in-
put query, we evaluate the relevance of the top-20 re-
trieved question-answer pairs in the following way.
For each of the 645 Unseen queries, we know the
human-generated answer. We manually annotated
each unseen query with the Best-Matched QA pair
whose answer was the closest semantic match to the
human-generated answer for that unseen query. We
evaluate the position of the Best-Matched QA in the
list of top twenty retrieved QA pairs for each re-
trieval method.
5.3.2 Results
On the Seen set of queries, as expected the re-
trieval accuracy scores for the various retrieval tech-
niques performed exceedingly well. The unigram
based tf.idf method retrieved 93% of the user?s query
in the first position, 97% in one of top-5 positions
and 100% in one of top-10 positions. All the other
retrieval methods retrieved the user?s query in the
first position for all the Seen queries (100% accu-
racy).
In Table 1, we tabulate the results of the Coher-
ence scores for the top-5 questions retrieved using
the different retrieval techniques for the Seen set of
queries. Here, the higher the n-gram the more co-
herent is the set of the results to the user?s query. It
is interesting to note that the BLEU-score and Lev-
enshtein similarity driven retrieval methods do not
differ significantly in their scores from the n-gram
tf.idf based metrics.
Method Coherence Metric
for top-5 results
TF-IDF unigram 61.58
bigram 66.23
trigram 66.23
4-gram 69.74
BLEU-score 66.29
Levenshtein 67.36
Table 1: Coherence metric results for top-5 queries re-
trieved using different retrieval techniques for the seen
set.
In Table 2, we present the retrieval results using
different methods on the Unseen queries. For 240 of
the 645 unseen queries, the human expert found that
that there was no answer in the data repository that
could be considered semantically equivalent to the
human-generated response to that query. So, these
240 queries cannot be answered using the current
database. For the remaining 405 unseen queries,
over 60% have their Best-Matched question-answer
pair retrieved in the top-1 position. We expect the
coverage to improve considerably by increasing the
size of the QA archive.
Method Top-1 Top-20
TFIDF Unigram 69.13 75.81
Bigram 62.46 67.41
Trigram 61.97 65.93
4-gram 56.54 58.77
WCN 70.12 78.52
Levenshtein 67.9 77.29
BLEU-score 72.0 75.31
Table 2: Retrieval results for the Unseen queries
5.3.3 Speech-driven query retrieval
In Equation 6, we show the tight integration of
WCNs and SearchFST using the FST composition
operation (?). ? is used to scale the weights6 from
the acoustic/language models on the WCNs against
the weights on the SearchFST. As before, we use
Equation 3 to retrieve the top N QA pairs. The tight
integration is expected to improve both the ASR and
Search accuracies by co-constraining both compo-
nents.
D = pi2(Unigrams(WCN)
??SearchFST ) (6)
For this experiment, we use the speech utterances
corresponding to the Unseen set as the test set. We
use a different set of 250 speech queries as the
6fixed using the development set
59
development set. In Table 3, we show the Word
and Sentence Accuracy measures for the best path
in the WCN before and after the composition of
SearchFST with the WCN on the development and
test sets. We note that by integrating the constraints
from the search index, the ASR accuracies can be
improved by about 1% absolute.
Set # of Word Sentence
utterances Accuracy Accuracy
Dev Set 250 77.1(78.2) 54(54)
Test Set 645 70.8(72.1) 36.7(37.1)
Table 3: ASR accuracies of the best path before and after
(in parenthesis) the composition of SearchFST
Since we have the speech utterances of the Un-
seen set, we were also able to compute the search
results obtained by integrating the ASR WCNs with
the SearchFST, as shown in line 5 of Table 2. These
results show that the the integration of the ASR
WCNs with the SearchFST produces higher search
accuracy compared to ASR 1-best.
6 Dynamic and Static Questions
Storing previously answered questions and their an-
swers allows Qme!to retrieve the answers to a sub-
class of questions quickly and accurately. We term
this subclass as static questions since the answers
to these questions remain the same irrespective of
when and where the questions are asked. Examples
of such questions are What is the speed of light?,
When is George Washington?s birthday?. In con-
trast, there is a subclass of questions, which we term
dynamic questions, for which the answers depend
on when and where they are asked. For such ques-
tions the above method results in less than satisfac-
tory and sometimes inaccurate answers. Examples
of such questions are What is the stock price of Gen-
eral Motors?, Who won the game last night?, What
is playing at the theaters near me?.
We define dynamic questions as questions whose
answers change more frequently than once a year.
In dynamic questions, there may be no explicit ref-
erence to time, unlike the questions in the TERQAS
corpus (Radev and Sundheim., 2002) which explic-
itly refer to the temporal properties of the entities
being questioned or the relative ordering of past and
future events. The time-dependency of a dynamic
question lies in the temporal nature of its answer.
For example, consider the dynamic question, ?What
is the address of the theater ?White Christmas? is
playing at in New York??. White Christmas is a sea-
sonal play that plays in New York every year for a
few weeks in December and January, but it does not
necessarily at the same theater every year. So, de-
pending when this question is asked, the answer will
be different.
Interest in temporal analysis for question-
answering has been growing since the late 1990?s.
Early work on temporal expressions identifica-
tion using a tagger led to the development of
TimeML (Pustejovsky et al, 2001), a markup
language for annotating temporal expressions and
events in text. Other examples include QA-by-
Dossier with Constraints (Prager et al, 2004), a
method of improving QA accuracy by asking auxil-
iary questions related to the original question in or-
der to temporally verify and restrict the original an-
swer. (Moldovan et al, 2005) detect and represent
temporally related events in natural language using
logical form representation. (Saquete et al, 2009)
use the temporal relations in a question to decom-
pose it into simpler questions, the answers of which
are recomposed to produce the answers to the origi-
nal question.
6.1 Dynamic/Static Classification
We automatically classify questions as dynamic and
static questions. Answers to static questions can be
retrieved from the QA archive. To answer dynamic
questions, we query the database(s) associated with
the topic of the question through web forms on the
Internet. We use a topic classifier to detect the topic
of a question followed by a dynamic/static classifier
trained on questions related to a topic, as shown in
figure 5. Given the question what movies are play-
ing around me?, we detect it is a movie related dy-
namic question and query a movie information web
site (e.g. www.fandango.com) to retrieve the results
based on the user?s GPS information.
Figure 5: Chaining two classifiers
We used supervised learning to train the topic
60
classifier, since our entire dataset is annotated by hu-
man experts with topic labels. In contrast, to train a
dynamic/static classifier, we experimented with the
following three different techniques.
Baseline: We treat questions as dynamic if they
contain temporal indexicals, e.g. today, now, this
week, two summers ago, currently, recently, which
were based on the TimeML corpus. We also in-
cluded spatial indexicals such as here, and other sub-
strings such as cost of and how much is. A question
is considered static if it does not contain any such
words/phrases.
Self-training with bagging: The general self-
training with bagging algorithm (Banko and Brill,
2001) is presented in Table 6 and illustrated in Fig-
ure 7(a). The benefit of self-training is that we can
build a better classifier than that built from the small
seed corpus by simply adding in the large unlabeled
corpus without requiring hand-labeling.
1. Create k bags of data, each of size |L|, by sampling
with replacement from labeled set L.
2. Train k classifiers; one classifier on each of k bags.
3. Each classifier predicts labels of the unlabeled set.
4. The N labeled instances that j of k classifiers agree
on with the highest average confidence is added to the
labeled set L, to produce a new labeled set L?.
5. Repeat all 5 steps until stopping criteria is reached.
Figure 6: Self-training with bagging
(a) (b)
Figure 7: (a) Self-training with bagging (b) Committee-
based active-learning
In order to prevent a bias towards the majority
class, in step 4, we ensure that the distribution of
the static and dynamic questions remains the same
as in the annotated seed corpus. The benefit of bag-
ging (Breiman, 1996) is to present different views of
the same training set, and thus have a way to assess
the certainty with which a potential training instance
can be labeled.
Active-learning: This is another popular method for
training classifiers when not much annotated data is
available. The key idea in active learning is to anno-
tate only those instances of the dataset that are most
difficult for the classifier to learn to classify. It is
expected that training classifiers using this method
shows better performance than if samples were cho-
sen randomly for the same human annotation effort.
Figure 7(b) illustrates the algorithm and Figure 8
describes the algorithm, also known as committee-
based active-learning (Banko and Brill, 2001).
1. Create k bags of data, each of size |L|, by sampling
with replacement from the labeled set L.
2. Train k classifiers, one on each bag of the k bags.
3. Each classifier predicts the labels of the unlabeled set.
4. Choose N instances from the unlabeled set for human
labeling. N/2 of the instances are those whose labels the
committee of classifiers have highest vote entropy (un-
certainity). The other N/2 of the instances are selected
randomly from the unlabeled set.
5. Repeat all 5 steps until stopping criteria is reached.
Figure 8: Active Learning algorithm
We used the maximum entropy classifier in
Llama (Haffner, 2006) for all of the above classi-
fication tasks.
6.2 Experiments and Results
6.2.1 Topic Classification
The topic classifier was trained using a training
set consisted of over one million questions down-
loaded from the web which were manually labeled
by human experts as part of answering the questions.
The test set consisted of 15,000 randomly selected
questions. Word trigrams of the question are used
as features for a MaxEnt classifier which outputs a
score distribution on all of the 104 possible topic
labels. The error rate results for models selecting
the top topic and the top two topics according to the
score distribution are shown in Table 4. As can be
seen these error rates are far lower than the baseline
model of selecting the most frequent topic.
Model Error Rate
Baseline 98.79%
Top topic 23.9%
Top-two topics 12.23%
Table 4: Results of topic classification
61
Figure 9: Change in classification results
6.2.2 Dynamic/static Classification
As mentioned before, we experimented with
three different approaches to bootstrapping a dy-
namic/static question classifier. We evaluate these
methods on a 250 question test set drawn from the
broad topic of Movies. For the baseline model, we
used the words/phrases discussed earlier based on
temporal and spatial indexicals. For the ?super-
vised? model, we use the baseline model to tag 500K
examples and use the machine-annotated corpus to
train a MaxEnt binary classifier with word trigrams
as features. The error rate in Table 5 shows that it
performs better than the baseline model mostly due
to better lexical coverage contributed by the 500K
examples.
Training approach Lowest Error rate
Baseline 27.70%
?Supervised? learning 22.09%
Self-training 8.84%
Active-learning 4.02%
Table 5: Best Results of dynamic/static classification
In the self-training approach, we start with a small
seed corpus of 250 hand-labeled examples from the
Movies topic annotated with dynamic or static tags.
We used the same set of 500K unlabeled examples
as before and word trigrams from the question were
used as the features for a MaxEnt classifier. We used
11 bags in the bagging phase of this approach and
required that all 11 classifiers agree unanimously
about the label of a new instance. Of all such in-
stances, we randomly selected N instances to be
added to the training set of the next iteration, while
maintaining the distribution of the static and dy-
namic questions to be the same as that in the seed
corpus. We experimented with various values of N ,
the number of newly labeled instances added at each
iteration. The error rate at initialization is 10.4%
compared to 22.1% of the ?supervised? approach
which can be directly attributed to the 250 hand-
labeled questions. The lowest error rate of the self-
training approach, obtained at N=100, is 8.84%, as
shown in Table 5. In Figure 9, we show the change
in error rate for N=40 (line S1 in the graph) and
N=100 (line S2 in the graph).
For the active learning approach, we used the
same set of 250 questions as the seed corpus, the
same set of 500K unlabeled examples, the same test
set, and the same set of word trigrams features as in
the self-training approach. We used 11 bags for the
bagging phase and selected top 20 new unlabeled in-
stances on which the 11 classifiers had the greatest
vote entropy to be presented to the human labeler for
annotation. We also randomly selected 20 instances
from the rest of the unlabeled set to be presented for
annotation. The best error rate of this classifier on
the test set is 4.02%, as shown in Table 5. The error
rate over successive iterations is shown by line A1
in Figure 9.
In order to illustrate the benefits of selecting the
examples actively, we repeated the experiment de-
scribed above but with all 40 unlabeled instances se-
lected randomly for annotation. The error rate over
successive iterations is shown by line R1 in Fig-
ure 9. Comparing A1 to R1, we see that the error de-
creases faster when we select some of the unlabeled
instances for annotation actively at each iteration.
7 Conclusion
In this paper, we have presented a system Qme!,
a speech-driven question-answering system for mo-
bile devices. We have proposed a query retrieval
model for question-answering and demonstrated the
mutual benefits of tightly coupling the ASR and
Search components of the system. We have pre-
sented a novel concept of distinguishing questions
that need dynamic information to be answered from
those questions whose answers can be retrieved from
an archive. We have shown results on bootstrap-
ping such a classifier using semi-supervised learning
techniques.
62
References
L. Androutsopoulos. 1995. Natural language interfaces
to databases - an introduction. Journal of Natural Lan-
guage Engineering, 1:29?81.
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of the 39th annual meeting of the association
for computational linguistics: ACL 2001, pages 26?
33.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002.
Web question answering: is more always better? In
SIGIR ?02: Proceedings of the 25th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 291?298, New
York, NY, USA. ACM.
Google, 2009. http://www.google.com/mobile.
B.F. Green, A.K. Wolf, C. Chomsky, and K. Laughery.
1961. Baseball, an automatic question answerer. In
Proceedings of the Western Joint Computer Confer-
ence, pages 219?224.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(iv):239?261.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Action
(In Action series). Manning Publications Co., Green-
wich, CT, USA.
J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding sim-
ilar questions in large question and answer archives.
In CIKM ?05: Proceedings of the 14th ACM interna-
tional conference on Information and knowledge man-
agement, pages 84?90, New York, NY, USA. ACM.
B. Katz. 1997. Annotating the world wide web using
natural language. In Proceedings of RIAO.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertion and reversals. Soviet Physics
Doklady, 10:707?710.
M. T.Maybury, editor. 2004. NewDirections in Question
Answering. AAAI Press.
Microsoft, 2009. http://www.live.com.
D. Moldovan, C. Clark, and S. Harabagiu. 2005. Tem-
poral context representation and reasoning. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, pages 1009?1104.
MSN-QnA, 2009. http://qna.live.com/.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meeting
of the Association of Computational Linguistics, pages
313?318, Philadelphia, PA, July.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: Qa-by-
dossier-with-contraints. In Proceedings of the 42nd
annual meeting of the association for computational
linguistics: ACL 2004, pages 574?581.
J. Pustejovsky, R. Ingria, R. Saur??, J. Casta no, J. Littman,
and R. Gaizauskas., 2001. The language of time: A
reader, chapter The specification languae ? TimeML.
Oxford University Press.
D. Radev and B. Sundheim. 2002. Using timeml in ques-
tion answering. Technical report, Brandies University.
S. Robertson. 2004. Understanding inverse document
frequency: On theoretical arguments for idf. Journal
of Documentation, 60.
E. Saquete, J. L. Vicedo, P. Mart??nez-Barco, R. Mu
noz, and H. Llorens. 2009. Enhancing qa sys-
tems with complex temporal question processing ca-
pabilities. Journal of Artificial Intelligence Research,
35:775?811.
N. Tomuro and S. L. Lytinen. 2004. Retrieval models
and Q and A learning with FAQ files. In New Direc-
tions in Question Answering, pages 183?202.
vlingo.com, 2009. http://www.vlingomobile.com/downloads.html.
R. J. Waldinger, D. E. Appelt, J. L. Dungan, J. Fry, J. R.
Hobbs, D. J. Israel, P. Jarvis, D. L. Martin, S. Riehe-
mann, M. E. Stickel, and M. Tyson. 2004. Deductive
question answering from multiple resources. In New
Directions in Question Answering, pages 253?262.
J. Weizenbaum. 1966. ELIZA - a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 1:36?45.
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
W. A. Woods. 1973. Progress in natural language un-
derstanding - an application to lunar geology. In Pro-
ceedings of American Federation of Information Pro-
cessing Societies (AFIPS) Conference.
X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval models
for question and answer archives. In SIGIR ?08: Pro-
ceedings of the 31st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 475?482, New York, NY, USA.
ACM.
Yahoo!, 2009. http://answers.yahoo.com/.
F. Yang, J. Feng, and G. DiFabbrizio. 2006. A data
driven approach to relevancy recognition for contex-
tual question answering. In HLT-NAACL 2006 Work-
shop on Interactive Question Answering, New York,
USA, June 8-9.
YellowPages, 2009. http://www.speak4it.com.
63
Proceedings of the ACL 2010 System Demonstrations, pages 60?65,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Speech-driven Access to the Deep Web on Mobile Devices
Taniya Mishra and Srinivas Bangalore
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932 USA.
{taniya,srini}@research.att.com.
Abstract
The Deep Web is the collection of infor-
mation repositories that are not indexed
by search engines. These repositories are
typically accessible through web forms
and contain dynamically changing infor-
mation. In this paper, we present a sys-
tem that allows users to access such rich
repositories of information on mobile de-
vices using spoken language.
1 Introduction
The World Wide Web (WWW) is the largest
repository of information known to mankind. It
is generally agreed that the WWW continues to
significantly enrich and transform our lives in un-
precedent ways. Be that as it may, the WWW that
we encounter is limited by the information that
is accessible through search engines. Search en-
gines, however, do not index a large portion of
WWW that is variously termed as the Deep Web,
Hidden Web, or Invisible Web.
Deep Web is the information that is in propri-
etory databases. Information in such databases is
usually more structured and changes at higher fre-
quency than textual web pages. It is conjectured
that the Deep Web is 500 times the size of the
surface web. Search engines are unable to index
this information and hence, unable to retrieve it
for the user who may be searching for such infor-
mation. So, the only way for users to access this
information is to find the appropriate web-form,
fill in the necessary search parameters, and use it
to query the database that contains the information
that is being searched for. Examples of such web
forms include, movie, train and bus times, and air-
line/hotel/restaurant reservations.
Contemporaneously, the devices to access infor-
mation have moved out of the office and home en-
vironment into the open world. The ubiquity of
mobile devices has made information access an
any time, any place activity. However, informa-
tion access using text input on mobile devices is te-
dious and unnatural because of the limited screen
space and the small (or soft) keyboards. In addi-
tion, by the mobile nature of these devices, users
often like to use them in hands-busy environments,
ruling out the possibility of typing text. Filling
web-forms using the small screens and tiny key-
boards of mobile devices is neither easy nor quick.
In this paper, we present a system, Qme!, de-
signed towards providing a spoken language inter-
face to the Deep Web. In its current form, Qme!
provides a unifed interface onn iPhone (shown in
Figure 1) that can be used by users to search for
static and dynamic questions. Static questions are
questions whose answers to these questions re-
main the same irrespective of when and where the
questions are asked. Examples of such questions
are What is the speed of light?, When is George
Washington?s birthday?. For static questions, the
system retrieves the answers from an archive of
human generated answers to questions. This en-
sures higher accuracy for the answers retrieved (if
found in the archive) and also allows us to retrieve
related questions on the user?s topic of interest.
Figure 1: Retrieval results for static and dynamic
questions using Qme!
Dynamic questions are questions whose an-
swers depend on when and where they are asked.
Examples of such questions are What is the stock
price of General Motors?, Who won the game last
night?, What is playing at the theaters near me?.
60
The answers to dynamic questions are often part of
the DeepWeb. Our system retrieves the answers to
such dynamic questions by parsing the questions
to retrieve pertinent search keywords, which are in
turn used to query information databases accessi-
ble over the Internet using web forms. However,
the internal distinction between dynamic and static
questions, and the subsequent differential treat-
ment within the system is seamless to the user. The
user simply uses a single unified interface to ask a
question and receive a collection of answers that
potentially address her question directly.
The layout of the paper is as follows. In Sec-
tion 2, we present the system architecture. In
Section 3, we present bootstrap techniques to dis-
tinguish dynamic questions from static questions,
and evaluate the efficacy of these techniques on a
test corpus. In Section 4, we show how our system
retrieves answers to dynamic questions. In Sec-
tion 5, we show how our system retrieves answers
to static questions. We conclude in Section 6.
2 Speech-driven Question Answer
System
Speech-driven access to information has been a
popular application deployed by many compa-
nies on a variety of information resources (Mi-
crosoft, 2009; Google, 2009; YellowPages, 2009;
vlingo.com, 2009). In this prototype demonstra-
tion, we describe a speech-driven question-answer
application. The system architecture is shown in
Figure 2.
The user of this application provides a spoken
language query to a mobile device intending to
find an answer to the question. The speech recog-
nition module of the system recognizes the spo-
ken query. The result from the speech recognizer
can be either a single-best string or a weighted
word lattice.1 This textual output of recognition is
then used to classify the user query either as a dy-
namic query or a static query. If the user query is
static, the result of the speech recognizer is used to
search a large corpus of question-answer pairs to
retrieve the relevant answers. The retrieved results
are ranked using tf.idf based metric discussed in
Section 5. If the user query is dynamic, the an-
swers are retrieved by querying a web form from
the appropriate web site (e.g www.fandango.com
for movie information). In Figure 1, we illustrate
the answers that Qme!returns for static and dy-
1For this paper, the ASR used to recognize these utter-
ances incorporates an acoustic model adapted to speech col-
lected from mobile devices and a four-gram language model
that is built from the corpus of questions.
namic questions.
Lattice1?best
Q&A corpus
ASRSpeech
Dynamic
Classify
from WebRetrieve
Rank
Search
Ranked ResultsMatch
Figure 2: The architecture of the speech-driven
question-answering system
2.1 Demonstration
In the demonstration, we plan to show the users
static and dynamic query handling on an iPhone
using spoken language queries. Users can use the
iphone and speak their queries using an interface
provided by Qme!. A Wi-Fi access spot will make
this demonstation more compelling.
3 Dynamic and Static Questions
As mentioned in the introduction, dynamic ques-
tions require accessing the hidden web through a
web form with the appropriate parameters. An-
swers to dynamic questions cannot be preindexed
as can be done for static questions. They depend
on the time and geographical location of the ques-
tion. In dynamic questions, there may be no ex-
plicit reference to time, unlike the questions in the
TERQAS corpus (Radev and Sundheim., 2002)
which explicitly refer to the temporal properties
of the entities being questioned or the relative or-
dering of past and future events.
The time-dependency of a dynamic question
lies in the temporal nature of its answer. For exam-
ple, consider the question, What is the address of
the theater White Christmas is playing at in New
York?. White Christmas is a seasonal play that
plays in New York every year for a few weeks
in December and January, but not necessarily at
the same theater every year. So, depending when
this question is asked, the answer will be differ-
ent. If the question is asked in the summer, the
answer will be ?This play is not currently playing
anywhere in NYC.? If the question is asked dur-
ing December, 2009, the answer might be different
than the answer given in December 2010, because
the theater at which White Christmas is playing
differs from 2009 to 2010.
There has been a growing interest in tempo-
ral analysis for question-answering since the late
1990?s. Early work on temporal expressions iden-
61
tification using a tagger culminated in the devel-
opment of TimeML (Pustejovsky et al, 2001),
a markup language for annotating temporal ex-
pressions and events in text. Other examples in-
clude, QA-by-Dossier with Constraints (Prager et
al., 2004), a method of improving QA accuracy by
asking auxiliary questions related to the original
question in order to temporally verify and restrict
the original answer. (Moldovan et al, 2005) detect
and represent temporally related events in natural
language using logical form representation. (Sa-
quete et al, 2009) use the temporal relations in a
question to decompose it into simpler questions,
the answers of which are recomposed to produce
the answers to the original question.
3.1 Question Classification: Dynamic and
Static Questions
We automatically classify questions as dynamic
and static questions. The answers to static ques-
tions can be retrieved from the QA archive. To an-
swer dynamic questions, we query the database(s)
associated with the topic of the question through
web forms on the Internet. We first use a topic
classifier to detect the topic of a question followed
by a dynamic/static classifier trained on questions
related to a topic, as shown in Figure 3. For the
question what movies are playing around me?,
we detect it is a movie related dynamic ques-
tion and query a movie information web site (e.g.
www.fandango.com) to retrieve the results based
on the user?s GPS information.
Dynamic questions often contain temporal in-
dexicals, i.e., expressions of the form today, now,
this week, two summers ago, currently, recently,
etc. Our initial approach was to use such signal
words and phrases to automatically identify dy-
namic questions. The chosen signals were based
on annotations in TimeML. We also included spa-
tial indexicals, such as here and other clauses that
were observed to be contained in dynamic ques-
tions such as cost of, and how much is in the list of
signal phrases. These signals words and phrases
were encoded into a regular-expression-based rec-
ognizer.
This regular-expression based recognizer iden-
tified 3.5% of our dataset ? which consisted of
several million questions ? as dynamic. The type
of questions identified were What is playing in
the movie theaters tonight?, What is tomorrow?s
weather forecast for LA?, Where can I go to get
Thai food near here? However, random samplings
of the same dataset, annotated by four independent
human labelers, indicated that on average 13.5%
of the dataset is considered dynamic. This shows
that the temporal and spatial indexicals encoded as
a regular-expression based recognizer is unable to
identify a large percentage of the dynamic ques-
tions.
This approach leaves out dynamic questions
that do not contain temporal or spatial indexicals.
For example, What is playing at AMC Loew?s?, or
What is the score of the Chargers and Dolphines
game?. For such examples, considering the tense
of the verb in question may help. The last two ex-
amples are both in the present continuous tense.
But verb tense does not help for a question such
as Who got voted off Survivor?. This question is
certainly dynamic. The information that is most
likely being sought by this question is what is the
name of the person who got voted off the TV show
Survivor most recently, and not what is the name
of the person (or persons) who have gotten voted
off the Survivor at some point in the past.
Knowing the broad topic (such as movies, cur-
rent affairs, and music) of the question may be
very useful. It is likely that there may be many
dynamic questions about movies, sports, and fi-
nance, while history and geography may have few
or none. This idea is bolstered by the following
analysis. The questions in our dataset are anno-
tated with a broad topic tag. Binning the 3.5%
of our dataset identified as dynamic questions by
their broad topic produced a long-tailed distribu-
tion. Of the 104 broad topics, the top-5 topics con-
tained over 50% of the dynamic questions. These
top five topics were sports, TV and radio, events,
movies, and finance.
Considering the issues laid out in the previ-
ous section, our classification approach is to chain
two machine-learning-based classifiers: a topic
classifier chained to a dynamic/static classifier, as
shown in Figure 3. In this architecture, we build
one topic classifier, but several dynamic/static
classifiers, each trained on data pertaining to one
broad topic.
Figure 3: Chaining two classifiers
We used supervised learning to train the topic
62
classifier, since our entire dataset is annotated by
human experts with topic labels. In contrast, to
train a dynamic/static classifier, we experimented
with the following three different techniques.
Baseline: We treat questions as dynamic if they
contain temporal indexicals, e.g. today, now, this
week, two summers ago, currently, recently, which
were based on the TimeML corpus. We also in-
cluded spatial indexicals such as here, and other
substrings such as cost of and how much is. A
question is considered static if it does not contain
any such words/phrases.
Self-training with bagging: The general self-
training with bagging algorithm (Banko and Brill,
2001). The benefit of self-training is that we can
build a better classifier than that built from the
small seed corpus by simply adding in the large
unlabeled corpus without requiring hand-labeling.
Active-learning: This is another popular method
for training classifiers when not much annotated
data is available. The key idea in active learning
is to annotate only those instances of the dataset
that are most difficult for the classifier to learn to
classify. It is expected that training classifiers us-
ing this method shows better performance than if
samples were chosen randomly for the same hu-
man annotation effort.
We used the maximum entropy classifier in
LLAMA (Haffner, 2006) for all of the above clas-
sification tasks. We have chosen the active learn-
ing classifier due to its superior performance and
integrated it into the Qme! system. We pro-
vide further details about the learning methods in
(Mishra and Bangalore, 2010).
3.2 Experiments and Results
3.2.1 Topic Classification
The topic classifier was trained using a training
set consisting of over one million questions down-
loaded from the web which were manually labeled
by human experts as part of answering the ques-
tions. The test set consisted of 15,000 randomly
selected questions. Word trigrams of the question
are used as features for a MaxEnt classifier which
outputs a score distribution on all of the 104 pos-
sible topic labels. The error rate results for models
selecting the top topic and the top two topics ac-
cording to the score distribution are shown in Ta-
ble 1. As can be seen these error rates are far lower
than the baseline model of selecting the most fre-
quent topic.
Model Error Rate
Baseline 98.79%
Top topic 23.9%
Top-two topics 12.23%
Table 1: Results of topic classification
3.2.2 Dynamic/static Classification
As mentioned before, we experimented with
three different approaches to bootstrapping a dy-
namic/static question classifier. We evaluated
these methods on a 250 question test set drawn
from the broad topic of Movies. The error rates
are summarized in Table 2. We provide further de-
tails of this experiment in (Mishra and Bangalore,
2010).
Training approach Lowest Error rate
Baseline 27.70%
?Supervised? learning 22.09%
Self-training 8.84%
Active-learning 4.02%
Table 2: Best Results of dynamic/static classifica-
tion
4 Retrieving answers to dynamic
questions
Following the classification step outlined in Sec-
tion 3.1, we know whether a user query is static or
dynamic, and the broad category of the question.
If the question is dynamic, then our system per-
forms a vertical search based on the broad topic
of the question. In our system, so far, we have in-
corporated vertical searches on three broad topics:
Movies, Mass Transit, and Yellow Pages.
For each broad topic, we have identified a few
trusted content aggregator websites. For example,
for dynamic questions related to Movies-related
dynamic user queries, www.fandango.com is
a trusted content aggregator website. Other such
trusted content aggregator websites have been
identified for Mass Transit related and for Yellow-
pages related dynamic user queries. We have also
identified the web-forms that can be used to search
these aggregator sites and the search parameters
that these web-forms need for searching. So, given
a user query, whose broad category has been deter-
mined and which has been classified as a dynamic
query by the system, the next step is to parse the
query to obtain pertinent search parameters.
The search parameters are dependent on the
broad category of the question, the trusted con-
tent aggregator website(s), the web-forms associ-
ated with this category, and of course, the content
63
of the user query. From the search parameters, a
search query to the associated web-form is issued
to search the related aggregator site. For exam-
ple, for a movie-related query, What time is Twi-
light playing in Madison, New Jersey?, the per-
tinent search parameters that are parsed out are
movie-name: Twilight, city: Madison, and state:
New Jersey, which are used to build a search string
that Fandango?s web-form can use to search the
Fandango site. For a yellow-pages type of query,
Where is the Saigon Kitchen in Austin, Texas?, the
pertinent search parameters that are parsed out are
business-name: Saigon Kitchen, city: Austin, and
state: Texas, which are used to construct a search
string to search the Yellowpages website. These
are just two examples of the kinds of dynamic user
queries that we encounter. Within each broad cat-
egory, there is a wide variety of the sub-types of
user queries, and for each sub-type, we have to
parse out different search parameters and use dif-
ferent web-forms. Details of this extraction are
presented in (Feng and Bangalore, 2009).
It is quite likely that many of the dynamic
queries may not have all the pertinent search pa-
rameters explicitly outlined. For example, a mass
transit query may be When is the next train to
Princeton?. The bare minimum search parameters
needed to answer this query are a from-location,
and a to-location. However, the from-location is
not explicitly present in this query. In this case,
the from-location is inferred using the GPS sensor
present on the iPhone (on which our system is built
to run). Depending on the web-form that we are
querying, it is possible that we may be able to sim-
ply use the latitude-longitude obtained from the
GPS sensor as the value for the from-location pa-
rameter. At other times, we may have to perform
an intermediate latitude-longitude to city/state (or
zip-code) conversion in order to obtain the appro-
priate search parameter value.
Other examples of dynamic queries in which
search parameters are not explicit in the query, and
hence, have to be deduced by the system, include
queries such as Where is XMen playing? and How
long is Ace Hardware open?. In each of these
examples, the user has not specified a location.
Based on our understanding of natural language,
in such a scenario, our system is built to assume
that the user wants to find a movie theatre (or, is
referring to a hardware store) nearwhere he is cur-
rently located. So, the system obtains the user?s
location from the GPS sensor and uses it to search
for a theatre (or locate the hardware store) within
a five-mile radius of her location.
In the last few paragraphs, we have discussed
how we search for answers to dynamic user
queries from the hidden web by using web-forms.
However, the search results returned by these web-
forms usually cannot be displayed as is in our
Qme! interface. The reason is that the results are
often HTML pages that are designed to be dis-
played on a desktop or a laptop screen, not a small
mobile phone screen. Displaying the results as
they are returned from search would make read-
ability difficult. So, we parse the HTML-encoded
result pages to get just the answers to the user
query and reformat it, to fit the Qme! interface,
which is designed to be easily readable on the
iPhone (as seen in Figure 1).2
5 Retrieving answers to static questions
Answers to static user queries ? questions whose
answers do not change over time ? are retrieved
in a different way than answers to dynamic ques-
tions. A description of how our system retrieves
the answers to static questions is presented in this
section.
0
how:qa25/c1
old:qa25/c2
is:qa25/c3
obama:qa25/c4
old:qa150/c5
how:qa12/c6
obama:qa450/c7
is:qa1450/c8
Figure 4: An example of an FST representing the
search index.
5.1 Representing Search Index as an FST
To obtain results for static user queries, we
have implemented our own search engine using
finite-state transducers (FST), in contrast to using
Lucene (Hatcher and Gospodnetic., 2004) as it is
a more efficient representation of the search index
that allows us to consider word lattices output by
ASR as input queries.
The FST search index is built as follows. We
index each question-answer (QA) pair from our
repository ((qi, ai), qai for short) using the words
(wqi) in question qi. This index is represented as
a weighted finite-state transducer (SearchFST) as
shown in Figure 4. Here a word wqi (e.g old) is the
input symbol for a set of arcs whose output sym-
bol is the index of the QA pairs where old appears
2We are aware that we could use SOAP (Simple Object
Access Protocol) encoding to do the search, however not all
aggregator sites use SOAP yet.
64
in the question. The weight of the arc c(wqi ,qi) is
one of the similarity based weights discussed in
Section 4.1. As can be seen from Figure 4, the
words how, old, is and obama contribute a score to
the question-answer pair qa25; while other pairs,
qa150, qa12, qa450 are scored by only one of
these words.
5.2 Search Process using FSTs
A user?s speech query, after speech recogni-
tion, is represented as a finite state automaton
(FSA, either 1-best or WCN), QueryFSA. The
QueryFSA is then transformed into another FSA
(NgramFSA) that represents the set of n-grams
of the QueryFSA. In contrast to most text search
engines, where stop words are removed from the
query, we weight the query terms with their idf val-
ues which results in a weighted NgramFSA. The
NgramFSA is composed with the SearchFST and
we obtain all the arcs (wq, qawq , c(wq ,qawq )) where
wq is a query term, qawq is a QA index with the
query term and, c(wq ,qawq ) is the weight associ-
ated with that pair. Using this information, we
aggregate the weight for a QA pair (qaq) across
all query words and rank the retrieved QAs in the
descending order of this aggregated weight. We
select the top N QA pairs from this ranked list.
The query composition, QA weight aggregation
and selection of top N QA pairs are computed
with finite-state transducer operations as shown
in Equations 1 and 23. An evaluation of this
search methodology on word lattices is presented
in (Mishra and Bangalore, 2010).
D = pi2(NgramFSA ? SearchFST ) (1)
TopN = fsmbestpath(fsmdeterminize(D), N)
(2)
6 Summary
In this demonstration paper, we have presented
Qme!, a speech-driven question answering system
for use on mobile devices. The novelty of this sys-
tem is that it provides users with a single unified
interface for searching both the visible and the hid-
den web using the most natural input modality for
use on mobile phones ? spoken language.
7 Acknowledgments
We would like to thank Junlan Feng, Michael
Johnston and Mazin Gilbert for the help we re-
ceived in putting this system together. We would
3We have dropped the need to convert the weights into the
real semiring for aggregation, to simplify the discussion.
also like to thank ChaCha for providing us the data
included in this system.
References
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of the 39th annual meeting of the as-
sociation for computational linguistics: ACL 2001,
pages 26?33.
J. Feng and S. Bangalore. 2009. Effects of word con-
fusion networks on voice search. In Proceedings of
EACL-2009, Athens, Greece.
Google, 2009. http://www.google.com/mobile.
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Communi-
cation, 48(iv):239?261.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Ac-
tion (In Action series). Manning Publications Co.,
Greenwich, CT, USA.
Microsoft, 2009. http://www.live.com.
T. Mishra and S. Bangalore. 2010. Qme!: A speech-
based question-answering system on mobile de-
vices. In Proceedings of NAACL-HLT.
D. Moldovan, C. Clark, and S. Harabagiu. 2005. Tem-
poral context representation and reasoning. In Pro-
ceedings of the 19th International Joint Conference
on Artificial Intelligence, pages 1009?1104.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: Qa-by-
dossier-with-contraints. In Proceedings of the 42nd
annual meeting of the association for computational
linguistics: ACL 2004, pages 574?581.
J. Pustejovsky, R. Ingria, R. Saur??, J. Casta no,
J. Littman, and R. Gaizauskas., 2001. The language
of time: A reader, chapter The specification languae
? TimeML. Oxford University Press.
D. Radev and B. Sundheim. 2002. Using timeml in
question answering. Technical report, Brandies Uni-
versity.
E. Saquete, J. L. Vicedo, P. Mart??nez-Barco, R. Mu noz,
and H. Llorens. 2009. Enhancing qa systems with
complex temporal question processing capabilities.
Journal of Artificial Intelligence Research, 35:775?
811.
vlingo.com, 2009.
http://www.vlingomobile.com/downloads.html.
YellowPages, 2009. http://www.speak4it.com.
65
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 609?613,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Predicting Relative Prominence in Noun-Noun Compounds
Taniya Mishra
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
taniya@research.att.com
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Abstract
There are several theories regarding what in-
fluences prominence assignment in English
noun-noun compounds. We have developed
corpus-driven models for automatically pre-
dicting prominence assignment in noun-noun
compounds using feature sets based on two
such theories: the informativeness theory and
the semantic composition theory. The eval-
uation of the prediction models indicate that
though both of these theories are relevant, they
account for different types of variability in
prominence assignment.
1 Introduction
Text-to-speech synthesis (TTS) systems stand to
gain in improved intelligibility and naturalness if
we have good control of the prosody. Typically,
prosodic labels are predicted through text analysis
and are used to control the acoustic parameters for
a TTS system. An important aspect of prosody pre-
diction is predicting which words should be prosod-
ically prominent, i.e., produced with greater en-
ergy, higher pitch, and/or longer duration than the
neighboring words, in order to indicate the for-
mer?s greater communicative salience. Appropriate
prominence assignment is crucial for listeners? un-
derstanding of the intended message. However, the
immense prosodic variability found in spoken lan-
guage makes prominence prediction a challenging
problem. A particular sub-problem of prominence
prediction that still defies a complete solution is pre-
diction of relative prominence in noun-noun com-
pounds.
Noun-noun compounds such as White House,
cherry pie, parking lot, Madison Avenue, Wall
Street, nail polish, french fries, computer program-
mer, dog catcher, silk tie, and self reliance, oc-
cur quite frequently in the English language. In a
discourse neutral context, such constructions usu-
ally have leftmost prominence, i.e., speakers produce
the left-hand noun with greater prominence than the
right-hand noun. However, a significant portion ?
about 25% (Liberman and Sproat, 1992) ? of them
are assigned rightmost prominence (such as cherry
pie, Madison Avenue, silk tie, computer program-
mer, and self reliance from the list above). What
factors influence speakers? decision to assign left or
right prominence is still an open question.
There are several different theories about rela-
tive prominence assignment in noun-noun (hence-
forth, NN) compounds, such as the structural the-
ory (Bloomfield, 1933; Marchand, 1969; Heinz,
2004), the analogical theory (Schmerling, 1971;
Olsen, 2000), the semantic theory (Fudge, 1984;
Liberman and Sproat, 1992) and the informativeness
theory (Bolinger, 1972; Ladd, 1984).1 However, in
most studies, the different theories are examined and
applied in isolation, thus making it difficult to com-
pare them directly. It would be informative and il-
luminating to apply these theories to the same task
and the same dataset.
For this paper, we focus on two particular the-
ories, the informativeness theory and the seman-
tic composition theory. The informativeness theory
posits that the relatively more informative and un-
expected noun is given greater prominence in the
NN compound than the less informative and more
predictable noun. The semantic composition theory
posits that relative prominence assignment in NN
compounds is decided according to the semantic re-
lationship between the two nouns.
We apply these two theories to the task of pre-
dicting relative prominence in NN compounds via
statistical corpus-driven methods, within the larger
context of building a system that can predict appro-
priate prominence patterns for text-to-speech syn-
thesis. Here we are only focusing on predicting rela-
tive prominence of NN compounds in a neutral con-
text, where there are no pragmatic reasons (such as
contrastiveness or given/new distinction) for shifting
prominence.
1In-depth reviews of the different theories can be found in
Plag (2006) and Bell and Plag (2010).
609
2 Informativeness Measures
We used the following five metrics to capture the
individual and relative informativeness of nouns in
each NN compound:
? Unigram Predictability (UP): Defined as the
predictability of a word given a text corpus, it
is measured as the log probability of the word
in the text corpus. Here, we use the maximum
likelihood formulation of this measure.
UP = log
Freq(wi)
?
i Freq(wi)
(1)
This is a very simple measure of word informa-
tiveness that has been shown to be effective in
a similar task (Pan and McKeown, 1999).
? Bigram Predictability (BP): Defined as the pre-
dictability of a word given a previous word, it
is measured as the log probability of noun N2
given noun N1.
BP = log (Prob(N2 | N1)) (2)
? Pointwise Mutual Information (PMI): Defined
as a measure of how collocated two words are,
it is measured as the log of the ratio of probabil-
ity of the joint event of the two words occurring
and the probability of them occurring indepen-
dent of each other.
PMI = log
Prob(N1, N2)
Prob(N1)Prob(N2)
(3)
? Dice Coefficient (DC): Dice is another colloca-
tion measure used in information retrieval.
DC =
2? Prob(N1, N2)
Prob(N1) + Prob(N2)
(4)
? Pointwise Kullback-Leibler Divergence (PKL):
In this context, Pointwise Kullback-Leibler di-
vergence (a formulation of relative entropy)
measures the degree to which one over-
approximates the information content of N2 by
failing to take into account the immediately
preceding word N1. (PKL values are always
negative.) A high absolute value of PKL indi-
cates that there is not much information con-
tained in N2 if N1 is taken into account. We
define PKL as
Prob(N2 | N1) log
Prob(N2 | N1)
Prob(N2)
(5)
Another way to consider PKL is as PMI nor-
malized by the predictability of N2 given N1.
All except the first the aforementioned five infor-
mativeness measures are relative measures. Of
these, PMI and Dice Coefficient are symmetric mea-
sures while Bigram Predictability and PKL are non-
symmetric (unidirectional) measures.
3 Semantic Relationship Modeling
We modeled the semantic relationship between the
two nouns in the NN compound as follows. For
each of the two nouns in each NN compound, we
maintain a semantic category vector of 26 elements.
The 26 elements are associated with 26 semantic
categories (such as food, event, act, location, arti-
fact, etc.) assigned to nouns in WordNet (Fellbaum,
1998). For each noun, each element of the semantic
category vector is assigned a value of 1, if the lem-
matized noun (i.e., the associated uninflected dic-
tionary entry) is assigned the associated semantic
category by WordNet, otherwise, the element is as-
signed a value of 0. (If a semantic category vector is
entirely populated by zeros, then that noun has not
been assigned any semantic category information by
WordNet.) We expected the cross-product of the se-
mantic category vectors of the two nouns in the NN
compound to roughly encode the possible semantic
relationships between the two nouns, which ? fol-
lowing the semantic composition theory ? corre-
lates with prominence assignment to some extent.
4 Semantic Informativeness Features
For each noun in each NN compound, we also
maintain three semantic informativeness features:
(1) Number of possible synsets associated with the
noun. A synset is a set of words that have the same
sense or meaning. (2) Left positional family size and
(3) Right positional family size. Positional family
size is the number of unique NN compounds that in-
clude the particular noun, either on the left or on the
right (Bell and Plag, 2010). These features are ex-
tracted from WordNet as well.
The intuition behind extracting synset counts and
positional family size was, once again, to measure
the relative informativeness of the nouns in NN com-
pounds. Smaller synset counts indicate more spe-
cific meaning of the noun, and thus perhaps more
information content. Larger right (or left) posi-
tional family size indicates that the noun is present
610
in the right (left) position of many possible NN com-
pounds, and thus less likely to receive higher promi-
nence in such compounds.
These features capture type-based informative-
ness, in contrast to the measures described in Sec-
tion 2, which capture token-based informativeness.
5 Experimental evaluation
For our evaluation, we used a hand-labeled corpus
of 7831 NN compounds randomly selected from the
1990 Associated Press newswire, and hand-tagged
for leftmost or rightmost prominence (Sproat, 1994).
This corpus contains 64 pairs of NN compounds that
differ in terms of capitalization but not in terms of
relative prominence assignment. It only contains
four pairs of NN compounds that differ in terms of
capitalization and in terms of relative prominence
assignment. Since there is not enough data in this
corpus to consider capitalization as a feature, we re-
moved the case information (by lowercasing the en-
tire corpora), and removed any duplicates. Of the
four pairs that differed in terms of capitalization,
we only retained the lower-cased NN compounds.
By normalizing Sproat?s hand-labeled corpus in this
way, we created a slightly smaller corpus 7767 ut-
terances that was used for the evaluation.
For each of the NN compounds in this corpus, we
computed the three aforementioned feature sets. To
compute the informativeness features, we used the
LDC English Gigaword corpus. The semantic cate-
gory vectors and the semantic informativeness fea-
tures were obtained from Wordnet. Using each of
the three feature sets individually as well as com-
bined together, we built automatic relative promi-
nence prediction models using Boostexter, a dis-
criminative classification model based on the boost-
ing family of algorithms, which was first proposed
in Freund and Schapire (1996).
Following an experimental methodology similar
to Sproat (1994), we used 88% (6835 samples) of
the corpus as training data and the remaining 12%
(932 samples) as test data. For each test case, the
output of the prediction models was either a 0 (indi-
cating that the leftmost noun receive higher promi-
nence) or a 1 (indicating that the rightmost noun re-
ceive higher prominence). We estimated the model
error of the different prediction models by comput-
ing the relative error reduction from the baseline er-
ror. The baseline error was obtained by assigning
the majority class to all test cases. We avoided over-
fitting by using 5-fold cross validation.
5.1 Results
The results of the evaluation of the different models
are presented in Table 1. In this table, INF denotes
informativeness features (Sec. 2), SRF denotes se-
mantic relationship modeling features (Sec. 3) and
SIF denotes semantic informativeness features (Sec.
4). We also present the results of building prediction
models by combining different features sets.
These results show that each of the prediction
models reduces the baseline error, thus indicating
that the different types of feature sets are each cor-
related with prominence assignment in NN com-
pounds to some extent. However, it appears that
some feature sets are more predictive. Of the indi-
vidual feature sets, SRF and INF features appear to
be more predictive than the SIF features. Combined
together, the three feature sets are most predictive,
reducing model error over the baseline error by al-
most 33% (compared to 16-22% for individual fea-
ture sets), though combining INF with SRF features
almost achieves the same reduction in baseline error.
Note that none of the three types of feature sets
that we have defined contain any direct lexical infor-
mation such as the nouns themselves or their lem-
mata. However, considering that the lexical con-
tent of the words is a rich source of information that
could have substantial predictive power, we included
the lemmata associated with the nouns in the NN
compounds as additional features to each feature set
and rebuilt the prediction models. An evaluation of
these lexically-enhanced models is shown in Table
2. Indeed, addition of the lemmatized form of the
NN compounds substantially increases the predic-
tive power of all the models. The baseline error is
reduced by almost 50% in each of the models ?
the error reduction being the greatest (53%) for the
model built by combining all three feature sets.
6 Discussion and Conclusion
Several other studies have examined the main idea of
relative prominence assignment using one or more
of the theories that we have focused on in this paper
(though the particular tasks and terminology used
were different) and found similar results. For exam-
ple, Pan and Hirschberg (2000) have used some of
the same informativeness measures (denoted by INF
above) to predict pitch accent placement in word bi-
611
Feature Av. baseline Av. model % Error
Sets error (in %) error (in %) reduction
INF 29.18 22.85 21.69
SRF 28.04 21.84 22.00
SIF 29.22 24.36 16.66
INF-SRF 28.52 19.53 31.55
INF-SIF 28.04 21.25 24.33
SRF-SIF 29.74 21.30 28.31
All 28.98 19.61 32.36
Table 1: Results of prediction models
Feature Av. baseline Av. model % Error
Sets error (in %) error (in %) reduction
INF 28.6 14.67 48.74
SRF 28.34 14.29 49.55
SIF 29.48 14.85 49.49
INF-SRF 28.16 14.81 47.45
INF-SIF 28.38 14.16 50.03
SRF-SIF 29.24 14.51 50.30
All 28.12 13.19 52.95
Table 2: Results of lexically-enhanced prediction models
grams. Since pitch accents and perception of promi-
nence are strongly correlated, their conclusion that
informativeness measures are a good predictor of
pitch accent placement agrees with our conclusion
that informativeness measures are useful predictors
of relative prominence assignment. However, we
cannot compare their results to ours directly, since
their corpus and baseline error measurement2 were
different from ours.
Our results are more directly comparable to those
shown in Sproat (1994). For the same task as we
consider in this study, besides developing a rule-
based system, Sproat also developed a statistical
corpus-based model. His feature set was developed
to model the semantic relationship between the two
nouns in the NN compound, and included the lem-
mata related to the nouns. The model was trained
and tested on the same hand-labeled corpus that we
used for this study and the baseline error was mea-
sured in the same way. So, we can directly com-
pare the results of our lexically-enhanced SRF-based
models to Sproat?s corpus-driven statistical model.
2Pan and Hirschberg present error obtained by using a
unigram-based predictability model as baseline error. It is un-
clear what is the error obtained by assigning left prominence to
all words in their database, which was our baseline error.
In his work, Sproat reported a baseline error of 30%
and a model error of 16%. The reported relative im-
provement over the baseline error in Sproat?s study
was 46.6%, while our relative improvement using
the lexically enhanced SRF based model was 49.5%,
and the relative improvement using the combined
model is 52.95%.
Type-based semantic informativeness features of
the kind that we grouped as SIF were analyzed
in Bell and Plag (2010) as potential predictors of
prominence assignment in compound nouns. Like
us, they too found such features to be predictive
of prominence assignment and that combining them
with features that model the semantic relationship in
the NN compound makes them more predictive.
7 Conclusion
The goal of the presented work was predicting rel-
ative prominence in NN compounds via statistical
corpus-driven methods. We constructed automatic
prediction models using feature sets based on two
different theories about relative prominence assign-
ment in NN compounds: the informativeness theory
and the semantic composition theory. In doing so,
we were able to compare the two theories.
Our evaluation indicates that each of these theo-
ries is relevant, though perhaps to different degrees.
This is supported by the observation that the com-
bined model (in Table 1) is substantially more pre-
dictive than any of the individual models. This indi-
cates that the different feature sets capture different
correlations, and that perhaps each of the theories
(on which the feature sets are based) account for dif-
ferent types of variability in prominence assignment.
Our results also highlight the difference between
being able to use lexical information in prominence
prediction of NN compounds, or not. Using lexical
features, we can improve prediction over the default
case (i.e., assigning prominence to the left noun in
all cases) by over 50%. But if the given input is an
out-of-vocabulary NN compound, our non-lexically
enhanced best model can still improve prediction
over the default by about 33%.
Acknowledgment We would like to thank
Richard Sproat for freely providing the dataset on
which the developed models were trained and tested.
We would also like to thank him for his advice on
this topic.
612
References
M. Bell and I. Plag. 2010. Informativeness
is a determinant of compound stress in En-
glish. Submitted for publication. Obtained from
http://www2.uni-siegen.de/?engspra/
publicat.html on February 12, 2010.
L. Bloomfield. 1933. Language, Holt, New York.
D. Bolinger. 1972. Accent is predictable (if you?re a
mind-reader). Language 48.
C. Fellbaum (editor). 1998. WordNet: An Electronic
Lexical Database, The MIT Press, Boston.
Y. Freund and R. E. Schapire, 1996. Experiments with
a new boosting alogrithm. Machine Learning: Pro-
ceedings of the Thirteenth International Conference,
pp. 148-156.
E. Fudge. 1984. English Word-Stress, Allen and Unwin,
London and Boston.
H. J. Giegerich. Compound or phrase? English noun-
plus-noun constructions and the stress criterion. In
English Language and Linguistics, 8:1?24.
R. D. Ladd, 1984. English compound stress. In Dafydd
Gibbon and Helmut Richter (eds.) Intonation, Accent
and Rhythm: Studies in 1188 Discourse Phonology,
W de Gruyter, Berlin.
M. Liberman and R. Sproat. 1992. The Stress and Struc-
ture of Modified Noun Phrases in English. In I. Sag
(ed.), Lexical Matters, pp. 131?181, CSLI Publica-
tions, Chicago, University of Chicago Press.
H. Marchand. The categories and types of present-day
English word-formation, Beck, Munich.
S. Olsen. 2000. Compounding and stress in English: A
closer look at the boundary between morphology and
syntax. Linguistische Berichte, 181:55?70.
S. Pan and J. Hirschberg. 2000. Modeling local context
for pitch accent prediction. Proceedings of the 38th
Annual Conference of the Association for Computa-
tional Linguistics (ACL-00), pp. 233-240, Hong Kong.
ACL.
S. Pan and K. McKeown. 1999. Word informativeness
and automatic pitch accent modeling. Proceedings of
the Joint SIGDAT Conference on EMNLP and VLC,
pp. 148?157.
I. Plag. 2006. The variability of compound stress in En-
glish: structural, semantic and analogical factors. En-
glish Language and Linguistics, 10.1, pp. 143?172.
R. Sproat. 1994. English Noun-Phrase Accent Prediction
for Text-to-Speech. Computer Speech and Language,
8, pp. 79?94.
R.E. Schapire, A brief introduction to boosting. In Pro-
ceedings of IJCAI, 1999.
S. F. Schmerling. 1971. A stress mess. Studies in the
Linguistic Sciences, 1:52?65.
613
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 40?49,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
From Speaker Identification to Affective Analysis:
A Multi-Step System for Analyzing Children?s Stories
Elias Iosif? and Taniya Mishra?
? School of ECE, Technical University of Crete, Chania 73100, Greece
? AT&T Labs, 33 Thomas Street, New York, NY 10007, USA
iosife@telecom.tuc.gr, taniya@research.att.com
Abstract
We propose a multi-step system for the
analysis of children?s stories that is in-
tended to be part of a larger text-to-speech-
based storytelling system. A hybrid ap-
proach is adopted, where pattern-based
and statistical methods are used along with
utilization of external knowledge sources.
This system performs the following story
analysis tasks: identification of charac-
ters in each story; attribution of quotes
to specific story characters; identification
of character age, gender and other salient
personality attributes; and finally, affective
analysis of the quoted material. The differ-
ent types of analyses were evaluated using
several datasets. For the quote attribution,
as well as for the gender and age estima-
tion, substantial improvement over base-
line was realized, whereas results for per-
sonality attribute estimation and valence
estimation are more modest.
1 Introduction
Children love listening to stories. Listening to
stories ? read or narrated ? has been shown
to be positively correlated with children?s linguis-
tic and intellectual development (Natsiopoulou et
al., 2006). Shared story reading with parents or
teachers helps children to learn about vocabulary,
syntax and phonology, and to develop narrative
comprehension and awareness of the concepts of
print, all of which are linked to developing reading
and writing skills (National Early Literacy Panel
2008). While acknowledging that the parental
role in storytelling is irreplaceable, we consider
text-to-speech (TTS) enabled storytelling systems
(Rusko et al., 2013; Zhang et al., 2003; Theune
et al., 2006) to be aligned with the class of child-
oriented applications that aim to aid learning.
For a TTS-based digital storytelling system to
successfully create an experience as engaging as
human storytelling, the underlying speech synthe-
sis system has to narrate the story in a ?story-
telling speech style? (Theune et al., 2006), gen-
erate dialogs uttered by different characters using
synthetic voices appropriate for each character?s
gender, age and personality (Greene et al., 2012),
and express quotes demonstrating emotions such
as sadness, fear, happiness, anger and surprise
(Alm, 2008) with realistic expression (Murray and
Arnott, 2008). However, before any of the afore-
mentioned requirements ? all related to speech
generation ? can be met, the text of the story has
to be analyzed to identify which portions of the
text should be rendered by the narrator and which
by each of the characters in the story, who are the
different characters in the story, what is each char-
acter?s gender, age, or other salient personality at-
tributes that may influence the voice assigned to
that character, and what is the expressed affect in
each of the character quotes.
Each of these text analysis tasks has been ap-
proached in past work (as described in our Re-
lated Works section). However, there appears to
be no single story analysis system that performs
all four of these tasks, which can be pipelined with
one of the many currently available text-to-speech
systems to build a TTS-based storyteller system.
Without such a story analysis system, it will not be
possible to develop an engaging and lively digital
storyteller system, despite the prevalence of sev-
eral mature TTS systems.
40
In this paper, we present a multi-step text analy-
sis system for analyzing children?s stories that per-
forms all four analysis tasks: (i) Character Identi-
fication, i.e., identifying the different characters in
the story, (ii) Quote Attribution, i.e., identifying
which portions of the text should be rendered by
the narrator versus by particular characters in the
story, (iii) Character Attribute Identification, i.e.,
identifying each character?s gender, age, or salient
personality attributes that may influence the voice
that the speech synthesis system assigns to each
character, and (iv) Affective Analysis, i.e., esti-
mating the affect of the character quotes.
This story analysis system was developed to
be part of a larger TTS-based storyteller system
aimed at children. As a result, the data used for
developing the computational models or rules in
each step of our system were obtained from chil-
dren?s stories. A majority of children?s stories
are short. They often contain multiple characters,
each with different personalities, genders, age,
ethnicities, etc., with some characters even be-
ing anthropomorphic, e.g., the singing candlestick
or the talking teapot. In addition, there are sev-
eral prototypical templates characterizing the main
characters in the story (Rusko et al., 2013). How-
ever, character development is limited in these sto-
ries due to the shorter length of text. Overall,
children?s stories can be regarded as a parsimo-
nious yet fertile framework for developing compu-
tational models for literature analysis in general.
2 Related Work
Elson and McKeown (2010) used rule-based and
statistical learning approaches to identify candi-
date characters and attribute each quote to the most
likely speaker. Two broad approaches for the iden-
tification of story characters were followed: (i)
named entity recognition, and (ii) identification
of character nominals, e.g., ?her grandma?, using
syntactic patterns. A long list of heuristics for
character identification is proposed in (Mamede
and Chaleira, 2004). He et al. (2013) use a su-
pervised machine learning approach to address the
same problem, though many of their preliminary
steps and input features are similar to those used in
(Elson and McKeown, 2010). Our character iden-
tification and quote attribution is based on syntac-
tic and heuristic rules that is motivated by each of
these works.
There are two interesting sub-problems related
to quote attribution. First is the problem of iden-
tifying anaphoric speakers, i.e., in the utterance
?Hello?, he said, which character is referred to by
the pronoun he? This problem is addressed in (El-
son and McKeown, 2010) and (He et al., 2013) but
not in (Mamede and Chaleira, 2004). The second
problem is resolving utterance chains with implicit
speakers. Elson and McKeown (2010) describe
and address two basic types of utterance chains: (i)
one-character chains, and (ii) intertwined chains.
In these chains of utterances, the speaker is not
explicitly mentioned because the author relies on
the shared understanding with the reader that adja-
cent pieces of quoted speech are not independent
(Zhang et al., 2003; Elson and McKeown, 2010).
They are either a continuation of the same charac-
ter?s speech (one-character chains) or a dialogue
between the two characters (intertwined chains).
In (Zhang et al., 2003), the quote-identification
module detects whether a piece of quoted speech
is a new quote (NEW), spoken by a speaker dif-
ferent from the previous speaker, or a continuation
quote (CONT) spoken by the same speaker as that
of the previous quote. He et al. (2013) also iden-
tified similar chains of utterances and addressed
their attribution to characters using a model-based
approach. In this work, we address both sub-
problems, namely, anaphoric speaker and implicit
speaker identification.
Cabral et al. (2006) have shown that assign-
ing an appropriate voice for a character in a digi-
tal storyteller system is significant for understand-
ing a story, perceiving affective content, perceiv-
ing the voice as credible, and overall listener sat-
isfaction. Greene et al. (2012) have shown that
the appropriateness of the voice assigned to a syn-
thetic character is strongly related to knowing the
gender, age and other salient personality attributes
of the character. Given this, we have developed
rule-based, machine-learning-based and resource-
based approaches for estimation of character gen-
der, age and salient personality attributes. In con-
trast, the majority of past works on the analysis of
children stories for TTS-based storytelling is lim-
ited to the attribution of quotes to speakers, though
studies that focused on anaphoric speaker iden-
tification have also approached character gender
estimation such as (Elson and McKeown, 2010)
and (He et al., 2013). The utilization of available
resources containing associations between person
names and gender was followed in (Elson and
41
McKeown, 2010). In (He et al., 2013), associ-
ations between characters and their gender were
performed using anaphora rules (Mitkov, 2002).
There is of course a significant body of work
from other research areas that are related to the
estimation of character attributes, similar to what
we have attempted in our work. Several shal-
low linguistic features were proposed in (Schler
et al., 2006) for gender identification, applied to
the identification of users in social media. Several
socio-linguistic features were proposed in (Rao et
al., 2010) for estimating the age and gender of
Twitter users. The identification of personality at-
tributes from text is often motivated by psycho-
logical models. In (Celli, 2012), a list of linguis-
tic features were used for the creation of character
models in terms of the the Big Five personality di-
mensions (Norman, 1963).
Analysis of text to estimate affect or sentiment
is a relatively recent research topic that has at-
tracted great interest, as reflected by a series of
shared evaluation tasks, e.g., analysis of news
headlines (Strapparava and Mihalcea, 2007) and
tweets (Nakov et al., 2013). Relevant applications
deal with numerous domains such as blogs (Balog
et al., 2006), news stories (Lloyd et al., 2005), and
product reviews (Hu and Liu, 2004). In (Turney
and Littman, 2002), the affective ratings of un-
known words were predicted using the affective
ratings for a small set of words (seeds) and the se-
mantic relatedness between the unknown and the
seed words. An example of sentence-level analy-
sis was proposed in (Malandrakis et al., 2013). In
(Alm et al., 2005) and (Alm, 2008), linguistic fea-
tures were used for affect analysis in fairy tales. In
our work, we employ a feature set similar to that
in (Alm et al., 2005). We deal with the prediction
of three basic affective labels which are adequate
for the intended application (i.e., storytelling sys-
tem), while in (Alm, 2008) more fine-grained pre-
dictions are considered.
The integration of various types of analysis con-
stitutes the distinguishing character of our work.
3 Overview of System Architecture
The system consists of several sub-systems that
are linked in a pipeline. The input to the system
is simply the text of a story with no additional
annotation. The story analysis is performed se-
quentially, with each sub-system extracting spe-
cific information needed to perform the four anal-
ysis tasks laid out in this paper.
3.1 Linguistic Preprocessing
The first step is linguistic pre-processing of the
stories. This includes (i) tokenization, (ii) sen-
tence splitting and identification of paragraph
boundaries, (iii) part-of-speech (POS) tagging,
(iv) lemmatization, (v) named entity recognition,
(vi) dependency parsing, and (vii) co-reference
analysis. These sub-tasks ? except task (ii) ?
were performed using the Stanford CoreNLP suite
of tools (CoreNLP, 2014). Sentence splitting and
identification of paragraph boundaries was per-
formed using a splitter developed by Piao (2014).
Linguistic information extracted by this analysis is
exploited by the subsequent parts of the pipeline.
3.2 Identification of Story Characters
The second step is identifying candidate charac-
ters (i.e., entities) that appear in the stories under
analysis. A story character is not necessarily a
story speaker. A character may appear in the story
but may not have any quote associated with him
and hence, is not a speaker. Characters in chil-
dren?s stories can either be human or non-human
entities, i.e., animals and non-living objects, ex-
hibiting anthropomorphic traits. The interactions
among characters can either be human-to-human
or human-to-non-human interactions.
We used two approaches for identifying story
characters motivated by (Elson and McKeown,
2010): 1) named entity recognition was used for
identifying proper names, e.g., ?Hansel?, 2) a
set of part-of-speech patterns was used for the
extraction of human and non-human characters
that were not represented by proper names, e.g.,
?wolf?. The used patterns are: 1) (DT|CD)
(NN|NNS), 2) DT JJ (NN|NNS), 3) NN POS
(NN|NNS), and 4) PRP$ JJ (NN|NNS).
These POS-based patterns are quite generic, al-
lowing for the creation of large sets of characters.
In order to restrict the characters, world knowl-
edge was incorporated through the use of Word-
Net (Fellbaum, 2005). A similar approach was
also followed in (Elson and McKeown, 2010). For
each candidate character the hierarchy of its hy-
pernyms was traversed up to the root. Regarding
polysemous characters the first two senses were
considered. A character was retained if any of its
hypernyms was found to fall into certain types of
WordNet concepts: person, animal, plant, artifact,
spiritual being, physical entity.
42
3.3 Quote Attribution & Speaker
Identification
Here the goal is to attribute (or assign) each quote
to a specific story character from the set identified
in the previous step. The identification of quotes
in the story is based on a simple pattern-based ap-
proach: the quote boundaries are signified by the
respective symbols, e.g., ? and ?. The pattern is
applied at the sentence level.
The quotes are not modeled as NEW/CONT as
in (Zhang et al., 2003), however, we adopt a more
sophisticated approach for the quote attribution.
Three types of attribution are possible in our sys-
tem: 1) explicit mention of speakers, e.g., ?Done!?
said Hans, merrily, 2) anaphoric mention of speak-
ers, e.g., ?How happy am I!? cried he, 3) sequence
of quotes, e.g., ?And where did you get the pig??
. . . ?I gave a horse for it.?. In the first type of attri-
bution, the speaker is explicitly mentioned in the
vicinity of the quote. This is also true for the sec-
ond type, however, a pronominal anaphora is used
to refer to the the speaker. The first two attribution
types are characterized by the presence of ?within-
quote? (e.g., ?Done!?) and ?out-of-quote? (e.g.,
?said Hans, merrily.?) content. This is not the
case for the third attribution type for which only
?in-quote? content is available. We refer to such
quotes as ?pure? quotes. Each attribution type is
detailed below.
Preliminary filtering of characters. Before
quote-attribution is performed, the list of story
characters is pruned by identifying the characters
that are ?passively? associated with speech verbs
(SV). This is applied at the sentence level. Some
examples of speech verbs are: said, responds, sing,
etc. For instance, in ?. . . Hans was told . . . ?,
?Hans? is a passive character. The passive char-
acters were identified via the following relations
extracted by dependency parsing: nsubjpass
(passive nominal subject) and pobj (object of a
preposition). Given a sentence that includes one
or more quotes, the respective passive characters
were not considered as candidate speakers. Some
other criteria for pruning of list of characters to
identify candidate speakers are presented in Sec-
tion 4.2 (see the three schemes for Tasks 1-2).
Explicit mention of speakers. Several syntac-
tic patterns were applied to associate quotes with
explicit mention of speakers in their vicinity to
characters from the pruned list of story charac-
ters. These patterns were developed around SV.
In the example above, ?Hans? is associated with
the quote ?Done!? via the SV ?said?. Variations
of the following basic patterns (Elson and McKe-
own, 2010) were used: 1) QT SV CH, 2) QT CH
SV, and 3) CH SV QT, where QT denotes a quote
boundary and CH stands for a story character. For
example, a variation of the first pattern is QT SV
the? CH, where ? stands for zero or one oc-
currence of ?the?.
A limitation of the aforementioned patterns
is that they capture associations when the CH
and SV occur in close textual distance. As
a result, distant associations are missed, e.g.,
?Hans stood looking on for a while, and at last
said, ? You must . . . ??. In order to address
this distant association issue, we examined the
collapsed-ccprocessed-dependencies
output besides the basic-dependenciesout-
put of the Stanford CoreNLP dependency engine
(de Marneffe and Manning, 2012). The former
captures more distant relations compared to
the latter. We specifically extract the character
reference CH either from the dependency relation
nsubj, which links a speech verb SV with a CH
that is the syntactic subject of a clause, or from the
dependency relation dobj, which links a SV with
a CH that is the direct object of the speech verb,
across a conjunct (e.g., and). A similar approach
was used in (He et al., 2013).
Anaphoric mention of speakers. The same
procedure was followed as in the case of the ex-
plicit mentions of speakers described above. The
difference is that CH included the following pro-
nouns: ?he?, ?she?, ?they?, ?himself?, ?herself?,
and ?themselves?. After associating a pronoun
with a quote, the quote was attributed to a story
character via co-reference resolution. This was
done using the co-reference analysis performed
by CoreNLP. If a pronominal anaphora was not
resolved by the CoreNLP analysis, the follow-
ing heuristic was adopted. The previous n para-
graphs1 were searched and the pronoun under in-
vestigation was mapped to the closest (in terms
of textual proximity) story character that had the
same gender as the pronoun (see Section 3.4.1 re-
garding gender estimation). During the paragraph
search, anaphoric mentions were also taken into
consideration followed by co-reference resolution.
Despite the above approaches, it is possible to
have non-attributed quotes. In such cases, the fol-
1For the reported results n was set to 5.
43
lowing procedure is followed for those story sen-
tences that: (i) do not constitute ?pure? quotes
(i.e., consist of ?in-quote? and ?out-of-quote? con-
tent), and (ii) include at least one ?out-of-quote?
SV: 1) all the characters (as well as pronouns) that
occur within the ?out-of-quote? content are aggre-
gated and serve as valid candidates for attribution,
2) if multiple characters and pronouns exist, then
they are mapped (if possible) via co-reference res-
olution in order to narrow down the list of attri-
bution candidates, and 3) the quote is attributed
to the nearest quote character (or pronoun). For
the computation of the textual distance both quote
boundaries (i.e., start and end) are considered. If
the quote is attributed to a pronoun that was not
mapped to any character, then co-reference reso-
lution is applied.
Sequence of ?pure? quotes. Sentences that
are ?pure? quotes (i.e., include ?in-quote? con-
tent only) are not attributed to any story charac-
ter via the last two attribution methods. ?Pure?
quotes are attributed as follows: The sentences
are parsed sequentially starting from the begin-
ning of the story. Each time a character is encoun-
tered within a sentence, it is pushed into a ?bag-
of-characters?. This is done until a non-attributed
?pure? quote is found. At this point we assume
that the candidate speakers for the current (and
next) ?pure? quote are included within the ?bag-
of-characters?. This is based on the hypothesis
that the author ?introduces? the speakers before
their utterances. The subsequent ?pure? quotes are
examined in order to spot any included characters.
Such characters are regarded as ?good? candidates
enabling the pruning of the list of candidate speak-
ers. The goal is to end up with exactly two candi-
date speakers for a back and forth dialogue. Then,
the initiating speaker is identified by taking into
account the order of names mentioned within the
quote. Then, the quote attribution is performed in
an alternating fashion. For example, consider a
sequence of four non-attributed ?pure? quotes and
a bag of two2 candidate speakers, s
i
and s
j
. If s
i
was identified as the initiating speaker, then the 1st
and the 3th quote are attributed to it, while the 2nd
and the 4th quote are attributed to s
j
. Finally, the
?bag-of-characters? is reset, and the same process
is repeated for the rest of the story.
Identification of speakers. The speakers for a
2If more than two candidates exist, then the system gives
ambiguous attributions, i.e., multiple speakers for one quote.
given story are identified by selecting those char-
acters that were attributed at least one quote.
3.4 Gender, Age and Personality Attributes
The next three steps in our system involve estima-
tion of the (i) gender, (ii) age, and (iii) personality
attributes for the identified speakers.
3.4.1 Gender Estimation
We used a hybrid approach for estimating the gen-
der of the story characters. This is applied to char-
acters (rather than only speakers) because the gen-
der information is exploited during the attribution
of quotes (see Section 3.3). The characterization
?hybrid? refers to the fusion of two different types
of information: (i) linguistic information extracted
from the story under analysis, and (ii) information
taken from external resources that do not depend
on the analyzed story. Regarding the story-specific
information, the associations between characters
and third person pronouns (identified via anaphora
resolution) were counted. The counts were used in
order to estimate the gender probability.
The story-independent resources that we used
are: (a) the U.S. Social Security Administration
baby name database (Security, 2014), in which
person names are linked with gender and (b) a
large name-gender association list developed us-
ing a corpus-based bootstrapping approach, which
even included the estimated gender for non-person
entities (Bergsma and Lin, 2006). For each entity
included in (b) a numerical estimate is provided
for each gender. As in the case of story-specific in-
formation, those estimates were utilized for com-
puting the gender probability. Using the above in-
formation the following procedure was followed
for each character: The external resource (a) was
used when the character name occurred in it. Oth-
erwise, the information from the external resource
(b) and the story-specific information was taken
into account. If the speaker was covered by both
types of information, the respective gender prob-
abilities were compared and the gender was esti-
mated to be the one corresponding to the high-
est probability. If the character was not covered
by the story-specific information, the external re-
source (b) was used.
3.4.2 Age Estimation
We used a machine-learning based approach for
age estimation. The used features are presented in
Table 1, while they were extracted from speaker
44
quotes, based on the assumption that speakers of
different ages use language differently. The
No. Description
1 count of . , ;
2 count of ,
3 count of !
4 count of 1st person singular pronouns
5 count of negative particles
6 count of numbers
7 count of prepositions
8 count of pronouns
9 count of ?
10 count of tokens longer than 6 letters
11 count of 1st pers. (sing. & plur.) pronouns
12 count of quote tokens
13 count of 1st person plural pronouns
14 count of 2nd person singular pronouns
15 count of quote positive words
16 count of quote negative words
17 count of nouns
18 count of verbs
19 count of adjectives
20 count of adverbs
21 up to 3-grams extracted from quote
Table 1: Common feature set.
development of this feature set was inspired by
(Celli, 2012) and (Alm et al., 2005). All fea-
tures were extracted from the lemmatized form of
quotes. Also, all feature counts (except Feature
21) were normalized by Feature 12. For com-
puting the counts of positive and negative words
(Feature 15 and 16) we used the General Inquirer
database (Stone et al., 1966). Feature 21 stands
for n-grams (up to 3-grams) extracted from the
speaker quotes. Two different schemes were fol-
lowed for extracting this feature: (i) using the
quote as-is, i.e., its lexical form, and (ii) using the
part-of-speech tags of quote. So, two slightly dif-
ferent feature sets were defined: 1) ?lex?: No.1-20
+ lexical form for No.21, 2) ?pos?: No.1-20 + POS
tags for No.21
3.4.3 Estimation of Personality Attributes
A machine-learning based approach was also used
for personality attribute estimation. For estimat-
ing the personality attributes of story speakers, the
linguistic feature set (see Table 1) used in the task
for age estimation was used again . Again our ap-
proach was based on the assumption that words
people use reflect their personality, and the latter
can be estimated by these linguistic features.
3.5 Affective Analysis
The last step of our system is the estimation of
the affective content of stories. The analysis is
performed for each identified quote. The features
presented in Table 1 are extracted for each quote
and affect is estimated using a machine-learning
model, based on the assumption that such features
serve as cues for revealing the underlying affective
content (Alm et al., 2005; Alm, 2008).
4 Experiments and Evaluation
Here we present the experimental evaluation of
our system in performing the following tasks: 1)
speaker-to-quote attribution, 2) gender estimation,
3) age estimation, 4) identification of personality
attributes, and 5) affective analysis of stories.
4.1 Datasets Used
The datasets used for our experiments along with
the related tasks are presented in Table 2.
No. Task Type of dataset
1 Quote attribution STORIES
2 Gender estimation STORIES
3 Age estimation QUOTES(1,2)
4 Personality attrib. QUOTES(3,4)
5 Affective analysis STORY-AFFECT
Table 2: Experiment datasets and related tasks.
Tasks 1-2. For the first two tasks (quote-to-
speaker attribution, and gender estimation) we
used a dataset (STORIES) consisting of 17 chil-
dren stories selected from Project Gutenberg3 .
This set of stories includes 98 unique speakers
with 554 quotes assigned to them. The average
number of sentences and quotes per story is 61.8
and 32.5, respectively. The average sentence and
quote length is 30.4 and 29.0 tokens, respectively.
Each speaker was attributed 5.7 quotes on aver-
age. Ground truth annotation, which involved as-
signing quotes to speakers and labeling gender,
was performed by one4 annotator. The follow-
ing ground truth labels were used to mark gender:
?male?, ?female?, and ?plural?.
3www.telecom.tuc.gr/
?
iosife/chst.html
4Due to the limited ambiguity of the task, the availability
of a single annotator was considered acceptable.
45
Task 3. Evaluation of the age estimation task was
performed with respect to two different (propri-
etary) datasets QUOTES1 and QUOTES2. These
datasets consisted of individual quotes assigned to
popular children?s story characters. The dataset
QUOTES1 consisted of 6361 quotes assigned to
69 unique speakers. The average quote length
equals 7.6 tokens, while each speaker was at-
tributed 141.4 quotes on average. The dataset
QUOTES2 consisted of 23605 quotes assigned to
262 unique speakers. The average quote length
equals 8.3 tokens, while each speaker was at-
tributed 142.6 quotes on average. For ground truth
annotation, four annotators were employed. The
annotators were asked to use the following age
labels: ?child? (0?15 years old), ?young adult?
(16?35 y.o.), ?middle-aged? (36?55 y.o.), and ?el-
derly? (56? y.o.). The age of each character was
inferred by the annotators either based on personal
knowledge of these stories or by consulting pub-
licly available sources online. The inter-annotator
agreement equals to 70%.
Task 4. To evaluate system performance on Task
4, two datasets QUOTES3 and QUOTES4, con-
sisting of individual quotes assigned to popular
children?s story characters, were used. The set
QUOTES3 consisted of 68 individual characters
and QUOTES4 consisted of 328 individual charac-
ters. The ground truth assignment, assigning each
character with personality attributes, was extracted
from a free, public collaborative wiki (Wiki,
2014). Since the wiki format allows people to add
or edit information, we considered the personality
attributes extracted from this wiki to be the aver-
age ?crowd?s opinion? of these characters. Of the
open-ended list of attributes that were used to de-
scribe the characters, in this task we attempted to
extract the following salient personality attributes:
?beautiful?, ?brave?, ?cowardly?, ?evil?, ?feisty?,
?greedy?, ?handsome?, ?kind?, ?loving?, ?loyal?,
?motherly?, ?optimistic?, ?spunky?, ?sweet?, and
?wise?. The pseudo-attribute ?none? was used
when a character was not described with any of
those aforementioned attributes.
Task 5. An annotated dataset, referred to as
STORY-AFFECT in this paper, consisting of 176
stories was used. Each story sentence (regard-
less if quotes were included or not) was anno-
tated regarding primary emotions and mood us-
ing the following labels: ?angry? (AN), ?dis-
gusted? (DI), ?fearful? (FE), ?happy? (HA), ?neu-
tral? (NE), ?sad? (SA), ?positive surprise? (SU+),
and ?negative surprise? (SU?). Overall, two anno-
tators were employed, while each annotator pro-
vided two annotations: one for emotion and one
for mood. More details about this dataset are pro-
vided in (Alm, 2008).
Instead of using the aforementioned emo-
tions/moods as annotated, we adopted a 3-class
scheme for sentence affect (valence): ?negative?,
?neutral?, and ?positive?. In order to align the
existing annotations to our three-class scheme the
following mapping5 was adopted: (i) AN, DI, FE,
SA were mapped to negative affect, (ii) NE was
mapped to neutral affect, and (iii) HA was mapped
to positive affect. Given the proposed mapping,
we retained those sentences (in total 11018) that
exhibited at least 75% annotation agreement.
4.2 Evaluation Results
The evaluation results for the aforementioned
tasks are presented below.
Tasks 1-2. The quote-to-speaker attribution was
evaluated in terms of precision (AT
p
), while the
estimation of speakers? gender was evaluated in
terms of precision (G
p
) and recall (G
r
). Note that
G
p
includes both types of errors: (i) erroneous age
estimation, and (ii) estimations for story charac-
ters that are not true speakers. In order to exclude
the second type of error, the precision of gender
estimation was also computed for only the true
story speaker identified by the system (G?
p
). For
Speaker filter. AT
p
G
p
G
r
G
?
p
Baseline 0.010 0.333
10 stories (subset of dataset)
Scheme 1 0.833 0.780 0.672 0.929
Scheme 2 0.868 0.710 0.759 0.917
Scheme 3 0.835 0.710 0.759 0.917
17 stories (full dataset)
Scheme 2 0.845 0.688 0.733 0.892
Table 3: Quote attribution and gender estimation.
a subset of the STORIES dataset that included 10
stories, the following schemes were used for filter-
ing of candidate speakers: (i) Scheme 1: all speak-
ers linked with speech verbs, (ii) Scheme 2: speak-
ers, who are persons or animals or spiritual entities
according to their first WordNet sense, linked with
speech verbs , and (iii) Scheme 3: as Scheme 2,
5SU+/? were excluded for simplicity.
46
but the first two WordNet senses were considered.
For the full STORIES dataset (17 stories) Scheme
2 was used. The results are presented in Table 3 in-
cluding the weighted averages of precision and re-
call. Using random guesses, the baseline precision
is 0.010 and 0.333 for quote-to-speaker attribution
and gender estimation, respectively. For the subset
of 10 stories, the highest speaker-to-quote attribu-
tion attribution is obtained by Scheme 2. When
this scheme is applied over the entire dataset, sub-
stantially high6 precision (0.892) is achieved in the
estimation of gender of true story speakers.
Task 3. For the estimation of age using quote-
based features, a boosting approach was fol-
lowed using BoosTexter (Schapire and Singer,
2000). For evaluation, 10-fold cross valida-
Dataset Relaxed Exact
lex pos lex pos
Baseline 0.625 0.250
QUOTES1 0.869 0.883 0.445 0.373
QUOTES2 0.877 0.831 0.450 0.435
BOTH 0.886 0.858 0.464 0.383
Table 4: Age estimation: average accuracy.
tion (10FCV) was used for the QUOTES1 and
QUOTES2 datasets for the ?lex? and ?pos? fea-
ture sets. The results are reported in Table 4 in
terms of average classification accuracy. In this
table, BOTH refers to the datasets QUOTES1 and
QUOTES2 combined together. The evaluation
was performed according to two schemes: (i) ?re-
laxed match?: the prediction is considered as cor-
rect even if it deviates one class from the true one,
e.g., ?child? and ?middle-aged? considered as cor-
rect for ?young adult?, and (ii) ?exact match?: the
prediction should exactly match the true label. The
relaxed scheme was motivated by the nature of in-
tended application (storytelling system) for which
such errors are tolerable. For the exact match
scheme, the obtained performance is higher7 than
the baseline (random guess) that equals to 0.250.
The accuracy for the relaxed scheme is quite high,
i.e., greater than 0.85 for almost all cases. On aver-
age, the ?lex? feature set appears to yield slightly
higher performance than the ?pos? set.
Task 4. The personality attributes were estimated
using BoosTexter fed with the ?lex? feature set.
10FCV was used for evaluation, while the aver-
6Statistically significant at 95% lev. (t-test wrt baseline).
7Statistically significant at 95% lev. (t-test wrt baseline).
age accuracy was computed by taking into account
the top five attributes predicted for each charac-
ter. The baseline accuracy equals 0.31 given that
random guesses are used. Moderate performance
was achieved for the QUOTES3 and QUOTES4
datasets, 0.426 and 0.411, respectively.
Task 5. The affect of story sentences was esti-
mated via BoosTexter using the ?lex? and ?pos?
feature sets. As in the previous two tasks 10FCV
was applied for evaluation purposes. Using ran-
dom guesses, the baseline accuracy is 0.33. The
average accuracy for the ?lex? and ?pos? feature
sets is 0.838 and 0.658, respectively8 . It is clear
that the use of the ?lex? set outperforms the results
yielded by the ?pos? set.
5 Conclusions and Future Directions
In this paper, we described the development of a
multi-step system aimed for story analysis with
particular emphasis on analyzing children?s sto-
ries. The core idea was the integration of sev-
eral systems into a single pipelined system. The
proposed methodology has a strong hybrid char-
acter in that it employs different approaches that
range from pattern-based to machine learning-
based to the incorporation of external knowledge
resources. Going beyond the usual task of works
in this genre, i.e., speaker-to-quote attribution, the
proposed system also supports the estimation of
speaker-oriented attributes and affect estimation.
Very promising results were obtained for quote at-
tribution and estimation of speaker gender, as well
as for age assuming an application-depended error
tolerance. The estimation of personality attributes
and the affective analysis of story sentences re-
main open research problems, while the results are
more modest especially for the former task.
In the next phase of our work, we hope to im-
prove and generalize each individual component
of the proposed system. The most challenging as-
pects of the system, dealing with personality at-
tributes and affective analysis, will be further in-
vestigated. Towards this task, psychological mod-
els, e.g., the Big Five model, can provide useful
theoretical and empirical findings. Last but not
least, the proposed system will be evaluated within
the framework of a digital storytelling application
including metrics related with user experience.
8Statistically significant at 90% lev. (t-test wrt baseline).
47
References
C. O. Alm, D. Roth, and R. Sproat. 2005. Emotions
from text: Machine learning for text-based emotion
prediction. In Proc. of Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 579?586.
C. O. Alm. 2008. Affect in Text and Speech. Ph.D.
thesis, University of Illinois at Urbana-Champaign.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why
are they excited? identifying and explaining spikes
in blog mood levels. In Proc. 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 207?210.
S. Bergsma and D. Lin. 2006. Bootstrapping path-
based pronoun resolution. In Proc. of Conference
on Computational Lingustics / Association for Com-
putational Linguistics, pages 33?40.
J. Cabral, L. Oliveira, G. Raimundo, and A. Paiva.
2006. What voice do we expect from a synthetic
character? In Proceedings of SPECOM, pages 536?
539.
F. Celli. 2012. Unsupervised personality recognition
for social network sites. In Proc. of Sixth Interna-
tional Conference on Digital Society.
CoreNLP. 2014. Stanford CoreNLP tool.
http://nlp.stanford.edu/software/
corenlp.shtml.
M.-C. de Marneffe and C. D. Manning. 2012. Stanford
typed dependencies manual.
D. K. Elson and K. R. McKeown. 2010. Automatic
attribution of quoted speech in literary narrative. In
Proc. of Twenty-Fourth AAAI Conference on Artifi-
cial Intelligence.
C. Fellbaum. 2005. Wordnet and wordnets. In
K. Brown et al., editor, Encyclopedia of Language
and Linguistics, pages 665?670. Oxford: Elsevier.
E. Greene, T. Mishra, P. Haffner, and A. Conkie. 2012.
Predicting character-appropriate voices for a TTS-
based storyteller system. In Proc. of Interspeech.
H. He, D. Barbosa, and G. Kondrak. 2013. Identifica-
tion of speakers in novels. In Proc. of 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 1312?1320.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. of Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177.
L. Lloyd, D. Kechagias, and S. Skiena. 2005. Lydia:
A system for large-scale news analysis. In Proc.
SPIRE, number 3772 in Lecture Notes in Computer
Science, pages 161?166.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2013. Distributional semantic
models for affective text analysis. IEEE Transac-
tions on Audio, Speech, and Language Processing,
21(11):2379?2392.
N. Mamede and P. Chaleira. 2004. Character identifi-
cation in children stories. In J. Vicedo, P. Martnez-
Barco, R. Muoz, and M. Saiz Noeda, editors, Ad-
vances in Natural Language Processing, volume
3230 of Lecture Notes in Computer Science, pages
82?90. Springer Berlin Heidelberg.
R. Mitkov. 2002. Anaphora Resolution. Longman.
I. R. Murray and J. L. Arnott. 2008. Applying an anal-
ysis of acted vocal emotions to improve the simu-
lation of synthetic speech. Computer Speech and
Language, 22(2):107?129.
P. Nakov, S. Rosenthal, Z. Kozareva, V. Stoyanov,
A. Ritter, and T. Wilson. 2013. Semeval 2013 task
2: Sentiment analysis in twitter. In Proc. of Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Seventh International Workshop on
Semantic Evaluation, pages 312?320.
T. Natsiopoulou, M. Souliotis, and A. G. Kyridis.
2006. Narrating and reading folktales and pic-
ture books: storytelling techniques and approaches
with preschool children. Early Childhood Re-
search and Practice, 8(1). Retrieved on Jan 13th,
2014 from http://ecrp.uiuc.edu/v8n1/
natsiopoulou.html.
T. W. Norman. 1963. Toward an adequate taxonomy of
personality attributes: Replicated factor structure in
peer nomination personality rating. Journal of Ab-
normal and Social Psychology, 66:574?583.
S. Piao. 2014. Sentence splitting pro-
gram. http://text0.mib.man.ac.uk:
8080/scottpiao/sent_detector.
D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta.
2010. Classifying latent user attributes in twitter. In
Proc. of the 2nd International Workshop on Search
and Mining User-generated Contents, pages 37?44.
M. Rusko, M. Trnka, S. Darjaa, and J. Hamar. 2013.
The dramatic piece reader for the blind and visu-
ally impaired. In Proc. of 4th Workshop on Speech
and Language Processing for Assistive Technolo-
gies, pages 83?91.
R. E. Schapire and Y. Singer. 2000. Boostexter: A
boosting-based system for text categorization. Ma-
chine. Learning, 39(2-3):135?168.
J. Schler, M. Koppel, S. Argamon, and J. W. Pen-
nebaker. 2006. Effects of age and gender on blog-
ging. In Proc. of AAAI Spring Symposium: Compu-
tational Approaches to Analyzing Weblogs.
Social Security. 2014. U.S. social security adminis-
tration baby name database. http://www.ssa.
gov/OACT/babynames/limits.html.
48
P. J. Stone, D. C. Dunphy, M. S. Smith, and D. M.
Ogilvie. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
C. Strapparava and R. Mihalcea. 2007. Semeval 2007
task 14: Affective text. In Proc. SemEval, pages 70?
74.
M. Theune, K. Meijs, and D. Heylen. 2006. Gener-
ating expressive speech for storytelling applications.
In IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1137?1144.
P. Turney and M. L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus (technical report erc-1094).
Disney Wiki. 2014. Description of Disney char-
acters. http://disney.wikia.com/wiki/
Category:Disney_characters#.
J. Y. Zhang, A. W. Black, and R. Sproat. 2003. Iden-
tifying speakers in children?s stories for speech syn-
thesis. In Proc. of Interspeech.
49
