Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 271?279,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Metric-based Framework for Automatic Taxonomy Induction 
 
 
Hui Yang 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
huiyang@cs.cmu.edu 
Jamie Callan 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
callan@cs.cmu.edu 
  
 
Abstract 
This paper presents a novel metric-based 
framework for the task of automatic taxonomy 
induction. The framework incrementally clus-
ters terms based on ontology metric, a score 
indicating semantic distance; and transforms 
the task into a multi-criteria optimization 
based on minimization of taxonomy structures 
and modeling of term abstractness. It com-
bines the strengths of both lexico-syntactic 
patterns and clustering through incorporating 
heterogeneous features. The flexible design of 
the framework allows a further study on which 
features are the best for the task under various 
conditions. The experiments not only show 
that our system achieves higher F1-measure 
than other state-of-the-art systems, but also re-
veal the interaction between features and vari-
ous types of relations, as well as the interac-
tion between features and term abstractness.  
1 Introduction 
Automatic taxonomy induction is an important 
task in the fields of Natural Language 
Processing, Knowledge Management, and Se-
mantic Web. It has been receiving increasing 
attention because semantic taxonomies, such as 
WordNet (Fellbaum, 1998), play an important 
role in solving knowledge-rich problems, includ-
ing question answering (Harabagiu et al, 2003) 
and textual entailment (Geffet and Dagan, 2005). 
Nevertheless, most existing taxonomies are ma-
nually created at great cost. These taxonomies 
are rarely complete; it is difficult to include new 
terms in them from emerging or rapidly changing 
domains. Moreover, manual taxonomy construc-
tion is time-consuming, which may make it un-
feasible for specialized domains and personalized 
tasks. Automatic taxonomy induction is a solu-
tion to augment existing resources and to pro-
duce new taxonomies for such domains and 
tasks. 
Automatic taxonomy induction can be decom-
posed into two subtasks: term extraction and re-
lation formation. Since term extraction is rela-
tively easy, relation formation becomes the focus 
of most research on automatic taxonomy induc-
tion. In this paper, we also assume that terms in a 
taxonomy are given and concentrate on the sub-
task of relation formation. 
Existing work on automatic taxonomy induc-
tion has been conducted under a variety of 
names, such as ontology learning, semantic class 
learning, semantic relation classification, and 
relation extraction. The approaches fall into two 
main categories: pattern-based and clustering-
based. Pattern-based approaches define lexical-
syntactic patterns for relations, and use these pat-
terns to discover instances of relations. Cluster-
ing-based approaches hierarchically cluster terms 
based on similarities of their meanings usually 
represented by a vector of quantifiable features. 
Pattern-based approaches are known for their 
high accuracy in recognizing instances of rela-
tions if the patterns are carefully chosen, either 
manually (Berland and Charniak, 1999; Kozare-
va et al, 2008) or via automatic bootstrapping 
(Hearst, 1992; Widdows and Dorow, 2002; Girju 
et al, 2003). The approaches, however, suffer 
from sparse coverage of patterns in a given cor-
pus. Recent studies (Etzioni et al, 2005; Kozare-
va et al, 2008) show that if the size of a corpus, 
such as the Web, is nearly unlimited, a pattern 
has a higher chance to explicitly appear in the 
corpus. However, corpus size is often not that 
large; hence the problem still exists. Moreover, 
since patterns usually extract instances in pairs, 
the approaches suffer from the problem of incon-
sistent concept chains after connecting pairs of 
instances to form taxonomy hierarchies.  
Clustering-based approaches have a main ad-
vantage that they are able to discover relations 
271
which do not explicitly appear in text. They also 
avoid the problem of inconsistent chains by ad-
dressing the structure of a taxonomy globally 
from the outset. Nevertheless, it is generally be-
lieved that clustering-based approaches cannot 
generate relations as accurate as pattern-based 
approaches. Moreover, their performance is 
largely influenced by the types of features used. 
The common types of features include contextual 
(Lin, 1998), co-occurrence (Yang and Callan, 
2008), and syntactic dependency (Pantel and Lin, 
2002; Pantel and Ravichandran, 2004). So far 
there is no systematic study on which features 
are the best for automatic taxonomy induction 
under various conditions. 
This paper presents a metric-based taxonomy 
induction framework. It combines the strengths 
of both pattern-based and clustering-based ap-
proaches by incorporating lexico-syntactic pat-
terns as one type of features in a clustering 
framework. The framework integrates contex-
tual, co-occurrence, syntactic dependency, lexi-
cal-syntactic patterns, and other features to learn 
an ontology metric, a score indicating semantic 
distance, for each pair of terms in a taxonomy; it 
then incrementally clusters terms based on their 
ontology metric scores. The incremental cluster-
ing is transformed into an optimization problem 
based on two assumptions: minimum evolution 
and abstractness. The flexible design of the 
framework allows a further study of the interac-
tion between features and relations, as well as 
that between features and term abstractness. 
2 Related Work 
There has been a substantial amount of research 
on automatic taxonomy induction. As we men-
tioned earlier, two main approaches are pattern-
based and clustering-based.  
Pattern-based approaches are the main trend 
for automatic taxonomy induction. Though suf-
fering from the problems of sparse coverage and 
inconsistent chains, they are still popular due to 
their simplicity and high accuracy. They have 
been applied to extract various types of lexical 
and semantic relations, including is-a, part-of, 
sibling, synonym, causal, and many others.  
Pattern-based approaches started from and still 
pay a great deal of attention to the most common  
is-a relations. Hearst (1992) pioneered using a 
hand crafted list of hyponym patterns as seeds 
and employing bootstrapping to discover is-a 
relations. Since then, many approaches (Mann, 
2002; Etzioni et al, 2005; Snow et al, 2005) 
have used Hearst-style patterns in their work on 
is-a relations. For instance, Mann (2002) ex-
tracted is-a relations for proper nouns by Hearst-
style patterns. Pantel et al (2004) extended is-a 
relation acquisition towards terascale, and auto-
matically identified hypernym patterns by mi-
nimal edit distance. 
Another common relation is sibling, which de-
scribes the relation of sharing similar meanings 
and being members of the same class. Terms in 
sibling relations are also known as class mem-
bers or similar terms. Inspired by the conjunction 
and appositive structures, Riloff and Shepherd 
(1997), Roark and Charniak (1998) used co-
occurrence statistics in local context to discover 
sibling relations. The KnowItAll system (Etzioni 
et al, 2005) extended the work in (Hearst, 1992) 
and bootstrapped patterns on the Web to discover 
siblings; it also ranked and selected the patterns 
by statistical measures. Widdows and Dorow 
(2002) combined symmetric patterns and graph 
link analysis to discover sibling relations. Davi-
dov and Rappoport (2006) also used symmetric 
patterns for this task. Recently, Kozareva et al 
(2008) combined a double-anchored hyponym 
pattern with graph structure to extract siblings. 
The third common relation is part-of. Berland 
and Charniak (1999) used two meronym patterns 
to discover part-of relations, and also used statis-
tical measures to rank and select the matching 
instances. Girju et al (2003) took a similar ap-
proach to Hearst (1992) for part-of relations. 
Other types of relations that have been studied 
by pattern-based approaches include question-
answer relations (such as birthdates and inven-
tor) (Ravichandran and Hovy, 2002), synonyms 
and antonyms (Lin et al, 2003), general purpose 
analogy (Turney et al, 2003), verb relations (in-
cluding similarity, strength, antonym, enable-
ment and temporal) (Chklovski and Pantel, 
2004), entailment (Szpektor et al, 2004), and 
more specific relations, such as purpose, creation 
(Cimiano and Wenderoth, 2007), LivesIn, and 
EmployedBy (Bunescu and Mooney , 2007).  
 The most commonly used technique in pat-
tern-based approaches is bootstrapping (Hearst, 
1992; Etzioni et al, 2005; Girju et al, 2003; Ra-
vichandran and Hovy, 2002; Pantel and Pennac-
chiotti, 2006). It utilizes a few man-crafted seed 
patterns to extract instances from corpora, then 
extracts new patterns using these instances, and 
continues the cycle to find new instances and 
new patterns. It is effective and scalable to large 
datasets; however, uncontrolled bootstrapping 
272
soon generates undesired instances once a noisy 
pattern brought into the cycle. 
 To aid bootstrapping, methods of pattern 
quality control are widely applied. Statistical 
measures, such as point-wise mutual information 
(Etzioni et al, 2005; Pantel and Pennacchiotti, 
2006) and conditional probability (Cimiano and 
Wenderoth, 2007),   have been shown to be ef-
fective to rank and select patterns and instances. 
Pattern quality control is also investigated by 
using WordNet (Girju et al, 2006), graph struc-
tures built among terms (Widdows and Dorow, 
2002; Kozareva et al, 2008), and pattern clusters 
(Davidov and Rappoport, 2008). 
Clustering-based approaches usually represent 
word contexts as vectors and cluster words based 
on similarities of the vectors (Brown et al, 1992; 
Lin, 1998). Besides contextual features, the vec-
tors can also be represented by verb-noun rela-
tions (Pereira et al, 1993), syntactic dependency 
(Pantel and Ravichandran, 2004; Snow et al, 
2005), co-occurrence (Yang and Callan, 2008), 
conjunction and appositive features (Caraballo, 
1999). More work is described in (Buitelaar et 
al., 2005; Cimiano and Volker, 2005). Cluster-
ing-based approaches allow discovery of rela-
tions which do not explicitly appear in text. Pan-
tel and Pennacchiotti (2006), however, pointed 
out that clustering-based approaches generally 
fail to produce coherent cluster for small corpora. 
In addition, clustering-based approaches had on-
ly applied to solve is-a and sibling relations. 
Many clustering-based approaches face the 
challenge of appropriately labeling non-leaf clus-
ters. The labeling amplifies the difficulty in crea-
tion and evaluation of taxonomies. Agglomera-
tive clustering (Brown et al, 1992; Caraballo, 
1999; Rosenfeld and Feldman, 2007; Yang and 
Callan, 2008) iteratively merges the most similar 
clusters into bigger clusters, which need to be 
labeled. Divisive clustering, such as CBC (Clus-
tering By Committee) which constructs cluster 
centroids by averaging the feature vectors of a 
subset of carefully chosen cluster members (Pan-
tel and Lin, 2002; Pantel and Ravichandran, 
2004), also need to label the parents of split clus-
ters. In this paper, we take an incremental clus-
tering approach, in which terms and relations are 
added into a taxonomy one at a time, and their 
parents are from the existing taxonomy. The ad-
vantage of the incremental approach is that it 
eliminates the trouble of inventing cluster labels 
and concentrates on placing terms in the correct 
positions in a taxonomy hierarchy.  
The work by Snow et al (2006) is the most 
similar to ours because they also took an incre-
mental approach to construct taxonomies. In their 
work, a taxonomy grows based on maximization 
of conditional probability of relations given evi-
dence; while in our work based on optimization 
of taxonomy structures and modeling of term 
abstractness. Moreover, our approach employs 
heterogeneous features from a wide range; while 
their approach only used syntactic dependency. 
We compare system performance between (Snow 
et al, 2006) and our framework in Section 5.  
3 The Features 
The features used in this work are indicators of 
semantic relations between terms. Given two in-
put terms yx cc , , a feature is defined as a func-
tion generating a single numeric score 
?),( yx cch ? or a vector of numeric scores 
?),( yx cch ?n. The features include contextual, 
co-occurrence, syntactic dependency, lexical-
syntactic patterns, and miscellaneous.  
The first set of features captures contextual in-
formation of terms. According to Distributional 
Hypothesis (Harris, 1954), words appearing in 
similar contexts tend to be similar. Therefore, 
word meanings can be inferred from and 
represented by contexts. Based on the hypothe-
sis, we develop the following features: (1) Glob-
al Context KL-Divergence: The global context of 
each input term is the search results collected 
through querying search engines against several 
corpora (Details in Section 5.1). It is built into a 
unigram language model without smoothing for 
each term. This feature function measures the 
Kullback-Leibler divergence (KL divergence) 
between the language models associated with the 
two inputs. (2) Local Context KL-Divergence: 
The local context is the collection of all the left 
two and the right two words surrounding an input 
term. Similarly, the local context is built into a 
unigram language model without smoothing for 
each term; the feature function outputs KL diver-
gence between the models. 
The second set of features is co-occurrence. In 
our work, co-occurrence is measured by point-
wise mutual information between two terms:  
)()(
),(
log),(
yx
yx
yx
cCountcCount
ccCount
ccpmi =  
where Count(.) is defined as the number of doc-
uments or sentences containing the term(s); or n 
as in ?Results 1-10 of about n for term? appear-
ing on the first page of Google search results for 
a term or the concatenation of a term pair. Based 
273
on different definitions of Count(.), we have (3) 
Document PMI, (4) Sentence PMI, and (5) 
Google PMI as the co-occurrence features. 
The third set of features employs syntactic de-
pendency analysis. We have (6) Minipar Syntac-
tic Distance to measure the average length of the 
shortest syntactic paths (in the first syntactic 
parse tree returned by Minipar1) between two 
terms in sentences containing them, (7) Modifier 
Overlap, (8) Object Overlap, (9) Subject Over-
lap, and (10) Verb Overlap to measure the num-
ber of overlaps between modifiers, objects, sub-
jects, and verbs, respectively, for the two terms 
in sentences containing them. We use Assert2 to 
label the semantic roles. 
The fourth set of features is lexical-syntactic 
patterns. We have (11) Hypernym Patterns based 
on patterns proposed by (Hearst, 1992) and 
(Snow et al, 2005), (12) Sibling Patterns which 
are basically conjunctions, and (13) Part-of Pat-
terns based on patterns proposed by (Girju et al, 
2003) and (Cimiano and Wenderoth, 2007). Ta-
ble 1 lists all patterns. Each feature function re-
turns a vector of scores for two input terms, one 
score per pattern. A score is 1 if two terms match 
a pattern in text, 0 otherwise. 
The last set of features is miscellaneous. We 
have (14) Word Length Difference to measure the 
length difference between two terms, and (15) 
Definition Overlap to measure the number of 
word overlaps between the term definitions ob-
tained by querying Google with ?define:term?. 
These heterogeneous features vary from sim-
ple statistics to complicated syntactic dependen-
cy features, basic word length to comprehensive 
Web-based contextual features. The flexible de-
sign of our learning framework allows us to use 
all of them, and even allows us to use different 
sets of them under different conditions, for in-
stance, different types of relations and different 
abstraction levels. We study the interaction be-
                                                 
1
 http://www.cs.ualberta.ca/lindek/minipar.htm. 
2
 http://cemantix.org/assert. 
tween features and relations and that between 
features and abstractness in Section 5. 
4 The Metric-based Framework 
This section presents the metric-based frame-
work which incrementally clusters terms to form 
taxonomies. By minimizing the changes of tax-
onomy structures and modeling term abstractness 
at each step, it finds the optimal position for each 
term in a taxonomy. We first introduce defini-
tions, terminologies and assumptions about tax-
onomies; then, we formulate automatic taxono-
my induction as a multi-criterion optimization 
and solve it by a greedy algorithm; lastly, we 
show how to estimate ontology metrics.      
4.1 Taxonomies, Ontology Metric, Assump-
tions, and Information Functions 
We define a taxonomy T as a data model that 
represents a set of terms C and a set of relations 
R between these terms. T can be written as 
T(C,R). Note that for the subtask of relation for-
mation, we assume that the term set C is given. A 
full taxonomy is a tree containing all the terms in 
C. A partial taxonomy is a tree containing only a 
subset of terms in C.  
In our framework, automatic taxonomy induc-
tion is the process to construct a full taxonomy T?
given a set of terms C and an initial partial tax-
onomy ),( 000 RST , where CS ?0 . Note that T0 is 
possibly empty. The process starts from the ini-
tial partial taxonomy T0 and randomly adds terms 
from C to T0 one by one, until a full taxonomy is 
formed, i.e., all terms in C are added. 
Ontology Metric 
We define an ontology metric as a distance 
measure between two terms (cx,cy) in a taxonomy 
T(C,R). Formally, it is a function ?? CCd : ?+, 
where C is the set of terms in T.  An ontology 
metric d on a taxonomy T with edge weights w 
for any term pair (cx,cy)?C is the sum of all edge 
weights along the shortest path between the pair: 
?
?
=
),(
,),(
,
)(),(
yxPe
yxyxwT
yx
ewccd
 
Hypernym Patterns Sibling Patterns 
NPx (,)?and/or other NPy NPx and/or NPy 
such NPy as NPx Part-of Patterns 
NPy (,)? such as NPx NPx of NPy 
NPy (,)? including NPx NPy?s NPx 
NPy (,)? especially NPx NPy has/had/have NPx 
NPy like NPx NPy is made (up)? of NPx 
NPy called NPx NPy comprises NPx 
NPx is a/an NPy NPy consists of NPx 
NPx , a/an NPy  
Table 1. Lexico-Syntactic Patterns. 
 
Figure 1. Illustration of Ontology Metric. 
274
where ),( yxP  is the set of edges defining the 
shortest path from term cx to cy . Figure 1 illu-
strates ontology metrics for a 5-node taxonomy. 
Section 4.3 presents the details of learning ontol-
ogy metrics. 
Information Functions 
The amount of information in a taxonomy T is 
measured and represented by an information 
function Info(T). An information function is de-
fined as the sum of the ontology metrics among a 
set of term pairs. The function can be defined 
over a taxonomy, or on a single level of a tax-
onomy. For a taxonomy T(C,R), we define its 
information function as: 
?
?<
=
Cycxcyx
yx ccdTInfo
,,
),()(   (1) 
Similarly, we define the information function 
for an abstraction level Li as:  
?
?<
=
iLycxcyx
yxii ccdLInfo
,,
),()(   (2) 
where Li is the subset of terms lying at the ith lev-
el of a taxonomy T. For example, in Figure 1, 
node 1 is at level L1, node 2 and node 5 level L2. 
Assumptions 
Given the above definitions about taxonomies, 
we make the following assumptions: 
Minimum Evolution Assumption. Inspired by 
the minimum evolution tree selection criterion 
widely used in phylogeny (Hendy and Penny, 
1985), we assume that a good taxonomy not only 
minimizes the overall semantic distance among 
the terms but also avoid dramatic changes. Con-
struction of a full taxonomy is proceeded by add-
ing terms one at a time, which yields a series of 
partial taxonomies. After adding each term, the 
current taxonomy Tn+1 from the previous tax-
onomy Tn is one that introduces the least changes 
between the information in the two taxonomies: 
),(minarg '
'
1 TTInfoT n
T
n ?=+  
where the information change function is 
|)()(| ),( baba TInfoTInfoTTInfo ?=? .  
Abstractness Assumption. In a taxonomy, con-
crete concepts usually lay at the bottom of the 
hierarchy while abstract concepts often occupy 
the intermediate and top levels. Concrete con-
cepts often represent physical entities, such as 
?basketball? and ?mercury pollution?. While ab-
stract concepts, such as ?science? and ?econo-
my?, do not have a physical form thus we must 
imagine their existence. This obvious difference 
suggests that there is a need to treat them diffe-
rently in taxonomy induction. Hence we assume 
that terms at the same abstraction level have 
common characteristics and share the same Info(.) 
function. We also assume that terms at different 
abstraction levels have different characteristics; 
hence they do not necessarily share the same  
Info(.) function. That is to say, ,concept  Tc ??
, leveln abstractio TLi ?  (.). uses ii InfocLc ??  
4.2 Problem Formulation 
The Minimum Evolution Objective 
Based on the minimum evolution assumption, we 
define the goal of taxonomy induction is to find 
the optimal full taxonomy T?  such that the infor-
mation changes are the least since the initial par-
tial taxonomy T0, i.e., to find:  
),(minarg? '0
'
TTInfoT
T
?=   (3) 
where 'T  is a full taxonomy, i.e., the set of terms 
in 'T  equals C. 
To find the optimal solution for Equation (3),  
T? , we need to find the optimal term set C? and 
the optimal relation set R? . Since the optimal term 
set for a full taxonomy is always C, the only un-
known part left is R? . Thus, Equation (3) can be 
transformed equivalently into:
 
)),(),,((minarg? 000''
'
RSTRCTInfoR
R
?=  
Note that in the framework, terms are added 
incrementally into a taxonomy. Each term inser-
tion yields a new partial taxonomy T. By the 
minimum evolution assumption, the optimal next 
partial taxonomy is one gives the least informa-
tion change. Therefore, the updating function for 
the set of relations 1+nR after a new term z is in-
serted can be calculated as: 
)),(),},{((minarg? '
'
nnn
R
RSTRzSTInfoR ??=
 
By plugging in the definition of the information 
change function (.,.)Info? in Section 4.1 and Equ-
ation (1), the updating function becomes: 
|),(),(|minarg?
,}{,'
??
???
?=
nSycxc
yx
znSycxc
yx
R
ccdccdR
 
The above updating function can be transformed 
into a minimization problem: 
yx
ccdccdu
ccdccdu
u
znSycxc
yx
nSycxc
yx
nSycxc
yx
znSycxc
yx
<
??
??
??
??
???
???
}{,,
,}{,
),(),(                 
),(),(    subject to
 
min
 
The minimization follows the minimum evolu-
tion assumption; hence we call it the minimum 
evolution objective. 
 
275
The Abstractness Objective 
The abstractness assumption suggests that term 
abstractness should be modeled explicitly by 
learning separate information functions for terms 
at different abstraction levels. We approximate 
an information function by a linear interpolation 
of some underlying feature functions. Each ab-
straction level Li is characterized by its own in-
formation function Infoi(.). The least square fit of 
Infoi(.) is: .|)(|min 2iTiii HWLInfo ?  
By plugging Equation (2) and minimizing over 
every abstraction level, we have: 
2
,,
,
)),(),((min yxji
j
ji
i iLycxc
yx cchwccd ?? ? ?
?
where jih , (.,.) is the jth underlying feature func-
tion for term pairs at level Li, jiw , is the weight 
for jih , (.,.). This minimization follows the ab-
stractness assumption; hence we call it the ab-
stractness objective. 
The Multi-Criterion Optimization Algorithm 
We propose that both minimum evolution and 
abstractness objectives need to be satisfied. To 
optimize multiple criteria, the Pareto optimality 
needs to be satisfied (Boyd and Vandenberghe, 
2004). We handle this by introducing   0,1 to 
control the contribution of each objective. The 
multi-criterion optimization function is: 
yx
cchwccdv
ccdccdu
ccdccdu
vu
yxji
j
ji
i Lcc
yx
zScc
yx
Scc
yx
Scc
yx
zScc
yx
iyx
n
yx
n
yx
n
yx
n
yx
<
?=
??
??
?+
?? ?
??
??
?
???
???
2)),(),((                              
),(),(                   
),(),(      subject to
)1(min
,,
,
}{,,
,}{,
??
The above optimization can be solved by a gree-
dy optimization algorithm. At each term insertion 
step, it produces a new partial taxonomy by add-
ing to the existing partial taxonomy a new term z, 
and a new set of relations R(z,.). z is attached to 
every nodes in the existing partial taxonomy; and 
the algorithm selects the optimal position indi-
cated by R(z,.), which minimizes the multi-
criterion objective function. The algorithm is: 
);,(
)};)1((min{arg
;
\
RST
vuRR
{z}SS
SCz
(z,.)R
Output 
            
            
 foreach
?? ?+??
??
?
   
The above algorithm presents a general incre-
mental clustering procedure to construct taxono-
mies. By minimizing the taxonomy structure 
changes and modeling term abstractness at each 
step, it finds the optimal position of each term in 
the taxonomy hierarchy. 
4.3 Estimating Ontology Metric 
Learning a good ontology metric is important for 
the multi-criterion optimization algorithm. In this 
work, the estimation and prediction of ontology 
metric are achieved by ridge regression (Hastie et 
al., 2001). In the training data, an ontology me-
tric d(cx,cy) for a term pair (cx,cy) is generated by 
assuming every edge weight as 1 and summing 
up all the edge weights along the shortest path 
from cx to cy. We assume that there are some un-
derlying feature functions which measure the 
semantic distance from term cx to cy. A weighted 
combination of these functions approximates the 
ontology metric for (cx,cy): 
?= ),(),( yxjjj cchwyxd  
where jw  is the jth weight for ),( yxj cch , the jth 
feature function. The feature functions are gener-
ated as mentioned in Section 3.  
5 Experiments  
5.1 Data 
The gold standards used in the evaluation are 
hypernym taxonomies extracted from WordNet 
and ODP (Open Directory Project), and me-
ronym taxonomies extracted from WordNet. In 
WordNet taxonomy extraction, we only use the 
word senses within a particular taxonomy to en-
sure no ambiguity. In ODP taxonomy extraction, 
we parse the topic lines, such as ?Topic 
r:id=`Top/Arts/Movies??, in the XML databases 
to obtain relations, such as is_a(movies, arts). In 
total, there are 100 hypernym taxonomies, 50 
each extracted from WordNet3 and ODP4, and 50 
meronym taxonomies from WordNet5. Table 2  
                                                 
3
 WordNet hypernym taxonomies are from 12 topics: ga-
thering, professional, people, building, place, milk, meal, 
water, beverage, alcohol, dish, and herb. 
4
 ODP hypernym taxonomies are from 16 topics: computers, 
robotics, intranet, mobile computing, database, operating 
system, linux, tex, software, computer science, data commu-
nication, algorithms, data formats, security multimedia, and 
artificial intelligence. 
5
 WordNet meronym taxonomies are from 15 topics: bed, 
car, building, lamp, earth, television, body, drama, theatre, 
water, airplane, piano, book, computer, and watch. 
Statistics WN/is-a ODP/is-a WN/part-of 
#taxonomies 50 50 50 
#terms 1,964 2,210 1,812 
Avg #terms 39 44 37 
Avg depth 6 6 5 
Table 2. Data Statistics. 
 
276
summarizes the data statistics. 
We also use two Web-based auxiliary datasets 
to generate features mentioned in Section 3: 
? Wikipedia corpus. The entire Wikipedia corpus 
is downloaded and indexed by Indri6. The top 
100 documents returned by Indri are the global 
context of a term when querying with the term.  
? Google corpus. A collection of the top 1000 
documents by querying Google using each 
term, and each term pair. Each top 1000 docu-
ments are the global context of a query term. 
Both corpora are split into sentences and are used 
to generate contextual, co-occurrence, syntactic 
dependency and lexico-syntactic pattern features.  
5.2 Methodology 
We evaluate the quality of automatic generated 
taxonomies by comparing them with the gold 
standards in terms of precision, recall and F1-
measure. F1-measure is calculated as 2*P*R/ 
(P+R), where P is precision, the percentage of 
correctly returned relations out of the total re-
turned relations, R is recall, the percentage of 
correctly returned relations out of the total rela-
tions in the gold standard. 
Leave-one-out cross validation is used to aver-
age the system performance across different 
training and test datasets. For each 50 datasets 
from WordNet hypernyms, WordNet meronyms 
or ODP hypernyms, we randomly pick 49 of 
them to generate training data, and test on the 
remaining dataset. We repeat the process for 50 
times, with different training and test sets at each 
                                                 
6
 http://www.lemurproject.org/indri/. 
time, and report the averaged precision, recall 
and F1-measure across all 50 runs. 
We also group the fifteen features in Section 3 
into six sets: contextual, co-concurrence, pat-
terns, syntactic dependency, word length differ-
ence and definition. Each set is turned on one by 
one for experiments in Section 5.4 and 5.5. 
5.3 Performance of Taxonomy Induction 
In this section, we compare the following auto-
matic taxonomy induction systems: HE, the sys-
tem by Hearst (1992) with 6 hypernym patterns; 
GI, the system by Girju et al (2003) with 3 me-
ronym patterns; PR, the probabilistic framework 
by Snow et al (2006); and ME, the metric-based 
framework proposed in this paper. To have a fair 
comparison, for PR, we estimate the conditional 
probability of a relation given the evidence 
P(Rij|Eij), as in (Snow et al 2006), by using the 
same set of features as in ME. 
Table 3 shows precision, recall, and F1-
measure of each system for WordNet hypernyms 
(is-a), WordNet meronyms (part-of) and ODP 
hypernyms (is-a). Bold font indicates the best 
performance in a column. Note that HE is not 
applicable to part-of, so is GI to is-a. 
Table 3 shows that systems using heterogene-
ous features (PR and ME) achieve higher F1-
measure than systems only using patterns (HE 
and GI) with a significant absolute gain of >30%. 
Generally speaking, pattern-based systems show 
higher precision and lower recall, while systems 
using heterogeneous features show lower preci-
sion and higher recall. However, when consider-
ing both precision and recall, using heterogene-
ous features is more effective than just using pat-
terns. The proposed system ME consistently pro-
duces the best F1-measure for all three tasks.  
The performance of the systems for ODP/is-a 
is worse than that for WordNet/is-a. This may be 
because there is more noise in ODP than in 
WordNet/is-a 
System Precision Recall F1-measure 
HE 0.85 0.32 0.46 
GI n/a n/a n/a 
PR 0.75 0.73 0.74 
ME 0.82 0.79 0.82 
ODP/is-a 
System Precision Recall F1-measure 
HE 0.31 0.29 0.30 
GI n/a n/a n/a 
PR 0.60 0.72 0.65 
ME 0.64 0.70 0.67 
WordNet/part-of 
System Precision Recall F1-measure 
HE n/a n/a n/a 
GI 0.75 0.25 0.38 
PR 0.68 0.52 0.59 
ME 0.69 0.55 0.61 
Table 3. System Performance. 
Feature  is-a sibling part-
of 
Benefited 
Relations  
Contextual 0.21 0.42 0.12 sibling 
Co-occur. 0.48 0.41 0.28 All 
Patterns 0.46 0.41 0.30 All 
Syntactic 0.22 0.36 0.12 sibling 
Word Leng. 0.16 0.16 0.15 All but 
limited 
Definition 0.12 0.18 0.10 Sibling but 
limited 
Best Features Co-
occur., 
patterns  
Contextual, 
co-occur., 
patterns 
Co-
occur., 
patterns 
 
Table 4. F1-measure for Features vs. Relations: WordNet. 
277
WordNet. For example, under artificial intelli-
gence, ODP has neural networks, natural lan-
guage and academic departments. Clearly, aca-
demic departments is not a hyponym of artificial 
intelligence. The noise in ODP interferes with 
the learning process, thus hurts the performance. 
5.4 Features vs. Relations 
This section studies the impact of different sets 
of features on different types of relations. Table 4 
shows F1-measure of using each set of features 
alone on taxonomy induction for WordNet is-a, 
sibling, and part-of relations. Bold font means a 
feature set gives a major contribution to the task 
of automatic taxonomy induction for a particular 
type of relation. 
Table 4 shows that different relations favor 
different sets of features.  Both co-occurrence 
and lexico-syntactic patterns work well for all 
three types of relations. It is interesting to see 
that simple co-occurrence statistics work as good 
as lexico-syntactic patterns. Contextual features 
work well for sibling relations, but not for is-a 
and part-of. Syntactic features also work well for 
sibling, but not for is-a and part-of. The similar 
behavior of contextual and syntactic features 
may be because that four out of five syntactic 
features (Modifier, Subject, Object, and Verb 
overlaps) are just surrounding context for a term. 
Comparing the is-a and part-of columns in 
Table 4 and the ME rows in Table 3, we notice a 
significant difference in F1-measure. It indicates 
that combination of heterogeneous features gives 
more rise to the system performance than a sin-
gle set of features does. 
5.5 Features vs. Abstractness 
This section studies the impact of different sets 
of features on terms at different abstraction le-
vels. In the experiments, F1-measure is evaluated 
for terms at each level of a taxonomy, not the 
whole taxonomy. Table 5 and 6 demonstrate F1-
measure of using each set of features alone on 
each abstraction levels. Columns 2-6 are indices 
of the levels in a taxonomy. The larger the indic-
es are, the lower the levels. Higher levels contain 
abstract terms, while lower levels contain con-
crete terms. L1 is ignored here since it only con-
tains a single term, the root. Bold font indicates 
good performance in a column. 
Both tables show that abstract terms and con-
crete terms favor different sets of features. In 
particular, contextual, co-occurrence, pattern, 
and syntactic features work well for terms at L4-
L6, i.e., concrete terms; co-occurrence works well 
for terms at L2-L3, i.e., abstract terms. This differ-
ence indicates that terms at different abstraction 
levels have different characteristics; it confirms 
our abstractness assumption in Section 4.1.  
We also observe that for abstract terms in 
WordNet, patterns work better than contextual 
features; while for abstract terms in ODP, the 
conclusion is the opposite. This may be because 
that WordNet has a richer vocabulary and a more 
rigid definition of hypernyms, and hence is-a 
relations in WordNet are recognized more effec-
tively by using lexico-syntactic patterns; while 
ODP contains more noise, and hence it favors 
features requiring less rigidity, such as the con-
textual features generated from the Web. 
6 Conclusions  
This paper presents a novel metric-based tax-
onomy induction framework combining the 
strengths of lexico-syntactic patterns and cluster-
ing. The framework incrementally clusters terms 
and transforms automatic taxonomy induction 
into a multi-criteria optimization based on mini-
mization of taxonomy structures and modeling of 
term abstractness. The experiments show that our 
framework is effective; it achieves higher F1-
measure than three state-of-the-art systems. The 
paper also studies which features are the best for 
different types of relations and for terms at dif-
ferent abstraction levels.  
Most prior work uses a single rule or feature 
function for automatic taxonomy induction at all 
levels of abstraction. Our work is a more general 
framework which allows a wider range of fea-
tures and different metric functions at different 
abstraction levels.  This more general framework 
has the potential to learn more complex taxono-
mies than previous approaches. 
Acknowledgements 
This research was supported by NSF grant IIS-
0704210. Any opinions, findings, conclusions, or 
recommendations expressed in this paper are of 
the authors, and do not necessarily reflect those 
of the sponsor. 
Feature  L2 L3 L4 L5 L6 
Contextual 0.29 0.31 0.35 0.36 0.36 
Co-occurrence 0.47 0.56 0.45 0.41 0.41 
Patterns 0.47 0.44 0.42 0.39 0.40 
Syntactic 0.31 0.28 0.36 0.38 0.39 
Word Length 0.16 0.16 0.16 0.16 0.16 
Definition 0.12 0.12 0.12 0.12 0.12 
Table 5. F1-measure for Features vs. Abstractness: 
WordNet/is-a. 
Feature  L2 L3 L4 L5 L6 
Contextual 0.30 0.30 0.33 0.29 0.29 
Co-occurrence 0.34 0.36 0.34 0.31 0.31 
Patterns 0.23 0.25 0.30 0.28 0.28 
Syntactic 0.18 0.18 0.23 0.27 0.27 
Word Length 0.15 0.15 0.15 0.14 0.14 
Definition 0.13 0.13 0.13 0.12 0.12 
Table 6. F1-measure for Features vs. Abstractness: 
ODP/is-a. 
278
References 
M. Berland and E. Charniak. 1999. Finding parts in very 
large corpora. ACL?99. 
S. Boyd and L. Vandenberghe. 2004. Convex optimization. 
In Cambridge University Press, 2004.  
P. Brown, V. D. Pietra, P. deSouza, J. Lai, and R. Mercer. 
1992. Class-based ngram models for natural language. 
Computational Linguistics, 18(4):468?479. 
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. Ontology 
Learning from Text: Methods, Evaluation and Applica-
tions. Volume 123 Frontiers in Artificial Intelligence and 
Applications. 
R. Bunescu and R. Mooney. 2007.  Learning to Extract 
Relations from the Web using Minimal Supervision. 
ACL?07. 
S. Caraballo. 1999.  Automatic construction of a hypernym-
labeled noun hierarchy from text. ACL?99. 
T. Chklovski and P. Pantel. 2004. VerbOcean: mining the 
web for fine-grained semantic verb relations. EMNLP 
?04. 
P. Cimiano and J. Volker. 2005. Towards large-scale, open-
domain and ontology-based named entity classification. 
RANLP?07. 
P. Cimiano and J. Wenderoth. 2007. Automatic Acquisition 
of Ranked Qualia Structures from the Web. ACL?07. 
D. Davidov and A. Rappoport. 2006. Efficient Unsuper-
vised Discovery of Word Categories Using Symmetric 
Patterns and High Frequency Words. ACL?06. 
D. Davidov and A. Rappoport. 2008. Classification of Se-
mantic Relationships between Nominals Using Pattern  
Clusters. ACL?08. 
D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabil-
istic model of redundancy in information extraction. IJ-
CAI?05.  
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: an ex-
perimental study. Artificial Intelligence, 165(1):91?134. 
C. Fellbuam. 1998. WordNet: An Electronic Lexical Data-
base. MIT Press. 1998. 
M. Geffet and I. Dagan. 2005. The Distributional Inclusion 
Hypotheses and Lexical Entailment. ACL?05. 
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning 
Semantic Constraints for the Automatic Discovery of 
Part-Whole Relations. HLT?03. 
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic 
Discovery of Part-Whole Relations. Computational Lin-
guistics, 32(1): 83-135. 
Z. Harris. 1985. Distributional structure. In Word, 10(23): 
146-162s, 1954.  
T. Hastie, R. Tibshirani and J. Friedman. 2001. The Ele-
ments of Statistical Learning: Data Mining, Inference, 
and Prediction. Springer-Verlag, 2001. 
M. Hearst. 1992. Automatic acquisition of hyponyms from 
large text corpora. COLING?92. 
M. D. Hendy and D. Penny. 1982. Branch and bound algo-
rithms to determine minimal evolutionary trees. Mathe-
matical Biosciences 59: 277-290. 
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic Class 
Learning from the Web with Hyponym Pattern Linkage 
Graphs. ACL?08. 
D. Lin, 1998. Automatic retrieval and clustering of similar 
words. COLING?98. 
D. Lin, S. Zhao, L. Qin, and M. Zhou. 2003. Identifying 
Synonyms among Distributionally Similar Words. IJ-
CAI?03. 
G. S. Mann. 2002. Fine-Grained Proper Noun Ontologies 
for Question Answering. In Proceedings of SemaNet? 02: 
Building and Using Semantic Networks, Taipei. 
P. Pantel and D Lin. 2002. Discovering word senses from 
text. SIGKDD?02. 
P. Pantel and D. Ravichandran. 2004. Automatically labe-
ling semantic classes. HLT/NAACL?04.  
P. Pantel, D. Ravichandran, and E. Hovy. 2004. Towards 
terascale knowledge acquisition. COLING?04. 
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging 
Generic Patterns for Automatically Harvesting Semantic 
Relations. ACL?06. 
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of English words. ACL?93. 
D. Ravichandran and E. Hovy. 2002. Learning surface text 
patterns for a question answering system. ACL?02. 
E. Riloff and J. Shepherd. 1997. A corpus-based approach 
for building semantic lexicons. EMNLP?97. 
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic lexicon 
construction. ACL/COLING?98. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntac-
tic patterns for automatic hypernym discovery. NIPS?05. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogeneous Evidence. 
ACL?06. 
B. Rosenfeld and R. Feldman. 2007. Clustering for unsu-
pervised relation identification. CIKM?07. 
P. Turney, M. Littman, J. Bigham, and V. Shnayder. 2003. 
Combining independent modules to solve multiple-
choice synonym and analogy problems. RANLP?03.  
S. M. Harabagiu, S. J. Maiorano and M. A. Pasca. 2003. 
Open-Domain Textual Question Answering Techniques. 
Natural Language Engineering 9 (3): 1-38, 2003. 
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. 
Scaling web-based acquisition of entailment relations. 
EMNLP?04.  
D. Widdows and B. Dorow. 2002. A graph model for unsu-
pervised Lexical acquisition. COLING ?02. 
H. Yang and J. Callan. 2008. Learning the Distance Metric 
in a Personal Ontology. Workshop on Ontologies and In-
formation Systems for the Semantic Web of CIKM?08. 
279
Web-Based List Question Answering  
Hui Yang, Tat-Seng Chua 
School of Computing 
National University of Singapore 
3 Science Drive 2, 117543, Singapore  
yangh@lycos.co.uk,chuats@comp.nus.edu.sg  
 
Abstract 
While research on question answering has be-
come popular in recent years, the problem of ef-
ficiently locating a complete set of distinct 
answers to list questions in huge corpora or the 
Web is still far from being solved. This paper ex-
ploits the wealth of freely available text and link 
structures on the Web to seek complete answers 
to list questions. We introduce our system, 
FADA, which relies on question parsing, web 
page classification/clustering, and content extrac-
tion to find reliable distinct answers with high re-
call.  
1 Introduction 
The Text REtrieval Conference Series (TREC) has 
greatly encouraged Question Answering (QA) re-
search in the recent years. The QA main task in the 
recent TREC-12 involved retrieving short concise 
answers to factoid and list questions, and answer 
nuggets for definition questions (Voorhees, 2003). 
The list task in TREC-12 required systems to as-
semble a set of distinct and complete exact answers 
as responses to questions like ?What are the brand 
names of Belgian chocolates??. Unlike the questions 
in previous TREC conferences, TREC-12 list ques-
tions did not specify a target number of instances to 
return but expected all answers contained in the cor-
pus. Current QA systems (Harabagiu et al, 2003; 
Katz et al, 2003) usually extract a ranked list of fac-
toid answers from the top returned documents by 
retrieval engines. This is actually the traditional way 
to find factoid answers. The only difference between 
answering list questions and factoid questions here is 
that list QA systems allow for multiple answers, 
whose scores are above a cut-off threshold. 
An analysis of the results of TREC-12 list QA 
systems (Voorhees, 2003) reveals that many of them 
severely suffer from two general problems: low re-
call and non-distinctive answers. The median aver-
age F1 performance of list runs was only 21.3% 
while the best performer could only achieve 39.6% 
(Table 1). This unsatisfactory performance exposes 
the limitation of using only traditional Information 
Retrieval and Natural Language Processing tech-
niques to find an exhaustive set of factoid answers as 
compared to only one. 
TREC-12 Run Tag Avg F1  
LCCmainS03 0.396 
nusmml03r2 0.319 
MITCSAIL03c 0.134 
isi03a 0.118 
BBN2003B 0.097 
Average 0.213 
Table 1: TREC-12 Top 5 Performers (Voorhees, 2003) 
In contrast to the traditional techniques, the Web 
is used extensively in systems to rally round factoid 
questions. QA researchers have explored a variety of 
uses of the Web, ranging from surface pattern min-
ing (Ravichandran et al, 2002), query formulation 
(Yang et al, 2003), answer validation (Magnini et 
al., 2002), to directly finding answers on the Web by 
data redundancy analysis (Brill et al, 2001). These 
systems demonstrated that with the help of the Web 
they could generally boost baseline performance by 
25%-30% (Lin 2002). 
The well-known redundancy-based approach iden-
tifies the factoid answer as an N-gram appearing 
most frequently on the Web (Brill et al 2001). This 
idea works well on factoid questions because factoid 
questions require only one instance and web docu-
ments contains a large number of repeated informa-
tion about possible answers. However, when dealing 
with list questions, we need to find all distinct in-
stances and hence we cannot ignore the less frequent 
answer candidates. The redundancy-based approach 
fails to spot novel or unexpectedly valuable informa-
tion in lower ranked web pages with few occur-
rences.  
In this paper, we propose a novel framework to 
employ the Web to support list question answering.  
Based on the observations that multiple answer in-
stances often appear in the list or table of a single 
web page while multiple web pages may also con-
tain information about the same instance, we differ-
entiate these two types of web pages. For the first 
category, which we call Collection Page (CP), we 
need to extract table/list content from the web page. 
For the second category, which we call Topic Page 
(TP), we need to find distinct web pages relating to 
different answer instances. We will demonstrate that 
the resulting system, FADA (Find All Distinct An-
swers), could achieve effective list question answer-
ing in the TREC corpus.  
Figure 1: Examples of Collection Page (top) 
and Topic Page (bottom)  
The remainder of this paper is organized as fol-
lowing. Section 2 gives the design considerations of 
our approach. Section 3 details our question analysis 
and web query formulation. Section 4 describes the 
web page classification and web document features 
used in FADA. Section 5 shows the algorithm of 
topic page clustering while Section 6 details the an-
swer extraction process. Section 7 discusses experi-
mental results. Section 8 concludes the paper. 
2 Design Considerations  
Our goal is to find as many distinct exact answers on 
the Web as possible. This requires us to:  
? perform effective and exhaustive search; and  
? extract distinct answers. 
In order to perform effective search, we employ 
question transformation to get effectual web queries. 
However, this is not a trivial task. If the query is too 
general, too many documents may be retrieved and 
the system would not have sufficient resources to 
scan through all of them. If the query is too specific, 
no pages may be retrieved. 
Given millions of web pages returned by search 
engines, our strategy is to divide-and-conquer by 
first identify Collection Pages (CP) that contain a list 
of answer instances. For example, for the question 
?What breeds of dog have won the "Best in Show" 
award at the Westminster Dog Show??, we can find 
a Collection Page as shown in Figure 1 (top). Such a 
web page is a very good resource of answers. In 
general, we observe that there is a large number of 
named entities of the type desired appearing in a 
Collection Page, typically in a list or table. Our in-
tuition is that if we can find a Collection Page that 
contains almost all the answers, then the rest of the 
work is simply to extract answers from it or related 
web pages by wrapper rule induction.  
Another kind of ?good? web page is a Topic Page, 
that contains just one answer instance (Figure 1, bot-
tom). It typically contains many named entities, 
which correspond to our original query terms and 
some other named entities of the answer target type. 
Given the huge amount of web data, there will be 
many Topic Pages that refer to the same answer in-
stance.  There is hence a need to group the pages and 
to identify a pertinent and distinctive page in order 
to represent a distinct answer.  
 
Table 2: Web Page Classes 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The rest of the top returned web pages could be 
either relevant or irrelevant to the question. In sum-
mary, we need to classify web pages into four 
classes: Collection Page, Topic Page, Relevant Page, 
and Irrelevant Page (Table 2), based on their func-
tionality and contribution in finding list answers. 
Based on the above considerations, we propose a 
general framework to find list answers on the Web 
using the following steps: 
a) Retrieve a good set of web documents. 
b) Identify Collection Pages and distinct Topic 
Pages as main resources of answers. 
c) Perform clustering on other web pages based on 
their similarities to distinct Topic Pages to form 
clusters that correspond to distinct answer in-
stances. 
d) Extract answers from Collection Pages and Topic 
Page clusters. 
3 Question Transformation and Web Page 
Retrieval  
Agichtein et al (2001) presented a technique on 
learning search engine specific query transforma-
tions for question answering. A set of transformation 
rules are learned from a training corpus and applied 
to the questions at the search time. Related work 
could also be found in Kwok et al (2001) where the 
user?s question is processed by a parser to learn its 
syntactic structure and various query modulation 
techniques are applied to the initial questions to get 
high quality results for later answer extraction. 
FADA performs question parsing to identify key 
question words and the expected answer type. It ex-
tracts several sets of words from the original ques-
tion and identifies the detailed question classes. It 
Web page class Description 
Collection Page Containing a list of  answers  
Topic Page The best page to represent an answer 
instance 
Relevant Page Relevant to an  answer instance by 
providing either support or objection to 
the Topic Page 
Irrelevant Page Not related to any answer instance 
then formulates a number of queries by combining 
the known facets together with heuristic patterns for 
list questions.  
We perform both shallow and full parsing on a 
question followed by Named Entity Recognition 
(NER) to get the known query facets and their types. 
The shallow parser we used is the free online mem-
ory-based chunker1 and the full parser is MINIPAR2. 
Both parsers are very efficient and usually parse 300 
words within a second. The procedure of query pars-
ing is as follows:  
a) Remove head words. The head words in a ques-
tion could be wh-question words and leading 
verbs. The list of head words includes ?who, what, 
when, where, which, how, how much, how many, 
list, name, give, provide, tell?, etc. Removing 
them enables us to get the correct subject/object 
relation and verb in the question. For example, for 
question ?What breeds of dog have won the ?Best 
in Show? award at the Westminster Dog Show??, 
after removing the head word, the question be-
comes ?breeds of dog have won the ?Best in 
Show? award at the Westminster Dog Show?.  
b) Detect subject and object for the remaining ques-
tion segments by shallow parsing. For example, 
after parsing the above question, we get: 
[NP1Subject breeds//NNS NP1Subject] {PNP [P 
of/IN P] [NP dog/NN NP] PNP} [VP1 
have/VBP won/VBN VP1] [NP1Object the/DT 
``/`` Best/JJS NP1Object] {PNP [P in/IN 
P] [NP Show/NNP ''/'' award/NN NP] 
PNP} {PNP [P at/IN P] [NP the/DT 
Westminster/NNP Dog//NNP Show/NNP NP] 
PNP} 
From the parsed sentence, we want to get the logi-
cal subject as the sentence subject or its immedi-
ate modifiers. Here we have the logical subject-
?breeds of dog?, verb-?won?, and logical object-
?the best in show award?. If the resulting logical 
subject/object is the term ?that? as in the follow-
ing parsed query for ?U.S. entertainers that later 
became politicians?: 
[NP U.S./NNP entertainers//NNS NP] 
[NP1Subject that/WDT NP1Subject] [ADVP 
later/RB ADVP] [VP1 became/VBD VP1] 
[NP1Object politicians//NNS NP1Object] 
we get the noun or noun phrase before the clause 
as the logical subject/object. Hence, we have the 
logical subject-?entertainers?, action-?became?, 
and logical object-?politician?. 
c) Extract all the noun phrases as potential descrip-
tions from the remaining question segments, 
which are usually prepositional phrases or clauses. 
For the ?dog breeds? example, we get the descrip-
tions-?Westminster Dog Show?.  
                                                          
1 http://ilk.kub.nl/cgi-bin/tstchunk/demo.pl 
2 http://www.cs.ualberta.ca/~lindek/minipar.htm 
d) Apply named entity recognition to the resulting 
description phrases by using NEParser, a fine-
grained named entity recognizer used in our 
TREC-12 system (Yang et al, 2003). It assigns 
tags like ?person?, ?location?, ?time?, ?date?, 
?number?. For the ?dog breed? example, ?West-
minster? gets the location tag. 
After the above analysis, we obtain all the known 
facets provided in the original question. We then 
make use of this knowledge to form web queries to 
get the right set of pages. This is a crucial task in 
dealing with the Web. One of the query transforma-
tion rules is given as follows:  
(list|directoty|category|top|favorite
)? (:|of)? <subj> <action>? <object>? 
<description1>? <description2>? ? 
<descriptionN>? 
The rule starts the query optionally with leading 
words (list, directory, category), optionally followed 
by a colon or ?of?, followed by subject phrase 
(<subj>), optionally followed by action (<action>), 
optionally followed by object (<object>) and de-
scription phrases (<description1>?<descriptionN>). 
In the above pattern, ??? denotes optional, ??? omit, 
and ?|? alternative. For example, for the ?dog breed? 
question, we form queries ?breed of dog won best in 
show Westminster Dog Show?, ?directory breed of 
dog best in show Westminster Dog Show?, and ?list 
breed of dog won best in show? etc. 
Transforming the initial natural language ques-
tions into a good query can dramatically improve the 
chances of finding good answers. FADA submits 
these queries to well-known search engines (Google, 
AltaVista, Yahoo) to get the top 1,000 Web pages 
per search engine per query. Here we attempt to re-
trieve a large number of web pages to serve our goal 
- find All Distinct answers. Usually, there are a large 
number of web pages which are redundant as they 
come from the same URL addresses. We remove the 
redundant web pages using the URL addresses as the 
guide. We also filter out files whose formats are nei-
ther HTML nor plain text and those whose lengths 
are too short or too long. Hence the size of the re-
sulting document set for each question varies from a 
few thousands to ten of thousands. 
4 Web Page Classification  
In order to group the web pages returned by the 
search engines into the four categories discussed 
earlier, it is crucial to find a good set of features to 
represent the web pages. Many techniques such as 
td.idf (Salton and Buckley, 1988) and a stop word 
list have been proposed to extract lexical features to 
help document clustering. However, they do not 
work well for question answering. As pointed out by 
Ye et al (2003) in their discussion on the per-
son/organization finding task, given two resume 
pages about different persons, it is highly possible 
that they are grouped into one cluster because they 
share many similar words and phrases. On the other 
hand, it is difficult to group together a news page 
and a resume page about the same target entity, due 
to the diversity in subject matter, word choice, liter-
ary styles and document format.  To overcome this 
problem, they used mostly named entity and link 
information as the basis for clustering. Compared to 
their task, our task of finding good web documents 
containing answers is much more complex. The fea-
tures are more heterogeneous, and it is more difficult 
to choose those that reflect the essential characteris-
tics of list answers. 
In our approach, we obtain the query words 
through subject/object detection and named entity 
recognition.  We found that there are a large number 
of named entities of the same type appearing in a 
Collection Page, typically within a list or table.  And 
in a Topic Page, there is also typically a group of 
named entities, which could correspond to our origi-
nal query terms or answer target type. Therefore, 
named entities play important roles in semantic ex-
pression and should be used to reflect the content of 
the pages.  
The Web track in past TREC conferences shows 
that URL, HTML structure, anchor text, hyperlinks, 
and document length tend to contain important heu-
ristic clues for web clustering and information re-
trieval (Craswell and Hawking, 2002). We have 
found that a Topic Page is highly likely to repeat the 
subject in its URL, title, or at the beginning of its 
page. In general, if the subject appears in important 
locations, such as in HTML tags <title>, <H1> and 
<H2>, or appears frequently, then the corresponding 
pages should be Topic Pages and their topic is about 
the answer target.  
Followed the above discussion, we design a set of 29 
features based on Known Named Entity Type, An-
swer Named Entity Type, ordinary Named Entities, 
list, table, URL, HTML structure, Anchor, Hyper-
links, and document length to rep resent the web 
pages. Table 3 lists the features used in our system. 
In the table and subsequent sections, NE refers to 
Named Entity. 
We trained two classifiers: the Collection Page 
classifier and the Topic Page classifier. The former 
classifies web pages into Collection Pages and non-
collection pages while the later further classifies the 
non-collection pages into Topic Pages and Others. 
Both Classifiers are implemented using Decision 
Tree C4.5 (Quinlan 1993). We used 50 list questions 
from TREC-10 and TREC-11 for training and 
TREC-12 list questions for testing. We parse the 
questions, formulate web queries and collect web 
pages by using the algorithm described in Section 2.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3: Web Page Features 
Each sample is represented using the features 
listed in Table 3. Some of the decision rules are as 
follows: 
a) OUT_Link >= 25 & NE > 78 &  
b) Answer_NE >= 30 -> Class CP OUT_Link 
<= 25 & Answer_NE <= 5 & NE > 46 -> 
Class TP  
c) OUT_Link >= 25 & URL_Depth > 3 -> 
Others 
d) NE <= 4  -> Others 
Rule a) implies that good Collection Pages should 
have many outlinks, NEs and especially answer NEs. 
Rule b) implies that good Topic Pages should have 
many NEs but relatively few links and answer NEs. 
Rule c) show that Others have deeper URL depth; 
while Rule d) shows that they have fewer NEs. 
 Feature Meaning 
1 |PER| # of Person NEs 
2 |ORG| # of Organization NEs 
3 |LOC| # of Location NEs 
4 |TME| # of Time NEs, including date, year, month 
5 |NUM| # of Numer NEs 
6 |COD| # of Code NEs, including phone number, 
zip code, etc 
7 |OBJ| # of Object NEs, including animal, planet, 
book,  etc 
8 |NE| Total number of the above NEs 
9 |Known_NE| Total # of NEs within the same NE type as 
in the question. In the ?dog breed? example, 
it is the number of Location NEs since 
?Westminster? is identified as Location by 
NER. 
10 |Unknown_N
E| 
# of NEs belonging to other NE type. In the 
?dog breed? example, it is the total number 
of Time and Breed NEs 
11 |Answer_NE| # of NEs belonging to expected answer 
type. In the ?dog breed? example, it is the 
number of Breed NEs 
12 |Known_NE| 
/ |NE| 
Ratio of  | Known _NE| to |NE| 
13 |Unknown_N
E| / |NE| 
Ratio of  | Unknown _NE| to |NE| 
14 |Answer_NE| 
/ |NE| 
Ratio of  |Answer_NE| to |NE| 
15 Length # of tokens in a page 
16 Content_Len
gth 
# of words in a page excluding HTML 
tags 
17 |NE|/Length Ratio of |NE| to |Token| 
18 |NE|/Content
_Length 
Ratio of |NE| to |Word| 
19 |In_Link| # of in-links 
20 |Out_Link| # of out-links  
21 |All_Link| The sum of in-links and out-links 
22 Keyword_in
_Title 
Boolean indicating presence of keywords in 
page title 
23 Keyword_ 
in_URL 
Boolean indicating presence of keywords in 
URL 
24 Keyword_ 
in_Page 
Boolean indicating presence of keywords in 
the page 
25 |Answer_NE
_in_Title| 
# of NEs belonging to expected answer type 
presenting in page title 
26 |Answer_NE
_in_URL| 
# of NEs belonging to expected answer type 
presenting in URL 
27 |<li>| # of HTML tags representing a list or table, 
including <li>, <ol>, <ul>, <br>,<td> 
28 |<li><a 
href=| 
# of HTML tags, including <li>, <ol>, 
<ul>, <br>,<td> to represent a list/table of 
anchors, 
29 URL_Depth The depth of URL 
 
Web page classification enables us to get Collec-
tion Pages, Topic Pages and the rest of the pages. 
Our experiments on TREC-12 list questions showed 
that we can achieve a classification precision of 
91.1% and 92% for Collection Pages and Topic 
Pages respectively. 
5 Finding Answer Sources  
Based on Web page classification, we form the ini-
tial sets of Collection Pages CPSet, Topic Pages 
TPSet and OtherSet. In order to boost the recall, we 
first use the outgoing links of Collection Pages to 
find more Topic Pages.  These outgoing pages are 
potential Topic Pages but not necessarily appearing 
among the top returned web documents. Our subse-
quent tests reveal that the new Topic Pages intro-
duced by links from Collection Pages greatly 
increase the overall answer recall by 23%. The new 
Topic Page set becomes: 
TPSet? = TPSet + {outgoing pages of CPs} 
Second, we select distinct Topic Pages. We com-
pare the page similarity between each pair of Topic 
Pages using the algorithm below.  
for each pair {tpi, tpj} in TPSet?  
  if (sim(tpi,tpj)> ?) 
    if ?ANE_in_tpi > ?ANE_in_tpj 
  move tpj into OtherSet; 
Here the page similarity function sim() is a linear 
combination of overlaps between Known_NE, An-
swer_NE, URL similarity and link similarity. ? is 
preset at 0.75 and may be overridden by the user. 
?ANE_in_tpi is the number of named entities of 
answer type in Topic Page tpi. For those pairs with 
high similarity, we keep the page that contains more 
named entities of answer type in TPSet? and move 
the other into OtherSet. The resulting Topic Pages 
in TPSet? are distinct and will be used as cluster 
seeds for the next step. 
Third, we identify and dispatch Relevant Pages 
from OtherSet into appropriate clusters based on 
their similarities with the cluster seeds.  
for each rpi in OtherSet { 
  k = argmax {sim(rpi , tpk) } 
 if (sim(rpi , tpk ) >  ? )  
   insert rpi into clusterk; 
  else  
    insert rpi into IrrelevantSet; } 
Here ?  is preset at 0.55, and sim() is defined as 
above. Each cluster corresponds to a distinct answer 
instance. The Topic Page provides the main facts 
about that answer instance while Relevant Pages 
provide supporting materials for the unique answer 
instance. The average ratio of correct clustering is 
54.1% in our experiments. 
Through web page clustering, we avoid early an-
swer redundancy, and have a higher chance to find-
ing distinct answers on the noisy Web. 
6 Answer Extraction 
6.1 HTML Source Page Cleaning  
Many HTML web pages contain common HTML 
mistakes, including missing or unmatched tags, end 
tags in the wrong order, missing quotes round attrib-
utes, missed / in end tags, and missing > closing tags, 
etc. We use HtmlTidy3 to clean up the web pages 
before classification and clustering. FADA also uses 
an efficient technique to remove advertisements. We 
periodically update the list from Accs-Net4, a site 
that specializes in creating such blacklists of adver-
tisers. If a link address matches an entry in a black-
list, the HTML portion that contained the link is 
removed.  
6.2 Answer Extraction from CP 
Collection Pages are very good answer resources for 
list QA. However, to extract the ?exact? answers 
from the resource page, we need to perform wrapper 
rule induction to extract the useful content. There is 
a large body of related work in content extraction, 
which enables us to process only extracted content 
rather than cluttered data coming directly from the 
web. Gupta et al (2003) parsed HTML documents 
to a Document Object Model tree and to extract the 
main content of a web page by removing the link 
lists and empty tables. In contrast, our link list ex-
tractor finds all link lists, which are table cells or 
lists for which the ratio of the number of links to the 
number of non-linked words is greater than a spe-
cific ratio. We have written separate extractors for 
each answer target type. The answers obtained in 
Collection Pages are then ?projected? onto the 
TREC AQUAINT corpus to get the TREC answers 
(Brill et al, 2001). 
6.3 Answer Extraction from TP Cluster 
Having web pages clustered for a certain question, 
especially when the clusters nicely match distinct 
answer, facilitates the task of extracting the possible 
answers based on the answer target type. We per-
form this by first analyzing the main Topic Pages in 
each cluster.  In case we find multiple passages con-
taining different answer candidates in the same 
Topic Page, we select the answer candidate from the 
passage that has the most variety of NE types since 
it is likely to be a comprehensive description about 
different facets of a question topic. The answer 
found in the Topic Page is then ?projected? onto the 
QA corpus to get the TREC answers as with the Col-
lection Page. In case no TREC answers can be found 
                                                          
3 http://htmltrim.sourceforge.net/tidy.html 
4 http://www.accs-net.com/hosts/get_hosts.html 
based on the Topic Page, we go to the next most 
relevant page in the same cluster to search for the 
answer. The process is repeated until either an an-
swer from the cluster is found in the TREC corpus 
or when all Relevant Pages in the cluster have been 
exhausted.   
For the question ?Which countries did the first 
lady Hillary Clinton visit??, we extracted the Loca-
tions after performing Named Entity analysis on 
each cluster and get 38 country names as answers. 
The recall is much higher than the best performing 
system (Harabagiu et al, 2003) in TREC-12 which 
found 26 out of 44 answers. 
7 Evaluation on TREC-12 Question Set 
We used the 37 TREC-12 list questions to test the 
overall performance of our system and compare the 
answers we found in the TREC AQUAINT corpus 
(after answer projection (Brill et al 2001)) with the 
answers provided by NIST.  
7.1 Tests of Web Page Classification 
In Section 3, the web pages are classified into three 
classes: Collection Pages, Topic Pages, and Others. 
Table 4 shows the system performance of the classi-
fication. We then perform a redistribution of classi-
fied pages, where the outgoing pages from CPs go to 
TP collection, and the Relevant Pages are grouped as 
supportive materials into clusters, which are based 
on distinct Topic Page. Nevertheless, the perform-
ance of web page classification will influence the 
later clustering and answer finding task. Table 4 
shows that we could achieve an overall classification 
average precision of 0.897 and average recall of 
0.851. This performance is adequate to support the 
subsequent steps of finding complete answers. 
 
 
 
 
 
 
Table 4: Performance of Web Page Classification 
7.2 Performance and Effects of Web Page 
Clustering 
Relevant Pages are put into clusters to provide sup-
portive material for a certain answer instance. The 
performance of Relevant Page dispatch/clustering is 
54.1%. We also test different clustering thresholds 
for our web page clustering as defined in Section 5. 
We use the F1 measure of the TREC-12 list QA re-
sults as the basis to compare the performance of dif-
ferent clustering threshold combinations as shown in 
xx. We obtain the best performance of F1 = 0.464 
when ?=0.55 and ?=0.75. 
 
 ?(0.55) ?(0.65) ?(0.75) ?=0.85
?=0.25 0.130 0.234 0. 324 0.236 
?=0.35 0.136 0.244 0. 338 0.232 
?=0.45 0.148 0.332 0. 428 0.146 
?=0.55 0.166 0.408 0. 464 0.244 
?=0.65 0.200 0.322 0. 432 0.236 
Table 5: Clustering Threshold Effects 
7.3 Overall Performance 
Table 6 compares a baseline list question answering 
system with FADA. The baseline is based on a sys-
tem which we used in participation in the TREC-12 
QA task (Yang et al, 2003).  It extends the tradi-
tional IR/NLP approach for factoid QA to perform 
list QA, as is done in most other TREC-12 systems. 
It achieves an average F1 of 0.319, and is ranked 2nd 
in the list QA task.   
We test two variants of FADA ? one without in-
troducing the outgoing pages from CPs as potential 
TPs (FADA1), and one with (FADA2). The two 
variants are used to evaluate the effects of CPs in the 
list QA task. The results of these two variants of 
FADA on the TREC-12 list task are presented in 
Table 6.  
Table 6: Performance on TREC-12 Test Set 
Without the benefit of the outgoing pages from 
CPs to find potential answers, FADA1 could boost 
the average recall by 30% and average F1 by 16.6% 
as compared to the baseline. The great improvement 
in recall is rather encouraging because it is crucial 
for a list QA system to find a complete set of an-
swers, which is how list QA differ from factoid QA. 
By taking advantage of the outgoing pages from 
CPs, FADA2 further improves performance to an 
average recall of 0.422 and average F1 of 0.464. It 
outperforms the best TREC-12 QA system (Voorhees, 
2003) by 19.6% in average F1 score. 
From Table 6, we found that the outgoing pages 
from the Collection Pages (or resource pages) con-
tribute much to answer finding task. It gives rise to 
an improvement in recall of 22.7% as compared to 
the variant of FADA1 that does not take advantage 
of outgoing pages. We think this is mainly due to the 
characteristics of the TREC-12 questions. Most 
questions ask for well-known things, and famous 
events, people, and organization. For this kind of 
questions, we can easily find a Collection Page that 
contains tabulated answers since there are web sites 
that host and maintain such information. For in-
stance, ?Westminster Dog Show? has an official 
 Avg P Avg R Avg F1
Baseline 0.568 0.264 0.319 
FADA1 (w/o outgoing pages) 0.406 0.344 0.372 
FADA2 (w/ outgoing pages) 0.516 0.422 0.464 
TREC-12 best run - - 0.396 
Page Class Avg Prec. Avg Rec. 
Collection 91.1% 89.5% 
Topic 92.0% 88.4% 
Relevant 86.5% 83.4% 
Overall 89.7% 85.1% 
 
web site5. However, for those questions that lack 
Collection Pages, such as ?Which countries did the 
first lady Hillary Clinton visit??, we still need to rely 
more on Topic Pages and Relevant Pages. 
With the emphasis on answer completeness and 
uniqueness, FADA uses a large set of documents 
obtained from the Web to find answers. As com-
pared to the baseline system, this results in a drop in 
average answer precision although both recall and F1 
are significantly improved. This is due to the fact 
that we seek most answers from the noisy Web di-
rectly, whereas in the baseline system, the Web is 
merely used to form new queries and the answers are 
found from the TREC AQUAINT corpus.  We are 
still working to find a good balance between preci-
sion and recall. 
The idea behind FADA system is simple: Since 
Web knowledge helps in answering factoid ques-
tions, why not list questions? Our approach in 
FADA demonstrates that this is possible. We believe 
that list QA should benefit even more than factoid 
QA from using Web knowledge. 
8 Conclusion 
We have presented the techniques used in FADA, 
a system which aims to find complete and distinct 
answers on the Web using question parsing, web 
page classification/clustering and content extraction. 
By using the novel approach, we can achieve a recall 
of 0.422 and F1 of 0.464, which is significantly bet-
ter than the top performing systems in the TREC-12 
List QA task. The method has been found to be ef-
fective. Our future work includes discovering an-
swers on non-text web information, such as images. 
Much text information is stored as images on the 
web, and hence, cannot be accessed by our approach, 
and some do contain valuable information. 
References  
E. Agichtein, S. Lawrence, and L. Gravano. 2001. 
"Learning search engine specific query transforma-
tions for question answering.? In the Proceedings of 
the 10th ACM World Wide Web Conference (WWW 
2001).  
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. 
?Data-intensive question answering?. In the Pro-
ceedings of the 10th  Text REtrieval Conference 
(TREC 2001). 
N. Craswell, D Hawking. 2002. ?Overview of the 
TREC-2002 Web Track?, In the Proceedings of the 
11th Text REtrieval Conference. (TREC 2002). 
S. Gupta, G. Kaiser, D. Neistadt, P. Grimm, 2003. 
?DOM-based Content Extraction of HTML Docu-
ments?, In the Proceedings of the 12th ACM World 
Wide Web conference. (WWW 2003). 
                                                          
5 http://www.westminsterkennelclub.org/ 
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. 
Williams, J. Bensley, 2003 ?Answer Mining by 
Combining Extraction Techniques with Abductive 
Reasoning,? In the notebook of the 12th Text RE-
trieval Conference (TREC 2003), 46-53.  
B. Katz, J. Lin, D. Loreto, W. Hildebrandt, M. Bilotti, 
S. Felshin, A. Fernandes, G. Marton, F. Mora, 2003, 
?Integrating Web-Based and Corpus-Based Tech-
niques for Question Answering?, In the notebook of 
the 12th Text REtrieval Conference (TREC 2003), 
472-480. 
C. Kwok, O. Etzioni, and D. S. Weld, 2001, ?Scaling 
Question Answering to the Web?, In the Proceed-
ings of the 10th ACM World Wide Web conference. 
(WWW 2001). 
C. Y. Lin, ?The Effectiveness of Dictionary and Web-
Based Answer Reranking.? In the Proceedings of the 
19th International Conference on Computational 
Linguistics (COLING 2002). 
B. Magnini, M. Negri, R. Prevete and H. Tanev. 2002. 
?Is it the Right Answer? Exploiting Web Redun-
dancy for Answer Validation?. In the Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics. (ACL 2002), 425-432. 
J. R. Quinlan, 1993. C4.5: Programs for Machine 
Learning. Morgan-Kaufmann, San Francisco. 
D. Ravichandran, and E. H. Hovy. 2002. ?Learning 
Surface Text Patterns for a Question Answering 
System.? In the Proceedings of the 40th  ACL con-
ference. (ACL 2002). 
G. Salton and C. Buckley, "Term-weighting ap-
proaches in automatic text retrieval", Information 
Processing and Management: an International Jour-
nal, v.24 n.5, 1988 
E.M.Voorhees. 2003. ?Overview of the TREC 2003 
Question Answering Track.? In the notebook of the 
12th Text REtrieval Conference (TREC 2003), 14-27. 
H. Yang, T. S. Chua, S Wang, C. K. Koh. 2003. 
?Structured Use of External Knowledge for Event-
based Open Domain Question Answering?, In the 
Proceedings of the 26th Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval (SIGIR 2003).  
H. Yang, H. Cui, M. Maslennikov, L. Qiu, M. Y. Kan, 
T. S. Chua. 2003. ?QUALIFIER in the TREC12 QA 
Main Task?, In the notebook of the 12th Text RE-
trieval Conference (TREC 2003). 
S. Ye, T. S. Chua, J. R. Kei. 2003. ?Querying and 
Clustering Web Pages about Persons and Organiza-
tions?. Web Intelligence 2003, 344-350. 
363
364
365
366
367
368
369
370
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1218?1226,
Beijing, August 2010
A Methodology for Automatic Identification of Nocuous Ambiguity 
 
Hui Yang1            Anne de Roeck1            Alistair Willis1            Bashar Nuseibeh1, 2 
1Department of Computing, The Open University 
2Lero, University of Limerick 
 {h.yang, a.deroeck, a.g.willis, b.nuseibeh}@open.ac.uk 
 
Abstract 
Nocuous ambiguity occurs when a lin-
guistic expression is interpreted differ-
ently by different readers in a given con-
text. We present an approach to auto-
matically identify nocuous ambiguity 
that is likely to lead to misunderstand-
ings among readers. Our model is built 
on a machine learning architecture. It 
learns from a set of heuristics each of 
which predicts a factor that may lead a 
reader to favor a particular interpretation. 
An ambiguity threshold indicates the ex-
tent to which ambiguity can be tolerated 
in the application domain. Collections of 
human judgments are used to train heu-
ristics and set ambiguity thresholds, and 
for evaluation. We report results from 
applying the methodology to coordina-
tion and anaphora ambiguity. Results 
show that the method can identify nocu-
ous ambiguity in text, and may be wid-
ened to cover further types of ambiguity. 
We discuss approaches to evaluation. 
1 Introduction 
Traditional accounts of ambiguity have generally 
assumed that each use of a linguistic expression 
has a unique intended interpretation in context, 
and attempted to develop a model to determine it 
(Nakov and Hearst, 2005; Brill and Resnik, 
1994). However, disambiguation is not always 
appropriate or even desirable (Poesio and Art-
stein, 2008). Ambiguous text may be interpreted 
differently by different readers, with no consen-
sus about which reading is the intended one. At-
tempting to assign a preferred interpretation may 
therefore be inappropriate. Misunderstandings 
among readers do occur and may have undesir-
able consequences. In requirements engineering 
processes, for example, this results in costly im-
plementation errors (Boyd et al, 2005).  
Nonetheless, most text does not lead to sig-
nificant misinterpretation. Our research aims to 
establish a model that estimates how likely an 
ambiguity is to lead to misunderstandings. Our 
previous work on nocuous ambiguity (Chantree 
et al, 2006; Willis et al, 2008) cast ambiguity 
not as a property of a text, but as a property of 
text in relation to a set of stakeholders. We drew 
on human judgments - interpretations held by a 
group of readers of a text ? to establish criteria 
for judging the presence of nocuous ambiguity. 
An ambiguity is innocuous if it is read in the 
same way by different people, and nocuous oth-
erwise. The model was tested on co-ordination 
ambiguity only. 
In this paper, we implement, refine and extend 
the model. We investigate two typical ambiguity 
types arising from coordination and anaphora. 
We extend the previous work (Willis et al, 
2008) with additional heuristics, and refine the 
concept of ambiguity threshold. We experiment 
with alternative machine learning algorithms to 
find optimal ways of combining the output of the 
heuristics. Yang et al (2010a) describes a com-
plete implementation in a prototype tool running 
on full text. Here we present our experimental 
results, to illustrate and evaluate the extended 
methodology. 
The rest of the paper is structured as follows. 
Section 2 introduces the methodology for auto-
matic detection of nocuous ambiguity. Sections 
3 and 4 provide details on how the model is ap-
plied to coordination and anaphora ambiguity. 
Experimental setup and results are reported in 
Section 5, and discussed in Section 6. Section 7 
reports on related work. Conclusions and future 
work are found in Section 8.          
1218
2 Methodology for Nocuous Ambiguity 
Identification 
This section describes the main ideas underpin-
ning our model of ambiguity. We distinguish 
between structural and interpretative aspects. 
The former captures the fact that text may have 
structure (i.e. syntax) which, in principle, per-
mits multiple readings. These are relatively 
straightforward to identify from the linguistic 
constructs present in the text. The latter ac-
knowledges that if text is interpreted in the same 
way by different readers, it has a low risk of be-
ing misunderstood. Modelling interpretive as-
pects requires access to human judgments about 
texts. Our approach has three elements, which 
we describe in turn: collection of human judg-
ments; heuristics that model those judgments, 
and a machine learning component to train the 
heuristics.  
 
Human judgments. We define an ambiguity as 
nocuous if it gives rise to diverging interpreta-
tions. Wasow et al (2003) suggests that ambigu-
ity is always a product of the meaning that peo-
ple assign to language, and thus a subjective 
phenomenon. We capture individual interpreta-
tions of instances of ambiguity by surveying par-
ticipants, asking them for their interpretation. 
We use this information to decide whether, 
given some ambiguity threshold, a particular 
instance is seen as innocuous or nocuous de-
pending on the degree of dissent between judges. 
A key concept in determining when ambiguity 
is nocuous is the ambiguity threshold. Different 
application areas may need to be more or less 
tolerant of ambiguity (Poesio and Artstein, 2008). 
For instance, requirements documents describing 
safety critical systems should seek to avoid mis-
understandings between stakeholders. Other 
cases, such as cookbooks, could be less sensitive. 
Willis et al (2008)?s general concept of ambigu-
ity threshold sought to implement a flexible tol-
erance level to nocuous ambiguity. Given an 
instance of ambiguous text, and a set of judg-
ments as to the correct interpretation, the cer-
tainty of an interpretation is the percentage of 
readers who assign that interpretation to the text. 
For example, in Table 1 below (sec. 3.1), the 
certainty of the two interpretations, HA and LA 
of expression (a) are 12/17=71% and 1/17=5.9% 
respectively. Here, an expression shows nocuous 
ambiguity if none of the possible interpretations 
have a certainty exceeding the chosen threshold. 
Later in this section, we will describe further 
experiments with alternative, finer grained ap-
proaches to setting and measuring thresholds, 
that affect the classifier?s behaviour. 
 
Heuristics. Heuristics capture factors that may 
favour specific interpretations. Each heuristic 
embodies a hypothesis, drawn from the literature, 
about a linguistic phenomenon signifying a pre-
ferred reading. Some use statistical information 
(e.g., word distribution information obtained 
from a generic corpus, the BNC 1 , using the 
Sketch Engine2). Others flag the presence of sur-
face features in the text, or draw on semantic or 
world knowledge extracted from linguistic re-
sources like WordNet3 or VerbNet4. 
 
Machine learning (ML). Individual heuristics 
have limited predictive power: their effective-
ness lies in their ability to operate in concert. 
Importantly, the information they encapsulate 
may be interdependent. We harness this by using 
ML techniques to combine the outputs of indi-
vidual heuristics. ML is an established method 
for recognizing complex patterns automatically, 
making intelligent decisions based on empirical 
data, and learning of complex and nonlinear re-
lations between data points. Our model uses su-
pervised learning ML techniques, deducing a 
function from training data, to classify instances 
of ambiguity into nocuous or innocuous cases. 
The classifier training data consists of pairs of 
input objects (i.e. vectors made up of heuristics 
scores) and desired outputs (i.e. the class labels 
determined by the distribution of human judg-
ments as captured by thresholds). To select an 
appropriate ML algorithm for the nocuity classi-
fier, we tested our datasets (described in later 
sections) on several algorithms in the WEKA5 
package (e.g., decision tree, J48, Naive Bayes, 
SVM, Logistic Regression, LogitBoost, etc.)  
To train, and validate, a nocuity classifier for 
a particular form of ambiguity, we build a data-
set of judgments, and select heuristics that model 
                                                 
1
 http://www.natcorp.ox.ac.uk/ 
2
 http://sketchengine.co.uk/ 
3
 http://wordnet.princeton.edu/ 
4
 http://verbs.colorado.edu/~mpalmer/projects/verbnet.html 
5
 http://www.cs.waikato.ac.nz/~ml/index.html 
1219
the information underlying the human judge-
ments about a preferred interpretation.  
We validated the approach on two forms of 
ambiguity. Sections 3 and 4 discuss how the 
methodology is applied to forms of coordination 
and anaphoric ambiguity, and evaluate the per-
formance of the final classifiers.                       
3 Automatic Identification of Nocuous 
Coordination Ambiguity 
Our previous work on nocuous ambiguity has 
focused on coordination ambiguity: a common 
kind of structural ambiguity. A coordination 
structure connects two words, phrases, or clauses 
together via a coordination conjunction (e.g., 
?and?, ?or?, etc) as in the following examples:  
 
(1) They support a typing system for architec-
tural components and connectors.  
(2) It might be rejected or flagged for further 
processing. 
 
     In (1), the coordination construction ?architec-
tural components and connectors? consists of a 
near conjunct (NC) (i.e. ?components?), a far 
conjunct (FC) (i.e. ?connectors?), and the at-
tached modifier (M) (i.e. ?architectural?). This 
construction allows two bracketings correspond-
ing to high modifier attachment ([architectural 
[components and connectors]]) or low modifier 
attachement ([[architectural components] and 
connector]). Our aim is to refine Chantree et al
(2006) and Willis et al(2008), hence our focus is 
on the two phenomena they treated: modification 
in noun phrase coordination (as in (1)) and in 
verb phrase coordination (as in (2)).   
     We implemented the heuristics described in 
the earlier work, and introduced two further ones 
(local document collocation frequency, and se-
mantic similarity). We used the Chantree et al
(2006) dataset of human judgments, but em-
ployed the LogitBoost algorithm for implement-
ing the nocuity classifier (rather than the Logis-
tic Regression equation). The following subsec-
tions give more detail. 
3.1 Building a dataset 
Coordination instances. Our dataset was col-
lected and described by Chantree et al (2006). It 
contains 138 coordination instances gathered 
from a set of requirement documents. Noun 
compound conjunctions account for the majority 
(85.5%) of cases (118 instances). Nearly half of 
these arose as a result of noun modifiers, while 
there are 36 cases with adjective and 18 with 
preposition modifiers. 
 
Human judgment collection. The coordination 
instances containing potential ambiguity were 
presented to a group of 17 computing profes-
sionals including academic staff or research stu-
dents. For each instance, the judges were asked 
to select one of three options: high modifier at-
tachment (HA), low modifier attachment (LA), 
or ambiguous (A). Table 1 shows the judgment 
count for two sample instances. In instance (a) in 
table 1, the certainty of HA is 12/17=71%, and 
the certainty of LA is 1/17=6%. Instance (b) was 
judged mainly to be ambiguous.  
 
 
 Judgments 
 HA LA A 
(a) security and privacy requirements 12 1 4 
(b) electrical characteristics and interface 4 4 9 
Table 1. Judgment count for the sample instances (HA=high at-
tachment; LA=low attachment; and A=Ambiguous) 
 
We set an ambiguity threshold, ?, to determine 
whether the distribution of interpretations is 
nocuous or innocuous with respect to that par-
ticular ?. If the certainty of neither interpretation, 
HA or LA, exceeds the threshold ?, we say this 
is an instance of nocuous coordination. Other-
wise it is innocuous. Here, (a) displays nocuous 
ambiguity for ?>71%. 
0
10
20
30
40
50
60
70
80
90
100
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ambiguity Thresholds 
Am
bi
gu
iti
e
s 
(%
)
Inno
Nocu
 
Figure 1. Proportions of interpretations at different ambiguity 
thresholds in the coordination instances 
Figure 1 shows the systematic relationship be-
tween ambiguity threshold and the incidence of 
nocuous ambiguity in the dataset. Low thresh-
olds can be satisfied with a very low certainty 
scores resulting in few instances being consid-
ered nocuous. At high thresholds, almost all in-
stances are classified as nocuous unless the 
judges report a consensus interpretation.  
1220
3.2 Heuristics to predict Nocuity 
Each heuristic tests a factor favouring a high or 
low modifier attachment (HA or LA). We im-
plemented and extended Willis et al (2008). 
 
Coordination matching favours HA when the 
head words of near and far conjuncts are fre-
quently found coordinated in a general corpus 
like BNC, suggesting they may form a single 
syntactic unit. 
 
Distribution similarity measures how often two 
words are found in the same contexts. It favours 
HA where it detects a strong distributional simi-
larity between the headwords of the two con-
juncts, suggesting these form a syntactic unit 
(Kilgariff 2003).  
 
Collocation frequency favours LA when the 
modifier is collocated much more frequently 
with the headword of the near conjunct than the 
far conjunct, in the document, or in the BNC. 
 
Morphology favours HA when the conjunct 
headwords share a morphological marker (suf-
fix) (Okumura and Muraki 1994).  
 
Semantic similarity favours HA when the con-
junct headwords display strong similarity in the 
taxonomic structure in WordNet6.  
3.3 Nocuity classification 
To train, and test, the nocuity classifier, each 
ambiguity training/test instance is represented as 
an attribute-value vector, with the values set to 
the score of a particular heuristic. The class label 
of each instance (nocuous (Y) or innocuous (N) 
at a given ambiguity threshold) is determined by 
the certainty measure as discussed earlier. We 
selected the LogitBoost algorithm for building 
the classifier, because it outperformed other can-
didates on our training data than. To determine 
whether a test instance displays nocuity or not, 
we presented its feature vector to the classifier, 
and obtained a predicted class label (Y or N). 
4 Automatic Identification of Nocuous 
Anaphora Ambiguity 
An anaphor is an expression referring to an an-
tecedent, usually a noun phrase (NP) found in 
                                                 
6
 Implemented by the NLP tool - Java WordNet Similarity Library. 
http://nlp.shef.ac.uk/result/software.html 
the preceding text. Anaphora ambiguity occurs 
when there are two or more candidate antece-
dents, as in example (3). 
 
(3) The procedure shall convert the 24 bit image to 
an 8 bit image, then display it in a dynamic window. 
 
In this case, both of the NPs, ?the 24 bit im-
age? and ?an 8 bit image?, are considered poten-
tial candidate antecedents of the anaphor ?it?. 
Anaphora ambiguity is difficult to handle due 
to contextual effects spread over several sen-
tences. Our goal is to determine whether a case 
of anaphora ambiguity is nocuous or innocuous, 
automatically, by using our methodology.  
4.1 The building of the Dataset 
Anaphora instances. We collected 200 anaph-
ora instances from requirements documents from 
RE@UTS website 7 . We are specifically con-
cerned with 3rd person pronouns, which are 
widespread in requirements texts. The dataset 
contains different pronoun types. Nearly half  
the cases (48%) involve subject pronouns, al-
though pronouns also occurred in objective and 
possessive positions (15% and 33%, respec-
tively).  Pronouns in prepositional phrases (e.g., 
?under it?) are rarer (4% - only 8 instances).  
 
Human judgment collection. The instances 
were presented to a group of 38 computing pro-
fessionals (academic staff, research students, 
software developers). For each instance, the 
judges were asked to select the antecedent from 
the list of NP candidates. Each instance was 
judged by at least 13 people. Table 2 shows an 
example of judgment counts, where 12 out of 13 
judges committed to ?supervisors? as the antece-
dent of ?they?, whereas 1 chose ?tasks?.   
 
1. Supervisors may only modify tasks they supervise to the 
agents they supervise.  
 Response 
Percent 
Response 
Count 
(a) supervisors 
(b) tasks 
92.3% 
7.7% 
12 
1 
Table 2. Judgment count for an anaphora ambiguity instance. 
 
Ambiguity threshold. Given an anaphor, the 
interpretation certainty of a particular NP candi-
date is calculated as the percentage of the judg-
ments for this NP against the total judgments for 
the instance. For example, consider the example 
in Table 2. The certainty of the NP ?supervisors? 
                                                 
7
 http://research.it.uts.edu.au/re/ 
1221
is 12/13=92.3% and the certainty of the NP 
?tasks? is 1/13=7.7%. Thus, at an ambiguity 
threshold of, for instance, ? = 0.8, the ambiguity 
in Table 2 is innocuous because the agreement 
between the judges exceeds the threshold. 
Figure 2 shows the relationship between am-
biguity threshold and occurrence of nocuous 
ambiguity. As in Figure 1, the number of nocu-
ous ambiguities increases with threshold ?. For 
high thresholds (e.g., ??0.9), more than 60% of 
instances are classified as nocuous. Below 
threshold (??0.4), fewer than 8 cases are judged 
nocuous. Also, comparing Figures 1 and 2 would 
appear to suggest that, in technical documents, 
anaphora ambiguity is less likely to lead to mis-
understandings than coordination.  
0
10
20
30
40
50
60
70
80
90
100
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ambiguity Thresholds
Am
bi
gu
iti
e
s 
(%
)
Inno
Nocu
Figure 2. Proportions of interpretations at different ambiguity 
thresholds in the anaphora instances. 
4.2 Antecedent Preference Heuristics 
Drawing on the literature on anaphoric reference, 
we developed 12 heuristics of three types: re-
lated to linguistic properties of text components, 
to context and discourse information, or to sta-
tistical information drawn from standard corpora. 
Yang et al (2010b) gives more detail. A heuris-
tic marks candidate antecedents which it favours, 
or disfavours. For instance, heuristics favour 
definite NPs as antecedents, candidate NPs 
which agree in number and syntactic role with 
the anaphor, and those which share a syntactic 
collocation pattern in the text. They also favour 
those which respect the semantic constraints 
(e.g., animacy) propagated from subcategorisa-
tion information, and reward proximity to the 
anaphor. They disfavour candidate antecedents 
that occur in prepositional phrases, and those 
occupying a syntactic role distinct from the ana-
phor. Note: not all NPs are marked by all heuris-
tics, and some heuristics are interdependent.   
4.3 Nocuous Ambiguity Identification 
Unlike coordination ambiguity, where judges 
chose for high or low modifier attachment, 
anaphora have scope over a variable set of po-
tential antecedents, depending on each particular 
instance. To accommodate this, we developed an 
antecedent classifier which assigns a weighted 
antecedent tag to each NP candidate associated 
with an instance. Tag information is used subse-
quently to predict the whether the instance dis-
plays nocuous ambiguity. 
The antecedent classifier is built using the Na-
ive Bayes algorithm within the WEKA package 
and is trained to return three classes of candidate 
antecedent: positive (Y), questionable (Q), or 
negative (N). In an innocuous case, a candidate 
NP will be classed as Y if its interpretation cer-
tainty exceeds the threshold set by ?, and tagged 
as N otherwise; in a nocuous case, it will be 
classed as N if its certainty is 0%, and classified 
as Q otherwise.  
 
1. The LPS operational scenarios represent sequences of activi-
ties performed by operations personnel as they relate to the LPS 
software. 
 Response Label 
(a) the LPS operational scenarios 
(b) sequences of activities 
(c) activities 
(d) operations personnel 
33.3% 
66.7% 
0% 
0% 
Q 
Q 
N 
N 
Table 3. The determination of antecedent label for the NP candi-
dates in a NOCUOUS ambiguity case (? =0.8) 
 
2. Testing performed to demonstrate to the acquirer that a 
CSCI system meets its specified requirements. 
 Response 
Percent 
Class 
Label 
(a) Testing 
(b) the acquirer 
(c) a CSCI system 
0% 
16.7% 
83.3% 
N 
N 
Y 
Table 4. The determination of antecedent label for the NP candi-
dates in a INNOCUOUS ambiguity case (? =0.8) 
 
Antecedent Class Label  
Y Q N 
? = 0.5 181 54 623 
? = 0.6 160 99 599 
? = 0.7 137 149 572 
? = 0.8 107 209 542 
? = 0.9 77 261 520 
? = 1.0 41 314 503 
Table 5. The distribution of three antecedent class label at different 
ambiguity thresholds 
 
Table 3 and 4 illustrate antecedent labels for 
NP antecedent candidates in a nocuous and in-
nocuous case. Candidates (a) and (b) in Table 3 
are labeled Q because their certainty falls below 
the threshold (? = 0.8). For the same threshold, 
candidate (c) in Table 4 is tagged as Y. Table 5 
1222
shows the distribution of tags at certainty thresh-
olds ? ? 0.5 for all (858) candidate antecedents 
in our sample. 
Our intended application is a system to alert 
experts to risk of misunderstandings. This sug-
gests we should emphasise recall even at the ex-
pense of some precision (Berry et al 2003). We 
developed two versions of the algorithm that 
determines whether an instance is nocuous or not, 
depending on the contribution made by its ante-
cedent candidates tagged Y. We relax constraints 
by introducing two concepts: a weak positive 
threshold W
Y
 and a weak negative threshold WN 
set at 0.5 and 0.4, respectively8. The rationale for 
weak thresholds is that antecedent preference 
reflects a spectrum with Y (high), Q (medium), 
and N (low). Weak positive and negative thresh-
olds act as buffers to the Q area. Antecedent NPs 
that fall in the W
Y
 or WN buffer area are treated 
as possible false negative (FN) for the classifica-
tion of the label Q. An antecedent tag Y/N is la-
beled as weak positive or negative depending on 
these thresholds. The algorithm for identifying 
nocuous ambiguity is given in Figure 3. It treats 
as innocuous those cases where the antecedent 
label list contains one clear Y candidate, whose 
certainty exceeds all others by a margin.  
 
Given an anaphora ambiguity instance with multiple potential NPs, 
the antecedent classifier returns a label list, },,,{ 21 nrrrR K= , for 
individual NPs. 
 
Parameters:  
1) W
Y
 - the threshold for the weak positive label. The label Y is 
viewed as weak positive when the positive prediction score ri < WY 
2) W
N
 - the threshold for the weak negative label. The label N is 
viewed as weak negative when the negative prediction score ri < 
W
N
 
 
Procedure: 
if the label list R contains  
         (one Y, no Q, one or more N ) 
    or  
         (no Y, one Q, one or more N but not weak negative ) 
    or  
        (one Y but not weak positive, any number of Q or N)    
then 
         the ambiguity is INNOCUOUS 
else 
         the ambiguity is NOCUOUS          
Figure 3. The algorithm for nocuous ambiguity identification 
5 Experiments and Results 
In all experiments, the performance was evalu-
ated using 5-fold cross-validation, using  stan-
                                                 
8
 Weak positive and negative thresholds are set experimentally. 
dard measures of Precision (P), Recall (R), F-
measure (F), and Accuracy. We use two naive 
baselines: BL-1 assumes that all ambiguity in-
stances are innocuous; BL-2 assumes that they 
are all nocuous. For fair comparison against the 
baselines, for both forms of ambiguity, we only 
report the performance of our ML-based models 
when the incidence of nocuous ambiguities falls 
between 10% ~ 90% of the set (see Figures 1 
and 2). We first report our findings for the iden-
tification of nocuous coordination ambiguities 
and then discuss the effectiveness of our model 
in distinguishing possible nocuous ambiguities 
from a set of ambiguity instances.    
5.1 Nocuous Coordination Ambiguity Iden-
tification 
Willis et al(2008) demonstrated the ability of 
their approach to adapt to different thresholds by 
plotting results against the two na?ve base lines. 
Since we extended and refined their approach 
described we plot our experimental results (CM-
1), for comparison, using the same measures, 
against their evaluation data (CM-2), in Figure 4.   
0
10
20
30
40
50
60
70
80
90
100
40 45 50 55 60 65 70 75 80
Ambiguity Threshold (%)
Ac
cu
ra
cy
 (%
) BL-1
BL-2
CM-1
CM-2
Figure 4. The performance comparison of the ML-based models, 
CM-1 and CM-2, to the two baseline models, BL-1 and BL-2, in 
nocuous coordination ambiguity identification.  
 
Our CM-1 model performed well with an ac-
curacy of above 75% on average at all ambiguity 
threshold levels. As expected, at very high and 
very low thresholds, we did not improve on the 
naive baselines (which have perfect recall and 
hence high accuracy). The CM-1 model dis-
played its advantage when the ambiguity thresh-
old fell in the range between 0.45 and 0.75 (a 
significantly wider range than reported for CM-2 
Willis et al(2008)). CM-1 maximum improve-
ment was achieved around the 58% crossover 
point where the two na?ve baselines intersect and  
our model achieved around 21% increased accu-
1223
racy. This suggests that the combined heuristics 
do have strong capability of distinguishing 
nocuous from innocuous ambiguity at the weak-
est region of the baseline models. 
Figure 4 also shows that, the CM-1 model 
benefitted from the extended heuristics and the 
LogitBoost algorithm with an increased accuracy 
of around 5.54% on average compared with CM-
2.  This suggests that local context information 
and semantic relationships between coordinating 
conjuncts provide useful clues for the identifica-
tion of nocuous ambiguity. Furthermore, the 
LogitBoost algorithm is more suitable for deal-
ing with a numeric-attribute feature vector than 
the previous Logistic Regression algorithm.  
5.2 Nocuous Anaphora Ambiguity Identifi-
cation 
We report on two implementations: one with 
weak thresholds (AM-1) and one without (AM-
2). We compare both approaches using the base-
lines, BL-1 and BL-2 (in Figure 5). It shows that 
AM-1 and AM-2 achieve consistent improve-
ments on baseline accuracy at high thresholds 
(??0.75). Here also, the improvement maximises 
around the 83% threshold point where the two 
baselines intersect. However, the ML-based 
models perform worse than BL-1 at the lower 
thresholds (0.5???0.7). One possible explanation 
is that, at low thresholds, performance is affected 
by lack of data for training of the Q class label, 
an important indicator for nocuous ambiguity 
(see Table 5). This is also consistent with the 
ML models performing well at higher thresh-
olds, when enough nocuous instances are avail-
able for training. 
0
10
20
30
40
50
60
70
80
90
100
50 55 60 65 70 75 80 85 90 100
Ambiguity Threshold (%)
Ac
cu
ra
cy
 
(%
)
BL-1
BL-2
AM-1
AM-2
 
Figure 5. The performance comparison of the ML-based models, 
AM-1 and AM-2, to the two baseline models, BL-1 and BL-2, in 
nocuous anaphora ambiguity identification.  
     
 Figure 5 further shows that the model with 
weak thresholds (AM-1) did not perform as well 
as the model without weak thresholds (AM-2) on 
accuracy. Although both models perform much 
better than the baselines on precision (more ex-
perimental results are reported in Yang et al 
(2010b)), the actual precisions for both models 
are relatively low, ranging from 0.3 ~ 0.6 at dif-
ferent thresholds. When the AM-1 model at-
tempts to discover more nocuous instances using 
weak thresholds, it also introduces more false 
positives (innocuous instances incorrectly 
classed as nocuous). The side-effect of introduc-
ing false positives for AM-1 is to lower accu-
racy. However, the AM-1 model outperforms 
both AM-2 and BL-2 models on F-measure 
(Figure 6), with an average increase of 5.2 and 
3.4 percentage points respectively. This reveals 
that relaxing sensitivity to the ambiguity thresh-
old helps catch more instances of nocuous 
anaphora ambiguity.             
10
15
20
25
30
35
40
45
50
55
60
50 55 60 65 70 75 80 85 90 100
Ambiguity Threshold (%)
F-
m
e
a
su
re
 
(%
) BL-2
AM-1
AM-2
Figure 6. The performance comparison of the ML-based models, 
AM-1 and AM-2, to the baseline model BL-2 (na?ve nocuous) 
6 Discussions 
We presented judges with sentences containing 
ambiguities without any surrounding context, 
even though contextual information (e.g., dis-
course focus) clearly contributes to interpreta-
tion. This is a weakness in our data collection 
technique. Besides contextual information, van 
Deemter?s Principle of Idiosyncratic Interpreta-
tion (1998) suggests that some factors, including 
the reader?s degree of language competence, can 
affect perceptions of ambiguity. Similarly, fa-
miliarity with a domain, including tacit specialist 
information (Polanyi, 1966), and the extent to 
which this is shared by a group, will have an ef-
fect on the extent to which stakeholders arrive at 
diverging interpretations. 
In our case, we extracted instances from re-
quirements documents covering several techni-
1224
cal domains. Judgements are sensitive to the 
backgrounds of the participants, and the extent 
to which stakeholder groups share such a back-
ground. Also, we used several large, generic NL 
resources, including the BNC and WordNet. The 
performance of several heuristics would change 
if they drew on domain specific resources. Dif-
ferent interpretations may be compatible, and so 
not necessarily contribute to misunderstanding.  
Finally, we used different machine learning 
algorithms to tackle different types of ambiguity 
instances: LogitBoost for coordination ambigu-
ity and Naive Bayes for anaphora ambiguity. 
The main reason is that coordination heuristics 
returned numeric values, whereas the anaphora 
heuristics were Boolean. Our method assumes 
tailoring of the ML algorithm to the choice of 
heuristic. These limitations indicate that the 
methodology has a high degree of flexibility, but 
also that it has several interdependent compo-
nents and background assumptions that have to 
be managed if an application is to be developed. 
7 Related Work 
Many researchers have remarked on the fact that 
some ambiguities are more likely than others to 
lead to misunderstandings, and suggested classi-
fying them accordingly. Poesio (1996) discussed 
cases where multiple readings are intended to 
coexist, and distinguished between language in-
herent and human disambiguation factors from a 
philosophical perspective. His notion of ?per-
ceived ambiguity? suggests that human percep-
tions are what actually cause an ambiguity to be 
misunderstood. Van Deemter?s (2004) ?vicious 
ambiguity? refers to an ambiguity that has no 
single, strongly preferred interpretation. He pro-
posed quantifying ?viciousness? using probabili-
ties taken from corpus data. Van Rooy (2004) 
defined a notion of ?true ambiguity?: a sentence 
is truly ambiguous only if there are at least two 
interpretations that are optimally relevant. These 
last two approaches rely on probability analysis 
of language usage, and not directly on human 
perception, which we believe to be the key to 
evaluating ambiguity. Our work differs in that it 
takes into account the distribution of interpreta-
tions arrived at by a group of human judges en-
gaged with a text. Our model treats ambiguity 
not as a property of a linguistic construct or a 
text, or a relation between a text and the percep-
tions of a single reader, but seeks to understand 
the mechanisms that lead to misunderstandings 
between people in a group or process. 
    Poesio et al(2006) have pointed out that dis-
ambiguation is not always necessary; for in-
stance, in some complex anaphora cases, the fi-
nal interpretation may not be fully specified, but 
only ?good enough?. Our work does not attempt 
disambiguation. It seeks to highlight the risk of 
multiple interpretations (whatever those are).   
8 Conclusions and Future Work 
We have presented a general methodology for 
automatically identifying nocuous ambiguity 
(i.e. cases of ambiguity where there is a risk that 
people will hold different interpretations) rela-
tive to some tolerance level set for such a risk. 
The methodology has been implemented in a 
ML based architecture, which combines a num-
ber of heuristics each highlighting factors which 
may affect how humans interpret ambiguous 
constructs. We have validated the methodology 
by identifying instances of nocuous ambiguity in 
coordination and anaphoric constructs. Human 
judgments were collected in a dataset used for 
training the ML algorithm and evaluation. Re-
sults are encouraging, showing an improvement 
of approximately 21% on accuracy for coordina-
tion ambiguity and about 3.4% on F-measure for 
anaphora ambiguity compared with naive base-
lines at different ambiguity threshold levels. We 
showed, by comparison with results reported in 
Willis et al(2008) that the methodology can be 
fine tuned, and extended to other ambiguity 
types, by including different heuristics.  
Our method can highlight the risk of different 
interpretations arising: this is not a task a single 
human could perform, as readers typically have 
access only to their own interpretation and are 
not routinely aware that others hold a different 
one. Nonetheless, our approach has limitations, 
particularly around data collection, and for 
anaphora ambiguity at low thresholds. We en-
visage further work on the implementation of 
ambiguity tolerance thresholds 
Several interesting issues remain to be inves-
tigated to improve our system?s performance and 
validate its use in practice. We need to explore 
how to include different and complex ambiguity 
types (e.g., PP attachment and quantifier scop-
1225
ing), and investigate whether these are equally 
amenable to a heuristics based approach.  
Acknowledgement  
This work is supported financially by UK EPSRC for 
the MaTREx project (EP/F068859/1), and Irish SFI 
for the grant 03/CE2/I303_1. 
References 
Daniel M. Berry, Erik Kamsties, and Michael M. 
Krieger.  2003. From Contract Drafting to Soft-
ware Specification: Linguistic Sources of Ambigu-
ity. Technical Report, School of Computer Sci-
ence, University of Waterloo.  
Stephen Boyd, Didar Zowghi, and Alia Farroukh.  
2005. Measuring the Expressiveness of a Con-
strained Natural Language: An Empirical Study. In 
Proceedings of the 13th IEEE International Con-
ference on Requirements Engineering (RE?05), 
Washington, DC, pages 339-52. 
Eric Brill and Philip Resnik.  1994. A Rule-Based 
Approach to Prepositional Phrase Attachment Dis-
ambiguation. In Proceedings of the 15th Interna-
tional Conference on Computational Linguistics, 
pages 1198-204. 
Francis Chantree, Bashar Nuseibeh, Anne de Roeck, 
and Alistair Willis.  2006. Identifying Nocuous 
Ambiguities in Natural Language Requirements. 
In Proceedings of 14th IEEE International Re-
quirements Engineering Conference (RE'06), Min-
neapolis, USA, pages 59-68. 
Adam Kilgarriff.  2003. Thesauruses for Natural Lan-
guage Processing. In Proceedings of NLP-KE, 
pages 5-13. 
Preslav Nakov and Marti  Hearst.  2005. Using the 
Web as an Implicit Training Set: Application to 
Structural Ambiguity Resolution. In Proceedings 
of HLT-NAACL?05, pages 835-42. 
Akitoshi Okumura and Kazunori Muraki.  1994. 
Symmetric Pattern Matching Analysis for English 
Coordinate Structures. In Proceedings of the 4th 
Conference on Applied Natural Language Proc-
essing, pages 41-46. 
Massimo Poesio. 1996. Semantic Ambiguity and Per-
ceived Ambiguity In Semantic Ambiguity and Un-
derspecification edited by K. van Deemter and S. 
Peters, pages 159-201. 
Massimo Poesio and Ron Artstein. 2008. Introduction 
to the Special Issue on Ambiguity and Semantic 
Judgements. Research on Language & Computa-
tion 6: 241-45. 
Massimo Poesio, Patick Sturt, Ron Artstein, and Ruth 
Filik. 2006. Underspecification and Anaphora: 
Theoretical Issues and Preliminary Evidence. Dis-
course Processes 42(2): 157-75. 
Michael Polanyi.  1966. The Tacit Dimension. RKP, 
London. 
Kees van Deemter. 1998. Ambiguity and Idiosyn-
cratic Interpretation. Journal of Semantics 15(1): 
5-36. 
Kees van Deemter. 2004. Towards a Probabilistic 
Version of Bidirectional Ot Syntax and Semantics. 
Journal of Semantics 21(3): 251-80. 
Robert van Rooy. 2004. Relevance and Bidirectional 
Ot. In Optimality Theory and Pragmatic, edited by 
R. Blutner and H. Zeevat, pages 173-210. 
Thomas Wasow, Amy Perfors, and David Beaver. 
2003. The Puzzle of Ambiguity. In Morphology 
and the Web of Grammar: Essays in Menory of 
Steven G. Lapointe, edited by O. Orgun and P. 
Sells. 
Alistair Willis, Francis Chantree, and Anne De 
Roeck. 2008. Automatic Identification of Nocuous 
Ambiguity. Research on Language & Computa-
tion 6(3-4): 1-23. 
Hui Yang, Alistair Willis, Anne de Roeck, and Ba-
shar Nuseibeh. 2010a. Automatic Detection of 
Nocuous Coordination Ambiguities in Natural 
Language Requirements. In Proceedings of the 
25th IEEE/ACM International Conference on 
Automated Software Engineering Conference 
(ASE?10). (In press) 
Hui Yang, Anne de Roeck, Alistair Willis, and Ba-
shar Nuseibeh. 2010b. Extending Nocuous Ambi-
guity Analysis for Anaphora in Natural Language 
Requirements. In Proceedings of the 18th Interna-
tional Requirements Engineering Conference 
(RE?10). (In press) 
 
1226
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1278?1289, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Constructing Task-Specific Taxonomies for Document Collection Browsing
Hui Yang
Department of Computer Science
Georgetown University
37th and O street, NW
Washington, DC, 20057
huiyang@cs.georgetown.edu
Abstract
Taxonomies can serve as browsing tools for
document collections. However, given an ar-
bitrary collection, pre-constructed taxonomies
could not easily adapt to the specific topic/task
present in the collection. This paper explores
techniques to quickly derive task-specific tax-
onomies supporting browsing in arbitrary
document collections. The supervised ap-
proach directly learns semantic distances from
users to propose meaningful task-specific tax-
onomies. The approach aims to produce glob-
ally optimized taxonomy structures by incor-
porating path consistency control and user-
generated task specification into the general
learning framework. A comparison to state-
of-the-art systems and a user study jointly
demonstrate that our techniques are highly ef-
fective.
1 Introduction
Taxonomies are widely used for knowledge stan-
dardization, knowledge sharing, and inferencing in
natural language processing (NLP) tasks (Harabagiu
et al 2003; Szpektor et al 2004). However, an-
other common function of taxonomies, browsing,
has received little attention in the NLP community.
Browsing is the task of exploring and accessing in-
formation through a structure, e.g. a hierarchy, built
upon a given document collection. In fact, tax-
onomies serve as browsing tools in many venues,
including the Library of Congress Subject Headings
(LCSH, 2011) for the U.S. Library of Congress and
the Open Directory Project (ODP, 2011) for about
5% of the entire Web. We call taxonomies support-
ing browsing as browsing taxonomies.
When used for browsing, concepts1 in taxonomies
are linked to documents containing them and taxo-
nomic structures are navigated to find particular doc-
uments. Users can navigate through a browsing tax-
onomy to explore the documents in the collection.
A browsing taxonomy benefits information access
by providing corpus overview for a document col-
lection and allowing more focused reading by pre-
senting together documents about the same concept.
Most existing browsing taxonomies, such as
LCSH and ODP, are manually constructed to sup-
port large collections in general domains. Not only
their constructions are expensive and slow, but also
their structures are static and difficult to adapt to spe-
cific tasks. In situations where document collections
are given ad-hoc, such as search result organization
(Carpineto et al 2009), email collection exploration
(Yang and Callan, 2008), and literature investigation
(Chau et al 2011), existing taxonomies may even
not be able to provide the right coverage of concepts.
It is necessary to explore ad-hoc (semi-)automatic
techniques to quickly derive task-specific browsing
taxonomies for arbitrary document collections.
(Hovy, 2002) pointed out that one key challenge
in taxonomy construction is multiple perspectives
embedded in concepts and relations. One cause for
multiple perspectives is the inherent facets in con-
cepts, e.g., jewelries can be organized by price or by
gemstone types. Another cause is task specification
or even personalization. For example, when build-
ing a taxonomy for search results of query trip to
1English terms or entities; usually nouns or noun phrases.
1278
DC, Jane may organize the concepts based on places
of interests while Tom may organize them based on
dates in visit. Typically, a taxonomy only conveys
one or two perspectives from many choices. It is dif-
ficult to decide which perspective should be present.
One realistic solution is to leave the decision to the
constructor independent of the confusion that comes
from facets, task specification or personalization.
When multiple perspectives present in the same
taxonomy, it is not uncommon that the per-
spectives are mixed. For example, along a
path financial institute?bank?river bank, finan-
cial institute?bank shows one perspective and
bank?river bank shows another. We call this prob-
lem path inconsistency. Many approaches on auto-
matic taxonomy construction suffer from this prob-
lem because their foci are on accurately identifying
local relations between concept pairs (Etzioni et al
2005; Pantel and Pennacchiotti, 2006) instead of on
global control over the entire taxonomic structure.
More recently, approaches attempted to build the full
taxonomy structure (Snow et al 2006; Yang and
Callan, 2009; Kozareva and Hovy, 2010), however,
few have looked into how to incorporate task speci-
fications into taxonomy construction.
In this paper, we extended an existing taxonomy
construction approach (Yang and Callan, 2009) to
build task-specific taxonomies for document collec-
tion browsing. The extension comes in two parts:
handling path consistency and incorporating spec-
ifications from users. We uniquely employ pair-
wise semantic distance as an entry point to incre-
mentally build browsing taxonomies. A supervised
distance learning algorithm not only allows us to
incorporate multiple semantic features to evaluate
the proximity between concepts, but also allows us
to learn the metric function from personal prefer-
ences. Users can thus manually modify the tax-
onomies and to some extent teach the algorithm to
predict his/her way to organize the concepts. More-
over, by minimizing the overall semantic distances
among concepts and restricting minimal semantic
distances along a path, we find the best hierarchical
structure as the browsing taxonomy.
Our contributions include:
- A supervised learning mechanism to capture
task-specific or personalized requirements for orga-
nizing a browsing taxonomy;
- A strategy to address path inconsistency due to
word sense ambiguity and/or mixed perspectives;
- A general scheme to capture user inputs in tax-
onomy construction;
- A user study to evaluate the effectiveness of
task-specific taxonomies for browsing activities.
2 Related Work
Document collection browsing has been studied as
an alternative to the ranked list representation for
search results by the Information Retrieval (IR)
community. The popular IR approaches include
clustering (Cutting et al 1992) and monothetic con-
cept hierarchies (Sanderson and Croft, 1999; Lawrie
et al 2001; Kummamuru et al 2004; Carpineto
et al 2009). Clustering approaches hierarchically
cluster documents in a collection and label the clus-
ters. Monothetic approaches organize the concepts
into hierarchies and link documents to related con-
cepts. Both approaches are mainly based on pure
statistics, such as document frequency (Sanderson
and Croft, 1999) and conditional probability (Lawrie
et al 2001). The major drawback of these pure
statistical approaches is their neglect of semantics
among concepts. As an consequence, they often fail
to produce semantically meaningful taxonomies.
The NLP community has extensively studied
automatic taxonomy construction. Although tra-
ditional research on taxonomy construction fo-
cuses on extracting local relations between concept
pairs (Hearst, 1992; Berland and Charniak, 1999;
Ravichandran and Hovy, 2002; Girju et al 2003;
Etzioni et al 2005; Pantel and Pennacchiotti, 2006;
Kozareva et al 2008), more recent efforts has been
made in building full taxonomies. For example,
(Snow et al 2006) proposed to estimate taxonomic
structure via maximizing the overall likelihood of a
taxonomy. (Kozareva and Hovy, 2010) proposed to
connect local concept pairs by finding the longest
path in a subsumption graph. Yang and Callan pro-
posed the Minimum Evolution (ME) framework
to model the semantic distance d(cx, cy) between
concepts cx and cy as a weighted combination of
various lexical, statistical, and semantic features:
?
j weightj ? featurej(cx, cy) and estimate the taxo-
nomic structure by minimizing the overall semantic
distances.
1279
Researcher also attempted to carve out tax-
onomies from existing ones. For example, Stoica
et al(Stoica and Hearst, 2007) managed to extract a
browsing taxonomy from hypernym relations within
WordNet (Fellbaum, 1998).
To support browsing in arbitrary collections, in
this paper, we propose to incorporate task specifica-
tion in a taxonomy. One way to achieve it is to define
task-specific distances among concepts. Moreover,
through controlling distance scores among concepts,
we can enforce path consistency in taxonomies. For
example, when the distance between financial in-
stitute and river bank is big, the path financial
institute?bank?river bank will be pruned and the
concepts will be repositioned. Inspired by ME, we
take a distance learning approach to deal with path
consistency (Section 3) and task specification (Sec-
tion 4) in taxonomy construction.
3 Build Structure-Optimized Taxonomies
This section presents how to automatically build tax-
onomies. We take two steps to build browsing tax-
onomy for a given document collection. The first
step is to extract the concepts and the second is to
organize the concepts. For concept extraction, we
take a simple but effective approach: (1) We first
parse the document collection and exhaustively ex-
tract nouns, noun phrases, and named entities that
occur >5 times in the collection. (2) We then fil-
ter out part-of-speech errors and typos by a Web-
based frequency test. In the test, we search each
candidate concept in the Google search engine and
remove a candidate if it appears <4 times within
the top 10 Google snippets. (3) We finally cluster
similar concept candidates into groups by Latent Se-
mantic Analysis (Bellegarda et al 1996) and select
the candidate with the highest tfidf value within a
group to form the concept set C. Although our ex-
traction algorithm is very effective with 95% preci-
sion and 80% recall in a manual evaluation, some-
times C may still miss some important concepts for
the collection. This can be later corrected by users
interactively through adding new concepts (Section
4).
To organize the concepts in C into taxonomic
structures, we extend the incremental clustering
framework proposed by ME (Yang and Callan,
2009). In ME, concepts are inserted one at a time.
At each insertion, a concept cz is at the parent (or
child) position for every existing node in the current
taxonomy. The evaluation of the best position de-
pends on the semantic distance between cz and its
temporary child (or parent) node and the semantic
distance among all other concepts in the taxonomy.
An advantage in ME is that it allows incorporat-
ing various constraints to the taxonomic structure.
For example, ME can handle concept generality-
specificity by learning different semantic distance
functions for general concepts which are located at
upper levels and specific concepts which are located
at lower levels in a taxonomy.
In this section, we introduce a new semantic dis-
tance learning method (Section 3.1) and extendME
by controlling path consistency (Section 3.2).
3.1 Estimating Semantic Distances
Pair-wise semantic distances among concepts build
the foundation for taxonomy construction. ME
models the semantic distance d(cx, cy) between con-
cepts cx and cy as a linear combination of underly-
ing feature functions. Similar to ME, we also as-
sume that ?there are some underlying feature func-
tions that measure semantic dissimilarity for con-
cepts and a good semantic distance is a combination
of these features?. Different from ME, we model
the semantic distance d(cx, cy) between concepts
(cx, cy) as a Mahalanobis distance (Mahalanobis,
1936): dcx,cy =
?
?(cx, cy)TW?1?(cx, xy), where
?(cx, cy) is the set of underlying feature functions
{?k : (cx, cy)}with k=1,...,|?|. W is the weight ma-
trix, whose diagonal values weigh the various fea-
ture functions. We use the same set of features as
proposed in ME.
Mahalanobis distance is a general parametric
function widely used in distance metric learning
(Yang, 2006). It measures the dissimilarity between
two random vectors of the same distribution with a
covariance matrix W , which scales the data points
from their original values by W 1/2. When only di-
agonal values of W are taken into account, W is
equivalent to assigning weights to different axes in
the random vectors.
We choose Mahalanobis distance for two reasons.
(1) It is in a parametric form so that it allows us to
learn a distance function by supervised learning and
1280
provides an opportunity to assign different weights
for each type of semantic features. (2) When W
is properly constrained to be positive semi-definite
(PSD) (Bhatia, 2006), a Mahalanobis-formatted dis-
tance will be guaranteed to satisfy non-negativity
and triangle inequality, which was not addressed in
ME. As long as these two conditions are satisfied,
one may learn other forms of distance functions to
represent a semantic distance.
We can estimateW by minimizing the squared er-
rors between training semantic distances d and the
expected value d?. We also need to constrain W
to be PSD to satisfy triangle inequality and non-
negativity. The objective function for semantic dis-
tance estimation is:
min
W
|C|?
x=1
|C|?
y=1
(
dcx,cy ?
?
?(cx, cy)TW?1?(cx, cy)
)2
subject to W  0
(1)
In this implementation, we used (Sedumi, 2011) and
(Yalmip, 2011) to solve the semi-definite program-
ming (SDP).
To generate the training semantic distances, we
collected 100 hypernym taxonomy fragments from
WordNet (Fellbaum, 1998) and ODP. The seman-
tic distance for a concept pair (cx, cy) in a training
taxonomy fragment is generated by assuming ev-
ery edge is weighted as 1 and summing up the edge
weights along the shortest path from cx to cy in the
taxonomy fragment. In Section 4, we will show how
to use user inputs as training data to capture task-
specifications in taxonomy construction.
3.2 Enforcing Path Consistency
In ME, the main taxonomy structure optimization
framework is based on minimization of overall se-
mantic distance among all concepts in the taxonomy
and the minimum evolution assumption. We extend
the framework by introducing another optimization
objective to the framework: path consistency objec-
tive. The idea is that in any root-to-leaf path in a tax-
onomy, all concepts on the path should be about the
same topic or the same perspective. Within a root-to-
leaf path, the concepts need to be coherent no mat-
ter how far away they are apart. It suggests that a
good path?s sum of the semantic distances should be
small.
Algorithm: Automatic Taxonomy Optimization.
W = minW
?
x=1
?|N(ctrx )|
y=1 ((dctrx ,ctry??
?(ctrx , ctry )TW?1?(ctrx , ctry ))
2;
foreach cz ? C \ S
S ? S ? {cz};
if W  0
d(cz , .) =
?
?(cz , .)TW?1?(cz , );
R? R ? {arg minR(cz,.) (? objME + (1? ?) objpath)};
Output T (S,R)
Figure 1: An algorithm for taxonomy structure optimiza-
tion with path consistency control. C denotes the entire
concept set, S the current concept set, and R the current
relation set. N(ctrx) is the neighborhood of a training
concept ctrx , including its parent and child(ten). R(cz, .)
indicates the set of relations between a new concept cz
and all other existing concepts. T is the taxonomy with
concept set S and relation set R.
Therefore, we propose to minimize the sum of se-
mantic distances along a root-to-leaf path. Particu-
larly, when adding a new concept cz into an existing
browsing hierarchy T , we try it at different positions
in T . At each temporary position, we can calculate
the sum of the semantic distances along the root-to-
leaf path Pcz that contains the new concept cx. The
path consistency objective is given by:
objpath = minPcz
?
cx,cy?Pcz ,x<y
d(cx, cy) (2)
where x < y defines the order of the concepts to
avoid counting the same pair of pair-wise distances
twice.
Towards modeling path consistency in taxonomy
construction, we introduce a Pareto co-efficient ? ?
[0, 1] to control the contributions from objME , the
overall semantic distance minimization objective as
proposed inME, and objpath, the path distance min-
imization objective. The optimization is:
min? objME + (1? ?) objpath (3)
where objME = |
?
cx,cy?Cn?{cz},x<y d(cx, cy) ??
cx,cy?Cn,x<y d(cx, cy)|, 0 ? ? ? 1, and C
n is the
concept set after nth concept is added. Empirically,
we set ? = 0.8.
The algorithm shown in Figure 1 outlines our
greedy algorithm to build taxonomies with path con-
sistency control. Each time when a new concept ar-
rives, the algorithm first estimates its semantic dis-
tances based on W learned from the training data,
1281
then finds the optimal position for the concept by
minimizing overall semantic distances and path in-
consistency, and gradually grows the structure into a
full taxonomy.
The order of adding concepts may affect the final
taxonomy structure. We hence insert concepts in an
arbitrary order with 10 random restarts with differ-
ent initial concepts and pick the taxonomy that min-
imizes both objectives among all candidate struc-
tures.
4 Incorporating Task Specification
This section studies how to incorporate user-defined
task specifications in taxonomy construction. Al-
though the automatic algorithm proposed in Section
3 is able to well-organize most concepts for a given
document collection, it has not yet addressed the is-
sue of mixed perspective in taxonomy construction.
For concepts with multiple perspectives, we need to
decide which perspective is more appropriate for the
browsing taxonomy. This task-specific requirement
can only be captured by the user/constructor who
builds and uses the browsing taxonomy. Moreover,
the automatic algorithm relies on training data from
WordNet and ODP, which are known for imperfect
term organizations such as unbalanced granularity
among terms at the same level. To correct the wrong
relations learned from imperfect training data, we
propose to utilize user inputs in the learning process.
Particularly, we formulate taxonomy construction
as a user-teaching-machine-learning process. To
guide how to organize the concepts, a user trains
the supervised distance learning model via a taxon-
omy construction interface that allows the user to
intuitively modify a taxonomy. The interface sup-
ports editing actions such as dragging and dropping,
adding, deleting, and renaming nodes. When a user
put cx under cy, i.e. cx ? cy, this action indi-
cates that the user wants a relation represented by
cx ? cy to be true in this taxonomy. We did not
expect users to make all the edits. In a human-
computer-interaction cycle, a user is not restricted
to give a certain number of edits. Based on a user
study (Section 5.5), an average number of edits per
interaction is 3.6, which can be achieved with ease
by most users.
The algorithm shown in Figure 2 provides the
Algorithm: Interactive Taxonomy Construction.
1. T (S,R) =CreateInitialTaxonomy();
2. U(0)={Unmodified Concepts}=C \ S,
G(0)={Modified concepts}=S, M(0) = ?, i = 0;
3. while (not Satisfied) or U(i) 6= ?
4. M(i)=CollectManualGuidance(G(i),U(i));
5. W (i)=LearnDistanceMetricFunction(M(i));
6. D(i)=PredictDistanceScores(W (i),U(i));
7. (G(i+1), U(i+1)) = UpdateTaxonomy(D(i),U(i),G(i));
8. i = i+ 1;
9. Output G(i) as the taxonomy.
Figure 2: Interactive taxonomy construction procedure.
pseudo code for the interactive taxonomy construc-
tion procedure. It starts with automatic construction
of initial taxonomies using the techniques presented
in Section 3 (Line 1). We then capture the user in-
puts as manual guidance (Line 4) and make use of it
to adjust the distance learning model (Line 5), make
new predictions for semantic distances of other con-
cepts (Line 6), and organize those concepts to agree
with the user and update the taxonomy accordingly
(Line 7). Line 2 initiates three variables, the unmod-
ified concepts U , the modified concepts G, and the
manual guidance M , indexed by the iteration num-
ber i. The process iterates until the user is satisfied
with the taxonomy?s organization (Line 3).
Learning and predicting distances have been pre-
sented in Section 3.1. In this section, we present how
to capture manual guidance (Section 4.1) and update
the taxonomies accordingly (Section 4.2).
4.1 Manual Guidance as the Training Data
Taxonomies are tree-structured. It is not trivial to
model a taxonomy, especially changes in a taxon-
omy, and feed that into a learning algorithm. In
this section, we propose a general scheme to cap-
ture changes, i.e., user inputs during interactions, in
taxonomy construction.
We propose to convert a taxonomy into matrices
of neighboring nodes. We compare the changes be-
tween a series of snapshots of the changing taxon-
omy to identify the user inputs. Specifically, before
a user starts editing in an interaction cycle, we repre-
sent the organization of concepts as a before matrix;
likewise, after the user finishes all edits in one cy-
cle, we represent the new organization of concepts
as an after matrix. For both matrices, the (x, y)th
entry indicates whether (or how confident) a rela-
tion r(cx, cy) is true. r could be any type of relation
1282
Before	 ?human	 ?edits	 ?
Before	 ?Matrix	 ?
A?r 	 ?human	 ?edits	 ?
A?er	 ?Matrix	 ?
????
????
????
??
??
????
????
????
??
??
10000
01000
00110
00110
00001
????
????
????
??
??
????
????
????
??
??
10000
01100
01100
00010
00001Movie awards  Oscars Best supporting. Best pictures George Clooney 
Movie	 ?awards	 ?
Oscars	 ? Best	 ?suppor?ng	 ?actors	 ?
Best	 ?pictures	 ? George	 ?Clooney	 ?
Movie	 ?awards	 ?
Oscars	 ?
Best	 ?suppor?ng	 ?actors	 ?Best	 ?pictures	 ?
George	 ?Clooney	 ?
mov. osc. sup. pic. geo. mov. osc. sup. pic. geo. Movie awards  Oscars Best supporting. Best pictures George Clooney 
Figure 3: An example taxonomy before and after human
edits (Concepts unchanged; relation type = sibling).
between the concepts. Figure 3 shows an example
taxonomy?s before and after matrices.
We define manual guidance M as a submatrix
which consists of entries in the after matrix B; at
these entries, there exist differences between the be-
fore matrix A and the after matrix B. Formally,
M = B[r; c]
r = {i : bij ? aij 6= 0}
c = {j : bij ? aij 6= 0}
(4)
where aij is the (i, j)th entry in A, bij is the (i, j)th
entry in B, r indicates the rows and c indicates the
columns.
Note that manual guidance is not simply the
matrix difference between A and B. It is part of
the after matrix because it is the after matrix that
indicates where the user wants the concept hierarchy
to develop. The manual guidance for the example
shown in Figure 3 is: M = B[2, 3, 4; 2, 3, 4] =
Oscars Best supporting Best picture
Oscars 1 0 0
Best supporting 0 1 1
Best picture 0 1 1
.
When the user adds or deletes concepts, we ex-
pand rows and columns in A and B by filling 0 for
non-diagonal entries and 1 for diagonal entries. The
expanded before and after matrices A? and B? are
used in the calculation.
For taxonomies with concept changes, we define
manual guidance with concept set change Mchange
as a submatrix which consists of some entries of the
after matrix B; at these entries, there exist differ-
ences from the expanded before matrix A? to the ex-
panded after matrix B?. Note that the concepts cor-
responding to these entries should exist in the unex-
panded set of concepts. Formally, manual guidance
with concept set change
Mchange = B[r
?; c?]
r? = {i : b?ij ? a
?
ij 6= 0, concept ci ? CB}
c? = {j : b?ij ? a
?
ij 6= 0, concept cj ? CB}
(5)
where a?ij is the (i, j)
th entry in A?, b?ij is the (i, j)
th
entry in B?, CB is the set of concepts in the unex-
panded after matrix B, r indicates the rows and c
indicates the columns.
Based on manual guidance M , we can create
training data for the supervised distance learning
algorithm (Section 3.1). In particular, we trans-
form the manual guidance into a distance matrix
D = 1 ? M , which is used as the training data.
The learning algorithm is then able to learn a good
model which best preserves the regularity defined by
the task and the user. The difference is that the train-
ing data here is derived from manual guidance while
in the automatic algorithm we use training data from
WordNet and ODP.
4.2 Update the Taxonomy
According to the algorithm shown in Figure 2, after
learning W (i), the weight matrix at the ith iteration,
from the manual guidance, we can use it to predict
the pair-wise semantic distances for the unmodified
concepts and further group them in the taxonomy.
When the pair-wise distance score for a concept
pair (cl, cm) is small (<0.5), we consider the rela-
tion between the concept pair is true; when it is big
(?0.5), false. How to organize concepts whose re-
lations are true, is decided again by the relation type
in the distance matrix. If r is ?sibling?, cl and cm are
put under the same parent. If r is ?is-a?, cm is put
under cl as one of cl?s children. The user interface
then presents the updated taxonomy to the user and
waits for the next round of manual guidance.
Since only a few changes are made during each
human-computer interaction, the learning model
may suffer from overfitting and the taxonomic struc-
ture may change too rapidly. To avoid such is-
sues caused by too few manual guidance, we em-
ploy background training taxonomy fragments from
WordNet and ODP, to smooth the learning models
and achieve less variance.
1283
5 Evaluation
We conduct experiments and a user study to evalu-
ate the effectiveness of our approach. We have two
goals for the evaluation. One is to evaluate how the
browsing taxonomies constructed by our approach
compare with those constructed by other baseline
systems. Another is to investigate how well our
system can learns from task-specifications based on
user supervision.
5.1 Datasets
The datasets we used in the evaluation are collec-
tions of Web documents crawled for complex search
tasks. For each task, we created the dataset by sub-
mitting 4 to 5 queries to and collecting the returned
Web documents from two search engines bing.com
and google.com. For example, queries ?trip to DC?,
?Washington DC?, ?DC?, and ?Washington? were
submitted for the task ?planning a trip to DC?. In to-
tal, we created 50 Web datasets on the topics such as
find a good kindergarten, purchase a used car, plan
a trip to DC, how to make a cake, find a good wed-
ding videographer, write a survey paper for health
care systems, find the best deals for a Mother?s day
gift, write a survey paper for social network, write
a survey paper for EU?s finance, and write a survey
paper for information technology.
Around 1000 Web documents are collected for
each dataset. We filter out spams and advertisements
and then search for more relevant Web documents
to make the total number 1000. However, not all
topics can retrieve 1000 documents. Among all 50
datasets, the average number of documents is 988.5.
The average number of unique words in a dataset is
698,875.
5.2 Comparing with Baseline Systems
We compare the following 5 systems.
? Subsumption: the automatic algorithm pro-
posed by (Sanderson and Croft, 1999), the
most effective state-of-the-art browsing hier-
archy construction technique as reported by
(Lawrie et al 2001).
? KH: the automatic taxonomy construction al-
gorithm proposed by (Kozareva and Hovy,
2010).
? ME: the automatic taxonomy construction al-
gorithm proposed by (Yang and Callan, 2009).
This framework does not perform path consis-
tency control nor learning from users.
? DistOpt: our automatic taxonomy construction
algorithm with path consistency control.
? PDistOpt: our interactive approach with human
supervision. The process starts from a flat list
of concepts. The user built the browsing taxon-
omy from the list in a user study (Section 5.5).
5.3 Browsing Effectiveness
A popular measure to evaluate the quality of the
browsing taxonomies is the expected mutual infor-
mation measure (EMIM (Lawrie et al 2001)). It
calculates the mutual information between the lan-
guage model in a taxonomy T and the language
model in a document collection Z. It is defined as:
I(C;V ) =
?
c?C,v?V
P (c, v)log
P (c, v)
P (c)P (v)
,
where P (c, v) =
?
d?Z P (d)P (c|d)P (v|d), C is
the set of concepts in T , V is the set of non-
stopwords in Z, and d is a document in Z. EMIM
only evaluates the content of a browsing taxonomy,
not its structure. However, it is still popularly used
to indicate how representative a browsing taxonomy
is for a document collection.
Table 1 shows the EMIM of the browsing tax-
onomies constructed by the five systems under eval-
uation. Based on the mean EMIM over the 50
datasets, we can rank the systems in terms of EMIM
in the descending order as PDistOpt >> DistOpt
>> ME > KH >> Subsumption.2 It shows that
DistOpt is the best performing automatic algorithm
to generate browsing taxonomies. DistOpt is 109%
and statistically significantly more effective than
ME (p-value<.001, t-test), 159% and statistically
significantly more effective than KH (p-value<.001,
t-test), and 17 times and statistically significantly
more effective than Subsumption (p-value<.001, t-
test). It strongly suggests that our techniques are
2>> indicates statistically significant difference between
the left and the right hand sides (p < .001, t-test) and > indi-
cates moderate statistical significance between the left and the
right hand sides (p < .05, t-test). We will use the same symbols
throughout the remainder of this paper.
1284
Table 1: Expected Mutual Information (in 1000*EMIM).
Example dataset Subs. KH ME DistOpt PDistOpt
kindergarten 0.4 3.8 3.9 5.6 7.3
health care 0.5 2.8 3.1 7.8 8.3
used car 0.1 0.2 0.1 2.8 3.6
trip to DC 0.2 4.3 4.5 6.4 6.8
finance 0.01 0.01 0.1 0.6 0.6
gift 0.2 1.2 1.2 3.8 4.7
social network 0.1 1.5 1.3 2.4 3.2
information 0.3 1.9 2.3 3.5 4.9
cake 0.2 1.2 3.1 6.6 6.8
videographer 0.4 1.8 1.6 4.9 5.6
Mean of 50 sets 0.24 1.7 2.1 4.4 5.2
more effective than the state-of-the-art systems in
constructing browsing taxonomies.
Moreover, Table 1 shows that the PDistOpt tax-
onomies is 18% more effective than the DistOpt tax-
onomies in terms of EMIM. The result is also sta-
tistically significant (p-value<.01, t-test). It indi-
cates that incorporating user preferences in brows-
ing taxonomy construction is able to produce even
more effective browsing taxonomies than all auto-
mated methods.
Another popular evaluation measure3 for brows-
ing effectiveness is reach time (Carpineto et al
2009). It is defined as:
treach =
1
|R|
?
di?R
L(ci) + pi,
where R is the relevant documents, ci is the concept
that connects to a relevant document di, L(ci) is the
path length from the root to reach ci, and pi is the
position that di appears in the document cluster as-
sociated with ci. Reach time evaluates both the con-
tent and the structure of a browsing taxonomy. This
measure needs relevance judgements about a query
for the documents organized by the taxonomies. We
obtained the relevance judgements by using the ma-
jority votes from a user study involving 29 subjects
followed by expert reviews. Three experts manu-
ally examined the majority votes and reached agree-
ments on all relevance judgements.
Table 2 elaborates reach time for the systems.
Based on the mean reach time over 50 datasets,
we obtain a similar ranking of the systems as sug-
gested by EMIM. The ranking based on reach time
3Other proposed measures include coverage and compact-
ness (Kummamuru et al 2004).
Table 2: Reach time.
Example dataset Subs. KH ME DistOpt PDistOpt
kindergarten 14.4 9.8 9.9 8.7 4.3
health care 12.3 9.8 6.8 4.5 3.3
used car 15.4 12.4 10.2 8.7 7.6
trip to DC 11.2 10.3 9,8 8.7 5.8
finance 24.5 18.3 19.7 18.7 15.6
gift 11.2 8.4 7.7 5.6 5.4
social network 14.3 9.8 7.8 7.6 6.8
information 10.6 9.5 8.8 8.9 6.7
cake 8.9 4.8 4.5 3.4 3.2
videographer 9.5 8.8 7.6 6.9 4.5
Mean of 50 sets 14.2 12.2 9.8 7.2 5.2
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
Pat
h	 ?Er
ror	 ?
Rat
e	 ?
Subsump?n	 ? ME	 ? KH	 ? DistOpt	 ?
Figure 4: Path error rate.
is: PDistOpt >> DistOpt >> ME > KH >>
Subsumption. It shows that the best performing
automatic system is DistOpt, which on average can
produce taxonomies to reach a relevant document by
visiting only 7.2 nodes, including 5.2 non-leaf con-
cepts and 2 documents in the leaf cluster on average.
To find all relevant documents in a collection sized
around 1000, this reach time is very fast. The in-
teractive PDistOpt unsurprisingly gives even better
reach time, 5.2 nodes on average.
5.4 Path Consistency
To evaluate how well path consistency is handled,
we compare the path error rate generated by our ap-
proach and by other baseline systems. This evalua-
tion is only applied to automatic algorithms.
The path error is defined as the average number of
wrong ancestor-descendant pairs in a taxonomy. It is
only applied for concepts are not immediately con-
nected. It can be judged and calculated as follows.
Three human assessors manually evaluated the path
errors by (1) gathering the paths by performing a
depth-first traverse in the taxonomy from the root
concept; (2) along each path, counting the number
of wrong ancestor-descendant pairs; (3) summing up
1285
Subsumption KH ME DistOpt PDistOpt
Per
ceiv
ed B
row
sing
 Eff
ecti
ven
ess
1
2
3
4
5
Figure 5: Perceived browsing effectiveness.
the errors that all assessors agree on and normalizing
the sum by the taxonomy size.
Figure 4 shows the path error rate generated by
all the automated algorithms under evaluation. We
can see that DistOpt produces the least path error.
The algorithms can be ranked in terms of the ability
to handle path consistency as DistOpt >> ME >>
KH >> Subsumption. DistOpt statistically signif-
icantly reduces path errors from not using the path
consistency control (ME) by 500% (p-value<.001,
t-test). It strongly indicates that our technique is ef-
fective to maintain path consistency. We conclude
that DistOpt best handles path consistency among
all the system under evaluation.
5.5 User Study
Besides objective evaluations, we conducted an user
study consisting of two parts: qualitative compari-
son of the systems under evaluation, and using our
taxonomy construction user interface to interactively
construct personalized browsing taxonomies.
Twenty-nine (Thirty subjects initially, one was ex-
cluded because of incomplete data entry) graduate
and undergraduate students from various majors in
two universities participated in the study. They were
all familiar with use of computers and highly profi-
cient in English. Each user study lasted for 4 hours.
In the first half of the user study, the participants
were first introduced to the taxonomy construction
user interface for about 10 minutes to get famil-
iar with its functions. After that, the participants
performed an exercise task which lasted about 5
minutes and then started the real tasks. For each
dataset, the participants were asked to interactively
work with PDistOpt to build browsing taxonomies.
Once the real tasks were done, the participants spend
5 minutes to answer a questionnaire regarding their
experience and opinions.
In the second half of the user study, we asked the
participants to use and compare the provided brows-
ing taxonomies with the following task in mind.
Imagine your have a task [task name].
You use a browsing taxonomy designed
for the collection of Web documents about
this task. Use the browsing taxonomy to
find all useful topics for your task. Iden-
tify at least one document for each topic.
For each dataset, we asked the participants to rate
the browsing taxonomies built by the systems un-
der evaluation by answering the following question
about perceived browsing effectiveness - ?How well
did the browsing taxonomy help you to complete the
task??. Ratings in the 5-point Likert-type scale,
ranging from ?very good?(5), ?good?(4), ?fair?(3),
?bad?(2), to ?trash?(1), are used to rate browsing ef-
fectiveness perceived by the participants.
5.5.1 Perceived Browsing Effectiveness
Figure 5 shows the mean and 95% confidence in-
terval for the perceived browsing effectiveness for
browsing taxonomies constructed by the systems
under evaluation. These perceived browsing ef-
fectiveness can be ranked in descending order as
PDistOpt >> DistOpt > ME >> KH > Subsump-
tion. PDistOpt shows the highest mean perceived
browsing effectiveness, which is as high as 4.4. Such
high rating shows that browsing taxonomy with per-
sonalization could well satisfy users? information
needs and are perceived as very effective in brows-
ing by the users.
5.5.2 Accuracy of System Predictions
When a user provided manual guidance to the in-
teractive system, during each human-computer in-
teraction cycle, the system made predictions based
on the user?s edits. He or she could directly judge
the correctness of these machine-predicted modi-
fications on-the-fly by selecting an option ?yes?
or ?no? from the ?Accept the change?? menu.
Note that these were personalized tasks and the
predictions were evaluated by the user according
1286
Table 3: Accuracy of system predictions.
Max Min Avg
accuracy of system predictions 0.98 0.92 0.94
Table 4: Perceived learning ability.
Max Min Avg
perceived learning ability 4.2 2.8 3.61
which dataset health care finance -
to his/her own standard. We calculate the ac-
curacy of system predictions as: Accuracy =
1
I
?I
i=1
number of accepted predictions in ith cycle
number of predictions in ith cycle , where I
is the total number of human-computer interaction
cycles when constructing a browsing taxonomy. A
high accuracy indicates that the system learns well
from user edits. This evaluation is only applied to
PDistOpt.
Table 3 shows that for all datasets, the mean ac-
curacy of the system predictions is above 0.92. The
average value is 0.94. This high accuracy clearly
demonstrates that the system successfully learns
from a user and makes highly accurate predictions
on how the user would organize the concepts.
5.5.3 Perceived Learning Ability
After completing constructing a browsing taxon-
omy, a participant was asked immediately to rate
how well the system learned from his/her edits. The
question was ?How well did the system appear to
learn your method of organizing the concepts??. We
also used the 5-point Likert-type scale to rate this
perceived system learning ability. This evaluation is
only applied to PDistOpt.
Table 4 shows the max, min, and average re-
sponses of perceived system learning ability. The
mean perceived learning ability is 3.61, with a stan-
dard derivation of 0.45. It suggests that the learning
ability of the system was only perceived as moder-
ately good. This result contradicts with the conclu-
sion that we drew based on the more objective mea-
sure, accuracy of system prediction (Section 5.5.2).
We further investigate why the participants were
only moderately satisfied with the system?s learn-
ing ability. From the after session questionnaire, we
found that participants thought that some datasets
such as ?finance? were more difficult than other
datasets such as ?health care?. For example, the
dataset ?finance? was considered by all participants
as ?very difficult? while ?health care? was consid-
ered as ?very easy?. The participants also com-
plained that they were not familiar with the difficult
datasets. It is interesting that when a dataset is less
familiar for the users, the system was perceived per-
forming badly too. It may suggest that when peo-
ple are not familiar with the tasks, they provide less
promising edits, the system learns from the lower
quality training data, and in the end the users per-
ceive the output as poor system learning ability.
6 Conclusion
Document collection browsing is another common
use of taxonomies. Given an arbitrary collection, a
taxonomy must suit the specific domain in order to
support browsing. This paper explores techniques
to quickly derive task-specific taxonomies support-
ing browsing in arbitrary document sets. In par-
ticular, we uniquely employ pair-wise semantic dis-
tance as an entry point to incrementally build brows-
ing taxonomies. The supervised distance learning
algorithm not only allows us to incorporate multi-
ple semantic features to evaluate the proximity be-
tween concepts, but also allows us to learn the met-
ric function from personal preferences. Users can
thus manually modify the taxonomies and to some
extent teach the algorithm to predict his/her way to
organize the concepts. Moreover, by minimizing the
overall semantic distances among concepts and re-
stricting minimal semantic distances along a path,
we find the best hierarchical structure as the brows-
ing hierarchy. It guarantees that semantically close
concepts are put together so that users will have a
good idea about why the concepts are put together.
This greatly increases the interpretability of a con-
structed browsing hierarchy than the existing ap-
proaches. This makes our approach more flexible
and more general to effectively creating browsing
taxonomies to support more complicated and more
realistic tasks such as Web information triage.
Acknowledgments
The author sincerely thanks Prof. Jamie Callan for
in-depth discussions about the research and anony-
mous reviewers for valuable comments to this paper.
1287
References
J. R. Bellegarda, J. W. Butzberger, Yen-Lu Chow, N. B.
Coccaro, and D. Naik. 1996. A novel word clustering
algorithm based on latent semantic analysis. In Pro-
ceedings of the Acoustics, Speech, and Signal Process-
ing, 1996. on Conference Proceedings., 1996 IEEE
International Conference - Volume 01, ICASSP ?96,
pages 172?175, Washington, DC, USA. IEEE Com-
puter Society.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 27th
Annual Meeting for the Association for Computational
Linguistics (ACL 1999).
Rajendra Bhatia. 2006. Positive definite matrices
(princeton series in applied mathematics). Princeton
University Press, December.
Claudio Carpineto, Stefano Mizzaro, Giovanni Romano,
and Matteo Snidero. 2009. Mobile information re-
trieval with search results clustering: Prototypes and
evaluations. Journal of American Society for Informa-
tion Science and Technology (JASIST), pages 877?895.
Duen Horng Chau, Aniket Kittur, Jason I. Hong, and
Christos Faloutsos. 2011. Apolo: making sense of
large network data by combining rich user interaction
and machine learning. In CHI, pages 167?176.
Gouglass R. Cutting, David R. Karger, Jan R. Petersen,
and John W. Tukey. 1992. Scatter/Gather: A cluster-
based approach to browsing large document collec-
tions. In Proceedings of the fifteenth Annual ACM
Conference on Research and Development in Informa-
tion Retrieval (SIGIR 1992).
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. In Artificial Intelligence, 165(1):91-
134, June.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the Human Language Technology Conference/Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2003).
Sanda M. Harabagiu, Steve J. Maiorano, and Marius A.
Pasca. 2003. Open-domain textual question answer-
ing techniques. In Natural Language Engineering 9
(3): 1-38.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING 1992).
E. H. Hovy. 2002. Comparing Sets of Semantic Re-
lations in Ontologies. In R. Green, C. A. Bean,
and Myaeng S. H. (eds), editors, The Semantics of
Relationships: An Interdisciplinary Perspective. Dor-
drecht: Kluwer.
Zornitsa Kozareva and Eduard Hovy. 2010. A semi-
supervised method to learn and construct taxonomies
using the web. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1110?1118, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of the 46th An-
nual Meeting for the Association for Computational
Linguistics (ACL 2008).
Krishna Kummamuru, Rohit Lotlikar, Shourya Roy,
Karan Singal, and Raghu Krishnapuram. 2004. A hi-
erarchical monothetic document clustering algorithm
for summarization and browsing search results. Pro-
ceedings of the 13th conference on World Wide Web
WWW 04, page 658.
Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg.
2001. Finding topic words for hierarchical summa-
rization. In Proceedings of the 24th Annual ACM Con-
ference on Research and Development in Information
Retrieval (SIGIR 2001), pages 349?357.
LCSH. 2011. Library of congress subject headings.
http://www.loc.gov/.
P. C. Mahalanobis. 1936. On the generalised distance in
statistics. In Proceedings of the National Institute of
Sciences of India 2 (1): 495.
ODP. 2011. Open directory project. http://www.
dmoz.org/.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 44th An-
nual Meeting for the Association for Computational
Linguistics (ACL 2006).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting for the As-
sociation for Computational Linguistics (ACL 2002).
Mark Sanderson and W. Bruce Croft. 1999. Deriving
concept hierarchies from text. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR 1999).
Sedumi. 2011. http://sedumi.mcmaster.ca.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
1288
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics (ACL/COLING 2006).
Emilia Stoica and Marti A. Hearst. 2007. Automating
Creation of Hierarchical Faceted Metadata Structures.
In Proceedings of the Human Language Technology
Conference (NAACL-HLT).
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura
Coppola. 2004. Scaling web-based acquisition of en-
tailment relations. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004).
Yalmip. 2011. http://users.isy.liu.se/
johanl/yalmip.
Hui Yang and Jamie Callan. 2008. Ontology generation
for large email collections. In Proceedings of the 8th
National Conference on Digital Government Research
(Dg.O 2008).
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In Pro-
ceedings of the 47th Annual Meeting for the Associa-
tion for Computational Linguistics (ACL 2009).
Liu Yang. 2006. Distance metric learning: A com-
prehensive survey. http://www.cs.cmu.edu/
?liuy/frame_survey_v2.pdf.
Ka-Ping Yee, Kirsten Swearingen, Kevin Li, and Marti
Hearst. 2003. Faceted metadata for image search and
browsing. In Human factors in computing systems.
ACM.
1289
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 97?105,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A Generalised Hybrid Architecture for NLP
Alistair Willis
Department of Computing
The Open University,
Milton Keynes, UK
a.g.willis@open.ac.uk
Hui Yang
Department of Computing
The Open University,
Milton Keynes, UK
h.yang@open.ac.uk
Anne De Roeck
Department of Computing
The Open University,
Milton Keynes, UK
a.deroeck@open.ac.uk
Abstract
Many tasks in natural language process-
ing require that sentences be classified from
a set of discrete interpretations. In these
cases, there appear to be great benefits in
using hybrid systems which apply multiple
analyses to the test cases. In this paper, we
examine a general principle for building hy-
brid systems, based on combining the re-
sults of several, high precision heuristics.
By generalising the results of systems for
sentiment analysis and ambiguity recogni-
tion, we argue that if correctly combined,
multiple techniques classify better than sin-
gle techniques. More importantly, the com-
bined techniques can be used in tasks where
no single classification is appropriate.
1 Introduction
The success of hybrid NLP systems has demon-
strated that complex linguistic phenomena and
tasks can be successfully addressed using a com-
bination of techniques. At the same time, it is
clear from the NLP literature, that the perfor-
mance of any specific technique is highly depen-
dent on the characteristics of the data. Thus, a
specific technique which performs well on one
dataset might perform very differently on another,
even on similar tasks, and even if the two datasets
are taken from the same domain. Also, it is possi-
ble that the properties affecting the effectiveness
of a particular technique may vary within a single
document (De Roeck, 2007).
As a result of this, for many important NLP
applications there is no single technique which
is clearly to be preferred. For example, recent
approaches to the task of anaphora resolution
include syntactic analyses (Haghighi and Klein,
2009), Maximum Entropy models (Charniak and
Elsner, 2009) and Support Vector Machines (Yang
et al, 2006; Versley et al, 2008). The perfor-
mance of each of these techniques varies depend-
ing upon the particular choice of training and test
data.
This state of affairs provides a particular op-
portunity for hybrid system development. The
overall performance of an NLP system depends
on complex interactions between the various phe-
nomena exhibited by the text under analysis, and
the success of a given technique can be sensitive
to the different properties of that text. In partic-
ular, the text?s or document?s properties are not
generally known until the document comes to be
analysed. Therefore, there is a need for systems
which are able to adapt to different text styles at
the point of analysis, and select the most appropri-
ate combination of techniques for the individual
cases. This should lead to hybridising techniques
which are robust or adaptive in the face of varying
textual styles and properties.
We present a generalisation of two hybridi-
sation techniques first described in Yang et al
(2012) and Chantree et al (2006). Each uses
hybrid techniques in a detection task: the first is
emotion detection from suicide notes, the second
is detecting nocuous ambiguity in requirements
documents. The distinguishing characteristic of
both tasks is that a successful solution needs to
accommodate uncertainty in the outcome. The
generalised methodology described here is partic-
ularly suited to such tasks, where as well as se-
lecting between possible solutions, there is a need
to identify a class of instances where no single so-
lution is most appropriate.
97
2 Hybridisation as a Solution to
Classification Tasks
The methodology described in this paper pro-
poses hybrid systems as a solution to NLP tasks
which attempt to determine an appropriate inter-
pretation from a set of discrete alternatives, in par-
ticular where no one outcome is clearly prefer-
able. One such task is nocuous ambiguity detec-
tion. For example, in sentence (1), the pronoun he
could refer to Bill, John or to John?s father.
(1) When Bill met John?s father, he was pleased.
Here, there are three possible antecedents for he,
and it does not follow that all human readers
would agree on a common interpretation of the
anaphor. For example, readers might divide be-
tween interpreting he as Bill or as John?s father.
Or perhaps a majority of readers feel that the
sentence is sufficiently ambiguous that they can-
not decide on the intended interpretation. These
are cases of nocuous ambiguity (Chantree et al,
2006), where a group of readers do not interpret a
piece of text in the same way, and may be unaware
that the misunderstanding has even arisen.
Similarly, as a classification task, sentiment
analysis for sentences or fragments may need
to accommodate instances where multiple senti-
ments can be identified, or possibly none at all.
Example (2) contains evidence of both guilt and
love:
(2) Darling wife, ? I?m sorry for everything.
Hybrid solutions are particularly suited to such
tasks, in contrast to approaches which use a single
technique to select between possible alternatives.
The hybrid methodology proposed in this paper
approaches such tasks in two stages:
1. Define and apply a set of heuristics, where
each heuristic captures an aspect of the phe-
nomenon and estimates the likelihood of a
particular interpretation.
2. Apply a combination function to either com-
bine or select between the values contributed
by the individual heuristics to obtain better
overall system performance.
The model makes certain assumptions about
the design of heuristics. They can draw on a mul-
titude of techniques such as a set of selection fea-
tures based on domain knowledge, linguistic anal-
ysis and statistical models. Each heuristic is a
partial descriptor of an aspect of a particular phe-
nomenon and is intended as an ?expert?, whose
opinion competes against the opinion offered by
other heuristics. Heuristics may or may not be in-
dependent. The crucial aspect is that each of the
heuristics should seek to maximise precision or
complement the performance of another heuristic.
The purpose of step 2 is to maximise the contri-
bution of each heuristic for optimal performance
of the overall system. Experimental results anal-
ysed below show that selecting an appropriate
mode of combination helps accommodate dif-
ferences between datasets and can introduce ad-
ditional robustness to the overall system. The
experimental results also show that appropriate
combination of the contribution of high precision
heuristics significantly increases recall.
For the tasks under investigation here, it proves
possible to select combination functions that al-
low the system to identify behaviour beyond clas-
sifying the subject text into a single category. Be-
cause the individual heuristics are partial descrip-
tions of the whole language model of the text, it
is possible to reason about the interaction of these
partial descriptions, and identify cases where ei-
ther none, or many, of the potential interpretations
of the text are possible. The systems use either a
machine learning technique or a voting strategies
to combine the individual heuristics.
In sections 3 and 4, we explore how the pre-
viously proposed solutions can be classed as in-
stances of the proposed hybridisation model.
3 Case study: Sentiment Analysis
Following Pang et al (2002) and the release of the
polarity 2.0 dataset, it is common for sentiment
analysis tasks to attempt to classify text segments
as either of positive or negative sentiment. The
task has been extended to allow sentences to be
annotated as displaying both positive and negative
sentiment (Wilson et al, 2009) or indicating the
degree of intensity (Thelwall et al, 2010).
The data set used for the 2011 i2b2 shared chal-
lenge (Pestian et al, 2012) differs from this model
by containing a total of 15 different sentiments to
classify the sentences. Each text fragment was
labelled with zero, one or more of the 15 senti-
ments. For example, sentence (2) was annotated
with both Love and Guilt. The fragments varied
between phrases and full sentences, and the task
aims to identify all the sentiments displayed by
98
each text fragment.
In fact, several of the proposed sentiments were
identified using keyword recognition alone, so the
hybrid framework was applied only to recognise
the sentiments Thankfulness, Love, Guilt, Hope-
lessness, Information and Instruction; instances
of the other sentiments were too sparse to be reli-
ably classified with the hybrid system. A keyword
cue list of 984 terms was manually constructed
from the training data based on their frequency in
the annotated set; no other public emotion lexicon
was used. This cue list was used both to recognise
the sparse sentiments, and as input to the CRF.
3.1 Architecture
An overview of the architecture is shown in figure
1. Heuristics are used which operate at the word
level (Conditional Random Fields), and at the
sentence level (Support Vector Machine, Naive
Bayes and Maximum Entropy). These are com-
bined using a voting strategy that selects the most
appropriate combination of methods in each case.
Input
text
?
Preprocess
text
?
Negation
detection
? ?
Combine
values
?
Token level Sentence level
classifier classifiers
CRF SVM
NB
ME
Figure 1: Architecture for sentiment classification task
The text is preprocessed using the tokeniser,
POS tagger and chunker from the Genia tagger,
and parsed using the Stanford dependency parser.
This information, along with a negation recog-
niser, is used to generate training vectors for the
heuristics. Negation is known to have a major ef-
fect on sentiment interpretation (Jia et al, 2009).
3.2 Sentiment recognition heuristics
The system uses a total of four classifiers for each
of the emotions to be recognised. The only token-
level classification was carried out using CRFs
(Lafferty et al, 2001) which have been success-
fully used on Named Entity Recognition tasks.
However, both token- and phrase-level recogni-
tion are necessary to capture cases where sen-
tences convey more than one sentiment. The
CRF-based classifiers were trained to recognise
each of the main emotions based on the main key-
word cues and the surrounding context. The CRF
is trained on the set of features shown in figure 2,
and implemented using CRF++1.
Feature Description
Words word, lemma, POS tag, phrase
chunk tag
Context 2 previous words and 2 following
words with lemma, POS tags and
chunk tags
Syntax Dependency relation label and
the lemma of the governer word
in focus
Semantics Is it negated?
Figure 2: Features used for CRF classifier
Three sentence-level classifiers were trained
for each emotion, those being Naive Bayes and
Maximum Entropy learners implemented by the
MALLET toolkit2, and a Support Vector Machine
model implemented using SVM light3 with the
linear kernel. In each case, the learners were
trained using a feature vector using the two fea-
ture vectors as shown in figure 3.
Feature vector Description
Words word lemmas
Semantics negation terms identified by
the negative term lexicon,
and cue terms from the emo-
tion term lexicon
Figure 3: Features used for sentence-level classifiers
A classifier was built for each of the main emo-
tions under study. For each of the six emotions,
four learners were trained to identify whether the
text contains an instance of that emotion. That is,
an instance of text receives 6 groups of results,
and each group contains 4 results obtained from
different classifiers estimating whether one par-
ticular emotion occurs. The combination func-
tion predicts the final sentiment(s) exhibited by
the sentence.
1http://crfpp.sourceforge.net/
2http://mallet.cs.umass.edu/
3http://svmlight.joachims.org/
99
3.3 Combination function
To combine the outputs of the heuristics, Yang et
al. (2012) use a voting model. Three different
combination methods are investigated:
Any If a sentence is identified as an emotion in-
stance by any one of the ML-based models, it
is considered a true instance of that emotion.
Majority If a sentence is identified as an emotion
instance by two or more of the ML-based
models, it is considered a true instance of
that emotion.
Combined If a sentence is identified as an emo-
tion instance by two or more of the ML-
based models or it is identified as an emo-
tion instance by the ML-based model with
the best precision for that emotion, it is con-
sidered a true instance of that emotion.
This combined measure reflects the intuition
that where an individual heuristic is reliable for a
particular phenomenon, then that heuristic?s vote
should be awarded a greater weight. The preci-
sion scores of the individual heuristics is shown
in table 1, where the heuristic with the best preci-
sion for that emotion is highlighted.
Emotion CRF NB ME SVM
Thankfulness 60.6 58.8 57.6 52.6
Love 76.2 68.5 77.6 76.9
Guilt 58.1 46.8 35.3 58.3
Hopelessness 73.5 63.3 68.7 74.5
Information 53.1 41.0 48.1 76.2
Instruction 76.3 63.6 70.9 75.9
Table 1: Precision scores (%) for individual heuristics
3.4 Results
Table 2 reports the system performance on 6 emo-
tions by both individual and combined heuristics.
In each case, the best performer among the four
individual heuristics is highlighted. As can be
seen from the table, the Any combinator and the
Combined combinators both outperform each of
the individual classifiers. This supports the hy-
pothesis that hybrid systems work better overall.
3.5 Additional comments
The overall performance improvement obtained
by combining the individual measures raises the
question of how the individual elements interact.
Table 3 shows the performance of the combined
systems on the different emotion classes. For
each emotion, the highest precision, recall and f-
measure is highlighted.
As we would have expected, the Any strategy
has the highest recall in all cases, while the Major-
ity strategy, with the highest bar for acceptance,
has the highest precision for most cases. The
Any and Combined measures appear to be broadly
comparable: for the measures we have used, it ap-
pears that the precision of the individual classi-
fiers is sufficiently high that the combination pro-
cess of improving recall does not impact exces-
sively on the overall precision.
A further point of interest is that table 2 demon-
strates that the Naive Bayes classifier often re-
turns the highest f-score of the individual classi-
fiers, even though it never has the best precision
(table 1). This supports our thesis that a success-
ful hybrid system can be built from multiple clas-
sifiers with high precision, rather than focussing
on single classifiers which have the best individ-
ual performance (the Combined strategy favours
the highest precision heuristic).
4 Nocuous ambiguity detection
It is a cornerstone of NLP that all text contains
a high number of potentially ambiguous words or
constructs. Only some of those will lead to misun-
derstandings, where two (or more) participants in
a text-mediated interchange will interpret the text
in different, and incompatible ways, without real-
ising that this is the case. This is defined as nocu-
ous ambiguity (Willis et al, 2008), in contrast to
innocuous ambiguity, where the text is interpreted
in the same way by different readers, even if that
text supports different possible analyses.
The phenomenon of nocuous ambiguity is par-
ticularly problematic in high stake situations. For
example, in software engineering, a failure to
share a common interpretation of requirements
stated in natural language may lead to incorrect
system implementation and the attendant risk of
system failure, or higher maintenance costs. The
systems described by Chantree et al (2006) and
Yang et al (2010a) aim not to resolve ambigu-
100
Individual heuristics Hybrid models
Emotion CRF NB ME SVM Any Majority Combined
Thankfulness 59.5 59.6 61.9 60.3 63.9 63.0 64.2
Love 63.7 69.3 66.5 61.5 72.0 70.3 71.0
Guilt 35.3 40.5 27.7 37.8 46.3 29.9 45.8
Hopelessness 63.2 64.1 59.9 57.0 67.3 65.4 67.3
Information 42.3 47.7 43.7 43.4 50.2 45.5 47.8
Instruction 65.7 65.7 63.4 58.8 72.1 65.4 72.0
Table 2: F-scores (%) for individual and combined heuristics (sentiment analysis)
Any Majority Combined
P R F P R F P R F
Thankfulness 52.6 81.6 63.9 60.6 65.7 63.0 55.0 77.1 64.2
Love 68.7 75.6 72.0 77.9 64.0 70.3 74.6 67.7 71.0
Guilt 46.6 46.2 46.3 50.0 21.4 29.9 50.5 41.9 45.8
Hopelessness 64.1 70.8 67.3 80.3 55.2 65.4 66.3 68.4 67.3
Information 40.9 64.9 50.2 49.9 41.8 45.5 45.2 50.7 47.8
Instruction 68.5 76.1 72.1 80.8 54.9 65.4 70.3 73.7 72.0
Table 3: Precision, recall and F-scores (%) for the combined systems (sentiment analysis)
ous text in requirements, but to identify where in-
stances of text might display nocuous ambiguity.
These systems demonstrate how, for hybrid
systems, the correct choice of combination func-
tion is crucial to how the individual heuristics
work together to optimise overall system perfor-
mance.
4.1 Nocuous Ambiguity: Coordination
Chantree et al (2006) focus on coordination at-
tachment ambiguity, which occurs when a mod-
ifier can attach to one or more conjuncts of a
coordinated phrase. For example, in sentence
(3), readers may divide over whether the modi-
fier short attaches to both books and papers (wide
scope), or only to books (narrow scope).
(3) I read some short books and papers.
In each case, the coordination involves a near
conjunct, (books in (3)), a far conjunct, (papers)
and a modifier (short). The modifier might also
be a PP, or an adverb in the case where a VP con-
tains the conjunction. In disambiguation, the task
would be to identify the correct scope of the mod-
ifier (i.e. which of two possible bracketings is the
correct one). For nocuous ambiguity detection,
the task is to identify to what extent people inter-
pret the text in the same way, and to flag the in-
stance as nocuous if they diverge relative to some
threshold.
4.1.1 The dataset
17 human judgements were collected for each
of 138 instances of sentences exhibiting coor-
dination ambiguity drawn from a collection of
software requirements documents. The majority
of cases (118 instances) were noun compounds,
with some adjective and some preposition modi-
fiers (36 and 18 instances respectively). Partici-
pants were asked to choose between wide scope
or narrow scope modifier attachment, or to indi-
cate that they experienced the example as ambigu-
ous. Each instance is assigned a certainty for wide
and narrow scope modification reflecting the dis-
tribution of judgements. For instance, if 12 judges
favoured wide scope for some instance, 3 judges
favoured narrow scope and 1 judge thought the
instance ambiguous, then the certainty for wide
scope is 71% (12/17), and the certainty for nar-
row scope is 18% (3/17).
A key concept in nocuous ambiguity is that of
an ambiguity threshold, ? . For some ? :
? if at least ? judges agree on the interpretation
101
of the text, then the ambiguity is innocuous,
? otherwise the ambiguity is nocuous.
So for ? = 70%, at least 70% of the judges must
agree on an interpretation. Clearly, the higher ?
is set, the more agreement is required, and the
greater the number of examples which will be
considered nocuous.
4.1.2 Selectional heuristics
A series of heuristics was developed, each cap-
turing information that would lead to a preference
for either wide or narrow scope modifier attach-
ment. Examples from Chantree et al (2006) pro-
pose seven heuristics, including the following:
Co-ordination Matching If the head words
of the two conjuncts are frequently co-
ordinated, this is taken to predict wide
modifier scope.
Distributional Similarity If the head words of
the two conjuncts have high distributional
similarity (Lee, 1999), this is taken to pre-
dict wide modifier scope.
Collocation Frequency If the head word of the
near conjunct has a higher collocation with
the modifier than the far conjunct, this is
taken to predict narrow modifier scope.
Morphology If the conjunct headwords have
similar morphological markers, this is taken
to predict wide modifier scope (Okumura
and Muraki, 1994).
As with the sentiment recognition heuristics
(section 3.2), each predicts one interpretation of
the sentence with high precision, but potentially
low recall. Recall of the system is improved by
combining the heuristics, as described in the next
section. Note that for the first three of these
heuristics, Chantree et al (2006) use the British
National Corpus4, accessed via the Sketch Engine
(Kilgarriff et al, 2004), although a domain spe-
cific corpus could potentially be constructed.
4.1.3 Combining the heuristics
Chantree et al (2006) combine the heuristics
using the logistic regression algorithms contained
in the WEKA machine learning package (Witten
and Frank, 2005). The regression algorithm was
4http://www.natcorp.ox.ac.uk/
trained against the training data so that the text
was interpreted as nocuous either if there was ev-
idence for both wide and narrow modifier scope
or if there was no evidence for either.
This system performed reasonably for mid-
range ambiguity thresholds (around 50% < ? <
80%; for high and low thresholds, naive base-
lines give very high accuracy). However, in sub-
sequent work, Yang et al (2010b) have demon-
strated that by combining the results in a similar
way, but using the LogitBoost algorithm, signifi-
cant improvements can be gained over the logis-
tic regression approach. Their paper suggests that
LogitBoost provides an improvement in accuracy
of up to 21% in the range of interest for ? over
that of logistic regression.
We believe that this improvement reflects that
LogitBoost handles interacting variables better
than logistic regression, which assumes a linear
relationship between individual variables. This
supports our hybridisation method, which as-
sumes that the individual heuristics can interact.
In these cases, the heuristics bring into play dif-
ferent types of information (some structural, some
distributional, some morphological) where each
relies on partial information and favours one par-
ticular outcome over another. It would be unusual
to find strong evidence of both wide and narrow
scope modifier attachment from a single heuristic
and the effect of one heuristic can modulate, or
enhance the effect of another. This is supported by
Chantree et al?s (2006) observation that although
some of the proposed heuristics (such as the mor-
phology heuristic) perform poorly on their own,
their inclusion in the regression model does im-
prove the overall performance of the system
To conclude, comparing the results of Chantree
et al (2006) and Yang et al (2010b) demonstrates
that the technique of combining individual, high
precision heuristics is a successful one. However,
the combination function needs careful consider-
ation, and can have as large an effect on the final
results as the choice of the heuristics themselves.
4.2 Nocuous Ambiguity: Anaphora
As example (1) demonstrates, nocuous ambigu-
ity can occur where there are multiple possible
antecedents for an anaphor. Yang et al (2010a)
have addressed the task of nocuous ambiguity de-
tection for anaphora in requirements documents,
in sentences such as (4), where the pronoun it has
102
three potential antecedents (italicised).
(4) The procedure shall convert the 24 bit image
to an 8 bit image, then display it in a dynamic
window.
As with the coordination task, the aim is to
identify nocuous ambiguity, rather than attempt to
disambiguate the sentence.
4.2.1 The dataset
The data set used for the anaphora task con-
sisted of 200 sentences collected from require-
ments documents which contained a third person
pronoun and multiple possible antecedents. Each
instance was judged by at least 13 people.
The concept of ambiguity threshold, ? , remains
central to nocuous ambiguity for anaphora. The
definition remains the same as in section 4.1.1, so
that an anaphor displays innocuous ambiguity if
there is an antecedent that at least ? judges agree
on, and nocuous ambiguity otherwise. So if, say,
75% of the judges considered an 8 bit image to
be the correct antecedent in (4), then the sentence
would display nocuous ambiguity at ? = 80%,
but innocuous ambiguity at ? = 70%.
For innocuous cases, the potential antecedent
NP with certainty of at least ? is tagged as Y,
and all other NPs are tagged as N. For nocuous
cases, potential antecedents with ? greater than 0
are tagged as Q (questionable), or are tagged N
otherwise (? = 0, ie. unselected).
4.2.2 Selectional Heuristics
The approach to this task uses only one selec-
tion function (Naive Bayes), but uses the output
to support two different voting strategies. Twelve
heuristics (described fully in Yang et al (2010a))
fall broadly into three types which signal the like-
lihood that the NP is a possible antecedent:
linguistic such as whether the potential an-
tecedent is a definite or indefinite NP
contextual such as the potential antecedent?s re-
cency, and
statistical such as collocation frequencies.
To treat a sentence, the classifier is applied to
each of the potential antecedents and assigns a
pair of values: the first is the predicted class of
the antecedent (Y, N or Q), and the second is the
associated probability of that classification.
Given a list of class assignments to potential an-
tecedents with associated probabilities, a weak
positive threshold, WY , and a weak negative
threshold, WN :
if the list of potential antecedents contains:
one Y, no Q, one or more N
or
no Y, one Q, one or more N but no weak
negatives
or
one strong positive Y , any number of Q or N
then
the ambiguity is INNOCUOUS
else
the ambiguity is NOCUOUS
where a classification Y is strong positive if its
associated probability is greater than WY , and a
classification N is weak negative if its associated
probability is smaller than WN .
Figure 4: Combination function for nocuous anaphora
detection with weak thresholds
4.2.3 The combination function
As suggested previously, the choice of com-
bination function can strongly affect the system
performance, even on the same set of selectional
heuristics. Yang et al (2010a) demonstrate two
different combination functions which exploit the
selectional heuristics in different ways. Both
combination functions use a voting strategy.
The first voting strategy states that a sentence
exhibits innocuous ambiguity if either:
? there is a single antecedent labelled Y, and all
others are labelled N, or
? there is a single antecedent labelled Q, and
all others are labelled N.
The second strategy is more sophisticated, and
depends on the use of weak thresholds: intu-
itively, the aim is to classify the text as innocu-
ous if is (exactly) one clearly preferred antecedent
among the alternatives. The combination function
is shown in figure 4. The second clause states
that a single potential antecedent labelled Q can
be enough to suggest innocuous ambiguity if all
the alternatives are N with a high probability.
103
Model without Model with
weak thresholds weak thresholds
? P R F P R F
0.50 27.2 55.0 45.7 24.1 95.0 59.7
0.60 33.9 67.5 56.3 30.9 97.5 68.1
0.70 45.1 76.2 66.9 43.9 98.4 78.8
0.80 58.0 85.0 77.7 56.1 97.9 85.5
0.90 69.1 88.6 83.9 67.4 98.4 90.1
1.0 82.2 95.0 92.1 82.0 99.4 95.3
Table 4: Precision, Recall and f-measure (%) for the
two combination functions (anaphora)
Task Selectional
heuristics
Combination
functions
Sentiment CRF Voting
analysis NB - any
SVM - majority
ME - combined
Nocuous 3 distributional logistic
ambiguity metrics regression
(coordin-
ation) 4 others LogitBoost
Nocuous NB Voting
ambiguity
(anaphora) Voting
(+ threshold)
Table 5: Hybridisation approaches used
The performance of the two voting strategies
is shown in table 4. It is clear that the improved
overall performance of the strategy with weak
thresholds is due to the improved recall when the
functions are combined; the precision is compa-
rable in both cases. Again, this shows the desired
combinatorial behaviour; a combination of high
precision heuristics can yield good overall results.
5 Conclusion
The hybridised systems we have considered are
summarised in table 5. This examination suggests
that hybridisation can be a powerful technique for
classifying linguistic phenomena. However, there
is currently little guidance on principles regarding
hybrid system design. The studies here show that
there is room for more systematic study of the de-
sign principles underlying hybridisation, and for
investigating systematic methodologies.
This small scale study suggests several prin-
ciples. First, the sentiment analysis study has
shown that a set of heuristics and a suitable com-
bination function can outperform the best individ-
ually performing heuristic or technique. In partic-
ular, our results suggest that hybrid systems of the
kind described here are most valuable when there
is significant interaction between the various lin-
guistic phenomena present in the text. This occurs
both with nocuous ambiguity (where competition
between the different interpretations creates dis-
agreement overall), and with sentiment analysis
(where a sentence can convey multiple emotions).
As a result, hybridisation is particularly power-
ful where there are multiple competing factors, or
where it is unclear whether there is sufficient evi-
dence for a particular classification.
Second, successful hybrid systems can be built
using multiple heuristics, even if each of the
heuristics has low recall on its own. Our case
studies show that with the correct choice of hy-
bridisation functions, high precision heuristics
can be combined to give good overall recall while
maintaining acceptable overall precision.
Finally, the mode of combination matters. The
voting system is successful in the sentiment anal-
ysis task, where different outcomes are not exclu-
sive (the presence of guilt does not preclude the
presence of love). On the other hand, the log-
itBoost combinator is appropriate when the dif-
ferent interpretations are exclusive (narrow modi-
fier scope does preclude wide scope). Here, logit-
Boost can be interpreted as conveying the degree
of uncertainty among the alternatives. The coor-
dination ambiguity case demonstrates that the in-
dividual heuristics do not need to be independent,
but if the method of combining them assumes in-
dependence, the benefits of hybridisation will be
lost (logistic regression compared to LogitBoost).
This analysis has highlighted the interplay be-
tween task, heuristics and combinator. Currently,
the nature of this interplay is not well understood,
and we believe that there is scope for investigating
the broader range of hybrid systems that might be
applied to different tasks.
Acknowledgments
The authors would like to thank the UK Engi-
neering and Physical Sciences Research Coun-
cil who funded this work through the MaTREx
project (EP/F068859/1), and the anonymous re-
viewers for helpful comments and suggestions.
104
References
Francis Chantree, Bashar Nuseibeh, Anne De Roeck,
and Alistair Willis. 2006. Identifying nocuous
ambiguities in natural language requirements. In
Proceedings of 14th IEEE International Require-
ments Engineering conference (RE?06), Minneapo-
lis/St Paul, Minnesota, USA, September.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL
?09), pages 148?156.
Anne De Roeck. 2007. The role of data in NLP:
The case for dataset profiling. In Nicolas Nicolov,
Ruslan Mitkov, and Galia Angelova, editors, Re-
cent Advances in Natural Language Processing IV,
volume 292 of Current Issues in Linguistic Theory,
pages 259?266. John Benjamin Publishing Com-
pany, Amsterdam.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1152?1161, Singapore, August.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009.
The effect of negation on sentiment analysis and
retrieval effectiveness. In The 18th ACM Confer-
ence on Information and Knowledge Management
(CIKM?09), Hong Kong, China, November.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The sketch engine. Technical Re-
port ITRI-04-08, University of Brighton.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning (ICML-2001),
pages 282?289.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 25?32, College Park, Maryland, USA, June.
Association for Computational Linguistics.
Akitoshi Okumura and Kazunori Muraki. 1994. Sym-
metric pattern matching analysis for english coor-
dinate structures. In Proceedings of the 4th Con-
ference on Applied Natural Language Processing,
pages 41?46.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86, Philadelphia, July.
John P. Pestian, Pawel Matykiewicz, Michelle Linn-
Gust, Brett South, Ozlem Uzuner, Jan Wiebe,
K. Bretonnel Cohen, John Hurdle, and Christopher
Brew. 2012. Sentiment analysis of suicide notes:
A shared task. Biomedical Informatics Insights,
5(Suppl 1):3?16.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in
short strength detection informal text. Journal of
the American Society for Information Science &
Technology, 61(12):2544?2558, December.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 961?968,
Manchester, August.
Alistair Willis, Francis Chantree, and Anne DeRoeck.
2008. Automatic identification of nocuous ambigu-
ity. Research on Language and Computation, 6(3-
4):355?374, December.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399?433.
Ian H. Witten and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, 2nd edition.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages 41?
48, Sydney, July.
Hui Yang, Anne De Roeck, Vincenzo Gervasi, Al-
istair Willis, and Bashar Nuseibeh. 2010a. Ex-
tending nocuous ambiguity analysis for anaphora
in natural language requirements. In 18th Interna-
tional IEEE Requirements Engineering Conference
(RE?10), Sydney, Australia, Oct.
Hui Yang, Anne De Roeck, Alistair Willis, and Bashar
Nuseibeh. 2010b. A methodology for automatic
identification of nocuous ambiguity. In 23rd Inter-
national Conference on Computational Linguistics
(COLING 2010), Beijing, China.
Hui Yang, Alistair Willis, Anne De Roeck, and Bashar
Nuseibeh. 2012. A hybrid model for automatic
emotion recognition in suicide notes. Biomedical
Informatics Insights, 5(Suppl. 1):17?30, January.
105
Proceedings of the 3rd Workshop on the People?s Web Meets NLP, ACL 2012, pages 20?28,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Resolving Task Specification and Path Inconsistency in Taxonomy
Construction
Hui Yang
Department of Computer Science
Georgetown University
37th and O street NW
Washington, DC, 20057
huiyang@cs.georgetown.edu
Abstract
Taxonomies, such as Library of Congress Subject
Headings and Open Directory Project, are widely
used to support browsing-style information access
in document collections. We call them browsing
taxonomies. Most existing browsing taxonomies
are manually constructed thus they could not eas-
ily adapt to arbitrary document collections. In this
paper, we investigate both automatic and interactive
techniques to derive taxonomies from scratch for ar-
bitrary document collections. Particular, we focus
on encoding user feedback in taxonomy construc-
tion process to handle task-specification rising from
a given document collection. We also addresses the
problem of path inconsistency due to local relation
recognition in existing taxonomy construction algo-
rithms. The user studies strongly suggest that the
proposed approach successfully resolve task specifi-
cation and path inconsistency in taxonomy construc-
tion.
1 Introduction
Taxonomies, such as Library of Congress Subject
Headings (LCSH, 2011) and Open Directory Project
(ODP, 2011), are widely used to support browsing-
style information access in document collections.
We call them browsing taxonomies. Browsing tax-
onomies are tree-structured hierarchies built upon a
given document collection. Each term in a browsing
hierarchy categorizes a set of documents related to
this term. Driven by their needs, users can navigate
through a the hierarchical structure of a browsing
taxonomy to access particular documents. A brows-
ing taxonomy can benefit information access via (1)
providing an overview of (important) concepts in a
document collection, (2) increasing the visibility of
documents ranked low in a list (e.g. documents or-
dered by search relevance), and (3) presenting to-
gether documents about the same concept to allow
more focused reading.
Most existing browsing taxonomies are manually
constructed thus they could not easily adapt to arbi-
trary document collections. However, it is not un-
common that document collections are given ad-hoc
for specific tasks, such as search result organiza-
tion in for individual search queries (Carpineto et al,
2009) and literature investigation for a new research
topic (Chau et al, 2011). There is a necessity to ex-
plore automatic or interactive techniques to support
quick construction of browsing taxonomies for arbi-
trary document collections.
Most research on automatic taxonomy construc-
tion focuses on identifying local relations between
concept pairs (Etzioni et al, 2005; Pantel and Pen-
nacchiotti, 2006). The infamous problem of path
inconsistency, which are usually caused by the lo-
cal nature of most relation recognition algorithms
when building a taxonomy, commonly exists in cur-
rent research. Oftentimes, when a connecting con-
cept for two pairs of parent-child concepts has mul-
tiple senses or represent mixed perspectives, the
problem shows up. For example, while financial
institute?bank and bank?river bank are correct;
the path financial institute?bank?river bank is se-
mantically inconsistent.
20
In this paper, we propose a semi-supervised dis-
tance learning method to construct task-specific tax-
onomies. Assuming that a user is present to con-
struct a taxonomy for browsing, the proposed ap-
proach directly learns semantic distances from the
manual guidance provided by the user to predict se-
mantically meaningful browsing taxonomies. More-
over, We tackle path inconsistency by posing con-
straints over root-to-leaf paths in a hierarchy to en-
sure concept consistency within paths
The contributions of our work include:
? It offers an opportunity for handling task spec-
ifications.
? Unlike most algorithms, our work takes care
of path consistency during taxonomy construc-
tion.
The remainder of this paper is organized as fol-
lows: Section 2 describes the related work. Sec-
tion 3 details the proposed automated algorithm for
taxonomy construction. Section 4 presents the in-
teractive algorithm to incorporate user feedback un-
der a supervised semantic distance learning frame-
work. Section 5 describes the evaluation and Section
6 concludes the paper.
2 Related Work
Most research conducted in the NLP community fo-
cuses on extracting local relations between concept
pairs (Hearst, 1992; Berland and Charniak, 1999;
Ravichandran and Hovy, 2002; Girju et al, 2003;
Etzioni et al, 2005; Pantel and Pennacchiotti, 2006;
Kozareva et al, 2008). More recently, more atten-
tion has been paid in building full taxonomies. For
example, (Kozareva and Hovy, 2010) proposed to
connect local concept pairs by finding the longest
path in a subsumption graph. Both (Snow et al,
2006) and (Yang and Callan, 2009) incrementally
grew taxonomies by adding new concepts at opti-
mal positions within the existing structures. Specifi-
cally, Snow et al estimated conditional probabilities
by using syntactic parse features and decided taxo-
nomic structure via maximizing overall likelihood
of taxonomy. Yang and Callan proposed the ME
framework to model the semantic distance d(cx, cy)
between concepts cx and cy as a weighted combi-
nation of numerous lexical and semantic features:
?
j weightj ? featurej(cx, cy) and determine the tax-
onomic structure by minimizing overall distances.
An advantage in ME is that it allows manipu-
lations to concept positions by incorporating vari-
ous constraints to taxonomic structures. For exam-
ple, ME handled concept generality-specificity by
learning different distance functions for general con-
cepts (located at upper levels) and specific concepts
(located at lower levels) in a taxonomy.
In the Information Retrieval (IR) community,
browsing taxonomies. also often called browsing
hierarchies or Web directories, has been studied
as an alternative to the ranked list representation
for search results by the Information Retrieval (IR)
community. The proposed forms of browsing struc-
tures include topic clusters (Cutting et al, 1992)
and monothetic concept hierarchies (Sanderson and
Croft, 1999; Lawrie et al, 2001; Kummamuru et al,
2004; Carpineto et al, 2009). The latter uses single
concepts to represent documents containing them
and organizes the concepts into hierarchies; they are
in fact taxonomies. The major drawback of these
approaches is that they often fail to produce mean-
ingful taxonomic structures due to neglecting the
semantics among concepts. For instance, (Sander-
son and Croft, 1999) used document frequency and
(Lawrie et al, 2001) used conditional probability to
derive is-a relations. Moreover, they also suffer from
path inconsistency when building full taxonomies.
3 Browsing Taxonomy Construction
To build browsing taxonomy for a document collec-
tion, the first step is to extract the concepts. We take
a simple but effective approach. We exhaustively ex-
amine the collection and output a large set of terms,
formed by nouns, noun phrases, and named entities
occurring >5 times in the collection. We then fil-
ter out invalid terms due to part-of-speech errors or
misspelling by removing terms that occur <4 times
out of the top 10 returned snippets when submitting
the term to google.com as a search query. We fur-
ther conflate similar terms into clusters using LSA
(Bellegarda et al, 1996) and select the most frequent
terms as concepts from each term group. We select
theN most frequent concepts to form the concept set
C. N usually ranges from 30 to 100. We assume that
C contains all concepts in the browsing taxonomy;
21
even when an important concept for the collection is
missing, we will ?make do? with C. This may lead
to some errors, but can be later corrected by users
through proposing new concepts interactively (Sec-
tion 4).
This section presents how to automatically build
taxonomies. We introduce the semantic distance
learning method in Section 3.1 and present how to
achieve path consistency control in Section 3.2.
3.1 Semantic Distance Learning
To support browsing in arbitrary collections, in this
paper, we propose to incorporate task specification
in a taxonomy. One way to achieve it is to define
task-specific distances among concepts. Moreover,
through controlling distance scores among concepts,
we can enforce path consistency in taxonomies. For
example, when the distance between financial in-
stitute and river bank is big, the path financial
institute?bank?river bank will be pruned and the
concepts will be repositioned. Inspired by ME, we
take a distance learning approach to deal with path
consistency (Section 3) and task specification (Sec-
tion 4) in taxonomy construction. In this section,
we demonstrate how to estimate semantic distances
from training data.
We assume that there are some underlying fea-
ture functions that measure semantic dissimilarity
for two concepts from various aspects and a good
semantic distance is a combination of all features.
Different fromME, we model the semantic distance
between concepts (cx, cy) as a Mahalanobis distance
(Mahalanobis, 1936):
dcx,cy =
?
?(cx, cy)TW?1?(cx, xy) (1)
dcx,cy =
?
?(cx, cy)TW?1?(cx, xy), where
?(cx, cy) represents the set of pairwise underlying
feature functions, where each feature function is
?k : (cx, cy) with k=1,...,|?|. W is a weight ma-
trix, whose diagonal values weigh the underlying
feature functions. When only diagonal values of W
are taken into account, W is equivalent to assigning
weights to different axes in the random vectors.
Note that a semantic distance is still a distance
metric. One important characteristic of a valid dis-
tance metric is that it must represent valid cluster-
ing partitions, which means that the clustering parti-
tions represented by the distance metric should be
consistent. Therefore, certain constraints need to
be satisfied. An obvious one is that concepts in
the same cluster should have smaller distance scores
than those in different clusters. Moreover, a valid
distance metric should be non-negative and satisfy
the triangle inequality. To ensure such regularities,
we need to constrain W to be positive semi-definite
(PSD) (Bhatia, 2006):
W  0.
Since we assume that a good semantic distance is
a combination of all these features, we can decom-
pose the task of semantic distance learning into two
subtasks - identifying good features and learning the
weight matrix from training data.
In our approach, we employ a wide range of fea-
tures to cover various aspects in measuring dissimi-
larity between concepts. Given two concepts cx and
cy, a feature is defined as a function ? : (cx, cy) en-
erating a value within [0,1]. In total, we used 31
features, including lexical-syntactic patterns, con-
textual, co-occurrence, syntactic dependency, and
definitions.
Similar to the linguistic approaches, we use
lexical-syntactic patterns to evaluate relations
among concepts. Our patterns include hypernym
patterns such as ?cx, and other cy?, sibling patterns
such as ?cx and cy?, and part-of patterns such as ?cx
consists of cy?. Each feature returns a boolean value
of wether it can find instances for the pattern in text.
Besides patterns, we used more semantic features.
For example, since word meanings can be inferred
from and represented by contexts, we develop sev-
eral contextual features. One is Local Context KL-
Divergence, which measures the Kullback-Leibler
divergence between two unigram language models
built for cx and cy upon all left two and right two
words surrounding them. Moreover, we formulate
the co-occurrence features as point-wise mutual in-
formation between (cx, cy):
pmi(cx, cy) = log
Count(cx, cy)
Count(cx)Count(cy)
,
where Count(.) is defined as the number of docu-
ments or sentences containing the concept(s), or n
as in ?Results 1-10 of about n for term? appearing
22
on the first page of Google search results for query-
ing cx, cy, or cxcy.
We also generate syntactic dependency features
via syntactic parse1 and semantic role labeling2. For
example, we measure how many overlaps exist be-
tween cx?s and cy?s modifiers. Lastly, we measure
definition overlaps between cx and cy by counting
the number of nonstop word overlaps between their
definitions obtained by querying google.com with
?define:cx? and ?define:cy?.
To achieve a comprehensive distance measure for
concepts, we propose to effectively combine these
features. Our goal is to find a parametric distance
metric functions which allows combining various
features and assigning different weights for them.
It also needs to produce distances that satisfy non-
negativity and triangle inequality.
We further estimateW by minimizing the squared
errors between the semantic distances d generated
from the training data and the expected value d?.
Moreover, we constrain W to be PSD. The parame-
ter estimation is:
min
W
|C|?
x=1
|C|?
y=1
(
dcx,cy ?
?
?(cx, cy)TW?1?(cx, cy)
)2
(2)
subject to W  0. The optimization can be done
by any standard semi-definite programming (SDP)
solver. We used (Sedumi, 2011) and (Yalmip, 2011)
to perform the optimization.
In our framework, the major source of training
data is user feedback. Another source is existing
hierarchies such as WordNet (Fellbaum, 1998) and
ODP (ODP, 2011) (Section 3). Nonetheless, we ob-
tain the semantic distance for a concept pair (cx, cy)
in training data by summing up edge weights along
the shortest path from cx to cy in a training hierar-
chy. The edge weight can be assigned based on the
types of relations that an edge represent as in Section
4.1.
The learned model W can be used to predict dis-
tance scores for testing concept pairs by applying
Eq. 1 on them.
1Done by Minipar: http://www.cs.ualberta.ca/lindek/minipar.htm.
2Done by Assert: http://cemantix.org/assert/.
3.2 Resolving Path Inconsistency
With the pair-wise semantic distances, we are ready
to build the full taxonomy. As in ME, we also take
an incremental taxonomy construction framework,
where concepts are inserted one at a time. Partic-
ularly, we propose that at each insertion, a concept
cz is tried as either a parent or a child concept to all
existing nodes in the current partial taxonomy Tn.
The evaluation of the best position depends on the
semantic distances between cz and all other concepts
in the taxonomy.
To enforce consistency along a path from the root
to a leaf in a taxonomy, we propose to require all
concepts on the path to be about the same topic.
They need to be coherent no matter how far away
two concepts are apart in this path. We achieve this
by enforcing the sum of semantic distances in a path
to be as small as possible. Particularly, when a new
concept cz is added into a taxonomy T , we require
that the optimal root-to-leaf path P? containing cx
should satisfy the following condition:
P?cz = arg min
P ?cz
?
cx,cy?P ?cz ,x<y
d(cx, cy) (3)
where Pcz is a root-to-leaf path including cz , x < y
defines the order of the concepts so we only compute
a pair-wise distance between two concepts once.
To incorporate path consistency into taxonomy
construction, we introduce a variable ? ? [0, 1] to
control the contributions from overall semantic dis-
tance minimization (as in ME) and path distance
minimization. We formulate the optimization as:
min?u+ (1? ?)v (4)
subject to u = |
?
cx,cy?Cn?{cz},x<y d(cx, cy) ??
cx,cy?Cn,x<y d(cx, cy)|, v =?
cj ,ck?P ?cz ,j<k
d(cj , ck), 0 ? ? ? 1, where u
denotes ?minimization of overall semantic dis-
tance?, v denotes the ?path consistency?, and Cn is
the concept set for the nth partial taxonomy.
4 Resolving Task Specification
Give an arbitrary document collection and its con-
cept set C, most concepts can be organized nicely
according to the automatic algorithm proposed in
Section 3. However, for concepts with multiple per-
spectives, we need to decide which perspective the
23
task wants to keep in the browsing taxonomy. More-
over, Section 3 learns distance functions from Word-
Net and ODP, which suggests that the algorithm will
roughly follow how WordNet and ODP define rela-
tions. In practice, a task may require completely dif-
ferent organizations, e.g., by question-answer pairs
or by topics. The ever-changing task specifications
can only be captured by the user/constructor who ad-
justs a browsing taxonomy to suit the requirements.
This section studies how to incorporate task spec-
ifications in the taxonomy construction. Particularly,
how to allow the machine learning algorithm to learn
from the user, and how to produce a task-specific
browsing taxonomy according to the user?s guid-
ance. The framework is expected to produce tax-
onomies that reflect personal preferences as a con-
sequence of learning from manual guidance.
We present a general framework that enables tax-
onomy construction taking into account user-defined
concept organization. Basically, to guide how to or-
ganize the concepts, a user trains the supervised dis-
tance learning model using a taxonomy construction
tool that supports editing functions such as dragging
and dropping, adding, deleting, and renaming nodes
that allows the user to intuitively modify a taxon-
omy.
Particularly, an initial taxonomy is constructed
by the automatic taxonomy construction framework
presented in Section 3. Starting from the initial tax-
onomy, a user can teach the machine learning algo-
rithm by providing manual guidance to it. The algo-
rithm learns from the manual guidance and adjusts
the distance learning function and modifies the tax-
onomy accordingly. When a user put cx under cy, it
indicates that the user wants a relation demonstrated
by cx ? cy to be true in this taxonomy. We cap-
ture the user inputs as manual guidance and make
use of it to adjust the distance learning model to or-
ganize other concepts agreeing with the user. The
teaching and the learning alternate until the user is
satisfied with the taxonomy. The resulting taxonomy
contains both the user?s inputs and the machine?s ad-
justed organization for the concepts.
4.1 Collecting and Learning from Manual
Guidance
The most challenging part of incorporating manual
guidance in the machine learning process is how to
translate it into a format that the machine can easily
understand and incorporate into its learning models.
In this research, browsing taxonomies are tree struc-
tures. Trees. however, are not straightforward for
a machine learning algorithm to manipulate. In or-
der to capture the changes between each version of
the manual editions, the learning algorithm needs
both the training and the test data to be in a for-
mat which is easy to handle. Matrix representation
can be easily understood and manipulated by many
machine learning algorithms. We therefore convert
taxonomies from trees to matrices and use a matrix
representation for all the intermediate editions in the
taxonomy construction process.
We propose to convert a taxonomy from a tree to
matrices of neighboring nodes and represent the dif-
ferences in matrices before and after human edits as
manual guidance. We then train the learning frame-
work to adjust to it and make predictions for unor-
ganized concepts.
We represent the organization of concepts before
a user?s modifications as a before matrix; likewise,
the new organization of concepts after her modifica-
tions is represented as a after matrix. Given these
two matrixes, manual guidance is a submatrix in af-
ter matrix that shows the differences between before
matrix and after matrix.
We compare the before matrix A and the after ma-
trix B to derive the manual guidance M. The man-
ual guidance is not simply the matrix difference be-
tween the before matrix and the after hierarchy ma-
trix. It is part of the after matrix because it is the
after matrix that indicates where the user wants the
taxonomy to develop. We define manual guidance
M as a submatrix which consists of some entries of
the after matrix B; at these entries, there exist dif-
ferences between the before matrix A and the after
matrix B.
For simple cases when the set of concepts re-
main unchanged before and after human modifica-
tions, the above definition and calculation of manual
guidance work. However, oftentimes the user adds,
deletes or renames concepts, and the concept set
changes. When the concept set changes, the above
definition of manual guidance M needs a slight al-
teration.
Figure 1 shows an example taxonomy whose con-
cept set changes. The original concept set before
24
Figure 1: A taxonomy before and after human modifica-
tions (concept set changes; relation type = sibling).
the human modification is {person, leader, presi-
dent, Hu, Obama}. The taxonomy?s before matrix
A is:
A =
person leader president Hu Obama
person 1 0 0 0 0
leader 0 1 1 0 0
president 0 1 1 0 0
Hu 0 0 0 1 0
Obama 0 0 0 0 1
.
The user modifies the taxonomy at several places.
In particular, leader is deleted, Hu is moved to be
under president, and prime minister is inserted as a
new concept into this taxonomy. Therefore the con-
cept set changes to {person, president, Hu, Obama,
prime minister}. The after matrix B is:
B =
person president Hu Obama PM
person 1 0 0 0 0
president 0 1 0 0 1
Hu 0 0 1 1 0
Obama 0 0 1 1 0
PM 0 1 0 0 1
.
Since the concept sets before and after the human
modifications change, we cannot simply use matrix
subtraction to get the difference between the before
and after matrices. Suppose the concept set in the
taxonomy before the modifications is CA, and the
concept set after modifications is CB , we define an
expanded set of concepts CE as the union of CA and
CB .
For taxonomies with concept changes, we define
the manual We then define manual guidance M as a
submatrix which consists of some entries of the af-
ter matrix B; at these entries, there exist differences
from the expanded before matrix A? to the expanded
after matrix B?. The expanded rows and columns in
A? and B? are filled with 0 for non-diagonal entries,
and 1 for diagonal entries. Note that the concepts
corresponding to these entries should exist in CB ,
the unexpanded set of concepts after human modifi-
cations. Formally,
M = B[r; c]
where r = {i : bij ? aij 6= 0, ci ? CB}, c = {j :
bij ? aij 6= 0, cj ? CB}, aij is the (i, j)th entry in
A?, and bij is the (i, j)th entry in B?.
For the example in Figure 1, the manual guidance
M is:
M = B[2, 3, 4, 5; 2, 3, 4, 5] =
?
?
1 0 0 1
0 1 1 0
0 1 1 0
1 0 0 1
?
? .
Based on M , we can create training data D =
1 ? M , for the supervised distance learning algo-
rithm, which aims to learn a good model which best
preserves the regularity defined by the task and the
user using the techniques proposed in Section 3.1.
5 Evaluation
To evaluate the effectiveness of our approach, We
conducted two user studies, one to evaluate brows-
ing effectiveness and another to evaluate quality of
taxonomies. Five users (graduate students and rela-
tives of the authors) in the first study were asked to
construct browsing taxonomies with a task in mind
- ?writing a survey paper about the collection?.
In the second study (24 graduates and undergrad-
uates), we compared taxonomies constructed by dif-
ferent users to identify where mixed perspectives in
taxonomies come from in Section 5.3. We also in-
vestigated whether the differences are due to self-
inconsistency in Section 5.4. Moreover, we manu-
ally select relations violating path consistency and
report our approach?s ability to handle path consis-
tency in Section 5.2.
5.1 Datasets
To show that task-specific taxonomies are more suit-
able for browsing than general taxonomies, we com-
pared excerpts of the official North America Indus-
try Classification Systems (we call them NAICS-
25
0	 ?0.1	 ?
0.2	 ?0.3	 ?
0.4	 ?0.5	 ?
0.6	 ?0.7	 ?
0.8	 ?0.9	 ?
1	 ?
NAICS-??2	 ? Web	 ?
Path
	 ?Erro
r	 ?
w/	 ?path	 ?consistency	 ?w/o	 ?path	 ?consistency	 ?
Figure 2: Path error w/ and w/o path consistency control.
1) with comparable taxonomies derived by tech-
niques presented in this paper (we call them NAICS-
2). Since the original collection used to build of-
ficial NAICS taxonomies is not available, we cre-
ated document collections by crawling search results
from google.com for concepts in NAICS-1 excerpts.
The participants worked on the collection to create
NAICS-2 taxonomies. Each NAICS-1 or NAICS-2
taxonomy contains about 40 concepts.
We also evaluate our techniques on Web search
result organization. Five Web datasets were created
by submitting 4 to 5 queries3 to and collecting the
returned Web documents from search engines Bing
and Google. Around 100 Web documents and 40
concepts are collected for a topic. We manually
judged relevant documents for each topic.
5.2 Path Consistency
To evaluate how well our method can handle path
inconsistency, we compare the path error rate before
and after applying path consistency control. The
evaluation is only conducted for the automated algo-
rithm (Section 3) on the NAICS-2 and Web datasets.
No user study is involved.
Two human assessors manually evaluated the path
errors4 in a taxonomy by the following procedure:
(1) Starting from the root concept, perform a depth-
first traverse in the taxonomy; (2) along each path,
count the number of wrong ancestor-descendant
pairs due to word sense ambiguity or mixed perspec-
tives; (3) sum up the errors that both assessors agree
and normalize them by the taxonomy size. Note
that path errors are evaluated for concepts are not
immediately connected, whereas differences due to
mixed perspectives (Section 5.3) refer to immediate
relations. Figure 2 shows that with path consistency
3E.g., queries ?trip to DC?, ?Washington DC?, ?DC?, and
?Washington? were submitted for the topic ?plan a trip to DC?.
4Other types of errors were ignored in the assessment.
0 100 200 300
0
51
01
52
0
Information
Number of concept pairs
Num
ber o
f agr
eem
ents
0 100 200 300
0
51
01
52
0
Kindergarten
Number of concept pairs
Num
ber o
f agr
eem
ents
Figure 3: Agreements among participants for the parent-
child pairs for datasets information and kindergarten.
control, we can statistically significantly reduce path
errors due to word sense ambiguity and mixed per-
spectives by 500% (p-value<.001, t-test). It strongly
indicates that our technique to control path inconsis-
tency in taxonomy construction is effective.
5.3 Mixed Perspectives in Taxonomies
To better understand mixed perspectives in tax-
onomies constructed, we look for commonality and
differences among the taxonomies constructed by
the 24 participants for the same topic in the second
user study. We break each taxonomy into parent-
child pairs, and count how many participants agreed
on a pair. The agreements range from 1 to 24. The
taxonomies we examined are NAICS-2 and Web.
We plot the number of agreements for every con-
cept pair and observe a long-tail power-law distri-
bution for all datasets. Figure 3 shows that for
the dataset ?information?, which contains about 300
unique concept pairs, while in ?kindergarten?, more
than 200 unique concept pairs exist. This suggests
that people use rich and diverse expressions to con-
struct taxonomies and organize information differ-
ently within them. Although commonality (can be as
high as 24 out of 24) and differences co-exist in tax-
onomies created for the same topic, the differences
are much more dominate than the commonality.
We manually break down the types of differ-
ences in producing parent-child pairs into the fol-
lowing categories: mixed parents (a concept has
different parent concepts due to word sense ambi-
guity), mixed ancestors (a concept is assigned to
grandparents, not the direct parent), mixed relation
types (a pair show relations other than is-a, such as
part-of and affiliation), new concepts (participants
add new concepts), morphological differences (plu-
rals, -tion, etc), errors (clearly wrong relations, e.g.,
26
mixed	 ?parents	 ?23%	 ?
mixed	 ?ancesters	 ?10%	 ?
flat	 ?structure	 ?13%	 ?
mixed	 ?rela?n	 ?types	 ?17%	 ?
morphological	 ?	 ?13%	 ?
new	 ?concepts	 ?18%	 ?
errors	 ?5%	 ? typo	 ?1%	 ?
Figure 4: Sources of differences in NAICS-2 and Web.
infant?school director), flat structure (some partic-
ipants liked to assign a large portion of concepts as
children to the root), and typo.
Figure 4 illustrates the break-down of various
types of differences. Mixed parents is the largest
contributor with 23% share, followed by new con-
cepts (18%) and mixed relation types (17%). Among
all the types, mixed parents, new concepts, and
mixed relation types indicate mixed perspectives or
word sense ambiguity; in total they contribute about
58% differences in taxonomies. Flat structure and
mixed ancestors are about confusions in taxonomy
topology, which contribute about 23% differences.
Other differences due to morphological changes, ty-
pos and errors contribute about 19% differences.
The break-down reveals that mixed perspective, one
of main foci in this paper, is indeed the biggest
source of difference in taxonomy construction.
5.4 Self-agreement
Another doubt is that maybe the differences come
from randomness? To find out if the variations
among taxonomies is due to randomness, we de-
signed a repeat phase in the second user study. We
randomly invited 12 participants to repeat the same
tasks in the same order 3 weeks5 after the initial
phase and compare the taxonomies constructed in
both phases for the NAICS-2 and Web datasets.
We use Fragment-Based Similarity (FBS) pro-
posed by (Yang, 2011) to calculate the similarity
between taxonomies constructed in the initial phase
and in the repeat phase by the same participant.
FBS for two taxonomies Ti and Tj is calculated as:
FBS(Ti, Tj) = 1max(U,V )
?m
p=1 simcos(tip, tjp),
where U and V is the number of concepts in Ti
and Tj respectively, m is the number of matched
5The three week period ensured that participants only had
limited memory of the details about the tasks.
Self agreement (in FBS) Max Min Average
per participant per dataset 1 0.37 0.74
per participant 0.81 0.63 0.74
per dataset 0.95 0.62 0.74
Table 1: Self-agreement; measured in FBS.
pairs based on the highest cosine similarity, simcos
is the cosine similarity between vectors for subtrees
of concepts tip and tjp.
Table 1 indicate the self-agreement between tax-
onomies for any participant and/or any topic. The
max self-agreement is as high as 1. The average
self-agreement is 0.74, which is high at the range of
FBS. It suggests that the participants are quite self-
consistent when constructing taxonomies at differ-
ent times. It builds the foundation for our study on
multiple perspectives in taxonomy construction.
6 Conclusion
This paper explores techniques to quickly derive
task-specific taxonomies supporting browsing in ar-
bitrary document sets. It addresses two issues in tax-
onomy construction: path inconsistency due to word
sense ambiguity and mixed perspectives, and task
specifications in arbitrary collections. We tackle
both issues in a supervised distance learning frame-
work via minimizing distances along a path and us-
ing user inputs as training data, respectively. The
user studies strongly suggest that the proposed tech-
niques are highly effective in constructing browsing
taxonomies as well as handling path consistency.
References
J. R. Bellegarda, J. W. Butzberger, Yen-Lu Chow, N. B.
Coccaro, and D. Naik. 1996. A novel word clustering
algorithm based on latent semantic analysis. In Pro-
ceedings of the Acoustics, Speech, and Signal Process-
ing, 1996. on Conference Proceedings., 1996 IEEE
International Conference - Volume 01, ICASSP ?96,
pages 172?175, Washington, DC, USA. IEEE Com-
puter Society.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 27th
Annual Meeting for the Association for Computational
Linguistics (ACL 1999).
Rajendra Bhatia. 2006. Positive definite matrices
(princeton series in applied mathematics). Princeton
University Press, December.
27
Claudio Carpineto, Stefano Mizzaro, Giovanni Romano,
and Matteo Snidero. 2009. Mobile information re-
trieval with search results clustering: Prototypes and
evaluations. Journal of American Society for Informa-
tion Science and Technology (JASIST), pages 877?895.
Duen Horng Chau, Aniket Kittur, Jason I. Hong, and
Christos Faloutsos. 2011. Apolo: making sense of
large network data by combining rich user interaction
and machine learning. In CHI, pages 167?176.
Gouglass R. Cutting, David R. Karger, Jan R. Petersen,
and John W. Tukey. 1992. Scatter/Gather: A cluster-
based approach to browsing large document collec-
tions. In Proceedings of the fifteenth Annual ACM
Conference on Research and Development in Informa-
tion Retrieval (SIGIR 1992).
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. In Artificial Intelligence, 165(1):91-
134, June.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the Human Language Technology Conference/Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2003).
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING 1992).
Zornitsa Kozareva and Eduard Hovy. 2010. A semi-
supervised method to learn and construct taxonomies
using the web. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1110?1118, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of the 46th An-
nual Meeting for the Association for Computational
Linguistics (ACL 2008).
Krishna Kummamuru, Rohit Lotlikar, Shourya Roy,
Karan Singal, and Raghu Krishnapuram. 2004. A hi-
erarchical monothetic document clustering algorithm
for summarization and browsing search results. Pro-
ceedings of the 13th conference on World Wide Web
WWW 04, page 658.
Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg.
2001. Finding topic words for hierarchical summa-
rization. In Proceedings of the 24th Annual ACM Con-
ference on Research and Development in Information
Retrieval (SIGIR 2001), pages 349?357.
LCSH. 2011. Library of congress subject headings.
http://www.loc.gov/.
P. C. Mahalanobis. 1936. On the generalised distance in
statistics. In Proceedings of the National Institute of
Sciences of India 2 (1): 495.
ODP. 2011. Open directory project. http://www.
dmoz.org/.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 44th An-
nual Meeting for the Association for Computational
Linguistics (ACL 2006).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting for the As-
sociation for Computational Linguistics (ACL 2002).
Mark Sanderson and W. Bruce Croft. 1999. Deriving
concept hierarchies from text. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR 1999).
Sedumi. 2011. http://sedumi.mcmaster.ca.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics (ACL/COLING 2006).
Yalmip. 2011. http://users.isy.liu.se/
johanl/yalmip.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In Pro-
ceedings of the 47th Annual Meeting for the Associa-
tion for Computational Linguistics (ACL 2009).
Hui Yang. 2011. Personalized Concept Hierarchy
Construction. Ph.D. thesis, Carnegie Mellon Univer-
sity. http://www.cs.cmu.edu/?huiyang/
publication/dissertation.pdf.
28
