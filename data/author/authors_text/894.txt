Proceedings of the Linguistic Annotation Workshop, pages 69?76,
Prague, June 2007. c?2007 Association for Computational Linguistics
Computing translation units and quantifying parallelism
in parallel dependency treebanks
Matthias Buch-Kromann
ISV Computational Linguistics Group
Copenhagen Business School
mbk.isv@cbs.dk
Abstract
The linguistic quality of a parallel tree-
bank depends crucially on the parallelism
between the source and target language an-
notations. We propose a linguistic notion
of translation units and a quantitative mea-
sure of parallelism for parallel dependency
treebanks, and demonstrate how the pro-
posed translation units and parallelism mea-
sure can be used to compute transfer rules,
spot annotation errors, and compare differ-
ent annotation schemes with respect to each
other. The proposal is evaluated on the
100,000 word Copenhagen Danish-English
Dependency Treebank.
1 Introduction
Parallel treebanks are increasingly seen as a valuable
resource for many different tasks, including machine
translation, word alignment, translation studies and
contrastive linguistics (C?mejrek et al, 2004; Cyrus,
2006; Hansen-Schirra et al, 2006). However, the
usefulness of a parallel treebank for these purposes
is directly correlated with the degree of syntactic
parallelism in the treebank. Some non-parallelism
is inevitable because two languages always differ
with respect to their syntactic structure. But non-
parallelism can also be the result of differences in
the linguistic analyses of the source text and target
text, eg, with respect to whether noun phrases are
headed by nouns or determiners, whether conjunc-
tions are headed by the first conjunct or the coordi-
nator, whether prepositions are analyzed as heads or
adjuncts in prepositional phrases, etc.
In this paper, we focus on parallel dependency
treebanks that consist of source texts and trans-
lations annotated with dependency analyses and
word-alignments. These requirements are directly
satisfied by the analytical layer of the Prague
Czech-English Dependency Treebank (C?mejrek et
al., 2004) and by the dependency layer of the
Copenhagen Danish-English Dependency Treebank
(Buch-Kromann et al, 2007). The requirements are
also indirectly satisifed by parallel treebanks with a
constituent layer and a word alignment, eg (Han et
al., 2002; Cyrus, 2006; Hansen-Schirra et al, 2006;
Samuelsson and Volk, 2006), since it is possible
to transform constituent structures into dependency
structures ? a procedure used in the CoNLL shared
tasks in 2006 and 2007 (Buchholz and Marsi, 2006).
Finally, it is worth pointing out that the requirements
are also met by any corpus equipped with two dif-
ferent dependency annotations since a text is always
trivially word-aligned with itself. The methods pro-
posed in the paper therefore apply to a wide range
of parallel treebanks, as well as to comparing two
monolingual treebank annotations with each other.
The paper is structured as follows. In section 2,
we define our notions of word alignments and de-
pendencies. In section 3, we define our notion of
translation units and state an algorithm for comput-
ing the translation units in a parallel dependency
treebank. Finally, in sections 4, 5 and 6, we demon-
strate how translation units can be used to compute
transfer rules, quantify parallelism, spot annotation
errors, and compare monolingual and bilingual an-
notation schemes with respect to each other.
69
Complement roles Adjunct roles
aobj adjectival object appa parenthetical apposition
avobj adverbial object appr restrictive apposition
conj conjunct of coordinator coord coordination
dobj direct object list unanalyzed sequence
expl expletive subject mod modifier
iobj indirect object modo dobj-oriented modifier
lobj locative-directional obj. modp parenthetical modifier
nobj nominal object modr restrictive modifier
numa additive numeral mods subject-oriented mod.
numm multiplicative numeral name additional proper name
part verbal particle namef additional first name
pobj prepositional object namel additional last name
possd possessed in genitives pnct punctuation modifier
pred subject/object predicate rel relative clause
qobj quotation object title title of person
subj subject xpl explification (colon)
vobj verbal object
Figure 1: The main dependency roles in the dependency framework Discontinuous Grammar.
2 Word alignments and dependencies
In our linguistic analyses, we will assume that a
word alignment W ?W ? encodes a translational cor-
respondence between the word clusters W and W ? in
the source text and target text, ie, the word align-
ment expresses the human intuition that the subset
W of words in the source text corresponds roughly
in meaning or function to the subset W ? of words in
the target text. The translations may contain addi-
tions or deletions, ie, W and W ? may be empty.
We also assume that a dependency edge g r??d
encodes a complement or adjunct relation between a
word g (the governor) and a complement or adjunct
phrase headed by the word d (the dependent), where
the edge label r specifies the complement or adjunct
dependency role.1 As an illustration of how comple-
ment and adjunct relations can be encoded by means
of dependency roles, the most important dependency
roles used in the dependency framework Discontin-
uous Grammar (Buch-Kromann, 2006) are shown in
Figure 1. Finally, we will assume that the depen-
dencies form a tree (or a forest). The tree may be
non-projective, ie, it may contain crossing branches
(technically, a dependency g r??d is projective if
1Following standard dependency theoretic assumptions, we
will assume the following differences between complement and
adjunct relations: (a) complements are lexically licensed by
their governor, whereas adjuncts license their adjunct governor;
(b) in the functor-argument structure, complements act as ar-
guments of their governor, whereas adjuncts act as modifiers
of their governor; (c) a governor can have several adjuncts with
the same adjunct role, whereas no two complements of the same
governor can have the same complement role.
 
X
X
skal
must
kun
only
koncentrere
concentrate
sig
self
om
about
Y
Y


subj vobjmod dobj pobj nobj
X has to concentrate only on Y
subj dobj vobj pobjmod nobj
Figure 2: Parallel dependency treebank analysis
with word alignment and two monolingual depen-
dency analyses.
and only if g is a transitive governor of all the words
between g and d).
Figure 2 shows an example of this kind of analy-
sis, based on the annotation conventions used in Dis-
continuous Grammar and the associated Copenha-
gen Danish-English Dependency Treebank (Buch-
Kromann et al, 2007). In the example, word align-
ments are indicated by lines connecting Danish word
clusters with English word clusters, and dependen-
cies are indicated by means of arrows that point
from the governor to the dependent, with the de-
pendency role written at the arrow tip. For ex-
ample, the Danish word cluster ?koncentrere sig?
(?concentrate self?) has been aligned with the En-
glish word ?concentrate?, and the English phrase
70
headed by ?on? is analyzed as a prepositional ob-
ject of the verb ?concentrate.? In the Danish de-
pendency analysis, the dependency between the ad-
verbial ?kun? (?only?) and its prepositional gover-
nor ?om? (?about?) is non-projective because ?om?
does not dominate the words ?koncentrere? (?con-
centrate?) and ?selv? (?self?).
Dependency analyses differ from phrase-structure
analyses in that phrases are a derived notion: in a de-
pendency tree, each word has a derived phrase that
consists of all the words that can be reached from the
word by following the arrows. For example, the En-
glish word ?concentrate? heads the phrase ?concen-
trate only on Y,? and the Danish word ?om? heads
the discontinuous phrase ?kun . . . om Y.?
If a parallel dependency analysis is well-formed,
in a sense to be made clear in the following sec-
tion, each alignment edge corresponds to what we
will call a translation unit. Intuitively, given an
aligment edge W ?W ?, we can create the cor-
responding translation unit by taking the source
and target subtrees headed by the words in W
and W ?, deleting all parallel adjuncts of W ?W ?,
and replacing all remaining parallel dependents
of W ?W ? with variables x1, . . . ,xn and x?1, . . . ,x?n.
The resulting translation unit will be denoted by
T (x1, . . . ,xn)?T ?(x?1, . . . ,x?n), where T and T ? de-
note the source and target dependency trees in the
translation unit. For convenience, we will some-
times use vector notation and write T (x)?T ?(x?)
instead of T (x1, . . . ,xn)?T ?(x?1, . . . ,x?n). Dependen-
cies are usually defined as relations between words,
but by an abuse of terminology, we will say that a
word d is a dependent of an alignment edge W ?W ?
provided d is a dependent of some word in W ?W ?
and d is not itself contained in W ?W ?.
Figure 3 shows the six translation units that can
be derived from the parallel dependency analysis in
Figure 2, by means of the procedure outlined above.
Each translation unit can be interpreted as a bidi-
rectional translation rules: eg, the second translation
unit in Figure 3 can be interpreted as a translation
rule stating that a Danish dependency tree with ter-
minals ?x1 skal x2? can be translated into an English
dependency tree with terminals ?x?1 has to x?2? where
the English phrases x?1,x?2 are translations of the Dan-
ish phrases x1,x2, and vice versa.
In the following section, we will go deeper into
 
X
X
x1
x1
skal
must
x2
x2
koncentrere
concentrate
sig
self
x1
x1
kun
only
om
about
x1
x1
Y
Y


subj vobj dobj pobj nobj
X x1? has to x2? concentrate x1? only on x1? Y
subj dobj vobj pobj nobj
Figure 3: The six translation units derived from the
parallel dependency analysis in Figure 2.
the definition and interpretation of these rules. In
particular, unlike the essentially context-free trans-
lation rules used in frameworks such as (Quirk et
al., 2005; Ding, 2006; Chiang, 2007), we will not
assume that the words in the translation rules are or-
dered, and that the translation rules can only be used
in a way that leads to projective dependency trees.
3 Translation units within a simple
dependency-based translation model
In many parallel treebanks, word alignments and
syntactic annotations are created independently of
each other, and there is therefore no guarantee that
the word or phrase alignments coincide with any
meaningful notion of translation units. To rectify
this problem, we need to define a notion of trans-
lation units that links the word alignments and the
source and target dependency analysis in a meaning-
ful way, and we need to specify a procedure for con-
structing a meaningful set of word alignments from
the actual treebank annotation.
Statistical machine translation models often em-
body an explicit notion of translation units. How-
ever, many of these models are not applicable to
parallel treebanks because they assume translation
units where either the source text, the target text
or both are represented as word sequences without
any syntactic structure (Galley et al, 2004; Marcu
et al, 2006; Koehn et al, 2003). Other SMT models
assume translation units where the source and tar-
get language annotation is based on either context-
free grammar (Yamada and Knight, 2001; Chiang,
2007) or context-free dependency grammar (Quirk
et al, 2005; Ding, 2006). However, since non-
71
projectivity is not directly compatible with context-
free grammar, and parallel dependency treebanks
tend to encode non-projective dependencies directly,
the context-free SMT models are not directly appli-
cable to parallel dependency treebanks in general.
But the context-free SMT models are an important
inspiration for the simple dependency-based trans-
lation model and notion of translation units that we
will present below.
In our translation model, we will for simplicity as-
sume that both the source dependency analysis and
the target dependency analysis are unordered trees,
ie, dependency transfer and word ordering are mod-
elled as two separate processes. In this paper, we
only look at the dependency transfer and ignore the
word ordering, as well as the probabilistic modelling
of the rules for transfer and word ordering. There are
three kinds of translation rules in the model:
A. Complement rules have the form T (x)?T ?(x?)
where T (x) is a source dependency tree with vari-
ables x = (x1, . . . ,xn), T ?(x?) is a target dependency
tree with variables x? = (x?1, . . . ,x?n), the words in T
are aligned to the words in T ?, and the variables xk,x?k
denote parallel source and target subtrees. The rule
states that a source tree T (x) can be transferred into
a target tree T ?(x?) by transferring the source sub-
trees in x into the target subtrees in x?.
B. Adjunct rules have the form (x a??T (y))?
(x? a???T ?(y?)) where T (y) is a source dependency
tree, T ?(y?) is a target dependency tree, and x,x? are
variables that denote parallel adjunct subtrees with
adjunct roles a,a?, respectively. The rule states that
given a translation unit T (y)?T (y?), an a-adjunct
of any word in T can be translated into an a?-adjunct
of any word in T ?.2
C. Addition/deletion rules have the form T (y)?
(x? a???T ?(y?)) and (x a??T (y))?T ?(y?) where x,x?
are variables that denote adjunct subtrees, and a,a?
are adjunct relations. The addition rule states that
an adjunct subtree x? can be introduced into the tar-
get tree T ? in a translation unit T (y)?T (y?) without
any corresponding adjunct in the source tree T . Sim-
ilarly, the deletion rule states that the adjunct subtree
2In the form stated here, adjunct rules obviously overgener-
ate because they do not place any restrictions on the words in T ?
that the target adjunct can attach to. In a full-fledged translation
model, the adjunct rules must be augmented with a probabilistic
model that can keep track of these restrictions.
 
X
X
skal
must
kun
only
koncentrere
concentrate
sig
self
om
about
Y
Y


subj mod vobj dobj pobj nobj
X has to concentrate only on Y
subj dobj vobj pobjmod nobj
Figure 4: Parallel dependency analysis that is in-
compatible with our translation model.
x in the source tree T does not have to correspond to
any adjunct in the target tree T ?.3
The translation model places severe restrictions
on the parallel dependency annotations. For exam-
ple, the annotation in Figure 4 is incompatible with
our proposed translation model with respect to the
adjunct ?only?, since ?only? attaches to the verb
?skal/must? in the Danish analysis, but attaches to
the preposition ?on? in the English analysis ? ie, it
does not satisfy a requirement that follows implicitly
from the adjunct rule: that corresponding source and
target adjunct governors must belong to the same
translation unit. In our example, there are two ways
of rectifying the problem: we can (a) correct the de-
pendency analysis as shown in Figure 2, or (b) cor-
rect the word alignment as shown in Figure 5.
It can be shown that our translation model trans-
lates into the following four requirements on paral-
lel analyses ? ie, the requirements are necessary
and sufficient for ensuring that the linguistic anno-
tations are compatible with our translation model.
In the following, two words are said to be coaligned
if they belong to the same alignment edge. A de-
pendency edge d r??g is called internal if d and g
are coaligned, and external otherwise. A word w is
called singular if it fails to be coaligned with at least
one word in the other language.
Requirement I. The internal dependencies within
a translation unit must form two connected trees. Ie,
3As with adjunct rules, addition/deletion rules obviously
overgenerate, and must be augmented with probabilistic mod-
els that keep track of the precise characteristics of the adjunct
subtrees that are added to or deleted from the parallel analyses.
72
 X
X
skal
must
kun
only
koncentrere
concentrate
sig
self
om
about
Y
Y


subj mod vobj dobj pobj nobj
X has to concentrate only on Y
subj dobj vobj pobjmod nobj
Figure 5: Making the analysis from Figure 4 com-
patible with our translation model by changing the
alignment edges.
in an alignment W ?W ?, the internal dependencies
within W must form a connected source tree, and
similarly for W ?.
Requirement II. The external dependencies be-
tween translation units must form an acyclic graph.
Ie, in an alignment W ?W ?, no word w ?W can be
coaligned with an external transitive dependent of
any word in W ?, and vice versa.
Requirement III. Parallel external governors must
be aligned to each other. Ie, if two nodes n,n? are
coaligned with external governor edges n r??g and
n? r???g?, then g and g? must be coaligned.
Requirement IV. The graph contains no singular
external complements. If the source word c is a com-
plement of governor g and c is not coaligned to any
target word, then c and g must be coaligned to each
other; and similarly for target complements.
A graph that satisfies all four requirements is said
to be well-formed with respect to our translation
model. It can be shown that we can always trans-
form an ill-formed graph G into a well-formed graph
G? by merging alignment edges; G? is then called a
reduction of G, and a reduction with a minimal num-
ber of mergings is called a minimal reduction of G.
In a well-formed graph, we will refer to an align-
ment edge and its associated source and target de-
pendency tree as a translation unit.
It can be shown that minimal reductions can be
computed by means of the algorithm shown in Fig-
ure 6.4 The body of the for-loop in Figure 6 ensures
4In the algorithm, G is viewed as a directed graph that con-
tains the source and target dependencies, and alignment edges
procedure minimal-reduction(graph G)
merge each alignment edge in G with itself
(ie, ensure int. connectedness & ext. acyclicity)
for each W ?W ? in bottom-up order do
merge W ?W ? with all of its external
singular complements
merge all external governors of W ?W ?
return the modified graph G
Figure 6: Algorithm for computing the minimal re-
duction of a graph G.
Requirements III (coaligned external governors) and
IV (no singular complements), and the merging op-
eration is designed so that it ensures Requirements I
(internal connectedness) and II (acyclicity).5
The ill-formed analysis in Figure 4 has the mini-
mal reduction shown in Figure 2, whereas the anal-
yses in Figure 2 and 5 are well-formed, ie, they are
their own minimal reductions. In the remainder of
the paper, we will describe how minimal reductions
and translation units can be used for extracting trans-
fer rules, detecting annotation errors, and comparing
different annotation schemes with each other.
4 Extracting transfer rules and
quantifying parallelism
The complement, adjunct, and addition/deletion
rules in our simple dependency transfer model can
be read off directly from the minimal reductions.
Figure 7 shows the three complement rules induced
from Figure 4 via the minimal reduction in Figure
5. Figure 8 (repeated from Figure 3) shows the six
complement rules induced from the alternative anal-
ysis in Figure 2.
We have tested the extraction procedure on a
large scale by applying it to the 100,000 word
Copenhagen Danish-English Dependency Treebank
(Buch-Kromann et al, 2007). Figure 9 shows the
percentage of translation units with size at least n
W ?W ? are viewed as short-hands for the set of all bidirectional
edges that link two distinct nodes in W ?W ?.
5The merging operation performs three steps: (a) replace
two alignment edges W1?W ?1 and W2?W ?2 with their unionW ?W ? where W = W1?W2 and W ? = W ?1?W ?2; (b) mergeW ?W ? with the smallest set of nodes that turns W and W ? into
connected dependency trees; (c) merge W ?W ? with all nodes
on cycles that involve at least one node from W ?W ?.
73
 X
X
x1
x1
skal
must
koncentrere
concentrate
sig
self
om
about
x2
x2
  Y
Y


subj vobj dobj pobj nobj
X   x1? has to concentrate on x2?        Y
subj dobj vobj pobj nobj
Figure 7: The three complement rules induced from
Figure 4 via the minimal reduction in Figure 5.
 
X
X
x1
x1
skal
must
x2
x2
koncentrere
concentrate
sig
self
x1
x1
kun
only
om
about
x1
x1
Y
Y


subj vobj dobj pobj nobj
X x1? has to x2? concentrate x1? only on x1? Y
subj dobj vobj pobj nobj
Figure 8: The six complement rules induced from
the minimal reduction in Figure 2 (repeated from
Figure 3).
translation unit size n
percent
tunits with
size ? n
normal scale
(solid line)
logarithmic scale
(dotted line)
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
2 10 20 30 40 50
100%
10%
1%
0.1%
0.01%
0.001%
Figure 9: The percentage of translation units in the
Copenhagen Danish-English Dependency Treebank
with size at least n, plotted on normal and logarith-
mic scales.
in the parallel treebank, where the size of a transla-
tion unit is measured as the number of nodes in the
associated complement transfer rule. The extracted
transfer rules are useful for many purposes, includ-
ing machine translation, lexicography, contrastive
linguistics, and translation studies, but describing
these applications is outside the scope of this paper.
5 Spotting annotation errors
To the human annotator who must check the word-
aligned dependency analyses in a parallel depen-
dency treebank, the analyses in Figure 2 and Fig-
ure 4 look almost identical. However, from the in-
duced translation units and the associated comple-
ment rules shown above, it would have been im-
mediately obvious to the annotator that the analy-
sis in Figure 2 is significantly better than the analy-
sis in Figure 4. This suggests that we can increase
the quality of the human annotation in parallel tree-
bank projects by designing annotation tools that con-
tinuously compute the induced translation units and
present them visibly to the human annotator.
From a linguistic point of view, it can be expected
that errors in the dependency annotation will often
show up as non-parallelism that results in a large
induced translation unit. So in a parallel depen-
dency treebank, we can identify the most egregious
examples of non-parallelism errors automatically by
computing the induced translation units, and sorting
them with respect to their size; the largest translation
units will then have a high probability of being the
result of annotation errors.
To confirm our linguistic expectation that large
translation units are often caused by annotation er-
rors, we have selected a sample of 75 translation
units from the Copenhagen Danish-English Depen-
dency Treebank, distributed more or less uniformly
with respect to translation unit size in order to ensure
that all translation unit sizes are sampled evenly. We
have then hand-checked each translation unit care-
fully in order to determine whether the translation
unit contains any annotation errors or not, giving us
a data set of the form (C,N) where N is the size
of the translation unit and C indicates whether the
translation unit is correct (C = 1) or not (C = 0).
Figure 10 shows our maximum likelihood estimate
of the conditional probability P(C = 1|N = n) that
a translation unit with size n is correct.6 From the
6In order to estimate the conditional probability p(n) =
P(C = 1|S = n) that a translation unit with size n is correct, we
have fitted p(n) to the parametric family p(n) = ?n? by means
of conditional maximum likelihood estimation with conditional
likelihood L = ?75i=1 p(ni)ci(1? p(ni))1?ci . The resulting esti-
74
translation unit size n
est. percent
correct tunits
with size = n
normal scale
(solid line)
logarithmic scale
(dotted line)
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
2 10 20 30 40 50
100%
10%
1%
0.1%
0.01%
0.001%
Figure 10: The estimated percentage of translation
units with size n that are correct, plotted on normal
and logarithmic scales.
graph, we see that the correctness rate decreases
quickly with n. For example, only 55% of all trans-
lation units with size 10 are correct, and only 13% of
all translation units with size 20 are correct. Thus,
the statistics confirm that large translation units are
often caused by annotation errors in the treebank,
so focusing the effort on large translation units can
make the postediting more cost-efficient. This also
suggests that when developing algorithms for auto-
matic annotation of parallel dependency treebanks,
the algorithms can improve their accuracy by penal-
izing large translation units.
6 Comparing annotation schemes
Translation units can also be used to compare dif-
ferent annotation schemes. This is relevant in par-
allel treebank projects where there are several pos-
sible annotation schemes for one of the languages
? eg, because there is more than one treebank or
rule-based parser for that language. In this situa-
tion, we have the freedom of choosing the anno-
tation schemes for the source and target languages
so that they maximize the parallelism between the
source and target language annotations. To make an
informed choice, we can create a small pilot parallel
treebank for each annotation scheme, and compare
mates are ?? = 0.99 and ?? = 1.77 with confidence value 0.87, ie,
if a data set D with the same translation unit sizes is generated
randomly from the distribution p?(n) = ??n?? , then the conditional
likelihood of D will be larger than the likelihood of our observed
data set in 87% of the cases. This means that a two-sided test
does not reject that the data are generated from the estimated
distribution p?(n).
the treebank annotations qualitatively by looking at
their induced translation units, and quantitatively by
looking at their average translation unit size. The
best choice of annotation schemes is then the com-
bination that leads to the smallest and most sensible
translation units.
Since texts are always trivially word-aligned with
themselves, the same procedure applies to monolin-
gual corpora where we want to compare two differ-
ent dependency annotations with each other. In this
setup, structural differences between the two mono-
lingual annotation schemes will show up as large
translation units. While these structural differences
between annotation schemes could have been re-
vealed by careful manual inspection, the automatic
computation of translation units speeds up the pro-
cess of identifying the differences. The method also
suggests that the conversion from one annotation
scheme to another can be viewed as a machine trans-
lation problem ? that is, if we can create a machine
translation algorithm that learns to translate from
one language to another on the basis of a parallel
dependency treebank, then this algorithm can also
be used to convert from one dependency annotation
scheme to another, given a training corpus that has
been annotated with both annotation schemes.
7 Conclusion
In this paper, we have addressed the problem that
the linguistic annotations in parallel treebanks often
fail to correspond to meaningful translation units,
because of internal incompatibilities between the de-
pendency analyses and the word alignment. We have
defined a meaningful notion of translation units and
provided an algorithm for computing these transla-
tion units from any parallel dependency treebank.
Finally, we have sketched how our notion of trans-
lation units can be used to aid the creation of par-
allel dependency treebanks by using the translation
units as a visual aid for the human annotator, by us-
ing translation unit sizes to identify likely annota-
tion errors, and by allowing a quantitative and qual-
itative comparison of different annotation schemes,
both for parallel and monolingual treebanks.
75
8 Acknowledgments
The work was supported by two grants from
the Danish Research Council for the Humanities.
Thanks to the anonymous reviewers for their help-
ful comments.
References
Matthias Buch-Kromann, Ju?rgen Wedekind, and Jakob
Elming. 2007. The Copenhagen Danish-English De-
pendency Treebank. http://www.id.cbs.dk/?mbk/ddt-
en.
Matthias Buch-Kromann. 2006. Discontinuous
Grammar. A dependency-based model of human
parsing and language learning. Dr.ling.merc.
dissertation, Copenhagen Business School.
http://www.id.cbs.dk/?mbk/thesis.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on Multilingual Dependency Parsing. In
Proc. CoNLL-2006.
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith, and
A. Way. 2004. Long-distance dependency resolution
in automatically acquired wide-coverage PCFG-based
LFG approximations. In Proc. of ACL-2004.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Martin C?mejrek, Jan Cur???n, Jir??? Havelka, Jan Hajic?, and
Vladislav Kubon?. 2004. Prague Czech-English De-
pendency Treebank. Syntactically annotated resources
for machine translation. In Proc. LREC-2004.
Lea Cyrus. 2006. Building a resource for studying trans-
lation shifts. In Proc. LREC-2006.
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using Probabilistic Synchronous Dependency In-
sertion Grammars. In Proc. ACL-2005.
Yuan Ding. 2006. Machine translation using Prob-
abilistic Synchronous Dependency Insertion Gram-
mars. Ph.D. thesis, Univ. of Pennsylvania.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
HLT/NAACL-2004.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, and Martha
Palmer. 2002. Development and evaluation of a Ko-
rean treebank and its application to NLP. In Proc.
LREC-2002.
Silvia Hansen-Schirra, Stella Neumann, and Mihaela
Vela. 2006. Multi-dimensional annotation and align-
ment in an English-German translation corpus. In
Proc. NLPXML-2006.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT/NAACL-2003.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language phrases. In
Proc. EMNLP-2006.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. ACL-2005.
Yvonne Samuelsson and Martin Volk. 2006. Phrase
alignment in parallel treebanks. In Proc. TLT-2006.
K. Uchimoto, Y. Zhang, K. Sudo, M. Murata, S. Sekine,
and H. Isahara. 2004. Multilingual aligned parallel
treebank corpus reflecting contextual information and
its applications. In Proc. MLR-2004.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. ACL-2001.
76
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 127?131,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The unified annotation of syntax and discourse
in the Copenhagen Dependency Treebanks
Matthias Buch-Kromann I?rn Korzen
Center for Research and Innovation in Translation and Translation Technology
Copenhagen Business School
Abstract
We propose a unified model of syntax and dis-
course in which text structure is viewed as a 
tree structure augmented with anaphoric rela-
tions  and  other  secondary  relations.  We  de-
scribe how the model accounts  for discourse 
connectives  and  the  syntax-discourse-seman-
tics interface. Our model is dependency-based, 
ie, words are the basic building blocks in our 
analyses.  The  analyses  have  been  applied 
cross-linguistically in the Copenhagen Depen-
dency  Treebanks,  a  set  of  parallel  treebanks 
for Danish, English, German, Italian, and Spa-
nish which are currently being annotated with 
respect  to  discourse,  anaphora,  syntax,  mor-
phology, and translational equivalence. 
1 Introduction
The Copenhagen Dependency Treebanks, CDT, 
consist of five parallel open-source treebanks for 
Danish, English, German, Italian, and Spanish.1 
The treebanks are  annotated manually  with re-
spect  to  syntax,  discourse,  anaphora,  morpho-
logy, as well as translational equivalence (word 
alignment) between the Danish source text and 
the target texts in the four other languages. 
The treebanks build on the syntactic annota-
tion  in  the  100,000-word  Danish  Dependency 
Treebank  (Kromann  2003)  and  Danish-English 
Parallel  Dependency  Treebank  (Buch-Kromann 
et al 2007). Compared to these treebanks, which 
are  only  annotated  for  syntax  and  word  align-
ment,  the new treebanks are also annotated for 
discourse,  anaphora,  and  morphology,  and  the 
syntax annotation has been revised with a much 
more fine-grained set of adverbial relations and a 
number  of  other  adjustments.  The  underlying 
Danish  PAROLE  text  corpus  (Keson  and 
Norling-Christensen  1998)  consists  of  a  broad 
mixture of 200-250 word excerpts from general-
purpose texts.2 The texts were translated into the 
1The treebanks, the annotation manual, and the relation hier-
archy can be downloaded from the web site:
http://code.google.com/p/copenhagen-dependency-treebank
2In practice, the use of text excerpts has not been a problem 
for our discourse annotation: we mainly annotate text ex-
other languages by professional translators who 
had the target language as their native language.
The final treebanks are planned to consist of 
approximately 480 fully annotated parallel texts 
for Danish and English, and a subset of approx-
imately  300  fully  annotated  parallel  texts  for 
German, Italian, and Spanish, with a total of ap-
proximately 380,000 (2?100,000 + 3?60,000) an-
notated word or punctuation tokens  in  the five 
treebanks  in  total.  So  far,  the  annotators  have 
made complete draft annotations for 67% of the 
texts for syntax, 40% for word alignments, 11% 
for discourse and anaphora, and 3% for morpho-
logy. The annotation will be completed in 2010.
In this paper, we focus on how the CDT tree-
banks are annotated with respect  to syntax and 
discourse,  and largely ignore  the  annotation  of 
anaphora, morphology, and word alignments. In 
sections 2 and 3, we present the syntax and dis-
course annotation in the CDT. In section 4, we 
present our account of discourse connectives. In 
section 5, we briefly discuss the syntax-discour-
se-semantics  interface,  and  some  criticisms 
against tree-based theories of discourse. 
2 The syntax annotation of the CDT
The syntactic annotation of the CDT treebanks is 
based on the linguistic principles outlined in the 
dependency  theory  Discontinuous  Grammar 
(Buch-Kromann 2006) and the syntactic annota-
tion  principles  described  in  Kromann  (2003), 
Buch-Kromann et al (2007), and Buch-Kromann 
et al(2009). All linguistic relations are represen-
ted as  directed labelled relations between words 
or morphemes. The model operates with a prima-
ry dependency tree structure in which each word 
or morpheme is assumed to act as a complement 
or adjunct to another word or morpheme, called 
the  governor (or  head), except for the top node 
cerpts  that  have a  coherent discourse structure,  which in-
cludes 80% of the excerpts in our text corpus.  Moreover, 
given the upper limit on the corpus size that we can afford 
to annotate, small text excerpts allow our corpus to have a 
diversity in text type and genre that may well offset the the-
oretical disadvantage of working with reduced texts.
127
of the sentence or unit, typically the finite verb. 
This structure is augmented with secondary rela-
tions,  e.g.,  between  non-finite  verb  forms  and 
their  subjects,  and  in  antecedent-anaphor  rela-
tions.  Primary  relations  are  drawn  above  the 
nodes and secondary below, all with directed ar-
rows pointing from governor to dependent. The 
relation label is written at the arrow tip, or in the 
middle of the arrow if a word has more than one 
incoming arrow. 
Figure 1. Primary dependency tree (top) and sec-
ondary relations (bottom) for the sentence ?It had 
forced her to give up all she had worked for?.
An example is given in Figure 1 above. Here, the 
arrow from ?had2? to ?It1? identifies ?It? as the 
subject of ?had?, and the arrow from ?forced3? to 
?to5? identifies the phrase headed by ?to? as the 
prepositional object of ?forced?. Every word de-
fines a unique phrase consisting of the words that 
can be reached from the head word by following 
the downward arrows in the primary tree.3 For 
example,  in  Figure  1,  ?worked11?  heads  the 
phrase ?worked11 for12?, which has a secondary 
noun  object  nobj  in  ?all8?;  ?had10?  heads  the 
phrase  ?she9 had10 worked11 for12?;  and  ?It1? 
heads  the  phrase  ?It1?.  Examples  of  secondary 
dependencies  include  the  coreferential  relation 
between ?her4? and ?she9?, and the anaphoric re-
lation in Figure 2.  Part-of-speech functions are 
written in capital letters under each word. The in-
ventory of relations is described in detail in our 
annotation manual (posted on the CDT web site).
Dependency arrows are  allowed to  cross,  so 
discontinuous word orders such as topicalisations 
and  extrapositions  do  not  require  special  treat-
ment.  This is  exemplified by the discontinuous 
dependency tree in Figure 2, in which the relat-
ive clause headed by ?was7? has been extraposed 
from the direct object and placed after the time 
adverbial ?today5?.4
3Because  of  this  isomorphism between  phrases  and  head 
words,  a dependency tree can always be represented as a 
phrase-structure  tree  in  which every phrase  has  a  unique 
lexical head; the resulting phrase-structure tree is allowed to 
contain crossing branches.
4In our current syntax annotation, we analyze the initial con-
nective or conjunction as the head of the subordinate clause; 
Figure 2. Primary dependency tree and second-
ary relations for the sentence ?We discussed a 
book today which was written by Chomsky?. 
Buch-Kromann (2006) provides a detailed theory 
of how the dependency structure can be used to 
construct a word-order structure which provides 
fine-grained control over the linear order of the 
sentence,  and  how  the  dependency  structure 
provides an interface to compositional semantics 
by determining a unique functor-argument struc-
ture given a particular modifier scope (ie, a spe-
cification of the order in which the adjuncts are 
applied in the meaning construction).5 
3 The discourse annotation of the CDT 
Just like sentence structures can be seen as de-
pendency structures that link up the words and 
morphemes  within  a  sentence  (or,  more  preci-
sely, the phrases headed by these words), so dis-
course structures can be viewed as dependency 
structures that link up the words and morphemes 
within an entire discourse. In Figures 1 and 2, the 
top  nodes  of  the  analysed  sentences  (the  only 
words  without  incoming  arrows)  are  the  finite 
verbs  ?had2? and ?discussed2? respectively, and 
these are shown in boldface. Basically, the CDT 
discourse annotation consists in linking up each 
such sentence top node with its nucleus (under-
stood as the unique word within another sentence 
that  is  deemed  to  govern  the  relation)  and  la-
belling the relations between the two nodes. 
The inventory of discourse relations in CDT is 
described in the CDT manual. It borrows heavily 
from other  discourse  frameworks,  in  particular 
Rhetorical  Structure  Theory,  RST  (Mann  and 
Thompson,  1987;  Tabaoda  and  Mann,  2006; 
Carlson  et  al,  2001)  and  the  Penn  Discourse 
Treebank,  PDTB  (Webber  2004;  Dinesh  et  al., 
2005,  Prasad  et  al.,  2007,  2008),  as  well  as 
(Korzen,  2006,  2007),  although  the  inventory 
had  to  be  extended  to  accommodate  the  great 
in relative clauses, the relative verb functions as the head, 
i.e., the arrow goes from ?a (book)? to ?was (written)?.
5In terms of their formal semantics, complements function 
as arguments to their governor, whereas adjuncts function as 
modifiers; i.e., semantically, the governor (type X) acts as 
an argument with the modifier (type X/X) as its functor.
128
variety  of  text  types  in  the  CDT corpus  other 
than news stories. The inventory allows relation 
names to be formed as disjunctions or conjunc-
tions of  simple relation names,  to specify mul-
tiple relations or ambiguous alternatives. 
One  of  the  most  important  differences  be-
tween the CDT framework and other discourse 
frameworks lies in the way texts are segmented. 
In particular, CDT uses words as the basic build-
ing blocks in the discourse structure, while most 
other discourse frameworks use clauses as their 
atomic  discourse  units,  including  RST,  PDTB, 
GraphBank  (Wolf  and  Gibson,  2005),  and  the 
Pottsdam  Commentary  Corpus,  PCC  (Stede 
2009).6 This allows the nucleus and satellite in a 
discourse  relation  to  be  identified  precisely  by 
means of their head words, as in the example (1) 
below from the CDT corpus, where the second 
paragraph is analyzed as an elaboration of the de-
verbal noun phrase ?their judgment? (words that 
are included in our condensed CDT analysis in 
Figure  4  are  indicated  with  boldface  and  sub-
scripted with numbers that identify them):
6As noted by Carlson and Marcu (2001), the boundary be-
tween  syntax  and  discourse  is  rather  unclear:  the  same 
meaning can be expressed in a continuum of ways that ran-
ge from clear discourse constructions (?He laughed.  That 
annoyed me.?) to clear syntactic constructions (?His laugh 
annoyed me.?). Moreover, long discourse units may func-
tion  as  objects  of  attribution  verbs  in  direct  or  indirect 
speech, or as parenthetical remarks embedded within an oth-
erwise normal sentence. CDT's use of words as basic build-
ing blocks,  along with a primary tree structure that  spans 
syntax and discourse, largely eliminates these problems.
(1) Two convicted executives of the July 6 Bank ap?
pealed1 their2 judgment  on  the  spot  from  the 
Copenhagen Municipal Court with a demand for 
acquittal. The prosecuting authority has3 also re-
served the possibility of appeal.
The chairman of the board received4 a year in 
jail and a fine of DKK one million for fraudulent 
abuse of authority [?]. The bank?s director  re?
ceived5 6  months  in  jail  and  a  fine of  DKK 
90,000. (Text 0531)
The full CDT analysis of (1) is given in Figure 3, 
a more readable condensed version in Figure 4. 
The  last  sentence  of  the  first  paragraph,  ?The 
prosecuting authority has3 also reserved the pos-
sibility of appeal?, is a conjunct to the first sen-
tence, and its top node ?has3? is linked to the top 
node of the first sentence, ?appealed1?. The slash 
after a relation name indicates an explicit or im-
plicit discourse connective used by the annotat-
ors to support their choice of relation type. 
As  in  CDT's  syntax annotation,  the  primary 
syntax and discourse relations must form a tree 
that spans all the words in the text, possibly sup-
plemented  by  secondary  relations  that  encode 
anaphoric relations and other secondary depen-
dencies. Apart from this, CDT does not place any 
restrictions on the relations; in particular, a word 
Figure 4. Condensed version of Figure 3.
Figure 3. The full CDT analysis of (1) wrt. syntax, discourse, and anaphora.  
129
may  function  as  nucleus  for  several  different 
satellites, discourse relations may join non-adja-
cent clauses, and are allowed to cross; and sec-
ondary  discourse  relations  are  used  to  account 
for the distinction between story line level  and 
speech level in attributions. 
4 Discourse connectives
Discourse connectives play a prominent role in 
PDTB, and inspire the analysis of connectives in 
CDT. However, there are important  differences 
in analysis, which affect the way discourse struc-
tures are construed. In a construction of the form 
?X C Y? where  X and  Y are clauses and  C is a 
discourse  connective  (such  as  ?because?,  ?sin-
ce?, ?when?), three dependency analyses suggest 
themselves, as summarized in Table 5.
Head Conjunction Marker
Syntax
Semantics C'(X',Y') [C'(Y')] (X') [Y'(C')] (X')
Table 5. Three analyses of discourse connectives.
When analyzed as the head of the construction, 
C takes X and Y as its (discourse) complements; 
semantically, the meaning  C'  of  C acts as func-
tor,  and the meanings  X',Y'  of  X,Y act as argu-
ments of  C'.  When analyzed as a subordinating 
conjunction, C subcategorizes for Y and modifies 
X;  semantically,  C'  computes  a  meaning  C'(Y') 
from  Y',  which acts as functor with  X'  as argu-
ment. Finally, analyzed as a marker,  C modifies 
Y which in turn modifies  X; semantically,  Y' se-
lects its meaning  Y'(C')  based on the marker  C' 
(i.e., the marker merely helps disambiguate  Y'); 
Y'(C') then acts as functor with argument X'. 
The three  analyses  are  markedly different  in 
terms  of  their  headedness,  but  quite  similar  in 
terms of their semantics. CDT opts for the mark-
er analysis, with the obvious benefit that there is 
no need to postulate the presence of a phonetic-
ally  empty  head  for  implicit  connectives.  This 
analysis also implies that since discourse markers 
always modify the satellite, explicit and implicit 
discourse markers can be used to determine the 
discourse relation and its direction. 
It is interesting that almost all theories of dis-
course structure, including RST, PDTB, Graph-
Bank, PCC, and the dependency-based discourse 
analysis  proposed by Mladov? (2008),  seem to 
analyze connectives as heads  ? even in the case 
where  C+Y is an adverbial clause modifying X, 
where virtually all mainstream theories of syntax 
opt  for  one of  the  two other  analyses.  Perhaps 
current  theories  of  discourse  structure  perceive 
discourse structure as a semantic rather than syn-
tactic structure. In any case, it  is not clear that 
this is the most fruitful analysis. A clear distinc-
tion  between  syntactic  structure  and  semantic 
structure has proved crucial to the understanding 
of headedness in syntax (e.g. Croft 1995, Man-
ning 1995), and it is one of the hardwon insights 
of syntax that semantic centrality or prominence 
is not directly reflected in the syntactic surface 
structure.  Something  similar  might  be  true  for 
discourse structure as well.
5 Syntax?discourse?semantics interface
CDT models discourse structure as a primary de-
pendency tree supplemented by secondary rela-
tions. We believe that a tree-based view of dis-
course  provides  many important  benefits,  most 
importantly a clear interface to syntax and com-
positional semantics. There has been several at-
tempts to refute the tree hypothesis on empirical 
grounds,  though,  including  Wolf  and  Gibson 
(2005), Prasad et al(2005), Lee et al(2008), and 
Stede (2009),  who have  put  forward important 
criticisms.  Our  framework  addresses  many  of 
these  objections,  including  the  many  problems 
related to attribution verbs,  which do require a 
complicated  treatment  in  our  framework  with 
secondary dependencies. A full discussion of this 
topic is, however, beyond the scope of this paper.
6 Conclusion
In this paper, we have presented  a dependency-
based view of  discourse  and syntax annotation 
where the syntax and discourse relations in a text 
form a primary dependency tree structure linking 
all the words in the text, supplemented by ana-
phoric relations and other secondary dependen-
cies. The framework forms the basis for the an-
notation  of  syntax,  discourse,  and  anaphora  in 
the Copenhagen Dependency Treebanks.  In fu-
ture  papers,  we will  address  some of  the criti-
cisms  that  have  been  raised  against  tree-based 
theories of discourse. 
7 Acknowledgments
This  work  was  supported  by  a  grant  from the 
Danish  Research  Council  for  the  Humanities. 
Thanks to Bonnie Webber, Henrik H?eg M?ller, 
Per Anker Jensen, Peter Colliander, and our three 
reviewers for their valuable commments. 
130
References 
Matthias Buch-Kromann 2006.  Discontinuous Gram?
mar. A dependency?based model of human parsing 
and language learning. Copenhagen: Copenhagen 
Business School.
Matthias  Buch-Kromann,  I?rn  Korzen,  and  Henrik 
H?eg M?ller. 2009. Uncovering the ?lost? structure 
of  translations  with  parallel  treebanks. Copen?
hagen Studies in Language 38: 199-224.
Matthias  Buch-Kromann,  J?rgen  Wedekind,  and 
Jakob  Elming.  2007.  The  Copenhagen  Danish-
English Dependency  Treebank v.  2.0.  http://code.-
google.com/p/copenhagen-dependency-treebank
Lynn  Carlson  and  Daniel  Marcu.  2001.  Discourse 
Tagging Reference Manual.  ISI Technical  Report 
ISI-TR-545.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okur-
owski. 2001. Building a Discourse-Tagged Corpus 
in the Framework of Rhetorical Structure Theory. 
In  Proc.  of  the  2nd  SIGdial  Workshop  on  Dis?
course  and  Dialogue.  Association  for  Computa-
tional Linguistics: 1-10.
William Croft.  1995. What's  a  head?  In J.  Rooryck 
and L. Zaring (eds.). Phrase Structure and the Lex?
icon. Kluwer. 
Nikhil  Dinesh,  Alan  Lee, Eleni  Miltsakaki,  Rashmi 
Prasad, Aravind Joshi, and Bonnie Webber. 2005. 
Attribution and the (Non-)Alignment of Syntactic 
and Discourse Arguments of Connectives. Proc. of  
the Workshop on Frontiers in Corpus Annotation 
II: Pie in the Sky, pp. 29-36.
Britt  Keson and Ole Norling-Christensen. 1998. PA-
ROLE-DK. The Danish Society for Language and 
Literature.
I?rn Korzen. 2006. Endocentric and Exocentric Lan-
guages  in  Translation.  Perspectives:  Studies  in  
Translatology, 13 (1): 21-37.
I?rn Korzen. 2007. Linguistic typology, text structure 
and appositions. In I. Korzen, M. Lambert, H. Vas-
siliadou. Langues d?Europe, l?Europe des langues.  
Croisements linguistiques. Scolia 22: 21-42.
Matthias T. Kromann. 2003. The Danish Dependency 
Treebank and the DTAG treebank tool. In Proc. of  
Treebanks and Linguistic Theories (TLT 2003), 14?
15 November, V?xj?. 217?220.
Alan Lee, Rashmi Prasad, Aravind Joshi, and Bonnie 
Webber 2008. Departures from Tree Structures in 
Discourse:  Shared  Arguments  in  the  Penn  Dis-
course Treebank. Proceedings of the Constraints in  
Discourse III Workshop.
Willliam C.  Mann and  Sandra  A.  Thompson 1987. 
Rhetorical Structure Theory. A Theory of Text  
Organization. ISI: Information Sciences Institute, 
Los Angeles, CA, ISI/RS-87-190, 1-81. 
Christopher D. Manning. 1995. Dissociating functor-
argument  structure from surface  phrase  structure: 
the relationship of HPSG Order Domains to LFG. 
Ms., Carnegie Mellon University.
Lucie Mladov?,  ?arka Zik?nov?, and Eva Haji?ov?. 
2008.  From  Sentence  to  Discourse:  Building  an 
Annotation Scheme for Discourse Based on Prague 
Dependency Treebank. In  Proc. 6th International  
Conference  on  Language Resources  and  Evalua?
tion (LREC 2008).
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan 
Lee,  Aravind  Joshi,  Livio  Robaldo,  and  Bonnie 
Webber. 2007. The Penn Discourse TreeBank 2.0. 
Annotation  Manual.  The  PDTB Research  Group. 
http://  www.seas.upenn.edu/~pdtb/PDTBAPI/pdtb-anno  - 
tation-manual.pdf
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie 
Webber. 2008. The Penn Discourse TreeBank 2.0. 
In Proc. 6th Int. Conf. on Language Resources and 
Evaluation, Marrakech, Morocco.
Manfred  Stede.  2008.  Disambiguating  Rhetorical 
Structure.  Research  on  Language  and  Computa?
tion (6), pp. 311-332.. 
Maite Taboada and William C. Mann. 2006a. Rhetori-
cal  Structure  Theory:  looking  back  and  moving 
ahead. Discourse Studies 8/3/423.
Maite Taboada and William C. Mann. 2006b. Appli-
cations of  Rhetorical Structure Theory.  Discourse 
Studies 8/4/567. http://dis.sagepub.com
Bonnie  Webber.  2004.  D-LTAG:  extending  lexical-
ized TAG to discourse. Cognitive Science 28: 751-
779.
Bonnie Webber. 2006. Accounting for Discourse Re-
lation: Constituency and Dependency. M. Dalrym-
ple (ed.).  Festschrift for Ron Kaplan. CSLI Publi-
cations.
Florian Wolf and Edward Gibson 2005. Representing 
Discourse  Coherence:  A  Corpus-Based  Study. 
Computational Linguistics 31(2), 249-287.
131
