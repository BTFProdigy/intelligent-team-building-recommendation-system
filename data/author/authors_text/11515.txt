Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 70?73,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Presupposed Content and Entailments in Natural Language Inference 
David Clausen Department of Linguistics Stanford University clausend@stanford.edu 
Christopher D. Manning Departments of Computer Science and Linguistics Stanford University manning@cs.stanford.edu  Abstract 
Previous work has presented an accurate natural logic model for natural language in-ference.  Other work has demonstrated the ef-fectiveness of computing presuppositions for solving natural language inference problems.  We extend this work to create a system for correctly computing lexical presuppositions and their interactions within the natural logic framework.  The combination allows our sys-tem to properly handle presupposition projec-tion from the lexical to the sentential level while taking advantage of the accuracy and coverage of the natural logic system.  To solve an inference problem, our system com-putes a sequence of edits from premise to hy-pothesis.  For each edit the system computes an entailment relation and a presupposition entailment relation.  The relations are then separately composed according to a syntactic tree and the semantic properties of its nodes.  Presuppositions are projected based on the properties of their syntactic and semantic en-vironment.  The edits are then composed and the resulting entailment relations are com-bined with the presupposition relation to yield an answer to the inference problem.   1 Introduction Various approaches to the task of Natural Lan-guage Inference (NLI) have demonstrated dis-tinct areas of expertise.  Systems based on full semantic interpretation in first order logic are highly accurate but lack broad coverage, requir-ing large amounts of background knowledge to do open-domain NLI (Bos and Markert, 2006).  Other systems based on statistical classifiers and machine learning achieve broad coverage but sacrifice accuracy by using shallow semantic representations (MacCartney et al, 2006).  Natu-ral logic was developed as a compromise be-tween these two extremes (MacCartney and Manning, 2009).  It makes use of rich semantic features while using syntactic representations closely related to the natural language surface strings to achieve broad coverage. Other work 
has demonstrated the effectiveness of lexically triggered inferences and presuppositions to the task of natural language entailment and contra-diction detection (Nairn et al 2006; Hickl et al, 2006).   The natural logic model attempted to inte-grate these insights but recognized the difficulty of treating presuppositions within their current framework.  Natural logic models negation, monotonicity, lexical relations and implicatures together as part of a sentence?s asserted content allowing them to be treated through a single pro-jection mechanism. Presuppositions notoriously do not interact with these features although they do interact with other semantic features requiring a separate projection mechanism.  We present a model for presupposition detection and computa-tion separate from asserted content.  We extend the natural logic model to compute lexically trig-gered presuppositions covered by Nairn et al  We then integrate this information to produce improved coverage for the NLI task.   2 Presuppositions Presuppositions are propositions that are taken to be true as a prerequisite for uttering a sentence.  The set of phenomena often grouped as presup-positions are diverse, although they are fre-quently systematically related to certain lexical items in a sentence, in which case they are said to be lexically triggered.  Lexically triggered pre-suppositions like (1c) from (1a) can be used by an NLI system to expand the information avail-able for solving a particular problem without full semantic interpretation.  (1a) Bush knew that Gore won the election. (1b) Bush did not know that Gore won the election. (1c) Gore won the election. (1d) If Gore won the election, Bush knew that Gore won the election.      In (1a) the factive verb ?knew? triggers the lo-cal factive presupposition that the sentential complement ?Gore won the election? is true.  (1a) is a simple sentence so the sentence as a whole 
70
presupposes (1c) and we can make use of this information for an NLI problem.  A defining fea-ture of presuppositions is their invariance under negation so we have (1b) also entailing (1c).  The factive presupposition is said to project through negation to become a presupposition of the entire sentence.  In other cases such as the consequent of a conditional, the presupposition sometimes does not project so sentence (1d) does not pre-suppose (1c).  Whether or not a lexically trig-gered local presupposition becomes a presuppo-sition of the entire sentence is known as the problem of presupposition projection.   A complete treatment of the projection prob-lem for all types of presupposition triggers is outside the bounds of current NLI systems but for most purposes we can compute presupposi-tion projections based on a simple model first outlined by Karttunen (1973).  The model cate-gorizes lexical items as either filters, plugs or holes and uses these properties to determine how local presuppositions project upwards through a syntactic tree to become presuppositions of the entire sentence.  Lexical items are categorized according to their effect on presuppositions they dominate syntactically.  The verb ?realize? is a hole, and projects the presuppositions of its com-plement unchanged so (2a) has a sentential pre-supposition of (2c).  The verb ?pretend? is a plug and projects none of the presuppositions of its complement so (2b) does not entail (2c).  The conditional is a filter and will sometimes project the presuppositions of its antecedent and conse-quent based on the entailment relation that holds between the two.  In the case of (1d) the antece-dent entails the presupposition of the consequent so the presupposition of the consequent is not projected and it does not entail (1c).    (2a) Rehnquist realized Bush knew that Gore won the election (2b) Rehnquist pretended Bush knew that Gore won the election (2c) Gore won the election  The verbs ?realize? and ?pretend? represent two modest size classes of verbs and nouns called factives and antifactives.  The sentential presuppositions for any given factive or antifac-tive operator depend on its position in the sen-tence?s syntactic tree and the number and type of holes, plugs or filters that dominate it.   To implement this theory we model the local factivity presuppositions triggered by various sentential complement taking operators.  We 
then calculate the presuppositions of the entire sentence by projecting the local presuppositions according to Karttunen?s theory.  For each opera-tor our system traverses the sentence?s syntactic tree from operator node to root calculating how the local factivity presuppositions project through the various holes, plugs and filters. The result is a set of sentential level presuppositions that can be used to determine inference relations to other sentences.    3 Presupposition in NatLog The NatLog system of MacCartney and Manning (2008; 2009) is a multi-stage NLI system that decomposes the NLI task into 5 stages: (1) lin-guistic analysis, (2) alignment, (3) lexical en-tailment classification, (4) entailment projection, and (5) entailment composition.  The NatLog architecture and the theory of presupposition pro-jection outlined in section 2 reflect two parallel methods for computing entailment relations be-tween premise and hypothesis.  We augment the NatLog system at steps (1), (4) and (5) to com-pute entailment relations and presuppositions in parallel.  The result is two separate entailment relations which are combined to form an answer to an NLI problem. At stage (1) we calculate the lexically triggered factivity presuppositions for a given sentence.  At stage (4) we project the pre-suppositions to determine the effective factivity according to the theory outlined in section 2.  In stage (5) we compose the presuppositions across the alignment between premise and hypothesis to determine the presupposition entailment relation.  Finally we combine the presupposition entail-ment relation with the entailment relation gener-ated from the standard NatLog system to produce a more informed inference.   3.1 Lexical Factivity Presuppositions Lexical factivity presuppositions are detected by regular expressions over lemmatized lexical items taken from the classes of factive and anti-factive verbs and nouns.  Figure 1 gives example entries for two operators.  A sentence is analyzed for factivity operators by matching the regular expressions to the tree structure and when one is detected its terminal projection is marked as a factive operator with the appropriate factivity.  The sentential complement of the operator is marked as being in the scope of a factive opera-tor of the appropriate type.    
71
Operator: know Pattern: VP<(/^VB/</^know$/) Scope: /^SBAR|S$/ Factivity: FACT  Operator: pretend Pattern: VP<(/^VB/</^pretend$/) Scope: /^SBAR|S$/ Factivity: ANTI  Figure 1: A factive and antifactive operator  3.2 Presupposition Projection For any given constituent of a sentence we can calculate its effective factivity presupposition by determining the number and type of factivity op-erators which dominate it.  This is analogous to computing the projected presuppositions for a sentence but instead stores the information lo-cally on the representation of the sentence.  Let?s compute the factivity of ?Gore won the election? in (2b).  First we look for the immediately domi-nating factivity operator and find that it is domi-nated by the factive operator ?know? which as-signs the local factivity FACT.  We then traverse up the tree and find the operator ?pretend?, which assigns the local factivity ANTI and dominates the constituent and the operator ?know?.  We then compose the two according to table 1. to determine the effective factivity for the constitu-ent is ANTI.  If the sentence included more fac-tive or antifactive operators we would continue to calculate the effective factivity recursively using the effective factivity output at each level as the dominated input for the next level.    Dominated Dominating Effective ANTI ANTI ANTI ANTI FACT ANTI FACT ANTI ANTI FACT FACT FACT  Table 1:  The effective factivity for any pair of dominated and dominating factivity assignments.  The result tells us that the sentence in (2b) has an antifactive presupposition that ?Gore won the election?.  This is equivalent to the presupposi-tion that ?Gore did not win the election?.  This contradicts (2c) and we can conclude that (2b) does not entail (2c).  Detecting that the presup-positions of a premise are incompatible with the 
hypothesis is achieved in step (5) presupposition composition. 3.3 Presupposition Composition The NatLog model for NLI computes a sequence of atomic edits from premise to hypothesis.  The entailment relation between each atomic edit is computed and then composed across the se-quence of edits to determine the entailment rela-tion that holds between premise and hypothesis.   An atomic edit consists of an insertion (INS), deletion (DELN) or substitution (SUB) opera-tion.  To compose the presuppositions calculated in step (4) we compare the factivity presupposi-tions before and after each atomic edit.  In our simplified model the only edits that can change the factivity presuppositions are INS, DELN or SUB of factive or antifactive operators.  Using table 2 we compute an atomic presupposition entailment relation between each atomic edit based on the edit type, local factivity and effec-tive factivity.  We then compose the atomic pre-supposition entailment relations to produce the presupposition entailment relation that holds be-tween the premise and the conclusion.   Finally we combine the presupposition entailment rela-tion with the entailment relation generated by the standard NatLog architecture to yield the answer to the NLI problem.  Atomic presuppositions are computed according to table 2.  Operator DEL INS ANTI Alternation Alternation FACT Forward Reverse  Table 2: Operator effective factivity and the re-sulting atomic presupposition entailment relation for DEL and INS edits.    The sequence of atomic edits converting the premise (2b) to the hypothesis (2c) involves DEL of one antifactive operator ?pretend? and one fac-tive operator ?know?.  The first DEL of ?pretend? results in an atomic presupposition entailment relation of Alternation.  The second DEL of ?know? results in an atomic presupposition en-tailment relation of Forward, together yielding a presupposition entailment relation between the premise and hypothesis of Alternation.  This al-lows our system to correctly predict (2c) is in-compatible with and a contradiction of (2b).  
72
4 Improvements Previous implementations of the NatLog system were unable to handle NLI problems with (1b) as the premise and (1c) as the hypothesis because atomic presupposition entailment relations were treated together with normal entailment relations.  The sequence of atomic edits from (1b) to (1c) would involve the DEL of ?know? resulting in an atomic entailment relation of Forward while DEL of ?not? would result in an atomic entail-ment relation of Negation together yielding Al-ternation instead of Forward.  Our augmented system handles these types of inferences by sepa-rating presupposition entailment relations from normal entailment relations.  In our augmented system only the DEL edit of ?know? produces an atomic presupposition entailment relation of Forward.  Since no other operators in (1b) pro-duce atomic presupposition entailment relations the resulting presupposition entailment relation between (1b) and (1c) is the correct Forward en-tailment.   Evaluating on a set of 3-way entailment NLI test problems developed at PARC by the authors of (Nairn et al 2006) the Augmented NatLog system achieved an accuracy of 60.53% com-pared to the original NatLog system accuracy of 53.95% by correctly treating problems like (3) where (3b) should be inferred form (3a).  (3a) Bush didn?t realize that Afghanistan is land-locked. (3b) Afghanistan is landlocked.  With further development we expect to extend these results to other NLI test sets.   5 Conclusion Our system extends the coverage of the NatLog system to correctly handle factive presupposi-tions.  By computing entailments based on se-mantic containment and exclusion separately from those based on presupposition we avoid unwanted interaction between the two dimen-sions of meaning while leveraging the informa-tion contained in presuppositions to improve NLI performance. Although they are invariant under negation, presuppositions do not uniformly pro-ject.  Projection is determined by a myriad of complex factors which ultimately require logical formalisms much more complex than predicate logic to compute (Beaver 2001).  Our treatment does not currently take into account other types of presuppositions including those based on as-
pectual relations, (Mary has/hasn?t stopped beat-ing her boyfriend ? Mary has been beating her boyfriend), definitine descriptions, (The king of France is/isn?t bald ? There is a king of France), or iteratives, (The boy cried/didn?t cry wolf again ? The boy cried wolf before).  We have, however, provided a framework that can be extended to compute many types of lexically triggered presupposition and their projections.  This work continues the theme of MacCartney and Manning in asserting ?open-domain NLI is likely to require combining dis-parate reasoners?.  By augmenting NatLog with a reasoner based on factive presupposi-tions we take one step closer to the goal of achieving open-domain NLI.   References Beaver, David I. 2001. Presupposition and assertion in dynamic semantics. Stanford: CSLI. Bos, Johan and Katja Markert. 2006. When logical inference helps determining textual entailment (and when it doesn?t). In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment. Hickl, Andrew, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recog-nizing textual entailment with LCC?s GROUND-HOG system. In Proceedings of the Second PAS-CAL Challenges Workshop on Recognizing Textual Entailment. Karttunen, Lauri. 1973. Presuppositions of compound sentences. Linguistic Inquiry 4: 169-93. MacCartney Bill, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Man-ning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the North American Association of Computational Linguis-tics (NAACL-06). MacCartney, Bill and Christopher D. Manning. 2007. Natural logic for textual inference. In ACL-07 Workshop on Textual Entailment and Paraphras-ing, Prague. MacCartney, Bill and Christopher D. Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of Col-ing-08. MacCartney, Bill and Christopher D. Manning. 2009. An extended model of natural logic. In The Eight International Conference on Computational Se-mantics (IWCS-8), Tilburg, Netherlands, January 2009 Nairn, Rowan, Cleo Condoravdi, and Lauri Kart-tunen. 2006. Computing relative polarity for tex-tual inference. In Proceedings of ICoS-5 (Inference in Computational Semantics), Buxton, UK. 
73
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 120?125,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
HedgeHunter: A System for Hedge Detection and Uncertainty
Classification
David Clausen
Department of Linguistics
Stanford University
Stanford, CA 94305, USA.
clausend@stanford.edu
Abstract
With the dramatic growth of scientific
publishing, Information Extraction (IE)
systems are becoming an increasingly im-
portant tool for large scale data analy-
sis. Hedge detection and uncertainty clas-
sification are important components of a
high precision IE system. This paper
describes a two part supervised system
which classifies words as hedge or non-
hedged and sentences as certain or uncer-
tain in biomedical and Wikipedia data. In
the first stage, our system trains a logistic
regression classifier to detect hedges based
on lexical and Part-of-Speech collocation
features. In the second stage, we use the
output of the hedge classifier to generate
sentence level features based on the num-
ber of hedge cues, the identity of hedge
cues, and a Bag-of-Words feature vector
to train a logistic regression classifier for
sentence level uncertainty. With the result-
ing classification, an IE system can then
discard facts and relations extracted from
these sentences or treat them as appropri-
ately doubtful. We present results for in
domain training and testing and cross do-
main training and testing based on a sim-
ple union of training sets.
1 Introduction
With the rapid increase in domain specific (bio-
medical) and domain general (WWW) text collec-
tions information extraction is an increasingly im-
portant tool for making use of these data sets. In
order to maximize the usefulness of extracted rela-
tions an Information Extraction (IE) system needs
the ability to separate the factual and reliable re-
lationships from the uncertain and unreliable rela-
tionships. Most work on this problem has focused
on the task of hedge detection where the goal is
to classify a span of text as hedged or as non-
hedged with the goal of facilitating sentence level
classification of certain or uncertain. Much of the
work was conducted within the framework of the
BioNLP 2009 shared task sub task on uncertainty
detection focusing on biomedical datasets (Kim et
al., 2009) motivating further work in the biomedi-
cal NLP field (Aramaki et al, 2009; Conway et al,
2009). Other work has focused on creating anno-
tated datasets from both a linguistically sophisti-
cated perspective (Saur?? and Pustejovsky, 2009) or
from a language engineering perspective (Vincze
et al, 2008).
Early work by Light et al (2004) framed the
task as determining the degree of speculation or
uncertainty at the sentence level. The presence
of a hedge cue, a phrase indicating that authors
cannot back up their opinions or statements with
facts, is a high precision feature of sentence level
uncertainty. Other early work focused on semi-
supervised learning due to a lack of annotated
datasets (Medlock and Briscoe, 2007). Linguis-
tically motivated approaches achieved a robust
baseline on the sentence classification task (Kil-
icoglu and Bergler, 2008) although their training
methods are hand tuned. Morante and Daele-
mans (2009) cast the problem as a sequence label-
ing task and show that performance is highly do-
main dependent and requires high precision hedge
detection in order to perform the complex task
of hedge scope labeling. Szarvas (2008) demon-
strates that semi-supervised learning is even more
effective with more labeled training data and so-
phisticated feature selection.
HedgeHunter is built to perform the CoNLL-
2010 sentence uncertainty classification task. The
task is a supervised learning task with training
data drawn from Wikipedia and biomolecular ar-
ticles and abstracts. Each training sentence is la-
120
beled as certain or uncertain and every hedge cue
is also labeled. HedgeHunter separates the task
into two stages: hedge detection and uncertainty
classification, with the goal of producing an in-
dependent high precision hedge detection system
for use in other tasks such as hedge scope detec-
tion. The system is designed to be expanded using
semi-supervised learning although this is not im-
plemented at this time. This paper will describe
the hedge detection stage in Section 2 and the sen-
tence classification stage in Section 3. Section 4
describes the evaluation of the system and Section
5 discusses the results. Section 6 discusses the re-
sults in a larger context and suggest future areas
for improvement. Section 7 summarizes the con-
clusions.
2 Hedge Detection
Hedge detection is largely based on the identi-
fication of lexical items like suggest and might
which indicate sentence level uncertainty. As a
result, reasonable hedge detection in English can
be accomplished by collecting a list of all lexical
items that convey hedging. These include epis-
temic verbs (may, might, could, should, can, ought
to), psychological verbs of perception, knowing or
concluding (seems, guess, suppose, hope, assume,
speculate, estimate), adverbs (possibly, unlikely,
probably, approximately), adjectives (quite, rare,
apparent) and many nouns. While some of these,
especially the epistemic verbs, are often applied
across domains to indicate hedge cues, many are
unique to a particular domain. Further complicat-
ing hedge detection in English is the fact that the
same word types occasionally have different, non-
hedging uses.
The form of a hedge cue often acts as a high pre-
cision feature, whenever one is present in a sen-
tence it is highly likely to be labeled as a hedge
cue in the training set. Lexical hedge cues often
vary from domain to domain and contain multi-
ple words so non-lexical features are required for
recognizing hedge cues robustly across domains
although they are unlikely to provide a large bene-
fit due to the largely lexical nature of hedges. As a
result HedgeHunter uses both lexical and POS fea-
tures for classification. Some hedges like ought to
span multiple words so we also use positional fea-
tures in order to capture multi-word hedges.
The hedge detection stage labels each word in a
sentence independently. Labeling is done by lo-
gistic regression using Quasi-Newton minimiza-
tion to set feature weights. This is a classifica-
tion method that is both fast and robust for binary
classification tasks like the one at hand. Features
are drawn from the target word to be labeled and
its context, the three words to the left and right of
the target word. For the target word we extract
features based on the word form, the word lemma
and its POS as determined by a maximum entropy
POS tagger trained on the PennTreebank imple-
mented in Stanford JavaNLP. For the 6 words in
the context window we also extract features based
on the word, its lemma and its POS.
3 Uncertainty Classification
Uncertainty classification involves partitioning the
set of sentences in a dataset into certain and uncer-
tain classes. In most scientific writing sentences
are generally certain so uncertain sentences are the
minority class. This holds even more so for the
Wikipedia dataset due to the method by which an-
notations were obtained and the encyclopedic na-
ture of the dataset. Wikipedia hedge cues were
identified by the presence of the weasel word tag
which editors are allowed to append to spans of
text in a Wikipedia article. These are often applied
in a manner similar to hedge cues in the annotated
biomedical datasets but they also focus on identi-
fying non universal statements like those quanti-
fied by some or few. Due to the collaborative na-
ture of Wikipedia, what qualifies as a weasel word
varies greatly contributing to the increased varia-
tion in hedge cues in this dataset. Weasel words
often get edited quickly so there are not many ex-
amples in the training set creating further difficul-
ties.
The presence of one or more hedge cues in a
sentence is a good indication that the sentence
should be classified as uncertain, although as we
will see in the results section, non-hedge features
are also useful for this task. To capture this we
extract features from each sentence including the
number of hedge cues found by the hedge detec-
tion stage and the string value of the first four lex-
ical hedge cues found in each sentence. To cap-
ture any other non-hedge words which may con-
tribute to sentence level uncertainty, we also in-
clude BOW features based on vocabulary items
with frequencies above the mean frequency in the
corpus. This is achieved by creating binary fea-
tures for the presence of every word in the vocab-
121
ulary.
Classification is again performed by a logis-
tic regression using Quasi-Newton minimization.
It should be stressed that all hedge related fea-
tures used by the uncertainty classification stage
are taken from the results of the hedge detection
stage and not from the gold standard annotation
data. This was done to allow the system to fold
new unannotated sentences into the training set
to perform semi-supervised learning. Time con-
straints and implementation difficulties prevented
fully implementing this system component. Fu-
ture work plans to extract high class conditional
likelihood features from unannotated sentences,
annotate the sentences based on treating these fea-
tures as hedges, and retrain the hedge detection
stage and uncertainty classification stage in an it-
erative manner to improve coverage.
4 Evaluation
The dataset provided for the CoNLL-2010 shared
task consists of documents drawn from three sepa-
rate domains. Two domains, biomedical abstracts
and full articles, are relatively similar while the
third, selected Wikipedia articles, differs consider-
ably in both content and hedge cues for the reasons
previously discussed. Overall the dataset contains
11,871 sentences from abstracts, 2,670 from full
articles, and 11,111 from Wikipedia articles.
Performance for the hedge detection system
was calculated at the word level while perfor-
mance for the uncertainty classification stage was
calculated at the sentence level using the classes of
hedged and uncertain as the positive class for pre-
cision, recall and F1 statistics. We compare our
hedge detection system to a state of the art sys-
tem presented in Morante and Daelemans (2009)
and trained on a dataset of 20,924 sentences drawn
from clinical reports and biomedical abstracts and
articles. The Morante system used 10 fold cross
validation while our system randomly withholds
10 percent of the dataset for testing so our results
may be viewed as less reliable. We do provide
the first evaluation of one system on both domain
specific and domain general datasets. Table 1 pro-
vides a breakdown of performance by system and
dataset.
We evaluated the performance of the Hedge-
Hunter system on the withheld training data
including 5003 evaluation sentences from the
biomedical domain and 9634 sentences from
System Precision Recall F1
Morante
Abstracts .9081 .7984 .8477
Articles .7535 .6818 .7159
Clinical .8810 .2751 .41.92
HedgeHunter
Abstracts .8758 .5800 .6979
Articles .8704 .4052 .5529
Wikipedia .5453 .2434 .3369
All .6289 .3464 .4467
Table 1: Hedge detection performance
Wikipedia. For uncertainty classification we com-
pare our system to the results from the CoNLL-
2010 shared task comparing to the state of the art
systems. For more details see the task description
paper (Farkas et al, 2010). Table 2 summarizes
the results for the closed domain training subtask.
Table 3 summarizes the best performing systems
in the Wikipedia and biomedical domain on the
cross domain training subtask and compares to the
HedgeHunter system.
System Precision Recall F1
Tang
Biomedical .8503 .8777 .8636
Georgescul
Wikipedia .7204 .5166 .6017
HedgeHunter
Biomedical .7933 .8063 .7997
Wikipedia .7512 .4203 .5390
Table 2: Uncertainty classification performance
closed
System Precision Recall F1
Li
Biomedical .9040 .8101 .8545
Ji
Wikipedia .6266 .5528 .5874
HedgeHunter
Biomedical .7323 .6405 .6833
Wikipedia .7173 .4168 .5272
Table 3: Uncertainty classification performance
cross
122
5 Results
The Hedge Detection stage performed slightly
worse than the state of the art system. Although
precision was comparable for biomedical articles
and abstracts our system suffered from very low
recall compared to the Morante system. The
Morante system included chunk tagging as an ap-
proximation of syntactic constituency. Since many
multi word hedge cues are constituents of high
precision words and very frequent words (ought
to) this constituency information likely boosts re-
call. Like the Morante system, HedgeHunter suf-
fered a significant performance drop when tested
across domains, although our system suffered
more due to the greater difference in domains be-
tween biomedical and Wikipedia articles than be-
tween biomedical and clinical reports and due to
the annotation standards for each dataset. Hedge-
Hunter achieved better results on biomedical ab-
stracts than the full articles due to higher recall
based on the significantly larger dataset. Our
system produced the worst performance on the
Wikipedia data although this was mostly due to
a drop in precision compared to the biomedical
domain. This is in line with the drop in perfor-
mance experienced by other systems outside of the
biomedical domain and indicates that Wikipedia
data is noisier than the peer reviewed articles that
appear in the biomedical literature confirming our
informal observations. Since the dataset has an
overwhelming number of certain sentences and
unhedged words, there is already a large bias to-
wards those classes as evidenced by high over-
all classification accuracy (87% for certainty de-
tection and 97% for hedge detection on all data)
despite sometimes poor F1 scores for the minor-
ity classes. During development we experimented
with SVMs for training but abandoned them due
to longer training times and it is possible that we
could improve the recall of our system by using a
different classifier, a weaker prior or different pa-
rameters that allowed for more recall by paying
less attention to class priors. We plan to expand
our system using semi-supervised learning so it is
not necessarily a bad thing to have high precision
and low recall as this will allow us to expand our
dataset with high quality sentences and by lever-
aging the vast amounts of unannotated data we
should be able to overcome our low recall.
The uncertainty classification system performed
robustly despite the relatively poor performance of
the hedge detection classifier. The use of BOW
features supplemented the low recall of the hedge
detection stage while still relying on the hedge fea-
tures when they were available as shown by fea-
ture analysis. We did not implement bi or tri-
gram features although this would likely give a
further boost in recall. Wikipedia data was still
the worst performing domain although our cross
domain system performed near the state of the art
system with higher precision.
Overall our system produced a high precision
hedge detection system for biomedical domain
data which fed a high precision uncertainty classi-
fier. Recall for the hedge detection stage was low
overall but the use of BOW features for the uncer-
tainty classification stage overcame this to a small
degree. The amount of annotated training data has
a significant impact on performance of the Hedge-
Hunter system with more data increasing recall for
the hedge detection task. For the sentence uncer-
tainty task the system still performed acceptably
on the Wikipedia data.
6 Discussion
HedgeHunter confirmed many of the findings of
previous research. The most significant finding is
that domain adaptation in the task of hedge detec-
tion is difficult. Most new domains contain differ-
ent vocabulary and hedges tend to be highly lex-
icalized and subject to variation across domains.
This is reinforced by feature analysis where the
top weighted features for our hedge detection clas-
sifier were based on the word or its lemma and not
on its POS. Once our system learns that a partic-
ular lexical item is a hedge it is easy enough to
apply it precisely, the difficulty is getting the nec-
essary training examples covering all the possible
lexical hedge cues the system may encounter. The
lexicon of hedge cues used in biomedical articles
tends to be smaller so it is easier to get higher re-
call in this domain because the chance of seeing a
particular hedge cue in training is increased. With
the Wikipedia data, however, the set of hedge cues
is more varied due to the informal nature of the
articles. This makes it less likely that the hedge
detection system will be exposed to a particular
hedge in training.
One possible avenue for future work should
consider using lexical resources like WordNet,
measures of lexical similarity, or n-gram language
models to provide backoff feature weights for un-
123
seen lexical items. This would increase the recall
of the system despite the limited nature of anno-
tated training sets by leveraging the lexical nature
of hedges and their relatively closed class status.
We also found that the size of the training set
matters significantly. Each domain employs a cer-
tain number of domain specific hedge cues along
with domain general cues. While it is easy enough
to learn the domain general cues, domain specific
cues are difficult and can only be learned by see-
ing the specific lexical items to be learned. It is
important that the training dataset include enough
examples of all the lexical hedge cues for a spe-
cific domain if the system is to have decent re-
call. Even with thousands of sentences to train on,
HedgeHunter had low recall presumably because
there were still unseen lexical hedge cues in the
test set. Future work should concentrate on meth-
ods of expanding the size of the training sets in or-
der to cover a larger portion of the domain specific
hedging vocabulary because it does not appear that
there are good non-lexical features that are robust
at detecting hedges across domains. This may in-
clude using lexical resources as described previ-
ously or by leveraging the high precision nature
of hedge cues and the tendency for multiple cues
to appear in the same sentence to perform semi-
supervised learning.
This work also confirmed that hedge cues pro-
vide a very high precision feature for uncertainty
classification. The highest weighed features for
the classifier trained in the uncertainty classifi-
cation stage were those that indicated the pres-
ence and number of lexical hedge cues. Contrary
to some previous work which found that features
counting the number of hedge cues did not im-
prove performance, HedgeHunter found that the
number of hedge cues was a strong feature with
more hedge cues indicating an increased likeli-
hood of being uncertain (Szarvas, 2008). It is
largely a limitation of the task that we treat all un-
certain sentences as equally uncertain. From a lin-
guistic perspective a speaker uses multiple hedge
cues to reinforce their uncertainty and our system
seems to confirm that in terms of the likelihood
of class membership even if the datasets do not
encode the degree of uncertainty directly. Future
work should focus on creating more sophisticated
models of uncertainty that recognize the fact that
it is at least a scalar phenomena and not a binary
classification. Ideally a hedge detection and uncer-
tainty quantification system would function to at-
tach a probability to every fact or relation extracted
from a sentence in an IE system determined in part
by the hedging vocabulary used to express that
fact or relation. This would yield a more nuanced
view of how language conveys certainty and allow
for interesting inference possibilities for systems
leveraging the resulting IE system output.
One surprising finding was that uncertain sen-
tences often contained multiple hedge cues, some-
times up to 4 or more. This is useful because it
allows us to hypothesize that a sentence that is
unannotated and has a high chance of being un-
certain due to containing a hedge cue that we have
seen in training, possibly contains other hedge
cues that we have not seen. We can then use the
large amounts of unannotated sentences that are
available to extract n-gram features that have high
uncertainty class conditional probability and add
them to our training set with those features labeled
as hedges as described in Medlock and Briscoe
(2007). Because hedges are high precision fea-
tures for uncertainty this should not hurt precision
greatly. This allows us to increase the size of our
training set substantially in order to expose our
system to a greater variety of hedge cues in a semi-
supervised manner. As with most semi-supervised
systems we run the risk of drift resulting in a drop
in precision. Future work will have to determine
the correct balance between precision and recall,
ideally by embedding this task within the larger
IE framework to provide extrinsic evaluation
This work neglected to address the more diffi-
cult task of hedge scope detection. Determining
hedge scope requires paring spans of sentences
that fall within the hedge scope to a given hedge
cue. Along with a move towards a scalar notion
of uncertainty we should move towards a scope
based instead of sentence based representation of
uncertainty. Hedges take scope over subparts of
a sentence so just because a relation occurs in the
same sentence as a hedge cue does not mean that
the given relation is hedged. It seems unnecessar-
ily strict to ignore all relations or facts in a sen-
tence just because it contains a hedge. Hedge de-
tection is an important precursor to hedge scope
detection. Without a high performing hedge de-
tection system we cannot hope to link hedge cues
with their respective scopes. This work hopes to
produce a method for training such a hedge de-
tection system for use as a component of a hedge
124
scope finding system.
This work also failed to integrate constituency
or dependency features into either stage of the
system. Dependencies encode important informa-
tion and we plan to include features based on de-
pendency relationships into future versions of the
system. At the hedge detection stage it should
improve recall by allowing the system to detect
which multi word hedge cues are part of the same
cue. At the uncertainty classification stage it
should allow the extraction of multiword features
not just based on n-gram frequency. For semi-
supervised learning it should allow the system
to more accurately annotated multi word features
that have a high class conditional probability. This
should be even more important when performing
the task of hedge scope detection where scope is
often delimitated at the phrase level and determin-
ing the dependency relations between words can
capture this observation.
7 Conclusion
This work described HedgeHunter, a two stage
hedge detection and uncertainty classification sys-
tem. It confirmed the lexical nature of the hedge
detection task, the importance of hedge cues to un-
certainty classification and sharpened the need for
large amounts of training data in order to achieve
broad coverage. It highlights the issues involved in
developing an open domain system by evaluating
across very disparate datasets. It provides a frame-
work that can be extended to semi-supervised
learning in order to leverage large amounts of
unannotated data to improve both in domain and
cross domain performance.
References
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recogni-
tion and Modality Identification. In Proceedings of
the BioNLP 2009 Workshop, pages 185?192, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Mike Conway, Son Doan, and Nigel Collier. 2009. Us-
ing Hedges to Enhance a Disease Outbreak Report
Text Mining System. In Proceedings of the BioNLP
2009 Workshop, pages 142?143, Boulder, Colorado,
June. Association for Computational Linguistics.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing Speculative Language in Biomedical Research
Articles: A Linguistically Motivated Perspective.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
46?53, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Specu-
lations, and Statements in Between. In Proc. of the
HLT-NAACL 2004 Workshop: Biolink 2004, Linking
Biological Literature, Ontologies and Databases,
pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the ACL, pages 992?
999, Prague, Czech Republic, June.
Roser Morante andWalter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Roser Saur?? and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Gyo?rgy Szarvas. 2008. Hedge Classification in
Biomedical Texts with a Weakly Supervised Selec-
tion of Keywords. In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, Ohio, June. Association
for Computational Linguistics.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
125
