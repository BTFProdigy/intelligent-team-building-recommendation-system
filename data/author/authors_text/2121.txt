Understanding Information Graphics: A Discourse-Level Problem ?
?Sandra Carberry, ?Stephanie Elzer, ??Nancy Green, ?Kathleen McCoy, and ?Daniel Chester
?Dept. of Computer Science, University of Delaware, Newark, DE 19716
(carberry, elzer, mccoy, chester@cis.udel.edu)
??Dept. of Math. Sciences, Univ. of North Carolina at Greensboro, Greensboro, NC 27402
(nlgreen@uncg.edu)
Abstract
Keywords: graphics, understanding, dis-
course, plan-based models
Information graphics that appear in newspa-
pers and magazines generally have a message that
the viewer is intended to recognize. This paper ar-
gues that understanding such information graph-
ics is a discourse-level problem. In particular,
it requires assimilating information from multi-
ple knowledge sources to recognize the intended
message of the graphic, just as recognizing in-
tention in text does. Moreover, when an article
is composed of text and graphics, the intended
message of the information graphic (its discourse
intention) must be integrated into the discourse
structure of the surrounding text and contributes
to the overall discourse intention of the article.
This paper describes how we extend plan-based
techniques that have been used for understanding
traditional discourse to the understanding of in-
formation graphics. This work is part of a project
to develop an interactive natural language system
that provides sight-impaired users with access to
information graphics.
1 Introduction
Information graphics (non-pictorial graphics
such as bar charts and line graphs) are a variant of
language with many similarities to other forms of
communication. Information graphics are preva-
lent in information resources since they enable
complex information to be assimilated perceptu-
ally with ease. Unfortunately, knowledge sources
such as information graphics are not accessible to
some users. For example, individuals with im-
0The work of the third author was supported by the Na-
tional Science Foundation under Grant No. 0132821.
paired eyesight have limited access to information
graphics, thus preventing them from fully utiliz-
ing information resources.
Some information graphics are only intended
to display data values; (Yu et al, 2002) devel-
oped a pattern recognition algorithm for summa-
rizing interesting features of automatically gener-
ated graphics of time-series data from a gas tur-
bine engine. However, the overwhelming major-
ity of the graphics that we have examined (taken
from newspaper, magazine, and web articles) ap-
pear to have some underlying goal, such as get-
ting the viewer to believe that interest rates have
fallen substantially and that this would therefore
be a good time to refinance a mortgage. We
have found that understanding information graph-
ics is a discourse-level problem. In particular,
it requires assimilating information from multi-
ple knowledge sources to recognize the intended
message of the graphic, just as recognizing inten-
tion in text does. Moreover, the communicative
intention of the information graphic must be in-
tegrated into the discourse intentions of the sur-
rounding text.
We are developing an interactive natural lan-
guage system that infers the intended message
underlying an information graphic, augments it
with related interesting features of the graphic,
provides an initial summary of the graphic, and
then responds to followup questions from the
user. This paper presents the system architecture,
shows why interpreting information graphics is
a discourse-level problem, and outlines how we
extend techniques that have been used for under-
standing traditional discourse to the understand-
ing of information graphics.
2 A Natural Language Modality
Information is the key to knowledge and ef-
fective decision-making. But information is use-
ful only if it is accessible in a form that can be
easily assimilated. For sighted users, information
graphics capture complex information and enable
it to be assimilated perceptually with ease. For
individuals who have serious sight-impairments,
documents that contain information graphics pose
challenging problems. Although devices have
been developed for conveying information graph-
ics in alternative mediums such as musical tones
or tactile images, these approaches have serious
limitations. For example, systems that attempt to
convey graphics via a soundscape(Meijer, 1992)
do not facilitate easy comparison of two line
graphs linked in a single graphical display. More-
over, these approaches require the user to con-
struct a ?mental map? of the graphic, which is
difficult for congenitally blind users who do not
have the personal knowledge to assist them in the
interpretation of the image(Kennel, 1996). The
underlying hypothesis of our work is that alterna-
tive access to what the graphic looks like is not
enough ? the user should be provided with the
message and knowledge that one would gain from
viewing the graphic in order to enable effective
and efficient use of this information resource. To
accomplish this objective, we are developing an
interactive natural language system for communi-
cating the content of an information graphic. Our
methodology offers promise as a means of provid-
ing access to information graphics without expen-
sive equipment, with few limitations on the com-
plexity of the graphic that can be handled, and
with relatively little cognitive load on the user.
3 Architecture and Overview
Our current work is concerned with bar charts,
line graphs, and pie charts, although eventually
we will handle other kinds of graphics. Figure 1
shows the architecture of our system for convey-
ing information graphics. The visual extraction
component (VEC) analyzes the graphic and pro-
vides an XML representation of the graphic to
the intention recognition component (IRC). The
IRC is responsible for recognizing the intended
message of the information graphic and sending it
to the content planning component (CPC), which
will augment the intended message of the graphic
with related interesting features. The message or-
ganization component (MOC) then organizes the
most salient propositions into a coherent sum-
mary, which will be rendered in natural language
and conveyed to the user via speech synthesis.
The followup question component (FQC) will al-
low the user to interactively seek additional infor-
mation about the graphic.
Our work thus far (Section 4) has focused on
understanding an information graphic so that its
intended message can be conveyed to the user.
Section 4.1 discusses the extension of speech act
theory to the generation and understanding of in-
formation graphics. Section 4.2 argues that un-
derstanding information graphics is a discourse-
level problem in which the system must recog-
nize the intended message of the graphic and in-
tegrate it into the intentions of any surrounding
text; it further argues that understanding informa-
tion graphics requires similar kinds of knowledge
and processing as does the understanding of tra-
ditional textual discourse. Section 4.3 provides
a brief overview of the visual extraction compo-
nent that analyzes the graphical image and con-
structs an XML representation of the graphic for
use by the graphic understanding system. Sec-
tion 4.4 then describes how we have extended
techniques used for understanding traditional dis-
course and dialogue to the understanding of infor-
mation graphics. Section 5 gives a brief overview
of future work on the rest of the system. The Ap-
pendix contains information graphics that are part
of the corpus on which our work is based.
4 Understanding Information Graphics
4.1 Intention in Information Graphics
Information graphics are a variant of language.
As noted by Clark(Clark, 1996), language is more
than just words. It is any ?signal? (or lack of sig-
nal when one is expected), where a signal is a de-
liberate action that is intended to convey a mes-
sage. According to speech act theory, a speaker
or writer executes a speech act whose intended
meaning he expects the listener or reader to be
able to deduce(Searle, 1970; Grice, 1969; Clark,
1996). In their work on multimedia generation,
Figure 1: System Architecture
the AutoBrief group proposed that speech act the-
ory could be extended to cover the generation
of graphical representations(Kerpedjiev and Roth,
2000). They developed a multimedia presenta-
tion system that generated text and information
graphics. It included 1) an algorithm that could
map communicative goals to a set of perceptual
and cognitive tasks that must be enabled for a
viewer to recognize the goals and 2) an automatic
graph designer that used constraint satisfaction to
construct an information graphic that best facili-
tated those tasks, subject to competing constraints
among the tasks.
The overwhelming majority of information
graphics accompanying newspaper and magazine
articles appear to carry a message that the de-
signer intends to convey to the viewer by virtue
of the graphic?s design and the data presented in
the graphic. Consider the graphic in Figure 9. It
conveys the message that the salary of women
in science, mathematics, and engineering fields
is consistently less than that of men in the same
fields. Other messages could have been con-
veyed by a different graphic design. For ex-
ample, by grouping the bars for men together,
grouping the bars for women together, and or-
dering the bars for each group by height, the
graphic would have conveyed the message that
both men and women earn the least in the so-
cial sciences and the most in engineering. Or
if the bars for Computer/Mathematical Sciences
were highlighted in Figure 9 by coloring them
significantly differently from the other bars in the
graphic, the graphic would have invoked a com-
parison of the discrepancies between male and
female salaries in Computer/Mathematical Sci-
ences and the salary discrepancies between men
and women in other fields. Although a graphic?s
caption can be helpful in identifying its intended
message (as in Figure 8), Corio performed a large
corpus study(Corio and Lapalme, 1999) in which
he found that captions are often missing or fail
to provide any indication of what the information
graphic conveys (as in Figures 6 and 10). Thus
we cannot rely entirely on the presence of useful
captions to identify the intended message of an
information graphic.
Language research has posited that the listener
or reader who is interpreting a speech act identi-
fies its intended meaning by reasoning about the
observed signals and the mutual beliefs of author
and interpreter(Grice, 1969; Clark, 1996). Ap-
plying this to graphical displays, it is reasonable
to presume that the author of a graphic similarly
expects the viewer to use perceptual skills along
with other knowledge sources to deduce from the
graphic the message that he intended to convey.
Thus we are applying speech act theory in the re-
verse direction of the AutoBrief project, namely
to the recognition of the intended message under-
lying an information graphic.
4.2 A Discourse Level Problem
This section argues that interpreting informa-
tion graphics is a discourse-level problem ? not
only is it necessary to recognize the intention of
the graphic as noted in Section 4.1, but under-
standing an information graphic requires similar
kinds of knowledge and processing as does un-
derstanding traditional discourse.
Grosz and Sidner contended that discourse
has a structure comprised of discourse segments.
Each discourse segment has a discourse seg-
ment purpose that contributes to the discourse
purpose or intention underlying the overall dis-
course(Grosz and Sidner, 1986). When an arti-
cle is comprised of text and graphics, the graphic
generally expands on the text and contributes to
the discourse purpose of the article. Consider the
graphic and partial surrounding text reproduced
in Figure 6. Nowhere in the text is it stated that
the income of black women has risen dramati-
cally over the last decade and has reached the
level of white women. Yet this message is clearly
conveyed by the graphic and contributes to the
overall communicative intention of this portion
of the article ? namely, that there has been a
?monumental shifting of the sands? with regard
to the achievements of black women. Not only
does the intended message of the graphic (its dis-
course segment purpose) contribute to this overall
intention, but in fact the discourse intention of the
graphic helps to recognize the overall intention.
Even when the graphic stands in isolation as
in Figures 7 and 8, understanding the graphic
is a discourse-level problem. Grosz and Sid-
ner(Grosz and Sidner, 1986) claim that a robust
model of discourse understanding must use mul-
tiple knowledge sources in order to recognize the
complex relationships that utterances have to one
another. Information graphics have similar com-
plex relationships among their component ele-
ments. Not only might the graphic include mul-
tiple elements that must be related to one another
(such as multiple lines in a line graph, or individ-
ual bars in a bar chart), but information graphics
often include highlighting of certain elements to
make them particularly salient (as in Figure 10)
or include captions that might contribute to rec-
ognizing the graphic?s intention. The graphic in
Figure 8 includes such a helpful caption, although
many graphics, such as the ones in Figures 7
and 9, do not.
Furthermore, identifying the intended message
of a composite graphic (one comprised of multi-
ple individual graphics) requires relating the in-
dividual graphics to one another to identify the
intended message of the composite. Figure 11 il-
lustrates a composite information graphic. The
discourse purpose of the composite graphic is that
audits of affluent taxpayers are declining with re-
spect to audits of all taxpayers. This message can
only be deduced by relating the two individual
graphics and their underlying messages.
Moreover, understanding information graphics
requires the use of multiple knowledge sources.
In earlier work on recognizing expressions of
doubt, we developed an algorithm that combined
linguistic, contextual, and world knowledge and
applied it to the recognition of complex discourse
acts(Carberry and Lambert, 1999). In the case
of information graphics, the corollary to linguis-
tic knowledge is perceptual knowledge, by which
one recognizes the individual elements of the
graphic (for example, the bars in a bar chart), the
relation of the individual elements in the graphic
to one another, the type of graphic (line graph,
bar chart, pie chart, etc.), and what the different
graphic types can be used to convey. For exam-
ple, both a scatter plot and a pie chart can be
used to portray how an entity (such as govern-
ment income) is divided up among several cate-
gories (such as social welfare, military spending,
etc.); however, a graphic designer will choose a
pie chart if the intent is to convey the relative dis-
tributions as opposed to their absolute amounts.
Furthermore, a particular type of graphic (such as
a line graph) might be appropriate for conveying
several different intentions (maximum data point,
data trend, data variation, etc.).
Contextual and world knowledge are also es-
sential for understanding information graphics.
Contextual knowledge includes the caption as-
sociated with the graphic, any highlighting of
graphic elements that affects the focus of atten-
tion in the graphic, and the discourse structure and
focus of attention in any surrounding text. World
knowledge consists of mutual beliefs between de-
signer and viewer about entities of interest to the
intended viewing audience. For example, if an in-
formation graphic appears in a document targeted
at residents of New York City, then both the de-
signer and the viewer will mutually believe that
entities such as New York City, its football and
baseball teams, etc. will be particularly salient
to the viewer. Our methodology for understand-
ing information graphics takes these knowledge
sources into account.
4.3 The Visual Extraction Component
The visual extraction component (VEC) cap-
tures much of the perceptual knowledge discussed
in Section 4.2. It is responsible for recogniz-
ing the individual components comprising the
graphic, identifying the relationship of the differ-
ent components to one another and to the graphic
as a whole, and classifying the graphic as to type.
Extracted components include not only the bars,
lines, or wedges of a graphic but also the titles of
the axes, the legend, and the graphic?s title or cap-
tion. The present implementation deals only with
gray scale images (in pgm format) of bar charts,
pie charts, and line graphs, though eventually it
will be extended to handle color and other kinds
of information graphics. Words and numbers that
appear in the chart are associated with particular
bars, wedges and lines by their proximity to the
chart component in question. The output of the
visual extraction component is an XML file that
describes the chart and all of its components.
4.4 Applying Discourse Understanding
Strategies
Many researchers have cast the understanding
of discourse and dialogue as a plan recognition
problem ? that is, the writer or speaker (or char-
acters in the case of a story) has an underlying
goal and a plan for accomplishing that goal, and
understanding requires that the reader or listener
infer the plan and in turn the goal that the plan is
intended to achieve. (Perrault and Allen, 1980;
Wilensky, 1983; Litman and Allen, 1987; Car-
berry, 1990; Charniak and Goldman, 1993; Ardis-
onno and Sestero, 1996) are just a few examples
of such systems.
Since understanding information graphics is a
discourse-level problem, we are extending plan
inference techniques to recognizing the intended
message of an information graphic(Elzer et al,
2003) and to identifying its contribution to an
extended discourse that includes both text and
graphics. Planning and plan inference systems re-
quire knowledge about goals and how they can
be achieved. Typically, this is provided by a li-
brary of operators. Each operator encodes a goal
in its header; the body of the operator encodes
the subgoals that must be accomplished in order
to achieve the operator?s goal. A planning sys-
tem starts with a high-level goal, and uses oper-
ators to decompose the goal into a set of simpler
subgoals, which eventually decompose into prim-
itive subgoals that can be accomplished by prim-
itive actions in the domain. On the other hand,
a plan inference system starts with the primitive
goals associated with observed actions, and uses
the operators to chain backwards to higher-level
goals which the lower-level subgoals contribute to
achieving. In the case of traditional discourse and
dialogue, the subgoals in the plan operators are ei-
ther communicative or domain goals, and the ob-
served actions that start the plan inference process
are the speech acts represented by the utterances
in a story or a dialogue.
To extend plan inference to information graph-
ics, the plan operators must include goals that
can be accomplished by viewing an information
graphic, as opposed to being the recipient of an
utterance. As discussed in Section 4.1, the Auto-
Brief project(Kerpedjiev and Roth, 2000) devel-
oped an algorithm to map communicative goals to
a sequence of perceptual and cognitive tasks that
the graphic should support. Perceptual tasks are
tasks that can be performed by simply viewing the
graphic, such as finding the top of a bar in a bar
chart; cognitive tasks are tasks that are performed
via mental computations, such as computing the
difference between two numbers. We draw on
the AutoBrief notion of perceptual and cognitive
tasks enabled by an information graphic. Our plan
operators not only encode knowledge about how
to achieve domain and communicative goals (the
latter of which may require that the viewer per-
form perceptual and cognitive tasks) but they also
encode knowledge about how information-access
tasks, such as finding the value of an entity in
a graphic, can be decomposed into simpler sub-
goals. Figures 2 and 3 present two plan operators
for achieving the goal of finding the value <v> of
an attribute <att> for a graphical element <e>
(for example, the value associated with the top of
a bar in a bar chart). The body of the operator in
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Dependent-variable(<att>, <ds>)
Body: 1. Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Figure 2: Operator for achieving a goal perceptually
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Natural-quantitative-ordering(<att>)
Display-const: Ordered-values-on-axis(<g>, <axis>, <att>)
Body: 1. Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
2. Interpolate(<viewer>, <l1>, <l2>, <f>, <v>)
Figure 3: Operator that employs both perceptual and cognitive subgoals
Figure 2 specifies that the goal can be achieved
by a primitive perceptual task in which the viewer
just perceives the value; this could be done, for
example, if the element in the graphic is annotated
with its value, as are the bars in the bar chart in
Figure 8 of the Appendix. On the other hand, the
body of the operator in Figure 3 captures a differ-
ent way of finding the value, one that presumably
requires more effort. It specifies the perceptual
task of finding the values <l1> and <l2> sur-
rounding the desired value on the axis along with
the fraction <f> of the distance that the desired
value lies between <l1> and <l2>, followed by
the cognitive task of interpolating between the re-
trieved values <l1> and <l2>.
Our operators contain data requirements (la-
belled Data-req) which the data must satisfy in
order for the operator to be applicable in a graphic
planning paradigm; they may also contain display
constraints (labelled Display-const) which con-
strain how the information graphic is constructed
if this operator is part of a final plan. In the case
of plan recognition, these constraints are used
in reverse. The display constraints are used to
eliminate operators from consideration, since if
a graphic does not satisfy the operator?s display
constraints, then the operator could not be part of
a plan that led to the graphic. If a graphic meets
the display constraints of an operator, then the
data requirements are used to limit how the op-
erator?s parameters might be instantiated.
4.4.1 Beginning the Plan Inference Process
Traditional plan inference systems used for
language understanding start with the primitive
goal achieved by the speech act in the dialogue
or discourse. In the case of information graphics,
the role of the speech act is played by the primi-
tive perceptual tasks that the viewer performs on
the graphic. To limit the set of perceptual tasks
that are considered, we make two observations:
? The graphic designer has many alternative
ways of designing a graphic, and the de-
sign choices facilitate some perceptual tasks
more than others. Following the Auto-
Brief work(Kerpedjiev and Roth, 2000) on
generating graphics that fulfill communica-
tive goals, we hypothesize that the designer
chooses a design that best facilitates the
tasks that are most important to conveying
his intended message, subject to the con-
straints imposed by competing tasks.
? Entities may become particularly salient by
virtue of highlighting in the graphic (for ex-
ample, coloring certain elements different
from the others, annotating an element with
an asterisk, or exploding one piece of a pie
chart1), by their mention in the caption or
surrounding text, or via world knowledge
1(Mittal, 1997) discusses a variety of such design tech-
niques in the context of distorting the message inferred from
a graphic.
capturing mutual beliefs about entities of in-
terest to the intended audience. We hypoth-
esize that the designer relies on the viewer
recognizing particularly salient entities, in
order to make certain perceptual tasks more
salient to the viewer.
As noted in Section 4.1, one cannot rely on a
graphic?s caption to provide the intended mes-
sage of the graphic. Consequently, the plan in-
ference process starts with both the set of tasks
that are best enabled by the information graphic
and the set of tasks (if any) that are particularly
salient. These will be referred to as candidate
tasks. The next two subsections describe how
candidate tasks are identified.
Identifying the Best Enabled Tasks The
APTE (Analysis of Perceptual Task Effort) sub-
module, shown in Figure 1 as part of the Inten-
tion Recognition Component, captures perceptual
knowledge about performing primitive perceptual
tasks2, and it encapsulates the results of cognitive
psychology research to estimate the relative effort
required for different tasks. The output of APTE
is the set of perceptual tasks that are best enabled
by the graphic. These become candidate tasks.
Each APTE rule captures a primitive percep-
tual task that can be performed on a particu-
lar type of information graphic, the conditions
(graphic design choices) that affect the difficulty
of performing that task, and the estimated effort
expended by a viewer if those conditions are sat-
isfied in the graphic. The condition-computation
pairs are ordered so that the ones producing the
lowest effort estimates appear first in a rule.
To derive the effort estimates in the rules, we
have followed the GOMS approach(Card et al,
1983) by breaking down the tasks that are re-
garded as primitive in our plan operators into
even more basic component tasks, and then sum-
ming the effort estimates for these very basic
tasks. Lohse?s work(Lohse, 1993) is an exam-
ple of the GOMS architecture applied to predict-
ing performance on graph comprehension tasks,
and many of our effort estimates are based on
Lohse?s research. For example, Figure 4 dis-
2Primitive perceptual tasks are those that we do not de-
compose into a set of simpler subtasks; this is not to be con-
fused with the notion of a psychological primitive.
plays the APTE rule for the task of finding the
value associated with the top of a bar in a bar
chart. If the bar is annotated with its value,
then condition-computation pair B1-1 estimates
its effort as 150 units for discriminating the label
(based on work by Lohse(Lohse, 1993)) and 300
units for recognizing a 6-letter word (John and
Newell, 1990). If the bar is not annotated with its
value but is aligned with a tick mark on the axis,
then condition-computation pair B1-2 estimates
the perceptual effort in terms of the distance to the
dependent axis (in order to capture the degrees of
visual arc scanned(Kosslyn, 1989)) plus the effort
of discriminating and recognizing the label. Fig-
ure 5 displays the APTE rule associated with the
first subgoal in Figure 3. It estimates the effort for
the primitive task Perceive-info-to-interpolate as
the effort of the scan to the dependent axis (based
on (Kosslyn, 1989)), the effort of discriminating
the intersection location on the axis (150 units
based on (Lohse, 1993)), plus the effort of the sac-
cade to each label (230 units each (Russo, 1978))
along with the effort involved in discriminating
and recognizing the labels. Similarly, there is a
cognitive rule (not discussed here) for estimating
the effort associated with the cognitive task Inter-
polate (the second subgoal in the operator in Fig-
ure 3). (Elzer et al, 2003a) presents a more ex-
tensive discussion of the cognitive principles un-
derlying the APTE rules.
Given the XML representation of an informa-
tion graphic, each APTE rule that is applicable
to the graphic produces an effort estimate for the
task captured by the rule. When a task might be
instantiated in multiple ways and still satisfy the
conditions of a condition-computation pair (for
example, the task of finding the value of the top
of a bar could be instantiated for each bar in a
bar chart), only the instantiation that produces the
lowest effort estimate becomes a candidate task.
(If the bars are not annotated with values, then the
instantiation that will produce the lowest effort es-
timate for the task of finding the value of the top
of a bar in a bar chart would be the bar with the
shortest scan to the dependent axis.) This is con-
sistent with the idea that the graphic designer will
make the important tasks easy to perform. The
set of perceptual tasks that require the least effort
become candidate tasks.
Rule-1:Estimate effort for task Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Graphic-type: bar-chart
Gloss: Compute effort for finding the exact value <v> for attribute <att> represented by top <e>
of a bar <b> in graph <g>
B1-1: IF the top of bar <b> is annotated with a value,
THEN effort=150 + 300
B1-2: IF the top <e> of bar <b> aligns with a labelled tick mark on the dependent axis,
THEN effort=scan + 150 + 300
Figure 4: A rule for estimating effort for the primitive perceptual task Perceive-value
Rule-2:Estimate effort for task
Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
Graphic-type: bar-chart
Gloss: Compute effort for finding the information needed for interpolation, including the labels
<l1> and <l2> on either side of entity <e> on axis <axis> in graph <g>,
and the fraction <f> that is the distance between <l1> and entity <e> on <axis>
relative to the distance between <l1> and <l2>
B2-1: IF <axis> is labelled with values THEN effort=scan + 150 + ((230 + 150 + 300) x 2)
Figure 5: A rule for estimating effort for the primitive perceptual task Perceive-info-to-interpolate
Identifying Particularly Salient Tasks
Salient tasks are those that the viewer might
perform because they relate to entities that are
in the viewer?s current focus of attention, as
determined by contextual knowledge provided
by the caption, highlighting, and the surrounding
text and by world knowledge in the form of
mutual beliefs about items of particular interest
to the viewing audience.
Ideally, a caption will provide clues about the
message that an information graphic is intended
to convey, and thus noun phrases in captions rep-
resent salient entities.3 The graphic designer can
also call into focus certain aspects of the graphic
by using attention-getting devices such as col-
oring it differently from the rest of the graphic,
annotating it with an arrow, etc. Our working
hypothesis is that if the graphic designer goes
to the effort of employing such attention-getting
devices, then the highlighted items almost cer-
tainly contribute to the intended message. Thus
the attributes of these highlighted items (for ex-
ample, the attributes of a highlighted bar in a bar
chart), which are captured in the XML represen-
3Verb phrases in captions also provide evidence, but they
suggest particular operators of interest rather than instanti-
ated perceptual tasks, and thus we associate verbs with oper-
ators in the plan library.
tation of the graphic, are also regarded as salient
entities. Salient entities also include those that
world knowledge suggests are mutually believed
to be of interest to the viewing audience. We en-
vision in the future using the notion of lexical
chains(Silber and McCoy, 2000) to identify enti-
ties that the accompanying text makes particularly
salient. Perceptual tasks that are instantiated with
a salient entity and that can be performed on the
graphic are designated salient tasks.
4.4.2 The Search Process
Candidate tasks consist of the set of percep-
tual tasks that require the least effort and the set of
salient tasks. Once the set of candidate tasks has
been identified, plan inference begins. Initial can-
didate plans are constructed from each operator
in which a candidate task appears as a subgoal;
the root of the candidate plan is the goal of the
operator, and its children are the subgoals in the
body of the operator. Chaining from the root goal
to other operators whose body contains the root
goal as a subgoal produces larger candidate plans
with higher-level goals as the new root goal.
Plan inference systems have used a variety of
heuristics to evaluate candidate plans and to se-
lect the candidate plan to expand further. These
heuristics help to guide the search through the
space of candidate plans in order to hypothe-
size the plan that best represents the user?s in-
tentions. These heuristics have included increas-
ing the rating of partial plans as their arguments
become instantiated(Perrault and Allen, 1980),
preferring coherent discourse moves(Litman and
Allen, 1987; Carberry, 1990), and biasing the
plan inference process based on knowledge about
the user group(Gertner and Webber, 1996). We
have identified several kinds of evidence for guid-
ing plan inference from information graphics, in-
cluding the estimated effort required by a candi-
date plan, the basis for instantiating parameters in
the plan, adherence to the proximity compatibil-
ity principle from cognitive science research, and
the relation between a candidate plan and the es-
tablished discourse context.
Since our working hypothesis is that the
graphic designer tried to enable those tasks neces-
sary to recognize his intended message, candidate
plans that require substantially more effort than
other candidate plans are less likely to represent
the intentions of the designer. The effort associ-
ated with a candidate plan is measured as the sum
of the effort of the tasks comprising it.
There are many ways that a parameter in a task
or subgoal might become instantiated, and the ba-
sis for the instantiation provides evidence about
the likelihood that a hypothesized candidate plan
represents the graphic designer?s intentions. If
an instantiation is suggested by highlighting or
a caption or entities that are particularly salient
to the targeted audience, that partial plan should
be evaluated more favorably since the designer of
the graphic has provided reasons for the viewer to
use these instantiations in recognizing his inten-
tions. Similarly, if the instantiation is one of sev-
eral possible alternatives with no reason for pre-
ferring one over the other, then the partial plan
should be evaluated less favorably since the de-
signer did not give the viewer any reason to prefer
one over the other. This relates to Allen?s forking
heuristic(Perrault and Allen, 1980). The proxim-
ity compatibility principle(Wickens and Carswell,
1995) also suggests that candidate plans which
use similarly encoded elements (for example, all
red bars) in an integrated fashion should be eval-
uated more favorably than those that do not.
If there is a context established by the text pre-
ceding or surrounding the graphic, then candidate
plans whose root goal contributes to the exist-
ing discourse context should be preferred. If the
surrounding text has a reference to the graphic,
then focusing heuristics(Carberry, 1990) will pre-
fer candidate plans that relate most closely to the
current focus of attention at that point in the sur-
rounding text. However, the surrounding text of-
ten does not refer to accompanying graphics, as is
the case in the Newsweek article whose excerpt is
shown in Figure 6. Future work will investigate
how we should handle instances such as this.
5 Response Generation and Followup
The intended message of the graphic must be
augmented with additional propositions that con-
vey interesting features that a viewer would glean
from the graphic. For example, the intended mes-
sage of the graphic in Figure 6 appears to be that
the income of black women has risen dramati-
cally over the last decade and reached the level
of white women. But other interesting features of
the graphic might include the trends over the past
several decades, periods where they were closest,
etc. In future work, we anticipate developing a
methodology for identifying propositions that ex-
pand on the message of the graphic designer and
for including the most salient of these in the sum-
marization of the graphic. We also envision re-
sponding to followup requests for further infor-
mation about the graphic by selecting the highest
ranking propositions that were not included in the
initial message, organizing them into a coherent
response, and conveying it to the user.
6 Summary
This paper has argued that understanding
information graphics is a discourse-level prob-
lem. Not only must the system recognize the in-
tended message of the information graphic, but
the recognition process requires similar kinds of
knowledge sources and similar kinds of process-
ing as does the understanding of traditional dis-
course and dialogue. Moreover, when an article
is composed of text and graphics, the intended
message of the information graphic must be in-
tegrated into the discourse structure of the sur-
rounding text, and it contributes to the overall dis-
course intention of the article.
References
L. Ardisonno and D. Sestero. 1996. Using dynamic
user models in the recognition of the plans of the
user. User Modeling and User-Adapted Interac-
tion, 5(2):157?190.
S. Carberry and L. Lambert. 1999. A process model
for recognizing communicative acts and modeling
negotiation subdialogues. Computational Linguis-
tics, 25(1):1?53.
S. Carberry. 1990. Plan Recognition in Natural Lan-
guage Dialogue. ACL-MIT Press Series on Natural
Language Processing. MIT Press, Cambridge, MA.
S. Card, T. Moran, and A. Newell. 1983. The Psychol-
ogy of Human-Computer Interaction. Lawrence
Erlbaum Associates, Inc., Hillsdale, NJ.
E. Charniak and R. Goldman. 1993. A bayesian
model of plan recognition. Artificial Intelligence,
64:53?79.
H. Clark. 1996. Using Language. Cambridge Univer-
sity Press.
M. Corio and G. Lapalme. 1999. Generation of texts
for information graphics. In Proceedings of the 7th
European Workshop on Natural Language Genera-
tion EWNLG?99, pages 49?58.
S. Elzer, N. Green, and S. Carberry. 2003a. Exploit-
ing cognitive psychology research for recognizing
intention in information graphics. In Proceedings
of the 25th Annual Meeting of the Cognitive Science
Society. To appear.
S. Elzer, N. Green, S. Carberry, and K. McCoy. 2003.
Extending plan inference techniques to recognize
intentions in information graphics. In Proceedings
of the Ninth International Conference on User Mod-
eling. To appear.
A. Gertner and B. Webber. 1996. A Bias Towards
Relevance: Recognizing Plans Where Goal Mini-
mization Fails. In Proc. of the Thirteenth National
Conference on Artificial Intelligence, pages 1133?
1138.
H. P. Grice. 1969. Utterer?s Meaning and Intentions.
Philosophical Review, 68:147?177.
B. Grosz and C. Sidner. 1986. Attention, Intentions,
and the Structure of Discourse. Computational Lin-
guistics, 12(3):175?204.
B. John and A. Newell. 1990. Toward an engi-
neering model of stimulus response compatibility.
In R. Gilmore and T. Reeve, editors, Stimulus-
response compatibility: An integrated approach,
pages 107?115. North-Holland, New York.
A. Kennel. 1996. Audiograf: A diagram-reader for
the blind. In Second Annual ACM Conference on
Assistive Technologies, pages 51?56.
S. Kerpedjiev and S. Roth. 2000. Mapping com-
municative goals into conceptual tasks to generate
graphics in discourse. In Proc. of the International
Conference on Intelligent User Interfaces, pages
60?67.
S. Kosslyn. 1989. Understanding charts and graphs.
Applied Cognitive Psychology, 3:185?226.
D. Litman and J. Allen. 1987. A Plan Recognition
Model for Subdialogues in Conversation. Cognitive
Science, 11:163?200.
G. Lohse. 1993. A cognitive model for understand-
ing graphical perception. Human-Computer Inter-
action, 8:353?388.
Peter B. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions
on Biomedical Engineering, 39(2):291?300, Febru-
ary.
V. Mittal. 1997. Visual prompts and graphical design:
A framework for exploring the design space of 2-D
charts and graphs. In Proc. of the Fourteenth Na-
tional Conference on Artificial Intelligence, pages
57?63.
R. Perrault and J. Allen. 1980. A Plan-Based Anal-
ysis of Indirect Speech Acts. American Journal of
Computational Linguistics, 6(3-4):167?182.
J. Russo. 1978. Adaptation of cognitive processes to
eye movement systems. In J. Senders, D. Fisher,
and R. Monty, editors, Eye movements and higher
psychological functions. Lawrence Erlbaum, Hills-
dale, NJ.
J. Searle. 1970. Speech Acts: An Essay in the Phi-
losophy of Language. Cambridge University Press,
London.
G. Silber and K. McCoy. 2000. Efficient text summa-
rization using lexical chains. In Proc. of the Inter-
national Conference on Intelligent User Interfaces,
pages 252?255.
C. Wickens and M. Carswell. 1995. The proximity
compatibility principle: Its psychological founda-
tion and relevance to display design. Human Fac-
tors, 37(3):473?494.
R. Wilensky. 1983. Planning and Understanding.
Addison-Wesley.
J. Yu, J. Hunter, E. Reiter, and S. Sripada. 2002.
Recognising visual patterns to communicate gas
turbine time-series data. In ES2002, pages 105?
118.
Appendix of Graphics from our Corpus
Graphic from Newsweek Article
60 70 80 90 01
$15
10
5
Black women
White women
Median Income
In thousands of 2001 dollars
1948
Relevant Text from Newsweek Article
This is not to say that black women have
climbed the storied crystal stair. They remain
?in the proving stage?, observes Alabama ex-
ecutive Alice Gordon. Nearly 14 percent of
working black women remain below the poverty
level. And women don?t yet out-earn black men.
But the growing educational-achievement gap
portends a monumental shifting of the sands.
College-educated black women already earn
more than the median for all black working men
? or, for that matter, for all women. And as
women in general move up the corporate pyra-
mid, black women, increasingly, are part of the
parade. In 1995 women held less than 9 per-
cent of corporate-officer positions in Fortune
500 companies, according to Catalyst, a New
York-based organization that promotes the inter-
ests of women in business. Last year they held
close to 16 percent, a significant step up. Of
those 2,140 women, 163 were black ? a minus-
cule proportion, but one that is certain to grow.
Figure 6: Excerpt from Newsweek Magazine
How reliable adults think DNA tests are for identifying an individual:
Trusting DNA
Don?t know
Very unreliable
unreliable
Somewhat 
reliable
Somewhat
Very reliable
2%
3%
4%
68%
23%
Figure 7: Standalone Graphic from USA Today
Europe
Canada
African
Other
countries
South
United
States
Africa
21
metric tons
Leading producers in
in gold production
South Africa tops
428
355
187 155
Gold Production
M
et
ric
 T
on
s
Figure 8: Standalone Graphic from USA Today
010,000
Median Salaries (in dollars), Full?Time Employed SMET Doctorates, by Field and Gender, 1997
Computer/All SMET Physical Sciences
Sal
ari
es 
(in 
dol
lars
)
SciencesMathematical
Engineering Life Sciences Social Sciences
80,000
70,000
60,000
50,000
40,000
30,000
20,000
Male
Female
Figure 9: Graphic from Report of the NSF Committee on Equal Opportunities in Science & Engineering
personal filings
Delaware bankruptcy
3000
2500
1000
1500
2000
1998 1999 2000 2001
Figure 10: Graphic from Wilmington News
Journal
0.6%
?96 ?97 ?98 ?99 ?00 ?01
1.0%
2.0%
3.0%
0.8%
0
?96 ?97 ?98 ?99 ?00 ?01
All taxpayers
Affluent taxpayers
1.2%
continue to slide
Audits of affluent
were audited by the IRS:
Percentage of taxpayers who
0
0.6%
1.8%
Figure 11: Graphic from USA Today
Proceedings of the 43rd Annual Meeting of the ACL, pages 223?230,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploring and Exploiting the Limited Utility of Captions in Recognizing
Intention in Information Graphics?
Stephanie Elzer1 and Sandra Carberry2 and Daniel Chester2 and Seniz Demir2 and
Nancy Green3 and Ingrid Zukerman4 and Keith Trnka2
1Dept. of Computer Science, Millersville University, Millersville, PA 17551
2Dept. of Computer Science, University of Delaware, Newark, DE 19716
3Dept. of Mathematical Sciences, Univ. of NC at Greensboro, Greensboro, NC 27402
4School of CS & Software Engrg, Monash Univ., Clayton, Victoria 3800 Australia
Abstract
This paper presents a corpus study that ex-
plores the extent to which captions con-
tribute to recognizing the intended mes-
sage of an information graphic. It then
presents an implemented graphic interpre-
tation system that takes into account a va-
riety of communicative signals, and an
evaluation study showing that evidence
obtained from shallow processing of the
graphic?s caption has a significant impact
on the system?s success. This work is part
of a larger project whose goal is to provide
sight-impaired users with effective access
to information graphics.
1 Introduction
Language research has posited that a speaker or
writer executes a speech act whose intended mean-
ing he expects the listener to be able to deduce, and
that the listener identifies the intended meaning by
reasoning about the observed signals and the mutual
beliefs of author and interpreter (Grice, 1969; Clark,
1996). But as noted by Clark (Clark, 1996), lan-
guage is more than just words. It is any ?signal? (or
lack of signal when one is expected), where a sig-
nal is a deliberate action that is intended to convey a
message.
Although some information graphics are only in-
tended to display data values, the overwhelming ma-
jority of the graphics that we have examined (taken
?Authors can be reached via email as fol-
lows: elzer@cs.millersville.edu, nlgreen@uncg.edu,
{carberry, chester, demir, trnka}@cis.udel.edu, In-
grid.Zukerman@infotech.monash.edu.au.
1998 1999 2000 20011000
1500
2000
2500
3000
personal filingsLocal bankruptcy
Figure 1: Graphic from a 2001 Local Newspaper
from newspaper, magazine, and web articles) ap-
pear to have some underlying goal or intended mes-
sage, such as the graphic in Figure 1 whose com-
municative goal is ostensibly to convey the sharp in-
crease in local bankruptcies in the current year com-
pared with the previous decreasing trend. Applying
Clark?s view of language, it is reasonable to presume
that the author of an information graphic expects the
viewer to deduce from the graphic the message that
the graphic was intended to convey, by reasoning
about the graphic itself, the salience of entities in
the graphic, and the graphic?s caption.
This paper adopts Clark?s view of language as any
deliberate signal that is intended to convey a mes-
sage. Section 3 investigates the kinds of signals used
in information graphics. Section 4 presents a cor-
pus study that investigates the extent to which cap-
tions capture the message of the graphic, illustrates
the issues that would arise in trying to fully under-
stand such captions, and proposes shallow process-
ing of the caption to extract evidence from it. Sec-
tion 5 then describes how evidence obtained from
a variety of communicative signals, including shal-
low processing of the graphic?s caption, is used in a
probabilistic system for hypothesizing the intended
message of the graphic. Section 6 presents an eval-
223
10
 5
15
0?680+ 65?79 7?19 35?4980+65?7950?6435?49
10
 5
15
20?347?190?6 20?3450?64
(a) (b)
Figure 2: Two Alternative Graphs from the Same Data
uation showing the system?s success, with particu-
lar attention given to the impact of evidence from
shallow processing of the caption, and Section 7 dis-
cusses future work.
Although we believe that our findings are ex-
tendible to other kinds of information graphics, our
current work focuses on bar charts. This research is
part of a larger project whose goal is a natural lan-
guage system that will provide effective access to
information graphics for individuals with sight im-
pairments, by inferring the intended message under-
lying the graphic, providing an initial summary of
the graphic that includes the intended message along
with notable features of the graphic, and then re-
sponding to follow-up questions from the user.
2 Related Work
Our work is related to efforts on graph summariza-
tion. (Yu et al, 2002) used pattern recognition tech-
niques to summarize interesting features of automat-
ically generated graphs of time-series data from a
gas turbine engine. (Futrelle and Nikolakis, 1995)
developed a constraint grammar for parsing vector-
based visual displays and producing representations
of the elements comprising the display. The goal
of Futrelle?s project is to produce a graphic that
summarizes one or more graphics from a document
(Futrelle, 1999). The summary graphic might be a
simplification of a graphic or a merger of several
graphics from the document, along with an appropri-
ate summary caption. Thus the end result of summa-
rization will itself be a graphic. The long range goal
of our project, on the other hand, is to provide alter-
native access to information graphics via an initial
textual summary followed by an interactive follow-
up component for additional information. The in-
tended message of the graphic will be an important
component of the initial summary, and hypothesiz-
ing it is the goal of our current work.
3 Evidence about the Intended Message
The graphic designer has many alternative ways of
designing a graphic; different designs contain differ-
ent communicative signals and thus convey differ-
ent communicative intents. For example, consider
the two graphics in Figure 2. The graphic in Fig-
ure 2a conveys that average doctor visits per year
is U-shaped by age; it starts out high when one is
very young, decreases into middle age, and then
rises again as one ages. The graphic in Figure 2b
presents the same data; but instead of conveying a
trend, this graphic seems to convey that the elderly
and the young have the highest number of doctor vis-
its per year. These graphics illustrate how choice of
design affects the message that the graphic conveys.
Following the AutoBrief work (Kerpedjiev and
Roth, 2000) (Green et al, 2004) on generating
graphics that fulfill communicative goals, we hy-
pothesize that the designer chooses a design that best
facilitates the perceptual and cognitive tasks that
are most important to conveying his intended mes-
sage, subject to the constraints imposed by compet-
ing tasks. By perceptual tasks we mean tasks that
can be performed by simply viewing the graphic,
such as finding the top of a bar in a bar chart; by
cognitive tasks we mean tasks that are done via men-
tal computations, such as computing the difference
between two numbers.
Thus one source of evidence about the intended
message is the relative difficulty of the perceptual
tasks that the viewer would need to perform in order
to recognize the message. For example, determining
224
the entity with maximum value in a bar chart will be
easiest if the bars are arranged in ascending or de-
scending order of height. We have constructed a set
of rules, based on research by cognitive psycholo-
gists, that estimate the relative difficulty of perform-
ing different perceptual tasks; these rules have been
validated by eye-tracking experiments and are pre-
sented in (Elzer et al, 2004).
Another source of evidence is entities that have
been made salient in the graphic by some kind of fo-
cusing device, such as coloring some elements of the
graphic, annotations such as an asterisk, or an arrow
pointing to a particular location in a graphic. Enti-
ties that have been made salient suggest particular
instantiations of perceptual tasks that the viewer is
expected to perform, such as comparing the heights
of two highlighted bars in a bar chart.
And lastly, one would expect captions to help con-
vey the intended message of an information graphic.
The next section describes a corpus study that we
performed in order to explore the usefulness of cap-
tions and how we might exploit evidence from them.
4 A Corpus Study of Captions
Although one might suggest relying almost ex-
clusively on captions to interpret an information
graphic, (Corio and Lapalme, 1999) found in a cor-
pus study that captions are often very general. The
objective of their corpus study was to categorize the
kinds of information in captions so that their find-
ings could be used in forming rules for generating
graphics with captions.
Our project is instead concerned with recogniz-
ing the intended message of an information graphic.
To investigate how captions might be used in a sys-
tem for understanding information graphics, we per-
formed a corpus study in which we analyzed the
first 100 bar charts from our corpus of information
graphics; this corpus contains a variety of bar charts
from different publication venues. The following
subsections present the results of this corpus study.
4.1 Do Captions Convey the Intended
Message?
Our first investigation explored the extent to which
captions capture the intended message of an infor-
mation graphic. We extracted the first 100 graphics
Category #
Category-1: Captures intention (mostly) 34
Category-2: Captures intention (somewhat) 15
Category-3: Hints at intention 7
Category-4: No contribution to intention 44
Figure 3: Analysis of 100 Captions on Bar Charts
from our corpus of bar charts. The intended mes-
sage of each bar chart had been previously annotated
by two coders. The coders were asked to identify
1) the intended message of the graphic using a list
of 12 high-level intentions (see Section 5 for exam-
ples) and 2) the instantiation of the parameters. For
example, if the coder classified the intended mes-
sage of a graphic as Change-trend, the coder was
also asked to identify where the first trend began,
its general slope (increasing, decreasing, or stable),
where the change in trend occurred, the end of the
second trend, and the slope of the second trend. If
there was disagreement between the coders on either
the intention or the instantiation of the parameters,
we utilized consensus-based annotation (Ang et al,
2002), in which the coders discussed the graphic to
try to come to an agreement. As observed by (Ang
et al, 2002), this allowed us to include the ?harder?
or less obvious graphics in our study, thus lowering
our expected system performance. We then exam-
ined the caption of each graphic, and determined to
what extent the caption captured the graphic?s in-
tended message. Figure 3 shows the results. 44%
of the captions in our corpus did not convey to any
extent the message of the information graphic. The
following categorizes the purposes that these cap-
tions served, along with an example of each:
? general heading (8 captions): ?UGI Monthly
Gas Rates? on a graphic conveying a recent
spike in home heating bills.
? reference to dependent axis (15 captions):
?Lancaster rainfall totals for July? on a
graphic conveying that July-02 was the driest
of the previous decade.
? commentary relevant to graphic (4 captions):
?Basic performers: One look at the best per-
forming stocks in the Standard&Poor?s 500 in-
dex this year shows that companies with ba-
sic businesses are rewarding investors? on a
225
graphic conveying the relative rank of different
stocks, some of which were basic businesses
and some of which were not. This type of in-
formation was classified as deductive by (Corio
and Lapalme, 1999) since it draws a conclusion
from the data depicted in the graphic.
? commentary extending message of graphic (8
captions): ?Profits are getting squeezed? on
a graphic conveying that Southwest Airlines
net income is estimated to increase in 2003 af-
ter falling the preceding three years. Here the
commentary does not draw a conclusion from
the data in the graphic but instead supplements
the graphic?s message. However this type of
caption would probably fall into the deductive
class in (Corio and Lapalme, 1999).
? humor (7 captions): ?The Sound of Sales? on
a graphic conveying the changing trend (down-
ward after years of increase) in record album
sales. This caption has nothing to do with the
change-trend message of the graphic, but ap-
pears to be an attempt at humor.
? conclusion unwarranted by graphic (2 cap-
tions): ?Defense spending declines? on a
graphic that in fact conveys that recent defense
spending is increasing.
Slightly over half the captions (56%) contributed
to understanding the graphic?s intended message.
34% were judged to convey most of the intended
message. For example, the caption ?Tennis play-
ers top nominees? appeared on a graphic whose in-
tended message is to convey that more tennis players
were nominated for the 2003 Laureus World Sports
Award than athletes from any other sport. Since we
argue that captions alone are insufficient for inter-
preting information graphics, in the few cases where
it was unclear whether a caption should be placed
in Category-1 or Category-2, we erred on the side
of over-rating the contribution of a caption to the
graphic?s intended message. For example, consider
the caption ?Chirac is riding high in the polls?
which appeared on a graphic conveying that there
has been a steady increase in Chirac?s approval rat-
ings from 55% to about 75%. Although this caption
does not fully capture the communicative intention
of the graphic (since it does not capture the steady
increase conveyed by the graphic), we placed it in
the first category since one might argue that riding
high in the polls would suggest both high and im-
proving ratings.
15% of the captions were judged to convey only
part of the graphic?s intended message; an example
is ?Drug spending for young outpace seniors? that
appears on a graphic whose intended message ap-
pears to be that there is a downward trend by age for
increased drug spending; we classified the caption
in Category-2 since the caption fails to capture that
the graphic is talking about percent increases in drug
spending, not absolute drug spending, and that the
graphic conveys the downward trend for increases in
drug spending by age group, not just that increases
for the young were greater than for the elderly.
7% of the captions were judged to only hint at the
graphic?s message. An example is ?GM?s Money
Machine? which appeared on a graphic whose in-
tended message was a contrast of recent perfor-
mance against the previous trend ? ie., that al-
though there had been a steady decrease in the per-
centage of GM?s overall income produced by its fi-
nance unit, there was now a substantial increase in
the percentage provided by the finance unit. Since
the term money machine is a colloquialism that sug-
gests making a lot of money, the caption was judged
to hint at the graphic?s intended message.
4.2 Understanding Captions
For the 49 captions in Category 1 or 2 (where the
caption conveyed at least some of the message of
the graphic), we examined how well the caption
could be parsed and understood by a natural lan-
guage system. We found that 47% were fragments
(for example, ?A Growing Biotech Market?), or in-
volved some other kind of ill-formedness (for ex-
ample, ?Running tops in sneaker wear in 2002? or
?More seek financial aid?1). 16% would require ex-
tensive domain knowledge or analogical reasoning
to understand. One example is ?Chirac is riding
high in the polls? which would require understand-
ing the meaning of riding high in the polls. Another
example is ?Bad Moon Rising?; here the verb ris-
ing suggests that something is increasing, but the
1Here we judge the caption to be ill-formed due to the ellip-
sis since More should be More students.
226
system would need to understand that a bad moon
refers to something undesirable (in this case, delin-
quent loans).
4.3 Simple Evidence from Captions
Although our corpus analysis showed that captions
can be helpful in understanding the message con-
veyed by an information graphic, it also showed that
full understanding of a caption would be problem-
atic; moreover, once the caption was understood, we
would still need to relate it to the information ex-
tracted from the graphic itself, which appears to be
a difficult problem.
Thus we began investigating whether shallow pro-
cessing of the caption might provide evidence that
could be effectively combined with other evidence
obtained from the graphic itself. Our analysis pro-
vided the following observations:
? Verbs in a caption often suggest the kind of
message being conveyed by the graphic. An
example from our corpus is ?Boating deaths
decline?; the verb decline suggests that the
graphic conveys a decreasing trend. Another
example from our corpus is ?American Express
total billings still lag?; the verb lag suggests
that the graphic conveys that some entity (in
this case American Express) is ranked behind
some others.
? Adjectives in a caption also often suggest the
kind of message being conveyed by the graphic.
An example from our corpus is ?Air Force has
largest percentage of women?; the adjective
largest suggests that the graphic is conveying
an entity whose value is largest. Adjectives de-
rived from verbs function similarly to verbs.
An example from our corpus is ?Soaring De-
mand for Servers? which is the caption on a
graphic that conveys the rapid increase in de-
mand for servers. Here the adjective soaring is
derived from the verb soar, and suggests that
the graphic is conveying a strong increase.
? Nouns in a caption often refer to an entity that
is a label on the independent axis. When this
occurs, the caption brings the entity into focus
and suggests that it is part of the intended mes-
sage of the graphic. An example from our cor-
pus is ?Germans miss their marks? where the
graphic displays a bar chart that is intended to
convey that Germans are the least happy with
the Euro. Words that usually appear as verbs,
but are used in the caption as a noun, may func-
tion similarly to verbs. An example is ?Cable
On The Rise?; in this caption, rise is used as a
noun, but suggests that the graphic is conveying
an increase.
5 Utilizing Evidence
We developed and implemented a probabilistic
framework for utilizing evidence from a graphic and
its caption to hypothesize the graphic?s intended
message. To identify the intended message of a
new information graphic, the graphic is first given
to a Visual Extraction Module (Chester and Elzer,
2005) that is responsible for recognizing the indi-
vidual components of a graphic, identifying the re-
lationship of the components to one another and to
the graphic as a whole, and classifying the graphic
as to type (bar chart, line graph, etc.); the result is
an XML file that describes the graphic and all of its
components.
Next a Caption Processing Module analyzes the
caption. To utilize verb-related evidence from cap-
tions, we identified a set of verbs that would indicate
each category of high-level goal2, such as recover
for Change-trend and beats for Relative-difference;
we then extended the set of verbs by examining
WordNet for verbs that were closely related in mean-
ing, and constructed a verb class for each set of
closely related verbs. Adjectives such as more and
most were handled in a similar manner. The Caption
Processing Module applies a part-of-speech tagger
and a stemmer to the caption in order to identify
nouns, adjectives, and the root form of verbs and
adjectives derived from verbs. The XML represen-
tation of the graphic is augmented to indicate any
independent axis labels that match nouns in the cap-
tion, and the presence of a verb or adjective class in
the caption.
The Intention Recognition Module then analyzes
the XML file to build the appropriate Bayesian net-
work; the current system is limited to bar charts, but
2As described in the next paragraph, there are 12 categories
of high-level goals.
227
the principles underlying the system should be ex-
tendible to other kinds of information graphics. The
network is described in (Elzer et al, 2005). Very
briefly, our analysis of simple bar charts has shown
that the intended message can be classified into one
of 12 high-level goals; examples of such goals in-
clude:
? Change-trend: Viewer to believe that there
is a <slope-1> trend from <param1>
to <param2> and a significantly differ-
ent <slope-2> trend from <param3> to
<param4>
? Relative-difference: Viewer to believe that the
value of element <param1> is <comparison>
the value of element <param2> where
<comparison> is greater-than, less-than, or
equal-to.
Each category of high-level goal is represented by a
node in the network (whose parent is the top-level
goal node), and instances of these goals (ie., goals
with their parameters instantiated) appear as chil-
dren with inhibitory links (Huber et al, 1994) cap-
turing their mutual exclusivity. Each goal is broken
down further into subtasks (perceptual or cognitive)
that the viewer would need to perform in order to
accomplish the goal of the parent node. The net-
work is built dynamically when the system is pre-
sented with a new information graphic, so that nodes
are added to the network only as suggested by the
graphic. For example, low-level nodes are added for
the easiest primitive perceptual tasks and for per-
ceptual tasks in which a parameter is instantiated
with a salient entity (such as an entity colored dif-
ferently from others in the graphic or an entity that
appears as a noun in the caption), since the graphic
designer might have intended the viewer to perform
these tasks; then higher-level goals that involve these
tasks are added, until eventually a link is established
to the top-level goal node.
Next evidence nodes are added to the network to
capture the kinds of evidence noted in Sections 3
and 4.3. For example, evidence nodes are added to
the network as children of each low-level perceptual
task; these evidence nodes capture the relative dif-
ficulty (categorized as easy, medium, hard, or im-
possible) of performing the perceptual task as esti-
mated by our effort estimation rules mentioned in
Section 3, whether a parameter in the task refers to
an entity that is salient in the graphic, and whether
a parameter in the task refers to an entity that is a
noun in the caption. An evidence node, indicating
for each verb class whether that verb class appears
in the caption (either as a verb, or as an adjective de-
rived from a verb, or as a noun that can also serve as
a verb) is added as a child of the top level goal node.
Adjectives such as more and most that provide evi-
dence are handled in a similar manner.
In a Bayesian network, conditional probability ta-
bles capture the conditional probability of a child
node given the value of its parent(s). For example,
the network requires the conditional probability of
an entity appearing as a noun in the caption given
that recognizing the intended message entails per-
forming a particular perceptual task involving that
entity. Similarly, the network requires the condi-
tional probability, for each class of verb, that the
verb class appears in the caption given that the in-
tended message falls into a particular intention cat-
egory. These probabilities are learned from our cor-
pus of graphics, as described in (Elzer et al, 2005).
6 Evaluation
In this paper, we are particularly interested in
whether shallow processing of captions can con-
tribute to recognizing the intended message of an
information graphic. As mentioned earlier, the in-
tended message of each information graphic in our
corpus of bar charts had been previously annotated
by two coders. To evaluate our approach, we used
leave-one-out cross validation. We performed a se-
ries of experiments in which each graphic in the cor-
pus is selected once as the test graphic, the probabil-
ity tables in the Bayesian network are learned from
the remaining graphics, and the test graphic is pre-
sented to the system as a test case. The system was
judged to fail if either its top-rated hypothesis did
not match the intended message that was assigned
to the graphic by the coders or the probability rat-
ing of the system?s top-rated hypothesis did not ex-
ceed 50%. Overall success was then computed by
averaging together the results of the whole series of
experiments.
Each experiment consisted of two parts, one in
228
Diner?s Club
Discover
American Express
Mastercard
Visa
400 600200
Total credit card purchases per year in billions
Figure 4: A Graphic from Business Week3
which captions were not taken into account in the
Bayesian network and one in which the Bayesian
network included evidence from captions. Our
overall accuracy without the caption evidence was
64.5%, while the inclusion of caption evidence in-
creased accuracy to 79.1% for an absolute increase
in accuracy of 14.6% and a relative improvement of
22.6% over the system?s accuracy without caption
evidence. Thus we conclude that shallow process-
ing of a caption provides evidence that can be effec-
tively utilized in a Bayesian network to recognize
the intended message of an information graphic.
Our analysis of the results provides some interest-
ing insights on the role of elements of the caption.
There appear to be two primary functions of verbs.
The first is to reflect what is in the data, thereby
strengthening the message that would be recognized
without the caption. One example from our corpus
is a graphic with the caption ?Legal immigration to
the U.S. has been rising for decades?. Although
the early part of the graphic displays a change from
decreasing immigration to a steadily increasing im-
migration trend, most of the graphic focuses on the
decades of increasing immigration and the caption
strengthens increasing trend in immigration as the
intended message of the graphic. If we do not in-
clude the caption, our system hypothesizes an in-
creasing trend message with a probability of 66.4%;
other hypotheses include an intended message that
emphasizes the change in trend with a probability
of 15.3%. However, when the verb increasing from
the caption is taken into account, the probability of
increasing trend in immigration being the intended
message rises to 97.9%.
3This is a slight variation of the graphic from Business
Week. In the Business Week graphic, the labels sometimes ap-
The second function of a verb is to focus atten-
tion on some aspect of the data. For example, con-
sider the graphic in Figure 4. Without a caption, our
system hypothesizes that the graphic is intended to
convey the relative rank in billings of different credit
card issuers and assigns it a probability of 72.7%.
Other possibilities have some probability assigned
to them. For example, the intention of conveying
that Visa has the highest billings is assigned a prob-
ability of 26%. Suppose that the graphic had a cap-
tion of ?Billings still lag?; if the verb lag is taken
into account, our system hypothesizes an intended
message of conveying the credit card issuer whose
billings are lowest, namely Diner?s Club; the prob-
ability assigned to this intention is now 88.4%, and
the probability assigned to the intention of convey-
ing the relative rank of different credit card issuers
drops to 7.8%. This is because the verb class con-
taining lag appeared in our corpus as part of the cap-
tion for graphics whose message conveyed an en-
tity with a minimum value, and not with graphics
whose message conveyed the relative rank of all the
depicted entities. On the other hand, if the caption
is ?American Express total billings still lag? (which
is the caption associated with the graphic in our cor-
pus), then we have two pieces of evidence from the
caption ? the verb lag, and the noun American Ex-
press which matches a label. In this case, the proba-
bilities change dramatically; the hypothesis that the
graphic is intended to convey the rank of American
Express (namely third behind Visa and Mastercard)
is assigned a probability of 76% and the probability
drops to 24% that the graphic is intended to con-
vey that Diner?s Club has the lowest billings. This is
not surprising. The presence of the noun American
Express in the caption makes that entity salient and
is very strong evidence that the intended message
places an emphasis on American Express, thus sig-
nificantly affecting the probabilities of the different
hypotheses. On the other hand, the verb class con-
taining lag occurred both in the caption of graphics
whose message was judged to convey the entity with
the minimum value and in the caption of graphics
pear on the bars and sometimes next to them, and the heading
for the dependent axis appears in the empty white space of the
graphic instead of below the values on the horizontal axis as we
show it. Our vision system does not yet have heuristics for rec-
ognizing non-standard placement of labels and axis headings.
229
that conveyed an entity ranked behind some others.
Therefore, conveying the entity with minimum value
is still assigned a non-negligible probability.
7 Future Work
It is rare that a caption contains more than one verb
class; when it does happen, our current system by
default uses the first one that appears. We need to
examine how to handle the occurrence of multiple
verb classes in a caption. Occasionally, labels in the
graphic appear differently in the caption. An exam-
ple is DJIA (for Dow Jones Industrial Average) that
occurs in one graphic as a label but appears as Dow
in the caption. We need to investigate resolving such
coreferences.
We currently limit ourselves to recognizing what
appears to be the primary communicative intention
of an information graphic; in the future we will also
consider secondary intentions. We will also extend
our work to other kinds of information graphics such
as line graphs and pie charts, and to complex graph-
ics, such as grouped and composite bar charts.
8 Summary
To our knowledge, our project is the first to inves-
tigate the problem of understanding the intended
message of an information graphic. This paper
has focused on the communicative evidence present
in an information graphic and how it can be used
in a probabilistic framework to reason about the
graphic?s intended message. The paper has given
particular attention to evidence provided by the
graphic?s caption. Our corpus study showed that
about half of all captions contain some evidence that
contributes to understanding the graphic?s message,
but that fully understanding captions is a difficult
problem. We presented a strategy for extracting ev-
idence from a shallow analysis of the caption and
utilizing it, along with communicative signals from
the graphic itself, in a Bayesian network that hy-
pothesizes the intended message of an information
graphic, and our results demonstrate the effective-
ness of our methodology. Our research is part of a
larger project aimed at providing alternative access
to information graphics for individuals with sight
impairments.
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. of the Int?l Conf. on Spoken Language Process-
ing (ICSLP).
D. Chester and S. Elzer. 2005. Getting computers to see
information graphics so users do not have to. To ap-
pear in Proc. of the 15th Int?l Symposium on Method-
ologies for Intelligent Systems.
H. Clark. 1996. Using Language. Cambridge University
Press.
M. Corio and G. Lapalme. 1999. Generation of texts
for information graphics. In Proc. of the 7th European
Workshop on Natural Language Generation, 49?58.
S. Elzer, S. Carberry, N. Green, and J. Hoffman. 2004.
Incorporating perceptual task effort into the recogni-
tion of intention in information graphics. In Proceed-
ings of the 3rd Int?l Conference on Diagrams, LNAI
2980, 255?270.
S. Elzer, S. Carberry, I. Zukerman, D. Chester, N. Green,
S. Demir. 2005. A probabilistic framework for recog-
nizing intention in information graphics. To appear in
Proceedings of the Int?l Joint Conf. on AI (IJCAI).
R. Futrelle and N. Nikolakis. 1995. Efficient analysis of
complex diagrams using constraint-based parsing. In
Proc. of the Third International Conference on Docu-
ment Analysis and Recognition.
R. Futrelle. 1999. Summarization of diagrams in docu-
ments. In I. Mani and M. Maybury, editors, Advances
in Automated Text Summarization. MIT Press.
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev,
Joe Mattis, Johanna Moore, and Steven Roth. Auto-
brief: an experimental system for the automatic gen-
eration of briefings in integrated text and information
graphics. International Journal of Human-Computer
Studies, 61(1):32?70, 2004.
H. P. Grice. 1969. Utterer?s Meaning and Intentions.
Philosophical Review, 68:147?177.
M. Huber, E. Durfee, and M. Wellman. 1994. The auto-
mated mapping of plans for plan recognition. In Proc.
of Uncertainty in AI, 344?351.
S. Kerpedjiev and S. Roth. 2000. Mapping communica-
tive goals into conceptual tasks to generate graphics in
discourse. In Proc. of Int. Conf. on Intelligent User
Interfaces, 60?67.
J. Yu, J. Hunter, E. Reiter, and S. Sripada. 2002.
Recognising visual patterns to communicate gas tur-
bine time-series data. In ES2002, 105?118.
230
Extending Document Summarization to Information Graphics
?Sandra Carberry, ??Stephanie Elzer, ? ? ?Nancy Green, ?Kathleen McCoy and ?Daniel Chester
?Dept. of Computer Science, University of Delaware, Newark, DE 19716
(carberry, mccoy, chester@cis.udel.edu)
??Dept. of Computer Science, Millersville Univ., Millersville, PA 17551
(elzer@cs.millersville.edu)
? ? ?Dept. of Math. Sciences, Univ. of North Carolina at Greensboro, Greensboro, NC 27402
(nlgreen@uncg.edu)
Abstract
Information graphics (non-pictorial graphics such
as bar charts or line graphs) are an important
component of multimedia documents. Often such
graphics convey information that is not contained
elsewhere in the document. Thus document summa-
rization must be extended to include summarization
of information graphics. This paper addresses our
work on graphic summarization. It argues that the
message that the graphic designer intended to con-
vey must play a major role in determining the con-
tent of the summary, and it outlines our approach
to identifying this intended message and using it to
construct the summary.
1 Introduction
Summarization work has focused primarily on the
written words in a document. However, graphics
are an important part of many documents, and they
often convey information that is not included else-
where in the document. Thus as text summarization
branches out, it is essential that it consider the sum-
marization of graphical information in documents.
Graph summarization has received some atten-
tion. (Yu et al, 2002) has used pattern recogni-
tion techniques to summarize interesting features of
automatically generated graphs of time-series data
from a gas turbine engine. (Futrelle and Nikolakis,
1995) developed a constraint grammar formalism
for parsing vector-based visual displays and produc-
ing structured representations of the elements com-
prising the display. The goal of Futrelle?s project
is to produce a graphic that summarizes one or
more graphics from a document (Futrelle, 1999).
The summary graphic might be a simplification of
a graphic or a merger of several graphics from the
document, along with an appropriate summary cap-
tion. Thus the end result of summarization will it-
self be a graphic.
Our project is concerned with information graph-
ics (non-pictorial graphics such as bar charts or line
graphs). Our current focus is on providing an ini-
tial summary of an information graphic, within a
larger interactive natural language system that can
respond to followup questions about the graphic.
There are several useful applications for a system
that can summarize information graphics. For dig-
ital libraries, the initial summary of the graphic
will be used in conjunction with the document
text/summary to provide a more complete represen-
tation of the content of the document to be used
for searching and indexing. In the case of environ-
ments with low-bandwidth transmission and minia-
ture viewing facilities, such as cellular telephones
for accessing the web, the initial summary and fol-
lowup capability will provide an alternative modal-
ity for access to the document.
However, the most compelling application of the
overall system is to provide effective access to in-
formation graphics for individuals with sight im-
pairments. The rapidly growing Information Infras-
tructure has had a major impact on society and the
development of technology. However, the growing
reliance on visual information display paradigms
obliges society to ensure that individuals with visual
impairments can access and assimilate information
resources as effectively as their sighted counter-
parts. The underlying hypothesis of our work is that
alternative access to what the graphic looks like is
not enough ? the user should be provided with the
message and knowledge that one would gain from
viewing the graphic in order to enable effective and
efficient use of this information resource. Thus our
system will present the user with an initial summary
that includes the primary message that the graphic
designer intended to convey, augmented with rel-
evant interesting features of the graphic, and then
interactively allow the user to access more detailed
summaries of information contained in the graphic.
As an example of the kinds of summaries that we
envision, consider the information graphic in Fig-
ure 1. The graphic designer?s communicative goal is
ostensibly to convey the sharp increase in bankrupt-
cies in 2001 compared with the previous decreasing
trend. More detailed features that might be of inter-
est include 1) that bankruptcies had been decreasing
at a steady rate since 1998, 2) that bankruptcies had
been decreasing slowly since 1998, 3) the percent-
age decrease each year, 4) the percentage increase
in bankruptcies in 2001, 5) the absolute increase in
bankruptcies in 2001, and 6) the total number of
bankruptcies in 2001. Thus the initial summary of
this graphic might be
This graphic shows that although
Delaware bankruptcy personal filings
decreased slowly and steadily from 1998
to 2000, they rose sharply in 2001.
Note that the proposed summary includes the hy-
pothesized intended message of the graphic, along
with the first two of the additional interesting fea-
tures of the graphic. The selection of additional fea-
tures to augment the summary is discussed further
in Section 3.3. The system would then respond to
user requests for additional information by present-
ing some or all of the other interesting features that
had been identified, as discussed in Section 3.4.
This paper provides an overview of our project.
Section 2 discusses the essential role of intention
recognition in graphics summarization. It argues
not only that the intended message of the graphic
designer must be inferred and included in a sum-
mary of a graphic, but also that the intended mes-
sage significantly influences the additional propo-
sitions that should be included in the summary.
Section 3 presents our approach to graph summa-
rization. It discusses how we use a computer vi-
sion module to construct an XML representation
that captures the components of the graphic and
their relationship to one another, and how we use
a Bayesian belief network to hypothesize the inten-
tions of the graph designer. The paper then dis-
cusses our plans for constructing a summary that
includes the graphic designer?s intended message
along with highly ranked additional propositions,
and how the lesser ranked propositions will be used
in an interactive natural language system that re-
sponds to the user?s requests for further summaries
of additional features of the graphic.
2 The Role of Intention in Graphics
Summarization
Text summarization has generally relied on statis-
tical techniques and identification and extraction
of key sentences from documents. However, it is
widely acknowledged that to truly understand a text
and produce the best summary, one must under-
stand the document and recognize the intentions of
the author. Recent work in text summarization has
personal filings
Delaware bankruptcy
3000
2500
1000
1500
2000
1998 1999 2000 2001
Figure 1: Graphic from a City Newspaper
60 70 80 90 01
$15
10
5
Black women
White women
Median Income
In thousands of 2001 dollars
1948
Figure 2: Graphic from Newsweek Magazine
begun to address this issue. For example, (Marcu,
2000) presents algorithms for automatically identi-
fying the rhetorical structure of a text and argues
that the hypothesized rhetorical structure can be
successfully used in text summarization.
Information graphics are an important component
of many documents. In some cases, information
graphics are stand-alone and constitute the entire
document. This is the case for many graphics ap-
pearing in newspapers, such as the graphic shown
in Figure 1. On the other hand, when an article is
comprised of text and graphics, the graphic gener-
ally expands on the text and contributes to the dis-
course purpose (Grosz and Sidner, 1986) of the arti-
cle. For example, Figure 2 illustrates a graphic from
Newsweek showing that the income of black women
has risen dramatically over the last decade and has
reached the level of white women. Although this in-
formation is not conveyed elsewhere in the article, it
contributes to the overall communicative intention
of this portion of the article ? namely, that there
has been a ?monumental shifting of the sands? with
regard to the achievements of black women.
Our project is concerned with the understand-
ing and summarization of information graphics: bar
charts, line graphs, pie charts, etc. We contend that
analyzing the data points underlying an informa-
tion graphic is insufficient. One must instead iden-
tify the message that the graphic designer intended
to convey via the design choices that were made
in constructing the graphic. (Although one might
suggest relying on captions to provide the intended
message of a graphic, Corio and Lapalme found
in a large corpus study (Corio and Lapalme, 1999)
that captions are often missing or are very general
and uninformative; our collected corpus of informa-
tion graphics supports their observations.) Design
choices include selection of chart type (bar chart,
pie chart, line graph, etc.), organization of informa-
tion in the chart (for example, aggregation of bars in
a bar chart), and attention-getting devices that high-
light certain aspects of a chart (such as coloring one
bar of a bar chart different from the others). Not
only should the graphic designer?s intended mes-
sage comprise the primary component of any sum-
mary, but this intended message has a strong influ-
ence on the salience of additional propositions that
might be included in the summary.
To see the importance of recognizing the graphic
designer?s intended message, consider the two
graphics in Figure 3. The one on the left, Fig-
ure 3a, appeared in an NSF publication. Both graph-
ics were constructed from the same data set. The
intended message of the graphic in Figure 3a is that
the salary of females is consistently less than that of
males for each of the science and engineering dis-
ciplines.1 Notice that the graphic designer selected
an organization for the graphic in Figure 3a that fa-
cilitated the comparison between male and female
salaries in each field. A different display of the
same data would facilitate different analyses. For
example, the graph in Figure 3b depicts the same
data as the graph in Figure 3a, yet the organiza-
tion tends to draw attention to comparisons within
male and female groups rather than between them,
1This graphic was constructed by a colleague who served
on the NSF panel that prepared the report. Thus we know the
intentions underlying the graphic.
and perhaps an integration/comparison of the mes-
sages conveyed by the two subgraphs. Thus the in-
tended message of the graphic in Figure 3b appears
to be that the ranking of the disciplines by salary are
about the same for both men and women. The dis-
tinctions between presentation formats illustrate the
extent to which the format can itself convey infor-
mation relevant to the graphic designer?s intended
message.
Now let us consider how the intended message
influences additional information that might be in-
cluded in a summary. Suppose that 1) the salary
differential between females and males was signif-
icantly larger in the life sciences than in other dis-
ciplines and 2) the average salary for both females
and males was much larger in engineering than in
any of the other disciplines. Feature 1) would be
particularly interesting and relevant to the intended
message of Figure 3a, and thus should be included
as part of the graphic?s summary. On the other hand,
this aspect would be less relevant to the intended
message of Figure 3b and thus not as important to
include. Similarly, Feature 2) would be particularly
relevant to the intended message of Figure 3b and
thus should be given high priority for inclusion in
its summary. Although an interactive system that
could analyze a graphic to any desired level of de-
tail might extract from the graphic the information
in both 1) and 2) above, we contend that a summary
of the graphic should prioritize content according to
its relevance to the designer?s intended message.
3 Graphic Summarization
Our architecture for graphic summarization consists
of modules for identifying the components of the
graphic, hypothesizing the graphic designer?s in-
tended message, planning the content of the sum-
mary, organizing a coherent summary, and interac-
tive followup. The following sections discuss four
of these modules.
3.1 Analyzing and Classifying a Graphic
The visual extraction module takes a screen image
of an information graphic. It is responsible for rec-
ognizing the individual components comprising the
graphic, identifying the relationship of the different
components to one another and to the graphic as a
whole, and classifying the graphic as to type. This
includes using heuristics (such as relative position
of a string of characters) to identify the axis labels
? for example, that the y-axis label is Delaware
2The source of the leftmost graph is the National Science
Foundation, Survey of Doctorate Recipients, 1997.
  
 
 
 
 
 
 

















 
 
 
 
 
 
 
 
 




































		
		
		
		
		
		
		
		
		





































80,000
70,000
60,000
50,000 50,000
60,000
70,000
80,000
40,000
30,000
20,000 20,000
30,000
40,000
FEMALE SALARIES MALE SALARIES
Computer/All
Math Sci
Engin. Phys.
Sci. Sci.
Social
Sci.
Life Sci.
Social Sci.
A
ll
Com
puter/M
ath Sci.
Phys Sci.
Engineering
Social Sci.
Life Sci.
Com
puter/M
ath Sci.
A
ll
Phys Sci.
Engineering
Life
Female
Male
(a) (b)
Figure 3: Two alternative graphs from the same data2
bankruptcy personal filings in Figure 1. Our cur-
rent implementation deals only with gray scale im-
ages (in pgm format) of bar charts, pie charts, and
line graphs, though eventually it will be extended to
handle color and other kinds of information graph-
ics. The output of the visual extraction component
is an XML file that describes the chart and all of its
components.
3.2 Identifying the Intended Message
The second module of our architecture is respon-
sible for inferring the graphic designer?s intended
message. In their work on multimedia generation,
the AutoBrief group proposed that speech act the-
ory can be extended to the generation of graphical
presentations (Kerpedjiev and Roth, 2000; Green et
al., 2004). They contended that the graphic design
was intended to convey its message by facilitating
requisite perceptual and cognitive tasks. By percep-
tual tasks we mean tasks that can be performed by
simply viewing the graphic, such as finding the top
of a bar in a bar chart; by cognitive tasks we mean
tasks that are done via mental computations, such as
computing the difference between two numbers.
The goal of our intention recognizer is the inverse
of the design process: namely, to use the displayed
graphic as evidence to hypothesize the communica-
tive intentions of its author. This is done by an-
alyzing the graphic to identify evidence about the
designer?s intended message and then using plan
recognition (Carberry, 1990) to hypothesize the au-
thor?s communicative intent.
3.2.1 Evidence about Intention
Following AutoBrief (Kerpedjiev and Roth, 2000),
we hypothesize that the graphic designer chooses
a design that makes important tasks (the ones that
the viewer is intended to perform in recognizing the
graphic?s message) as salient or as easy as possi-
ble. Thus salience and ease of performance should
be taken into account in reasoning about the graphic
designer?s intentions.
There are several ways that a task can be made
salient. The graphic designer can draw attention
to a component of a graphic (make it salient) by
an attention-getting or highlighting device, such as
by coloring a bar in a bar chart differently from
the other bars as in Figure 1 or by exploding a
wedge in a pie chart (Mittal, 1997). Attributes of
the highlighted graphic component are treated as
focused entities. Nouns in captions also serve to
establish focused entities. For example, a caption
such as ?Studying not top priority? would estab-
lish the noun studying as a focused entity. Focused
entities that appear as instantiations of parameters
in perceptual or cognitive tasks serve as evidence
that those tasks might be particularly salient. Sim-
ilarly, verbs that appear in captions serve as evi-
dence for the salience of particular tasks. For ex-
ample, the verb beats in a caption such as ?Canada
Beats Europe? serves as evidence for the salience
of a Recognize relative difference task. In the fu-
ture, we plan to capture the influence of surrounding
text by identifying the important concepts from the
text using lexical chains. Lexical chains have been
used in text summarization (Barzilay et al, 1999),
and our linear time algorithm (Silber and McCoy,
2002) makes their computation feasible even for
large texts. Whether a task is salient and the method
by which it was made salient are used as evidence
in our plan inference system.
The graphic design makes some tasks easier than
others. We use a set of rules, based on research by
cognitive psychologists, to estimate the relative ef-
fort of performing different perceptual and cogni-
tive tasks. These rules, described in (Elzer et al,
2004), have been validated by eye-tracking experi-
ments. Since the viewer is intended to recognize the
message that the graphic designer wants to convey,
we contend that the designer will choose a graphic
design that makes the requisite tasks easy to per-
form. This was illustrated in the two graphics in
Figure 3. The relative effort of performing a task is
thus used as another source of evidence in our plan
inference framework.
3.2.2 The Plan Inference Process
Our plan inference framework takes the form of
a Bayesian belief network. Bayesian belief net-
works have been applied to a variety of problems,
including reasoning about utterances (Charniak and
Goldman, 1993) and observed actions (Albrecht et
al., 1997). The belief network uses plan operators,
along with evidence that is gleaned from the infor-
mation graphic itself (as discussed in the preceding
section), to reason about the likelihood that vari-
ous hypothesized candidate plans represent the in-
tentions of the graphic designer.
Plan Operators for Information Graphics Our
system uses plan operators that capture knowledge
about how the graphic designer?s goal of conveying
a message can be achieved via the viewer perform-
ing certain perceptual and cognitive tasks, as well
as knowledge about how information-access tasks,
such as finding the value of an entity in a graphic,
can be decomposed into simpler subgoals. Our plan
operators consist of:
? Goal: the goal that the operator achieves
? Data-requirements: requirements that the data
must satisfy in order for the operator to be ap-
plicable in a graphic planning paradigm
? Display-constraints: features that constrain
how the graphic is eventually constructed if
this operator is part of the final plan
? Body: lower-level subgoals that must be ac-
complished in order to achieve the overall goal
of the operator.
Figures 4 and 5 present two plan operators for the
goal of finding the value <v> of an attribute <att>
for a graphical element <e> (for example, the value
associated with the top of a bar in a bar chart). The
body of the operator in Figure 4 specifies that the
goal can be achieved by a primitive perceptual task
in which the viewer just perceives the value; this
could be done, for example, if the element in the
graphic is annotated with its value. On the other
hand, the body of the operator in Figure 5 captures a
different way of finding the value, one that presum-
ably requires more effort. It specifies the perceptual
task of finding the values <l1> and <l2> surround-
ing the desired value on the axis along with the frac-
tion <f> of the distance that the desired value lies
between <l1> and <l2>, followed by the cogni-
tive task of interpolating between the retrieved val-
ues <l1> and <l2>.
Plan inference uses the plan operators to reasons
backwards from the XML representation of the ob-
served graphic (constructed by the visual extraction
module briefly described in Section 3.1). The dis-
play constraints are used to eliminate operators from
consideration ? if the graphic does not capture the
operator?s constraints on the display, then the opera-
tor could not have been part of a plan that produced
the graphic. The data requirements are used to in-
stantiate parameters in the operator ? the data must
have had certain characteristics for the operator to
have been included in the graphic designer?s plan,
and these often limit how the operator?s arguments
can be instantiated.
The Bayesian Belief Network The plan operators
are used to dynamically construct a Bayesian net-
work for each new information graphic. The net-
work includes the possible top level communicative
intentions (with uninstantiated parameters), such as
the intention to convey a trend, and the alternative
ways of achieving them via different plan opera-
tors. The perceptual tasks of lowest effort and the
tasks that are hypothesized as potentially salient are
added to the network. Other tasks are entered into
the network as they are inferred during chaining on
the plan operators; unification serves to instantiate
parameters in higher-level nodes. Evidence nodes
are added for each of the tasks entered into the net-
work, and they provide evidence (such as the degree
of perceptual effort required for a task or whether
a parameter of the task is a focused entity in the
graphic as discussed in Section 3.2.1) for or against
the instantiated tasks to which they are linked. Af-
ter propagation of evidence, the top-level intention
with the highest probability is hypothesized as the
graphic designer?s primary intention for the graphic.
Of course, a Bayesian network requires a set of
conditional probabilities, such as 1) the probability
that perceptual Task-A will be of low, medium, or
high effort given that the graphic designer?s plan in-
cludes the viewer performing Task-A, 2) the prob-
ability that parameter <x> of Task-A will be a fo-
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Dependent-variable(<att>, <ds>)
Body: 1. Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Figure 4: Operator for achieving a goal perceptually
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Natural-quantitative-ordering(<att>)
Display-const: Ordered-values-on-axis(<g>, <axis>, <att>)
Body: 1. Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
2. Interpolate(<viewer>, <l1>, <l2>, <f>, <v>)
Figure 5: Operator that employs both perceptual and cognitive subgoals
cused entity in the caption given that the graphic de-
signer?s plan includes the viewer performing Task-
A, or 3) the probability that the viewer perform-
ing Task-B will be part of the designer?s intended
plan given that Task-A is part of his plan. (Note that
there may be several alternative ways of perform-
ing a particular task, as illustrated by the two plan
operators displayed in Figures 4 and 5.) We have
collected a rapidly expanding corpus of information
graphics, and have analyzed a small part of this cor-
pus to construct an initial set of probabilities. The
results suggest that our approach is very promising.
We will increase the number of analyzed graphics
to improve the probability estimates.
3.3 Planning the Content of the Summary
The recognized intention of the graphic designer,
such as to convey an overall increasing trend or to
compare salaries of females and males in different
disciplines as in Figure 3a, will provide one set of
highly salient propositions that should be included
in the graphic?s summary. Once the intentions have
been recognized, other visual features of the graphic
will influence the identification of additional salient
propositions.
We conducted a set of experiments in which sub-
jects were asked to write a brief summary of a set of
line graphs, each of which arguably could be said
to have the same high-level intention. Although
each summary included the high-level intention, the
summaries often differed significantly for different
graphs. By comparing these with summaries of the
same graph by different subjects, we have hypoth-
esized that certain features, such as the variance of
the data, can influence the generated summary, and
that the importance of including a specific feature in
a summary is related to the high-level intention of
the graphic. For example, variation in the data will
be relevant for an intention of conveying a trend,
but it will be less important than the overall slope
of the data points. This impact of the intended mes-
sage on the priority of including a specific feature
in a graphic was illustrated in Section 2, where we
showed how a significantly larger differential be-
tween female and male salaries for one particular
discipline would be more relevant to the summary of
the graphic in Figure 3a than for the graphic in Fig-
ure 3b. In addition, our experiments indicate that the
strength of a feature in the graphic also influences
its inclusion in a summary. For example, the more
ragged a sequence of line segments, the more salient
variance becomes for inclusion in a summary.
Once the content planning module has identified
and ranked interesting features that might augment
the intended message of the graphic, the most im-
portant propositions will be organized into a coher-
ent summary that can be stored for access in a digital
library or presented to a user. In the future, we will
also investigate integrating the summary of an infor-
mation graphic with the summary of its surrounding
text.
3.4 Interactive Followup
One of the primary goals of our work is an inter-
active natural language system that can convey the
content of an information graphic to a user with
sight impairments. For this application, the sum-
mary will be rendered in natural language and con-
veyed as an initial summary to the user via speech
synthesis. The system will then provide the user
with the opportunity to seek additional information.
We will utilize the propositions that were not in-
cluded in the initial message as indicative of ad-
ditional information about the graphic that might
be useful. Several kinds of followup will be pro-
vided. For example, if the user requests focused
followup, the system will categorize the remaining
propositions (for example, extreme values, trend de-
tail, etc.) and ask the user to select one of the cate-
gories of further information. The system will then
construct a followup message summarizing the most
important (often all) of the remaining propositions
in the selected category. This interactive followup
will continue until either all the propositions have
been conveyed or the user terminates the followup
cycle.
4 Summary
This paper extends document summarization to the
summarization of information graphics. It argues
that an effective summary must be based on the
message that the graphic designer intended to con-
vey in constructing the graphic, and that this in-
tended message strongly influences the relevance
of other propositions that might be included in the
summary. The paper describes our approach to
graphic summarization, including our plan infer-
ence system for inferring the intended message un-
derlying a graphic. This work has many applica-
tions. These include enabling information graphics
to be accessed via content in a digital library, allow-
ing access to information graphics via devices with
small bandwidth (such as cellular phones), and most
importantly making information graphics accessible
to individuals with sight impairments via an interac-
tive natural language system that can provide sum-
maries at various levels of detail.
References
David Albrecht, Ingrid Zukerman, Ann Nicholson,
and A. Bud. 1997. Towards a bayesian model
for keyhole plan recognition in large domains.
In Proceedings of the Sixth International Confer-
ence on User Modeling, pages 365?376.
R. Barzilay, K. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-
document summarization. In Proc. of the 37th
Annual Meeting of the ACL, pages 550?557.
Sandra Carberry. 1990. Plan Recognition in Natu-
ral Language Dialogue. ACL-MIT Press Series
on Natural Language Processing. MIT Press.
Eugene Charniak and Robert Goldman. 1993. A
bayesian model of plan recognition. Artificial In-
telligence Journal, 64:53?79.
Marc Corio and Guy Lapalme. 1999. Generation of
texts for information graphics. In Proceedings of
the 7th European Workshop on Natural Language
Generation EWNLG?99, pages 49?58.
Stephanie Elzer, Nancy Green, Sandra Carberry,
and James Hoffman. 2004. Incorporating per-
ceptual task effort into the recognition of inten-
tion in information graphics. In Diagrammatic
Representation and Inference: Proceedings of
the Third International Conference on the Theory
and Application of Diagrams, LNAI 2980, pages
255?270.
Robert Futrelle and Nikos Nikolakis. 1995. Ef-
ficient analysis of complex diagrams using
constraint-based parsing. In Proceedings of the
Third International Conference on Document
Analysis and Recognition.
Robert Futrelle. 1999. Summarization of diagrams
in documents. In I. Mani and M. Maybury, edi-
tors, Advances in Automated Text Summarization.
MIT Press.
Nancy Green, Giuseppe Carenini, Stephan Kerped-
jiev, Joe Mattis, Johanna Moore, and Steven
Roth. 2004. Autobrief: An experimental system
for the automatic generation of briefings in inte-
grated text and graphics. International Journal of
Human-Computer Studies. to appear.
Barbara Grosz and Candace Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse.
Computational Linguistics, 12(3):175?204.
Stephan Kerpedjiev and Steven Roth. 2000. Map-
ping communicative goals into conceptual tasks
to generate graphics in discourse. In Proceed-
ings of the International Conference on Intelli-
gent User Interfaces, pages 60?67.
Daniel Marcu. 2000. The rhetorical parsing of un-
restricted texts: A surface-based approach. Com-
putational Linguistics, 26(3):395?448.
Vibhu Mittal. 1997. Visual prompts and graphical
design: A framework for exploring the design
space of 2-d charts and graphs. In Proceedings
of the Fourteenth National Conference on Artifi-
cial Intelligence, pages 57?63.
Gregory Silber and Kathleen McCoy. 2002. Effi-
ciently computed lexical chains as an intermedi-
ate representation for automatic text summariza-
tion. Computational Linguistics, 28(4):487?496.
Jin Yu, Jim Hunter, Ehud Reiter, and Somaya-
julu Sripada. 2002. Recognising visual patterns
to communicate gas turbine time-series data. In
ES2002, pages 105?118.
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 41?48,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Abstractive Summarization of Line Graphs from Popular Media
Charles F. Greenbacker Peng Wu
Sandra Carberry Kathleen F. McCoy Stephanie Elzer*
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
[charlieg|pwu|carberry|mccoy]@cis.udel.edu
*Department of Computer Science
Millersville University, Millersville, Pennsylvania, USA
elzer@cs.millersville.edu
Abstract
Information graphics (bar charts, line graphs,
etc.) in popular media generally have a dis-
course goal that contributes to achieving the
communicative intent of a multimodal docu-
ment. This paper presents our work on ab-
stractive summarization of line graphs. Our
methodology involves hypothesizing the in-
tended message of a line graph and using it
as the core of a summary of the graphic. This
core is then augmented with salient proposi-
tions that elaborate on the intended message.
1 Introduction
Summarization research has focused primarily on
summarizing textual documents, and until recently,
other kinds of communicative vehicles have been
largely ignored. As noted by Clark (1996), language
is more than just words ? it is any signal that is
intended to convey a message. Information graph-
ics (non-pictorial graphics such as bar charts, line
graphs, etc.) in popular media such as Newsweek,
Businessweek, or newspapers, generally have a com-
municative goal or intended message. For exam-
ple, the graphic in Figure 1 is intended to convey
a changing trend in sea levels ? relatively flat from
1900 to 1930 and then rising from 1930 to 2003.
Thus, using Clark?s view of language, information
graphics are a means of communication.
Research has shown that the content of informa-
tion graphics in popular media is usually not re-
peated in the text of the accompanying article (Car-
berry et al, 2006). The captions of such graphics
are also often uninformative or convey little of the
graphic?s high-level message (Elzer et al, 2005).
This contrasts with scientific documents in which
graphics are often used to visualize data, with ex-
plicit references to the graphic being used to explain
their content (e.g., ?As shown in Fig. A...?). Infor-
mation graphics in popular media contribute to the
overall communicative goal of a multimodal docu-
ment and should not be ignored.
Our work is concerned with the summarization
of information graphics from popular media. Such
summaries have several major applications: 1) they
can be integrated with the summary of a multimodal
document?s text, thereby producing a richer sum-
mary of the overall document?s content; 2) they can
be stored in a digital library along with the graphic
itself and used to retrieve appropriate graphics in re-
sponse to user queries; and 3) for individuals with
sight impairments, they can be used along with a
screen reader to convey not only the text of a docu-
ment, but also the content of the document?s graph-
ics. In this paper we present our work on summariz-
ing line graphs. This builds on our previous efforts
into summarizing bar charts (Demir et al, 2008;
Elzer et al, 2011); however, line graphs have dif-
ferent messages and communicative signals than bar
charts and their continuous nature requires different
processing. In addition, a very different set of visual
features must be taken into account in deciding the
importance of including a proposition in a summary.
2 Methodology
Most summarization research has focused on ex-
tractive techniques by which segments of text are
extracted and put together to form the summary.
41
?
102468 1900?1
0?20
?50?60
?70?80
?90
?03
?30?40
2000
10
8.9
1.979 inches over
 the past ce
ntury. Ann
ual differen
ce from Se
attle?s
In the seatt
le area, for
 example, t
he Pacific 
Ocean has 
risen nearly
 
they are ris
ing about 0
.04?0.09 o
f an inch ea
ch year.
Sea levels 
fluctuate ar
ound the gl
obe, but oc
eanographe
rs believe
Ocean leve
ls rising
1899 sea le
vel, in inch
es:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
However, the Holy Grail of summarization work is
abstractive summarization in which the document?s
content is understood and the important concepts are
integrated into a coherent summary. For informa-
tion graphics, extractive summarization might mean
treating the text in the graphic (e.g., the caption) as if
it were document text. One could imagine perhaps
expanding this view to include selecting particular
data points or segments and constructing sentences
that convey them. Abstractive summarization, on
the other hand, requires that the high-level content
of the graphic be identified and conveyed in the sum-
mary. The goal of our work is abstractive summa-
rization. The main issues are identifying the knowl-
edge conveyed by a graphic, selecting the concepts
that should be conveyed in a summary, and integrat-
ing them into coherent natural language sentences.
As noted in the Introduction, information graphics
in popular media generally have a high-level mes-
sage that they are intended to convey. This mes-
sage constitutes the primary communicative or dis-
course goal (Grosz and Sidner, 1986) of the graphic
and captures its main contribution to the overall dis-
course goal of the entire document. However, the
graphic also includes salient features that are impor-
tant components of the graphic?s content. For exam-
ple, the graphic in Figure 1 is very jagged with sharp
fluctuations, indicating that short-term changes have
been inconsistent. Since the graphic?s intended mes-
sage represents its primary discourse goal, we con-
tend that this message should form the core or fo-
cus of the graphic?s summary. The salient features
should be used to augment the summary of the graph
and elaborate on its intended message. Thus, our
methodology consists of the following steps: 1) hy-
pothesize the graphic?s primary discourse or com-
municative goal (i.e., its intended message), 2) iden-
tify additional propositions that are salient in the
graphic, and 3) construct a natural language sum-
mary that integrates the intended message and the
additional salient propositions into a coherent text.
Section 3 presents our methodology for hypothe-
sizing a line graph?s intended message or discourse
goal. It starts with an XML representation of the
graphic that specifies the x-y coordinates of the sam-
pled pixels along the data series in the line graph, the
axes with tick marks and labels, the caption, etc.;
constructing the XML representation is the respon-
sibility of a Visual Extraction Module similar to the
one for bar charts described by Chester and Elzer
(2005). Section 4 presents our work on identifying
the additional propositions that elaborate on the in-
tended message and should be included in the sum-
mary. Section 5 discusses future work on realizing
the propositions in a natural language summary, and
Section 6 reviews related work in multimodal and
abstractive summarization.
3 Identifying a Line Graph?s Message
Research has shown that human subjects have a
strong tendency to use line graphs to portray trend
relationships, as well as a strong tendency to de-
scribe line graphs in terms of trends (Zacks and
Tversky, 1999). We analyzed a corpus of sim-
ple line graphs collected from various popular me-
dia including USA Today, Businessweek, and The
(Wilmington) News Journal, and identified a set of
10 high-level message categories that capture the
kinds of messages that are conveyed by a simple
line graph. Table 1 defines four of them. The com-
plete list can be found in (Wu et al, 2010b). Each
of these messages requires recognizing the visual
trend(s) in the depicted data. We use a support vec-
tor machine (SVM) to first segment the line graph
into a sequence of visually-distinguishable trends;
this sequence is then input into a Bayesian net-
work that reasons with evidence from the graphic
42
Intention Category Description
RT: Rising-trend There is a rising trend from <param1> to <param2>.
CT: Change-trend There is a <direction2> trend from <param2> to <param3> that is signifi-
cantly different from the <direction1> trend from <param1> to <param2>.
CTR:
Change-trend-return
There is a <direction1> trend from <param3> to <param4> that is different
from the <direction2> trend between <param2> and <param3> and reflects
a return to the kind of <direction1> trend from <param1> to <param2>.
BJ: Big-jump There was a very significant sudden jump in value between <param1> and
<param2> which may or may not be sustained.
Table 1: Four categories of High Level Messages for Line Graphs
in order to recognize the graphic?s intended mes-
sage. The next two subsections outline these
steps. (Our corpus of line graphs can be found at
www.cis.udel.edu/?carberry/Graphs/viewallgraphs.php)
3.1 Segmenting a Line Graph
A line graph can consist of many short, jagged
line segments, although a viewer of the graphic ab-
stracts from it a sequence of visually-distinguishable
trends. For example, the line graph in Figure 1 con-
sists of two trends: a relatively stable trend from
1900 to 1930 and a longer, increasing trend from
1930 to 2003. Our Graph Segmentation Module
(GSM) takes a top-down approach (Keogh et al,
2001) to generalize the line graph into sequences of
rising, falling, and stable segments, where a segment
is a series of connected data points. The GSM starts
with the entire line graph as a single segment and
uses a learned model to recursively decide whether
each segment should be split into two subsegments;
if the decision is to split, the division is made at the
point being the greatest distance from a straight line
between the two end points of the original segment.
This process is repeated on each subsegment until
no further splits are identified. The GSM returns a
sequence of straight lines representing a linear re-
gression of the points in each subsegment, where
each straight line is presumed to capture a visually-
distinguishable trend in the original graphic.
We used Sequential Minimal Optimization (Platt,
1999) in training an SVM to make segment split-
ting decisions. We chose to use an SVM because it
works well with high-dimensional data and a rela-
tively small training set, and lessens the chance of
overfitting by using the maximum margin separat-
ing hyperplane which minimizes the worst-case gen-
eralization errors (Tan et al, 2005). 18 attributes,
falling into two categories, were used in building
the data model (Wu et al, 2010a). The first cat-
egory captures statistical tests computed from the
sampled data points in the XML representation of
the graphic; these tests estimate how different the
segment is from a linear regression (i.e., a straight
line). The second category of attributes captures
global features of the graphic. For example, one
such attribute relates the segment size to the size of
the entire graphic, based on the hypothesis that seg-
ments comprising more of the total graph may be
stronger candidates for splitting than segments that
comprise only a small portion of the graph.
Our Graph Segmentation Module was trained
on a set of 649 instances that required a split/no-
split decision. Using leave-one-out cross validation,
in which one instance is used for testing and the
other 648 instances are used for training, our model
achieved an overall accuracy rate of 88.29%.
3.2 A Bayesian Recognition System
Once the line graph has been converted into
a sequence of visually-distinguishable trends, a
Bayesian network is built that captures the possible
intended messages for the graphic and the evidence
for or against each message. We adopted a Bayesian
network because it weighs different pieces of evi-
dence and assigns a probability to each candidate
intended message. The next subsections briefly out-
line the Bayesian network and its evaluation; details
can be found in (Wu et al, 2010b).
Structure of the Bayesian Network Figure 2
shows a portion of the Bayesian network constructed
for Figure 1. The top-level node in our Bayesian net-
work represents all of the high-level message cat-
43
Intended Message
... ...
... ...
...
CT?Suggestion?1
CT IntentionRT Intention
EvidenceOtherPointsAnnotated
Have SuggestionEvidence
Portion of GraphicEvidence EndpointsAnnotatedEvidence EvidenceSplittingPointsAnnotated
Adjective in CaptionEvidence
Verb in CaptionEvidence
Figure 2: A portion of the Bayesian network
egories. Each of these possible non-parameterized
message categories is repeated as a child of the
top-level node; this is purely for ease of repre-
sentation. Up to this point, the Bayesian net-
work is a static structure with conditional proba-
bility tables capturing the a priori probability of
each category of intended message. When given
a line graph to analyze, an extension of this net-
work is built dynamically according to the partic-
ulars of the graph itself. Candidate (concrete) in-
tended messages, having actual instantiated param-
eters, appear beneath the high-level message cat-
egory nodes. These candidates are introduced by
a Suggestion Generation Module; it dynamically
constructs all possible intended messages with con-
crete parameters using the visually-distinguishable
trends (rising, falling, or stable) identified by the
Graph Segmentation Module. For example, for each
visually-distinguishable trend, a Rising, Falling, or
Stable trend message is suggested; similary, for each
sequence of two visually-distinguishable trends, a
Change-trend message is suggested. For the graphic
in Figure 1, six candidate messages will be gener-
ated, including RT(1930, 2003), CT(1900, stable,
1930, rise, 2003) and BJ(1930, 2003) (see Table 1).
Entering Evidence into the Bayesian Network
Just as listeners use evidence to identify the intended
meaning of a speaker?s utterance, so also must a
viewer use evidence to recognize a graphic?s in-
tended message. The evidence for or against each
of the candidate intended messages must be entered
into the Bayesian network. We identified three kinds
of evidence that are used in line graphs: attention-
getting devices explicitly added by the graphic de-
signer (e.g., the annotation of a point with its value),
aspects of a graphic that are perceptually-salient
(e.g., the slope of a segment), and clues that sug-
gest the general message category (e.g., a verb [or
noun derived from a verb such as rebound] in the
caption which might indicate a Change-trend mes-
sage). The first two kinds of evidence are attached
to the Bayesian network as children of each candi-
date message node, such as the child nodes of ?CT-
Suggestion-1? in Figure 2. The third kind of evi-
dence is attached to the top level node as child nodes
named ?Verb in Caption Evidence? and ?Adjective
in Caption Evidence? in Figure 2.
Bayesian Network Inference We evaluated the
performance of our system for recognizing a line
graph?s intended message on a corpus of 215 line
graphs using leave-one-out cross validation in which
one graph is held out as a test graph and the con-
ditional probability tables for the Bayesian network
are computed from the other 214 graphs. Our sys-
tem recognized the correct intended message with
the correct parameters for 157 line graphs, resulting
in a 73.36% overall accuracy rate.
4 Identifying Elaborative Propositions
Once the intended message has been determined,
the next step is to identify additional important
informational propositions1 conveyed by the line
graph which should be included in the summary.
To accomplish this, we collected data to determine
what kinds of propositions in what situations were
deemed most important by human subjects, and de-
veloped rules designed to make similar assessments
based on the graphic?s intended message and visual
features present in the graphic.
4.1 Collecting Data from Human Subjects
Participants in our study were given 23 different line
graphs. With each graph, the subjects were provided
1We define a ?proposition? as a logical representation de-
scribing a relationship between one or more concepts, while a
?sentence? is a surface form realizing one or more propositions.
44
Figure 3: From ?This Cable Outfit Is Getting Tuned In?
in Businessweek magazine, Oct 4, 1999.
with an initial sentence describing the overall in-
tended message of the graphic. The subjects were
asked to add additional sentences so that the com-
pleted summary captured the most important infor-
mation conveyed by the graphic. The graphs were
presented to the subjects in different orders, and the
subjects completed as many graphs as they wanted
during the one hour study session. The set covered
the eight most prevalent of our intended message
categories and a variety of visual features. Roughly
half of the graphs were real-world examples from
the corpus used to train the Bayesian network in
Section 3.2, (e.g., Figure 3), with the others created
specifically to fill a gap in the coverage of intended
messages and visual features.
We collected a total of 998 summaries written by
69 human subjects for the 23 different line graphs.
The number of summaries we received per graph
ranged from 37 to 50. Most of the summaries were
between one and four sentences long, in addition to
the initial sentence (capturing the graphic?s intended
message) that was provided for each graph. A rep-
resentative sample summary collected for the line
graph shown in Figure 3 is as follows, with the initial
sentence provided to the study participants in italics:
This line graph shows a big jump in Blon-
der Tongue Laboratories stock price in
August ?99. The graph has many peaks
and valleys between March 26th 1999 to
August ?99 but maintains an average stock
price of around 6 dollars. However, in Au-
gust ?99 the stock price jumps sharply to
around 10 dollars before dropping quickly
to around 9 dollars by September 21st.
4.2 Extracting & Weighting Propositions
The data collected during the study was analyzed by
a human annotator who manually coded the propo-
sitions that appeared in each individual summary in
order to determine, for each graphic, which proposi-
tions were used and how often. For example, the set
of propositions coded in the sample summary from
Section 4.1 were:
? volatile(26Mar99, Aug99)
? average val(26Mar99, Aug99, $6)
? jump 1(Aug99, $10)
? steep(jump 1)
? decrease 1(Aug99, $10, 21Sep99, $9)
? steep(decrease 1)
From this information, we formulated a set of
rules governing the use of each proposition accord-
ing to the intended message category and various
visual features. Our intuition was that by finding
and exploiting a correlation between the intended
message category and/or certain visual features and
the propositions appearing most often in the human-
written summaries, our system could use these in-
dicators to determine which propositions are most
salient in new graphs. Our rules assign a weight
to each proposition in the situation captured by the
rule; these weights are based on the relative fre-
quency of the proposition being used in summaries
reflecting similar situations in our corpus study. The
rules are organized into three types:
1. Message Category-only (M):
IF M = m THEN select P with weight w1
2. Visual Feature-only (V):
IF V = v THEN select P with weight w2
3. Message Category + Visual Feature:
IF M = m and V = v
THEN select P with weight w2
We constructed type 1 (Message Category-only)
rules when a plurality of human-written summaries
45
in our corpus for all line graphs belonging to a
given message category contain the proposition. A
weight was assigned according to the frequency with
which the proposition was included. This weighting,
shown in Equation 1, is based on the proportion of
summaries for each line graph in the corpus having
intended message m and containing proposition P.
w1 =
n?
i=1
Pi
Si
(1)
In this equation, n is the number of line graphs in
this intended message category, Si is the total num-
ber of summaries for a particular line graph with this
intended message category, and Pi is the number of
these summaries that contain the proposition.
Intuitively, a proposition appearing in all sum-
maries for all graphs in a given message category
will have a weight of 1.0, while a proposition which
never appears will have a weight of zero. How-
ever, a proposition appearing in all summaries for
half of the graphs in a category, and rarely for the
other half of the graphs in that category, will have a
much lower weight than one which appears in half
of the summaries for all the graphs in that category,
even though the overall frequencies could be equal
for both. In this case, the message category is an
insufficient signal, and it is likely that the former
proposition is more highly correlated to some par-
ticular visual feature than to the message category.
Weights for type 2 and type 3 rules (Visual
Feature-only and Message Category + Visual Fea-
ture) are slightly more complicated in that they in-
volve a measure of degree for the associated visual
feature rather than simply its presence. The defini-
tion of this measure varies depending on the nature
of the visual feature (e.g., steepness of a trend line,
volatility), but all such measures range from zero to
one. Additionally, since the impact of a visual fea-
ture is a matter of degree, the weighting cannot rely
on a simple proportion of summaries containing the
proposition as in type 1 rules. Instead, it is neces-
sary to find the covariance between the magnitude of
the visual feature (|v|) and how frequently the corre-
sponding proposition is used (PS ) in the corpus sum-
maries for the n graphs having this visual feature, as
shown in Equation 2.
Cov(|v|,
P
S
) =
[(?n
i=1 |vi|
n
?n
i=1
Pi
Si
n
)
?
?n
i=1 |vi|
Pi
Si
n
] (2)
Then for a particular graphic whose magnitude for
this feature is |v|, we compute the weight w2 for the
proposition P as shown in Equation 3.
w2 = |v| ? Cov(|v|,
P
S
) (3)
This way, the stronger a certain visual feature is in a
given line graph, the higher the weight for the asso-
ciated proposition.
Type 3 rules (Message Category + Visual Fea-
ture) differ only from type 2 rules in that they are
restricted to a particular intended message category,
rather than any line graph having the visual feature
in question. For example, a proposition compar-
ing the slope of two trends may be appropriate for
a graph in the Change-trend message category, but
does not make sense for a line graph with only a sin-
gle trend (e.g., Rising-trend).
Once all propositions have been extracted and
ranked, these weights are passed along to a graph-
based content selection framework (Demir et al,
2010) that iteratively selects for inclusion in the ini-
tial summary those propositions which provide the
best coverage of the highest-ranked information.
4.3 Sample Rule Application
Figures 1 and 4 consist of two different line graphs
with the same intended message category: Change-
trend. Figure 1 shows a stable trend in annual sea
level difference from 1900 to 1930, followed by a
rising trend through 2003, while Figure 4 shows a
rising trend in Durango sales from 1997 to 1999,
followed by a falling trend through 2006. Proposi-
tions associated with type 1 rules will have the same
weights for both graphs, but propositions related to
visual features may have different weights. For ex-
ample, the graph in Figure 1 is far more volatile than
the graph in Figure 4. Thus, the type 2 rule associ-
ated with volatility will have a very high weight for
the graph in Figure 1 and will almost certainly be in-
cluded in the initial summary of that line graph (e.g.,
46
20062005200420032002
19971998
1999
20012000
200,000 150,000199
9: 189,840
70,6062006:
50,000100,000Declining Du
rango sales
0
Figure 4: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
?The values vary a lot...?, ?The trend is unstable...?),
possibly displacing a type 1 proposition that would
still appear in the summary for the graph in Figure 4.
5 Future Work
Once the propositions that should be included in the
summary have been selected, they must be coher-
ently organized and realized as natural language sen-
tences. We anticipate using the FUF/SURGE sur-
face realizer (Elhadad and Robin, 1996); our col-
lected corpus of line graph summaries provides a
large set of real-world expressions to draw from
when crafting the surface realization forms our sys-
tem will produce for the final-output summaries.
Our summarization methodology must also be eval-
uated. In particular, we must evaluate the rules for
identifying the additional informational propositions
that are used to elaborate the overall intended mes-
sage, and the quality of the summaries both in terms
of content and coherence.
6 Related Work
Image summarization has focused on constructing a
smaller image that contains the important content of
a larger image (Shi et al, 2009), selecting a set of
representative images that summarize a collection
of images (Baratis et al, 2008), or constructing a
new diagram that summarizes one or more diagrams
(Futrelle, 1999). However, all of these efforts pro-
duce an image as the end product, not a textual sum-
mary of the content of the image(s).
Ferres et al (2007) developed a system for con-
veying graphs to blind users, but it generates the
same basic information for each instance of a graph
type (e.g., line graphs) regardless of the individual
graph?s specific characteristics. Efforts toward sum-
marizing multimodal documents containing graph-
ics have included na??ve approaches relying on cap-
tions and direct references to the image in the text
(Bhatia et al, 2009), while content-based image
analysis and NLP techniques are being combined for
multimodal document indexing and retrieval in the
medical domain (Ne?ve?ol et al, 2009).
Jing and McKeown (1999) approached abstrac-
tive summarization as a text-to-text generation task,
modifying sentences from the original document via
editing and rewriting. There have been some at-
tempts to do abstractive summarization from seman-
tic models, but most of it has focused on text docu-
ments (Rau et al, 1989; Reimer and Hahn, 1988),
though Alexandersson (2003) used abstraction and
semantic modeling for speech-to-speech translation
and multilingual summary generation.
7 Discussion
Information graphics play an important communica-
tive role in popular media and cannot be ignored.
We have presented our methodology for construct-
ing a summary of a line graph. Our method is ab-
stractive, in that we identify the important high-level
knowledge conveyed by a graphic and capture it in
propositions to be realized in novel, coherent natu-
ral language sentences. The resulting summary can
be integrated with a summary of the document?s text
to produce a rich summary of the entire multimodal
document. In addition, the graphic?s summary can
be used along with a screen reader to provide sight-
impaired users with full access to the knowledge
conveyed by multimodal documents.
Acknowledgments
This work was supported in part by the National In-
stitute on Disability and Rehabilitation Research un-
der Grant No. H133G080047.
References
Jan Alexandersson. 2003. Hybrid Discourse Modeling
and Summarization for a Speech-to-Speech Transla-
tion System. Ph.D. thesis, Saarland University.
Evdoxios Baratis, Euripides Petrakis, and Evangelos Mil-
ios. 2008. Automatic web site summarization by im-
age content: A case study with logo and trademark
47
images. IEEE Transactions on Knowledge and Data
Engineering, 20(9):1195?1204.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proc. of the 29th Annual Int?l ACM
SIGIR Conf. on Research & Development in Informa-
tion Retrieval, SIGIR ?06, pages 581?588, Seattle, Au-
gust. ACM.
Daniel Chester and Stephanie Elzer. 2005. Getting com-
puters to see information graphics so users do not have
to. In Proceedings of the 15th International Sympo-
sium on Methodologies for Intelligent Systems (LNAI
3488), ISMIS 2005, pages 660?668, Saratoga Springs,
NY, June. Springer-Verlag.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters & Demos), Sussex, UK, June. ACL.
Stephanie Elzer, Sandra Carberry, Daniel Chester, Seniz
Demir, Nancy Green, Ingrid Zukerman, and Keith
Trnka. 2005. Exploring and exploiting the limited
utility of captions in recognizing intention in infor-
mation graphics. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 223?230, Ann Arbor, June. ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proc. of the 9th Int?l ACM
SIGACCESS Conf. on Computers & Accessibility, AS-
SETS ?07, pages 67?74, Tempe, October. ACM.
Robert P. Futrelle. 1999. Summarization of diagrams in
documents. In I. Mani and M. Maybury, editors, Ad-
vances in Automatic Text Summarization. MIT Press.
Barbara Grosz and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. Computa-
tional Linguistics, 12(3):175?204.
Hongyan Jing and Kathleen R. McKeown. 1999. The
decomposition of human-written summary sentences.
In Proc. of the 22nd Annual Int?l ACM SIGIR Conf.
on Research & Development in Information Retrieval,
SIGIR ?99, pages 129?136, Berkeley, August. ACM.
Eamonn J. Keogh, Selina Chu, David Hart, and
Michael J. Pazzani. 2001. An online algorithm
for segmenting time series. In Proceedings of the
2001 IEEE International Conference on Data Mining,
ICDM ?01, pages 289?296, Washington, DC. IEEE.
Aure?lie Ne?ve?ol, Thomas M. Deserno, Ste?fan J. Darmoni,
Mark Oliver Gu?ld, and Alan R. Aronson. 2009. Nat-
ural language processing versus content-based image
analysis for medical document retrieval. Journal of the
American Society for Information Science and Tech-
nology, 60(1):123?134.
John C. Platt. 1999. Fast training of support vector
machines using sequential minimal optimization. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in kernel methods: support vector learning,
pages 185?208. MIT Press, Cambridge, MA, USA.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using lin-
guistic knowledge acquisition. Information Process-
ing & Management, 25(4):419 ? 428.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Liang Shi, Jinqiao Wang, Lei Xu, Hanqing Lu, and
Changsheng Xu. 2009. Context saliency based im-
age summarization. In Proceedings of the 2009 IEEE
international conference on Multimedia and Expo,
ICME ?09, pages 270?273, New York. IEEE.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining. Addison Wesley.
Peng Wu, Sandra Carberry, and Stephanie Elzer. 2010a.
Segmenting line graphs into trends. In Proceedings of
the 2010 International Conference on Artificial Intel-
ligence, ICAI ?10, pages 697?703, Las Vegas, July.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel
Chester. 2010b. Recognizing the intended message
of line graphs. In Proc. of the 6th Int?l Conf. on Dia-
grammatic Representation & Inference, Diagrams ?10,
pages 220?234, Portland. Springer-Verlag.
Jeff Zacks and Barbara Tversky. 1999. Bars and lines:
A study of graphic communication. Memory & Cog-
nition, 27:1073?1079.
48
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 52?62,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Improving the Accessibility of Line Graphs in Multimodal Documents
Charles F. Greenbacker Peng Wu Sandra Carberry Kathleen F. McCoy
Stephanie Elzer* David D. McDonald? Daniel Chester Seniz Demir?
Dept. of Computer & Information Sciences, University of Delaware, USA
[charlieg|pwu|carberry|mccoy|chester]@cis.udel.edu
*Dept. of Computer Science, Millersville University, USA elzer@cs.millersville.edu
?SIFT LLC., Boston, Massachusetts, USA dmcdonald@sift.info
?TU?BI?TAK BI?LGEM, Gebze, Kocaeli, Turkey senizd@uekae.tubitak.gov.tr
Abstract
This paper describes our work on improv-
ing access to the content of multimodal docu-
ments containing line graphs in popular media
for people with visual impairments. We pro-
vide an overview of our implemented system,
including our method for recognizing and con-
veying the intended message of a line graph.
The textual description of the graphic gener-
ated by our system is presented at the most rel-
evant point in the document. We also describe
ongoing work into obtaining additional propo-
sitions that elaborate on the intended message,
and examine the potential benefits of analyz-
ing the text and graphical content together in
order to extend our system to produce sum-
maries of entire multimodal documents.
1 Introduction
Individuals with visual impairments have difficulty
accessing the information contained in multimodal
documents. Although screen-reading software can
render the text of the document as speech, the graph-
ical content is largely inaccessible. Here we con-
sider information graphics (e.g., bar charts, line
graphs) often found in popular media sources such
as Time magazine, Businessweek, and USA Today.
These graphics are typically intended to convey a
message that is an important part of the overall story,
yet this message is generally not repeated in the ar-
ticle text (Carberry et al, 2006). People who are
unable to see and assimilate the graphical material
will be left with only partial information.
While some work has addressed the accessibility
of scientific graphics through alternative means like
touch or sound (see Section 7), such graphs are de-
signed for an audience of experts trained to use them
for data visualization. In contrast, graphs in popular
media are constructed to make a point which should
be obvious without complicated scientific reasoning.
We are thus interested in generating a textual pre-
sentation of the content of graphs in popular media.
Other research has focused on textual descriptions
(e.g., Ferres et al (2007)); however in that work the
same information is included in the textual summary
for each instance of a graph type (i.e., all summaries
of line graphs contain the same sorts of informa-
tion), and the summary does not attempt to present
the overall intended message of the graph.
SIGHT (Demir et al, 2008; Elzer et al, 2011) is
a natural language system whose overall goal is pro-
viding blind users with interactive access to multi-
modal documents from electronically-available pop-
ular media sources. To date, the SIGHT project
has concentrated on simple bar charts. Its user in-
terface is implemented as a browser helper object
within Internet Explorer that works with the JAWS
screen reader. When the system detects a bar chart
in a document being read by the user, it prompts the
user to use keystrokes to request a brief summary of
the graphic capturing its primary contribution to the
overall communicative goal of the document. The
summary text can either be read to the user with
JAWS or read by the user with a screen magnifier
tool. The interface also enables the user to request
further information about the graphic, if desired.
However, SIGHT is limited to bar charts only.
In this work, we follow the methodology put forth
by SIGHT, but investigate producing a summary of
52
?
102468 1900?10
?20
?50?60
?70?80
?90?
03
?30?40
2000
10
8.9
1.979 inches over t
he past centu
ry. Annual d
ifference fro
m Seattle?s
In the seattl
e area, for e
xample, the
 Pacific Oce
an has risen
 nearly 
they are risi
ng about 0.0
4?0.09 of an
 inch each y
ear.
Sea levels fl
uctuate arou
nd the globe
, but oceano
graphers bel
ieve
Ocean level
s rising
1899 sea lev
el, in inches
:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
line graphs. Line graphs have different discourse
goals and communicative signals than bar charts,1
and thus require significantly different processing.
In addition, our work addresses the issue of coher-
ent placement of a graphic?s summary when reading
the text to the user and considers the summarization
of entire documents ? not just their graphics.
2 Message Recognition for Line Graphs
This section provides an overview of our imple-
mented method for identifying the intended message
of a line graph. In processing a line graph, a vi-
sual extraction module first analyzes the image file
and produces an XML representation which fully
specifies the graphic (including the beginning and
ending points of each segment, any annotations on
points, axis labels, the caption, etc.). To identify
the intended message of a line graph consisting of
many short, jagged segments, we must generalize
it into a sequence of visually-distinguishable trends.
This is performed by a graph segmentation module
which uses a support vector machine and a variety
of attributes (including statistical tests) to produce a
model that transforms the graphic into a sequence of
straight lines representing visually-distinguishable
trends. For example, the line graph in Figure 1 is
divided into a stable trend from 1900 to 1930 and a
rising trend from 1930 to 2003. Similarly, the line
graph in Figure 2 is divided into a rising trend from
1Bar charts present data as discrete bars and are often used
to compare entities, while line graphs contain continuous data
series and are designed to portray longer trend relationships.
20062005200420032002
19971998
1999
20012000
200,000 150,0001999
: 189,840
70,6062006:
50,000100,000Declining Dur
ango sales
0
Figure 2: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
1997 to 1999 and a falling trend from 1999 to 2006.
In analyzing a corpus of around 100 line graphs
collected from several popular media sources, we
identified 10 intended message categories (includ-
ing rising-trend, change-trend, change-trend-return,
and big-jump, etc.), that seem to capture the kinds
of high-level messages conveyed by line graphs. A
suggestion generation module uses the sequence of
trends identified in the line graph to construct all
of its possible candidate messages in these message
categories. For example, if a graph contains three
trends, several candidate messages are constructed,
including two change-trend messages (one for each
adjacent pair of trends), a change-trend-return mes-
sage if the first and third trends are of the same type
(rising, falling, or stable), as well as a rising, falling,
or stable trend message for each individual trend.
Next, various communicative signals are ex-
tracted from the graphic, including visual features
(such as a point annotated with its value) that draw
attention to a particular part of the line graph, and
linguistic clues (such as the presence of certain
words in the caption) that suggest a particular in-
tended message category. Figure 2 contains several
such signals, including two annotated points and the
word declining in its caption. Next, a Bayesian net-
work is built to estimate the probability of the can-
didate messages; the extracted communicative sig-
nals serve as evidence for or against each candidate
message. For Figure 2, our system produces change-
trend(1997, rise, 1999, fall, 2006) as the logical rep-
resentation of the most probable intended message.
Since the dependent axis is often not explicitly la-
beled, a series of heuristics are used to identify an
appropriate referent, which we term the measure-
ment axis descriptor. In Figure 2, the measurement
axis descriptor is identified as durango sales. The
53
intended message and measurement axis descriptor
are then passed to a realization component which
uses FUF/SURGE (Elhadad and Robin, 1996) to
generate the following initial description:
This graphic conveys a changing trend in
durango sales, rising from 1997 to 1999
and then falling to 2006.
3 Identifying a Relevant Paragraph
In presenting a multimodal document to a user via a
screen reader, if the author does not specify a read-
ing order in the accessibility preferences, it is not
entirely clear where the description of the graph-
ical content should be given. The text of scien-
tific articles normally makes explicit references to
any graphs contained in the document; in this case,
it makes sense to insert the graphical description
alongside the first such reference. However, popular
media articles rarely contain explicit references to
graphics. We hypothesize that describing the graphi-
cal content together with the most relevant portion of
the article text will result in a more coherent presen-
tation. Results of an experiment described in Sec-
tion 3.3 suggest the paragraph which is geograph-
ically closest to the graphic is very often not rele-
vant. Thus, our task becomes identifying the portion
of the text that is most relevant to the graph.
We have developed a method for identifying the
most relevant paragraph by measuring the similarity
between the graphic?s textual components and the
content of each individual paragraph in the docu-
ment. An information graphic?s textual components
may consist of a title, caption, and any additional
descriptions it contains (e.g., the five lines of text in
Figure 1 beneath the caption Ocean levels rising).
An initial method (P-KL) based on KL divergence
measures the similarity between a paragraph and the
graphic?s textual component; a second method (P-
KLA) is an extension of the first that incorporates
an augmented version of the textual component.
3.1 Method P-KL: KL Divergence
Kullback-Leibler (KL) divergence (Kullback, 1968)
is widely used to measure the similarity between two
language models. It can be expressed as:
DKL(p||q) =
?
i?V
p(i)log
p(i)
q(i)
where i is the index of a word in vocabulary V , and
p and q are two distributions of words. Liu et al
(Liu and Croft, 2002) applied KL divergence to text
passages in order to improve the accuracy of docu-
ment retrieval. For our task, p is a smoothed word
distribution built from the line graph?s textual com-
ponent, and q is another smoothed word distribution
built from a paragraph in the article text. Smoothing
addresses the problem of zero occurrences of a word
in the distributions. We rank the paragraphs by their
KL divergence scores from lowest to highest, since
lower scores indicate a higher similarity.
3.2 Method P-KLA: Using Augmented Text
In analyzing paragraphs relevant to the graphics, we
realized that they included words that were germane
to describing information graphics in general, but
not related to the domains of individual graphs. This
led us to build a set of ?expansion words? that tend to
appear in paragraphs relevant to information graph-
ics. If we could identify domain-independent terms
that were correlated with information graphics in
general, these expansion words could then be added
to the textual component of a graphic when measur-
ing its similarity to a paragraph in the article text.
We constructed the expansion word set using an
iterative process. The first step is to use P-KL to
identify m pseudo-relevant paragraphs in the cor-
responding document for each graphic in the train-
ing set (the current implementation uses m = 3).
This is similar to pseudo-relevance feedback used in
IR (Zhai, 2008), except only a single query is used
in the IR application, whereas we consider many
pairs of graphics and documents to obtain an ex-
pansion set applicable to any subsequent informa-
tion graphic. Given n graphics in the training set,
we identify (up to) m ? n relevant paragraphs.
The second step is to extract a set of words re-
lated to information graphics from these m ?n para-
graphs. We assume the collection of pseudo-relevant
paragraphs was generated by two models, one pro-
ducing words relevant to the information graphics
and another producing words relevant to the topics
of the individual documents. Let Wg represent the
word frequency vector yielding words relevant to
the graphics, Wa represent the word frequency vec-
tor yielding words relevant to the document topics,
and Wp represent the word frequency vector of the
54
pseudo-relevant paragraphs. We compute Wp from
the pseudo-relevant paragraphs themselves, and we
estimate Wa using the word frequencies from the
article text in the documents. Finally, we compute
Wg by filtering-out the components ofWa fromWp.
This process is related to the work by Widdows
(2003) on orthogonal negation of vector spaces.
The task can be formulated as follows:
1. Wp = ?Wa + ?Wg where ? > 0 and ? > 0,
which means the word frequency vector for
the pseudo-relevant paragraphs is a linear com-
bination of the background (topic) word fre-
quency vector and the graphic word vector.
2. < Wa,Wg >= 0 which means the background
word vector is orthogonal to the graph descrip-
tion word vector, under the assumption that the
graph description word vector is independent of
the background word vector and that these two
share minimal information.
3. Wg is assumed to be a unit vector, since we are
only interested in the relative rank of the word
frequencies, not their actual values.
Solving the above equations, we obtain:
? =
< Wp,Wa >
< Wa,Wa >
Wg = normalized
(
Wp ?
< Wp,Wa >
< Wa,Wa >
?Wa
)
After computing Wg, we use WordNet to filter-
out words having a predominant sense other than
verb or adjective, under the assumption that nouns
will be mainly relevant to the domains or topics
of the graphs (and are thus ?noise?) whereas we
want a general set of words (e.g., ?increasing?)
that are typically used when describing the data in
any graph. As a rough estimate of whether a word
is predominantly a verb or adjective, we determine
whether there are more verb and adjective senses of
the word in WordNet than there are noun senses.
Next, we rank the words in the filteredWg accord-
ing to frequency and select the k most frequent as
our expansion word list (we used k = 25 in our ex-
periments). The two steps (identifyingm?n pseudo-
relevant paragraphs and then extracting a word list of
size k to expand the graphics? textual components)
are applied iteratively until convergence occurs or
minimal changes are observed between iterations.
In addition, parameters of the intended message
that represent points on the x-axis capture domain-
specific content of the graphic?s communicative
goal. For example, the intended message of the line
graph in Figure 1 conveys a changing trend from
1900 to 2003 with the change occurring in 1930. To
help identify relevant paragraphs mentioning these
years, we also add these parameters of the intended
message to the augmented word list.
The result of this process is the final expansion
word list used in method P-KLA. Because the tex-
tual component may be even shorter than the expan-
sion word list, we do not add a word from the expan-
sion word list to the textual component unless the
paragraph being compared also contains this word.
3.3 Results of P-KL and P-KLA
334 training graphs with their accompanying articles
were used to build the expansion word set. A sepa-
rate set of 66 test graphs and articles was analyzed
by two human annotators who identified the para-
graphs in each document that were most relevant to
its associated information graphic, ranking them in
terms of relevance. On average, annotator 1 selected
2.00 paragraphs and annotator 2 selected 1.71 para-
graphs. The annotators agreed on the top ranked
paragraph for only 63.6% of the graphs. Consid-
ering the agreement by chance, we can calculate the
kappa statistic as 0.594. This fact shows that the
most relevant paragraph is not necessarily obvious
and multiple plausible options may exist.
We applied both P-KL and P-KLA to the test set,
with each method producing a list of the paragraphs
ranked by relevance. Since our goal is to provide
the summary of the graphic at a suitable point in the
article text, two evaluation criteria are appropriate:
1. TOP: the method?s success rate in selecting
the most relevant paragraph, measured as how
often it chooses the paragraph ranked highest
by either of the annotators
2. COVERED: the method?s success rate in se-
lecting a relevant paragraph, measured as how
often it chooses one of the relevant paragraphs
identified by the annotators
Table 1 provides the success rates of both of our
methods for the TOP and COVERED criteria, along
with a simple baseline that selected the paragraph
55
geographically-closest to the graphic. These results
show that both methods outperform the baseline,
and that P-KLA further improves on P-KL. P-KLA
selects the best paragraph in 60.6% of test cases,
and selects a relevant paragraph in 71.2% of the
cases. For both TOP and COVERED, P-KLA nearly
doubles the baseline success rate. The improve-
ment of P-KLA over P-KL suggests that our expan-
sion set successfully adds salient words to the tex-
tual component. A one-sided Z-test for proportion
based on binomial distribution is shown in Table 1
and indicates that the improvements of P-KL over
the baseline and P-KLA over P-KL are statistically-
significant at the 0.05 level across both criteria. The
Z-test is calculated as:
p? p0
?
p0(1?p0)
n
where p0 is the lower result and p is the improved
result. The null hypothesis is H0 : p = p0 and the
alternative hypothesis is H1 : p > p0.
3.4 Using relevant paragraph identification to
improve the accessibility of line graphs
Our system improves on SIGHT by using method
P-KLA to identify the paragraph that is most rele-
vant to an information graphic. When this paragraph
is encountered, the user is asked whether he or she
would like to access the content of the graphic. For
example, our system identifies the following para-
graph as most relevant to Figure 2:
Doing so likely would require the com-
pany to bring in a new model. Sales of
the Durango and other gas-guzzling SUVs
have slumped in recent years as prices at
the pump spiked.
In contrast, the geographically-closest paragraph has
little relevance to the graphic:
?We have three years to prove to them
we need to stay open,? said Sam Latham,
president of the AFL-CIO in Delaware,
who retired from Chrysler after 39 years.
4 Identifying Additional Propositions
After the intended message has been identified, the
system next looks to identify elaborative informa-
tional propositions that are salient in the graphic.
These additional propositions expand on the initial
description of the graph by filling-in details about
the knowledge being conveyed (e.g., noteworthy
points, properties of trends, visual features) in order
to round-out a summary of the graphic.
We collected a corpus of 965 human-written sum-
maries for 23 different line graphs to discover which
propositions were deemed most salient under varied
conditions.2 Subjects received an initial description
of the graph?s intended message, and were asked to
write additional sentences capturing the most impor-
tant information conveyed by the graph. The propo-
sitions appearing in each summary were manually
coded by an annotator to determine which were most
prevalent. From this data, we developed rules to
identify important propositions in new graphs. The
rules assign weights to propositions indicating their
importance, and the weights can be compared to de-
cide which propositions to include in a summary.
Three types of rules were built. Type-1 (message
category-only) rules were created when a plurality
of summaries for all graphs having a given intended
message contained the same proposition (e.g., pro-
vide the final value for all rising-trend and falling-
trend graphs). Weights for type-1 rules were based
on the frequency with which the proposition ap-
peared in summaries for graphs in this category.
Type-2 (visual feature-only) rules were built when
there was a correlation between a visual feature and
the use of a proposition describing that feature, re-
gardless of the graph?s message category (e.g., men-
tion whether the graph is highly volatile). Type-2
rule weights are a function of the covariance be-
tween the magnitude of the visual feature (e.g., de-
gree of volatility) and the proportion of summaries
mentioning this proposition for each graph.
For propositions associated with visual features
linked to a particular message category (e.g., de-
scribe the trend immediately following a big-jump
or big-fall when it terminates prior to the end of the
graph), we constructed Type-3 (message category
+ visual feature) rules. Type-3 weights were cal-
culated just like Type-2 weights, except the graphs
were limited to the given category.
As an example of identifying additional proposi-
2This corpus is described in greater detail by Greenbacker et
al. (2011) and is available at www.cis.udel.edu/~mccoy/corpora
56
closest P-KL significance level over closest P-KLA significance level over P-KL
TOP 0.272 0.469 (z = 3.5966, p < 0.01) 0.606 (z = 2.2303, p < 0.025)
COVERED 0.378 0.606 (z = 3.8200, p < 0.01) 0.712 (z = 1.7624, p < 0.05)
Table 1: Success rates for baseline method (?closest?), P-KL, and P-KLA using the TOP and COVERED criteria.
tions, consider Figures 1 and 2. Both line graphs
belong to the same intended message category:
change-trend. However, the graph in Figure 1 is far
more volatile than Figure 2, and thus it is likely that
we would want to mention this proposition (i.e., ?the
graph shows a high degree of volatility...?) in a sum-
mary of Figure 1. By finding the covariance between
the visual feature (i.e., volatility) and the frequency
with which a corresponding proposition was anno-
tated in the corpus summaries, a Type-2 rule assigns
a weight to this proposition based on the magnitude
of the visual feature. Thus, the volatility proposi-
tion will be weighted strongly for Figure 1, and will
likely be selected to appear in the initial summary,
while the weight for Figure 2 will be very low.
5 Integrating Text and Graphics
Until now, our system has only produced summaries
for the graphical content of multimodal documents.
However, a user might prefer a summary of the en-
tire document. Possible use cases include examining
this summary to decide whether to invest the time re-
quired to read a lengthy article with a screen reader,
or simply addressing the common problem of having
too much material to review in too little time (i.e.,
information overload). We are developing a system
extension that will allow users to request summaries
of arbitrary length that cover both the text and graph-
ical content of a multimodal document.
Graphics in popular media convey a message that
is generally not repeated in the article text. For ex-
ample, the March 3, 2003 issue of Newsweek con-
tained an article entitled, ?The Black Gender Gap,?
which described the professional achievements of
black women. It included a line graph (Figure 3)
showing that the historical gap in income equality
between white women and black women had been
closed, yet this important message appears nowhere
in the article text. Other work in multimodal doc-
ument summarization has relied on image captions
and direct references to the graphic in the text (Bha-
tia et al, 2009); however, these textual elements do
Figure 3: From ?The Black Gender Gap? in Newsweek,
Mar 3, 2003.
not necessarily capture the message conveyed by in-
formation graphics in popular media. Thus, the user
may miss out on an essential component of the over-
all communicative goal of the document if the sum-
mary covers only material presented in the text.
One approach to producing a summary of the en-
tire multimodal document might be to ?concatenate?
a traditional extraction-based summary of the text
(Kupiec et al, 1995; Witbrock and Mittal, 1999)
with the description generated for the graphics by
our existing system. The summary of the graphi-
cal content could be simply inserted wherever it is
deemed most relevant in the text summary. How-
ever, such an approach would overlook the relation-
ships and interactions between the text and graphical
content. The information graphics may make certain
concepts mentioned in the text more salient, and vice
versa. Unless we consider the contributions of both
the text and graphics together during the content se-
lection phase, the most important information might
not appear in the summary of the document.
Instead, we must produce a summary that inte-
grates the content conveyed by the text and graphics.
We contend that this integration must occur at the se-
mantic level if it is to take into account the influence
of the graphic?s content on the salience of concepts
in the text and vice versa. Our tack is to first build
a single semantic model of the concepts expressed
in both the article text and information graphics, and
then use this model as the basis for generating an
abstractive summary of the multimodal document.
57
Drawing from a model of the semantic content of the
document, we select as many or as few concepts as
we wish, at any level of detail, to produce summaries
of arbitrary length. This will permit the user to re-
quest a quick overview in order to decide whether to
read the original document, or a more comprehen-
sive synopsis to obtain the most important content
without having to read the entire article.
5.1 Semantic Modeling of Multimodal
Documents
Content gathered from the article text by a seman-
tic parser and from the information graphics by
our graph understanding system is combined into
a single semantic model based on typed, struc-
tured objects organized under a foundational ontol-
ogy (McDonald, 2000a). For the semantic pars-
ing of text, we use Sparser (McDonald, 1992), a
bottom-up, phrase-structure-based chart parser, op-
timized for semantic grammars and partial parsing.3
Using a built-in model of core English grammar
plus domain-specific grammars, Sparser extracts in-
formation from the text and produces categorized
objects as a semantic representation (McDonald,
2000b). The intended message and salient additional
propositions identified by our system for the infor-
mation graphics are decomposed and added to the
model constructed by Sparser.4
Model entries contain slots for attributes in the
concept category?s ontology definition (fillable by
other concepts or symbols), the original phrasings
mentioning this concept in the text (represented as
parameterized synchronous TAG derivation trees),
and markers recording document structure (i.e.,
where in the text [including title, headings, etc.] or
graphic the concept appeared). Figure 4 shows some
of the information contained in a small portion of
the semantic model built for an article entitled ?Will
Medtronic?s Pulse Quicken?? from the May 29,
2006 edition of Businessweek magazine5, which in-
cluded a line graph. Nodes correspond to concepts
3https://github.com/charlieg/Sparser
4Although the framework is general enough to accommo-
date any modality (e.g., images, video) given suitable seman-
tic analysis tools, our prototype implementation focuses on bar
charts and line graphs analyzed by SIGHT.
5http://www.businessweek.com/magazine/
content/06_22/b3986120.htm
and edges denote relationships between concepts;
dashed lines indicate links to concepts not shown in
this figure. Nodes are labelled with the name of the
conceptual category they instantiate, and a number
to distinguish between individuals. The middle of
each box displays the attributes of the concept, while
the bottom portion shows some of the original text
phrasings. Angle brackets (<>) note references to
other concepts, and hash marks (#) indicate a sym-
bol that has not been instantiated as a concept.
P1S1: "medical device
    giant Medtronic"
P1S5: "Medtronic"
Name: "Medtronic"
Stock: "MDT"
Industry: (#pacemakers,
    #defibrillators,
    #medical devices)
Company1
P1S4: "Joanne
    Wuensch"
P1S7: "Wuensch"
FirstName: "Joanne"
LastName: "Wuensch"
Person1
P1S4: "a 12-month
    target of 62"
Person: <Person 1>
Company: <Company 1>
Price: $62.00
Horizon: #12_months
TargetStockPrice1
Figure 4: Detail of model for Businessweek article.
5.2 Rating Content in Semantic Models
The model is then rated to determine which items are
most salient. The concepts conveying the most in-
formation and having the most connections to other
important concepts in the model are the ones that
should be chosen for the summary. The importance
of each concept is rated according to a measure of
information density (ID) involving several factors:6
Saturation Level Completeness of attributes in
model entry: a concept?s filled-in slots (f ) vs. its
total slots (s), and the importance of the concepts
(ci) filling those slots:
f
s ? log(s) ?
?f
i=1 ID(ci)
Connectedness Number of connections (n) with
other concepts (cj), and the importance of these con-
nected concepts:
?n
j=1 ID(cj)
Frequency Number of observed phrasings (e) re-
alizing the concept in text of the current document
Prominence in Text Prominence based on docu-
ment structure (WD) and rhetorical devices (WR)
Graph Salience Salience assessed by the graph
understanding system (WG) ? only applies to con-
cepts appearing in the graphics
6The first three factors are similar to the dominant slot
fillers, connectivity patterns, and frequency criteria described
by Reimer and Hahn (1988).
58
Saturation corresponds to the completeness of the
concept in the model. The more attribute slots that
are filled, the more we know about a particular con-
cept instance. However, this measure is highly sen-
sitive to the degree of detail provided in the seman-
tic grammar and ontology class definition (whether
created by hand or automatically). A concept having
two slots, both of which are filled-out, is not neces-
sarily more important than a concept with only 12
of its 15 slots filled. The more important a concept
category is in a given domain, the more detailed its
ontology class definition will likely be. Thus, we
can assume that a concept definition having a dozen
or more slots is, broadly speaking, more important
in the domain than a less well-defined concept hav-
ing only one or two slots. This insight is the basis of
a normalization factor (log(s)) used in ID.
Saturation differs somewhat from repetition in
that it attempts to measure the amount of informa-
tion associated with a concept, rather than simply
the number of times a concept is mentioned in the
text. For example, a news article about a proposed
law might mention ?Washington? several times, but
the fact that the debate took place in Washington,
D.C. is unlikely to be an important part of the article.
However, the key provisions of the bill, which may
individually be mentioned only once, are likely more
important as a greater amount of detail is provided
concerning them. Simple repetition is not necessar-
ily indicative of the importance of a concept, but if a
large amount of information is provided for a given
concept, it is safe to assume the concept is important
in the context of that document.
Document structure (WD) is another important
clue in determining which elements of a text are
important enough to include in a summary (Marcu,
1997). If a concept is featured prominently in the
title, or appears in the first or final paragraphs, it is
likely more important than a concept buried in the
middle of the document. Importance is also affected
by certain rhetorical devices (WR) which serve to
highlight particular concepts. Being used in an id-
iom, or compared to another concept by means of
juxtaposition suggests that a given concept may hold
special significance. Finally, the weights assigned
by our graph understanding system for the additional
propositions identified in the graphics are incorpo-
rated into the ID of the concepts involved as WG.
5.3 Selecting Content for a Summary
To select concepts for inclusion in the summary,
the model will then be passed to a discourse-aware
graph-based content selection framework (Demir et
al., 2010), which selects concepts one at a time
and iteratively re-weights the remaining items so
as to include related concepts and avoid redun-
dancy. This algorithm incorporates PageRank (Page
et al, 1999), but with several modifications. In ad-
dition to centrality assessment based on relation-
ships between concepts, it includes apriori impor-
tance nodes enabling us to incorporate concept com-
pleteness, number of expressions, document struc-
ture, and rhetorical devices. More importantly from
a summary generation perspective, the algorithm it-
eratively picks concepts one at a time, and re-ranks
the remaining entries by increasing the weight of re-
lated items and discounting redundant ones. This
allows us to select concepts that complement each
other while simultaneously avoiding redundancy.
6 Generating an Abstractive Summary of
a Multimodal Document
Figure 4 shows the two most important concepts
(Company1 & Person1) selected from the Medtronic
article in Section 5.1. Following McDonald and
Greenbacker (2010), we use the phrasings observed
by the parser as the ?raw material? for expressing
these selected concepts. Reusing the original phras-
ings reduces the reliance on built-in or ?canned?
constructions, and allows the summary to reflect the
style of the original text. The derivation trees stored
in the model to realize a particular concept may use
different syntactic constituents (e.g., noun phrases,
verb phrases). Multiple trees are often available for
each concept, and we must select particular trees that
fit together to form a complete sentence.
The semantic model also contains concepts rep-
resenting propositions extracted from the graphics,
as well as relationships connecting these graphical
concepts with those derived from the text, and there
are no existing phrasings in the original document
that can be reused to convey this graphical content.
However, the set of proposition types that can be ex-
tracted from the graphics is finite. To ensure that we
have realizations for every concept in our model, we
create TAG derivation trees for each type of graphi-
59
cal proposition. As long as realizations are supplied
for every proposition that can be decomposed in the
model, our system will never be stuck with a concept
without the means to express it.
The set of expressions is augmented by many
built-in realizations for common semantic relation-
ships (e.g., ?is-a,? ?has-a?), as well as expressions
inherited from other conceptual categories in the hi-
erarchy. If the observed expressions are retained as
the system analyzes multiple documents over time,
making these realizations available for later use by
concepts in the same category, the variety of utter-
ances we can generate is increased greatly.
By using synchronous TAG trees, we know that
the syntactic realizations of two semantically-related
concepts will fit together syntactically (via substitu-
tion or adjunction). However, the concepts selected
for the summary of the Medtronic article (Com-
pany1 & Person1), are not directly connected in the
model. To produce a single summary sentence for
these two concepts, we must find a way of express-
ing them together with the available phrasings. This
can be accomplished by using an intermediary con-
cept that connects both of the selected items in the
semantic model, in order to ?bridge the gap? be-
tween them. In this example, a reasonable option
would be TargetStockPrice1, one of the many con-
cepts linking Company1 and Person1. Combining
original phrasings from all three concepts (via sub-
stitution and adjunction operations on the underly-
ing TAG trees), along with a ?built-in? realization
inherited by the TargetStockPrice category (a sub-
type of Expectation), yields this surface form:
Wuensch expects a 12-month target of 62
for medical device giant Medtronic.
7 Related Work
Research into providing alternative access to graph-
ics has taken both verbal and non-verbal approaches.
Kurze (1995) presented a verbal description of the
properties (e.g., diagram style, number of data sets,
range and labels of axes) of business graphics. Fer-
res et al (2007) produced short descriptions of the
information in graphs using template-driven genera-
tion based on the graph type. The SIGHT project
(Demir et al, 2008; Elzer et al, 2011) generated
summaries of the high-level message content con-
veyed by simple bar charts. Other modalities, like
sound (Meijer, 1992; Alty and Rigas, 1998; Choi
and Walker, 2010) and touch (Ina, 1996; Krufka et
al., 2007), have been used to impart graphics via a
substitute medium. Yu et al (2002) and Abu Doush
et al (2010) combined haptic and aural feedback,
enabling users to navigate and explore a chart.
8 Discussion
This paper presented our system for providing ac-
cess to the full content of multimodal documents
with line graphs in popular media. Such graph-
ics generally have a high-level communicative goal
which should constitute the core of a graphic?s sum-
mary. Rather than providing this summary at the
point where the graphic is first encountered, our sys-
tem identifies the most relevant paragraph in the
article and relays the graphic?s summary at this
point, thus increasing the presentation?s coherence.
System extensions currently in development will
provide a more integrative and accessible way for
visually-impaired readers to experience multimodal
documents. By producing abstractive summaries of
the entire document, we reduce the amount of time
and effort required to assimiliate the information
conveyed by such documents in popular media.
Several tasks remain as future work. The intended
message descriptions generated by our system need
to be evaluated by both sighted and non-sighted hu-
man subjects for clarity and accuracy. We intend
to test our hypothesis that graphics ought to be de-
scribed alongside the most relevant part of the text
by performing an experiment designed to determine
the presentation order preferred by people who are
blind. The rules developed to identify elaborative
propositions also must be validated by a corpus or
user study. Finally, once the system is fully imple-
mented, the abstractive summaries generated for en-
tire multimodal documents will need to be evaluated
by both sighted and sight-impaired judges.
Acknowledgments
This work was supported in part by the by the Na-
tional Institute on Disability and Rehabilitation Re-
search under grant H133G080047 and by the Na-
tional Science Foundation under grant IIS-0534948.
60
References
Iyad Abu Doush, Enrico Pontelli, Tran Cao Son, Dominic
Simon, and Ou Ma. 2010. Multimodal presenta-
tion of two-dimensional charts: An investigation using
Open Office XML and Microsoft Excel. ACM Trans-
actions on Accessible Computing (TACCESS), 3:8:1?
8:50, November.
James L. Alty and Dimitrios I. Rigas. 1998. Communi-
cating graphical information to blind users using mu-
sic: the role of context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
CHI ?98, pages 574?581, Los Angeles, April. ACM.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
Stephen H. Choi and Bruce N. Walker. 2010. Digitizer
auditory graph: making graphs accessible to the visu-
ally impaired. In Proceedings of the 28th International
Conference on Human Factors in Computing Systems,
CHI ?10, pages 3445?3450, Atlanta, April. ACM.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters and Demonstrations), Sussex, UK, June.
ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
Charles F. Greenbacker, Sandra Carberry, and Kathleen F.
McCoy. 2011. A corpus of human-written summaries
of line graphs. In Proceedings of the EMNLP 2011
Workshop on Language Generation and Evaluation,
UCNLG+Eval, Edinburgh, July. ACL. (to appear).
Satoshi Ina. 1996. Computer graphics for the blind. SIG-
CAPH Newsletter on Computers and the Physically
Handicapped, pages 16?23, June. Issue 55.
Stephen E. Krufka, Kenneth E. Barner, and Tuncer Can
Aysal. 2007. Visual to tactile conversion of vector
graphics. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 15(2):310?321, June.
Solomon Kullback. 1968. Information Theory and
Statistics. Dover, revised 2nd edition.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ?95, pages 68?73, Seattle, July. ACM.
Martin Kurze. 1995. Giving blind people access
to graphics (example: Business graphics). In Pro-
ceedings of the Software-Ergonomie ?95 Workshop
on Nicht-visuelle graphische Benutzungsoberfla?chen
(Non-visual Graphical User Interfaces), Darmstadt,
Germany, February.
Xiaoyong Liu and W. Bruce Croft. 2002. Passage re-
trieval based on language models. In Proceedings of
the eleventh international conference on Information
and knowledge management, CIKM ?02, pages 375?
382.
Daniel C. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto, December.
David D. McDonald and Charles F. Greenbacker. 2010.
?If you?ve heard it, you can say it? - towards an ac-
count of expressibility. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 185?190, Trim, Ireland, July. ACL.
David D. McDonald. 1992. An efficient chart-based
algorithm for partial-parsing of unrestricted texts. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing, pages 193?200, Trento, March.
ACL.
David D. McDonald. 2000a. Issues in the repre-
sentation of real texts: the design of KRISP. In
Lucja M. Iwan?ska and Stuart C. Shapiro, editors, Nat-
ural Language Processing and Knowledge Represen-
tation, pages 77?110. MIT Press, Cambridge, MA.
David D. McDonald. 2000b. Partially saturated refer-
ents as a source of complexity in semantic interpreta-
tion. In Proceedings of the NAACL-ANLP 2000 Work-
shop on Syntactic and Semantic Complexity in Natural
61
Language Processing Systems, pages 51?58, Seattle,
April. ACL.
Peter B.L. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions on
Biomedical Engineering, 39(2):112?121, February.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number:
SIDL-WP-1999-0120.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Dominic Widdows. 2003. Orthogonal negation in vector
spaces for modelling word-meanings and document
retrieval. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics - Volume
1, ACL ?03, pages 136?143, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: a statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ?99, pages 315?316, Berkeley,
August. ACM.
Wai Yu, Douglas Reid, and Stephen Brewster. 2002.
Web-based multimodal graphs for visually impaired
people. In Proceedings of the 1st Cambridge Work-
shop on Universal Access and Assistive Technology,
CWUAAT ?02, pages 97?108, Cambridge, March.
Chengxiang Zhai. 2008. Statistical Language Models
for Information Retrieval. Morgan and Claypool Pub-
lishers, December.
62
