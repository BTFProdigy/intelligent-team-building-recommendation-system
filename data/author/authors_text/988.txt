NAACL HLT Demonstration Program, pages 5?6,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Adaptive Tutorial Dialogue Systems Using Deep NLP Techniques
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow,
Manuel Marques-Pita, Colin Matheson and Johanna D. Moore
ICCS-HCRC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, United Kingdom
(mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk ?
Abstract
We present tutorial dialogue systems in
two different domains that demonstrate
the use of dialogue management and deep
natural language processing techniques.
Generation techniques are used to produce
natural sounding feedback adapted to stu-
dent performance and the dialogue his-
tory, and context is used to interpret ten-
tative answers phrased as questions.
1 Introduction
Intelligent tutoring systems help students improve
learning compared to reading textbooks, though not
quite as much as human tutors (Anderson et al,
1995). The specific properties of human-human di-
alogue that help students learn are still being stud-
ied, but the proposed features important for learn-
ing include allowing students to explain their actions
(Chi et al, 1994), adapting tutorial feedback to the
learner?s level, and engagement/affect. Some tuto-
rial dialogue systems use NLP techniques to analyze
student responses to ?why? questions. (Aleven et al,
2001; Jordan et al, 2006). However, for remediation
they revert to scripted dialogue, relying on short-
answer questions and canned feedback. The result-
ing dialogue may be redundant in ways detrimental
to student understanding (Jordan et al, 2005) and
allows for only limited adaptivity (Jordan, 2004).
?This work was supported under the 6th Framework Pro-
gramme of the European Commission, Ref. IST-507826, and
by a grant from The Office of Naval Research N000149910165.
We demonstrate two tutorial dialogue systems
that use techniques from task-oriented dialogue sys-
tems to improve the interaction. The systems are
built using the Information State Update approach
(Larsson and Traum, 2000) for dialogue manage-
ment and generic components for deep natural lan-
guage understanding and generation. Tutorial feed-
back is generated adaptively based on the student
model, and the interpretation is used to process
explanations and to differentiate between student
queries and hedged answers phrased as questions.
The systems are intended for testing hypotheses
about tutoring. By comparing student learning gains
between versions of the same system using different
tutoring strategies, as well as between the systems
and human tutors, we can test hypotheses about the
role of factors such as free natural language input,
adaptivity and student affect.
2 The BEEDIFF Tutor
The BEEDIFF tutor helps students solve symbolic
differentiation problems, a procedural task. Solu-
tion graphs generated by a domain reasoner are used
to interpret student actions and to generate feed-
back.1 Student input is relatively limited and con-
sists mostly of mathematical formulas, but the sys-
tem generates adaptive feedback based on the notion
of student performance and on the dialogue history.
For example, if an average student asks for a hint
on differentiating sin(x2), the first level of feedback
may be ?Think about which rule to apply?, which
1Solution graphs are generated automatically for arbitrary
expressions, with no limit on the complexity of expressions ex-
cept for possible efficiency considerations.
5
can then be specialized to ?Use the chain rule? and
then to giving away the complete answer. For stu-
dents with low performance, more specific feed-
back can be given from the start. The same strat-
egy (based on an initial corpus analysis) is used in
producing feedback after incorrect answers, and we
intend to use the system to evaluate its effectiveness.
The feedback is generated automatically from a
single diagnosis and generation techniques are used
to produce appropriate discourse cues. For example,
when a student repeats the same mistake, the feed-
back may be ?You?ve differentiated the inner layer
correctly, but you?re still missing the minus sign?.
The two clauses are joined by a contrast relationship,
and the second indicates that an error was repeated
by using the adverbial ?still?.
3 The BEETLE Tutor
The BEETLE tutor is designed to teach students ba-
sic electricity and electronics concepts. Unlike the
BEEDIFF tutor, the BEETLE tutor is built around
a pre-planned course where the students alternate
reading with exercises involving answering ?why?
questions and interacting with a circuit simulator.
Since this is a conceptual domain, for most exer-
cises there is no structured sequence of steps that the
students should follow, but students need to name a
correct set of objects and relationships in their re-
sponse. We model the process of building an answer
to an exercise as co-constructing a solution, where
the student and tutor may contribute parts of the an-
swer. For example, consider the question ?For each
circuit, which components are in a closed path?.
The solution can be built up gradually, with the stu-
dent naming different components, and the system
providing feedback until the list is complete. This
generic process of gradually building up a solution is
also applied to giving explanations. For example, in
answer to the question ?What is required for a light
bulb to light? the student may say ?The bulb must be
in a closed path?, which is correct but not complete.
The system may then say ?Correct, but is that every-
thing?? to prompt the student towards mentioning
the battery as well. The diagnosis of the student an-
swer is represented as a set of correctly given objects
or relationships, incorrect parts, and objects and re-
lationships that have yet to be mentioned, and the
system uses the same dialogue strategy of eliciting
the missing parts for all types of questions.
Students often phrase their answers tentatively,
for example ?Is the bulb in a closed path??. In the
context of a tutor question the interpretation process
treats yes-no questions from the student as poten-
tially hedged answers. The dialogue manager at-
tempts to match the objects and relationships in the
student input with those in the question. If a close
match can be found, then the student utterance is
interpreted as giving an answer rather than a true
query. In contrast, if the student said ?Is the bulb
connected to the battery??, this would be interpreted
as a proper query and the system would attempt to
answer it.
Conclusion We demonstrate two tutorial dialogue
systems in different domains built by adapting di-
alogue techniques from task-oriented dialogue sys-
tems. Improved interpretation and generation help
support adaptivity and a wider range of inputs than
possible in scripted dialogue.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In Proc. AI-ED 2001.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons learned.
The Journal of the Learning Sciences, 4(2):167?207.
M. T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. La-
Vancher. 1994. Eliciting self-explanations improves
understanding. Cognitive Science, 18(3):439?477.
P. Jordan, P. Albacete, and K. VanLehn. 2005. Taking
control of redundancy in scripted tutorial dialogue. In
Proc. of AIED2005, pages 314?321.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tutorial
dialogue system for physics. In Proc. of FLAIRS-06.
P. W. Jordan. 2004. Using student explanations as mod-
els for adapting tutorial dialogue. In V. Barr and
Z. Markov, editors, FLAIRS Conference. AAAI Press.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3-
4):323?340.
6
The Types and Distributions of Errors
in a Wide Coverage Surface Realizer Evaluation
Charles B. Callaway
HCRC, University of Edinburgh
2 Buccleuch Place
Edinburgh, UK EH8 9LW
ccallawa@inf.ed.ac.uk
Abstract
Recent empirical experiments on surface realizers
have shown that grammars for generation can be
effectively evaluated using large corpora. Evalu-
ation metrics are usually reported as single aver-
ages across all possible types of errors and syntac-
tic forms. But the causes of these errors are diverse,
and the extent to which the accuracy of generation
over individual syntactic phenomena is unknown.
This article explores the types of errors, both com-
putational and linguistic, inherent in the evaluation
of a surface realizer when using large corpora. We
analyze data from an earlier wide coverage exper-
iment on the FUF/SURGE surface realizer with the
Penn TreeBank in order to empirically classify the
sources of errors and describe their frequency and
distribution. This both provides a baseline for fu-
ture evaluations and allows designers of NLG ap-
plications needing off-the-shelf surface realizers to
choose on a quantitative basis.
1 Introduction
Surface realization is the process of converting the semantic
and syntactic representation of a sentence or series of sen-
tences into a surface form for a particular language. Most
reusable deep surface realizers [Elhadad, 1991; Bateman,
1995; Lavoie and Rambow, 1997; White and Caldwell, 1998 ]
have been symbolic, hand-written grammar-based systems,
often based on syntactic linguistic theories such as Halliday?s
[Halliday, 1976] systemic functional theory (FUF/SURGE and
KPML) or Mel?cuk?s [Mel?cuk, 1988] Meaning-Text Theory
(REALPRO).
However, corpus-based components, and in particular sta-
tistical surface realizers [Langkilde and Knight, 1998; Ban-
galore and Rambow, 2000; Ratnaparkhi, 2000; Langkilde-
Geary, 2002] have focused attention on a number of problems
facing symbolic NLG systems that until now have been gen-
erally considered future work: large-scale, data-robust and
language- and domain-independent generation. In each case,
empirical evaluation plays a fundamental role in determin-
ing performance at the task of surface realization and setting
baselines for future performance evaluation.
For instance, the HALOGEN statistical realizer [Langkilde-
Geary, 2002] underwent the most comprehensive evaluation
of any surface realizer, which was conducted by measuring
sentences extracted from the Penn TreeBank [Marcus et al,
1993], converting them into its input formalism, and then pro-
ducing output strings. Using automatic metrics from machine
translation then quickly produces figures for global character-
istics of traits such as accuracy.
In a recent experiment, we compared the performance
of HALOGEN relative to the grammar-based FUF/SURGE
surface realizer on the identical corpus and with a similar
methodology [Callaway, 2003; 2004]. Although FUF/SURGE
scored higher than the HALOGEN realizer, we were interested
in absolute as well as relative performance: e.g., what par-
ticular grammatical rules are not well-covered by SURGE?s
grammar?
While the above methodology gives an average figure for
what coverage and accuracy are on a corpus like the Penn
TreeBank, it describes a very wide array of errors as simple
numerical averages. Thus it is impossible to know without
working experience which types of realizer errors HALOGEN
is likely to make. From the perspective of syntactic analysis,
statistical realizers behave as black boxes and thus there is lit-
tle or no attempt made to look at incorrect sentences to trace
back exactly what caused a particular error. Instead, either the
language model is adjusted or a new corpus is obtained. How-
ever, the grammars in symbolic realizers can also be evaluated
as glass boxes, allowing the improvement of an incorrect or
missing grammatical rule to also improve the generation of all
other instances of the same grammatical rule in the corpus.
Those who are looking to use a surface realizer typically
fall into three cases: (1) those employing one for the first
time, often looking to use a well-known system, (2) those
seeking to use a different surface realizer because their cur-
rent realizer has undesirable operational parameters such as
slowness or lack of multilingual support, and (3) those seek-
ing to change realizers due to lack of grammatical coverage
for their domain.
Those falling in this latter category are not interested in a
general quantitative measure for coverage, but rather in find-
ing out, without expending too much effort, whether a given
surface realizer will generate the types of sentences they need.
We were thus interested in a range of questions about the eval-
uation itself as well as the results of the evaluation, which
Realizer Sentences Coverage Exact Matches SS Accuracy BLEU Accuracy
SURGE 2.2 1192 49.5% 368 (30.9%) 0.8206 0.7350
SURGE 2.3 2372 98.5% 1644 (69.3%) 0.9605 0.9321
HALOGEN 1968 82.8% 1132 (57.5%) 0.9450 0.9240
Table 1: Comparing two SURGE versions with HALOGEN on 2416 sentences from Section 23 of the Penn TreeBank.
would allow a number of questions to be answered:
 What types of problems might be encountered during the
experiment itself, and what are their sources?
 What kinds of constructions are difficult to handle in
general, such as adverb positioning, verb argument or-
dering, or very long noun phrases?
 What coverage does FUF/SURGE have for specific types
of infrequent syntactic phenomena, like indirect ques-
tions or topicalizations?
 What is a reasonable distribution of these errors?
 What data is necessary to allow one to predict if a par-
ticular surface realizer will match the expectations of a
new application?
We look at these questions from two perspectives: (1) es-
tablishing baselines to inform future, more detailed evalua-
tions, and (2) providing information about the current state
of the FUF/SURGE grammar and morphology for those who
may wish to use it as a surface realizer in a new project and
need to know whether it will support the types of syntactic
constructions needed in their domain.
Indeed, for some domains and genres, particular phenom-
ena like direct questions may be more important than overall
coverage, and thus when creating an application for those do-
mains, a surface realizer should be chosen with these data for
these phenomena in mind. For instance, domains involving
dialogue must frequently generate sentence fragments rather
than complete sentences, although this latter type is the sub-
ject of the most scrutiny in surface realizer research.
To provide a foundation for answering these questions, we
performed a manual analysis on a set of specific errors in sen-
tences generated from the Penn TreeBank by the FUF/SURGE
surface realizer. To do this, we generated 4,240 sentences, se-
lecting those which did not match the target sentence and had
a high probability of being incorrect due to problems with
wrong or missing syntactic rules in the grammar. Each of the
resulting sentences was analyzed to determine the source of
the error, and in the case of poor grammatical coverage, the
missing or incorrect syntactic construction that was at fault.
We conclude that grammatical errors are actually a small
part of the errors found, and in particular, four types of bad
grammatical rules were responsible for almost two thirds of
accuracy errors. But first we describe in more detail how sen-
tences were generated from the corpus, how they are mea-
sured for accuracy, and what high-level types of errors pre-
vent sentences from being perfectly realized.
2 Methodology
Undertaking a large-scale evaluation for a symbolic surface
realizer requires a large corpus of sentence plans. Since text
planners cannot generate either the requisite syntactic varia-
tion or quantity of text, [Langkilde-Geary, 2002] developed
an evaluation strategy for HALOGEN employing a substi-
tute: sentence parses from the Penn TreeBank [Marcus et al,
1993], a corpus that includes texts from newspapers such as
the Wall Street Journal, and which have been hand-annotated
for syntax by linguists.
However, surface realizers typically have idiosyncratic in-
put representations, and none use the Penn TreeBank parse
representation. Thus a transformer is needed to convert the
TreeBank notation into the language accepted by the sur-
face realizer. As we were interested in comparing the cov-
erage and accuracy of FUF/SURGE with Langkilde?s HALO-
GEN system, we implemented a similar transformer [Call-
away, 2003] to convert Penn TreeBank notation into the rep-
resentation used by FUF/SURGE .
As with the HALOGEN evaluation, we used Simple String
Accuracy [Doddington, 2002] and BLEU [Papineni et al,
2001] to determine the average accuracy for FUF/SURGE .
To obtain a meaningful comparison, we utilized the same ap-
proach as HALOGEN, treating Section 23 of the TreeBank
as an unseen test set. A recent evaluation showed that the
combination of the transformer and an augmented version
of FUF/SURGE had higher coverage and accuracy (Table 1)
compared to both HALOGEN and version 2.2 of FUF/SURGE
.
The difference between the two versions of FUF/SURGE
was especially striking, with the augmented version almost
doubling coverage and more than doubling exact match ac-
curacy. One example of these differences is the addition of
grammatical rules for direct and indirect written dialogue,
which comprise approximately 15% of Penn TreeBank sen-
tences, and which is vital for domains such as written fiction.
However, this evaluation method does not allow for seam-
less comparisons. Inserting a transformation component be-
tween the corpus and realizer means that not only is the
surface realizer being evaluated, but also the accompanying
transformation component itself. We were thus interested in
determining what proportion of reported errors could be at-
tributed to the surface realizer as opposed to the transformer
or to the corpus. To do this, as well as to assist application de-
signers in better interpreting the results of these formal eval-
uations, we needed to identify more precisely what types of
failures are involved.
We thus undertook a manual analysis of errors in Sections
20?22 by hand, individually examining 629 erroneous sen-
tences to determine the reason for their failure. Although
more than 629 out of the 5,383 sentences in these develop-
ment sections produced accuracy errors, we eliminated ap-
proximately 600 others from consideration:
 Simple string accuracy less than 10 characters: Sen-
tences with very small error rates are almost always in-
correct due to errors in morphology, punctuation, and
capitalization. For instance, a single incorrect placement
of quotation marks has a penalty of 4. Thus it made little
sense to manually examine them all in the off chance a
handful had true syntactic errors.
 Sentences of less than 10 or more than 35 words: Sen-
tences with 9 or fewer words were extremely unlikely
to contain complex syntactic constructs, and collectively
had an accuracy of over 99.1%. Sentences larger than
35 words with errors typically had more than one major
grammatical error, making it very difficult to determine a
single ?exact? cause. 96% of sentences within the range
had a single grammatical cause, and the remaining 4%
had only 2 syntactic errors.
Each error instance in the resulting 629 sentences was clas-
sified twice: first to find the source of the error (corpus, trans-
former, or grammar), and then in the case of grammatical er-
rors, to note the syntactic rule that caused the error. These two
classifications are discussed in the following two sections.
We were not realistically able to perform a similar compar-
ison for coverage errors, because out of the 4,240 sentences
satisfying the second criterion of sentence length, only 17 of
them did not generate some string.
3 Types of Methodological Errors
Errors in the surface realizer evaluation, which can manifest
themselves either as empty sentences or as generated sen-
tences which do not exactly match the target string, can arise
from the corpus itself, the transformation component, or the
surface realizer, which consists of the grammar, linearization
rules, and the morphology component.
The corpus itself can be a source of errors due to two main
reasons: (1) the corpus annotators have incorrectly analyzed
the syntactic structure, for instance, attaching prepositions to
the wrong head, or including grammatically impossible rules,
such as NP ! VB CC VB, or (2) the parts of speech were
mistagged by the automatic POS tagger and were not cor-
rected during the supervision process, as in (NP (NN pe-
tition) (VBZ drives)).
Unfortunately, the corpus cannot easily be cleaned up to re-
move these errors, as this significantly complicates the com-
parison of results across corpus versions. We must thus sub-
tract the proportion of corpus errors from the results, creating
a ?topline? which defines a maximum performance measure
for the realizer. Manually analyzing incorrect sentences pro-
duced from the corpus allows this topline to be determined
with reasonable accuracy.
In addition to errors in the corpus, other types of errors
originate in the transformation component, as it attempts to
match TreeBank annotations with rules that produce the req-
uisite input notation for the surface realization. While such
transformers are highly idiosyncratic due to differing input
and output notations, the following categories are abstract,
and thus likely to apply to many different transformers.
 Missing Tag: While there is a standardized set of tags,
the semantic subtags and coreference identifiers can
combine to create unpredictable tags, such as PP-LOC-
PRD-TPC-3 or PP-EXT=2.
 Missing Rule: Often each of the individual tags are rec-
ognized, but no rule exists to be selected for a given
ordered combination of tags, like ADVP-MNR ! RB
COMMA RB RB .
 Incorrect Rule: The transformation component may se-
lect the wrong rule to apply, or the rule itself may be
written incorrectly or may not have been written with all
possible combinations of tag sequences in mind.
 Ordering: Some phrasal elements such as adverbial
clauses can be placed in five or even six different posi-
tions in the matrix clause. Choosing the wrong position
will result in errors reported by the automatic accuracy
metrics, as discussed in [Callaway, 2003]. An important
note is that the order can be incorrect but still make sense
semantically.
Finally, even given a correct input representation, a surface
realizer can also produce errors during the realization pro-
cess. Of the four main surface realizer functions below, only
syntactic rules provide a significant source of accuracy errors
from the point of view of averaged metrics:
 Syntactic Rules: The grammar may be missing a partic-
ular syntactic rule or set of features, or may have been
encoded incorrectly. For instance, the stock version of
FUF/SURGE did not have a rule allowing noun phrases to
terminate in an adverb like ?ago? as in ?five years ago?,
which occurs frequently in the Penn TreeBank, causing
the word to be missing from the generated sentence.
 Morphology: While morphological errors occasionally
appear, they are usually very small and do not contribute
much to the overall accuracy score. The most com-
mon problems are irregular verbs, foreign plural nouns,
and the plurals of acronyms, as well as the marking of
acronyms and y/u initial letters with indefinite a/an.
 Punctuation: While most errors involving punctuation
marks also contribute very little statistically to the over-
all score of a sentence (e.g., a missing comma), the Tree-
Bank also contains combinations of punctuation like
long dashes followed by quotation marks. Addition-
ally, incorrect generation of mixed quotations can lead
to repeated penalties when incorrectly determining the
boundaries of the quoted speech, and large penalties
if the multiple forms of punctuation occur at the same
boundary [Callaway, 2003].
 Linear Precedence: In our analysis of realizer errors,
no examples of obligatory precedence violations were
found (as opposed to ?Ordering? problems described
above.)
4 Types of Syntactic Errors
While general types of errors in the evaluation process are
helpful for improving future evaluations, a more pressing
question for those wishing to use an off-the-shelf surface re-
alizer is how well it will work in their own application do-
main. The coverage and accuracy metrics used by Langkilde
are very broad measures which say nothing about the effec-
tiveness of a surface realizer when generating individual syn-
tactic constructions. The advantage of these metrics are that
they are easy to compute over the entire corpus, but lose this
capability when the same question is asked about particular
subsets of a general corpus, such as all sentences containing
an indirect question.
When performing an analysis of the types of syntactic er-
rors produced by FUF/SURGE when given correct inputs, we
found nine syntactic constructions that resulted in at least two
or more sentences being generated incorrectly. The analysis
allows us to conclude that FUF/SURGE is either not reliably
capable or else incapable of correctly producing the following
syntactic constructions (a manual analysis of all 5,383 sen-
tences to find all correct instances of these constructions is
impractical, although we were able to automate some corpus
searches based on particular semantic tags):
 Inversion: Pragmatic inversions of auxiliaries in embed-
ded questions [Green, 2001] or in any other construction
besides questions, negations, and quoted speech. Thus a
TreeBank sentence like ?This is the best time to buy, as
was the case two years ago.? cannot be generated.
 Missing verb tense: While FUF/SURGE has 36 pre-
defined verb tenses, the corpus contained several in-
stances of another tense: ?. . . which fellow officers re-
member as having been $300.?
 Mixed conjunctions: Often in the Penn TreeBank the
UCP tag (unlike coordinated phrase) marks conjunctions
where the constituents are not all of the same grammat-
ical category, but in compound verb phrases, they are
often marked as simple conjunctions of mixed types.
But FUF/SURGE requires verb phrases in conjunctions
to be compatible on certain clausal features with all con-
stituents, which is violated in the following example:
[VP ! VP CC VP COMMA SBAR-ADV] ?Instead,
they bought on weakness and sold into the strength,
which kept the market orderly.?
 Mixed type NP modifiers: FUF/SURGE?s NP system as-
sumes that cardinal numbers will precede adjective mod-
ifiers, which will precede nominal modifiers, although
the newspaper texts in the Penn TreeBank have more
complex NPs than were considered during the design of
the NP system: ?a $100 million Oregon general obliga-
tion veterans? tax note issue?.
 Direct questions: Direct questions are not very common
in newspaper text, in fact there are only 61 of them out
of the entire 5,383 sentences of Sections 20?22. More
complex questions involving negations and modal aux-
iliaries are not handled well, for example ?Couldn?t we
save $20 billion by shifting it to the reserves?? though
simpler questions are generated correctly.
 Indirect questions: The Penn TreeBank contains a
roughly equivalent number of instances of indirect ques-
tions as direct, such as ?It?s a question of how much
credibility you gain.? and again the reliability of gen-
erating this construction depends on the complexity of
the verbal clause and the question phrase.
 Mixed level quotations: One of the most difficult syntac-
tic phenomena to reproduce is the introduction of sym-
metric punctuation that cuts across categorial boundaries
[Doran, 1998]. For instance, in the following sentence,
the first pair of quote marks are at the beginning of an
adverbial phrase, and the second pair are in the mid-
dle, separating two of its constituents: . . . the U.S. would
stand by its security commitments ?as long as there is a
threat? from Communist North Korea.
 Complex relative pronouns: While simple relatives
are almost always handled correctly except in certain
conjunctions, complex relatives like partitive relatives
(?all of which?, ?some of which?), relatives of indi-
rect objects or peripheral verbal arguments like locatives
(?to whom?, ?in which?), complex possessives (?whose
$275-a-share offer?) and raised NP relatives (?. . . the
swap, details of which. . . ?) were not considered when
FUF/SURGE was designed.
 Topicalization: The clausal system of FUF/SURGE is
based on functional grammar [Halliday, 1976], and so
does not expressly consider syntactic phenomena such
as left dislocation or preposing of prepositional phrases.
Thus sentences like ?Among those sighing with relief
was John H. Gutfreund? may generate correctly depend-
ing on their clausal thematic type, like material or
equative.
While we present the results of a manual analysis of
the data in the next section, it is important to remember
that the large majority of syntactic constructions, punctua-
tion and morphology worked flawlessly in the evaluation of
FUF/SURGE as described in [Callaway, 2004]. As described
earlier, almost 7 out of every 10 sentences in the unseen test
set were exact matches, including punctuation and capital-
ization. Additionally, most errors that did occur were in the
transformation component rather than the surface realizer, as
we will describe shortly. Finally, some well-studied but rare
syntactic constructions did not occur in the sections of the
Penn TreeBank that we examined, such as left dislocation and
negative NP preposing.
5 Data Analysis
As mentioned previously, we undertook a manual analysis of
Sections 20?22 of the Penn TreeBank by hand to determine
specific reasons behind the failure of 629 sentences out of
4,240 that met the criteria of having between 15 and 44 words,
and having a character error rate of more than 9 as determined
by the SSA metric.
Table 2 presents the results for high-level error types as
described in Section 3. It shows that the greatest proportion
of errors is due to the transformation process: 390 sentences
(62.0%) or 15,733 (63.4%) of the character-based accuracy
error. This is expected given that the transformation compo-
nent has been developed in a year or so, while FUF/SURGE
has been in use for around 15 years. Each of the 166 sen-
tences that were incorrect due to inaccurate transformer rules
was verified by ensuring that the sentence would correctly
generate with minor changes to the automatically produced
Error Type # Occurrences Total SSA Penalty Avg. Penalty
Corpus Error 40 6.36% 1922 7.74% 48.05
Transformer Rule Error 166 26.39% 7201 29.01% 43.38
No Transformer Tag 12 1.91% 679 2.74% 56.58
No Transformer Rule 102 16.22% 4320 17.40% 42.35
Ordering (Good) 55 8.74% 2137 8.61% 38.85
Ordering (Bad) 55 8.74% 1396 5.62% 25.38
Punctuation/Morphology 14 2.23% 389 1.57% 27.79
Syntax 185 29.41% 6777 27.30% 33.70
Total 629 100.0% 24821 100.0% 38.60
Table 2: Distribution of 629 high-level errors in the 4,240 tested sentences from Sections 20?22.
functional description. The error rate of the Penn TreeBank
annotation is a reasonably well-known quantity, and there is
a specialized literature describing automatic correction meth-
ods (e.g., [Dickinson and Meurers, 2003]).
One surprise though is that while the number of errors due
to the ordering of floating constituents is the same, the error in
accuracy is skewed to semantically acceptable interpretations.
And while the distribution of the order seems like random
chance, it should be remembered that there can potentially
be up to 10 acceptable placements when there are multiple
floating constituents. Additionally, unrecognized annotation
tags seem to invoke the heaviest average penalty for any error
type, but have the lowest rate of occurrence.
Some advice then for future evaluations of this type would
be to systematically ensure that all tags are normalized in the
corpus before writing transformation rules. Missing trans-
formation rules were always single-case errors, and a large
amount of effort would need to be expended to account for
them, following the well-known 80/20 rule. The data in Ta-
ble 3 then allows other surface realizer researchers to priori-
tize their time when developing their own evaluations.
Finally, slightly over a quarter of the reduction in accuracy
is due to syntactic phenomena that are not handled correctly
by the surface realizer. Given that this error category is most
of interest in determining which surface realizer has the nec-
essary coverage for a particular domain, we investigated fur-
ther the interactions between error rates and individual syn-
tactic phenomena.
Table 3 presents the number of occurrences of errors for
each of the syntactic phenomena presented in the previous
section. We can see that topicalizations, direct questions and
inversions were on average most likely to produce the largest
error per instance, at 73.18, 56.00 and 50.08 edit distances
each. The most frequent error types were mixed NP mod-
ifiers, but such constructions were small enough (often in-
volving only two words in switched order) that they had the
second lowest SSA penalty.
Knowing the ratios of errors allows those weighing differ-
ent surface realizers for a new project to select based on a
number of criteria. For instance, in some domains, it may be
undesirable to have the reader see a large number of surface
language errors where the extent of each error is unimportant,
whereas in other situations, large mistakes that completely
obscure the intent of the sentence are more of a problem.
While Table 3 tells us which syntactic type is most likely
to produce the largest accuracy penalty, it does not tell us
which syntactic types are most frequent in the corpus, since
this would require also counting all correct instances, which
would be very prohibitive to do manually and inaccurate to
do automatically. Knowing this quantity would be of great-
est help to an NLG application designer wanting to compare
surface realizers, but is difficult to do in practice.
We thus decided to look at correct instances of a small
number of rare phenomena which can easily be found by
searching for tags in the TreeBank. For instance, it-clefts are
marked with the annotation S-CLF, of which there are 4 in
the 5,383 sentences in Sections 20?22. However, by search-
ing through the text representations with the regular expres-
sion it is * that and it was * that, we found an
additional 2 it-clefts that were incorrectly marked (although
all 6 examples were exact matches when generated by the sur-
face realizer). By a similar process, we discovered 7 marked
and 1 unmarked wh-clefts, which also were exact matches.
A further investigation for topicalized sentences uncovered 6
instances that were correctly generated versus the 11 incor-
rectly generated.
The number of errors in the Penn TreeBank annotations on
these rare constructions should give pause to those who want
to create statistical selection algorithms from such data, given
that the signal-to-noise ratio may be very high. Additionally,
all of the data presented above reflects only this corpus; spo-
ken dialogue corpora may vary significantly in frequencies of
topicalization and left dislocation, for example.
6 Conclusions
Recent empirical experiments on surface realizers have
shown that grammars for generation can be effectively eval-
uated using large corpora. We have helped clarify to what
extent errors in accuracy may be due to the corpora itself and
in the transformation process necessary to convert annotated
sentences into the surface realizer?s notation. Furthermore,
we have performed a set of quantitative, manual analyses
that have classified with increasing rigor the types of syn-
tactic phenomena missing from the generation grammar of
FUF/SURGE . The results demonstrate that FUF/SURGE is
surprisingly robust with coverage lacking for a few important
but somewhat infrequent syntactic phenomena. Finally, we
Error Type # Occurrences Total SSA Penalty Avg. Penalty
Inversion 12 6.22% 601 8.99% 50.08
Missing Verb Tense 3 1.55% 47 0.70% 15.67
Mixed Conjunction 26 13.47% 1080 16.15% 41.54
Mixed NP Modifiers 64 33.16% 1197 17.90% 18.70
Question, Direct 10 5.18% 560 8.37% 56.00
Question, Indirect 9 4.66% 307 4.59% 34.11
Quotation, Unquoted 13 6.74% 542 8.11% 41.69
Quotation, Mixed 29 15.02% 1231 18.41% 42.45
Relative Clause 16 8.29% 317 4.74% 19.81
Topicalization 11 5.70% 805 12.04% 73.18
Total 193 100.0% 6687 100.0% 34.65
Table 3: Distribution of 193 major syntactic errors in the 4,240 tested sentences from Sections 20?22.
have established a topline and baseline performance measure
for use in future comparisons between surface realizers.
References
[Bangalore and Rambow, 2000] S. Bangalore and O. Ram-
bow. Exploiting a probabilistic hierarchical model for gen-
eration. In COLING?2000: Proceedings of the 18th Inter-
national Conference on Computational Linguistics, Saar-
bruecken, Germany, 2000.
[Bateman, 1995] John A. Bateman. KPML: The KOMET-
penman (multilingual) development environment.
Technical Report Release 0.8, Institut fu?r Integrierte
Publikations- und Informationssysteme (IPSI), GMD,
Darmstadt, 1995.
[Callaway, 2003] Charles B. Callaway. Evaluating coverage
for large symbolic NLG grammars. In Proceedings of
the Eighteenth International Joint Conference on Artificial
Intelligence, pages 811?817, Acapulco, Mexico, August
2003.
[Callaway, 2004] Charles Callaway. Wide coverage sym-
bolic surface realization. In Proceedings of the 41st An-
nual Meeting of the Association for Computational Lin-
guistics, pages 125?128, Barcelona, Spain, July 2004.
[Dickinson and Meurers, 2003] M. Dickinson and D. Meur-
ers. Detecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European Chapter
of the ACL, Budapest, Hungary, April 2003.
[Doddington, 2002] George Doddington. Automatic eval-
uation of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the 2002 Confer-
ence on Human Language Technology, San Diego, CA,
March 2002.
[Doran, 1998] Christine Doran. Incorporating Punctuation
into the Sentence Grammar: A Lexicalized Tree Adjoining
Grammar Perspective. PhD thesis, University of Pennsyl-
vania, Philadelphia, PA, 1998.
[Elhadad, 1991] Michael Elhadad. FUF: The universal uni-
fier user manual version 5.0. Technical Report CUCS-
038-91, Dept. of Computer Science, Columbia University,
1991.
[Green, 2001] Georgia Green. Pragmatic motivation and
exploitation of syntactic rules. Technical report,
University of Illinois, http://www.linguistics.uiuc.edu/g-
green/441/prmoex/index.html, 2001.
[Halliday, 1976] Michael Halliday. System and Function in
Language. Oxford University Press, Oxford, 1976.
[Langkilde and Knight, 1998] Irene Langkilde and Kevin
Knight. Generation that exploits corpus-based statistical
knowledge. In COLING-ACL-98: Proceedings of the Joint
36th Meeting of the ACL andthe 17th International COL-
ING, pages 704?710, Montre?al, Canada, August 1998.
[Langkilde-Geary, 2002] Irene Langkilde-Geary. An empir-
ical verification of coverage and correctness for a general-
purpose sentence generator. In Second International Natu-
ral Language Generation Conference, Harriman, NY, July
2002.
[Lavoie and Rambow, 1997] Benoit Lavoie and Owen Ram-
bow. A fast and portable realizer for text generation sys-
tems. In Proceedings of the 5th Conference on Applied
Natural Language Processing, 1997.
[Marcus et al, 1993] M. Marcus, B. Santorini, and
M. Marcinkiewicz. Building a large annotated cor-
pus of English: The PennTreeBank. Computational
Linguistics, 26(2), 1993.
[Mel?cuk, 1988] Igor A. Mel?cuk. Dependency Syntax: The-
ory and Practice. SUNY Publications, 1988.
[Papineni et al, 2001] K. Papineni, S. Roukos, T. Ward, and
W. J. Zhu. BLEU: A method for automatic evaluation
of MT. Technical Report RC22176, IBM Research, New
York, September 2001.
[Ratnaparkhi, 2000] Adwait Ratnaparkhi. Trainable meth-
ods for surface natural language generation. In Proceed-
ings of the First North American Conference of the ACL,
Seattle, WA, May 2000.
[White and Caldwell, 1998] Michael White and Ted Cald-
well. EXEMPLARS: A practical, extensible framework
for dynamic text generation. In Proceedings of the Ninth
International Workshop on NLG, pages 266?275, Niagara-
on-the-Lake, Ontario, August 1998.
Interpretation and Generation in a Knowledge-Based Tutorial System
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW,
United Kingdom,
{mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk
Abstract
We discuss how deep interpretation and
generation can be integrated with a know-
ledge representation designed for question
answering to build a tutorial dialogue sys-
tem. We use a knowledge representa-
tion known to perform well in answering
exam-type questions and show that to sup-
port tutorial dialogue it needs additional
features, in particular, compositional rep-
resentations for interpretation and struc-
tured explanation representations.
1 Introduction
Human tutoring is known to help students learn
compared with reading textbooks, producing up to
two standard deviations in learning gain (Bloom,
1984). Tutorial systems, in particular cognitive
tutors which model the inner state of a student?s
knowledge, help learning but result in only up to 1
standard deviation learning gain (Anderson et al,
1995). One current research hypothesis is that this
difference is accounted for by interactive dialogue,
which allows students to ask questions freely, and
tutors to adapt their direct feedback and presenta-
tion style to the individual student?s needs.
Adding natural language dialogue to a tutorial
system is a complex task. Many existing tuto-
rial dialogue systems rely on pre-authored curricu-
lum scripts (Person et al, 2000) or finite-state ma-
chines (Rose? et al, 2001) without detailed knowl-
edge representations. These systems are easy to
design for curriculum providers, but offer limited
flexibility because the writer has to predict all pos-
sible student questions and answers.
We argue that the ability to interpret novel,
context-dependent student questions and answers,
and offer tailored feedback and explanations is
important in tutorial dialogue, and that a domain
knowledge representation and reasoning engine is
necessary to support these applications. We dis-
cuss our knowledge representation, and the issues
of integrating it with state-of-the-art interpretation
and generation components to build a knowledge-
based tutorial dialogue system.
Our application domain is in basic electricity
and electronics, specifically teaching a student
how to predict the behavior and interpret measure-
ments in series and parallel circuits. This is a con-
ceptual domain - that is, students are primarily fo-
cused on learning concepts such as voltage and
current, and their relationships with the real world.
The students use a circuit simulator to build cir-
cuits, and their questions and answers depend on
the current context.
There are various sources of context-
dependency in our domain. Students and
tutors refer to specific items in the simulation
(e.g., ?Which lightbulbs will be lit in these
circuits??), and may phrase their answers in an
unexpected way, for example, by saying ?the
lightbulbs in 2 and 4 will be out? instead of
naming the lit lightbulbs. Moreover, students
may build arbitrary circuits not included in the
question, either because they make mistakes, or
because a tutor instructs them to do so as part of
remediation. Thus it would be difficult to produce
and maintain a finite-state machine to predict all
possible situations, both for interpreting the input
and for generating feedback based on the state
of the environment and the previous dialogue
context: a domain reasoner is necessary to handle
such unanticipated situations correctly.
We describe a tutorial system which uses a de-
scription logic-based knowledge representation to
4 KRAQ06
generate intelligent explanations and answers to
a student?s questions, as well as to interpret the
student?s language at all stages of the dialogue.
Our approach relies on using an existing wide-
coverage parser for domain-independent syntactic
parsing and semantic interpretation, as well as a
wide-coverage deep generation system. We dis-
cuss the issues which arise in connecting such re-
sources to a domain knowledge representation in a
practical system.
2 Motivation
A good teaching method for basic electricity
and electronics is eliciting cognitive dissonance
(Schaffer and McDermott, 1992; Arnold and
Millar, 1987) which we are implementing as a
?predict-verify-evaluate? (PVE) cycle. The stu-
dents are asked to make predictions about the be-
havior of a schematic circuit and then build it in a
simulation environment. If the observed results do
not match their predictions, a discussion ensues,
where the computer tutor helps a student learn the
relevant concepts. The PVE exercises are comple-
mented with exercises asking the students to iden-
tify properties of circuits in diagrams and to inter-
pret a circuit?s behavior.
Thus, the system has to answer questions about
circuits which students build and manipulate dy-
namically in a simulation environment, and pro-
duce explanations and feedback tailored to that in-
dividual context. This relies on the following sys-
tem capabilities:
? Understanding and giving explanations.
Since the system relies on inducing cognitive
dissonance, it should be able to explain to the
student why their prediction for a specific cir-
cuit was incorrect, and also verify explana-
tions given by a student.
? Unrestricted language input with reference
resolution. Similar to other conceptual do-
mains (VanLehn et al, 2002) the language
observed in corpus studies is varied and syn-
tactically complex. Additionally, in our do-
main students refer to items on screen, e.g.
?the lightbulb in 5?, which requires the sys-
tem to make the connection between the lan-
guage descriptions and the actual objects in
the environment.
? Tailored generation. The level of detail in
the explanations offered should be sensitive
to student knowledge of the domain. Tutorial
utterances should be natural and use correct
terminology even if a student doesn?t.
To support answering questions and giving ex-
planations, we chose the KM knowledge represen-
tation environment (Clark and Porter, 1999) as a
basis for our implementation. KM is a description-
logic based language which has been used to rep-
resent facts and rules in a HALO system for AP
chemistry tests (Barker et al, 2004). It supports
the generation of explanations and obtained the
highest explanation scores in an independent eval-
uation based on an AP chemistry exam (Friedland
et al, 2004). Thus it is a good choice to provide
reasoning support for explanations and answering
novel questions in a tutorial system. However, KM
has not been used previously in connection with
natural language input for question answering, and
we discuss how the limitations of KM representa-
tions affect the interpretation process in Section 4.
We use a deep domain-independent parser and
grammar to support language interpretation, and
a deep generator to provide natural sounding and
context-dependent text. Both deep parsing and
generation provide the context adaptivity we need,
but they are time-consuming to build for a spe-
cific domain. Now that a number of deep domain-
independent parsing and generation systems are
available in the community, our research goal is to
investigate the issues in integrating them with the
knowledge representation for question answering
to support the requirements of a tutorial dialogue
system. We focus on context-dependent explana-
tion understanding and generation as a primary tu-
toring task in our domain. Section 3 discusses
our representations, Section 4 presents the issues
arising in knowledge representation to support in-
terpretation, and Section 5 discusses the require-
ments for appropriate explanation generation and
how it can be integrated into the system.
3 Representations
From the point of view of tutoring, the most im-
portant requirement on the knowledge representa-
tion is that system reasoning should closely match
human reasoning, so that it can be explained to
students in meaningful terms. Thus, for exam-
ple, a numerical circuit simulator is well suited for
dynamically displaying circuit behaviors, but not
for conceptually tutoring basic circuits, because it
5 KRAQ06
hides physics principles behind complex mathe-
matical equations that are not suitable for learners.
To design our knowledge representation we
started with a set of lessons for our domain de-
signed by psychologists experienced in designing
training courses for physics and simulated envi-
ronments. The lessons were used in a data col-
lection environment with experienced tutors con-
ducting tutoring sessions over a text chat interface.
Each student and tutor were required to go through
the materials presented as a set of slides and solve
pre-defined exercises, but the students asked ques-
tions to get help with problem-solving. The tutor
had complete freedom to choose how to answer
student questions and how to remediate when stu-
dents made mistakes. We are using this data set
to study the types of errors that students make as
well as the language used by both students and tu-
tors. The latter serves as a guide to developing our
interpretation and generation components.
In addition to the set of materials, the course
designers provided a ?glossary? of concepts and
facts that students need to learn and use in expla-
nations, containing approximately 200 concepts
and rules in a form which should be used in model
explanations. We then developed our knowledge
representation so that concepts listed in the glos-
sary were represented as KM concepts, and facts
are represented as rules for computing slots.
An example KM representation for our domain
is shown in Figure 1. It represents the fact that a
lightbulb will be on if it is in a complete path (i.e. a
closed path containing a battery). The explanation
is generated using the comment structure [light-
bulbstate] shown in Figure 2 (slightly simplified
for readability). Explanations are generated sepa-
rately from reasoning in the KM system because
reasoning in general contains too many low-level
details. For example, our rule for computing a
lightbulb state includes two facts: that the light-
bulb has to be in a complete path with a battery,
and that a lightbulb is always in one state only (i.e.
it cannot be broken and on at the same time). The
latter is required for proof completeness, but is too
trivial to be mentioned to students. KM therefore
requires knowledge engineers to explicitly desig-
nate the facts to be used in explanations.
This representation allows KM to generate de-
tailed explanations by using a template string and
then explaining the supporting facts. An example
of a full explanation, together with the adjustments
needed to use it in dialogue rather than as a com-
plete answer, is given in Section 5.
Currently, KM only supports generating expla-
nations, but not verifying them. The explanation
mechanism produces explanations as text directly
from the knowledge representation, as shown in
Figure 2(a). This generation method is not well
suited for a tutorial dialogue system, because it
does not take context into account, as discussed
in Section 5. Therefore, we are designing a struc-
tured representation for explanations to be pro-
duced by the KM explanation mechanism instead
of using English sentences, shown in Figure 2(b).
This will allow us to generate more flexible ex-
planations (Section 5) and also to interpret student
explanations (Section 4).
4 Interpretation
The interpretation process consists of parsing,
reference resolution, dialogue act recognition
and diagnosing student answers. We discuss
reference resolution and diagnosis here as these
are the two steps impacted by the knowledge
representation issues. As a basic example we will
use the student answer from the following pair:1
Problem: For each circuit, which lightbulbs will
be lit? Explain.
Student: the bulbs in 1 and 3 are lit because they
are in a closed path with a battery
To respond to this answer properly, the system
must complete at least the following tasks. First, it
must resolve ?the bulbs in 1 and 3? to correspond-
ing object IDs in the knowledge base, for exam-
ple, LB-13-1-1 and LB-13-3-1. Then, it must ver-
ify that the student statement is factually correct.
This includes verifying that the lightbulbs in 1 and
3 will be lit, and that each of them is in a closed
path with a battery. Finally, it must verify that
the student explanation is correct. This is sepa-
rate from verifying factual correctness. For exam-
ple, a statement ?because they are in a closed path?
is true for both of those lightbulbs, but it is not a
complete explanation, because a lightbulb may be
in a closed path which does not contain a battery,
where it won?t be lit.
1These utterances come from our corpus, though most of
the student answers are not as easy to parse. We are working
on robust parsing methods to address the issues in parsing
less coherent utterances.
6 KRAQ06
(every LightBulb has
(state ((must-be-a Electrical-Usage-State) (exactly 1 Electrical-Usage-State)
(if (the is-damaged of Self) then *Broken-Usage-State
else (if (has-value (oneof ?batt in (the powered-by of Self)))
where ((the state of ?batt) = *Charged-Power-State)))
then *On-Usage-State else *Off-Usage-State) [lightbulbstate])))
Figure 1: The representation of a lightbulb in our KM database
(a)(comment [lightbulbstate]
(:sentence (?a working lightbulb is on if it is in a complete path with a charged battery?))
(:supporting-facts (:triple Self powered-by *) (forall (the powered-by of Self) (:triple It state *)))
(b)(comment [lightbulbstate]
(:rule :object LightBulb :fact (?lb state *On-Usage-State)
:requires ((?lb powered-by ?v1) (?v1 instance-of Battery) (?v1 state *Charged-Power-State))
:bindings ((?lb ? Self ?) (?v1 ? the powered-by of Self ?)) )
Figure 2: A sample comment structure to generate an explanation for a lit lightbulb (a) The KM text
template (b) a new structured representation. Items in angle brackets are computed dynamically
4.1 Interpreting Factual Statements
We use the TRIPS dialogue parser (Dzikovska,
2004) for interpretation. The TRIPS parser pro-
vides a two-layer architecture where the utter-
ance meaning is represented using a domain-
independent semantic ontology and syntax. The
domain-independent representation is used for dis-
course processing tasks such as reference reso-
lution, but it is connected to the domain-specific
knowledge representation by mapping between
the domain-independent and domain-specific on-
tologies (Dzikovska et al, 2003; Dzikovska,
2004). This architecture allows us to separate lin-
guistic and domain-specific knowledge and easily
specialize to new domains.
When applied in our domain, the TRIPS inter-
pretation architecture was helpful in getting the
interpretation started quickly, because we only
needed to extend the lexicon with specific terms
related to basic electricity and electronics (e.g.,
?multimeter?), while other lexical items and syn-
tactic constructions were provided in the domain-
independent part.
The reference resolution module operates on
TRIPS domain-independent representations send-
ing queries to KM as necessary, because the
TRIPS representations offer linguistic features to
guide reference resolution not available in the rep-
resentations used for reasoning. We use a recur-
sive reference resolution algorithm similar to By-
ron (2002) which first resolves ?1 and 3? are re-
solved as names for Circuit-13-1 and Circuit-13-
3,2 and then queries KM to find all lightbulbs in
those circuits. Dialogue context is used to inter-
pret the reference resolution results. In this case,
the context does not matter because the question
sets up all lightbulbs on screen as contextually rel-
evant. But if the student had said ?the ones in 1
and 3?, the query would be for all components in
circuits 1 and 3, and then our algorithm will filter
the query results based on the question context to
retain only lightbulbs.
Once the references are resolved, the whole sen-
tence is converted to a KM statement which repre-
sents the student utterance, in our case (the state
of LB13-1-1) = *On-Usage-State, where LB13-1-
1 is the lightbulb obtained by reference resolution.
This statement is sent to the KM system, which
verifies that it is correct. This procedure allows us
to use dialogue context in understanding, and also
to check correctness of answers easily, even if they
are phrased in an unanticipated way.
However, even with the layer of separation of
linguistic and domain knowledge provided by the
TRIPS architecture, we found that the need to sup-
port interpretation in a compositional way influ-
ences the interaction with knowledge representa-
tion. There are many ways to express the same
query to KM, which differ in efficiency. Two ex-
2This step is not trivial, because on other slides the label
?1? refers to terminals or other components rather than whole
circuits, and therefore there is no 1-to-1 correspondence be-
tween names and objects in the environment.
7 KRAQ06
(a) (allof ?x in (the all-instances of LightBulb) where ((the components of Circuit-13-1) include ?x))
(allof ?x (LightBulb ?x) and (components Circuit-13-1 ?x))
(b) (allof ?comp in (the components of Circuit-13-1) where (?comp isa LightBulb))
(allof ?x (components Circuit-13-1 ?x) and (LightBulb ?x) )
Figure 3: KM Queries to to retrieve all lightbulbs in a circuit with corresponding first-order logic glosses.
ample queries to ask the same question are given in
Figure 3. While their first order logic semantics is
equivalent except for the order of conjuncts, they
are expressed in a very different way in the KM
syntax. Version (b) is more efficient to ask, be-
cause it retrieves the components of circuit 1 first,
a smaller set than the set of all lightbulbs.
This asymmetry presents a challenge to both
language interpretation and knowledge engineer-
ing. Existing reference resolution algorithms (By-
ron, 2002; Bos, 2004) expect the queries for ?the
lightbulb? and ?the lightbulb in 1? to be strictly
compositional in the sense that the phrase ?the
lightbulb? will be represented identically in both
cases, and ?in 1? is represented as an additional
constraint on the lightbulbs. This corresponds to
the query variant (a) in the system. Otherwise
a large amount of query-specific transformations
may be required to produce queries for complex
noun phrase descriptions, diminishing the scala-
bility of the approach.
We had to spend a significant portion of time
in the project developing an efficient and com-
positional knowledge representation. Our cur-
rent solution is to prefer compositionality over ef-
ficiency, even though it impacts performance in
some cases, but we are working on a more gen-
eral solution. Instead of converting directly to
KM from domain-independent language represen-
tations, we will convert all queries in a FOL-like
syntax shown in Figure 3 which uses concepts
from the KM representation, but where all con-
juncts are treated identically in the syntax. The
problem of converting this representation to the
optimal KM form can then be seen as an instance
of query optimization. For example, we can re-
order the conjuncts putting the relations which in-
clude an instance constant (e.g., (the components
of Circuit-13-1)) first in the query, because they
are more likely to limit the search to small sets
of objects. This representation can be easily con-
verted in the KM syntax, and is also useful for
explanation understanding and generation as dis-
cussed below.
4.2 Explanation Understanding
While KM has facilities for generating explana-
tions, it does not have support for reading in a stu-
dent explanation and verifying it. We devised a
method to support this functionality with the aid of
KM explanation generation mechanism. Any time
a student offers an explanation, the KM reasoner
will be called to generate its own explanation for
the same fact, in the structured format shown in
Figure 2(b). Then the student explanation (con-
verted into the same intermediate syntax) can be
matched against the KM-generated explanation to
verify that it is complete, or else that certain parts
are missing.
In our example, the student explanation ?be-
cause they are in a closed path with a battery? will
be represented as (?pa instance-of Path) (?pa is-
closed t) (?b instance-of Battery) (?pa contains
?b) (?pa contains LB-13-1-1).3 This explanation
does not directly match into the explanation struc-
ture from Figure 2(b), because it uses the more
specific term ?in closed path with a battery? rather
than the more general term ?in complete path?
(represented by the powered-by slot). However,
as part of generating the explanation, an explana-
tion structure for the powered-by will be gener-
ated, and it will include the facts (?pa is-closed
t) (?pa contains ?b). This will match the student
explanation. It will be up to the tutorial module to
decide whether to accept the explanation ?as is?,
or lead the student to use the more precise termi-
nology, as discussed in Section 5.
This method can address student explanations
as long as they correspond to parts of typical ex-
planations, and identify missing parts. The biggest
open problem we face is equivalent inferences.
For example, a student may say ?A lightbulb is
not on? instead of ?a lightbulb is off?. KM rea-
soning handles those differences when verifying
factual correctness, but KM does not support sim-
ilar reasoning for matching explanations (which
3Here ?they? would be resolved first to a set of lightbulbs,
and each instance will be treated separately to verify that the
explanation applies.
8 KRAQ06
would correspond to verifying full proofs rather
than individual facts). We are considering bring-
ing a theorem prover to reason over intermediate
representations together with KM axioms to help
interpret explanations, as done in (Makatchev et
al., 2004; Bos, 2005).
5 Generation
The task of the utterance generation component is
to produce tutorial dialogue, such as asking new
questions of the student, conveying the correctness
of their answers, and giving explanations. Expla-
nations may be given in response to a student?s di-
rect ?why? question or when a student has erred
and the pedagogical reasoner has decided that an
explanation is the best remediation strategy. In
each case, the utterance generator must not only
provide a correct, thorough and coherent explana-
tion, but must tailor it so that the student doesn?t
receive too much or too little information. To
be tailorable, explanations must be derived from
the represented domain knowledge and from what
the tutoring system knows about the student (e.g.,
their recent performance).
Directly producing explanations by appending
together pieces of hand-written strings as in Fig-
ure 2(a) usually results in long explanations that
contain little detail of interest to the student. Fig-
ure 4 contains one such example explanation gen-
erated by the KM system in our domain and de-
rived from a query based on the production rule in
Figure 1. This explanation makes sense in answer-
ing an exam question, as intended in the KM sys-
tem, but it is not necessarily helpful in dialogue.
As an example, suppose the student had incor-
rectly answered the question in Section 4, and the
tutoring system decides to correctly explain why
the lightbulbs are lit. Usually, a full explanation
is not necessary in these cases. In the case where
a student gave an incomplete explanation, namely
leaving out the necessary mention of the battery,
a simple response of the form ?Yes, but don?t for-
get the battery? will be infinitely more helpful than
the full explanation. If the student?s explanation is
completely correct, but they have failed to notice
a change in the environment, the more appropriate
explanation is ?The lightbulb is in a closed path,
as well as the battery, but the battery is not oper-
ational?. Furthermore, if a student has shown that
they are knowledgeable about certain fundamen-
tal facts, such as what states a lightbulb may be
in, statements like ?A lightbulb can be on, off or
broken? should be removed.
Adding this reasoning directly to the knowledge
base would make it unwieldy and unmodifiable,
and the string-based generation in KM comments
does not allow for adapting explanations based on
external knowledge such as a student model. To
adapt the KM explanation mechanism to support
such context-dependent generation, instead of cre-
ating explanations via template strings, we have
devised the representation presented in Figure 2(b)
that is based on semantics and allows us to mod-
ify an explanation after it has been produced by
the KM reasoning process but before it has been
converted into a string representation.
Based on this semantic representation, explana-
tion content can be selected more appropriately. If
the interpreter discussed in Section 4.2 determines
that parts of the explanation from the :requires
field are missing, the generation can focus only on
that part of the explanation. The requirements list
would also be used to determine if the student is
not aware of environment properties, such as that a
battery is damaged. Finally, the facts known to the
student can be removed if the corresponding se-
mantic forms were used in previous explanations.
In addition to selecting the explanation content
properly, it is important that the responses given to
the student sound fluid and are easy to understand.
In dialogue, in particular, it is important that pro-
nouns can be generated based on references im-
portant for the student, and avoid repetitiveness in
syntax. Knowledge of linguistic features such as
number and gender, and also knowledge of what
was previously mentioned in the discourse, is nec-
essary to support such natural text generation.
Deep generation utilizes this represented
knowledge along with grammatical and lexical
knowledge of a language, rather than hand-written
strings, to produce utterances. Our current im-
plementation uses a custom utterance generation
component and the STORYBOOK (Callaway and
Lester, 2002) deep text generator modified to
work in a dialogue context. Once the explanation
content is selected, it is passed to the STORYBOOK
system to produce the actual utterance text.
6 Discussion and Related Work
Existing tutorial dialogue systems most often rely
on one of two approaches for interpretation: they
either use wide coverage but shallow language
9 KRAQ06
A lightbulb can be on, off or broken.
A working lightbulb is on if it is in a complete path with a charged battery.
The complete paths of a component are those which are valid, closed, and complete.
A path is complete if it is a closed path with at least one battery and at least...
A path is closed if it is a valid path, a sequence of more than two terminals, ...
A path is valid if it is a single sequence with more than one terminal, all ...
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is valid.
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is closed.
... 6 lines showing that the path contains both L1-13-1-1 and B1-13-1-1 ...
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is complete.
L1-13-1-1 is in a complete path with B1-13-1-1.
A battery is charged unless it is damaged.
B1-13-1-1 is charged.
L1-13-1-1 is on.
Figure 4: Untailored text produced by appending strings from production rules.
interpretation techniques (e.g. LSA (Person et
al., 2000), finite-state parsing (Glass, 2001)) in
combination with shallow knowledge represen-
tations (tutorial scripts or FSA-based knowledge
construction dialogues), or they use deep KR&R
systems but with highly domain-specific parsing
and semantic interpretation (e.g. ATLAS-ANDES
(Rose? et al, 2001), PACT (Aleven et al, 2002)).
The Why2-Atlas system (VanLehn et al, 2002)
makes progress on combining wide coverage in-
terpretation with deep knowledge representation
by utilizing a wide-coverage syntactic grammar
(Rose?, 2000) and a theorem prover to interpret stu-
dent essays (Makatchev et al, 2004). However,
once the misconceptions are diagnosed, the reme-
diation is done via KCDs, with very limited lan-
guage input and pre-authored responses, and with-
out allowing students to ask questions. Our ap-
proach attempts to address issues which arise in
making remediation more flexible and dependent
on context, while still relying on wide-coverage
language interpretation and generation.
The issues we encountered in integrating com-
positional interpretation and reference resolution
with efficient knowledge representation is simi-
lar to a known problem in natural language inter-
faces to databases which may contain slots with
complex meanings. (Stallard, 1986) solves this
problem by providing inference schemas linking
complex-valued slots with compositional repre-
sentations. Our solution in mapping domain-
independent to domain-specific representation is
similar, but stricter compositionality is needed for
reference resolution support, placing additional
constraints on knowledge engineering as we dis-
cussed in Section 4.
We glossed over the interpretation issues related
to metonymy and other imprecise formulations in
questions (Aleven et al, 2002). A taxonomy of
imprecise manual question encodings by domain
experts is presented in (Fan and Porter, 2004).
They also propose an algorithm to address loosely
encoded questions using ontological knowledge.
This algorithm in effect performs question inter-
pretation, and we are planning to incorporate it
into our interpretation mechanism to help inter-
pret question representations obtained automati-
cally during language interpretation.
Text generation of the type that can handle the
necessary linguistic phenomena needed have not
been implemented in tutoring systems that use di-
alogue. The DIAG-NLP tutorial dialogue sys-
tem (Eugenio et al, 2005) shows that structured
explanations from deep generation supported by
knowledge representation and reasoning improve
learning. However, it does not engage in a dia-
logue with the user, and in this paper we showed
that explanations need to be further adjusted in di-
alogue based on previous student responses and
knowledge. Deep generation using context has
been used in some other types of dialogue sys-
tems such as collaborative problem solving (Stent,
2001), and we expect that the approaches used in
content selection and planning in those systems
will also transfer to our deep generation system.
7 Conclusions
We discussed the implementation of a tutorial di-
alogue system which relies on a domain knowl-
edge representation to verify student answers and
offer appropriate explanations. Integration with
domain-independent interpretation and generation
components places additional requirements on
knowledge representation, and we showed how
an existing knowledge representation mechanisms
used in answering exam questions can be adapted
to the more complex task of tutoring, including in-
terpreting student explanations and generating ap-
10 KRAQ06
propriate feedback.
Acknowledgments
This material is based upon work supported by a
grant from The Office of Naval Research number
N000149910165.
References
V. Aleven, O. Popescu, and K. Koedinger. 2002. Pilot-
testing a tutorial dialogue system that supports self-
explanation. Lecture Notes in Computer Science,
2363.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons
learned. The Journal of the Learning Sciences,
4(2):167?207.
M. Arnold and R. Millar. 1987. Being constructive:
An alternative approach to the teaching of introduc-
tory ideas in electricity. International Journal of
Science Education, 9:553?563.
K. Barker, V. K. Chaudhri, S. Y. Chaw, P. Clark, J. Fan,
D. Israel, S. Mishra, B. W. Porter, P. Romero, D.
Tecuci, and P. Z. Yeh. 2004. A question-answering
system for AP chemistry: Assessing KR&R tech-
nologies. In KR, pages 488?497.
B. S. Bloom. 1984. The two sigma problem: The
search for methods of group instruction as effec-
tive as one-to-one tutoring. Educational Researcher,
13:3?16.
Johan Bos. 2004. Computational semantics in dis-
course: Underspecification, resolution, and infer-
ence. Journal of Logic, Language and Information,
13(2):139?157.
Johan Bos. 2005. Towards wide-coverage semantic
interpretation. In Proceedings of Sixth International
Workshop on Computational Semantics (IWCS-6).
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway and James C. Lester. 2002.
Narrative prose generation. Artificial Intelligence,
139(2):213?252, August.
P. Clark and B. Porter, 1999. KM (1.4): Users Manual.
http://www.cs.utexas.edu/users/mfkb/km.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2003.
Integrating linguistic and domain knowledge for
spoken dialogue systems in multiple domains. In
Proceedings of IJCAI-03 Workshop on Knowledge
and Reasoning in Practical Dialogue Systems.
M. O. Dzikovska. 2004. A Practical Semantic Rep-
resentation For Natural Language Parsing. Ph.D.
thesis, University of Rochester.
B. Di Eugenio, D. Fossati, D. Yu, S. Haller, and
M. Glass. 2005. Natural language generation for in-
telligent tutoring systems: A case study. In 12th In-
ternational Conference on Artificial Intelligence in
Education, pages 217?224.
J. Fan and B. W. Porter. 2004. Interpreting loosely en-
coded questions. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence, Six-
teenth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 399?405.
N. S. Friedland, P. G. Allen, M. J. Witbrock, G.
Matthews, N. Salay, P. Miraglia, J. Angele, S.
Staab, D. Israel, V. K. Chaudhri, B. W. Porter, K.
Barker, and P. Clark. 2004. Towards a quanti-
tative, platform-independent analysis of knowledge
systems. In KR, pages 507?515.
M. Glass. 2001. Processing language input in the
CIRCSIM-tutor intelligent tutoring system. In J.
Moore, C. L. Redfield, and W. L. Johnson, editors,
Artificial Intelligence in Education. IOS press.
M. Makatchev, P. W. Jordan, and K. VanLehn. 2004.
Abductive theorem proving for analyzing student
explanations to guide feedback in intelligent tutor-
ing systems. J. Autom. Reasoning, 32(3):187?226.
N. Person, A.C. Graesser, D. Harter, and E. Math-
ews. 2000. Dialog move generation and conversa-
tion management in autotutor. In Workshop Notes of
the AAAI ?00 Fall Symposium on Building Dialogue
Systems for Tutorial Applications.
C. Rose?, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive concep-
tual tutoring in atlas-andes. In Proceedings of AI in
Education 2001 Conference.
C. Rose?. 2000. A framework for robust semantic inter-
pretation. In Proceedings 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
P. S. Schaffer and L. C. McDermott. 1992. Research
as a guide for curriculum development: An example
from introductory electricity. part ii: Design of in-
structional strategies. American Journal of Physics,
60(11):1003?1013.
D. G. Stallard. 1986. A terminological simplifica-
tion transformation for natural language question-
answering systems. In ACL Proceedings, 24th An-
nual Meeting, pages 241?246.
A. Stent. 2001. Dialogue Systems as Conversational
Partners: Applying conversation acts theory to nat-
ural language generation for task-oriented mixed-
initiative spoken dialogue. Ph.D. thesis, University
of Rochester, Rochester, NY, August.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural
Language Tutoring Group. 2002. The architecture
of why2-atlas: a coach for qualitative physics essay
writing. In Proceedings of Intelligent Tutoring Sys-
tems Conference.
11 KRAQ06
Tools for hierarchical annotation of typed dialogue
Myroslava O. Dzikovska, Charles Callaway, Elaine Farrow
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, United Kingdom,
{mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk
1 Introduction
We discuss a set of tools for annotating a complex
hierarchical and linguistic structure of tutorial di-
alogue based on the NITE XML Toolkit (NXT)
(Carletta et al, 2003). The NXT API supports
multi-layered stand-off data annotation and syn-
chronisation with timed and speech data. Using
NXT, we built a set of extensible tools for de-
tailed structure annotation of typed tutorial dia-
logue, collected from a tutor and student typing
via a chat interface. There are several corpora of
tutoring done with such chat-style communication
techniques (Shah et al, 2002; Jordan and Siler,
2002), however, our annotation presents a special
problem because of its detailed hierarchical struc-
ture. We applied our annotation methodology to
annotating corpora in two different tutoring do-
mains: basic electricity and electronics, and sym-
bolic differentiation.
2 Data Structures
Our corpus has two sources of overlapping anno-
tations: the turn structure of the corpus and situ-
ational factors annotation. The data are naturally
split into turns whenever a participant presses their
?submit? button. Timing information is associated
with individual turns, representing the time when
the entire message was sent to the other partici-
pant, rather than with individual words and sounds
as it would be in spoken corpora.
However, turns are too large to be used as units
in the annotation for dialogue phenomena. For
example, the single turn ?Well done. Let?s try a
harder one.? consists of two utterances making
different dialogue contributions: positive tutorial
feedback for the previous student utterance and a
statement of a new tutorial goal. Thus, turns must
be segmented into smaller units which can serve
as a basis for dialogue annotation. We call these
utterances by analogy with spoken language, be-
cause they are often fragments such as ?well done?
rather than complete sentences.
Thus, the corpus has two inherently overlap-
ping layers: the turn segmentation layer, grouping
utterances into turns, and the dialogue structure
layer built up over individual utterances. The NXT
toolkit supports such overlapping annotations, and
we built two individual tools to support corpus an-
notation: an utterance segmentation tool and a tu-
torial annotation tool.
Additionally, the corpus contains annotation
done by the tutor herself at collection time which
we call ?situational factors?. The tutors were
asked to submit a set of these factors after each
turn describing the progress and state of the stu-
dent, such as answer correctness, confidence and
engagement. The factors were submitted sepa-
rately from dialogue contributions and provide an-
other layer of dialogue annotation which has to
be coordinated with other annotations. The fac-
tors are typically related to the preceding student?s
utterance, but the link is implicit in the submis-
sion time.1 Currently we include the factors in the
tool?s transcript display based on the submission
time, so they are displayed after the appropriate
turn in the transcript allowing the annotators to vi-
sually synchronise them with the dialogue. We
also provide an option to annotators for making
them visible or not. In the future we plan to make
factors a separate layer of the annotation linked by
pointers with the preceding student and tutor turns.
1The factor interface was designed to be quick to use and
minimally impact the dialogue flow, so the submission tim-
ings are generally reliable.
57
3 Utterance Segmentation
We process the raw data with an automatic seg-
menter/tokenizer which subdivides turns into indi-
vidual utterances, and utterances into tokens, pro-
viding an initial segmentation for the annotation.
However, perfect automatic segmentation is not
possible, because punctuation is often either in-
consistent or missing in typed dialogue and this
task therefore requires human judgement. The
output of our automatic segmentation algorithm
was verified and corrected by a human annotator.
A screen-shot of the interface we developed for
segmentation verification is displayed in Figure 1.
With the aid of this tool, it took 6 person-hours
to check and correct the automatically segmented
utterances for the 18 dialogues in our corpus.
4 Tutorial Annotation
To provide a detailed analysis of tutorial dialogue
and remediation strategies, we employ a hierarchi-
cal annotation scheme which encodes the recur-
sive dialogue structure. Each tutorial session con-
sists of a sequence of tasks, which may be either
teaching specific domain concepts or doing indi-
vidual exercises. Each task?s structure includes
one or more of the following: giving definitions,
formulating a question, obtaining the student an-
swer and remediation by the tutor.
Generally speaking, the structure of tutorial di-
alogue is governed by the task structure just as in
task-oriented dialogue (Grosz and Sidner, 1986).
However, the specific annotation structure differs
depending on the tutoring method. In our basic
electricity and electronics domain, a tutorial ses-
sion consists of a set of ?teach? segments, and
within each segment a number of ?task? segments.
Task segments usually contain exercises in which
the student is asked a question requiring a simple
(one- or two-line) answer, which may be followed
by a long remediation segment to address the con-
ceptual problems revealed by the answer.
In contrast, in our calculus domain the students
have to do multi-step procedures to differentiate
complex math expressions, but most of the reme-
diations are very short, fixing the immediate prob-
lem and letting the student continue on with the
procedure. Thus even though the dialogue is hier-
archically structured in both cases, the annotation
schemes differ depending on the domain. We de-
veloped a generic tool for annotating hierarchical
dialogue structure which can be configured with
the specific annotation scheme.
The tool interface (Figure 2) consists of a tran-
script of a session and a linked tree representation.
Individual utterances displayed in the transcript
are leaves of the tree. It is not possible to display
them as tree leaves directly as would be done in
syntactic trees, because they are too large to fit in
graphical tree display. Instead, a segment is high-
lighted in a transcript whenever it is selected in the
tutorial structure, and a hotkey is provided to ex-
pand the tree to see all annotations of a particular
utterance in the transcript.
The hierarchical tree structure is supported by a
schema which describes the annotations possible
on each hierarchical tree level. Since the multi-
layered annotation scheme is quite complex, the
tool uses the annotation schema to limit the num-
ber of codes presented to the annotator to be only
those consistent with the tree level. For exam-
ple, in our basic electricity domain annotation de-
scribed above, there are about 20 codes at different
level, but an annotator will only have ?teach? as an
option for assigning a code to a top tree level, and
only ?task? and ?test? (with appropriate subtypes)
for assigning codes immediately below the teach
level, based on the schema defined for the domain.
5 Transcript Segmentation
We had to conduct several simpler data analy-
ses where the utterances in the transcript are seg-
mented according to their purpose. For exam-
ple, in tutorial differentiation the dialogue con-
centrates on 4 main purposes: general discussion,
introducing problems, performing differentiation
proper, or doing algebraic transformations to sim-
plify the resulting expressions. In another analysis
we needed to mark the segments where the student
was making errors and the nature of those errors.
We developed a generic annotation tool to sup-
port such segmentation annotation over the utter-
ance layer. The tool is configured with the name
of the segment tag and colours indicating different
segment types. The annotator can enter a segment
type, and use a freetext field for other information.
A screenshot of the annotation tool with utterance
purposes marked is given in Figure 3.
6 Data Analysis
The NITE query language (NQL) enables us to ac-
cess the data as a directed acyclic graph to cor-
relate simple annotations, such as finding out the
58
Figure 1: Utterance Segmentation Tool.
Figure 2: Tutorial Strategy Annotation Tool.
59
Figure 3: Segmentation tool. The segment labels are shown on the left.
number of turns which contain only mathematical
expressions but no words. We use the NITE query
interface for simpler analysis tasks such as finding
all instances of specific tags and tag combinations.
However, we found the query language less use-
ful for coordinating the situational factors anno-
tated by tutors with other annotation. Each set of
factors submitted is normally associated with the
first student turn which precedes it, but the factors
were not linked to student utterances explicitly.
NQL does not have a ?direct precedence? opera-
tor.2 Thus it is easier derive this information using
the JAVA API. To make the data analysis simpler,
we are planning to add a pointer layer, generated
automatically based on timing information, which
will use explicit pointers between the factor sub-
missions and preceding tutor and student turns.
7 Conclusions
We presented a set of tools for hierarchically an-
notating dialogue structure, suitable for annotating
typed dialogue. The turns in these dialogues are
complex and overlap with dialogue structure, and
our toolset supports segmenting turns into smaller
2It?s possible to express the query in NQL us-
ing its precedence operator ?? as ?($f factor)
($u utterance) (forall $u1 utterance) :
(($f  $u) && ($f  u1)) ? (u  u1)?.
However, this is very inefficient since it must check all
utterance pairs in the corpus to determine direct precedence,
especially if it needs to be included as part of a bigger query.
utterance units and annotating hierarchical dia-
logue structure over the utterances, as well as pro-
viding simpler segmentation annotation.
Acknowledgements
This material is based upon work supported by a
grant from The Office of Naval Research num-
ber N000149910165 and European Union 6th
framework programme grant EC-FP6-2002-IST-
1-507826 (LeActiveMath).
References
Jean Carletta, J. Kilgour, T. O?Donnell, S. Evert, and
H. Voormann. 2003. The NITE object model li-
brary for handling structured linguistic annotation
on multimodal data sets. In Proceedings of the
EACL Workshop on Language Technology and the
Semantic Web.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175?204.
Pamela Jordan and Stephanie Siler. 2002. Student
initiative and questioning strategies in computer-
mediated human tutoring dialogues. In Proceedings
of ITS 2002 Workshop on Empirical Methods for Tu-
torial Dialogue Systems.
Farhana Shah, Martha W. Evens, Joel Michael, and
Allen Rovick. 2002. Classifying student initiatives
and tutor responses in human keyboard-to-keyboard
tutoring sessions. Discourse Processes, 33(1).
60
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 38?45,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Dealing with Interpretation Errors in Tutorial Dialogue
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow, Johanna D. Moore
School of Informatics
University of Edinburgh, Edinburgh, United Kingdom
mdzikovs,ccallawa,efarrow,jmoore@inf.ed.ac.uk
Natalie Steinhauser, Gwendolyn Campbell
Naval Air Warfare Training Systems Division
Orlando, Florida, USA
Abstract
We describe an approach to dealing with
interpretation errors in a tutorial dialogue
system. Allowing students to provide ex-
planations and generate contentful talk can
be helpful for learning, but the language
that can be understood by a computer sys-
tem is limited by the current technology.
Techniques for dealing with understanding
problems have been developed primarily for
spoken dialogue systems in information-
seeking domains, and are not always appro-
priate for tutorial dialogue. We present a
classification of interpretation errors and our
approach for dealing with them within an
implemented tutorial dialogue system.
1 Introduction
Error detection and recovery is a known problem in
the spoken dialogue community, with much research
devoted to determining the best strategies, and learn-
ing how to choose an appropriate strategy from data.
Most existing research is focused on dealing with
problems in an interaction resulting from speech
recognition errors. This focus is justified, since the
majority of understanding problems observed in cur-
rent spoken dialogue systems (SDS) are indeed due
to speech recognition errors.
Recovery strategies, therefore, are sometimes de-
vised specifically to target speech recognition prob-
lems - for example, asking the user to repeat the ut-
terance, or to speak more softly, which only makes
sense if speech recognition is the source of trouble.
However, errors can occur at all levels of process-
ing, including parsing, semantic interpretation, in-
tention recognition, etc. As speech recognition im-
proves and more sophisticated systems are devel-
oped, strategies for dealing with errors coming from
higher (and potentially more complex) levels of pro-
cessing will have to be developed.
This paper presents a classification of non-
understandings, defined as the errors where the sys-
tem fails to arrive at an interpretation of the user?s
utterance (Bohus and Rudnicky, 2005), and a set of
strategies for dealing with them in an implemented
tutorial dialogue system. Our system differs from
many existing systems in two ways. First, all di-
alogue is typed. This was done in part to avoid
speech recognition issues and allow for more com-
plex language input than would otherwise be pos-
sible. But it is also a valid modality for tutoring -
there are now many GUI-based tutoring systems in
existence, and as distance and online learning have
become more popular, students are increasingly fa-
miliar with typed dialogue in chat rooms and discus-
sion boards. Second, different genres impose dif-
ferent constraints on the set of applicable recovery
strategies - as we discuss in Section 2, certain help
strategies developed for task-oriented dialogue sys-
tems are not suitable for tutorial dialogue, because
tutoring systems should not give away the answer.
We propose a targeted help approach for dealing
with interpretation problems in tutorial dialogue by
providing help messages that target errors at differ-
ent points in the pipeline. In our system they are
combined with hints as a way to lead the student
to an answer that can be understood. While some
38
parts of the system response are specific to tutorial
dialogue, the targeted help messages themselves can
serve as a starting point for developing appropriate
recovery strategies in other systems where errors at
higher levels of interpretation are a problem.
The rest of this paper is organized as follows. In
Section 2, we motivate the need for error handling
strategies in tutorial dialogue. In Section 3 we de-
scribe the design of our system. Section 4 discusses
a classification of interpretation problems and our
targeted help strategy. Section 5 provides a prelim-
inary evaluation based on a set of system tests con-
ducted to date. Finally, we discuss how the approach
taken by our system compares to other systems.
2 Background and Motivation
Tutorial dialogue systems aim to improve learning
by engaging students in contentful dialogue. There
is a mounting body of evidence that dialogue which
encourages students to explain their actions (Aleven
and Koedinger, 2000), or to generate contentful talk
(Purandare and Litman, 2008), results in improved
learning. However, the systems? ability to under-
stand student language, and therefore to encourage
contentful talk, is limited by the state of current lan-
guage technology. Moreover, student language may
be particularly difficult to interpret since students
are often unaware of proper terminology, and may
phrase their answers in unexpected ways. For exam-
ple, a recent error analysis for a domain-independent
diagnoser trained on a large corpus showed that a
high proportion of errors were due to unexpected
paraphrases (Nielsen et al, 2008).
In small domains, domain-specific grammars and
lexicons can cover most common phrasings used
by students to ensure robust interpretation (Aleven,
2003; Glass, 2000). However, as the size of the
domain and the range of possible questions and an-
swers grows, achieving complete coverage becomes
more difficult. For essays in large domains, sta-
tistical methods can be used to identify problems
with the answer (Jordan et al, 2006; Graesser et
al., 1999), but these approaches do not perform well
on relatively short single-sentence explanations, and
such systems often revert to short-answer questions
during remediation to ensure robustness.
To the best of our knowledge, none of these tu-
torial systems use sophisticated error handling tech-
niques. They rely on the small size of the domain
or simplicity of expected answers to limit the range
of student input. They reject utterances they cannot
interpret, asking the user to repeat or rephrase, or
tolerate the possibility that interpretation problems
will lead to repetitive or confusing feedback.
We are developing a tutorial dialogue system that
behaves more like human tutors by supporting open-
ended questions, as well as remediations that allow
for open-ended answers, and gives students detailed
feedback on their answers, similar to what we ob-
served with human tutors. This paper takes the first
step towards addressing the problem of handling er-
rors in tutorial dialogue by developing a set of non-
understanding recovery strategies - i.e. strategies
used where the system cannot find an interpretation
for an utterance.
In early pilot experiments we observed that if the
system simply rejects a problematic student utter-
ance, saying that it was not understood, then stu-
dents are unable to determine the reason for this
rejection. They either resubmit their answer mak-
ing only minimal changes, or else they rephrase the
sentence in a progressively more complicated fash-
ion, causing even more interpretation errors. Even
after interacting with the system for over an hour,
our students did not have an accurate picture as to
which phrasings are well understood by the system
and which should be avoided. Previous research also
shows that users are rarely able to perceive the true
causes of ASR errors, and tend to form incorrect the-
ories about the types of input a system is able to ac-
cept (Karsenty, 2001).
A common approach for dealing with these is-
sues in spoken dialogue systems is to either change
to system initiative with short-answer questions (?Is
your destination London??), or provide targeted help
(?You can say plane, car or hotel?). Neither of these
is suitable for our system. The expected utterances
in our system are often more complex (e.g., ?The
bulb must be in a closed path with the battery?), and
therefore suggesting an utterance may be equivalent
to giving away the entire answer. Giving students
short-answer questions such as ?Are the terminals
connected or not connected?? is a valid tutoring
strategy sometimes used by the tutors. However,
it changes the nature of the question from a recall
39
task to a recognition task, which may affect the stu-
dent?s ability to remember the correct solution in-
dependently. Therefore, we decided to implement
strategies that give the student information about the
nature of the mistake without directly giving infor-
mation about the expected answer, and encourage
them to rephrase their answers in ways that can be
understood by the system.
We currently focus on strategies for dealing
with non-understanding rather than misunderstand-
ing strategies (i.e. cases where the system finds an
interpretation, but an incorrect one). It is less clear
in tutorial dialogue what it means for a misunder-
standing to be corrected. In task-oriented dialogue,
if the system gets a slot value different from what
the user intended, it should make immediate correc-
tions at the user?s request. In tutoring, however, it
is the system which knows the expected correct an-
swer. So if the student gives an answer that does not
match the expected answer, when they try to correct
it later, it may not always be obvious whether the
correction is due to a true misunderstanding, or due
to the student arriving at a better understanding of
the question. Obviously, true misunderstandings can
and will still occur - for example, when the system
resolves a pronoun incorrectly. Dealing with such
situations is planned as part of future work.
3 System Architecture
Our target application is a system for tutoring ba-
sic electricity and electronics. The students read
some introductory material, and interact with a sim-
ulator where they can build circuits using batteries,
bulbs and switches, and measure voltage and cur-
rent. They are then asked two types of questions:
factual questions, like ?If the switch is open, will
bulb A be on or off??, and explanation questions.
The explanation questions ask the student to explain
what they observed in a circuit simulation, for exam-
ple, ?Explain why you got the voltage of 1.5 here?,
or define generic concepts, such as ?What is volt-
age??. The expected answers are fairly short, one or
two sentences, but they involve complex linguistic
phenomena, including conjunction, negation, rela-
tive clauses, anaphora and ellipsis.
The system is connected to a knowledge base
which serves as a model for the domain and a rea-
soning engine. It represents the objects and rela-
tionships the system can reason about, and is used
to compute answers to factual questions.1 The stu-
dent answers are processed using a standard NLP
pipeline. All utterances are parsed to obtain syntac-
tic analyses.2 The lexical-semantic interpreter takes
analyses from the parser and maps them to seman-
tic representations using concepts from the domain
model. A reference resolution algorithm similar to
(Byron, 2002) is used to find referents for named ob-
jects such as ?bulb A? and for pronouns.
Once an interpretation of a student utterance has
been obtained, it is checked in two ways. First, its
internal consistency is verified. For example, if the
student says ?Bulb A will be on because it is in a
closed path?, we first must ensure that their answer
is consistent with what is on the screen - that bulb A
is indeed in a closed path. Otherwise the student
probably has a problem either with understanding
the diagrams or with understanding concepts such as
?closed path?. These problems indicate lack of basic
background knowledge, and need to be remediated
using a separate tutorial strategy.
Assuming that the utterance is consistent with the
state of the world, the explanation is then checked
for correctness. Even though the student utterance
may be factually correct (Bulb A is indeed in a
closed path), it may still be incomplete or irrelevant.
In the example above, the full answer is ?Bulb A
is in a closed path with the battery?, hence the stu-
dent explanation is factually correct but incomplete,
missing the mention of the battery.
In the current version of our system, we are partic-
ularly concerned about avoiding misunderstandings,
since they can result in misleading tutorial feedback.
Consider an example of what can happen if there is
a misunderstanding due to a lexical coverage gap.
The student sentence ?the path is broken? should be
interpreted as ?the path is no longer closed?, corre-
sponding to the is-open relation. However, the
1Answers to explanation questions are hand-coded by tutors
because they are not always required to be logically complete
(Dzikovska et al, 2008). However, they are checked for consis-
tency as described later, so they have to be expressed in terms
that the knowledge base can reason about.
2We are using a deep parser that produces semantic analyses
of student?s input (Allen et al, 2007). However, these have to
undergo further lexical interpretation, so we are treating them
as syntactic analyses for purposes of this paper.
40
most frequent sense of ?broken? is is-damaged,
as in ?the bulb is broken?. Ideally, the system lex-
icon would define ?broken? as ambiguous between
those two senses. If only the ?damaged? sense is
defined, the system will arrive at an incorrect inter-
pretation (misunderstanding), which is false by defi-
nition, as the is-damaged relation applies only to
bulbs in our domain. Thus the system will say ?you
said that the path is damaged, but that?s not true?.
Since the students who used this phrasing were un-
aware of the proper terminology in the first instance,
they dismissed such feedback as a system error. A
more helpful feedback message is to say that the sys-
tem does not know about damaged paths, and the
sentence needs to be rephrased.3
Obviously, frequent non-understanding messages
can also lead to communication breakdowns and im-
pair tutoring. Thus we aim to balance the need to
avoid misunderstandings with the need to avoid stu-
dent frustration due to a large number of sentences
which are not understood. We approach this by us-
ing robust parsing and interpretation tools, but bal-
ancing them with a set of checks that indicate poten-
tial problems. These include checking that the stu-
dent answer fits with the sortal constraints encoded
in the domain model, that it can be interpreted un-
ambiguously, and that pronouns can be resolved.
4 Error Handling Policies
All interpretation problems in our system are han-
dled with a unified tutorial policy. Each message to
the user consists of three parts: a social response,
the explanation of the problem, and the tutorial re-
sponse. The social response is currently a simple
apology, as in ?I?m sorry, I?m having trouble under-
standing.? Research on spoken dialogue shows that
users are less frustrated if systems apologize for er-
rors (Bulyko et al, 2005).
The explanation of the problem depends on the
problem itself, and is discussed in more detail below.
The tutorial response depends on the general tu-
torial situation. If this is the first misunderstanding,
the student will be asked to rephrase/try again. If
3This was a real coverage problem we encountered early on.
While we extended the coverage of the lexical interpreter based
on corpus data, other gaps in coverage may remain. We discuss
the issues related to the treatment of vague or incorrect termi-
nology in Section 4.
they continue to phrase things in a way that is mis-
understood, they will be given up to two different
hints (a less specific hint followed by a more spe-
cific hint); and finally the system will bottom out
with a correct answer. Correct answers produced by
the generator are guaranteed to be parsed and under-
stood by the interpretation module, so they can serve
as templates for future student answers.
The tutorial policy is also adjusted depending
on the interaction history. For example, if a non-
understanding comes after a few incorrect answers,
the system may decide to bottom out immediately in
order to avoid student frustration due to multiple er-
rors. At present we are using a heuristic policy based
on the total number of incorrect or uninterpretable
answers. In the future, such policy could be learned
from data, using, for example, reinforcement learn-
ing (Williams and Young, 2007).
In the rest of this section we discuss the explana-
tions used for different problems. For brevity, we
omit the tutorial response from our examples.
4.1 Parse Failures
An utterance that cannot be parsed represents the
worst possible outcome for the system, since detect-
ing the reason for a syntactic parse failure isn?t pos-
sible for complex parsers and grammars. Thus, in
this instance the system does not give any descrip-
tion of the problem at all, saying simply ?I?m sorry,
I didn?t understand.?
Since we are unable to explain the source of the
problem, we try hard to avoid such failures. We use
a spelling corrector and a robust parser that outputs
a set of fragments covering the student?s input when
a full parse cannot be found. The downstream com-
ponents are designed to merge interpretations of the
fragments into a single representation that is sent to
the reasoning components.
Our policy is to allow the system to use such frag-
mentary parses when handling explanation ques-
tions, where students tend to use complex language.
However, we require full parses for factual ques-
tions, such as ?Which bulbs will be off?? We found
that for those simpler questions students are able to
easily phrase an acceptable answer, and the lack of
a full parse signals some unusually complex lan-
guage that downstream components are likely to
have problems with as well.
41
One risk associated with using fragmentary parses
is that relationships between objects from different
fragments would be missed by the parser. Our cur-
rent policy is to confirm the correct part of the stu-
dent?s answer, and prompt for the missing parts, e.g.,
? Right. The battery is contained in a closed path.
And then?? We can do this because we use a diag-
noser that explicitly identifies the correct objects and
relationships in the answer (Dzikovska et al, 2008),
and we are using a deep generation system that can
take those relationships and automatically generate
a rephrasing of the correct portion of the content.
4.2 Lexical Interpretation Errors
Errors in lexical interpretation typically come from
three main sources: unknown words which the lex-
ical interpreter cannot map into domain concepts,
unexpected word combinations, and incorrect uses
of terminology that violate the sortal constraints en-
coded in the domain model.
Unknown words are the simplest to deal with in
the context of our lexical interpretation policy. We
do not require that every single word of an utter-
ance should be interpreted, because we want the
system to be able to skip over irrelevant asides.
However, we require that if a predicate is inter-
preted, all its arguments should be interpreted as
well. To illustrate, in our system the interpretation of
?the bulb is still lit? is (LightBulb Bulb-1-1)
(is-lit Bulb-1-1 true). The adverbial
?still? is not interpreted because the system is un-
able to reason about time.4 But since all arguments
of the is-lit predicate are defined, we consider
the interpretation complete.
In contrast, in the sentence ?voltage is the mea-
surement of the power available in a battery?, ?mea-
surement? is known to the system. Thus, its argu-
ment ?power? should also be interpreted. However,
the reading material in the lessons never talks about
power (the expected answer is ?Voltage is a mea-
surement of the difference in electrical states be-
tween two terminals?). Therefore the unknown word
detector marks ?power? as an unknown word, and
tells the student ?I?m sorry, I?m having a problem
understanding. I don?t know the word power.?
4The lexical interpretation algorithm makes sure that fre-
quency and negation adverbs are accounted for.
The system can still have trouble interpreting sen-
tences with words which are known to the lexical
interpreter, but which appear in unexpected combi-
nations. This involves two possible scenarios. First,
unambiguous words could be used in a way that
contradicts the system?s domain model. For exam-
ple, the students often mention ?closed circuit? in-
stead of the correct term ?closed path?. The former
is valid in colloquial usage, but is not well defined
for parallel circuits which can contain many differ-
ent paths, and therefore cannot be represented in a
consistent knowledge base. Thus, the system con-
sults its knowledge base to tell the student about the
appropriate arguments for a relation with which the
failure occurred. In this instance, the feedback will
be ?I?m sorry, I?m having a problem understanding.
I don?t understand it when you say that circuits can
be closed. Only paths and switches can be closed.?5
The second case arises when a highly ambiguous
word is used in an unexpected combination. The
knowledge base uses a number of fine-grained rela-
tions, and therefore some words can map to a large
number of relations. For example, the word ?has?
means circuit-component in ?The circuit has
2 bulbs?, terminals-of in ?The bulb has ter-
minals? and voltage-property in ?The bat-
tery has voltage?. The last relation only applies to
batteries, but not to other components. These dis-
tinctions are common for knowledge representation
and reasoning systems, since they improve reason-
ing efficiency, but this adds to the difficulty of lex-
ical interpretation. If a student says ?Bulb A has a
voltage of 0.5?, we cannot determine the concept to
which the word ?has? corresponds. It could be either
terminals-of or voltage-property, since
each of those relations uses one possible argument
from the student?s utterance. Thus, we cannot sug-
gest appropriate argument types and instead we in-
dicate the problematic word combination, for exam-
ple, ?I?m sorry, I?m having trouble understanding. I
didn?t understand bulb has voltage.?
Finally, certain syntactic constructions involving
comparatives or ellipsis are known to be difficult
5Note that these error messages are based strictly on the fact
that sortal constraints from the knowledge base for the relation
that the student used were violated. In the future, we may also
want to adjust the recovery strategy depending on whether the
problematic relation is relevant to the expected answer.
42
open problems for interpretation. While we are
working on interpretation algorithms to be included
in future system versions, the system currently de-
tects these special relations, and produces a mes-
sage telling the student to rephrase without the prob-
lematic construction, e.g., ?I?m sorry. I?m having a
problem understanding. I do not understand same
as. Please try rephrasing without the word as.?
4.3 Reference Errors
Reference errors arise when a student uses an am-
biguous pronoun, and the system cannot find a suit-
able object in the knowledge base to match, or on
certain occasions when an attachment error in a
parse causes an incorrect interpretation. We use a
generic message that indicates the type of the ob-
ject the system perceived, and the actual word used,
for example, ?I?m sorry. I don?t know which switch
you?re referring to with it.?
To some extent, reference errors are instances of
misunderstandings rather than non-understandings.
There are actually 2 underlying cases for reference
failure: either the system cannot find any referent at
all, or it is finding too many referents. In the future
a better policy would be to ask the student which of
the ambiguous referents was intended. We expect to
pilot this policy in one of our future system tests.
5 Evaluation
So far, we have run 13 pilot sessions with our sys-
tem. Each pilot consisted of a student going through
1 or 2 lessons with the system. Each lesson lasts
about 2 hours and has 100-150 student utterances
(additional time is taken with building circuits and
reading material). Both the coverage of the interpre-
tation component and the specificity of error mes-
sages were improved between each set of pilots, thus
it does not make sense to aggregate the data from
them. However, over time we observed the trend
that students are more likely to change their behav-
ior when the system issues more specific messages.
Examples of successful and unsuccessful interac-
tions are shown in Figure 1. In (a), the student used
incorrect terminology, and a reminder about how the
word ?complete? is interpreted was enough to get
the conversation back on track.
The dialogue fragment in (b) shows how mes-
sages which are not specific enough can cause a
breakdown in conversation. The system used an in-
sufficiently specific message at the beginning (omit-
ting the part that says that only switches and paths
can be closed). This led the student away from an
answer which was nearly correct with slightly im-
perfect terminology to an answer which was insuffi-
cient (it?s not enough for the components to be con-
nected, they have to be in a closed path), and then
to rephrase it in a more complicated way that was
impossible for the system to understand (consistent
with findings of Bulyko et al (2005)).
The next step would be to conduct a formal evalu-
ation of our policy. We are planning to do this using
recovery rate after different strategies as our evalu-
ation measure (Bohus and Rudnicky, 2005), which
is the percentage of times when the utterance imme-
diately following a non-understanding was success-
fully interpreted by the system.
6 Discussion
Our error handling policy attempts to balance ro-
bustness in interpretation with the need for pre-
cise tutorial feedback. Human tutors accept a wide
range of variation in terminology and phrasing.
However, there is a known trade-off between non-
understandings and misunderstandings. We decided
that misunderstandings are likely to cause more seri-
ous problems with respect to tutoring. Therefore we
set strict understanding requirements and focused on
developing a policy that would encourage students
to use constructions that the system can interpret re-
liably. This is also supported by our observations in
the human-human data ? tutors believe that teaching
correct terminology is important, and we observed
them saying things like ?It?s good to put it into your
own words like that. Just for the sake of the lesson,
could you also use the term closed path??.
Our approach is similar to the TargetedHelp ap-
proach of (Hockey et al, 2003), where in the case of
an interpretation failure the task assistant produces
a report of what the recognizer heard, a description
of the problem, and a possible in-domain utterance
as an alternative. In a user evaluation such targeted
help messages resulted in significantly lower task
completion times compared to a system that pro-
duced a generic ?I didn?t understand? message, and
43
(a)
S: the open switch makes the path incomplete
T: I?m sorry, I?m having a problem understanding. Paths can?t be complete. Only circuits can be
complete. Please try again.
S: the open switch makes the path open
T: Right. An open switch creates a gap.
(b)
T: What are the conditions that are required to make a bulb light up?
S: The lightbulb has to be in a closed circuit with a battery
T: I?m sorry, I?m having a problem understanding. I don?t understand when you say that circuits
can be closed. Please try again.
S: The lightbulb has to be connected to a battery
T: Yes. You correctly mentioned a battery and a bulb. Keep trying.
S: Both the positive and negative connectors have to be connected to
T: I?m sorry, I?m having a problem understanding. Try rephrasing your answer.
S: The lightbulb has to be connected to the positive and negative connectors of the battery
T: I?m sorry, I didn?t understand. Here?s the answer. The battery and the bulb must be contained
in the same closed path.
Figure 1: Sample student-system interactions. S: is student, T: is tutor (system). (a) A successful interaction
where the student changes their language; (b) a failed interaction where system feedback was unhelpful.
subjects gradually learned how to talk to the sys-
tem, reducing the number of misunderstandings over
time. This gives us reason to believe that our sys-
tem can achieve similar effects in tutorial dialogue.
While we don?t suggest alternative domain utter-
ances due to the tutoring reasons described earlier,
the progressively more specific hints serve a simi-
lar function. To what extent this impacts learning
and interaction with the system will have to be de-
termined in future evaluations.
The error handling in our system is significantly
different from systems that analyze user essays be-
cause it needs to focus on a single sentence at a time.
In a system that does essay analysis, such as AUTO-
TUTOR (Graesser et al, 1999) or Why2-Atlas (Jor-
dan et al, 2006) a single essay can have many flaws.
So it doesn?t matter if some sentences are not fully
understood as long as the essay is understood well
enough to identify at least one flaw. Then that par-
ticular flaw can be remediated, and the student can
resubmit the essay. However, this can also cause stu-
dent frustration and potentially affect learning if the
student is asked to re-write an essay many times due
to interpretation errors.
Previous systems in the circuit domain focused on
troubleshooting rather than conceptual knowledge.
The SHERLOCK tutor (Katz et al, 1998) used only
menu-based input, limiting possible dialogue. Cir-
cuit Fix-It Shop (Smith and Gordon, 1997) was a
task-oriented system which allowed for speech in-
put, but with very limited vocabulary. Our system?s
larger vocabulary and complex input result in differ-
ent types of non-understandings that cannot be re-
solved with simple confirmation messages.
A number of researchers have developed er-
ror taxonomies for spoken dialogue systems (Paek,
2003; Mo?ller et al, 2007). Our classification does
not have speech recognition errors (since we are us-
ing typed dialogue), and we have a more complex
interpretation stack than the domain-specific pars-
ing utilized by many SDSs. However, some types
of errors are shared, in particular, our ?no parse?,
?unknown word? and ?unknown attachment? errors
correspond to command-level errors, and our sor-
tal constraint and reference errors correspond to
concept-level errors in the taxonomy of Mo?ller et al
(2007). This correspondence is not perfect because
of the nature of the task - there are no commands in
a tutoring system. However, the underlying causes
are very similar, and so research on the best way
44
to communicate about system failures would benefit
both tutoring and task-oriented dialogue systems. In
the long run, we would like to reconcile these differ-
ent taxonomies, leading to a unified classification of
system errors and recovery strategies.
7 Conclusion
In this paper we described our approach to handling
non-understanding errors in a tutorial dialogue sys-
tem. Explaining the source of errors, without giving
away the full answer, is crucial to establishing ef-
fective communication between the system and the
student. We described a classification of common
problems and our approach to dealing with different
classes of errors. Our experience with pilot studies,
as well as evidence from spoken dialogue systems,
indicates that our approach can help improve dia-
logue efficiency. We will be evaluating its impact on
both student learning and on dialogue efficiency in
the future.
8 Acknowledgments
This work has been supported in part by Office of
Naval Research grant N000140810043.
References
V. A. Aleven and K. R. Koedinger. 2000. The need for
tutorial dialog to support self-explanation. In Proc. of
AAAI Fall Symposion on Building Dialogue Systems
for Tutorial Applications.
O. P. V. Aleven. 2003. A knowledge-based approach
to understanding students? explanations. In School of
Information Technologies, University of Sydney.
J. Allen, M. Dzikovska, M. Manshadi, and M. Swift.
2007. Deep linguistic processing for spoken dialogue
systems. In Proceedings of the ACL-07 Workshop on
Deep Linguistic Processing.
D. Bohus and A. Rudnicky. 2005. Sorry, i didn?t catch
that! - an investigation of non-understanding errors
and recovery strategies. In Proceedings of SIGdial-
2005, Lisbon, Portugal.
I. Bulyko, K. Kirchhoff, M. Ostendorf, and J. Goldberg.
2005. Error-correction detection and response gener-
ation in a spoken dialogue system. Speech Communi-
cation, 45(3):271?288.
D. K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
M. O. Dzikovska, G. E. Campbell, C. B. Callaway, N. B.
Steinhauser, E. Farrow, J. D. Moore, L. A. Butler, and
C. Matheson. 2008. Diagnosing natural language an-
swers to support adaptive tutoring. In Proceedings
21st International FLAIRS Conference.
M. Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Proc.
of the AAAI Fall Symposium on Building Dialogue Sys-
tems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-Hastings,
and R. Kreuz. 1999. Autotutor: A simulation of a
human tutor. Cognitive Systems Research, 1:35?51.
B. A. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist,
J. Hieronymus, A. Gruenstein, and J. Dowding. 2003.
Targeted help for spoken dialogue systems: intelligent
feedback improves naive users? performance. In Pro-
ceedings of EACL.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tuto-
rial dialogue system for physics. In Proceedings of
FLAIRS?06.
L. Karsenty. 2001. Adapting verbal protocol methods to
investigate speech systems use. Applied Ergonomics,
32:15?22.
S. Katz, A. Lesgold, E. Hughes, D. Peters, G. Eggan,
M. Gordin, and L. Greenberg. 1998. Sherlock 2: An
intelligent tutoring system built on the lrdc framework.
In C. Bloom and R. Loftin, editors, Facilitating the
development and use of interactive learning environ-
ments. ERLBAUM.
S. Mo?ller, K.-P. Engelbrecht, and A. Oulasvirta. 2007.
Analysis of communication failures for spoken dia-
logue systems. In Proceedings of Interspeech.
R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Clas-
sification errors in a domain-independent assessment
system. In Proc. of the Third Workshop on Innovative
Use of NLP for Building Educational Applications.
T. Paek. 2003. Toward a taxonomy of communication
errors. In Proceedings of ISCA Workshop on Error
Handling in Spoken Dialogue Systems.
A. Purandare and D. Litman. 2008. Content-learning
correlations in spoken tutoring dialogs at word, turn
and discourse levels. In Proc.of FLAIRS.
R. W. Smith and S. A. Gordon. 1997. Effects of variable
initiative on linguistic behavior in human-computer
spoken natural language dialogue. Computational
Linguistics.
J. D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
45
Pronominalization in Generated Discourse and Dialogue
Charles B. Callaway
Istituto per la Ricerca Scientifica e
Tecnologica (ITC-irst), Italy
callaway@irst.itc.it
James C. Lester
Department of Computer Science
North Carolina State University, USA
lester@adm.csc.ncsu.edu
Abstract
Previous approaches to pronominalization
have largely been theoretical rather than
applied in nature. Frequently, such meth-
ods are based on Centering Theory, which
deals with the resolution of anaphoric pro-
nouns. But it is not clear that complex the-
oretical mechanisms, while having satis-
fying explanatory power, are necessary for
the actual generation of pronouns. We first
illustrate examples of pronouns from vari-
ous domains, describe a simple method for
generating pronouns in an implemented
multi-page generation system, and present
an evaluation of its performance.
1 Introduction
Pronominalization is an important element in the au-
tomatic creation of multi-paragraph and multi-page
texts using natural language generation (NLG). Au-
thors routinely use pronouns in texts spanning all
types of genres, such as newspaper copy, science
fiction and even academic papers. Indeed, without
pronouns, texts quickly become confusing as readers
begin to pay more attention to the writing style than
to the content that makes the text informative or en-
joyable (Callaway and Lester, 2001a). Even worse,
incorrect pronouns can lead readers to misinterpret
the text or draw unsound inferences.
Furthermore, current pronominalization strategies
are ill-equipped to deal with the wide variety of
reasons that pronouns are used in naturally occur-
ring texts. Almost without exception, they focus on
anaphoric pronouns as described in Focus/Centering
Theory (Webber, 1979; Sidner, 1983; Grosz and Sid-
ner, 1986; Walker, 1998), ignoring the multitude of
other possible types. However, it is certainly true
that authors make use of pronouns which are not mo-
tivated by anaphoric reference.
In addition, because such approaches are oriented
towards anaphora resolution during parsing, they ig-
nore structures such as the discourse plan which are
present during generation but not parsing. A typi-
cal discourse plan can include vital information for
pronominalization such as time and clause bound-
aries, ordering of propositions, and semantic de-
tails verbal arguments. Current approaches based
on Centering algorithms thus attempt to recreate a
text coherence structure that duplicates work already
done by the discourse planner.
Finally, there are significant obstacles to verifying
the correctness of existing pronominalization algo-
rithms for any pronominalization theory (Not, 1996;
Yeh and Mellish, 1997; McCoy and Strube, 1999;
Henschel et al, 2000; Kibble and Power, 2000): the
lack of natural language generation systems that can
produce large enough texts to bring discourse-level
processes into play. Because of this, researchers are
forced to simulate by hand how their algorithms will
work on a given text. It is also not sufficient to use
template generation systems to perform this task be-
cause they lack the low-level discourse representa-
tion needed to provide the information upon which
most algorithms base their decisions.
In this paper we first summarize related work
in both anaphora resolution and anaphora genera-
tion. We next describe the range of pronoun types
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 88-95.
                         Proceedings of the 40th Annual Meeting of the Association for
that we found in a wide variety of texts. We pro-
ceed to describe an algorithm for determining ap-
propriate pronominalizations that uses existing NLG
structures and simple numeric techniques. We also
briefly describe an implemented generation system
that contains enough low-level discourse informa-
tion to motivate pronominalization decisions using
this method. Finally, we quantitatively demonstrate
the performance of this simple numerical approach
in both a newspaper and fictional narrative domain.
2 Background and Related Work
Because most NLG systems have focused on lin-
guistic phenomena at the paragraph level and be-
low, there has been intensive investigation into the
core areas of generation that are required to pro-
duce them: discourse planning, sentence planning
and surface realization. Since pronouns are more
likely to be a multiparagraph, discourse-level phe-
nomenon, it has been possible to ignore their inclu-
sion into working NLG systems which are not called
upon to generate lengthy passages.
Indeed, most work on pronouns in computational
linguistics has come under the heading of anaphora
resolution as an element of parsing rather than the
heading of pronominalization as an element of gen-
eration. Since discourse anaphora resolution was
first studied theoretically (Grosz, 1977; Webber,
1979; Sidner, 1983; Grosz and Sidner, 1986), it has
come to be dominated by Centering Theory (Grosz
et al, 1995; Di Eugenio, 1998; Walker, 1998) which
proposes rules for the determination of focus and
salience within a given segment of discourse. Rel-
atively little work has been done on alternate ap-
proaches to pronoun resolution (Hobbs, 1976; Bald-
win, 1995).
While many NLG researchers have attempted to
transfer the ideas of Centering Theory to genera-
tion (Not, 1996; Yeh and Mellish, 1997; McCoy
and Strube, 1999; Henschel et al, 2000; Kibble
and Power, 2000), there has yet been no substan-
tial return contribution to the field of anaphora res-
olution. There are two principal reasons for this.
First, it is extremely difficult to create an NLG sys-
tem that generates the large quantity of texts needed
to exhibit discourse-level phenomena while consis-
tently employing the deep linguistic representations
needed to determine appropriate pronominal forms.
Second, Centering Theory is still vague on the ex-
act definition of terms such as ?segment? (Poesio et
al., 1999a), making it difficult to create a mutually
agreeable implementation.
An additional area of NLG research that deals
with pronouns is that of referring expression gen-
eration (Appelt, 1985; Heeman and Hirst, 1986;
Claassen, 1992; Dale, 1992), which attempts to find
the optimal noun phrase (whether full description,
definite description, deixis, pronoun, or reduced
noun phrase) to enable a reader to mentally select the
intended referent from the set of possible referents
(Reiter and Dale, 1997). Comparatively, referring
expression generation is a process for local disam-
biguation and is not generally concerned with single
phenomena spanning multiple paragraphs. Because
of this, and because the domains and genres we have
studied typically do not involve sets of very simi-
lar referents, we concentrate on discourse-motivated
sources of pronominalization.
3 Examples of Pronominalization
Pronominalization is the appropriate determination,
marking and grammatical agreement of pronouns
(he, she, their, herself, it, mine, those, each other,
one, etc.) as a short-hand reference to an entity or
event mentioned in the discourse. As with anaphora
resolution, the task of a pronominalization algorithm
is to correctly predict which pronoun a person would
prefer in the same situation. The range of possibili-
ties includes leaving the noun phrase as it is, reduc-
ing it by removing some of its modifiers, or replac-
ing it with a pronoun construction.
Our corpora analyses have identified a number of
motivations for converting nouns into pronouns:
1. Anaphoric pronouns: These are the most-
studied cases of pronoun occurrences, which
sequentially follow a specific entity known as
the referent. Anaphors are divided into two
classes, short-distance (within the same sen-
tence) and long-distance (previous sentences).
But John
i
had never been to New Orleans,
and he
i
couldn?t remember if anyone in his
i
family had either.
2. Cataphoric pronouns: According to Quirk et
al. (1985), cataphors are those pronouns which
occur before their referents in the linear flow of
text within the same sentence, where the pro-
noun is either at a lower structural level or is
part of a fronted circumstantial clause or prepo-
sitional phrase which could have appeared after
the reference. Additionally, this category could
include clefting pronouns.
Before he
i
joined the navy, Gerald
i
made
peace with his family.
3. Pronouns Lacking Textual Antecedents: This
category includes document deixis (via a
demonstrative pronoun), authorial or reader
reference, and situational pronouns.
This is the first document to show . . .
We discuss these strategies in the next section.
The group had never seen anything like it.
4. Reflexive and Reciprocal Pronouns: Most
verbs use special pronouns when the subject
and object corefer. A discourse history algo-
rithm can employ that knowledge to mark re-
flexive and reciprocal pronouns appropriately.
Kittens
i
often watch themselves
i
in mirrors.
Baby lions
j
tackle each other
j
when playing.
5. Partitive pronouns: It is important to know con-
ceptually what it is that the pronoun is trying
to replace. Otherwise, it becomes impossible
to achieve the types of pronominalizations that
authors are routinely capable of creating. This
requires accurate information in the knowledge
base or linguistic structure from which the sen-
tences are derived.
As the horses ran by, she roped one.
* As the horses ran by, she roped it.
* As the horses ran by, she roped them.
In addition to these motivations, we identified
several factors that prevent pronouns from occurring
where they otherwise might:
6. Pronouns across boundaries: After a chapter,
section or other obvious boundary, such as a
change in time, place, or both, as in (McCoy
and Strube, 1999), authors will typically ?re-
set? pronominalization just as if it were the
beginning of the entire text. Antecedent ref-
erences that break these boundaries are some-
times marked by the authors in academic texts:
As we saw in the previous section, . . .
7. Restrictions from modifiers: Because pronouns
cannot have modifiers like nouns, adding an ad-
jective, relative clause, or some other modifier
prevents a noun from being replaced by a pro-
noun. For instance:
The mayor had already read the full proposal.
* The mayor had already read the full it.
8. Focused nouns: Especially after a vocally
stressed discourse marker (Wolters and Byron,
2000) or some other marked shift in topic, a
word that normally would be pronominalized
is often not, as in this example:
. . . and you frequently find that mice occupy
an important part of the modern medical labo-
ratory. In other words, mice are especially nec-
essary for diagnosing human cancers . . .
9. Semantic and syntactic considerations: A
small number of semantic relations and syntac-
tic constructions prohibit pronominalization:
* The stranger was just called him. (Bob)
* Roberta was no longer a her. (child)
* The father, a tyrant of a him, . . . (man)
10. Optional pronominalization: Often there are
borderline cases where some authors will use
pronouns while others won?t. A single algo-
rithm may be tuned to match a particular au-
thor?s style, but parameterization will be nec-
essary to match a variety of styles. Thus it is
extremely difficult to exactly match any partic-
ular text without having the ability to adjust the
pronominalization algorithm.
Pronominalization occurs equally as often in ex-
position as in dialogue, but dialogue can have
slightly different pronominalizations depending on
the relationship between the utterer and the hearer:
11. Speaker self-reference:
?John thinks John will go find John?s shoes,?
John said.
changes to first person singular pronouns:
?I think I will go find my shoes,? John said.
12. Speaker references hearer(s):
?Mary should go find Mary?s shoes,? John
said.
changes to second person pronouns:
?You should go find your shoes,? John said.
13. Reference to speaker and hearer (or to speaker
and a third party):
?John and Mary should go find John and
Mary?s shoes,? John said.
changes to first person plural pronouns:
?We should go find our shoes,? John said.
14. Reference to a third party:
?Bob and Mary went to eat Bob and Mary?s
breakfast,? John said.
changes to third person plural pronouns:
?They went to eat their breakfast,? John said.
15. Finally, the treatment of pronouns differs de-
pending if they are inside or outside of the di-
rect quotation. For example:
?Oh man, I forgot to eat my breakfast!? John
muttered to himself while grabbing his shoes.
Although this enumeration is surely incomplete,
it provides a basic description of the types of phe-
nomena that must be handled by a generation system
in order to produce text with the types of pronouns
found in routine human-produced prose.
4 Architectural Concerns
In order to correctly account for these phenomena
during generation, it is necessary to have detailed
information about the underlying discourse struc-
ture. Although a template generation system could
be augmented to record this information, in practice
only deep structure, full-scale NLG systems have the
requisite flexibility. Because a pronominalization al-
gorithm typically follows the discourse planner, it
frequently has access to the full discourse plan.
A typical discourse plan is a tree structure, where
internal nodes represent structuring relations while
leaf nodes represent individual sentential elements
that are organized semantically. In addition, the ele-
ments of the discourse tree are typically rooted in the
semantic knowledge base which the discourse plan-
ner drew from when constructing the discourse plan.
The discourse plan supplies the following informa-
tion that is useful for pronominalization:
 Linearization: The sequencing information
stored in the discourse tree can be used to mo-
tivate anaphoric and cataphoric pronouns as
shown in items 1 & 2 of Section 3.
 Semantic Structure: The original subgraphs
(or semantic subnetworks) derived from the
knowledge base can motivate content vs. sit-
uational knowledge (item 3) reflexive and re-
ciprocal pronouns via argument lists (item 4),
partitive pronouns (item 5), and the existence
of NP modifiers (item 7), and can identify se-
mantic types in relations (item 9).
 Discourse Structure: The rhetorical relations
that hold between different sentences typically
imply where section boundaries are located
(item 6), indicate what types of discourse mark-
ers are employed (item 8), and in the case of
dialogue, know which actors are speaking, lis-
tening, or not present (items 11-15).
This detailed knowledge of the discourse is avail-
able to an implemented pronominalization compo-
nent utilizing any theory, including Centering the-
ory. We thus now turn our attention to what role this
information plays in a pronominalization algorithm.
5 A Simple Pronominalization Algorithm
At an abstract level, the pronominalization algo-
rithms derived from Centering theory are easily ex-
pressed: if Centering theory predicts a pronoun
would be used in anaphora resolution in a given seg-
ment of text, then generate the appropriate pronoun.
While this works for many cases of anaphoric pro-
nouns [84.7% in (McCoy and Strube, 1999), 87-
90% in (Henschel et al, 2000)], we have seen that
these form only a subset of the potential reasons for
pronominalization. Furthermore, this approach as-
sumes that the discourse tree was constructed with
Centering theory in mind.
Given:
LNE, the linearized list of nominal elements
NE, the current nominal element
SEEN , the list of encountered nominal elements
D, the dialogue state of the current leaf node
RS, the rhetorical structure near the leaf node
SC, the sentence counter
Do:
SEEN ( ; SC ( 0
while LNE 6=  do
NE ( first(LNE)
if NE 62 SEEN
then reset-counters(NE),
SEEN ( SEEN NE
else update-counters(NE)
D ( updateDialogueState()
RS ( updateLocalRhetoricalStructure()
if (topic-shift _ time-shift) 2 RS
then SC ( SC + 10
else if modifiers(NE;RS) =  ^
(special-relation _ appositive) 62 RS
if D == QuotedDialogue
then mark(quoted-pronoun(NE;RS))
else if subject-matches-object(NE;RS)
then mark(ReflexivePronoun)
else if sent-distance(NE;SC) = 0
then mark(MultipleInSentencePronoun)
else if 3 <= sent-distance(NE;SC) < 1
and nominal-distance(NE) < 3
then mark(LongDistancePronoun),
else if recency(NE) > 3
then mark(ShortDistancePronoun),
LNE ( remove-first(LNE); SC ( SC + 1
Figure 1: The Pronominalization Algorithm
However, it is not clear that Centering theory itself
is necessary in generation, let alne its accompany-
ing algorithms and data structures. Because Cen-
tering theory is typically applied to parsing (which
starts with no discourse tree), it may not be the most
efficient technique to use in generation (which has a
complete discourse tree available for inference).
Instead, we attempted to determine if the informa-
tion already present in the discourse tree was enough
to motivate a simpler algorithm based on the follow-
ing available data:
 Ordered sequence of nominal elements: Be-
cause the discourse tree is linearized and in-
dividual leaves of the tree annotate which ele-
ments have certain semantic roles, a very good
guess can be made as to which nominal ele-
ments precede others at the clause level.
 Known paragraph and sentence boundaries:
Analysis of the rhetorical structure of the dis-
course tree allows for the determination of
boundaries and thus the concept of metric dis-
tance between elements.
 Rhetorical relations: The rhetorical relations
can tell us which nominal elements follow dis-
course markers and which are used reflexively
or reciprocally.
 Dialogue: By recording the participants in dia-
logue, the discourse tree allows for the appro-
priate assignment of pronouns both inside and
outside of the direct quote itself.
The algorithm we developed considers the cur-
rent discourse leaf node and the rhetorical structure
above it, and also makes use of the following data:
 Nominal element distance: How many total
(non-distinct) nominal elements ago a particu-
lar element was last used.
 Recency: How many distinct nominal elements
have been seen since its last use.
 Sentential distance: How many sentences (pro-
totypical clauses) have appeared since the last
usage of this nominal element.
The algorithm itself (Figure 1) is best character-
ized as a counting method, that is, it loops once
through the linearized list of nominal elements and
makes pronominalization decisions based on the lo-
cal information described above, and then updates
those numerical counters. Numerical parameters
(e.g., recency(NE) > 3) are derived from empir-
ical experimentation in generating multi-page prose
in a narrative domain.
While it lacks the explanatory power of a rela-
tively mature linguistic theory, it also lacks the ac-
companying complexity and is immediately appli-
cable to real-world deep generation systems. The al-
gorithm is traced in Figure 2, although due to space
limitations some phenomena such as dialogue, long
distance and reflexive pronouns are not shown.
6 Implementation and Evaluation
STORYBOOK (Callaway and Lester, 2001b; Call-
away and Lester, in press) is an implemented nar-
rative generation system that converts a pre-existing
Sentences as seen by the reader (antecedents underlined, pronouns in bold):
Now, it happened that a wolf
1
, a very cruel, greedy creature
2
also heard Little Red Riding Hood
3
as
she
4
passed, and he
5
longed to eat her
6
for his
7
breakfast
8
. But he
9
knew Hugh
10
, the woodman
11
,
was at work
12
very near with his
13
great dog
14
.
Sentences as produced by the discourse planner before revision:
S1: Now, it happened that a wolf
1
, a very cruel, greedy creature
2
also heard Little Red Riding Hood
3
as Little Red Riding Hood
4
passed.
S2: The wolf
5
longed to eat Little Red Riding Hood
6
for the wolf?s
7
breakfast
8
.
S3: But the wolf
9
knew Hugh
10
, the woodman
11
, was at work
12
very near with Hugh?s
13
great dog
14
.
Each noun element is processed in the order linearized from the discourse plan:
1. The first mention of wolf
1
in the narrative resets its discourse history entry.
2. Creature
2
is the second mention of wolf, but it is in an appositive structure (see pronoun category #9).
3. LRRH
3
was mentioned just before in the prior paragraph, but ?Now,? is a prosodic discourse marker
(see pronoun category #8), thus modifiers(NE, RS) 6= .
4. For LRRH
3
and LRRH
4
, sentence-distance(NE, SC) = 0 resulting in a multiple-in-sentence-pronoun.
5. Sentence-distance(NE, SC) = 1, but recency(NE) = 2, resulting in a short-distance-pronoun.
6. Similarly, LRRH
6
is converted into a short-distance-pronoun.
7. As with element #4, this is a case resulting in a multiple-in-sentence-pronoun.
9. As with element #5, this is a case resulting in a short-distance-pronoun.
10. The first mention of Hugh
10
in the narrative resets its discourse history entry.
11. As with element #2, the discourse plan reports that this is an appositive.
13. Finally, Hugh
13
is repeated in the same sentence.
Figure 2: A Brief Trace of the Pronominalization Algorithm for Anaphoric Pronouns from STORYBOOK
narrative (discourse) plan into a multi-page fic-
tional narrative in the fairy tale domain. Using a
pipelined generation architecture, STORYBOOK per-
forms pronominalization before sentence planning,
and includes a revision component that is sensitive
to pronominalization choices during clause aggre-
gation. A previous large-scale evaluation of STORY-
BOOK (Callaway and Lester, 2001a) which included
both a full version and a version with the pronomi-
nalization component ablated showed that including
such a component significantly increases the quality
of the resulting prose.
However, there are significant practical obstacles
to comparing the performance of different pronomi-
nalization algorithms using corpus matching criteria
instead of ?quality? as evaluated by human judges.
Because systems that can handle a large quantity of
text are very recent and because it can require years
to create and organize the necessary knowledge to
produce even one multi-paragraph text, much re-
search on anaphora generation has instead relied on
one of two techniques:
 Checking algorithms by hand: One verification
method is to manually examine a text, identify-
ing candidates for pronominalization and simu-
lating the rules of a particular theory. However,
this method is prone to human error.
 Checking algorithms semiautomatically: Other
researchers opt instead to annotate a corpus
for pronominalization and their antecedents as
well as the pronoun forms that should occur,
and then simulate a pronominalization algo-
rithm on the marked-up text (Henschel et al,
2000). Similarly, this approach can suffer from
interannotator agreement errors (Poesio et al,
1999b).
To verify our pronominalization algorithm more
rigorously, we instead used the STORYBOOK deep
generation system to recreate pre-existing multi-
page texts with automatically selected pronouns.
McCoy & Strube Henschel et al STORYBOOK STORYBOOK
NYT News NYT News NYT News LRRH Narrative
Animate Anaphora 370/437 (84.7%) N/A 415/449 (92.4%) 170/174 (97.7%)
All Anaphora N/A 469/527 (89.0%) 441/475 (92.8%) 177/181 (97.8%)
Cataphora N/A N/A 1/2 (50.0%) 1/2 (50.0%)
Dialogue N/A N/A 46/46 (100.0%) 65/65 (100.0%)
Deixis N/A N/A 9/9 (100.0%) None present
Reflex./Recip. N/A N/A 5/6 (83.3%) 2/2 (100.0%)
Partitive N/A N/A 1/2 (50.0%) 1/1 (100.0%)
Table 1: Pronouns Correct by Algorithm/Text vs. Pronoun Type
Without a full-scale implementation, it is impossible
to determine whether an algorithm performs imper-
fectly due to human error, a lack of available corpus
data for making decisions, or if it is a fault with the
algorithm itself.
Using the algorithm described in Figure 1, we
modified STORYBOOK to substitute the types of
pronouns described in Section 3. We then created
the discourse plan and lexicon necessary to generate
the same three articles from the New York Times as
(McCoy and Strube, 1999). The results for both the
newspaper texts and the Little Red Riding Hood nar-
rative described in (Callaway and Lester, in press)
are shown in Table 1.
With the same three texts from the New York
Times, STORYBOOK performed better than the pre-
vious reported results of 85-90% described in (Mc-
Coy and Strube, 1999; Henschel et al, 2000) on both
animate and all anaphora using a corpus matching
technique. Furthermore, this was obtained solely by
adjusting the recency parameter to 4 (it was 3 in our
narrative domain), and without considering other en-
hancements such as gender/number constraints or
domain-specific alterations.1
7 Conclusions
Pronominalization is an important element in the au-
tomatic creation of multi-paragraph and multi-page
1It is important to note, however, that our counts of pronouns
and antecedents do not match theirs. This may stem from a vari-
ety of factors, such as including single instances of nominal de-
scriptions, whether dialogue pronouns were considered, and if
borderline quantifiers and words like ?everyone? were counted.
The generation community to-date has not settled on standard,
marked corpora for comparison purposes as has the rest of the
computational linguistics community.
texts. Previous approaches, based largely on theo-
retical approaches such as Centering Theory, deal
exclusively with anaphoric pronouns and have com-
plex processing and definitional requirements.
Given the full rhetorical structure available to an
implemented generation system, we devised a sim-
pler method of determining appropriate pronom-
inalizations which was more accurate than exist-
ing methods simulated by hand or performed semi-
automatically. This shows that approaches designed
for use with anaphora resolution, which must build
up discourse knowledge from scratch, may not be
the most desirable method for use in NLG, where
discourse knowledge already exists. The positive re-
sults from our simple counting algorithm, after only
minor changes in parameters from a narrative do-
main to that of newspaper text, indicates that future
high-quality prose generation systems are very near.
8 Acknowledgements
We would like to thank Michael Young and Renate
Henschel for their helpful comments; Kathy McCoy
very quickly provided the original 3 NYT articles
upon request; the anonymous reviewers whose com-
ments greatly improved this paper. Support for this
work was provided by ITC-irst and the IntelliMedia
Initiative of North Carolina State University.
References
Douglas E. Appelt. 1985. Planning English referring
expressions. Artificial Intelligence, 26:1?33.
Frederick Baldwin. 1995. CogNIAC: A Discourse Pro-
cessing Engine. Ph.D. thesis, The University of Penn-
sylvania, Philadelphia, PA.
Charles B. Callaway and James C. Lester. 2001a. Eval-
uating the effects of natural language generation on
reader satisfaction. In Proceedings of the Twenty-
Third Annual Conference of the Cognitive Science So-
ciety, pages 164?169, Edinburgh, UK.
Charles B. Callaway and James C. Lester. 2001b. Nar-
rative prose generation. In Proceedings of the Seven-
teenth International Joint Conference on Artificial In-
telligence, pages 1241?1248, Seattle, WA.
Charles B. Callaway and James C. Lester. 2003. Narra-
tive prose generation. Artificial Intelligence. In press.
Wim Claassen. 1992. Generating referring expressions
in a multimodal environment. In R. Dale, E. Hovy,
D. Rosner, and O. Stock, editors, Aspects of Auto-
mated Natural Language Generation, pages 247?62.
Springer-Verlag, Berlin.
Robert Dale. 1992. Generating Referring Expressions.
MIT Press.
Barbara Di Eugenio. 1998. Centering in Italian. In
Marilyn A. Walker, Aravind K. Joshi, and Ellen F.
Prince, editors, Centering in Discourse. Oxford Uni-
versity Press, Cambridge, MA.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2).
Barbara J. Grosz. 1977. The representation and use of
focus in a system for understanding dialogs. In Pro-
ceedings of the Fifth International Joint Conference on
Artificial Intelligence, pages 67?76, Cambridge, MA.
Peter Heeman and Graeme Hirst. 1986. Collaborating
on referring expressions. Computational Linguistics,
12(3):351?382.
Renate Henschel, Hua Cheng, and Massimo Poesio.
2000. Pronominalization revisited. In COLING?
2000: Proceedings of the 18th International Con-
ference on Computational Linguistics, Saarbruecken,
Germany.
Jerry R. Hobbs. 1976. Pronoun resolution. Technical
Report 76-1, Department of Computer Science, City
College, CUNY, New York, NY.
Roger Kibble and Richard Power. 2000. An inte-
grated framework for text planning and pronominali-
sation. In Proceedings of the First International Con-
ference on Natural Language Generation, pages 194?
200, Mitzpe Ramon, Israel.
Kathleen F. McCoy and Michael Strube. 1999. Taking
time to structure discourse: Pronoun generation be-
yond accessibility. In Proceedings of the Twenty-First
Conference of the Cognitive Science Society, pages
378?383, Vancouver, CA, August.
Elena Not. 1996. A computational model for generating
referring expressions in a multilingual application do-
main. In COLING?1996: Proceedings of the 16th In-
ternational Conference on Computational Linguistics,
Copenhagen, Denmark, August.
M. Poesio, H. Cheng, R. Henschel, J. Hitzeman, R. Kib-
ble, and R. Stevenson. 1999a. Specifying the parame-
ters of centering theory: A corpus-based evaluation us-
ing text from application-oriented domains. In Book-
title, page Pages, Address, Month.
M. Poesio, R. Henschel, J. Hitzeman, R. Kibble, S. Mon-
tague, and K. van Deemter. 1999b. Towards an anno-
tation scheme for noun phrase generation. In Bookti-
tle, page Pages, Address, Month.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman Publishers.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural-language generation systems. Journal of
Natural-Language Engineering, 3:57?87.
Candace L. Sidner. 1983. Focusing in the com-
prehension of definite anaphora. In M. Brady and
R. Berwick, editors, Computational Models of Dis-
course, pages 267?330. MIT Press, Cambridge, MA.
Marilyn A. Walker. 1998. Centering, anaphora resolu-
tion, and discourse structure. In Marilyn A. Walker,
Aravind K. Joshi, and Ellen F. Prince, editors, Center-
ing in Discourse. Oxford University Press, Cambridge,
MA.
Bonnie Webber. 1979. A Formal Approach to Discourse
Anaphora. Garland, NY.
Maria Wolters and Donna K. Byron. 2000. Prosody and
the resolution of pronominal anaphora. In COLING?
2000: Proceedings of the 18th International Con-
ference on Computational Linguistics, Saarbruecken,
Germany.
C. Yeh and C. Mellish. 1997. An empirical study on
the generation of anaphora in Chinese. Computational
Linguistics, 23(1):169?190.
Integrating Discourse Markers into a
Pipelined Natural Language Generation Architecture
Charles B. Callaway
ITC-irst, Trento, Italy
via Sommarive, 18
Povo (Trento), Italy, I-38050
callaway@itc.it
Abstract
Pipelined Natural Language Generation
(NLG) systems have grown increasingly
complex as architectural modules were
added to support language functionali-
ties such as referring expressions, lexical
choice, and revision. This has given rise to
discussions about the relative placement
of these new modules in the overall archi-
tecture. Recent work on another aspect
of multi-paragraph text, discourse mark-
ers, indicates it is time to consider where a
discourse marker insertion algorithm fits
in. We present examples which suggest
that in a pipelined NLG architecture, the
best approach is to strongly tie it to a revi-
sion component. Finally, we evaluate the
approach in a working multi-page system.
1 Introduction
Historically, work on NLG architecture has focused
on integrating major disparate architectural modules
such as discourse and sentence planners and sur-
face realizers. More recently, as it was discovered
that these components by themselves did not cre-
ate highly readable prose, new types of architectural
modules were introduced to deal with newly desired
linguistic phenomena such as referring expressions,
lexical choice, revision, and pronominalization.
Adding each new module typically entailed that
an NLG system designer would justify not only the
reason for including the new module (i.e., what lin-
guistic phenomena it produced that had been pre-
viously unattainable) but how it was integrated into
their architecture and why its placement was reason-
ably optimal (cf., (Elhadad et al, 1997), pp. 4?7).
At the same time, (Reiter, 1994) argued that im-
plemented NLG systems were converging toward
a de facto pipelined architecture (Figure 1) with
minimal-to-nonexistent feedback between modules.
Although several NLG architectures were pro-
posed in opposition to such a linear arrangement
(Kantrowitz and Bates, 1992; Cline, 1994), these re-
search projects have not continued while pipelined
architectures are still actively being pursued.
In addition, Reiter concludes that although com-
plete integration of architectural components is the-
oretically a good idea, in practical engineering terms
such a system would be too inefficient to operate and
too complex to actually implement. Significantly,
Reiter states that fully interconnecting every module
would entail constructing N(N   1) interfaces be-
tween them. As the number of modules rises (i.e., as
the number of large-scale features an NLG engineer
wants to implement rises) the implementation cost
rises exponentially. Moreover, this cost does not in-
clude modifications that are not component specific,
such as multilingualism.
As text planners scale up to produce ever larger
texts, the switch to multi-page prose will introduce
new features, and consequentially the number of
architectural modules will increase. For example,
Mooney?s EEG system (Mooney, 1994), which cre-
ated a full-page description of the Three-Mile Island
nuclear plant disaster, contains components for dis-
course knowledge, discourse organization, rhetori-
Figure 1: A Typical Pipelined NLG Architecture
cal relation structuring, sentence planning, and sur-
face realization. Similarly, the STORYBOOK system
(Callaway and Lester, 2002), which generated 2 to
3 pages of narrative prose in the Little Red Riding
Hood fairy tale domain, contained seven separate
components.
This paper examines the interactions of two lin-
guistic phenomena at the paragraph level: revision
(specifically, clause aggregation, migration and de-
motion) and discourse markers. Clause aggregation
involves the syntactic joining of two simple sen-
tences into a more complex sentence. Discourse
markers link two sentences semantically without
necessarily joining them syntactically. Because both
of these phenomena produce changes in the text
at the clause-level, a lack of coordination between
them can produce interference effects.
We thus hypothesize that the architectural mod-
ules corresponding to revision and discourse marker
selection should be tightly coupled. We then first
summarize current work in discourse markers and
revision, provide examples where these phenomena
interfere with each other, describe an implemented
technique for integrating the two, and report on a
preliminary system evaluation.
2 Discourse Markers in NLG
Discourse markers, or cue words, are single words
or small phrases which mark specific semantic rela-
tions between adjacent sentences or small groups of
sentences in a text. Typical examples include words
like however, next, and because. Discourse markers
pose a problem for both the parsing and generation
of clauses in a way similar to the problems that re-
ferring expressions pose to noun phrases: changing
the lexicalization of a discourse marker can change
the semantic interpretation of the clauses affected.
Recent work in the analysis of both the distribu-
tion and role of discourse markers has greatly ex-
tended our knowledge over even the most expansive
previous accounts of discourse connectives (Quirk
et al, 1985) from previous decades. For example,
using a large scale corpus analysis and human sub-
jects employing a substitution test over the corpus
sentences containing discourse markers, Knott and
Mellish (1996) distilled a taxonomy of individual
lexical discourse markers and 8 binary-valued fea-
tures that could be used to drive a discourse marker
selection algorithm.
Other work often focuses on particular semantic
categories, such as temporal discourse markers. For
instance, Grote (1998) attempted to create declar-
ative lexicons that contain applicability conditions
and other constraints to aid in the process of dis-
course marker selection. Other theoretical research
consists, for example, of adapting existing grammat-
ical formalisms such as TAGs (Webber and Joshi,
1998) for discourse-level phenomena.
Alternatively, there are several implemented sys-
tems that automatically insert discourse markers into
multi-sentential text. In an early instance, Elhadad
and McKeown (1990) followed Quirk?s pre-existing
non-computational account of discourse connectives
to produce single argumentative discourse markers
inside a functional unification surface realizer (and
thereby postponing lexicalization till the last possi-
ble moment).
More recent approaches have tended to move the
decision time for marker lexicalization higher up the
pipelined architecture. For example, the MOOSE
system (Stede and Umbach, 1998; Grote and Stede,
1999) lexicalized discourse markers at the sentence
planning level by pushing them directly into the
lexicon. Similarly, Power et al (1999) produce
multiple discourse markers for Patient Information
Leaflets using a constraint-based method applied to
RST trees during sentence planning.
Finally, in the CIRC-SIM intelligent tutoring sys-
tem (Yang et al, 2000) that generates connected di-
alogues for students studying heart ailments, dis-
course marker lexicalization has been pushed all the
way up to the discourse planning level. In this case,
CIRC-SIM lexicalizes discourse markers inside of
the discourse schema templates themselves.
Given that these different implemented discourse
marker insertion algorithms lexicalize their markers
at three distinct places in a pipelined NLG archi-
tecture, it is not clear if lexicalization can occur at
any point without restriction, or if it is in fact tied
to the particular architectural modules that a system
designer chooses to include.
The answer becomes clearer after noting that none
of the implemented discourse marker algorithms de-
scribed above have been incorporated into a com-
prehensive NLG architecture containing additional
significant components such as revision (with the
exception of MOOSE?s lexical choice component,
which Stede considers to be a submodule of the sen-
tence planner).
3 Current Implemented Revision Systems
Revision (or clause aggregation) is principally con-
cerned with taking sets of small, single-proposition
sentences and finding ways to combine them into
more fluent, multiple-proposition sentences. Sen-
tences can be combined using a wide range of differ-
ent syntactic forms, such as conjunction with ?and?,
making relative clauses with noun phrases common
to both sentences, and introducing ellipsis.
Typically, revision modules arise because of dis-
satisfaction with the quality of text produced by a
simple pipelined NLG system. As noted by Reape
and Mellish (1999), there is a wide variety in re-
vision definitions, objectives, operating level, and
type. Similarly, Dalianis and Hovy (1993) tried to
distinguish between different revision parameters by
having users perform revision thought experiments
and proposing rules in RST form which mimic the
behavior they observed.
While neither of these were implemented revi-
sion systems, there have been several attempts to im-
prove the quality of text from existing NLG systems.
There are two approaches to the architectural posi-
tion of revision systems: those that operate on se-
mantic representations before the sentence planning
level, of which a prototypical example is (Horacek,
2002), and those placed after the sentence planner,
operating on syntactic/linguistic data. Here we treat
mainly the second type, which have typically been
conceived of as ?add-on? components to existing
pipelined architectures. An important implication of
this architectural order is that the revision compo-
nents expect to receive lexicalized sentence plans.
Of these systems, Robin?s STREAK system
(Robin, 1994) is the only one that accepts both lex-
icalized and non-lexicalized data. After a sentence
planner produces the required lexicalized informa-
tion that can form a complete and grammatical sen-
tence, STREAK attempts to gradually aggregate that
data. It then proceeds to try to opportunistically in-
clude additional optional information from a data
set of statistics, performing aggregation operations
at various syntactic levels. Because STREAK only
produces single sentences, it does not attempt to add
discourse markers. In addition, there is no a priori
way to determine whether adjacent propositions in
the input will remain adjacent in the final sentence.
The REVISOR system (Callaway and Lester,
1997) takes an entire sentence plan at once and it-
erates through it in paragraph-sized chunks, em-
ploying clause- and phrase-level aggregation and re-
ordering operations before passing a revised sen-
tence plan to the surface realizer. However, at no
point does it add information that previously did not
exist in the sentence plan. The RTPI system (Har-
vey and Carberry, 1998) takes in sets of multiple,
lexicalized sentential plans over a number of medi-
cal diagnoses from different critiquing systems and
produces a single, unified sentence plan which is
both coherent and cohesive.
Like STREAK, Shaw?s CASPER system (Shaw,
1998) produces single sentences from sets of sen-
tences and doesn?t attempt to deal with discourse
markers. CASPER also delays lexicalization when
aggregating by looking at the lexicon twice during
the revision process. This is due mainly to the effi-
ciency costs of the unification procedure. However,
CASPER?s sentence planner essentially uses the first
lexicon lookup to find a ?set of lexicalizations? be-
fore eventually selecting a particular one.
An important similarity of these pipelined revi-
sion systems is that they all manipulate lexical-
ized representations at the clause level. Given that
both aggregation and reordering operators may sep-
arate clauses that were previously adjacent upon
leaving the sentence planner, the inclusion of a re-
vision component has important implications for
any upstream architectural module which assumed
that initially adjacent clauses would remain adjacent
throughout the generation process.
4 Architectural Implications
The current state of the art in NLG can be described
as small pipelined generation systems that incorpo-
rate some, but not all, of the available pipelined
NLG modules. Specifically, there is no system to-
date which both revises its output and inserts ap-
propriate discourse markers. Additionally, there are
no systems which utilize the latest theoretical work
in discourse markers described in Section 2. But
as NLG systems begin to reach toward multi-page
text, combining both modules into a single architec-
ture will quickly become a necessity if such systems
are to achieve the quality of prose that is routinely
achieved by human authors.
This integration will not come without con-
straints. For instance, discourse marker insertion al-
gorithms assume that sentence plans are static ob-
jects. Thus any change to the static nature of sen-
tence plans will inevitably disrupt them. On the
other hand, revision systems currently do not add in-
formation not specified by the discourse planner, and
do not perform true lexicalization: any new lexemes
not present in the sentence plan are merely delayed
lexicon entry lookups. Finally, because revision is
potentially destructive, the sentence elements that
lead to a particular discourse marker being chosen
may be significantly altered or may not even exist in
a post-revision sentence plan.
These factors lead to two partial order constraints
on a system that both inserts discourse markers and
revises at the clause level after sentence planning:
 Discourse marker lexicalization cannot pre-
cede revision
 Revision cannot precede discourse marker
lexicalization
In the first case, assume that a sentence plan ar-
rives at the revision module with discourse mark-
ers already lexicalized. Then the original discourse
marker may not be appropraite in the revised sen-
tence plan. For example, consider how the applica-
tion of the following revision types requires different
lexicalizations for the initial discourse markers:
 Clause Aggregation: The merging of two
main clauses into one main clause and one sub-
ordinate clause:
John had always liked to ride motorbikes. 
On account of this, his wife passionately hated
motorbikes. )
John had always liked to ride motorbikes,
which his wife f* on account of this j thusg
passionately hated.
 Reordering: Two originally adjacent main
clauses no longer have the same fixed position
relative to each other:
Diesel motors are well known for emitting ex-
cessive pollutants.  Furthermore, diesel is
often transported unsafely.  However, diesel
motors are becoming cleaner. )
Diesel motors are well known for emitting ex-
cessive pollutants, f* however j althoughg
they are becoming cleaner. Furthermore,
diesel is often transported unsafely.
 Clause Demotion: Two main clauses are
merged where one of them no longer has a
clause structure:
The happy man went home.  However, the
man was poor. )
The happy f* however j butg poor man went
home.
These examples show that if discourse marker
lexicalization occurs before clause revision, the
changes that the revision module makes can render
those discourse markers undesirable or even gram-
matically incorrect. Furthermore, these effects span
a wide range of potential revision types.
In the second case, assume that a sentence plan is
passed to the revision component, which performs
various revision operations before discourse mark-
ers are considered. In order to insert appropriate dis-
course markers, the insertion algorithm must access
the appropriate rhetorical structure produced by the
discourse planner. However, there is no guarantee
that the revision module has not altered the initial
organization imposed by the discourse planner. In
such a case, the underlying data used for discourse
marker selection may no longer be valid.
For example, consider the following generically
represented discourse plan:
C1: ?John and his friends went to the party.?
[temporal ?before? relation, time(C1, C2)]
C2: ?John and his friends gathered at the mall.?
[causal relation, cause(C2, C3)]
C3: ?John had been grounded.?
One possible revision that preserved the discourse
plan might be:
?Before John and his friends went to the party,
they gathered at the mall since he had been
grounded.?
In this case, the discourse marker algorithm has
selected ?before? and ?since? as lexicalized dis-
course markers prior to revision. But there are other
possible revisions that would destroy the ordering
established by the discourse plan and make the se-
lected discourse markers unwieldy:
?John, f* since j g who had been grounded,
gathered with his friends at the mall before go-
ing to the party.?
?f* Since j Becauseg he had been grounded,
John and his friends gathered at the mall and
f* before j theng went to the party.?
Reordering sentences without updating the dis-
course relations in the discourse plan itself would
result in many wrong or misplaced discourse marker
lexicalizations. Given that discourse markers can-
not be lexicalized before clause revision is enacted,
and that clause revision may alter the original dis-
course plan upon which a later discourse marker in-
sertion algorithm may rely, it follows that the revi-
sion algorithm should update the discourse plan as
it progresses, and the discourse marker insertion al-
gorithm should be responsive to these changes, thus
delaying discourse marker lexicalization.
5 Implementation
To demonstrate the application of this problem to
real world discourse, we took the STORYBOOK
(Callaway and Lester, 2001; Callaway and Lester,
2002) NLG system that generates multi-page text
in the form of Little Red Riding Hood stories and
New York Times articles, using a pipelined architec-
ture with a large number of modules such as revision
(Callaway and Lester, 1997). But although it was ca-
pable of inserting discourse markers, it did so in an
ad-hoc way, and required that the document author
notice possible interferences between revision and
discourse marker insertion and hard-wire the docu-
ment representation accordingly.
Upon adding a principled discourse marker selec-
tion algorithm to the system, we soon noticed vari-
ous unwanted interactions between revision and dis-
course markers of the type described in Section 4
above. Thus, in addition to the other constraints al-
ready considered during clause aggregation, we al-
tered the revision module to also take into account
the information available to our discourse marker in-
sertion algorithm (in our case, intention and rhetori-
cal predicates). We were thus able to incorporate the
discourse marker selection algorithm into the revi-
sion module itself.
This is contrary to most NLG systems where dis-
course marker lexicalization is performed as late as
possible using the modified discourse plan leaves af-
ter the revision rules have reorganized all the origi-
nal clauses. In an architecture that doesn?t consider
discourse markers, a generic revision rule without
access to the original discourse plan might appear
like this (where type refers to the main clause syn-
tax, and rhetorical type refers to its intention):
If type(clause1) = <type>
type(clause2) = <type>
subject(clause1) = subject(clause2)
then make-subject-relative-clause(clause1, clause2)
But by making available the intentional and
rhetorical information from the discourse plan, our
modified revision rules instead have this form:
If rhetorical-type(clause1) = <type>
rhetorical-type(clause2) = <type>
subject(clause1) = subject(clause2)
rhetorical-relation(clause1, clause2)  set-of-features
then make-subject-relative-clause(clause1, clause2)
lexicalize-discourse-marker(clause1, set-of-features)
update-rhetorical-relation(clause1, current-relations)
where the function lexicalize-discourse-marker de-
termines the appropriate discourse marker lexical-
ization given a set of features such as those de-
scribed in (Knott and Mellish, 1996) or (Grote and
Stede, 1999), and update-rhetorical-relation causes
the appropriate changes to be made to the running
discourse plan so that future revision rules can take
those alterations into account.
STORYBOOK takes a discourse plan augmented
with appropriate low-level (i.e., unlexicalized, or
conceptual) rhetorical features and produces a sen-
tence plan without discarding rhetorical informa-
tion. It then revises and lexicalizes discourse mark-
ers concurrently before passing the results to the sur-
face realization module for production of the surface
text.
Consider the following sentences in a short text
plan produced by the generation system:
1. ?In this case, Mr. Curtis could no longer be
tried for the shooting of his former girlfriend?s
companion.? <agent-action>
[causal relation]
2. ?There is a five-year statute of limitations on
that crime.? <existential>
[opposition relation]
3. ?There is no statute of limitations in murder
cases.? <existential>
Without revision, a discourse marker insertion al-
gorithm is only capable of adding discourse markers
before or after a clause boundary:
?In this case, Mr. Curtis could no longer be tried
for the shooting of his former girlfriend?s compan-
ion. This is because there is a five-year statute
of limitations on that crime. However, there is no
statute of limitations in murder cases.?
But a revised version with access to the discourse
plan and integrating discourse markers that our sys-
tem generates is:
?In this case, Mr. Curtis could no longer be tried
for the shooting of his former girlfriend?s compan-
ion, because there is a five-year statute of limita-
tions on that crime even though there is no statue of
limitations in murder cases.?
A revision module without access to the discourse
plan and a method for lexicalizing discourse mark-
ers will be unable to generate the second, improved
version. Furthermore, a discourse marker insertion
algorithm that lexicalizes before the revision algo-
rithm begins will not have enough basis to decide
and frequently produce wrong lexicalizations. The
actual implemented rules in our system (which gen-
erate the example above) are consistent with the ab-
stract rule presented earlier.
Revising sentence 1 with 2:
If rhetorical-type(clause1) = agent-action
rhetorical-type(clause2) = existential
rhetorical-relation(clause1, clause2)
 fcausation, simple, . . . g
then make-subordinate-bound-clause(clause2, clause1)
lexicalize-discourse-marker(clause2, fcausation, simpleg)
update-rhetorical-relation(clause1, clause2, agent-action,
existential, causation)
Revising sentence 2 with 3:
If rhetorical-type(clause2) = existential
rhetorical-type(clause3) = existential
rhetorical-relation(clause2, clause3)
 fopposition, simple, . . . g
then make-subject-relative-clause(clause2, clause3)
lexicalize-discourse-marker(clause1,
fopposition, simpleg)
update-rhetorical-relation(clause1, clause2, existential,
existential, current-relations)
Given these parameters, the discourse markers
will be lexicalized as because and even though
respectively, and the revision component will be
able to combine all three base sentences plus the
discourse markers into the single sentence shown
above.
6 Preliminary Evaluation
Evaluation of multi-paragraph text generation is ex-
ceedingly difficult, as empirically-driven methods
are not sufficiently sophisticated, and subjective hu-
man evaluations that require multiple comparisons
of large quantities of text is both difficult to control
for and time-consuming. Evaluating our approach is
even more difficult in that the interference between
discourse markers and revision is not a highly fre-
# Sentences # Revisions # DMs # Co-occurring DM/Rev Separate Integrated
Article 1 112 90 29 14 17 (56.8%) 26 (89.7%)
Article 2 54 93 50 30 24 (48.0%) 45 (90.0%)
Article 3 72 117 46 26 21 (45.7%) 42 (91.3%)
Table 1: Interactions between revision and discourse markers
quent occurrence in multi-page text. For instance, in
our corpora we found that these interference effects
occurred 23% of the time for revised clauses and
56% of the time with discourse markers. In other
words, almost one of every four clause revisions po-
tentially forces a change in discourse marker lexi-
calizations and one in every two discourse markers
occur near a clause revision boundary.
However, the ?penalty? associated with incor-
rectly selecting discourse markers is fairly high lead-
ing to confusing sentences, although there is no cog-
nitive science evidence that states exactly how high
for a typical reader, despite recent work in this direc-
tion (Tree and Schrock, 1999). Furthermore, there is
little agreement on exactly what constitutes a dis-
course marker, especially between the spoken and
written dialogue communities (e.g., many members
of the latter consider ?uh? to be a discourse marker).
We thus present an analysis of the frequencies
of various features from three separate New York
Times articles generated by the STORYBOOK sys-
tem. We then describe the results of running our
combined revision and discourse marker module
with the discourse plans used to generate them.
While three NYT articles is not a substantial enough
evaluation in ideal terms, the cost of evaluation in
such a knowledge-intensive undertaking will con-
tinue to be prohibitive until large-scale automatic or
semiautomatic techniques are developed.
The left side of table 1 presents an analysis of the
frequencies of revisions and discourse markers as
found in each of the three NYT articles. In addition,
we have indicated the number of times in our opin-
ion that revisions and discourse markers co-occurred
(i.e., a discourse marker was present at the junction
site of the clauses being aggregated).
The right side of the table indicates the differ-
ence between the accuracy of two different versions
of the system: separate signifies the initial configu-
ration of the STORYBOOK system where discourse
marker insertion and revision were performed as
separate process, while integrated signifies that dis-
course markers were lexicalized during revision as
described in this paper. The difference between
these two numbers thus represents the number of
times per article that the integrated clause aggrega-
tion and discourse marker module was able to im-
prove the resulting text.
7 Conclusion
Efficiency and software engineering considerations
dictate that current large-scale NLG systems must
be constructed in a pipeline fashion that minimizes
backtracking and communication between modules.
Yet discourse markers and revision both operate at
the clause level, which leads to the potential of inter-
ference effects if they are not resolved at the same lo-
cation in a pipelined architecture. We have analyzed
recent theoretical and applied work in both discourse
markers and revision, showing that although no pre-
vious NLG system has yet integrated both compo-
nents into a single architecture, an architecture for
multi-paragraph generation which separated the two
into distinct, unlinked modules would not be able
to guarantee that the final text contained appropri-
ately lexicalized discourse markers. Instead, our
combined revision and discourse marker module in
an implemented pipelined NLG system is able to
correctly insert appropriate discourse markers de-
spite changes made by the revision system. A cor-
pus analysis indicated that significant interference
effects between revision and discourse marker lex-
icalization are possible. Future work may show that
similar interference effects are possible as succes-
sive modules are added to pipelined NLG systems.
References
Charles B. Callaway and James C. Lester. 1997. Dy-
namically improving explanations: A revision-based
approach to explanation generation. In Proceedings of
the Fifteenth International Joint Conference on Artifi-
cial Intelligence, pages 952?58, Nagoya, Japan.
Charles B. Callaway and James C. Lester. 2001. Nar-
rative prose generation. In Proceedings of the Seven-
teenth International Joint Conference on Artificial In-
telligence, pages 1241?1248, Seattle, WA.
Charles B. Callaway and James C. Lester. 2002.
Narrative prose generation. Artificial Intelligence,
139(2):213?252.
Ben E. Cline. 1994. Knowledge Intensive Natural Lan-
guage Generation with Revision. Ph.D. thesis, Vir-
ginia Polytechnic and State University, Blacksburg,
Virginia.
Hercules Dalianis and Eduard Hovy. 1993. Aggrega-
tion in natural language generation. In Proceedings of
the Fourth European Workshop on Natural Language
Generation, Pisa, Italy.
Michael Elhadad and Kathy McKeown. 1990. Gener-
ating connectives. In COLING ?90: Proceedings of
the Thirteenth International Conference on Computa-
tional Linguistics, pages 97?101, Helsinki, Finland.
Michael Elhadad, Kathleen McKeown, and Jacques
Robin. 1997. Floating constraints in lexical choice.
Computational Linguistics, 23(2):195?240.
Brigitte Grote. 1998. Representing temporal discourse
markers for generation purposes. In Proceedings of
the Discourse Relations and Discourse Markers Work-
shop, pages 22?28, Montre?al, Canada.
Brigitte Grote and Manfred Stede. 1999. Ontology and
lexical semantics for generating temporal discourse
markers. In Proceedings of the 7th European Work-
shop on Natural Language Generation, Toulouse,
France, May.
Terrence Harvey and Sandra Carberry. 1998. Integrating
text plans for conciseness and coherence. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics, pages 512?518, August.
Helmut Horacek. 2002. Aggregation with strong regu-
larities and alternatives. In Second International Natu-
ral Language Generation Conference, pages 105?112,
Harriman, NY, July.
M. Kantrowitz and J. Bates. 1992. Integrated natural
language generation systems. In R. Dale, E. Hovy,
D. Rosner, and O. Stock, editors, Aspects of Auto-
mated Natural Language Generation, pages 247?262.
Springer-Verlag, Berlin.
Alistair Knott and Chris Mellish. 1996. A data-driven
method for classifying connective phrases. Journal of
Language and Speech, 39.
David J. Mooney. 1994. Generating High-Level Struc-
ture for Extended Explanations. Ph.D. thesis, The
University of Delaware, Newark, Delaware.
Richard Power, Christine Doran, and Donia Scott. 1999.
Generating embedded discourse markers from rhetor-
ical structure. In Proceedings of the Seventh Eu-
ropean Workshop on Natural Language Generation,
Toulouse, France.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman Publishers.
Mike Reape and Chris Mellish. 1999. Just what is ag-
gregation anyway? In Proceedings of the 7th Eu-
ropean Workshop on Natural Language Generation,
Toulouse, France, May.
Ehud Reiter. 1994. Has a consensus NL generation
architecture appeared, and is it psycholinguistically
plausible? In Proceedings of the Seventh International
Workshop on Natural Language Generation, pages
163?170, Kennebunkport, ME.
Jacques Robin. 1994. Revision-Based Generation of
Natural Language Summaries Providing Historical
Background. Ph.D. thesis, Columbia University, De-
cember.
James Shaw. 1998. Clause aggregation using linguistic
knowledge. In Proceedings of the 9th International
Workshop on Natural Language Generation, pages
138?147, Niagara-on-the-Lake, Canada.
Manfred Stede and Carla Umbach. 1998. DiM-Lex: A
lexicon of discourse markers for text generation and
understanding. In Proceedings of the Joint 36th Meet-
ing of the ACL and the 17th Meeting of COLING,
pages 1238?1242, Montre?al, Canada, August.
J. E. Fox Tree and J. C. Schrock. 1999. Discourse mark-
ers in spontaneous speech. Journal of Memory and
Language, 27:35?53.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
lexicalized tree-adjoining grammar for discourse. In
Proceedings of the COLING-ACL ?96 Discourse Rela-
tions and Discourse Markers Workshop, pages 86?92,
Montre?al, Canada, August.
Feng-Jen Yang, Jung Hee Kim, Michael Glass, and
Martha Evens. 2000. Lexical usage in the tutoring
schemata of Circsim-Tutor: Analysis of variable ref-
erences and discourse markers. In The Fifth Annual
Conference on Human Interaction and Complex Sys-
tems, pages 27?31, Urbana, IL.
Wide Coverage Symbolic Surface Realization
Charles B. Callaway
Istituto per la Ricerca Scientica e Tecnologica
Istituto Trentino di Cultura, Italy (ITC-irst)
callaway@itc.it
Abstract
Recent evaluation techniques applied to corpus-
based systems have been introduced that can
predict quantitatively how well surface realizers
will generate unseen sentences in isolation. We
introduce a similar method for determining the
coverage on the Fuf/Surge symbolic surface re-
alizer, report that its coverage and accuracy on
the Penn TreeBank is higher than that of a sim-
ilar statistics-based generator, describe several
benets that can be used in other areas of com-
putational linguistics, and present an updated
version of Surge for use in the NLG community.
1 Introduction
Surface realization is the process of converting
the semantic and syntactic representation of a
sentence or series of sentences into the text, or
surface form, of a particular language (Elhadad,
1991; Bateman, 1995). Most surface realiz-
ers have been symbolic, grammar-based systems
using syntactic linguistic theories like HPSG.
These systems were often developed as either
proof-of-concept implementations or to support
larger end-to-end NLG systems which have pro-
duced limited amounts of domain-specic texts.
As such, determining the generic coverage of
a language has been substituted by the goal of
producing the necessary syntactic coverage for a
particular project. As described in (Langkilde-
Geary, 2002), the result has been the use of
regression testing with hand-picked examples
rather than broad evaluations of linguistic com-
petence. Instead, large syntactically annotated
corpora such as the Penn TreeBank (Marcus et
al., 1993) have allowed statistically based sys-
tems to produce large quantities of sentences
and then more objectively determine generation
coverage with automatic evaluation measures.
We conducted a similar corpus-based exper-
iment (Callaway, 2003) with the Fuf/Surge
symbolic surface realizer (Elhadad, 1991). We
describe a direct comparison with HALogen
(Langkilde-Geary, 2002) using Section 23 of
the TreeBank, showing that the symbolic ap-
proach improves upon the statistical system in
both coverage and accuracy. We also present
a longitudinal comparison of two versions of
Fuf/Surge showing a signicant improvement
in its coverage and accuracy after new gram-
mar and morphology rules were added. This
improved version of Surge is available for use
in the NLG community.
2 Related Work in Wide Coverage
Generation
Verifying wide coverage generation depends on
(1) a large, well structured corpus, (2) a trans-
formation algorithm that converts annotated
sentences into the surface realizer's expected in-
put form, (3) the surface realizer itself, and (4)
an automatic metric for determining the accu-
racy of the generated sentences. Large, well
structured, syntactically marked corpora such
as the Penn TreeBank (Marcus et al, 1993) can
provide a source of example sentences, while au-
tomatic metrics like simple string accuracy are
capable of giving a fast, rough estimate of qual-
ity for individual sentences.
Realization of text from corpora has been ap-
proached in several ways. In the case of Rat-
naparkhi's generator for ight information in
the air travel domain (Ratnaparkhi, 2000), the
transformation algorithm is trivial as the gen-
erator uses the corpus itself (annotated with se-
mantic information such as destination or ight
number) as input to a surface realizer with an
n-gram model of the domain, along with a max-
imum entropy probability model for selecting
when to use which phrase.
Fergus (Bangalore and Rambow, 2000) used
the Penn TreeBank as a corpus, requiring
a more substantial transformation algorithm
since it requires a lexical predicate-argument
structure instead of the TreeBank's represen-
tation. The system uses an underlying tree-
(S (NP-SBJ ((cat clause)
(NP (JJ overall) (process ((type ascriptive) (tense past)))
(NNS sales))) (participants
(VP (VBD were) ((carrier ((cat common) (lex "sale") (number plural)
(ADJP-PRD (describer ((cat adj) (lex "overall")))))
(RB roughly) (attribute ((cat ap) (lex "flat")
(JJ flat)))) (modifier ((cat adv) (lex "roughly")))))))
Figure 1: A Penn TreeBank Sentence and Corresponding Surge Input Representation
based syntactic model to generate a set of pos-
sible candidate realizations, and then chooses
the best candidate with a trigram model of the
Treebank text. An evaluation of three versions
of Fergus on randomly chosen Wall Street
Journal sentences of the TreeBank showed sim-
ple string accuracy up to 58.9%.
Finally, Langkilde's work on HALogen
(Langkilde-Geary, 2002) uses a rewriting algo-
rithm to convert the syntactically annotated
sentences from the TreeBank into a semantic in-
put notation via rewrite rules. The system uses
the transformed semantic input to create mil-
lions of possible realizations (most of which are
grammatical but unwieldy) in a lattice struc-
ture and then also uses n-grams to select the
most probable as its output sentence. Langk-
ilde evaluated the system using the standard
train-and-test methodology with Section 23 of
the TreeBank as the unseen set.
These systems represent a statistical ap-
proach to wide coverage realization, turning to
automatic methods to evaluate coverage and
quality based on corpus statistics. However, a
symbolic realizer can use the same evaluation
technique if a method exists to transform the
corpus annotation into the realizer's input rep-
resentation. Thus symbolic realizers can also
use the same types of evaluations employed by
the parsing and MT communities, allowing for
meaningful comparisons of their performance on
metrics such as coverage and accuracy.
3 The Penn TreeBank
The Penn TreeBank (Marcus et al, 1993) is a
large set of sentences bracketed for syntactic de-
pendency and part of speech, covering almost 5
million words of text. The corpus is divided into
24 sections, with each section having on average
2000 sentences. The representation of an exam-
ple sentence is shown at the left of Figure 1.
In general, many sentences contained in the
TreeBank are not typical of those produced by
current NLG systems. For instance, newspaper
text requires extensive quoting for conveying di-
alogue, special formatting for stock reports, and
methods for dealing with contractions. These
types of constructions are not available in cur-
rent general purpose, rule-based generators:
 Direct and indirect quotations from re-
porters' interviews (Callaway, 2001):
\It's turning out to be a real blockbuster,"
Mr. Sweig said.
 Incomplete quotations:
Then retailers \will probably push them
out altogether," he says.
 Simple lists of facts from stock reports:
8 13/16% high, 8 1/2% low, 8 5/8% near
closing bid, 8 3/4% oered.
 Both formal and informal language:
You've either got a chair or you don't.
 A variety of punctuation mixed with text:
$55,730,000 of school nancing bonds,
1989 Series B (1987 resolution).
 Combinations of infrequent syntactic rules:
Then how should we think about service?
 Irregular and rare words:
\I was upset with Roger, I fumpered and
schmumpered," says Mr. Peters.
By adding rules for these phenomena, NLG
realizers can signicantly increase their cover-
age. For instance, approximately 15% of Penn
TreeBank sentences contain either direct, indi-
rect or incomplete written dialogue. Thus for a
newspaper domain, excluding dialogue from the
grammar greatly limits potential coverage. Fur-
thermore, using a corpus for testing a surface
realizer is akin to having a very large regression
test set, with the added benet of being able to
robustly generate real-world sentences.
In order to compare a symbolic surface real-
izer with its statistical counterparts, we tested
an enhanced version of an o-the-shelf symbolic
generation system, the Fuf/Surge (Elhadad,
1991) surface realizer. To obtain a meaningful
comparison, we utilized the same approach as
Realizer Sentences Coverage Matches Covered Matches Total Matches Accuracy
Surge 2.2 2416 48.1% 102 8.8% 4.2% 0.8542
Surge+ 2416 98.9% 1474 61.7% 61.0% 0.9483
Halogen 2416 83.3% 1157 57.5% 47.9% 0.9450
Table 1: Comparing two Surge versions with HALogen [Langkilde 2002].
HALogen, treating Section 23 of the Treebank
as an unseen test set. We created an analo-
gous transformation algorithm (Callaway, 2003)
to convert TreeBank sentences into the Surge
representation (Figure 1), which are then given
to the symbolic surface realizer, allowing us to
measure both coverage and accuracy.
4 Coverage and Accuracy Evaluation
Of the three statistical systems presented above,
only (Langkilde-Geary, 2002) used a standard,
recoverable method for replicating the gener-
ation experiment. Because of the sheer num-
ber of sentences (2416), and to enable a direct
comparison with HALogen, we similarly used
the simple string accuracy (Doddington, 2002),
where the smallest number of Adds, Deletions,
and Insertions were used to calculate accuracy:
1 - (A + D + I) / #Characters.
Unlike typical statistical and machine learn-
ing experiments, the grammar was \trained" by
hand, though the evaluation of the resulting
sentences was performed automatically. This
resulted in numerous generalized syntactic and
morphology rules being added to the Surge
grammar, as well as specialized rules pertain-
ing to specic domain elements from the texts.
Table 1 shows a comparative coverage and
accuracy analysis of three surface realizers on
Section 23 of the Penn TreeBank: the original
Surge 2.2 distribution, our modied version of
Surge, and the HALogen system described in
(Langkilde-Geary, 2002). The surface realizers
are measured in terms of:
 Coverage: The number of sentences for
which the realizer returned a recognizable
string rather than failure or an error.
 Matches: The number of identical sen-
tences (including punctuation/capitals).
 Percent of covered matches: How often the
realizer returned a sentence match given
that a sentence is produced.
 Percent of matches for all sentences: A
measure of matches from all inputs, which
penalizes systems that improve accuracy
at the expense of coverage (Matches /
2416, or Coverage * Covered Matches).
 Accuracy : The aggregate simple string ac-
curacy score for all covered sentences (as
opposed to the entire sentence set).
The rst thing to note is the drastic improve-
ment between the two versions of Surge. As
the analysis in Section 3 showed, studying the
elements of a particular domain are very impor-
tant in determining what parts of a grammar
should be improved. For instance, the TreeBank
contains many constructions which are not han-
dled by Surge 2.2, such as quotations, which
account for 15% of the sentences. When Surge
2.2 encounters a quotation, it fails to produce a
text string, accounting for a large chunk of the
sentences not covered (51.9% compared to 1.1%
for our enhanced version of Surge).
Additionally, a number of morphology en-
hancements, such as contractions and punctua-
tion placement contributed to the much higher
percentage of exact matches. While some of
these are domain-specic, many are broader
generalizations which although useful, were not
included in the original grammar because they
were not encountered in previous domains or
arose only in complex sentences.
On all four measures the enhanced version of
Surge performed much better than the statisti-
cal approach to surface realization embodied in
HALogen. The accuracy measure is especially
surprising given that statistical and machine
learning approaches employ maximization algo-
rithms to ensure that grammar rules are chosen
to get the highest possible accuracy. However,
given that the dierence in accuracy from Surge
2.2 is relatively small while its quality is obvi-
ously poor, using such accuracy measures alone
is a bad way to compare surface realizers.
Finally, the coverage dierence between the
enhanced version of Surge and that of HALo-
gen is especially striking. Some explanations
may be that statistical systems are not yet capa-
ble of handling certain linguistic phenomena like
long-distance dependencies (due to n-gram ap-
proaches), or given that statistical systems are
typically robust and very unlikely to produce no
output, that there were problems in the trans-
formation algorithm that converted individual
sentence representations from the corpus.
5 Additional Benets
The evaluation approach presented here has
other advantages besides calculating the cover-
age and accuracy of a grammar. For instance,
in realizers where linguists must add new lexical
resources by hand, such a system allows them
to generate text by rst creating sample sen-
tences in the more familiar TreeBank notation.
Sentences could also be directly generated by
feeding an example text to a parser capable of
producing TreeBank structures. This would be
especially useful in new domains to quickly see
what new specialized syntax they might need.
Additionally, the transformation program can
be used as an error-checker to assist in anno-
tating sentences in a new corpus. Rules could
be (and have been) added alongside the normal
transformation rules that detect when errors are
encountered, categorize them, and make them
available to the corpus creator for correction.
This can extend beyond the syntax level, de-
tecting even morphology errors such as incorrect
verbs, typos, or dialect dierences.
Finally, such an approach can help test pars-
ing systems without the need for the time-
consuming process of annotating corpora in the
rst place. If a parser creates a TreeBank repre-
sentation for a sentence, the generation system
can then attempt to regenerate that same sen-
tence automatically. Exact matches are highly
likely to have been correctly parsed, and more
time can be spent locating and resolving parses
that returned very low accuracy scores.
6 Conclusions and Future Work
Recent statistical systems for generation have
focused on surface realizers, oering robust-
ness, wide coverage, and domain- and language-
independence given certain resources. This pa-
per represents the analogous eort for a sym-
bolic generation system using an enhanced ver-
sion of the Fuf/Surge systemic realizer. We
presented a grammatical coverage and accu-
racy experiment showing the symbolic system
had a much higher level of coverage of English
and better accuracy as represented by the Penn
TreeBank. The improved Surge grammar, ver-
sion 2.4, will be made freely available to the
NLG community.
While we feel that both coverage and accu-
racy could be improved even more, additional
gains would not imply a substantial improve-
ment in the quality of the grammar itself. The
reason is that most problems aecting accuracy
lie in transforming the TreeBank representation
as opposed to the grammar, which has remained
relatively stable.
References
S. Bangalore and O. Rambow. 2000. Exploiting
a probabilistic hierarchical model for genera-
tion. In COLING{2000: Proceedings of the
18th International Conference on Computa-
tional Linguistics, Saarbruecken, Germany.
John A. Bateman. 1995. KPML: The KOMET-
penman (multilingual) development environ-
ment. Technical Report Release 0.8, Insti-
tut fur Integrierte Publikations- und Informa-
tionssysteme (IPSI), GMD, Darmstadt.
Charles Callaway. 2001. A computational fea-
ture analysis for multilingual character-to-
character dialogue. In Proceedings of the Sec-
ond International Conference on Intelligent
Text Processing and Computational Linguis-
tics, pages 251{264, Mexico City, Mexico.
Charles B. Callaway. 2003. Evaluating coverage
for large symbolic NLG grammars. In Pro-
ceedings of the Eighteenth International Joint
Conference on Articial Intelligence, pages
811{817, Acapulco, Mexico, August.
George Doddington. 2002. Automatic evalua-
tion of machine translation quality using n-
gram co-occurrence statistics. In Proceedings
of the 2002 Conference on Human Language
Technology, San Diego, CA, March.
Michael Elhadad. 1991. FUF: The universal
unier user manual version 5.0. Technical Re-
port CUCS-038-91, Dept. of Computer Sci-
ence, Columbia University.
Irene Langkilde-Geary. 2002. An empirical ver-
ication of coverage and correctness for a
general-purpose sentence generator. In Sec-
ond International Natural Language Genera-
tion Conference, Harriman, NY, July.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of
English: The PennTreeBank. Computational
Linguistics, 26(2).
Adwait Ratnaparkhi. 2000. Trainable meth-
ods for surface natural language generation.
In Proceedings of the First North American
Conference of the ACL, Seattle, WA, May.
Proceedings of the ACL 2010 System Demonstrations, pages 13?18,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
BEETLE II: a system for tutoring and computational linguistics
experimentation
Myroslava O. Dzikovska and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{gwendolyn.campbell,natalie.steihauser}@navy.mil
Elaine Farrow
Heriot-Watt University
Edinburgh, United Kingdom
e.farrow@hw.ac.uk
Charles B. Callaway
University of Haifa
Mount Carmel, Haifa, Israel
ccallawa@gmail.com
Abstract
We present BEETLE II, a tutorial dia-
logue system designed to accept unre-
stricted language input and support exper-
imentation with different tutorial planning
and dialogue strategies. Our first system
evaluation used two different tutorial poli-
cies and demonstrated that the system can
be successfully used to study the impact
of different approaches to tutoring. In the
future, the system can also be used to ex-
periment with a variety of natural language
interpretation and generation techniques.
1 Introduction
Over the last decade there has been a lot of inter-
est in developing tutorial dialogue systems that un-
derstand student explanations (Jordan et al, 2006;
Graesser et al, 1999; Aleven et al, 2001; Buckley
and Wolska, 2007; Nielsen et al, 2008; VanLehn
et al, 2007), because high percentages of self-
explanation and student contentful talk are known
to be correlated with better learning in human-
human tutoring (Chi et al, 1994; Litman et al,
2009; Purandare and Litman, 2008; Steinhauser et
al., 2007). However, most existing systems use
pre-authored tutor responses for addressing stu-
dent errors. The advantage of this approach is that
tutors can devise remediation dialogues that are
highly tailored to specific misconceptions many
students share, providing step-by-step scaffolding
and potentially suggesting additional problems.
The disadvantage is a lack of adaptivity and gen-
erality: students often get the same remediation
for the same error regardless of their past perfor-
mance or dialogue context, as it is infeasible to
author a different remediation dialogue for every
possible dialogue state. It also becomes more dif-
ficult to experiment with different tutorial policies
within the system due to the inherent completixites
in applying tutoring strategies consistently across
a large number of individual hand-authored reme-
diations.
The BEETLE II system architecture is designed
to overcome these limitations (Callaway et al,
2007). It uses a deep parser and generator, to-
gether with a domain reasoner and a diagnoser,
to produce detailed analyses of student utterances
and generate feedback automatically. This allows
the system to consistently apply the same tutorial
policy across a range of questions. To some extent,
this comes at the expense of being able to address
individual student misconceptions. However, the
system?s modular setup and extensibility make it
a suitable testbed for both computational linguis-
tics algorithms and more general questions about
theories of learning.
A distinguishing feature of the system is that it
is based on an introductory electricity and elec-
tronics course developed by experienced instruc-
tional designers. The course was first created for
use in a human-human tutoring study, without tak-
ing into account possible limitations of computer
tutoring. The exercises were then transferred into
a computer system with only minor adjustments
(e.g., breaking down compound questions into in-
dividual questions). This resulted in a realistic tu-
toring setup, which presents interesting challenges
to language processing components, involving a
wide variety of language phenomena.
We demonstrate a version of the system that
has undergone a successful user evaluation in
13
2009. The evaluation results indicate that addi-
tional improvements to remediation strategies, and
especially to strategies dealing with interpretation
problems, are necessary for effective tutoring. At
the same time, the successful large-scale evalua-
tion shows that BEETLE II can be used as a plat-
form for future experimentation.
The rest of this paper discusses the BEETLE II
system architecture (Section 2), system evaluation
(Section 3), and the range of computational lin-
guistics problems that can be investigated using
BEETLE II (Section 4).
2 System Architecture
The BEETLE II system delivers basic electricity
and electronics tutoring to students with no prior
knowledge of the subject. A screenshot of the sys-
tem is shown in Figure 1. The student interface in-
cludes an area to display reading material, a circuit
simulator, and a dialogue history window. All in-
teractions with the system are typed. Students read
pre-authored curriculum slides and carry out exer-
cises which involve experimenting with the circuit
simulator and explaining the observed behavior.
The system also asks some high-level questions,
such as ?What is voltage??.
The system architecture is shown in Figure 2.
The system uses a standard interpretation pipeline,
with domain-independent parsing and generation
components supported by domain specific reason-
ers for decision making. The architecture is dis-
cussed in detail in the rest of this section.
2.1 Interpretation Components
We use the TRIPS dialogue parser (Allen et al,
2007) to parse the utterances. The parser provides
a domain-independent semantic representation in-
cluding high-level word senses and semantic role
labels. The contextual interpreter then uses a refer-
ence resolution approach similar to Byron (2002),
and an ontology mapping mechanism (Dzikovska
et al, 2008a) to produce a domain-specific seman-
tic representation of the student?s output. Utter-
ance content is represented as a set of extracted
objects and relations between them. Negation is
supported, together with a heuristic scoping algo-
rithm. The interpreter also performs basic ellipsis
resolution. For example, it can determine that in
the answer to the question ?Which bulbs will be
on and which bulbs will be off in this diagram??,
?off? can be taken to mean ?all bulbs in the di-
agram will be off.? The resulting output is then
passed on to the domain reasoning and diagnosis
components.
2.2 Domain Reasoning and Diagnosis
The system uses a knowledge base implemented in
the KM representation language (Clark and Porter,
1999; Dzikovska et al, 2006) to represent the state
of the world. At present, the knowledge base rep-
resents 14 object types and supports the curricu-
lum containing over 200 questions and 40 differ-
ent circuits.
Student explanations are checked on two levels,
verifying factual and explanation correctness. For
example, for a question ?Why is bulb A lit??, if
the student says ?it is in a closed path?, the system
checks two things: a) is the bulb indeed in a closed
path? and b) is being in a closed path a reason-
able explanation for the bulb being lit? Different
remediation strategies need to be used depending
on whether the student made a factual error (i.e.,
they misread the diagram and the bulb is not in a
closed path) or produced an incorrect explanation
(i.e., the bulb is indeed in a closed path, but they
failed to mention that a battery needs to be in the
same closed path for the bulb to light).
The knowledge base is used to check the fac-
tual correctness of the answers first, and then a di-
agnoser checks the explanation correctness. The
diagnoser, based on Dzikovska et al (2008b), out-
puts a diagnosis which consists of lists of correct,
contradictory and non-mentioned objects and re-
lations from the student?s answer. At present, the
system uses a heuristic matching algorithm to clas-
sify relations into the appropriate category, though
in the future we may consider a classifier similar
to Nielsen et al (2008).
2.3 Tutorial Planner
The tutorial planner implements a set of generic
tutoring strategies, as well as a policy to choose
an appropriate strategy at each point of the inter-
action. It is designed so that different policies can
be defined for the system. The currently imple-
mented strategies are: acknowledging the correct
part of the answer; suggesting a slide to read with
background material; prompting for missing parts
of the answer; hinting (low- and high- specificity);
and giving away the answer. Two or more strate-
gies can be used together if necessary.
The hint selection mechanism generates hints
automatically. For a low specificity hint it selects
14
Figure 1: Screenshot of the BEETLE II system
Dialogue ManagerParserContextualInterpreter
Interpretation
CurriculumPlanner
KnowledgeBase
Content Planner & Generator
TutorialPlanner
Tutoring
GUI
Diagnoser
Figure 2: System architecture diagram
15
an as-yet unmentioned object and hints at it, for
example, ?Here?s a hint: Your answer should men-
tion a battery.? For high-specificity, it attempts to
hint at a two-place relation, for example, ?Here?s
a hint: the battery is connected to something.?
The tutorial policy makes a high-level decision
as to which strategy to use (for example, ?ac-
knowledge the correct part and give a high speci-
ficity hint?) based on the answer analysis and di-
alogue context. At present, the system takes into
consideration the number of incorrect answers re-
ceived in response to the current question and the
number of uninterpretable answers.1
In addition to a remediation policy, the tuto-
rial planner implements an error recovery policy
(Dzikovska et al, 2009). Since the system ac-
cepts unrestricted input, interpretation errors are
unavoidable. Our recovery policy is modeled on
the TargetedHelp (Hockey et al, 2003) policy used
in task-oriented dialogue. If the system cannot
find an interpretation for an utterance, it attempts
to produce a message that describes the problem
but without giving away the answer, for example,
?I?m sorry, I?m having a problem understanding. I
don?t know the word power.? The help message is
accompanied with a hint at the appropriate level,
also depending on the number of previous incor-
rect and non-interpretable answers.
2.4 Generation
The strategy decision made by the tutorial plan-
ner, together with relevant semantic content from
the student?s answer (e.g., part of the answer to
confirm), is passed to content planning and gen-
eration. The system uses a domain-specific con-
tent planner to produce input to the surface realizer
based on the strategy decision, and a FUF/SURGE
(Elhadad and Robin, 1992) generation system to
produce the appropriate text. Templates are used
to generate some stock phrases such as ?When you
are ready, go on to the next slide.?
2.5 Dialogue Management
Interaction between components is coordinated by
the dialogue manager which uses the information-
state approach (Larsson and Traum, 2000). The
dialogue state is represented by a cumulative an-
swer analysis which tracks, over multiple turns,
the correct, incorrect, and not-yet-mentioned parts
1Other factors such as student confidence could be con-
sidered as well (Callaway et al, 2007).
of the answer. Once the complete answer has been
accumulated, the system accepts it and moves on.
Tutor hints can contribute parts of the answer to
the cumulative state as well, allowing the system
to jointly construct the solution with the student.
3 Evaluation
The first experimental evaluation involving 81 par-
ticipants (undergraduates recruited from a South-
eastern University in the USA) was completed in
2009. Participants had little or no prior knowledge
of the domain. Each participant took a pre-test,
worked through a lesson with the system, took a
post-test, and completed a user satisfaction survey.
Each session lasted approximately 4 hours.
We implemented two different tutoring policies
in the system for this evaluation. The baseline
policy used an ?accept and bottom out? strategy
for all student answers, regardless of their con-
tent. The students were always given the correct
answer, but the system made no attempt at reme-
diation, and never indicated whether the student
was understood (or correct). In comparison, the
full adaptive policy attempted to select an appro-
priate strategy based on student answer analysis
and dialogue context, as described above. Stu-
dents were randomly assigned to a condition. On
average, each student session contained 232 stu-
dent language turns in the fully adaptive condition
(SD = 25.6) and 156 turns in the baseline condi-
tion (SD = 2.02). Additional time was taken by
reading and interacting with the simulation envi-
ronment.
Out of 81 students, 76 successfully completed
the evaluation (data from 5 participants had to be
discarded due to system crashes). There was no
significant difference in learning gain between the
two conditions (baseline: M = 0.63, SD = 0.26,
full: M = 0.57, SD = 0.23, t-test p = 0.32),
and students gave higher satisfaction scores to the
baseline condition (baseline: M = 3.32, SD =
0.65, full: M = 2.56, SD = 0.65, t-test, p <
0.01). We are currently analyzing the data to in-
vestigate the reasons for this outcome. However,
the evaluation demonstrated that the system can be
successfully deployed in experiments with naive
users and therefore can be used to study the im-
pact of different natural language processing algo-
rithms on learning gain and user satisfaction.
An example dialogue with the system, taken
from the evaluation corpus, is shown in Figure 3.
16
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I?m sorry, I?m having a problem un-
derstanding. I don?t understand the
word ?effect? here. Here?s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
Tutor: You?re on the right track. Here?s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 3: Example interaction with the system
from our corpus
It shows three key system properties: after the stu-
dent?s first turn, the system rephrases its under-
standing of the correct part of the student answer
and prompts the student to supply the missing in-
formation. In the second turn, the student utter-
ance could not be interpreted and the system re-
sponds with a targeted help message and a hint
about the object that needs to be mentioned. Fi-
nally, in the last turn the system combines the in-
formation from the tutor?s hint and the student?s
answers and restates the complete answer since the
current answer was completed over multiple turns.
4 Conclusions and Future Work
The BEETLE II system we present was built to
serve as a platform for research in computational
linguistics and tutoring, and can be used for task-
based evaluation of algorithms developed for other
domains. We are currently developing an annota-
tion scheme for the data we collected to identify
student paraphrases of correct answers. The an-
notated data will be used to evaluate the accuracy
of existing paraphrasing and textual entailment ap-
proaches and to investigate how to combine such
algorithms with the current deep linguistic analy-
sis to improve system robustness. We also plan
to annotate the data we collected for evidence of
misunderstandings, i.e., situations where the sys-
tem arrived at an incorrect interpretation of a stu-
dent utterance and took action on it. Such annota-
tion can provide useful input for statistical learn-
ing algorithms to detect and recover from misun-
derstandings.
In dialogue management and generation, the
key issue we are planning to investigate is that of
linguistic alignment. The analysis of the data we
have collected indicates that student satisfaction
may be affected if the system rephrases student
answers using different words (for example, using
better terminology) but doesn?t explicitly explain
the reason why different terminology is needed
(Dzikovska et al, 2010). Results from other sys-
tems show that measures of semantic coherence
between a student and a system were positively as-
sociated with higher learning gain (Ward and Lit-
man, 2006). Using a deep generator to automati-
cally generate system feedback gives us a level of
control over the output and will allow us to devise
experiments to study those issues in more detail.
From the point of view of tutoring research,
we are planning to use the system to answer
questions about the effectiveness of different ap-
proaches to tutoring, and the differences between
human-human and human-computer tutoring. Pre-
vious comparisons of human-human and human-
computer dialogue were limited to systems that
asked short-answer questions (Litman et al, 2006;
Rose? and Torrey, 2005). Having a system that al-
lows more unrestricted language input will pro-
vide a more balanced comparison. We are also
planning experiments that will allow us to eval-
uate the effectiveness of individual strategies im-
plemented in the system by comparing system ver-
sions using different tutoring policies.
Acknowledgments
This work has been supported in part by US Office
of Naval Research grants N000140810043 and
N0001410WX20278. We thank Katherine Harri-
son and Leanne Taylor for their help running the
evaluation.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
James Allen, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL-07 Workshop on Deep Linguistic Processing.
17
Mark Buckley and Magdalena Wolska. 2007. To-
wards modelling and using common ground in tu-
torial dialogue. In Proceedings of DECALOG, the
2007 Workshop on the Semantics and Pragmatics of
Dialogue, pages 41?48.
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway, Myroslava Dzikovska, Elaine
Farrow, Manuel Marques-Pita, Colin Matheson, and
Johanna D. Moore. 2007. The Beetle and BeeD-
iff tutoring systems. In Proceedings of SLaTE?07
(Speech and Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting
self-explanations improves understanding. Cogni-
tive Science, 18(3):439?477.
Peter Clark and Bruce Porter, 1999. KM (1.4): Users
Manual. http://www.cs.utexas.edu/users/mfkb/km.
Myroslava O. Dzikovska, Charles B. Callaway, and
Elaine Farrow. 2006. Interpretation and generation
in a knowledge-based tutorial system. In Proceed-
ings of EACL-06 workshop on knowledge and rea-
soning for language processing, Trento, Italy, April.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2008a. Linking semantic and knowledge
representations in a multi-domain dialogue system.
Journal of Logic and Computation, 18(3):405?430.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008b. Diagnosing natural lan-
guage answers to support adaptive tutoring. In
Proceedings 21st International FLAIRS Conference,
Coconut Grove, Florida, May.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn C. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of SIGDIAL-09, London, UK, Sep.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010. The
impact of interpretation problems on tutorial dia-
logue. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics(ACL-
2010).
Michael Elhadad and Jacques Robin. 1992. Control-
ling content realization with functional unification
grammars. In R. Dale, E. Hovy, D. Ro?sner, and
O. Stock, editors, Proceedings of the Sixth Interna-
tional Workshop on Natural Language Generation,
pages 89?104, Berlin, April. Springer-Verlag.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simula-
tion of a human tutor. Cognitive Systems Research,
1:35?51.
Beth Ann Hockey, Oliver Lemon, Ellen Campana,
Laura Hiatt, Gregory Aist, James Hieronymus,
Alexander Gruenstein, and John Dowding. 2003.
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics, pages 147?154, Morristown, NJ, USA.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323?340.
Diane Litman, Carolyn P. Rose?, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Sil-
liman. 2006. Spoken versus typed human and com-
puter dialogue tutoring. International Journal of Ar-
tificial Intelligence in Education, 16:145?170.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Generalizing tutorial dia-
logue results. In Proceedings of 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
C.P. Rose? and C. Torrey. 2005. Interactivity versus ex-
pectation: Eliciting learning oriented behavior with
tutorial dialogue systems. In Proceedings of Inter-
act?05.
N. B. Steinhauser, L. A. Butler, and G. E. Campbell.
2007. Simulated tutors in immersive learning envi-
ronments: Empirically-derived design principles. In
Proceedings of the 2007 Interservice/Industry Train-
ing, Simulation and Education Conference, Orlando,
FL.
Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007. Developing pedagogically effective tutorial
dialogue tactics: Experiments and a testbed. In Pro-
ceedings of SLaTE Workshop on Speech and Lan-
guage Technology in Education, Farmington, PA,
October.
Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of 19th International FLAIRS (Florida Ar-
tificial Intelligence Research Society) Conference,
Melbourne Beach, FL.
18
The TEXTCAP Semantic
Interpreter
Charles B. Callaway
University of Edinburgh (UK)
email: ccallawa@inf.ed.ac.uk
Abstract
The lack of large amounts of readily available, explicitly represented
knowledge has long been recognized as a barrier to applications requir-
ing semantic knowledge such as machine translation and question an-
swering. This problem is analogous to that facing machine translation
decades ago, where one proposed solution was to use human translators
to post-edit automatically produced, low quality translations rather than
expect a computer to independently create high-quality translations. This
paper describes an attempt at implementing a semantic parser that takes
unrestricted English text, uses publically available computational linguis-
tics tools and lexical resources and as output produces semantic triples
which can be used in a variety of tasks such as generating knowledge
bases, providing raw material for question answering systems, or creating
RDF structures. We describe the TEXTCAP system, detail the semantic
triple representation it produces, illustrate step by step how TEXTCAP
processes a short text, and use its results on unseen texts to discuss the
amount of post-editing that might be realistically required.
327
328 Callaway
1 Introduction
A number of applications depend on explicitly represented knowledge to perform ba-
sic tasks or add customization to existing tasks. Improving the quantity and quality of
the knowledge contained in knowledge bases could lead to the improved performance
of many applications that depend on knowledge and inference such as:
? Generating scientific or educational explanations of natural or mechanical sys-
tems and phenomena (Lester and Porter, 1997),
? Question answering systems (Clark et al, 2001) that use reasoning to solve
problems rather than looking up answers,
? Multimodal information presentation systems that depend on specific real world
knowledge in order to describe or refer to it for audiences (Callaway et al, 2005;
Stock et al, 2007).
These systems have typically relied on hand-built and domain specific knowledge
bases requiring years of effort to produce. The need to speed up this process as well
as make the resulting representations more consistent are well-known problems that
have yielded a number of potential solutions (Blythe et al, 2001; Reiter et al, 2003;
Carenini et al, 2005; Barker et al, 2007), but large scale, domain independent, and
fully automatic knowledge acquisition on unrestricted text is still in its infancy.
Over the last decade research in applied computational linguistics has extended the
various components necessary for semantic parsing, but have tended to focus on in-
creasing the measurable performance of individual subtask in isolation (e.g., parsing,
anaphora resolution, semantic role labelling, and word sense disambiguation) rather
than on an entire end-to-end system. Meanwhile, theoretical CL research has exam-
ined issues such as underspecification, scoping and reference resolution in discourse
contexts, but has set aside issues such as large-scale robustness, ontology integration
and evaluation which are vital for applied uses of semantic parsing.
In this paper we discuss an implementation to automatically extract explicitly coded
conceptual and ontological knowledge from unrestricted text using a pipeline of NLP
components, as part of the STEP shared task (Bos, 2008). The TEXTCAP system per-
forms the basic steps towards this task by gluing together an off-the-shelf parser with
semantic interpretation methods. It is intended to be a test case for (1) establishing
baseline performance measures for semantic parsing and (2) determining what degree
of post-editing might be necessary in real-world environments.
Because major components of such a system would not be tailored towards the se-
mantic parsing task, we would rightly expect its output to be imperfect. This problem
is analogous to that facing machine translation decades ago, where one proposed so-
lution was to use human translators to post-edit automatically produced, low quality
translations rather than expect a computer to independently create high-quality trans-
lations. One aspect of this research is thus to investigate howmuch post-editing would
be required to convert the system?s output to usable semantic triples.
Finally, this paper presents the results of TEXTCAP on the 2008 STEP shared task
corpus, giving specific comments about the difficulties in encountered. Although not
a formal evaluation, we were satisfied with its performance in terms of accuracy and
efficiency for helping humans post-edit semantic triples.
The TEXTCAP Semantic Interpreter 329
2 System Description
TEXTCAP performs basic steps towards the task of converting free text into semantic
triples by gluing together an off-the-shelf parser with ad-hoc semantic interpretation
methods. TEXTCAP parses a document into Penn TreeBank form and then traverses
each syntactic parse tree performing a series of step-by-step tasks such as discourse
parsing, clause separation, word sense disambiguation, anaphora resolution and se-
mantic role labelling. Ad hoc rules then create a set of triples from the resulting
semantically-enhanced parse tree.
TEXTCAP first uses the domain-independent Charniak parser (Charniak, 2000) to
convert sentences in the source document into a sequence of syntactic parses. It then
applies syntax-based discourse parsing rules (such as Soricut and Marcu (2003)) to
reduce coordinate, subordinate, and relative clauses into coindexed, simpler sentence
parses headed by single verbal relations.
It then marks for grammatical roles (subject, object, etc.) and syntactic features
(e.g., passivity) before using a simple anaphora resolution algorithm based on those
features and a word sense disambiguation algorithm grounded in WordNet (Fellbaum,
1998) senses that helps determine additional features such as animacy. A two-pass
method is applied where first monosemous words are assigned senses, and then re-
maining senses are selected together with verb types (TEXTCAP uses ad hoc rules
rather than current verb taxonomies like FrameNet). Selectional restrictions from the
verb type then allows for labelling of peripheral grammatical roles as semantic roles.
Finally, entities representing specific objects are marked with ontological relations and
discourse relations are realized between individual verbal relations.
The end product of TEXTCAP is thus a list of coindexed semantic triples represent-
ing the explicitly recoverable semantic content of the input text.
3 Text Processing Components
Corpus methods underlie many of the recent improvements in a wide array of generic
NLP tools. For instance, the introduction of large-scale lexical and syntactic resources
like the Penn TreeBank (Marcus et al, 1993) have led to highly accurate, domain inde-
pendent parsers (Collins, 1999; Charniak, 2000). Wide-coverage anaphora resolution
systems process references across multiple sentences, and recent work on anaphora
resolution by Poesio and Kabadjov (2004) describes itself as the first such system
which can be used off-the-shelf.
Word sense disambiguation (Gliozzo et al, 2005), often based on term frequency
analyses of large annotated corpora, can help localize search in a particular area of a
knowledge base to find the most related concepts and instances. Semantic role labelers
(Gildea and Jurafsky, 2002; Yeh et al, 2006) annotate what role each entity has in
relation to its local man verb, and can provide additional clues for disambiguating
words and locating them in an ontological space.
In addition to lexical and semantic tasks, multi-sentence linguistic analysis such
as discourse segmentation and parsing is needed to semantically label the roles of
verb phrases in relation to one other. Soricut and Marcu (2003) presented a statistical
system that automatically produces an analysis of the rhetorical structure that holds
between sets of sentences or clauses at the paragraph level.
330 Callaway
As a generalization, NLP research has been conducted separately and few attempts
have been made to connect each of them into the longer chains and pipelines needed
for more complete and deeper text processing such as is needed for tasks like knowl-
edge acquisition. Additionally, most of these tools are intended to iteratively examine
each sentence individually within a larger document. But often important linguistic
phenomena cross sentence boundaries, yet are just as necessary to properly understand
the semantic content of a document.
4 Knowledge Representation in TEXTCAP
A common method of representing semantic knowledge in the Knowledge Base com-
munity (Brachman and Schmolze, 1985; Clark and Porter, 1998) is through three-
place predicates, or triples, of the form (CONCEPT RELATION CONCEPT). A concept
can signify either a generic entity or class, like ?houses?, or a particular instance, such
as ?my house at 35 Lincoln Avenue?; instances are coindexed to indicate they are the
same entity in multiple predicates. Relations are typed according to domain, range and
cardinality, and can also be marked as instances to indicate that they refer to specific
events or properties that hold at a particular time place, etc.
Databases and knowledge bases can both be represented as large collections of
triples. Knowledge bases differ from databases in that they are generally organized
around a hierarchical taxonomy, or ontology, of both entities and relations, allow-
ing for subsumption as inference and for knowledge to be separated into subgroups.
Knowledge bases differ from ontologies in that, like databases, they also contain a
larger set of specific knowledge (instances) that describes non-taxonomic relation-
ships between members and instances of the ontology?s concepts.
For instance, the sentence ?My dog chases rabbits.? talks about a specific instance
of the class dog and its relationship to the generic class representing rabbits, per-
haps represented as the triple (DOG492 CHASING RABBIT-ANIMAL). To know that
this dog really is a member of what we consider as the class of all dogs, we would
need to add an ontological triple such as (DOG492 INSTANCE-OF DOG). To represent
the possessive grammatical relation in ?my dog? we would need to agree on some
particular person (an instance) to represent the speaker of the utterance (PERSON142
INSTANCE-OF PERSON) and then also add a relation to indicate possession (DOG492
OWNED-BY PERSON142). Because language is ambiguous compared to semantic triples,
we wouldn?t want the word ?my? to always be mapped to the same relation, for in-
stance, obtaining (PERSON366 OWNED-BY PERSON142) from the phrase ?my friend?.
Like concepts, relations can also have instances since they can refer to particular
events with particular modifiers. For instance, in the sentence ?My dog quickly chased
rabbits yesterday.? we would need to change the relation CHASING from the triple
above to (DOG492 CHASING141 RABBIT-ANIMAL) to indicate its modifiers, perhaps
with (CHASING141 SPEED QUICKLY) and (CHASING141 EVENT-TIME YESTERDAY).
We would also need to indicate the taxonomic relationship between the two relations,
(CHASING141 INSTANCE-OF CHASING).
Because over the years different research groups have created differing ontologies,
it is important to have a common ontology (and arguably, mapping of lexical items to
classes in that ontology) for purposes such as evaluative comparison, even if imple-
mentations that acquire semantic triples can use any available ontology.
The TEXTCAP Semantic Interpreter 331
In keeping with the practice of much recent large-scale NLP, TEXTCAP uses Word-
Net (Fellbaum, 1998) as an underlying ontology and sense repository for generic
classes, giving it the ability to leverage recent NLP tools that rely on it, such as
for word sense disambiguation (Gliozzo et al, 2005). Thus given the sentence in
Figure 1(a), we are interested in producing the semantic triples in (b) where generic
entities and relations are grounded in WordNet.
5 Processing The Text
To illustrate how TEXTCAP works, we follow how it processes the following para-
graph of newspaper text from the New York Times:
Amid the tightly packed row houses of North Philadelphia, a pioneer-
ing urban farm is providing fresh local food for a community that often
lacks it, and making money in the process. Greensgrow, a one-acre plot
of raised beds and greenhouses on the site of a former steel-galvanizing
factory, is turning a profit by selling its own vegetables and herbs as well
as a range of produce from local growers, and by running a nursery sell-
ing plants and seedlings. The farm earned about $10,000 on revenue of
$450,000 in 2007, and hopes to make a profit of 5 percent on $650,000 in
revenue in this, its 10th year, so it can open another operation elsewhere
in Philadelphia.
The first sentence as parsed by Charniak and converted into Lisp notation is:
(S (PP (IN "Amid")
(NP (NP (DT "the") (ADJP (RB "tightly") (VBN "packed"))
(NN "row") (NNS "houses"))
(PP (IN "of") (NP (NNP "North") (NNP "Philadelphia")))))
(PUNCTUATION COMMA)
(NP (DT "a") (JJ "pioneering") (JJ "urban") (NN "farm"))
(VP (AUX "is")
(VP (VP (VBG "providing")
(NP (JJ "fresh") (JJ "local") (NN "food"))
(PP (IN "for")
(NP (NP (DT "a") (NN "community"))
(SBAR (WHNP (WDT "that"))
(S (ADVP (RB "often"))
(VP (VBZ "lacks") (NP (PRP "it"))))))))
(PUNCTUATION COMMA)
(CC "and")
(VP (VBG "making")
(NP (NN "money"))
(PP (IN "in") (NP (DT "the") (NN "process")))))))
(a) "My dog quickly chased rabbits yesterday."
(b) (DOG492 INSTANCE-OF DOG#n1)
(PERSON142 INSTANCE-OF PERSON#n1)
(CHASING141 INSTANCE-OF CHASING#v1)
(DOG492 CHASING141 RABBIT#n1)
(CHASING141 SPEED QUICKLY#adv1)
(CHASING141 EVENT-TIME YESTERDAY#adv1)
Figure 1: WordNet senses as generic entities and relations
332 Callaway
We first normalize this from the form used by the Charniak and Collins parsers
(which do no semantic role labelling and introduce some simplifications) into a cor-
rected version following the original Penn TreeBank format. In the above parse, the
following lines are normalized to mark grammatical subject and correctly mark the
auxiliary verb:
. . .
(NP-SBJ (DT "a") (JJ "pioneering") (JJ "urban") (NN "farm"))
(VP (VBZ "is")
(VP (VP (VBG "providing")
. . .
We then apply a customized discourse parser which converts full syntactic parses
into subparses headed by single verb relations. This is done using purely syntac-
tic information to break up coordinate, subordinate and relative clauses while adding
coindexed traces at the appropriate parse level and introducing a new tree-level tag
DR for discourse relations marked according to Rhetorical Structure Theory (Mann
and Thompson, 1987). All three sentences in the paragraph above are thus converted
into the following 13 discourse parses:
(S (PP (IN "Amid")
(NP (NP (DT "the") (ADJP (RB "tightly") (VBN "packed"))
(NN "row") (NNS "houses"))
(PP (IN "of") (NP (NNP "North") (NNP "Philadelphia")))))
(PUNCTUATION COMMA)
(NP-SBJ (DT "a") (JJ "pioneering") (JJ "urban") (NN "farm") (TRACE 1))
(VP (VBZ "is")
(VP (VP (VBG "providing")
(NP (JJ "fresh") (JJ "local") (NN "food"))
(PP (IN "for")
(NP (DT "a") (NN "community") (TRACE 2)))))))
(S (NP-SBJ (DT "a") (NN "community") (TRACE 2))
(ADVP (RB "often"))
(VP (VBZ "lacks") (NP (PRP "it"))))
(S (NP-SBJ (DT "a") (JJ "pioneering") (JJ "urban") (NN "farm") (TRACE 1))
(VP (VBZ "is")
(VP (VBG "making")
(NP (NN "money"))
(PP (IN "in") (NP (DT "the") (NN "process"))))))
(S (NP-SBJ (NNP "Greensgrow") (TRACE 3))
(VP (VBZ "is")
(NP (NP (DT "a") (JJ "one-acre") (NN "plot"))
(PP (IN "of")
(NP (NP (VBN "raised") (NNS "beds")
(CC "and") (NNS "greenhouses"))
(PP (IN "on")
(NP (NP (DT "the") (NN "site"))
(PP (IN "of")
(NP (DT "a") (JJ "former")
(JJ "steel-galvanizing")
(NN "factory"))))))))))
(S (NP-SBJ (NNP "Greensgrow") (TRACE 3))
(VP (VBZ "is")
(VP (VBG "turning")
(NP (DT "a") (NN "profit"))))
(TRACE 4))
(S (NP-SBJ (NNP "Greensgrow") (TRACE 3))
(VP (VBZ "is")
(VP (VBG "selling")
(NP (NP (PRP-POSS "its") (JJ "own")
(NNS "vegetables") (CC "and") (NNS "herbs"))
(CONJP (RB "as") (RB "well") (IN "as"))
(NP (NP (DT "a") (NN "range"))
(PP (IN "of") (NP (NN "produce")))))
(PP (IN "from") (NP (JJ "local") (NNS "growers")))))
(TRACE 5))
(S (NP-SBJ (NNP "Greensgrow") (TRACE 3))
(VP (VBZ "is")
(VP (VBG "running")
(NP (NP (DT "a") (NN "nursery")))))
(TRACE 6))
(S (NP-SBJ (NNP "Greensgrow") (TRACE 3))
The TEXTCAP Semantic Interpreter 333
(VP (VBZ "is")
(VP (VBG "selling")
(NP (NNS "plants") (CC "and") (NNS "seedlings")))))
(DR (MEANS (TRACE 4) (TRACE 5) (TRACE 6)))
(S (NP-SBJ (DT "The") (NN "farm") (TRACE 7))
(VP (VBD "earned")
(NP (QP (RB "about") (CURRENCY DOLLAR-SIGN) (CD 10000)))
(PP (IN "on")
(NP (NP (NN "revenue"))
(PP (IN "of")
(NP (CURRENCY DOLLAR-SIGN) (CD 450000)))))
(PP (IN "in") (NP (CD 2007)))))
(S (NP-SBJ (DT "The") (NN "farm") (TRACE 7))
(VP (VBZ "hopes") (S (VP (TO "to") (VP (VBP "make")
(NP (NP (NP (DT "a") (NN "profit"))
(PP (IN "of")
(NP (NP (CD 5) (NN "percent"))
(PP (IN "on")
(NP (NP (CURRENCY DOLLAR-SIGN) (CD 650000))
(PP (IN "in")
(NP (NP (NN "revenue"))
(PP (IN "in")
(NP (DT "this"))))))))))
(PUNCTUATION COMMA)
(NP-TMP (PRP-POSS "its") (JJ "10th")
(NN "year")))))))
(TRACE 8))
(S (NP-SBJ (PRP "it"))
(VP (MD "can")
(VP (VBP "open")
(NP (DT "another") (NN "operation"))
(PP (ADVP (RB "elsewhere"))
(IN "in")
(NP (NNP "Philadelphia")))))
(TRACE 9))
(DR (EVENT-ENABLES (TRACE 8) (TRACE 9)))
Next, TEXTCAP adds grammatical features at the NP level to allow for eventual
anaphora resolution. Given a simplified version of sentence 5 above, ?Greensgrow
sells vegetables.?:
(S (NP-SBJ (NNP "Greensgrow"))
(VP (VBZ "sells")
(NP (NNS "vegetables"))))
One ad-hoc rule matches to the unmodified plural noun and marks it as being a
generic class rather than an instance and stems the lexical item. Another rule notes
that the subject is a proper noun that is not in its stoplist of person names. As it is not
the object of a preposition, it is marked as a company name (via the WordNet sense).
Additional senses are assigned if, for instance, only one sense is possible.
(S (NP-SBJ (NNP "Greensgrow") (TYPE COMPANY#n1)
(GENDER NEUTRAL))
(VP (VBZ "sells")
(NP (NN "vegetable") (NUMBER PLURAL)
(GENERIC YES) (GENDER NEUTRAL))))
Next, we map grammatical subjects and objects to logical ones, undoing passiviza-
tion, etc. Then we mark verb type and semantic roles by matching selectional restric-
tions (currently based on ad-hoc rules) between the verb and its principal arguments.
Modifiers of an NP are processed as semantic triples dependent on that NP?s instance,
and similarly for verbal modifiers.
334 Callaway
<relation> = (NP-SBJ (VP (VBZ "sells") NP)
(TRACE 5))
--> (<agent> SELL#v? <patient>)
<agent> = ((NNP "Greensgrow") (TYPE COMPANY#n1)
--> COMPANY#n1(name="Greensgrow",
gender=neutral)
<patient> = (NP (NNS "vegetable") ...)
--> VEGETABLE#n?(generic=yes,
gender=neutral,
number=plural)
Anaphora resolution rules search NPs and their feature lists in reverse to exclude
impossible coreferences; TEXTCAP currently uses the first acceptable NP as its coref-
erent. Next, word sense disambiguation is applied. Because we use WordNet senses
as an underlying foundation, we can pass a bag of nearby senses using existing pub-
lished WSD algorithms, although we are currently testing the degree of performance
improvement between simple baselines and custom algorithms. After WSD, we give
instance names to each type/sense and drop information on generic entities.
<relation> = (<agent> SELL#v1 <patient>), trace=5
<agent> = COMPANY#n1(name="Greensgrow",
inst=COMPANY549)
<patient> = VEGETABLE#n1
Next, we build a list of coindexed semantic triples directly from the above repre-
sentation. If no sentence-level traces or modifiers are dependent on the verbal relation,
it is treated as a generic instance.
(COMPANY549 INSTANCE-OF COMPANY#n1)
(COMPANY549 NAME "Greensgrow")
(COMPANY549 SELL#v1 VEGETABLE#n1)
After repeating this process for each standard sentence-level parse, triples repre-
senting discourse relations are then included for each dependency, for instance:
(DR (EVENT-ENABLES (TRACE 8) (TRACE 9)))
(FARM381 MAKING287 PROFIT#n1)
(FARM381 OPENING286 OPERATION#n2)
(MAKING287 EVENT-ENABLES OPENING286)
6 Performance on the Shared Task
Overall, TEXTCAP performedwell for its intended purpose, but many limitations were
encountered on unseen texts, as expected. Principally, word sense disambiguation and
pronoun resolution initially caused significant problems in terms of robustness and the
capabilities of these text processing steps were significantly downgraded in order that
TEXTCAP could run to completion on all seven sets of unseen texts. Thus WSD was
run only for WordNet noun senses and pronoun resolution was not run across sentence
boundaries within each set. Additionally, the discourse parser lacked rules to correctly
convert sentences #1 and #4 in set #5, so the input sentences were manually split in
that case.
The TEXTCAP Semantic Interpreter 335
However, TEXTCAP was able to do a good job at producing semantic triples for
every text, and the number of triples was proportional to the length of each sentence,
as expected. The use of existing lexical tools and resources allows for more time to
be spent on adding and correcting semantic mappings. Some necessary lexical tools
are either not available or still limited in terms of accuracy, and some resources do not
exist, for instance, there is no good ontological inventory of prepositions and how they
should be mapped semantically. In general, overall accuracy (as measured by human
inspection) was much better on shorter sentences.
The system performed poorly in some areas such as interpreting questions and quo-
tations involving multiple sentences. Additionally, the structure of many of the triples
that TEXTCAP produced were highly reflective on the original syntactic parses ? it is
not clear, for instance, that they would enable a question answering system to locate
correct answers reliably. However, overall, we believe that post-editing of triples with
TEXTCAP would provide a significant time speedup compared to manual knowledge
engineering alone, and we are looking at methods of showing this empirically.
The following data represent the performance of TEXTCAP on the 2008 STEP
shared task. Sentences were processed in an average time of 4 seconds each.
Set #1
[1] ?An object is thrown with a horizontal speed of 20 m/s from a cliff that is 125
m high.?
Notes: (a) the parser interpreted ?m/s? as a plural noun; (b) source is a very vague
relation; (c) cliff was correctly recognized as a relative clause subject.
((OBJECT001 INSTANCE-OF OBJECT#1)
(SPEED001 INSTANCE-OF SPEED#1)
(M/001 INSTANCE-OF M/#0)
(CLIFF001 INSTANCE-OF CLIFF#1)
(NUMBER10 INSTANCE-OF NUMBER)
(UNKNOWN-AGENT THROWING OBJECT001)
(MANNER-WITH THROWING SPEED001)
(SPEED001 RANGE-OF M/001)
(M/001 SOURCE CLIFF001)
(M/001 NUMERIC-QUANTITY 20)
(SPEED001 CHARACTERISTICS HORIZONTAL)
(TIME-PERIOD THROWING PRESENT)
(NUMBER10 HAS-VALUE 125)
(CLIFF001 BEING NUMBER10))
[2] ?The object falls for the height of the cliff.?
Notes: (a) for was incorrectly intepreted as purpose (?object? would be animate); (b)
of yielded the wrong relation; (c) it?s unclear what should be the 3rd element of the
triple for falling.
((OBJECT001 INSTANCE-OF OBJECT#1)
(HEIGHT001 INSTANCE-OF HEIGHT#1)
(CLIFF001 INSTANCE-OF CLIFF#1)
(OBJECT001 FALLING INTRANSITIVE-ARGUMENT)
(FALLING PURPOSE-FOR HEIGHT001)
(HEIGHT001 RANGE-OF CLIFF001))
336 Callaway
[3] ?If air resistance is negligible, how long does it take the object to fall to the
ground??
Notes: This sentence was not processed satisfactorily due to no rules to detect ques-
tions of the form ?how [adjp]?.
[4] ?What is the duration of the fall??
Notes: This sentence was processed satisfactorily.
Set #2
[1] ?Cervical cancer is caused by a virus.?
Notes: (a) probably better to map cervical to cervix to allow for semantic process-
ing in, e.g., a question answering system.
((VIRUS001 INSTANCE-OF VIRUS#1)
(CANCER001 INSTANCE-OF CANCER#1)
(VIRUS001 CAUSING CANCER001)
(CANCER001 CHARACTERISTICS CERVICAL))
[2 ?That has been known for some time and it has led to a vaccine that seems to
prevent it.?
Notes: (a) the system has more trouble mapping situational referents, but it did cor-
rectly notice one was present; (b) need more mappings for for besides purpose; (c)
need to map from grammatical tense to relational tense.
((SITUATION001 INSTANCE-OF UNKNOWN-REFERENT)
(TIME001 INSTANCE-OF TIME#1)
(VACCINE001 INSTANCE-OF VACCINE#1)
(SITUATION001 KNOWING INTRANSITIVE-ARGUMENT)
(KNOWING PURPOSE-FOR TIME001)
(TIME001 QUANTIFIER-VALUE SOME)
(TIME-PERIOD KNOWING PAST-HABITUAL-ACTION)
(SINGLE-NEUTER-REFERENT LEADING INTRANSITIVE-ARGUMENT)
(DESTINATION LEADING VACCINE001)
(TIME-PERIOD LEADING PRESENT-PERFECT)
(VACCINE001 PREVENTING001 SINGLE-NEUTER-REFERENT)
(VACCINE001 SEEMING PREVENTING001))
[3] ?Researchers have been looking for other cancers that may be caused by viruses.?
Notes: (a) didn?t map looking and for as a single verbal relation; (b) the treatment
of quantifiers is too simplistic (other).
((RESEARCHER001 INSTANCE-OF RESEARCHER#1)
(CANCER001 INSTANCE-OF CANCER#1)
(VIRUS001 INSTANCE-OF VIRUS#1)
(RESEARCHER001 LOOKING INTRANSITIVE-ARGUMENT)
(RESEARCHER001 NUMBER-OF-UNITS MORE-THAN-ONE)
(LOOKING PURPOSE-FOR CANCER001)
(CANCER001 CHARACTERISTICS OTHER-ADJ)
(CANCER001 NUMBER-OF-UNITS MORE-THAN-ONE)
(TIME-PERIOD LOOKING PAST-HABITUAL-ACTION)
(VIRUS001 CAUSING CANCER001)
(VIRUS001 NUMBER-OF-UNITS MORE-THAN-ONE)
(MODALITY CAUSING MODAL-MAY))
The TEXTCAP Semantic Interpreter 337
Set #3
We skip this set of sentences as TEXTCAP seemed to perform very well on set #3
excepting the pronouns in sentence #5.
Set #4
[1] ?The first school for the training of leader dogs in the country is going to be
created in Mortagua and will train 22 leader dogs per year.?
Notes: (a) Mortagua wasn?t treated as a city name; (b) incorrect treatment of complex
passive verb phrases (?going? is not a main verb); the same semantic object school
is correctly noted as being involved in both phrases.
((SCHOOL001 INSTANCE-OF SCHOOL#1)
(MORTAGUA001 INSTANCE-OF PERSON#1)
(MORTAGUA001 ACTOR-NAME "Mortagua")
(MORTAGUA001 ACTOR-GENDER NEUTER)
(DOG001 INSTANCE-OF DOG#1)
(YEAR001 INSTANCE-OF YEAR#1)
(SCHOOL001 GOING INTRANSITIVE-ARGUMENT)
(SCHOOL001 CHARACTERISTICS FIRST)
(GOING LOCATION-IN MORTAGUA001)
(TIME-PERIOD GOING PRESENT-PROGRESSIVE)
(SCHOOL001 TRAINING DOG001)
(DOG001 PER YEAR001)
(DOG001 NAMED-TYPE LEADER#1)
(DOG001 NUMBER-OF-UNITS MORE-THAN-ONE)
(TIME-PERIOD TRAINING FUTURE))
[2] ?In Mortagua, Joao Pedro Fonseca and Marta Gomes coordinate the project that
seven people develop in this school.?
Notes: This sentence was processed satisfactorily.
[3] ?They visited several similar places in England and in France, and two future
trainers are already doing internship in one of the French Schools.?
Notes: (a) not a good quantifier representation for several; (b) any proper NP is
being interpreted as a person.
((PLACE001 INSTANCE-OF PLACE#1)
(TRAINER001 INSTANCE-OF TRAINER#1)
(INTERNSHIP001 INSTANCE-OF INTERNSHIP#1)
(NUMBER11 INSTANCE-OF NUMBER)
(FRENCH-SCHOOLS001 INSTANCE-OF PERSON#1)
(FRENCH-SCHOOLS001 ACTOR-NAME "French Schools")
(FRENCH-SCHOOLS001 ACTOR-GENDER NEUTER)
(PLURAL-THIRD-PERSON-REFERENT VISITING PLACE001)
(PLACE001 CHARACTERISTICS SEVERAL)
(PLACE001 CHARACTERISTICS SIMILAR)
(PLACE001 NUMBER-OF-UNITS MORE-THAN-ONE)
(TIME-PERIOD VISITING PAST)
(TRAINER001 DOING INTERNSHIP001)
(TRAINER001 WRITTEN-NUMERIC-QUANTITY 2)
(TRAINER001 CHARACTERISTICS FUTURE)
338 Callaway
(TRAINER001 NUMBER-OF-UNITS MORE-THAN-ONE)
(DURATION DOING ALREADY)
(DOING LOCATION-IN NUMBER11)
(NUMBER11 RANGE-OF FRENCH-SCHOOLS001)
(TIME-PERIOD DOING PRESENT-PROGRESSIVE))
[4] ?The communitarian funding ensures the operation of the school until 1999.?
Notes: This sentence was relatively uninteresting.
[5] ?We would like our school to work similarly to the French ones, which live
from donations, from the merchandising and even from the raffles that children sell in
school.?
Notes: This sentence was not processed satisfactorily due to missing discourse pars-
ing rules.
Set #5
[1] ?As the 3 guns of Turret 2 were being loaded, a crewman who was operating
the center gun yelled into the phone, ?I have a problem here. I am not ready yet.? ?
Notes: (a) this sentence was manually split before the quotation; (b) another proper
NP interpreted as a person; (c) the system in general works well with quotations, but
not when they are composed of multiple sentences.
((GUN001 INSTANCE-OF GUN#1)
(TURRET-2001 INSTANCE-OF PERSON#1)
(TURRET-2001 ACTOR-NAME "Turret 2")
(TURRET-2001 ACTOR-GENDER NEUTER)
(CREWMAN001 INSTANCE-OF CREWMAN#1)
(CENTER-GUN001 INSTANCE-OF CENTER-GUN#0)
(PROBLEM001 INSTANCE-OF PROBLEM#1)
(UNKNOWN-AGENT LOADING GUN001)
(GUN001 RANGE-OF TURRET-2001)
(GUN001 NUMERIC-QUANTITY 3)
(TIME-PERIOD LOADING PAST-PROGRESSIVE)
(CREWMAN001 OPERATING CENTER-GUN001)
(TIME-PERIOD OPERATING PAST-PROGRESSIVE)
(PROBLEM001 BEING READY)
(DURATION BEING YET)
(POLARITY BEING NEGATIVE))
[2] ?Then the propellant exploded.?
Notes: This sentence was processed satisfactorily.
[3] ?When the gun crew was killed they were crouching unnaturally, which sug-
gested that they knew that an explosion would happen.?
Notes: This sentence presented more syntactic than semantic issues.
[4] ?The propellant that was used was made from nitrocellulose chunks that were
produced during World War II and were repackaged in 1987 in bags that were made
in 1945.?
Notes:
[5] ?Initially it was suspected that this storage might have reduced the powder?s
stability.?
The TEXTCAP Semantic Interpreter 339
Notes: (a) the possessive noun powder was incorrectly marked as a person; (b) the
time and modality markers are a bit vague.
((STORAGE001 INSTANCE-OF STORAGE#1)
(STABILITY001 INSTANCE-OF STABILITY#1)
(POWDER001 INSTANCE-OF PERSON#1)
(POWDER001 ACTOR-NAME "powder")
(POWDER001 ACTOR-GENDER NEUTER)
(UNKNOWN-AGENT SUSPECTING REDUCING)
(STORAGE001 REDUCING STABILITY001)
(TIME SUSPECTING INITIALLY)
(TIME-PERIOD REDUCING PRESENT-PERFECT)
(MODALITY REDUCING MODAL-MIGHT)
(TIME-PERIOD SUSPECTING PAST))
Set #6
Data in this set was used to test TEXTCAP and so is not analyzed here.
Set #7
[1] ?Modern development of wind-energy technology and applications was well
underway by the 1930s, when an estimated 600,000 windmills supplied rural areas
with electricity and water-pumping services.?
Notes: (a) couldn?t convert 1930s to a date range; (b) underway was treated as a verb
by the parser; (c) more problems mapping prepositional relations.
((DEVELOPMENT001 INSTANCE-OF DEVELOPMENT#1)
(TECHNOLOGY001 INSTANCE-OF TECHNOLOGY#1)
(APPLICATION001 INSTANCE-OF APPLICATION#1)
(NUMBER24 INSTANCE-OF NUMBER)
(NUMBER24 HAS-VALUE "1930")
(WINDMILL001 INSTANCE-OF WINDMILL#1)
(AREA001 INSTANCE-OF AREA#1)
(ELECTRICITY001 INSTANCE-OF ELECTRICITY#1)
(SERVICE001 INSTANCE-OF SERVICE#1)
(UNKNOWN-AGENT UNDERWAY DEVELOPMENT001)
(DEVELOPMENT001 RANGE-OF TECHNOLOGY001)
(DEVELOPMENT001 RANGE-OF APPLICATION001)
(TECHNOLOGY001 CHARACTERISTICS WIND-ENERGY)
(APPLICATION001 NUMBER-OF-UNITS MORE-THAN-ONE)
(DEVELOPMENT001 CHARACTERISTICS MODERN)
(DURATION UNDERWAY WELL)
(TIME-BY UNDERWAY NUMBER24)
(NUMBER24 NUMBER-OF-UNITS MORE-THAN-ONE)
(TIME-PERIOD UNDERWAY PAST)
(WINDMILL001 SUPPLYING AREA001)
(WINDMILL001 NUMERIC-QUANTITY 600000)
(AREA001 HAVE-WITH ELECTRICITY001)
(AREA001 HAVE-WITH SERVICE001)
(SERVICE001 NAMED-TYPE WATER-PUMPING#0)
(SERVICE001 NUMBER-OF-UNITS MORE-THAN-ONE)
(AREA001 CHARACTERISTICS RURAL)
(AREA001 NUMBER-OF-UNITS MORE-THAN-ONE)
(TIME-PERIOD SUPPLYING PAST))
340 Callaway
[2] ?Once broad-scale electricity distribution spread to farms and country towns,
use of wind energy in the United States started to subside, but it picked up again after
the U.S. oil shortage in the early 1970s.?
Notes: Notes: This sentence was processed satisfactorily, but only when manually
split due to missing discourse parsing rules.
[3] ?Over the past 30 years, research and development has fluctuated with federal
government interest and tax incentives.?
Notes: This sentence was processed satisfactorily.
[4] ?In the mid-?80s, wind turbines had a typical maximum power rating of 150
kW.?
Notes: This sentence had problems understanding the phrase ?mid-?80s?, perhaps as
a result of the off-the-shelf parser being very generic.
[5] ?In 2006, commercial, utility-scale turbines are commonly rated at over 1 MW
and are available in up to 4 MW capacity.?
Notes: (a) the fact that someone rates turbines isn?t the same as turbines carrying a
rating; commonly wasn?t interpreted correctly; (c) the last phrase after available
wasn?t mapped to anything.
((TURBINE001 INSTANCE-OF TURBINE#1)
(DATE26 INSTANCE-OF DATE)
(DATE26 HAS-YEAR 2006)
(UNKNOWN-AGENT RATING TURBINE001)
(TURBINE001 CHARACTERISTICS COMMERCIAL)
(TURBINE001 CHARACTERISTICS UTILITY-SCALE)
(TURBINE001 NUMBER-OF-UNITS MORE-THAN-ONE)
(FREQUENCY RATING COMMONLY)
(TIME-IN RATING DATE26)
(TURBINE001 BEING AVAILABLE)
(TURBINE001 NUMBER-OF-UNITS MORE-THAN-ONE))
7 Conclusions
We introduced TEXTCAP, a semantic parser which uses a combination of off-the-
shelf NLP technology and ad-hoc rules to produce semantic triples corresponding to
the explicit semantic content in unrestricted text. We ran TEXTCAP on 7 sets of short
text in the STEP 2008 Shared Task, and the system successfully generated triples
for almost all inputs and provided, as we expected, a set of triples that while not fully
correct, could be post-edited for accuracy and which should provide a significant speed
up over completely manual production of semantic triples from text. On average,
TEXTCAP processed a sentence from the corpus in about 4 seconds.
While TEXTCAP only captures explicit knowledge (but not commonsense knowl-
edge, unmentioned knowledge, implicit relationships, etc.) it can save knowledge
engineers time by providing reasonably accurate semantic representations of domain
text. In future work we plan on improving methods of knowledge integration (e.g.,
ontology population), testing within real-world applications such as question answer-
ing systems, and empirically evaluating the time and accuracy for producing semantic
triples via various methods.
The TEXTCAP Semantic Interpreter 341
References
Barker, K., B. Agashe, S. Chaw, J. Fan, N. Friedland, M. Glass, J. Hobbs, E. Hovy,
D. Israel, D. S. Kim, R. Mulkar-Mehta, S. Patwardhan, B. Porter, D. Tecuci, and
P. Yeh (2007, July). Learning by reading: A prototype system, performance base-
line and lessons learned. In Proceedings of the 22nd National Conference on Arti-
ficial Intelligence (AAAI), Vancouver, Canada.
Blythe, J., J. Kim, S. Ramachandran, and Y. Gil (2001). An integrated environment
for knowledge acquisition. In Proceedings of the 2001 International Conference
on Intelligent User Interfaces, Santa Fe, NM, USA.
Bos, J. (2008). Introduction to the Shared Task on Comparing Semantic Representa-
tions. In J. Bos and R. Delmonte (Eds.), Semantics in Text Processing. STEP 2008
Conference Proceedings, Volume 1 of Research in Computational Semantics, pp.
257?261. College Publications.
Brachman, R. J. and J. G. Schmolze (1985, April). An overview of the KL-ONE
knowledge representation system. Cognitive Science 9(2), 171?216.
Callaway, C., E. Not, A. Novello, C. Rocchi, O. Stock, and M. Zancanaro (2005,
June). Automatic cinematography and multilingual NLG for generating video doc-
umentaries. Artificial Intelligence 165(1), 57?89.
Carenini, G., R. T. Ng, and E. Zwart (2005). Extracting knowledge from evaluative
text. In K-CAP ?05: Proceedings of the 3rd International Conference on Knowl-
edge Capture, Banff, Canada, pp. 11?18.
Charniak, E. (2000, April). A maximum-entropy-inspired parser. In Proceedings of
the 2000 NAACL, Seattle, WA.
Clark, P. and B. Porter (1998). KM ? the knowledge machine: Users manual. Techni-
cal report, AI Lab, University of Texas at Austin.
Clark, P., J. Thompson, K. Barker, B. Porter, V. Chaudhri, A. Rodriguez, J. Thomr,
Y. Gil, and P. Hayes (2001, October). Knowledge entry as the graphical assembly
of components: The SHAKEN system. In Proceedings of the First International
Conference on Knowledge Capture (KCAP), Victoria BC, Canada.
Collins, M. (1999). Head-driven Statistical Models for Natural Language Parsing.
Ph. D. thesis, University of Pennsylvania.
Fellbaum, C. (1998). WordNet: An electronic lexical database. The MIT Press.
Gildea, D. and D. Jurafsky (2002). Automatic labeling of semantic roles. Computa-
tional Linguistics 28(3), 245?288.
Gliozzo, A., C. Giuliano, and C. Strapparava (2005, June). Domain kernels for word
sense disambiguation. In Proceedings of the 43th Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor, MI, pp. 403?410.
342 Callaway
Lester, J. C. and B. W. Porter (1997). Developing and empirically evaluating ro-
bust explanation generators: The KNIGHT experiments. Computational Linguis-
tics 23(1), 65?101.
Mann, W. C. and S. A. Thompson (1987, June). Rhetorical structure theory: A theory
of text organization. Technical Report ISI/RS-87-190, USC/Information Sciences
Institute, Marina del Rey, CA.
Marcus, M., B. Santorini, and M. Marcinkiewicz (1993). Building a large annotated
corpus of English: The PennTreeBank. Computational Linguistics 19(2), 313?330.
Poesio, M. and M. A. Kabadjov (2004, May). A general-purpose, off-the-shelf system
for anaphora resolution. In Proceedings of the Language Resources and Evaluation
Conference, Lisbon, Portugal.
Reiter, E., S. Sripada, and R. Robertson (2003). Acquiring correct knowledge for
natural language generation. Journal of Artificial Intelligence Research 18, 491?
516.
Soricut, R. and D. Marcu (2003, May). Sentence level discourse parsing using syn-
tactic and lexical information. In Proceedings of HLT-NAACL, Edmonton, Alberta.
Stock, O., M. Zancanaro, P. Busetta, C. Callaway, A. Krueger, M. Kruppa, T. Kuflik,
E. Not, and C. Rocchi (2007). Adaptive, intelligent presentation of information
for the museum visitor in peach. User Modeling and User Adapted Interaction 17,
257?304.
Yeh, P., B. Porter, and K. Barker (2006, July). A unified knowledge based approach
for sense disambiguation and semantic role labeling. In Proceedings of the Twenty-
First National Conference on Artificial Intelligence, Boston, MA.
