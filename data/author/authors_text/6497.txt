Tagging and Chunking with Bigrams 
Ferran P la ,  Anton io  Mo l ina  and Nat iv idad  Pr ie to  
Univers i ta t  Po l i tbcn ica  de Val5ncia 
Depar tament  de S is temes In form\ t i cs  i Computac i6  
Camf  de Vera s /n  
46(120 ValSncia 
{ fp la ,amol ina ,npr ie to}@ds ic .upv .es  
Abst ract  
In this paper we present an integrated system for 
tagging and chunking texts from a certain language. 
The approach is based on stochastic finite-state 
models that are learnt automatically. This includes 
bigrmn models or tinite-state automata learnt using 
grammatical inference techniques. As the models in- 
volved in our system are learnt automatically, this 
is a very flexible and portable system. 
Itl order to show the viability of our approach we 
t)resent results for tagging mid chunking using bi- 
grain models on the Wall Street Journal corpus. We 
have achieved an accuracy rate for tagging of 96.8%, 
and a precision rate tbr NP chunks of 94.6% with a 
recall rate of 93.6%. 
1 In t roduct ion  
Part of Speech Tagging and Shallow Parsing are two 
well-known problems in Natural Language Process- 
ing. A Tagger can be considered as 2 translator that 
reads sentences from a certain language and outputs 
the corresponding sequences of part of speech (POS) 
tags, taking into account he context in which each 
word of the sentence appears. A Shallow Parser in- 
volves dividing sentences into non-overlapping seg- 
ments on the basis of very superticial analysis. It; 
includes discovering the main constituents of the 
sentences (NPs, VPs, PPs, ...) and their heads. 
Shallow Parsing usually identifies non-recnrsive con- 
stituents, also called chunks (Abney, 1991) (such as 
non-recursive Noun Phrases or base NP, base VP, 
and so on). It can include deterlnining syntactical 
relationships such as subject-verb, verb-object, etc., 
Shallow parsing wlfich always follows tlm tagging 
process, is used as a fast and reliable pre-l)rocessing 
phase for full or partial parsing. It can be used for 
hffbrmation Retrieval Systems, Information Extrac- 
tion, Text Summarization and Bilingual Alignment. 
In addition, it is also used to solve colnputational 
linguistics tasks such as disambiguation t)roblems. 
1.1 POS Tagging Approaches 
The different aI)proaches for solving this problem 
can be classified into two main classes depending 
oi1 tile tendencies followed for establishing tile Lan- 
guage Model (LM): tile linguistic apI)roach, based 
oil hand-coded linguistic rules and the learning ap- 
I)roach derived fi'om a corpora (labelled or non- 
labelled). Other at)proximations that use hybrid 
methods have also been proposed (Voutilaiuen and 
Padr6, 1997). 
In tim linguistic apl)roach, an exI)ert linguist is 
needed to formalise the restrictions of the language. 
This implies a very lfigh cost and it is very depen- 
dent on each particular language. We can lind an 
important contribution (Voutilainen, :1995) that uses 
Constraint Grammar tbrmalism. Supervised learn- 
ing methods were proposed in (Brill, 1995) to learn 
a set, of transforlnation rules that repair tim error 
committed by a probabilistic tagger. The main a(t- 
vantage of the linguistic approach is that the model 
is constructed from a linguistic I)oint of view and 
contains many and complex kinds of knowledge_ 
iI1 tim lem'ning approach, tile most extended 
tbrmalism is based on n-grains or IIMM. In tiffs 
case, the language inodel can be estimated from 
a labelled corpus (supervised methods) (Church, 
1988)(Weisehedel t al., 1.993) or from a non- 
labelled corpus (unsupervised methods) (Cutting et 
21., 1992). In the first; case, the model is trained from 
the relative observed Dequencies. In the second one, 
the model is learned using the Baunl-\?elch algo- 
rithm from an initial model which is estimated using 
labelled corpora (Merialdo, 1994). The advantages 
of the unsupervised approach are the facility to tmild 
language models, the flexibility of choice of cate- 
gories and the ease of apt)lication to other languages. 
We can find some other machine-learning approaches 
that use more sophisticated LMs, such as Decision 
Trees (Mhrquez and Rodrfguez, 1998)(Magerman, 
1996), memory-based approaclms to learn special de- 
cision trees (Daelemans et al, 1996), maximmn en- 
tropy approaches that combine statistical informa- 
tion from different sources (Ratnaparkhi, 1996), fi- 
nite state autonmt2 inferred using Grammatical In- 
ference (Pla and Prieto, 1998), etc. 
The comparison among different al)t)roaches is d i f  
ficult due to the nmltiple factors that can be eonsid- 
614 
ered: tile languagK, tile mmfl)er and tyt)e of the tags, 
the size of tilt vocabulary, thK ambiguity, the diiti- 
culty of the test ski, Kte. The best rKsults rel)orted 
on the Wall Street ,lore'hal (WSJ) %'e('.l)ank (\]~'\[al'CllS 
el al., 1993), using statistical language models, have 
an ae(:uracy rack) between 95% and 97% (del)Knding 
on the different factors mKntiono.d al)ove). For the 
linguistic al)proach tim results ark l)etter. For exmn- 
p\]e, in (Voutilaineu, 1995) an accuracy of 99.7% is 
rel)orted , but cKrtain ambiguities ill thK ou|;tnl(; re- 
main unsolved. Some works have recently l)een pul)- 
lished (Brill and Wu, 1998) in which a sel; of taggers 
are combined in order to lint)rove the.Jr l/erfornmn(:e. 
In some cases, these methods achieve an accuracy of 
97.9% (llalterKn (31; al., 1998). 
1,2 Shal low Pars ing  A1)t)roaches 
Since the early 90's~ sKveral l;Kchni(tues for carry- 
ing out shalh)w parsing have been d(3velol)ed. Tlms(~ 
techniques can also bK classified into two main 
groups: basKd on hand-codKd linguistic rules and 
based on iKarning algorithms. ThKsK approadms 
ll~we a conunon chara(:tcristi(:: thKy take, l;he se- 
(lUKnCK of 1Kxi(:al tags 1)rot)oscd t)y a POS tagger as 
input, for both the h;arning and the (:bunking pro- 
C(~sses. 
1.2.1 Techniques  based  on hand-coded 
linguistiK rules 
These methods use a hand-written set of rules that 
ark defined l lsing POS as tKrnfinals of tim gI'gtlll- 
mar. Most of these works use tinit(! slate \]nel;llo(ls 
for (tel;Kcl;ing (:hunks or f()r a(:(:olni)lishing el;her lin- 
guisti(: l;asks (EjKrhed, 1988), (:\lm(~y, 1996), (At o 
Mokhtar and Chanod, :19!)7). ()ther works use (tit'-- 
ferellI; ~ralt l l l lgd;ical \]'orlllalislllS~ S/l(;h as (:OllSl;r;/illl; 
grmnmars (Voutilainen, 1993), or (:oral)inK th('. gram- 
mar rules with a set of heuristi(:s (Bourigault, :1992). 
ThesK works usually use. a small test SKi that is lllall- 
ually evaluated, so the achieved results are not sig- 
ni\[icant. The regular KXln:cssions defined in (Ejer- 
lied, 1988) identified both non-recursive clauses and 
non-recursive NPs in English text. The cxperimKn- 
tation on l;he Brown (:ortms achiKvKd a prK(:ision ratK 
of 87% (for clauses) and 97.8 % (for NPs). Ab- 
hey introduced the concept of chunk (Almey, 1991) 
m)d l/resentKd an incremental l)artial parser (Abney, 
1996). This parsKr identities chunks l)ase on the 
parts of Sl)eKch, and it then chooses how to con> 
bine them tbr higher level analysis using lexical in- 
tbrmation. ThK average 1)rKcision and recall rates for 
chunks were 87.9% and 87.1%, rest)ectivKly , on a tKst 
set of 1000 sKntKneKS. An iimrenmntal architKcture 
of finite--state transducers for French is pres(mted in 
(At-Mokhtar and Chanod, 1.997). Each transducer 
1)ert'orms a linguisti(; task su(:h as id(3ntif~ying sKg- 
ments or syntactic strueturKs and dKtecting subjects 
and ol)jects. The system was (3wfluated on various 
corpora for subject and object detKction. The pre- 
cision rate varied between 9(,).2% and 92.6%. The 
recall rate varied between 97.8% and 82.6%. 
The NP2bol llarsKr described in (Voutilainen, 
1993) identified nmximal-length noun phrases. 
NPtool gave a precision ral, e of 95-98% and a re- 
call ratK of 98.5-100%. These results were criticised 
in (Raulshaw and Marcus, 1.995) due to some in- 
consistencies and aplmrenl; mistakKs which appeared 
on thK sample given in (Voutilainen, 1993). Bouri- 
gault dKvelopKd the LECTER parser fin" French us- 
ing grmnmatical rules and soum hem'istics (Bouri- 
gault, 1992). lit achieved a recall rate of 95% iden- 
tit~ying maxilnal ength ternfinological noun phrases, 
but tie (lid not givK a prKcision ratK, so it is difficult; 
to Kvaluate the actual pKribrmance of tile parsKr. 
1..2.2 LKarning Techn iques  
These al)lnoachcs automa.tica.lly (:onstruel; a lan- 
guage model from a labello.d alld brackKted corpus. 
The lirst probabilistic approach was proposed in 
(Church, 1988). This method learn(; a bigram model 
for detecting simph3 noun phrasKs on the Brown cor- 
pus. Civ('n a sequen('e of parts of st)(3eeh as inl)ug ,
the Church program inserts the most prol)able open- 
ings and Kndings of NPs, using a Viterbiqiko. dy- 
namic programming algorithm. Church did not giVK 
precision and recall rates. He showKd that 5 out of 
24:3 NP were omitted, but in a very small test with 
a POS tagging ac(:uraey of 99.5%. 
Transfornlation-based 1Karning (TBI,) was USKd in 
(\]~;unshaw an(l Mar(:us, 1995) to (lc, t(',('t baSK NP. 
In this work ('hunldng was considKre(1 as a tagging 
technique, so that each P()S could be tagged with 
I (inside lmseNP), O (outside baseNl )) or B (inside 
a baseNP, but 1;11(3 pre(:eding word was ill mlother 
basKNP). This at)preach rKsulted in a precision rate 
of 91.8% and a rKcall rate of 92.3%. This iesult 
was automatically Kwlhlat;ed el l  ,q. (;est set; (200,000 
words) extracl;Kd from the WS.\] Treebank. The main 
drawlmek to this approach are the high requiremKnts 
tbr tilne and space which ark needed to train ~he sys- 
l;elll; it needs to train 100 tKmplates of combinations 
of words. 
There are s(;v(;ral works that use a m('mory-based 
h,arning algorithm. ThKse at)proaehKs construct a 
classifier tbr a task by storing a sKI; of exmnples in 
inemory. Each (;xamI)le is definKd l)y a set of fhatures 
that havK to 1)c. learnt from a 1)racketed corpus. The 
Memory-Based Learning (MBL) algorithm (l)aele,- 
roans (3t al., 1999) takes into account lexical and POS 
information. It stores the following features: thK 
word form mid POS tag of thK two words to the left, 
the tbeus word and onK word to the right. This sys- 
tKm achiKved a precision rate of 93.7'7o and a recall 
rate of 94.0% on t\]lK WSJ Treebank. HowevKr, when 
only POS information was used the l)erformance de- 
creased a.chiKving a precision rate of 90.3% mid a 
615 
recall rate of 90.1%. Tile Memory-Based Sequence 
Learning (MBSL) algorithm (Argamon et al, 1998) 
learns substrings or sequences of POS and brackets. 
Precision and recall rates were 92.4% on the same 
data used in (Ramshaw and Marcus, 1995). 
A simple approach is presented in (Cardie and 
Pierce, 1998) called Treebank Apl)roach (TA). This 
techtfique matches POS sequences from an initial 
noun phrase grammar which was extracted fl'om an 
annotated corpus. The precision achieved for each 
rule is used to rank and prune the rules, discarding 
those rules whose score is lower than a predefined 
threshold. It uses a longest match heuristic to de- 
termine base NP. Precision and recall on the WSJ 
Treebank was 89.4% and 90.0%, respectively. 
It is difficult to compare the different al)proaches 
due fbr various reasons. Each one uses a different 
definition of base NP. Each one is evaluated on a 
different corpus or on different parts of the same 
cortms. Some systems have even been evaluated by 
hand on a very small test set. Table 1 summarizes 
tile precision and recall rates for learning approaches 
that use data extracted from the WSJ Treebank. 
Method NP-Pl'ecision NP-Recall 
TBL 91.8 92.3 
MBSL 92.4 92.4 
TA 89.4 90.9 
MBL 93.7 94.0 
MBL (only POS) 90.3 90.1 
Tat)le 1: Precision and recall rates tbr diflhrent NP 
parsers. 
2 General Descript ion of our 
Integrated approach to Tagging 
and Chunking 
We propose an integrated system (Figure 1) that 
combines different knowledge sources (lexical prob- 
abilities, LM for chunks and Contextual LM tbr 
the sentences) in order to obtain the correspond- 
ing sequence of POS tags and the shallow parsing 
(\[su WllC~W.~/c~ su\] W.~lC~ ... \[su W, lC,, su\]) 
from a certain input string (1'I:1,I?.2, ...,I/l:n). Our 
system is a transducer composed by two levels: the 
upper one represents the Contextual LM for tile 
sentences, and the lower one modelize the chunks 
considered. The formalism that we have used in all 
levels are finite-state automata. To be exact, we 
have used models of bigrmns which are smoothed 
using the backoff technique (Katz, 1987) in order to 
achieve flfll coverage of the language. The bigrams 
LMs (bigram probabilities) was obtained by means 
of the SLM TOOLKIT  (Clarksond and Ronsenfeld, 
LEAIINING ~-  
\[-C,m,zxtuall.~ I2"l"?'~.Chunks \] l l'e?icalPmbabilities J 
CIUNKIN(; ~ ~  
Figure 1: Overview of the System. 
1997) from tile sequences of categories in the 
training set. Then, they have been rei)resented like 
finite-state automata. 
2.1 The learning phase. 
The models have been estimated from labelled and 
bracketed corpora. The training set is composed by 
sentences like: 
\[su w,/c,w.,/c., su\] w~/c~ ... \[su ~,~:,~/c,~ su\] ./. 
where Wi are the words, Ci are part-of-speech tags 
and SU are tile chunks considered. 
Tile models learnt are: 
? Contextual LM: it is a smoothed bigram model 
learnt from tile sequences of part -o f  speech tags 
(Ci) and chunk descrit)tors (XU) present in the 
training corpus (see Figure 2a). 
? Models for the chunks: they are smoothed bi- 
gram models learnt fl'om the sequences of part- 
of-speech tags eorrest)onding to each chunk of 
the training corpus (see Figure 2b). 
? Lexical Probabilities: they are estilnated from 
the word frequencies, tile tag frequencies and 
the word per tag frequencies. A tag dictio- 
nary is used which is built from the full cor- 
pus which gives us the possible lexical categories 
(POS tags) for each word; this is equivalent o 
having an ideal morphological analyzer. The 
probabilities for each possible tag are assigned 
from this information taking into account the 
obtained statistics. Due to the fact that the 
word cannot have been seen at training, or it 
has only been seen in some of the possible cat- 
egories, it is compulsory to apply a smoothing 
mechanism. In our case, if the word has not 
previously been seen~ the same probability is 
assigned to all the categories given by the die- 
tionary; if it has been seen, but not in all the 
616 
(b)  LM fo r  Chunks  
. . . . . . . . . . . . . . . . . . . . . . . . .  
i 
, ', z f+@, ,  - -_ . . .  
J 
i 
i 
t 
i 
i . . . . . . . . . . . . . . . . . . . . . . . . . .  
(c) Integrated LM 
i I(<SU>\[( ) x * 
Figure 2: Integrated Language Model fin" Tagging and Chunking. 
categories, the smoothing called "add one" is 
applied. Afterwards, a renormalization process 
is carried out. 
Once the LMs have been learnt, a regular substi- 
tution of the lower model(s) into the upper one is 
made. In this way, we get a single Illtegrated LM 
which shows the possible concatenations of lexical 
tags and syntactical uu i ts ,  with their own transition 
probabilities which also include the lexical probabil- 
ities ms well (see Figure 2c). Not(', that the models 
in Figure 2 are not smoothed). 
2.2 The Decod ing  Process: Wagging and 
Pars ing  
The tagging and shallow parsing process consists of 
finding out the sequence of states of maximum 1)rob- 
ability on the Integrated LM tor an input sentence. 
Therefore, this sequence must be compatible with 
the contextual, syntactical and lexical constraints. 
This process can be carried out by Dynamic Pro- 
gt'ammiitg using the Viterbi algorithm, which is con- 
veniently modified to allow for (;ransitions between 
certain states of the autotnata without consmning 
any symbols (epsilon l;ransitious). A portion of the 
Dynamic Progranmfing trellis for a generic sentence 
us ing  the Integrated LM shown in Figure 2c can be 
seen in Figure 3. The states of the automata that 
can be reached and that are compatible with the 
lexical constraints are marked with a black circle 
(i.e., fl'om the state Ck it is possible to reach the 
state Ci if the transition is in the automata nd the 
lexical probability P(Wi\[Ci) is not null). Also, the 
transitions to initial and final states of the models 
for chunks (i.e., fl'om Ci to < SU >) are allowed; 
these states are marked in Figure 3 with a white cir- 
cle and in this case no symbol is consumed. Ill all 
these cases, the transitions to initial and final pro- 
duce transitions to their successors (the dotted lines 
in Figure 3) where now symbols must be consumed. 
Once the Dynamic Programing trellis is built, we 
can obtain the maximum probability path for the 
input sentence, and thus the best sequence of lexical 
tags and the best segmentation i chunks. 
<s> 
Ci 
cj 
<Is> . . . . . . .  \]\ ~ "~ \]',  `% l{inal 
x\ ". ' State 
<~u> . . . . . . . . .  . . . . . . . .  it>, . . . .  ~>; ,  . . . . .  
Ci / "" / " ',' 
: ~ t I L 
(:k ' , s', 
{. J i l l  , / t 
? / / 
c,, ............ ........ 7~-o.. . ....... //~ . . . . .  
</S U> "%3~ 
- - - - t ~ - -  
hlput: . . .  Wll-2 Wll- I  Wn </S> 
Output: . . .  Wn~2/Ci I SU Wnq/Cn SUI Wn/Ck </s> 
Figure 3: Partial %'ellis for Programming Decoding 
based oil tile Integrated LM. 
3 Exper imenta l  Work  
In this section we will describe a set of experiments 
that we carried out in order to demonstrate the ca- 
pabilities of the proposed approach for tagging and 
shallow parsing. The experiments were carried out 
617 
on the WSJ corpus, using the POS tag set; defined 
in (Marcus etlal. , 1993), considering only the NP 
chunt{s (lefine~l by (Church, 1988) and using tile 
models that we have presented above. Nevertheless, 
the use of this apt)roach on other corpora (chang- 
ing the reference language), other lexical tag sets or 
other kinds of chunks can be done in a direct way. 
3.1 Corpus Description. 
We used a t)ortion of the WSJ corpus (900,000 
words), which was tagged according to the Penn 
Treebank tag set and bracketed with NP markers, 
to train and test the system. 
The tag set contained 45 different tags. About 
36.5% of the words in the cortms were mnbiguous, 
with an ambiguity ratio of 2.44 tag/word over the 
ambiguous words, 1.52 overall. 
3.2 Exper imental  Results. 
In order to train the models and to test the system, 
we randomly divided the corpora into two parts: ap- 
proximately 800,000 words for training aud 100,000 
words tbr testing. 
Both the bigram models for representing contex- 
tual information mid syntactic description of the NP 
chunk and the lexical probabilities were estimated 
from training sets of different sizes. Due to the fact 
that we did not use a morphological nalyser for En- 
glish, we constructed a tag dictionary with the lex- 
icon of the training set and the test set used. This 
dictionary gave us tile possible lexical tags for each 
word fl'om the corpus. In no case, was the test used 
to estimate the lexical probabilities. 
100 
99 
98 
07 
96 
95 
94 
93 
92 
BIG 
BIG-BIG 
\[\[ 
100 200 
\[i (~ {\] {1 
LI 
i i i i 
300 400 500 60O 
#Words x 1000 
Figure 4: Accuracy Rate of Tagging on WSJ for 
incrementM training sets. 
In Figure 4, we show the results of tagging on the 
test set in terms of the training set size using three 
at)proaches: the simplest (LEX) is a tagging process 
which does not take contextual information into ac- 
count, so the lexical tag associated to a word will 
100 
99 
90 
97 
06 
95 
04 
93 
92 
Prec is ion  ? 
Recall 
~, + 
<,, 
+ 
, i , _ _  i i 
100 200 300 4o0 500 600 7(30 800 
#Words x 1000 
Figure 5: NP-chunldng results on WSJ for incremen- 
tal training sets. 
Tagger 
Tagging 
Accuracy 
BIG-BIG 96.8 
Lex 94.3 
BIG 96.9 
IDEAL 100 (assumed) 
NP-Clmnking 
Precision I Recall 
94.6 193.6 
90.8 91.3 
94.9 94.1 
95.5 94.7 
Table 2: Tagging and NP-Chunking results t'or dif- 
ferents taggers (training set of 800,000 words). 
be that which has aI)peared more often in the train- 
ing set. Tile second method corresponds to a tagger 
based on a bigram model (BIG). The third one uses 
the Integrated LM described in this pai)er (BIG- 
BIG). The tagging accuracy for BIG and BIG-BIG 
was close, 96.9% and 96.8% respectively, whereas 
without the use of the language model (LEX), tile 
tagging accuracy was 2.5 points lower. The trend in 
all the cases was that an increment in the size of the 
training set resulted in an increase in the tagging 
accuracy. After 300,000 training words, the result 
became stabilized. 
In Figure 5, we show the precision (#correct 
proposed NP/#proposed  NP) and recall (#correct 
proposed NP/#NP in the reference) rates for NP 
chunking. The results obtained using the Integrated 
LM were very satisfactory achieving a precision rate 
of 94.6% and a recall rate of 93.6%. The perfor- 
mance of the NP chunker improves as the train- 
ing set size increases. This is obviously due to the 
fact that tile model is better learnt when the size 
of the training set increases, and the tagging error 
decreases as we have seen above. 
The usual sequential 1)rocess for chunking a sen- 
tence can also be used. That is, first we tag the sen- 
tence and then we use the Integrated LM to carry 
out the chunking. In this case, only tim contextual 
t)robabilities are taken into account in the decoding 
618 
1)recess. In Table 2, we show the most relevant re- 
suits that we obtained for tagging and tbr NP chunk- 
ing. The first row shows the result when the tagging 
and the chunking are done in a integrated way. The 
following rows show the performmme of the sequen- 
tial process using different aggers: 
? LEX: it takes into account only lexical proba- 
t)ilities. In this case, the tagging accuracy was 
94.3%. 
? BIG: it is based on a bigram model that 
achieved an accuracy of 96.9%. 
? IDEAL: it siinulates a tagger with an accuracy 
rate of 100%. To do this, we used the tagged 
sentences of the WSJ corlms directly. 
These results confirm that precision and recall 
rates increase when the accuracy of the tagger is 
beN;er. The pert'ormmme of 1;he, se(tuential process 
(u:dng the BIG tagger) is slightly 1letter than the 
pet'formance of the integrated process (BIG-BIG). 
We think that this is 1)robably b(;cause of the way 
we combined the I)robabilities of t;he ditthrent mod- 
els. 
4 Conclusions and Future  Work  
In this 1)aper, we have t)rcscntcd a system tot" Tag- 
ging and Chunldng based on an Integrated Lan- 
guage Model that uses a homogeneous tbrmalism 
(finite-state machine) to combine different knowl- 
edge sources: lexical, syntacti(:al and contextual 
inodels. It is feasible l)oth in terms of 1)erfl)rmanc(; 
and also in terms of computational (:tliciency. 
All the models involv(:d are learnt automatically 
fi'om data, so the system is very tlexibte and 1)ortable 
and changes in the reference language., lexical tags 
or other kinds of chunks can be made in a direct way. 
The tagging accuracy (96.9% using BIG and 
96.8% using BIG-BIG) is higher tlmn other similar 
alIl)roaches. This is because we have used the tag 
di('tionary (including the test set in it) to restrict 
the possible tags for unknown words, this assmnp- 
lion obviously in(:rease the rates of tagging (we have 
not done a quantitative study of this factor). 
As we have mentioned above, the comparison with 
other approaches i  ditficult due mnong other reasons 
to tim following ones: the definitions of base NP are 
not always the stone, the sizes of the train and the 
test sets are difl'erent and the knowledge sources used 
in the learning process are also different. The pre- 
cision for NP-chunking is similm' to other statistical 
at)preaches t)resented in section 1, tbr 1)oth the in- 
tegrated process (94.6%) and l;tm sequential process 
using a tagger based on 1)igrams (94.9%). The recall 
rate is slightly lower than for some apl)roaches using 
the integrated system (93.6%) and is similar for the 
sequential process (94.1%). When we used the se- 
quential system taking an error ti'ee input (IDEAL), 
the performance of the system obviously increased 
(95.5% precision and 94.7% recall). These results 
show the influence of tagging errors on the process. 
Nevertheless, we are studying why the results lie- 
tween the integrated process and the sequential pro- 
cess are diflbrent. We are testing how the introduc- 
tion of soIne adjustnmnt factors among the models 
tk)r we, ighting the difl'erent 1)robability distribution 
can lint)rove the results. 
The models that we have used in this work, are ill- 
grams, but trigrams or any stochastic regular model 
can be used. In this respect, we have worked on a 
more coml)lex LMs, formalized as a. finite-state au- 
tomata which is learnt using Grammatical Inference 
tectufiques. Also, our ai)l)roach would benefit fl'om 
the inclusion of lexical-contextual in%rmation into 
the LM. 
5 Acknowledgments  
This work has been partially supl)orted 1)y the 
Stmnish I{esem'ch Projct:t CICYT (TIC97-0671-C02- 
O11O2). 
References  
S. Abney. 1991. Parsing by Chunks. R. Berwick, S. 
Almey and C. Tcnny (eds.) Principle -based Pars- 
ing.  Kluwer Acadenfic Publishers, Dordrecht. 
S. Almey. 1996. Partial Parsing via Finit('.-Sta~e 
Cascades. In Proceedings of the ES,S'LLI'96 Ro- 
bust Parsinfl Workshop, l?rague, Czech l{elmblie. 
S. Argamon, I. Dagan, and Y. Krymolowski. 1.998. 
A Memory based Approach to Learning Shallow 
Natural Language, Patterns. In l~roceedi'ngs of
t,h,e joint 17th, International Conference on Com- 
putational Linguistics and 36th Annual Meeting 
of the Association for Computational Linguistics, 
COLING-ACL, pages 67 73, Montrdal, Canada. 
S. At-Mokhtar and ,l.P. Chanod. 1997. Incremen- 
tal Finite-State Parsing. In Proceedings of the 5th, 
Conference on Applied Natural Language Process- 
ing, \Vashington D.C., USA. 
D. Bourigault. 1992. Surface Grmnmatical Anal- 
,),sis for tim Extraction of ~l~.~rminological Noml 
Phrases. In Proceedings of the 15th International 
Conference on Computational Linguistics, pages 
977-981. 
Eric Brill and Jun Wu. 1998. Classifier Combi- 
nation for hnproved Lexical Disambiguation. In 
Procccdings of the joint 17th, International Con- 
fcrcncc on Computational Linguistics and 36th 
Annual Meeting of thc Association for Computa- 
tional Linguistics, COLING-ACL, pages 191-195, 
Montrdal, Canada. 
E. Brill. 1995. Transibnnation-based Error-driven 
Learning and Natural Language Processing: A 
619 
Case Study in Part-of-sI)eech Tagging. Compu- 
tational Linguistics, 21 (4) :543-565. 
C. Car(lie and D. Pierce. 1998. Error-Driven Prun- 
ning of Treebank Grammars for Base Noun Phrase 
Identification. In Proceedings of the joint 17th 
International Conference on Computational Lin- 
guistics and 36th Annual Meeting of the Asso- 
ciation for Computational Linguistics, COLING- 
ACL, pages 218 224, Montrdal, Canada, August. 
K. W. Church. 1988. A Stochastic Parts Program 
and Noun Phrase Parser for Unrestricted Text. 
In Proceedings of the 1st Conference on Applied 
Natural Language Processing, ANLP, pages 136- 
143. ACL. 
P. Clarksond and R. Ronsenfeld. 1997. Statistical 
Language Modeling using the CMU-Cambridge 
Toolkit. In Procccdinfls of Eurospccch, Rhodes, 
C,-reece. 
D. Cutting, J. Kut)iec , J. Pederson, and P. Nil)un. 
1992. A Practical Part-of-speech Tagger. In Pfv- 
cccdings of the 3rd Confcrcnce oft Applied Natu- 
ral Language Processing, ANLP, pages 133 140. 
ACL. 
W. Daelelnans, J. Zavrel, P. Berck, and S. Gillis. 
1996. MBT: A MeInory-Based Part of speech 
Tagger Generator. In Proceedings of the /tth 
Workshop on Very Large Cmpora, pages 14-27, 
Copenhagen, Denmark. 
W. Daelemans, S. Buchholz, and J. Veenstra. 1999. 
Memory-Based Shallow Parsing. In Proceedings 
of EMNLP/VLC-99, pages 239 246, University of 
Maryla.nd, USA, June. 
E. Ejerhed. 1988. Finding Clauses in Unrestricted 
Text by Finitary and Stochastic Methods. In Pro- 
cccdings of Second Confcrcncc on Applied Natural 
Language Processing, pages 219-227. ACL. 
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. 
Improving Data Driven Wordclass Tagging by 
System Combination. In Proceedings of the joint 
17th International Confcr'cncc oft Computational 
Linguistics and 36th Annual Mccting of the Asso- 
ciation for Computational Linguistics, COLING- 
ACL, pages 491-497, Montrdal, Canada, August. 
S. M. Katz. 1987. Estimation of Probabilities from 
Sparse Data for tile Language Model Component 
of a Speech Recognizer. IEEE T~nnsactions on 
Acoustics, Speech and Signal Processing, 35. 
D. M. Magerman. 1996. Learning Grammatical 
Structure Using Statistical Decision-Trees. In 
Proceedings of the 3rd International Colloquium 
on GTnmmatical Inference, ICGI, pages 1-21. 
Springer-Verlag Lecture Notes Series in Artificial 
Intelligence 1147. 
M. P. Marcus, M. A. Marcinkiewicz, and B. San- 
torini. 1993. Building a Large Annotated Cortms 
of English: Tile Penn Treebank. Computational 
Linguistics, 19(2). 
Llu/s Mhrquez and Horacio RodHguez. 1998. Part- 
of Speech T~gging Using Decision Trees. In C. 
Nddellee and C. Rouveirol, editor, LNAI 1398: 
Proceedings of thc lOth European Conference 
on Machine Learning, ECML'98, pages 25-36, 
Chemnitz, GermNly. Springer. 
B. Merialdo. 1994. Tagging English Text with a 
Probabilistic Model. Computational Linguistics, 
20(2):155-171. 
F. Pla and N. Prieto. 1998. Using Grammatical 
Inference Methods tbr Automatic Part of speech 
Tagging. In Proceedings of 1st International Con- 
ference on Language Resources and Evaluation, 
LREC, Granada, Spain. 
L. Ramshaw and M. Marcus. 1995. Text Chunking 
Using ~lYansfbrmation-Based Learning. In Pro- 
cccdings of third Workshop on Very Large Col 
pora, pages 82 94, June. 
A. Ratnapm'khi. 1996. A Maximum Entrol)y Part 
of-speech Tagger. In Proceedings of the 1st Con- 
fcrcncc on Empirical Methods in Natural Lan- 
guagc Processing, EMNLP. 
Atro Voutilainen and Llufs Padrd. 1997. Develol)- 
inn a Hybrid NP Parser. In Proceedings ofthe 5th 
Conference on Applied Natural Language Prvecss- 
ing, ANLP, pages 80 87, Washington DC. ACL. 
Atro Voutilainen. 1993. NPTool, a Detector of En- 
glish Noun Phrases. In Proceedings of the Work- 
shop on Very Lafflc Corpora. ACL, June. 
Atro Voutilainen. 1995. A Syntax-Based Part o f  
speech Analyzer. In Prvcccdings of the 7th Con- 
ference of the European Ch, aptcr of the Association 
for Computational Linguistics, EACL, Dut)lin, 
h'eland. 
R. Weischedel, R. Schwartz, J. Pahnueci, M. Meteer, 
and L. Ramshaw. 1993. Coping with Ambiguity 
and Unknown \~or(ls through Probabilistic Mod- 
els. Computational Linguistics, 19(2):260-269. 
620 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 148-150, Lisbon, Portugal, 2000. 
Improving Chunking by Means of Lexical-Contextual Information 
in Statistical Language Models 
Fer ran  P la  and Anton io  Mo l ina  and Nat iv idad  Pr ie to  
Universitat Polit~cnica de Valencia 
Camf de Vera s/n 
46020 Val~ncia (Spain) 
{fpla, amolina, nprieto}@dsic.upv.es 
1 In t roduct ion  
In this work, we present a stochastic approach 
to shallow parsing. Most of the current ap- 
proaches to shallow parsing have a common 
characteristic: they take the sequence of lex- 
ical tags proposed by a POS tagger as input 
for the chunking process. Our system produces 
tagging and chunking in a single process using 
an Integrated Language Model (ILM) formal- 
ized as Markov Models. This model integrates 
several knowledge sources: lexical probabilities, 
a contextual Language Model (LM) for every 
chunk, and a contextual LM for the sentences. 
We have extended the ILM by adding lexical in- 
formation to the contextual LMs. We have ap- 
plied this approach to the CoNLL-2000 shared 
task improving the performance of tile chunker. 
2 Overv iew o f  the  sys tem 
The baseline system described in (Pla et al, 
2000a) uses bigrams, formalized as finite-state 
automata. It is a transducer composed of two 
levels (see Figure 1). The upper one (Figure la) 
represents he contextual LM for the sentences. 
The symbols associated to the states are POS 
tags (Ci) and chunk descriptors (Si). The lower 
one modelizes the different chunks considered 
(Figure lb). In this case, the symbols are the 
POS tags (Ci) that belong to the correspond- 
ing chunk (Si). Next, a regular substitution of 
the lower models into the upper level is made 
(Figure lc). In this way, we get a single Inte- 
grated LM which shows the possible concate- 
nations of lexical tags and chunks. Also, each 
state is relabeled with a tuple (Ci, Sj) where 
Ci E g and Sj E S. g is the POS tag set used 
and S = {\[Si, Si\], Si, S0} is the chunk set de- 
fined. \[Si and Si\] stand for the initial and the 
final state of chunk whose descriptor is Si. The 
label Si is assigned to those states which are in- 
side Si chunk, and So is assigned to those states 
which are outside of any chunk. All the LMs 
involved have been smoothed by using a back- 
off technique (Katz, 1987). We have not spec- 
ified lexical probabilities in every state of the 
different contextual models. We assumed that 
P(WjI(Ci, Si)) = P(WjlCi ) for every Si E S. 
Once the integrated transducer has been 
made, the tagging and shallow parsing process 
consists of finding the sequence of states of max- 
imum probability on it for an input sentence. 
Therefore, this sequence must be compatible 
with the contextual, syntactical and lexical con- 
straints. This process can be carried out by 
dynamic programming using the Viterbi algo- 
r ithm (Viterbi, 1967), which has been appropri- 
ately modified to use our models. From the dy- 
namic programming trellis, we can obtain the 
maximum probability path for the input sen- 
tence through the model, and thus the best se- 
quence of lexical tags and the best segmentation 
in chunks, in a single process. 
3 Spec ia l i zed  Contextua l  Language 
Mode ls  
The contextual model for the sentences and the 
models for chunks (and, therefore, the ILM) can 
be modified taking into account certain words 
in the context where they appear. This spe- 
cialization us allows to set certain contextual 
constraints which modify the contextual LMs 
and improve the performance of the chunker (as 
shown below). This set of words can be defined 
using some heuristics uch as: the most frequent 
words in the training corpus, the words with a 
higher tagging error rate, the words that belong 
to closed classes (prepositions, pronouns, etc.), 
or whatever word chosen following some linguis- 
148 
(a )  Contextual LM - . .  
(b )  LM for Chunks (S  l ) 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . .  
(c) lntegratedLM 
~ I I i I 
Figure 1: Integrated Language Model for Tagging and Chunking. 
tic criterion. 
To do this, we added to the POS tag set the 
set of structural tags (Wi, Cj) for each special- 
ized word Wi in all of their possible categories 
Cj. Then, we relabelled the training corpus: if 
a word Wi was labelled with the POS tag Cj, 
we changed Cj for the pair (Wi, Cj). The learn- 
ing process of the bigram LMs was carried out 
from this new training data set. 
The Contextual LMs obtained has some spe- 
cific states which are related to the specialized 
words. In the basic Language Model (ILM), a 
state was labelled by (Ci, Sj). In the specialized 
ILM, a state was specified for a certain word Wk 
(only if the Wk word belongs to the category 
Ci). In this way, the state is relabelled with the 
tuple (Wk, Ci, Sj) and only the word Wk can be 
emitted with a probability equal to 1. 
4 Experimental Work 
We applied both approaches (ILM and spe- 
cialized ILM) using the training and test data 
of the CoNLL-2000 shared task (http://lcg- 
www.uia.ac.be/conll2000). We also evaluated 
how the performance of the chunker varies when 
we modify the specialized word set. Neverthe- 
less, the use of our approach on other corpora 
(including different languages), other lexical tag 
sets or other kinds of chunks can be done in a 
direct way. 
Although our system is able to carry out tag- 
ging and chunking in a single process, we will 
not present agging results for this task, as the 
POS tags of the data set used are not supervised 
and, therefore, a comparison is not possible. 
We would like to point out that we have simu- 
lated a morphological nalyzer for English. We 
have constructed a tag dictionary with the lex- 
icon of the training set and the test set used. 
This dictionary gave us the possible lexical tags 
for each word from the corpus. In no case, was 
the test used to estimate the lexical probabili- 
ties. 
As stated above, several criterion can be cho- 
sen to define the set of specialized words. We 
have selected the most frequent words in the 
training data set. We have not taken into ac- 
count certain words such as punctuation sym- 
bols, proper nouns, numbers, etc. This fact did 
not decrease the performance of the chunker and 
also reduced the number of states of the contex- 
tual LMs. Figure 2 shows how the performance 
of the chunker (Fz=I) improves as a function of 
the size of the specialized word set. The best re- 
sults were obtained with the set of words whose 
frequency in the training corpus was larger than 
80 (about 470 words). We obtained similar re- 
sults when only considering the words of the 
training set belonging to closed classes (that, 
about, as, if, out, while, whether, for, to, ...). 
In Table 1 we present he results of chunk- 
ing with the specialized ILM. When comparing 
these results with the results obtained using the 
basic ILM, we observed that, in general, the F- 
149 
score was improved for each chunk. The best 
improvement was observed for SBAR (from 0.37 
to 79.46), PP (from 88.94 to 95.51) and PRT 
(38.82 to 66.67). 
5 Conc lus ions  
In this paper, we have presented a system for 
Tagging and Chunking based on an Integrated 
Language Model that uses a homogeneous for- 
malism (finite-state machine) to combine differ- 
ent knowledge sources. It is feasible both in 
terms of performance and also in terms of com- 
putational efficiency. 
All the models involved are learnt automat- 
ically from data, so the system is very flexible 
with changes in the reference language, changes 
in POS tags or changes in the definition of 
chunks. 
Our approach allows us to use any regular 
model which has been previously defined or 
learnt. In previous works, we have used bi- 
grams (Pla et al, 2000a), and we have com- 
bined them with other more complex models 
which had been learnt using grammatical in- 
ference techniques (Pla et al, 2000b). In this 
work, we used only bigram models improved 
with lexical-contextual information. 
The Ff~ score obtained increased from 86.64 to 
90.14 when we used the specialized ILM. Never- 
( 
I I I I I I I I I 
50  100  150  L200 250  300  350  400  450  500  
#SPECIAL IZED WORDS 
test data precision recall 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
72.89 % 
79.65% 
40.00% 
lOO.OO% 
0.o0% 
90.28% 
95.89% 
60.31% 
82.07% 
91.53% 
66.89 % 
74.13% 
66.67% 
100.00% 
0.00% 
89.41% 
95.14% 
74.53% 
77.01% 
91.58% 
F/3=l 
69.76 
76.79 
50.00 
100.00 
0.00 
89.84 
95.51 
66.67 
79.46 
91.55 
all 90.63% 89.65% 90.14 
Table 1: Chunking results using specialized ILM 
(Accuracy= 93.79%) 
theless, we believe that the models could be im- 
proved with a more detailed study of the words 
whose contextual information is really relevant 
to tagging and chunking. 
6 Acknowledgments  
This work has been partially supported by the 
Spanish Research Project CICYT (TIC97-0671- 
C02-01/02). 
Re ferences  
S. M. Katz. 1987. Estimation of Probabilities from 
Sparse Data for the Language Model Component 
of a Speech Recognizer. IEEE Transactions on 
Acoustics, Speech and Signal Processing, 35. 
F. Pla, A. Molina, and N. Prieto. 2000a. Tagging 
and Chunking with Bigrams. In Proceedings of the 
COLING-2000, Saarbrficken, Germany, August. 
F. Pla, A. Molina, and N. Prieto. 2000b. An Inte- 
grated Statistical Model for Tagging and Chunk- 
ing Unrestricted Text. In Proceedings of the Text, 
Speech and Dialogue 2000, Brno, Czech Republic, 
September. 
A. J. Viterbi. 1967. Error Bounds for Convolutional 
Codes and an Asymptotically Optimal Decoding 
Algorithm. IEEE Transactions on Information 
Theory, pages 260-269, April. 
Figure 2: F-score as a function of the number 
of specialized words in the ILM 
150 
 
			WSD system based on Specialized Hidden Markov Model (upv-shmm-eaw)
Antonio Molina, Ferran Pla and Encarna Segarra
Departament de Sistemes Informa`tics i Computacio?
Universitat Polite`cnica de Vale`ncia
Cam?? de Vera s/n Vale`ncia (Spain)
{amolina,fpla,esegarra}@dsic.upv.es
Abstract
We present a supervised approach to Word Sense
Disambiguation (WSD) based on Specialized Hid-
den Markov Models. We used as training data the
Semcor corpus and the test data set provided by
Senseval 2 competition and as dictionary the Word-
net 1.6. We evaluated our system on the English
all-word task of the Senseval-3 competition.
1 Description of the WSD System
We consider WSD to be a tagging problem (Molina
et al, 2002a). The tagging process can be formu-
lated as a maximization problem using the Hidden
Markov Model (HMM) formalism. Let O be the
set of output tags considered, and I , the input vo-
cabulary of the application. Given an input sen-
tence, I = i1, . . . , iT , where ij ? I , the tag-
ging process consists of finding the sequence of tags
(O = o1, . . . , oT , where oj ? O) of maximum
probability on the model, that is:
O? = arg max
O
P (O|I)
= arg max
O
(
P (O) ? P (I|O)
P (I)
)
; O ? OT (1)
Due to the fact that the probability P (I) is a con-
stant that can be ignored in the maximization pro-
cess, the problem is reduced to maximize the nu-
merator of equation 1. To solve this equation, the
Markov assumptions should be made in order to
simplify the problem. For a first-order HMM, the
problem is reduced to solve the following equation:
arg max
O
?
? ?
j:1...T
P (oj |oj?1) ? P (ij |oj)
?
? (2)
The parameters of equation 2 can be represented
as a first-order HMM where each state corresponds
to an output tag oj , P (oj |oj?1) represent the transi-
tion probabilities between states and P (ij |oj) rep-
resent the probability of emission of input symbols,
ij , in every state, oj . The parameters of this model
are estimated by maximum likelihood from seman-
tic annotated corpora using an appropriate smooth-
ing method (linear interpolation in our work).
Different kinds of available linguistic information
can be useful to solve WSD. The training corpus we
used provides as input features: words (W), lemmas
(L) and the corresponding POS tags (P); and it also
provides as output tags the WordNet senses.
WordNet senses can be represented by a sense key
which has the form lemma%lex sense. The high
number of different sense keys and the scarce an-
notated training data make difficult the estimation
of the models. In order to alleviate this sparness
problem we considered the lex sense field (S) of the
sense key associated to each lemma as the semantic
tag. This assumption reduces the size of the output
tag set and it does not lead to any loss of information
because we can obtain the sense key by concatenat-
ing the lemma to the output tag.
Therefore, in our system the input vocabulary is
I = W ? L ? P , and the output vocabulary is
O = S . In order to incorporate this kind of in-
formation to the model we used Specialized HMM
(SHMM) (Molina et al, 2002b). This technique
has been successfully applied to other disambigua-
tion tasks such as part-of-speech tagging (Pla and
Molina, 2004) and shallow parsing (Molina and Pla,
2002).
Other HMM-based approaches have also been
applied to WSD. In (Segond et al, 1997), they esti-
mated a bigram model of ambiguity classes from the
SemCor corpus for the task of disambiguating the
semantic categories corresponding to the lexicogra-
pher level. These semantic categories are codified
into the lex sense field. A second-order HMM was
used in (Loupy et al, 1998) in a two-step strategy.
First, they determined the semantic category associ-
ated to a word. Then, they assigned the most prob-
able sense according to the word and the semantic
category.
A SHMM consists of changing the topology of
the HMM in order to get a more accurate model
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
which includes more information. This is done by
means of an initial step previous to the learning pro-
cess. It consists of the redefinition of the input vo-
cabulary and the output tags. This redefinition is
done by means of two processes which transform
the training set: the selection process, which is ap-
plied to the input vocabulary, and the specialization
process, which redefines the output tags.
1.1 Selection process
The aim of the selection process is to choose which
input features are relevant to the task. This pro-
cess applies a determined selection criterion to I
that produces a new input vocabulary (I?). This new
vocabulary consists of the concatenation of the rel-
evant input features selected.
Taking into account the input vocabulary I =
W ?L?P , some selection criteria could be as fol-
lows: to consider only the word (wi), to consider
only the lemma (li), to consider the concatenation
of the word and its POS1 (wi ? pi), and to consider
the concatenation of the lemma and its POS (li ? pi).
Moreover, different criteria can be applied depend-
ing on the kind of word (e.g. distinguishing content
and non-content words).
For example, for the input word interest, which
has an entry in WordNet and whose lemma and POS
are interest and NN (common noun) respectively,
the input considered could be interest?1. For a non-
content word, such as the article a, we could con-
sider only its lemma a as input.
1.2 Specialization process
The specialization process allows for the codifica-
tion of certain information into the context (that is,
into the states of the model). It consists of redefin-
ing the output tag set by adding information from
the input. This redefinition produces some changes
in the model topology, in order to allow the model
to better capture some contextual restrictions and to
get a more accurate model.
The application of a specialization criterion to O
produces a new output tag set (O?), whose elements
are the result of the concatenation of some relevant
input features to the original output tags.
Taking into account that the POS input feature is
already codified in the lex sense field, only words
or lemmas can be considered in the specialization
process (wi? lex sensei or li? lex sensei).
This specialization can be total or partial depend-
ing on whether we specialize the model with all the
elements of a feature or only with a subset of them.
1We mapped the POS tags to the following tags: 1 for
nouns, 2 for verbs, 3 for adjectives and 4 for adverbs.
For instance, the input token interest?1 is tagged
with the semantic tag 1:09:00:: in the training data
set. If we estimate that the lemma interest should
specialize the model, then the semantic tag is rede-
fined as interest?1:09:00::. Non-content words, that
share the same output tag (the symbol notag in our
system), could be also considered to specialize the
model. For example, for the word a, the specialized
output tag associated could be a?notag.
1.3 System scheme
The disambiguation process is presented in (Figure
1). First, the original input sentence (I) is processed
in order to select its relevant features, providing the
input sentence (I?). Then, the semantic tagging is
carried out through the Viterbi algorithm using the
estimated SHMM. WordNet is used to know all the
possible semantic tags associated to an input word.
If the input word is unknown for the model (i.e., the
word has not been seen in the training data set) the
system takes the first sense provided by WordNet.
The learning process of a SHMM is similar to the
learning of a basic HMM. The only difference is that
SHMM are based on an appropriate definition of the
input information to the learning process. This in-
formation consists of the input features (words, lem-
mas and POS tags) and the output tag set (senses)
provided by the training corpus. A SHMM is built
according to the following steps (see Figure 2):
1. To define which available input information is
relevant to the task (selection criterion).
2. To define which input features are relevant to
redefine or specialize the output tag set (spe-
cialization criterion).
3. To apply the chosen criteria to the original
training data set to produce a new one.
4. To learn a model from the new training data
set.
5. To disambiguate a development data set using
that model.
6. To evaluate the output of the WSD system in
order to compare the behavior of the selected
criteria on the development set.
These steps are done using different combina-
tions of input features in order to determine the best
selection criterion and the best total specialization
criterion. Once these criteria are determined, some
partial specializations are tested in order to improve
the performance of the model.
Selection
      of Relevant
Features
Disambiguated sentence
WSD
HMM WORDNETSelection
criterion
I~I
Original Input sentence Input sentence
Figure 1: System Description
Specialization
criterion (2)
SET
TRAINING
SET
DEVELOPMENT
   Output Tags
      of
Specialization
REFERENCE SET
DEVELOPMENT
Selection
      of Relevant
Features
HMM WORDNET
Training set
sentence
Input
Selection
Disambiguated
sentence
criterion (1)
New
       Model
      the
Learning
4 6
WSD
5
3
Evaluation
Figure 2: Learning Phase Description
2 Experimental Work
We used as training data the part of the SemCor cor-
pus which is semantically annotated and supervised
for nouns, verbs, adjectives and adverbs (that is, the
files contained in the Brown1 and the Brown2 fold-
ers of SemCor corpus), and the test data set provided
by Senseval-2. We used 10% of the training corpus
as a development data set in order to determine the
best selection and specialization criteria.
In the experiments, we used WordNet 1.6 as a
dictionary which supplies all the possible semantic
senses for a given word. Our system disambiguated
all the polysemic lemmas, that is, the coverage of
our system was 100% (therefore, precision and re-
call were the same). For unknown words (words
that did not appear in the training data set), we as-
signed the first sense in WordNet.
The best selection criterion determined from the
experimental work on the development set is as fol-
lows: if a word wi has a sense in WordNet we con-
catenate the lemma (li) and the POS (pi) associ-
ated to the word (wi) as input vocabulary. For non-
content words, we only consider their lemma (li) as
input.
The best specialization criterion consisted of se-
lecting the lemmas whose frequency in the training
data set was higher than a certain threshold (other
specialization criteria could have been chosen, but
frequency criterion usually worked well in other
tasks as we reported in (Molina and Pla, 2002)). In
order to determine which threshold maximized the
performance of the model, we conducted a tuning
experiment on the development set. The best per-
formance was obtained using the lemmas whose fre-
quency was higher than 20 (about 1,600 lemmas).
The performance of our system on the Senseval 3
data test set was 60.9% of precision and recall.
3 Concluding remarks
In our WSD system, the choice of the best special-
ization criterion is based on the results of the system
on the development set. The tuning experiments in-
cluded totally specialized models, which is equiva-
lent to consider the sense keys as the output vocab-
ulary, non-specialized models, which is equivalent
to consider the lex senses as the output vocabulary,
and partially specialized models using different sets
of lemmas.
For the best specialization criterion, we have not
studied the linguistic characteristics of the different
groups of synsets associated to the same lex sense
for non-specialized output tags. We think that we
could improve our WSD system through a more ad-
equate definition of the selection and specialization
criteria. This definition could be done using seman-
tic knowledge about the domain of the task.
4 Acknowledgments
This work has been supported by the Spanish
research projects CICYT TIC2003-07158-C04-03
and TIC2003-08681-C02-02.
References
C. Loupy, M. El-Beze, and P. F. Marteau. 1998.
Word Sense Disambiguation using HMM Tag-
ger. In Proceedings of the 1st International Con-
ference on Language Resources and Evaluation,
LREC, pages 1255?1258, Granada, Spain, May.
Antonio Molina and Ferran Pla. 2002. Shallow
Parsing using Specialized HMMs. Journal of
Machine Learning Research, 2:595?613.
Antonio Molina, Ferran Pla, and Encarna Segarra.
2002a. A Hidden Markov Model Approach to
Word Sense Disambiguation. In Proceedings
of the VIII Conferencia Iberoamericana de In-
teligencia Artificial, IBERAMIA2002, Sevilla,
Spain.
Antonio Molina, Ferran Pla, and Encarna Segarra.
2002b. Una formulaci o?n unificada para resolver
distinto problemas de ambigu?edad en PLN. Re-
vista para el Procesamiento del Lenguaje Natu-
ral, (SEPLN?02), Septiembre.
Ferran Pla and Antonio Molina. 2004. Improv-
ing Part-of-Speech Tagging using Lexicalized
HMMs. Natural Language Engineering, 10. In
press.
F. Segond, A. Schiller, G. Grefenstette, and J-P.
Chanod. 1997. An Experiment in Semantic Tag-
ging using Hidden Markov Model Tagging. In
Proceedings of the Joint ACL/EACL Workshop
on Automatic Information Extraction and Build-
ing of Lexical Semantic Resources, pages 78?81,
Madrid, Spain.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 183?192, Dublin, Ireland, August 23-29 2014.
Political Tendency Identification in Twitter
using Sentiment Analysis Techniques
Ferran Pla and Llu??s-F. Hurtado
Departament de Sistemes Inform`atics i Computaci?o
Universitat Polit`ecnica de Val`encia
Cam?? Vera s/n 46022 Val`encia (Spain)
{lhurtado|fpla}@dsic.upv.es
Abstract
This paper describes an approach for political tendency identification of Twitter users. We define
some metrics that take into account the polarity of the political entities in the tweets of each user.
To obtain this polarities we present the sentiment analysis system developed. The evaluation was
performed on the general corpus developed at TASS2013 workshop for Spanish. To our knowl-
edge, the results obtained for the sentiment analysis task and the political tendency identification
task are the best results published until now using this data set.
1 Introduction
Social media are usually used to express opinions and feelings about companies, products, services,
hobbies, politics, etc. Therefore, enterprises, organizations, governments, and different groups in general
have shown interest in the opinions that users have for their activities. They are also interested to known
the way users use these media, the communication behaviour, and some users attributes such as gender,
age, geographical location, political orientation, etc. In general, the main aim is to provide personalized
services, particularized offers, or simply to know what people think about something in order to improve
their activities.
The scientific community has made a great effort to provide effective solutions to analyse, structure,
and process the large amount of on-line reviews in social media. A wide set of techniques of Senti-
ment Analysis (SA) are used in micro-blogging texts to extract the polarity (positive, negative, mixed or
neutral) that users express in these texts. In this respect, Twitter has become a popular micro-blogging
site in which users express their opinions on a variety of topics in real time. The texts used in Twit-
ter are called tweets, which are short texts of a maximum of 140 characters and a language that does
not have any restriction on the form and content. The nature of these texts poses new challenges for
researchers in Natural Language Processing (NLP). In some cases, the tweets are written with ungram-
matical sentences with a lot of emoticons, abbreviations, specific terminology, slang, etc. Therefore, the
usual techniques of NLP must be adapted to these characteristics of the language, and new approaches
must be proposed in order to successfully address this problem. NLP tools like POS taggers, parsers, or
Named Entity Recognition (NER) tools usually fail when processing tweets because they generally are
trained on grammatical texts and they perform poorly in micro-blogging texts.
In this work we present a system for addressing the task of political tendency identification of Twitter
users based on SA techniques. For each user, we collect all their tweets and we extract all the entities
related to the political subject. Then, we automatically assign a polarity to these entities and we define a
political tendency metric that uses this entity polarity information combined with another tendency metric
for classifying the political tendency of each user in four categories: Left, Right, Center, or Undefined.
The evaluation of our system is performed on the General Corpus, a corpus of Spanish tweets provided
by the organization of the TASS2013 workshop.
The paper is organized as follows. In section 2 we present relevant works for Twitter user classification
and Sentiment Analysis. In Section 3 we present a description of the corpus used to evaluate our user
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
183
political tendency system. This system is based on SA techniques. A description of our SA system is
described in section 4. In Section 5 we describe the way we classify users according to their political
leading. The evaluation and discussion of the results obtained are presented in section 6. Finally, in
section 7 we present some conclusions and possible directions for future works.
2 Related works
The different approaches for estimating the political leaning of Twitter users explore features that range
from text content, users behavior (taking into account the tweets and retweets information) and the
Twitter structure (by considering the followers users, following users, etc.). An interesting study of some
useful features to classify latent users attributes (gender, age, regional origin, and political orientation) is
presented in (Rao et al., 2010). In (Conover et al., 2011a; Conover et al., 2011b) and study of the political
alignment of Twitter users is performed by analyzing the way users communicates by means of retweets
and user mentions. In (O?Connor et al., 2010a) SA techniques are used to determine the positive and
negative polarity of Twitter messages. They also study the connexion between these polarities and the
public opinion derived from traditional polling in order to substitute or complement them. (Pennacchiotti
and Popescu, 2011) present a machine learning approach to Twitter user classification in democrats or
republicans. With respect to the linguistic content they considered prototypical words and hashtags that
are common in democrats or republicans users which provides clues for the classification. They also use
SA tecniques based on lexicons for the classification task. In (Boutet et al., 2012) polical leading of users
is performed by counting the amount of tweets related to political parties analysing the hashtags. They
also consider the interaction among parties by analyzing the retweets and mentions. Users interaction by
analysing tweets and retweets is also the main idea of the work presented in (Wong et al., 2013).
In (Cohen and Ruths, 2013) previous works on political orientation of Twitter users are analyzed to
conclude that the accuracy results reported are overstimated do to the way the data sets are constructed.
When these approaches are applied to normal Twitter users accuracy results significantly decrease.
Sentiment Analysis (SA) has been widely studied in the last decade in multiple domains. Most work
focuses on classifying the polarity of the texts as positive, negative, mixed, or neutral. The pioneering
works in this field used supervised (Pang et al., 2002) or unsupervised (knowledge-based) (Turney, 2002)
approaches. In (Pang et al., 2002), the performance of different classifiers on movie reviews was eval-
uated. In (Turney, 2002), some patterns containing POS information were used to identify subjective
sentences in reviews to then estimate their semantic orientation.
The construction of polarity lexicons is another widely explored field of research. Opinion lexicons
have been obtained for English language (Liu et al., 2005) (Wilson et al., 2005) and also for Spanish
language (Perez-Rosas et al., 2012). A good presentation of the SA problem and a description of the
state-of-the-art of the more relevant approaches to SA can be found in (Liu, 2012). An overview of
the current state of different approaches to the subjectivity and SA task is presented in (Montoyo et al.,
2012).
Research works about SA on Twitter are much more recent. Twitter appeared in the year 2006 and
the early works in this field are from 2009 when Twitter started to achieve popularity. Some of the most
significant works are (Barbosa and Feng, 2010), (Jansen et al., 2009), and (O?Connor et al., 2010b). A
survey of the most relevant approaches to SA on Twitter can be see in (Mart??nez-C?amara et al., 2012) ,
(Vinodhini and Chandrasekaran, 2012). The SemEval2013 competition has also dedicated a specific task
for SA on Twitter (Wilson et al., 2013), which shows the great interest of the scientific community in this
field. The TASS2013 workshop has proposed different tasks for SA and political tendency identification
focused on the Spanish language (Villena-Rom?an and Garc??a-Morera, 2013).
3 The Corpus
The General Corpus of TASS2013
1
(Villena-Rom?an and Garc??a-Morera, 2013) contains approximately
68000 Twitter messages (tweets) written in Spanish (between November 2011 and March 2012) by 158
well-known personalities of the world of politics, economy, communication, mass media, and culture.
1
This corpus is freely available on the web page of TASS2013 (http://www.daedalus.es/TASS2013).
184
The corpus is encoded in XML. Each tweet includes its ID (tweetid), the creation date (date), and
the user ID (user). It is tagged with its global polarity using N and N+ labels for negative polarity with
different intensity, P and P+ labels for positive polarity with different intensity, and the NEU label for
neutral polarity. Label NONE was used to represent tweets with no polarity at all. Moreover, the polarity
to the entities that are mentioned in the tweet was also included. The level of agreement of the expressed
sentiment is annotated both for global and entity level. Also, a selection of a set of topics was made
based on the thematic areas covered by the corpus, such as politics, soccer, literature, entertainment, etc.
Each message is also assigned to one or several of these topics.
N N+ NEU NONE P P+
training 1,335 (18.49%) 847 (11.73%) 670 ( 9.28%) 1,483 (20.54%) 1,232 (17.07%) 1,652 (22.88%)
test 11,287 (18.56%) 4,557 ( 7.50%) 1,305 ( 2.15%) 21,416 (35.22%) 1,488 ( 2.45%) 20,745 (34.12%)
Table 1: The distribution of the polarity of the tweets in the corpus.
Table 1 shows the distribution of tweets per polarity in the corpus. It is divided into two sets: training
(about 10%, 7219 tweets) and test (about 90%, 60798 tweets). It can be observed that this distribution
is not balanced for the different polarities. Finally, each user from the test set of the General corpus is
labeled with their political tendency in four possible values: Left, Right, Centre, and Undefined.
4 Description and Evaluation of the Sentiment Analysis System
Figure 1 shows an overview of our system for the SA problem. The system consists of 4 modules. The
first module is the Pre-processing module, which performs the tokenization, lemmatization, and Named
Entities recognition of the input tweet. A lemma reduction and a POS tagging process is also carried
out in this module. The second module is optional. It allows us to obtain the polarity of the entities
contained in the tweet. If we omitted this step the global polarity of the tweet is obtained. The third
module is the Feature Extraction module, which selects the features from the pre-processed tweet (or
from the segments of tweets) and obtains a feature vector. Some features require the use of a polarity
lexicon of lemmas and words. To determine the best features, a tuning process is required during the
training phase. The fourth module is the Polarity Classifier module, which uses a classifier (learned from
feature vectors of the training set) to assign a polarity label to the tweet.
Figure 1: Sentiment Analysis System Overview
4.1 Pre-processing of Tweets
Before addressing the SA task, it is necessary to make a proper tokenization of the tweets. Although there
are a lot of tokenizers available on the web, they need to be adapted in order to address the segmentation
of tokens of a tweet. Furthermore, most of these resources are for the English language, which adds a
degree of difficulty for their use in processing Spanish tweets.
185
Moreover, the use of NLP resources such as stemmers, POS taggers, parsers, NER systems, Word
Sense Disambiguation (WSD) systems, etc. are impractical if the characteristics of the tweets are not
taken into account. Therefore, an adjustment and adaptation must be made for the Twitter domain.
In our system, we decided to use and adapt available tools for tokenization, lemmatization, NER,
and POS tagging. We adapted the package Tweetmotif
2
that is described in (O?Connor et al., 2010b)
to process Spanish tweets. We also used Freeling
3
(Padr?o and Stanilovsky, 2012) (with the appropri-
ate modifications for handling Twitter messages) for stemming, Named Entity Recognition, and POS
tagging.
We added some functions to process special tokens (e.g., grouping all hashtags into a single token,
grouping all web addresses into a single token or grouping all url into a single token). We also grouped
the dates into a single token, the numbers into a single token, and the punctuation marks into a single
token.
4.2 The Segmenter
For the proposed approach we need to determine the polarity of political entities that contains a tweet.
It is because the polarity of each entity could be different of the global polarity of the tweet. In the
tweet of the corpus
4
: ?Rajoy?s government goes up the pensions. PSOE cuts back all things except the
unemployment.? we have two entities, Rajoy (the president of Spanish government from the right-wing
party PP) and PSOE (a Spanish left-wing party). This tweet is labeled with a neutral global polarity, but
each entity have a different polarity (ENTITY (Rajoy): Positive. ENTITY (PSOE): Negative).
Even for tweets with only one entity we must decide what fragments of text refers to that entity. In the
example: ?Rajoy already has been talking for an hour. Not saving anywhere only expenses, all reforms
cost a lot of money. Did he tell us something at the end??, to determine the polarity of entity Rajoy,
we must take into account all the tweet, because the two last sentences references to ENTITY(Rajoy).
In contrast, in the example: ?Today 349 members attending to the formation of the lower house. Only
the AMAIUR deputy for Navarra is missing?, only the sentence containing the AMAIUR entity is being
required to determine its polarity.
Obtaining the polarity at entity level is a hard problem and introduces additional complexity because
the part of the tweet refers to each of the entities must be determined. To resolve this problem it should
make a deep parsing of the tweet and perform a study of such dependencies. This is not a solved problem
in NLP even considering normative texts and is further aggravated in Twitter texts. Besides, in many
cases, the dependencies are between different sentences, and problems such as coreference must be
taking into account in order to determine, for example, which pronoun refers to a certain entity. Other
problems such as synonyms and acronyms of certain entities can make this problem harder.
We have chosen a more simple and practice approach that consists in defining a set of heuristics to
determine which segment of the tweet refers to each of the entities present on it. We defined some rules
to do this segmentation. If the tweet contains only one entity the context considered was all the tweet.
We evaluated other alternatives, but due to the short length of tweets, with that decision the best global
results were obtained. If the tweet contains two entities, the casuistry is greater. If both entities are placed
together at the beginning or the end of the tweet all the tweet is considered as a context for both entities.
By contrast, if separate, and has sufficient context, the tweet is segmented by defining the context of each
entity. Next, we show some examples and the segmentation produced by the defined rules.
Example 1 is the easier case due the two entities are in separated sentences. When both entities are in
the same sentence, in Example 2 the rule applied determines that the context for the first entity is from
the beginning until the second entity, and the rest of the sentence is the context for the second entity.
Example 3 is more difficult, and the rules applied produce segmentations like this [On March 25 we elect
between the immobility of the @PSOE] [and the renovation and the hope of the @ppandaluz.]), that are
not correct but can be useful for determining the polarity of each entity. In addition, due to the short
length of the tweets, the context of an entity is often so small that it does not contain information enough
2
https://github.com/brendano/tweetmotif.
3
http://nlp.lsi.upc.edu/freeling/
4
All the exemples have been translated to English.
186
Example 1
[Rajoy?s government goes up the pensions.] [PSOE cuts back all things except the unemployment.]
GLOBAL POLARITY: NEU. ENTITY (Rajoy): Positive. ENTITY (PSOE): Negative
Example 2
[As IU gains confidence in Andaluc??a] [PP loses members.]
GLOBAL POLARITY: NEU. ENTITY (IU): Positive. ENTITY (PP): Negative
Example 3
On March 25 we elect between [the immobility of the @PSOE] and [the renovation and the hope of the @ppandaluz.]
GLOBAL POLARITY: NEU. ENTITY (@PSOE): Negative. ENTITY (@ppandaluz): Positive
to correctly classify the polarity of the entity. In such case, the option that was chosen is to establish a
threshold of context, and if it is below than this threshold, it was assigned the same polarity to all the
entities of the tweet. When the number of entities is greater than two in much cases we assigned the
same polarity to all the entities of the tweet because we had not enough context.
4.3 Feature Selection
The feature selection process was performed by cross validation (10-fold validation) using the training
set to select the set of relevant features.
We considered the following set of features: unigrams and bigrams of lemmas obtained in the prepro-
cessing of the tweets that belong to a set of selected POS. We considered only the lemmas of a minimum
frequency (f) in the training set. We unified all hashtags, user references, dates, punctuations as a single
feature. We classified the emoticons in the following categories: happy, sad, tongue, wink, and other.
Finally, we used external polarity lexicons of lemmas and words.
Some of the features required further adjustment. For the POS feature we selected the lemas that
belongs to the nouns, verbs, adjectives, and adverbs POS and also exclamations and emoticons. We
estimated the minimum frequency of the lemmas to be selected (f =2). Finally, we selected the external
lexicons to be used. One of the lexicons used was originally for English language (Wilson et al., 2005)
that was translated into Spanish automatically, and other (Perez-Rosas et al., 2012) lexicon was a list of
words that was originally in Spanish. Then, we combined these two resource with the lexicon presented
in (Saralegi and San Vicente, 2013).
4.4 Polarity Classifier
The task was addressed as a classification problem that consisted of determining the polarity of each
tweet. We used WEKA
5
, which is a tool that includes (among other utilities) a collection of machine-
learning algorithms that can be used for classification tasks. Specifically, we used a SVM-based approach
because it is a well-founded formalism, that has been successfully used in many classification problems.
In the SA task, SVM has shown it ability to handle large feature spaces and to determine the relevant
features (Joachims, 1998).
We used the NU-SVM algorithm (Sch?olkopf et al., 2000) from an external library called LibSVM
6
,
which is very efficient software for building SVM classifiers. It is easy to integrate this software with
WEKA thus allowing us to use all of WEKA?s features. We used the bag of words approach to represent
each tweet as a feature vector that contains the frequency of the selected features of the training set.
4.5 Evaluation of the Sentiment Analysis System
We evaluated our system on the SA tasks defined at the TASS2013 workshop. Two different sub-tasks
called 5-level and 3-level were proposed. Both sub-tasks differ only in the polarity granularity consid-
ered. The 5-level sub-task uses the labels N, N+, P, P+, and NEU. The 3-level sub-task uses the labels N,
P, and NEU. In both sub-tasks, an additional label (NONE) was used to represent tweets with no polarity.
The accuracy results obtained on the unseen data test were: 62.88%?0.38% for 5-level task and
70.25%?0.36% for 3-level task. This results outperformed all the approaches at TASS2013 workshop
with statistical significance (with a 95% level of confidence). The official results ranged from 61.6% to
13.5% for the 5-level task and from 66.3% to 38.8% for the 3-level task. The F
1
result obtained in the
5
http://www.cs.waikato.ac.nz/ml/weka/
6
http://www.cs.iastate.edu/? yasser/wlsvm/
187
Sentiment Analysis at Entity level task was worse (F
1
=0.40), but it still is the best result reported in the
sentiment analysis at entity level task at TASS2013 competition.
5 Political tendency identification
The objective of this task is to estimate the political tendency of each user from the test set of the General
corpus in four possible values: Left, Right, Centre, and Undefined. Next, we describe the approach
we proposed for this task. This approach uses the SA system previously described in section 4.
To perform the classification of users we assume the following hypothesis: the positive opinions on a
political party is a political orientation similarly to the user performing the review for this party, on the
contrary, a negative opinion about a party is a political orientation opposite to that shown by this party.
In this way, to classify users by their political orientation, first we identify entities associated with
political parties and secondly we analyze the polarity of these entities in the tweets of each user.
We consider three types of entities: entities labeled by Freeling as proper names (i.e.,
comit?e del pp de madrid), Twitter users (i.e., @38congresopsoe), and Twitter hashtags (i.e., #upyd).
Among all possible entities we selected those containing the acronym for a political party or the name
of a political leader. A total of 864 entities related to political parties and political leaders were detected.
Table 2 shows the parties and political leaders considered and some examples of the selected entities.
Party Tendency Examples of Entities
PP right #17congesoPP, congreso nacional pp, ppopular, congresopp, #ppfachas
PSOE left elpsoe, #adiosalpsoeenandalucia, #38congresopsoe
IU left asamblea de iu, iumalaga, diputados de iu, #iu
UPyD centre upydeuskadi, #demagogiaupyd, #mareamagenta, upyd asturias
CiU right ciu+tripartito, #ciu, ciu-mintiendo-crujen
Political Leader Party Examples of Entities
Rajoy PP #rajoynoeslasoluci?on, espa?na de rajoy, irpf de rajoy
Gonz?alez Pons PP @gonzalezpons, rajoy para gonz?alez pons
Rubalcaba PSOE #rubalcabaenlaser, @conrubalcaba, rubalcaba para el psoe
Zapatero PSOE nueva via de zapatero, presidente zapatero, zapatero tv
Cayo Lara IU @cayo lara, cayo lara, cayo
Table 2: Tendency of political parties and political leaders.
We defined a tendency measure Tendency that assigned a value of ?1 to those entities related to left
parties, a value of +1 to entities related to right parties and a value of 0 to the entities related to centre
parties.
Next we show how has been numerically calculated the political orientation of users. For each user U
i
of the General corpus we obtain the set T
i
that includes all of their tweets that contain political entities.
For users who do not have any tweet that contain political entities the Undefined label is assigned.
For each tweet T
i
j
? T
i
, j = 1 ? ? ? |T
i
|, we identify the political entities that are contained on it. Let
E
i
j
be the set of entities of the tweet T
i
j
. We denote each of the entities contained in E
i
j
as E
i
j
k
?
E
i
j
, k = 1 ? ? ? |E
i
j
|.
We obtained the polarity of each entity by using the system described in section 4. After that, we
assigned a numerical value to each polarity. In this respect, we assigned Polarity = +1 to the entities
with positive polarity (label P), Polarity = ?1 to the entities with negative polarity (label N) and finally,
Polarity = 0 to the entities without polarity, that is, to the NEU and NONE labels.
We combined
7
the Tendency and Polarity measures previously presented to define a new measure
(Political Tendency) to obtain the political orientation of each user.
7
We have considered multiple combination strategies, in this work we present the combination with the best results.
188
Political Tendency(U
i
) =
?
j=1???|T
i
|
?
k=1???|E
i
j
|
Polarity(E
i
j
k
) ? Tendency(E
i
j
k
)
?
j=1???|T
i
|
|E
i
j
|
(1)
From the Political Tendency values obtained for each user, we classified the user tendency
tanking into account the following: users without political entities in their tweets are classified as
Undefined; users with Political Tendency between -0.05 and +0.05 are classified as Centre; users
with Political Tendency lower than -0.05 are classified as Left; and users with Political Tendency
greater than +0.05 are classified as Right.
6 Experimental Evaluation of the Political Identification System
The measures selected to evaluate our approach were the Precision, the Recall, and the F-measure for
? = 1 (F
1
). Table 3 summarizes the experimental results of our proposal. The table includes both the
overall results (Global) and the results for each one of the political tendencies (Left, Right, Centre, and
Undefined). It also includes the distribution of the tendencies in the gold-standard (%Ref). For the global
result, the precision and the recall are the same since each user in the test set had a tendency assigned
and the task consist to assign a tendency to all the users.
Tendency %Ref Precision Recall F
1
Left 21.5 0.658 0.735 0.694
Centre 17.7 0.478 0.393 0.431
Right 39.9 0.786 0.698 0.739
Undefined 20.9 0.780 0.970 0.865
Global 100 0.709 0.709 0.709
Table 3: Experimental results obtained in the political tendency identification task of TASS2013.
The result obtained by our system (0.709) is the best result reported so far for this corpus, to our
knowledge. The tendency for what we get better results is the Undefined (F
1
=0.865). We consider
the political tendency of a user to be Undefined if he did not have any tweet that references any of the
majority parties. This assumption may be too strict for common users, but it seems reasonable for the
well-know users that form the test corpus.
The tendency that our system had more trouble identifying was Centre (F
1
=0.431). The tendency of a
user can be identified as Centre when he expressed -in his tweets- opinions about entities related to centre
parties, even when these opinions were negative. This is because the neutral value of Centre entities. In
addition, users with opinions on right and left parties with the same polarity may be identified as Centre,
which can be wrong in many cases.
 
P
r
e
c
i
s
i
o
n
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
Polarity_Tendency
?1 ?0,8 ?0,6 ?0,4 ?0,2 0 0,2 0,4 0,6 0,8 1
Precision
Figure 2: Precision results depending on the Political Tendency assigned by the system.
189
Although it seems that the ability of our system to identify Left and Right tendencies was similar
(F
1
=0.694 for Left and F
1
=0.739 for Right), analyzing the results considering the values of Politi-
cal Tendency some significant differences can be observed. Figure 2 shows the results, in terms of
Precision, considering the value of Political Tendency assigned to each user by our system, from a value
of -1 (the maximum value for Left) to a value of +1 (the maximum for Right).
As expected, most identification errors occurred for Political Tendency value near zero, should re-
member that values between -0.05 and 0.05 were considered Centre. Considering the Right tendency,
all users that obtained Political Tendency value greater than 0.25 were correctly identified as Right, per-
formed better than would be expected. However, the behavior of the Left tendency was not symmetrical.
It seems that values between -0.3 and -0.1 were better to determine correctly this tendency.
Although we have no clear explanation for this behavior, it could be due to multiple factors, including:
the simplicity of the proposal, labeling errors in the polarity of certain entities, or the greater difficulty of
numerically identify the Left tendency (at least in this corpus).
7 Conclusions
We have described our approach for political tendency identification of Twitter users. We have defined a
metric, called Political Tendency, that takes into account the polarity of entities related to political parties
that appear in the tweets of the user. The Sentiment Analysis system developed in order to obtain the
polarity of these entities was also presented.
The evaluation was performed using a corpus of Spanish tweets developed at TASS2013 workshop.
This corpus was used for a specific political tendency identification task at this workshop. To our knowl-
edge, the results obtained by our system are the best results published until now using this corpus.
We are very interested in SA tasks and in identifying tendencies in social media. In this sense, we
have several ideas on how to improve our approach to identifying the political tendency in Twitter.
It would be interesting to test our approach using a larger corpus of tweets from normal user. We think
that the characteristics of the users of the test corpus -figures of culture, journalism and politics in Spain-
made the task a little easier. Perhaps the political tendency of ordinary users would be more difficult
to identify. Moreover, the political spectrum would be more diverse and should increase the catalog of
political parties. Moreover, the political spectrum would be more varied and, consequently, the catalog
of political parties should be increased.
It should be emphasized the difficulty of building an annotated corpus of tweets that could be used
to evaluate and compare different alternative systems. A great effort of acquisition of the tweets and a
subsequent manual labeling process is required. In addition, a validation process is needed to correct the
errors introduced by manual labeling. Even using crowdsourcing-based solutions it is a very expensive
task both in money and time. In this context, to have a labeled corpus as the one provided by TASS2013
is a great help for the scientific community.
On the portability of the system, we think that it will be easy to adapt our proposal to another political
context. This adaptation should focus on two different aspects. First, the Sentiment Analysis System
should be adapted to a new language. In the case of languages with linguistic resources freely available
the adaptation would be very simple. Second, political entities should be changed to fit the political
context where we want to test the system. It would be sufficient to identify the most relevant parties and
their leaders and classify them according to their political tendency. However, it is possible that in other
political contexts different to Spanish, the Left, Centre, and Right tendencies also need to be adapted.
Finally, we have interest in using Machine Learning techniques for the task of identifying political
tendency on twitter. On this point, we are working on a system in which Political Tendency, as defined
in this paper, will be a feature within a wider classification system. In this new system, we want to
include additional information (not available in the TASS2013 corpus) about user behavior and Twitter
structure in order to improve our approach.
Acknowledgements
This work has been partially funded by the Spanish MEC projects DIANA (TIN2012-38603-C02-01)
and T??mpano (TIN2011-28169-C05-01).
190
References
Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 36?44.
Association for Computational Linguistics.
Antoine Boutet, Hyoungshick Kim, and Eiko Yoneki. 2012. What?s in Your Tweets? I Know Who You Sup-
ported in the UK 2010 General Election. In The International AAAI Conference on Weblogs and Social Media
(ICWSM), Dublin, Irlande, June.
Raviv Cohen and Derek Ruths. 2013. Classifying political orientation on twitter: It?s not easy! In International
AAAI Conference on Weblogs and Social Media.
M Conover, B Gonc?alves, J Ratkiewicz, A Flammini, and F Menczer. 2011a. Predicting the political alignment of
twitter users. In Proceedings of 3rd IEEE Conference on Social Computing (SocialCom).
Michael Conover, Jacob Ratkiewicz, Matthew Francisco, Bruno Gonc?alves, Alessandro Flammini, and Filippo
Menczer. 2011b. Political polarization on twitter. In Proc. 5th International AAAI Conference on Weblogs and
Social Media (ICWSM).
Bernard J Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter power: Tweets as electronic
word of mouth. Journal of the American society for information science and technology, 60(11):2169?2188.
Thorsten Joachims. 1998. Text categorization with support vector machines: learning with many relevant features.
In Claire N?edellec and C?eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on
Machine Learning, number 1398, pages 137?142, Chemnitz, DE. Springer Verlag, Heidelberg, DE.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: Analyzing and comparing opinions on
the web. In Proceedings of the 14th International Conference on World Wide Web, WWW ?05, pages 342?351,
New York, NY, USA. ACM.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. A Comprehensive Introduction and Survey. Morgan &
Claypool Publishers.
Eugenio Mart??nez-C?amara, M. Teresa Mart??n-Valdivia, L. Alfonso Ure?na-L?opez, and Arturo Montejo-Ra?ez. 2012.
Sentiment analysis in twitter. Natural Language Engineering, 1(1):1?28.
Andr?es Montoyo, Patricio Mart??nez-Barco, and Alexandra Balahur. 2012. Subjectivity and sentiment analysis: An
overview of the current state of the area and envisaged developments. Decision Support Systems, 53(4):675?
679.
Brendan O?Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010a. From tweets to
polls: Linking text sentiment to public opinion time series. In William W. Cohen and Samuel Gosling, editors,
ICWSM. The AAAI Press.
Brendan O?Connor, Michel Krieger, and David Ahn. 2010b. Tweetmotif: Exploratory search and topic summa-
rization for twitter. In William W. Cohen and Samuel Gosling, editors, Proceedings of the Fourth International
Conference on Weblogs and Social Media, ICWSM 2010, Washington, DC, USA, May 23-26, 2010. The AAAI
Press.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards wider multilinguality. In Proceedings of the
Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May. ELRA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine
learning techniques. In IN PROCEEDINGS OF EMNLP, pages 79?86.
Marco Pennacchiotti and Ana-Maria Popescu. 2011. A machine learning approach to twitter user classification.
In Lada A. Adamic, Ricardo A. Baeza-Yates, and Scott Counts, editors, ICWSM. The AAAI Press.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihalcea. 2012. Learning sentiment lexicons in spanish. In
Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet U?gur Do?gan, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Confer-
ence on Language Resources and Evaluation (LREC?12), Istanbul, Turkey, may. European Language Resources
Association (ELRA).
Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes
in twitter. In Proceedings of the 2Nd International Workshop on Search and Mining User-generated Contents,
SMUC ?10, pages 37?44, New York, NY, USA. ACM.
191
Xabier Saralegi and I?naki San Vicente. 2013. Elhuyar at tass 2013. In Proceedings of the TASS workshop at
SEPLN 2013. IV Congreso Espa?nol de Inform?atica.
Bernhard Sch?olkopf, Alex J. Smola, Robert C. Williamson, and Peter L. Bartlett. 2000. New support vector
algorithms. Neural Comput., 12(5):1207?1245, May.
Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification
of reviews. In ACL, pages 417?424.
Julio Villena-Rom?an and Janine Garc??a-Morera. 2013. Workshop on sentiment analysis at sepln 2013: An over
view. In Proceedings of the TASS workshop at SEPLN 2013. IV Congreso Espa?nol de Inform?atica.
G Vinodhini and RM Chandrasekaran. 2012. Sentiment analysis and opinion mining: A survey. International
Journal, 2(6).
Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,
Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: A system for subjectivity analysis. In Proceed-
ings of HLT/EMNLP on Interactive Demonstrations, pages 34?35. Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter. Proceedings of the International Workshop on Semantic
Evaluation, SemEval, 13.
Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and Mung Chiang. 2013. Quantifying political leaning from
tweets and retweets. In Emre Kiciman, Nicole B. Ellison, Bernie Hogan, Paul Resnick, and Ian Soboroff,
editors, ICWSM. The AAAI Press.
192
