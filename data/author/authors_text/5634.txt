Removing Left Recursion from Context-Free Grammars 
Rober t  C. Moore  
Microso f t  Research  
One Microsof t  Way 
Redmond,  Wash ington  98052 
bobmoore @microsoft.  corn 
Abst ract  
A long-standing issue regarding algorithms that ma- 
nipulate context-free grammars (CFGs) in a "top- 
down" left-to-right fashion is that left recursion can 
lead to nontermination. An algorithm is known 
that transforms any CFG into an equivalent non- 
left-recursive CFG, but the resulting grammars are 
often too large for practical use. We present a new 
method for removing left recursion from CFGs that 
is both theoretically superior to the standard algo- 
rithm, and produces very compact non-left-recursive 
CFGs in practice. 
1 In t roduct ion  
A long-standing issue regarding algorithms that ma- 
nipulate context-free grammars (CFGs) in a "top- 
down" left-to-right fashion is that left recursion can 
lead to nontermination. This is most familiar in the 
case of top-down recursive-descent parsing (Aho et 
al., 1986, pp. 181-182). A more recent motivation 
is that off-the-shelf speech recognition systems are 
now available (e.g., from Nuance Communications 
and Microsoft) that accept CFGs as language models 
for constraining recognition; but as these recogniz- 
ers process CFGs top-down, they also require that 
the CFGs used be non-left-recursive. 
The source of the problem can be seen by consid- 
ering a directly left-recursive grammar production 
such as A -4 As. Suppose we are trying to parse, 
or recognize using a speech recognizer, an A at a 
given position in the input. If we apply this pro- 
duction top-down and left-to-right, our first subgoal 
will be to parse or recognize an A at the same input 
position. This immediately puts us into an infinite 
recursion. The same thing will happen with an indi- 
rectly left-recursive grammar, via a chain of subgoals 
that will lead us from the goal of parsing or recogniz- 
ing an A at a given position to a descendant subgoal 
of parsing or recognizing an A at that position. 
In theory, the restriction to non-left-recursive 
CFGs puts no additional constraints on the lan- 
guages that can be described, because any CFG 
can in principle be transformed into an equivalent 
non-left-recursive CFG. However, the standard algo- 
rithm for carrying out this transformation (Aho et 
al., 1986, pp. 176-178) (Hopcroft and Ullman, 1979, 
p. 96)--attributed to M. C. Panll by Hopcroft and 
Ullman (1979, p. 106)--can produce transformed 
grammars that are orders of magnitude larger than 
the original grammars. In this paper we develop a 
number of improvements to Panll's algorithm, which 
help somewhat but do not completely solve the prob- 
lem. We then go on to develop an alternative ap- 
proach based on the left-corner grammar transform, 
which makes it possible to remove left recursion with 
no significant increase in size for several grammars 
for which Paull's original algorithm is impractical. 
2 Notat ion  and  Termino logy  
Grammar nonterminals will be designated by "low 
order" upper-case letters (A, B, etc.); and termi- 
nals will be designated by lower-case letters. We 
will use "high order" upper-case letters (X, Y, Z) 
to denote single symbols that could be either ter- 
minals or nonterminals, and Greek letters to denote 
(possibly empty) sequences of terminals and/or non- 
terminals. Any production of the form A --4 a will 
be said to be an A-production, and a will be said to 
be an expansion of A. 
We will say that a symbol X is a direct left corner 
of a nonterminal A, if there is an A-production with 
X as the left-most symbol on the right-hand side. 
We define the left-corner elation to be the reflexive 
transitive closure of the direct-left-corner relation, 
and we define the proper-left-corner relation to be 
the transitive closure of the direct-left-corner rela- 
tion. A nonterminal is left recursive if it is a proper 
left corner of itself; a nonterminal is directly left re- 
cursive if it is a direct left corner of itself; and a 
nonterminal is indirectly left recursive if it is left re- 
cursive, but not directly left recursive. 
3 Test  Grammars  
We will test the algorithms considered here on three 
large, independently-motivated, natural-language 
grammars. The CT grammar 1 was compiled into 
a CFG from a task-specific unification grammar 
1Courtesy of John Dowding, SRI International 
249 
Grammar size 
Terminals 
Nonterminals 
Productions 
LR nonterminals 
Productions for LR nonterminals 
Toy CT ATIS PT  
Grammar Grammar Grammar Grammar 
88 
40 
16 
55 
4 
27 
55,830 
1,032 
3,946 
24,456 
535 
2,211 
16,872 
357 
192 
4,592 
9 
1,109 
67,904 
47 
38 
15,039 
33 
14,993 
Table 1: Grammars used for evaluation. 
written for CommandTalk (Moore et al, 1997), a 
spoken-language interface to a military simulation 
system. The ATIS grammar was extracted from an 
internally generated treebank of the DARPA ATIS3 
training sentences (Dahl et al, 1994). The PT  gram- 
mar 2 was extracted from the Penn Treebank (Mar- 
cus et al, 1993). To these grammars we add a small 
"toy" grammar, simply because some of the algo- 
rithms cannot be run to completion on any of the 
"real" grammars within reasonable time and space 
bounds. 
Some statistics on the test grammars are con- 
tained in Table 1. The criterion we use to judge 
effectiveness of the algorithms under test is the size 
of the' resulting grammar, measured in terms of the 
total number of terminal and nonterminal symbols 
needed to express the productions of the grammar. 
We use a slightly nonstandard metric, counting the 
symbols as if, for each nonterminal, there were a 
single production of the form A --+ al  I ..- \[ a,~. 
This reflects the size of files and data structures typ- 
ically used to store grammars for top-down process- 
ing more accurately than counting a separate occur- 
rence of the left-hand side for each distinct right- 
hand side. 
It should be noted that the CT grammar has a 
very special property: none of the 535 left recursive 
nonterminals i indirectly left recursive. The gram- 
mar was designed to have this property specifically 
because Paull's algorithm does not handle indirect 
left recursion well. 
It should also be noted that none of these gram- 
mars contains empty productions or cycles, which 
can cause problems for algorithms for removing left 
recursion. It is relatively easy to trasform an arbi- 
trary CFG into an equivalent grammar which does 
not contain any of the probelmatical cases. In its 
initial form the PT  grammar contained cycles, but 
these were removed at a cost of increasing the size 
of the grammar by 78 productions and 89 total sym- 
bols. No empty productions or cycles existed any- 
where else in the original grammars. 
2Courtesy of Eugene Charniak, Brown University 
4 Paull's A lgor i thm 
Panll's algorithm for eliminating left recursion from 
CFGs attacks the problem by an iterative procedure 
for transforming indirect left recursion into direct 
left recursion, with a subprocedure for eliminating 
direct left recursion, This algorithm is perhaps more 
familiar to some as the first phase of the textbook 
algorithm for transfomrming CFGs to Greibach nor- 
real form (Greibach, 1965). 3 The subprocedure to 
eliminate direct left recursion performs the following 
transformation (Hopcroft and UUman, 1979, p. 96): 
Let 
A Aa11... IAa  
be the set of all directly left recursive A- 
productions, and let 
I/?s 
be the remaining A-productions. Replace 
all these productions with 
A --+/71 \ [ / ? IA '  \[ . . .  \[/?8 \[/?sA',  
and 
A'  --+ az \[ a lA '  \[ . . .  I as \[ asA ' ,  
where A ~ is a new nonterminal not used 
elsewhere in the grammar. 
This transformation is embedded in the full algo- 
rithm (Aho et al, 1986, p. 177), displayed in Fig- 
ure 1. 
The idea of the algorithm is to eliminate left re- 
cursion by transforming the grammar so that all the 
direct left corners of each nonterminal strictly follow 
that nonterminal in a fixed total ordering, in which 
case, no nonterminal can be left recursive. This is 
accomplished by iteratively replacing direct left cor- 
ners that precede a given nonterminal with all their 
expansions in terms of other nonterminals that are 
greater in the ordering, until the nonterminal has 
only itself and greater nonterminals as direct left 
3This has led some readers to attribute the algorithm to 
Greibach, but Greibach's original method was quite different 
and much more complicated. 
250 
Assign an ordering A1, . . . ,  A,~ to the nonterminals of the grammar. 
for i := 1 to n do begin 
for j :-- 1 to i - 1 do begin 
for each production of the form Ai ~ Aja do begin 
remove Ai -+ Aja from the grammar 
for each production of the form Aj -~/~ do begin 
add Ai --~/~a to the grammar 
end 
end 
end 
transform the Ai-productions to eliminate direct left recursion 
end 
Figure 1: Paull's algorithm. 
Grammar Description Grammar Size 
original toy grammar 88 
PA, "best" ordering 156 
PA, lexicographical ordering 970 
PA, "worst" ordering 5696 
Table 2: Effect of nonterminal ordering on Paull's algorithm. 
corners. Any direct left recursion for that nonter- 
minal is then eliminated by the first transformation 
discussed. 
The difficulty with this approach is that the it- 
erated substitutions can lead to an exponential in- 
crease in the size of the grammar. Consider the 
grammar consisting of the productions Az -+ 0 I 1, 
plus Ai+z -+ AiO I Ail for I < i < n. It is easy to see 
that Paull's algorithm will transform the grammar 
so that it consists of all possible Ai-productions with 
a binary sequence of length i on the right-hand side, 
for 1 < i < n, which is exponentially larger than 
the original grammar. Notice that the efficiency of 
PauU's algorithm crucially depends on the ordering 
of the nonterminals. If the ordering is reversed in 
the grammar of this example, Paull's algorithm will 
make no changes, since the grammar will already 
satisfy the condition that all the direct left corners 
of each nonterminal strictly follow that nonterminal 
in the revised ordering. The textbook discussions of 
Paull's algorithm, however, are silent on this issue. 
In the inner loop of Panll's algorithm, for nonter- 
minals Ai and Aj, such that i > j and Aj is a direct 
left corner of Ai, we replace all occurrences of Aj as a 
direct left corner of Ai with all possible expansions 
of Aj. This only contributes to elimination of left 
recursion from the grammar if Ai is a left-recursive 
nonterminal, and Aj \]ies on a path that makes Ai 
left recursive; that is, if Ai is a left corner of A3 (in 
addition to Aj being a left corner of Ai). We could 
eliminate replacements hat are useless in removing 
left recursion if we could order the nonterminals of 
the grammar so that, if i > j and Aj is a direct left 
corner of Ai, then Ai is also a left corner of Aj. We 
can achieve this by ordering the nonterminals in de- 
creasing order of the number of distinct left corners 
they have. Since the left-corner relation is transitive, 
if C is a direct left corner of B, every left corner of 
C is also a left corner of /3.  In addition, since we 
defined the left-corner relation to be reflexive, B is a 
left corner of itself. Hence, if C is a direct left corner 
of B, it must follow B in decreasing order of number 
of distinct left corners, unless B is a left corner of 
C. 
Table 2 shows the effect on Paull's algorithm of 
ordering the nonterminals according to decreasing 
number of distinct left corners, with respect o the 
toy grammar. 4 In the table, "best" means an or- 
dering consistent with this constraint. Note that 
if a grammar has indirect left recursion, there will 
be multiple orderings consistent with our constraint, 
since indirect left recursion creates cycles in the the 
left-corner elation, so every nonterminal in one of 
these cycles will have the same set of left corners. 
Our "best" ordering is simply an arbitrarily chosen 
4As mentioned previously, grammar sizes are given in 
terms of total terminal and nonterminal symbols needed to 
express the grammar. 
251 
original grammar 
PA 
LF 
LF+PA 
LF+NLRG+PA 
CT Grammar ATIS Grammar 
55,830 
62,499 
54,991 
59,797 
57,924 
16,872 
> 5,000,000 
11,582 
2,004,473 
72,035 
PT Grammar 
67,904 
> 5,000,000 
37,811 
> 5,000,000 
> 5,000,000 
Table 3: Grammar size comparisons with Panll's algorithm variants 
ordering respecting the constraint; we are unaware 
of any method for finding a unique best ordering, 
other than trying all the orderings respecting the 
constraint. 
As a neutral comparison, we also ran the algo- 
rithm with the nonterminals ordered lexicographi- 
cally. Finally, to test how bad the algorithm could 
be with a really poor choice of nonterminal ordering, 
we defined a "worst" ordering to be one with increas- 
ing numbers of distinct left corners. It should be 
noted that with either the lexicographical or worst 
ordering, on all of our three large grammars Panll's 
algorithm exceeded a cut-off of 5,000,000 grammar 
symbols, which we chose as being well beyond what 
might be considered a tolerable increase in the size 
of the grammar. 
Let PA refer to Paull's algorithm with the non- 
terminals ordered according to decreasing number 
of distinct left corners. The second line of Table 3 
shows the results of running PA on our three large 
grammars. The CT grammar increases only mod- 
estly in size, because as previously noted, it has no 
indirect left recursion. Thus the combinatorial phase 
of Paull's algorithm is never invoked, and the in- 
crease is solely due to the transformation applied to 
directly left-recursive productions. With the ATIS 
grammar and PT grammar, which do not have this 
special property, Panll's algorithm exceeded our cut- 
off, even with our best ordering of nonterminals. 
Some additional optimizations of Panll's aglo- 
rithm are possible. One way to reduce the num- 
ber of substitutions made by the inner loop of the 
algorithm is to "left factor" the grammar (Aho et 
al., 1986, pp. 178-179). The left-factoring transfor- 
mation (LF) applies the following grammar ewrite 
schema repeatedly, until it is no longer applicable: 
LF: For each nonterminal A, let a be the 
longest nonempty sequence such that there 
is more than one grammar production of 
the form A --+ a~. Replace the set of all 
productions 
A-+af t1 ,  . . . ,  A -+a~n 
with the productions 
A -+ aA ' ,  A '  --~ il l, . . . ,  A '  --~ fin, 
where A' is a new nonterminal symbol. 
With left factoring, for each nonterminal A there will 
be only one A-production for each direct left corner 
of A, which will in general reduce the number of 
substitutions performed by the algorithm. 
The effect of left factoring by itself is shown in 
the third line of Table 3. Left factoring actually re- 
duces the size of all three grammars, which may be 
unintuitive, since left factoring necessarily increases 
the number of productions in the grammar. How- 
ever, the transformed productions axe shorter, and 
the grammar size as measured by total number of 
symbols can be smaller because common left factors 
are represented only once. 
The result of applying PA to the left-factored 
grammars is shown in the fourth line of Table 3 
(LF+PA). This produces a modest decrease in the 
size of the non-left-recursive form of the CT gram- 
mar, and brings the nomleft-recursive form of the 
ATIS grammar under the cut-off size, but the non- 
left-recursive form of the PT  grammar still exceeds 
the cut-off. 
The final optimization we have developed for 
Paull's algorithm is to transform the grammar to 
combine all the non-left-recursive possibilities for 
each left-recursive nonterminal under a new nonter- 
minal symbol. This transformation, which we might 
call "non-left-recursion grouping" (NLRG), can be 
defined as follows: 
NLRG: For each left-recursive nontermi- 
nal A, let a l , . . . ,an  be all the expansions 
of A that do not have a left recursive non- 
terminal as the left most symbol. If n > 1, 
replace the set of productions 
A -~ a l  , . . . ,  A --~ a,~ 
with the productions 
A~A ~,A ~a l ,  . . . ,A  ~-~an,  
where A t is a new nonterminal symbol. 
Since all the new nonterminals introduced by this 
transformation will be non-left-recursive, Paull's al- 
gorithm with our best ordering will never substitute 
the expansions of any of these new nonterminals into 
the productions for any other nonterminal, which 
in general reduces the number of substitutions the 
algorithm makes. We did not empirically measure 
252 
original grammar 
LF 
LF+NLRG+PA 
LC 
LCLR 
LF?LCLn 
LF+NLRG+LCLR 
CT Grammar ATIS Grammar PT  Grammar 
55,830 
54,991 
57,924 
762,576 
60,556 
58,893 
57,380 
16,872 
11,582 
72,035 
287,649 
40,660 
13,641 
12,243 
67,904 
37,811 
> 5,000,000 
1,567,162 
1,498,112 
67,167 
50,277 
Table 4: Grammar size comparisons for LC transform variants 
the effect on grammar size of applying the NLRG 
transformation by itself, but it is easy to see that 
it increases the grammar size by exactly two sym- 
bols for each left-recursive nontermina\] to which it 
is applied. Thus an addition of twice the number of 
left-recursive nontermina\]s will be an upper bound 
on the increase in the size of the grammar, but since 
not every left-recursive nonterminal necessarily has 
more than one non-left-recursive expansion, the in- 
crease may be less than this. 
The fifth line of Table 3 (LF+NLRG+PA)  shows 
the result of applying LF, followed by NLRG, fol- 
lowed by PA. This produces another modest de- 
crease in the size of the non-left-recursive form of 
the CT grammar and reduces the size of the non- 
left-recursive form of the ATIS grammar by a factor 
of 27.8, compared to LF?PA. The non-left-recursive 
form of the PT  grammar emains larger than the 
cut-off size of 5,000,000 symbols, however. 
5 Le f t -Recurs ion  E l iminat ion  Based  
on the  Lef t -Corner  T rans form 
An alternate approach to eliminating left-recursion 
is based on the left-corner (LC) grammar transform 
of Rosenkrantz and Lewis (1970) as presented and 
modified by Johnson (1998). Johnson's second form 
of the LC transform can be expressed as follows, with 
expressions of the form A-a, A -X ,  and A-B  being 
new nonterminals in the transformed grammar: 
1. If a terminal symbol a is a proper left corner of 
A in the original grammar, add A -4 aA-a to 
the transformed grammar. 
2. If B is a proper left corner of A and B --+ X~ 
is a production of the original grammar, add 
A-X  -+ ~A-B to the transformed grammar. 
3. If X is a proper left corner of A and A --+ X~ 
is a production of the original grammar, add 
A-X  -+ ~ to the transformed grammar. 
In Rosenkrantz and Lewis's original LC transform, 
schema 2 applied whenever B is a left corner of A, 
including all cases where B = A. In Johnson's ver- 
sion schema 2 applies when B -- A only if A is a 
proper left corner of itself. Johnson then introduces 
schema 3 handle the residual cases, without intro- 
ducing instances of nonterminals of the form A-A 
that need to be allowed to derive the empty string. 
The original purpose of the LC transform is to 
allow simulation of left-corner parsing by top-down 
parsing, but it also eliminates left recursion from any 
noncyclic CFG. 5 Fhrthermore, in the worst case, the 
total number of symbols in the transformed gram- 
mar cannot exceed a fixed multiple of the square of 
the number of symbols in the original grammar, in 
contrast o Paull's algorithm, which exponentiates 
the size of the grammar in the worst case. 
Thus, we can use Johnson's version of the LC 
transform directly to eliminate left-recursion. Be- 
fore applying this idea, however, we have one gen- 
era\] improvement to make in the transform. Johnson 
notes that in his version of the LC transform, a new 
nontermina\] of the form A-X  is useless unless X is 
a proper left corner of A. We further note that a 
new nonterminal of the form A-X ,  as well as the 
orginal nonterminal A, is useless in the transformed 
grammar, unless A is either the top nonterminal of 
the grammar or appears on the right-hand side of 
an original grammar production in other than the 
left-most position. This can be shown by induction 
on the length of top-down derivations using the pro- 
ductions of the transformed grammar. Therefore, 
we will call the original nonterminals meeting this 
condition "retained nontermina\]s" and restrict the 
LC transform so that productions involving nonter- 
minals of the form A-X  are created only if A is a 
retained nonterminal. 
Let LC refer to Johnson's version of the LC trans- 
form restricted to retained nonterminals. In Table 4 
the first three lines repeat he previously shown sizes 
for our three original grammars, their left-factored 
form, and their non-left-recursive form using our 
best variant of Panll's algorithm (LF+NLRG+PA).  
The fourth line shows the results of applying LC to 
the three original grammars. Note that this pro- 
duces a non-left-recursive form of the PT  gram- 
mar smaller than the cut-off size, but the non-left- 
recursive forms of the CT and ATIS grammars are 
Sin the case of a cyclic CFG, the schema 2 fails to guar- 
antee a non-left-recursive transformed grammar.  
253 
considerably arger than the most compact versions 
created with Paull's algorithm. 
We can improve on this result by noting that, 
since we are interested in the LC transform only as 
a means of eliminating left-recursion, we can greatly 
reduce the size of the transformed grammars by ap- 
plying the transform only to left-recursive nonter- 
minals. More precisely, we can retain in the trans- 
formed grammar all the productions expanding non- 
left-recursive nonterminals of the original grammar, 
and for the purposes of the LC transform, we can 
treat nomleft-recursive nonterminals as if they were 
terminals: 
1. If a terminal symbol or non-left-recursive non- 
terminal X is a proper left corner of a re- 
tained left-recursive nonterminal A in the orig- 
inal grammar, add A -+ XA-X  to the trans- 
formed grammar. 
2. If B is a left-recursive proper left corner of a 
retained left-recursive nonterminal A and B --~ 
X/~ is a production of the original grammar, add 
A-X -~ ~A-B to the transformed grammar. 
3. If X is a proper left corner of a retained left- 
recursive nonterminal A and A --~ X/~ is a pro- 
duction of the original grammar, add A-X --~ 
to the transformed grammar. 
4. If A is a non-left-recursive nonterminal nd A -~ 
/3 is a production of the original grammar, add 
A -~/~ to the transformed grammar. 
Let LCLR refer to the LC transform restricted 
by these modifications so as to apply only to left- 
recursive nonterminals. The fifth line of Table 4 
shows the results of applying LCLR to the three orig- 
inal grammars. LCLR greatly reduces the size of the 
non-left-recursive forms of the CT and ATIS gram- 
mars, but the size of the non-left-recursive form of 
the PT grammar is only slightly reduced. This is 
not surprising if we note from Table 1 that almost 
all the productions of the PT grammar are produc- 
tions for left-recursive nonterminals. However, we 
can apply the additional transformations that we 
used with Paull's algorithm, to reduce the num- 
ber of productions for left-recursive nonterminals 
before applying our modified LC transform. The 
effects of left factoring the grammar before apply- 
ing LCLR (LF+LCLR), and additionally combining 
non-left-recursive productions for left-recursive non- 
terminals between left factoring and applying LCLR 
(LF+NLRG+LCLR), are shown in the sixth and 
seventh lines of Table 4. 
With all optimizations applied, the non-left- 
recursive forms of the ATIS and PT grammars are 
smaller than the originals (although not smaller 
than the left-factored forms of these grammars), 
and the non-left-recursive form of the CT gram- 
mar is only slightly larger than the original. In all 
cases, LF+NLRG+LCLR produces more compact 
grammars than LF+NLRG+PA, the best variant of 
Paull's algorithm--slightly more compact in the case 
of the CT grammar, more compact by a factor of 5.9 
in the case of the ATIS grammar, and more compact 
by at least two orders of magnitude in the case of the 
PT grammar. 
6 Conc lus ions  
We have shown that, in its textbook form, 
the standard algorithm for eliminating left recur- 
sion from CFGs is impractical for three diverse, 
independently-motivated, natural-language ram- 
mars. We apply a number of optimizations to the 
algorithm--most notably a novel strategy for order- 
ing the nonterminals of the grammar--but one of 
the three grammars remains essentially intractable. 
We then explore an alternative approach based on 
the LC grammar transform. With several optimiza- 
tions of this approach, we are able to obtain quite 
compact non-left-recursive forms of all three gram- 
mars. Given the diverse nature of these grammars, 
we conclude that our techniques based on the LC 
transform are likely to be applicable to a wide range 
of CFGs used for natural-language processing. 
References  
A. V. Aho, R. Sethi, and J. D. Ullman. 1986. 
Compilers: Principles, Techniques, and Tools. 
Addison-Wesley Publishing Company, Reading, 
Massachusetts. 
D. A. Da.hl et al 1994. Expanding the scope of the 
ATIS task: the ATIS-3 corpus. In Proceedings o/ 
the Spoken Language Technology Workshop, pages 
3-8, Plainsboro, New Jersey. Advanced Research 
Projects Agency. 
S. A. Greibach. 1965. A new normal-form theorem 
for context-free phrase structure grammars. Jour- 
nal of the Association for Computing Machinery, 
12(1):42-52, January. 
J. E. Hopcroft and J. D. Ullman. 1979. Introduc- 
tion to Automata Theory, Languages, and Com- 
putation. Addison-Wesley Publishing Company, 
Reading, Massachusetts. 
M. Johnson. 1998. Finite-state approximation 
of constraint-based grammars using left-corner 
grammar transforms. In Proceedings, COLING- 
ACL '98, pages 619-623, Montreal, Quebec, 
Canada. Association for Computational Linguis- 
tics. 
M. P. Marcus, B. Santorini, and M. A. 
Marcinkiewicz. 1993. Building a large anno- 
tated corpus of English: The Penn Treebank. 
Computational Linguistics, 19(2):313-330, June. 
R. Moore, J. Dowding, H. Bratt, J. M. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. Commandtalk: 
254 
A spoken-language interface for battlefield simu- 
lations. In Proceedings of the Fifth Conference on 
Applied Natural Language Processing, pages 1-7, 
Washington, DC. Association for Computational 
Linguistics. 
S. J. Rosenkrantz and P. M. Lewis. 1970. Deter- 
ministic left corner parser. In IEEE Conference 
Record of the 11th Annual Symposium on Switch- 
ing and Automata Theory, pages 139-152. 
255 
Extraposition: A case study in German sentence realization 
Michael GAMON?, Eric RINGGER?, Zhu ZHANG?, 
Robert MOORE?, Simon CORSTON-OLIVER? 
 
?Microsoft Research 
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, bobmoore, 
simonco}@microsoft.com 
 
 
?University of Michigan 
Ann Arbor, MI 48109 
zhuzhang@umich.edu 
 
Abstract 
We profile the occurrence of clausal 
extraposition in corpora from different 
domains and demonstrate that extraposition 
is a pervasive phenomenon in German that 
must be addressed in German sentence 
realization. We present two different 
approaches to the modeling of extraposition, 
both based on machine learned decision tree 
classifiers. The two approaches differ in their 
view of the movement operation: one 
approach models multi-step movement 
through intermediate nodes to the ultimate 
target node, while the other approach models 
one-step movement to the target node. We 
compare the resulting models, trained on data 
from two domains and discuss the 
differences between the two types of models 
and between the results obtained in the 
different domains. 
Introduction 
Sentence realization, the last stage in natural 
language generation, derives a surface string 
from a more abstract representation. Numerous 
complex operations are necessary to produce 
fluent output, including syntactic aggregation, 
constituent ordering, word inflection, etc. We 
argue that for fluent output from German 
sentence realization, clausal extraposition needs 
to be included. We show how to accomplish this 
task by applying machine learning techniques. 
A comparison between English and German 
illustrates that it is possible in both languages to 
extrapose clausal material to the right periphery 
of a clause, as the following examples show: 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave 
the country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumor has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Unlike obligatory movement phenomena such as 
Wh-movement, extraposition is subject to 
pragmatic variability. A widely-cited factor 
influencing extraposition is clausal heaviness; in 
general, extraposition of heavy clauses is 
preferred over leaving them in place. Consider 
the following example from the technical 
domain: 
German: Es werden Datenstrukturen 
verwendet, die f?r die Benutzer nicht 
sichtbar sind. 
English: Data structures are used which 
are not visible to the user. 
This perfectly fluent sentence contains an 
extraposed relative clause. If the relative clause is 
left in place, as in the following example, the 
result is less fluent, though still grammatical: 
? Es werden Datenstrukturen, die f?r die 
Benutzer nicht sichtbar sind, verwendet. 
Data structures which are not visible to 
the users are used. 
Table 1 presents a quantitative analysis of the 
frequency of extraposition in different corpora in 
both English and German. This analysis is based 
on automatic data profiling using the NLPWin 
system (Heidorn 2000). The technical manual 
corpus consists of 100,000 aligned 
English-German sentence pairs from Microsoft 
technical manuals. The Encarta corpora consist 
of 100,000 randomly selected sentences from the 
Encarta encyclopedia in both English and 
German. The output of the parser was 
post-processed to identify relative clauses 
(RELCL), infinitival clauses (INFCL), and 
complement clauses (COMPCL) that have been 
moved from a position adjacent to the term they  
modify. According to this data profile, 
approximately one third of German relative 
clauses are extraposed in technical writing, while 
only 0.22% of English relative clauses are 
extraposed in the corresponding sentence set. The 
high number of extraposed relative clauses in 
German is corroborated by numbers from the 
German hand-annotated NEGRA corpus. In 
NEGRA, 26.75% of relative clauses are 
extraposed. Uszkoreit et al (1998) report 24% of 
relative clauses being extraposed in NEGRA, but 
their number is based on an earlier version of 
NEGRA, which is about half the size of the 
current NEGRA corpus. 
We also used the NEGRA corpus to verify the 
accuracy of our data profiling with NLPWin. 
These results are presented in Table 2. We only 
took into account sentences that received a 
complete parse in NLPWin. Of the 20,602 
sentences in NEGRA, 17,756 (86.19%) fell into 
that category. The results indicate that NLPWin 
is sufficiently reliable for the identification of 
relative clauses to make our conclusions 
noteworthy and to make learning from 
NLPWin-parsed data compelling. 
Extraposition is so rare in English that a sentence 
realization module may safely ignore it and still 
yield fluent output. The fluency of sentence 
realization for German, however, will suffer from 
the lack of a good extraposition mechanism.
 
 
German  
technical 
manuals 
English  
technical 
manuals 
German  
Encarta 
English  
Encarta 
RELCL 34.97% 0.22% 18.97% 0.30% 
INFCL 3.2% 0.53% 2.77% 0.33% 
COMPCL 1.50% 0.00% 2.54% 0.15% 
Table 1: Percentage of extraposed clauses in English and German corpora 
Relative clause 
identification overall 
Identification of 
extraposed relative clauses 
Identification of non-
extraposed relative clauses 
Recall Precision Recall Precision Recall Precision 
94.55 93.40 74.50 90.02 94.64 87.76 
Table 2: NLPWin recall and precision for relative clauses on the NEGRA corpus 
 
This evidence makes it clear that any serious 
sentence realization component for German 
needs to be able to produce extraposed relative 
clauses in order to achieve reasonable fluency. In 
the German sentence realization module, 
code-named Amalgam (Gamon et al 2002, 
Corston-Oliver et al 2002), we have successfully 
implemented both extraposition models as 
described here. 
1 Two strategies for modeling 
extraposition 
The linguistic and pragmatic factors involved in 
clause extraposition are inherently complex. We 
use machine learning techniques to leverage large 
amounts of data for discovering the relevant 
conditioning features for extraposition. As a 
machine learning technique for the problem at 
hand, we chose decision tree learning, a practical 
approach to inductive inference in widespread 
use. We employ decision tree learning to 
approximate discrete-valued functions from large 
feature sets that are robust to noisy data. Decision 
trees provide an easily accessible inventory of the 
selected features and some indication of their 
relative importance in predicting the target 
features in question. The particular tool we used 
to build our decision trees is the WinMine toolkit 
(Chickering et al, 1997, n.d.). Decision trees 
built by WinMine predict a probability 
distribution over all possible target values. 
We consider two different strategies for the 
machine-learned modeling of extraposition. The 
two strategies are a series of movements versus a 
single reattachment. 
1.1 Multi-step movement 
In the multi-step movement approach, the 
question to model for each potential attachment 
site of an extraposable clause is whether the 
clause should move up to its grandparent (a ?yes? 
answer) or remain attached to its current parent (a 
?no? answer). In other words, we have cast the 
problem as a staged classification task. At 
generation runtime, for a given extraposable 
clause, the movement question is posed, and if 
the DT classifier answers ?yes?, then the clause is 
reattached one level up, and the question is posed 
again. The final attachment site is reached when 
the answer to the classification task is ?no?, and 
hence further movement is barred. Figure 1 
illustrates the multi-step movement of a clause 
(lower triangle) through two steps to a new 
landing site (the reattached clause is the upper 
triangle). Note that in both Figure 1 and Figure 2 
linear order is ignored; only the hierarchical 
aspects of extraposition are represented. 
 
Figure 1: Multi-step movement 
1.2 One-step movement 
Modeling extraposition as a one-step movement 
involves a classification decision for each node in 
the parent chain of an extraposable clause. The 
classification task can be formulated as ?should 
the extraposable clause move up to this target 
from its base position??. Figure 2 shows the 
one-step movement approach to extraposition in 
the same structural configuration as in Figure 1. 
In this example, out of the three potential landing 
sites, only one qualifies. At generation runtime, if 
more than one node in the parent chain qualifies 
as a target for extraposition movement, the node 
with the highest probability of being a target is 
chosen. In the event of equally likely target nodes, 
the target node highest in the tree is chosen. 
 
Figure 2: One-step movement 
2 Data and features 
We employed two different sets of data to build 
the models for German: the 100,000 sentence 
technical manual corpus, and the 100,000 
sentence Encarta corpus. The data were split 
70/30 for training and parameter tuning purposes, 
respectively. We extracted features for each data 
point, using the syntactic and semantic analysis 
provided by the Microsoft NLPWin system (see 
Gamon et al 2002 for more details). We only 
considered sentences for feature extraction which 
received a complete spanning parse in NLPwin. 
85.14% of the sentences in the technical domain, 
and 88.37% of the sentences in the Encarta 
corpus qualified. The following features were 
extracted: 
? syntactic label of the node under 
consideration (i.e., the starting node for a 
single-step movement), its parent and 
grandparent, and the extraposable clause 
? semantic relation to the parent node of 
the node under consideration, the parent 
and the grandparent, and the 
extraposable clause 
? status of the head of the node under 
consideration as a separable prefix verb, 
the same for the parent and the 
grandparent 
? verb position information (verb-second 
versus verb-final) for the node under 
consideration, the parent and grandparent 
? all available analysis features and 
attributes in NLPWin (see Gamon et al 
2002 for a complete list of the currently 
used features and attributes) on the node 
under consideration, the parent and 
grandparent, and on the extraposable 
clause and its parent and grandparent 
? two features indicating whether the 
extraposable node has any verbal 
ancestor node with verb-final or 
verb-second properties 
? ?heaviness? of extraposable clause as 
measured in both number of words and 
number of characters 
? ?heaviness? of the whole sentence as 
measured in both number of words and 
number of characters 
A total of 1397 features were extracted for the 
multi-step movement model. For the single-step 
movement model, we extracted an additional 21 
features. Those features indicate for each of the 
21 labels for non-terminal nodes whether a node 
with that label intervenes between the parent of 
the extraposable clause and the putative landing 
site. 
Another linguistic feature commonly cited as 
influencing extraposition is the length and 
complexity of the part of the structure between 
the original position and the extraposed clause. 
Since in the Amalgam generation module 
extraposition is applied before word and 
constituent order is established, length of 
intervening strings is not accessible as a feature.  
For each training set, we built decision trees at 
varying levels of granularity (by manipulating the 
prior probability of tree structures to favor 
simpler structures) and selected the model with 
maximal accuracy on the corresponding 
parameter tuning data set. 
Since the syntactic label of the extraposable 
clause is one of the extracted features, we decided 
to build one general extraposition model, instead 
of building separate models for each of the three 
extraposable clause types (complement clause 
COMPCL, infinitival clause INFCL, and relative 
clause RELCL). If different conditions apply to 
the three types of extraposition, the decision tree 
model is expected to pick up on the syntactic 
label of the extraposable clause as a predictive 
feature. If, on the other hand, conditions for 
extraposition tend to be neutral with respect to the 
type of extraposable clause, the modeling of 
INFCL and COMPCL extraposition can greatly 
benefit from the much larger set of data points in 
relative clause extraposition. 
3 Comparison  
To compare the one-step and multi-step models, 
we processed a new blind test set of 10,000 
sentences from each domain, Microsoft technical 
manuals and Encarta, respectively. These 
sentences were extracted randomly from data in 
these domains that were neither included in the 
training nor in the parameter tuning set. For each 
extraposable clause, three different outputs were 
computed: the observed behavior, the prediction 
obtained by iteratively applying the multi-step 
model as described in Section 1.1, and the 
prediction obtained by applying the one-step 
model. The values for these outputs were either 
?no extraposition? or a specific target node. If 
either the general extraposition prediction or the 
predicted specific target node did not match the 
observed behavior, this was counted as an error. 
3.1 One-step versus multi-step in the 
technical domain 
Accuracy data on a blind set of 10,000 sentences 
from the technical manuals domain are presented 
in Table 3. 
 One-step Multi-step Baseline 
RELCL 81.56% 83.87% 60.93% 
INFCL 93.70% 92.02% 93.70% 
COMPCL 98.10% 98.57% 94.29% 
Overall 84.42% 86.12% 67.58% 
Table 3: Accuracy numbers for the two models in 
the technical domain 
The baseline score is the accuracy for a system 
that never extraposes. Both models outperform 
the overall baseline by a large margin; the 
multi-step movement model achieves an 
accuracy 1.7% higher than the one-step model. 
The baselines in INFCL and COMPCL 
extraposition are very high. In the test set there 
were only 15 cases of extraposed INFCLs and 12 
cases of extraposed COMPCLs, making it 
impossible to draw definite conclusions. 
3.2 One-step versus multi-step in the 
Encarta domain 
Results from a blind test set of 10,000 sentences 
from the Encarta domain are presented in Table 
4. 
 One-step Multi-step Baseline 
RELCL 87.59% 88.45% 80.48% 
INFCL 97.73% 97.48% 95.72% 
COMPCL 97.32% 97.32% 95.97% 
Overall 89.99% 90.61% 84.15% 
Table 4: Accuracy numbers for the two models in 
the Encarta domain 
As in the technical domain, the multi-step model 
outperforms the one-step model, and both 
outperform the baseline significantly. Again, 
extraposed COMPCLs and INFCLs are rare in 
the dataset (there were only 17 and 6 instances, 
respectively), making the results on these types of 
clauses inconclusive. 
3.3 Domain-specificity of the models 
Since we have data from two very different 
domains we considered the extent to which the 
domain-specific models overlapped. This is a 
linguistically interesting question: from a 
linguistic perspective one would expect both 
universal properties of extraposition as well as 
domain specific generalizations to emerge from 
such a comparison. 
3.3.1 Feature selection in the technical domain 
versus Encarta 
Of the 1397 features that were extracted for the 
multi-step model, the best model for the technical 
domain was created by the WinMine tools by 
selecting 60 features. In the Encarta domain, 49 
features were selected. 27 features are shared by 
the two models. This overlap in selected features 
indicates that the models indeed capture 
linguistic generalizations that are valid across 
domains. The shared features fall into the 
following categories (where node refers to the 
starting node for multi-step movement): 
? features relating to verbal properties of 
the node 
o a separable prefix verb as 
ancestor node 
o tense and mood of ancestor 
nodes 
o presence of a verb-final or 
verb-second VP ancestor 
o presence of Modals attribute 
(indicating the presence of a 
modal verb) on ancestors 
o verb-position in the current node 
and ancestors 
? ?heaviness?-related features on the 
extraposable clause and the whole 
sentence: 
o sentence length in characters 
o number of words in the 
extraposable clause 
? syntactic labels 
? the presence of a prepositional relation 
? the presence of semantic subjects and 
objects on the node and ancestors 
? definiteness features 
? the presence of modifiers on the parent 
? person and number features 
? some basic subcategorization features 
(e.g., transitive versus intransitive) 
Interestingly, the features that are not shared (33 
in the model for the technical domain and 27 in 
the model for the Encarta domain) fall roughly 
into the same categories as the features that are 
shared. To give some examples: 
? The Encarta model refers to the presence 
of a possessor on the parent node, the 
technical domain model does not. 
? The technical domain model selects more 
person and number features on ancestors 
of the node and ancestors of the 
extraposable clause than the Encarta 
model. 
For the one-step model, 1418 total features were 
extracted. Of these features, the number of 
features selected as being predictive is 49 both in 
the Encarta and in the technical domain. 
Twenty-eight of the selected features are shared 
by the models in the two domains. Again, this 
overlap indicates that the models do pick up on 
linguistically relevant generalizations. 
The shared features between the one-step models 
fall into the same categories as the shared features 
between the multi-step models. 
The results from these experiments suggest that 
the categories of selected features are 
domain-independent, while the choice of 
individual features from a particular category 
depends on the domain. 
3.3.2 Model complexity 
In order to assess the complexity of the models, 
we use the simple metric of number of branching 
nodes in the decision tree. The complexity of the 
models clearly differs across domains. Table 5 
illustrates that for both multi-step and one-step 
movement the model size is considerably smaller 
in the Encarta domain versus the technical 
domain. 
 One-step Multi-step 
Encarta 68 82 
Technical 87 116 
Table 5: Number of branching nodes in the 
decision trees 
We hypothesize that this difference in model 
complexity may be attributable to the fact that 
NLPWin assigns a higher percentage of spanning 
parses to the Encarta data, indicating that in 
general, the Encarta data may yield more reliable 
parsing output. 
3.3.3 Cross-domain accuracy 
The results in Table 3 and Table 4 above show 
that the models based on the Encarta domain 
achieve a much higher overall accuracy (89.99% 
and 90.61%) than the models based on the 
technical domain (84.42% and 86.12%), but they 
are also based on a much higher baseline of 
non-extraposed clauses (84.15% versus 67.58% 
in the technical domain). To quantify the domain 
specificity of the models, we applied the models 
across domains; i.e., we measured the 
performance of the Encarta models on the 
technical domain and vice versa. The results 
contrasted with the in-domain overall accuracy 
from Table 3 and Table 4 are given in Table 6. 
Encarta Model Technical Model  
1-step Multi 1-step Multi 
On 
Enc. 
89.99% 90.61% 84.42% 86.12% 
On 
Tech. 
79.39% 83.03% 88.54% 89.20% 
Table 6: Cross-domain accuracy of the models 
The results show that for both one-step and 
multi-step models, the models trained on a given 
domain will outperform the models trained on a 
different domain. These results are not surprising; 
they confirm domain-specificity of the 
phenomenon. Viewed from a linguistic 
perspective, this indicates that the generalizations 
governing clausal extraposition cannot be 
formulated independently of the text domain. 
Conclusion 
We have shown that it is possible to model 
extraposition in German using decision tree 
classifiers trained on automatic linguistic 
analyses of corpora. This method is particularly 
effective for extraposed relative clauses, which 
are pervasive in German text in domains as 
disparate as news, technical manuals, and 
encyclopedic text. Both one-step and multi-step 
models very clearly outperform the baseline in 
the two domains in which we experimented. This 
in itself is a significant result, given the 
complexity of the linguistic phenomenon of 
clausal extraposition. The machine learning 
approach to extraposition has two clear 
advantages: it eliminates the need for 
hand-coding of complex conditioning 
environments for extraposition, and it is 
adaptable to new domains. The latter point is 
supported by the cross-domain accuracy 
experiment and the conclusion that extraposition 
is governed by domain-specific regularities. 
We have shown that across domains, the 
multi-step model outperforms the one-step model. 
In the German sentence realization system 
code-named Amalgam (Corston-Oliver et al 
2002, Gamon et al 2002), we have experimented 
with implementations of both the one-step and 
multi-step extraposition models, and based on the 
results reported here we have chosen the 
multi-step model for inclusion in the end-to-end 
system. 
As we have shown, extraposed relative clauses 
outnumber other extraposed clause types by a 
large margin. Still, the combined model for 
clausal extraposition outperforms the baseline 
even for infinitival clauses and complement 
clauses, although the conclusions here are not 
very firm, given the small number of relevant 
data points in the test corpus. Since the syntactic 
label of the extraposed clause is one of the 
features extracted from the training data, 
however, the setup that we have used will adapt 
easily once more training data (especially for 
infinitival and complement clauses) become 
available. The models will automatically pick up 
distinctions between the generalizations covering 
relative clauses versus infinitival/complement 
clauses when they become relevant, by selecting 
the syntactic label feature as predictive. 
Finally, evaluation of the types of features that 
were selected by the extraposition models show 
that besides the ?heaviness? of the extraposed 
clause, a number of other factors from the 
structural context enter the determination of 
likelihood of extraposition. This, in itself, is an 
interesting result: it shows how qualitative 
inspection of a machine learned model can yield 
empirically based linguistic insights. 
Acknowledgements 
Our thanks go to Max Chickering for his 
assistance with the WinMine toolkit and to the 
anonymous reviewers for helpful comments. 
References  
Chickering D. M., Heckerman D. and Meek C. (1997). 
A Bayesian approach to learning Bayesian 
networks with local structure. In "Uncertainty in 
Artificial Intelligence: Proceedings of the 
Thirteenth Conference", D. Geiger and P. Punadlik 
Shenoy, ed., Morgan Kaufman, San Francisco, 
California,  pp. 80-89. 
Chickering, D. Max. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/To
oldoc.htm 
Corston-Oliver S., Gamon M., Ringger E. and Moore 
R. (2002). An overview of Amalgam: a 
machine-learned generation module. To appear in 
Proceedings of the Second International Natural 
Language Generation Conference 2002, New York. 
Gamon M., Ringger E., Corston-Oliver S.. (2002). 
Amalgam: A machine-learned generation module. 
Microsoft Technical Report MSR-TR-2002-57. 
Heidorn, G. E. (2000): Intelligent Writing Assistance. 
In "A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text", R. Dale, H. Moisl, and H. 
Somers (ed.), Marcel Dekker, New York, pp. 
181-207. 
Uszkoreit, H., Brants T., Duchier D., Krenn B., 
Konieczny L., Oepen S. and Skut W. (1998). 
Aspekte der Relativsatzextraposition im Deutschen. 
Claus-Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
Part of Speech Tagging in Context 
Michele BANKO and Robert C. MOORE 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{mbanko, bobmoore}@microsoft.com 
 
Abstract 
We present a new HMM tagger that exploits 
context on both sides of a word to be tagged, and 
evaluate it in both the unsupervised and supervised 
case. Along the way, we present the first 
comprehensive comparison of unsupervised 
methods for part-of-speech tagging, noting that 
published results to date have not been comparable 
across corpora or lexicons. Observing that the 
quality of the lexicon greatly impacts the accuracy 
that can be achieved by the algorithms, we present 
a method of HMM training that improves accuracy 
when training of lexical probabilities is unstable. 
Finally, we show how this new tagger achieves 
state-of-the-art results in a supervised, non-training 
intensive framework. 
1 Introduction 
The empiricist revolution in computational 
linguistics has dramatically shifted the accepted 
boundary between what kinds of knowledge are 
best supplied by humans and what kinds are best 
learned from data, with much of the human-
supplied knowledge now being in the form of 
annotations of data.  As we look to the future, we 
expect that relatively unsupervised methods will 
grow in applicability, reducing the need for 
expensive human annotation of data. 
With respect to part-of-speech tagging, we 
believe that the way forward from the relatively 
small number of languages for which we can 
currently identify parts of speech in context with 
reasonable accuracy will make use of unsupervised 
methods that require only an untagged corpus and 
a lexicon of words and their possible parts of 
speech.  We believe this based on the fact that such 
lexicons exist for many more languages (in the 
form of conventional dictionaries) than extensive 
human-tagged training corpora exist for. 
Unsupervised part-of-speech tagging, as defined 
above, has been attempted using a variety of 
learning algorithms (Brill 1995, Church, 1988, 
Cutting et. al. 1992, Elworthy, 1994 Kupiec 1992, 
Merialdo 1991).  While this makes unsupervised 
part-of-speech tagging a relatively well-studied 
problem, published results to date have not been 
comparable with respect to the training and test 
data used, or the lexicons which have been made 
available to the learners.  
In this paper, we provide the first comprehensive 
comparison of methods for unsupervised part-of-
speech tagging.  In addition, we explore two new 
ideas for improving tagging accuracy.  First, we 
explore an HMM approach to tagging that uses 
context on both sides of the word to be tagged, 
inspired by previous work on building 
bidirectionality into graphical models (Lafferty et. 
al. 2001, Toutanova et. al. 2003).   Second we 
describe a method for sequential unsupervised 
training of tag sequence and lexical probabilities in 
an HMM, which we observe leads to improved 
accuracy over simultaneous training with certain 
types of models. 
In section 2, we provide a brief description of 
the methods we evaluate and review published 
results. Section 3 describes the contextualized 
variation on HMM tagging that we have explored. 
In Section 4 we provide a direct comparison of 
several unsupervised part-of-speech taggers, which 
is followed by Section 5, in which we present a 
new method for training with suboptimal lexicons. 
In section 6, we revisit our new approach to HMM 
tagging, this time, in the supervised framework. 
2 Previous Work 
A common formulation of an unsupervised part-of-
speech tagger takes the form of a hidden Markov 
model (HMM), where the states correspond to 
part-of-speech tags, ti, and words, wi, are emitted 
each time a state is visited. The training of HMM?
based taggers involves estimating lexical 
probabilities, P(wi|ti), and tag sequence 
probabilities, P(ti | ti-1 ... ti-n). The ultimate goal of 
HMM training is to find the model that maximizes 
the probability of a given training text, which can 
be done easily using the forward-backward, or 
Baum-Welch algorithm (Baum et al1970, Bahl, 
Jelinek and Mercer, 1983). These model 
probabilities are then used in conjunction with the 
Viterbi algorithm (Viterbi, 1967) to find the most 
probable sequence of part-of-speech tags for a 
given sentence. 
When estimating tag sequence probabilities, an 
HMM tagger, such as that described in Merialdo 
(1991), typically takes into account a history 
consisting of the previous two tags -- e.g. we 
compute  P(ti | ti-1, ti-2). Kupiec (1992) describes a 
modified trigram HMM tagger in which he 
computes word classes for which lexical 
probabilities are then estimated, instead of 
computing probabilities for individual words. 
Words contained within the same equivalence 
classes are those which possess the same set of 
possible parts of speech. 
Another highly-accurate method for part-of-
speech tagging from unlabelled data is Brill?s 
unsupervised transformation-based learner (UTBL) 
(Brill, 1995). Derived from his supervised 
transformation-based tagger (Brill, 1992), UTBL 
uses information from the distribution of 
unambiguously tagged data to make informed 
labeling decisions in ambiguous contexts. In 
contrast to the HMM taggers previously described, 
which make use of contextual information coming 
from the left side only, UTBL considers both left 
and right contexts. 
Reported tagging accuracies for these methods 
range from 87% to 96%, but are not directly 
comparable. Kupiec?s HMM class-based tagger, 
when trained on a sample of 440,000 words of the 
original Brown corpus, obtained a test set accuracy 
of 95.7%. Brill assessed his UTBL tagger using 
350,000 words of the Brown corpus for training, 
and found that 96% of words in a separate 
200,000-word test set could be tagged correctly.  
Furthermore, he reported test set accuracy of 
95.1% for the UTBL tagger trained on 120,000 
words of Penn Treebank and tested on a separate 
test set of 200,000 words taken from the same 
corpus.  Finally, using 1 million words from the 
Associated Press for training, Merialdo?s trigram 
tagger was reported to have an accuracy of 86.6%. 
This tagger was assessed using a tag set other than 
that which is employed by the Penn Treebank.  
Unfortunately none of these results can be 
directly compared to the others, as they have used 
different, randomized and irreproducible splits of 
training and test data (Brill and Kupiec), different 
tag sets (Merialdo) or different corpora altogether.  
The HMM taggers we have discussed so far are 
similar in that they use condition only on left 
context when estimating probabilities of tag 
sequences. Recently, Toutanova et al (2003) 
presented a supervised conditional Markov Model 
part-of-speech tagger (CMM) which exploited 
information coming from both left and right 
contexts.  Accuracy on the Penn Treebank using 
two tags to the left as features in addition to the 
current tag was 96.10%. When using tag to the left 
and tag to the right as features in addition to the 
current tag, accuracy improved to 96.55%.  
Lafferty et al (2001) also compared the 
accuracies of several supervised part-of-speech 
tagging models, while examining the effect of 
directionality in graphical models. Using a 50%-
50% train-test split of the Penn Treebank to assess 
HMMs, maximum entropy Markov models 
(MEMMs) and conditional random fields (CRFs), 
they found that CRFs, which make use of 
observation features from both the past and future, 
outperformed HMMs which in turn outperformed 
MEMMs. 
3 Building More Context into HMM Tagging 
In a traditional HMM tagger, the probability of 
transitioning into a state representing tag ti is 
computed based on the previous two tags ti-1 and ti-
2, and the probability of a word wi is conditioned 
only on the current tag ti. This formulation ignores 
dependencies that may exist between a word and 
the part-of-speech tags of the words which precede 
and follow it. For example, verbs which 
subcategorize strongly for a particular part-of-
speech but can also be tagged as nouns or 
pronouns (e.g. ?thinking that?) may benefit from 
modeling dependencies on future tags. 
To model this relationship, we now estimate the 
probability of a word wi based on tags ti-1 and ti-+1. 
This change in structure, which we will call a 
contextualized HMM, is depicted in Figure 1. This 
type of structure is analogous to context-dependent 
phone models used in acoustic modeling for 
speech recognition (e.g.Young, 1999, Section 4.3). 
 
3.1 Model Definition 
In order to build both left and right-context into an 
HMM part-of-speech tagger, we reformulate the 
 
 
 
Figure 1: Graphical Structure of Traditional 
HMM Tagger (top) and Contextualized HMM 
Tagger (bottom) 
trigram HMM model traditionally described as 
?
=
???? ?=
n
i
iiiiiiiii twtwtpttwtwwpTWp
1
111111 )..|()...|(),(
 
by replacing the approximation: 
 
)|()..|(
)|()...|(
1211
1111
????
??
=
=
iiiiiiii
iiiii
tttptwtwtp
twptwtwwp
 
 
with the approximation: 
)|()..|(
)|()...|(
1211
111111
????
+???
=
=
iiiiiiii
iiiiiii
tttptwtwtp
tttwptwtwwp
 
 
Given that we are using an increased context size 
during the estimation of lexical probabilities, thus 
fragmenting the data, we have found it desirable to 
smooth these estimates, for which we use a 
standard absolute discounting scheme (Ney, Essen 
and Knesser, 1994). 
4 Unsupervised Tagging: A Comparison 
4.1 Corpora and Lexicon Construction 
For our comparison of unsupervised tagging 
methods, we implemented the HMM taggers 
described in Merialdo (1991) and Kupiec (1992), 
as well as the UTBL tagger described in Brill 
(1995). We also implemented a version of the 
contextualized HMM using the type of word 
classes utilized in the Kupiec model. The 
algorithms were trained and tested using version 3 
of the Penn Treebank, using the training, 
development, and test split described in Collins 
(2002) and also employed by Toutanova et al 
(2003) in testing their supervised tagging 
algorithm. Specifically, we allocated sections 00-
18 for training, 19-21 for development, and 22-24 
for testing. To avoid the problem of unknown 
words, each learner was provided with a lexicon 
constructed from tagged versions of the full 
Treebank. We did not begin with any estimates of 
the likelihoods of tags for words, but only the 
knowledge of what tags are possible for each word 
in the lexicon, i.e., something we could obtain 
from a manually-constructed dictionary. 
4.2 The Effect of Lexicon Construction on 
Tagging Accuracy 
To our surprise, we found initial tag accuracies of 
all methods using the full lexicon extracted from 
the Penn Treebank to be significantly lower than 
previously reported. We discovered this was due to 
several factors.  
One issue we noticed which impacted tagging 
accuracy was that of a frequently occurring word  
(a) The/VB Lyneses/NNP ,/, of/IN Powder/NNP 
Springs/NNP ,/, Ga./NNP ,/, have/VBP 
filed/VBN suit/NN in/IN Georgia/NNP 
state/NN court/NN against/IN Stuart/NNP 
James/NNP ,/, *-1/-NONE- alleging/VBG 
fraud/NN ./. 
(b) Last/JJ week/NN CBS/NNP Inc./NNP 
cancelled/VBD ``/`` The/NNP People/NNP 
Next/NNP Door/NNP ./. ''/'' 
(c) a/SYM -/: Discounted/VBN rate/NN ./. 
Figure 2:  Manually-Tagged Examples 
being mistagged during Treebank construction, as 
shown in the example in Figure 2a. Since we are 
not starting out with any known estimates for 
probabilities of tags given a word, the learner 
considers this tag to be just as likely as the word?s 
other, more probable, possibilities. In another, 
more frequently occurring scenario, human 
annotators have chosen to tag all words in multi-
word names, such as titles, with the proper-noun 
tag, NNP (Figure 2b). This has the effect of adding 
noise to the set of tags for many closed-class 
words. 
Finally, we noticed that a certain number of 
frequently occurring words (e.g. a, to, of) are 
sometimes labeled with infrequently occurring tags 
(e.g.  SYM, RB), as exemplified in Figure 2c. In the 
case of the HMM taggers, where we begin with 
uniform estimates of both the state transition 
probabilities and the lexical probabilities, the 
learner finds it difficult to distinguish between 
more and less probable tag assignments. 
We later discovered that previous 
implementations of UTBL involved limiting which 
possible part of speech assignments were placed 
into the lexicon1, which was not explicitly detailed 
in the published reports.  We then simulated, in a 
similar fashion, the construction of higher quality 
lexicons by using relative frequencies of tags for 
each word from the tagged Treebank to limit 
allowable word-tag assignments.  That is, tags that 
appeared the tag of a particular word less than X% 
of the time were omitted from the set of possible 
tags for that word.  We varied this threshold until 
accuracy did not significantly change on our set of 
heldout data. The effect of thresholding tags based 
on relative frequency in the training set is shown 
for our set of part-of-speech taggers in the curve in 
Figure 3. As shown in Table 1, the elimination of 
noisy possible part-of-speech assignments raised 
accuracy back into the realm of previously 
published results. The best test set accuracies for 
the learners in the class of HMM taggers are  
                                                     
1
 Eric Brill, Personal Communication 
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 0.1 0.2 0.3
Threshold
Ta
g 
A
cc
u
ra
c
y
Merialdo Trigram
Contextual Trigram
Kupiec Trigram
UTBL
 
Figure 3:  The effect of lexicon construction on 
unsupervised part-of-speech taggers 
 
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 1 2 3 4 5
Iteration
Ta
g 
A
cc
u
ra
cy
Contextual Trigram
Kupiec Trigram
Merialdo Trigram
 Figure 4: Test Accuracy of HMMs using 
Optimzed Lexicons 
 
 
plotted against the number of training iterations in 
Figure 4. 
5 Unsupervised Training With Noisy 
Lexicons 
While placing informed limitations on the tags that 
can be included in a lexicon can dramatically 
improve results, it is dependent on some form of 
supervision ? either from manually tagged data or 
by a human editor who post-filters an 
automatically constructed list. In the interest of 
being as unsupervised as possible, we sought to 
find a way to cope with the noisy aspects of the  
unfiltered lexicon described in the previous 
section. 
We suspected that in order to better control the 
training of lexical probabilities, having a stable 
model of state transition probabilities would be of 
help. We stabilized this model in two ways. 
 
 
 
 Unfiltered 
Lexicon 
Optimized 
Lexicon 
Merialdo HMM 71.9 93.9 
Contextualized 
HMM 76.9 94.0 
Kupiec HMM 77.1 95.9 
UTBL 77.2 95.9 
Contextualized 
HMM with Classes 77.2 95.9 
Table 1: Tag Accuracy of Unsupervised POS 
Taggers 
 
5.1 Using Unambiguous Tag Sequences To 
Initialize Contextual Probabilities 
First, we used our unfiltered lexicon along with our 
tagged corpus to extract non-ambiguous tag 
sequences. Specifically, we looked for trigrams in 
which all words contained at most one possible 
part-of-speech tag. We then used these n-grams 
and their counts to bias the initial estimates of state 
transitions in the HMM taggers. This approach is 
similar to that described in Ratnaparhki (1998), 
who used unambiguous phrasal attachments to 
train an unsupervised prepositional phrase 
attachment model. 
5.2 HMM Model Training Revised 
Second, we revised the training paradigm for 
HMMs, in which lexical and transition 
probabilities are typically estimated 
simultaneously. We decided to train the transition 
model probabilities first, keeping the lexical 
probabilities constant and uniform. Using the 
estimates initially biased by the method previously 
mentioned, we train the transition model until it 
reaches convergence on a heldout set. We then use 
this model, keeping it fixed, to train the lexical 
probabilities, until they eventually converge on 
heldout data. 
5.3 Results 
We implemented this technique for the Kupiec, 
Merialdo and Contextualized HMM taggers. From 
our training data, we were able to extract data for 
on the order of 10,000 unique unambiguous tag 
sequences which were then be used for better 
initializing the state transition probabilities. As 
shown in Table 2, this method improved tagging 
accuracy of the Merialdo and contextual taggers 
over traditional simultaneous HMM training, 
reducing error by 0.4 in the case of Merialdo and 
0.7 for the contextual HMM part-of-speech tagger.  
 HMM Tagger 
Simultaneous 
Model    
Training 
Sequential 
Model  
Training 
Merialdo 93.9 94.3 
Contextualized 94.0 94.7 
Kupiec 95.9 95.9 
Table 2: Effects of HMM Training on Tagger 
Accuracy 
In this paradigm, tagging accuracy of the Kupiec 
HMM did not change. 
6 Contextualized Tagging with Supervision 
As one more way to assess the potential benefit 
from using left and right context in an HMM 
tagger, we tested our tagging model in the 
supervised framework, using the same sections of 
the Treebank previously allocated for unsupervised 
training, development and testing. In addition to 
comparing against a baseline tagger, which always 
chooses a word?s most frequent tag, we 
implemented and trained a version of a standard 
HMM trigram tagger. For further comparison, we 
evaluated these part of speech taggers against 
Toutanova et als supervised dependency-network 
based tagger, which currently achieves the highest 
accuracy on this dataset to date. The best result for 
this tagger, at 97.24%, makes use of both lexical 
and tag features coming from the left and right 
sides of the target. We also chose to examine this 
tagger?s results when using only <ti, t i-1, t i+1> as 
feature templates, which represents the same 
amount of context built into our contextualized 
tagger.  
As shown in Table 3, incorporating more 
context into an HMM when estimating lexical 
probabilities improved accuracy from 95.87% to 
96.59%, relatively reducing error rate by 17.4%. 
With the contextualized tagger we witness a small 
improvement in accuracy over the current state of 
the art when using the same amount of context. It 
is important to note that this accuracy can be 
obtained without the intensive training required by 
Toutanova et. al?s log-linear models. This result 
falls only slightly below the full-blown training-
intensive dependency-based conditional model. 
7 Conclusions 
We have presented a comprehensive evaluation of 
several methods for unsupervised part-of-speech 
tagging, comparing several variations of hidden 
Markov model taggers and unsupervised 
transformation-based learning using the same 
corpus and same lexicons.  We discovered that the  
 
 
Supervised Tagger Test Accuracy 
Baseline 92.19 
Standard HMM 95.87 
Contextualized HMM 96.59 
Dependency  
Using LR tag features 96.55 
Dependency  
Best Feature Set 97.24 
Table 3: Comparison of Supervised Taggers 
quality of the lexicon made available to 
unsupervised learner made the greatest difference 
to tagging accuracy. Filtering the possible part-of-
speech assignments contained in a basic lexicon 
automatically constructed from the commonly-
used Penn Treebank improved results by as much 
as 22%. This finding highlights the importance of 
the need for clean dictionaries whether they are 
constructed by hand or automatically when we 
seek to be fully unsupervised. 
In addition, we presented a variation on HMM 
model training in which the tag sequence and 
lexical probabilities are estimated in sequence. 
This helped stabilize training when estimation of 
lexical probabilities can be noisy. 
Finally, we experimented with using left and 
right context in the estimation of lexical 
probabilities, which we refer to as a contextualized 
HMM. Without supervision, this new HMM 
structure improved results slightly compared to a 
simple trigram tagger as described in Merialdo, 
which takes into account only the current tag in 
predicting the lexical item.  With supervision, this 
model achieves state of the art results without the 
lengthy training procedure involved in other high-
performing models. In the future, we will consider 
making an increase the context-size, which helped 
Toutanova et al (2003). 
8 Acknowledgements 
The authors wish to thank Gideon Mann for 
performing some initial experiments with a 
publicly available implementation of UTBL, and 
Eric Brill for discussions regarding his work on 
unsupervised transformation based learning. 
 
References  
L.R. Bahl, F. Jelinek, and R. Mercer. 1983. A 
maximum likelihood approach to continuous 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
5(2):179--190. 
L.E. Baum, T. Petrie, G. Soules, and N. Weiss.  A 
maximization technique in the statistical analysis 
of probabilistic functions of Markov chains. 
Annals of Mathematical Statistics, 41:164-171. 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings of the Third Conference 
on Applied Natural Language Processing, ACL. 
Trento, Italy. 
E. Brill. 1995. Unsupervised learning of 
disambiguation rules for part of speech tagging. 
In Proceedings of the Third Workshop on Very 
Large Corpora, Cambridge, MA. 
K. Church. 1998. A stochastic parts program and 
noun phrase parser for unrestricted text. In 
Second Conference on Applied Natural 
Language Processing, ACL. 
M. Collins. 2002. Discriminative training methods 
for hidden Markov models: theory and 
experiments with perceptron algorithms. In 
Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, 
Philadelphia, PA. 
D. Cutting, J. Kupiec, J. Pedersen and P. Sibun. 
1992. A practical part-of-speech tagger. In Third 
Conference on Applied Natural Language 
Processing. ACL. 
D. Elworthy. 1994. Does Baum-Welch re-
estimation help taggers. In Proceedings of the 
Fourth Conference on Applied Natural 
Language Processing, ACL. 
J. Kupiec. 1992. Robust part-of-speech tagging 
using a hidden Markov model. Computer Speech 
and Language 6. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In 
Proceedings of ICML-01, pages 282-289. 
B. Merialdo. 1991. Tagging English text with a 
probabilistic model. In Proceedings of ICASSP. 
Toronto, pp. 809-812. 
H. Ney, U. Essen and R. Kneser. 1994. On 
structuring probabilistic dependencies in 
stochastic language modeling. Computer, Speech 
and Language, 8:1-38. 
A. Ratnaparkhi. 1998. Unsupervised statistical 
models for prepositional phrase attachment. In 
Proceedings of the Seventeenth International 
Conference on Computational Linguistics. 
Montreal,  Canada. 
K. Toutanova, D. Klein, C. Manning, and Y. 
Singer. 2003. Feature-Rich Part-of-Speech 
Tagging with a Cyclic Dependency Network. In 
Proceedings of HLT-NAACL. pp. 252-259. 
A.J. Viterbi. 1967. Error bounds for convolutional 
codes and an asymptotically optimal decoding 
algorithm. IEEE Transactions on Information 
Theory, 13:260--269.  
S. Young. 1999. Acoustic modelling for large 
vocabulary continuous speech recognition. 
Computational Models of Speech Pattern 
Processing: Proc NATO Advance Study Institute. 
K. Ponting, Springer-Verlag: 18-38. 
 
Linguistically Informed Statistical Models of Constituent Structure for 
Ordering in Sentence Realization 
Eric RINGGER1, Michael GAMON1, Robert C. MOORE1, 
David ROJAS2, Martine SMETS1, Simon CORSTON-OLIVER1 
 
1Microsoft Research 
One Microsoft Way 
Redmond, Washington 98052, USA 
{ringger, mgamon, bobmoore, msmets, 
simonco}@microsoft.com 
2Butler Hill Group, LLC 
& Indiana University Linguistics Dept. 
1021 East 3rd Street, MM 322 
Bloomington, Indiana 47405, USA 
drojas@indiana.edu 
 
 
Abstract 
We present several statistical models of syntactic 
constituent order for sentence realization. We 
compare several models, including simple joint 
models inspired by existing statistical parsing 
models, and several novel conditional models. The 
conditional models leverage a large set of linguistic 
features without manual feature selection. We apply 
and evaluate the models in sentence realization for 
French and German and find that a particular 
conditional model outperforms all others. We 
employ a version of that model in an evaluation on 
unordered trees from the Penn TreeBank. We offer 
this result on standard data as a reference-point for 
evaluations of ordering in sentence realization. 
1 Introduction 
Word and constituent order play a crucial role in 
establishing the fluency and intelligibility of a 
sentence. In some systems, establishing order 
during the sentence realization stage of natural 
language generation has been accomplished by 
hand-crafted generation grammars in the past (see 
for example, Aikawa et al (2001) and Reiter and 
Dale (2000)). In contrast, the Nitrogen (Langkilde 
and Knight, 1998a, 1998b) system employs a word 
n-gram language model to choose among a large 
set of word sequence candidates which vary in 
constituent order, word order, lexical choice, and 
morphological inflection. Nitrogen?s model does 
not take into consideration any non-surface 
linguistic features available during realization.  
The Fergus system (Bangalore and Rambow, 
2000) employs a statistical tree model to select 
probable trees and a word n-gram model to rank 
the string candidates generated from the best trees. 
Like Nitrogen, the HALogen system (Langkilde, 
2000; Langkilde-Geary, 2002a, 2002b) uses word 
n-grams, but it extracts the best-scoring surface 
realizations efficiently from a packed forest by 
constraining the search first within the scope of 
each constituent.  
Our research is carried out within the Amalgam 
broad coverage sentence realization system. 
Amalgam generates sentence strings from abstract 
predicate-argument structures (Figure 1), using a 
pipeline of stages, many of which employ 
machine-learned models to predict where to 
perform specific linguistic operations based on the 
linguistic context (Corston-Oliver et al, 2002; 
Gamon et al, 2002a, 2002b; Smets et al, 2003). 
Amalgam has an explicit ordering stage that 
determines the order of constituents and their 
daughters. The input for this stage is an unordered 
tree of constituents; the output is an ordered tree of 
constituents or a ranked list of such trees. For 
ordering, Amalgam leverages tree constituent 
structure and, importantly, features of those 
constituents and the surrounding context. By 
separately establishing order within constituents, 
Amalgam heavily constrains the possible 
alternatives in later stages of the realization 
process.  The design allows for interaction between 
ordering choices and other realization decisions, 
such as lexical choice (not considered in the 
present work), through score combinations from 
distinct Amalgam pipeline stages. 
Most previous research into the problem of 
establishing order for sentence realization has 
focused on English, a language with fairly strict 
word and constituent order. In the experiments 
described here we first focus on German and 
French which present novel challenges.1 We also 
describe an English experiment involving data 
from the Penn Treebank. Our ultimate goal is to 
develop a model that handles all ordering 
phenomena in a unified and elegant way across 
typologically diverse languages. In the present 
paper, we explore the space of possible models and 
examine some of these closely. 
                                                          
1
 For an overview of some of the issues in 
determining word and constituent order in German and 
French, see (Ringger et al, 2003).  
 Figure 1: Abstract predicate-argument structure (NLPWin logical form) for the German sentence: 
In der folgenden Tabelle werden die Optionen sowie deren Funktionen aufgelistet. 
(The options and their functions are listed in the following table.) 
2 Models of Constituent Order 
In order to develop a model of constituent 
structure that captures important order phenomena, 
we will consider the space of possible joint and 
conditional models in increasing complexity. For 
each of the models, we will survey the 
independence assumptions and the feature set used 
in the models. 
Our models differ from the previous statistical 
approaches in the range of input features. Like the 
knowledge-engineered approaches, the models 
presented here incorporate lexical features, parts-
of-speech, constituent-types, constituent 
boundaries, long-distance dependencies, and 
semantic relations between heads and their 
modifiers. 
Our experiments do not cover the entire space of 
possible models, but we have chosen significant 
points in the space for evaluation and comparison. 
2.1 Joint Models 
We begin by considering joint models of 
constituent structure of the form ( ),P ? ?  over 
ordered syntax trees ?  and unordered syntax trees 
? . An ordered tree ?  contains non-terminal 
constituents C, each of which is the parent of an 
ordered sequence of daughters ( 1,..., nD D ), one of 
which is the head constituent H.2 Given an ordered 
tree ? , the value of the function 
_ ( )unordered tree ?  is an unordered tree ?  
corresponding to ?  that contains a constituent B 
for each C in ? , such that 
( ) ( )
1
_ ( )
{ ,..., }n
unordered set daughters Cdaughters B
D D
=
=
 
again with iH D=  for some i in ( )1..n . The 
hierarchical structure of ?  is identical to that of 
? . 
We employ joint models for scoring alternative 
ordered trees as follows: given an unordered 
syntax tree ? , we want the ordered syntax tree ??  
that maximizes the joint probability: 
                                                          
2 All capital Latin letters denote constituents, and 
corresponding lower-case Latin letters denote their 
labels (syntactic categories). 
( ) ( )
: _ ( )
? arg max , arg max
unordered tree
P P
? ? ? ?
? ? ? ?
=
= =    (1) 
As equation (1) indicates, we can limit our search 
to those trees ?  which are alternative orderings of 
the given tree ? . 
Inter-dependencies among ordering decisions 
within different constituents (e.g., for achieving 
parallelism) make the global sentence ordering 
problem challenging and are certainly worth 
investigating in future work.  For the present, we 
constrain the possible model types considered here 
by assuming that the ordering of any constituent is 
independent of the ordering within other 
constituents in the tree, including its daughters; 
consequently, 
( ) ( )
( )C constits
P P C
?
?
?
= ?  
Given this independence assumption, the only 
possible ordered trees are trees built with non-
terminal constituents computed as follows: for 
each ( )B constits ?? , 
( )
: _ ( )
* arg max
C B unordered set C
C P C
=
=  
In fact, we can further constrain our search for the 
best ordering of each unordered constituent B, 
since C?s head must match B?s head: 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C
=
=
=  
Thus, we have reduced the problem to finding the 
best ordering of each constituent of the unordered 
tree. 
Now if we wish to condition on some feature ( )x f ?= , then we must first predict it as follows: 
( ) ( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P x P C x
=
=
=  
If x is truly a feature of ?  and does not depend on 
any particular ordering of any constituent in ? , 
then ( )P x  is constant, and we do not need to 
compute it in practice. In other words, 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C x
=
=
=       (2) 
Hence, even for a joint model ( )P C , we can 
condition on features that are fixed in the given 
unordered tree ?  without first predicting them. 
The joint models described here are of this form. 
For this reason, when we describe a distribution ( )P C x , unless we explicitly state otherwise, we 
are actually describing the part of the joint model 
that is of interest. As justified above, we do not 
need to compute ( )P x  and will simply present 
alternative forms of ( )P C x . 
We can factor a distribution ( )P C x  in many 
different ways using the chain rule. As our starting 
point we adopt the class of models called Markov 
grammars.3 We first consider a left-to-right 
Markov grammar of order j that expands C by 
predicting its daughters 1,..., nD D  from left-to-
right, one at a time, as shown in Figure 2: in the 
figure. iD  depends only on ( i jD ? , ?, 1iD ? ), and 
the parent category C ., according to the 
distribution in equation (3). 
 
i?
Figure 2: Left-to-right Markov grammar. 
( ) ( )1
1
,..., , ,
n
i i i j
i
P C h P d d d c h
? ?
=
= ?  (3) 
In order to condition on another feature of each 
ordered daughter iD , such as its semantic relation 
i? to the head constituent H, we also first predict 
it, according to the chain rule. The result is the 
semantic Markov grammar in equation (4):  
( ) ( )( )
1 1
1 1 1
, ,..., , , ,
, , ,..., , , ,
n i i i i j i j
i i i i i i j i j
P d d c h
P C h
P d d d c h
? ? ?
? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (4) 
Thus, the model predicts semantic relation i? and 
then the label id  in the context of that semantic 
relation. We will refer to this model as Type 1 
(T1). 
As an extension to model Type 1, we include 
features computed by the following functions on 
the set i?  of daughters of C already ordered (see 
Figure 2): 
? Number of daughters already ordered (size 
of i? ) 
? Number of daughters in i?  having a 
particular label for each of the possible 
constituent labels {NP, AUXP, VP, etc.} 
(24 for German, 23 for French) 
We denote that set of features in shorthand as ( )if ? . With this extension, a model of Markov 
                                                          
3
 A ?Markov grammar? is a model of constituent 
structure that starts at the root of the tree and assigns 
probability to the expansion of a non-terminal one 
daughter at a time, rather than as entire productions 
(Charniak, 1997 & 2000). 
order j can potentially have an actual Markov order 
greater than j. Equation (5) is the extended model, 
which we will refer to as Type 2 (T2): 
( ) ( )( )( )( )
1 1
1 1 1
, ,..., , , , ,
, , ,..., , , , ,
n i i i i j i j i
i i i i i i j i j i
P d d c h f
P C h
P d d d c h f
? ? ? ?
? ? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (5) 
As an alternative to a left-to-right expansion, we 
can also expand a constituent in a head-driven 
fashion. We refer the reader to (Ringger et al, 
2003) for details and evaluations of several head-
driven models (the missing ?T3?, ?T4?, and ?T6? 
in this discussion). 
2.2 Conditional Models 
We now consider more complex models that use 
additional features. We define a function ( )g X on 
constituents, where the value of ( )g X represents a 
set of many lexical, syntactic, and semantic 
features of X (see section 5.2 for more details). No 
discourse features are included for the present 
work. We condition on 
? ( )g B , where B is the unordered constituent 
being ordered 
? ( )g H , where H is the head of B 
? ( )Bg P , where BP  is the parent of B, and 
? ( )Bg G , where BG  is the grandparent of B. 
These features are fixed in the given unordered tree 
? , as in the discussion of equation (2), hence the 
resulting complex model is still a joint model.   
Up until this point, we have been describing joint 
generative models that describe how to generate an 
ordered tree from an unordered tree. These models 
require extra effort and capacity to accurately 
model the inter-relations among all features. Now 
we move on to truly conditional models by 
including features that are functions on the set i?  
of daughters of C yet to be ordered. In the 
conditional models we do not need to model the 
interdependencies among all features. We include 
the following: 
? Number of daughters remaining to be 
ordered (size of i? ) 
? Number of daughters in i?  having a 
particular label 
As before, we denote these feature sets in 
shorthand as ( )if ? . The resulting distribution is 
represented in equation (6), which we will refer to 
as Type 5 (T5): 
( )
( )
( )
1 1
1 1 1
( ), ( ), ( ), ( )
, ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
, , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
B B
i i i j i j
i
n B B i i
i i i i i j i j
i
B B i i
P C g H g B g P g G
d d c h
P
g H g B g P g G f f
d d c h
P d
g H g B g P g G f f
? ?
?
? ?
? ? ?
? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
=
? ?
? ?
? ?
?
? ?
? ?
? ?
? ?
? ?
? ?
?
   (6) 
All models in this paper are nominally Markov 
order 2, although those models incorporating the 
additional feature functions ( )if ?  and ( )if ?  
defined in Section 2.2 can be said to have higher 
order. 
2.3 Binary conditional model 
We introduce one more model type called the 
binary conditional model. It estimates a much 
simpler distribution over the binary variable ?  
called ?sort-next? with values in {yes, no} 
representing the event that an as-yet unordered 
member D of i?  (the set of as-yet unordered 
daughters of parent C, as defined above) should be 
?sorted? next, as illustrated in Figure 3. 
i?i?
?
 
Figure 3: Binary conditional model. 
The conditioning features are almost identical to 
those used in the left-to-right conditional models 
represented in equation (6) above, except that id  
and i?  (the semantic relation of D with head H) 
appear in the conditional context and need not first 
be predicted. In its simple form, the model 
estimates the following distribution: 
( )
1 1, , , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
i i i i i j i j
i
B B i i
d d d c h
P
g H g B g P g G f f
? ? ?
?
? ?
? ? ? ?
? ?
? ?
? ?
? ?
   (7) 
In our shorthand, we will call this Type 7 (T7). We 
describe how to apply this model directly in a left-
to-right ?sorting? search later in the section on 
search. 
3 Estimation 
We estimate a model?s distributions with 
probabilistic decision trees (DTs).4 We build 
decision trees using the WinMine toolkit 
(Chickering, 2002). WinMine-learned decision 
trees are not just classifiers; each leaf is a 
conditional probability distribution over the target 
random variable, given all features available in 
training; hence the tree as a whole is an estimate of 
the conditional distribution of interest. The primary 
advantage of using decision trees, is the automatic 
feature selection and induction from a large pool of 
features. 
We train four models for German and French 
each. One model is joint (T1); one is joint with 
additional features on the set of daughters already 
ordered (T2); one is conditional (T5). In addition, 
we employ one binary conditional DT model (T7), 
both with and without normalization (see equation 
(8)). 
                                                          
4
 Other approaches to feature selection, feature 
induction, and distribution estimation are certainly 
possible, but they are beyond the scope of this paper. 
One experiment using interpolated language modeling 
techniques is described in (Ringger et al, 2003) 
4 Search 
4.1 Exhaustive search 
Given an unordered tree ?  and a model of 
constituent structure O of any of the types already 
presented, we search for the best ordered tree ?  
that maximizes ( )OP ?  or ( )OP ? ? , as 
appropriate, with the context varying according to 
the complexity of the model. Each of our models 
(except the binary conditional model) estimates the 
probability of an ordering of any given constituent 
C in ? , independently of the ordering inside other 
constituents in ? . The complete search is a 
dynamic programming (DP) algorithm, either left-
to-right in the daughters of C (or head-driven, 
depending on the model type). The search can 
optionally maintain one non-statistical constraint 
we call Input-Output Coordination Consistency 
(IOCC), so that the order of coordinated 
constituents is preserved as they were specified in 
the given unordered tree. For these experiments, 
we employ the constraint. 
4.2 Greedy search for binary conditional 
model 
The binary conditional model can be applied in a 
left-to-right ?sorting? mode (Figure 3). At stage i, 
for each unordered daughter jD , in i? , the model 
is consulted for the probability of j yes? = , 
namely the probability that jD  should be placed to 
the right of the already ordered sister constituents 
i? . The daughter in i?  with the highest 
probability is removed from i?  to produce 1i? +  
and added to the right of i? to produce 1i? + . The 
search proceeds through the remaining unordered 
constituents until all constituents have been 
ordered in this greedy fashion. 
4.3 Exhaustive search for binary conditional 
model 
In order to apply the binary conditional model in 
the exhaustive DP search, we normalize the model 
at every stage of the search and thereby coerce it 
into a probability distribution over the remaining 
daughters in i? . We represent the distribution in 
?equation? (7) in short-hand as ( ), , iP d? ? ? , 
with i?  representing the contextual features for the 
given search hypothesis at search stage i. Thus, our 
normalized distribution for stage i is given by 
equation (8). Free variable j represents an index on 
unordered daughters in i? , as does k. 
( ) ( )
( )
1
, ,
, ,
, ,
i
j j j i
j j j i
k k k i
k
P yes d
P D d
P yes d
?
? ?
?
? ?
=
= ?
? =
= ?
?
 (8) 
This turns out to be the decision tree analogue of a 
Maximum Entropy Markov Model (MEMM) 
(McCallum et al, 2000), which we can refer to as a 
DTMM. 
5 Experiments 
5.1 Training 
We use a training set of 20,000 sentences, both 
for French and German. The data come from 
technical manuals in the computer domain. For a 
given sentence in our training set, we begin by 
analyzing the sentence as a surface syntax tree and 
an abstract predicate argument structure using the 
NLPWin system (Heidorn, 2000). By consulting 
these two linked structures, we produce a tree with 
all of the characteristics of trees seen by the 
Amalgam ordering stage at generation run-time 
with one exception: these training trees are 
properly ordered. The training trees include all 
features of interest, including the semantic 
relations among a syntactic head and its modifiers. 
We train our order models from the constituents of 
these trees. NLPWin parser output naturally 
contains errors; hence, the Amalgam training data 
is imperfect. 
5.2 Selected Features 
A wide range of linguistic features is extracted 
for the different decision tree models. The number 
of selected features for German reaches 280 (out of 
651 possible features) in the binary conditional 
model T7. For the French binary conditional 
model, the number of selected features is 218 (out 
of 550). The binary conditional models draw from 
the full set of available features, including: 
? lexical sub-categorization features such as 
transitivity and compatibility with clausal 
complements 
? lemmas (word-stems) 
? semantic features such as the semantic 
relation and the presence of 
quantificational operators 
? length of constituent in words 
? syntactic information such as the label and 
the presence of syntactic modifiers 
5.3 Evaluation 
To evaluate the constituent order models in 
isolation, we designed our experiments to be 
independent of the rest of the Amalgam sentence 
realization process. We use test sets of 1,000 
sentences, also from technical manuals, for each 
language. To isolate ordering, for a given test 
sentence, we process the sentence as in training to 
produce an ordered tree ?  (the reference for 
evaluation) and from it an unordered tree ? . 
Given ? , we then search for the best ordered tree 
hypothesis ??  using the model in question. 
We then compare ?  and ?? . Because we are 
only ordering constituents, we can compare ? and 
??  by comparing their respective constituents. For 
each C in ? , we measure the per-constituent edit 
distance D, between C and its counterpart C? in ??  
as  follows: 
1. Let d be the edit distance between the 
ordered set of daughters in each, with the 
only possible edit operators being insert and 
delete 
2. Let the number of moves / 2m d= , since 
insertions and deletions can be paired 
uniquely 
3. Divide by the total number of 
daughters: ( )/D m daughters C=  
This metric is like the ?Generation Tree Accuracy? 
metric of Bangalore & Rambow (2000), except 
that there is no need to consider cross-constituent 
moves. The total score for the hypothesis tree ??  is 
the mean of the per-constituent edit distances. 
For each of the models under consideration and 
each language, we report in Table 1 the average 
score across the test set for the given language. The 
first row is a baseline computed from randomly 
scrambling constituents (mean over four 
iterations). 
Model German French 
Baseline (random) 35.14 % 34.36 % 
T1: DT joint 5.948% 3.828% 
T2: DT joint 
with ( )if ?   5.852% 4.008% 
T5: DT conditional 6.053% 4.271% 
T7: DT binary cond., 
greedy search 3.516% 1.986% 
T7: DT normalized 
binary conditional, 
exhaustive search 
3.400% 1.810% 
Table 1: Mean per-constituent edit distances for 
German & French. 
5.4 Discussion 
For both German and French, the binary 
conditional DT model outperforms all other 
models. Normalizing the binary conditional model 
and applying it in an exhaustive search performs 
better than a greedy search. All score differences 
are statistically significant; moreover, manual 
inspection of the differences for the various models 
also substantiates the better quality of those models 
with lower scores. 
With regard to the question of conditional versus 
joint models, the joint models (T1, T2) outperform 
their conditional counterparts (T5). This may be 
due to a lack of sufficient training data for the 
conditional models. At this time, the training time 
of the conditional models is the limiting factor. 
There is a clear disparity between the 
performance of the German models and the 
performance of the French models. The best 
German model is twice as bad as the best French 
model.  (For a discussion of the impact of 
modeling German verb position, please consult 
(Ringger et al, 2003).) 
 Baseline 
(random) 
Greedy, 
IOCC Greedy 
DP,  
IOCC DP 
Total Sentences 2416 2416 2416 2416 2416 
Mean Tokens/Sentence 23.59 23.59 23.59 23.59 23.59 
Time/Input (sec.) n/a 0.01 0.01 0.39 0.43 
Exact Match 0.424% 33.14% 27.53% 33.53% 35.72% 
Coverage 100% 100% 100% 100% 100% 
Mean Per-Const. Edit Dist. 38.3% 6.02% 6.84% 5.25% 4.98% 
Mean NIST SSA -16.75 74.98 67.19 74.65 73.24 
Mean IBM Bleu Score 0.136 0.828 0.785 0.817 0.836 
Table 2: DSIF-Amalgam ordering performance on WSJ section 23. 
6 Evaluation on the Penn TreeBank 
Our goal in evaluating on Penn Tree Bank (PTB) 
data is two-fold: (1) to enable a comparison of 
Amalgam?s performance with other systems 
operating on similar input, and (2) to measure 
Amalgam?s capabilities on less domain-specific 
data than technical software manuals. We derive 
from the bracketed tree structures in the PTB using 
a deterministic procedure an abstract 
representation we refer to as a Dependency 
Structure Input Format (DSIF), which is only 
loosely related to NLPWin?s abstract predicate-
argument structures. 
The PTB to DSIF transformation pipeline 
includes the following stages, inspired by 
Langkilde-Geary?s (2002b) description: 
A. Deserialize the tree 
B. Label heads, according to Charniak?s head 
labeling rules (Charniak, 2000) 
C. Remove empty nodes and flatten any 
remaining empty non-terminals 
D. Relabel heads to conform more closely to the 
head conventions of NLPWin 
E. Label with logical roles, inferred from PTB 
functional roles 
F. Flatten to maximal projections of heads 
(MPH), except in the case of conjunctions 
G. Flatten non-branching non-terminals 
H. Perform dictionary look-up and 
morphological analysis 
I. Introduce structure for material between 
paired delimiters and for any coordination 
not already represented in the PTB 
J. Remove punctuation 
K. Remove function words 
L. Map the head of each maximal projection to 
a dependency node, and map the head?s 
modifiers to the first node?s dependents, 
thereby forming a complete dependency tree. 
To evaluate ordering performance alone, our data 
are obtained by performing all of the steps above 
except for (J) and (K). We employ only a binary 
conditional ordering model, found in the previous 
section to be the best of the models considered. To 
train the order models, we use a set of 10,000 
sentences drawn from the standard PTB training 
set, namely sections 02?21 from the Wall Street 
Journal portion of the PTB (the full set contains 
approx. 40,000 sentences). For development and 
parameter tuning we used a separate set of 2000 
sentences drawn from sections 02?21. 
Decision trees are trained for each of five 
constituent types characterized by their head 
labels: adjectival, nominal, verbal, conjunctions 
(coordinated material), and other constituents not 
already covered. The split DTs can be thought of 
as a single DT with a five-way split at the top 
node. 
Our DSIF test set consists of the blind test set 
(section 23) of the WSJ portion of the PTB. At 
run-time, for each converted tree in the test set, all 
daughters of a given constituent are first permuted 
randomly with one another (scrambled), with the 
option for coordinated constituents to remain 
unscrambled, according to the Input-Output 
Coordination Consistency (IOCC) option. For a 
given unordered (scrambled) constituent, the 
appropriate order model (noun-head, verb-head, 
etc.) is used in the ordering search to order the 
daughters. Note that for the greedy search, the 
input order can influence the final result; therefore, 
we repeat this process for multiple random 
scramblings and average the results. 
We use the evaluation metrics employed in 
published evaluations of HALogen, FUF/SURGE, 
and FERGUS (e.g., Calloway, 2003), although our 
results are for ordering only. Coverage, or the 
percentage of inputs for which a system can 
produce a corresponding output, is uninformative 
for the Amalgam system, since in all cases, it can 
generate an output for any given DSIF. In addition 
to processing time per input, we apply four other 
metrics: exact match, NIST simple string accuracy 
(the complement of the familiar word error rate), 
the IBM Bleu score (Papineni et al, 2001), and the 
intra-constituent edit distance metric introduced 
earlier. 
We evaluate against ideal trees, directly 
computed from PTB bracketed tree structures. The 
results in Table 2 show the effects of varying the 
IOCC parameter. For both trials involving a greedy 
search, the results were averaged across 25 
iterations. As should be expected, turning on the 
input-output faithfulness option (IOCC) improves 
the performance of the greedy search. Keeping 
coordinated material in the same relative order 
would only be called for in applications that plan 
discourse structure before or during generation. 
7 Conclusions and Future Work 
The experiments presented here provide 
conclusive reasons to favor the binary conditional 
model as a model of constituent order. The 
inclusion of linguistic features is of great value to 
the modeling of order, specifically in verbal 
constituents for both French and German. 
Unfortunately space did not permit a thorough 
discussion of the linguistic features used. Judging 
from the high number of features that were 
selected during training for participation in the 
conditional and binary conditional models, the 
availability of automatic feature selection is 
critical. 
Our conditional and binary conditional models 
are currently lexicalized only for function words; 
the joint models not at all. Experiments by Daum? 
et al(2002) and the parsing work of Charniak 
(2000) and others indicate that further 
lexicalization may yield some additional 
improvements for ordering. However, the parsing 
results of Klein & Manning (2003) involving 
unlexicalized grammars suggest that gains may be 
limited. 
For comparison, we encourage implementers of 
other sentence realization systems to conduct 
order-only evaluations using PTB data. 
Acknowledgements 
We wish to thank Irene Langkilde-Geary and 
members of the MSR NLP group for helpful 
discussions.  Thanks also go to the anonymous 
reviewers for helpful feedback. 
References  
Aikawa T., Melero M., Schwartz L. Wu A. 2001. 
Multilingual sentence generation. In Proc. of 8th 
European Workshop on NLG. pp. 57-63. 
Bangalore S. Rambow O. 2000. Exploiting a 
probabilistic hierarchical model for generation. 
In Proc. of COLING. pp. 42-48. 
Calloway, C. 2003. Evaluating Coverage for Large 
Symbolic NLG Grammars.  In Proc. of IJCAI 
2003. pp 811-817. 
Charniak E. 1997. Statistical Techniques for 
Natural Language Parsing, In AI Magazine. 
Charniak E. 2000. A Maximum-Entropy-Inspired 
Parser. In Proc. of ACL. pp.132-139. 
Chickering D. M. 2002. The WinMine Toolkit. 
Microsoft Technical Report 2002-103. 
Corston-Oliver S., Gamon M., Ringger E., Moore 
R. 2002. An overview of Amalgam: a machine-
learned generation module. In Proc. of INLG. 
pp.33-40. 
Daum? III H., Knight K., Langkilde-Geary I., 
Marcu D., Yamada K. 2002. The Importance of 
Lexicalized Syntax Models for Natural 
Language Generation Tasks. In Proc. of INLG. 
pp. 9-16. 
Gamon M., Ringger E., Corston-Oliver S. 2002a. 
Amalgam: A machine-learned generation 
module. Microsoft Technical Report 2002-57. 
Gamon M., Ringger E., Corston-Oliver S., Moore 
R. 2002b. Machine-learned contexts for 
linguistic operations in German sentence 
realization. In Proc. of ACL. pp. 25-32. 
Heidorn G. 2000. Intelligent Writing Assistance. In 
A Handbook of Natural Language Processing,, 
R. Dale, H. Moisl, H. Somers (eds.). Marcel 
Dekker, NY. 
Klein D., Manning C. 2003. "Accurate 
Unlexicalized Parsing." In Proceedings of ACL-
03. 
Langkilde I. 2000. Forest-Based Statistical 
Sentence generation. In Proc. of NAACL. pp. 
170-177. 
Langkilde-Geary I. 2002a. An Empirical 
Verification of Coverage and Correctness for a 
General-Purpose Sentence Generator. In Proc. of 
INLG. pp.17-24. 
Langkilde-Geary, I. 2002b. A Foundation for 
General-purpose Natural Language Generation: 
Sentence Realization Using Probabilistic Models 
of Language. PhD Thesis, University of 
Southern California. 
Langkilde I., Knight K. 1998a. The practical value 
of n-grams in generation. In Proc. of 9th 
International Workshop on NLG. pp. 248-255. 
Langkilde I., Knight K. 1998b. Generation that 
exploits corpus-based statistical knowledge. In 
Proc. of ACL and COLING. pp. 704-710. 
McCallum A., Freitag D., & Pereira F. 2000. 
?Maximum Entropy Markov Models for 
Information Extraction and Segmentation.? In 
Proc. Of ICML-2000. 
Papineni, K.A., Roukos, S., Ward, T., and Zhu, 
W.J. 2001. Bleu: a method for automatic 
evaluation of machine translation. IBM 
Technical Report RC22176 (W0109-022). 
Reiter E. and Dale R. 2000. Building natural 
language generation systems. Cambridge 
University Press, Cambridge. 
Ringger E., Gamon M., Smets M., Corston-Oliver 
S. and Moore R. 2003 Linguistically informed 
models of constituent structure for ordering in 
sentence realization. Microsoft Research 
technical report MSR-TR-2003-54. 
Smets M., Gamon M., Corston-Oliver S. and 
Ringger E. (2003) The adaptation of a machine-
learned sentence realization system to French. 
In Proceedings of EACL. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 585?592
Manchester, August 2008
Random Restarts in Minimum Error Rate Training for Statistical
Machine Translation
Robert C. Moore and Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
bobmoore@microsoft.com, chrisq@microsoft.com
Abstract
Och?s (2003) minimum error rate training
(MERT) procedure is the most commonly
used method for training feature weights in
statistical machine translation (SMT) mod-
els. The use of multiple randomized start-
ing points in MERT is a well-established
practice, although there seems to be no
published systematic study of its bene-
fits. We compare several ways of perform-
ing random restarts with MERT. We find
that all of our random restart methods out-
perform MERT without random restarts,
and we develop some refinements of ran-
dom restarts that are superior to the most
common approach with regard to resulting
model quality and training time.
1 Introduction
Och (2003) introduced minimum error rate train-
ing (MERT) for optimizing feature weights in sta-
tistical machine translation (SMT) models, and
demonstrated that it produced higher translation
quality scores than maximizing the conditional
likelihood of a maximum entropy model using the
same features. Och?s method performs a series
of one-dimensional optimizations of the feature
weight vector, using an innovative line search that
takes advantage of special properties of the map-
ping from sets of feature weights to the resulting
translation quality measurement. Och?s line search
is guaranteed to find a global optimum, whereas
more general line search methods are guaranteed
only to find a local optimum.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Global optimization along one dimension at a
time, however, does not insure global optimization
in all dimensions. Hence, as Och briefly men-
tions, ?to avoid finding a poor local optimum,?
MERT can be augmented by trying multiple ran-
dom starting points for each optimization search
within the overall algorithm. However, we are not
aware of any published study of the effects of ran-
dom restarts in the MERT optimization search.
We first compare a variant of Och?s method with
and without multiple starting points for the op-
timization search, selecting initial starting points
randomly according to a uniform distribution. We
find that using multiple random restarts can sub-
stantially improve the resulting model in terms of
translation quality as measured by the BLEU met-
ric, but that training time also increases substan-
tially. We next try selecting starting points by a
random walk from the last local optimum reached,
rather than by sampling from a uniform distrib-
ution. We find this provides a slight additional
improvement in BLEU score, and is significantly
faster, although still slower than training without
random restarts.
Finally we look at two methods for speeding up
training by pruning the set of hypotheses consid-
ered. We find that, under some circumstances, this
can speed up training so that it takes very little
additional time compared to the original method
without restarts, with no significant reduction in
BLEU score compared to the best training methods
in our experiments.
2 Och?s MERT procedure
While minimum error rate training for SMT is
theoretically possible by directly applying gen-
eral numerical optimization techniques, such as
the downhill simplex method or Powell?s method
585
(Press, 2002), naive use of these techniques would
involve repeated translation of the training sen-
tences using hundreds or thousands of combina-
tions of feature weights, which is clearly impracti-
cal given the speed of most SMT decoders.
Och?s optimization method saves expensive
SMT decoding time by generating lists of n-best
translation hypotheses, and their feature values ac-
cording to the SMT model, and then optimizing
feature weights just with respect to those hypothe-
ses. In this way, as many different feature weight
settings as necessary can be explored without re-
running the decoder. The translation quality mea-
surement for the training corpus can be estimated
for a given point in feature weight space by find-
ing the highest scoring translation hypothesis, out
of the current set of hypotheses, for each sentence
in the training set. This is typically orders of mag-
nitude faster than re-running the decoder for each
combination of feature weights.
Since the resulting feature weights are opti-
mized only for one particular set of translation hy-
potheses, the decoder may actually produce differ-
ent results when run with those weights. There-
fore Och iterates the process, re-running the de-
coder with the optimized feature weights to pro-
duce new sets of n-best translation hypotheses,
merging these with the previous sets of hypothe-
ses, and re-optimizing the feature weights relative
to the expanded hypothesis sets. This process is re-
peated until no more new hypotheses are obtained
for any sentence in the training set.
Another innovation by Och is a method of nu-
merical optimization that takes advantage of the
fact that, while translation quality metrics may
have continous values, they are always applied to
the discrete outputs of a translation decoder. This
means that any measure of translation quality can
change with variation in feature weights only at
discrete points where the decoder output changes.
Och takes advantage of this through an efficient
procedure for finding all the points along a one-
dimensional line in feature weight space at which
the highest scoring translation hypothesis changes,
given the current set of hypotheses for a particu-
lar sentence. By merging the lists of such points
for all sentences in the training set, he finds all
the points at which the highest scoring hypothesis
changes for any training sentence.
Finding the optimal value of the feature weights
along the line being optimized then requires sim-
ply evaluating the translation quality metric for
each range of values for the feature weights be-
tween two such consecutive points. This can be
done efficiently by tracking incremental changes
in the sufficient statistics for the translation qual-
ity metric as we iterate through the points where
things change. Och uses this procedure as a line
search method in an iterative optimization proce-
dure, until no additional improvement in the trans-
lation quality metric is obtained, given the current
sets of translation hypotheses.
3 Optimization with Random Restarts
Although Och?s line search is globally optimal,
this is not sufficient to guarantee that a series of
line searches will find the globally optimal com-
bination of all feature weights. To avoid getting
stuck at an inferior local optimum during MERT, it
is usual to perform multiple optimization searches
over each expanded set of translation hypotheses
starting from different initial points. Typically, one
of these points is the best point found while op-
timizing over the previous set of translation hy-
potheses.1 Additional starting points are then se-
lected by independently choosing initial values for
each feature weight according to a uniform distri-
bution over a fixed interval, say ?1.0 to +1.0. The
best point reached, starting from either the previ-
ous optimum or one of the random restart points,
is selected as the optimum for the current set of hy-
potheses. This widely-used procedure is described
by Koehn et al (2007, p. 50).
3.1 Preliminary evaluation
In our first experiments, we compared a variant
of Och?s MERT procedure with and without ran-
dom restarts as described above. For our training
and test data we used the English-French subset
of the Europarl corpus provided for the shared task
(Koehn and Monz, 2006) at the Statistical Machine
Translation workshop held in conjunction with the
2006 HLT-NAACL conference. We built a stan-
dard baseline phrasal SMT system, as described
by Koehn et al (2003), for translating from Eng-
lish to French (E-to-F), using the word alignments
and French target language model provided by the
workshop organizers.
We trained a model with the standard eight fea-
tures: E-to-F and F-to-E phrase translation log
1Since additional hypotheses have been added, initiating
an optimization search from this point on the new set of hy-
potheses will often lead to a higher local optimum.
586
probabilities, E-to-F and F-to-E phrase translation
lexical scores, French language model log proba-
bilities, phrase pair count, French word count, and
distortion score. Feature weight optimization was
performed on the designated 2000-sentence-pair
development set, and the resulting feature weights
were evaluated on the designated 2000-sentence-
pair development test set, using the BLEU-4 metric
with one reference translation per sentence.
At each decoding iteration we generated the
100-best translation hypotheses found by our
phrasal SMT decoder. To generate the initial
100-best list, we applied the policy of setting the
weights for features we expected to be positively
correlated with BLEU to 1, the weights for fea-
tures we expected to be negatively correlated with
BLEU to ?1, and the remaining weights to 0. In
this case, we set the initial distortion score weight
to ?1, the phrase count weight to 0, and all other
feature weights to 1.
We made a common modification of the MERT
procedure described by Och, by replacing Pow-
ell?s method (Press, 2002) for selecting the direc-
tions in which to search the feature weight space,
with simple co-ordinate ascent?following Koehn
et al (2007)?repeatedly optimizing one feature
weight at a time while holding the others fixed, un-
til all feature weights are at optimum values, given
the values of the other feature weights. Powell?s
method is not designed to reach a better local op-
timum than co-ordinate ascent, but does have con-
vergence guarantees under certain idealized condi-
tions. However, we have observed informally that,
in MERT, co-ordinate ascent always seems to con-
verge relatively quickly, with Powell?s method of-
fering no clear advantage.
We also modified Och?s termination test slightly.
As noted above, Och suggests terminating the
overall procedure when n-best decoding fails to
produce any hypotheses that have not already been
seen. Without random restarts, this will guarantee
convergence because the last set of feature weights
selected will still be a local optimum.2 However,
if we go through the coordinate ascent procedure
without finding a better set of feature weights, then
we do not have to perform the last iteration of n-
best decoding, because it will necessarily produce
the same n-best lists as the previous iteration, as
2With random restarts, there can be no guarantee of con-
vergence, unless we have a true global optimization method,
or we enumerate all possible hypotheses permitted by the
model.
long as the decoder is deterministic. Thus we can
terminate the overall procedure if either we either
fail to generate any new hypotheses in n-best de-
coding, or the optimium set of feature weights does
not change in the coordinate ascent phase of train-
ing. In fact, we relax the termination test a bit
more than this, and terminate if no feature weight
changes by more than 1.0%.
Without random restarts, we found that MERT
converged in 8 decoding iterations, with the result-
ing model producing a BLEU score of 31.12 on
the development test set. For the experiment with
random restarts, after each iteration of 100-best
decoding, we searched from 20 initial points, 19
points selected by uniform sampling over the in-
terval [?1, 1] for each feature weight, plus the op-
timum point found for the previous set of hypothe-
ses. This procedure converged in 10 decoding it-
erations, with a BLEU score of 32.02 on the de-
velopment test set, an improvement of 0.90 BLEU,
compared to MERT without random restarts.
While the difference in BLEU score with and
without random restarts was substantial, training
with random restarts took much longer. With
our phrasal decoder and our MERT implementa-
tion, optimizing feature weights took 3894 seconds
without random restarts and 12690 seconds with
random restarts.3 We therefore asked the question
whether there was some other way to invest extra
time in training feature weights that might be just
as effective as performing random restarts. The ob-
vious thing to try is using larger n-best lists, so we
re-ran the training without random restarts, using
n-best lists of 200 and 300.
Using n-best lists of 200 produced a noticeable
improvement in training without restarts, converg-
ing in 9 decoding iterations taking 7877 seconds,
and producing a BLEU score of 31.83 on the de-
velopment test set. Using n-best lists of 300 con-
verged in 8 decoding iterations taking 8973 sec-
onds, but the BLEU score on the development test
set fell back to 31.16. Thus, simply increasing the
size of the n-best list does not seem to be a reli-
able method for improving the results obtained by
MERT without random restarts.
4 Random Walk Restarts
In the procedure described above, the initial values
for each feature weight are independently sampled
3Timings are for single-threaded execution using a desk-
top PC with 3.60 GHz Intel Xeon processors.
587
from a uniform distribution over the range [?1, 1].
We have observed anecdotally, however, that if
the selected starting point itself produces a BLEU
score much below the best we have seen so far, co-
ordinate ascent search is very unlikely to take us to
a point that is better than the current best. In order
to bias the selection of restarting points towards
better scores, we select starting points by random
walk from the ending point of the last coordinate
ascent search.
The idea is to perform a series of cautious steps
in feature weight space guided by training set
BLEU. We begin the walk at the ending point of the
last coordinate ascent search; let us call this point
~w
(0)
. Each step updates the feature weights in a
manner inspired by Metropolis-Hastings sampling
(Hastings, 1970). Starting from the current feature
weight vector ~w(i), we sample a small update from
a multivariate Gaussian distribution with mean of
0 and diagonal covariance matrix ?2I . This update
is added to the current value to produce a new po-
tential feature weight vector. The BLEU scores for
the old and the new feature weight vector are com-
pared. The new feature weight vector is always
accepted if the BLEU score on the training set is
improved; however if the BLEU score drops, the
new vector is accepted with a probability that de-
pends on how close the new BLEU score is to the
previous one. After a fixed number of steps, the
walk is terminated, and we produce a value to use
as the initial point for the next round of coordinate
ascent.
There are several wrinkles, however. First, we
prefer that the scores not fall substantially during
the random walk. Therefore we establish a base-
line value of m = BLEU(~w(0)) ? 0.005 (i.e., 1/2
BLEU point below the initial value) and do not al-
low a step to go below this baseline value. To en-
sure this, each step progresses as follows:
~
d
(i)
? GAUSSIAN(0, ?2I)
~v
(i)
= ~w
(i)
+
~
d
(i)
u
(i)
? UNIFORM(0, 1)
~w
(i+1)
=
?
?
?
~v
(i) if BLEU(~v
(i)
)?m
BLEU(~w(i))?m ? u
(i)
~w
(i) otherwise.
With this update rule, we know that ~w(i+1) will
never go below m, since the initial value is not be-
low m, and any step moving below m will result
in a negative ratio and therefore not be accepted.
So far, ?2 is left as a free parameter. An ini-
tial value of 0.001 performs well in our experi-
ence, though in general it may result in steps that
are consistently too small (so that only a very lo-
cal neighborhood is explored) or too large (so that
the vast majority of steps are rejected). Therefore
we devote the first half of the steps to ?burn-in?;
that is, tuning the variance parameter so that ap-
proximately 60% of the steps are accepted. During
burn-in, we compute the acceptance rate after each
step. If it is less than 60%, we multiply ?2 by 0.99;
if greater, we multiply by 1.01.
The final twist is in selection of the point used
for the next iteration of coordinate ascent. Rather
than using the final point of the random walk ~w(n),
we return the feature weight vector that achieved
the highest BLEU score after burn-in: ~w? =
argmax
~w
(i)
,n/2<i?n
BLEU(~w). This ensures that
the new feature weight vector has a relatively high
objective function value yet is likely very different
from the initial point.
4.1 Preliminary evaluation
To evaluate the random walk selection procedure,
we used a similar experimental set-up to the previ-
ous one, testing on the 2006 English-French Eu-
roparl corpus, using n-best lists of 100, and 20
starting points for each coordinate ascent search?
one being the best point found for the previous
hypothesis set, and the other 19 selected by our
random walk procedure. We set the number of
steps to be used in each random walk to 500. This
procedure converged in 6 decoding iterations tak-
ing 8458 seconds, with a BLEU score of 32.13 on
the development test set. This is an improvement
of 0.11 BLEU over the uniform random restart
method, and it also took only 67% as much time.
The speed up was due to the fact that random walk
method converged in 2 fewer decoding iterations,
although the average time per iteration was greater
(1410 seconds vs. 1269 seconds) because of the
extra time needed for the random walk.
5 Hypothesis Set Pruning
MERT with random walk restarts seems to pro-
duce better models than either MERT with uniform
random restarts or with no restarts, but it is still
slower than MERT with no restarts by more than
a factor of 2. The difference between 3894 sec-
onds (1.08 hours) and 8458 seconds (2.35 hours) to
optimize feature weights may not seem important,
given how long the rest of the process of build-
588
ing and training an SMT system takes; however,
to truly optimize an SMT system would actually
require performing feature weight training many
times to find optimum values of hyper-parameters
such as maximum phrase size and distortion limit.
This kind of optimization is rarely done for every
small model change, because of how long feature
weight optimization takes; so it seems well worth
the effort to speed up the optimization process as
much as possible.
To try to speed up the feature weight optimiza-
tion process, we have tried pruning the set of hy-
potheses that MERT is applied to. The time taken
by the random walk and coordinate ascent phases
of MERT with random walk restarts is roughly lin-
ear in the number of translation hypotheses exam-
ined. In the experiment described in Section 4.1,
after the first 100-best decoding iteration there
were 196,319 hypotheses in the n-best lists, and
MERT took 347 seconds. After merging all hy-
potheses from 6 iterations of 100-best decoding
there were 800,580 hypotheses, and MERT took
1380 seconds.
We conjectured that a large proportion of these
hypotheses are both low scoring according to most
submodels and low in measured translation qual-
ity, so that omitting them would make little differ-
ence to the feature weight optimization optimiza-
tion process.4 We attempt to identify such hy-
potheses by extracting some additional informa-
tion from Och?s line search procedure.
Och?s line search procedure takes note of every
hypothesis that is the highest scoring hypothesis
for a particular sentence for some value of the fea-
ture weight being optimized by the line search.
The hypotheses that are never the highest scoring
hypothesis for any combination of feature values
explored effectively play no role in the MERT pro-
cedure. We conjectured that hypotheses that are
never selected as potentially highest scoring in a
particular round of MERT could be pruned from
the hypothesis set without adversely affecting the
quality of the feature weights eventually produced.
We tested two implementations of this type of
hypothesis pruning. In the more conservative im-
plementation, after each decoding iteration, we
note all the hypotheses that are ever ?touched?
(i.e., ever the highest scoring) during the coordi-
nate ascent search either from the initial starting
4Hypotheses that are of low translation quality, but high
scoring according to some submodels, need to be retained so
that the feature weights are tuned to avoid selecting them.
point or from one of the random restarts. Any hy-
pothesis that is never touched is pruned from the
sets of hypotheses that are merged with the results
of subsequent n-best decoding iterations. We refer
to this as ?post-restart? pruning.
In the more aggressive implementation, after
each decoding iteration, we note all the hypothe-
ses that are touched during the coordinate ascent
search from the initial starting point. The hypothe-
ses that are not touched are pruned from the hy-
pothesis set before any random restarts. We refer
to this as ?pre-restart? pruning.
5.1 Preliminary evaluation
We evaluated both post- and pre-restart pruning
with random walk restarts, under the same condi-
tions used to evaluate random walk restarts without
pruning. With post-restart pruning, feature weight
training converged in 8 decoding iterations taking
7790 seconds, with a BLEU score of 32.14 on the
development test set. The last set of restarts of
MERT had 276,134 hypotheses to consider, a re-
duction of more than 65% compared to no prun-
ing. With pre-restart pruning, feature weight train-
ing converged in 7 decoding iterations taking 4556
seconds, with a BLEU score of 32.17 on the devel-
opment test set. The last set of restarts of MERT
had only 64,346 hypotheses to consider, a reduc-
tion of 92% compared to no pruning.
Neither method of pruning degraded translation
quality as measured by BLEU; in fact, BLEU scores
increased by a trivial amount with pruning. Post-
restart pruning speeded up training only slightly,
primarily because it took more decoding iterations
to converge. Time per decoding iteration was re-
duced from 1409 seconds to 974 seconds. Pre-
restart pruning was substantially faster overall, as
well as in terms of time per decoding iteration,
which was 650 seconds.
Additional insight into the differences between
feature weight optimization methods can be gained
by evaluating the feature weight sets produced af-
ter each decoding iteration. Figure 1 plots the
BLEU score obtained on the development test set
as a function of the cumulative time taken to pro-
duce the corresponding feature weights, for each
of the training runs we have described so far.
We observe a clear gap between the results
obtained from random walk restarts and those
from either uniform random restarts or no restarts.
Note in particular that, although the uniform ran-
589
Restart Pruning Num Num Decoding Total MERT Dev-test Conf
Method Method Starts N-best Iterations Seconds Seconds BLEU Level
none none 1 100 8 3894 276 31.12 > 0.999
none none 1 300 8 8973 1173 31.17 > 0.999
none none 1 200 9 7877 717 31.83 0.999
uniform rand none 5 100 7 4294 917 31.95 0.993
uniform rand none 30 100 11 19345 13306 31.98 0.995
uniform rand none 20 100 10 12690 7613 32.02 0.984
uniform rand none 10 100 10 9059 3898 32.02 0.984
random walk pre-restart 30 100 12 9963 3016 32.04 0.999
random walk pre-restart 5 100 11 7696 619 32.10 0.962
random walk none 30 100 7 14055 10887 32.10 0.959
random walk pre-restart 10 100 14 8581 1254 32.10 0.986
random walk post-restart 10 100 9 6985 2236 32.11 0.938
random walk none 20 100 6 8458 5766 32.13 0.909
random walk post-restart 20 100 8 7790 3965 32.14 0.857
random walk none 10 100 8 8338 4280 32.15 0.840
random walk pre-restart 20 100 7 4556 1179 32.17 0.712
random walk post-restart 5 100 10 6103 1114 32.18 0.794
random walk post-restart 30 100 8 9811 6218 32.20 0.554
random walk none 5 100 10 7741 3047 32.21 0.000
Table 1: Extended Evaluation Results.
dom restart method eventually comes within 0.15
BLEU points of the best result using random walk
restarts, it takes far longer to get there. The uni-
form restart run produces noticably inferior BLEU
scores until just before convergence, while with the
random walk method, the BLEU score increases
quite quickly and then stays essentially flat for sev-
eral iterations before convergence.
We also note that there appears to be less real
difference among our three variations on random
walk restarts than there might seem to be from
their times to convergence. Although pre-restart
pruning was much faster to convergence than ei-
ther of the other variants, all three reached approx-
imately the same BLEU score in the same amount
of time, if intermediate points are considered. This
suggests that our convergence test, while more lib-
eral than Och?s, still may be more conservative
than necessary when using random walk restarts.
6 Extended Evaluation
We now extend the previous evaluations in two
ways. First, we repeat all the experiments on opti-
mization with 20 starting points, using 5, 10, and
30 starting points, to see whether we can trade off
training time for translation quality by changing
that parameter setting, and if so, whether any set-
tings seem clearly better than others.
Second, we note that different optimization
methods lead to convergence at different numbers
of decoding iterations. This means that which
method produces the shortest total training time
will depend on the relative time taken by n-best
decoding and the MERT procedure itself (includ-
ing the random walk selection procedure, if that
is used). By co-incidence, these times happened
to be roughly comparable in our experiments.5 In
a situation where decoding is much slower than
MERT, however, the main determinant of overall
training time would be how many decoding iter-
ations are needed. On the other hand, if decod-
ing was made much faster, say, through algorith-
mic improvements or by using a compute cluster,
total training time would be dominated by MERT
proper. We therefore report number of decoding it-
erations to convergence and pure MERT time (ex-
cluding decoding and hypothesis set merging) for
each of our experiments, in addition to total feature
weight training time.
Table 1 reports these three measures of com-
5In our complete set of training experiments encompass-
ing 187 decoding iterations, decoding averaged 521 seconds
per iteration, and MERT (excluding decoding and hypothesis
set merging) averaged 421 seconds per (decoding) iteration.
590
puational effort, plus BLEU score on the devel-
opment test set, sorted by ascending BLEU score,
for 19 variations on MERT: 3 n-best list sizes for
MERT without restarts, and 4 different numbers
of restarts for 4 different versions of MERT with
restarts (uniform random selection, random walk
selection without pruning, random walk selection
with post-restart pruning, and random walk selec-
tion with pre-restart pruning).
The final column of Table 1 is a confidence
score reflecting the estimated probability that the
translation model produced (at convergence) by
the MERT variant for that row of the table is not
as good in terms of BLEU score as the variant that
yielded the highest BLEU score (at convergence)
that we observed in these experiments. These
probabilities were estimated by Koehn?s (2004)
paired bootstrap resampling method, run for at
least 100,000 samples per comparison.
The 11 models obtaining a BLEU score of 31.10
or less are all estimated to be at least 95% likely to
have worse translation quality than the best scor-
ing model. We therefore dismiss these models
from further consideration,6 including all models
trained without random restarts, as well as all mod-
els trained with uniform random restarts, leaving
only models trained with random walk restarts.
With random walk restarts, post-restart prun-
ing remains under consideration at all numbers
of restarts tried. For random walk restarts with-
out pruning, only the model produced by 30 start-
ing points has been eliminated, and for pre-restart
pruning, only the model produced by 20 starts re-
mains under consideration. This suggests that pre-
restart pruning may be too aggressive and, thus,
overly sensitive to the number of restarts.
To get a better picture of the remaining 8 mod-
els, see Figures 2?4. Despite convergence times
ranging from 4456 to 9811 seconds, in Figure 2 we
observe that, if feature weights after each decoding
iteration are considered, the relationships between
training time and BLEU score are remarkably sim-
ilar. In Figure 3, BLEU score varies considerably
up to 3 decoding iterations, but above that, BLEU
scores are very close, and almost flat. In fact, we
see almost no benefit from more than 7 decoding
iterations for any model.
Finally, Figure 4 shows some noticeable differ-
ences between random walk variants in respect to
6We estimate the joint probability that these 11 models are
all worse than our best scoring model to be 0.882, by multi-
plying the confidence scores for all these models.
MERT time proper. Thus, while the choice of ran-
dom walk variant chosen may matter little if de-
coding is slow, it seems that it can have an im-
portant impact if decoding is fast. If we com-
bine the results shown here with the observation
from Figure 3 that there seems to be no benefit to
trying more than 7 decoding iterations, it appears
that perhaps the best trade-off between translation
quality and training time would be obtained by us-
ing post-restart pruning, with 5 starting points per
decoding iteration, cutting off training after 7 iter-
ations. This took a total of 4009 seconds to train,
compared to 7741 seconds for the highest scoring
model on the development test set considered in
Table 1 (produced by random walk restarts with no
pruning, 5 starting points per decoding iteration, at
convergence after 10 iterations).
To validate the proposal to use the suggested
faster training procedure, we compared the two
models under discussion on the 2000 in-domain
sentence pairs for the designated final test set for
our corpus. The model produced by the sug-
gested training procedure resulted in a BLEU score
of 31.92, with the model that scored highest on
the development test set scoring an insignificantly
worse 31.89. In contast, the highest scoring model
of the three trained with no restarts produced a
BLEU score of 31.55 on the final test set, which
was worse than either of the random walk methods
evaluated on the final test set, at confidence levels
exceeding 0.996 according to the paired bootstrap
resampling test.
7 Conclusions
We believe that our results show very convinc-
ingly that using random restarts in MERT im-
proves the BLEU scores produced by the result-
ing models. They also seem to show that starting
point selection by random walk is slightly superior
to uniform random selection. Finally, our exper-
iments suggest that time to carry out MERT can
be significantly reduced by using as few as 5 start-
ing points per decoding iteration, performing post-
restart pruning of hypothesis sets, and cutting off
training after a fixed number of decoding iterations
(perhaps 7) rather than waiting for convergence.
References
Hastings, W. Keith. 1970. Monte Carlo sampling
methods using Markov chains and their applica-
tions. Biometrika 57: 97?109.
591
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation, New York City, USA, 102?121.
Koehn, Philipp, Franz Josep Och, and Daniel
Marcu. 2003. Statistical Phrase-Based Trans-
lation. In Proceedings of Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, Edmondton, Alberta, Canada, 127?
133.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In Pro-
ceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing,
Barcelona, Spain, 388?395.
Koehn, Philipp, et al 2007. Open Source Toolkit
for Statistical Machine Translation: Factored
Translation Models and Confusion Network De-
coding. Final Report of the 2006 Language En-
gineering Workshop, Johns Hopkins University,
Center for Speech and Language Processing.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, Sapporo,
Japan, 160?167.
Press, William H., et al 2002. Numerical Recipes
in C++. Cambridge University Press, Cam-
bridge, UK.
30
31
32
33
0 2000 4000 6000 8000 10000 12000 14000
B
L
E
U
s
c
o
r
e
total time in seconds
Figure 1
random walk, pre-
restart pruning, 20 
iterationsrandom walk, post-
restart pruning, 20 
iterationsrandom walk, no 
pruning, 20 iterations
uniform random 
restarts, 20 iterations
no restarts, n-best = 100
no restarts, n-best = 200
no restarts, n-best = 300
30
31
32
33
0 5000 10000
B
L
E
U
s
c
o
r
e
total time in seconds
Figure 2
random walk, pre-restart 
pruning, 20 iterations
random walk, post-
restart pruning, 5 
iterations
random walk, post-
restart pruning, 10 
iterations
random walk, post-
restart pruning, 20 
iterations
random walk, post-
restart pruning, 30 
iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
30
31
32
33
0 1 2 3 4 5 6 7 8 9 10 11
B
L
E
U
s
c
o
r
e
decoding iterations
Figure 3
random walk, pre-restart 
pruning, 20 iterations
random walk, post-
restart pruning, 5 
iterations
random walk, post-
restart pruning, 10 
iterations
random walk, post-
restart pruning, 20 
iterations
random walk, post-
restart pruning, 30 
iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
30
31
32
33
0 1000 2000 3000 4000 5000 6000 7000
B
L
E
U
s
c
o
r
e
MERT time in seconds
Figure 4
random walk, pre-restart 
pruning, 20 iterations
random walk, post-restart 
pruning, 5 iterations
random walk, post-restart 
pruning, 10 iterations
random walk, post-restart 
pruning, 20 iterations
random walk, post-restart 
pruning, 30 iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
592
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98?107,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Indirect-HMM-based Hypothesis Alignment for Combining Outputs 
from Machine Translation Systems 
 
Xiaodong He?, Mei Yang? *, Jianfeng Gao?, Patrick Nguyen?, and Robert Moore? 
 
?Microsoft Research ?Dept. of Electrical Engineering 
One Microsoft Way University of Washington 
Redmond, WA 98052 USA Seattle, WA 98195, USA 
{xiaohe,jfgao, panguyen, 
bobmoore}@microsoft.com 
yangmei@u.washington.edu 
 
 
Abstract 
This paper presents a new hypothesis alignment method 
for combining outputs of multiple machine translation 
(MT) systems. An indirect hidden Markov model 
(IHMM) is proposed to address the synonym matching 
and word ordering issues in hypothesis alignment.  
Unlike traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly from a 
variety of sources including word semantic similarity, 
word surface similarity, and a distance-based distortion 
penalty. The IHMM-based method significantly 
outperforms the state-of-the-art TER-based alignment 
model in our experiments on NIST benchmark 
datasets.  Our combined SMT system using the 
proposed method achieved the best Chinese-to-English 
translation result in the constrained training track of the 
2008 NIST Open MT Evaluation. 
1 Introduction* 
System combination has been applied successfully 
to various machine translation tasks. Recently, 
confusion-network-based system combination 
algorithms have been developed to combine 
outputs of multiple machine translation (MT) 
systems to form a consensus output (Bangalore, et 
al. 2001, Matusov et al, 2006, Rosti et al, 2007, 
Sim et al, 2007). A confusion network comprises a 
sequence of sets of alternative words, possibly 
including null?s, with associated scores. The 
consensus output is then derived by selecting one 
word from each set of alternatives, to produce the 
sequence with the best overall score, which could 
be assigned in various ways such as by voting, by 
                                                          
* Mei Yang performed this work when she was an intern with 
Microsoft Research. 
using posterior probability estimates, or by using a 
combination of these measures and other features. 
Constructing a confusion network requires 
choosing one of the hypotheses as the backbone 
(also called ?skeleton? in the literature), and other 
hypotheses are aligned to it at the word level. High 
quality hypothesis alignment is crucial to the 
performance of the resulting system combination. 
However, there are two challenging issues that 
make MT hypothesis alignment difficult. First, 
different hypotheses may use different 
synonymous words to express the same meaning, 
and these synonyms need to be aligned to each 
other. Second, correct translations may have 
different word orderings in different hypotheses 
and these words need to be properly reordered in 
hypothesis alignment.  
In this paper, we propose an indirect hidden 
Markov model (IHMM) for MT hypothesis 
alignment. The HMM provides a way to model 
both synonym matching and word ordering. Unlike 
traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly 
from a variety of sources including word semantic 
similarity, word surface similarity, and a distance-
based distortion penalty, without using large 
amount of training data. Our combined SMT 
system using the proposed method gave the best 
result on the Chinese-to-English test in the 
constrained training track of the 2008 NIST Open 
MT Evaluation (MT08). 
2 Confusion-network-based MT system 
combination 
The current state-of-the-art is confusion-network-
based MT system combination as described by 
98
 Rosti and colleagues (Rosti et al, 2007a, Rosti et 
al., 2007b). The major steps are illustrated in 
Figure 1. In Fig. 1 (a), hypotheses from different 
MT systems are first collected. Then in Fig. 1 (b), 
one of the hypotheses is selected as the backbone 
for hypothesis alignment. This is usually done by a 
sentence-level minimum Bayes risk (MBR) 
method which selects a hypothesis that has the 
minimum average distance compared to all 
hypotheses. The backbone determines the word 
order of the combined output. Then as illustrated in 
Fig. 1 (c), all other hypotheses are aligned to the 
backbone. Note that in Fig. 1 (c) the symbol ? 
denotes a null word, which is inserted by the 
alignment normalization algorithm described in 
section 3.4. Fig. 1 (c) also illustrates the handling 
of synonym alignment (e.g., aligning ?car? to 
?sedan?), and word re-ordering of the hypothesis. 
Then in Fig. 1 (d), a confusion network is 
constructed based on the aligned hypotheses, 
which consists of a sequence of sets in which each 
word is aligned to a list of alternative words 
(including null) in the same set. Then, a set of 
global and local features are used to decode the 
confusion network.  
  
E1 he have good car 
argmin ( , )B E EE TER E E?? ? ?? ?E E
 
E2 he has nice sedan 
E3 it a nice car        e.g., EB = E1 E4 a sedan he has 
(a)  hypothesis set                    (b) backbone selection 
 
EB he have ? good car      he  have   ?   good   car 
       he   has    ?   nice    sedan 
       it     ?       a   nice    car   
E4 a  ?  sedan  he   has      he   has    a     ?       sedan 
(c)  hypothesis alignment        (d) confusion network 
 
Figure 1: Confusion-network-based MT system 
combination.  
3 Indirect-HMM-based Hypothesis 
Alignment  
In confusion-network-based system combination 
for SMT, a major difficulty is aligning hypotheses 
to the backbone. One possible statistical model for 
word alignment is the HMM, which has been 
widely used for bilingual word alignment (Vogel et 
al., 1996, Och and Ney, 2003). In this paper, we 
propose an indirect-HMM method for monolingual 
hypothesis alignment. 
 
3.1 IHMM for hypothesis alignment  
 
Let 
1 1( ,..., )I Ie e e? denote the backbone, 
1 1( ,..., )J Je e e? ? ??  a hypothesis to be aligned to 1Ie , 
and 
1 1( ,..., )J Ja a a?  the alignment that specifies 
the position of the backbone word aligned to each 
hypothesis word. We treat each word in the 
backbone as an HMM state and the words in the 
hypothesis as the observation sequence. We use a 
first-order HMM, assuming that the emission 
probability 
( | )jj ap e e?
 depends only on the 
backbone word, and the transition probability 
1( | , )j jp a a I?
 depends only on the position of the 
last state and the length of the backbone. Treating 
the alignment as hidden variable, the conditional 
probability that the hypothesis is generated by the 
backbone is given by  
 
 
1
1 1 1
1
( | ) ( | , ) ( | )jJ
JJ I
j j j a
ja
p e e p a a I p e e?
?
? ?? ?? ? ???
 (1) 
  
As in HMM-based bilingual word alignment 
(Och and Ney, 2003), we also associate a null with 
each backbone word to allow generating 
hypothesis words that do not align to any backbone 
word.  
In HMM-based hypothesis alignment, emission 
probabilities model the similarity between a 
backbone word and a hypothesis word, and will be 
referred to as the similarity model. The transition 
probabilities model word reordering, and will be 
called the distortion model. 
 
3.2 Estimation of the similarity model 
 
The similarity model, which specifies the emission 
probabilities of the HMM, models the similarity 
between a backbone word and a hypothesis word. 
Since both words are in the same language, the 
similarity model can be derived based on both 
semantic similarity and surface similarity, and the 
overall similarity model is a linear interpolation of 
the two: 
 
( | ) ( | ) (1 ) ( | )j i sem j i sur j ip e e p e e p e e? ?? ? ?? ? ? ? ?  (2) 
 
99
 where ( | )sem j ip e e?
 and ( | )sur j ip e e?
 reflect the 
semantic and surface similarity between 
je?
 and  
ie , respectively, and ? is the interpolation factor. 
Since the semantic similarity between two 
target words is source-dependent, the semantic 
similarity model is derived by using the source 
word sequence as a hidden layer: 
 
0
( | )
( | ) ( | , )
sem j i
K
k i j k i
k
p e e
p f e p e f e
?
?
???
 
0
( | ) ( | )K k i j k
k
p f e p e f
?
???     (3) 
 
where 
1 1( ,..., )K Kf f f?  is the source sentence. 
Moreover, in order to handle the case that two 
target words are synonyms but neither of them has 
counter-part in the source sentence, a null is 
introduced on the source side, which is represented 
by f0. The last step in (3) assumes that first ei 
generates all source words including null. Then ej? 
is generated by all source words including null.  
In the common SMT scenario where a large 
amount of bilingual parallel data is available, we 
can estimate the translation probabilities from a 
source word to a target word and vice versa via 
conventional bilingual word alignment. Then both 
( | )k ip f e  and ( | )j kp e f?
 in (3) can be derived:  
 
2( | ) ( | )j k s t j kp e f p e f? ??
 
 
where 
2 ( | )s t j kp e f?
 is the translation model from 
the source-to-target word alignment model, and 
( | )k ip f e  , which enforces the sum-to-1 constraint 
over all words in the source sentence, takes the 
following form, 
 
2
2
0
( | )( | )
( | )
t s k i
k i K
t s k i
k
p f ep f e
p f e
?
?
?
 
 
where 
2 ( | )t s k ip f e  is the translation model from 
the  target-to-source word alignment model. In our 
method, 
2 ( | )t s ip null e  for all target words is 
simply a constant pnull, whose value is optimized 
on held-out data 1.  
The surface similarity model can be estimated 
in several ways. A very simple model could be 
based on exact match: the surface similarity model, 
( | )sur j ip e e?
, would take the value 1.0 if e?= e, and 
0 otherwise 2 . However, a smoothed surface 
similarity model is used in our method. If the target 
language uses alphabetic orthography, as English 
does, we treat words as letter sequences and the 
similarity measure can be the length of the longest 
matched prefix (LMP) or the length of the longest 
common subsequence (LCS) between them. Then, 
this raw similarity measure is transformed to a 
surface similarity score between 0 and 1 through 
an exponential mapping,  
 
? ?( | ) exp ( , ) 1sur j i j ip e e s e e?? ?? ?? ? ?? ?    (4) 
 
where ( , )j is e e?
 is computed as 
 
( , )( , ) max(| |,| |)
j i
j i
j i
M e es e e e e
?? ? ?
 
 
and ( , )j iM e e?
 is the raw similarity measure of ej? 
ei, which is the length of the LMP or LCS of ej? 
and ei. and ? is a smoothing factor that 
characterizes the mapping, Thus as ? approaches 
infinity, ( | )sur j ip e e?
 backs off to the exact match 
model. We found the smoothed similarity model of 
(4) yields slightly better results than the exact 
match model. Both LMP- and LCS- based methods 
achieve similar performance but the computation 
of LMP is faster. Therefore, we only report results 
of the LMP-based smoothed similarity model.  
 
3.3 Estimation of the distortion model 
 
The distortion model, which specifies the transition 
probabilities of the HMM, models the first-order 
dependencies of word ordering. In bilingual 
HMM-based word alignment, it is commonly 
assumed that transition probabilities 
                                                          
1  The other direction, 
2 ( | )s t ip e null? , is available from the 
source-to-target translation model. 
2 Usually a small back-off value is assigned instead of 0.  
100
 1( | , )? ?? ?j jp a i a i I
 depend only on the jump 
distance (i - i')  (Vogel et al, 1996):  
 
1
( )( | , )
( )
I
l
c i ip i i I
c l i
?
??? ?
???
             (5) 
 
As suggested by Liang et al (2006), we can 
group the distortion parameters {c(d)}, d= i - i', 
into a few buckets. In our implementation, 11 
buckets are used for c(?-4),  c(-3), ... c(0), ..., c(5), 
c(?6). The probability mass for transitions with 
jump distance larger than 6 and less than -4 is 
uniformly divided. By doing this, only a handful of 
c(d) parameters need to be estimated. Although it 
is possible to estimate them using the EM 
algorithm on a small development set, we found 
that a particularly simple model, described below, 
works surprisingly well in our experiments.  
Since both the backbone and the hypothesis are 
in the same language, It seems intuitive that the 
distortion model should favor monotonic 
alignment and only allow non-monotonic 
alignment with a certain penalty. This leads us to 
use a distortion model of the following form, 
where K is a tuning factor optimized on held-out 
data. 
 
? ? ? ?1 1c d d ??? ? ?, d= ?4, ?, 6   (6) 
 
As shown in Fig. 2, the value of distortion score 
peaks at d=1, i.e., the monotonic alignment, and 
decays for non-monotonic alignments depending 
on how far it diverges from the monotonic 
alignment. 
 
Figure 2, the distance-based distortion parameters 
computed according to (6), where K=2. 
 
Following Och and Ney (2003), we use a fixed 
value p0 for the probability of jumping to a null 
state, which can be optimized on held-out data, and 
the overall distortion model becomes 
 
0
0
              if     state( | , ) (1 ) ( | , )  otherwise
p i nullp i i I p p i i I
??? ? ? ?? ???
 
 
3.4 Alignment normalization 
 
Given an HMM, the Viterbi alignment algorithm 
can be applied to find the best alignment between 
the backbone and the hypothesis, 
 
1
1 1
1
? argmax ( | , ) ( | )jJ
JJ
j j j aa j
a p a a I p e e?
?
? ??? ? ??
  (7) 
 
However, the alignment produced by the 
algorithm cannot be used directly to build a 
confusion network. There are two reasons for this. 
First, the alignment produced may contain 1-N 
mappings between the backbone and the 
hypothesis whereas 1-1 mappings are required in 
order to build a confusion network. Second, if 
hypothesis words are aligned to a null in the 
backbone or vice versa, we need to insert actual 
nulls into the right places in the hypothesis and the 
backbone, respectively. Therefore, we need to 
normalize the alignment produced by Viterbi 
search. 
 
EB ? e2  ?2   ?   
   ?    ?      e2        ?     ?      ? 
           e1'    e2'    e3'   e4'    
Eh e1'    e2'    e3'   e4'  
(a) hypothesis words are aligned to the backbone null  
 
EB e1  ?1  e2  ?2  e3  ?3    
   ?    e1     e2        e3      ? 
           e2'    ?      e1'   
Eh e1'    e2'    ?  
(b) a backbone word is aligned to no hypothesis word 
 
Figure 3: illustration of alignment normalization 
 
First, whenever more than one hypothesis 
words are aligned to one backbone word, we keep 
the link which gives the highest occupation 
probability computed via the forward-backward 
algorithm. The other hypothesis words originally 
 -4                     1                      6  
 1.0 
 0.0 
   c(d) 
  d 
101
 aligned to the backbone word will be aligned to the 
null associated with that backbone word. 
Second, for the hypothesis words that are 
aligned to a particular null on the backbone side, a 
set of nulls are inserted around that backbone word 
associated with the null such that no links cross 
each other. As illustrated in Fig. 3 (a), if a 
hypothesis word e2? is aligned to the backbone 
word e2, a null is inserted in front of the backbone 
word e2 linked to the hypothesis word e1? that 
comes before e2?. Nulls are also inserted for other 
hypothesis words such as e3? and e4? after the 
backbone word e2. If there is no hypothesis word 
aligned to that backbone word, all nulls are 
inserted after that backbone word .3 
For a backbone word that is aligned to no 
hypothesis word, a null is inserted on the 
hypothesis side, right after the hypothesis word 
which is aligned to the immediately preceding 
backbone word. An example is shown in Fig. 3 (b). 
4 Related work 
The two main hypothesis alignment methods for 
system combination in the previous literature are 
GIZA++ and TER-based methods. Matusov et al 
(2006) proposed using GIZA++ to align words 
between different MT hypotheses, where all 
hypotheses of the test corpus are collected to create 
hypothesis pairs for GIZA++ training. This 
approach uses the conventional HMM model 
bootstrapped from IBM Model-1 as implemented 
in GIZA++, and heuristically combines results 
from aligning in both directions. System 
combination based on this approach gives an 
improvement over the best single system. 
However, the number of hypothesis pairs for 
training is limited by the size of the test corpus. 
Also, MT hypotheses from the same source 
sentence are correlated with each other and these 
hypothesis pairs are not i.i.d. data samples. 
Therefore, GIZA++ training on such a data set may 
be unreliable.  
Bangalore et al (2001) used a multiple string-
matching algorithm based on Levenshtein edit 
distance, and later Sim et al (2007) and Rosti et al 
(2007) extended it to a TER-based method for 
hypothesis alignment. TER (Snover et al, 2006) 
                                                          
3  This only happens if no hypothesis word is aligned to a 
backbone word but some hypothesis words are aligned to the 
null associated with that backbone word. 
measures the minimum number of edits, including 
substitution, insertion, deletion, and shift of blocks 
of words, that are needed to modify a hypothesis so 
that it exactly matches the other hypothesis. The 
best alignment is the one that gives the minimum 
number of translation edits. TER-based confusion 
network construction and system combination has 
demonstrated superior performance on various 
large-scale MT tasks (Rosti. et al 2007). However, 
when searching for the optimal alignment, the 
TER-based method uses a strict surface hard match 
for counting edits. Therefore, it is not able to 
handle synonym matching well. Moreover, 
although TER-based alignment allows phrase 
shifts to accommodate the non-monotonic word 
ordering, all non-monotonic shifts are penalized 
equally no matter how short or how long the move 
is, and this penalty is set to be the same as that for 
substitution, deletion, and insertion edits. 
Therefore, its modeling of non-monotonic word 
ordering is very coarse-grained.  
In contrast to the GIZA++-based method, our 
IHMM-based method has a similarity model 
estimated using bilingual word alignment HMMs 
that are trained on a large amount of bi-text data. 
Moreover, the surface similarity information is 
explicitly incorporated in our model, while it is 
only used implicitly via parameter initialization for 
IBM Model-1 training by Matusov et al (2006). 
On the other hand, the TER-based alignment 
model is similar to a coarse-grained, non-
normalized version of our IHMM, in which the 
similarity model assigns no penalty to an exact 
surface match and a fixed penalty to all 
substitutions, insertions, and deletions, and the 
distortion model simply assigns no penalty to a 
monotonic jump, and a fixed penalty to all other 
jumps, equal to the non-exact-match penalty in the 
similarity model. 
There have been other hypothesis alignment 
methods. Karakos, et al (2008) proposed an ITG-
based method for hypothesis alignment, Rosti et al 
(2008) proposed an incremental alignment method, 
and a heuristic-based matching algorithm was 
proposed by Jayaraman and Lavie (2005).  
5 Evaluation 
In this section, we evaluate our IHMM-based 
hypothesis alignment method on the Chinese-to-
English (C2E) test in the constrained training track 
102
 of the 2008 NIST Open MT Evaluation (NIST, 
2008). We compare to the TER-based method used 
by Rosti et al (2007). In the following 
experiments, the NIST BLEU score is used as the 
evaluation metric (Papineni et al, 2002), which is 
reported as a percentage in the following sections.  
 
5.1 Implementation details 
 
In our implementation, the backbone is selected 
with MBR. Only the top hypothesis from each 
single system is considered as a backbone. A 
uniform posteriori probability is assigned to all 
hypotheses. TER is used as loss function in the 
MBR computation.  
Similar to (Rosti et al, 2007), each word in the 
confusion network is associated with a word 
posterior probability. Given a system S, each of its 
hypotheses is assigned with a rank-based score of 
1/(1+r)?, where r is the rank of the hypothesis, and 
? is a rank smoothing parameter. The system 
specific rank-based score of a word w for a given 
system S is the sum of all the rank-based scores of 
the hypotheses in system S that contain the word w 
at the given position (after hypothesis alignment). 
This score is then normalized by the sum of the 
scores of all the alternative words at the same 
position and from the same system S to generate 
the system specific word posterior. Then, the total 
word posterior of w over all systems is a sum of 
these system specific posteriors weighted by 
system weights. 
Beside the word posteriors, we use language 
model scores and a word count as features for 
confusion network decoding. 
Therefore, for an M-way system combination 
that uses N LMs, a total of M+N+1 decoding 
parameters, including M-1 system weights, one 
rank smoothing factor, N language model weights, 
and one weight for the word count feature, are 
optimized using Powell?s method (Brent, 1973) to 
maximize BLEU score on a development set4 . 
Two language models are used in our 
experiments. One is a trigram model estimated 
from the English side of the parallel training data, 
and the other is a 5-gram model trained on the 
English GigaWord corpus from LDC using the 
MSRLM toolkit (Nguyen et al 2007). 
                                                          
4 The parameters of IHMM are not tuned by maximum-BLEU 
training. 
In order to reduce the fluctuation of BLEU 
scores caused by the inconsistent translation output 
length, an unsupervised length adaptation method 
has been devised. We compute an expected length 
ratio between the MT output and the source 
sentences on the development set after maximum- 
BLEU training. Then during test, we adapt the 
length of the translation output by adjusting the 
weight of the word count feature such that the 
expected output/source length ratio is met. In our 
experiments, we apply length adaptation to the 
system combination output at the level of the 
whole test corpus. 
 
5.2  Development and test data  
 
The development (dev) set used for system 
combination parameter training contains 1002 
sentences sampled from the previous NIST MT 
Chinese-to-English test sets: 35% from MT04, 
55% from MT05, and 10% from MT06-newswire. 
The test set is the MT08 Chinese-to-English 
?current? test set, which includes 1357 sentences 
from both newswire and web-data genres. Both 
dev and test sets have four references per sentence. 
As inputs to the system combination, 10-best 
hypotheses for each source sentence in the dev and 
test sets are collected from each of the eight single 
systems. All outputs on the MT08 test set were 
true-cased before scoring using a log-linear 
conditional Markov model proposed by Toutanova 
et al (2008). However, to save computation effort, 
the results on the dev set are reported in case 
insensitive BLEU (ciBLEU) score instead. 
 
5.3  Experimental results 
 
In our main experiments, outputs from a total of 
eight single MT systems were combined. As listed 
in Table 1, Sys-1 is a tree-to-string system 
proposed by Quirk et al, (2005); Sys-2 is a phrase-
based system with fast pruning proposed by Moore 
and Quirk (2008); Sys-3 is a phrase-based system 
with syntactic source reordering proposed by 
Wang et al (2007a); Sys-4 is a syntax-based pre-
ordering system proposed by Li et. al. (2007); Sys-
5 is a hierarchical system proposed by Chiang 
(2007); Sys-6 is a lexicalized re-ordering system 
proposed by Xiong et al (2006); Sys-7 is a two-
pass phrase-based system with adapted LM 
proposed by Foster and Kuhn (2007); and  Sys-8 is 
103
 a hierarchical system with two-pass rescoring 
using a parser-based LM proposed by Wang et al, 
(2007b). All systems were trained within the 
confines of the constrained training condition of 
NIST MT08 evaluation. These single systems are 
optimized with maximum-BLEU training on 
different subsets of the previous NIST MT test 
data. The bilingual translation models used to 
compute the semantic similarity are from the word-
dependent HMMs proposed by He (2007), which 
are trained on two million parallel sentence-pairs 
selected from the training corpus allowed by the 
constrained training condition of MT08.  
 
5.3.1 Comparison with TER alignment 
In the IHMM-based method, the smoothing 
factor for surface similarity model is set to ? = 3, 
the interpolation factor of the overall similarity 
model is set to ? = 0.3, and the controlling factor of 
the distance-based distortion parameters is set to 
K=2. These settings are optimized on the dev set. 
Individual system results and system combination 
results using both IHMM and TER alignment, on 
both the dev and test sets, are presented in Table 1. 
The TER-based hypothesis alignment tool used in 
our experiments is the publicly available TER Java 
program, TERCOM (Snover et al, 2006). Default 
settings of TERCOM are used in the following 
experiments. 
On the dev set, the case insensitive BLEU score 
of the IHMM-based 8-way system combination 
output is about 5.8 points higher than that of the 
best single system. Compared to the TER-based 
method, the IHMM-based method is about 1.5 
BLEU points better. On the MT08 test set, the 
IHMM-based system combination gave a case 
sensitive BLEU score of 30.89%. It outperformed 
the best single system by 4.7 BLEU points and the 
TER-based system combination by 1.0 BLEU 
points. Note that the best single system on the dev 
set and the test set are different. The different 
single systems are optimized on different tuning 
sets, so this discrepancy between dev set and test 
set results is presumably due to differing degrees 
of mismatch between the dev and test sets and the 
various tuning sets. 
 
 
 
 
 
Table 1. Results of single and combined systems 
on the dev set and the MT08 test set  
System Dev 
ciBLEU% 
MT08 
BLEU% 
System 1 34.08 21.75 
System 2 33.78 20.42 
System 3 34.75 21.69 
System 4 37.85 25.52 
System 5 37.80 24.57 
System 6 37.28 24.40 
System 7 32.37 25.51 
System 8 34.98 26.24 
TER 42.11 29.89 
IHMM 43.62 30.89 
 
In order to evaluate how well our method 
performs when we combine more systems, we 
collected MT outputs on MT08 from seven 
additional single systems as summarized in Table 
2. These systems belong to two groups. Sys-9 to 
Sys-12 are in the first group. They are syntax-
augmented hierarchical systems similar to those 
described by Shen et al (2008) using different 
Chinese word segmentation and language models. 
The second group has Sys-13 to Sys-15. Sys-13 is 
a phrasal system proposed by Koehn et al (2003), 
Sys-14 is a hierarchical system proposed by 
Chiang (2007), and Sys-15 is a syntax-based 
system proposed by Galley et al (2006). All seven 
systems were trained within the confines of the 
constrained training condition of NIST MT08 
evaluation.  
We collected 10-best MT outputs only on the 
MT08 test set from these seven extra systems. No 
MT outputs on our dev set are available from them 
at present. Therefore, we directly adopt system 
combination parameters trained for the previous 8-
way system combination, except the system 
weights, which are re-set by the following 
heuristics: First, the total system weight mass 1.0 is 
evenly divided among the three groups of single 
systems: {Sys-1~8}, {Sys-9~12}, and {Sys-
13~15}. Each group receives a total system weight 
mass of 1/3. Then the weight mass is further 
divided in each group: in the first group, the 
original weights of systems 1~8 are multiplied by 
1/3; in the second and third groups, the weight 
mass is evenly distributed within the group, i.e., 
1/12 for each system in group 2, and 1/9 for each 
104
 system in group 35.  Length adaptation is applied to 
control the final output length, where the same 
expected length ratio of the previous 8-way system 
combination is adopted. 
The results of the 15-way system combination 
are presented in Table 3. It shows that the IHMM-
based method is still about 1 BLEU point better 
than the TER-based method. Moreover, combining 
15 single systems gives an output that has a NIST 
BLEU score of 34.82%, which is 3.9 points better 
than the best submission to the NIST MT08 
constrained training track (NIST, 2008). To our 
knowledge, this is the best result reported on this 
task. 
 
Table 2. Results of seven additional single systems 
on the NIST MT08 test set 
System MT08 
BLEU% 
System 9 29.59 
System 10 29.57 
System 11 29.64 
System 12 29.85 
System 13 25.53 
System 14 26.04 
System 15 29.70 
 
Table 3. Results of the 15-way system combination 
on the NIST MT08 C2E test set 
Sys. Comb.  MT08 
BLEU% 
TER 33.81 
IHMM 34.82 
 
5.3.2 Effect of the similarity model  
In this section, we evaluate the effect of the 
semantic similarity model and the surface 
similarity model by varying the interpolation 
weight ? of (2). The results on both the dev and 
test sets are reported in Table 4. In one extreme 
case, ? = 1, the overall similarity model is based 
only on semantic similarity. This gives a case 
insensitive BLEU score of 41.70% and a case 
sensitive BLEU score of 28.92% on the dev and 
test set, respectively. The accuracy is significantly 
improved to 43.62% on the dev set and 30.89% on 
test set when ? = 0.3. In another extreme case, ? = 
                                                          
5 This is just a rough guess because no dev set is available. We 
believe a better set of system weights could be obtained if MT 
outputs on a common dev set were available. 
0, in which only the surface similarity model is 
used for the overall similarity model, the 
performance degrades by about 0.2 point. 
Therefore, the surface similarity information seems 
more important for monolingual hypothesis 
alignment, but both sub-models are useful.  
 
Table 4. Effect of the similarity model 
 Dev 
ciBLEU% 
Test 
BLEU% 
? = 1.0 41.70 28.92 
? = 0.7 42.86 30.50 
? = 0.5 43.11 30.94 
? = 0.3 43.62 30.89 
? = 0.0 43.35 30.73 
 
5.3.3 Effect of the distortion model  
We investigate the effect of the distance-based 
distortion model by varying the controlling factor 
K in (6). For example, setting K=1.0 gives a linear-
decay distortion model, and setting K=2.0 gives a 
quadratic smoothed distance-based distortion 
model. As shown in Table 5, the optimal result can 
be achieved using a properly smoothed distance-
based distortion model. 
 
Table 5. Effect of the distortion model 
 Dev 
ciBLEU% 
Test 
BLEU% 
K=1.0 42.94 30.44 
K=2.0 43.62 30.89 
K=4.0 43.17 30.30 
K=8.0 43.09 30.01 
6 Conclusion 
Synonym matching and word ordering are two 
central issues for hypothesis alignment in 
confusion-network-based MT system combination. 
In this paper, an IHMM-based method is proposed 
for hypothesis alignment. It uses a similarity model 
for synonym matching and a distortion model for 
word ordering. In contrast to previous methods, the 
similarity model explicitly incorporates both 
semantic and surface word similarity, which is 
critical to monolingual word alignment, and a 
smoothed distance-based distortion model is used 
to model the first-order dependency of word 
ordering, which is shown to be better than simpler 
approaches. 
105
 Our experimental results show that the IHMM-
based hypothesis alignment method gave superior 
results on the NIST MT08 C2E test set compared 
to the TER-based method. Moreover, we show that 
our system combination method can scale up to 
combining more systems and produce a better 
output that has a case sensitive BLEU score of 
34.82, which is 3.9 BLEU points better than the 
best official submission of MT08.  
Acknowledgement 
The authors are grateful to Chris Quirk, Arul 
Menezes, Kristina Toutanova, William Dolan, Mu 
Li, Chi-Ho Li, Dongdong Zhang, Long Jiang, 
Ming Zhou, George Foster, Roland Kuhn, Jing 
Zheng, Wen Wang, Necip Fazil Ayan, Dimitra 
Vergyri, Nicolas Scheffer, Andreas Stolcke, Kevin 
Knight, Jens-Soenke Voeckler, Spyros Matsoukas, 
and Antti-Veikko Rosti for assistance with the MT 
systems and/or for the valuable suggestions and 
discussions.  
 
References  
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In Proc. 
of IEEE ASRU, pp. 351?354. 
Richard Brent, 1973. Algorithms for Minimization 
without Derivatives. Prentice-Hall, Chapter 7. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201?
228. 
George Foster and Roland Kuhn. 2007. Mixture-Model 
Adaptation for SMT. In Proc. of the Second ACL 
Workshop on Statistical Machine Translation. pp. 
128 ? 136. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable Inference and Training of 
Context-Rich Syntactic Translation Models. In Proc. 
of COLING-ACL, pp. 961?968. 
Xiaodong He. 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. In Proc. of the 
Second ACL Workshop on Statistical Machine 
Translation. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word 
matching. In Proc. of EAMT. pp. 143 ? 152. 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine Translation 
System Combination using ITG-based Alignments. 
In Proc. of ACL-HLT, pp. 81?84. 
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming 
Zhou, Yi Guan. 2007. A Probabilistic Approach to 
Syntax-based Reordering for Statistical Machine 
Translation. In Proc. of ACL. pp. 720 ? 727. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by Agreement. In Proc. of NAACL. pp 
104 ? 111.  
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing consensus translation from 
multiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL, pp. 33?40. 
Robert Moore and Chris Quirk. 2007. Faster Beam-
Search Decoding for Phrasal Statistical Machine 
Translation. In Proc. of MT Summit XI. 
Patrick Nguyen, Jianfeng Gao and Milind Mahajan. 
2007. MSRLM: a scalable language modeling 
toolkit. Microsoft Research Technical Report MSR-
TR-2007-144. 
NIST. 2008. The 2008 NIST Open Machine Translation 
Evaluation. www.nist.gov/speech/tests/mt/2008/doc/  
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proc. of ACL, 
pp. 311?318. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase based translation. In Proc. of 
NAACL. pp. 48 ? 54. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL. pp. 271?
279. 
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, 
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr. 2007a. Combining outputs from multiple 
machine translation systems. In Proc. of NAACL-
HLT, pp. 228?235. 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007b. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL, pp. 312?319. 
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental Hypothesis 
Alignment for Building Confusion Networks with 
Application to Machine Translation System 
Combination, In Proc. of the Third ACL Workshop 
on Statistical Machine Translation, pp. 183?186. 
Libin Shen, Jinxi Xu, Ralph Weischedel. 2008. A New 
String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. In Proc. of ACL-HLT, pp. 577?585. 
106
 Khe Chai Sim, William J. Byrne, Mark J.F. Gales, 
Hichem Sahbi, and Phil C. Woodland. 2007. 
Consensus network decoding for statistical machine 
translation system combination. In Proc. of ICASSP, 
vol. 4. pp. 105?108. 
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea 
Micciulla, and John Makhoul. 2006. A study of 
translation edit rate with targeted human annotation. 
In Proc. of AMTA. 
Kristina Toutanova, Hisami Suzuki and Achim Ruopp. 
2008. Applying Morphology Generation Models to 
Machine Translation. In Proc. of ACL. pp. 514 ? 522. 
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 
1996. HMM-based Word Alignment In Statistical 
Translation. In Proc. of COLING. pp. 836-841. 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007a. Chinese Syntactic Reordering for Statistical 
Machine Translation.  In Proc. of EMNLP-CoNLL. 
pp. 737-745. 
Wen Wang, Andreas Stolcke, Jing Zheng. 2007b. 
Reranking Machine Translation Hypotheses With 
Structured and Web-based Language Models. In 
Proc. of IEEE ASRU. pp. 159 ? 164. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In Proc. of ACL. 
pp. 521 ? 528. 
107
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 746?755,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Less is More: Significance-Based N-gram Selection
for Smaller, Better Language Models
Robert C. Moore Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,chrisq}@microsoft.com
Abstract
The recent availability of large corpora
for training N-gram language models has
shown the utility of models of higher or-
der than just trigrams. In this paper, we
investigate methods to control the increase
in model size resulting from applying stan-
dard methods at higher orders. We in-
troduce significance-based N-gram selec-
tion, which not only reduces model size,
but also improves perplexity for several
smoothing methods, including Katz back-
off and absolute discounting. We also
show that, when combined with a new
smoothing method and a novel variant of
weighted-difference pruning, our selection
method performs better in the trade-off be-
tween model size and perplexity than the
best pruning method we found for modi-
fied Kneser-Ney smoothing.
1 Introduction
Statistical language models are potentially useful
for any language technology task that produces
natural-language text as a final (or intermediate)
output. In particular, they are extensively used in
speech recognition and machine translation. De-
spite the criticism that they ignore the structure of
natural language, simple N-gram models, which
estimate the probability of each word in a text
string based on the N?1 preceding words, remain
the most widely-used type of model.
Until the late 1990s, N-gram language models
of order higher than trigrams were seldom used.
This was due, at least in part, to the fact the
amounts of training data available did not produce
significantly better results from higher-order mod-
els. Since that time, however, increasingly large
amounts of language model training data have be-
come available ranging from approximately one
billion words (the Gigaword corpora from the
Linguistic Data Consortium) to trillions of words
(Brants et al, 2007). With these larger resources,
the use of language models based on 5-grams to
7-grams is becoming increasingly common.
The problem we address here is that, even when
relatively modest amounts of training data are
used, high-order N-gram language models esti-
mated by standard techniques can be impractically
large. Hence, we investigate ways of building
high-order N-gram language models without dra-
matically increasing model size. This is, of course,
the same goal behind much previous work on lan-
guage model pruning, including that of Seymore
and Rosenfeld (1996), Stolcke (1998), and Good-
man and Gao (2000). We take a novel approach,
however, which we refer to as significance-based
N-gram selection. We reject a higher-order esti-
mate of the probability of a particular word in a
particular context whenever the distribution of ob-
servations for the higher-order estimate provides
no evidence that the higher-order estimate is bet-
ter than our backoff estimate.
Perhaps our most surprising result is that
significance-based N-gram selection not only re-
duces language model size, but it also improves
perplexity when applied to a number of widely-
used smoothing methods, including Katz backoff
and several variants of absolute discounting.1 In
contrast, experiments applying previous pruning
methods to Katz backoff (Seymore and Rosen-
feld, 1996; Stolcke, 1998) and absolute discount-
ing (Goodman and Gao, 2000) always found the
lowest perplexity model to be the unpruned model.
We tested significance-based selection on only
one smoothing method without obtaining im-
proved perplexity: modified Kneser-Ney (KN)
1For most of the standard smoothing methods mentioned
here, we refer the reader to the excellent comparative study
of smoothing methods by Chen and Goodman (1998). Refer-
ences to the original sources may be found there.
746
smoothing (Chen and Goodman, 1998). This
is unfortunate, because modified KN smoothing
generally seems to have the lowest perplexity of
any known smoothing method for N-gram lan-
guage models; in our tests it had a lower perplex-
ity than any of the other models, with or with-
out significance-based N-gram selection. How-
ever, when we compared modified KN smooth-
ing to our best results applying N-gram selection
to other smoothing methods for multiple N-gram
orders, two of our models outperformed modified
KN in terms of perplexity for a given model size.
Of course, the trade-off between perplexity and
model size for modified KN can also be im-
proved by pruning. So, in a final set of ex-
periments we found the best combinations we
could for pruned modified KN models, and we did
the same for our best model using significance-
based selection. The best pruning method for
the latter turned out to be a novel modifica-
tion of weighted-difference pruning (Seymore and
Rosenfeld, 1996) that was especially convenient
to compute given our method for performing
significance-based N-gram selection. The final re-
sult is that our best model using significance-based
selection and modified weighted difference prun-
ing always had a better size/perplexity trade-off
than pruned modified KN, with up to about 8%
perplexity reduction for a given model size.
2 Significance-Based N-gram Selection
The idea of using a statistical test to decide
whether to use a higher- or lower-order estimate of
an N-gram probablity is not new. It was perhaps
first proposed by Ron, et al (1996), who suggested
using a threshold on relative entropy (Kullback-
Liebler divergence) as an appropriate test to de-
cide whether to extend the context used to predict
the next token in a sequence. Stolcke (1998) used
the same metric in his work on language model
pruning, and he also pointed out that weighted dif-
ference pruning is, in fact, an approximation of
relative entropy pruning. However, while relative
entropy pruning is based on a statistical test, it is
not a significance test. The difference in probabil-
ity represented by a certain relative entropy value
can be statistically significant when measured on
a large corpus, but not significant when measured
on a small corpus.
The primary test we use to choose between
higher- or lower-order estimates of an N-gram
probablity is inspired by an insight of Jedynak and
Khudanpur (2005). They note that, given a set
of y observations of a multinomial distribution,
the observed counts will have the highest proba-
bilty of any possible set of y observations for the
maximum likelihood estimate (MLE) model de-
rived from the relative frequencies of those obser-
vations. In general, however, the MLE model will
not be the only model for which this set of obser-
vations is the most probable set of y observations.
Jedynak and Khudanpur call the set of such mod-
els the maximum likelihood set (MLS) for the ob-
servations.
Jedynak and Khudanpur argue that the obser-
vations alone do not support choosing the MLE
over other members of the MLS. The MLE may
assign the observations a higher probability than
other members of the MLS, but that may be an
accident of what outcomes are possible given the
number of observations. If we flip a coin 9 times
and get 5 heads, is there any reason to believe that
the probability of heads is closer to the MLE 5/9
than it is to 5/10? No, because 5/9 is as close as
we can come to 5/10, given 9 observations.
We apply this insight to the problem of N-
gram selection as follows: For each word w
n
in a context w
1
...w
n?1
with a backoff estimate
for the probability of that word in that context
? p(w
n
|w
2
...w
n?1
),
2 we do not include an explicit
estimate of p(w
n
|w
1
...w
n?1
) in our model, if the
backoff estimate is within the MLS of the counts
for w
1
...w
n
and w
1
...w
n?1
.
This requires finding the MLS of a set of obser-
vations only for binomial distributions (rather than
the general multinomial distributions studied by
Jedynak and Khudanpur), which has a very sim-
ple solution:
MLS(x, y) =
{
p
?
?
?
?
x
y + 1
? p ?
x+ 1
y + 1
}
where x is the count for w
1
...w
n
, y is the count for
w
1
...w
n?1
, and p is a possible backoff probabilty
estimate for p(w
n
|w
1
...w
n?1
). In this case, the
MLS is the set of binomial distributions that have
x successes as their mode given y trials, which is
well-known to be specified by this formula.
We describe this method as ?significance-
based? because we can consider our criterion as
a significance test in which we take the backoff
2
p(w
n
|w
2
...w
n?1
) being the next lower-order estimate,
and ? being the backoff weight for the context w
1
...w
n?1
.
747
probability estimate as the null hypothesis for the
estimate in the higher-order model, and we set the
rejection threshold to the lowest possible value;
we reject the null hypothesis (the backoff probabil-
ity) if there are any outcomes for the given number
of trials that are more likely, according to the null
hypothesis, than the one we observed.
We make a few refinements to this basic idea.
First, we never add an explicit higher-order esti-
mate to our model, if the next lower-order estimate
is not explicitly stored in the model. This enables
us to keep only the next lower-order model avail-
able while performing N-gram selection.
Next, we observe that in some cases the higher-
order estimate for p(w
n
|w
1
...w
n?1
) may not fall
within the MLS for the observed counts, due to
smoothing. In this case, we prefer the backoff
probability estimate if it lies within the MLS or be-
tween the smoothed higher-order estimate and the
MLS. Otherwise, we would reject the backoff es-
timate for being outside the MLS, only to replace
it with a higher-order estimate even farther outside
the MLS.
Finally, we note that the backoff probability es-
timate for an N-gram not observed in the train-
ing data sometimes falls outside the corresponding
MLS, which in the 0-count case simplifies to
MLS(0, y) =
{
p
?
?
?
?
0 ? p ?
1
y + 1
}
When this happens, we include an explicit higher-
order estimate p(w
n
|w
1
...w
n?1
) = 1/(y + 1),
which is the upper limit of the MLS. This is similar
to Rosenfeld and Huang?s (1993) ?confidence in-
terval capping? method for reducing unreasonably
high backoff estimates for unobserved N-grams.
In order to apply this treatment of 0-count N-
grams, we sort the explicitly-stored N-grams for
each backoff context by decreasing probability.
For each higher-order context, to find the 0-count
N-grams subject to the 1/(y + 1) limit, we tra-
verse the sorted list of explicitly-stored N-grams
for its backoff context. When we encounter an N-
gram whose extension to the higher-order context
was not observed in the training data, we give it
an explicit probability of 1/(y+1), if its weighted
backoff probability is greater than that. We stop
the traversal as soon as we encounter an N-gram
for the backoff context that has a weighted backoff
probability less than or equal to 1/(y+1), which in
practice means we actually examine only a small
number of backoff probabilities for each context.
3 Finding Backoff Weights by Iterative
Search
The approach described above is very attractive
from a theoretical perspective, but it has one prac-
tical complication. To decide which N-grams for
each context to explicitly include in the higher-
order model, we need to know the backoff weight
for the context, but we cannot compute the backoff
weight until we know exactly which higher-order
N-grams are included in the model.
We address this problem by iteratively solving
for a backoff weight that yields a normalized prob-
ability distribution. For each context, we guess
an initial value for the backoff weight and keep
track of the sum of the probabilites resulting from
applying our N-gram selection method with that
backoff weight. If the sum is greater than 1.0, by
more than a convergence threshold, we reduce the
estimated backoff weight and iterate. If the sum
is less than 1.0, by more than the threshold, we
increase the estimated weight and iterate.
It is easy to see that, for all standard smooth-
ing methods, the function from backoff weights
to probability sums is piece-wise linear. Within
a region where no decision changes about which
N-grams to include in the model, the probability
sum is a linear function of the backoff weight. At
values of the backoff weight where the set of se-
lected N-grams changes, the function can be dis-
continous. With a little more effort, one can see
that the linear segments overlap with respect to the
probability sum in such a way that there will al-
ways be one or more values of the backoff weight
that make the probability sum equal 1.0, with one
specific exception.
The exception arises because of the capping of
backoff probabilites for unobserved N-grams. It
is possible for there to be a context for which
all observed N-grams are included in the higher-
order model, the probabilities for all unobserved
N-grams are either capped at 1/(y + 1) or effec-
tively 0 due to arithmetic underflow, and the prob-
ability sum is less than 1.0. For some smoothing
methods, the probability sum cannot be increased
in this situation by increasing the backoff weight.
We check for this situation, and if it arises, we
increase the cap on the 0-count probability just
enough to make the probability sum equal 1.0.
That exception aside, we iteratively find back-
off weights as follows: For an initial estimate
of the backoff weight for a context, we compute
748
what the backoff weight would be for the base
smoothing method without N-gram selection. If
that value is less than 1.0, we use it as our ini-
tial estimate, otherwise we use 1.0, which annec-
dotally seems to produce better models than ini-
tial estimates greater than 1.0, in situations where
there are multiple solutions. If the first iteration of
N-gram selection produces a probability sum less
than 1.0, we repeatedly double the estimated back-
off weight until we obtain a sum greater than or
equal to 1.0, or we encounter the special situation
previously described. If the initial probability sum
is greater than 1.0, we repeatedly halve the esti-
mated backoff weight until we obtain a sum less
than or equal to 1.0.
Once we have values for the backoff weight that
produce probability sums on both sides of 1.0, we
have a solution bracketed, and we can use standard
numerical search techniques to find that solution.
At every subsequent iteration, we try a value for
the backoff weight between the largest value we
have tried that produces a sum less than 1.0 and
the smallest value we have tried that produces a
sum greater than 1.0. We stop when the difference
between these values of the backoff weight is less
than a convergence threshold.
We use a combination of simple techniques to
choose the next value of the backoff weight to try.
The primary technique we use is called the ?false
position method?, which basically solves the lin-
ear equation defined by the two current bracketing
values and corresponding probability sums. The
advantage of this method is that, if our bracket-
ing points lie on the same linear segment of our
function, we obtain a solution in one step. The
disadvantage of the method is that it sometimes
approaches the solution by a long sequence of tiny
steps from the same side.
We try to detect the latter situation by keeping
track of the number of consecutive iterations that
make a step in the same direction. If this num-
ber reaches 10, we take the next step by the bi-
section method, which simply tries the value of
the backoff weight halfway between our two cur-
rent bracketing values. In practice, this combined
search method works very well, taking an average
of less than four iterations per backoff weight.
4 Modified Weighted-Difference Pruning
While the N-gram selection method described
above considerably reduces the number of para-
meters in a high-order language model, we may
wish to reduce language model size even more.
The concept of significance-based N-gram selec-
tion to produce smaller models could be extended
by relaxing our criterion for using backoff distrib-
utions in place of explicit higher-order probability
estimates, but true significance tests at more re-
laxed thresholds that are accurate for small counts
are expensive to compute; so we resort to more
conventional language model pruning methods.
In our experiments, we tried four methods for
additional pruning: simple count cutoffs, relative
entropy pruning (REP) (Stolcke, 1998), and two
modified versions of Seymore and Rosenfeld?s
(1996) weighted-difference pruning (WDP). In the
notation we have been using, Seymore and Rosen-
feld?s WDP criterion for using a backoff estimate,
in place of an explicit higher-order estimate, is that
the quantity
K?
(
log(p(w
n
|w
1
...w
n?1
))?
log(?
u
p(w
n
|w
2
...w
n?1
))
)
be less than a pruning threshold, where K is
the Good-Turing-discounted training set count for
w
1
...w
n
, and ?
u
is the backoff weight for the un-
pruned model.
The first of our modified version of WDP uses
the following quantity instead:
p(w
1
...w
n
)?
?
?
?
?
?
log(p(w
n
|w
1
...w
n?1
))?
log(?
p
p(w
n
|w
2
...w
n?1
))
?
?
?
?
?
where p(w
1
...w
n
) is an estimate of the probability
of w
1
...w
n
and ?
p
is the backoff weight for the
pruned model.
We make three modifications to WDP in this
formula. First, we follow a suggestion of Stol-
cke (1998) by replacing the discounted training
set count K of w
1
...w
n
with an estimate the joint
probability of w
1
...w
n
, computed by chaining the
explicit probability estimates, according to our
model, for all N-gram lengths up to n.
The second modification to WDP is that we use
the absolute value of the difference of the log prob-
abilities. By using the signed difference of the log
probabilities, Seymore and Rosenfeld will always
prune a higher-order probability estimate if it is
less than the backoff estimate. But the backoff es-
timate may well be too high. Using the absolute
value of the difference avoids this problem.
749
p(w
n
|w
1
. . . w
n?1
) =
?
?
?
?
?
?
?
?
?
?
?
?
w
1
...w
n?1
C(w
1
...w
n
)?D
n,C(w
1
...w
n
)
C(w
1
...w
n?1
)
+ ?
w
1
...w
n?1
p(w
n
|w
2
. . . w
n?1
)
if C(w
1
. . . w
n
) > 0
?
w
1
...w
n?1
p(w
n
|w
2
. . . w
n?1
) if C(w
1
. . . w
n
) = 0
?
w
1
...w
n?1
= ?
|{w
?
|C(w
1
...w
n?1
w
?
)>0}|
C(w
1
...w
n?1
)
?
w
1
...w
n?1
= 1? ?
w
1
...w
n?1
Figure 1: New language model smoothing method
The final modification is that we compute the
difference in log probability with respect to the
backoff weight for the pruned model rather than
the unpruned model, which we are able to do by
performing the pruning inside our iterative search
for the value of the backoff weight. We do this
because, if the backoff weight is changed signifi-
cantly by pruning, backoff estimates that meet the
pruning criterion with the old backoff weight may
no longer meet the criterion with the new back-
off weight, and vice versa. Since the new backoff
weight is the one that will be used in the pruned
model, that seems to be the one that should be used
to make pruning decisions.
Our second variant of modified WDP is like the
first, but it estimates p(w
1
...w
n
) simply by divid-
ing Seymore and Rosenfeld?s discounted N-gram
count K by the total number of highest-order N-
grams in the training corpus. This is equivalent to
smoothing only the highest-order conditional N-
gram model in estimating p(w
1
...w
n
), estimating
all the lower-order probabilities in the chain by the
corresponding MLE model. We refer to this joint
probability estimate as ?partially-smoothed?, and
the one suggested by Stolcke as ?fully-smoothed?.
5 Evaluation
We carried out three sets of evaluations to test
the new techniques described above. First we
compared the perplexity of full models and mod-
els reduced by significance-based N-gram selec-
tion for seven language model smoothing meth-
ods. For the best three results in that comparison,
we looked at the trade-off between perplexity and
model size over a range of N-gram orders. Finally,
we tried various pruning methods to further reduce
model size, and then compared the best result we
obtained using previous techniques with the best
result we obtained using our new techniques.
5.1 Data and Base Smoothing Methods
For training, parameter optimzation, and test data
we used English text from the WMT-06 Europarl
corpus (Koehn and Monz, 2006). We trained on
the designated 1,003,349 sentences (27,493,499
words) of English language model training data,
and used 2000 sentences each for testing and pa-
rameter optimization, from the English half of the
English-French dev and devtest data sets.
We conducted our experiments on seven lan-
guage model smoothing methods. Five of these
are well-known: (1) interpolated absolute dis-
counting with one discount per N-gram length, es-
timated according to the formula derived by Ney
et al (1994); (2) Katz backoff with Good-Turing
discounts for N-grams occurring 5 times or less;
(3) backoff absolute discounting with Ney et al
formula discounts; (4) backoff absolute discount-
ing with one discount used for all N-gram lengths,
optimized on held-out data; (5) modified interpo-
lated Kneser-Ney smoothing with three discounts
per N-gram length, estimated according to the for-
mulas suggested by Chen and Goodman (1998).
We also experimented with two variants of a
new smoothing method that we have recently de-
veloped. Full details of the new method are given
elsewhere (Moore and Quirk, 2009), but since it is
not well-known, we summarize the method here.
Smoothed N-gram probabilities are defined by the
formulas shown in Figure 1, for all n such that
N ? n ? 2,
3 where N is the greatest N-gram
length used in the model. The novelty of this
model is that, while it is an interpolated model, the
interpolation weights ? for the lower-order model
3For n = 2, we take the expression p(w
n
|w
2
. . . w
n?1
)
to denote a unigram probability estimate p(w
2
).
750
base select percent
Method PP PP change
1 interp-AD-fix 62.6 61.6 -1.6
2 Katz backoff 59.8 56.1 -7.9
3 backoff-AD-fix 59.9 54.3 -9.3
4 backoff-AD-opt 58.8 54.4 -7.5
5 KN-mod-fix 51.6 54.6 +5.8
6 new-fix 56.1 52.1 -7.1
7 new-opt 53.7 52.0 -3.3
Table 1: Perplexity results for N-gram selection
are not constrained to match the backoff weights
? for the lower-order model. This allows the in-
terpolation weights to be set independently of the
discounts D, with the backoff weights being ad-
justed to normalize the resulting distributions.
The motivation for this is to let the D para-
meters correct for potential overestimation of the
probabilities for observed N-grams, while the ?
parameter (which determines the ? and ? interpo-
lation parameters) somewhat independently cor-
rects for quantization errors caused by the fact that
only certain probabilities can be derived from in-
teger observed counts, even after discounting. ? is
interpretable as the estimated mean quantization
error for each distinct count for a given context.
We tested two variants of the new method, (6)
one in which the D parameters and the ? parameter
are set by fixed criteria, and (7) one in which a sin-
gle value for all D parameters and the value of the
? parameter are optimized on held-out data. For
the fixed value of ?, we assume that, since the dis-
tance between possible N-gram counts, after dis-
counting, is approximately 1.0, their mean quan-
tization error would be approximately 0.5. For
the fixed discount parameters, we use three values
for each N-gram length: D
1
for N-grams whose
count is 1, D
2
for N-grams whose count is 2, and
D
3
for N-grams whose count is 3 or more. We
set these values to be the discounts for 1-counts,
2-counts, and 3-counts estimated by the Good-
Turing method. This yields the formula
D
r
= r ? (r + 1)
N
r+1
N
r
,
for 1 ? r ? 3, where N
r
is the number of distinct
N-grams of the length in question occuring r times
in the training set.
In all experiments, the unigram language
model is an un-smoothed, closed-vocabulary MLE
model. We use this unigram model, because there
is no simple, principled way of assigning prob-
abilities to individual out-of-vocabulary (OOV)
words. The only principled solution to this prob-
lem that we are aware of is to use a character-
based model, but this seems overly complicated
for something that is orthogonal to the main points
of this study, and of minor practical importance.
Since we make no provision for OOV words in the
models, OOV words are also omitted from all per-
plexity measurements. Thus, the perplexity num-
bers are systematically lower than they would be
if OOVs were taken into account, but they are all
comparable in this regard.
5.2 Results for Significance-Based N-gram
Selection
Table 1 shows the minimum perplexity (with re-
spect to N-gram order) of language models up to
7-grams for each of the seven smoothing methods
discussed above, with and without significance-
based N-gram selection. N-gram selection im-
proved the perplexity of all models, except for
modified KN. The lowest overall perplexity re-
mains that of the base modified KN method, but
with N-gram selection, the two variants of the new
smoothing method come very close to it.
If we cared only about perplexity, that would be
the end of the story, but we also care about lan-
guage model size. The results in Table 1 were ob-
tained on models estimated using just the counts
needed to cover the parameter optimization and
test sets; so to accurately measure model size, we
trained full language models using base modifed
KN, and the two variants of the new method with
N-gram selection. The resulting sizes of the mod-
els represented in backoff form (in terms of total
number of probability and backoff parameters) are
shown in Figure 2 as function of N-gram length,
from trigrams up to 7-grams for KN and up to
10-grams for the two new models. We see that
beyond 4-grams the model sizes diverge dramati-
cally, with the new models incorporating N-gram
selection leveling off, but the modified KN model
(or any standard model) continuing to grow in size,
apparently linearly in the N-gram order.
In Figure 3, we show the relationship between
perplexity and model size for the same three
models, varying N-gram order. We see that be-
tween about 20 million and 45 million parameters,
both of the new models incorporating significance-
751
020
40
60
80
100
120
140
160
180
3 4 5 6 7 8 9 10
M
illi
on
s o
f m
od
el 
pa
ra
m
et
er
s
N-gram length
No N-gram 
selection
new-opt +               
N-gram selection
new-fix +               
N-gram selection
Figure 2: Model size vs. N-gram length
51
52
53
54
55
56
57
58
59
60
61
0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160
Per
pl
exit
y
Millions of Model Parameters
KN -mod -fix
new-opt +               
N-gram selection
new-fix +               
N-gram selection
Figure 3: Perplexity vs. model size
based N-gram selection seem to outperform mod-
ified KN, and that the best of the three is, in fact,
the new model with fixed parameter values.
5.3 Results for Additional Pruning
We further tested modified KN smoothing, and our
new smoothing method with fixed parameter val-
ues and significance-based N-gram selection, with
additional pruning. We compared several pruning
methods on trigram models: count cutoffs, REP,4
and our two modified versions of WDP.
Figure 4 shows the resulting combinations of
perplexity and model size for REP and modified
WDP at various pruning thresholds, and for count
cutoffs of 1, 2, and 3 for both bigrams and trigrams
(n > 1) and for trigrams only (n > 2), applied to
4Thanks to Asela Gunawardana for the use of his REP
tool.
our new smoothing method with fixed parameter
values, together with significance-based N-gram
selection. Overall, modified WDP with fully-
smoothed joint probability estimates performs the
best. It is has lower perplexity than count cut-
offs at all model sizes tested, and is about equal
to REP at very severe pruning levels and superior
to REP with less pruning. Modified WDP with
fully-smoothed joint probabilities is about equal
to modified WDP with partially-smoothed joint
probabilities at the highest and lowest pruning lev-
els tested, but superior in between.
Figure 4 also shows the result of applying
modified WDP with fully-smoothed joint prob-
abilities to our new smoothing method with-
out significance-based N-gram selection, to test
whether the former subsumes the gains from the
latter. We see that modified WDP does not render
752
60
61
62
63
64
65
66
67
68
69
70
71
72
73
0 1 2 3 4 5 6 7 8 9 10
Per
pl
exit
y
Millions of Model Parameters
count cutoffs for n > 1
count cutoffs for n > 2
relative entropy pruning
modifed WDP w/fully -
smoothed joint probs,    
wo/N -gram selection
modified WDP w/partially -
smoothed joint probs
modified WDP w/fully -
smoothed joint probs
Figure 4: Pruning methods for new smoothing technique with N-gram selection
60
61
62
63
64
65
66
67
68
69
70
71
72
0 1 2 3 4 5 6 7 8 9 10 11
Per
pl
exit
y
Millions of Model Parameters
modified WDP w/fully -
smoothed joint probs
relative entropy pruning
modified WDP w/partially -
smoothed joint probs
count cutoffs for n > 1
count cutoffs for n > 2
Figure 5: Pruning methods for modified KN smoothing
N-gram selection redundant except at very severe
pruning levels, much like REP.
Figure 5 shows the results of applying the
same four pruning methods to KN smoothing.
Count cutoffs clearly perform the best with KN
smoothing. It is interesting to note, however,
that?contrary to the results for our new smooth-
ing method?with KN smoothing, modified WDP
with partially-smoothed joint probabilities is sig-
nificantly better than either REP or modified WDP
with fully-smoothed joint probabilities. We be-
lieve this is due to the fact that the latter two meth-
ods both estimate the joint probabilities by chain-
ing the lower-order conditional probabilities from
the fully-smoothed model, which in the case of
KN smoothing are designed specifically to cover
N-grams that have not been observed, and are poor
estimates for the probabilities of lower-order N-
grams that do occcur in the training data.
Finally, we compared the new smoothing
method with N-gram selection and modified WDP
with fully-smoothed joint probabilities against
modified KN smoothing with count cutoffs, us-
ing combinations of pruning parameter values and
N-gram order that yielded the best size/perplexity
trade-offs. The results are shown in Figure 6. At
all model sizes within the range of these experi-
ments, the new method with significance-based N-
gram selection and modified WDP had lower per-
plexity than modifed KN with count cutoffs?up
to about 8% lower at greater pruning levels.
This experiment also suggests that the
size/perplexity trade-off is easier to optimize
for our new combination of smoothing, N-gram
selection, and modified WDP, than for KN
smoothing with count cut-offs. Table 2 shows the
753
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
0 1 2 3 4 5 6 7 8 9 10 11 12 13
Per
pl
exit
y
Millions of Model Parameters
KN -mod -fix with 
count cutoffs
new-fix with N -gram 
selection and 
modified WDP
Figure 6: Comparison of two best pruned language models
PP N CC n >
69.9 3 4 1
64.7 4 4 1
62.1 4 3 1
59.0 4 2 1
56.5 4 2 2
54.4 4 1 2
53.6 5 1 2
53.4 6 1 2
53.3 7 1 2
Table 2: Optimal pruning parameters for KN-
mod-fix with count cutoffs
perplexity (PP), maximum N-gram length (N),
count cutoff (CC), and N-gram lengths to which
the count cutoffs are applied (n >) for the points
on the curve for pruned KN in Figure 6. Although
some tendencies are discernable, it seems clear
that a significant part of the space of combinations
of N, CC, and ?n >? parameter values must be
searched to find the best points for trading off
perplexity against model size. Table 3 shows
maximum N-gram length and pruning threshold
values for the points on the corresponding curve
for our new approach. Here the situation is much
simpler. The best trade-off points are found by
varying the pruning threshold, and including
in the model all N-grams that pass the pruning
threshold, regardless of N-gram length.
6 Conclusions
We have shown that significance-based N-gram
selection can simultaneously reduce both model
PP N threshold
67.2 10 10?6.5
62.7 10 10?6.75
59.3 10 10?7.0
56.4 10 10?7.25
54.6 10 10?7.5
53.7 10 10?7.75
53.2 10 10?8.0
Table 3: Optimal pruning parameters for new-fix
with N-gram selection and modified WDP
size and perplexity when applied to a number of
language model smoothing methods, including the
widely-used Katz backoff and absolute discount-
ing methods. We are not aware of any other tech-
nique that does this. We also found that, when
combined with a new smoothing method and a
novel variant of weighted difference pruning, our
N-gram selection method outperformed modified
Kneser-Ney smoothing?using the best form of
pruning we found for that approach?with respect
to the trade-off between model size and model
quality.
As our next steps, first, we need to verify that
the results obtained on a moderate-sized train-
ing corpus are repeatable on much larger corpora.
Second, we plan to extend this work to incorpo-
rate language model size reduction by word clus-
tering, which has been shown by Goodman and
Gao (2000) to produce additional gains when com-
bined with previous methods of language model
pruning.
754
References
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz
J. Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of EMNLP 2007, 858?867.
Chen, Stanley F., and Joshua Goodman. 1998.
An empirical study of smoothing techniques for
language modeling. Technical Report TR-10-
98, Harvard University.
Goodman, Joshua, and Jianfeng Gao. 2000. Lan-
guage model size reduction by pruning and
clustering. In Proceedings of ICSLP 2000, 110?
113.
Jedynak, Bruno M., and Sanjeev Khudanpur.
2005. Maximum likelihood set for estimating a
probability mass function. Neural Computation
17, 1?23.
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of WMT 2006, 102?121.
Moore, Robert C., and Chris Quirk. 2009. Im-
proved smoothing for N-gram language mod-
els based on ordinary counts. In Proceedings
of ACL-IJCNLP 2009.
Ney, Hermann, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependen-
cies in stochastic language modelling. Com-
puter Speech and Language, 8, 1?38.
Ron, Dana, Yoram Singer, and Naftali Tishby.
1996. The power of amnesia: learning proba-
bilistic automata with variable memory length.
Machine Learning, 25, 117?149.
Rosenfeld, Ronald, and Xuedong Huang. 1993.
Improvements in stochastic language modeling.
In Proceedings of HLT 1993, 107?111.
Seymore, Kristie, and Ronald Rosenfeld. 1996.
Scalable Trigram Backoff Language Models. In
Proceedings of ICSLP 1996. 232?235.
Stolcke, Andreas. 1998. Entropy-based pruning
of backoff language models. In Proceedings,
DARPA News Transcription and Understanding
Workshop 1998, 270?274.
755
259
260
261
262
263
264
265
266
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 81?88, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Discriminative Framework for Bilingual Word Alignment
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052
bobmoore@microsoft.com
Abstract
Bilingual word alignment forms the foun-
dation of most approaches to statistical
machine translation. Current word align-
ment methods are predominantly based
on generative models. In this paper,
we demonstrate a discriminative approach
to training simple word alignment mod-
els that are comparable in accuracy to
the more complex generative models nor-
mally used. These models have the the
advantages that they are easy to add fea-
tures to and they allow fast optimization
of model parameters using small amounts
of annotated data.
1 Motivation
Bilingual word alignment is the first step of most
current approaches to statistical machine translation.
Although the best performing systems are ?phrase-
based? (e.g, Och and Ney, 2004), possible phrase
translations are normally first extracted from word-
aligned bilingual text segments. The standard ap-
proach to word alignment makes use of various com-
binations of five generative models developed at
IBM by Brown et al (1993), sometimes augmented
by an HMM-based model or Och and Ney?s ?Model
6? (Och and Ney, 2003). The best combinations of
these models can produce high accuracy alignments,
at least when trained on a large corpus of fairly di-
rect translations in related languages.
These standard models are less than ideal, how-
ever, in a number of ways, two of which we address
in this paper. First, although the standard models can
theoretically be trained without supervision, in prac-
tice various parameters are introduced that should
be optimized using annotated data. For, example,
Och and Ney (2003) suggest supervised optimiza-
tion of a number of parameters, including the prob-
ablity of jumping to the empty word in the HMM
model, as well as smoothing parameters for the dis-
tortion probabilities and fertility probabilities of the
more complex models. Since the values of these pa-
rameters affect the values of the translation, align-
ment, and fertility probabilities trained by EM, there
is no effective way to optimize them other than to
run the training procedure with a particular combi-
nation of values and evaluate the accuracy of the re-
sulting alignments. Since evaluating each combina-
tion of parameter values in this way can take hours to
days on a large training corpus, it seems safe to say
that these parameters are rarely if ever truly jointly
optimized for a particular alignment task.
The second problem we address is the difficulty
of adding features to the standard generative models.
Generative models require a generative ?story? as to
how the observed data is generated by an interrelated
set of stochastic processes. For example, the gener-
ative story for IBM Models 1 and 2 and the HMM
alignment model is that a target language translation
of a given source language sentence is generated by
first choosing a length for the target language sen-
tence, then for each target sentence position choos-
ing a source sentence word, and then choosing the
corresponding target language word. When Brown
et al (1993) wanted to add a fertility component to
create Models 3, 4, and 5, however, this generative
81
story didn?t fit any longer, because it does not in-
clude how many target language words to align to
each source language word as a separate decision.
To model this explicitly, they had to come up with a
different generative story.
In this paper, we take a different approach to
word alignment, based on discriminative training of
a weighted linear combination of a small number
of features. For a given parallel sentence pair, for
each possible word alignment considered, we sim-
ply multiply the values of each of these features by a
corresponding weight to give a score for that feature,
and sum the features scores to give an overall score
for the alignment. The possible alignment having
the best overall score is selected as the word align-
ment for that sentence pair. Thus, for a sentence pair
(e, f) we seek the alignment a? such that
a? = argmaxa
n
?
i=1
?ifi(a, e, f)
where the fi are features and the ?i are weights.
We optimize the model weights using a modified
version of averaged perceptron learning as described
by Collins (2002). This is fast to train, because se-
lecting the feature weights is the last step in build-
ing the model and the ?online? nature of perceptron
learning allows the parameter optimization to con-
verge quickly. Furthermore, no generative story has
to be invented to explain how the features generate
the data, so new features can be easily added without
having to change the overall structure of the model.
In theory, a disadvantage of a discrimintative ap-
proach compared to a generative approach is that
it requires annotated data for training. In practice,
however, effective discriminative models for word
alignment require only a few parameters, which can
be optimized on a set of annotated sentence pairs
comparable in size to what is needed to tune the free
parameters used in the generative approach. As we
will show, a simple sequence of two such models
can achieve alignment accuracy comparable to that
of a combination of more complex standard models.
2 Discriminative Alignment Models
We develop two word alignment models, incorpo-
rating different word association features intended
to indicate how likely two words or groups of words
are to be mutual translations, plus additional features
measuring how much word reordering is required by
the alignment1, and how many words are left un-
linked. One of the models also includes a feature
measuring how often one word is linked to several
words.
Each of our feature scores have analogs in the
IBM and HMM models. The association scores cor-
responds to word translation probabilities; the re-
ordering scores correspond to distortion probabili-
ties; the scores for words left unlinked corresponds
to probabilities of words being linked to the null
word; and the scores for one-to-many links corre-
spond to fertility probabilities.
2.1 The Log-Likelihood-Based Model
In our first model, we use a log-likelihood-ratio
(LLR) statistic as our measure of word association.
We chose this statistic because it has previously been
found to be effective for automatically construct-
ing translation lexicons (e.g., Melamed, 2000). We
compute LLR scores using the following formula
presented by Moore (2004):
LLR(f, e) =
?
f??{f,?f}
?
e??{e,?e}
C(f?, e?) log p(f?|e?)p(f?)
In this formula f and e mean that the words whose
degree of association is being measured occur in the
respective target and source sentences of an aligned
sentence pair, ?f and ?e mean that the correspond-
ing words do not occur in the respective sentences,
f? and e? are variables ranging over these values,
and C(f?, e?) is the observed joint count for the val-
ues of f? and e?. All the probabilities in the for-
mula refer to maximum likelihood estimates. The
LLR score for a pair of words is high if the words
have either a strong positive association or a strong
negative association. Since we expect translation
pairs to be positively associated, we discard any
negatively associated word pairs by requiring that
p(f, e) > p(f) ? p(e). To reduce the memory re-
quirements of our algorithms we discard any word
pairs whose LLR score is less than 1.0.
1We will use the term ?alignment? to mean an overall word
alignment of a sentence pair, and the term ?link? to mean the
alignment of a particular pair of words or small group of words.
82
In our first model, the value of the word associa-
tion feature for an alignment is simply the sum of all
the individual LLR scores for the word pairs linked
by the alignment. The LLR-based model also in-
cludes the following features:
nonmonotonicity features It may be observed
that in closely related languages, word alignments
of sentences that are mutual translations tend to be
approximately monotonic (i.e., corresponding words
tend to be in nearly corresponding sentence posi-
tions). Even for distantly related languages, the
number of crossing links is far less than chance,
since phrases tend to be translated as contiguous
chunks. To model these tendencies, we introduce
two nonmonotonicity features.
To find the points of nonmonotonicity of a word
alignment, we arbitrarily designate one of the lan-
guages as the source and the other as the target. We
sort the word pairs in the alignment, first by source
word position, and then by target word position. We
then iterate through the sorted alignment, looking
only at the target word positions. The points of
nonmonotonicity in the alignment will be the places
where there are backward jumps in this sequence
of target word positions. For example, suppose we
have the sorted alignment ((1,1)(2,4)(2,5)(3,2)(5,6)).
The sequence of target word positions in this sorted
alignment is (1,4,5,2,6); hence, there is one point of
nonmonotonicity where target word position 2 fol-
lows target word position 5.
We still need to decide how to measure the degree
of nonmonotonicity of an alignment. Two meth-
ods immediately suggest themselves. One is to sum
the magnitudes of the backward jumps in the target
word sequence; the other is to simply count the num-
ber of backward jumps. Rather than choose between
them, we use both features.
the one-to-many feature It has often been ob-
served that word alignment links tend to be one-to-
one. Indeed, word alignment results can often be
improved by restricting more general models to per-
mit only one-to-one links. For example, Och and
Ney (2003) found that the intersection of the align-
ments found training the IBM models in both direc-
tions always outperformed either direction alone in
their experiments. Since the IBM models allow one-
to-many links only in one direction, this intersection
can contain only one-to-one links.
To model the tendency for links to be one-to-one,
we define a one-to-many feature as the number of
links connecting two words such that exactly one
of them participates in at least one other link. We
also define a many-to-many feature as the number of
links that connect two words that both participate in
other links. We don?t use this directly in the model,
but to cut down on the number of alignments we
need to consider, we discard any alignments having
a non-zero value of the many-to-many feature.
the unlinked word feature To control the number
of words that get linked to something, we introduce
an unlinked word feature that simply counts the total
number of unlinked words in both sentences in an
aligned sentence pair.
2.2 The Conditional-Link-Probability-Based
Model
In this model we replace the LLR-based word asso-
ciation statistic with the logarithm of the estimated
conditional probability of two words (or combina-
tions of words) being linked, given that they co-
occur in a pair of aligned sentences. These estimates
are derived from the best alignments according to
some other, simpler model. For example, if for-
mer occurs 1000 times in English sentences whose
French translations contain ancien, and the simpler
alignment model links them in 600 of those sentence
pairs, we might estimate the conditional link proba-
bility (CLP) for this word pair as 0.6. We find it
better, however, to adjust these probabilities by sub-
tracting a small fixed discount from the link count:
LPd(f, e) =
links
1
(f, e)? d
cooc(f, e)
LPd(f, e) represents the estimated conditional link
probability for the words f and e, links
1
(f, e) is
the number of times they are linked by the simpler
alignment model, d is the discount, and cooc(f, e)
is the number of times they co-occur. This adjust-
ment prevents assigning high probabilities to links
between pairs of words that rarely co-occur.
An important difference between the LLR-based
model and CLP-based model is that the LLR-based
model considers each word-to-word link separately,
but allows multiple links per word, as long as they
83
lead to an alignment consisting only of one-to-one
and one-to-many links (in either direction). In the
CLP-based model, however, we allow conditional
probabilities for both one-to-one and one-to-many
clusters, but we require all clusters to be disjoint.
For example, we estimate the conditional proba-
bility of linking not to ne...pas by considering the
number of sentence pairs in which not occurs in the
English sentence and both ne and pas occur in the
French sentence, compared to the number of times
not is linked to both ne and pas in pairs of corre-
sponding sentences. However, when we make this
estimate in the CLP-based model, we do not count a
link between not and ne...pas if the same instance of
not, ne, or pas is linked to any other words.
The CLP-based model incorporates the same ad-
dtional features as the LLR-based model, except that
it omits the one-to-many feature, since we assume
that the one-to-one vs. one-to-many trade-off is al-
ready modeled in the conditional link probabilities
for particular one-to-one and one-to-many clusters.
We have developed two versions of the CLP-
based model, using two different estimates for the
conditional link probabilities. One estimate of the
conditional link probabilities comes from the LLR-
based model described above, optimized on an an-
notated development set. The other estimate comes
from a heuristic alignment model that we previously
developed (Moore, 2005).2 Space does not permit
a full description of this heuristic model here, but
in brief, it utilizes a series of greedy searches in-
spired by Melamed?s competitive linking algorithm
(2000), in which constraints limiting alignments to
being one-to-one and monotonic are applied at dif-
ferent thresholds of the LLR score, with a final cut-
off of the LLR score below which no alignments are
made.
3 Alignment Search
While the discriminative models presented above
are very simple to describe, finding the optimal
alignment according to these models is non-trivial.
Adding a link for a new pair of words can affect the
nonmonotonicity scores, the one-to-many score, and
the unlinked word score differently, depending on
2The conditional link probabilities used in the current work
are those used in Method 4 of the earlier work. Full details are
provided in the reference.
what other links are present in the alignment. Never-
theless, we have found a beam-search procedure that
seems highly effective in finding good alignments
when used with these models.
For each sentence pair, we create a list of associa-
tion types and their corresponding scores, consisting
of the associations for which we have determined a
score and for which the words involved in the asso-
ciation type occur in the sentence pair.3 We sort the
resulting list of association types from best to worst
according to their scores.
Next, we initialize a list of possible alignments
with the empty alignment, assigning it a score equal
to the number of words in the sentence pair multi-
plied by the unlinked word weight. We then iterate
through our sorted list of association types from best
to worst, creating new alignments that add links for
all instances of the association type currently being
considered to existing alignments, potentially keep-
ing both the old and new alignments in our set of
possible alignments.
Without pruning, we would soon be overwhelmed
by a combinatorial explosion of alignments. The
set of alignments is therefore pruned in two ways.
First, we keep track at all times of the score of the
best alignment we have seen so far, and any new
alignment whose overall score is worse than the best
score so far by more than a fixed difference D is im-
mediately discarded. Second, for each instance of a
particular alignment type, when we have completed
creating modified versions of previous alignments to
include that instance, we merge the set of new align-
ments that we have created into the set of previous
alignments. When we do this merge, the resulting
set of alignments is sorted by overall score, and only
the N best alignments are kept, for a fixed N .
Some details of the search differ between the
LLR-based model and the CLP-based model. One
difference is how we add links to existing align-
ments. In both cases, if there are no existing links
involving any of the words involved in the new link,
we simply add it (keeping a copy of the original
alignment, subject to pruning).
If there are existing links involving word in-
stances also involved in the new link, the two mod-
3By association type we mean a possible link between a pair
of words, or, in the case of the CLP-based models, a possible
one-to-many or many-to-one linkage of words.
84
els are treated differently. For the CLP-based model,
each association score is for a cluster of words that
must be disjoint from any other association cluster,
so when we add links for a new cluster, we must
remove any other links involving the same word in-
stances. For the LLR-based model, we can add ad-
ditional links without removing old ones, but the re-
sulting alignment may be worse due to the degra-
dation in the one-to-many score. We therefore add
both an alignment that keeps all previous links, and
an additional set of alignments, each of which omits
one of the previous links involving one of the word
instances involved in the new link.
The other difference in how the two models are
treated is an extra pruning heuristic we use in the
LLR-based model. In generating the list of associ-
ation types to be used in aligning a given sentence
pair, we use only association types which have the
best association score for this sentence pair for one
of the word types involved in the association. We
initially explored limiting the number of associa-
tions considered for each word type simply as an ef-
ficiency heuristic, but we were surprised to discover
that the most extreme form of such pruning actually
reduced alignment error rate over any less restrictive
form or not pruning on this basis at all.
4 Parameter Optimization
We optimize the feature weights using a modified
version of averaged perceptron learning as described
by Collins (2002). Starting with an initial set of
feature weight values, perceptron learning iterates
through the annotated training data multiple times,
comparing, for each sentence pair, the best align-
ment ahyp according to the current model with the
reference alignment aref . At each sentence pair, the
weight for each feature is is incremented by the dif-
ference between the value of the feature for the best
alignment according to the model and the value of
the feature for the reference alignment:
?i ? ?i + (fi(aref , e, f)? fi(ahyp, e, f))
The updated feature weights are used to compute
ahyp for the next sentence pair.
Iterating through the data continues until the
weights stop changing, because aref = ahyp for
each sentence pair, or until some other stopping con-
dition is met. In the averaged perceptron, the feature
weights for the final model are the average of the
weight values over all the data rather than simply
the values after the final sentence pair of the final
iteration.
We make a few modifications to the procedure as
described by Collins. First, we average the weight
values over each pass through the data, rather than
over all passes, as we found this led to faster con-
vergence. After each pass of perceptron learning
through the data, we make another pass through the
data with feature weights fixed to their average val-
ues for the previous learning pass, to evaluate cur-
rent performance of the model. We iterate this pro-
cedure until a local optimum is found.
Next, we used a fixed weight of 1.0 for the word-
association feature, which we expect to be most im-
portant feature in the model. Allowing all weights to
vary allows many equivalent sets of weights that dif-
fer only by a constant scale factor. Fixing one weight
eliminates a spurious apparent degree of freedom.
This necessitates, however, employing a version of
perceptron learning that uses a learning rate param-
eter. As described by Collins, the perceptron up-
date rule involves incrementing each weight by the
difference in the feature values being compared. If
the feature values are discrete, however, the mini-
mum difference may be too large compared to the
unweighted association score. We therefore multi-
ply the feature value difference by a learning rate pa-
rameter ? to allow smaller increments when needed:
?i ? ?i + ?(fi(aref , e, f)? fi(ahyp, e, f))
For the CLP-based model, based on the typical
feature values we expected to see, we guessed that
0.01 might be a good value for the learning rate pa-
rameter. That seemed to produce good results, so we
did not attempt to further optimize the learning rate
parameter for this model.
The situation with the LLR-based model was
more complicated. Our previous experience using
LLR scores in statistical NLP applications indicated
that with large data sets, LLR values can get very
high (upwards of 100000 for our 500000 sentence
pair corpus), but small difference could be signifi-
cant, which led us to believe that the same would
be true of the weight values we were trying to learn.
That meant that a learning rate small enough to let
85
us converge on the desired weight values might take
a very large number of iterations through the data
to reach those values. We addressed this problem,
by using a progression of learning rates, starting at
1000, reducing each successive weight by an order
of magnitude, until we ended with a learning rate of
1.0. At each transition between learning rates, we re-
initialized the weights to the optimum values found
with the previous learning rate.
We experimented with one other idea for opti-
mizing the weight values. Perceptron learning does
not directly optimize error rate, but we have only
a small number of parameters that we need to op-
timize. We therefore thought it might be helpful
to apply a general optimization procedure directly
to the error rate, starting from the best parame-
ter values found by perceptron learning, using the
N -best alignments found with these parameter val-
ues. We experimented with both the downhill sim-
plex method (Press et al, 2002, Section 10.4) and
Powell?s method (Press et al, 2002, Section 10.5),
but we obtained slightly better results with a more
heuristic method designed to look past minor local
minima. We found that using this approach on top of
perceptron learning led to slightly lower error rates
on the development set with the CLP-based model,
but not with the LLR-base model, so we used it only
with the former in our final evaluations.
5 Data and Methodology for Evaluation
We evaluated our models using data from the bilin-
gual word alignment workshop held at HLT-NAACL
2003 (Mihalcea and Pedersen, 2003). We used
a subset of the Canadian Hansards bilingual cor-
pus supplied for the workshop, comprising 500,000
English-French sentences pairs, including 447 man-
ually word-aligned sentence pairs designated as test
data. The test data annotates particular pairs of
words either as ?sure? or ?possible? links. Auto-
matic sentence alignment of the training data was
provided by Ulrich Germann, and the hand align-
ments of the words in the test data were created by
Franz Och and Hermann Ney (Och and Ney, 2003).
Since our discriminative training approach re-
quires a small amount of annotated data for parame-
ter optimization, we split the test data set into two
virtually equal subsets, by randomly ordering the
test data pairs, and assigning alternate pairs from the
random order to the two subsets. We used one of
these subsets as a development set for parameter op-
timization, and held out the other for a final test set.
We report the performance of our alignment mod-
els in terms of precision, recall, and alignment error
rate (AER) as defined by Och and Ney (2003):
recall =
|A ? S|
|S|
precision =
|A ? P |
|A|
AER = 1?
|A ? P |+ |A ? S|
|A|+ |S|
In these definitions, S denotes the set of alignments
annotated as sure, P denotes the set of alignments
annotated possible or sure, and A denotes the set of
alignments produced by the method under test. Fol-
lowing standard practice in the field, we take AER,
which is derived from F-measure, as the primary
evaluation metric that we are attempting to optimize.
6 Experimental Results
We first trained the LLR-based model by perceptron
learning, using an N -best value of 20 and an un-
bounded allowable score difference in the alignment
search, using the development set as annotated train-
ing data. We then aligned all the sentences of length
100 or less in our 500,000 sentence pair corpus, us-
ing an N -best value of 20 and a maximum allowable
score difference of 125000. We collected link counts
and co-occurrence counts from these alignments for
estimating conditional link probabilities. We trained
CLP-based models from these counts for a range of
values for the discount used in the conditional link
probability estimation, finding a value of 0.4 to be a
roughly optimal value of the discount parameter for
the development set. We also trained a CLP-based
model using the conditional link probabilities from
the heuristic alignment model mentioned previously.
In training both CLP-based models, we also used
an N -best value of 20 and an unbounded allowable
score difference in the alignment search.
We evaluated three models on the final test data:
the LLR-based model (LLR) and the two CLP-based
models, one with conditional link probabilities from
86
Alignment Recall Precision AER
LLR 0.829 0.848 0.160
CLP
1
0.889 0.934 0.086
CLP
2
0.898 0.947 0.075
Table 1: Discriminative Model Results.
Alignment Recall Precision AER
E ? F 0.870 0.890 0.118
F ? E 0.876 0.907 0.106
Union 0.929 0.845 0.124
Intersection 0.817 0.981 0.097
Refined 0.908 0.929 0.079
Table 2: IBM Model 4 Results.
the LLR-based model (CLP
1
), and one with condi-
tional link probabilities from the heuristic alignment
model (CLP
2
). All parameters were optimized on
the development set. Recall, precision, and align-
ment error rates on the test set are shown in Table 1.
For comparison, we aligned our parallel corpus
with IBM Model 4 using Och?s Giza++ software
package (Och and Ney, 2003).4 We used the de-
fault configuration file included with the version of
Giza++ that we used, which resulted in five itera-
tions of Model 1, followed by five iterations of the
HMMmodel, followed by five iterations of Model 4.
We trained the models in both directions, English-
to-French and French-to-English, and computed the
union, intersection, and what Och and Ney (2003)
call the ?refined? combination of the two align-
ments. We evaluated the resulting alignments of the
final test set, with the results shown in Table 2.
As these tables show, our discriminatively trained
CLP-based models compare favorably to IBM
Model 4 on this data set. The one with condi-
tional link probabilities from the heuristic alignment
model, CLP
2
, performs slightly better than the best
of the Model 4 combinations, and the one with
conditional link probabilities from the LLR-based
model, CLP
1
, performs only slightly worse.
An interesting question is why CLP
2
outper-
formed CLP
1
. CLP
1
is the more ?principled? model,
so one might have expected it to perform better. We
believe the most likely explanation is the fact that
4Thanks to Chris Quirk for carrying out this alignment.
CLP
2
received 403,195 link probabilities from the
heuristic model, while CLP
1
received only 144,051
link probabilities from the LLR-based model. Hence
CLP
2
was able to consider more possible links.
In light of our claims about the ease of optimiz-
ing the models, we should make some comments
on the time need to train the parameters. Our cur-
rent implementation of the alignment search is writ-
ten in Perl, and is therefore quite slow. Alignment
of our 500,000 sentence pair corpus with the LLR-
based mode took over a day on a 2.8 GHz Pentium
IV workstation. Nevertheless, the parameter opti-
mization was still quite fast, since it took only a few
iterations over our 224 sentence pair development
set. With either the LLR-based or CLP-based mod-
els, one combined learning/evaluation pass of per-
ceptron training always took less than two minutes,
and it never took more that six passes to reach the
local optimum we took to indicate convergence. To-
tal training time was greater since we used multiple
runs of perceptron learning with different learning
rates for the LLR-based model and different condi-
tional link probability discounts for CLP
1
, but total
training time for each model was around an hour.
7 Related Work
When the first version of this paper was submitted
for review, we could honestly state, ?We are not
aware of any previous work on discriminative word
alignment models.? Callison-Burch et al (2004) had
investigated the use of small amounts of annotated
data to help train the IBM and HMMmodels, but the
models were still generative and were trained using
maximum-likelihood methods.
Recently, however, three efforts nearly simultane-
ous with ours have made use of discriminative meth-
ods to train alignment models. Fraser and Marcu
(2005) modify Model 4 to be a log-linear combina-
tion of 11 submodels (5 based on standard Model 4
parameters, and 6 based on additional features) and
discriminatively optimize the submodel weights on
each iteration of a Viterbi approximation to EM.
Liu et al (2005) also develop a log-linear model,
based on IBM Model 3. They train Model 3 us-
ing Giza++, and then use the Model 3 score of a
possible alignment as a feature value in a discrim-
inatively trained log-linear model, along with fea-
87
tures incorporating part-of-speech information, and
whether the aligned words are given as translations
in a bilingual dictionary. The log-linear model is
trained by standard maximum-entropy methods.
Klein and Taskar (2005), in a tutorial on maxi-
mum margin methods for natural-language process-
ing, described a weighted linear model incorporat-
ing association, position, and orthography features,
with its parameters trained by a structured-support-
vector-machine method. This model is in some re-
spects very similar to our LLR-based model, us-
ing Dice coefficient association scores where we use
LLR scores, and absolute position differences where
we use nonmonotonicity measures.
8 Conclusions
The results of our work and other recent efforts
on discriminatively trained alignment models show
that results comparable to or better than those ob-
tained with the IBM models are possible within a
framework that makes it easy to add arbitrary ad-
ditional features. After many years using the same
small set of alignment models, we now have an easy
way to experiment with a wide variety of knowledge
sources to improve word-alignment accuracy.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2005. Statistical Marchine Translation
with Word- and Sentences-Aligned Parallel Cor-
pora. In Proceedings of the 42nd Annual Meeting
of the ACL, pp. 176?183, Barcelona, Spain.
Michael Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory and
Experiments with Perceptron Algorithms. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pp. 1?8,
Philadelphia, Pennsylvania.
Alexander Fraser and Daniel Marcu. 2005. ISI?s
Participation in the Romanian-English Alignment
Task. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pp. 91?94,
Ann Arbor, Michigan.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear Models for Word Alignment. In Proceed-
ings of the 43rd Annual Meeting of the ACL,
pp. 459?466, Ann Arbor, Michigan.
Dan Klein and Ben Taskar. 2005. Max-Margin
Methods for NLP: Estimation, Structure, and Ap-
plications. Tutorial presented at ACL 2005, Ann
Arbor, Michigan.
I. Dan Melamed. 2000. Models of Transla-
tional Equivalence. Computational Linguistics,
26(2):221?249.
Rada Mihalcea and Ted Pedersen. 2003. An Evalu-
ation Exercise for Word Alignment. In Proceed-
ings of the HLT-NAACL 2003 Workshop, Building
and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pp. 1?6, Edmonton, Al-
berta, Canada.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Proceed-
ings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing, pp. 333?
340, Barcelona, Spain.
Robert C. Moore. 2005. Association-Based Bilin-
gual Word Alignment. In Proceedings of the ACL
Workshop on Building and Using Parallel Texts,
pp. 1?8, Ann Arbor, Michigan.
Franz Joseph Och and Hermann Ney. 2003.
A Systematic Comparison of Various Statistical
Alignment Models. Computational Linguistics,
29(1):19?51.
Franz Joseph Och and Hermann Ney. 2004. The
Alignment Template Approach to Statistical Ma-
chine Translation. Computational Linguistics,
30(4):417?449.
William H. Press, Saul A. Teukolsky, William T.
Vetterling, and Brian P. Flannery. 1992. Numer-
ical Recipies in C: The Art of Scientific Comput-
ing, Second Edition. Cambridge University Press,
Cambridge, England.
88
Proceedings of NAACL HLT 2007, Companion Volume, pages 209?212,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Selective Phrase Pair Extraction for
Improved Statistical Machine Translation
Luke S. Zettlemoyer
MIT CSAIL
Cambridge, MA 02139
lsz@csail.mit.edu
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052
bobmoore@microsoft.com
Abstract
Phrase-based statistical machine transla-
tion systems depend heavily on the knowl-
edge represented in their phrase transla-
tion tables. However, the phrase pairs
included in these tables are typically se-
lected using simple heuristics that poten-
tially leave much room for improvement.
In this paper, we present a technique for
selecting the phrase pairs to include in
phrase translation tables based on their es-
timated quality according to a translation
model. This method not only reduces the
size of the phrase translation table, but
also improves translation quality as mea-
sured by the BLEU metric.
1 Introduction
Phrase translation tables are the heart of phrase-
based statistical machine translation (SMT) systems.
They provide pairs of phrases that are used to con-
struct a large set of potential translations for each
input sentence, along with feature values associated
with each phrase pair that are used to select the best
translation from this set.1
The most widely used method for building phrase
translation tables (Koehn et al, 2003) selects, from
a word alignment of a parallel bilingual training cor-
pus, all pairs of phrases (up to a given length) that
are consistent with the alignment. This procedure
1A ?phrase? in this sense can be any contiguous sequence of
words, and need not be a complete linguistic constituent.
typically generates many phrase pairs that are not re-
motely reasonable translation candidates.2 To avoid
creating translations that use these pairs, a set of fea-
tures is computed for each pair. These features are
used to train a translation model, and phrase pairs
that produce low scoring translations are avoided. In
practice, it is often assumed that current translation
models are good enough to avoid building transla-
tions with these unreasonable phrase pairs.
In this paper, we question this assumption by in-
vestigating methods for pruning low quality phrase
pairs. We present a simple procedure that reduces
the overall phrase translation table size while in-
creasing translation quality. The basic idea is to
initially gather the phrase pairs and train an trans-
lation model as usual, but to then select a subset of
the overall phrases that performs the best, prune the
others, and retrain the translation model. In experi-
ments, this approach reduced the size of the phrase
tranlsation table by half, and improved the BLEU
score of the resulting translations by up to 1.5 points.
2 Background
As a baseline, we present a relatively standard SMT
approach, following Koehn et al (2003). Potential
translations are scored using a linear model where
the best translation is computed as
argmax
t,a
n?
i=1
?ifi(s, a, t)
where s is the input sentence, t is the output sen-
tence, and a is a phrasal alignment that specifies how
2In one experiment, we managed to generate more than
117,000 English phrases for the the French word ?de?.
209
Monsieur le Orateur , je invoque le Re`gement
"" ,,  
""
Mr. Speaker , I rise on a point of order
Figure 1: A word aligned sentence pair.
t is constructed from s. The weights ?i associated
with each feature fi are tuned to maximize the qual-
ity of the translations.
The training procedure starts by computing a
word alignment for each sentence pair in the train-
ing corpus. A word alignment is a relation between
the words in two sentences where, intuitively, words
are aligned to their translation in the other language.
In this work, we use a discriminatively trained word
aligner (Moore et al, 2006) that has state of the art
performance. Figure 1 presents a high quality align-
ment produced by this aligner.
Given a word aligned corpus, the second step is to
extract a phrase translation table. Each entry in this
table contains a source language phrase s, a target
language phrase t, and a list of feature values ?(s, t).
It is usual to extract every phrase pair, up to a cer-
tain phrase length, that is consistent with the word
alignment that is annotated in the corpus. Each con-
sistent pair must have at least one word alignment
between words within the phrases and no words in
either phrase can be aligned any words outside of the
phrases. For example, Figure 2 shows some of the
phrase pairs that would be extracted from the word-
aligned sentence pair in Figure 1. A full list using
phrases of up to three words would include 28 pairs.
For each extracted phrase pair (s, t), feature val-
ues ?(s, t) = ?log p(s|t), log p(t|s), log l(s, t)? are
computed. The first two features, the log translation
and inverse translation probabilities, are estimated
by counting phrase cooccurrences, following Koehn
et al (2003). The third feature is the logarithm of
a lexical score l(s, t) that provides a simple form of
smoothing by weighting a phrase pair based on how
likely individual words within the phrases are to be
translations of each other. We use a version from
Foster et al (2006), modified from (Koehn et al,
2003), which is an average of pairwise word transla-
tion probabilities.
In phrase-based SMT, the decoder produces trans-
lations by dividing the source sentence into a se-
quence of phrases, choosing a target language phrase
# Source Lang. Phrase Target Lang. Phrase
1 Monsieur Mr.
2 Monsieur le Mr.
3 Monsieur le Orateur Mr. Speaker
4 le Orateur Speaker
5 Orateur Speaker
. . . . . . . . .
23 le Re`glement point of order
24 le Re`glement of order
25 le Re`glement order
26 Re`glement point of order
27 Re`glement of order
28 Re`glement order
Figure 2: Phrase pairs consistent with the word
alignment in Figure 1.
as a translation for each source language phrase, and
ordering the target language phrases to build the fi-
nal translated sentence. Each potential translation is
scored according to a weighted linear model. We
use the three features from the phrase translation ta-
ble, summing their values for each phrase pair used
in the translation. We also use four additional fea-
tures: a target language model, a distortion penalty,
the target sentence word count, and the phrase pair
count, all computed as described in (Koehn, 2004).
For all of the experiments in this paper, we used the
Pharaoh beam-search decoder (Koehn, 2004) with
the features described above.
Finally, to estimate the parameters ?i of the
weighted linear model, we adopt the popular min-
imum error rate training procedure (Och, 2003)
which directly optimizes translation quality as mea-
sured by the BLEU metric.
3 Selective Phrase Pair Extraction
In order to improve performance, it is important to
select high quality phrase pairs for the phrase trans-
lation table. We use two key ideas to guide selection:
? Preferential Scoring: Phrase pairs are selected
using a function q(s, t) that returns a high score
for source, target phrase pairs (s, t) that lead to
high quality translations.
? Redundancy Constraints: Our intuition is
that each occurrence of a source or target lan-
guage phrase really has at most one translation
for that sentence pair. Redundancy constraints
minimize the number of possible translations
that are extracted for each phrase occurrence.
210
Selecting phrases that a translation model prefers
and eliminating at least some of the ambiguity that
comes with extracting multiple translations for a sin-
gle phrase occurrence creates a smaller phrase trans-
lation table with higher quality entries.
The ideal scoring metric would give high scores
to phrase pairs that lead to high-quality translations
and low scores to those that would decrease transla-
tion quality. The best such metric we have available
is provided by the overall translation model. Our
scoring metric q(s, t) is therefore computed by first
extracting a full phrase translation table, then train-
ing a full translation model, and finally using a sub-
part of the model to score individual phrase pairs in
isolation. Because the scoring is tied to a model that
is optimized to maximize translation quality, more
desirable phrase pairs should be given higher scores.
More specifically, q(s, t) = ?(s, t) ? ? where
?(s, t) is the length three vector that contains the
feature values stored with the phrase pair (s, t) in the
phrase translation table, and ? is a vector of the three
parameter values that were learned for these features
by the full translation model. The rest of the features
are ignored because they are either constant or de-
pend on the target language sentence which is fixed
during phrase extraction. In essence, we are using
the subpart of a full translation model that looks at
phrase pair identity and scoring the pair based on
how the full model would like it.
This scoring metric is used in a phrase pair se-
lection algorithm inspired by competitive linking
for word alignment (Melamed, 2000). Local com-
petitive linking extracts high scoring phrase pairs
while enforcing a redundancy constraint that mini-
mizes the number of phrase pairs that share a com-
mon phrase. For each sentence pair in the training
set, this algorithm marks the highest scoring phrase
pair, according to q(s, t), containing each source
language phrase and the highest scoring phrase pair
containing each target language phrase. Each of
these marked phrase pairs is selected and the phrase
translation table is rebuilt. This is a soft redundancy
constraint because a phrase pair will only be ex-
cluded if there is a higher scoring pair that shares
its source language phrase and a higher scoring pair
that shares its target language phrase. For example,
consider again the phrase pairs in Figure 2 and as-
sume they are sorted by their scores. Local compet-
itive linking will select every phrase pair except for
27 and 28. All other pairs are the highest scoring
options for at least one of their phrases.
Selective phrase extraction with competitive link-
ing can be seen as a Viterbi reestimation algorithm.
Because we are extracting fewer phrase pairs, the
features associated with each phrase pair will differ.
If the removed phrases were not real translations of
each other in the first place, the translation features
p(s|t) and p(t|s) should be better estimates because
the high quality phrases that remain will be given
the probability mass that was assigned to the pruned
phrase pairs. Although we are running it in a purely
discriminative setting, it has a similar feel to an EM
algorithm. First, a full phrase translation table and
parameter estimate is computed. Then, based on that
estimate, a subset of the phrases is selected which,
in turn, supplies a new estimate for the parameters.
One question is howmany times to run this reestima-
tion procedure. We found, on the development set,
that it never helped to run more than one iteration.
Perhaps because of the hard nature of the algorithm,
repeated iterations caused slight decreases in phrase
translation table size and overall performance.
4 Experiments
In this section, we report experiments conducted
with Canadian Hansards data from the 2003 HLT-
NAACL word-alignment workshop (Mihalcea and
Pedersen, 2003). Phrase pairs are extracted
from 500,000 word-aligned French-English sen-
tence pairs. Translation quality is evaluated accord-
ing to the BLEU metric (with one reference trans-
lation). Three additional disjoint data sets (from the
same source) were used, one with 500 sentence pairs
for minimum error rate training, another with 1000
pairs for development testing, and a final set of 2000
sentence pairs for the final test. For each experiment,
we trained the full translation model as described in
Section 2. Each trial varied only in the phrase trans-
lation table that was used.3
One important question is what the maximum
phrase length should be for extraction. To inves-
tigate this issue, we ran experiments on the devel-
3These experiments also used the default pruning from the
Pharaoh decoder, allowing only the 10 best output phrases to be
considered for each input phrase. This simple global pruning
cannot be substituted for the competitive linking described here.
211
 
26
 
27
 
28
 
29  3
 
4
 
5
 
6
 
7
BLEU Score
Maximum
 Phrase 
Length
Full Phra
se Trans
. Table
Local Co
mp. Linki
ng
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  3
 
4
 
5
 
6
 
7
Num. of Phrase Pairs (Millions)
Maximum
 Phrase 
Length
Full Phra
se Trans
. Table
Local Co
mp. Linki
ng
Figure 3: Scaling the maximum phrase length.
opment set. Figure 3 shows a comparison of the
full phrase table to local competitive linking as the
maximum phrase length is varied. Local competi-
tive linking consistently outperforms the full table
and the difference in BLEU score seems to increase
with the length. The growth in the size of the phrase
translation table seems to be linear with maximum
phrase length in both cases, with the table size grow-
ing at a slower rate under local competitive linking.
To verify these results, we tested the model
trained with the full phrase translation table against
the model trained with the table selected by local
competitive linking on the heldout test data. Both ta-
bles included phrases up to length 7 and the models
were tested on a set of 2000 unseen sentence pairs.
The results matched the development experiments.
The full system scored 26.78 while the local linking
achieved 28.30, a difference of 1.52 BLEU points.
5 Discussion
The most closely related work attempts to create
higher quality phrase translation tables by learning
a generative model that directly incorporates phrase
pair selection. The original approach (Marcu and
Wong, 2002) was limited due to computational con-
straints but recent work (DeNero et al, 2006; Birch
et al, 2006) has improved the efficiency by using
word alignments as constraints on the set of possible
phrase pairs. The best results from this line of work
allow for a significantly smaller phrase translation
table, but never improve translation performance.
In this paper, we presented an algorithm that
improves translation quality by selecting a smaller
phrase translation table. We hope that this work
highlights the need to think carefully about the qual-
ity of the phrase translation table, which is the cen-
tral knowledge source for most modern statistical
machine translation systems. The methods used in
the experiments are so simple that we believe that
there is significant potential for improvement by us-
ing better methods for scoring phrase pairs and se-
lecting phrase pairs based those scores.
References
Alexandra Birch, Chris Callison-Burch, Miles Osborne, and
Philipp Koehn. 2006. Constraining the phrase-based, join
probability statistical translation model. In Proceedings of
the Workshop on Stastical Machine Translation.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Proceedings of the Workshop on Stastical Machine
Translation.
George Foster, Roland Kuhn, and Howard Johnson. 2006.
Phrasetable smoothing for stastical machine translation. In
Proceedings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Stastical phrase-based translation. In Proceedings of the
North American Chapter of the Association for Computa-
tional Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of The Sixth Conference of the Association for Ma-
chine Translation in the Americas.
Daniel Marcu and William Wong. 2002. A phrase-based, joint
probability model for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing.
I. Dan Melamed. 2000. Models of translation equivalence
amoung words. Computational Linguistics, 26(2):221?249.
RadaMihalcea and Ted Pedersen. 2003. An evaluation exercise
for word alignment. In Proceedings of the HLT-NAACL 2003
Workshop, Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006. Im-
proved discriminative bilingual word alignment. In Proceed-
ings of the 44th Annual Meeting of the Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.
212
An Improved Error Model for Noisy Channel Spelling Correction
Abstract
The noisy channel model has been applied
to a wide range of problems, including
spelling correction.  These models consist
of two components: a source model and a
channel model.  Very little research has
gone into improving the channel model
for spelling correction.  This paper
describes a new channel model for
spelling correction, based on generic
string to string edits.  Using this model
gives significant performance
improvements compared to previously
proposed models.
Introduction
The noisy channel model (Shannon 1948)
has been successfully applied to a wide
range of problems, including spelling
correction.  These models consist of two
components: a source model and a channel
model.  For many applications, people have
devoted considerable energy to improving
both components, with resulting
improvements in overall system accuracy.
However, relatively little research has gone
into improving the channel model for
spelling correction.  This paper describes an
improvement to noisy channel spelling
correction via a more powerful model of
spelling errors, be they typing mistakes or
cognitive errors, than has previously been
employed.  Our model works by learning
generic string to string edits, along with the
probabilities of each of these edits.  This
more powerful model gives significant
improvements in accuracy over previous
approaches to noisy channel spelling
correction.
1 Noisy Channel Spelling Correction
This paper will address the problem of
automatically training a system to correct
generic single word spelling errors.1  We do
not address the problem of correcting
specific word set confusions such as
{to,too,two} (see (Golding and Roth 1999)).
We will define the spelling correction
problem abstractly as follows: Given an
alphabet ? , a dictionary D consisting of
strings in ? * and a string s, where
Ds ? and *??s , find the word Dw ?  that
is most likely to have been erroneously input
as s.  The requirement that Ds ? can be
dropped, but it only makes sense to do so in
the context of a sufficiently powerful
language model.
In a probabilistic system, we want to
find )|(argmax   w swP .  Applying Bayes?
Rule and dropping the constant
denominator, we get the unnormalized
posterior: )(*)|(argmax   w wPwsP .  We now
have a noisy channel model for spelling
correction, with two components, the source
model P(w) and the channel model P(s | w).
The model assumes that natural language
text is generated as follows: First a person
chooses a word to output, according to the
probability distribution P(w).  Then the
person attempts to output the word w, but
the noisy channel induces the person to
output string s instead, according to the
                                                     
1
 Two very nice overviews of spelling correction can
be found in (Kukich 1992) and (Jurafsky and Martin
2000).
Eric Brill and Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, Wa. 98052
{brill,bobmoore}@microsoft.com
distribution P(s | w).  For instance, under
typical circumstances we would expect
P(the | the) to be very high, P(teh | the) to be
relatively high and P(hippopotamus | the) to
be extremely low.  In this paper, we will
refer to the channel model as the error
model.
Two seminal papers first posed a
noisy channel model solution to the spelling
correction problem.  In (Mayes, Damerau et
al. 1991), word bigrams are used for the
source model.  For the error model, they first
define the confusion set of a string s to
include s, along with all words w in the
dictionary D such that s can be derived from
w by a single application of one of the four
edit operations:
(1) Add a single letter.
(2) Delete a single letter.
(3) Replace one letter with another.
(4) Transpose two adjacent letters.
Let C be the number of words in the
confusion set of d.  Then they define the
error model, for all s in the confusion set of
d, as:
??
??
?
?
=
=
otherwise )1(
)-(1
d  s if  
)|(
C
dsP ?
?
7KLVLVDYHU\VLPSOHHUURUPRGHOZKHUH LV
the prior on a typed word being correct, and
the remaining probability mass is distributed
evenly among all other words in the
confusion set.
Church and Gale (1991) propose a
more sophisticated error model.  Like
Mayes, Damerau, et al (1991), they
consider as candidate source words only
those words that are a single basic edit away
from s, using the same edit set as above.
However, two improvements are made.
First, instead of weighing all edits equally,
each unique edit has a probability associated
with it. Second, insertion and deletion
probabilities are conditioned on context.
The probability of inserting or deleting a
character is conditioned on the letter
appearing immediately to the left of that
character.
The error probabilities are derived by
first assuming all edits are equiprobable.
They use as a training corpus a set of space-
delimited strings that were found in a large
collection of text, and that (a) do not appear
in their dictionary and (b) are no more than
one edit away from a word that does appear
in the dictionary.  They iteratively run the
spell checker over the training corpus to find
corrections, then use these corrections to
update the edit probabilities. Ristad and
Yianilos (1997) present another algorithm
for deriving these edit probabilities from a
training corpus, and show that for the
problem of word pronunciation, using the
learned string edit distance gives one fourth
the error rate compared to using unweighted
edits.
2 An Improved Error Model
Previous error models have all been based
on Damerau-Levenshtein distance measures
(Damerau 1964; Levenshtein 1966), where
the distance between two strings is the
minimum number of single character
insertions, substitutions and deletions (and
in some cases, character pair transpositions)
necessary to derive one string from another.
Improvements have been made by
associating probabilities with individual edit
operations.
We propose a much more generic
HUURU PRGHO  /HW  EH DQ DOSKDEHW  2XU
model allows all edit operations of the form
?  ZKHUH *??, .  3 ?  LV WKH
probability that when users intends to type
WKH VWULQJ  WKH\ W\SH  LQVWHDG 1RWH WKDW
the edit operations allowed in Church and
Gale (1991), Mayes, Damerau et al (1991)
and Ristad and Yianilos (1997), are properly
subsumed by our generic string to string
substitutions.
In addition, we condition on the
position in the string that the edit occurs in,
3 ?  _ 361 ZKHUH 361   ^VWDUW RI
word, middle of word, end of word}.2  The
position is determined by the location of
VXEVWULQJ  LQ WKH VRXUFH GLFWLRQDU\ZRUG
Positional information is a powerful
conditioning feature for rich edit operations.
For instance, P(e | a) does not vary greatly
between the three positions mentioned
above.  However, P(ent | ant) is highly
dependent upon position.  People rarely
mistype antler as entler, but often mistype
reluctant as reluctent.
Within the noisy channel framework,
we can informally think of our error model
as follows.  First, a person picks a word to
generate.  Then she picks a partition of the
characters of that word.  Then she types
each partition, possibly erroneously.  For
example, a person might choose to generate
the word physical.  She would then pick a
partition from the set of all possible
partitions, say: ph y s i c al.  Then she
would generate each partition, possibly with
errors.  After choosing this particular word
and partition, the probability of generating
the string fisikle with the partition f i s i k le
would be P(f | ph) *P(i | y) * P(s | s) *P(i | i)
* P(k | c) *P(le | al).3
The above example points to
advantages of our model compared to
previous models based on weighted
Damerau-Levenshtein distance.  Note that
neither P(f | ph) nor P(le | al) are modeled
directly in the previous approaches to error
modeling.  A number of studies have
pointed out that a high percentage of
misspelled words are wrong due to a single
letter insertion, substitution, or deletion, or
from a letter pair transposition (Damerau
1964; Peterson 1986).  However, even if this
is the case, it does not imply that nothing is
                                                     
2
 Another good PSN feature would be morpheme
boundary.
3
 We will leave off the positional conditioning
information for simplicity.
to be gained by modeling more powerful
edit operations.  If somebody types the
string confidant, we do not really want to
model this error as P(a | e), but rather P(ant |
ent).  And anticedent can more accurately be
modeled by P(anti | ante), rather than P(i | e).
By taking a more generic approach to error
modeling, we can more accurately model the
errors people make.
A formal presentation of our model
follows.  Let Part(w) be the set of all
possible ways of partitioning string w into
adjacent (possibly null) substrings.  For a
particular partition R?Part(w), where |R|=j
(R consists of j contiguous segments), let Ri
be the ith segment.  Under our model,
P(s | w) =
? ??
? =
=
?)(
||
1|||| )(
)|()|(
wPartR
R
i
ii
RT
sPartT
RTPwRP
One particular pair of alignments for
s and w induces a set of edits that derive s
from w.  By only considering the best
partitioning of s and w, we can simplify this
to:
P(s | w) =
max R ?Part(w),T?Part(s) P(R|w)?
=
||
1
R
i
P(Ti|Ri)
We do not yet have a good way to
derive P(R | w), and in running experiments
we determined that poorly modeling this
distribution gave slightly worse performance
than not modeling it at all, so in practice we
drop this term.
3 Training the Model
To train the model, we need a training set
consisting of {si, wi} string pairs,
representing spelling errors si paired with
the correct spelling of the word wi.  We
begin by aligning the letters in si with those
in wi based on minimizing the edit distance
between si and wi, based on single character
insertions, deletions and substitutions.   For
instance, given the training pair <akgsual,
actual>, this could be aligned as:
a     c       t     u     a      l
a      k     g     s     u     a     l
This corresponds to the sequence of edit
operations:
a?a    c?N ?g   t?s   u?u   a?a   l?l
To allow for richer contextual
information, we expand each nonmatch
substitution to incorporate up to N additional
adjacent edits. For example, for the first
nonmatch edit in the example above, with
N=2, we would generate the following
substitutions:
c ? k
ac ? ak
c ? kg
ac ? akg
ct ? kgs
We would do similarly for the other
nonmatch edits, and give each of these
substitutions a fractional count.
We can then calculate the probability
RI HDFK VXEVWLWXWLRQ ?  DV FRXQW ?
FRXQW FRXQW ? LVVLPSO\WKHVXP
of the counts derived from our training data
as explained above.  Estimating FRXQW LVD
bit tricky.  If we took a text corpus, then
extracted all the spelling errors found in the
corpus and then used those errors for
training, FRXQW  ZRXOG VLPSO\ EH WKH
number of times VXEVWULQJ  RFFXUV LQ WKH
text corpus.  But if we are training from a set
of {si, wi} tuples and not given an associated
corpus, we can do the following:
(a) From a large collection of representative
WH[WFRXQWWKHQXPEHURIRFFXUUHQFHVRI 
(b) Adjust the count based on an estimate of
the rate with which people make typing
errors.
Since the rate of errors varies widely
and is difficult to measure, we can only
crudely approximate it.  Fortunately, we
have found empirically that the results are
not very sensitive to the value chosen.
Essentially, we are doing one
iteration of the Expectation-Maximization
algorithm (Dempster, Laird et al 1977).
The idea is that contexts that are useful will
accumulate fractional counts across multiple
instances, whereas contexts that are noise
will not accumulate significant counts.
4 Applying the Model
Given a string s, where Ds ? , we want to
return )|()|(argmax   w contextwPswP .  Our
approach will be to return an n-best list of
candidates according to the error model, and
then rescore these candidates by taking into
account the source probabilities.
We are given a dictionary D and a
set of parameters P, where each parameter is
3 ?  IRU VRPH *??, , meaning the
SUREDELOLW\WKDWLIDVWULQJ  LV LQWHQGHG WKH
QRLV\FKDQQHOZLOOSURGXFH  LQVWHDG )LUVW
note that for a particular pair of strings {s,
w} we can use the standard dynamic
programming algorithm for finding edit
distance by filling a |s|*|w| weight matrix
(Wagner and Fisher 1974; Hall and Dowling
1980), with only minor changes.  For
computing the Damerau-Levenshtein
distance between two strings, this can be
done in O(|s|*|w|) time.  When we allow
generic edit operations, the complexity
increases to O(|s|2*|w|2).  In filling in a cell
(i,j) in the matrix for computing Damerau-
Levenshtein distance we need only examine
cells (i,j-1), (i-1,j) and (i-1,j-1).  With
generic edits, we have to examine all cells
(a,b) where a ? i and b ? j.
We first precompile the dictionary
into a trie, with each node in the trie
corresponding to a vector of weights.  If we
think of the x-axis of the standard weight
matrix for computing edit distance as
corresponding to w (a word in the
dictionary), then the vector at each node in
the trie corresponds to a column in the
weight matrix associated with computing the
distance between s and the string prefix
ending at that trie node.
:HVWRUHWKH ? SDUDPHWHUVDVDtrie
of tries. We have one trie corresponding to
DOOVWULQJV WKDWDSSHDURQWKHOHIWKDQGVLGH
of some substitution in our parameter set.
At every node in this trie, corresponding to a
VWULQJ ZHSRLQW WR D trie consisting of all
VWULQJV  WKDW DSSHDURQ WKH ULJKW KDQG VLGH
RIDVXEVWLWXWLRQLQRXUSDUDPHWHUVHWZLWK
on the left hand side.  We store the
substitution probabilities at the terminal
QRGHVRIWKH WULHV
%\ VWRULQJ ERWK  DQG  VWULQJV LQ
reverse order, we can efficiently compute
edit distance over the entire dictionary.  We
process the dictionary trie from the root
downwards, filling in the weight vector at
each node.  To find the substitution
parameters that are applicable, given a
particular node in the trie and a particular
position in the input string s (this
corresponds to filling in one cell in one
vector of a dictionary trie node) we trace up
from the node to the root, while tracing
GRZQ WKH  trie from the root. As we trace
GRZQ WKH  trie, if we encounter a terminal
node, we follow the pointer to the
FRUUHVSRQGLQJ  trie, and then trace
backwards from the position in s while
WUDFLQJGRZQWKH trie.
Note that searching through a static
dictionary D is not a requirement of our
error model.  It is possible that with a
different search technique, we could apply
our model to languages such as Turkish for
which a static dictionary is inappropriate
(Oflazer 1994).
Given a 200,000-word dictionary, and
using our best error model, we are able to
spell check strings not in the dictionary in
approximately 50 milliseconds on average,
running on a Dell 610 500mhz Pentium III
workstation.
5 Results
5.1  Error Model in Isolation
We ran experiments using a 10,000-
word corpus of common English spelling
errors, paired with their correct spelling.
We used 80% of this corpus for training and
20% for evaluation.  Our dictionary
contained approximately 200,000 entries,
including all words in the test set.  The
results in this section are obtained with a
language model that assigns uniform
probability to all words in the dictionary.  In
Table 1 we show K-best results for different
maximum context window sizes, without
using positional information.  For instance,
the 2-best accuracy is the percentage of time
the correct answer is one of the top two
answers returned by the system. Note that a
maximum window of zero corresponds to
the set of single character insertion, deletion
and substitution edits, weighted with their
probabilities.  We see that, up to a point,
additional context provides us with more
accurate spelling correction and beyond that,
additional context neither helps nor hurts.
Max
Window 1-Best 2-Best 3-Best
0 87.0 93.9 95.9
CG 89.5 94.9 96.5
1 90.9 95.6 96.8
2 92.9 97.1 98.1
3 93.6 97.4 98.5
4 93.6 97.4 98.5
Table 1 Results without positional
information
In Table 1, the row labelled CG
shows the results when we allow the
equivalent set of edit operations to those
used in (Church and Gale 1991).  This is a
proper superset of the set of edits where the
maximum window is zero and a proper
subset of the edits where the maximum
window is one.  The CG model is essentially
equivalent to the Church and Gale error
model, except (a) the models above can
posit an arbitrary number of edits and (b) we
did not do parameter reestimation (see
below).
Next, we measured how much we
gain by conditioning on the position of the
edit relative to the source word.  These
results are shown in Table 2.  As we
expected, positional information helps more
when using a richer edit set than when using
only single character edits.  For a maximum
window size of 0, using positional
information gives a 13% relative
improvement in 1-best accuracy, whereas
for a maximum window size of 4, the gain is
22%.  Our full strength model gives a 52%
relative error reduction on 1-best accuracy
compared to the CG model (95.0%
compared to 89.5%).
Max Window 1-Best 2-Best 3-Best
0 88.7 95.1 96.6
1 92.8 96.5 97.4
2 94.6 98.0 98.7
3 95.0 98.0 98.8
4 95.0 98.0 98.8
5 95.1 98.0 98.8
Table 2 Results with positional
information.
We experimented with iteratively
reestimating parameters, as was done in the
original formulation in (Church and Gale
1991).  Doing so resulted in a slight
degradation in performance.  The data we
are using is much cleaner than that used in
(Church and Gale 1991) which probably
explains why reestimation benefited them in
their experiments and did not give any
benefit to the error models in our
experiments.
5.2  Adding a Language Model
Next, we explore what happens to
our results as we add a language model.  In
order to get errors in context, we took the
Brown Corpus and found all occurrences of
all words in our test set.  Then we mapped
these words to the incorrect spellings they
were paired with in the test set, and ran our
spell checker to correct the misspellings.
We used two language models.  The first
assumed all words are equally likely, i.e. the
null language model used above.    The
second used a trigram language model
derived from a large collection of on-line
text (not including the Brown Corpus).
Because a spell checker is typically applied
right after a word is typed, the language
model only used left context.
We show the results in Figure 1,
where we used the error model with
positional information and with a maximum
context window of four, and used the
language model to rescore the 5 best word
candidates returned by the error model.
Note that for the case of no language model,
the results are lower than the results quoted
above (e.g. a 1-best score above of 95.0%,
compared to 93.9% in the graph).  This is
because the results on the Brown Corpus are
computed per token, whereas above we were
computing results per type.
One question we wanted to ask is whether
using a good language model would obviate
the need for a good error model.  In Figure
2, we applied the trigram model to resort the
5-best results of the CG model.  We see that
while a language model improves results,
using the better error model (Figure 1) still
gives significantly better results.  Using a
language model with our best error model
gives a 73.6% error reduction compared to
using a language model with the CG error
model.  Rescoring the 20-best output of the
CG model instead of the 5-best only
improves the 1-best accuracy from 90.9% to
91.0%.
93
94
95
96
97
98
99
100
1 2 3 4 5
N-Best
A
cc
ur
ac
y
No
Language
Model
Trigram
Language
Model
Figure 1 Spelling Correction
Improvement When Using a Language
Model
84
86
88
90
92
94
96
1 2 3 4 5
N-Best
A
cc
ur
ac
y
No
Language
Model
Trigram
Language
Model
Figure 2 Using the CG Error Model with
a Trigram Language Model
Conclusion
We have presented a new error model for
noisy channel spelling correction based on
generic string to string edits, and have
demonstrated that it results in a significant
improvement in performance compared to
previous approaches.  Without a language
model, our error model gives a 52%
reduction in spelling correction error rate
compared to the weighted Damerau-
Levenshtein distance technique of Church
and Gale.  With a language model, our
model gives a 74% reduction in error.
One exciting future line of research
is to explore error models that adapt to an
individual or subpopulation.  With a rich set
of edits, we hope highly accurate
individualized spell checking can soon
become a reality.
References
Church, K. and W. Gale (1991). ?Probability Scoring
for Spelling Correction.? Statistics and Computing
1: 93-103.
Damerau, F. (1964). ?A technique for computer
detection and correction of spelling errors.?
Communications of the ACM 7(3): 659-664.
Dempster, A., N. Laird, et al (1977). ?Maximum
likelihood from incomplete data via the EM
algorithm.? Journal of the Royal Statistical Society
39(1): 1-21.
Golding, A. and D. Roth (1999). ?A Winnow-Based
Approach to Spelling Correction.? Machine
Learning 34: 107-130.
Hall, P. and G. Dowling (1980). ?Approximate string
matching.? ACM Computing Surveys 12(4): 17-38.
Jurafsky, D. and J. Martin (2000). Speech and
Language Processing, Prentice Hall.
Kukich, K. (1992). ?Techniques for Automatically
Correcting Words in Text.? ACM Computing
Surveys 24(4): 377-439.
Levenshtein, V. (1966). ?Binary codes capable of
correcting deletions, insertions and reversals.?
Soviet Physice -- Doklady 10: 707-710.
Mayes, E., F. Damerau, et al (1991). ?Context Based
Spelling Correction.? Information Processing and
Management 27(5): 517-522.
Oflazer, K. (1994). Spelling Correction in
Agglutinative Languages. Applied Natural
Language Processing, Stuttgart, Germany.
Peterson, J. (1986). ?A note on undetected typing
errors.? Communications of the ACM 29(7): 633-
637.
Ristad, E. and P. Yianilos (1997). Learning String
Edit Distance. International Conference on
Machine Learning, Morgan Kaufmann.
Shannon, C. (1948). ?A mathematical theory of
communication.? Bell System Technical Journal
27(3): 379-423.
Wagner, R. and M. Fisher (1974). ?The string to
string correction problem.? JACM 21: 168-173.
Machine-learned contexts for linguistic operations 
in German sentence realization 
 
Michael GAMON, Eric RINGGER, Simon CORSTON-OLIVER, Robert MOORE 
Microsoft Research  
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, simonco, bobmoore}@microsoft.com 
 
Abstract 
We show that it is possible to learn the 
contexts for linguistic operations which 
map a semantic representation to a 
surface syntactic tree in sentence 
realization with high accuracy. We cast 
the problem of learning the contexts for 
the linguistic operations as 
classification tasks, and apply 
straightforward machine learning 
techniques, such as decision tree 
learning. The training data consist of 
linguistic features extracted from 
syntactic and semantic representations 
produced by a linguistic analysis 
system. The target features are extracted 
from links to surface syntax trees. Our 
evidence consists of four examples from 
the German sentence realization system 
code-named Amalgam: case 
assignment, assignment of verb position 
features, extraposition, and syntactic 
aggregation 
1 Introduction 
The last stage of natural language generation, 
sentence realization, creates the surface string 
from an abstract (typically semantic) 
representation. This mapping from abstract 
representation to surface string can be direct, or it 
can employ intermediate syntactic representations 
which significantly constrain the output. 
Furthermore, the mapping can be performed 
purely by rules, by application of statistical 
models, or by a combination of both techniques. 
Among the systems that use statistical or 
machine learned techniques in sentence 
realization, there are various degrees of 
intermediate syntactic structure. Nitrogen 
(Langkilde and Knight, 1998a, 1998b) produces a 
large set of alternative surface realizations of an 
input structure (which can vary in abstractness). 
This set of candidate surface strings, represented 
as a word lattice, is then rescored by a word-
bigram language model, to produce the best-
ranked output sentence. FERGUS (Bangalore and 
Rambow, 2000), on the other hand, employs a 
model of syntactic structure during sentence 
realization. In simple terms, it adds a tree-based 
stochastic model to the approach taken by the 
Nitrogen system. This tree-based model chooses a 
best-ranked XTAG representation for a given 
dependency structure. Possible linearizations of 
the XTAG representation are generated and then 
evaluated by a language model to pick the best 
possible linearization, as in Nitrogen. 
In contrast, the sentence realization system 
code-named Amalgam (A Machine Learned 
Generation Module) (Corston-Oliver et al, 2002; 
Gamon et al, 2002b) employs a series of 
linguistic operations which map a semantic 
representation to a surface syntactic tree via 
intermediate syntactic representations. The 
contexts for most of these operations in Amalgam 
are machine learned. The resulting syntactic tree 
contains all the necessary information on its leaf 
nodes from which a surface string can be read.  
The goal of this paper is to show that it is 
possible to learn accurately the contexts for 
linguistically complex operations in sentence 
realization. We propose that learning the contexts 
for the application of these linguistic operations 
can be viewed as per-operation classification 
problems. This approach combines advantages of 
a linguistically informed approach to sentence 
realization with the advantages of a machine 
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 25-32.
                         Proceedings of the 40th Annual Meeting of the Association for
learning approach. The linguistically informed 
approach allows us to deal with complex linguistic 
phenomena, while machine learning automates the 
discovery of contexts that are linguistically 
relevant and relevant for the domain of the data. 
The machine learning approach also facilitates 
adaptation of the system to a new domain or 
language. Furthermore, the quantitative nature of 
the machine learned models permits finer 
distinctions and ranking among possible solutions. 
To substantiate our claim, we provide four 
examples from Amalgam: assignment of case, 
assignment of verb position features, 
extraposition, and syntactic aggregation. 
2 Overview of Amalgam 
Amalgam takes as its input a sentence-level 
semantic graph representation with fixed lexical 
choices for content words (the logical form graph 
of the NLPWin system ? see (Heidorn, 2000)). 
This representation is first degraphed into a tree, 
and then gradually augmented by the insertion of 
function words, assignment of case and verb 
position features, syntactic labels, etc., and 
transformed into a syntactic surface tree. A 
generative statistical language model establishes 
linear order in the surface tree (Ringger et al, in 
preparation), and a surface string is generated 
from the leaf nodes. Amalgam consists of eight 
stages. We label these ML (machine-learned 
context) or RB (rule-based). 
Stage 1 Pre-processing (RB): 
? degraphing of the semantic representation 
? retrieval of lexical information 
Stage 2 Flesh-out (ML): 
? assignment of syntactic labels 
? insertion of function words 
? assignment of case and verb position 
features 
Stage 3 Conversion to syntactic tree (RB): 
? introduction of syntactic representation 
for coordination 
? splitting of separable prefix verbs based 
on both lexical information and 
previously assigned verb position features 
? reversal of heads (e.g., in quantitative 
expressions) (ML) 
Stage 4 Movement: 
? extraposition (ML) 
? raising, wh movement (RB) 
Stage 5 Ordering (ML): 
? ordering of constituents and leaf nodes in 
the tree 
Stage 6 Surface cleanup (ML): 
? lexical choice of determiners and relative 
pronouns 
? syntactic aggregation 
Stage 7 Punctuation (ML) 
Stage 8 Inflectional generation (RB) 
All machine learned components, with the 
exception of the generative language model for 
ordering of constituents (stage 5), are decision tree 
classifiers built with the WinMine toolkit 
(Chickering et al, 1997; Chickering, nd.). There 
are a total of eighteen decision tree classifiers in 
the system. The complexity of the decision trees 
varies with the complexity of the modeled task. 
The number of branching nodes in the decision 
tree models in Amalgam ranges from 3 to 447. 
3 Data and feature extraction 
The data for all of the models were drawn from a 
set of 100,000 sentences from technical software 
manuals and help files. The sentences are 
analyzed by the NLPWin system, which provides 
a syntactic and logical form analysis. Nodes in the 
logical form representation are linked to the 
corresponding syntactic nodes, allowing us to 
learn contexts for the mapping from the semantic 
representation to a surface syntax tree. The data is 
split 70/30 for training versus model parameter 
tuning. For each set of data we built decision trees 
at several different levels of granularity (by 
manipulating the prior probability of tree 
structures to favor simpler structures) and selected 
the model with the maximal accuracy as 
determined on the parameter tuning set. All 
models are then tested on data extracted from a 
separate blind set of 10,000 sentences from the 
same domain. For both training and test, we only 
extract features from sentences that have received 
a complete, spanning parse: 85.14% of the 
sentences in the training and parameter tuning set, 
and 84.59% in the blind test set fall into that 
category. Most sentences yield more than one 
training case. 
We attempt to standardize as much as possible 
the set of features to be extracted. We exploit the 
full set of features and attributes available in the 
analysis, instead of pre-determining a small set of 
potentially relevant features (Gamon et al, 
2002b). This allows us to share the majority of 
code between the individual feature extraction 
tasks. More importantly, it enables us to discover 
new linguistically interesting and/or domain-
specific generalizations from the data. Typically, 
we extract the full set of available analysis 
features of the node under investigation, its parent 
and its grandparent, with the only restriction being 
that these features need to be available at the stage 
where the model is consulted at generation run-
time. This provides us with a sufficiently large 
structural context for the operations. In addition, 
for some of the models we add a small set of 
features that we believe to be important for the 
task at hand, and that cannot easily be expressed 
as a combination of analysis features/attributes on 
constituents. Most features, such as lexical 
subcategorization features and semantic features 
such as [Definite] are binary. Other features, such 
as syntactic label or semantic relation, have as 
many as 25 values. Training time on a standard 
500MHz PC ranges from one hour to six hours. 
4 Assignment of case 
In German sentence realization, proper 
assignment of morphological case is essential for 
fluent and comprehensible output. German is a 
language with fairly free constituent order, and the 
identification of functional roles, such as subject 
versus object, is not determined by position in the 
sentence, as in English, but by morphological 
marking of one of the four cases: nominative, 
accusative, genitive or dative. In Amalgam, case 
assignment is one of the last steps in the Flesh-out 
stage (stage 2). Morphological realization of case 
can be ambiguous in German (for example, a 
feminine singular NP is ambiguous between 
accusative and nominative case). Since the 
morphological realization of case depends on the 
gender, number and morphological paradigm of a 
given NP, we chose to only consider NP nodes 
with unambiguous case as training data for the 
model1. As the target feature for this model is 
                                                     
1
 Ideally, we should train the case assignment model on 
a corpus that is hand-disambiguated for case. In the 
absence of such a corpus, though, we believe that our 
approach is linguistically justified. The case of an NP 
depends solely on the syntactic context it appears in. 
morphological case, it has four possible values for 
the four cases in German. 
4.1 Features in the case assignment 
model 
For each data point, a total of 712 features was 
extracted. Of the 712 features available to the 
decision tree building tools, 72 were selected as 
having predictive value in the model. The selected 
features fall into the following categories: 
? syntactic label of the node, its parent and 
grandparent 
? lemma (i.e., citation form) of the parent, 
and lemma of the governing preposition 
? subcategorization information, including 
case governing properties of governing 
preposition and parent 
? semantic relation of the node itself to its 
parent, of the parent to its grandparent, 
and of the grandparent to its great-
grandparent 
? number information on the parent and 
grandparent 
? tense and mood on the parent and 
grandparent 
? definiteness on the node, its parent and 
grandparent 
? the presence of various semantic 
dependents such as subject, direct and 
indirect objects, operators, attributive 
adjuncts and unspecified modifiers on the 
node and its parent and grandparent 
? quantification, negation, coordination on 
the node, the parent and grandparent 
? part of speech of the node, the parent and 
the grandparent 
? miscellaneous semantic features on the 
node itself and the parent 
4.2 The case assignment model 
The decision tree model for case assignment 
has 226 branching nodes, making it one of the 
most complex models in Amalgam. For each 
nominal node in the 10,000 sentence test set, we 
compared the prediction of the model to the 
                                                                                  
Since we want to learn the syntactically determining 
factors for case, using unambiguously case marked NPs 
for training seems justified. 
morphological case compatible with that node. 
The previously mentioned example of a singular 
feminine NP, for example, would yield a ?correct? 
if the model had predicted nominative or 
accusative case (because the NP is 
morphologically ambiguous between accusative 
and nominative), and it would yield an ?incorrect? 
if the model had predicted genitive or dative. This 
particular evaluation setup was a necessary 
compromise because of the absence of a hand-
annotated corpus with disambiguated case in our 
domain. The caveat here is that downstream 
models in the Amalgam pipeline that pick up on 
case as one of their features rely on the absolute 
accuracy of the assigned case, not the relative 
accuracy with respect to morphological 
ambiguity. Accuracy numbers for each of the four 
case assignments are given in Table 1. Note that it 
is impossible to give precision/recall numbers, 
without a hand-disambiguated test set. The 
baseline for this task is 0.7049 (accuracy if the 
most frequent case (nominative) had been 
assigned to all NPs). 
Table 1. Accuracy of the case assignment model. 
Value Accuracy 
Dat 0.8705 
Acc 0.9707 
Gen 0.9457 
Nom 0.9654 
overall 0.9352 
5 Assignment of verb position 
features 
One of the most striking properties of German is 
the distributional pattern of verbs in main and 
subordinate clauses. Most descriptive accounts of 
German syntax are based on a topology of the 
German sentence that treats the position of the 
verb as the fixed frame around which other 
syntactic constituents are organized in relatively 
free order (cf. Eisenberg, 1999; Engel, 1996). The 
position of the verb in German is non-negotiable; 
errors in the positioning of the verb result in 
gibberish, whereas most permutations of other 
constituents only result in less fluent output. 
Depending on the position of the finite verb, 
German sentences and verb phrases are classified 
as being ?verb-initial?, ?verb-second? or ?verb-
final?. In verb-initial clauses (e.g., in imperatives), 
the finite verb is in initial position. Verb-second 
sentences contain one constituent preceding the 
finite verb, in the so-called ?pre-field?. The finite 
verb is followed by any number of constituents in 
the ?middle-field?, and any non-finite verbs are 
positioned at the right periphery of the clause, 
possibly followed by extraposed material or 
complement clauses (the ?post-field?). Verb-final 
clauses contain no verbal element in the verb-
second position: all verbs are clustered at the right 
periphery, preceded by any number of constituents 
and followed only by complement clauses and 
extraposed material. 
During the Flesh-out stage in Amalgam, a 
decision tree classifier is consulted to make a 
classification decision among the four verb 
positions: ?verb-initial?, ?verb-second?, ?verb-
final?, and ?undefined?. The value ?undefined? 
for the target feature of verb position is extracted 
for those verbal constituents where the local 
syntactic context is too limited to make a clear 
distinction between initial, second, or final 
position of the verb. The number of ?undefined? 
verb positions is small compared to the number of 
clearly established verb positions: in the test set, 
there were only 690 observed cases of 
?undefined? verb position out of a total of 15,492 
data points. At runtime in Amalgam, verb position 
features are assigned based on the classification 
provided by the decision tree model. 
5.1 Features in the verb position model 
For each data point, 713 features were extracted. 
Of those features, 41 were selected by the decision 
tree algorithm. The selected features fall into the 
following categories: 
? syntactic label of the node and the parent 
? subcategorization features 
? semantic relations of the node to its parent 
and of the parent node to its parent 
? tense and mood features 
? presence of empty, uncontrolled subject 
? semantic features on the node and the 
parent 
5.2 The verb position model 
The decision tree model for verb position has 
115 branching nodes. Precision, recall and F-
measure for the model are given in Table 2. As a 
point of reference for the verb position classifier, 
assigning the most frequent value (second) of the 
target feature yields a baseline score of 0.4240. 
Table 2. Precision, recall, and F-measure for the verb 
position model. 
Value Precision Recall F-measure 
Initial 0.9650 0.9809 0.9729 
Second 0.9754 0.9740 0.9743 
Final 0.9420 0.9749 0.9581 
Undefined 0.5868 0.3869 0.4663 
Overall 
accuracy 
0.9491 
6 Extraposition 
In both German and English it is possible to 
extrapose clausal material to the right periphery of 
the sentence (extraposed clauses underlined in the 
examples below): 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave the 
country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumour has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Extraposition is not obligatory like other types 
of movement (such as Wh-movement). Both 
extraposed and non-extraposed versions of a 
sentence are acceptable, with varying degrees of 
fluency. 
The interesting difference between English and 
German is the frequency of this phenomenon. 
While it can easily be argued that English 
sentence realization may ignore extraposition and 
still result in very fluent output, the fluency of 
sentence realization for German will suffer much 
more from the lack of a good extraposition 
mechanism. We profiled data from various 
domains (Gamon et al 2002a) to substantiate this 
linguistic claim (see Uszkoreit et al 1998 for 
similar results). In the technical domain, more 
than one third of German relative clauses are 
extraposed, as compared to a meagre 0.22% of 
English relative clauses. In encyclopaedia text 
(Microsoft Encarta), approximately every fifth 
German relative clause is extraposed, compared to 
only 0.3% of English relative clauses. For 
complement clauses and infinitival clauses, the 
differences are not as striking, but still significant: 
in the technical and encyclopaedia domains, 
extraposition of infinitival and complement 
clauses in German ranges from 1.5% to 3.2%, 
whereas English only shows a range from 0% to 
0.53%. 
We chose to model extraposition as an iterative 
movement process from the original attachment 
site to the next higher node in the tree (for an 
alternative one-step solution and a comparison of 
the two approaches see (Gamon et al, 2002a)). 
The target feature of the model is the answer to 
the yes/no question ?Should the clause move from 
node X to the parent of node X??. 
6.1 Features in the extraposition model 
The tendency of a clause to be extraposed depends 
on properties of both the clause itself (e.g., some 
notion of ?heaviness?) and the current attachment 
site. Very coarse linguistic generalizations are that 
a relative clause tends to be extraposed if it is 
sufficiently ?heavy? and if it is followed by verbal 
material in the same clause. Feature extraction for 
this model reflects that fact by taking into 
consideration features on the extraposition 
candidate, the current attachment site, and 
potential next higher landing site. This results in a 
total of 1168 features. Each node in the parent 
chain of an extraposable clause, up to the actual 
attachment node, constitutes a single data point 
During the decision tree building process, 60 
features were selected as predictive. They can be 
classified as follows: 
General feature: 
? overall sentence length 
Features on the extraposable clause: 
? presence of verb-final and verb-second 
ancestor nodes 
? ?heaviness? both in number of characters 
and number of tokens 
? various linguistic features in the local 
context (parent node and grandparent 
node): number and person, definiteness, 
voice, mood, transitivity, presence of 
logical subject and object, presence of 
certain semantic attributes, coordination, 
prepositional relations 
? syntactic label 
? presence of modal verbs 
? prepositional relations 
? transitivity 
Features on the attachment site 
? presence of logical subject 
? status of the parent and grandparent as a 
separable prefix verb 
? voice and presence of modal verbs on the 
parent and grandparent 
? presence of arguments and transitivity 
features on the parent and grandparent 
? number, person and definiteness; the same 
on parent and grandparent 
? syntactic label; the same on the parent and 
grandparent 
? verb position; the same on the parent 
? prepositional relation on parent and 
grandparent 
? semantic relation that parent and 
grandparent have to their respective 
parent node 
6.2 The extraposition model 
During testing of the extraposition model, the 
model was consulted for each extraposable clause 
to find the highest node to which that clause could 
be extraposed. In other words, the target node for 
extraposition is the highest node in the parent 
chain for which the answer to the classification 
task ?Should the clause move from node X to the 
parent of node X?? is ?yes? with no interceding 
?no? answer. The prediction of the model was 
compared with the actual observed attachment site 
of the extraposable clause to yield the accuracy 
figures shown in Table 3. The model has 116 
branching nodes. The baseline for this task is 
calculated by applying the most frequent value for 
the target feature (?don't move?) to all nodes. The 
baseline for extraposition of infinitival and 
complement clauses is very high. The number of 
extraposed clauses of both types in the test set 
(fifteen extraposed infinitival clauses and twelve 
extraposed complement clauses) is very small, so 
it comes as no surprise that the model accuracy 
ranges around the baseline for these two types of 
extraposed clauses. 
Table 3. Accuracy of the extraposition model. 
Extraposable clause Accuracy Baseline 
RELCL 0.8387 0.6093 
INFCL 0.9202 0.9370 
COMPCL 0.9857 0.9429 
Overall 0.8612 0.6758 
7 Syntactic aggregation 
Any sentence realization component that 
generates from an abstract semantic representation 
and strives to produce fluent output beyond simple 
templates will have to deal with coordination and 
the problem of duplicated material in 
coordination. This is generally viewed as a sub-
area of aggregation in the generation literature 
(Wilkinson, 1995; Shaw, 1998; Reape and 
Mellish, 1999; Dalianis and Hovy, 1993). In 
Amalgam, the approach we take is strictly intra-
sentential, along the lines of what has been called 
conjunction reduction in the linguistic literature 
(McCawley, 1988). While this may seem a fairly 
straightforward task compared to inter-sentential, 
semantic and lexical aggregation, it should be 
noted that the cross-linguistic complexity of the 
phenomenon makes it much less trivial than a first 
glance at English would suggest. In German, for 
example, position of the verb in the coordinated 
VPs plays an important role in determining which 
duplicated constituent can be omitted. 
The target feature for the classification task is 
formulated as follows: ?In which coordinated 
constituent is the duplicated constituent to be 
realized??. There are three values for the target 
feature: ?first?, ?last?, and ?middle?. The third 
value (?middle?) is a default value for cases where 
neither the first, nor the last coordinated 
constituent can be identified as the location for the 
realization of duplicated constituents. At 
generation runtime, multiple realizations of a 
constituent in coordination are collected and the 
aggregation model is consulted to decide on the 
optimal position in which to realize that 
constituent. The constituent in that position is 
retained, while all other duplicates are removed 
from the tree. 
7.1 Features in the syntactic aggregation 
model 
A total of 714 features were extracted for the 
syntactic aggregation model. Each instance of 
coordination which exhibits duplicated material at 
the semantic level without corresponding 
duplication at the syntactic level constitutes a data 
point. 
Of these features, 15 were selected as 
predictive in the process of building the decision 
tree model: 
? syntactic label and syntactic label of the 
parent node 
? semantic relation to the parent of the 
duplicated node, its parent and grandparent 
? part of speech of the duplicated node 
? verb position across the coordinated node 
? position of the duplicated node in 
premodifiers or postmodifiers of the parent 
? coordination of the duplicated node and 
the grandparent of the duplicated node 
? status of parent and grandparent as a 
proposition 
? number feature on the parent 
? transitivity and presence of a direct object 
on the parent 
7.2 The syntactic aggregation model 
The syntactic aggregation model has 21 branching 
nodes. Precision, recall and F-measure for the 
model are given in Table 4. As was to be expected 
on the basis of linguistic intuition, the value 
?middle? for the target feature did not play any 
role. In the test set there were only 2 observed 
instances of that value. The baseline for this task 
is 0.8566 (assuming ?first? as the default value). 
Table 4. Precision, recall, and F-measure for the 
syntactic aggregation model. 
Value Precision Recall F-measure 
last 0.9191 0.9082 0.9136 
first 0.9837 0.9867 0.9851 
middle 0.0000 0.0000 0.0000 
overall 
accuracy 
0.9746 
8 Conclusion and future research 
We have demonstrated on the basis of four 
examples that it is possible to learn the contexts 
for complex linguistic operations in sentence 
realization with high accuracy. We proposed to 
standardize most of the feature extraction for the 
machine learning tasks to all available linguistic 
features on the node, and its parent and 
grandparent node. This generalized set of features 
allows us to rapidly train on new sets of data and 
to experiment with new machine learning tasks. 
Furthermore, it prevents us from focusing on a 
small set of hand-selected features for a given 
phenomenon; hence, it allows us to learn new (and 
unexpected) generalizations from new data. 
We have found decision trees to be useful for 
our classification problems, but other classifiers 
are certainly applicable. Decision trees provided 
an easily accessible inventory of the selected 
features and some indication of their relative 
importance in predicting the target features in 
question. Although our exposition has focused on 
the preferred value (the mode) predicted by the 
models, decision trees built by WinMine predict a 
probability distribution over all possible target 
values. For a system such as Amalgam, built as a 
pipeline of stages, this point is critical, since 
finding the best final hypothesis requires the 
consideration of multiple hypotheses and the 
concomitant combination of probabilities assigned 
by the various models in the pipeline to all 
possible target values. For example, our 
extraposition model presented above depends 
upon the value of the verb-position feature, which 
is predicted upstream in the pipeline. Currently, 
we greedily pursue the best hypothesis, which 
includes only the mode of the verb-position 
model?s prediction. However, work in progress 
involves a search that constructs multiple 
hypotheses incorporating each of the predictions 
of the verb-position model and their scores, and 
likewise for all other models. 
We have found the combination of knowledge-
engineered linguistic operations with machine-
learned contexts to be advantageous. The 
knowledge-engineered choice of linguistic 
operations, allows us to deal with complex 
linguistic phenomena. Machine learning, on the 
other hand, automates the discovery of general 
and domain-specific contexts. This facilitates 
adaptation of the system to a new domain or even 
to a new language. 
It should also be noted that none of the learned 
models can be easily replaced by a rule. While 
case assignment, for example, depends to a high 
degree on the lexical properties of the governing 
preposition or governing verb, other factors such 
as semantic relations, etc., play a significant role, 
so that any rule approaching the accuracy of the 
model would have to be quite complex.  
We are currently adapting Amalgam to the task 
of French sentence realization, as a test of the 
linguistic generality of the system. Initial results 
are encouraging. It appears that much of the 
feature extraction and many of the linguistic 
operations are reusable. 
Acknowledgements 
Our thanks go to Max Chickering for assistance 
with the WinMine decision tree tools and to Zhu 
Zhang who made significant contributions to the 
development of the extraposition models. 
References 
S. Bangalore and O. Rambow 2000. Exploiting a 
probabilistic hierarchical model for generation. 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING 2000). 
Saarbr?cken, Germany. 42-48. 
D. M. Chickering. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/Tool
doc.htm 
D. M. Chickering, D. Heckerman and C. Meek. 1997. 
A Bayesian approach to learning Bayesian networks 
with local structure. In ?Uncertainty in Artificial 
Intelligence: Proceedings of the Thirteenth 
Conference?, D. Geiger and P. Punadlik Shenoy, 
ed., Morgan Kaufman, San Francisco, California, 
pp. 80-89. 
S. Corston-Oliver, M. Gamon, E. Ringger, and R. 
Moore. 2002. An overview of Amalgam: A machine-
learned generation module. To be presented at 
INLG 2002. 
H. Dalianis and E. Hovy 1993 Aggregation in natural 
language generation. Proceedings of the 4th 
European Workshop on Natural Language 
Generation, Pisa, Italy. 
P. Eisenberg 1999. Grundriss der deutschen 
Grammatik. Band2: Der Satz. Metzler, 
Stuttgart/Weimar. 
U. Engel. 1996. Deutsche Grammatik. Groos, 
Heidelberg. 
M. Gamon, E. Ringger, Z. Zhang, R. Moore and S. 
Corston-Oliver. 2002a. Extraposition: A case study 
in German sentence realization. To be presented at 
the 19th International Conference on Computational 
Linguistics (COLING) 2002. 
M. Gamon, E. Ringger, S. Corston-Oliver. 2002b. 
Amalgam: A machine-learned generation module. 
Microsoft Research Technical Report, to appear. 
G. E. Heidorn. 2002. Intelligent Writing Assistance. In 
?A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text?, R. Dale, H. Moisl, and H. 
Somers (ed.), Marce Dekker, New York. 
I. Langkilde. and K. Knight. 1998a. The practical value 
of n-grams in generation. Proceedings of the 9th 
International Workshop on Natural Language 
Generation, Niagara-on-the-Lake, Canada. pp. 248-
255. 
I. Langkilde and K. Knight. 1998b. Generation that 
exploits corpus-based statistical knowledge. 
Proceedings of the 36th ACL and 17th COLING 
(COLING-ACL 1998). Montr?al, Qu?bec, Canada. 
704-710. 
J. D. McCawley. 1988 The Syntactic Phenomena of 
English. The University of Chicago Press, Chicago 
and London. 
M. Reape. and C. Mellish. 1999. Just what is 
aggregation anyway? Proceedings of the 7th 
European Workshop on Natural Language 
Generation, Toulouse, France. 
E. Ringger, R. Moore, M. Gamon, and S. Corston-
Oliver. In preparation. A Linguistically Informed 
Generative Language Model for Intra-Constituent 
Ordering during Sentence Realization. 
J. Shaw. 1998 Segregatory Coordination and Ellipsis in 
Text Generation. Proceedings of COLING-ACL, 
1998, pp 1220-1226. 
H. Uszkoreit, T. Brants, D. Duchier, B. Krenn, L. 
Konieczny, S. Oepen and W. Skut. 1998. Aspekte 
der Relativsatzextraposition im Deutschen. Claus-
Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
J. Wilkinson 1995 Aggregation in Natural Language 
Generation: Another Look. Co-op work term report, 
Department of Computer Science, University of 
Waterloo. 
Pronunciation Modeling for Improved Spelling Correction
Kristina Toutanova
Computer Science Department
Stanford University
Stanford, CA 94305 USA
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
Abstract
This paper presents a method for incor-
porating word pronunciation information
in a noisy channel model for spelling cor-
rection. The proposed method builds an
explicit error model for word pronuncia-
tions. By modeling pronunciation simi-
larities between words we achieve a sub-
stantial performance improvement over
the previous best performing models for
spelling correction.
1 Introduction
Spelling errors are generally grouped into two
classes (Kuckich, 1992) ? typographic and cogni-
tive. Cognitive errors occur when the writer does
not know how to spell a word. In these cases the
misspelling often has the same pronunciation as the
correct word ( for example writing latex as latecks).
Typographic errors are mostly errors related to the
keyboard; e.g., substitution or transposition of two
letters because their keys are close on the keyboard.
Damerau (1964) found that 80% of misspelled
words that are non-word errors are the result of a sin-
gle insertion, deletion, substitution or transposition
of letters. Many of the early algorithms for spelling
correction are based on the assumption that the cor-
rect word differs from the misspelling by exactly
one of these operations (M. D. Kernigan and Gale,
1990; Church and Gale, 1991; Mayes and F. Dam-
erau, 1991).
By estimating probabilities or weights for the
different edit operations and conditioning on the
left and right context for insertions and deletions
and allowing multiple edit operations, high spelling
correction accuracy has been achieved. At ACL
2000, Brill and Moore (2000) introduced a new error
model, allowing generic string-to-string edits. This
model reduced the error rate of the best previous
model by nearly 50%. It proved advantageous to
model substitutions of up to 5-letter sequences (e.g
ent being mistyped as ant, ph as f, al as le, etc.) This
model deals with phonetic errors significantly better
than previous models since it allows a much larger
context size.
However this model makes residual errors, many
of which have to do with word pronunciation. For
example, the following are triples of misspelling,
correct word and (incorrect) guess that the Brill and
Moore model made:
edelvise edelweiss advise
bouncie bouncy bounce
latecks latex lacks
In this work we take the approach of modeling
phonetic errors explicitly by building a separate er-
ror model for phonetic errors. More specifically,
we build two different error models using the Brill
and Moore learning algorithm. One of them is a
letter-based model which is exactly the Brill and
Moore model trained on a similar dataset. The other
is a phone-sequence-to-phone-sequence error model
trained on the same data as the first model, but using
the pronunciations of the correct words and the es-
timated pronunciations of the misspellings to learn
phone-sequence-to-phone-sequence edits and esti-
mate their probabilities. At classification time, N -
best list predictions of the two models are combined
using a log linear model.
A requirement for our model is the availability of
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 144-151.
                         Proceedings of the 40th Annual Meeting of the Association for
a letter-to-phone model that can generate pronunci-
ations for misspellings. We build a letter-to-phone
model automatically from a dictionary.
The rest of the paper is structured as follows:
Section 2 describes the Brill and Moore model and
briefly describes how we use it to build our er-
ror models. Section 3 presents our letter-to-phone
model, which is the result of a series of improve-
ments on a previously proposed N-gram letter-to-
phone model (Fisher, 1999). Section 4 describes the
training and test phases of our algorithm in more de-
tail and reports on experiments comparing the new
model to the Brill and Moore model. Section 6 con-
tains conclusions and ideas for future work.
2 Brill and Moore Noisy Channel Spelling
Correction Model
Many statistical spelling correction methods can be
viewed as instances of the noisy channel model. The
misspelling of a word is viewed as the result of cor-
ruption of the intended word as it passes through a
noisy communications channel.
The task of spelling correction is a task of finding,
for a misspelling w, a correct word r 2 D, where
D is a given dictionary and r is the most probable
word to have been garbled into w. Equivalently, the
problem is to find a word r for which
P (rjw) =
P (r)P (wjr)
P (w)
is maximized. Since the denominator is constant,
this is the same as maximizing P (r)P (wjr). In the
terminology of noisy channel modeling, the distribu-
tion P (r) is referred to as the source model, and the
distribution P (wjr) is the error or channel model.
Typically, spelling correction models are not used
for identifying misspelled words, only for propos-
ing corrections for words that are not found in a
dictionary. Notice, however, that the noisy chan-
nel model offers the possibility of correcting mis-
spellings without a dictionary, as long as sufficient
data is available to estimate the source model fac-
tors. For example, if r = Osama bin Laden and
w = Ossama bin Laden, the model will predict that
the correct spelling r is more likely than the incor-
rect spelling w, provided that
P (w)
P (r)
<
P (wjr)
P (wjw)
where P (wjr)=P (wjw) would be approximately the
odds of doubling the s in Osama. We do not pursue
this, here, however.
Brill and Moore (2000) present an improved er-
ror model for noisy channel spelling correction that
goes beyond single insertions, deletions, substitu-
tions, and transpositions. The model has a set of pa-
rameters P ( ! ) for letter sequences of lengths
up to 5. An extension they presented has refined pa-
rameters P ( ! jPSN) which also depend on
the position of the substitution in the source word.
According to this model, the misspelling is gener-
ated by the correct word as follows: First, a person
picks a partition of the correct word and then types
each partition independently, possibly making some
errors. The probability for the generation of the mis-
spelling will then be the product of the substitution
probabilities for each of the parts in the partition.
For example, if a person chooses to type the word
bouncy and picks the partition boun cy, the proba-
bility that she mistypes this word as boun cie will
be P (boun ! boun)P (cie ! cy). The probability
P (wjr) is estimated as the maximum over all parti-
tions of r of the probability that w is generated from
r given that partition.
We use this method to build an error model for
letter strings and a separate error model for phone
sequences. Two models are learned; one model LTR
(standing for ?letter?) has a set of substitution prob-
abilities P ( ! ) where  and  are character
strings, and another model PH (for ?phone?) has a
set of substitution probabilities P ( ! ) where 
and  are phone sequences.
We learn these two models on the same data set
of misspellings and correct words. For LTR, we use
the training data as is and run the Brill and Moore
training algorithm over it to learn the parameters of
LTR. For PH, we convert the misspelling/correct-
word pairs into pairs of pronunciations of the mis-
spelling and the correct word, and run the Brill and
Moore training algorithm over that.
For PH, we need word pronunciations for the cor-
rect words and the misspellings. As the misspellings
are certainly not in the dictionary we need a letter-
to-phone converter that generates possible pronun-
ciations for them. The next section describes our
letter-to-phone model.
NETtalk MS Speech
Set Words Set Words
Training 14,876 Training 106,650
Test 4,964 Test 30,003
Table 1: Text-to-phone conversion data
3 Letter-to-Phone Model
There has been a lot of research on machine learn-
ing methods for letter-to-phone conversion. High
accuracy is achieved, for example, by using neural
networks (Sejnowski and Rosenberg, 1987), deci-
sion trees (Jiang et al, 1997), and N -grams (Fisher,
1999). We use a modified version of the method pro-
posed by Fisher, incorporating several extensions re-
sulting in substantial gains in performance. In this
section we first describe how we do alignment at
the phone level, then describe Fisher?s model, and fi-
nally present our extensions and the resulting letter-
to-phone conversion accuracy.
The machine learning algorithms for converting
text to phones usually start off with training data
in the form of a set of examples, consisting of let-
ters in context and their corresponding phones (clas-
sifications). Pronunciation dictionaries are the ma-
jor source of training data for these algorithms, but
they do not contain information for correspondences
between letters and phones directly; they have cor-
respondences between sequences of letters and se-
quences of phones.
A first step before running a machine learning
algorithm on a dictionary is, therefore, alignment
between individual letters and phones. The align-
ment algorithm is dependent on the phone set used.
We experimented with two dictionaries, the NETtalk
dataset and the Microsoft Speech dictionary. Statis-
tics about them and how we split them into training
and test sets are shown in Table 1. The NETtalk
dataset contains information for phone level align-
ment and we used it to test our algorithm for auto-
matic alignment. The Microsoft Speech dictionary
is not aligned at the phone level but it is much big-
ger and is the dictionary we used for learning our
final letter-to-phone model.
The NETtalk dictionary has been designed so that
each letter correspond to at most one phone, so a
word is always longer, or of the same length as, its
pronunciation. The alignment algorithm has to de-
cide which of the letters correspond to phones and
which ones correspond to nothing (i.e., are silent).
For example, the entry in NETtalk (when we remove
the empties, which contain information for phone
level alignment) for the word able is ABLE e b L.
The correct alignment is A/e B/b L/L E/?, where ? de-
notes the empty phone. In the Microsoft Speech dic-
tionary, on the other hand, each letter can naturally
correspond to 0, 1, or 2 phones. For example, the en-
try in that dictionary for able is ABLE ey b ax l. The
correct alignment is A/ey B/b L/ax&l E/?. If we also
allowed two letters as a group to correspond to two
phones as a group, the correct alignment might be
A/ey B/b LE/ax&l, but that would make it harder for
the machine learning algorithm.
Our alignment algorithm is an implementa-
tion of hard EM (Viterbi training) that starts off
with heuristically estimated initial parameters for
P (phonesjletter) and, at each iteration, finds the
most likely alignment for each word given the pa-
rameters and then re-estimates the parameters col-
lecting counts from the obtained alignments. Here
phones ranges over sequences of 0 (empty), 1,
and 2 phones for the Microsoft Speech dictionary
and 0 or 1 phones for NETtalk. The parameters
P (phonesjletter) were initialized by a method sim-
ilar to the one proposed in (Daelemans and van den
Bosch, 1996). Word frequencies were not taken into
consideration here as the dictionary contains no fre-
quency information.
3.1 Initial Letter-to-Phone Model
The method we started with was the N-gram model
of Fisher (1999). From training data, it learns rules
that predict the pronunciation of a letter based on m
letters of left and n letters of right context. The rules
are of the following form:
[Lm:T:Rn ! ph
1
p
1
ph
2
p
2
: : :]
Here Lm stands for a sequence of m letters to the
left of T and Rn is a sequence of n letters to the
right. The number of letters in the context to the left
and right varies. We used from 0 to 4 letters on each
side. For example, two rules learned for the letter B
were: [AB:B:OT !   1:0] and [B ! b :96   :04],
meaning that in the first context the letter B is silent
with probability 1:0, and in the second it is pro-
nounced as b with probability :96 and is silent with
probability :04.
Training this model consists of collecting counts
for the contexts that appear in the data with the se-
lected window size to the left and right. We col-
lected counts for all configurations Lm:T:Rn for
m 2 f0; 1; 2; 3; 4g, n 2 f0; 1; 2; 3; 4g that occurred
in the data. The model is applied by choosing for
each letter T the most probable translation as pre-
dicted by the most specific rule for the context of
occurrence of the letter. For example, if we want
to find how to pronounce the second b in abbot we
would chose the empty phone because the first rule
mentioned above is more specific than the second.
3.2 Extensions
We implemented five extensions to the initial model
which together decreased the error rate of the letter-
to-phone model by around 20%. These are :
 Combination of the predictions of several ap-
plicable rules by linear interpolation
 Rescoring of N -best proposed pronunciations
for a word using a trigram phone sequence lan-
guage model
 Explicit distinction between middle of word
versus start or end
 Rescoring of N -best proposed pronunciations
for a word using a fourgram vowel sequence
language model
The performance figures reported by Fisher
(1999) are significantly higher than our figures us-
ing the basic model, which is probably due to the
cleaner data used in their experiments and the dif-
ferences in phoneset size.
The extensions we implemented are inspired
largely by the work on letter-to-phone conversion
using decision trees (Jiang et al, 1997). The last
extension, rescoring based on vowel fourgams, has
not been proposed previously. We tested the algo-
rithms on the NETtalk and Microsoft Speech dic-
tionaries, by splitting them into training and test
sets in proportion 80%/20% training-set to test-set
size. We trained the letter-to-phone models using
the training splits and tested on the test splits. We
Model Phone Acc Word Acc
Initial 88.83% 53.28%
Interpolation
of contexts 90.55% 59.04%
Distinction
of middle 91.09% 60.81%
Phonetic
trigram 91.38% 62.95%
Vowel
fourgram 91.46% 63.63%
Table 2: Letter-to-phone accuracies
are reporting accuracy figures only on the NETtalk
dataset since this dataset has been used extensively
in building letter-to-phone models, and because
phone accuracy is hard to determine for the non-
phonetically-aligned Microsoft Speech dictionary.
For our spelling correction algorithm we use a letter-
to-phone model learned from the Microsoft Speech
dictionary, however.
The results for phone accuracy and word accuracy
of the initial model and extensions are shown in Ta-
ble 2. The phone accuracy is the percentage cor-
rect of all phones proposed (excluding the empties)
and the word accuracy is the percentage of words
for which pronunciations were guessed without any
error.
For our data we noticed that the most specific
rule that matches is often not a sufficiently good
predictor. By linearly interpolating the probabili-
ties given by the five most specific matching rules
we decreased the word error rate by 14.3%. The
weights for the individual rules in the top five were
set to be equal. It seems reasonable to combine the
predictions from several rules especially because the
choice of which rule is more specific of two is arbi-
trary when neither is a substring of the other. For
example, of the two rules with contexts A:B: and
:B:B, where the first has 0 right context and the
second has 0 left letter context, one heuristic is to
choose the latter as more specific since right context
seems more valuable than left (Fisher, 1999). How-
ever this choice may not always be the best and it
proves useful to combine predictions from several
rules. In Table 2 the row labeled ?Interpolation of
contexts? refers to this extension of the basic model.
Adding a symbol for interior of word produced a
gain in accuracy. Prior to adding this feature, we
had features for beginning and end of word. Explic-
itly modeling interior proved helpful and further de-
creased our error rate by 4.3%. The results after this
improvement are shown in the third row of Table 2.
After linearly combining the predictions from the
top matching rules we have a probability distribu-
tion over phones for each letter. It has been shown
that modeling the probability of sequences of phones
can greatly reduce the error (Jiang et al, 1997). We
learned a trigram phone sequence model and used
it to re-score the N -best predictions from the basic
model. We computed the score for a sequence of
phones given a sequence of letters, as follows:
Score(p
1
; p
2
; : : : ; p
n
jl
1
; l
2
: : : l
n
) =
log
Y
i=1:::n
P (p
i
jl
1
; l
2
: : : l
n
) +
 log
Y
i=1:::n
P (p
i
jp
i 1
; p
i 2
) (1)
Here the probabilities P (p
i
jl
1
; l
2
: : : l
n
) are the
distributions over phones that we obtain for each let-
ter from combination of the matching rules. The
weight  for the phone sequence model was esti-
mated from a held-out set by a linear search. This
model further improved our performance and the re-
sults it achieves are in the fourth row of Table 2.
The final improvement is adding a term from a
vowel fourgram language model to equation 1 with
a weight . The term is the log probability of the
sequence of vowels in the word according to a four-
gram model over vowel sequences learned from the
data. The final accuracy we achieve is shown in
the fifth row of the same table. As a comparison,
the best accuracy achieved by Jiang et al (1997)
on NETalk using a similar proportion of training
and test set sizes was 65:8%. Their system uses
more sources of information, such as phones in the
left context as features in the decision tree. They
also achieve a large performance gain by combining
multiple decision trees trained on separate portions
of the training data. The accuracy of our letter-to-
phone model is comparable to state of the art sys-
tems. Further improvements in this component may
lead to higher spelling correction accuracy.
4 Combining Pronunciation and
Letter-Based Models
Our combined error model gives the probability
P
CMB
(wjr) where w is the misspelling and r is a
word in the dictionary. The spelling correction algo-
rithm selects for a misspelling w the word r in the
dictionary for which the product P (r)P
CMB
(wjr)
is maximized. In our experiments we used a uniform
source language model over the words in the dictio-
nary. Therefore our spelling correction algorithm se-
lects the word r that maximizes P
CMB
(wjr). Brill
and Moore (2000) showed that adding a source lan-
guage model increases the accuracy significantly.
They also showed that the addition of a language
model does not obviate the need for a good error
model and that improvements in the error model lead
to significant improvements in the full noisy channel
model.
We build two separate error models, LTR and
PH (standing for ?letter? model and ?phone?
model). The letter-based model estimates a prob-
ability distribution P
LTR
(wjr) over words, and
the phone-based model estimates a distribution
P
PH
(pron wjpron r) over pronunciations. Using
the PH model and the letter-to-phone model, we de-
rive a distribution P
PHL
(wjr) in a way to be made
precise shortly. We combine the two models to esti-
mate scores as follows:
S
CMB
(wjr) =
logP
LTR
(wjr) +
 logP
PHL
(wjr)
The r that maximizes this score will also maxi-
mize the probability P
CMB
(wjr). The probabilities
P
PHL
(wjr) are computed as follows:
P
PHL
(wjr)
=
X
pron r
P (pron r;wjr)
=
X
pron r
P (pron rjr) 
P (wjpron r; r)
This equation is approximated by the expression
for P
PHL
shown in Figure 1 after several simplify-
ing assumptions. The probabilities P (pron rjr) are
PPHL
(wjr) 
X
pron r
1
num pron r
max
pron w
(
P
PH
(pron wjpron r) 
P (pron wjw)
)
Figure 1: Equation for approximation of P
PHL
taken to be equal for all possible pronunciations of r
in the dictionary. Next we assume independence of
the misspelling from the right word given the pro-
nunciation of the right word i.e. P (wjr; pron r) =
P (wjpron r). By inversion of the conditional prob-
ability this is equal to P (pron rjw) multiplied by
P (w)=P (pron r). Since we do not model these
marginal probabilities, we drop the latter factor.
Next the probability P (pron rjw) is expressed as
X
pron w
P (pron w; pron rjw)
which is approximated by the maximum term in the
sum. After the following decomposition:
P (pron w; pron rjw)
= P (pron wjw)P (pron rjw; pron w)
 P (pron wjw)P (pron rjpron w)
where the second part represents a final indepen-
dence assumption, we get the expression in Figure 1.
The probabilities P (pron wjw) are given by the
letter-to-phone model. In the following subsections,
we first describe how we train and apply the individ-
ual error models, and then we show performance re-
sults for the combined model compared to the letter-
based error model.
4.1 Training Individual Error Models
The error model LTR was trained exactly as de-
scribed originally by Brill and Moore (2000). Given
a training set of pairs fw
i
; r
i
g the algorithm es-
timates a set of rewrite probabilities p( ! )
which are the basis for computing probabilities
P
LTR
(wjr).
The parameters of the PH model
P
PH
(pron wjpron r) are obtained by training
a phone-sequence-to-phone-sequence error model
starting from the same training set of pairs fw
i
; r
i
g
of misspelling and correct word as for the LTR
model. We convert this set to a set of pronunciations
of misspellings and pronunciations of correct
words in the following way: For each training
sample fw
i
; r
i
g we generate m training samples
of corresponding pronunciations where m is the
number of pronunciations of the correct word r
i
in our dictionary. Each of those m samples is the
most probable pronunciation of w
i
according to
our letter-to-phone model paired with one of the
possible pronunciations of r
i
. Using this training
set, we run the algorithm of Brill and Moore to es-
timate a set of substitution probabilities  !  for
sequences of phones to sequences of phones. The
probability P
PH
(pron wjpron r) is then computed
as a product of the substitution probabilities in the
most probable alignment, as Brill and Moore did.
4.2 Results
We tested our system and compared it to the Brill
and Moore model on a dataset of around 10; 000
pairs of misspellings and corresponding correct
words, split into training and test sets. The ex-
act data sizes are 7; 385 word pairs in the training
set and 1; 812 word pairs in the test set. This set
is slightly different from the dataset used in Brill
and Moore?s experiments because we removed from
the original dataset the pairs for which we did not
have the correct word in the pronunciation dictio-
nary. Both models LTR and PH were trained on the
same training set. The interpolation weight that the
combined model CMB uses is also set on the train-
ing set to maximize the classification accuracy.
At test time we do not search through all possible
words r in the dictionary to find the one maximizing
Score
CMB
(wjr). Rather, we compute the combi-
nation score only for candidate words r that are in
the top N according to the P
LTR
(wjr) or are in the
top N according to P
PH
(pron wjpron r) for any
of the pronunciations of r from the dictionary and
any of the pronunciations for w that were proposed
by the letter-to-phone model. The letter-to-phone
Model 1-Best 2-Best 3-Best 4-Best
LTR 94.21% 98.18% 98.90 % 99.06%
PH 86.36% 93.65% 95.69 % 96.63%
CMB 95.58% 98.90% 99.34% 99.50%
Error
Reduction 23.8% 39.6% 40% 46.8%
Table 3: Spelling Correction Accuracy Results
model returned for each w the 3 most probable pro-
nunciations only. Our performance was better when
we considered the top 3 pronunciations of w rather
than a single most likely hypothesis. That is prob-
ably due to the fact that the 3-best accuracy of the
letter-to-phone model is significantly higher than its
1-best accuracy.
Table 3 shows the spelling correction accuracy
when using the model LTR, PH, or both in com-
bination. The table shows N -best accuracy results.
The N -best accuracy figures represent the percent
test cases for which the correct word was in the top
N words proposed by the model. We chose the con-
text size of 3 for the LTR model as this context size
maximized test set accuracy. Larger context sizes
neither helped nor hurt accuracy.
As we can see from the table, the phone-based
model alone produces respectable accuracy results
considering that it is only dealing with word pronun-
ciations. The error reduction of the combined model
compared to the letters-only model is substantial:
for 1-Best, the error reduction is over 23%; for 2-
Best, 3-Best, and 4-Best it is even higher, reaching
over 46% for 4-Best.
As an example of the influence of pronuncia-
tion modeling, in Table 4 we list some misspelling-
correct word pairs where the LTR model made
an incorrect guess and the combined model CMB
guessed accurately.
5 Conclusions and Future Work
We have presented a method for using word pro-
nunciation information to improve spelling correc-
tion accuracy. The proposed method substantially
reduces the error rate of the previous best spelling
correction model.
A subject of future research is looking for a bet-
ter way to combine the two error models or building
Misspelling Correct LTR Guess
bouncie bouncy bounce
edelvise edelweiss advise
grissel gristle grizzle
latecks latex lacks
neut newt nut
rench wrench ranch
saing saying sang
stail stale stall
Table 4: Examples of Corrected Errors
a single model that can recognize whether there is
a phonetic or typographic error. Another interest-
ing task is exploring the potential of our model in
different settings such as the Web, e-mail, or as a
specialized model for non-native English speakers
of particular origin.
References
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. In Proc.
of the 38th Annual Meeting of the ACL, pages 286?
293.
K. Church and W. Gale. 1991. Probability scoring for
spelling correction. In Statistics and Computing, vol-
ume 1, pages 93?103.
W. Daelemans and A. van den Bosch. 1996. Language-
independent data-oriented grapheme-to-phoneme con-
version. In Progress in Speech Synthesis, pages 77?90.
F. J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. In Communications
of the ACM, volume 7(3), pages 171?176.
W. M. Fisher. 1999. A statistical text-to-phone function
using ngrams and rules. In Proc. of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, pages 649?652.
L. Jiang, H.W. Hon, and X. Huang. 1997. Improvements
on a trainable letter-to-sound converter. In Proceed-
ings of the 5th European Conference on Speech Com-
munication and Technology.
K. Kuckich. 1992. Techniques for automatically correct-
ing words in text. In ACM Computing Surveys, volume
24(4), pages 377?439.
W. Church M. D. Kernigan and W. A. Gale. 1990. A
spelling correction program based on a noisy channel
model. In Proc. of COLING-90, volume II, pages 205?
211.
F. Mayes and et al F. Damerau. 1991. Conext based
spelling correction. In Information Processing and
Management, volume 27(5), pages 517?522.
T. J. Sejnowski and C. R. Rosenberg. 1987. Parallel net-
works that learn to pronounce english text. In Complex
Systems, pages 145?168.
Improving IBM Word-Alignment Model 1
Robert C. MOORE
Microsoft Research
One Microsoft Way
Redmond, WA 90052
USA
bobmoore@microsoft.com
Abstract
We investigate a number of simple methods for
improving the word-alignment accuracy of IBM
Model 1. We demonstrate reduction in alignment
error rate of approximately 30% resulting from (1)
giving extra weight to the probability of alignment
to the null word, (2) smoothing probability esti-
mates for rare words, and (3) using a simple heuris-
tic estimation method to initialize, or replace, EM
training of model parameters.
1 Introduction
IBM Model 1 (Brown et al, 1993a) is a word-
alignment model that is widely used in working
with parallel bilingual corpora. It was originally
developed to provide reasonable initial parameter
estimates for more complex word-alignment mod-
els, but it has subsequently found a host of ad-
ditional uses. Among the applications of Model
1 are segmenting long sentences into subsentental
units for improved word alignment (Nevado et al,
2003), extracting parallel sentences from compara-
ble corpora (Munteanu et al, 2004), bilingual sen-
tence alignment (Moore, 2002), aligning syntactic-
tree fragments (Ding et al, 2003), and estimating
phrase translation probabilities (Venugopal et al,
2003). Furthermore, at the 2003 Johns Hopkins
summer workshop on statistical machine transla-
tion, a large number of features were tested to dis-
cover which ones could improve a state-of-the-art
translation system, and the only feature that pro-
duced a ?truly significant improvement? was the
Model 1 score (Och et al, 2004).
Despite the fact that IBM Model 1 is so widely
used, essentially no attention seems to have been
paid to whether it is possible to improve on the stan-
dard Expectation-Maximization (EM) procedure for
estimating its parameters. This may be due in part
to the fact that Brown et al (1993a) proved that the
log-likelihood objective function for Model 1 is a
strictly concave function of the model parameters,
so that it has a unique local maximum. This, in turn,
means that EM training will converge to that max-
imum from any starting point in which none of the
initial parameter values is zero. If one equates opti-
mum parameter estimation with finding the global
maximum for the likelihood of the training data,
then this result would seem to show no improve-
ment is possible.
However, in virtually every application of statisti-
cal techniques in natural-language processing, max-
imizing the likelihood of the training data causes
overfitting, resulting in lower task performance than
some other estimates for the model parameters. This
is implicitly recognized in the widespread adoption
of early stopping in estimating the parameters of
Model 1. Brown et al (1993a) stopped after only
one iteration of EM in using Model 1 to initialize
their Model 2, and Och and Ney (2003) stop af-
ter five iterations in using Model 1 to initialize the
HMM word-alignment model. Both of these are far
short of convergence to the maximum likelihood es-
timates for the model parameters.
We have identified at least two ways in which
the standard EM training method for Model 1
leads to suboptimal performance in terms of word-
alignment accuracy. In this paper we show that by
addressing these issues, substantial improvements
in word-alignment accuracy can be achieved.
2 Definition of Model 1
Model 1 is a probabilistic generative model within
a framework that assumes a source sentence S of
length l translates as a target sentence T , according
to the following stochastic process:
? A length m for sentence T is generated.
? For each target sentence position j ?
{1, . . . ,m}:
? A generating word s
i
in S (including a
null word s
0
) is selected, and
? The target word t
j
at position j is gener-
ated depending on s
i
.
Model 1 is defined as a particularly simple in-
stance of this framework, by assuming all possible
lengths for T (less than some arbitrary upper bound)
have a uniform probability , all possible choices of
source sentence generating words are equally likely,
and the translation probability tr(t
j
|s
i
) of the gen-
erated target language word depends only on the
generating source language word?which Brown et
al. (1993a) show yields the following equation:
p(T |S) =

(l + 1)m
m
?
j=1
l
?
i=0
tr(t
j
|s
i
) (1)
Equation 1 gives the Model 1 estimate for the
probability of a target sentence, given a source sen-
tence. We may also be interested in the question of
what is the most likely alignment of a source sen-
tence and a target sentence, given an instance of
Model 1; where, by an alignment, we mean a speci-
fication of which source words generated which tar-
get words according to the generative model. Since
Model 1, like many other word-alignment models,
requires each target word to be generated by exactly
one source word (including the null word), an align-
ment a can be represented by a vector a
1
, . . . , a
m
,
where each a
j
is the sentence position of the source
word generating t
j
according to the alignment. It is
easy to show that for Model 1, the most likely align-
ment a? of S and T is given by this equation:
a? = argmax
a
m
?
j=1
tr(t
j
|s
a
j
) (2)
Since in applying Model 1, there are no depen-
dencies between any of the a
j
s, we can find the
most likely aligment simply by choosing, for each
j, the value for a
j
that leads to the highest value for
tr(t
j
|s
a
j
).
The parameters of Model 1 for a given pair of
languages are normally estimated using EM, taking
as training data a corpus of paired sentences of the
two languages, such that each pair consists of sen-
tence in one language and a possible translation in
the other language. The training is normally ini-
tialized by setting all translation probability distri-
butions to the uniform distribution over the target
language vocabulary.
3 Problems with Model 1
Model 1 clearly has many shortcomings as a model
of translation. Some of these are structural limita-
tions, and cannot be remedied without making the
model significantly more complicated. Some of the
major structural limitations include:
? (Many-to-one) Each word in the target sen-
tence can be generated by at most one word
in the source sentence. Situations in which a
phrase in the source sentence translates as a
single word in the target sentence are not well-
modeled.
? (Distortion) The position of any word in the
target sentence is independent of the position
of the corresponding word in the source sen-
tence, or the positions of any other source lan-
guage words or their translations. The ten-
dency for a contiguous phrase in one language
to be translated as a contiguous phrase in an-
other language is not modeled at all.
? (Fertility) Whether a particular source word is
selected to generate the target word for a given
position is independent of which or how many
other target words the same source word is se-
lected to generate.
These limitations of Model 1 are all well known,
they have been addressed in other word-alignment
models, and we will not discuss them further here.
Our concern in this paper is with two other problems
with Model 1 that are not deeply structural, and can
be addressed merely by changing how the parame-
ters of Model 1 are estimated.
The first of these nonstructural problems with
Model 1, as standardly trained, is that rare words
in the source language tend to act as ?garbage col-
lectors? (Brown et al, 1993b; Och and Ney, 2004),
aligning to too many words in the target language.
This problem is not unique to Model 1, but anec-
dotal examination of Model 1 alignments suggests
that it may be worse for Model 1, perhaps because
Model 1 lacks the fertility and distortion parameters
that may tend to mitigate the problem in more com-
plex models.
The cause of the problem can be easily under-
stood if we consider a situation in which the source
sentence contains a rare word that only occurs once
in our training data, plus a frequent word that has an
infrequent translation in the target sentence. Sup-
pose the frequent source word has the translation
present in the target sentence only 10% of the time
in our training data, and thus has an estimated trans-
lation probability of around 0.1 for this target word.
Since the rare source word has no other occurrences
in the data, EM training is free to assign whatever
probability distribution is required to maximize the
joint probability of this sentence pair. Even if the
rare word also needs to be used to generate its ac-
tual translation in the sentence pair, a relatively high
joint probability will be obtained by giving the rare
word a probability of 0.5 of generating its true trans-
lation and 0.5 of spuriously generating the transla-
tion of the frequent source word. The probability of
this incorrect alignment will be higher than that ob-
tained by assigning a probability of 1.0 to the rare
word generating its true translation, and generating
the true translation of the frequent source word with
a probability of 0.1. The usual fix for over-fitting
problems of this type in statistical NLP is to smooth
the probability estimates involved in some way.
The second nonstructural problem with Model 1
is that it seems to align too few target words to
the null source word. Anecdotal examination of
Model 1 alignments of English source sentences
with French target sentences reveals that null word
alignments rarely occur in the highest probability
alignment, despite the fact that French sentences
often contain function words that do not corre-
spond directly to anything in their English trans-
lation. For example, English phrases of the form
?noun
1
??noun
2
? are often expressed in French by a
phrase of the form ?noun
2
? de ?noun
1
?, which may
also be expressed in English (but less often) by a
phrase of the form ?noun
2
? of ?noun
1
?.
The structure of Model 1 again suggests why we
should not be surprised by this problem. As nor-
mally defined, Model 1 hypothesizes only one null
word per sentence. A target sentence may con-
tain many words that ideally should be aligned to
null, plus some other instances of the same word
that should be aligned to an actual source language
word. For example, we may have an English/French
sentence pair that contains two instances of of in
the English sentence, and five instances of de in the
French sentence. Even if the null word and of have
the same initial probabilty of generating de, in iter-
ating EM, this sentence is going to push the model
towards estimating a higher probabilty that of gen-
erates de and a lower estimate that the null word
generates de. This happens because there are are
two instances of of in the source sentence and only
one hypothetical null word, and Model 1 gives equal
weight to each occurrence of each source word. In
effect, of gets two votes, but the null word gets only
one. We seem to need more instances of the null
word for Model 1 to assign reasonable probabilities
to target words aligning to the null word.
4 Smoothing Translation Counts
We address the nonstructural problems of Model 1
discussed above by three methods. First, to address
the problem of rare words aligning to too many
words, at each interation of EM we smooth all the
translation probability estimates by adding virtual
counts according to a uniform probability distribu-
tion over all target words. This prevents the model
from becoming too confident about the translation
probabilities for rare source words on the basis of
very little evidence. To estimate the smoothed prob-
abilties we use the following formula:
tr(t|s) =
C(t, s) + n
C(s) + n ? |V |
(3)
where C(t, s) is the expected count of s generating
t, C(s) is the corresponding marginal count for s,
|V | is the hypothesized size of the target vocabulary
V , and n is the added count for each target word in
V . |V | and n are both free parameters in this equa-
tion. We could take |V | simply to be the total num-
ber of distinct words observed in the target language
training, but we know that the target language will
have many words that we have never observed. We
arbitrarily chose |V | to be 100,000, which is some-
what more than the total number of distinct words
in our target language training data. The value of n
is empirically optimized on annotated development
test data.
This sort of ?add-n? smoothing has a poor repu-
tation in statistical NLP, because it has repeatedly
been shown to perform badly compared to other
methods of smoothing higher-order n-gram mod-
els for statistical language modeling (e.g., Chen and
Goodman, 1996). In those studies, however, add-n
smoothing was used to smooth bigram or trigram
models. Add-n smoothing is a way of smooth-
ing with a uniform distribution, so it is not surpris-
ing that it performs poorly in language modeling
when it is compared to smoothing with higher or-
der models; e.g, smoothing trigrams with bigrams
or smoothing bigrams with unigrams. In situations
where smoothing with a uniform distribution is ap-
propriate, it is not clear that add-n is a bad way
to do it. Furthermore, we would argue that the
word translation probabilities of Model 1 are a case
where there is no clearly better alternative to a uni-
form distribution as the smoothing distribution. It
should certainly be better than smoothing with a un-
igram distribution, since we especially want to ben-
efit from smoothing the translation probabilities for
the rarest words, and smoothing with a unigram dis-
tribution would assume that rare words are more
likely to translate to frequent words than to other
rare words, which seems counterintuitive.
5 Adding Null Words to the Source
Sentence
We address the lack of sufficient alignments of tar-
get words to the null source word by adding extra
null words to each source sentence. Mathematically,
there is no reason we have to add an integral number
of null words, so in fact we let the number of null
words in a sentence be any positive number. One
can make arguments in favor of adding the same
number of null words to every sentence, or in fa-
vor of letting the number of null words be propor-
tional to the length of the sentence. We have chosen
to add a fixed number of null words to each source
sentence regardless of length, and will leave for an-
other time the question of whether this works better
or worse than adding a number of null words pro-
portional to the sentence length.
Conceptually, adding extra null words to source
sentences is a slight modification to the structure of
Model 1, but in fact, we can implement it without
any additional model parameters by the simple ex-
pedient of multiplying all the translation probabili-
ties for the null word by the number of null words
per sentence. This multiplication is performed dur-
ing every iteration of EM, as the translation proba-
bilities for the null word are re-estimated from the
corresponding expected counts. This makes these
probabilities look like they are not normalized, but
Model 1 can be applied in such a way that the trans-
lation probabilities for the null word are only ever
used when multiplied by the number of null words
in the sentence, so we are simply using the null word
translation parameters to keep track of this prod-
uct pre-computed. In training a version of Model
1 with only one null word per sentence, the param-
eters have their normal interpretation, since we are
multiplying the standard probability estimates by 1.
6 Initializing Model 1 with Heuristic
Parameter Estimates
Normally, the translation probabilities of Model 1
are initialized to a uniform distribution over the tar-
get language vocabulary to start iterating EM. The
unspoken justification for this is that EM training
of Model 1 will always converge to the same set of
parameter values from any set of initial values, so
the intial values should not matter. But this is only
the case if we want to obtain the parameter values at
convergence, and we have strong reasons to believe
that these values do not produce the most accurate
sentence alignments. Even though EM will head to-
wards those values from any initial position in the
parameter space, there may be some starting points
we can systematically find that will take us closer
to the optimal parameter values for alignment accu-
racy along the way.
To test whether a better set of initial parame-
ter estimates can improve Model 1 alignment ac-
curacy, we use a heuristic model based on the log-
likelihood-ratio (LLR) statistic recommended by
Dunning (1993). We chose this statistic because it
has previously been found to be effective for au-
tomatically constructing translation lexicons (e.g.,
Melamed, 2000; Moore, 2001). In our application,
the statistic can be defined by the following formula:
?
t??{t,?t}
?
s??{s,?s}
C(t?, s?) log p(t?|s?)
p(t?)
(4)
In this formula t and s mean that the correspond-
ing words occur in the respective target and source
sentences of an aligned sentence pair, ?t and ?s
mean that the corresponding words do not occur
in the respective sentences, t? and s? are variables
ranging over these values, and C(t?, s?) is the ob-
served joint count for the values of t? and s?. All
the probabilities in the formula refer to maximum
likelihood estimates.1
These LLR scores can range in value from 0 to
N ? log(2), where N is the number of sentence pairs
in the training data. The LLR score for a pair of
words is high if the words have either a strong pos-
itive association or a strong negative association.
Since we expect translation pairs to be positively as-
sociated, we discard any negatively associated word
pairs by requiring that p(t, s) > p(t) ? p(s).
To use LLR scores to obtain initial estimates for
the translation probabilities of Model 1, we have to
somehow transform them into numbers that range
from 0 to 1, and sum to no more than 1 for all the
target words associated with each source word. We
know that words with high LLR scores tend to be
translations, so we want high LLR scores to cor-
respond to high probabilities, and low LLR scores
to correspond to low probabilities. The simplest
approach would be to divide each LLR score by
the sum of the scores for the source word of the
pair, which would produce a normalized conditional
probability distribution for each source word.
Doing this, however, would discard one of the
major advantages of using LLR scores as a measure
of word association. All the LLR scores for rare
words tend to be small; thus we do not put too much
confidence in any of the hypothesized word associ-
ations for such words. This is exactly the property
needed to prevent rare source words from becom-
ing garbage collectors. To maintain this property,
for each source word we compute the sum of the
1This is not the form in which the LLR statistic is usually
presented, but it can easily be shown by basic algebra to be
equivalent to ?? in Dunning?s paper. See Moore (2004) for
details.
LLR scores over all target words, but we then di-
vide every LLR score by the single largest of these
sums. Thus the source word with the highest LLR
score sum receives a conditional probability distri-
bution over target words summing to 1, but the cor-
responding distribution for every other source word
sums to less than 1, reserving some probability mass
for target words not seen with that word, with more
probability mass being reserved the rarer the word.
There is no guarantee, of course, that this is the
optimal way of discounting the probabilities as-
signed to less frequent words. To allow a wider
range of possibilities, we add one more parameter
to the model by raising each LLR score to an empir-
ically optimized exponent before summing the re-
sulting scores and scaling them from 0 to 1 as de-
scribed above. Choosing an exponent less than 1.0
decreases the degree to which low scores are dis-
counted, and choosing an exponent greater than 1.0
increases degree of discounting.
We still have to define an initialization of the
translation probabilities for the null word. We can-
not make use of LLR scores because the null word
occurs in every source sentence, and any word oc-
curing in every source sentence will have an LLR
score of 0 with every target word, since p(t|s) =
p(t) in that case. We could leave the distribution
for the null word as the uniform distribution, but we
know that a high proportion of the words that should
align to the null word are frequently occuring func-
tion words. Hence we initialize the distribution for
the null word to be the unigram distribution of target
words, so that frequent function words will receive
a higher probability of aligning to the null word than
rare words, which tend to be content words that do
have a translation. Finally, we also effectively add
extra null words to every sentence in this heuristic
model, by multiplying the null word probabilities by
a constant, as described in Section 5.
7 Training and Evaluation
We trained and evaluated our various modifications
to Model 1 on data from the bilingual word align-
ment workshop held at HLT-NAACL 2003 (Mihal-
cea and Pedersen, 2003). We used a subset of the
Canadian Hansards bilingual corpus supplied for
the workshop, comprising 500,000 English-French
sentences pairs, including 37 sentence pairs desig-
nated as ?trial? data, and 447 sentence pairs desig-
nated as test data. The trial and test data had been
manually aligned at the word level, noting particular
pairs of words either as ?sure? or ?possible? align-
ments, as described by Och and Ney (2003).
To limit the number of translation probabilities
that we had to store, we first computed LLR associ-
ation scores for all bilingual word pairs with a posi-
tive association (p(t, s) > p(t)?p(s)), and discarded
from further consideration those with an LLR score
of less that 0.9, which was chosen to be just low
enough to retain all the ?sure? word alignments in
the trial data. This resulted in 13,285,942 possible
word-to-word translation pairs (plus 66,406 possi-
ble null-word-to-word pairs).
For most models, the word translation parame-
ters are set automatically by EM. We trained each
variation of each model for 20 iterations, which was
enough in almost all cases to discern a clear mini-
mum error on the 37 sentence pairs of trial data, and
we chose as the preferred iteration the one with the
lowest alignment error rate on the trial data. The
other parameters of the various versions of Model 1
described in Sections 4?6 were optimized with re-
spect to alignment error rate on the trial data using
simple hill climbing. All the results we report for
the 447 sentence pairs of test data use the parameter
values set to their optimal values for the trial data.
We report results for four principal versions of
Model 1, trained using English as the source lan-
guage and French as the target language:
? The standard model is initialized using
uniform distributions, and trained without
smoothing using EM, for a number of itera-
tions optimized on the trial data.
? The smoothed model is like the standard
model, but with optimized values of the null-
word weight and add-n parameter.
? The heuristic model simply uses the initial
heuristic estimates of the translation parameter
values, with an optimized LLR exponent and
null-word weight, but no EM re-estimation.
? The combined model initializes the translation
parameter values with the heuristic estimates,
using the LLR exponent and null-word weight
from the optimal heuristic model, and applies
EM using optimized values of the null-word
weight and add-n parameters. The null-word
weight used during EM is optimized separately
from the null-word weight used in the initial
heuristic parameter estimates.
We also performed ablation experiments in which
we ommitted each applicable modification in turn
from each principal version of Model 1, to observe
the effect on alignment error. All non-EM-trained
parameters were re-optimized on the trial data for
each version of Model 1 tested, with the exception
Model Trial Test Test Test LLR Init EM Add EM
(Ablation) AER AER Recall Precision Exp NW NW n Iter
Standard 0.311 0.298 0.810 0.646 NA NA 1.0 0.0000 17
Smoothed 0.261 0.271 0.646 0.798 NA NA 10.0 0.0100 15
(EM NW) 0.285 0.273 0.833 0.671 NA NA 1.0 0.0100 20
(Add n) 0.302 0.300 0.638 0.751 NA NA 13.0 0.0000 14
Heuristic 0.234 0.255 0.655 0.844 1.3 2.4 NA NA NA
(LLR Exp) 0.257 0.259 0.655 0.844 1.0 2.4 NA NA NA
(Init NW) 0.300 0.308 0.740 0.657 1.5 1.0 NA NA NA
Combined 0.203 0.215 0.724 0.839 1.3 2.4 7.0 0.005 1
(LLR Exp) 0.258 0.272 0.636 0.809 1.0 2.4 10.0 0.0035 3
(Init NW) 0.197 0.209 0.722 0.854 1.5 1.0 10.0 0.0005 1
(EM NW) 0.281 0.267 0.833 0.680 1.3 2.4 1.0 0.0080 8
(Add n) 0.208 0.221 0.724 0.826 1.3 2.4 8.0 0.0000 1
Table 1: Evaluation Results.
that the value of the LLR exponent and initial null-
word weight in the combined model were carried
over from the heuristic model.
8 Results
We report the performance of our different versions
of Model 1 in terms of precision, recall, and align-
ment error rate (AER) as defined by Och and Ney
(2003). These three performance statistics are de-
fined as
recall =
|A ? S|
|S|
(5)
precision =
|A ? P |
|A|
(6)
AER = 1 ?
|A ? S| + |A ? P |
|A| + |S|
(7)
where S denotes the annotated set of sure align-
ments, P denotes the annotated set of possible
alignments, and A denotes the set of alignments
produced by the model under test.2 We take AER,
which is derived from F-measure, as our primary
evaluation metric.
The results of our evaluation are presented in Ta-
ble 1. The columns of the table present (in order) a
description of the model being tested, the AER on
the trial data, the AER on the test data, test data re-
call, and test data precision, followed by the optimal
values on the trial data for the LLR exponent, the
initial (heuristic model) null-word weight, the null-
word weight used in EM re-estimation, the add-n
parameter value used in EM re-estimation, and the
number of iterations of EM. ?NA? means a parame-
ter is not applicable in a particular model.
2As is customary, alignments to the null word are not ex-
plicitly counted.
Results for the four principal versions of Model 1
are presented in bold. For each principal version, re-
sults of the corresponding ablation experiments are
presented in standard type, giving the name of each
omitted modification in parentheses.3 Probably the
most striking result is that the heuristic model sub-
stantially reduces the AER compared to the standard
or smoothed model, even without EM re-estimation.
The combined model produces an additional sub-
stantial reduction in alignment error, using a single
iteration of EM.
The ablation experiments show how important
the different modifications are to the various mod-
els. It is interesting to note that the importance of
a given modification varies from model to model.
For example, the re-estimation null-word weight
makes essentially no contribution to the smoothed
model. It can be tuned to reduce the error on the trial
data, but the improvement does not carry over to the
test data. The smoothed model with only the null-
word weight and no add-n smoothing has essen-
tially the same error as the standard model; and the
smoothed model with add-n smoothing alone has
essentially the same error as the smoothed model
with both the null-word weight and add-n smooth-
ing. On the other hand, the re-estimation null-word
weight is crucial to the combined model. With it, the
combined model has substantially lower error than
the heuristic model without re-estimation; without
it, for any number of EM iterations, the combined
model has higher error than the heuristic model.
A similar analysis shows that add-n smoothing
is much less important in the combined model than
3Modificiations are ?omitted? by setting the corresponding
parameter to a value that is equivalent to removing the modifi-
cation from the model.
the smoothed model. The probable explanation for
this is that add-n smoothing is designed to address
over-fitting from many iterations of EM. While the
smoothed model does require many EM iterations
to reach its minimum AER, the combined model,
with or without add-n smoothing, is at its minimum
AER with only one EM iteration.
Finally, we note that, while the initial null-word
weight is crucial to the heuristic model without re-
estimation, the combined model actually performs
better without it. Presumably, the re-estimation
null-word weight makes the inital null-word weight
redundant. In fact, the combined model without the
initial null word-weight has the lowest AER on both
the trial and test data of any variation tested (note
AERs in italics in Figure 1). The relative reduction
in AER for this model is 29.9% compared to the
standard model.
We tested the significance of the differences in
alignment error between each pair of our principal
versions of Model 1 by looking at the AER for each
sentence pair in the test set using a 2-tailed paired
t test. The differences between all these models
were significant at a level of 10?7 or better, except
for the difference between the standard model and
the smoothed model, which was ?significant? at the
0.61 level?that is, not at all significant. The rea-
son for this is probably the very different balance
between precision and recall with the standard and
smoothed models, which indicates that the models
make quite different sorts of errors, making statisti-
cal significance hard to establish. This conjecture is
supported by considering the smoothed model omit-
ting the re-estimation null-word weight, which has
substantially the same AER as the full smoothed
model, but with a precision/recall balance much
closer to the standard model. The 2-tailed paired
t test comparing this model to the standard model
showed significance at a level of better than 10?10.
We also compared the combined model with and
without the initial null-word weight, and found that
the improvement without the weight was significant
at the 0.008 level.
9 Conclusions
We have demonstrated that it is possible to improve
the performance of Model 1 in terms of alignment
error by about 30%, simply by changing the way its
parameters are estimated. Almost half this improve-
ment is obtained with a simple heuristic model that
does not require EM re-estimation.
It is interesting to contrast our heuristic model
with the heuristic models used by Och and Ney
(2003) as baselines in their comparative study of
alignment models. The major difference between
our model and theirs is that they base theirs on the
Dice coefficient, which is computed by the formula4
2 ? C(t, s)
C(t) + C(s)
(8)
while we use the log-likelihood-ratio statistic de-
fined in Section 6. Och and Ney find that the stan-
dard version of Model 1 produces more accurate
alignments after only one iteration of EM than ei-
ther of the heuristic models they consider, while we
find that our heuristic model outperforms the stan-
dard version of Model 1, even with an optimal num-
ber of iterations of EM.
While the Dice coefficient is simple and
intuitive?the value is 0 for words never found to-
gether, and 1 for words always found together?it
lacks the important property of the LLR statistic that
scores for rare words are discounted; thus it does not
address the over-fitting problem for rare words.
The list of applications of IBM word-alignment
Model 1 given in Section 1 should be sufficient to
convince anyone of the relevance of improving the
model. However, it is not clear that AER as defined
by Och and Ney (2003) is always the appropriate
way to evaluate the quality of the model, since the
Viterbi word alignment that AER is based on is sel-
dom used in applications of Model 1.5 Moreover, it
is notable that while the versions of Model 1 having
the lowest AER have dramatically higher precision
than the standard version, they also have quite a bit
lower recall. If AER does not reflect the optimal
balance between precision and recall for a particu-
lar application, then optimizing AER may not pro-
duce the best task-based performance for that appli-
cation. Thus the next step in this research must be
to test whether the improvements in AER we have
demonstrated for Model 1 lead to improvements on
task-based performance measures.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993a.
4Och and Ney give a different formula in their paper, in
which the addition in the denominator is replaced by a multi-
plication. According to Och (personal communication), how-
ever, this is merely a typographical error in the publication, and
the results reported are for the standard definition of the Dice
coefficient.
5A possible exception is suggested by the results of Koehn
et al (2003), which show that phrase translations extracted
from Model 1 alignments can perform almost as well in a
phrase-based statistical translation system as those extracted
from more sophisticated alignment models, provided enough
training data is used.
The mathematics of statistical machine transla-
tion: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993b.
But dictionaries are data too. In Proceedings of
the ARPA Workshop on Human Language Tech-
nology, pp. 202?205, Plainsboro, New Jersey,
USA.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of the 34th An-
nual Meeting of the Association for Computa-
tional Linguistics, pp. 310?318, Santa Cruz, Cal-
ifornia, USA.
Yuan Ding, Daniel Gildea, and Martha Palmer.
2003. An algorithm for word-level alignment of
parallel dependency trees. In Proceedings of the
Ninth Machine Translation Summit, pp. 95?101,
New Orleans, Louisiana, USA.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61?74.
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL 2003), pp. 127?133,
Edmonton, Alberta, Canada.
I. Dan Melamed. 2000. Models of Transla-
tional Equivalence. Computational Linguistics,
26(2):221?249.
Rada Mihalcea and Ted Pedersen. 2003. An eval-
uation exercise for word alignment. In Proceed-
ings of the HLT-NAACL 2003 Workshop, Building
and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pp. 1?6, Edmonton, Al-
berta, Canada.
Robert C. Moore. 2001. Towards a simple and ac-
curate statistical approach to learning translation
relationships among words. In Proceedings of
the Workshop Data-driven Machine Translation
at the 39th Annual Meeting of the Association for
Computational Linguistics, pp. 79?86, Toulouse,
France.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In S. Richard-
son (ed.), Machine Translation: From Research
to Real Users (Proceedings, 5th Conference of
the Association for Machine Translation in the
Americas, Tiburon, California), pp. 135?244,
Springer-Verlag, Heidelberg, Germany.
Robert C. Moore. 2004. On log-likelihood-ratios
and the significance of rare events. In Proceed-
ings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing, Barcelona,
Spain.
Dragos S. Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation per-
formance via parallel sentence extraction from
comparable corpora. In Proceedings of the Hu-
man Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2004),
pp. 265?272, Boston, Massachusetts, USA.
Francisco Nevado, Francisco Casacuberta, and En-
rique Vidal. 2003. Parallel corpora segmen-
tation using anchor words. In Proceedings of
the 7th International EAMT workshop on MT
and other language technology tools, Improving
MT through other language technology tools, Re-
sources and tools for building MT, pp. 33?40, Bu-
dapest, Hungary.
Franz Joseph Och and Hermann Ney. 2003.
A systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Franz Josef Och et al 2004. A smorgasbord of
features for statistical machine translation. In
Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguistics
(HLT-NAACL 2004), pp. 161?168, Boston, Mas-
sachusetts, USA.
Ashish Venugopal, Stephan Vogel, and Alex
Waibel. 2003. Effective phrase translation ex-
traction from alignment models. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics, pp. 319?326, Sap-
poro, Japan.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 513?520,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improved Discriminative Bilingual Word Alignment
Robert C. Moore Wen-tau Yih Andreas Bode
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,scottyhi,abode}@microsoft.com
Abstract
For many years, statistical machine trans-
lation relied on generative models to pro-
vide bilingual word alignments. In 2005,
several independent efforts showed that
discriminative models could be used to
enhance or replace the standard genera-
tive approach. Building on this work,
we demonstrate substantial improvement
in word-alignment accuracy, partly though
improved training methods, but predomi-
nantly through selection of more and bet-
ter features. Our best model produces the
lowest alignment error rate yet reported on
Canadian Hansards bilingual data.
1 Introduction
Until recently, almost all work in statistical ma-
chine translation was based on word alignments
obtained from combinations of generative prob-
abalistic models developed at IBM by Brown et
al. (1993), sometimes augmented by an HMM-
based model or Och and Ney?s ?Model 6? (Och
and Ney, 2003). In 2005, however, several in-
dependent efforts (Liu et al, 2005; Fraser and
Marcu, 2005; Ayan et al, 2005; Taskar et al,
2005; Moore, 2005; Ittycheriah and Roukos,
2005) demonstrated that discriminatively trained
models can equal or surpass the alignment accu-
racy of the standard models, if the usual unla-
beled bilingual training corpus is supplemented
with human-annotated word alignments for only
a small subset of the training data.
The work cited above makes use of various
training procedures and a wide variety of features.
Indeed, whereas it can be difficult to design a fac-
torization of a generative model that incorporates
all the desired information, it is relatively easy to
add arbitrary features to a discriminative model.
We take advantage of this, building on our ex-
isting framework (Moore, 2005), to substantially
reduce the alignment error rate (AER) we previ-
ously reported, given the same training and test
data. Through a careful choice of features, and
modest improvements in training procedures, we
obtain the lowest error rate yet reported for word
alignment of Canadian Hansards data.
2 Overall Approach
As in our previous work (Moore, 2005), we train
two models we call stage 1 and stage 2, both in
the form of a weighted linear combination of fea-
ture values extracted from a pair of sentences and
a proposed word alignment of them. The possible
alignment having the highest overall score is se-
lected for each sentence pair. Thus, for a sentence
pair (e, f) we seek the alignment a? such that
a? = argmaxa
n
?
i=1
?ifi(a, e, f)
where the fi are features and the ?i are weights.
The models are trained on a large number of bilin-
gual sentence pairs, a small number of which
have hand-created word alignments provided to
the training procedure. A set of hand alignments
of a different subset of the overall training corpus
is used to evaluate the models.
In the stage 1 model, all the features are based
on surface statistics of the training data, plus the
hypothesized alignment. The entire training cor-
pus is then automatically aligned using this model.
The stage 2 model uses features based not only
on the parallel sentences themselves but also on
statistics of the alignments produced by the stage
513
1 model. The stage 1 model is discussed in Sec-
tion 3, and the stage 2 model, in Section 4. After
experimenting with many features and combina-
tions of features, we made the final selection based
on minimizing training set AER.
For alignment search, we use a method nearly
identical to our previous beam search procedure,
which we do not discuss in detail. We made two
minor modifications to handle the possiblity that
more than one alignment may have the same score,
which we previously did not take into account.
First, we modified the beam search so that the
beam size dynamically expands if needed to ac-
comodate all the possible alignments that have the
same score. Second we implemented a structural
tie breaker, so that the same alignment will always
be chosen as the one-best from a set of alignments
having the same score. Neither of these changes
significantly affected the alignment results.
The principal training method is an adaptation
of averaged perceptron learning as described by
Collins (2002). The differences between our cur-
rent and earlier training methods mainly address
the observation that perceptron training is very
sensitive to the order in which data is presented to
the learner. We also investigated the large-margin
training technique described by Tsochantaridis et
al. (2004). The training procedures are described
in Sections 5 and 6.
3 Stage 1 Model
In our previous stage 1 model, we used five fea-
tures. The most informative feature was the sum
of bilingual word-association scores for all linked
word pairs, computed as a log likelihood ratio. We
used two features to measure the degree of non-
monotonicity of alignments, based on traversing
the alignment in the order of the source sentence
tokens, and noting the instances where the corre-
sponding target sentence tokens were not in left-
to-right order. One feature counted the number of
times there was a backwards jump in the order of
the target sentence tokens, and the other summed
the magnitudes of these jumps. In order to model
the trade-off between one-to-one and many-to-one
alignments, we included a feature that counted the
number of alignment links such that one of the
linked words participated in another link. Our fifth
feature was the count of the number of words in
the sentence pair left unaligned.
In addition to these five features, we employed
two hard constraints. One constraint was that the
only alignment patterns allowed were 1?1, 1?2, 1?
3, 2?1, and 3?1. Thus, many-to-many link pat-
terns were disallowed, and a single word could be
linked to at most three other words. The second
constraint was that a possible link was considered
only if it involved the strongest degree of associ-
ation within the sentence pair for at least one of
the words to be linked. If both words had stronger
associations with other words in the sentence pair,
then the link was disallowed.
Our new stage 1 model includes all the features
we used previously, plus the constraint on align-
ment patterns. The constraint involving strongest
association is not used. In addition, our new stage
1 model employs the following features:
association score rank features We define the
rank of an association with respect to a word in a
sentence pair to be the number of association types
(word-type to word-type) for that word that have
higher association scores, such that words of both
types occur in the sentence pair. The contraint on
strength of association we previously used can be
stated as a requirement that no link be considered
unless the corresponding association is of rank 0
for at least one of the words. We replace this hard
constraint with two features based on association
rank. One feature totals the sum of the associa-
tion ranks with respect to both words involved in
each link. The second feature sums the minimum
of association ranks with respect to both words in-
volved in each link. For alignments that obey the
previous hard constraint, the value of this second
feature would always be 0.
jump distance difference feature In our origi-
nal models, the only features relating to word or-
der were those measuring nonmonotonicity. The
likelihoods of various forward jump distances
were not modeled. If alignments are dense
enough, measuring nonmonotonicity gets at this
indirectly; if every word is aligned, it is impossible
to have large forward jumps without correspond-
ingly large backwards jumps, because something
has to link to the words that are jumped over. If
word alignments are sparse, however, due to free
translation, it is possible to have alignments with
very different forward jumps, but the same back-
wards jumps. To differentiate such alignments,
we introduce a feature that sums the differences
between the distance between consecutive aligned
514
source words and the distance between the closest
target words they are aligned to.
many-to-one jump distance features It seems
intuitive that the likelihood of a large forward
jump on either the source or target side of an align-
ment is much less if the jump is between words
that are both linked to the same word of the other
language. This motivates the distinction between
the d1 and d>1 parameters in IBM Models 4 and 5.
We model this by including two features. One fea-
ture sums, for each word w, the number of words
not linked to w that fall between the first and last
words linked to w. The other features counts only
such words that are linked to some word other than
w. The intuition here is that it is not so bad to have
a function word not linked to anything, between
two words linked to the same word.
exact match feature We have a feature that
sums the number of words linked to identical
words. This is motivated by the fact that proper
names or specialized terms are often the same in
both languages, and we want to take advantage of
this to link such words even when they are too rare
to have a high association score.
lexical features Taskar et al (2005) gain con-
siderable benefit by including features counting
the links between particular high frequency words.
They use 25 such features, covering all pairs of
the five most frequent non-punctuation words in
each language. We adopt this type of feature but
do so more agressively. We include features for
all bilingual word pairs that have at least two co-
occurrences in the labeled training data. In addi-
tion, we include features counting the number of
unlinked occurrences of each word having at least
two occurrences in the labeled training data.
In training our new stage 1 model, we were con-
cerned that using so many lexical features might
result in overfitting to the training data. To try to
prevent this, we train the stage 1 model by first op-
timizing the weights for all other features, then op-
timizing the weights for the lexical features, with
the other weights held fixed to their optimium val-
ues without lexical features.
4 Stage 2 Model
In our original stage 2 model, we replaced the log-
likelihood-based word association statistic with
the logarithm of the estimated conditional prob-
ability of a cluster of words being linked by the
stage 1 model, given that they co-occur in a
pair of aligned sentences, computed over the full
(500,000 sentence pairs) training data. We esti-
mated these probabilities using a discounted max-
imum likelihood estimate, in which a small fixed
amount was subtracted from each link count:
LPd(w1, . . . , wk) =
links1(w1, . . . , wk)? d
cooc(w1, . . . , wk)
LPd(w1, . . . , wk) represents the estimated condi-
tional link probability for the cluster of words
w1, . . . , wk; links1(w1, . . . , wk) is the number of
times they are linked by the stage 1 model, d is
the discount; and cooc(w1, . . . , wk) is the number
of times they co-occur. We found that d = 0.4
seemed to minimize training set AER.
An important difference between our stage 1
and stage 2 models is that the stage 1 model con-
siders each word-to-word link separately, but al-
lows multiple links per word, as long as they lead
to an alignment consisting only of one-to-one and
one-to-many links (in either direction). The stage
2 model, however, uses conditional probabilities
for both one-to-one and one-to-many clusters, but
requires all clusters to be disjoint. Our original
stage 2 model incorporated the same addtional fea-
tures as our original stage 1 model, except that the
feature that counts the number of links involved in
non-one-to-one link clusters was omitted.
Our new stage 2 model differs in a number of
ways from the original version. First we replace
the estimated conditional probability of a cluster
of words being linked with the estimated condi-
tional odds of a cluster of words being linked:
LO(w1, . . . , wk) =
links1(w1, . . . , wk) + 1
(cooc(w1, . . . , wk)? links1(w1, . . . , wk)) + 1
LO(w1, . . . , wk) represents the estimated con-
ditional link odds for the cluster of words
w1, . . . , wk. Note that we use ?add-one? smooth-
ing in place of a discount.
Additional features in our new stage 2 model in-
clude the unaligned word feature used previously,
plus the following features:
symmetrized nonmonotonicity feature We
symmetrize the previous nonmonontonicity fea-
ture that sums the magnitude of backwards jumps,
by averaging the sum of of backwards jumps in
the target sentence order relative to the source
515
sentence order, with the sum of the backwards
jumps in the source sentence order relative to the
target sentence order. We omit the feature that
counts the number of backwards jumps.
multi-link feature This feature counts the num-
ber of link clusters that are not one-to-one. This
enables us to model whether the link scores for
these clusters are more or less reliable than the link
scores for one-to-one clusters.
empirically parameterized jump distance fea-
ture We take advantage of the stage 1 alignment
to incorporate a feature measuring the jump dis-
tances between alignment links that are more so-
phisticated than simply measuring the difference
in source and target distances, as in our stage 1
model. We measure the (signed) source and target
distances between all pairs of links in the stage 1
alignment of the full training data. From this, we
estimate the odds of each possible target distance
given the corresponding source distance:
JO(dt|ds) =
C(target dist = dt ? source dist = ds) + 1
C(target dist 6= dt ? source dist = ds) + 1
We similarly estimate the odds of each possi-
ble source distance given the corresponding target
distance. The feature values consist of the sum
of the scaled log odds of the jumps between con-
secutive links in a hypothesized alignment, com-
puted in both source sentence and target sentence
order. This feature is applied only when both the
source and target jump distances are non-zero, so
that it applies only to jumps between clusters, not
to jumps on the ?many? side of a many-to-one
cluster. We found it necessary to linearly scale
these feature values in order to get good results (in
terms of training set AER) when using perceptron
training.1 We found empirically that we could get
good results in terms of training set AER by divid-
ing each log odds estimate by the largest absolute
value of any such estimate computed.
5 Perceptron Training
We optimize feature weights using a modification
of averaged perceptron learning as described by
Collins (2002). Given an initial set of feature
weight values, the algorithm iterates through the
1Note that this is purely for effective training, since after
training, one could adjust the feature weights according to the
scale factor, and use the original feature values.
labeled training data multiple times, comparing,
for each sentence pair, the best alignment ahyp ac-
cording to the current model with the reference
alignment aref . At each sentence pair, the weight
for each feature is is incremented by a multiple of
the difference between the value of the feature for
the best alignment according to the model and the
value of the feature for the reference alignment:
?i ? ?i + ?(fi(aref , e, f)? fi(ahyp, e, f))
The updated feature weights are used to compute
ahyp for the next sentence pair. The multiplier ?
is called the learning rate. In the averaged percep-
tron, the feature weights for the final model are
the average of the weight values over all the data
rather than simply the values after the final sen-
tence pair of the final iteration.
Differences between our approach and Collins?s
include averaging feature weights over each pass
through the data, rather than over all passes; ran-
domizing the order of the data for each learn-
ing pass; and performing an evaluation pass af-
ter each learning pass, with feature weights fixed
to their average values for the preceding learning
pass, during which training set AER is measured.
This procedure is iterated until a local minimum
on training set AER is found.
We initialize the weight of the anticipated most-
informative feature (word-association scores in
stage 1; conditional link probabilities or odds in
stage 2) to 1.0, with other feature weights intial-
ized to 0. The weight for the most informative fea-
ture is not updated. Allowing all weights to vary
allows many equivalent sets of weights that differ
only by a constant scale factor. Fixing one weight
eliminates a spurious apparent degree of freedom.
Previously, we set the learning rate ? differently
in training his stage 1 and stage 2 models. For the
stage 2 model, we used a single learning rate of
0.01. For the stage 1 model, we used a sequence
of learning rates: 1000, 100, 10, and 1.0. At each
transition between learning rates, we re-initialized
the feature weights to the optimum values found
with the previous learning rate.
In our current work, we make a number of mod-
ifications to this procedure. We reset the feature
weights to the best averaged values we have yet
seen at the begining of each learning pass through
the data. Anecdotally, this seems to result in faster
convergence to a local AER minimum. We also
use multiple learning rates for both the stage 1 and
516
stage 2 models, setting the learning rates automat-
ically. The initial learning rate is the maximum ab-
solute value (for one word pair/cluster) of the word
association, link probability, or link odds feature,
divided by the number of labeled training sentence
pairs. Since many of the feature values are simple
counts, this allows a minimal difference of 1 in
the feature value, if repeated in every training ex-
ample, to permit a count feature to have as large
a weighted value as the most informative feature,
after a single pass through the data.
After the learning search terminates for a given
learning rate, we reduce the learning rate by a fac-
tor of 10, and iterate until we judge that we are at
a local minimum for this learning rate. We con-
tinue with progressively smaller learning rates un-
til an entire pass through the data produces fea-
ture weights that differ so little from their values
at the beginning of the pass that the training set
AER does not change.
Two final modifications are inspired by the real-
ization that the results of perceptron training are
very sensitive to the order in which the data is
presented. Since we randomize the order of the
data on every pass, if we make a pass through the
training data, and the training set AER increases, it
may be that we simply encountered an unfortunate
ordering of the data. Therefore, when training set
AER increases, we retry two additional times with
the same initial weights, but different random or-
derings of the data, before giving up and trying a
smaller learning rate. Finally, we repeat the entire
training process multiple times, and average the
feature weights resulting from each of these runs.
We currently use 10 runs of each model. This final
averaging is inspired by the idea of ?Bayes-point
machines? (Herbrich and Graepel, 2001).
6 SVM Training
After extensive experiments with perceptron train-
ing, we wanted to see if we could improve the re-
sults obtained with our best stage 2 model by using
a more sophisticated training method. Perceptron
training has been shown to obtain good results for
some problems, but occasionally very poor results
are reported, notably by Taskar et al (2005) for the
word-alignment problem. We adopted the support
vector machine (SVM) method for structured out-
put spaces of Tsochantaridis et al (2005), using
Joachims? SV M struct package.
Like standard SVM learning, this method tries
to find the hyperplane that separates the training
examples with the largest margin. Despite a very
large number of possible output labels (e.g., all
possible alignments of a given pair of sentences),
the optimal hyperplane can be efficiently approx-
imated given the desired error rate, using a cut-
ting plane algorithm. In each iteration of the al-
gorithm, it adds the ?best? incorrect predictions
given the current model as constraints, and opti-
mizes the weight vector subject only to them.
The main advantage of this algorithm is that
it does not pose special restrictions on the out-
put structure, as long as ?decoding? can be done
efficiently. This is crucial to us because sev-
eral features we found very effective in this task
are difficult to incorporate into structured learning
methods that require decomposable features. This
method also allows a variety of loss functions, but
we use only simple 0-1 loss, which in our case
means whether or not the alignment of a sentence
pair is completely correct, since this worked as
well as anything else we tried.
Our SVM method has a number of free param-
eters, which we tried tuning in two different ways.
One way is minimizing training set AER, which
is how we chose the stopping points in perceptron
training. The other is five-fold cross validation.
In this method, we train five times on 80% of the
training data and test on the other 20%, with five
disjoint subsets used for testing. The parameter
values yielding the best averaged AER on the five
test subsets of the training set are used to train the
final model on the entire training set.
7 Evaluation
We used the same training and test data as in our
previous work, a subset of the Canadian Hansards
bilingual corpus supplied for the bilingual word
alignment workshop held at HLT-NAACL 2003
(Mihalcea and Pedersen, 2003). This subset com-
prised 500,000 English-French sentences pairs, in-
cluding 224 manually word-aligned sentence pairs
for labeled training data, and 223 labeled sen-
tences pairs as test data. Automatic sentence
alignment of the training data was provided by Ul-
rich Germann, and the hand alignments of the la-
beled data were created by Franz Och and Her-
mann Ney (Och and Ney, 2003).
For baselines, Table 1 shows the test set re-
sults we previously reported, along with results for
IBM Model 4, trained with Och?s Giza++ software
517
Alignment Recall Precision AER
Prev LLR 0.829 0.848 0.160
CLP1 0.889 0.934 0.086
CLP2 0.898 0.947 0.075
Giza E? F 0.870 0.890 0.118
Giza F? E 0.876 0.907 0.106
Giza union 0.929 0.845 0.124
Giza intersection 0.817 0.981 0.097
Giza refined 0.908 0.929 0.079
Table 1: Baseline Results.
package, using the default configuration file (Och
and Ney, 2003).2 ?Prev LLR? is our earlier stage
1 model, and CLP1 and CLP2 are two versions
of our earlier stage 2 model. For CLP1, condi-
tional link probabilities were estimated from the
alignments produced by our ?Prev LLR? model,
and for CLP2, they were obtained from a yet
earlier, heuristic alignment model. Results for
IBM Model 4 are reported for models trained in
both directions, English-to-French and French-to-
English, and for the union, intersection, and what
Och and Ney (2003) call the ?refined? combina-
tion of the those two alignments.
Results for our new stage 1 model are presented
in Table 2. The first line is for the model described
in Section 3, optimizing non-lexical features be-
fore lexical features. The second line gives results
for optimizing all features simultaneously. The
next line omits lexical features entirely. The last
line is for our original stage 1 model, but trained
using our improved perceptron training method.
As we can see, our best stage 1 model reduces
the error rate of previous stage 1 model by almost
half. Comparing the first two lines shows that two-
phase training of non-lexical and lexical features
produces a 0.7% reduction in test set error. Al-
though the purpose of the two-phase training was
to mitigate overfitting to the training data, we also
found training set AER was reduced (7.3% vs.
8.8%). Taken all together, the results show a 7.9%
total reduction in error rate: 4.0% from new non-
lexical features, 3.3% from lexical features with
two-phase training, and 0.6% from other improve-
ments in perceptron training.
Table 3 presents results for perceptron training
of our new stage 2 model. The first line is for the
model as described in Section 4. Since the use of
log odds is somewhat unusual, in the second line
2Thanks to Chris Quirk for providing Giza++ alignments.
Alignment Recall Precision AER
Two-phase train 0.907 0.928 0.081
One-phase train 0.911 0.912 0.088
No lex feats 0.889 0.885 0.114
Prev LLR (new train) 0.834 0.855 0.154
Table 2: Stage 1 Model Results.
Alignment Recall Precision AER
Log odds 0.935 0.964 0.049
Log probs 0.934 0.962 0.051
CLP1 (new A & T) 0.925 0.952 0.060
CLP1 (new A) 0.917 0.955 0.063
Table 3: Stage 2 Model Results.
we show results for a similiar model, but using log
probabilities instead of log odds for both the link
model and the jump model. This result is 0.2%
worse than the log-odds-based model, but the dif-
ference is small enough to warrant testing its sig-
nificance. Comparing the errors on each test sen-
tence pair with a 2-tailed paired t test, the results
were suggestive, but not significant (p = 0.28)
The third line of Table 3 shows results for our
earlier CLP1 model with probabilities estimated
from our new stage 1 model alignments (?new
A?), using our recent modifications to perceptron
training (?new T?). These results are significantly
worse than either of the two preceding models
(p < 0.0008). The fourth line is for the same
model and stage 1 alignments, but with our earlier
perceptron training method. While the results are
0.3% worse than with our new training method,
the difference is not significant (p = 0.62).
Table 4 shows the results of SVM training of
the model that was best under perceptron training,
tuning free parameters either by minimizing error
on the entire training set or by 5-fold cross val-
idation on the training set. The cross-validation
method produced slightly lower test-set AER, but
both results rounded to 4.7%. While these results
are somewhat better than with perceptron training,
the differences are not significant (p ? 0.47).
8 Comparisons to Other Work
At the time we carried out the experiments de-
scribed above, our sub-5% AER results were the
best we were aware of for word alignment of
Canadian Hansards bilingual data, although direct
comparisons are problematic due to differences in
518
Alignment Recall Precision AER
Min train err 0.941 0.962 0.047
5 ? CV 0.942 0.962 0.047
Table 4: SVM Training Results.
total training data, labeled training data, and test
data. The best previously reported result was by
Och and Ney (2003), who obtained 5.2% AER
for a combination including all the IBM mod-
els except Model 2, plus the HMM model and
their Model 6, together with a bilingual dictionary,
for the refined alignment combination, trained on
three times as much data as we used.
Cherry and Lin?s (2003) method obtained an
AER of 5.7% as reported by Mihalcea and Peder-
sen (2003), the previous lowest reported error rate
for a method that makes no use of the IBM mod-
els. Cherry and Lin?s method is similar to ours
in using explicit estimates of the probability of a
link given the co-occurence of the linked words;
but it is generative rather than discriminative, it re-
quires a parser for the English side of the corpus,
and it does not model many-to-one links. Taskar
et al (2005) reported 5.4% AER for a discrimina-
tive model that includes predictions from the inter-
section of IBM Model 4 alignments as a feature.
Their best result without using information from
the IBM models was 10.7% AER.
After completing the experiments described in
Section 7, we became aware further developments
in the line of research reported by Taskar et al
(Lacoste-Julien et al, 2006). By modifying their
previous approach to allow many-to-one align-
ments and first-order interactions between align-
ments, Lacoste-Julien et al have improved their
best AER without using information from the
more complex IBM models to 6.2%. Their best
result, however, is obtained from a model that in-
cludes both a feature recording intersected IBM
Model 4 predictions, plus a feature whose val-
ues are the alignment probabilities obtained from a
pair of HMM alignment models trained in both di-
rections in such a way that they agree on the align-
ment probabilities (Liang et al, 2006). With this
model, they obtained a much lower 3.8% AER.
Lacoste-Julien very graciously provided both
the IBM Model 4 predictions and the probabili-
ties estimated by the bidirectional HMM models
that they had used to compute these additional fea-
ture values. We then added features based on this
information to see how much we could improve
our best model. We also eliminated one other dif-
ference between our results and those of Lacoste-
Julien et al, by training on all 1.1 million English-
French sentence pairs from the 2003 word align-
ment workshop, rather than the 500,000 sentence
pairs we had been using.
Since all our other feature values derived from
probabilities are expressed as log odds, we also
converted the HMM probabilities estimated by
Liang et al to log odds. To make this well de-
fined in all cases, we thresholded high probabili-
ties (including 1.0) at 0.999999, and low probabil-
ities (including 0.0) at 0.1 (which we found pro-
duced lower training set error than using a very
small non-zero probability, although we have not
searched systematically for the optimal value).
In our latest experiments, we first established
that simply increasing the unlabled training data
to 1.1 million sentence pairs made very little dif-
ference, reducing the test-set AER of our stage 2
model under perceptron training only from 4.9%
to 4.8%. Combining our stage 2 model features
with the HMM log odds feature using SVM train-
ing with 5-fold cross validation yielded a substan-
tial reduction in test-set AER to 3.9% (96.9% pre-
cision, 95.1% recall). We found it somewhat dif-
ficult to improve these results further by including
IBM Model 4 intersection feature. We finally ob-
tained our best results, however, for both training-
set and test-set AER, by holding the stage 2 model
feature weights at the values obtained by SVM
training with the HMM log odds feature, and op-
timizing the HMM log odds feature weight and
IBM Model 4 intersection feature weight with per-
ceptron training.3 This produced a test-set AER of
3.7% (96.9% precision, 95.5% recall).
9 Conclusions
For Canadian Hansards data, the test-set AER of
4.7% for our stage 2 model is one of the lowest
yet reported for an aligner that makes no use of
the expensive IBM models, and our test-set AER
of 3.7% for the stage 2 model in combination with
the HMM log odds and Model 4 intersection fea-
tures is the lowest yet reported for any aligner.4
Perhaps if any general conclusion is to be drawn
from our results, it is that in creating a discrim-
3At this writing we have not yet had time to try this with
SVM training.
4However, the difference between our result and the 3.8%
of Lacoste-Julien et al is almost certainly not significant.
519
inative word alignment model, the model struc-
ture and features matter the most, with the dis-
criminative training method of secondary impor-
tance. While we obtained a small improvements
by varying the training method, few of the differ-
ences were statistically significant. Having better
features was much more important.
References
Necip Fazil Ayan, Bonnie J. Dorr, and
Christof Monz. 2005. NeurAlign: Combining
Word Alignments Using Neural Networks. In
Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical
Methods in Natural Language Processing,
pp. 65?72, Vancouver, British Columbia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A Proba-
bility Model to Improve Word Alignment. In
Proceedings of the 41st Annual Meeting of the
ACL, pp. 88?95, Sapporo, Japan.
Michael Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory
and Experiments with Perceptron Algorithms.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pp. 1?8, Philadelphia, Pennsylvania.
Alexander Fraser and Daniel Marcu. 2005. ISI?s
Participation in the Romanian-English Align-
ment Task. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts,
pp. 91?94, Ann Arbor, Michigan.
Ralf Herbrich and Thore Graepel. 2001. Large
Scale Bayes Point Machines Advances. In
Neural Information Processing Systems 13,
pp. 528?534.
Abraham Ittycheriah and Salim Roukos. 2005. A
Maximum Entropy Word Aligner for Arabic-
English Machine Translation. In Proceedings
of the Human Language Technology Conference
and Conference on Empirical Methods in Nat-
ural Language Processing, pp. 89?96, Vancou-
ver, British Columbia.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2006. Word Alignment via
Quadratic Assignment. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, pp. 112?119, New
York City.
Percy Liang, Ben Taskar, and Dan Klein. 2006.
Alignment by Agreement. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, pp. 104?111, New
York City.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear Models for Word Alignment. In Proceed-
ings of the 43rd Annual Meeting of the ACL,
pp. 459?466, Ann Arbor, Michigan.
Rada Mihalcea and Ted Pedersen. 2003. An Eval-
uation Exercise for Word Alignment. In Pro-
ceedings of the HLT-NAACL 2003 Workshop,
Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond, pp. 1?6, Ed-
monton, Alberta.
Robert C. Moore. 2005. A Discriminative Frame-
work for Bilingual Word Alignment. In Pro-
ceedings of the Human Language Technology
Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pp. 81?
88, Vancouver, British Columbia.
Franz Joseph Och and Hermann Ney. 2003. A
Systematic Comparison of Various Statistical
Alignment Models. Computational Linguistics,
29(1):19?51.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A Discriminative Matching Approach
to Word Alignment. In Proceedings of the
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pp. 73?80, Vancouver,
British Columbia.
Ioannis Tsochantaridis, Thomas Hofmann,
Thorsten Joachims, and Yasemin Altun. 2005.
Large Margin Methods for Structured and
Interdependent Output Variables. Journal
of Machine Learning Research (JMLR),
pp. 1453?1484.
520
Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Bayesian Learning of Non-compositional Phrases with Synchronous Parsing
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
chrisq@microsoft.com
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
bobmoore@microsoft.com
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Abstract
We combine the strengths of Bayesian mod-
eling and synchronous grammar in unsu-
pervised learning of basic translation phrase
pairs. The structured space of a synchronous
grammar is a natural fit for phrase pair proba-
bility estimation, though the search space can
be prohibitively large. Therefore we explore
efficient algorithms for pruning this space that
lead to empirically effective results. Incorpo-
rating a sparse prior using Variational Bayes,
biases the models toward generalizable, parsi-
monious parameter sets, leading to significant
improvements in word alignment. This pref-
erence for sparse solutions together with ef-
fective pruning methods forms a phrase align-
ment regimen that produces better end-to-end
translations than standard word alignment ap-
proaches.
1 Introduction
Most state-of-the-art statistical machine transla-
tion systems are based on large phrase tables ex-
tracted from parallel text using word-level align-
ments. These word-level alignments are most of-
ten obtained using Expectation Maximization on the
conditional generative models of Brown et al (1993)
and Vogel et al (1996). As these word-level align-
ment models restrict the word alignment complex-
ity by requiring each target word to align to zero
or one source words, results are improved by align-
ing both source-to-target as well as target-to-source,
then heuristically combining these alignments. Fi-
nally, the set of phrases consistent with the word
alignments are extracted from every sentence pair;
these form the basis of the decoding process. While
this approach has been very successful, poor word-
level alignments are nonetheless a common source
of error in machine translation systems.
A natural solution to several of these issues is
unite the word-level and phrase-level models into
one learning procedure. Ideally, such a procedure
would remedy the deficiencies of word-level align-
ment models, including the strong restrictions on
the form of the alignment, and the strong inde-
pendence assumption between words. Furthermore
it would obviate the need for heuristic combina-
tion of word alignments. A unified procedure may
also improve the identification of non-compositional
phrasal translations, and the attachment decisions
for unaligned words.
In this direction, Expectation Maximization at
the phrase level was proposed by Marcu and Wong
(2002), who, however, experienced two major dif-
ficulties: computational complexity and controlling
overfitting. Computational complexity arises from
the exponentially large number of decompositions
of a sentence pair into phrase pairs; overfitting is a
problem because as EM attempts to maximize the
likelihood of its training data, it prefers to directly
explain a sentence pair with a single phrase pair.
In this paper, we attempt to address these two is-
sues in order to apply EM above the word level.
97
We attack computational complexity by adopting
the polynomial-time Inversion Transduction Gram-
mar framework, and by only learning small non-
compositional phrases. We address the tendency of
EM to overfit by using Bayesian methods, where
sparse priors assign greater mass to parameter vec-
tors with fewer non-zero values therefore favoring
shorter, more frequent phrases. We test our model
by extracting longer phrases from our model?s align-
ments using traditional phrase extraction, and find
that a phrase table based on our system improves MT
results over a phrase table extracted from traditional
word-level alignments.
2 Phrasal Inversion Transduction
Grammar
We use a phrasal extension of Inversion Transduc-
tion Grammar (Wu, 1997) as the generative frame-
work. Our ITG has two nonterminals: X and
C, where X represents compositional phrase pairs
that can have recursive structures and C is the pre-
terminal over terminal phrase pairs. There are three
rules with X on the left-hand side:
X ? [X X],
X ? ?X X?,
X ? C.
The first two rules are the straight rule and in-
verted rule respectively. They split the left-hand side
constituent which represents a phrase pair into two
smaller phrase pairs on the right-hand side and order
them according to one of the two possible permuta-
tions. The rewriting process continues until the third
rule is invoked. C is our unique pre-terminal for
generating terminal multi-word pairs:
C ? e/f .
We parameterize our probabilistic model in the
manner of a PCFG: we associate a multinomial dis-
tribution with each nonterminal, where each out-
come in this distribution corresponds to an expan-
sion of that nonterminal. Specifically, we place one
multinomial distribution ?X over the three expan-
sions of the nonterminalX , and another multinomial
distribution ?C over the expansions of C. Thus, the
parameters in our model can be listed as
?X = (P??, P[], PC),
where P?? is for the inverted rule, P[] for the straight
rule, PC for the third rule, satisfyingP??+P[]+PC =
1, and
?C = (P (e/f), P (e?/f ?), . . . ),
where
?
e/f P (e/f) = 1 is a multinomial distribu-
tion over phrase pairs.
This is our model in a nutshell. We can train
this model using a two-dimensional extension of the
inside-outside algorithm on bilingual data, assuming
every phrase pair that can appear as a leaf in a parse
tree of the grammar a valid candidate. However, it is
easy to show that the maximum likelihood training
will lead to the saturated solution where PC = 1 ?
each sentence pair is generated by a single phrase
spanning the whole sentence. From the computa-
tional point of view, the full EM algorithm runs in
O(n6) where n is the average length of the two in-
put sentences, which is too slow in practice.
The key is to control the number of parameters,
and therefore the size of the set of candidate phrases.
We deal with this problem in two directions. First
we change the objective function by incorporating
a prior over the phrasal parameters. This has the
effect of preferring parameter vectors in ?C with
fewer non-zero values. Our second approach was
to constrain the search space using simpler align-
ment models, which has the further benefit of signif-
icantly speeding up training. First we train a lower
level word alignment model, then we place hard con-
straints on the phrasal alignment space using confi-
dent word links from this simpler model. Combining
the two approaches, we have a staged training pro-
cedure going from the simplest unconstrained word
based model to a constrained Bayesian word-level
ITG model, and finally proceeding to a constrained
Bayesian phrasal model.
3 Variational Bayes for ITG
Goldwater and Griffiths (2007) and Johnson (2007)
show that modifying an HMM to include a sparse
prior over its parameters and using Bayesian esti-
mation leads to improved accuracy for unsupervised
part-of-speech tagging. In this section, we describe
a Bayesian estimator for ITG: we select parame-
ters that optimize the probability of the data given
a prior. The traditional estimation method for word
98
alignment models is the EM algorithm (Brown et
al., 1993) which iteratively updates parameters to
maximize the likelihood of the data. The drawback
of maximum likelihood is obvious for phrase-based
models. If we do not put any constraint on the dis-
tribution of phrases, EM overfits the data by mem-
orizing every sentence pair. A sparse prior over a
multinomial distribution such as the distribution of
phrase pairs may bias the estimator toward skewed
distributions that generalize better. In the context of
phrasal models, this means learning the more repre-
sentative phrases in the space of all possible phrases.
The Dirichlet distribution, which is parameter-
ized by a vector of real values often interpreted as
pseudo-counts, is a natural choice for the prior, for
two main reasons. First, the Dirichlet is conjugate
to the multinomial distribution, meaning that if we
select a Dirichlet prior and a multinomial likelihood
function, the posterior distribution will again be a
Dirichlet. This makes parameter estimation quite
simple. Second, Dirichlet distributions with small,
non-zero parameters place more probability mass on
multinomials on the edges or faces of the probabil-
ity simplex, distributions with fewer non-zero pa-
rameters. Starting from the model from Section 2,
we propose the following Bayesian extension, where
A ? Dir(B) means the random variable A is dis-
tributed according to a Dirichlet with parameter B:
?X | ?X ? Dir(?X),
?C | ?C ? Dir(?C),
[X X]
?X X?
C
X ? Multi(?X),
e/f | C ? Multi(?C).
The parameters ?X and ?C control the sparsity of
the two distributions in our model. One is the distri-
bution of the three possible branching choices. The
other is the distribution of the phrase pairs. ?C is
crucial, since the multinomial it is controlling has a
high dimension. By adjusting ?C to a very small
number, we hope to place more posterior mass on
parsimonious solutions with fewer but more confi-
dent and general phrase pairs.
Having defined the Bayesian model, it remains
to decide the inference procedure. We chose Vari-
ational Bayes, for its procedural similarity to EM
and ease of implementation. Another potential op-
tion would be Gibbs sampling (or some other sam-
pling technique). However, in experiments in un-
supervised POS tag learning using HMM structured
models, Johnson (2007) shows that VB is more ef-
fective than Gibbs sampling in approaching distribu-
tions that agree with the Zipf?s law, which is promi-
nent in natural languages.
Kurihara and Sato (2006) describe VB for PCFGs,
showing the only need is to change the M step of
the EM algorithm. As in the case of maximum like-
lihood estimation, Bayesian estimation for ITGs is
very similar to PCFGs, which follows due to the
strong isomorphism between the two models. Spe-
cific to our ITG case, the M step becomes:
P? (l+1)[] =
exp(?(E(X ? [X X]) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)?? =
exp(?(E(X ? ?X X?) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)C =
exp(?(E(X ? C) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)(e/f) = exp(?(E(e/f) + ?C))exp(?(E(C) +m?C))
,
where ? is the digamma function (Beal, 2003), s =
3 is the number of right-hand-sides for X , and m is
the number of observed phrase pairs in the data. The
sole difference between EM and VB with a sparse
prior ? is that the raw fractional counts c are re-
placed by exp(?(c + ?)), an operation that resem-
bles smoothing. As pointed out by Johnson (2007),
in effect this expression adds to c a small value that
asymptotically approaches ? ? 0.5 as c approaches
?, and 0 as c approaches 0. For small values of
? the net effect is the opposite of typical smooth-
ing, since it tends to redistribute probably mass away
from unlikely events onto more likely ones.
4 Bitext Pruning Strategy
ITG is slow mainly because it considers every pair of
spans in two sentences as a possible chart element.
In reality, the set of useful chart elements is much
99
smaller than the possible scriptO(n4), where n is
the average sentence length. Pruning the span pairs
(bitext cells) that can participate in a tree (either as
terminals or non-terminals) serves to not only speed
up ITG parsing, but also to provide a kind of ini-
tialization hint to the training procedures, encourag-
ing it to focus on promising regions of the alignment
space.
Given a bitext cell defined by the four boundary
indices (i, j, l,m) as shown in Figure 1a, we prune
based on a figure of merit V (i, j, l,m) approximat-
ing the utility of that cell in a full ITG parse. The
figure of merit considers the Model 1 scores of not
only the words inside a given cell, but also all the
words not included in the source and target spans, as
in Moore (2003) and Vogel (2005). Like Zhang and
Gildea (2005), it is used to prune bitext cells rather
than score phrases. The total score is the product of
the Model 1 probabilities for each column; ?inside?
columns in the range [l,m] are scored according to
the sum (or maximum) of Model 1 probabilities for
[i, j], and ?outside? columns use the sum (or maxi-
mum) of all probabilities not in the range [i, j].
Our pruning differs from Zhang and Gildea
(2005) in two major ways. First, we perform prun-
ing using both directions of the IBM Model 1 scores;
instead of a single figure of merit V , we have two:
VF and VB . Only those spans that pass the prun-
ing threshold in both directions are kept. Second,
we allow whole spans to be pruned. The figure of
merit for a span is VF (i, j) = maxl,m VF (i, j, l,m).
Only spans that are within some threshold of the un-
restricted Model 1 scores VF and VB are kept:
VF (i, j)
VF
? ?s and
VB(l,m)
VB
? ?s.
Amongst those spans retained by this first threshold,
we keep only those bitext cells satisfying both
VF (i, j, l,m)
VF (i, j)
? ?b and
VB(i, j, l,m)
VB(l,m)
? ?b.
4.1 Fast Tic-tac-toe Pruning
The tic-tac-toe pruning algorithm (Zhang and
Gildea, 2005) uses dynamic programming to com-
pute the product of inside and outside scores for
all cells in O(n4) time. However, even this can be
slow for large values of n. Therefore we describe an
Figure 1: (a) shows the original tic-tac-toe score for a
bitext cell (i, j, l,m). (b) demonstrates the finite state
representation using the machine in (c), assuming a fixed
source span (i, j).
improved algorithm with best case n3 performance.
Although the worst case performance is also O(n4),
in practice it is significantly faster.
To begin, let us restrict our attention to the for-
ward direction for a fixed source span (i, j). Prun-
ing bitext spans and cells requires VF (i, j), the score
of the best bitext cell within a given span, as well
as all cells within a given threshold of that best
score. For a fixed i and j, we need to search over
the starting and ending points l and m of the in-
side region. Note that there is an isomorphism be-
tween the set of spans and a simple finite state ma-
chine: any span (l,m) can be represented by a se-
quence of l OUTSIDE columns, followed bym?l+1
INSIDE columns, followed by n ? m + 1 OUT-
SIDE columns. This simple machine has the re-
stricted form described in Figure 1c: it has three
states, L, M , and R; each transition generates ei-
ther an OUTSIDE column O or an INSIDE column
I . The cost of generating an OUTSIDE at posi-
tion a is O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb);
likewise the cost of generating an INSIDE column
is I(a) = P (ta|NULL) +
?
b?[i,j] P (ta|sb), with
100
O(0) = O(n+ 1) = 1 and I(0) = I(n+ 1) = 0.
Directly computing O and I would take time
O(n2) for each source span, leading to an overall
runtime of O(n4). Luckily there are faster ways to
find the inside and outside scores. First we can pre-
compute following arrays in O(n2) time and space:
pre[0, l] := P (tl|NULL)
pre[i, l] := pre[i? 1, l] + P (tl|si)
suf[n+ 1, l] := 0
suf[i, l] := suf[i+ 1, l] + P (tl|si)
Then for any (i, j), O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb) = pre[i ? 1, a] + suf[j + 1, a].
I(a) can be incrementally updated as the source
span varies: when i = j, I(a) = P (ta|NULL) +
P (ta|si). As j is incremented, we add P (ta|sj) to
I(a). Thus we have linear time updates for O and I .
We can then find the best scoring sequence using
the familiar Viterbi algorithm. Let ?[a, ?] be the cost
of the best scoring sequence ending at in state ? at
time a:
?[0, ?] := 1 if ? = L; 0 otherwise
?[a, L] := ?[a? 1, L] ?O(a)
?[a,M ] := max
??L,M
{?[a? 1, ?]} ? I(a)
?[a,R] := max
??M,R
{?[a? 1, ?]} ?O(a)
Then VF (i, j) = ?[n + 1, R], using the isomor-
phism between state sequences and spans. This lin-
ear time algorithm allows us to compute span prun-
ing in O(n3) time. The same algorithm may be
performed using the backward figure of merit after
transposing rows and columns.
Having cast the problem in terms of finite state au-
tomata, we can use finite state algorithms for prun-
ing. For instance, fixing a source span we can enu-
merate the target spans in decreasing order by score
(Soong and Huang, 1991), stopping once we en-
counter the first span below threshold. In practice
the overhead of maintaining the priority queue out-
weighs any benefit, as seen in Figure 2.
An alternate approach that avoids this overhead is
to enumerate spans by position. Note that ?[m,R] ?
?n
a=m+1O(a) is within threshold iff there is a
span with right boundary m? < m within thresh-
old. Furthermore if ?[m,M ] ? ?na=m+1O(a) is
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 10  20  30  40  50
Pr
un
in
g 
tim
e 
(th
ou
san
ds
 of
 se
co
nd
s)
Average sentence length
Baseline
k-best
Fast
Figure 2: Speed comparison of the O(n4) tic-tac-toe
pruning algorithm, the A* top-x algorithm, and the fast
tic-tac-toe pruning. All produce the same set of bitext
cells, those within threshold of the best bitext cell.
within threshold, thenm is the right boundary within
threshold. Using these facts, we can gradually
sweep the right boundary m from n toward 1 until
the first condition fails to hold. For each value where
the second condition holds, we pause to search for
the set of left boundaries within threshold.
Likewise for the left edge, ?[l,M ] ??ma=l+1 I(a) ?
?n
a=m+1O(a) is within threshold iff there is some
l? < l identifying a span (l?,m) within threshold.
Finally if V (i, j, l,m) = ?[l ? 1, L] ? ?ma=l I(a) ?
?n
a=m+1O(a) is within threshold, then (i, j, l,m)
is a bitext cell within threshold. For right edges that
are known to be within threshold, we can sweep the
left edges leftward until the first condition no longer
holds, keeping only those spans for which the sec-
ond condition holds.
The filtering algorithm behaves extremely well.
Although the worst case runtime is still O(n4), the
best case has improved to n3; empirically it seems to
significantly reduce the amount of time spent explor-
ing spans. Figure 2 compares the speed of the fast
tic-tac-toe algorithm against the algorithm in Zhang
and Gildea (2005).
101
Figure 3: Example output from the ITG using non-compositional phrases. (a) is the Viterbi alignment from the word-
based ITG. The shaded regions indicate phrasal alignments that are allowed by the non-compositional constraint; all
other phrasal alignments will not be considered. (b) is the Viterbi alignment from the phrasal ITG, with the multi-word
alignments highlighted.
5 Bootstrapping Phrasal ITG from
Word-based ITG
This section introduces a technique that bootstraps
candidate phrase pairs for phrase-based ITG from
word-based ITG Viterbi alignments. The word-
based ITG uses the same expansions for the non-
terminal X , but the expansions of C are limited to
generate only 1-1, 1-0, and 0-1 alignments:
C ? e/f,
C ? e/?,
C ? ?/f
where ? indicates that no word was generated.
Broadly speaking, the goal of this section is the same
as the previous section, namely, to limit the set of
phrase pairs that needs to be considered in the train-
ing process. The tic-tac-toe pruning relies on IBM
model 1 for scoring a given aligned area. In this
part, we use word-based ITG alignments as anchor
points in the alignment space to pin down the poten-
tial phrases. The scope of iterative phrasal ITG train-
ing, therefore, is limited to determining the bound-
aries of the phrases anchored on the given one-to-
one word alignments.
The heuristic method is based on the Non-
Compositional Constraint of Cherry and Lin (2007).
Cherry and Lin (2007) use GIZA++ intersections
which have high precision as anchor points in the
bitext space to constraint ITG phrases. We use ITG
Viterbi alignments instead. The benefit is two-fold.
First of all, we do not have to run a GIZA++ aligner.
Second, we do not need to worry about non-ITG
word alignments, such as the (2, 4, 1, 3) permutation
patterns. GIZA++ does not limit the set of permu-
tations allowed during translation, so it can produce
permutations that are not reachable using an ITG.
Formally, given a word-based ITG alignment, the
bootstrapping algorithm finds all the phrase pairs
according to the definition of Och and Ney (2004)
and Chiang (2005) with the additional constraint
that each phrase pair contains at most one word
link. Mathematically, let e(i, j) count the number of
word links that are emitted from the substring ei...j ,
and f(l,m) count the number of word links emit-
ted from the substring fl...m. The non-compositional
phrase pairs satisfy
e(i, j) = f(l,m) ? 1.
Figure 3 (a) shows all possible non-compositional
phrases given the Viterbi word alignment of the ex-
ample sentence pair.
6 Summary of the Pipeline
We summarize the pipeline of our system, demon-
strating the interactions between the three main con-
tributions of this paper: Variational Bayes, tic-tac-
toe pruning, and word-to-phrase bootstrapping. We
102
start from sentence-aligned bilingual data and run
IBM Model 1 in both directions to obtain two trans-
lation tables. Then we use the efficient bidirectional
tic-tac-toe pruning to prune the bitext space within
each of the sentence pairs; ITG parsing will be car-
ried out on only this this sparse set of bitext cells.
The first stage of training is word-based ITG, us-
ing the standard iterative training procedure, except
VB replaces EM to focus on a sparse prior. Af-
ter several training iterations, we obtain the Viterbi
alignments on the training data according to the fi-
nal model. Now we transition into the second stage
? the phrasal training. Before the training starts,
we apply the non-compositional constraints over the
pruned bitext space to further constrain the space
of phrase pairs. Finally, we run phrasal ITG itera-
tive training using VB for a certain number of itera-
tions. In the end, a Viterbi pass for the phrasal ITG is
executed to produce the non-compositional phrasal
alignments. From this alignment, phrase pairs are
extracted in the usual manner, and a phrase-based
translation system is trained.
7 Experiments
The training data was a subset of 175K sentence
pairs from the NIST Chinese-English training data,
automatically selected to maximize character-level
overlap with the source side of the test data. We put
a length limit of 35 on both sides, producing a train-
ing set of 141K sentence pairs. 500 Chinese-English
pairs from this set were manually aligned and used
as a gold standard.
7.1 Word Alignment Evaluation
First, using evaluations of alignment quality, we
demonstrate the effectiveness of VB over EM, and
explore the effect of the prior.
Figure 4 examines the difference between EM and
VB with varying sparse priors for the word-based
model of ITG on the 500 sentence pairs, both af-
ter 10 iterations of training. Using EM, because of
overfitting, AER drops first and increases again as
the number of iterations varies from 1 to 10. The
lowest AER using EM is achieved after the second
iteration, which is .40. At iteration 10, AER for EM
increases to .42. On the other hand, using VB, AER
decreases monotonically over the 10 iterations and
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 1e-009  1e-006  0.001  1
A
ER
Prior value
VB
EM
Figure 4: AER drops as ?C approaches zero; a more
sparse solution leads to better results.
stabilizes at iteration 10. When ?C is 1e ? 9, VB
gets AER close to .35 at iteration 10.
As we increase the bias toward sparsity, the AER
decreases, following a long slow plateau. Although
the magnitude of improvement is not large, the trend
is encouraging.
These experiments also indicate that a very sparse
prior is needed for machine translation tasks. Un-
like Johnson (2007), who found optimal perfor-
mance when ? was approximately 10?4, we ob-
served monotonic increases in performance as ?
dropped. The dimensionality of this MT problem is
significantly larger than that of the sequence prob-
lem, though, therefore it may take a stronger push
from the prior to achieve the desired result.
7.2 End-to-end Evaluation
Given an unlimited amount of time, we would tune
the prior to maximize end-to-end performance, us-
ing an objective function such as BLEU. Unfortu-
nately these experiments are very slow. Since we
observed monotonic increases in alignment perfor-
mance with smaller values of ?C , we simply fixed
the prior at a very small value (10?100) for all trans-
lation experiments. We do compare VB against EM
in terms of final BLEU scores in the translation ex-
periments to ensure that this sparse prior has a sig-
103
nificant impact on the output.
We also trained a baseline model with GIZA++
(Och and Ney, 2003) following a regimen of 5 it-
erations of Model 1, 5 iterations of HMM, and 5
iterations of Model 4. We computed Chinese-to-
English and English-to-Chinese word translation ta-
bles using five iterations of Model 1. These val-
ues were used to perform tic-tac-toe pruning with
?b = 1 ? 10?3 and ?s = 1 ? 10?6. Over the pruned
charts, we ran 10 iterations of word-based ITG using
EM or VB. The charts were then pruned further by
applying the non-compositional constraint from the
Viterbi alignment links of that model. Finally we ran
10 iterations of phrase-based ITG over the residual
charts, using EM or VB, and extracted the Viterbi
alignments.
For translation, we used the standard phrasal de-
coding approach, based on a re-implementation of
the Pharaoh system (Koehn, 2004). The output of
the word alignment systems (GIZA++ or ITG) were
fed to a standard phrase extraction procedure that
extracted all phrases of length up to 7 and esti-
mated the conditional probabilities of source given
target and target given source using relative fre-
quencies. Thus our phrasal ITG learns only the
minimal non-compositional phrases; the standard
phrase-extraction algorithm learns larger combina-
tions of these minimal units. In addition the phrases
were annotated with lexical weights using the IBM
Model 1 tables. The decoder also used a trigram lan-
guage model trained on the target side of the training
data, as well as word count, phrase count, and distor-
tion penalty features. Minimum Error Rate training
(Och, 2003) over BLEU was used to optimize the
weights for each of these models over the develop-
ment test data.
We used the NIST 2002 evaluation datasets for
tuning and evaluation; the 10-reference develop-
ment set was used for minimum error rate training,
and the 4-reference test set was used for evaluation.
We trained several phrasal translation systems, vary-
ing only the word alignment (or phrasal alignment)
method.
Table 1 compares the four systems: the GIZA++
baseline, the ITG word-based model, the ITG multi-
word model using EM training, and the ITG multi-
word model using VB training. ITG-mwm-VB is
our best model. We see an improvement of nearly
Development Test
GIZA++ 37.46 28.24
ITG-word 35.47 26.55
ITG-mwm (VB) 39.21 29.02
ITG-mwm (EM) 39.15 28.47
Table 1: Translation results on Chinese-English, using
the subset of training data (141K sentence pairs) that have
length limit 35 on both sides. (No length limit in transla-
tion. )
2 points dev set and nearly 1 point of improvement
on the test set. We also observe the consistent supe-
riority of VB over EM. The gain is especially large
on the test data set, indicating VB is less prone to
overfitting.
8 Conclusion
We have presented an improved and more efficient
method of estimating phrase pairs directly. By both
changing the objective function to include a bias
toward sparser models and improving the pruning
techniques and efficiency, we achieve significant
gains on test data with practical speed. In addition,
these gains were shown without resorting to external
models, such as GIZA++. We have shown that VB
is both practical and effective for use in MT models.
However, our best system does not apply VB to a
single probability model, as we found an apprecia-
ble benefit from bootstrapping each model from sim-
pler models, much as the IBM word alignment mod-
els are usually trained in succession. We find that
VB alone is not sufficient to counteract the tendency
of EM to prefer analyses with smaller trees using
fewer rules and longer phrases. Both the tic-tac-toe
pruning and the non-compositional constraint ad-
dress this problem by reducing the space of possible
phrase pairs. On top of these hard constraints, the
sparse prior of VB helps make the model less prone
to overfitting to infrequent phrase pairs, and thus
improves the quality of the phrase pairs the model
learns.
Acknowledgments This work was done while the
first author was at Microsoft Research; thanks to Xi-
aodong He, Mark Johnson, and Kristina Toutanova.
The last author was supported by NSF IIS-0546554.
104
References
Matthew Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience Unit, University College
London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17?24, Rochester, New York, April. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine translation
models. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 115?124, Washington, USA, Septem-
ber.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
International Colloquium on Grammatical Inference,
pages 84?96, Tokyo, Japan.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Robert C. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In Proceedings
of EACL, Budapest, Hungary.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan.
Frank Soong and Eng Huang. 1991. A tree-trellis based
fast search for finding the n best sentence hypotheses
in continuous speech recognition. In Proceedings of
ICASSP 1991.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?741,
Copenhagen, Denmark.
Stephan Vogel. 2005. PESA: Phrase pair extraction as
sentence splitting. In MT Summit X, Phuket, Thailand.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proceedings of ACL.
105
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349?352,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Improved Smoothing for N-gram Language Models
Based on Ordinary Counts
Robert C. Moore Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,chrisq}@microsoft.com
Abstract
Kneser-Ney (1995) smoothing and its vari-
ants are generally recognized as having
the best perplexity of any known method
for estimating N-gram language models.
Kneser-Ney smoothing, however, requires
nonstandard N-gram counts for the lower-
order models used to smooth the highest-
order model. For some applications, this
makes Kneser-Ney smoothing inappropri-
ate or inconvenient. In this paper, we in-
troduce a new smoothing method based on
ordinary counts that outperforms all of the
previous ordinary-count methods we have
tested, with the new method eliminating
most of the gap between Kneser-Ney and
those methods.
1 Introduction
Statistical language models are potentially useful
for any language technology task that produces
natural-language text as a final (or intermediate)
output. In particular, they are extensively used in
speech recognition and machine translation. De-
spite the criticism that they ignore the structure of
natural language, simple N-gram models, which
estimate the probability of each word in a text
string based on the N?1 preceding words, remain
the most widely used type of model.
The simplest possible N-gram model is the
maximum likelihood estimate (MLE), which takes
the probability of a word wn, given the preceding
context w1 . . . wn?1, to be the ratio of the num-
ber of occurrences in a training corpus of the N-
gram w1 . . . wn to the total number of occurrences
of any word in the same context:
p(wn|w1 . . . wn?1) =
C(w1 . . . wn)
?
w? C(w1 . . . wn?1w?)
One obvious problem with this method is that it
assigns a probability of zero to any N-gram that is
not observed in the training corpus; hence, numer-
ous smoothing methods have been invented that
reduce the probabilities assigned to some or all ob-
served N-grams, to provide a non-zero probability
for N-grams not observed in the training corpus.
The best methods for smoothing N-gram lan-
guage models all use a hierarchy of lower-order
models to smooth the highest-order model. Thus,
if w1w2w3w4w5 was not observed in the train-
ing corpus, p(w5|w1w2w3w4) is estimated based
on p(w5|w2w3w4), which is estimated based on
p(w5|w3w4) if w2w3w4w5 was not observed, etc.
In most smoothing methods, the lower-order
models, for all N > 1, are recursively estimated
in the same way as the highest-order model. How-
ever, the smoothing method of Kneser and Ney
(1995) and its variants are the most effective meth-
ods known (Chen and Goodman, 1998), and they
use a different way of computing N-gram counts
for all the lower-order models used for smooth-
ing. For these lower-order models, the actual cor-
pus counts C(w1 . . . wn) are replaced by
C ?(w1 . . . wn) =
?
?{w?|C(w?w1 . . . wn) > 0}
?
?
In other words, the count used for a lower-order
N-gram is the number of distinct word types that
precede it in the training corpus.
The fact that the lower-order models are es-
timated differently from the highest-order model
makes the use of Kneser-Ney (KN) smooth-
ing awkward in some situations. For example,
coarse-to-fine search using a sequence of lower-
order to higher-order language models has been
shown to be an efficient way of constraining high-
dimensional search spaces for speech recognition
(Murveit et al, 1993) and machine translation
(Petrov et al, 2008). The lower-order models used
in KN smoothing, however, are very poor esti-
mates of the probabilities for N-grams that have
been observed in the training corpus, so they are
349
p(wn|w1 . . . wn?1) =
?
?
?
?
?
?
?
?
?
?
?
?w1...wn?1
Cn(w1...wn)?Dn,Cn(w1...wn)
?
w? Cn(w1...wn?1w?)
+ ?w1...wn?1p(wn|w2 . . . wn?1) if Cn(w1 . . . wn) > 0
?w1...wn?1p(wn|w2 . . . wn?1) if Cn(w1 . . . wn) = 0
Figure 1: General language model smoothing schema
not suitable for use in coarse-to-fine search. Thus,
two versions of every language model below the
highest-order model would be needed to use KN
smoothing in this case.
Another case in which use of special KN counts
is problematic is the method presented by Nguyen
et al (2007) for building and applying language
models trained on very large corpora (up to 40 bil-
lion words in their experiments). The scalability
of their approach depends on a ?backsorted trie?,
but this data structure does not support efficient
computation of the special KN counts.
In this paper, we introduce a new smoothing
method for language models based on ordinary
counts. In our experiments, it outperformed all
of the previous ordinary-count methods we tested,
and it eliminated most of the gap between KN
smoothing and the other previous methods.
2 Overview of Previous Methods
All the language model smoothing methods we
will consider can be seen as instantiating the recur-
sive schema presented in Figure 1, for all n such
that N ? n ? 2,1 where N is the greatest N-gram
length used in the model.
In this schema, Cn denotes the counting method
used for N-grams of length n. For most smoothing
methods, Cn denotes actual training corpus counts
for all n. For KN smoothing and its variants, how-
ever, Cn denotes actual corpus counts only when
n is the greatest N-gram length used in the model,
and otherwise denotes the special KN C ? counts.
In this schema, each N-gram count is dis-
counted according to a D parameter that depends,
at most, on the N-gram length and the the N-gram
count itself. The values of the ?, ?, and ? parame-
ters depend on the context w1 . . . wn?1. For each
context, the values of ?, ?, and ? must be set to
produce a normalized conditional probability dis-
tribution. Additional constraints on the previous
1For n = 2, we take the expression p(wn|w2 . . . wn?1)
to denote a unigram probability estimate p(w2).
models we consider further reduce the degrees of
freedom so that ultimately the values of these para-
meters are completely fixed by the values selected
for the D parameters.
The previous smoothing methods we consider
can be classified as either ?pure backoff?, or ?pure
interpolation?. In pure backoff methods, all in-
stances of ? = 1 and all instances of ? = 0. The
pure backoff methods we consider are Katz back-
off and backoff absolute discounting, due to Ney
et al2 In Katz backoff, if C(w1 . . . wn) is greater
than a threshold (here set to 5, as recommended
by Katz) the corresponding D = 0; otherwise D
is set according to the Good-Turing method.3
In backoff absolute discounting, the D parame-
ters depends, at most, on n; there is either one dis-
count per N-gram length, or a single discount used
for all N-gram lengths. The values of D can be set
either by empirical optimization on held-out data,
or based on a theoretically optimal value derived
from a leaving-one-out analysis, which Ney et al
show to be approximated for each N-gram length
by N1/(N1 + 2N2), where Nr is the number of
distinct N-grams of that length occuring r times in
the training corpus.
In pure interpolation methods, for each context,
? and ? are constrained to be equal. The models
we consider that fall into this class are interpolated
absolute discounting, interpolated KN, and modi-
fied interpolated KN. In these three methods, all
instances of ? = 1.4 In interpolated absolute dis-
counting, the instances of D are set as in backoff
absolute discounting. The same is true for inter-
2For all previous smoothing methods other than KN, we
refer the reader only to the excellent comparative study of
smoothing methods by Chen and Goodman (1998). Refer-
ences to the original sources may be found there.
3Good-Turing discounting is usually expressed in terms
of a discount ratio, but this can be reformulated as Dr =
r ? drr, where Dr is the subtractive discount for an N-gram
occuring r times, and dr is the corresponding discount ratio.
4Jelinek-Mercer smoothing would also be a pure interpo-
lation instance of our language model schema, in which all
instances of D = 0 and, for each context, ?+ ? = 1.
350
polated KN, but the lower-order models are esti-
mated using the special KN counts.
In Chen and Goodman?s (1998) modified inter-
polated KN, instead of one D parameter for each
N-gram length, there are three: D1 for N-grams
whose count is 1, D2 for N-grams whose count is
2, and D3 for N-grams whose count is 3 or more.
The values of these parameters may be set either
by empirical optimization on held-out data, or by
a theoretically-derived formula analogous to the
Ney et al formula for the one-discount case:
Dr = r ? (r + 1)Y
Nr+1
Nr
,
for 1 ? r ? 3, where Y = N1/(N1 + 2N2), the
discount value derived by Ney et al
3 The New Method
Our new smoothing method is motivated by the
observation that unsmoothed MLE language mod-
els suffer from two somewhat independent sources
of error in estimating probabilities for the N-grams
observed in the training corpus. The problem that
has received the most attention is the fact that, on
the whole, the MLE probabilities for the observed
N-grams are overestimated, since they end up with
all the probability mass that should be assigned to
the unobserved N-grams. The discounting used in
Katz backoff is based on the Good-Turing estimate
of exactly this error.
Another source of error in MLE models, how-
ever, is quantization error, due to the fact that only
certain estimated probability values are possible
for a given context, depending on the number of
occurrences of the context in the training corpus.
No pure backoff model addresses this source of
error, since no matter how the discount parame-
ters are set, the number of possible probability val-
ues for a given context cannot be increased just
by discounting observed counts, as long as all N-
grams with the same count receive the same dis-
count. Interpolation models address quantization
error by interpolation with lower-order estimates,
which should have lower quantization error, due to
higher context counts. As we have noted, most ex-
isting interpolation models are constrained so that
the discount parameters fully determine the inter-
polation parameters. Thus the discount parameters
have to correct for both types of error.5
5Jelinek-Mercer smoothing is an exception to this gener-
alization, but since it has only interpolation parameters and
Our new model provides additional degrees of
freedom so the ? and ? interpolation parameters
can be set independently of the discount parame-
ters D, with the intention that the ? and ? para-
meters correct for quantization error, and the D
parameters correct for overestimation error. This
is accomplished by relaxing the link between the
? and ? parameters. We require that for each con-
text, ? ? 0, ? ? 0, and ? + ? = 1, and that
for every Dn,Cn(w1...wn) parameter, 0 ? D ?
Cn(w1 . . . wn). For each context, whatever values
we choose for these parameters within these con-
straints, we are guaranteed to have some probabil-
ity mass between 0 and 1 left over to be distributed
across the unobserved N-grams by a unique value
of ? that normalizes the conditional distribution.
Previous smoothing methods suggest several
approaches to setting the D parameters in our new
model. We try four such methods here:
1. The single theory-based discount for each N-
gram length proposed by Ney et al,
2. A single discount used for all N-gram
lengths, optimized on held-out data,
3. The three theory-based discounts for each N-
gram length proposed by Chen and Good-
man,
4. A novel set of three theory-based discounts
for each N-gram length, based on Good-
Turing discounting.
The fourth method is similar to the third, but
for the three D parameters per context, we use the
discounts for 1-counts, 2-counts, and 3-counts es-
timated by the Good-Turing method. This yields
the formula
Dr = r ? (r + 1)
Nr+1
Nr
,
which is identical to the Chen-Goodman formula,
except that the Y factor is omitted. Since Y is gen-
erally between 0 and 1, the resulting discounts will
be smaller than with the Chen-Goodman formula.
To set the ? and ? parameters, we assume that
there is a single unknown probability distribution
for the amount of quantization error in every N-
gram count. If so, the total quantization error for
a given context will tend to be proportional to the
no discount parameters, it forces the interpolation parameters
to do the same double duty that other models force the dis-
count parameters to do.
351
number of distinct counts for that context, in other
words, the number of distinct word types occur-
ring in that context. We then set ? and ? to replace
the proportion of the total probability mass for the
context represented by the estimated quantization
error with probability estimates derived from the
lower-order models:
?w1...wn?1 = ? |{w
?|Cn(w1...wn?1w?)>0}|
?
w? Cn(w1...wn?1w?)
?w1...wn?1 = 1? ?w1...wn?1
where ? is the estimated mean of the quantization
error introduced by each N-gram count.
We use a single value of ? for all contexts and
all N-gram lengths. As an a priori ?theory?-based
estimate, we assume that, since the distance be-
tween possible N-gram counts, after discounting,
is approximately 1.0, their mean quantization error
would be approximately 0.5. We also try setting ?
by optimization on held-out data.
4 Evaluation and Conclusions
We trained and measured the perplexity of 4-
gram language models using English data from
the WMT-06 Europarl corpus (Koehn and Monz,
2006). We took 1,003,349 sentences (27,493,499
words) for training, and 2000 sentences each for
testing and parameter optimization.
We built models based on six previous ap-
proaches: (1) Katz backoff, (2) interpolated ab-
solute discounting with Ney et al formula dis-
counts, backoff absolute discounting with (3) Ney
et al formula discounts and with (4) one empir-
ically optimized discount, (5) modified interpo-
lated KN with Chen-Goodman formula discounts,
and (6) interpolated KN with one empirically op-
timized discount. We built models based on four
ways of computing the D parameters of our new
model, with a fixed ? = 0.5: (7) Ney et al formula
discounts, (8) one empirically optimized discount,
(9) Chen-Goodman formula discounts, and (10)
Good-Turing formula discounts. We also built a
model (11) based on one empirically optimized
discount D = 0.55 and an empircially optimized
value of ? = 0.9. Table 1 shows that each of these
variants of our method had better perplexity than
every previous ordinary-count method tested.
Finally, we performed one more experiment, to
see if the best variant of our model (11) combined
with KN counts would outperform either variant
of interpolated KN. It did not, yielding a perplex-
ity of 53.9 after reoptimizing the two free parame-
Method PP
1 Katz backoff 59.8
2 interp-AD-fix 62.6
3 backoff-AD-fix 59.9
4 backoff-AD-opt 58.8
5 KN-mod-fix 52.8
6 KN-opt 53.0
7 new-AD-fix 56.3
8 new-AD-opt 55.6
9 new-CG-fix 57.4
10 new-GT-fix 56.1
11 new-AD-2-opt 54.9
Table 1: 4-gram perplexity results
ters of the model with the KN counts. However,
the best variant of our model eliminated 65% of
the difference in perplexity between the best pre-
vious ordinary-count method tested and the best
variant of KN smoothing tested, suggesting that it
may currently be the best approach when language
models based on ordinary counts are desired.
References
Chen, Stanley F., and Joshua Goodman. 1998.
An empirical study of smoothing techniques for
language modeling. Technical Report TR-10-
98, Harvard University.
Kneser, Reinhard, and Hermann Ney. 1995. Im-
proved backing-off for m-gram language mod-
eling. In Proceedings of ICASSP-95, vol. 1,
181?184.
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of WMT-06, 102?121.
Murveit, Hy, John Butzberger, Vassilios Digalakis,
and Mitch Weintraub. 1993. Progressive search
algorithms for large-vocabulary speech recogni-
tion. In Proceedings of HLT-93, 87?90.
Nguyen, Patrick, Jianfeng Gao, and Milind Maha-
jan. 2007. MSRLM: a scalable language mod-
eling toolkit. Technical Report MSR-TR-2007-
144. Microsoft Research.
Petrov, Slav, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation us-
ing language projections. In Proceedings of
ACL-08. 108?116.
352
Towards a Simple and Accurate Statistical Approach to Learning
Translation Relationships among Words
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
bobmoore@microsoft.com
Abstract
We report on a project to derive word
translation relationships automatically
from parallel corpora. Our effort is
distinguished by the use of simpler,
faster models than those used in pre-
vious high-accuracy approaches. Our
methods achieve accuracy on single-
word translations that seems compara-
ble to any work previously reported, up
to nearly 60% coverage of word types,
and they perform particularly well on a
class of multi-word compounds of spe-
cial interest to our translation effort.
1 Introduction
This paper is a report on work in progress aimed
at learning word translation relationships auto-
matically from parallel bilingual corpora. Our ef-
fort is distinguished by the use of simple statisti-
cal models that are easier to implement and faster
to run than previous high-accuracy approaches to
this problem.
Our overall approach to machine translation is
a deep-transfer approach in which the transfer re-
lationships are learned from a parallel bilingual
corpus (Richardson et al, 2001). More specifi-
cally, the transfer component is trained by pars-
ing both sides of the corpus to produce parallel
logical forms, using lexicons and analysis gram-
mars constructed by linguists. The parallel log-
ical forms are then aligned at the level of con-
tent word stems (lemmas), and logical-form trans-
fer patterns are learned from the aligned logical-
form corpus. At run time, the source language
text is parsed into logical forms employing the
source language grammar and lexicon used in
constructing the logical-form training corpus, and
the logical-form transfer patterns are used to con-
struct target language logical forms. These logical
forms are transformed into target language strings
using the target-language lexicon, and a genera-
tion grammar written by a linguist.
The principal roles played by the translation
relationships derived by the methods discussed
in this paper are to provide correspondences be-
tween content word lemmas in logical forms to
assist in the alignment process, and to augment
the lexicons used in parsing and generation, for a
special case described in Section 4.
2 Previous Work
The most common approach to deriving transla-
tion lexicons from empirical data (Catizone, Rus-
sell, and Warwick, 1989; Gale and Church, 1991;
Fung, 1995; Kumano and Hirakawa, 1994; Wu
and Xia, 1994; Melamed, 1995) is to use some
variant of the following procedure:1
 Pick a good measure of the degree of asso-
ciation between words in language L
1
and
words in language L
2
in aligned sentences
of a parallel bilingual corpus.
 Rank order pairs consisting of a word from
L
1
and a word from L
2
according to the
measure of association.
1The important work of Brown et al (1993) is not di-
rectly comparable, since their globally-optimized genera-
tive probabilistic model of translation never has to make a
firm commitment as to what can or cannot be a translation
pair. They assign some nonzero probability to every possible
translation pair.
 Choose a threshold, and add to the transla-
tion lexicon all pairs of words whose degree
of association is above the threshold.
As Melamed later (1996, 2000) pointed out,
however, this technique is hampered by the ex-
istence of indirect associations between words
that are not mutual translations. For exam-
ple, in our parallel French-English corpus (con-
sisting primarily of translated computer soft-
ware manuals), two of the most strongly associ-
ated word lemma translation pairs are fichier/file
and syste`me/system. However, because the
monolingual collocations syst`eme de fichiers,
fichiers syste`me, file system, and system files
are so common, the spurious translation pairs
fichier/system and syste`me/file also receive rather
high association scores?higher in fact that such
true translation pairs as confiance/trust, par-
alle?lisme/parallelism, and film/movie.
Melamed?s solution to this problem is not to re-
gard highly-associated word pairs as translations
in sentences in which there are even more highly-
associated pairs involving one or both of the same
words. Since indirect associations are normally
weaker than direct ones, this usually succeeds in
selecting true translation pairs over the spurious
ones. For example, in parallel sentences contain-
ing fichier and syste`me on the French side and file
and system on the English side, the associations of
fichier/system and syste`me/file will be discounted,
because the degrees of association for fichier/file
and syste`me/system are so much higher.
Melamed?s results using this approach extend
the range of high-accuracy output to much higher
coverage levels than previously reported. Our ba-
sic method is rooted the same insight regarding
competing associations for the same words, but
we embody it in simpler model that is easier to
implement and, we believe, faster to run.2 As we
will see below, our model yields results that seem
comparable to Melamed?s up to nearly 60% cov-
erage of the lexicon.
A second important issue regarding automatic
derivation of translation relationships is the as-
sumption implicit (or explicit) in most previous
work that lexical translation relationships involve
2Melamed does not report computation time for the ver-
sion of his approach without generation of compounds, but
our approach omits a number of computationally very ex-
pensive steps performed in his approach.
only single words. This is manifestly not the case,
as is shown by the following list of translation
pairs selected from our corpus:
base de donne?es/database
mot de passe/password
sauvegarder/back up
annuler/roll back
ouvrir session/log on
Some of the most sophisticated work on this
aspect of problem again seems to be that of
Melamed (1997). Our approach in this case is
quite different from Melamed?s. It is more gen-
eral in that it can propose compounds that are dis-
contiguous in the training text, as roll back would
be in a phrase such as roll the failed transac-
tion back. Melamed does allow skipping over
one or two function words, but our basic method
is not limited at all by word adjacency. Also,
our approach is again much simpler computation-
ally than Melamed?s and apparently runs orders
of magnitude faster.3
3 Our Basic Method
Our basic method for deriving translation pairs
consists of the following steps:
1. Extract word lemmas from the logical forms
produced by parsing the raw training data.
2. Compute association scores for individual
lemmas.
3. Hypothesize occurrences of compounds in
the training data, replacing lemmas consti-
tuting hypothesized occurrences of a com-
pound with a single token representing the
compound.
4. Recompute association scores for com-
pounds and remaining individual lemmas.
3Melamed reports that training on 13 million words took
over 800 hours in Perl on a 167-MHz UltraSPARC proces-
sor. Training our method on 6.6 million words took approx-
imately 0.5 hours in Perl on a 1-GHz Pentium III proces-
sor. Even allowing an order of magnitude for the differences
in processor speed and amount of data, there seems to be
a difference between the two methods of at least two or-
ders of magnitude in computation required. Unfortunately
Melamed evaluates accuracy in his work on translation com-
pounds differently from his work on single-word translation
pairs, so we are not able to compare our method to his in that
regard.
5. Recompute association scores, taking into
account only co-occurrences such that there
is no equally strong or stronger association
for either item in the aligned logical-form
pair.
We describe each of these steps in detail below.
3.1 Extracting word lemmas
In Step 1, we simply collect, for each sentence,
the word lemmas identified by our MT system
parser as the key content items in the logical form.
These are predominantly morphologically ana-
lyzed word stems, omitting most function words.
In addition, however, the parser treats certain lexi-
cal compounds as if they were single units. These
include multi-word expressions placed in the lex-
icon because they have a specific meaning or use,
plus a number of general categories including
proper names, names of places, time expressions,
dates, measure expressions, etc. We will refer to
all of these generically as ?multiwords?.
The existence of multiwords simplifies learn-
ing some translation relationships, but makes oth-
ers more complicated. For example, we do not,
in fact, have to learn base de donne?es as a com-
pound translation for database, because it is ex-
tracted from the French logical forms already
identified as a single unit. Thus we only need
to learn the base de donne?es/database correspon-
dence as a simple one-to-one mapping. On the
other hand, the disque dur/hard disk correspon-
dence is learned as two-to-one relationship inde-
pendently of disque/disk and dur/hard (which are
also learned) because hard disk appears as a mul-
tiword in our English logical forms, but disque
and dur always appear as separate tokens in our
French logical forms.
3.2 Computing association scores
For Step 2, we compute the degree of association
between a lemma w
L
1
and a lemma w
L
2
in terms
of the frequencies with which w
L
1
occurs in sen-
tences of the L
1
part of the training corpus and
w
L
2
occurs in sentences of the L
2
part of the train-
ing corpus, compared to the frequency with which
w
L
1
and w
L
2
co-occur in aligned sentences of the
training corpus. For this purpose, we ignore mul-
tiple occurrences of a lemma in a single sentence.
As a measure of association, we use the log-
likelihood-ratio statistic recommended by Dun-
ning (1993), which is the same statistic used by
Melamed to initialize his models. This statis-
tic gives a measure of the likelihood that two
samples are not generated by the same probabil-
ity distribution. We use it to compare the over-
all frequency of w
L
1
in our training data to the
frequency of w
L
1
given w
L
2
(i.e., the frequency
with which w
L
1
occurs in sentences of L
1
that
are aligned with sentences of L
2
in which w
L
2
occurs). Since p(w
L
1
) = p(w
L
1
jw
L
2
) only if
occurrences of w
L
1
and w
L
2
are independent, a
measure of the likelihood that these distributions
are different is, therefore, a measure of the like-
lihood that an observed positive association be-
tween w
L
1
and w
L
2
is not accidental.
Since this process generates association scores
for a huge number of lemma pairs for a large
training corpus, we prune the set to restrict our
consideration to those pairs having at least some
chance of being considered as translation pairs.
We heuristically set this threshold to be the degree
of association of a pair of lemmas that have one
co-occurrence, plus one other occurrence each.
3.3 Hypothesizing compounds and
recomputing association scores
If our data were very clean and all transla-
tions were one-to-one, we would expect that in
most aligned sentence pairs, each word or lemma
would be most strongly associated with its trans-
lation in that sentence pair; since, as Melamed
has argued, direct associations should be stronger
than indirect ones. Since translation is symmet-
ric, we would expect that if w
L
1
is most strongly
associated with w
L
2
, w
L
2
would be most strongly
associated with w
L
1
. Violations of this pattern are
suggestive of translation relationships involving
compounds. Thus, if we have a pair of aligned
sentences in which password occurs in the En-
glish sentence and mot de passe occurs in the
French side, we should not be surprised if mot
and passe are both most strongly associated with
password within this sentence pair. Password,
however, cannot be most strongly associated with
both mot and passe.
Our method carrying out Step 3 is based on
finding violations of the condition that whenever
w
L
1
is most strongly associated with w
L
2
, w
L
2
is
most strongly associated with w
L
1
. The method
is easiest to explain in graph-theoretic terms. Let
the nodes of a graph consist of all the lemmas of
L
1
and L
2
in a pair of aligned sentences. For each
lemma, add a link to the uniquely most strongly
associated lemma of the other language.4 Con-
sider the maximal, connected subgraphs of the re-
sulting graph. If all translations within the sen-
tence pair are one-to-one, each of these subgraphs
should contain exactly two lemmas, one from L
1
and one from L
2
. For every subgraph containing
more than two lemmas of one of the languages,
we consider all the lemmas of that language in the
subgraph to form a compound. In the case of mot,
passe, and password, as described above, there
would be a connected subgraph containing these
three lemmas; so the two French lemmas, mot and
passe, would be considered to form a compound
in the French sentence under consideration.
The output of this step of our process is a trans-
formed set of lemmas for each sentence in the cor-
pus. For each sentence and each subset of the
lemmas in that sentence that has been hypothe-
sized to form a compound in the sentence, we
replace those lemmas with a token representing
them as a single unit. Note that this process works
on a sentence-pair by sentence-pair basis, so that
a compound hypothesized for one sentence pair
may not be hypothesized for a different sentence
pair, if the pattern of strongest associations for the
two sentence pairs differ. Order of occurrence is
not considered in forming these compounds, and
the same token is always used to represent the
same set of lemmas.5
Once the sets of lemmas for the training cor-
pus have been reformulated in terms of the hy-
pothesized compounds, Step 4 consists simply in
repeating step 2 on the reformulated training data.
3.4 Recomputing association scores, taking
into account only strongest associations
If Steps 1?4 worked perfectly, we would have
correctly identified all the compounds needed for
translation and reformulated the training data to
treat each such compound as a single item. At this
4Because the data becomes quite noisy if a lemma has no
lemmas in the other language that are very strongly associ-
ated with it, we place a heuristically chosen threshold on the
minimum degree of association that is allowed to produce a
link.
5The surface order is not needed by the alignment proce-
dure intended to make use of the translation relationships we
discover.
point, we should be able to treat the training data
as if all translations are one-to-one. We therefore
choose our final set of ranked translation pairs on
the assumption that true translation pairs will al-
ways be mutually most strongly associated in a
given aligned sentence pair.
Step 5 thus proceeds exactly as step 4, except
that w
L
1
and w
L
2
are considered to have a joint
occurrence only if w
L
1
is uniquely most strongly
associated with w
L
2
, and w
L
2
is uniquely most
strongly associated with w
L
1
, among the lemmas
(or compound lemmas) present in a given aligned
sentence pair. (The associations computed by the
previous step are used to make these decisions.)
This final set of associations is then sorted in de-
creasing order of strength of association.
4 Identifying Translations of ?Captoids?
In addition to using these techniques to provide
translation relationships to the logical-form align-
ment process, we have applied related methods to
a problem that arises in parsing the raw input text.
Often in text?particularly the kind of technical
text we are experimenting with?phrases are used,
not in their usual way, but as the name of some-
thing in the domain. Consider, Click to remove
the View As Web Page check mark. In this sen-
tence, View As Web Page has the syntactic form
of a nonfinite verb phrase, but it is used as if it is a
proper name. If the parser does not recognize this
special use, it is virtually impossible to parse the
sentence correctly.
Expressions of this type are fairly easily han-
dled by our English parser, however, because cap-
italization conventions in English make them easy
to recognize. The tokenizer used to prepare sen-
tences for parsing, under certain conditions, hy-
pothesizes that sequences of capitalized words
such as View As Web Page should be treated as
lexicalized multi-word expressions, as discussed
in Section 3.1. We refer to this subclass of mul-
tiwords as ?captoids?. The capitalization conven-
tions of French (or Spanish) make it harder to rec-
ognize such expressions, however, because typi-
cally only the first word of such an expression is
capitalized.
We have adapted the methods described in Sec-
tion 3 to address this problem by finding se-
quences of French words that are highly asso-
ciated with English captoids. The sequences of
French words that we find are then added to the
French lexicon as multiwords.
The procedure for identifying translations of
captiods is as follows:
1. Tokenize the training data to separate words
from punctuation and identify multiwords
wherever possible.
2. Compute association scores for items in the
tokenized data.
3. Hypothesize sequences of French words as
compounds corresponding to English mul-
tiwords, replacing hypothesized occurrences
of a compound in the training data with a sin-
gle token representing the compound.
4. Recompute association scores for pairs of
items where either the English item or the
French item is a multiword beginning with a
capital letter.
5. Filter the resulting list to include only trans-
lation pairs such that there is no equally
strong or stronger association for either item
in the training data.
There are a number of key differences from our
previous procedure. First, since this process is
meant to provide input to parsing, it works on to-
kenized word sequences rather than lemmas ex-
tracted from logical forms. Because many of the
English multiwords are so rare that associations
for the entire multiword are rather weak, in Step
2 we count occurrences of the constituent words
contained in multiwords as well as occurrences of
the multiwords themselves. Thus an occurrence
of View As Web Page would also count as an oc-
currence of view, as, web, and page.6
The method of hypothesizing compounds in
Step 3 adds a number of special features to im-
prove accuracy and coverage. Since we know
we are trying to find French translations for En-
glish captoids, we look for compounds only in the
French data. If any of the association scores be-
tween a French word and the constituent words of
an English multiword are higher than the associa-
tion score between the French word and the entire
multiword, we use the highest such score to repre-
sent the degree of association between the French
6In identifying captoid translations, we ignore case dif-
ferences for computing and using association scores.
word and the English multiword. We reserve,
for consideration as the basis of compounds, only
sets of French words that are most strongly as-
sociated in a particular aligned sentence pair with
an English multiword that starts with a capitalized
word.
Finally we scan the French sentence of the
aligned pair from left to right, looking for a cap-
italized word that is a member of one of the
compound-defining sets for the pair. When we
find such a word, we begin constructing a French
multiword. We continue scanning to the right to
find other members of the compound-defining set,
allowing up to two consecutive words not in the
set, provided that another word in the set imme-
diately follows, in order to account for French
function words than might not have high asso-
ciations with anything in the English multiword.
We stop adding to the French multiword once we
have found all the French words in the compound-
defining set, or if we encounter a punctuation
symbol, or if we encounter three or more con-
secutive words not in the set. If either of the lat-
ter two conditions occurs before exhausting the
compound-defining set, we assume that the re-
maining members of the set represent spurious as-
sociations and we leave them out of the French
multiword.
The restriction in Step 4 to consider only asso-
ciations in which one of the items is a mutiword
beginning with a capital letter is simply for effi-
ciency, since from this point onward no other as-
sociations are of interest.
The final filter applied in Step 5 is more strin-
gent than in our basic method. The reasoning is
that, while a single word may have more than one
translation in different contexts, the sort of com-
plex multiword represented by a captoid would
normally be expected to receive the same trans-
lation in all contexts. Therefore we accept only
translations involving captoids that are mutually
uniquely most strongly associated across the en-
tire corpus. To focus on the cases we are most
interested in and to increase accuracy, we require
each translation pair generated to satisfy the fol-
lowing additional conditions:
 The French item must be one of the mulit-
words we constructed.
 The English item must be a multiword, all of
Type Mean Token Total Single-Word Multiword Compound
Coverage Count Coverage Accuracy Accuracy Accuracy Accuracy
0.040 1247.23 0.859 0.902?0.920 0.927?0.934 0.900?0.900 0.615?0.769
0.080 670.88 0.923 0.842?0.870 0.922?0.939 0.879?0.879 0.453?0.547
0.120 457.79 0.945 0.801?0.834 0.908?0.924 0.734?0.766 0.452?0.548
0.160 347.58 0.957 0.783?0.820 0.898?0.913 0.705?0.737 0.455?0.562
0.200 280.17 0.964 0.762?0.797 0.893?0.911 0.638?0.688 0.449?0.527
0.240 234.63 0.969 0.749?0.783 0.887?0.904 0.606?0.658 0.431?0.505
0.280 201.89 0.973 0.728?0.767 0.878?0.898 0.575?0.637 0.411?0.487
0.320 177.11 0.975 0.712?0.752 0.875?0.896 0.577?0.643 0.375?0.449
0.360 158.08 0.979 0.668?0.710 0.860?0.884 0.511?0.578 0.340?0.405
0.400 142.45 0.980 0.654?0.696 0.845?0.871 0.486?0.556 0.329?0.391
0.440 129.60 0.981 0.637?0.677 0.844?0.869 0.485?0.550 0.298?0.354
0.480 118.90 0.982 0.641?0.680 0.848?0.872 0.502?0.566 0.297?0.351
0.520 109.83 0.983 0.643?0.681 0.852?0.875 0.511?0.574 0.291?0.344
0.560 102.15 0.984 0.626?0.664 0.839?0.864 0.503?0.564 0.279?0.329
0.600 95.50 0.986 0.596?0.636 0.823?0.852 0.484?0.541 0.255?0.305
0.632 90.87 0.989 0.550?0.595 0.784?0.819 0.429?0.488 0.232?0.286
Table 1: Results for basic method.
whose constituent words are capitalized.
 The French item must contain at least as
many words as the English item.
The last condition corrects some errors made by
allowing highly associated French words to be left
out of the hypothesized compounds.
5 Experimental Results
Our basic method for finding translation pairs
was applied to a set of approximately 200,000
French and English aligned sentence pairs, de-
rived mainly from Microsoft technical manuals,
resulting in 46,599 potential translation pairs. The
top 42,486 pairs were incorporated in the align-
ment lexicon of our end-to-end translation sys-
tem.7 The procedure for finding translations of
captoids was applied to a slight superset of the
training data for the basic procedure, and yielded
2561 possible translation pairs. All of these were
added to our end-to-end translation system, with
the French multiwords being added to the lexicon
of the French parser, and the translation pairs be-
ing added to the alignment lexicon.
The improvements in end-to-end performance
due to these additions in a French-to-English
translation task are described elsewhere (Pinkham
and Corston-Oliver, 2001). For this report, we
have evaluated our techniques for finding trans-
7As of this writing, however, the alignment procedure
does not yet make use of the general translation pairs involv-
ing compounds, although it does make use of the captoid
translation compounds.
lation pairs by soliciting judgements of transla-
tion correctness from fluent French-English bilin-
guals. There were too many translation pairs to
obtain judgements on each one, so we randomly
selected about 10% of the 42,486 general transla-
tion pairs that were actually added to the system,
and about 25% of the 2561 captoid pairs.
The accuracy of the most strongly associated
translation pairs produced by the basic method
at various levels of coverage is displayed in Ta-
ble 1. We use the terms ?coverage? and ?accu-
racy? in essentially the same way as Melamed
(1996, 2000). ?Type coverage? means the pro-
portion of distinct lexical types in the entire
training corpus, including both French and En-
glish, for which there is at least one translation
given. As with the comparable results reported
by Melamed, these are predominantly single lem-
mas for content words, but we also include oc-
currences multiwords as distinct types. ?Mean
count? is the average number of occurrences of
each type at the given level of coverage. ?Token
coverage? is the proportion of the total number of
occurrences of items in the text represented by the
types included within the type coverage.
Since the judges were asked to evaluate the
proposed translations out of context, we allowed
them to give an answer of ?not sure?, as well as
?correct? and ?incorrect?. Our accuracy scores
are therefore given as a range, where the low
score combines answers of ?not sure? and ?in-
correct?, and the high score combines answers of
?not sure? and ?correct?.
Type Mean Token Single-Word
Coverage Count Coverage Accuracy
0.040 1628.57 0.791 0.948?0.948
0.080 909.48 0.884 0.938?0.942
0.120 626.84 0.914 0.926?0.943
0.160 480.50 0.934 0.909?0.924
0.200 389.11 0.945 0.896?0.912
0.240 327.03 0.953 0.891?0.910
0.280 281.76 0.958 0.896?0.913
0.320 247.67 0.963 0.876?0.896
0.360 220.62 0.965 0.876?0.898
0.400 199.42 0.969 0.864?0.887
0.440 181.69 0.971 0.846?0.872
0.480 166.64 0.971 0.843?0.868
0.520 153.90 0.972 0.848?0.872
0.560 143.22 0.974 0.844?0.868
0.600 133.87 0.976 0.830?0.859
0.636 127.46 0.984 0.784?0.819
Table 2: Results for single words only.
Type Mean Token Captoid
Coverage Count Coverage Accuracy
0.020 50.39 0.149 0.913?0.913
0.040 30.19 0.178 0.902?0.902
0.060 21.67 0.192 0.914?0.914
0.080 17.88 0.211 0.911?0.915
0.100 14.79 0.218 0.901?0.904
0.120 12.61 0.223 0.859?0.864
0.140 11.06 0.228 0.860?0.864
0.160 9.95 0.235 0.858?0.862
0.180 9.04 0.240 0.846?0.851
0.194 8.70 0.249 0.841?0.847
Table 3: Results for captoids.
The ?total accuracy? column gives results at
different levels of coverage over all the transla-
tion pairs generated by our basic method. For
a more detailed analysis, the remaining columns
provide a breakdown for single-word translations,
translations involving multiwords given to us by
the parser (?multiword accuracy?), and new mul-
tiwords hypothesized by our procedure (?com-
pound accuracy?). As the table shows, our perfor-
mance is quite good on single-word translations,
with accuracy of around 80% even at our cut-off
of 63% type coverage, which represents 99% of
the tokens in the corpus.
To compare our results more directly with
Melamed?s published results on single-word
translation, we show Table 2, where both cover-
age and accuracy are given for single-word trans-
lations only. According to the standard of cor-
rectness Melamed uses that is closest to ours, he
reports 92% accuracy at 36% type coverage, 89%
accuracy at 46% type coverage, and 87% accu-
racy at 90% type coverage, on a set of 300,000
aligned sentence pairs from the French-English
Hansard corpus of Candian Parliament proceed-
ings. Our accuracies at the first two of these cov-
erage points are 88?90% and 84?87%, which is
slightly lower than Melamed, but given the dif-
ferent corpus, different judges, and different eval-
uation conditions, one cannot draw any definite
conclusions about which method is more accu-
rate at these coverage levels. Our method, how-
ever, does not produce any result approaching
90% type coverage, and accuracy appears to start
dropping rapidly below 56% type coverage. Nev-
ertheless, this still represents good accuracy up to
97% token coverage.
Returning to Table 1, we see that our accu-
racy on multiwords is much lower than on single
words, especially the multiwords hypothesized by
our learning procedure. The results are much
better, however, when we look at the results for
our specialized method for finding translations of
captoids, as shown in Table 3. Our accuracy at
nearly 20% type coverage is around 84%, which
is higher than our accuracy for general translation
pairs (76?80%) at the same type coverage level. It
is lower than our single-word translation accuracy
(90?91%) at this coverage level, but it is strik-
ing how close it is, given far less data. At 20%
type coverage of single words, there are 389 to-
kens per word type, while at 20% type coverage
of captoids, there are fewer than 9 tokens per cap-
toid type. In fact, further analysis shows that of
the 2561 captoid translation pairs, 947 have only
a single example of the English captoid in the
training data, yet our accuracy on these is around
82%. We note, however, that our captoid learning
procedure cuts off at around 20% type coverage,
which is only 25% token coverage for these items.
6 Conclusions
We have evaluated our approach and found it to
be comparable in accuracy on single-word trans-
lations to Melamed?s results (which appear to be
the best previous results, as far as one can tell
given the lack of standard test corpora) up to
nearly 60% type coverage and 97% token cover-
age. Space does not permit a detailed compari-
son of Melamed?s methods to ours, but we repeat
that ours are far simpler to implement and much
faster to run. Our approach to generating trans-
lations involving muti-word compounds performs
less well in general, but the special-case modifica-
tion of it to deal with captoids performs with very
high accuracy for those captoids it is able to find
a translation for. Based on these results, the fo-
cus of our future work will be to try to extend our
region of high-accuracy single-word translation to
higher levels of coverage, improve the accuracy of
our general method for finding multiword transla-
tions, and extend the coverage of our method for
translating captoids.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The Mathemat-
ics of Statistical Machine Translation: Param-
eter Estimation. Computational Linguistics,
19(2):263?311.
R. Catizone, G. Russell, and S. Warwick. 1989.
Deriving translation data from bilingual texts.
In Proceedings of the First International Lexi-
cal Acquisition Workshop, Detroit, MI.
T. Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Compu-
tational Linguistics, 19(1):61?74.
P Fung. 1995. A pattern matching method for
finding noun and proper noun translations from
noisy parallel corpora, In Proceedings of the
33rd Annual Meeting, pages 236?243, Boston,
MA. Association for Computational Linguis-
tics.
W. Gale and K. Church . 1991. Identifying owrd
correspondences in parallel texts. In Proceed-
ings Speech and Natural Language Workshop,
pages 152?157, Asilomar, CA. DARPA.
A. Kumano and H. Hirakawa. 1994. Building an
MT dictionary from parallel texts based on lin-
guistic and statistical information. In Proceed-
ings of the 15th International Conference on
Computational Linguistics, pages 76?81, Ky-
oto, Japan.
I. D. Melamed. 1995. Automatic evaluation
and uniform filter cascades for inucing N -
best translation lexicons. In Proceedings of
the Third Workshop on Very Large Corpora,
pages 184?198, Cambridge, MA.
I. D. Melamed. 1996. Automatic construction of
clean broad coverage translation lexicons. In
Proceedings of the 2nd Conference of the As-
sociation for Machine Translation in the Amer-
icas, pages 125-134, Montreal, Canada.
I. D. Melamed. 1997. Automatic discovery of
non-compositional compounds in parallel data.
In Proceedings of the 2nd Conference on En-
pirical Methods in Natural Language Process-
ing (EMNLP ?97), Providence, RI.
I. D. Melamed. 2000. Models of Transla-
tional Equivalence. Computational Linguistics,
26(2):221?249.
J. Pinkham and M. Corston-Oliver. 2001. Adding
Domain Specificity to an MT System. In
Proceedings of the Workshop on Data-Driven
Machine Translation, 39th Annual Meeting of
the Association for Computational Linguistics,
Toulouse, France.
S. Richardson, W. B. Dolan, M. Corston-Oliver,
and A. Menezes. 2001. Overcoming the
customization bottleneck using example-based
MT. In Proceedings of the Workshop on Data-
Driven Machine Translation, 39th Annual
Meeting of the Association for Computational
Linguistics, Toulouse, France.
D. Wu and X. Xia. 1994. Learning an English-
Chinese lexicon from a parallel corpus. In Pro-
ceedings of the 1st Conference of the Associa-
tion for Machine Translation in the Americas,
pages 206?213, Columbia, MD.
On Log-Likelihood-Ratios and the Significance of Rare Events
Robert C. MOORE
Microsoft Research
One Microsoft Way
Redmond, WA 90052
USA
bobmoore@microsoft.com
Abstract
We address the issue of judging the significance of
rare events as it typically arises in statistical natural-
language processing. We first define a general ap-
proach to the problem, and we empirically com-
pare results obtained using log-likelihood-ratios and
Fisher?s exact test, applied to measuring strength of
bilingual word associations.
1 Introduction
Since it was first introduced to the NLP commu-
nity by Dunning (1993), the G2 log-likelihood-ratio
statistic1 has been widely used in statistical NLP
as a measure of strength of association, particularly
lexical associations. Nevertheless, its use remains
controversial on the grounds that it may be unreli-
able when applied to rare events. For instance Ped-
ersen, et al (1996) present data showing that signifi-
cance values for rare bigrams estimated with G2 can
differ substantially from the true values as computed
by Fisher?s exact test. Although Dunning argues
that G2 is superior to the chi-square statistic2 X2
for dealing with rare events, Agresti (1990, p. 246)
cites studies showing ?X2 is valid with smaller sam-
ple sizes and more sparse tables than G2,? and either
X2 or G2 can be unreliable when expected frequen-
cies of less than 5 are involved, depending on cir-
cumstances.
The problem of rare events invariably arises
whenever we deal with individual words because
of the Zipfian phenomenon that, typically, no mat-
ter how large a corpus one has, most of the distinct
words in it will occur only a small number of times.
For example, in 500,000 English sentences sampled
from the Canadian Hansards data supplied for the
bilingual word alignment workshop held at HLT-
NAACL 2003 (Mihalcea and Pedersen, 2003), there
are 52,921 distinct word types, of which 60.5% oc-
1Dunning did not use the name G2, but this appears to be
its preferred name among statisticians (e.g., Agresti, 1990).
2Following Agresti, we use X2 to denote the test statistic
and ?2 to denote the distribution it approximates.
cur five or fewer times, and 32.8% occur only once.
The G2 statistic has been most often used in NLP
as a measure of the strength of association between
words, but when we consider pairs of words, the
sparse data problem becomes even worse. If we
look at the 500,000 French sentences correspond-
ing to the English sentences described above, we
find 19,460,068 English-French word pairs that oc-
cur in aligned sentences more often than would be
expected by chance, given their monolingual fre-
quencies. Of these, 87.9% occur together five or
fewer times, and 62.4% occur together only once.
Moreover, if we look at the expected number of oc-
currences of these word pairs (which is the criteria
used for determining the applicability of the X2 or
G2 significance tests), we find that 93.2% would be
expected by chance to have fewer than five occur-
rences. Pedersen et al (1996) report similar propor-
tions for monolingual bigrams in the ACL/DCIWall
Street Journal corpus. Any statistical measure that
is unreliable for expected frequencies of less than 5
would be totally unusable with such data.
2 How to Estimate Significance for Rare
Events
A wide variety of statistics have been used to mea-
sure strength of word association. In one paper
alone (Inkpen and Hirst, 2002), pointwise mutual
information, the Dice coefficient, X2, G2, and
Fisher?s exact test statistic were all computed and
combined to aid in learning collocations. Despite
the fact that many of these statistics arise from
significance testing, the usual practice in applying
them in NLP is to choose a threshold heuristically
for the value of the statistic being used and discard
all the pairs below the threshold. Indeed, Inkpen
and Hirst say (p. 70) ?there is no principled way of
choosing these thresholds.?
This may seem an odd statement about the mea-
sures that arise directly from significance testing,
but it is clear that if standard statistical tests are used
naively, the results make no sense in these applica-
tions. One might suppose that this is merely the re-
sult of the statistics in question not being applicable
to the rare events that predominate in NLP, but it is
easy to show this is not so.
2.1 When is Something Seen Only Once
Signficant?
Consider the case of two words that each occur only
once in a corpus, but happen to co-occur. Con-
ventional wisdom strongly advises suspicion of any
event that occurs only once, yet it is easy to see that
applying standard statistical methods to this case
will tend to suggest that it is highly significant, with-
out using any questionable approximations at all.
The question that significance tests for associa-
tion, such as X2, G2, and Fisher?s exact test, are
designed to answer is, given the sample size and
the marginal frequencies of the two items in ques-
tion, what is the probability (or p-value) of seeing by
chance as many or more joint occurrences as were
observed? In the case of a joint occurrence of two
words that each occur only once, this is trivial to cal-
culate. For instance, suppose an English word and a
French word each occur only once in our corpus of
500,000 aligned sentence pairs of Hansard data, but
they happen to occur together. What is the proba-
bility that this joint occurrence happened by chance
alone? We can suppose that the English word oc-
curs in an arbitrary sentence pair. The probability
that the French word, purely by chance, would oc-
cur in the same sentence pair is clearly 1 in 500,000
or 0.000002. Since it is impossible to have more
than one joint occurrence of two words that each
have only a single occurrence, 0.000002 is the exact
p-value for the question we have asked.
Clearly, however, one cannot assume that the as-
sociation of these two words is 0.999998 certain on
this basis alone. The problem is that there are so
many possible singleton-singleton pairs, it is very
likely that some of them will occur jointly, purely
by chance. This, too, is easy to calculate. In our
500,000 sentence pairs there are 17,379 English
singletons and 22,512 French singletons; so there
are 391,236,048 possible singleton-singleton pairs.
For each pair, the probability of having a joint oc-
currence by chance is 0.000002, so the expected
number of chance joint occurrences of singleton-
singleton pairs is 391, 236, 048 ? 0.000002, or ap-
proximately 782.5.
The question of whether a singleton-singleton
pair is signficant or not then turns on how many
singleton-singleton pairs we observe. If we see only
about 800, then they are not signficant, because that
is just about the number we would expect to see by
chance. In our corpus, however, we see far more
than that: 19,312. Thus our best estimate of the
proportion of the singleton-singleton pairs that are
due to chance is 782.5/19312 = 0.0405, which we
can think of as the ?expected noise? in the singleton-
singleton pairs. Looked at another way, we can es-
timate that at least 95.9% of the observed singleton-
singleton pairs are not due to chance, which we can
think of as ?expected precision?.3 So, we conclude
that, for this data, seeing two singletons together is
significant at the 0.05 level, but this is more than five
orders of magnitude less significant than naive use
of standard p-values would suggest.
2.2 Generalizing the Method
In the previous section, we used the p-value for
the observed joint frequency given the marginal fre-
quencies and sample size as a our base statistical
measure. We used this in an indirect way, however,
that we could apply to any other measure of asso-
ciation. For example, for a joint occurrence of two
singletons in 500,000 samples, G2 is approximately
28.24. Therefore, if we wanted to use G2 as our
measure of association, we could compare the num-
ber of word pairs expected by chance to have a G2
score greater than or equal to 28.24 with the number
of word pairs observed to have a G2 score greater
than or equal to 28.24, and compute expected noise
and precision just as we did with p-values. In prin-
ciple, we can do the same for any measure of as-
sociation. The worst that can happen is that if the
measure of association is not a good one (i.e., if
it assigns values randomly), the expected precision
will not be very good no matter how high we set the
threshold.
This means that we can, if we wish, use two dif-
ferent statistics to estimate expected noise and pre-
cision, one as a measure of association and one
to estimate the number of word pairs expected by
chance to have a given level or higher of the asso-
ciation measure. In our experiments, we will use a
likelihood-ratio-based score as the measure of asso-
ciation, and contrast the results obtained using ei-
ther a likelihood-ratio-based test or Fisher?s exact
test to estimate expectations.
Computing the expected number of pairs with a
given association score or higher, for a large collec-
3Using the binomial distribution we can calculate that there
is a 0.99 probability that there are no more than 848 singleton-
singleton pairs by chance, and hence that there is a 0.99 prob-
ability that at least 95.6% of the observed singleton-singleton
pairs are not due to chance. Since this differs hardly at all from
the expected precision, and there is no a priori reason to think
overestimating precision is any worse than underestimating it,
we will use expected values of noise and precision as our pri-
mary metrics in the rest of the paper.
for each observed C(x) {
for each observed C(y) {
possible pairs =
|values of x with frequency C(x)| ?
|values of y with frequency C(y)| ;
C
0
(x, y) = int(C(x)C(y)/N) + 1 ;
i = 1 ;
loop: for each C(x, y) such that
C
0
(x, y) ? C(x, y) ? min(C(x), C(y)) {
score = assoc(C(x, y), C(x), C(y), N ) ;
if (score ? threshold[i]) {
prob = p-value(C(x, y), C(x), C(y), N ) ;
expected pairs = prob ? possible pairs ;
while (score ? threshold[i]) {
expected count[i] += expected pairs ;
if (i < number of thresholds) {
i++ ;
}
else {
exit loop ;
}
}
}
}
}
}
Figure 1: Algorithm for Expected Counts.
tion of word pairs having a wide range of marginal
frequencies, turns out to be somewhat tricky. We
must first compute the p-value for an association
score and then multiply the p-value by the appro-
priate number of word pairs. But if the association
score itself does not correlate exactly with p-value,
the relationship between association score and p-
value will vary with each combination of marginal
frequencies.4 Furthermore, even for a single com-
bination of marginal frequencies, there is in general
no way to go directly from an association score to
the corresponding p-value. Finally, until we have
computed all the expected frequencies and observed
frequencies of interest, we don?t know which as-
sociation score is going to correspond to a desired
level of expected precision.
These complications can be accomodated as fol-
lows: First compute the distinct marginal frequen-
cies of the words that occur in the corpus (sepa-
4We assume that for fixed marginals and sample size, and
joint frequencies higher than the expected joint frequency, the
association score will increase monotonically with the joint fre-
quency. It is hard to see how any function without this prop-
erty could be considered a measure of association (unless it
decreases monotonically as joint frequency increases).
rately for English and French), and how many dis-
tinct words there are for each marginal frequency.
Next, choose a set of association score thresholds
that we would like to know the expected precisions
for.
Accumulate the expected pair counts for each
threshold by iterating through all possible combi-
nations of observed marginals. For each combina-
tion, compute the association score for each possi-
ble joint count (given the marginals and the sam-
ple size), starting from the smallest one greater than
the expected joint count C(x)C(y)/N (where C(x)
and C(y) are the marginals and N is the sample
size). Whenever the first association score greater
than or equal to one of the thresholds is encountered,
compute the associated p-value, multiply it by the
number of possible word pairs corresponding to the
combination of marginals (to obtain the expected
number of word pairs with the given marginals hav-
ing that association score or higher), and add the
result to the accumluators for all the thresholds that
have just been passed. Stop incrementing the pos-
sible joint frequency when either the smaller of the
two marginals is reached or the highest association
threshold is passed. (See Figure 1 for the details.)
At this point, we have computed the number of
word pairs that would be expected by chance alone
to have an association score equal to or greater than
each of our thresholds. Next we compute the num-
ber of word pairs observed to have an association
score equal to or greater than each of the thresh-
olds. The expected noise for each threshold is just
the ratio of the expected number of word pairs for
the threshold to the observed number of word pairs
for the threshold, and the expected precision is 1 mi-
nus the expected noise.
What hidden assumptions have we made that
could call these estimates into question? First, there
might not be enough data for the estimates of ex-
pected and observed frequencies to be reliable. This
should seldom be a problem in statistical NLP. For
our 500,000 sentence pair corpus, the cumulative
number of observed word pairs is in the tens of
thousands for for any association score for which
the estimated noise level approaches or exceeds 1%,
which yields confidence bounds that should be more
than adequate for most purposes (see footnote 3).
A more subtle issue is that our method may over-
estimate the expected pair counts, resulting in ex-
cessively conservative estimates of precision. Our
estimate of the number of pairs seen by chance for a
particular value of the association measure is based
on considering all possible pairs as nonassociated,
which is a valid approximation only if the number
2 log
[
p(y|x)C(x,y) ? p(y|?x)C(?x,y) ? p(?y|x)C(x,?y) ? p(?y|?x)C(?x,?y)
p(y)C(y) ? p(?y)C(?y)
]
(1)
2 log
[
p(y|x)C(x,y) ? p(y|?x)C(?x,y) ? p(?y|x)C(x,?y) ? p(?y|?x)C(?x,?y)
p(y)C(x,y) ? p(y)C(?x,y) ? p(?y)C(x,?y) ? p(?y)C(?x,?y)
]
(2)
2 log
?
x??{x,?x}
?
y??{y,?y}
(
p(y?|x?)
p(y?)
)C(x?,y?)
(3)
2
?
?
?
x??{x,?x}
?
y??{y,?y}
C(x?, y?) log
p(y?|x?)
p(y?)
?
? (4)
2N
?
?
?
x??{x,?x}
?
y??{y,?y}
p(x?, y?) log
p(x?, y?)
p(x?)p(y?)
?
? (5)
Figure 2: Alternative Formulas for G2.
of pairs having a significant positive or negative as-
sociation is very small compared to the total number
of possible pairs.
For the corpus used in this paper, this seems un-
likely to be a problem. The corpus contains 52,921
distinct English words and 66,406 distinct French
words, for a total of 3,514,271,926 possible word
pairs. Of these only 19,460,068 have more than the
expected number of joint occurrences. Since most
word pairs have no joint occurrences and far less
than 1 expected occurrence, it is difficult to get a
handle on how many of these unseen pairs might be
negatively associated. Since we are measuring asso-
ciation on the sentence level, however, it seems rea-
sonable to expect fewer word pairs to have a signifi-
cant negative association than a positive association,
so 40,000,000 seems likely to be a upper bound on
how many word pairs are significantly nonindepen-
dent. This, however, is only about 1% of the total
number of possible word pairs, so adjusting for the
pairs that might be significantly related would not
make an appreciable difference in our estimates of
expected noise. In applications where the signifi-
cantly nonindependent pairs do make up a substan-
tial proportion of the total possible pairs, an adjust-
ment should be made to avoid overly conservative
estimates of precision.
3 Understanding G2
Dunning (1993) gives the formula for the statistic
we are calling G2 in a form that is very compact,
but not necessarily the most illuminating:
2 [ log L(p
1
, k
1
, n
1
) + log L(p
2
, k
2
, n
2
) ?
log L(p, k
1
, n
1
) ? log L(p, k
2
, n
2
) ],
where
L(p, k, n) = pk(1 ? p)n?k.
The interpretation of the statistic becomes clearer
if we re-express it in terms of frequencies and prob-
abilities as they naturally arise in association prob-
lems, as shown in a number of alternative formu-
lations in Figure 2. In these formulas, x and y
represent two words for which we wish to esti-
mate the strength of association. C(y) and C(?y)
are the observed frequencies of y occurring or not
occurring in the corpus; C(x, y), . . . , C(?x,?y)
are the joint frequencies of the different possible
combinations of x and y occuring and not occur-
ing; and p(y), p(?y), p(y|x), . . . , p(?y|?x) are the
maximum likelihood estimates of the corresponding
marginal and conditional probabilities.
Formula 1 expresses G2 as twice the logarithm
of a ratio of two estimates of the probability of a
sequence of observations of whether y occurs; one
estimate being conditioned on whether x occurs,
and the other not. The estimate in the numerator
is conditioned on whether x occurs, so the numera-
tor is a product of four factors, one for each possi-
ble combination of x occuring and y occuring. The
overall probability of the sequence is the product
of each conditional probability of the occurrence or
nonoccurrence of y conditioned on the occurrence
or nonoccurrence of x, to the power of the number
of times the corresponding combination occurs in
the sequence of observations. The denominator is
an estimate of the probability of the same sequence,
based only on the marginal probability of y. Hence
the denominator is simply the product of the prob-
abilty of y occuring, to the power of the number of
times y occurs, and the probabilty of y not occur-
ing, to the power of the number of times y fails to
occur.5
The rest of Figure 2 consists of a sequence of mi-
nor algebraic transformations that yield other equiv-
alent formulas. In Formula 2, we simply factor
the denominator into four factors corresponding to
the same combinations of occurrence and nonoc-
currence of x and y as in the numerator. Then,
by introducing x? and y? as variables ranging over
the events of x and y occurring or not occurring,
we can re-express the ratio as a doubly nested
product as shown in Formula 3. By distributing
the log operation over all the products and expo-
nentiations, we come to Formula 4. Noting that
C(x?, y?) = N ? p(x?, y?) (where N is the sam-
ple size), and p(y?|x?)/p(y?) times p(x?)/p(x?) is
p(x?, y?)/p(x?)p(y?), we arrive at Formula 5. This
can be immediately recognized as 2N times the for-
mula for the (average) mutual information of two
random variables,6 using the maximum likelihood
estimates for the probabilities involved.
The near equivalence of G2 and mutual informa-
tion is important for at least two reasons. First, it
gives us motivation for using G2 as a measure of
word association that is independent of whether it
usable for determining significance. Mutual infor-
mation can be viewed as a measure of the informa-
tion gained about whether one word will occur by
knowing whether the other word occurs. A priori,
this is at least as plausible a measure of strength of
association as is the degree to which we should be
surprised by the joint frequency of the two words.
Thus even if G2 turns out to be bad for estimating
significance, it does not follow that it is therefore a
bad measure of strength of association.
The second benefit of understanding the relation
between G2 and mutual information is that it an-
5The interpretation of G2 in terms of a likelihood ratio for
a particular sequence of observations omits the binomial co-
efficients that complicate the usual derivation in terms of all
possible sequences having the observed joint and marginal fre-
quencies. Since all such sequences have the same probability
for any given probability distributions, and the same number
of possible sequences are involved in both the numerator and
denominator, the binomial coefficients cancel out, yielding the
same likelihood ratio as for a single sequence.
6After discovering this derivation, we learned that it
is, in fact, an old result (Attneave, 1959), but it seems
to be almost unknown in statistical NLP. The only ref-
erence to it we have been able to find in the statisti-
cal NLP ?literature? is a comment in Pederson?s publically
distributed Perl module for computing mutual information
(http://search.cpan.org/src/TPEDERSE/Text-NSP-0.69/ Mea-
sures/tmi.pm). It can also be seen as a special case of a more
general result presented by Cover and Thomas (1991, p. 307,
12.187?12.192), but otherwise we have not found it in any con-
temporary textbook.
swers the question of how to compare G2 scores as
measures of strength of association, when they are
obtained from corpora of different sizes. Formula 5
makes it clear that G2 scores will increase linearly
with the size of the corpus, assuming the relevant
marginal and conditional probabilities remain the
same. The mutual information score is independent
of corpus size under the same conditions, and thus
offers a plausible measure to be used across corpora
of varying sizes.
4 Computing Fisher?s Exact Test
In Section 2 we developed a general method of es-
timating significance for virtually any measure of
association, given a way to estimate the expected
number of pairs of items having a specified degree
of association or better, conditioned on the marginal
frequencies of the items composing the pair and
the sample size. We noted that for some plausible
measures of association, the association metric it-
self can be used to estimate the p-values needed to
compute the expected counts. G2 is one such mea-
sure, but it is questionable whether it is usable for
computing p-values on the kind of data typical of
NLP applications. We will attempt to answer this
question empirically, at least with respect to bilin-
gual word association, by comparing p-values and
expected noise estimates derived from G2 to those
derived from a gold standard, Fisher?s exact test.
In this test, the hypergeometric probability distri-
bution is used to compute what the exact probability
of a particular joint frequency would be if there were
no association between the events in question, given
the marginal frequencies and the sample size. The
only assumption made is that all trials are indepen-
dent. The formula for this probability in our setting
is:
C(x)! C(?x)! C(y)! C(?y)!
N ! C(x, y)! C(?x, y)! C(x,?y)! C(?x,?y)!
The p-value for a given joint frequency is ob-
tained by summing the hypergeometric probability
for that joint frequency and every more extreme
joint frequency consistent with the marginal fre-
quencies. In our case ?more extreme? means larger,
since we are only interested in positive degrees of
association.7 Because it involves computing fac-
torials of potentially large numbers and summing
over many possible joint frequencies, this test has
traditionally been considered feasible only for rela-
tively small sample sizes. However, a number op-
7The null hypothesis that we wish to disprove is that a
pair of words is either negatively associated or not associated;
hence, a one-sided test is appropriate.
timizations enable efficient estimation of p-values
by Fisher?s exact test for sample sizes up to at least
10
11 on current ordinary desktop computers, where
the limiting factor is the precision of 64-bit floating
point arithmetic rather than computation time.
Some keys to efficient computation of Fisher?s
exact test are:
? The logarithms of factorials of large numbers
can be efficiently computed by highly accurate
numerical approximations of the gamma func-
tion (Press et al, 1992, Chapter 6.1), based on
the identity n! = ?(n + 1).
? The following well-known recurrence relation
for the hypergeometric distribution:
Pk =
Ck?1(?x, y) Ck?1(x,?y)
Ck(x, y) Ck(?x,?y)
Pk?1
makes it easy to calculate probabilities for a se-
quence of consecutive joint frequencies, once
the first one is obtained. (The subscript k indi-
cates parameters associated with the kth joint
frequency in the sequence.)
? The highest possible joint frequency will be the
smaller of the two marginal frequencies, so if
one of the marginals is small, few terms need
to be summed.
? If we iterate from less extreme joint frequen-
cies to more extreme joint frequencies, each
probability in the summation will be smaller
than the one before. If both the marginals
are large, the summation will often converge
to a constant value, given limited arithmetic
precision, long before the smaller marginal is
reached, at which point we can stop the sum-
mation.
By taking advantage of these observations, plus a
few other optimizations specific to our application,
we are able to estimate the necessary expected joint
frequencies for our 500,000 sentence pair corpus in
66.7 minutes using Fisher?s exact test, compared to
57.4 minutes using an approximate estimate based
on likelihood ratios, a time penalty of only 16% for
using the exact method.
5 Estimating P-Values with
Log-Likelihood-Ratios
The usual way of estimating p-values from log-
likelihood-ratios is to rely on the fact that the p-
values for G2 asymtotically approach the well-
understood ?2 distribution, as the sample size in-
creases. This is subject to the various caveats and
conditions that we discussed in Section 1, however.
Since we have the ability to compute all of the ex-
act p-values for our corpus, we do not need to rely
on the ?2 approximation to test whether we can use
log-likelihood-ratios to estimate p-values. We can
empirically measure whether there is any consis-
tent relationship between log-likelihood-ratios and
p-values, and if so, use it empirically to estimate p-
values from log-likelihood-ratios without resorting
to the ?2 approximation. For all we know at this
point, it may be possible to empirically predict p-
values from G2 under conditions where the corre-
spondence with ?2 breaks down.
This means we can drop the heretofore mysteri-
ous factor of 2 that has appeared in all the formulas
for G2, since this factor seems to have been intro-
duced just to be able to read p-values directly from
standard tables for the ?2 distribution. To make it
clear what we are doing, from this point on we will
use a statistic we will call LLR which we define to
be G2/2.
To look for a relationship between LLR and p-
values as computed by Fisher?s exact test, we first
computed both statistics for a various combinations
of joint frequency, marginal frequencies, and sam-
ple sizes. Exploratory data analysis suggested a
near-linear relationship between LLR scores and
the negative of the natural logarithm of the p-values.
To make sure the apparent relationship held for a
real dataset, we computed the LLR scores and neg-
ative log p-values for all 19,460,068 English-French
word pairs in our corpus with more joint occur-
rences than expected by chance, and carried out a
least-squares linear regression, treating LLR score
as the independent variable and negative log p-value
as the dependent variable, to see how well we can
predict p-values from LLR scores. The results are
as follows:
slope: 1.00025
intercept: 1.15226
Pearson?s r2: 0.999986
standard deviation: 0.552225
With an r2 value that rounds off to five nines,
LLR score proves to be a very good predictor of
the negative log p-values over the range of values
considered. Moreover, with a slope of very close to
1, the LLR score and negative log p-values are not
merely correlated, they are virtually the same except
for the small delta represented by the intercept. In
other words,
p-value ? e?(LLR+1.15)
would seem to be not too bad an approximation.
The standard deviation of 0.55, however, is at
least slightly worrying. As a range of differences in
the logarithms of the predicted and actual p-values,
it corresponds to a range of ratios between the pre-
dicted and actual p-values from about 0.57 to 1.7.
6 Estimating Noise in Bilingual Word
Association
For our final experiment, we estimated the noise in
the bilingual word associations in our data by the
method of Section 2, using both Fisher?s exact test
and LLR scores via our regression equation to es-
timate expected pair counts. In both cases, we use
LLR scores as the measure of association. We com-
puted the cumulative expected noise for every inte-
gral value of the LLR score from 1 through 20.
To try to determine the best results we could get
by using LLR scores to estimate expected noise in
the region where we would be likely to set a cut-
off threshold, we recomputed the least-squares fit of
LLR and negative log p-value, using only data with
LLR scores between 5 and 15.8 We obtained the
following values for the parameters of the regres-
sion equation from the re-estimation:
slope: 1.04179
intercept: 0.793324
Note that the re-estimated value of the intercept
makes it closer to the theoretical value, which is
? log(0.5) ? 0.69, since independence corresponds
to an LLR score of 0 and a p-value of 0.5.
The results of these experiments are summarized
in Table 1. The first column shows the potential
LLR association score cut-offs, the second col-
umn is the expected noise for each cut-off esti-
mated by Fisher?s exact test, the third column gives
the noise estimates derived from p-values estimated
from LLR scores, and the fourth column shows the
ratio between the two noise estimates. If we look
at the noise estimates based on our gold standard,
Fisher?s exact test, we see that the noise level is be-
low 1% above an LLR score of 11, and rises rapidly
below that. This confirms previous annecdotal ex-
perience that an LLR score above 10 seems to be a
reliable indicator of a significant association.
The comparison between the two noise estimates
indicates that the LLR score underestimates the
amount of noise except at very high noise levels.
It is worst when the LLR score cut-off equals 14,
8This constitutes training on the test data for the sake of ob-
taining an upper bound on what could be achieved using LLR
scores. Should we conclude that LLR scores look promising
for this use, one would want to re-run the test training the re-
gression parameters on held-out data.
Fisher LLR
Cut-Off Noise Est Noise Est Ratio
1 0.624 0.792 1.27
2 0.516 0.653 1.27
3 0.423 0.384 0.91
4 0.337 0.274 0.81
5 0.256 0.183 0.71
6 0.181 0.114 0.63
7 0.119 0.0650 0.55
8 0.0713 0.0338 0.47
9 0.0394 0.0159 0.40
10 0.0205 0.00695 0.34
11 0.00946 0.00260 0.27
12 0.00432 0.000961 0.22
13 0.00136 0.000221 0.16
14 0.00137 0.000166 0.12
15 3.52e-005 2.00e-005 0.57
16 1.56e-005 8.02e-006 0.51
17 6.82e-006 3.19e-006 0.47
18 2.94e-006 1.24e-006 0.42
19 1.24e-006 4.65e-007 0.38
20 5.16e-007 1.72e-007 0.33
Table 1: Word Association Noise Estimates.
which happens to be just below the LLR score
(14.122) for singleton-singleton pairs. Since, for
a given sample size, singleton-singleton pairs have
the lowest possible expected joint count, this is
probably the effect of known problems with estimat-
ing p-values from likelihood ratios when expected
counts are very small.
7 Conclusions
When we use Fisher?s exact test to estimate p-
values, our new method for estimating noise for col-
lections of rare events seems to give results that
are quite consistent with our previous annecdotal
experience in using LLR scores as a measure of
word association. Using likelihood ratios to esti-
mate p-values introduces a substantial amount of er-
ror, but not the orders-of-magnitude error that Dun-
ning (1993) demonstrated for estimates that rely on
the assumption of a normal distribution. However,
since we have also shown that Fisher?s exact test can
be applied to this type of problem without a major
computational penalty, there seems to be no reason
to compromise in this regard.
8 Acknowledgements
Thanks to Ken Church, Joshua Goodman, David
Heckerman, Mark Johnson, Chris Meek, Ted Peder-
sen, and Chris Quirk for many valuable discussions
of the issues raised in this paper. Thanks especially
to Joshua Goodman for pointing out the existence
of fast numerical approximations for the factorial
function, and to Mark Johnson for helping to track
down previous results on the relationship between
log-likelihood-ratios and mutual information.
References
Alan Agresti. 1990. Categorical Data Analysis.
John Wiley & Sons, New York, New York.
Fred Attneave. 1959. Applications of Information
Theory to Psychology. Holt, Rinehart and Win-
ston, New York, New York.
Thomas M. Cover and Joy A. Thomas. 1991. El-
ements of Information Theory. John Wiley &
Sons, New York, New York.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61?74.
Diana Z. Inkpen and Graeme Hirst. 2002. Acquir-
ing collocations for lexical choice between near-
synonyms. In Unsupervised Lexical Acquisition:
Proceedings of the Workshop of the ACL Special
Interest Group on the Lexicon (SIGLEX), pp. 67?
76, Philadelphia, Pennsylvania.
Rada Mihalcea and Ted Pedersen. 2003. An eval-
uation exercise for word alignment. In Proceed-
ings of the HLT-NAACL 2003 Workshop, Building
and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pp. 1?6, Edmonton, Al-
berta.
Ted Pedersen, Mehmet Kayaalp, and Rebecca
Bruce. 1996. Significant Lexical Relationships.
In Proceedings of the 13th National Conference
on Artificial Intelligence, Portland, Oregon.
William H. Press, Saul A. Teukolsky, William T.
Vetterling, and Brian P. Flannery. 1992. Numer-
ical Recipies in C: The Art of Scientific Com-
puting, Second Edition. Cambridge University
Press, Cambridge, England.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 1?8,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Association-Based Bilingual Word Alignment
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052
bobmoore@microsoft.com
Abstract
Bilingual word alignment forms the foun-
dation of current work on statistical
machine translation. Standard word-
alignment methods involve the use of
probabilistic generative models that are
complex to implement and slow to train.
In this paper we show that it is possible
to approach the alignment accuracy of the
standard models using algorithms that are
much faster, and in some ways simpler,
based on basic word-association statistics.
1 Motivation
Bilingual word alignment is the first step of most
current approaches to statistical machine translation.
Although the best performing systems are ?phrase-
based? (see, for instance, Och and Ney (2004) or
Koehn et al (2003)), possible phrase translations
must first be extracted from word-aligned bilingual
text segments. The standard approach to word align-
ment makes use of five translation models defined
by Brown et al (1993), sometimes augmented by
an HMM-based model or Och and Ney?s ?Model
6? (Och and Ney, 2003). The best of these mod-
els can produce high accuracy alignments, at least
when trained on a large parallel corpus of fairly di-
rect translations in closely related languages.
There are a number of ways in which these stan-
dard models are less than ideal, however. The
higher-accuracy models are mathematically com-
plex, and also difficult to train, as they do not factor
in a way that permits a dynamic programming solu-
tion. It can thus take many hours of processing time
on current standard computers to train the models
and produce an alignment of a large parallel corpus.
In this paper, we take a different approach to
word alignment, based on the use of bilingual word-
association statistics rather than the generative prob-
abilistic framework that the IBM and HMM models
use. In the end we obtain alignment algorithms that
are much faster, and in some ways simpler, whose
accuracy comes surprisingly close to the established
probabilistic generative approach.
2 Data and Methodology for these
Experiments
The experiments reported here were carried out us-
ing data from the workshop on building and using
parallel texts held at HLT-NAACL 2003 (Mihalcea
and Pedersen, 2003). For the majority of our experi-
ments, we used a subset of the Canadian Hansards
bilingual corpus supplied for the workshop, com-
prising 500,000 English-French sentences pairs, in-
cluding 37 sentence pairs designated as ?trial? data,
and 447 sentence pairs designated as test data. The
trial and test data have been manually aligned at
the word level, noting particular pairs of words ei-
ther as ?sure? or ?possible? alignments. As an
additional test, we evaluated our best alignment
method using the workshop corpus of approximately
49,000 English-Romanian sentences pairs from di-
verse sources, including 248 manually aligned sen-
tence pairs designated as test data.1
1For the English-French corpus, automatic sentence align-
ment of the training data was provided by Ulrich Germann,
1
We needed annotated development data to opti-
mize certain parameters of our algorithms, and we
were concerned that the small number of sentence
pairs designated as trial data would not be enough
for this purpose. We therefore randomly split each of
the English-French and English-Romanian test data
sets into two virtually equal subsets, by randomly
ordering the test data pairs, and assigning alternate
pairs from the random order to the two subsets. We
used one of these subsets as a development set for
parameter optimization, and held out the other for a
final test set.
We report the performance of various alignment
algorithms in terms of precision, recall, and align-
ment error rate (AER) as defined by Och and Ney
(2003):
recall =
|A ? S|
|S|
precision =
|A ? P |
|A|
AER = 1?
|A ? P |+ |A ? S|
|A| + |S|
In these definitions, S denotes the set of alignments
annotated as sure, P denotes the set of alignments
annotated possible or sure, and A denotes the set of
alignments produced by the method under test. Fol-
lowing standard practice in the field, we take AER,
which is derived from F-measure, as the primary
evaluation metric that we are attempting to optimize.
Our initial experiments involve algorithms that do
not consider the positions of words in the sentences.
Thus, they are incapable of distinguishing among
multiple instances of the same word type in a sen-
tence. We will say that these methods produce word
type alignments. We compare these algorithms on
the basis of the best possible alignment of word to-
kens given an alignment of word types. We go on
to consider various ways of choosing a word token
alignment for a given word type alignment, and all
our final evaluations are conducted on the basis of
the alignment of individual word tokens.
and the hand alignments of the words in the trial and test data
were created by Franz Och and Hermann Ney (Och and Ney,
2003). The manual word alignments for the English-Romanian
test data were created by Rada Mihalcea and Ted Pedersen.
3 The Log-Likelihood-Ratio Association
Measure
We base all our association-based word-alignment
methods on the log-likelihood-ratio (LLR) statis-
tic introduced to the NLP community by Dunning
(1993). We chose this statistic because it has previ-
ously been found to be effective for automatically
constructing translation lexicons (e.g., Melamed,
2000). We compute LLR scores using the follow-
ing formula presented by Moore (2004):
LLR(f, e) =
?
f??{f,?f}
?
e??{e,?e}
C(f?, e?) log p(f?|e?)p(f?)
In this formula f and e mean that the words whose
degree of association is being measured occur in the
respective target and source sentences of an aligned
sentence pair, ?f and ?e mean that the correspond-
ing words do not occur in the respective sentences,
f? and e? are variables ranging over these values,
and C(f?, e?) is the observed joint count for the val-
ues of f? and e?. The probabilities in the formula
refer to maximum likelihood estimates.
Since the LLR score for a pair of words is high
if the words have either a strong positive associ-
ation or a strong negative association, we discard
any negatively associated word pairs by requiring
that p(f, e) > p(f) ? p(e). Initially, we computed
the LLR scores for all positively associated En-
glish/French word pairs in our 500K sentence pair
corpus. To reduce the memory requirements of our
algorithms we discarded any word pairs whose LLR
score was less than 1.0. This left us with 12,797,697
word pairs out of a total of 21,451,083 pairs that had
at least one co-occurrence.
4 One-to-One, Word Type Alignment
Methods
4.1 Method 1
The first set of association-based word-aligment
methods we consider permit only one-to-one align-
ments and do not take word position into account.
The simplest method we consider uses the LLR
scores to link words according to Melamed?s (2000)
?competitive linking algorithm? for aligning words
in a pair of sentences. Since competitive linking has
2
no way to distinguish one instance of a particular
word type from another, we operate with counts of
linked and unlinked instances of word types, with-
out trying to designate the particular instances the
counts refer to. This version of competitive linking
can be described as follows:
? Find the pair consisting of an English word type
and a French word type that have the highest
association score of any pair of words types that
both have remaining unlinked instances.
? Increase by 1 the count of linked occurrences
of this pair of word types, and decrease by 1
the count of unlinked instances of each of these
word types.
? Repeat until no more words can be linked.
We will refer to this version of the competitive link-
ing algorithm using LLR scores as Method 1. This
is the method that Melamed uses to generate an ini-
tial alignment that he refines by re-estimation in his
?Method A? (Melamed, 2000).
Method 1 can terminate either because one or
both sentences of the pair have no more unlinked
words, or because no association scores exist for the
remaining unlinked words. We can use this fact to
trade off recall for precision by discarding associa-
tion scores below a given threshold. Table 1 shows
the precision/recall trade-off for Method 1 on our de-
velopment set. Since Method 1 produces only word
type alignments, these recall and precision scores
are computed with respect to an oracle that makes
the best possible choice among multiple occurrences
of the same word type.2 The best (oracular) AER is
0.216, with recall of 0.840 and precision of 0.747,
occurring at an LLR threshold of 11.7.
4.2 Method 2
A disadvantage of Method 1 is that it makes align-
ment decisions for each sentence pair independently
of the decisions for the same words in other sentence
pairs. It turns out that we can improve alignment
2The oracle goes through the word type pairs in the same
order as the competitive linking algorithm, linking particular
instances of the word types. It prefers a pair that has a sure
alignment in the annotated test data to a pair that has a possible
alignment; and prefers a pair with a possible alignment to one
with no alignment.
Recall Precision Threshold
0.111 0.991 168368
0.239 0.923 71074
0.304 0.902 53286
0.400 0.838 26001
0.501 0.822 11306
0.600 0.788 4224
0.700 0.778 1141
0.800 0.765 124
0.848 0.732 1
Table 1: Recall/Precision Trade-Off for Method 1.
accuracy by biasing the alignment method towards
linking words in a given sentence that are also linked
in many other sentences. A simple way to do this
is to perform a second alignment based on the con-
ditional probability of a pair of words being linked
according to Method 1, given that they both occur in
a given sentence pair. We estimate this link proba-
bility LP as
LP (f, e) = links1(f, e)cooc(f, e)
where links
1
(f, e) is the number of times f and e
are linked according to Method 1, and cooc(f, e) is
the number of times f and e co-occur in aligned sen-
tences.3
We now define alignment Method 2 as follows:
? Count the number of links in the training cor-
pus for each pair of words linked in any sen-
tence pair by Method 1.
? Count the number of co-occurrences in the
training corpus for each pair of words linked
in any sentence pair by Method 1.
? Compute LP scores for each pair of words
linked in any sentence pair by Method 1.
? Align sentence pairs by competitive linking us-
ing LP scores.
3Melamed (1998) points out there are at least three ways to
count the number of co-ccurrences of f and e in a given sen-
tence pair if one or both of f and e have more than one occur-
rence. Based on preliminary explorations, we chose to count
the co-occurrences of f and e as the maximum of the number
of occurrences of f and the number of occurrences of e, if both
f and e occur; otherwise cooc(f, e) = 0.
3
Recall Precision Threshold
0.100 0.887 0.989
0.230 0.941 0.982
0.301 0.952 0.967
0.400 0.964 0.938
0.501 0.967 0.875
0.600 0.967 0.811
0.705 0.948 0.649
0.816 0.921 0.441
0.880 0.775 0.000
Table 2: Recall/Precision Trade-Off for Method 2.
Table 2 shows the precision/recall trade-off for
Method 2 on our development set. Again, an ora-
cle is used to choose among multiple occurrences
of the same word type. The best (oracular) AER is
0.126, with recall of 0.830 and precision of 0.913,
occurring at an LP threshold of 0.215.
4.3 Method 3
It is apparent that Method 2 performs much better
than Method 1 at any but the lowest recall levels.
However, it fails to display a monotonic relation-
ship between recall and precision as the score cut-
off threshold is tightened or loosened. This seems
to be due to the fact that the LP measure, unlike
LLR, does not discount estimates made on the basis
of little data. Thus a pair of words that has one co-
occurrence in the corpus, which is linked by Method
1, gets the same LP score of 1.0 as a pair of words
that have 100 co-occurrences in the corpus and are
linked by Method 1 every time they co-occur.
A simple method of compensating for this over-
confidence in rare events is to apply absolute dis-
counting. We will define the discounted link proba-
bility LPd similarly to LP , except that a fixed dis-
count d is subtracted from each link count:
LPd(f, e) =
links
1
(f, e) ? d
cooc(f, e)
Method 3 is then identical to Method 2, except that
LPd is used in place of LP . We determined the op-
timal value of d for our development set to be ap-
proximately 0.9, using the optimal, oracular AER as
our objective function.
Table 3 shows the precision/recall trade-off for
Method 3 on our development set, with d = 0.9
Recall Precision Threshold
0.178 1.000 0.982
0.200 0.998 0.977
0.300 0.999 0.958
0.405 0.998 0.923
0.502 0.994 0.871
0.602 0.987 0.758
0.737 0.947 0.647
0.804 0.938 0.441
0.883 0.776 0.000
Table 3: Recall/Precision Trade-Off for Method 3.
and use of an oracle to choose among multiple oc-
currences of the same word type. The best (orac-
ular) AER is 0.119, with recall of 0.827 and pre-
cision of 0.929, occurring at an LPd threshold of
0.184. This is an improvement of 0.7% absolute
in AER, but perhaps as importantly, the monotonic
trade-off between precision and recall is essentially
restored. We can see in Table 3 that we can achieve
recall of 60% on this development set with precision
of 98.7%, and we can obtain even higher precision
by sacrificing recall slightly more. With Method 2,
96.7% was the highest precision that could be ob-
tained at any recall level measured.
5 Allowing Many-to-One Alignments
It appears from the results for Methods 2 and 3 on
the development set that reasonable alignment ac-
curacy may be achievable using association-based
techniques (pending a way of selecting the best word
token alignments for a given word type alignment).
However, we can never learn any many-to-one align-
ments with methods based on competitive linking, as
either we or Melamed have used it so far.
To address this issue, we introduce the notion of
bilingual word clusters and show how iterated appli-
cations of variations of Method 3 can learn many-to-
one mappings by building up clusters incrementally.
Consider the abstract data structure to which com-
petitive linking is applied as a tuple of bags (multi-
sets). In Methods 1?3, for each sentence pair, com-
petitive linking is applied to a tuple of a bag of
French words and a bag of English words. Sup-
pose we apply Method 3 with a high LPd cut-off
threshold so that we can be confident that almost all
4
the links we produce are correct, but many French
and English words remain unlinked. We can regard
this as producing for each sentence pair a tuple of
three bags: bags of the remaining unlinked English
and French words, plus a third bag of word clusters
consisting of the linked English and French words.
To produce more complex alignments, we can then
carry out an iteration of a generalized version of
Method 3, in which competitive linking connects re-
maining unlinked English and French words to each
other or to previously derived bilingual clusters.4
As just described, the approach does not work
very well, because it tends to build clusters too of-
ten when it should produce one-to-one alignments.
The problem seems to be that translation tends to
be nearly one-to-one, especially with closely re-
lated languages, and this bias is not reflected in the
method so far. To remedy this, we introduce two bi-
ases in favor of one-to-one alignments. First, we dis-
count the LLR scores between words and clusters,
so the competitive linking pass using these scores
must find a substantially stronger association for a
given word to a cluster than to any other unlinked
word before it will link the word to the cluster. Sec-
ond, we apply the same high LPd cut-off on word-
to-cluster links that we used in the first iteration
of Method 3 to generate word-to-word links. This
leaves many unlinked words, so we apply one more
iteration of yet another modified version of Method
3 in which competitive linking is allowed to link the
remaining unlinked words to other unlinked words,
but not to clusters. We refer to this sequence of three
iterations of variations of Method 3 as Method 4.
To evaluate alignments involving clusters accord-
ing Och and Ney?s method, we translate clusters
back into all possible word-to-word alignments con-
sistent with the cluster. We found the optimal value
on the development set for the LLR discount for
clusters to be about 2000, and the optimal value for
the LPd cut-off for the first two iterations of Method
3 to be about 0.7. With these parameter values, the
best (oracular) AER for Method 4 is 0.110, with re-
call of 0.845 and precision of 0.929, occurring at a
final LPd threshold of 0.188. This is an improve-
4In principle, the process can be further iterated to build up
clusters of arbitrary size, but at this stage we have not yet found
an effective way of deciding when a cluster should be expanded
beyond two-to-one or one-to-two.
ment of 0.9% absolute in AER over Method 3, re-
sulting from an improvement of 1.7% absolute in
recall, with virtually no change in precision.
6 Token Alignment Selection Methods
Finally, we turn to the problem of selecting the best
word token alignment for a given word type align-
ment, and more generally to the incorporation of
positional information into association-based word-
alignment. We consider three token alignment se-
lection methods, each of which can be combined
with any of the word type alignment methods we
have previously described. We will therefore refer
to these methods by letter rather than number, with
a complete word token alignment method being des-
ignated by a number/letter combination.
6.1 Method A
The simplest method for choosing a word token
alignment for a given word type alignment is to
make a random choice (without replacement) for
each word type in the alignment from among the to-
kens of that type. We refer to this as Method A.
6.2 Method B
In Method B, we find the word token alignment con-
sistent with a given word type alignment that is the
most nearly mononotonic. We decide this by defin-
ing the degree of nonmonotonicity of an alignment,
and minimizing that. If more than one word token
alignment has the lowest degree of nonmonotonic-
ity, we pick one of them arbitrarily.
To compute the nonmonotonicity of a word to-
ken alignment, we arbitrarily designate one of the
languages as the source and the other as the target.
We sort the word pairs in the alignment, primarily
by source word position, and secondarily by target
word position. We then iterate through the sorted
alignment, looking only at the target word positions.
The nonmonotonicity of the alignment is defined
as the sum of the absolute values of the backward
jumps in this sequence of target word positions.
For example, suppose we have the sorted align-
ment ((1,1)(2,4)(2,5)(3,2)). The sequence of target
word positions in this sorted alignment is (1,4,5,2).
This has only one backwards jump, which is of
size 3, so that is the nonmonotonicity value for this
alignment. For a complete or partial alignment, the
5
nonmonotonicity is clearly easy to compute, and
nonmonotonicity can never be decreased by adding
links to a partial alignment. The least nonmono-
tonic alignment is found by an incremental best-
first search over partial alignments kept in a priority
queue sorted by nonmonotonicity.
6.3 Method C
Method C is similiar to Method B, but it also uses
nonmonotonicity in deciding which word types to
align. In Method C, we modify the last pass of com-
petitive linking of the word type alignment method
to stop at a relatively high score threshold, and we
compute all minimally nonmonotonic word token
alignments for the resulting word type alignment.
We then continue the final competitive linking
pass applied to word tokens rather than types, but we
select only word token links that can be added to one
of the remaining word token alignments without in-
creasing its nonmonotonicity. Specifically, for each
remaining word type pair (in order of decreasing
score) we make repeated passes through all of the
word token alignments under consideration, adding
one link between previously unlinked instances of
the two word types to each alignment where it is
possible to do so without increasing nonmonotonic-
ity, until there are no longer unlinked instances of
both word types or no more links between the two
word types can be added to any alignment without
increasing its nonmonotonicity. At the end of each
pass, if some, but not all of the alignments have had a
link added, we discard the alignments that have not
had a link added; if no alignments have had a link
added, we go on to the next word type pair. This fi-
nal competitive linking pass continues until another,
lower score threshold is reached.
6.4 Comparison of Token Alignment Selection
Methods
Of these three methods, only C has additional free
parameters, which we jointly optimized on the de-
velopment set for each of the word type alignment
methods. All other parameters were left at their op-
timal values for the oracular choice of word token
alignment.
Table 4 shows the optimal AER on the develop-
ment set, for each combination of word type align-
ment method and token alignment selection method
Oracle A B C
1 0.216 0.307 0.255 0.243
2 0.126 0.210 0.147 0.109
3 0.119 0.208 0.138 0.103
4 0.110 0.196 0.130 0.098
Table 4: Development Set AER for all Methods.
that we have described. For comparison, the ora-
cle for each of the pure word type alignment meth-
ods is added to the table as a token alignment selec-
tion method. As we see from the table, Method 4
is the best word type alignment method for every
token alignment selection method, and Method C
is the best actual token alignment selection method
for every word type alignment method. Method C
even beats the token alignment selection oracle for
every word alignment type method except Method
1. This is possible because Method C incorporates
nonmonotonicity information into the selection of
linked word types, whereas the oracle is applied af-
ter all word type alignments have been chosen.
The best combined overall method is 4C. For this
combination, the optimal value on the development
set for the first score threshold of Method C was
about 0.65 and the optimal value of the second score
threshold of Method C was about 0.075.
7 Evaluation
We computed the recall, precision, and AER on the
held-out subset of the English-French data both for
our Method 4C (using parameter values optimized
on the development subset) and for IBM Model
4, computed using Och?s Giza++ software package
(Och and Ney, 2003) trained on the same data as
Method 4C. We used the default configuration file
included with the version of Giza++ that we used,
which resulted in five iterations of Model 1, fol-
lowed by five iterations of the HMM model, fol-
lowed by five iterations of Model 4. We trained and
evaluated the models in both directions, English-to-
French and French-to-English, as well as the union,
intersection, and what Och and Ney (2003) call the
?refined? combination of the two alignments. The
results are shown in Table 5. We applied the same
evaluation methodology to the English-Romanian
data, with the results shown in Table 6.
6
Alignment Recall Precision AER
Method 4C 0.879 0.929 0.094
E ? F 0.870 0.890 0.118
F ? E 0.876 0.907 0.106
Union 0.929 0.845 0.124
Intersection 0.817 0.981 0.097
Refined 0.908 0.929 0.079
Table 5: English-French Results.
Comparison of the AER for Method 4C and IBM
Model 4 shows that, in these experiments, only the
refined combination of both directions of the Model
4 alignments outperforms our method, and only on
the English-French data (and by a relatively small
amount: 16% relative reduction in error rate). Our
existing Perl implementation of Method 4C takes
about 3.5 hours for the 500K sentence pair data
set on a standard desk top computer. It took over
8 hours to train each direction of Model 4 using
Giza++ (which is written in C++). We believe that if
our method was ported to C++, our speed advantage
over Giza++ would be substantially greater. Previ-
ous experience porting algorithms of the same gen-
eral type as Method 4C from Perl to C++ has given
us speed ups of a factor of 10 or more.
Note that we were unable to optimize the many
options and free parameters of Giza++ on the de-
velopment data, as we did with the parameters of
Method 4C, which perhaps inhibits us from drawing
stronger conclusions from these experiments. How-
ever, it was simply impractical to do so, due the time
required to re-train the Giza++ models with new set-
tings. With Method 4C, on the other hand, most of
the time is spent either in computing initial corpus
statistics that are independent of the parameters set-
tings, or in performing the final corpus alignment
once the parameters settings have been optimized.
Of the five parameters Method 4C requires, changes
to three of them took less than one hour of retrain-
ing (on the English-French data ? much less on the
English-Romanian data), and settings of the last two
need to be tested only on the small amount of anno-
tated development data, which took only a few sec-
onds. This made it possible to optimize the parame-
ters of Method 4C in a small fraction of the time that
would have been required for Giza++.
Alignment Recall Precision AER
Method 4C 0.580 0.881 0.301
E ? R 0.545 0.759 0.365
R ? E 0.549 0.741 0.370
Union 0.570 0.423 0.515
Intersection 0.180 0.901 0.820
Refined 0.584 0.759 0.328
Table 6: English-Romanian Results.
8 Related Work
The literature on measures of bilingual word asso-
ciation is too large to review thoroughly, but mostly
it concerns extracting bilingual lexicons rather than
word alignment. We discuss three previous research
efforts that seem particularly relevant here.
Gale and Church (1991) made what may be the
first application of word association to word align-
ment. Their method seems somewhat like our
Method 1B. They use a word association score di-
rectly, although they use the ?2 statistic instead of
LLR, and they consider forward jumps as well as
backward jumps in a probability model in place of
our nonmonotonicity measure. They report 61% re-
call at 95% precision on Canadian Hansards data.
Obviously, we are building directly on the work of
Melamed (2000), sharing his use of the LLR statis-
tic and adopting his competitive linking algorithm.
We diverge in other details, however. Moreover,
Melamed makes no provision for other than one-to-
one alignments, and he does not deal with the prob-
lem of turning a word type alignment into a word
token alignment. As Table 4 shows, this is crucial to
obtaining high accuracy alignments.
Finally, our work is similar to that of Cherry and
Lin (2003) in our use of the conditional probabil-
ity of a link given the co-occurrence of the linked
words. Cherry and Lin generalize this idea to in-
corporate additional features of the aligned sentence
pair into the conditioning information. The chief
difference between their work and ours, however, is
their dependence on having parses for the sentences
in one of the languages being aligned. They use this
to enforce a phrasal coherence constraint, which ba-
sically says that word alignments cannot cross con-
stituent boundaries. They report excellent alignment
7
accuracy using this approach, and one way of com-
paring our results to theirs is to say that we show it is
also possible to get good results (at least for English
and French) by using nonmonotonicity information
in place of constituency information.
9 Conclusions
The conventional wisdom in the statistical MT com-
munity has been that ?heuristic? alignment meth-
ods based on word association statistics could not be
competitive with methods that have a ?well-founded
mathematical theory that underlies their parame-
ter estimation? (Och and Ney, 2003, p. 37). Our
results seem to suggest that this is not the case.
While we would not claim to have demonstated that
association-based methods are superior to the es-
tablished approach, they certainly now appear to be
worth investigating further.
Moreover, our alignment method is faster than
standard models to train; potentially much faster if
it were re-implemented in a language like C++. Ef-
ficiency issues, especially in training, are often dis-
missed as unimportant, but one should consider sim-
ply the number of experiments that it is possible to
do in the course of system development. In our case,
for example, it was impractical to try to try to opti-
mize all the options and parameters of the Giza++
models in a reasonable amount of time, given the
computational resources at our disposal.
While the wealth of details regarding various
passes through the data in our best methods might
seem to undercut our claim of simplicity, it is impor-
tant to realize that each of our methods makes a fixed
number of passes, and each of those passes involves
a simple procedure of computing LLR scores, col-
lecting co-occurrence counts to estimate link proba-
bilities, or performing competitive linking; plus one
best first search for minimally nonmonotonic align-
ments. All these procedures are simple to under-
stand and straightforward to implement, in contrast
to some of the difficult mathematical and computa-
tional issues with the standard models.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A Probability
Model to Improve Word Alignment. In Proceed-
ings of the 41st Meeting of the Association for
Computational Linguistics, pp. 88?95, Sapporo,
Japan.
Ted Dunning. 1993. Accurate Methods for the
Statistics of Surprise and Coincidence. Compu-
tational Linguistics, 19(1):61?74.
William A. Gale and Kenneth W. Church. 1991.
Identifying Word Correspondences in Parallel
Texts. In Proceedings of the Speech and Natural
Language Workshop, pp. 152?157, Pacific Grove,
California.
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. 2003. Statistical Phrase-Based Trans-
lation. In Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL 2003), pp. 127?133,
Edmonton, Alberta, Canada.
I. Dan Melamed. 1998. Models of Co-occurrence.
University of Pennsylvania, IRCS Technical Re-
port #98-05.
I. Dan Melamed. 2000. Models of Transla-
tional Equivalence. Computational Linguistics,
26(2):221?249.
Rada Mihalcea and Ted Pedersen. 2003. An Evalu-
ation Exercise for Word Alignment. In Proceed-
ings of the HLT-NAACL 2003 Workshop, Building
and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pp. 1?6, Edmonton, Al-
berta, Canada.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Proceed-
ings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing, pp. 333?
340, Barcelona, Spain.
Franz Joseph Och and Hermann Ney. 2003.
A Systematic Comparison of Various Statistical
Alignment Models. Computational Linguistics,
29(1):19?51.
8
Proceedings of the Second Workshop on Statistical Machine Translation, pages 112?119,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Iteratively-Trained Segmentation-Free Phrase Translation Model
for Statistical Machine Translation
Robert C. Moore Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,chrisq}@microsoft.com
Abstract
Attempts to estimate phrase translation
probablities for statistical machine transla-
tion using iteratively-trained models have
repeatedly failed to produce translations as
good as those obtained by estimating phrase
translation probablities from surface statis-
tics of bilingual word alignments as de-
scribed by Koehn, et al (2003). We pro-
pose a new iteratively-trained phrase trans-
lation model that produces translations of
quality equal to or better than those pro-
duced by Koehn, et al?s model. Moreover,
with the new model, translation quality de-
grades much more slowly as pruning is tigh-
tend to reduce translation time.
1 Introduction
Estimates of conditional phrase translation probabil-
ities provide a major source of translation knowl-
edge in phrase-based statistical machine translation
(SMT) systems. The most widely used method for
estimating these probabilities is that of Koehn, et
al. (2003), in which phrase pairs are extracted from
word-aligned bilingual sentence pairs, and their
translation probabilities estimated heuristically from
surface statistics of the extracted phrase pairs. We
will refer to this approach as ?the standard model?.
There have been several attempts to estimate
phrase translation probabilities directly, using gen-
erative models trained iteratively on a parallel cor-
pus using the Expectation Maximization (EM) algo-
rithm. The first of these models, that of Marcu and
Wong (2002), was found by Koehn, et al (2003),
to produce translations not quite as good as their
method. Recently, Birch et al (2006) tried the
Marcu and Wong model constrained by a word
alignment and also found that Koehn, et al?s model
worked better, with the advantage of the standard
model increasing as more features were added to the
overall translation model. DeNero et al (2006) tried
a different generative phrase translation model anal-
ogous to IBM word-translation Model 3 (Brown et
al., 1993), and again found that the standard model
outperformed their generative model.
DeNero et al (2006) attribute the inferiority of
their model and the Marcu and Wong model to a hid-
den segmentation variable, which enables the EM
algorithm to maximize the probability of the train-
ing data without really improving the quality of the
model. We propose an iteratively-trained phrase
translation model that does not require different seg-
mentations to compete against one another, and we
show that this produces translations of quality equal
to or better than those produced by the standard
model. We find, moreover, that with the new model,
translation quality degrades much more slowly as
pruning is tightend to reduce translation time.
Decoding efficiency is usually considered only in
the design and implementation of decoding algo-
rithms, or the choice of model structures to support
faster decoding algorithms. We are not aware of any
attention previously having been paid to the effect of
different methods of parameter estimation on trans-
lation efficiency for a given model structure.
The time required for decoding is of great im-
portance in the practical application of SMT tech-
112
nology. One of the criticisms of SMT often made
by adherents of rule-based machine translation is
that SMT is too slow for practical application. The
rapidly falling price of computer hardware has ame-
liorated this problem to a great extent, but the fact re-
mains that every factor of 2 improvement in transla-
tion efficiency means a factor of 2 decrease in hard-
ware cost for intensive applications of SMT, such
as a web-based translation service (?Translate this
page?). SMT surely needs all the help in can get in
this regard.
2 Previous Approaches
Koehn, et al?s (2003) method of estimating phrase-
translation probabilities is very simple. They start
with an automatically word-aligned corpus of bilin-
gual sentence pairs, in which certain words are
linked, indicating that they are translations of each
other, or that they are parts of phrases that are trans-
lations of each other. They extract every possi-
ble phrase pair (up to a given length limit) that (a)
contains at least one pair of linked words, and (b)
does not contain any words that have links to other
words not included in the phrase pair.1 In other
words, word alignment links cannot cross phrase
pair boundaries. Phrase translation probabilities are
estimated simply by marginalizing the counts of
phrase instances:
p(x|y) =
C(x, y)
?
x? C(x?, y)
This method is used to estimate the conditional
probabilities of both target phrases give source
phrases and source phrases given target phrases.
In contrast to the standard model, DeNero, et al
(2006) estimate phrase translation probabilities ac-
cording to the following generative model:
1. Begin with a source sentence a.
2. Stochastically segment a into some number of
phrases.
3. For each selected phrase in a, stochastically
choose a phrase position in the target sentence
b that is being generated.
1This method of phrase pair extraction was originally de-
scribed by Och et al (1999).
4. For each selected phrase in a and the corre-
sponding phrase position in b, stochastically
choose a target phrase.
5. Read off the target sentence b from the se-
quence of target phrases.
DeNero et al?s analysis of why their model per-
forms relatively poorly hinges on the fact that the
segmentation probabilities used in step 2 are, in
fact, not trained, but simply assumed to be uniform.
Given complete freedom to select whatever segmen-
tation maximizes the likelihood of any given sen-
tence pair, EM tends to favor segmentations that
yield source phrases with as few occurrences as pos-
sible, since more of the associated conditional prob-
ability mass can be concentrated on the target phrase
alignments that are possible in the sentence at hand.
Thus EM tends to maximize the probability of the
training data by concentrating probability mass on
the rarest source phrases it can construct to cover
the training data. The resulting probability estimates
thus have less generalizability to unseen data than
if probability mass were concentrated on more fre-
quently occurring source phrases.
3 A Segmentation-Free Model
To avoid the problem identified by DeNero et al,
we propose an iteratively-trained model that does
not assume a segmentation of the training data into
non-overlapping phrase pairs. We refer to our model
as ?iteratively-trained? rather than ?generative? be-
cause we have not proved any of the mathematical
properties usually associated with generative mod-
els; e.g., that the training procedure maximizes the
likelihood of the training data. We will motivate
the model, however, with a generative story as to
how phrase alignments are produced, given a pair of
source and target sentences. Our model extends to
phrase alignment the concept of a sentence pair gen-
erating a word alignment developed by Cherry and
Lin (2003).
Our model is defined in terms of two stochastic
processes, selection and alignment, as follows:
1. For each word-aligned sentence pair, we iden-
tify all the possible phrase pair instances ac-
cording to the criteria used by Koehn et al
113
2. Each source phrase instance that is included in
any of the possible phrase pair instances inde-
pendently selects one of the target phrase in-
stances that it forms a possible phrase pair in-
stance with.
3. Each target phrase instance that is included in
any of the possible phrase pair instances inde-
pendently selects one of the source phrase in-
stances that it forms a possible phrase pair in-
stance with.
4. A source phrase instance is aligned to a target
phrase instance, if and only if each selects the
other.
Given a set of selection probability distributions
and a word-aligned parallel corpus, we can eas-
ily compute the expected number of alignment in-
stances for a given phrase pair type. The probability
of a pair of phrase instances x and y being aligned is
simply ps(x|y) ? ps(y|x), where ps is the applica-
ble selection probability distribution. The expected
number of instances of alignment, E(x, y), for the
pair of phrases x and y, is just the sum of the align-
ment probabilities of all the possible instances of
that phrase pair type.
From the expected number of alignments and the
total number of occurrences of each source and tar-
get phrase type in the corpus (whether or not they
particpate in possible phrase pairs), we estimate the
conditional phrase translation probabilities as
pt(y|x) =
E(x, y)
C(x)
, pt(x|y) =
E(x, y)
C(y)
,
where E denotes expected counts, and C denotes
observed counts.
The use of the total observed counts of particu-
lar source and target phrases (instead of marginal-
ized expected joint counts) in estimating the condi-
tional phrase translation probabilities, together with
the multiplication of selection probabilities in com-
puting the alignment probability of particular phrase
pair instances, causes the conditional phrase transla-
tion probability distributions generally to sum to less
than 1.0. We interpret the missing probability mass
as the probability that a given word sequence does
not translate as any contiguous word sequence in the
other language.
We have seen how to derive phrase translation
probabilities from the selection probabilities, but
where do the latter come from? We answer this
question by adding the following constraint to the
model:
The probabilty of a phrase y selecting a
phrase x is proportional to the probability
of x translating as y, normalized over the
possible non-null choices for x presented
by the word-aligned sentence pair.
Symbolically, we can express this as
ps(x|y) =
pt(y|x)
?
x? pt(y|x?)
where ps denotes selection probability, pt denotes
translation probability, and x? ranges over the phrase
instances that could possibly align to y. We are, in
effect, inverting and renormalizing translation prob-
abilities to get selection probabilities. The reason
for the inversion may not be immediately apparent,
but it in fact simply generalizes the e-step formula
in the EM training for IBM Model 1 from words to
phrases.
This model immediately suggests (and, in fact,
was designed to suggest) the following EM-like
training procedure:
1. Initialize the translation probability distribu-
tions to be uniform. (It doesn?t matter at this
point whether the possibility of no translation
is included or not.)
2. E step: Compute the expected phrase alignment
counts according to the model, deriving the se-
lection probabilities from the current estimates
of the translation probabilities as described.
3. M step: Re-estimate the phrase translation
probabilities according to the expected phrase
alignment counts as described.
4. Repeat the E and M steps, until the desired de-
gree of convergence is obtained.
We view this training procedure as iteratively try-
ing to find a set of phrase translation probabilities
that satisfies all the constraints of the model, al-
though we have not proved that this training proce-
dure always converges. We also have not proved that
114
the procedure maximizes the likelihood of anything,
although we find empirically that each iteration de-
creases the conditional entropy of the phrase trans-
lation model. In any case, the training procedure
seems to work well in practice. It is also very simi-
lar to the joint training procedure for HMM word-
alignment models in both directions described by
Liang et al (2006), which was the original inspira-
tion for our training procedure.
4 Experimental Set-Up and Data
We evaluated our phrase translation model com-
pared to the standard model of Koehn et al in the
context of a fairly typical end-to-end phrase-based
SMT system. The overall translation model score
consists of a weighted sum of the following eight ag-
gregated feature values for each translation hypoth-
esis:
? the sum of the log probabilities of each source
phrase in the hypothesis given the correspond-
ing target phrase, computed either by our
model or the standard model,
? the sum of the log probabilities of each tar-
get phrase in the hypothesis given the corre-
sponding source phrase, computed either by
our model or the standard model,
? the sum of lexical scores for each source phrase
given the corresponding target phrase,
? the sum of lexical scores for each target phrase
given the corresponding source phrase,
? the log of the target language model probability
for the sequence of target phrases in the hypoth-
esis,
? the total number of words in the target phrases
in the hypothesis,
? the total number of source/target phrase pairs
composing the hypothesis,
? the distortion penalty as implemented in the
Pharaoh decoder (Koehn, 2003).
The lexical scores are computed as the (unnor-
malized) log probability of the Viterbi alignment for
a phrase pair under IBM word-translation Model 1
(Brown et al, 1993). The feature weights for the
overall translation models were trained using Och?s
(2003) minimum-error-rate training procedure. The
weights were optimized separately for our model
and for the standard phrase translation model. Our
decoder is a reimplementation in Perl of the algo-
rithm used by the Pharaoh decoder as described by
Koehn (2003).2
The data we used comes from an English-French
bilingual corpus of Canadian Hansards parliamen-
tary proceedings supplied for the bilingual word
alignment workshop held at HLT-NAACL 2003
(Mihalcea and Pedersen, 2003). Automatic sentence
alignment of this data was provided by Ulrich Ger-
mann. We used 500,000 sentences pairs from this
corpus for training both the phrase translation mod-
els and IBM Model 1 lexical scores. These 500,000
sentence pairs were word-aligned using a state-of-
the-art word-alignment method (Moore et al, 2006).
A separate set of 500 sentence pairs was used to train
the translation model weights, and two additional
held-out sets of 2000 sentence pairs each were used
as test data.
The two phrase translation models were trained
using the same set of possible phrase pairs extracted
from the word-aligned 500,000 sentence pair cor-
pus, finding all possible phrase pairs permitted by
the criteria followed by Koehn et al, up to a phrase
length of seven words. This produced approximately
69 million distinct phrase pair types. No pruning of
the set of possible phrase pairs was done during or
before training the phrase translation models. Our
phrase translation model and IBM Model 1 were
both trained for five iterations. The training pro-
cedure for our phrase translation model trains mod-
els in both directions simultaneously, but for IBM
Model 1, models were trained separately in each di-
rection. The models were then pruned to include
only phrase pairs that matched the source sides of
the small training and test sets.
5 Entropy Measurements
To verify that our iterative training procedure was
behaving as expected, after each training iteration
2Since Perl is a byte-code interpreted language, absolute de-
coding times will be slower than with the standard machine-
language-compiled implementation of Pharaoh, but relative
times between models should be comparable.
115
we measured the conditional entropy of the model
in predicting English phrases given French phrases,
according to the formula
H(E|F ) =
?
f
p(f)
?
e
pt(e|f) log2 pt(e|f),
where e and f range over the English and French
phrases that occur in the extracted phrase pairs, and
p(f) was estimated according to the relative fre-
quency of these French phrases in a 2000 sentence
sample of the French sentences from the 500,000
word-aligned sentence pairs. Over the five train-
ing iterations, we obtained a monotonically decreas-
ing sequence of entropy measurements in bits per
phrase: 1.329, 1.177, 1.146, 1.140, 1.136.
We also compared the conditional entropy of the
standard model to the final iteration of our model,
estimating p(f) using the first of our 2000 sentence
pair test sets. For this data, our model measured 1.38
bits per phrase, and the standard model measured
4.30 bits per phrase. DeNero et al obtained corre-
sponding measurements of 1.55 bits per phrase and
3.76 bits per phrase, for their model and the stan-
dard model, using a different data set and a slightly
different estimation method.
6 Translation Experiments
We wanted to look at the trade-off between decod-
ing time and translation quality for our new phrase
translation model compared to the standard model.
Since this trade-off is also affected by the settings of
various pruning parameters, we compared decoding
time and translation quality, as measured by BLEU
score (Papineni et al 2002), for the two models on
our first test set over a broad range of settings for the
decoder pruning parameters.
The Pharaoh decoding algorithm, has five pruning
parameters that affect decoding time:
? Distortion limit
? Translation table limit
? Translation table threshold
? Beam limit
? Beam threshold
The distortion limit is the maximum distance al-
lowed between two source phrases that produce ad-
jacent target phrases in the decoder output. The dis-
tortion limit can be viewed as a model parameter,
as well as a pruning paramter, because setting it to
an optimum value usually improves translation qual-
ity over leaving it unrestricted. We carried out ex-
periments with the distortion limit set to 1, which
seemed to produce the highest BLEU scores on our
data set with the standard model, and also set to 5,
which is perhaps a more typical value for phrase-
based SMT systems. Translation model weights
were trained separately for these two settings, be-
cause the greater the distortion limit, the higher the
distortion penalty weight needed for optimal trans-
lation quality.
The translation table limit and translation table
threshold are applied statically to the phrase trans-
lation table, which combines all components of the
overall translation model score that can be com-
puted for each phrase pair in isolation. This in-
cludes all information except the distortion penalty
score and the part of the language model score that
looks at n-grams that cross target phrase boundaries.
The translation table limit is the maximum number
of translations allowed in the table for any given
source phrase. The translation table threshold is
the maximum difference in combined translation ta-
ble score allowed between the highest scoring trans-
lation and lowest scoring translation for any given
source phrase. The beam limit and beam threshold
are defined similarly, but they apply dynamically to
the sets of competing partial hypotheses that cover
the same number of source words in the beam search
for the highest scoring translation.
For each of the two distortion limits we tried, we
carried out a systematic search for combinations of
settings of the other four pruning parameters that
gave the best trade-offs between decoding time and
BLEU score. Starting at a setting of 0.5 for the
threshold parameters3 and 5 for the limit parameters
we performed a hill-climbing search over step-wise
relaxations of all combinations of the four parame-
3We use difference in weighted linear scores directly for
our pruning thresholds, whereas the standard implementation of
Pharaoh expresses these as probability ratios. Hence the specific
values for these parameters are not comparable to published de-
scriptions of experiments using Pharaoh, although the effects of
pruning are exactly the same.
116
30.2
30.3
30.4
30.5
0.1 1 10 100
B
LE
U
[%
]
milliseconds per word
Figure 1: BLEU vs Decoding Time (DL = 1)
re-estimated phrase table
standard phrase table
re-estimated phrase table 
convex hull
standard phrase table convex 
hull
29.4
29.6
29.8
30
30.2
30.4
30.6
1 10 100 1000
B
LE
U
[%
]
milliseconds per word
Figure 2: BLEU vs Decoding Time (DL = 5)
re-estimated phrase table
standard phrase table
re-estimated phrase table 
convex hull
standard phrase table convex 
hull
ters, incrementing the threshold parameters by 0.5
and the limit parameters by 5 at each step. For each
resulting point that provided the best BLEU score yet
seen for the amount of decoding time used, we iter-
ated the search.
The resulting possible combinations of BLEU
score and decoding time for the two phrase trans-
lation models are displayed in Figure 1, for a distor-
tion limit of 1, and Figure 2, for a distortion limit
of 5. BLEU score is reported on a scale of 1?100
(BLEU[%]), and decoding time is measured in mil-
liseconds per word. Note that the decoding time axis
is presented on a log scale.
The points that represent pruning parameter set-
tings one might consider using in a practical system
are those on or near the upper convex hull of the
set of points for each model. These upper-convex-
hull points are highlighted in the figures. Points far
from these boundaries represent settings of one or
more of the parameters that are too restrictive to ob-
tain good translation quality, together with settings
of other parameters that are too permissive to obtain
good translation time.
Examining the results for a distortion limit of
1, we found that the BLEU score obtained with
the loosest pruning parameter settings (2.5 for both
threshold paramters, and 25 for both limit parame-
ters) were essentially identical for the two mod-
els: 30.42 BLEU[%]. As the pruning parameters
are tightened to reduce decoding time, however,
the new model performs much better. At a decod-
ing time almost 6 times faster than for the settings
that produced the highest BLEU score, the change
in score was only ?0.07 BLEU[%] with the new
model. To obtain a slightly worse4 BLEU score
(?0.08 BLEU[%]) using the standard model took
90% more decoding time.
It does appear, however, that the best BLEU score
for the standard model is slightly better than the best
BLEU score for the new model: 30.43 vs. 30.42.
It is in fact currious that there seem to be numer-
ous points where the standard model gets a slightly
4Points on the convex hulls with exactly comparable BLEU
scores do not often occur.
117
better BLEU score than it does with with the loos-
est pruning settings, which should have the lowest
search error.
We conjectured that this might be an artifact of
our test procedure. If a model is at all reasonable,
most search errors will reduce the ultimate objec-
tive function, in our case the BLEU score, but oc-
casionally a search error will increase the objective
function just by chance. The smaller the number of
search errors in a particular test, the greater the like-
lihood that, by chance, more search errors will in-
crease the objective function than decrease it. Since
we are sampling a fairly large number of combi-
nations of pruning parameter settings (179 for the
standard model with a distortion limit of 1), it is
possible that a small number of these have more
?good? search errors than ?bad? search errors sim-
ply by chance, and that this accounts for the small
number of points (13) at which the BLEU score ex-
ceeds that of the point which should have the fewest
search errors. This effect may be more pronounced
with the standard model than with the new model,
simply because there is more noise in the standard
model.
To test the hypothesis that the BLEU scores
greater than the score for the loosest pruning set-
tings simply represent noise in the data, we col-
lected all the pruning settings that produced BLEU
scores greater than or equal to the the one for the
loosest pruning settings, and evaluated the standard
model at those settings on our second held-out test
set. We then looked at the correlation between the
BLEU scores for these settings on the two test sets,
and found that it was very small and negative, with
r = ?0.099. The standard F-test for the significance
of a correlation yielded p = 0.74; in other words,
completely insignificant. This strongly suggests that
the apparent improvement in BLEU score for certain
tighter pruning settings is illusory.
As a sanity check, we tested the BLEU score cor-
relation between the two test sets for the points on
the upper convex hull of the plot for the standard
model, between the point with the fastest decod-
ing time and the point with the highest BLEU score.
That correlation was very high, with r = 0.94,
which was significant at the level p = 0.0004 ac-
cording to the F-test. Thus the BLEU score differ-
ences along most of the upper convex hull seem to
reflect reality, but not in the region where they equal
or exceed the score for the loosest pruning settings.
At a distortion limit of 5, there seems no question
that the new model performs better than the standard
model. The difference BLEU scores for the upper-
convex-hull points ranges from about 0.8 to 0.2
BLEU[%] for comparable decoding times. Again,
the advantage of the new model is greater at shorter
decoding times. Compared to the results with a dis-
tortion limit of 1, the standard model loses transla-
tion quality, with a change of about ?0.2 BLEU[%]
for the loosest pruning settings, while the new model
gains very slightly (+0.04 BLEU[%]).
7 Conclusions
This study seems to confirm DeNero et al?s diagno-
sis that the main reason for poor performance of pre-
vious iteratively-trained phrase translation models,
compared to Koehn et al?s model, is the effect of the
hidden segmentation variable in these models. We
have developed an iteratively-trained phrase transla-
tion model that is segmentation free, and shown that,
at a minimum, it eliminates the shortfall in BLEU
score compared to the standard model. With a larger
distortion limit, the new model produced transla-
tions with a noticably better BLEU score.
From a practical point of view, the main result
is probably that BLEU score degrades much more
slowly with our model than with the standard model,
when the decoding search is tuned for speed. For
some settings that appear reasonable, this difference
is close to a factor of 2, even if there is no differ-
ence in the translation quality obtainable when prun-
ing is loosened. For high-demand applications like
web page translation, roughly half of the investment
in translation servers could be saved while provid-
ing this level of translation quality with the same re-
sponse time.
Acknowledgement
The authors would like to thank Mark Johnson for
many valuable discussions of how to analyze and
present the results obtained in this study.
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constrain-
118
ing the Phrase-Based, Joint Probability Statistical
Translation Model. In Proceedings of the HLT-
NAACL 06 Workshop, Statistical Machine Trans-
lation, pp. 154?157, New York City, New York,
USA.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A Probabil-
ity Model to Improve Word Alignment. In Pro-
ceedings of the 41st Annual Meeting of the ACL,
pp. 88?95, Sapporo, Japan.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why Generative Phrase Models
Underperform Surface Heuristics. In Proceed-
ings of the HLT-NAACL 06 Workshop, Statistical
Machine Translation, pp. 31?38, New York City,
New York, USA.
Philipp Koehn. 2003. Noun Phrase Translation.
PhD Dissertation, Computer Science, University
of Southern California, Los Angeles, California,
USA.
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. 2003. Statistical Phrase-Based Trans-
lation. In Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, pp. 127?133, Edmonton, Alberta,
Canada.
Percy Liang, Ben Taskar, and Dan Klein. 2006.
Alignment by Agreement. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics, pp. 104?111, New
York City, New York, USA.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proceedings of the 2002
Conference on Empirical Methods in Natural
Language Processing, pp. 133?139, Philadelphia,
Pennsylvania, USA.
Rada Mihalcea and Ted Pedersen. 2003. An Evalu-
ation Exercise for Word Alignment. In Proceed-
ings of the HLT-NAACL 2003 Workshop, Building
and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pp. 1?6, Edmonton, Al-
berta, Canada.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved Discriminative Bilingual Word
Alignment. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pp. 513-520, Sydney,
Australia.
Franz Joseph Och, Christoff Tillmann, and Hermann
Ney. 1999. Improved Alignment Models for Sta-
tistical Machine Translation. In Proceedings of
the 1999 Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and
Very Large Corpora, pp. 20?28, College Park,
Maryland, USA.
Franz Joseph Och. 2003. Minimum Error Rate
Training in Statistical Machine Translation. In
Proceedings of the 41st Annual Meeting of the
ACL, pp. 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 311?
318, Philadelphia, Pennsylvania, USA.
119
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 41?42,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
What Do Computational Linguists Need to Know about Linguistics?
Robert C. Moore
Microsoft Research
Redmond, Washington, USA
bobmoore@microsoft.com
Abstract
In this position paper, we argue that al-
though the data-driven, empirical para-
digm for computational linguistics seems
to be the best way forward at the moment,
a thorough grounding in descriptive lin-
guistics is still needed to do competent
work in the field. Examples are given of
how knowledge of linguistic phenomena
leads to understanding the limitations of
particular statistical models and to better
feature selection for such models.
Over the last twenty years, the field of com-
putational linguistics has undergone a dramatic
shift in focus from hand encoding linguistic facts
in computer-oriented formalisms to applying sta-
tistical analysis and machine learning techniques
to large linguistic corpora. Speaking as someone
who has worked with both approaches, I believe
that this change has been largely for the good, but
I do not intend to argue that point here. Instead, I
wish to consider what computational linguists (if it
is still appropriate to call them that) need to know
about linguistics, in order to work most produc-
tively within the current data-driven paradigm.
My view is that, while computational linguists
may not need to know the details of particular lin-
guistic theories (e.g., minimalism, LFG, HPSG),
they do need to have an extensive understanding of
the phenomena of language at a descriptive level.
I can think of at least two somewhat distinct ap-
plications of this sort of knowledge in empirical
computational linguistics.
One application is to understand the structural
limitations of particular types of statistical models.
For example, a descriptive generalization about
language is that coordinated structures tend to be
interpreted in such a way as to maximize structural
parallelism. Thus, in the phrase ?young men and
women?, ?young? would normally be interpreted
as applying to both ?men? and ?women?, but in
the phrase ?young men and intelligent women?,
?young? would normally be interpreted as apply-
ing only to ?men?. Although both interpreta-
tions are structurally possible for both phrases, the
preferred interpretations are the ones that maxi-
mize structural parallelism. This is a phenom-
enon that is not describable in a general way in
a simple statistical model in the form of a proba-
bilistic context-free grammar (PCFG). We could
enumerate many specific cases by making fine-
grained distinctions in the nonterminals of the
grammar, but the tendency to favor parallel coordi-
nated structures in general would not be expressed.
This is not necessarily fatal to successful engi-
neering applications of PCFGs, but a competent
computational linguist should understand what the
limitations of the formalism are.
Let me give another example from the notori-
ously empirical field of statistical machine transla-
tion (SMT). At least some linguistic structure has
been creeping back into SMT recently in the form
of hierarchical translation models, many of which
can be viewed as instances of synchronous proba-
bilistic (or more generally, weighted) context-free
grammars (SPCFGs). This approach seems quite
promising, but since it is based on a bilingual ver-
sion of PCFGs, not only does it share the limi-
tations of monolingual PCFGs alluded to above,
but it also has additional structural limitations in
the kind of generalizations over types of bilingual
mappings it can model.
My favorite example of such a limitation is
the translation of constituent (i.e., ?WH?) ques-
tions between languages that move questioned
constituents to the front of the question (?WH-
movement?) and those that leave the questioned
constituents in situ. English is an example of the
former type of language, and Chinese (so I am
told) is an example of the latter. If we wanted to
make a model of question translation from Chi-
41
nese to English, we would like it to represent in
a unitary (or at least finitary) way the generaliza-
tion, ?Translate the questioned constituent from
Chinese to English and move it to the front of the
English sentence being constructed.? This gener-
alization cannot be expressed in an SPCFG, be-
cause this type of model allows reordering to take
place only among siblings of the same parent in
the constituent structure. Fronting a questioned
constituent, however, typically requires moving
an embedded constituent up several levels in the
constituent structure. While we can express spe-
cific instances of this type of movement using an
SPCFG by flattening the intervening structure, we
cannot hope to capture the generalization in full
because WH-movement in English is famously
unbounded, as in ?What translation formalism did
Moore claim to show that WH-movement could
not be modeled in??
In addition to providing a basis for understand-
ing the limitations of what phenomena various
statistical models can capture, a good knowledge
of descriptive linguistics is also very useful as
a source of features in statistical models. A
good example of this comes from acoustic mod-
eling in speech recognition. Acoustic models in
speech recognition are typically composed of se-
quences of ?phone? models, where a phone cor-
responds approximately to the linguistic unit of
a phoneme. For good recognition performance,
however, phone models need to be contextualized
according to the other phones around them. Com-
monly, ?triphone? models are used, in which a
separate model is used for each combination of
the phone preceding and following the phone be-
ing modeled. This can require over 100000 dis-
tinct models, depending on how many triphones
are possible in a given language, which creates
a sparse data problem for statistical estimation,
since many of the possible combinations are only
rarely observed.
One response to this sparse data problem is to
cluster the states of the triphone models to reduce
the number of separate models that need to be es-
timated, and an effective way to do this is to use
decision trees. Using a decision tree clustering
procedure, the set of all possible triphones is re-
cursively split on relevant features of the triphone.
At each decision point, the feature chosen for split-
ting is the one that produces the greatest improve-
ment in the resulting model. But what features
should be used in such a decision tree? I once
heard a leading speech recognition engineer say
that he chose his feature set by including all the
features he could find in the linguistic phonetics
literature. Given that feature set, the decision tree
learning procedure decided which ones to actually
use, and in what order.
The examples presented above illustrate some
of the kinds of linguistic knowledge that a compe-
tent computational linguist needs to know in order
perform research at the highest level. I am con-
cerned that many of the students currently grad-
uating in the field do not seem to have received
sufficient exposure to the structure of language at
this level of detail. For instance, a few years ago
I pointed out the problem of modeling question
translation between Chinese and English to one
of the brightest young researchers working with
SPCFGs, and the problem had never occurred to
him, even though he was a fluent speaker of both
languages. I am sure this would be one of the
first things that would occur to anyone brought
up on the debates of the 1980s about the limi-
tations of context-free grammar, upon first expo-
sure to the SPCFG formalism. So, although I am
a firm believer that the data-driven empirical ap-
proach computational linguistics will remain the
most fruitful research paradigm for the foresee-
able future, I also think that researchers need a
firm grounding in descriptive linguistics.
42
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1165?1176, Dublin, Ireland, August 23-29 2014.
Fast High-Accuracy Part-of-Speech Tagging by Independent Classifiers
Robert C. Moore
Google Inc.
bobmoore@google.com
Abstract
Part-of-speech (POS) taggers can be quite accurate, but for practical use, accuracy often has to
be sacrificed for speed. For example, the maintainers of the Stanford tagger (Toutanova et al.,
2003; Manning, 2011) recommend tagging with a model whose per tag error rate is 17% higher,
relatively, than their most accurate model, to gain a factor of 10 or more in speed. In this paper,
we treat POS tagging as a single-token independent multiclass classification task. We show that
by using a rich feature set we can obtain high tagging accuracy within this framework, and by
employing some novel feature-weight-combination and hypothesis-pruning techniques we can
also get very fast tagging with this model. A prototype tagger implemented in Perl is tested and
found to be at least 8 times faster than any publicly available tagger reported to have comparable
accuracy on the standard Penn Treebank Wall Street Journal test set.
1 Introduction
Part-of-speech (POS) tagging remains an important basic task in natural-language processing, often being
used as an initial step in addressing more complex problems such as parsing (e.g., McDonald et al.,
2005) or named-entity recognition (e.g., Florian et al., 2003). State-of-the-art-taggers typically employ
discriminatively-trained models with hidden tag-sequence features. These models include features of the
observable input sequence, plus hidden features consisting of tag sequences up to some fixed length.
With a tag-sequence model, the highest scoring tagging for an input sentence can be found by the
Viterbi algorithm, but exact search can be slow with a large tag set. If tri-tag features are used, the full
search space is O(|T |
3
n), where |T | is the size of the tag set and n is the length of the sentence. For the
English Penn Treebank (Marcus et al., 1993) , |T | = 45, hence |T |
3
= 91125. For efficiency, some form
of approximate search is normally used. For example, both Shen et al. (2007) and Huang et al. (2012)
use approximate search in both training and tagging. Shen et al. use a specialized bi-directional beam
search in which the search order is learned at training time and applied at tagging time, along with the
model. Huang et al. use a more conventional left-to-right beam search, but they explore various special
variants of the perceptron algorithm to cope with search errors during model training. These two taggers
represent the current state of the art on the Penn Treebank Wall Street Journal (WSJ) corpus, for models
trained using no additional resources, as measured on the standard training/development/test data split
introduced by Collins (2002a): 2.67% per tag error for Shen et al., and 2.65% for Huang et al.
Alternatively, one may omit hidden tag-sequence features, enrich the set of observable features, and
treat tagging each token as an independent multi-class classification problem. Toutanova et al. (2003)
were the first to note that such models could achieve fairly high accuracy for POS tagging, reporting
per-tag error of 3.43% on the standard WSJ development set. Liang et al. (2008) report 3.2% error on
the standard WSJ test set (using a slightly smaller than standard training set), which as far as we know
is the current state of the art for WSJ POS tagging by independent classifiers. The independent classifier
approach has the advantage of a simple model structure with a search space for tagging of O(|T |n). On
the other hand, while Liang et al.?s result would have been state-of-the-art before Collins (2002a), today
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1165
it represents an error rate about 20% higher than Huang et al.?s best result for tri-tag-based POS tagging,
under similar training conditions.
In the first part of this paper, we introduce new features for tagging by independent classifiers. We
introduce case-insensitive versions of several standard types of features, which enables our models to
generalize over different casings of the same underlying word. We also cluster the vocabulary of the
annotated training set, preserving as much information as possible about the tag probabilities for each
word, and use sequences of the resulting classes to approximate the contextual information provided
by hidden tri-tag features. With the further addition of another set of word-class features based on
distributional similarity over a large corpus of unnanotated data, we obtain a model with a WSJ test set
error of 2.66% (97.34% accuracy).
In the remainder of the paper, we show how to perform fast tagging with this model. Even with
the simple structure of an independent multiclass classifer, tagging can be slow with a rich model and
a large tag set, simply because feature extraction and model scoring take so much time. We address
this in two ways. First we effectively reduce the number of features that have to be considered for a
given token by combining the feature weights for more general features into those for more specific
features. For example, if a word is in the training set vocabulary, none of its sublexical features need
to be extracted or scored, if the weights of those features have already been combined into the weights
for the corresponding ?whole word? feature. Second, we limit the number of tags considered for each
token by a pruning method that refines Ratnaparkhi?s (1996) tag dictionary, employing a Kneser-Ney-
smoothed probability distribution over the possible tags for each word, and applying a threshold tuned
to reduce the number of tags considered while minimizing loss of accuracy. We have implemented a
prototype tagger in Perl using these methods, which we find to be at least 8 times faster than any of the
publicly available taggers reported to have comparable accuracy on the standard WSJ test set.
2 Models for Tagging by Independent Classifiers
We formulate the POS-tagging task as a linear multiclass classification problem defined by a set of tags
T and a set of indicator features F . Each training example consists of a set of features f ? F present in
that example and a correct tag t ? T . The feature set f for a particular example consists of observable
properties of the token to be tagged and the tokens surrounding it. A model is a vector w ? <
|T |?|F|
indexed by feature-tag pairs. We refer to the coordinates w
(f,t)
of w as feature weights. A model w
maximizes the sum of relevant feature weights to predict a tag t(f ,w):
t(f ,w) = argmax
t?T
?
f?f
w
(f,t)
(1)
In the remainder of this section we explain the feature sets we use and our method of training feature
weights, and we evaluate the accuracy of the resulting models on the usual Wall Street Journal corpus
from Penn Treebank III (Marcus et al., 1993).
2.1 Lexical Features
As noted above, the current state of the art for tagging by independent classifiers seems to be the results
presented by Liang et al. (2008). Their best model uses the following set of base features for each word:
Whether the first character of the word is a capital letter
Prefixes of the word up to three characters
Suffixes of the word up to three characters
Two ?shape? features described below
The full word
For each base feature, Liang et al. define three expanded features: whether the token being tagged has the
base feature, whether the preceding token has the base feature, and whether the following token has the
base feature. The shape features were first introduced by Collins (2002b) for named-entity recognition.
What we will call the ?Shape 1? feature is a generalization of the spelling of the word with all capital
1166
letters treated as equivalent, all lower-case letters treated as equivalent, and all digits treated as equivalent.
All other characters are treated as distinct. In the ?Shape 2? feature, all sequences of capital letters, all
sequences of lower case letters, and all sequences of digits are treated as equivalent, regardless of the
length of the sequence or the identity of the upper case letters, lower case letters, or digits.
With this feature set as our starting point, and partially drawing from the feature sets of Ratnaparkhi
(1996) and Collins (2002a), we settled on the following set of base features through experimentation on
the WSJ development set:
Whether the word contains a capital letter
Whether the word contains a digit
Whether the word contains a hyphen
Lower-cased prefixes of the word up to four characters
Lower-cased suffixes of the word up to four characters
The Shape 1 feature for the word
The Shape 2 feature for the word
The full lower-cased word
The full word
A distributional-similarity-based class for the full word
In all these features we ignore distinctions among digits (rather than just in the shape features, as Liang
et al. do). For the last feature, we used 256 word classes derived by unsupervised clustering for the most
frequent 999996 distinct tokens (ignoring distinctions among digits) in 121.6 billion tokens of English-
language newswire, using the method of Uszkoreit and Brants (2008). A 257th class was added for
tokens not found in this set. We use Liang et al.?s mapping of all base features into expanded features for
the token being tagged, the preceding token, and the following token. For the first token of a sentence we
include a beginning-of-sentence feature in place of the preceding-token features, and for the last token
of a sentence we include an end-of-sentence feature in place of the following-token features.
2.2 Word-Class-Sequence Features
In a hidden tri-tag model, the prediction for a particular tag t
i
is linked to the predictions for the preceding
tag t
i?1
, the following tag t
i+1
, the preceding tag pair ?t
i?2
, t
i?1
?, the following tag pair ?t
i+1
, t
i+2
?,
and the surrounding tag pair ?t
i?1
, t
i+1
?. In tagging by independent classifiers, we do not have access to
information regarding predictions for these nearby tags and tag combinations.
To substitute for these missing features, we carry out supervised clustering of the distinct words in
the training set (again ignoring distinctions among digits) into 50 classes, attempting to maximize the
information carried by each class regarding the tag probabilities for the words in the class. From these
classes, we construct the features
c(w
i?1
)
c(w
i+1
)
?c(w
i?2
), c(w
i?1
)?
?c(w
i+1
), c(w
i+2
)?
?c(w
i?1
), c(w
i+1
)?
The type of clustering we use here differs from the unsupervised clustering described previously. In
assigning each word to a cluster, the unsupervised clustering algorithm looks only at adjacent words in
unannoted data, while the supervised clustering algorithm looks only at the tags the word receives in
the annotated data. The unsupervised clustering tells us what known words a large number of unknown
words are simliar to, but the supervised clustering carries much more information about what tags the
known words are likely to receive.
2.2.1 Clustering Algorithm
Our supervised clustering algorithm is based on the method presented by Dhillon et al. (2003). This
is similar to the well-known Lloyd algorithm for k-means clustering, but uses KL-divergence between
1167
probability distributions, instead of Euclidian distance, to assign items to clusters. In our application of
this algorithm, we simply keep moving each word to the cluster that has the most similar probability
distribution over tags, and then re-estimating the tag probability distributions for the clusters, until the
clustering converges. At a high-level, our algorithm is:
? For each unique word w in the training set, estimate a smoothed probability distribution p(T |w)
over tags given w.
? Select k seed words, and initialize k clusters for clustering 0, with one seed word per cluster.
1
? Set i = 0.
? Repeat until the assignment of words to clusters in clustering i is the same as in clustering i ? 1,
returning clustering i:
? For each cluster c in clustering i, compute a probability distribution p(T |c) over tags given c,
such that
p(t|c) =
?
w?c
p(w|c)p(t|w)
? For each word w, find the cluster c that minimizes the KL-divergence D
KL
(p(T |w)||p(T |c)),
and assign w to cluster c in clustering i+ 1.
? Set i = i+ 1.
As indicated, the probability distributions p(T |c) over tags for a given cluster are computed as the
word-frequency-weighted mean of probability distributions p(T |w) over tags given the words in the
cluster. The p(T |w) distributions are estimated based on the relative frequencies of each tag for a given
word, smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1999) widely used in
statistical language modeling. (See Section 3.2 for more discussion of this smoothing method applied to
POS tag prediction.)
2.2.2 Cluster Initialization
Our clustering algorithm is identical to that of Dhillon et al., except for the method of initializing the
clusters. Their initialization method would assign all words with the same most likely tag to the same
initial cluster. Instead, we initialize the clusters using a set of seed words with the property that conflating
any two of them would result in a large loss of information about tag probabilities.
We define the distance between a pair of words (w
1
, w
2
) as the total decrease resulting from treating
w
1
and w
2
as indistinguishable, in the estimated log probability, based on p(T |W ), of the reference
tagging of the training data. Letting n
1
be the number of occurrences in the training data of w
1
, and
similarly for n
2
and w
2
, we compute the distance between w
1
and w
2
as
n
1
D
KL
(p(T |w
1
)||p
w
1
w
2
) + n
2
D
KL
(p(T |w
2
)||p
w
1
w
2
)
where p
w
1
w
2
= p(T |w
1
? w
2
), computed as
p
w
1
w
2
(t) =
n
1
n
1
+ n
2
p(t|w
1
) +
n
2
n
1
+ n
2
p(t|w
2
)
We select a set S of k seed words as follows:
? Choose a maximal subset V of the training data vocabulary, such that every word in V has a different
distribution of observed POS tags.
? Choose a random ordering of V .
? Initialize S to contain the first k words of V .
1
Note that most words in the training set are not assigned to any initial cluster.
1168
? Find the minimum distance d between any two words in S.
? Taking each remaining word w of V in order:
? Find the minimum distance d
?
between w and any word in S.
? If d
?
> d,
? Select from S a pair of words (w
?
, w
??
) separated by d.
? Find the minimum distance d
?
2
between w
?
and any word in S other than w
??
.
? Find the minimum distance d
??
2
between w
??
and any word in S other than w
?
.
? If d
?
2
< d
??
2
, remove w
?
from S, otherwise remove w
??
from S.
? Add w to S.
? Recompute the minimum distance d between any two words in S.
2.2.3 Random restarts
The clustering we find depends on the set of seed words, which in turn depends on the order in which
the words in V are enumerated to select the seed words. To ensure that we find a good clustering, we
try multiple runs of the algorithm based on different random enumerations of V , returning the clustering
yielding the lowest entropy for predicting the training set tags from the clusters.
We noticed in preliminary experiments that a poor clustering on the first iteration of the algorithm
seldom leads to a good final clustering, so we save the training set tag entropy for the first iteration of the
best clustering found so far, and we abandon a run of the algorithm if it results in higher training set tag
entropy on its first iteration than the best previously observed final clustering had on its first iteration. We
continue trying different random enumerations until a fixed number of runs has passed since the current
best clustering was found.
2.2.4 Classes for unknown words
Note that this clustering method assigns classes only to words observed in the training data. All words
(ignoring distinctions among digits) not seen in the training data are assigned to an additional class. In
training the tagging model, however, we treat each word that has a single occurrence in the training data
as a member of this unknown-word class, so that features based on that class will be seen in training; but
at tagging time, we give all words seen in the training data the class they are assigned by the clustering
algorithm, and apply the unknown-word class only to words not seen in the training data.
2.3 Feature Weight Training
Our models are trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer,
2001) using stochastic subgradient descent as described by Zhang (2004). We use a small, constant
learning rate of 2
?8
, which early in our experiments we found generally to be a good value, given the
size of our training set and the sorts of feature sets we were using. We did not re-optimize the learning
rate as we experimented with different feature sets. We do not use a numerical regularizer (such as L
1
or L
2
), but we avoid over-fitting by using early stopping, and averaging as Collins (2002a) does with
the averaged perceptron. To determine the stopping point, we evaluate the model on the development
set after each pass through the training data. We continue iterating until we have made 10 consecutive
passes through the training data without reducing the development set error, and we return the model
from the iteration with the lowest error.
2.4 Evaluation of Tagging Accuracy
We evaluate the tagging accuracy of three models: our new model with all the features discussed above,
our new model minus the unsupervised distributional clustering features (to give a ?no additional re-
sources? measurement), and the Liang et al. model that was our starting point. Our data is the ususal
Wall Street Journal corpus from Penn Treebank III (Marcus et al., 1993), split into standard training
(sections 0?18), development (sections 19?21), and test (sections 22-24) sets.
Table 1 shows WSJ development and test set error rates for all tokens and for unknown-word (OOV)
tokens for all three models. Our full model has an overall test set tag error rate of 2.66%, or 97.34%
1169
Dev Set Dev Set Test Set Test Set
Tagging Model All Tag OOV Tag All Tag OOV Tag
Error % Error % Error % Error %
Our full feature set 2.69 9.40 2.66 8.93
Our features minus unsupervised classes 2.83 10.45 2.77 10.14
Liang et al. feature set 3.23 12.47 3.17 11.92
Table 1: WSJ development and test set error rates for different feature sets
accuracy. Omitting unsupervised word-class features results in a relative increase in the error rate of
4.1% overall and 13.5% on unknown words. The model trained on the Liang et al. feature set gives
results consistent with their reported 3.2% test set error, but the error is 19.2% higher than the model
using our full feature set, and 14.4% higher than our model without unsupervised word-class features.
3 Efficient Tag Inference
Although the complexity of tag inference with our model is only O(|T |n), with a rich feature set and
many possible tags, the simple summation of feature weights and comparison of sums implied by Equa-
tion 1 can still be slow. With our full model, a given token occurrence can have up to 53 features present,
and on the WSJ development set, we measured the average number of features present with a non-zero
weight for at least one tag to be 38.0. Given 45 possible tags in the Penn Treebank tag set and our full
model, the average number of relevant non-zero feature weights per token on the WSJ development set is
1215.0. We reduce computational costs in two ways. First, we introduce a method of combining feature
weights that effectively reduces the number of features per token by a factor of 8. Then we introduce
a refined version of a tag dictionary that reduces the number of tags considered per token by a factor
of more than 12 without noticeably affecting tagging accuracy. The combination of these techniques
reduces the number of non-zero feature weights used per token by a factor of 75, which, in our Perl
implementation, speeds up tagging by a factor of 45.
3.1 Combining Feature Weights
The base lexical feature types in our model form a natural hierarchy as follows:
1. Original case tokens
1.1. Unsupervised distributional word clusters
1.2. Lower-cased tokens
1.2.1. Lower-cased 4-character prefixes
1.2.1.1. Lower-cased 3-character prefixes
1.2.1.1.1. Lower-cased 2-character prefixes
1.2.1.1.1.1. Lower-cased 1-character prefixes
1.2.2. Lower-cased 4-character suffixes
1.2.2.1. Lower-cased 3-character suffixes
1.2.2.1.1. Lower-cased 2-character suffixes
1.2.2.1.1.1. Lower-cased 1-character suffixes
1.3. Shape 1 features
1.3.1. Shape 2 features
1.3.1.1. Contains upper case token
1.3.1.2. Contains digit
1.3.1.3. Contains hyphen
The significance of the hierarchy is that the occurrence of a base feature of any of these types fully
determines which features of the types below it in the hierarchy also occur. For example, given a whole
token with its original casing, the corresponding features of all the other feature types in the hierarchy
are completely determined. Given just the lower-cased version of the token, the lower-cased prefixes
1170
and suffixes are determined, but the distributional word cluster and the shape features are not completely
determined, because they depend on capitalization.
2
We use this hierarchy to perform a simple transformation on the trained tagging model. For every
base lexical feature f found in the training data, we add to the value of each feature weight associated
with that base feature, the value of all corresponding feature weights for base features below f in the
hierarchy. For instance, to the feature weight for the 3-character suffix ion, the tag NN, and the position
-1 (i.e, the word preceding the word being tagged), we add the value of feature weight for the 2-character
suffix on, the tag NN, and the position -1, plus the value of the feature weight for the 1-character suffix
n, the tag NN, and the position -1.
To use this transformed model, we make a corresponding modification to feature extraction in the
tagger. We carry out feature extraction top-down with respect to the base feature hierarchy, and whenever
we find a base feature f for which there are any corresponding feature weights in the model, we skip
the extraction of all the base features below f in the hierarchy. We can do that because the model has
been transformed to incorporate the weights for all the skipped features into the corresponding feature
weights associated with f . The weights for the skipped features are still kept in the model, so that they
can be used when we encounter an unknown feature of the same type as f , such as an unknown whole
word, or an unknown 4-character suffix, when we have seen the corresponding 3-character suffix.
The word-class-sequence features are arranged into a similar hierarchy, which is used in a similar way.
1. ?c(w
i?2
), c(w
i?1
), c(w
i+1
), c(w
i+2
)?
1.1. ?c(w
i?2
), c(w
i?1
)?
1.2. ?c(w
i+1
), c(w
i+2
)?
1.3. ?c(w
i?1
), c(w
i+1
)?
1.3.1. c(w
i?1
)
1.3.2. c(w
i+1
)
Note that in this hierarchy, we have introduced a new feature type that does not actually exist in the trained
model, the combination of the word-class bigrams preceding and following the word being tagged. The
weights for the features of this type are constructed from the sums of the weights of other features lower
in the hierarchy. To keep the size of the transformed model from exploding, we limit the instances of
this feature type to those seen at least twice in the training data. We found this covered about 80%
of the tagging decisions for the WSJ development set. We also included in the transformed model all
possible instances (including those not observed in the training data) of the feature type 1.3 for word-
class bigrams surrounding the word being tagged, which allows us to drop the feature weights for the
lowest two feature types 1.3.1 and 1.3.2 after their feature weights have been added to the weights for
the word-class-bigram features.
Altogether, these transformations increase the size of our full model from 151,174 features with
861,111 non-zero feature weights to 392,318 features with 17,047,515 non-zero feature weights. While
this may be a substantial relative increase in size, the resulting model is still not particuarly large in
absolute terms.
Feature Weight Features Weights Tokens All Tag OOV Tag
Combination per Token per Token per Second Error % Error %
No 38.0 1215.0 1100 2.69 9.40
Yes 4.7 194.0 6400 2.69 9.40
Table 2: WSJ development set speeds and error rates without and with feature weight combination
In Table 2, we show the effect of these transformations on the speed of tagging the WSJ development
set while considering all possible labels for each token. As expected, feature weight combination has no
effect on tagging error, since it results in the same tagging decisions as the original model and feature
extraction method. The ?Features per Token? column shows the average number of features used for
2
Note that we could have placed the ?contains digit? or ?contains hyphen? features under ?lower-cased tokens? instead of
?Shape 2?; our choice here was arbitrary.
1171
each tagging decision, without and with the model and feature extraction feature-weight-combination
transformations. The transformations reduce this number by a factor of 8.13. The ?Weights per Token?
column is the corresponding number of non-zero feature weights used for each tagging decision. Feature
weight combination reduces this number by a factor of 6.26.
The ?Tokens per Second? measurements are rounded to two significant digits due to the limited preci-
sion of our observations. Time was measured to the nearest second, and for each tagger, the data set was
replicated enough times for the total tagging time to fall in the range of 100 to 200 seconds. The times
reported include only reading the sentence-split tokenized text, extracting features, and predicting tags;
time to read in model parameters and initialize the corresponding data structures is not inlcuded. Times
are for a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550
2.67 GHz processors. In this implementation, feature weight combination increases tagging throughput
by a factor of 5.82.
3.2 Pruning Possible Tags
It has long been standard practice to prune the set of possible tags considered for each word, in order to
speed up tagging. Ratnaparkhi (1996) may have been the first to use the common heuristic of defining a
tag dictionary allowing any tag for unknown words, but restricting each known word to the tags it was
observed with in the training data. In addition, the tag dictionary for known words is sometimes further
pruned (e.g., Banko and Moore, 2004; Gim?enez and M`arquez, 2004, 2012) according to the relative
frequency of tags for each word. Tags observed in the training data with less than some fixed proportion
of the occurrences of a particular word are not considered as possible tags for that word in test data.
In our experiments, we find these heuristics produce fast tagging, but lead to a noticable loss of ac-
curacy, because known words are never allowed to be labeled with tags they were not observed with in
the training data. This is similar to the problem of unseen n-grams in statistical language modeling, so
we apply methods developed in that field to the problem of dictionary pruning for POS tagging. We
construct our tag dictionary based on a ?bigram? model of the probability p(t|w) of a tag t given a word
w, estimated from the annotated training data. The probabilities for tags that have never been seen with
a given word, as well as all the tag probabilities for unknown words, are estimated by interpolation with
a ?unigram? distribution over the tags.
To estimate the probabilities of tags given words, we use the same interpolated-Kneser-Ney-smoothed
(Chen and Goodman, 1999) model that we used in Section 2.2.1 in our supervised word-clustering pro-
cedure. In this model, we estimate the probabilty p(t|w) by interpolating a discounted relative frequency
estimate with a lower-order estimate of p(t). The lower-order estimates are based on ?diversity counts?,
taking the count of a tag t to be the number of distinct words ever observed with that tag. This has the
desirable property for POS tagging that closed-class tags receive a very low estimated probabilty of be-
ing assigned to a rare or unknown word, even though they occur very frequently with a small number of
frequent words. We use a single value for the discount parameter in the Kneser-Ney formula, chosen to
maximize the estimated probability of the reference tagging of the development set. These probabilities
are estimated ignoring distinctions among digit characters, just as in the features of our tagging model.
We construct our tag dictionary by setting a threshold on the value of p(t|w). Whenever p(t|w) is
less than or equal to the threshold, the tag t is considered not to be a possible POS tag for the word w.
Our preferred threshold (p(t|w) > 0.0005) is set to prune as agressively as possible while maintaining
tagging accuracy on the WSJ development set. This threshold is applied to both known and unknown
words, which produces 24 possible tags for unknown words by applying the threshold to the lower-order
probability estimate p(t). Note that the probabilities we use for pruning can be viewed as posteriors of
a very simple POS tagging model, which makes inferring a tag dictionary an instance of coarse-to-fine
inference with posterior pruning (Charniak et al., 2006; Weiss and Taskar, 2010).
The standard tag dictionary pruning heuristics can be viewed as a application of the same approach,
but with the p(t|w) probabilities being unsmoothed relative-frequency estimates for known words and
a uniform distribution for unknown words. The original Ratnaparkhi heuristic amounts to thresholding
these probabilities at 0, with a higher threshold being applied when using additional pruning.
1172
Tags Weights Tokens All OOV Seen Unseen OOV Seen Unseen
Pruning per per per Tag Tag Tag Tag Mean Mean Mean
Method Token Token Second Error % Error % Error % Error % Tags Tags Tags
None 45.0 194.0 6400 2.69 9.40 2.19 52.0 45.0 45.0 45.0
Ratnaparkhi 3.7 19.0 47000 2.81 9.40 2.07 100.0 45.0 2.3 1.3
Ratnaparkhi+ 2.9 14.3 56000 2.81 9.40 2.07 100.0 45.0 1.4 1.2
Kneser-Ney 3.5 16.1 49000 2.69 9.45 2.18 55.5 21.3 2.8 10.2
Kneser-Ney+ 1.8 6.1 67000 2.81 9.74 2.14 83.8 10.6 1.4 2.8
Table 3: WSJ development set speeds and error rates for different tag dictionary pruning methods
In Table 3 we compare these methods of tag dictionary pruning on the WSJ development set, when
combined with our feature-weight-combination technique. The ?Tags per Token? column shows the
average number of tags considered for each tagging decision, depending on the tag pruning method
used. ?Weights per Token?, ?Tokens per Second?, ?All Tag Error %?, and ?OOV Tag Error %? are as
in Table 2. The first line repeats the experiment with no tag dictionary pruning from Table 2. The next
line gives results for Ratnaparkhi?s dictionary pruning method, and the next line, ?Ratnaparkhi+?, gives
results for the maximum additional pruning by thresholding based on unsmoothed relative frequencies
that does not increase overall tagging error (p(t|w) > 0.005). We see that these taggers are much faster
than the unpruned tagger, but noticeably less accurate.
The final two lines of Table 3 are for our tag dictionary pruning method, with different pruning thresh-
olds. The ?Kneser-Ney? line represents our preferred threshold, set to prune as agressively as possible
without noticeably degrading the overall tagging error on the WSJ development set. This produces a
lower error rate than either Ratnaparkhi or Ratnaparkhi+ pruning, but Ratnaparkhi+ pruning results in
faster tagging. However, if we increase the pruning threshold until we match the Ratnaparkhi+ error
rate, as shown in the final ?Kneser-Ney+? line, our method is faster than Ratnaparkhi+.
The remaining columns of Table 3 provide some insight as to why Kneser-Ney-smoothed pruning
with our preferred threshold results in lower error than Ratnaparkhi and Ratnaparkhi+ pruning. The
column labeled ?Seen Tag Error %? is the error rate for examples with word/tag pairs seen in training.
The column labeled ?Unseen Tag Error %? is the error rate for examples with word/tag pairs not seen in
training, but with a word that was seen in training. There are 660 of the latter examples in the WSJ de-
velopment set, which amounts to 0.5% of that data set. By construction, the error rate of the Ratnaparkhi
and Ratnaparkhi+ pruning methods on this subset of the data is 100%, but both the unpruned tagger and
the tagger with Kneser-Ney-smoothed pruning correctly tag nearly half of these examples.
The Ratnaparkhi and Ratnaparkhi+ pruning methods are somewhat more accurate than the Kneser-
Ney-smoothed pruning method on the seen word/tag pairs and the unknown words, but not enough to
overcome the losses on the unseen word/tag pairs with known words. In absolute numbers on the WSJ
development set, both the Ratnaparkhi and Ratnaparkhi+ pruning methods make 131 fewer errors on
the seen word/tag pairs and 2 fewer errors on the unknown words, but 294 more errors on the unseen
word/tag pairs with known words, compared to Kneser-Ney-smoothed pruning method with our pre-
ferred threshold. The final three columns of Table 3 show the mean number of tags allowed by each
dictionary for these three categories of examples. Compared to Ratnaparkhi and Ratnaparkhi+ pruning,
our preferred threshold for Kneser-Ney-smoothed pruning slightly increases the number of tags consid-
ered for seen word/tag pairs, substantially reduces the number of tags considered for unknown words,
and substantially increases the number of tags considered for unseen word/tag pairs with known words.
4 Comparison to Other Taggers
We compared our tagger to several publicly available taggers, on the standard WSJ POS tagging test
set. As far as we know, six taggers have been reported to have an error rate of less than 2.7% (accuracy
greater than 97.3%) on this test set. Three of these are publicly available: the Stanford tagger (Toutanova
et al., 2003; Manning, 2011), the Prague COMPOST tagger (Spoustov?a, et al., 2009), and the UPenn
1173
bidirectional tagger (Shen et al., 2007).
3
We tested two versions of the Stanford tagger, one based
on their most accurate model ?wsj-0-18-bidirectional-distsim?, and one based on the much faster, but
less accurate model ?english-left3words-distsim? recommended for practical use on the Stanford tagger
website. The UPenn tagger is run with a beam width of 3, which is the setting that gave their best reported
results.
These taggers were all tested on on the same Linux workstation as our Perl tagger. To obtain compa-
rable speed measurements omitting time for initialization, we performed two runs with each tagger. one
on the first 1000 sentences of the test set, and another with those 1000 sentences followed by the entire
test set replicated enough times to produce a difference in total time of at least 100 seconds. The tagging
speed was inferred from the difference in these two times. The Stanford tagger reports tagging times
directly, and these agreed with our measurements to two significant digits, which is the precision limit of
our measurements.
We also report on the SVMTool tagger of Gim?enez and M`arquez (2004). Gim?enez recently provided
us with benchmarks, which he obtained with a somewhat faster processor than ours, the Intel Xeon
X5660 2.80 GHz. We give results for two versions of this tagger, one in Perl and one in C++, both with
a combination of left-to-right and right-to-left tagging, which gives higher accuracy with this tagger than
either direction by itself.
WSJ WSJ WSJ Brown Brown Brown
Tagger Implementation Tokens All Tag OOV Tag Tokens All Tag OOV Tag
Language per Second Error % Error % per Second Error % Error %
This work Perl 51000 2.66 9.02 40000 3.46 10.64
Stanford fast Java 80000 3.13 10.31 50000 4.47 12.62
Stanford accurate Java 5900 2.67 7.90 1600 3.86 11.21
COMPOST C 2600 2.57 10.03 2700 3.36 12.16
UPenn Java 270 2.67 10.39 290 3.90 12.96
SVMTool Perl 1340 2.86 11.37
SVMTool C++ 7700 2.86 11.37
Table 4: WSJ test set and Brown corpus speeds and error rates compared to publicly available taggers
Results on the WSJ test set are shown in Table 4. We include a column giving the implementation
language of each tagger to help interpret the results. Generally, we would expect an algorithm imple-
mented in Perl to be slower than the same algorithm implemented in Java, which in turn would probably
be slower than the same algorithm implemented in C/C++; although depending on the libraries used
and the degree of optimization in the compilers, Java can sometimes be competitive with C/C++ (See,
for example, http://blog.famzah.net/2010/07/01/cpp-vs-python-vs-perl-vs-
php-performance-benchmark/).
?This work? refers to our tagger with feature weight combination and Kneser-Ney-smoothed dictio-
nary pruning, with the pruning threshold set to maximize pruning without decreasing overall tagging
accuracy on the WSJ development set. The fast Stanford tagger is the fastest overall by a wide margin,
but it is also the least accurate. Our tagger is both the second fastest and the second most accurate, hav-
ing an error rate relatively 3.9% higher (absolutely 0.09% higher) than the COMPOST tagger. But our
tagger is almost 20 times faster than COMPOST, and more than 8 times faster than the accurate Stanford
tagger, the second fastest tagger of equivalent or better accuracy. This is despite the fact that our tagger
is written in Perl, while the other high-accuracy taggers are written either in Java or C.
As a final, out-of-domain evaluation, we ran the five taggers that we had direct access to on the Brown
Corpus subset (3279 sentences, 83769 tokens) from the Penn Treebank. As might be expected, tagging
was in general both slower and less accurate than on in-domain data. Our tagger maintained its relative
position with respect to both speed and accuracy compared to all the other taggers. The only qualitative
change in position of any tagger is that on the Brown Corpus data, the accurate Stanford tagger is slower
than COMPOST, which actually runs faster than it does on the WSJ test set.
3
A fourth tagger, the semi-supervised condensed nearest neighbor tagger of S?gaard (2011), has some released source code,
but not a complete tagger nor detailed instructions on how to build the tagger S?gaard evaluates.
1174
5 Conclusions
We have shown that a feature-rich model for POS tagging by independent classifiers can reach tagging
accuracies comparable to several state-of-the art taggers, and we have introduced implementation strate-
gies that result in much faster tagging than any other high-accuracy tagger we are aware of, despite these
other taggers being implemented in faster programming languages.
A number of the techniques introduced here may have applications to other tasks. The sort of word-
class-sequence models derived by supervised clustering described in Section 2.2 may be useful for other
sequence labeling tasks, such as named-entity recognition. Our method of pruning the tag dictionary
with smoothed probability distributions could also be used for label pruning for other problems with
large label sets. Finally, the feature-weight-combination technique of Section 3.1 can be applied to any
rich feature space in which the features have the kind of hierarchical structure we see in POS tagging.
Such feature spaces are common in NLP, since we are almost always dealing with lexical items and their
sublexical features.
Acknowledgements
Thanks to Chris Manning, John Bauer, and Jenny Liu for help with the Stanford tagger; to Johanka
Spoustov?a and Jan Haji?c, for help with the COMPOST tagger; to Jes?us Gim?enez, for benchmarking the
SVMTool tagger; and to Kuzman Ganchev and Dan Bikel, for valuable comments on earlier versions of
this paper.
References
Michele Banko and Robert C. Moore. 2004. Part of speech tagging in context. In Proceedings of the
20th International Conference on Computational Linguistics, August 23?27, Geneva, Switzerland,
556?561.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R. Shrivaths, Jeremy Moore, Michael Pozar, and Theresa Vu. 2006. Multilevel coarse-to-ne
PCFG parsing. In Proceedings of the Human Language Technology Conference of the North American
Chapter of the ACL, June 4?9, New York, New York, USA, 168?175.
Stanley F. Chen and Joshua T. Goodman. 1999. An empirical study of smoothing techniques for language
modeling. Computer Speech and Language, 13(4):359?393.
Michael Collins. 2002a. Discriminative training methods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, July 6?7, Philadelphia, Pennsylvania, USA, 1?8.
Michael Collins. 2002b. Ranking algorithms for named-entity extraction: boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics,
July 7?12, Philadelphia, Pennsylvania, USA, 489?486.
Koby Crammer and Yoram Singer. 2001. On the algorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Research, 2:265?292.
Radu Florian, Abe Ittycheriah, Honyan Jing, and Tong Zhang. 2003. Named entity recognition through
classifier combination. In Proceedings of CoNLL-2003, May 31?June 1, Edmonton, Alberta, Canada.
Inderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar. 2003. A divisive information-theoretic
feature clustering algorithm for text classification. Journal of Machine Learning Research, 3:1265?
1287.
Jes?us Gim?enez and Llu??s M`arquez. 2004. SVMTool: a general POS tagger generator based on support
vector machines. In Proceedings of the 4th International Conference on Language Resources and
Evaluation, May 26?28, Lisbon, Portugal, 43?46.
1175
Jes?us Gim?enez and Llu??s M`arquez. 2012. SVMTool Technical Manual v1.4. http://www.lsi.
upc.edu/
?
nlp/SVMTool/SVMTool.v1.4.pdf
Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technology, June 3?8, Montreal, Quebec, Canada, 142?151.
Percy Liang, Hal Daum?e III, and Dan Klein. 2008. Structure compilation: trading structure for fea-
tures. In Proceedings of the 25th International Conference on Machine Learning, July 5?9, Helsinki,
Finland, 592?599.
Christopher D. Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguis-
tics? In Alexander Gelbukh (ed.), Computational Linguistics and Intelligent Text Processing, 12th
International Conference, CICLing 2011, Proceedings, Part I. Lecture Notes in Computer Science
6608, Springer, 171?189.
Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd Annual Meeting of the Association of Computational
Linguistics, June 25?30, Ann Arbor, Michigan, USA, 91?98.
Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of
the Conference on Empirical Methods in Natural Language Processing, May 17?18, Philadelphia,
Pennsylvania, USA, 133?142.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007. Guided learning for bidirectional sequence clas-
sification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,
June 23?30, Prague, Czech Republic, 760?767.
Drahom??ra ?johanka? Spoustov?a, Jan Haji?c, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised
training for the averaged perceptron POS tagger. In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational Linguistics, March 30?April 3, Athens, Greece,
763?771.
Anders S?gaard. 2011. Semisupervised condensed nearest neighbor for part-of-speech tagging. In
Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics, June 19?24,
Portland, Oregon, USA, 48?52.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Human Language
Technology Conference of the North American Chapter of the Association for Computational Linguis-
tics, May 27?June 1, Edmonton, Alberta, Canada, 173?180.
David Weiss and Ben Taskar. 2010. Structured prediction cascades. In Proceedings of the 13th Interna-
tional Conference on Artificial Intelligence and Statistics (AISTATS), May 13?15, Chia Laguna Resort,
Sardinia, Italy, 916-923.
Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent
algorithms. In Proceedings of the 21st International Conference on Machine Learning, July 4?8,
Banff, Alberta, Canada, 919?926.
1176
Proceedings of the ACL 2010 Conference Short Papers, pages 220?224,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Intelligent Selection of Language Model Training Data
Robert C. Moore William Lewis
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,wilewis}@microsoft.com
Abstract
We address the problem of selecting non-
domain-specific language model training
data to build auxiliary language models
for use in tasks such as machine transla-
tion. Our approach is based on comparing
the cross-entropy, according to domain-
specific and non-domain-specifc language
models, for each sentence of the text
source used to produce the latter language
model. We show that this produces better
language models, trained on less data, than
both random data selection and two other
previously proposed methods.
1 Introduction
Statistical N-gram language models are widely
used in applications that produce natural-language
text as output, particularly speech recognition and
machine translation. It seems to be a univer-
sal truth that output quality can always be im-
proved by using more language model training
data, but only if the training data is reasonably
well-matched to the desired output. This presents
a problem, because in virtually any particular ap-
plication the amount of in-domain data is limited.
Thus it has become standard practice to com-
bine in-domain data with other data, either by
combining N-gram counts from in-domain and
other data (usually weighting the counts in some
way), or building separate language models from
different data sources, interpolating the language
model probabilities either linearly or log-linearly.
Log-linear interpolation is particularly popular
in statistical machine translation (e.g., Brants et
al., 2007), because the interpolation weights can
easily be discriminatively trained to optimize an
end-to-end translation objective function (such as
BLEU) by making the log probability according to
each language model a separate feature function in
the overall translation model.
The normal practice when using multiple lan-
guages models in machine translation seems to be
to train models on as much data as feasible from
each source, and to depend on feature weight opti-
mization to down-weight the impact of data that is
less well-matched to the translation application. In
this paper, however, we show that for a data source
that is not entirely in-domain, we can improve the
match between the language model from that data
source and the desired application output by intel-
ligently selecting a subset of the available data as
language model training data. This not only pro-
duces a language model better matched to the do-
main of interest (as measured in terms of perplex-
ity on held-out in-domain data), but it reduces the
computational resources needed to exploit a large
amount of non-domain-specific data, since the re-
sources needed to filter a large amount of data are
much less (especially in terms of memory) than
those required to build a language model from all
the data.
2 Approaches to the Problem
Our approach to the problem assumes that we have
enough in-domain data to train a reasonable in-
domain language model, which we then use to
help score text segments from other data sources,
and we select segments based on a score cutoff op-
timized on held-out in-domain data.
We are aware of two comparable previous ap-
proaches. Lin et al (1997) and Gao et al (2002)
both used a method similar to ours, in which the
metric used to score text segments is their perplex-
ity according to the in-domain language model.
The candidate text segments with perplexity less
than some threshold are selected.
The second previous approach does not explic-
itly make use of an in-domain language model, but
is still applicable to our scenario. Klakow (2000)
estimates a unigram language model from the
entire non-domain-specific corpus to be selected
220
from, and scores each candidate text segment from
that corpus by the change in the log likelihood
of the in-domain data according to the unigram
model, if that segment were removed from the cor-
pus used to estimate the unigram model. Those
segments whose removal would decrease the log
likelihood of the in-domain data more than some
threshold are selected.
Our method is a fairly simple variant of scoring
by perplexity according to an in-domain language
model. First, note that selecting segments based
on a perplexity threshold is equivalent to selecting
based on a cross-entropy threshold. Perplexity and
cross-entropy are monotonically related, since the
perplexity of a string s according to a model M is
simply bHM (s), where HM (s) is the cross-entropy
of s according to M and b is the base with re-
spect to which the cross-entropy is measured (e.g.,
bits or nats). However, instead of scoring text seg-
ments by perplexity or cross-entropy according to
the in-domain language model, we score them by
the difference of the cross-entropy of a text seg-
ment according to the in-domain language model
and the cross-entropy of the text segment accord-
ing to a language model trained on a random sam-
ple of the data source from which the text segment
is drawn.
To state this formally, let I be an in-domain data
set and N be a non-domain-specific (or otherwise
not entirely in-domain) data set. Let HI(s) be the
per-word cross-entropy, according to a language
model trained on I , of a text segment s drawn from
N . Let HN (s) be the per-word cross-entropy of s
according to a language model trained on a ran-
dom sample of N . We partition N into text seg-
ments (e.g., sentences), and score the segments ac-
cording to HI(s) ? HN (s), selecting all text seg-
ments whose score is less than a threshold T .
This method can be justified by reasoning sim-
liar to that used to derive methods for training
binary text classifiers without labeled negative
examples (Denis et al, 2002; Elkin and Noto,
2008). Let us imagine that our non-domain-
specific corpus N contains an in-domain subcor-
pus NI , drawn from the same distribution as our
in-domain corpus I . Since NI is statistically just
like our in-domain data I , it would seem to be a
good candidate for the data that we want to extract
from N . By a simple variant of Bayes rule, the
probability P (NI |s,N) of a text segment s, drawn
randomly from N , being in NI is given by
P (NI |s,N) =
P (s|NI , N)P (NI |N)
P (s|N)
Since NI is a subset of N , P (s|NI , N) =
P (s|NI), and by our assumption about the rela-
tionship of I and NI , P (s|NI) = P (s|I). Hence,
P (NI |s,N) =
P (s|I)P (NI |N)
P (s|N)
If we could estimate all the probabilities in the
right-hand side of this equation, we could use it
to select text segments that have a high probability
of being in NI .
We can estimate P (s|I) and P (s|N) by train-
ing language models on I and a sample of N , re-
spectively. That leaves us only P (NI |N), to es-
timate, but we really don?t care what P (NI |N)
is, because knowing that would still leave us won-
dering what threshold to set on P (NI |s,N). We
don?t care about classification accuracy; we care
only about the quality of the resulting language
model, so we might as well just attempt to find
a threshold on P (s|I)/P (s|N) that optimizes the
fit of the resulting language model to held-out in-
domain data.
Equivalently, we can work in the log domain
with the quantity log(P (s|I)) ? log(P (s|N)).
This gets us very close to working with the differ-
ence in cross-entropies, because HI(s)?HN (s) is
just a length-normalized version of log(P (s|I))?
log(P (s|N)), with the sign reversed. The rea-
son that we need to normalize for length is that
the value of log(P (s|I)) ? log(P (s|N)) tends to
correlate very strongly with text segment length.
If the candidate text segments vary greatly in
length?e.g., if we partition N into sentences?
this correlation can be a serious problem.
We estimated this effect on a 1000-sentence
sample of our experimental data described be-
low, and found the correlation between sentence
log probability difference and sentence length to
be r = ?0.92, while the cross-entropy differ-
ence was almost uncorrelated with sentence length
(r = 0.04). Hence, using sentence probability ra-
tios or log probability differences as our scoring
function would result in selecting disproportion-
ately very short sentences. We tested this in an
experiment not described here in detail, and found
it not to be significantly better as a selection crite-
rion than random selection.
221
Corpus Sentence count Token count
Gigaword 133,310,562 3,445,946,266
Europarl train 1,651,392 48,230,859
Europarl test 2,000 55,566
Table 1: Corpus size statistics
3 Experiments
We have empirically evaluated our proposed
method for selecting data from a non-domain-
specific source to model text in a specific domain.
For the in-domain corpus, we chose the English
side of the English-French parallel text from re-
lease v5 of the Europarl corpus (Koehn, 2005).
This consists of proceedings of the European Par-
liament from 1999 through 2009. We used the
text from 1999 through 2008 as in-domain train-
ing data, and we used the first 2000 sentences
from January 2009 as test data. For the non-
domain-specific corpus, we used the LDC Eng-
lish Gigaword Third Edition (LDC Catalog No.:
LDC2007T07).
We used a simple tokenization scheme on all
data, splitting on white space and on boundaries
between alphanumeric and nonalphanumeric (e.g.,
punctuation) characters. With this tokenization,
the sizes of our data sets in terms of sentences and
tokens are shown in Table 1. The token counts in-
clude added end-of-sentence tokens.
To implement our data selection method we re-
quired one language model trained on the Europarl
training data and one trained on the Gigaword
data. To make these language models comparable,
and to show the feasibility of optimizing the fit to
the in-domain data without training a model on the
entire Gigaword corpus, we trained the Gigaword
language model for data selection on a random
sample of the Gigaword corpus of a similar size to
that of the Europarl training data: 1,874,051 sen-
tences, 48,459,945 tokens.
To further increase the comparability of these
Europarl and Gigaword language models, we re-
stricted the vocabulary of both models to the to-
kens appearing at least twice in the Europarl train-
ing data, treating all other tokens as instances of
<UNK>. With this vocabulary, 4-gram language
models were trained on both the Europarl training
data and the Gigaword random sample using back-
off absolute discounting (Ney et al 1994), with a
discount of 0.7 used for all N-gram lengths. The
discounted probability mass at the unigram level
was added to the probability of <UNK>. A count
cutoff of 2 occurrences was applied to the trigrams
and 4-grams in estimating these models.
We computed the cross-entropy of each sen-
tence in the Gigaword corpus according to both
models, and scored each sentence by the differ-
ence in cross-entropy, HEp(s)?HGw(s). We then
selected subsets of the Gigaword data correspond-
ing to 8 cutoff points in the cross-entropy differ-
ence scores, and trained 4-gram models (again us-
ing absolute discounting with a discount of 0.7) on
each of these subsets and on the full Gigaword cor-
pus. These language models were estimated with-
out restricting the vocabulary or applying count
cutoffs, but the only parameters computed were
those needed to determine the perplexity of the
held-out Europarl test set, which saves a substan-
tial amount of computation in determining the op-
timal selection threshold.
We compared our selection method to three
other methods. As a baseline, we trained lan-
guage models on random subsets of the Gigaword
corpus of approximately equal size to the data
sets produced by the cutoffs we selected for the
cross-entropy difference scores. Next, we scored
all the Gigaword sentences by the cross-entropy
according to the Europarl-trained model alone.
As we noted above, this is equivalent to the in-
domain perplexity scoring method used by Lin et
al. (1997) and Gao et al (2002). Finally, we im-
plemented Klakow?s (2000) method, scoring each
Gigaword sentence by removing it from the Giga-
word corpus and computing the difference in the
log likelihood of the Europarl corpus according to
unigram models trained on the Gigaword corpus
with and without that sentence. With the latter two
methods, we chose cutoff points in the resulting
scores to produce data sets approximately equal in
size to those obtained using our selection method.
4 Results
For all four selection methods, plots of test set per-
plexity vs. the number of training data tokens se-
lected are displayed in Figure 1. (Note that the
training data token counts are displayed on a log-
arithmic scale.) The test set perplexity for the lan-
guage model trained on the full Gigaword corpus
is 135. As we might expect, reducing training
data by random sampling always increases per-
plexity. Selecting Gigaword sentences by their
222
100
120
140
160
180
200
220
240
0.01 0.1 1 10
Te
st
-s
et
 p
er
pl
ex
it
y
Billions of words of training data
random selection
in-domain cross-entropy scoring
Klakow's method
cross-entropy difference scoring
Figure 1: Test set perplexity vs. training set size
Selection Method Original LM PPL Modified LM PPL
in-domain cross-entropy scoring 124.4 124.8
Klakow?s method 110.5 110.8
cross-entropy difference scoring 100.7 101.9
Table 2: Results adjusted for vocabulary coverage
cross-entropy according to the Europarl-trained
model is effective in reducing both test set perplex-
ity and training corpus size, with an optimum per-
plexity of 124, obtained with a model built from
36% of the Gigaword corpus. Klakow?s method
is even more effective, with an optimum perplex-
ity of 111, obtained with a model built from 21%
of the Gigaword corpus. The cross-entropy differ-
ence selection method, however, is yet more effec-
tive, with an optimum perplexity of 101, obtained
with a model built from less than 7% of the Giga-
word corpus.
The comparisons implied by Figure 1, how-
ever, are only approximate, because each perplex-
ity (even along the same curve) is computed with
respect to a different vocabulary, resulting in a dif-
ferent out-of-vocabulary (OOV) rate. OOV tokens
in the test data are excluded from the perplexity
computation, so the perplexity measurements are
not strictly comparable.
Out of the 55566 test set tokens, the number
of OOV tokens ranges from 418 (0.75%), for the
smallest training set based on in-domain cross-
entropy scoring, to 20 (0.03%), for training on
the full Gigaword corpus. If we consider only
the training sets that appear to produce the lowest
perplexity for each selection method, however, the
spread of OOV counts is much narrower, ranging
53 (0.10%) for best training set based on cross-
entropy difference scoring, to 20 (0.03%), for ran-
dom selection.
To control for the difference in vocabulary, we
estimated a modified 4-gram language model for
each selection method (other than random se-
lection) using the training set that appeared to
produce the lowest perplexity for that selection
method in our initial experiments. In the modified
language models, the unigram model based on the
selected training set is smoothed by absolute dis-
counting, and backed-off to an unsmoothed uni-
gram model based on the full Gigaword corpus.
This produces language models that are normal-
ized over the same vocabulary as a model trained
on the full Gigaword corpus; thus the test set has
the same OOVs for each model.
Test set perplexity for each of these modifed
language models is compared to that of the orig-
inal version of the model in Table 2. It can be
seen that adjusting the vocabulary in this way, so
that all models are based on the same vocabulary,
223
yields only very small changes in the measured
test-set perplexity, and these differences are much
smaller than the differences between the different
selection methods, whichever way the vocabulary
of the language models is determined.
5 Conclusions
The cross-entropy difference selection method in-
troduced here seems to produce language mod-
els that are both a better match to texts in a re-
stricted domain, and require less data for train-
ing, than any of the other data selection methods
tested. This study is preliminary, however, in that
we have not yet shown improved end-to-end task
performance applying this approach, such as im-
proved BLEU scores in a machine translation task.
However, we believe there is reason to be opti-
mistic about this. When a language model trained
on non-domain-specific data is used in a statisti-
cal translation model as a separate feature func-
tion (as is often the case), lower perplexity on in-
domain target language test data derived from ref-
erence translations corresponds directly to assign-
ing higher language model feature scores to those
reference translations, which should in turn lead to
translation system output that matches reference
translations better.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz
J. Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the Joint Conference on Empirical Methods
in Natural Language Processing and Computa-
tional Natural Language Learning, June 28?30,
Prague, Czech Republic, 858?867.
Franc?ois Denis, Remi Gilleron, and Marc Tom-
masi. 2002. Text classification from positive
and unlabeled examples. In The 9th Interna-
tional Conference on Information Processing
and Management of Uncertainty in Knowledge-
Based Systems (IPMU 2002), 1927?1934.
Charles Elkin and Keith Noto. 2008. Learn-
ing classifiers from only positive and unlabeled
data. In KDD 2008, August 24?27, Las Vegas,
Nevada, USA, 213?220.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and
Kai-Fu Lee. 2002. Toward a unified approach
to statistical language modeling for Chinese.
ACM Transactions on Asian Language Informa-
tion Processing, 1(1):3?33.
Dietrich Klakow. 2000. Selecting articles from
the language model training corpus. In ICASSP
2000, June 5?9, Istanbul, Turkey, vol. 3, 1695?
1698.
Philipp Koehn. 2005. Europarl: a parallel cor-
pus for statistical machine translation. In MT
Summit X, September 12?16, Phuket, Thailand,
79?86.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien,
Ker-Jiann Chen, and Lin-Shan Lee. 1997.
Chinese language model adaptation based on
document classification and multiple domain-
specific language models. In EUROSPEECH-
1997, 1463?1466.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring dependencies in stochas-
tic language modelling. Computer Speech and
Language, 8:1?38.
224
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1308?1317,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Gappy Phrasal Alignment by Agreement
Mohit Bansal?
UC Berkeley, CS Division
mbansal@cs.berkeley.edu
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Robert C. Moore
Google Research
robert.carter.moore@gmail.com
Abstract
We propose a principled and efficient phrase-
to-phrase alignment model, useful in machine
translation as well as other related natural lan-
guage processing problems. In a hidden semi-
Markov model, word-to-phrase and phrase-
to-word translations are modeled directly by
the system. Agreement between two direc-
tional models encourages the selection of par-
simonious phrasal alignments, avoiding the
overfitting commonly encountered in unsu-
pervised training with multi-word units. Ex-
panding the state space to include ?gappy
phrases? (such as French ne ? pas) makes the
alignment space more symmetric; thus, it al-
lows agreement between discontinuous align-
ments. The resulting system shows substantial
improvements in both alignment quality and
translation quality over word-based Hidden
Markov Models, while maintaining asymptot-
ically equivalent runtime.
1 Introduction
Word alignment is an important part of statisti-
cal machine translation (MT) pipelines. Phrase
tables containing pairs of source and target lan-
guage phrases are extracted from word alignments,
forming the core of phrase-based statistical ma-
chine translation systems (Koehn et al, 2003).
Most syntactic machine translation systems extract
synchronous context-free grammars (SCFGs) from
aligned syntactic fragments (Galley et al, 2004;
Zollmann et al, 2006), which in turn are de-
rived from bilingual word alignments and syntactic
?Author was a summer intern at Microsoft Research during
this project.
French
English
voudrais voyager par chemin de fer
would like traveling by railroad
ne pas
not
Figure 1: French-English pair with complex word alignment.
parses. Alignment is also used in various other NLP
problems such as entailment, paraphrasing, question
answering, summarization and spelling correction.
A limitation to word-based alignment is undesir-
able. As seen in the French-English example in Fig-
ure 1, many sentence pairs are naturally aligned with
multi-word units in both languages (chemin de fer;
would ? like, where ? indicates a gap). Much work
has addressed this problem: generative models for
direct phrasal alignment (Marcu and Wong, 2002),
heuristic word-alignment combinations (Koehn et
al., 2003; Och and Ney, 2003), models with pseudo-
word collocations (Lambert and Banchs, 2006; Ma
et al, 2007; Duan et al, 2010), synchronous gram-
mar based approaches (Wu, 1997), etc. Most have a
large state-space, using constraints and approxima-
tions for efficient inference.
We present a new phrasal alignment model based
on the hidden Markov framework (Vogel et al,
1996). Our approach is semi-Markov: each state can
generate multiple observations, representing word-
to-phrase alignments. We also augment the state
space to include contiguous sequences. This cor-
responds to phrase-to-word and phrase-to-phrase
alignments. We generalize alignment by agreement
(Liang et al, 2006) to this space, and find that agree-
ment discourages EM from overfitting. Finally, we
make the alignment space more symmetric by in-
cluding gappy (or non-contiguous) phrases. This al-
lows agreement to reinforce non-contiguous align-
1308
f1
f2
f3
e1 e2 e3 f1 f2 f3
e1
e2
e3
Observations? 
?
?
S
ta
tes?
 
HMM(E|F) HMM(F|E)
Figure 2: The model of E given F can represent the phrasal
alignment {e1, e2} ? {f1}. However, the model of F given
E cannot: the probability mass is distributed between {e1} ?
{f1} and {e2} ? {f1}. Agreement of the forward and back-
ward HMM alignments tends to place less mass on phrasal links
and greater mass on word-to-word links.
ments, such English not to French ne ? pas. Prun-
ing the set of allowed phrases preserves the time
complexity of the word-to-word HMM alignment
model.
1.1 Related Work
Our first major influence is that of conditional
phrase-based models. An early approach by Deng
and Byrne (2005) changed the parameterization of
the traditional word-based HMM model, modeling
subsequent words from the same state using a bi-
gram model. However, this model changes only the
parameterization and not the set of possible align-
ments. More closely related are the approaches
of Daume? III and Marcu (2004) and DeNero et
al. (2006), which allow phrase-to-phrase alignments
between the source and target domain. As DeN-
ero warns, though, an unconstrained model may
overfit using unusual segmentations. Interestingly,
the phrase-based hidden semi-Markov model of
Andre?s-Ferrer and Juan (2009) does not seem to
encounter these problems. We suspect two main
causes: first, the model interpolates with Model 1
(Brown et al, 1994), which may help prevent over-
fitting, and second, the model is monotonic, which
screens out many possible alignments. Monotonic-
ity is generally undesirable, though: almost all par-
allel sentences exhibit some reordering phenomena,
even when languages are syntactically very similar.
The second major inspiration is alignment by
agreement by Liang et al (2006). Here, soft inter-
section between the forward (F?E) and backward
(E?F) alignments during parameter estimation pro-
duces better word-to-word correspondences. This
unsupervised approach produced alignments with
incredibly low error rates on French-English, though
only moderate gains in end-to-end machine transla-
tion results. Likely this is because the symmetric
portion of the HMM space contains only single word
to single word links. As shown in Figure 2, in order
to retain the phrasal link f1 ? e1, e2 after agree-
ment, we need the reverse phrasal link e1, e2 v f1
in the backward direction. However, this is not pos-
sible in a word-based HMM where each observa-
tion must be generated by a single state. Agreement
tends to encourage 1-to-1 alignments with very high
precision and but lower recall. As each word align-
ment acts as a constraint on phrase extraction, the
phrase-pairs obtained from those alignments have
high recall and low precision.
2 Gappy Phrasal Alignment
Our goal is to unify phrasal alignment and align-
ment by agreement. We use a phrasal hidden semi-
Markov alignment model, but without the mono-
tonicity requirement of Andre?s-Ferrer and Juan
(2009). Since phrases may be used in both the state
and observation space of both sentences, agreement
during EM training no longer penalizes phrasal links
such as those in Figure 2. Moreover, the benefits of
agreement are preserved: meaningful phrasal links
that are likely in both directions of alignment will be
reinforced, while phrasal links likely in only one di-
rection will be discouraged. This avoids segmenta-
tion problems encountered by DeNero et al (2006).
Non-contiguous sequences of words present an
additional challenge. Even a semi-Markov model
with phrases can represent the alignment between
English not and French ne ? pas in one direction
only. To make the model more symmetric, we ex-
tend the state space to include gappy phrases as
well.1 The set of alignments in each model becomes
symmetric, though the two directions model gappy
phrases differently. Consider not and ne ? pas:
when predicting French given English, the align-
ment corresponds to generating multiple distinct ob-
1We only allow a single gap with one word on each end.
This is sufficient for the vast majority of the gapped phenomena
that we have seen in our training data.
1309
voudrais
voyager
par
chemin
de
fer
wo
ul
d
lik
e
tra
ve
lin
g
by ra
ilr
oa
d
C
would
like
traveling
by
railroad
vo
ud
ra
is
vo
ya
ge
r
pa
r
ch
em
in
de fe
r
no
t
pas
ne
not
ne pa
s
Observations? 
S
ta
tes?
 
Observations? 
S
ta
tes?
 
Figure 3: Example English-given-French and French-given-English alignments of the same sentence pair using the Hidden Semi-
Markov Model (HSMM) for gapped-phrase-to-phrase alignment. It allows the state side phrases (denoted by vertical blocks),
observation side phrases (denoted by horizontal blocks), and state-side gaps (denoted by discontinuous blocks in the same column
connected by a hollow vertical ?bridge?). Note both directions can capture the desired alignment for this sentence pair.
servations from the same state; in the other direction,
the word not is generated by a single gappy phrase
ne ? pas. Computing posteriors for agreement is
somewhat complicated, so we resort to an approx-
imation described later. Exact inference retains a
low-order polynomial runtime; we use pruning to in-
crease speed.
2.1 Hidden Markov Alignment Models
Our model can be seen as an extension of the stan-
dard word-based Hidden Markov Model (HMM)
used in alignment (Vogel et al, 1996). To
ground the discussion, we first review the struc-
ture of that model. This generative model has
the form p(O|S) =
?
A p(A,O|S), where S =
(s1, . . . , sI) ? ?? is a sequence of words from a
vocabulary ?; O = (o1, . . . , oJ) ? ?? is a sequence
from vocabulary ?; and A = (a1, . . . , aJ) is the
alignment between the two sequences. Since some
words are systematically inserted during translation,
the target (state) word sequence is augmented with
a special NULL word. To retain the position of the
last aligned word, the state space contains I copies
of the NULL word, one for each position (Och and
Ney, 2003). The alignment uses positive positions
for words and negative positions for NULL states, so
aj ? {1..I} ? {?1..? I}, and si = NULL if i < 0.
It uses the following generative procedure. First
the length of the observation sequence is selected
based on pl(J |I). Then for each observation posi-
tion, the state is selected based on the prior state: a
null state with probability p0, or a non-null state at
position aj with probability (1 ? p0) ? pj(aj |aj?1)
where pj is a jump distribution. Finally the observa-
tion word oj at that position is generated with prob-
ability pt(oj |saj ), where pt is an emission distribu-
tion:
p(A,O|S) = pl(J |I)
J?
j=1
pj(aj |aj?1)pt(oj |saj )
pj(a|a
?) =
{
(1? p0) ? pd(a? |a?|) a > 0
p0 ? ?(|a|, |a?|) a < 0
We pick p0 using grid search on the development
set, pl is uniform, and the pj and pt are optimized by
EM.2
2.2 Gappy Semi-Markov Models
The HMM alignment model identifies a word-
to-word correspondence between the observation
2Note that jump distances beyond -10 or 10 share a single
parameter to prevent sparsity.
1310
words and the state words. We make two changes
to expand this model. First, we allow contiguous
phrases on the observation side, which makes the
model semi-Markov: at each time stamp, the model
may emit more than one observation word. Next, we
also allow contiguous and gappy phrases on the state
side, leading to an alignment model that can retain
phrasal links after agreement (see Section 4).
The S and O random variables are unchanged.
Since a single state may generate multiple observa-
tion words, we add a new variable K representing
the number of states. K should be less than J , the
number of observations. The alignment variable is
augmented to allow contiguous and non-contiguous
ranges of words. We allow only a single gap, but of
unlimited length. The null state is still present, and
is again represented by negative numbers.
A =(a1, . . . , aK) ? A(I)
A(I) ={(i1, i2, g)|0 < i1 ? i2 ? I,
g ? {GAP, CONTIG}}?
{(?i,?i, CONTIG) | 0 < i ? I}
We add one more random variable to capture the to-
tal number of observations generated by each state.
L ? {(l0, l1, . . . , lK) | 0 = l0 < ? ? ? < lK = J}
The generative model takes the following form:
p(A,L,O|S) =pl(J |I)pf (K|J)
K?
k=1
pj(ak|ak?1)?
pt(lk, o
lk
lk?1+1
|S[ak], lk?1)
First, the length of the observation sequence (J)
is selected, based on the number of words in the
state-side sentence (I). Since it does not affect the
alignment, pl is modeled as a uniform distribution.
Next, we pick the total number of states to use (K),
which must be less than the number of observations
(J). Short state sequences receive an exponential
penalty: pf (K|J) ? ?(J?K) if 0 ? K ? J , or 0
otherwise. A harsh penalty (small positive value of
?) may prevent the systematic overuse of phrases.3
3We found that this penalty was crucial to prevent overfitting
in independent training. Joint training with agreement made it
basically unnecessary.
Next we decide the assignment of each state.
We retain the first-order Markov assumption: the
selection of each state is conditioned only on the
prior state. The transition distribution is identical
to the word-based HMM for single word states. For
phrasal and gappy states, we jump into the first word
of that state, and out of the last word of that state,
and then pay a cost according to how many words
are covered within that state. If a = (i1, i2, g), then
the beginning word of a is F (a) = i1, the end-
ing word is L(a) = i2, and the length N(a) is 2
for gapped states, 0 for null states, and last(a) ?
first(a) + 1 for all others. The transition probabil-
ity is:
pj(a|a
?) =
?
??
??
p0 ? ?(|F (a)|, |L(a?)|) if F (a) < 0
(1? p0)pd(F (a)? |L(a?)|)?
pn(N(a)) otherwise
where pn(c) ? ?c is an exponential distribution. As
in the word HMM case, we use a mixture parameter
p0 to determine the likelihood of landing in a NULL
state. The position of that NULL state remembers the
last position of the prior state. For non-null words,
we pick the first word of the state according to the
distance from the last word of the prior state. Finally,
we pick a length for that final state according to an
exponential distribution: values of ? less than one
will penalize the use of phrasal states.
For each set of state words, we maintain an emis-
sion distribution over observation word sequences.
Let S[a] be the set of state words referred to by
the alignment variable a. For example, the English
given French alignment of Figure 3 includes the fol-
lowing state word sets:
S[(2, 2, CONTIG)] = voudrais
S[(1, 3, GAP)] = ne ? pas
S[(6, 8, CONTIG)] = chemin de fer
For the emission distribution we keep a multinomial
over observation phrases for each set of state words:
p(l, oll? |S[a], l
?) ? c(oll? |S[a])
In contrast to the approach of Deng and Byrne
(2005), this encourages greater consistency across
instances, and more closely resembles the com-
monly used phrasal translation models.
1311
We note in passing that pf (K|J) may be moved
inside the product: pf (K|J) ? ?(J?K) =
?K
k=1 ?
(lk?lk?1?1). The following form derived us-
ing the above rearrangement is helpful during EM.
p(A,L,O|S) ?
K?
k=1
pj(ak|ak?1)?
pt(lk, o
lk
lk?1+1
|S[ak], lk?1)?
?(lk?lk?1?1)
where lk ? lk?1 ? 1 is the length of the observation
phrase emitted by state S[ak].
2.3 Minimality
At alignment time we focus on finding the minimal
phrase pairs, under the assumption that composed
phrase pairs can be extracted in terms of these min-
imal pairs. We are rather strict about this, allowing
only 1 ? k and k ? 1 phrasal alignment edges
(or links). This should not cause undue stress, since
edges of the form 2 ? 3 (say e1e2 ? f1f2f3) can
generally be decomposed into 1 ? 1 ? 1 ? 2 (i.e.,
e1 ? f1 ? e2 ? f2f3), etc. However, the model
does not require this to be true: we will describe re-
estimation for unconstrained general models, but use
the limited form for word alignment.
3 Parameter Estimation
We use Expectation-Maximization (EM) to estimate
parameters. The forward-backward algorithm effi-
ciently computes posteriors of transitions and emis-
sions in the word-based HMM. In a standard HMM,
emission always advances the observation position
by one, and the next transition is unaffected by
the emission. Neither of these assumptions hold
in our model: multiple observations may be emit-
ted at a time, and a state may cover multiple state-
side words, which affects the outgoing transition. A
modified dynamic program computes posteriors for
this generalized model.
The following formulation of the forward-
backward algorithm for word-to-word alignment is
a good starting point. ?[x, 0, y] indicates the total
mass of paths that have just transitioned into state y
at observation x but have not yet emitted; ?[x, 1, y]
represents the mass after emission but before subse-
quent transition. ? is defined similarly. (We omit
NULL states for brevity; the extension is straightfor-
ward.)
?[0, 0, y] = pj(y|INIT)
?[x, 1, y] = ?[x, 0, y] ? pt(ox|sy)
?[x, 0, y] =
?
y?
?[x? 1, 1, y?] ? pj(y|y
?)
?[n, 1, y] = 1
?[x, 0, y] = pt(ox|sy) ? ?[x, 1, y]
?[x, 1, y] =
?
y?
pj(y
?|y) ? ?[x+ 1, 0, y?]
Not only is it easy to compute posteriors of both
emissions (?[x, 0, y]pt(ox|sy)?[x, 1, y]) and transi-
tions (?[x, 1, y]pj(y?|y)?[x+ 1, 0, y?]) with this for-
mulation, it also simplifies the generalization to
complex emissions. We update the emission forward
probabilities to include a search over the possible
starting points in the state and observation space:
?[0, 0, y] =pj(y|INIT)
?[x, 1, y] =
?
x?<x,y??y
?[x?, 0, y?] ? EMIT(x? : x, y? : y)
?[x, 0, y] =
?
y?
?[x? 1, 1, y?] ? pj(y|y
?)
?[n, 1, y] =1
?[x?, 0, y?] =
?
x?<x,y??y
EMIT(x? : x, y? : y) ? ?[x, 1, y]
?[x, 1, y] =
?
y?
pj(y
?|y) ? ?[x+ 1, 0, y?]
Phrasal and gapped emissions are pooled into EMIT:
EMIT(w : x, y : z) =pt(o
x
w|s
z
y) ? ?
z?y+1 ? ?x?w+1+
pt(o
x
w|sy ? sz) ? ?
2 ? ?x?w+1
The transition posterior is the same as above. The
emission is very similar: the posterior probability
that oxw is aligned to s
z
y is proportional to ?[w, 0, y] ?
pt(oxw|s
z
y) ??
z?y+1 ??x?w+1 ??[x, 1, z]. For a gapped
phrase, the posterior is proportional to ?[w, 0, y] ?
pt(oxw|sy ? sz) ? ?
2 ? ?x?w+1 ? ?[x, 1, z].
Given an inference procedure for computing pos-
teriors, unsupervised training with EM follows im-
mediately. We use a simple maximum-likelihood
update of the parameters using expected counts
based on the posterior distribution.
1312
4 Alignment by Agreement
Following Liang et al (2006), we quantify agree-
ment between two models as the probability that the
alignments produced by the two models agree on the
alignment z of a sentence pair x = (S,O):
?
z
p1(z|x; ?1)p2(z|x; ?2)
To couple the two models, the (log) probability of
agreement is added to the standard log-likelihood
objective:
max
?1,?2
?
x
[
log p1(x; ?1) + log p2(x; ?2)+
log
?
z
p1(z|x; ?1)p2(z|x; ?2)
]
We use the heuristic estimator from Liang et al
(2006), letting q be a product of marginals:
E : q(z; x) :=
?
z?z
p1(z|x; ?1)p2(z|x; ?2)
where each pk(z|x; ?k) is the posterior marginal of
some edge z according to each model. Such a
heuristic E step computes the marginals for each
model separately, then multiplies the marginals cor-
responding to the same edge. This product of
marginals acts as the approximation to the posterior
used in the M step for each model. The intuition is
that if the two models disagree on a certain edge z,
then the marginal product is small, hence that edge
is dis-preferred in each model.
Contiguous phrase agreement. It is simple to
extend agreement to alignments in the absence of
gaps. Multi-word (phrasal) links are assigned some
posterior probability in both models, as shown in the
example in Figure 3, and we multiply the posteriors
of these phrasal links just as in the single word case.4
?F?E(fi, ej) := ?E?F (ej , fi)
:= [?F?E(fi, ej)? ?E?F (ej , fi)]
4Phrasal correspondences can be represented in multiple
ways: multiple adjacent words could be generated from the
same state either using one semi-Markov emission, or using
multiple single word emissions followed by self-jumps. Only
the first case is reinforced through agreement, so the latter is
implicitly discouraged. We explored an option to forbid same-
state transitions, but found it made little difference in practice.
Gappy phrase agreement. When we introduce
gappy phrasal states, agreement becomes more chal-
lenging. In the forward direction F?E, if we have a
gappy state aligned to an observation, say fi ? fj ?
ek, then its corresponding edge in the backward di-
rection E?F would be ek v fi ? fj . How-
ever, this is represented by two distinct and unre-
lated emissions. Although it is possible the compute
the posterior probability of two non-adjacent emis-
sions, this requires running a separate dynamic pro-
gram for each such combination to sum the mass be-
tween these emissions. For the sake of efficiency
we resort to an approximate computation of pos-
terior marginals using the two word-to-word edges
ek v fi and ek v fj .
The forward posterior ?F?E for edge fi ? fj ?
ek is multiplied with the min of the backward pos-
teriors of the edges ek v fi and ek v fj .
?F?E(fi ? fj , ek) := ?F?E(fi ? fj , ek)?
min
{
?E?F (ek, fi), ?E?F (ek, fj)
}
Note that this min is an upper bound on the desired
posterior of edge ek v fi ? fj , since every path
that passes through ek v fi and ek v fj must pass
through ek v fi, therefore the posterior of ek v
fi ? fj is less than that of ek v fi, and likewise less
than that of ek v fj .
The backward posteriors of the edges ek v fi and
ek v fj are also mixed with the forward posteriors
of the edges to which they correspond.
?E?F (ek, fi) := ?E?F (ek, fi)?
[
?F?E(fi, ek)+
?
h<i<j
{
?F?E(fh ? fi, ek) + ?F?E(fi ? fj , ek)
}
]
5 Pruned Lists of ?Allowed? Phrases
To identify contiguous and gapped phrases that are
more likely to lead to good alignments, we use word-
to-word HMM alignments from the full training data
in both directions (F?E and E?F). We collect ob-
servation phrases of length 2 toK aligned to a single
state, i.e. oji ? s, to add to a list of allowed phrases.
For gappy phrases, we find all non-consecutive ob-
servation pairs oi and oj such that: (a) both are
1313
aligned to the same state sk, (b) state sk is aligned to
only these two observations, and (c) at least one ob-
servation between oi and oj is aligned to a non-null
state other than sk. These observation phrases are
collected from F?E and E?F models to build con-
tiguous and gappy phrase lists for both languages.
Next, we order the phrases in each contiguous list
using the discounted probability:
p?(o
j
i ? s|o
j
i ) =
max(0, count(oji ? s)? ?)
count(oji )
where count(oji ? s) is the count of occurrence of
the observation-phrase oji , all aligned to some sin-
gle state s, and count(oji ) is the count of occur-
rence of the observation phrase oji , not all necessar-
ily aligned to a single state. Similarly, we rank the
gappy phrases using the discounted probability:
p?(oi ? oj ? s|oi ? oj) =
max(0, count(oi ? oj ? s)? ?)
count(oi ? oj)
where count(oi ? oj ? s) is the count of occur-
rence of the observations oi and oj aligned to a sin-
gle state s with the conditions mentioned above, and
count(oi ? oj) is the count of general occurrence of
the observations oi and oj in order. We find that 200
gappy phrases and 1000 contiguous phrases works
well, based on tuning with a development set.
6 Complexity Analysis
Let m be the length of the state sentence S and n
be the length of the observation sentence O. In IBM
Model 1 (Brown et al, 1994), with only a translation
model, we can infer posteriors or max alignments
in O(mn). HMM-based word-to-word alignment
model (Vogel et al, 1996) adds a distortion model,
increasing the complexity to O(m2n).
Introducing phrases (contiguous) on the observa-
tion side, we get a HSMM (Hidden Semi-Markov
Model). If we allow phrases of length no greater
than K, then the number of observation types
rises from n to Kn for an overall complexity of
O(m2Kn). Introducing state phrases (contiguous)
with length ? K grows the number of state types
from m to Km. Complexity further increases to
O((Km)2Kn) = O(K3m2n).
Finally, when we introduce gappy state phrases of
the type si ? sj , the number of such phrases is
O(m2), since we may choose a start and end point
independently. Thus, the total complexity rises to
O((Km + m2)2Kn) = O(Km4n). Although this
is less than the O(n6) complexity of exact ITG (In-
version Transduction Grammar) model (Wu, 1997),
a quintic algorithm is often quite slow.
The pruned lists of allowed phrases limit this
complexity. The model is allowed to use observa-
tion (contiguous) and state (contiguous and gappy)
phrases only from these lists. The number of
phrases that match any given sentence pair from
these pruned lists is very small (? 2 to 5). If the
number of phrases in the lists that match the obser-
vation and state side of a given sentence pair are
small constants, the complexity remains O(m2n),
equal to that of word-based models.
7 Results
We evaluate our models based on both word align-
ment and end-to-end translation with two language
pairs: English-French and English-German. For
French-English, we use the Hansards NAACL 2003
shared-task dataset, which contains nearly 1.1 mil-
lion training sentence pairs. We also evaluated
on German-English Europarl data from WMT2010,
with nearly 1.6 million training sentence pairs. The
model from Liang et al (2006) is our word-based
baseline.
7.1 Training Regimen
Our training regimen begins with both the forward
(F?E) and backward (E?F) iterations of Model 1
run independently (i.e. without agreement). Next,
we train several iterations of the forward and back-
ward word-to-word HMMs, again with independent
training. We do not use agreement during word
alignment since it tends to produce sparse 1-1 align-
ments, which in turn leads to low phrase emission
probabilities in the gappy model.
Initializing the emission probabilities of the semi-
Markov model is somewhat complicated, since the
word-based models do not assign any mass to
the phrasal or gapped configurations. Therefore
we use a heuristic method. We first retrieve the
Viterbi alignments of the forward and backward
1314
word-to-word HMM aligners. For phrasal corre-
spondences, we combine these forward and back-
ward Viterbi alignments using a common heuris-
tic (Union, Intersection, Refined, or Grow-Diag-
Final), and extract tight phrase-pairs (no unaligned
words on the boundary) from this alignment set.
We found that Grow-Diag-Final was most effective
in our experiments. The counts gathered from this
phrase extraction are used to initialize phrasal trans-
lation probabilities. For gappy states in a forward
(F?E) model, we use alignments from the back-
ward (E?F) model. If a state sk is aligned to two
non-consecutive observations oi and oj such that sk
is not aligned to any other observation, and at least
one observation between oi and oj is aligned to a
non-null state other than sk, then we reverse this
link to get oi ? oj ? sk and use it as a gapped-
state-phrase instance for adding fractional counts.
Given these approximate fractional counts, we per-
form a standard MLE M-step to initialize the emis-
sion probability distributions. The distortion proba-
bilities from the word-based model are used without
changes.
7.2 Alignment Results (F1)
The validation and test sentences have been hand-
aligned (see Och and Ney (2003)) and are marked
with both sure and possible alignments. For French-
English, following Liang et al (2006), we lowercase
all words, and use the validation set plus the first
100 test sentences as our development set and the
remaining 347 test-sentences as our test-set for fi-
nal F1 evaluation.5 In German-English, we have a
development set of 102 sentences, and a test set of
258 sentences, also annotated with a set of sure and
possible alignments. Given a predicted alignmentA,
precision and recall are computed using sure align-
ments S and possible alignments P (where S ? P )
as in Och and Ney (2003):
Precision =
|A ? P |
|A|
? 100%
Recall =
|A ? S|
|S|
? 100%
5We report F1 rather than AER because AER appears not to
correlate well with translation quality.(Fraser and Marcu, 2007)
Language pair Word-to-word Gappy
French-English 34.0 34.5
German-English 19.3 19.8
Table 2: BLEU results on German-English and French-English.
AER =
(
1?
|A ? S|+ |A ? P |
|A|+ |S|
)
? 100%
F1 =
2? Precision?Recall
Precision+Recall
? 100%
Many free parameters were tuned to optimize
alignment F1 on the development set, including the
number of iterations of each Model 1, HMM, and
Gappy; the NULL weight p0, the number of con-
tiguous and gappy phrases to include, and the max-
imum phrase length. Five iterations of all models,
p0 = 0.3, using the top 1000 contiguous phrases
and the top 200 gappy phrases, maximum phrase
length of 5, and penalties ? = ? = 1 produced
competitive results. Note that by setting ? and ? to
one, we have effectively removed the penalty alto-
gether without affecting our results. In Table 1 we
see a consistent improvement with the addition of
contiguous phrases, and some additional gains with
gappy phrases.
7.3 Translation Results (BLEU)
We assembled a phrase-based system from the align-
ments (using only contiguous phrases consistent
with the potentially gappy alignment), with 4 chan-
nel models, word and phrase count features, dis-
tortion penalty, lexicalized reordering model, and a
5-gram language model, weighted by MERT. The
same free parameters from above were tuned to opti-
mize development set BLEU using grid search. The
improvements in Table 2 are encouraging, especially
as a syntax-based or non-contiguous phrasal system
(Galley and Manning, 2010) may benefit more from
gappy phrases.
8 Conclusions and Future Work
We have described an algorithm for efficient unsu-
pervised alignment of phrases. Relatively straight-
forward extensions to the base HMM allow for ef-
ficient inference, and agreement between the two
1315
Data Decoding method Word-to-word +Contig phrases +Gappy phrases
FE 10K Viterbi 89.7 90.6 90.3
FE 10K Posterior ? 0.1 90.1 90.4 90.7
FE 100K Viterbi 93.0 93.6 93.8
FE 100K Posterior ? 0.1 93.1 93.7 93.8
FE All Viterbi 94.1 94.3 94.3
FE All Posterior ? 0.1 94.2 94.4 94.5
GE 10K Viterbi 76.2 79.6 79.7
GE 10K Posterior ? 0.1 76.7 79.3 79.3
GE 100K Viterbi 81.0 83.0 83.2
GE 100K Posterior ? 0.1 80.7 83.1 83.4
GE All Viterbi 83.0 85.2 85.6
GE All Posterior ? 0.1 83.7 85.3 85.7
Table 1: F1 scores of automatic word alignments, evaluated on the test set of the hand-aligned sentence pairs.
models prevents EM from overfitting, even in the ab-
sence of harsh penalties. We also allow gappy (non-
contiguous) phrases on the state side, which makes
agreement more successful but agreement needs ap-
proximation of posterior marginals. Using pruned
lists of good phrases, we maintain complexity equal
to the baseline word-to-word model.
There are several steps forward from this point.
Limiting the gap length also prevents combinato-
rial explosion; we hope to explore this in future
work. Clearly a translation system that uses discon-
tinuous mappings at runtime (Chiang, 2007; Gal-
ley and Manning, 2010) may make better use of
discontinuous alignments. This model can also be
applied at the morpheme or character level, allow-
ing joint inference of segmentation and alignment.
Furthermore the state space could be expanded and
enhanced to include more possibilities: states with
multiple gaps might be useful for alignment in lan-
guages with template morphology, such as Arabic or
Hebrew. More exploration in the model space could
be useful ? a better distortion model might place a
stronger distribution on the likely starting and end-
ing points of phrases.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful suggestions. This project is funded by
Microsoft Research.
References
Jesu?s Andre?s-Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-Markov approach to machine trans-
lation. In Proceedings of EAMT.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
Hal Daume? III and Daniel Marcu. 2004. A phrase-based
HMM approach to document/abstract alignment. In
Proceedings of EMNLP.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of ACL.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of HLT-EMNLP.
Xiangyu Duan, Min Zhang, and Haizhou Li. 2010.
Pseudo-word for phrase-based machine translation. In
Proceedings of ACL.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
HLT/NAACL.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of
HLT-NAACL.
Patrik Lambert and Rafael Banchs. 2006. Grouping
multi-word expressions according to part-of-speech in
1316
statistical machine translation. In Proc. of the EACL
Workshop on Multi-Word-Expressions in a Multilin-
gual Context.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Boostrapping word alignment via word packing. In
Proceedings of ACL.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In Processings of the Statistical Ma-
chine Translation Workshop at NAACL.
1317
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 200?209,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
On Hierarchical Re-ordering and Permutation Parsing
for Phrase-based Decoding
Colin Cherry
National Research Council
colin.cherry@nrc-cnrc.gc.ca
Robert C. Moore
Google
bobmoore@google.com
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Abstract
The addition of a deterministic permutation
parser can provide valuable hierarchical in-
formation to a phrase-based statistical ma-
chine translation (PBSMT) system. Permuta-
tion parsers have been used to implement hier-
archical re-ordering models (Galley and Man-
ning, 2008) and to enforce inversion trans-
duction grammar (ITG) constraints (Feng et
al., 2010). We present a number of theoret-
ical results regarding the use of permutation
parsers in PBSMT. In particular, we show that
an existing ITG constraint (Zens et al, 2004)
does not prevent all non-ITG permutations,
and we demonstrate that the hierarchical re-
ordering model can produce analyses during
decoding that are inconsistent with analyses
made during training. Experimentally, we ver-
ify the utility of hierarchical re-ordering, and
compare several theoretically-motivated vari-
ants in terms of both translation quality and
the syntactic complexity of their output.
1 Introduction
Despite the emergence of a number of syntax-based
techniques, phrase-based statistical machine transla-
tion remains a competitive and very efficient trans-
lation paradigm (Galley and Manning, 2010). How-
ever, it lacks the syntactically-informed movement
models and constraints that are provided implicitly
by working with synchronous grammars. There-
fore, re-ordering must be modeled and constrained
explicitly. Movement can be modeled with a dis-
tortion penalty or lexicalized re-ordering probabili-
ties (Koehn et al, 2003; Koehn et al, 2007), while
decoding can be constrained by distortion limits or
by mimicking the restrictions of inversion transduc-
tion grammars (Wu, 1997; Zens et al, 2004).
Recently, we have begun to see deterministic per-
mutation parsers incorporated into phrase-based de-
coders. These efficient parsers analyze the sequence
of phrases used to produce the target, and assem-
ble them into a hierarchical translation history that
can be used to inform re-ordering decisions. Thus
far, they have been used to enable a hierarchical
re-ordering model, or HRM (Galley and Manning,
2008), as well as an ITG constraint (Feng et al,
2010). We discuss each of these techniques in turn,
and then explore the implications of ITG violations
on hierarchical re-ordering.
We present one experimental and four theoreti-
cal contributions. Examining the HRM alone, we
present an improved algorithm for extracting HRM
statistics, reducing the complexity of Galley and
Manning?s solution from O(n4) to O(n2). Examin-
ing ITG constraints alone, we demonstrate that the
three-stack constraint of Feng et al can be reduced
to one augmented stack, and we show that another
phrase-based ITG constraint (Zens et al, 2004) ac-
tually allows some ITG violations to pass. Finally,
we show that in the presence of ITG violations, the
original HRM can fail to produce orientations that
are consistent with the orientations collected during
training. We propose three HRM variants to address
this situation, including an approximate HRM that
requires no permutation parser, and compare them
experimentally. The variants perform similarly to
the original in terms of BLEU score, but differently
in terms of how they permute the source sentence.
200
We begin by establishing some notation. We view
the phrase-based translation process as producing a
sequence of source/target blocks in their target or-
der. For the purposes of this paper, we disregard
the lexical content of these blocks, treating blocks
spanning the same source segment as equivalent.
The block [si, ti] indicates that the source segment
wsi+1, . . . , wti was translated as a unit to produce
the ith target phrase. We index between words;
therefore, a block?s length in tokens is t ? s, and
for a sentence of length n, 0 ? s ? t ? n. Empty
blocks have s = t, and are used only in special cases.
Two blocks [si?1, ti?1] and [si, ti] are adjacent iff
ti?1 = si or ti = si?1. Note that we concern our-
selves only with adjacency in the source. Adjacency
in the target is assumed, as the blocks are in target
order. Figure 1 shows an example block sequence,
where adjacency corresponds to cases where block
corners touch. In the shift-reduce permutation parser
we describe below, the parsing state is encoded as a
stack of these same blocks.
2 Hierarchical Re-ordering
Hierarchical re-ordering models (HRMs) for phrase-
based SMT are an extension of lexicalized re-
ordering models (LRMs), so we begin by briefly
reviewing the LRM (Tillmann, 2004; Koehn et al,
2007). The goal of an LRM is to characterize how
a phrase-pair tends to be placed with respect to the
block that immediately precedes it. Both the LRM
and the HRM track orientations traveling through
the target from left-to-right as well as right-to-left.
For the sake of brevity and clarity, we discuss only
the left-to-right direction except when stated oth-
erwise. Re-ordering is typically categorized into
three orientations, which are determined by exam-
ining two sequential blocks [si?1, ti?1] and [si, ti]:
? Monotone Adjacent (M): ti?1 = si
? Swap Adjacent (S): ti = si?1
? Disjoint (D): otherwise
Figure 1 shows a simple example, where the first
two blocks are placed in monotone orientation, fol-
lowed by a disjoint ?red?, a swapped ?dog? and a
disjoint period. The probability of an orientation
Oi ? {M,S,D} is determined by a conditional
distribution: Pr(Oi|source phrasei, target phrasei).
Em
ily	 ?
	 ?aim
e	 ?	 ?s
on
	 ?	 ?gr
os	 ?
	 ?ch
ien
	 ?	 ?ro
uge
	 ?	 ?	 ?	 ?.
	 ?
[0,	 ?2]	 ?
Emily	 ?	 ?loves	 ?	 ?
[2,	 ?4]	 ?
her	 ?	 ?big	 ?	 ?
[5,6]	 ?
red	 ?	 ?
[4,5]	 ?
dog	 ?
[6,7]	 ?
.	 ?
Figure 1: A French-to-English translation with 5 blocks.
To build this model, orientation counts can be ex-
tracted from aligned parallel text using a simple
heuristic (Koehn et al, 2007).
The HRM (Galley and Manning, 2008) maintains
similar re-ordering statistics, but determines orienta-
tion differently. It is designed to address the LRM?s
dependence on the previous block [si?1, ti?1]. Con-
sider the period [6,7] in Figure 1. If a different seg-
mentation of the source had preceded it, such as one
that translates ?chien rouge? as a single [4,6] block,
the period would have been in monotone orienta-
tion. Galley and Manning (2008) introduce a de-
terministic shift-reduce parser into decoding, so that
the decoder always has access to the largest possible
previous block, given the current translation history.
The parser has two operations: shift places a newly
translated block on the top of the stack. If the top
two blocks are adjacent, then a reduce is immedi-
ately performed, replacing them with a single block
spanning both. Table 1 shows the parser states cor-
responding to our running example. Whether ?chien
rouge? is translated using [5,6],[4,5] or [4,6] alone,
the shift-reduce parser provides a consolidated pre-
vious block of [0,6] at the top of the stack (shown
with dotted lines). Therefore, [6,7] is placed in
monotone orientation in both cases.
The parser can be easily integrated into a phrase-
based decoder?s translation state, so each partial hy-
pothesis carries its own shift-reduce stack. Time and
memory costs for copying and storing stacks can
be kept small by sharing tails across decoder states.
The stack subsumes the coverage vector in that it
contains strictly more information: every covered
201
Op Stack
S [0,2]
S [0,2],[2,4]
R [0,4]
S [0,4],[5,6]
S [0,4],[5,6],[4,5]
R [0,4],[4,6]
R [0,6]
S [0,6],[6,7]
R [0,7]
Table 1: Shift-reduce states corresponding to Figure 1.
word will be present in one of the stack?s blocks.
However, it can be useful to maintain both.
The top item of a parser?s stack can be approxi-
mated using only the coverage vector. The approx-
imate top is the largest block of covered words that
contains the last translated block. This approxima-
tion will always be as large or larger than the true top
of the stack, and it will often match the true top ex-
actly. For example, in Figure 1, after we have trans-
lated [2,4], we can see that the coverage vector con-
tains all of [0,4], making the approximate top [0,4],
which is also the true top. In fact, this approxima-
tion is correct at every time step shown in Figure 1.
Keep this approximation in mind, as we return to it
in Sections 3.2 and 4.3.
We do not use a shift-reduce parser that consumes
source words from right-to-left;1 therefore, we ap-
ply the above approximation to handle the right-to-
left HRM. Before doing so, we re-interpret the de-
coder state to simulate a right-to-left decoder. The
last block becomes [si, ti] and the next block be-
comes [si?1, ti?1], and the coverage vector is in-
verted so that covered words become uncovered and
vice versa. Taken all together, the approximate test
for right-to-left adjacency checks that any gap be-
tween [si?1, ti?1] and [si, ti] is uncovered in the
original coverage vector.2 Figure 2 illustrates how a
monotone right-to-left orientation can be (correctly)
determined for [2, 4] after placing [5, 6] in Figure 1.
Statistics for the HRM can be extracted from
word-aligned training data. Galley and Manning
(2008) propose an algorithm that begins by run-
1This would require a second, right-to-left decoding pass.
2Galley and Manning (2008) present an under-specified ap-
proximation that is consistent with what we present here.
Prev	 ?
2	 ? 4	 ? 5	 ? 7	 ?6	 ?0	 ?
Next	 ?
Coverage	 ?/	 ?Approx	 ?Top	 ?
Next	 ?
2	 ? 4	 ? 5	 ? 7	 ?6	 ?0	 ?
Prev	 ?
Cov	 ?/	 ?Approx	 ?Top	 ?
Le?-??to-??Right	 ?(Disjoint	 ?[5,6])	 ?
Implied	 ?Right-??to-??Le?	 ?(Monotone	 ?[2,4])	 ?
Figure 2: Illustration of the coverage-vector stack ap-
proximation, as applied to right-to-left HRM orientation.
Phrase	 ?Sou
rce	 ?
Target	 ?
??M	 ?
??S	 ? ??M	 ?
??S	 ?
Figure 3: Relevant corners in HRM extraction. ? indi-
cates left-to-right orientation, and? right-to-left.
ning standard phrase extraction (Och and Ney, 2004)
without a phrase-length limit, noting the corners of
each phrase found. Next, the left-to-right and right-
to-left orientation for each phrase of interest (those
within the phrase-length limit) can be determined by
checking to see if any corners noted in the previous
step are adjacent, as shown in Figure 3.
2.1 Efficient Extraction of HRM statistics
The time complexity of phrase extraction is bounded
by the number of phrases to be extracted, which is
determined by the sparsity of the input word align-
ment. Without a limit on phrase length, a sentence
pair with nwords in each language can have as many
as O(n4) phrase-pairs.3 Because it relies on unre-
stricted phrase extraction, the corner collection step
for determining HRM orientation is also O(n4).
By leveraging the fact that the first step col-
lects corners, not phrase-pairs, we can show that
HRM extraction can actually be done inO(n2) time,
through a process we call corner propagation. In-
stead of running unrestricted phrase-extraction, cor-
ner propagation begins by extracting all minimal
3Consider a word-alignment with only one link in the center
of the grid.
202
So
urc
e	 ?
Target	 ?
??M	 ?
??S	 ? ??M	 ?
??S	 ?
??S	 ???M	 ?
??M	 ???S	 ?
Figure 4: Corner Propagation: Each of the four passes
propagates two types of corners along a single dimension.
phrase-pairs; that is, those that do not include un-
aligned words at their boundaries. The complex-
ity of this step is O(n2), as the number of mini-
mal phrases is bounded by the minimum of the num-
ber of monolingual phrases in either language. We
note corners for each minimal pair, as in the orig-
inal HRM extractor. We then carry out four non-
nested propagation steps to handle unaligned words,
traversing the source (target) in forward and reverse
order, with each unaligned row (column) copying
corners from the previous row (column). Each pass
takes O(n2) time, for a total complexity of O(n2).
This process is analogous to the growing step in
phrase extraction, but computational complexity is
minimized because each corner is considered inde-
pendently. Pseudo-code is provided in Algorithm 1,
and the propagation step is diagrammed in Fig-
ure 4. In our implementation, corner propagation is
roughly two-times faster than running unrestricted
phrase-extraction to collect corners.
Note that the trickiest corners to catch are those
that are diagonally separated from their minimal
block (they result from unaligned growth in both
the source and target). These cases are handled cor-
rectly because each corner type is touched by two
propagators, one for the source and one for the tar-
get (see Figure 4). For example, the top-right-corner
array Aq is populated by both propagate-right and
propagate-up. Thus, one propagator can copy a cor-
ner along one dimension, while the next propagator
copies the copies along the other dimension, moving
the original corner diagonally.
Algorithm 1 Corner Propagation
Initialize target-source indexed binary arrays
Aq[m][n], Ay[m][n], Ap[m][n] and Ax[m][n] to
record corners found in minimal phrase-pairs.
{Propagate Right}
for i from 2 to m s.t. target [i] is unaligned do
for j from 1 to n do
Aq[i][j] = True if Aq[i? 1][j] is True
Ay[i][j] = True if Ay[i? 1][j] is True
{Propagate Up}
for j from 2 to n s.t. source[j] is unaligned do
for i from 1 to m do
Ap[i][j] = True if Ap[i][j ? 1] is True
Aq[i][j] = True if Aq[i][j ? 1] is True
{Propagate Left and Down are similar}
return Aq, Ay, Ap and Ax
3 ITG-Constrained Decoding
Phrase-based decoding places no implicit limits on
re-ordering; all n! permutations are theoretically
possible. This is undesirable, as it leads to in-
tractability (Knight, 1999). Therefore, re-ordering is
limited explicitly, typically using a distortion limit.
One particularly well-studied re-ordering constraint
is the ITG constraint, which limits source permu-
tations to those achievable by a binary bracketing
synchronous context-free grammar (Wu, 1997). ITG
constraints are known to stop permutations that gen-
eralize 3142 and 2413,4 and can drastically limit the
re-ordering space for long strings (Zens and Ney,
2003). There are two methods to incorporate ITG
constraints into a phrase-based decoder, one using
the coverage vector (Zens et al, 2004), and the
other using a shift-reduce parser (Feng et al, 2010).
We begin with the latter, returning to the coverage-
vector constraint later in this section.
Feng et al (2010) describe an ITG constraint that
is implemented using the same permutation parser
used in the HRM. To understand their method, it is
important to note that the set of ITG-compliant per-
mutations is exactly the same as those that can be
reduced to a single-item stack using the shift-reduce
permutation parser (Zhang and Gildea, 2007). In
fact, this manner of parsing was introduced to SMT
42413 is shorthand notation that denotes the block sequence
[1,2],[3,4],[0,1],[2,3] as diagrammed in Figure 5a.
203
Sou
rce	 ?
Target	 ?
0[1,2]4	 ?
[2,3]	 ?
[0,1]	 ?
2[3,4]4	 ?
0[1,2]5	 ?
2[2,3]4	 ?
[0,1]	 ?
[3,4]	 ?
Sou
rce	 ?
2[4,5]5	 ?
Target	 ?(a)	 ? (b)	 ?
Figure 5: Two non-ITG permutations. Violations of po-
tential adjacency are indicated with dotted spans. Bounds
for the one-stack constraint are shown as subscripts.
in order to binarize synchronous grammar produc-
tions (Zhang et al, 2006). Therefore, enforcing
an ITG constraint in the presence of a shift-reduce
parser amounts to ensuring that every shifted item
can eventually be reduced. To discuss this con-
straint, we introduce a notion of potential adjacency,
where two blocks are potentially adjacent if any
words separating them have not yet been covered.
Formally, blocks [s, t] and [s?, t?] are potentially ad-
jacent iff one of the following conditions holds:
? they are adjacent (t? = s or t = s?)
? t? < s and [t?, s] is uncovered
? t < s? and [t, s?] is uncovered
Recall that a reduction occurs when the top two
items of the stack are adjacent. To ensure that re-
ductions remain possible, we only shift items onto
the stack that are potentially adjacent to the cur-
rent top. Figure 5 diagrams two non-ITG permu-
tations and highlights where potential adjacency is
violated. Note that no reductions occur in either
of these examples; therefore, each block [si, ti] is
also the top of the stack at time i. Potential ad-
jacency can be confirmed with some overhead us-
ing the stack and coverage vector together, but Feng
et al (2010) present an elegant three-stack solution
that provides potentially adjacent regions in constant
time, without a coverage vector. We improve upon
their method later this section. From this point on,
we abbreviate potential adjacency as PA.
We briefly sketch a proof that maintaining po-
tential adjacency maintains reducibility, by showing
that non-PA shifts produce irreducible stacks, and
that PA shifts are reducible. It is easy to see that ev-
ery non-PA shift leads to an irreducible stack. Let
[s?, t?] be an item to be shifted onto the stack, and
[s, t] be the current top. Assume that t? < s and the
two items are not PA (the case where t < s? is simi-
lar). Because they are not PA, there is some index k
in [t?, s] that has been previously covered. Since it is
covered, k exists somewhere in the stack, buried be-
neath [s, t]. Because k cannot be re-used, no series
of additional shift and reduce operations can extend
[s?, t?] so that it becomes adjacent to [s, t]. Therefore,
[s, t] will never participate in a reduction, and pars-
ing will close with at least two items on the stack.
Similarly, one can easily show that every PA shift is
reducible, because the uncovered space [t?, s] can be
filled by extending the new top toward the previous
top using strictly adjacent shifts.
3.1 A One-stack ITG Constraint
As mentioned earlier, Feng et al (2010) provide a
method to track potential adjacency that does not re-
quire a coverage vector. Instead, they maintain three
stacks, the original stack and two others to track po-
tentially adjacent regions to the left and right respec-
tively. These regions become available to the de-
coder only when the top of the original stack is ad-
jacent to one of the adjacency stacks.
We show that the same goal can be achieved with
even less book-keeping by augmenting the items on
the original stack to track the regions of potential
adjacency around them. The intuition behind this
technique is that on a shift, the new top inherits all
of the constraints on the old top, and the old top be-
comes a constraint itself. Each stack item now has
four fields, the original block [s, t], plus a left and
right adjacency bound, denoted together as `[s, t]r,
where ` and r are indices for the maximal span con-
taining [s, t] that is uncovered except for [s, t]. If the
top of the stack is `[s, t]r, then shifted items must fall
inside one of the two PA regions, [`, s] or [t, r]. The
region shifted into determines new item?s bounds.
The stack is initialized with a special 0[0, 0]n item,
and we then shift unannotated blocks onto the stack.
As we shift [s?, t?] onto the stack, rules derive bounds
`? and r? for the new top based on the old top `[s, t]r:
? Shift-left (t? ? s): `? = `, r? = s
? Shift-right (t ? s?): `? = t, r? = r
204
[2,4]	 ?
[5,7]	 ?
0	 ? 9	 ?
Shi?	 ?[5,7]	 ?
4	 ? 9	 ?
[2,4]	 ? 9	 ?0	 ?
[2,7]	 ?
[4,7]	 ?
0	 ? 9	 ?
Reduce	 ?
4	 ? 9	 ?
[2,4]	 ? 9	 ?0	 ?
(a)	 ? (b)	 ?
Figure 6: Two examples of boundaries for the one-stack solution for potential adjacency. Stacks are built from bottom
to top, blocks indicate [s,t] blocks, while tails are left and right adjacency boundaries.
Meanwhile, when reducing a stack with `? [s
?, t?]r?
at the top and `[s, t]r below it, the new top simply
copies ` and r. The merged item is larger than [s, t],
but it is PA to the same regions. Figure 6 diagrams
a shift-right and a reduce, while Figure 5 annotates
bounds for blocks during its ITG violations.
3.2 The Coverage-Vector ITG Constraint is
Incomplete
The stack-based solution for ITG constraints is el-
egant, but there is also a proposed constraint that
uses only the coverage vector (Zens et al, 2004).
This constraint can be stated with one simple rule:
if the previously translated block is [si?1, ti?1] and
the next block to be translated is [si, ti], one must
be able to travel along the coverage vector from
[si?1, ti?1] to [si, ti] without transitioning from an
uncovered word to a covered word. Feng et al
(2010) compare the two ITG constraints, and show
that they perform similarly, but not identically. They
attribute the discrepancy to differences in when the
constraints are applied, which is strange, as the two
constraints need not be timed differently.
Let us examine the coverage-vector constraint
more carefully, assuming that ti < si?1 (the case
where ti?1 < si is similar). The constraint consists
of two phases: first, starting from si?1, we travel to
the left toward ti, consuming covered words until we
reach the first uncovered word. We then enter into
the second phase, and the path must remain uncov-
ered until we reach ti. The first step over covered
positions corresponds to finding the left boundary
of the largest covered block containing [si?1, ti?1],
which is an approximation to the top of the stack
(Section 2). The second step over uncovered posi-
tions corresponds to determining whether [si, ti] is
PA to the approximate top. That is, the coverage-
vector ITG constraint checks for potential adjacency
using the same top-of-stack approximation as the
right-to-left HRM.
This implicit approximation implies that there
may well be cases where the coverage-vector con-
straint makes the wrong decision. Indeed this is
the case, which we prove by example. Consider
the irreducible sequence 25314, illustrated in Fig-
ure 5b. This non-ITG permutation is allowed by
the coverage-vector approximation, but not by the
stack-based constraint. Both constraints allow the
placement of the first three blocks [1, 2], [4, 5] and
[2, 3]. After adding [0, 1], the stack-based solution
detects a PA-violation. Meanwhile, the vector-based
solution checks the path from 2 to 1 for a transition
from uncovered to covered. This short path touches
only covered words. Similarly, as we add [3, 4], the
path from 1 to 3 is also completely covered. The
entire permutation is accepted without complaint.
The proof provided by Zens et al (2004) misses
this case, as it accounts for phrasal generalizations
of the 2413 ITG-forbidden substructure, but it does
not account for generalizations where the substruc-
ture is interrupted by a discontiguous item, such as
in 25{3}14, where 2413 is revealed not by merging
items but by deleting 3.
4 Inconsistencies in HRM parsing
We have shown that the HRM and the ITG con-
straints for phrase-based decoding use the same de-
terministic shift-reduce parser. The entirety of the
ITG discussion was devoted to preventing the parser
from reaching an irreducible state. However, up
until now, work on the HRM has not addressed
the question of irreducibility (Galley and Manning,
2008; Nguyen et al, 2009).
Irreducible derivations do occur during HRM de-
coding, and when they do, they can create inconsis-
tencies with respect to HRM extraction from word-
205
?  
?  
??
  ?
?  
?  
??
  ?
?  
??
 	 ??	 ?
[4,6]	 ?How	 ?can	 ?
[0,1]	 ?you	 ?
[6,7]	 ?achieve	 ?
[1,2]	 ?the	 ?
[3,4]	 ?economic	 ?and	 ?
[2,3]	 ?tourism	 ?
[7,9]	 ?benefits	 ??	 ?
Figure 7: An example irreducible derivation, drawn from
our Chinese-to-English decoder?s k-best output.
Last translated block 2-red *-red approx
How can [4, 6] [4,6] [4,6] [4,6]
you [0, 1] [0,1] [0,1] [0,1]
achieve [6, 7] [6,7] [6,7] [4,7]
the [1, 2] [1,2] [1,2] [0,2]
economic and [3, 4] [3,4] [3,4] [3,7]
tourism [2, 3] [1,4] [0,7] [0,7]
benefits? [7, 9] [7,9] [0,9] [0,9]
Table 2: Top of stack at each time step in Figure 7, under
2-reduction (as in the original HRM), *-reduction, and
the coverage-vector approximation.
aligned training data. In Figure 7, we show an ir-
reducible block sequence, extracted from a Chinese-
English decoder. The parser can perform a few small
reductions, creating a [1,4] block indicated with a
dashed box, but translation closes with 5 items on
the stack. One can see that [7,9] is assigned a dis-
joint orientation by the HRM. However, if the same
translation and alignment were seen during train-
ing, the unrestricted phrase extractor would find a
phrase at [0,7], indicated with a dotted box, and [7,9]
would be assigned monotone orientation. This in-
consistency penalizes this derivation, as ?benefits ??
is forced into an unlikely disjoint orientation. One
potential implication is that the decoder will tend
to avoid irreducible states, as those states will tend
to force unlikely orientations, resulting in a hidden,
soft ITG-constraint. Indeed, our decoder does not
select this hypothesis, but instead a (worse) transla-
tion that is fully reducible. The impact of these in-
consistencies on translation quality can only be de-
termined empirically. However, to do so, we require
alternatives that address these inconsistencies. We
describe three such variants below.
4.1 ITG-constrained decoding
Perhaps the most obvious way to address irreducible
states is to activate ITG constraints whenever decod-
ing with an HRM. Irreducible derivations will disap-
pear from the decoder, along with the corresponding
inconsistencies in orientation. Since both techniques
require the same parser, there is very little overhead.
However, we will have also limited our decoder?s re-
ordering capabilities.
4.2 Unrestricted shift-reduce parsing
The deterministic shift-reduce parser used through-
out this paper is actually a special case of a general
class of permutation parsers, much in the same way
that a binary ITG is a special case of synchronous
context-free grammar. Zhang and Gildea (2007) de-
scribe a family of k-reducing permutation parsers,
which can reduce the top k items of the stack in-
stead of the top 2. For k ? 2 we can generalize the
adjacency requirement for reduction to a permuta-
tion requirement. Let {[si, ti]|i=1. . . k} be the top k
items of a stack; they are a permutation iff:
max
i
(ti)?min
i
(si) =
?
i
[ti ? si]
That is, every number between the max and min is
present somewhere in the set. Since two adjacent
items always fulfill this property, we know the orig-
inal parser is 2-reducing. k-reducing parsers reduce
by moving progressively deeper in the stack, looking
for the smallest 2 ? i ? k that satisfies the permu-
tation property (see Algorithm 2). As in the original
parser, a k-reduction is performed every time the top
of the stack changes; that is, after each shift and each
successful reduction.
If we set k = ?, the parser will find the small-
est possible reduction without restriction; we refer
to this as a *-reducing parser. This parser will never
reach an irreducible state. In the worst case, it re-
duces the entire permutation as a single n-reduction
after the last shift. This means it will exactly mimic
unrestricted phrase-extraction when predicting ori-
entations, eliminating inconsistencies without re-
stricting our re-ordering space. The disadvantage is
206
Algorithm 2 k-reduce a stack
input stack {[si, ti]|i = 1 . . . l}; i = 1 is the top
input max reduction size k, k ? 2
set s? = s1; t? = t1; size = t1 ? s1
for i from 2 to min(k, l) do
set s? = min(s?, si); t? = max(t?, ti)
set size = size + (ti ? si)
if t? ? s? == size then
pop {[sj , tj ]|j = 1 . . . i} from the stack
push [s?, t?] onto the stack;
return true // successful reduction
return false // failed to reduce
that reduction is no longer a constant-time operation,
but is insteadO(n) in the worst case (consider Algo-
rithm 2 with k =? and l = n items on the stack).5
As a result, we will carefully track the impact of this
parser on decoding speed.
4.3 Coverage vector approximation
One final option is to adopt the top-of-stack approxi-
mation for left-to-right orientations, in addition to its
current use for right-to-left orientations, eliminating
the need for any permutation parser. The next block
[si, ti] is adjacent to the approximate top of the stack
only if any space between [si, ti] and the previous
block [si?1, ti?1] is covered. But before committing
fully to this approximation, we should better under-
stand it. Thus far, we have implied that this approx-
imation can fail to predict correct orientations, but
we have not specified when these failures occur. We
now show that incorrect orientations can only occur
while producing a non-ITG permutation.
Let [si?1, ti?1] be the last translated block, and
[si, ti] be the next block. Recall that the approxima-
tion determines the top of the stack using the largest
block of covered words that contains [si?1, ti?1].
The approximate top always contains the true top,
because they both contain [si?1, ti?1] and the ap-
proximate top is the largest block that does so.
Therefore, the approximation errs on the side of ad-
jacency, meaning it can only make mistakes when
5Zhang and Gildea (2007) provide an efficient algorithm for
*-reduction that uses additional book-keeping so that the num-
ber of permutation checks as one traverses the entire sequence
is linear in aggregate; however, we implement the simpler, less
efficient version here to simplify decoder integration.
Prev	 ? Next	 ?
si-??1	 ? ti-??1	 ? si	 ? ti	 ?t?	 ?
True	 ?top	 ?
Approximate	 ?top	 ?
Breaks	 ?
PA	 ?
Figure 8: Indices for when the coverage approximation
predicts a false M.
assigning an M or S orientation; if it assigns a D, it
is always correct. Let us consider the false M case
(the false S case is similar). If we assign a false M,
then ti?1 < si and si is adjacent to the approximate
top; therefore, all positions between ti?1 and si are
covered. However, since the M is false, the true top
of the stack must end at some t? : ti?1 ? t? < si.
Since we know that every position between t? and si
is covered, [si, ti] cannot be PA to the true top of the
stack, and we must be in the midst of making a non-
ITG permutation. See Figure 8 for an illustration of
the various indices involved. As it turns out, both the
approximation and the 2-reducing parser assign in-
correct orientations only in the presence of ITG vio-
lations. However, the approximation may be prefer-
able, as it requires only a coverage vector.
4.4 Qualitative comparison
Each solution manages its stack differently, and we
illustrate the differences in terms of the top of the
stack at time i in Table 2. The *-reducing parser is
the gold standard, so we highlight deviations from
its decisions in bold. As one can see, the original 2-
reducing parser does fine before and during an ITG
violation, but can create false disjoint orientations
after the violation is complete, as the top of its stack
becomes too small due to missing reductions. Con-
versely, the coverage-vector approximation makes
errors inside the violation: the approximate top be-
comes too large, potentially creating false monotone
or swap orientations. Once the violation is complete,
it recovers nicely.
5 Experiments
We compare the LRM, the HRM and the three HRM
variants suggested in Section 4 on a Chinese-to-
English translation task. We measure the impact on
translation quality in terms of BLEU score (Papineni
et al, 2002), as well as the impact on permutation
207
BLEU NIST 08 Complexity Counts Speed
Method nist04 nist06 nist08 > 2 4 5 6 7 ? 8 sec/sent
LRM 38.00 33.79 27.12 241 146 40 32 12 11 3.187
HRM 2-red 38.53 34.20 27.57 176 113 31 20 8 4 3.353
HRM apprx 38.58 34.09 27.60 280 198 41 26 13 2 3.231
HRM *-red 38.39 34.22 27.41 328 189 71 34 20 14 3.585
HRM itg 38.70 34.26 27.33 0 0 0 0 0 0 3.274
Table 3: Chinese-to-English translation results, comparing the LRM and 4 HRM variants: the original 2-reducing
parser, the coverage vector approximation, the *-reducing parser, and an ITG-constrained decoder.
complexity, as measured by the largest k required to
k-reduce the translations.
5.1 Data
The system was trained on data from the NIST 2009
Chinese MT evaluation, consisting of more than
10M sentence pairs. The training corpora were split
into two phrase tables, one for Hong Kong and UN
data, and one for all other data. The dev set was
taken from the NIST 05 evaluation set, augmented
with some material reserved from other NIST cor-
pora; it consists of 1.5K sentence pairs. The NIST
04, 06, and 08 evaluation sets were used for testing.
5.2 System
We use a phrase-based translation system similar to
Moses (Koehn et al, 2007). In addition to our 8
translation model features (4 for each phrase table),
we have a distortion penalty incorporating the min-
imum possible completion cost described by Moore
and Quirk (2007), a length penalty, a 5-gram lan-
guage model trained on the NIST09 Gigaword cor-
pus, and a 4-gram language model trained on the tar-
get half of the parallel corpus. The LRM and HRM
are represented with six features, with separate
weights for M, S and D in both directions (Koehn et
al., 2007). We employ a gap constraint as our only
distortion limit (Chang and Collins, 2011). This re-
stricts the maximum distance between the start of a
phrase and the earliest uncovered word, and is set to
7 words. Parameters are tuned using a batch-lattice
version of hope-fear MIRA (Chiang et al, 2008;
Cherry and Foster, 2012). We re-tune parameters
for each variant.
5.3 Results
Our results are summarized in Table 3. Speed and
complexity are measured on the NIST08 test set,
which has 1357 sentences. We measure permutation
complexity by parsing the one-best derivations from
each system with an external *-reducing parser, and
noting the largest k-reduction for each derivation.
Therefore, the>2 column counts the number of non-
ITG derivations produced by each system.
Regarding quality, we have verified the effective-
ness of the HRM: each HRM variant outperforms
the LRM, with the 2-reducing HRM doing so by 0.4
BLEU points on average. Unlike Feng et al (2010),
we see no consistent benefit from adding hard ITG
constraints, perhaps because we are building on an
HRM-enabled system. In fact, all HRM variants
perform more or less the same, with no clear win-
ner emerging. Interestingly, the approximate HRM
is included in this pack, which implies that groups
wishing to augment their phrase-based decoder with
an HRM need not incorporate a shift-reduce parser.
Regarding complexity, the 2-reducing HRM pro-
duces about half as many non-ITG derivations as the
*-reducing system, confirming our hypothesis that
a 2-reducing HRM acts as a sort of soft ITG con-
straint. Both the approximate and *-reducing de-
coders produce more violating derivations than the
LRM. This is likely due to their encouragement of
more movement overall. The largest reduction we
observed was k = 11.
Our speed tests show that all of the systems trans-
late at roughly the same speed, with the LRM being
fastest and the *-reducing HRM being slowest. The
*-reducing system is less than 7% slower than the 2-
reducing system, alleviating our concerns regarding
the cost of *-reduction.
208
6 Discussion
We have presented a number of theoretical contribu-
tions on the topic of phrase-based decoding with an
on-board permutation parser. In particular, we have
shown that the coverage-vector ITG constraint is ac-
tually incomplete, and that the original HRM can
produce inconsistent orientations in the presence of
ITG violations. We have presented three HRM vari-
ants that address these inconsistencies, and we have
compared them in terms of both translation quality
and permutation complexity. Though our results in-
dicate that a permutation parser is actually unneces-
sary to reap the benefits of hierarchical re-ordering,
we are excited about the prospects of further ex-
ploring the information provided by these on-board
parsers. In particular, we are interested in using fea-
tures borrowed from transition-based parsing while
decoding.
References
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through la-
grangian relaxation. In EMNLP, pages 26?37, Edin-
burgh, Scotland, UK., July.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In HLT-
NAACL, Montreal, Canada, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, pages 224?233.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrase-
based machine translation. In COLING, pages 285?
293, Beijing, China, August.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP, pages 848?856, Honolulu, Hawaii,
October.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
HLT-NAACL, pages 966?974, Los Angeles, Califor-
nia, June.
Kevin Knight. 1999. Squibs and discussions: Decod-
ing complexity in word-replacement translation mod-
els. Computational Linguistics, 25(4):607?615, De-
cember.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180, Prague, Czech Republic, June.
Robert C. Moore and Chris Quirk. 2007. Faster beam-
search decoding for phrasal statistical machine trans-
lation. In MT Summit XI, September.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving a lexi-
calized hierarchical reordering model using maximum
entropy. In MT Summit XII, Ottawa, Canada, August.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4), December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In HLT-NAACL,
pages 101?104, Boston, USA, May.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL, pages 144?151.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In COL-
ING, pages 205?211, Geneva, Switzerland, August.
Hao Zhang and Daniel Gildea. 2007. Factorization
of synchronous context-free grammars in linear time.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 25?32, Rochester, New York, April.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In HLT-NAACL, pages 256?263, New
York City, USA, June.
209
