Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1773?1782, Dublin, Ireland, August 23-29 2014.
Morphological Analysis for Japanese Noisy Text
Based on Character-level and Word-level Normalization
SAITO Itsumi, SADAMITSU Kugatsu, ASANO Hisako and MATSUO Yoshihiro
NTT Media Intelligence Laboratories
{saito.itsumi, sadamitsu.kugatsu,
asano.hisako, matsuo.yoshihiro}@lab.ntt.co.jp
Abstract
Social media texts are often written in a non-standard style and include many lexical variants
such as insertions, phonetic substitutions, abbreviations that mimic spoken language. The nor-
malization of such a variety of non-standard tokens is one promising solution for handling noisy
text. A normalization task is very difficult to conduct in Japanese morphological analysis because
there are no explicit boundaries between words. To address this issue, in this paper we propose a
novel method for normalizing and morphologically analyzing Japanese noisy text. We generate
both character-level and word-level normalization candidates and use discriminative methods to
formulate a cost function. Experimental results show that the proposed method achieves accept-
able levels in both accuracy and recall for word segmentation, POS tagging, and normalization.
These levels exceed those achieved with the conventional rule-based system.
1 Introduction
Social media texts attract a lot of attention in the fields of information extraction and text mining. Al-
though texts of this type contain a lot of information, such as one?s reputation or emotions, they often
contain non-standard tokens (lexical variants) that are considered out-of-Vocabulary (OOV) terms. We
define an OOV as a word that does not exist in the dictionary. Texts in micro-blogging services such
as Twitter are particularly apt to contain words written in a non-standard style, e.g., by lengthening
them (?goooood? for ?good?) or abbreviating them (?thinkin? ? for ?thinking?). This is also seen in the
Japanese language, which has standard word forms and variants of them that are often used in social
media texts. To take one word as an example, the standard form is???? (oishii, ?It is delicious?) and
its variants include ???????(oishiiiii), ??? (?oishii), and ????(oishii), where the un-
derlined characters are the differences from the standard form. Such non-standard tokens often degrade
the accuracy of existing language processing systems, which are trained using a clean corpus.
Almost all text normalization tasks for languages other than Japanese (e.g., English), aim to replace
the non-standard tokens that are explicitly segmented using the context-appropriate standard words (Han
et al. (2012), Han and Baldwin (2011), Hassan and Menezes (2013), Li and Liu (2012), Liu et al. (2012),
Liu et al. (2011), Pennell and Liu (2011), Cook and Stevenson (2009), Aw et al. (2006)). On the other
hand, the problem is more complicated in Japanese morphological analysis because Japanese words are
not segmented by explicit delimiters. In traditional Japanese morphological analysis, word segmentation
and part-of-speech (POS) tagging are simultaneously estimated. Therefore, we have to simultaneously
analyze normalization, word segmentation, and POS tagging to estimate the normalized form using the
context information. For example, the input ??????? ???(pan-keiki oishiiii, ?This pancake
tastes good?) written in the standard form is????????? (pan-keiki oishii). The result obtained
with the conventional Japanese morphological analyzer MeCab (Kudo (2005)) for this input is????
? (pancake, noun)/? ?? (unk)/? (unk)/? (unk)/, where slashes indicate the word segmentations and
?unk? means an unknown word. As this result shows, Japanese morphological analyzers often fail to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1773
correctly estimate the word segmentation if there are unknown words, so the pipeline method (e.g., first
estimating the word segmentations and then estimating the normalization forms) is unsuitable.
Moreover, Japanese has several writing scripts, the main ones being Kanji, Hiragana, and Katakana.
Each word has its own formal written script (e.g., ??? (kyoukasyo, ?textbook?) as formally written
in Kanji), but in noisy text, there are many words that are intentionally written in a different script
(e.g., ?????? (kyoukasyo, ?textbook?) is the Hiragana form of???). These tokens written in
different script also degrade the performance of existing systems because dictionaries basically include
only the standard script. Unlike the character-level variation we described above, this type of variation
occurs on a word?level one. Therefore, there are both character-level and word-level non-standard
tokens in Japanese informal written text. Several normalization approaches have been applied to Japanese
text. Sasano et al. (2013) and Oka et al. (2011) introduced simple character level derivational rules for
Japanese morphological analysis that are used to normalize specific patterns of non-standard tokens, such
as for word lengthening and lower-case substitution. Although these approaches handle Japanese noisy
text fairly effectively, they can handle only limited kinds of non-standard tokens.
We propose a novel method of normalization in this study that can handle both character- and word-
level lexical variations in one model. Since it automatically extracts character-level transformation pat-
terns in character-level normalization, it can handle many types of character-level transformations. It
uses two steps (character- and word-level) to generate normalization candidates, and then formulates a
cost function of the word sequences as a discriminative model. The contributions this research makes
can be summarized by citing three points. First, the proposed system can analyze a wider variety of
non-standard token patterns than the conventional system by using our two-step normalization candidate
generation algorithms. Second, it can largely improve the accuracy of Japanese morphological analysis
for non-standard written text by simultaneously performing the normalization and morphological analy-
ses. Third, it can automatically extract character alignments and in so doing reduces the cost of manually
creating many types of transformation patterns. The rest of this paper is organized as follows. Section 2
describes the background to our research, including Japanese traditional morphological analysis, related
work, and data collection methods. Section 3 introduces the proposed approach, which includes lattice
generation and formulation, as a discriminative model. Section 4 discusses experiments we performed
and our analyses of the experimental results. Section 5 concludes the paper with a brief summary and a
mention of future work.
2 Background
2.1 Japanese Morphological Analysis
Many approaches to joint word segmentation and POS tagging including Japanese Morphological anal-
ysis can be interpreted as re-ranking while using a word lattice (Kaji and Kitsuregawa (2013)). There
are two points to consider in the analysis procedure: how to generate the word lattice and how to formu-
late the cost of each path. In Japanese morphological analysis, the dictionary-based approach has been
widely used to generate the word lattice (Kudo et al. (2004), Kurohashi et al. (1994)). In a traditional
approach, an optimal path is sought by using the sum of the two types of costs for the path: the cost
for a candidate word that reflects the word?s occurrence probability, and the cost for a pair of adjacent
POS that reflects the probability of an adjacent occurrence of the pair (Kudo et al. (2004), Kurohashi et
al. (1994)). A greater cost means less probability. The Viterbi algorithm is usually used for finding the
optimal path.
2.2 Related Work
Several studies have been conducted on Japanese morphological analysis in the normalized form. The
approach proposed by Sasano et al. (2013) aims to develop heuristics to flexibly search by using a simple,
manually created derivational rule. Their system generates normalized character sequence based on the
derivational rule, and adding new nodes that are generated from normalized character sequence when
generating the word lattice using dictionary lookup. Figure 1 presents an example of this approach.
If the non-standard written sentence ??????? (suugoku tanoshii, ?It is such fun?) is input, the
1774
Figure 1: Example of Japanese morphological analysis and normalization
type non-standard form standard form
(1) Insertion ??????? (arigatoou) ????? (arigatou, ?Thank you?)
(2) Deletion ?? (samu) ??? (samui, ?cold?)
(3) Substitution with phonetic variation ???? (kawaee) ???? (kawaii, ?cute?)
(4) Substitution with lowercases and uppercases ????? (arigatou) ????? (arigatou, ?Thank you?)
(5) Hiragana substitution ????? (aidei) ID (aidei, ?identification card?)
(6) Katakana substitution ????? (arigatou) ????? (arigatou, ?Thank you?)
(7) Any combination of (1) to (6) ????? (kaunta) ????? (kaunta, ?counter?)
???? (attsui) ??? (atsui, ?hot?)
Table 1: Types of non-standard tokens and examples of annotated data
traditional dictionary-based system generates Nodes that are described using solid lines, as shown in Fig.
1. Since ?????? (suugoku, ?such?) and ????? (tanoshii, ?fun?) are OOVs, the traditional system
cannot generate the correct word segments or POS tags. However, their system generates additional
nodes for the OOVs, shown as broken line rectangles in Fig. 1. In this case, derivational rules that
substitute ??? with ?null? and ??? (i) with ??? (i) are used and the system can generate the standard
forms ????? (sugoku, ?such?) and ????? (tanoshii, ?fun?) and their POS tags. If we can generate
sufficiently appropriate rules, these approaches seem to be effective. However, there are many types of
derivational patterns in SNS text and it is difficult to cover all of them by hand. Moreover, it becomes a
serious problem how to set the path cost for appropriately re-ranking the word lattice when the number
of candidates increases. Our approach is also based on the dictionary-based approach, however, our
approach is significantly dissimilar from their approach in two ways. First, we automatically generate
derivational patterns (we call them transformation tables) based on the character-level alignment between
non-standard tokens and their standard forms. Compared to generating the rules by hand, our approach
can generate broad coverage rules. Second, we use discriminative methods to formulate a cost function.
Jiang et al. (2008), Kaji and Kitsuregawa (2013) introduce several features to appropriately re-rank the
added nodes. This enables our system to perform well even when the number of candidates increases.
On the other hand, several studies have applied a statistical approach. For example, Sasaki et al.
(2013) proposed a character-level sequential labeling method for normalization. However, it handles
only one-to-one character transformations and does not take the word-level context into account. The
proposed method can handle many-to-many character transformations and takes word-level context into
account, so the scope for handling non-standard tokens is different. Many studies have been done on text
normalization for English; for example Han and Baldwin (2011) classifies whether or not OOVs are non-
standard tokens and estimates standard forms on the basis of contextual, string, and phonetic similarities.
In these studies it was assumed that clear word segmentations existed. However, since Japanese is an
unsegmented language the normalization problem needs to be treated as a joint normalization, word
segmentation, and POS tagging problem.
2.3 Data Collection and Analysis of Non-standard Tokens
In previous studies (Hassan and Menezes (2013), Ling et al. (2013), Liu et al. (2011)), the researchers
proposed unsupervised ways to extract non-standard tokens and their standard forms. For Japanese text,
however, it is very difficult to extract word pairs in an unsupervised way because there is no clear word
segmentation. To address this problem we first extracted non-standard tokens from Twitter text and blog
1775
Figure 2: Structure of proposed system
Figure 3: Example of candidate generation
text and manually annotated their standard (dictionary) forms. In total, we annotated 4808 tweets and
8023 blog text sentences. Table 1 lists the types of non-standard tokens that we targeted in this study
and examples of the annotated data. Types (1), (2), (3) and (4) are similar to English transform patterns.
Types (5) and (6) are distinctive patterns in Japanese. As previously mentioned Japanese has several
kinds of scripts, the main ones being Kanji, Hiragana, and Katakana. These scripts can be used to write
the same word in several ways. For example, the dictionary entry ?? (sensei, ?teacher?) can also
be written in Hiragana form ???? (sensei) or Katakana form ???? (sensei). Most words are
normally written in the standard form, but in informal written text (e.g., Twitter text), these same words
are often written in a non-standard form. In examining Twitter data for such non-standard tokens, we
found that 55.0% of them were types (1) to (3) in Table 1, 4.5% were type (4), 20.1% were types (5)
to (6), 2.7% were type (7), and the rest did not fall under any of these types since they were the result
of dialects, typos, and other factors. In other words, a large majority of the non-standard tokens fell
under types (1) to (7). We excluded those that did not as targets in this study because our proposed
method cannot easily handle them. Types (1) to (4) occur at character-level and so can be learned from
character-level alignment, but types (5) to (6) occur at word-level and it is inefficient to learn them on
a character?level basis. Accordingly, we considered generating candidates and features on two levels:
character-level and word-level.
3 Proposed Method
3.1 Overview of Proposed System
We showed the structure of the proposed system in Fig. 2. Our approach adds possible normalization
candidates to a word lattice and finds the best sequence using a Viterbi decoder based on a discriminative
model. We introduced several features that can be used to appropriately evaluate the confidence of the
added nodes as normalization candidates. We generate normalization candidates as indicated in Fig. 3.
1776
Figure 4: Example of character alignment
We describe the details in the following section.
3.2 Character-level Lattice
3.2.1 Character Alignment between Non-standard Tokens and Their Normalized Forms
We have to create a character-level transformation table to generate the character-level lattice. We used
the joint multigram model proposed by Sittichai et al. (2007) to create the transformation table because
this model can handle many-to-many character alignments between two character sequences. In ob-
serving non-standard tokens and their standard forms, we find there are not only one-to-one character
transformations but also many-to-many character transformations. Furthermore, unlike in translation,
there is no character reordering so the problems that arise are similar to those in transliteration. Accord-
ingly, we adopted a joint multigram model that is widely used for transliteration problems. The optimal
alignment can be formulated as q? = arg max
q?K
d
?
q?q
p(q) , where d is a pair of non-standard tokens
and its standard form (e.g., d is?????? (arigatoou), ????? (arigatou). Here, q is a partial
character alignment in d (e.g., q is ????, ???), q is the character alignment q set in d (e.g., q of
path 1 in Fig. 4 is {(??, ??), (??, ??), (??, ??), (????, ???)}. K
d
is the possible character
alignment sequence candidates generated from d. We generate n-best optimal path for K
d
in this study.
The maximum likelihood training can be performed using the EM algorithm derivated in Bisani and Ney
(2008) and Kubo et al. (2011) to estimate p(q). p(q) can be formulated as follow:
p(q) = ?
q
/
?
q?Q
?
q
(1)
?
q
=
?
d?D
?
q?K
d
p(q)n
q
(q) =
?
d?D
?
q?K
d
?
q?q
p?(q)
?
q?K
d
?
q?q
p?(q)
n
q
(q),
and where D is the number of the d pair, Q is the set of q, and n
q
(q) is the count of q that occurred in
q. In our system, we allow for standard form deletions (i.e., mapping of a non-standard character to a
null standard character) but not non-standard token deletions. Since we use this alignment as the trans-
formation table when generating a character-level lattice, the lattice size becomes unnecessarily large
if we allow for non-standard form deletions. In the calculation step of the EM algorithm, we calculate
the expectation (partial counts) ?
q
of each alignment in the E-step, calculate the joint probability p(q)
that maximizes the likelihood function in the M-step as described before, and repeat these steps until
convergence occurs. p?(q) indicates the result of p(q) calculated in the previous step over the iteration.
When generating the character-level lattice, we used alignments that were expected to exceed a prede-
fined threshold. We used ?
q
(q = (c
t
, c
v
)) and r(c
t
, c
v
) as thereshold, where c
t
and c
v
are the partial
character sequence of non-standard token and it?s standard form respectively. r(c
t
, c
v
) is calculated by
r(c
t
, c
v
) = ?
q
/n
c
v
., where n
c
v
is the number of occurrences of c
v
in the training data. We set the thresh-
old ?
q thres
= 0.5 , and r(c
t
, c
v
)
thres
= 0.0001 in this study. We also used r(c
t
, c
v
) as a feature of cost
1777
function in subsection. 3.4.2. When calculating initial value, we set p(c
t
, c
v
) high if the character c
t
and
c
v
are the same character and the length of each character is 1. We also give the limitation that a Kanji
character does not change to a different character and is aligned with same character in the calculation
step of the character alignment.
3.2.2 Generation of Character-level Lattice Based on Transformation Table
First, repetitions of more than one letter of ???, ???, ?-?, and ??? are reduced back to one letter (e.g.,
???????? (arigatooooou, ?Thank you?) is reduced to ?????? (arigatoou)) for the
input text. In addition, repetitions of more than three letters other than ???, ???, ?-?, and ??? are
reduced back to three letters (e.g.,???????? (uresiiiiiii, ?I?m happy?) is reduced back to??
???? (uresiiii)). These preprocessing rules are inspired by Han and Baldwin (2011) and determined
by taking the Japanese characteristics into consideration. We also used these rules when we estimated the
alignments of the non-standard tokens and their standard forms. Next, we generate the character-level
normalization candidates if they match the key transformation table in the input text. For example, if the
transformation table contains (q, logp(q))= (??? (yoo), ?? (you)?, -8.39), (?? (o), ? (o)?, -7.56),
and the input text includes the character sequence ???? ? (tyoo), we generate a new sequence ?????
(tyou) and ????? (tyoo). In other words, we add new nodes ???? (you) and ??? (o) in the position
of ??? ? (yoo) and ??? (o), respectively (see Fig. 3).
3.3 Generation of Word-level Lattice
We generate the word lattice based on the generated character-level lattice using dictionary lookup. We
exploit dictionary lookup by using the possible character sequence of the character-level lattice while
the traditional approach exploits it by using only the input character sequence. For example, we exploit
dictionary lookup for character sequences such as ???? ????? (tyoo kawaii) and ?????????
(tyou kawaii) and ????????? (chiyou kawaii) and ???? ????? (tyoo kawaii) (see Fig. 3)
Furthermore, we use the phonetic information of the dictionary to generate the normalization candi-
dates for Hiragana and Katakana substitution. For example, assume ??? (tyou, ?super?) and ??????
(kawaii, ?cute?) are the dictionary words. Then, if the input text contains the character sequences ???
?? (tyo) (which is written in Hiragana) and ?????? (kawaii) (which is written in Katakana), we add
??? (tyo, ?super?) and ?????? (kawaii, ?cute?) to the word lattice as the normalization candidates
since the two character sequences are pronounced identically. By using this two-step algorithm, we can
handle any combinational derivational patterns, such as Katakana substitutions or substitutions of lower-
cases like ?????? (kawaii)? ?????? (kawaii)? ?????? (kawaii, ?cute?) (see Fig. 3). Note
that we filtered candidates on the basis of a predefined threshold to prevent the generation of unneces-
sary candidates. The threshold was defined on the basis of the character sequence cost of normalization,
which is described in subsection 3.4.2. Furthermore, we limited the number of character transformations
to two per word.
3.4 Decoder
3.4.1 Objective Function
The decoder selects the optimal sequence y? from L(s) when given the candidate set L(s) for sentence
s. This is formulated as y? = arg min
y?L(s)
w ? f(y) (Jiang et al. (2008), Kaji and Kitsuregawa (2013)), where
y? is the optimal path, L(s) is the lattice created for sentence s, and w ? f(y) is the dot product between
weight vector w and feature vector f(y). The optimal path is selected according to the w ? f(y) value.
3.4.2 Features
The proposed lattice generation algorithm generates a lattice larger than that generated in traditional
dictionary-based lattice generation. Therefore, we need to introduce an appropriate normalization cost
into the objective function. We listed the features we used in Table 2. Let w
i
be the ith word candidate
and p
i
be the POS tag of w
i
. p
i?1
andw
i?1
are adjacent POS tag and word respectively. We also used the
word unigram cost f
w
i
p
i
, the cost for a pair of adjacent POS f
p
i?1
,p
i
that are quoted from MeCab (Kudo,
1778
Name Feature
Word unigram cost f
w
i
p
i
POS bi-gram cost f
p
i?1
,p
i
Word-POS bi-gram cost ?logp
w
i?1
p
i?1
,w
i
p
i
Character sequence cost log(p?
s
/p
?
t
i
)
where, p?
x
= p
1/length(x)
x
, p
x
=
?
n
j=1
p(c
j
|c
j?1
j?5
), x ? {s, t
i
}
Character transformation cost ?
trans
i
? (?logr(c
t
, c
v
))
Hiragana substitution cost ?
h
i
? f
w
i
p
i
Katakana substitution cost ?
k
i
? f
w
i
p
i
Table 2: Feature list of the decoder. ?
trans
i
is 1 if w
i
is generated by character transformation, otherwise
0. ?
h
i
is 1 ifw
i
is generated by Hiragana substitution, otherwise 0. ?
k
i
is 1 ifw
i
is generated by Katakana
substitution, otherwise 0.
2005), and five additional types of costs. These are the word-pos bi-gram cost ?logp
w
i?1
p
i?1
,w
i
p
i
of a
blog corpus; the character transformation cost ?
trans
i
?(?logr(c
t
, c
v
)), which is calculated in Section3.2,
for nodes generated by character transformation; the Hiragana substitution cost ?
h
i
? f
w
i
p
i
for nodes
generated by Hiragana substitution; the Katakana substitution cost ?
k
i
? f
w
i
p
i
for nodes generated by
Katakana substitution; and the character sequence cost log(p?
s
/p
?
t
i
) for all the normalized nodes. The
character sequence cost reflects the character sequence probability of the normalization candidates. Here,
s and t
i
are input string and transformed string respectively. (e.g., In Fig. 3, for the normalized node
?????? (cute, adjective), s is ???? ????? and t
i
is ???? ?????). Then p
s
and p
t
i
are
calculated by using the character 5-gram of a blog corpus, which is formulated by p
s
= p(c
1
? ? ? c
n
) =
?
n
j=1
p(c
j
|c
j?1
j?5
), where c
j
is the j th character of character sequence s. p?
t
i
and p?
s
are normalized by
using the length of each string s and t
i
as p?
t
i
= p
1/length(t
i
)
t
i
. We set the threshold (p?
s
/p
?
t
i
)
thres
= 1.5
for generating a Hiragana or Katakana normalization candidate in this study. Since all those features can
be factorized, the optimal path is searched for by using the Viterbi algorithm.
3.4.3 Training
We formulated the objective function for tuning weights w by using Eq. 2. The weights w are trained
by using the minimum error rate training (MERT) Machery et al. (2008). We defined the error function
as the differences between the reference word segmentations and the POS tags of the reference sequence
y
ref
and the system output arg min
y?L(s)
w ? f(y).
w? = arg min
w?W
N
?
i=1
error(y
ref
, arg min
y?L(s)
w ? f(y)) (2)
4 Experiments
4.1 Dataset and Estimated Transformation Table
We conducted experiments to confirm the effectiveness of the proposed method, in which we annotated
corpora of a Japanese blog and Twitter. The Twitter corpus was split into three parts: the training, devel-
opment, and test sets. The test data comprised 300 tweets, development data comprised 500 sentences
and the training data comprised 4208 tweets. We randomly selected the test data which contained at least
one non-standard token. The test data comprised 4635 words, 403 words of them are non-standard token
and are orthographically transformed into normalized form and POS tags. The blog corpus comprised
8023 sentences and all of them were used as training data. Training data was used for extracting char-
acter transformation table and development data was used for estimating parameters of discriminative
model. We used the IPA dictionary provided by MeCab to generate the word-level lattice and extracted
the dictionary-based features. We itemized the estimated character transformation patterns in Table 3.
There were 5228 transformation patterns that were learned from the training data and we used 3268 of
them, which meets the predefined condition. The learned patterns cover most of the previously pro-
1779
non-standard
character c
t
standard
character c
v
logp(q)
non-standard
character c
t
standard
character c
v
logp(q)
? null -4.233 ?? (ssu) ?? (desu) -5.999
??(maa) ?? (maa) -5.059 ?? (doo) ?? (dou) -6.210
??(syo) ??? (syou) -5.211 ?? (nee) ?? (nai) -6.232
?? (daro) ??? (darou) -5.570 ??(rya) ?? (reha) -6.492
?(ttsu) null -5.648 ?? (ten) ?? (teru) -6.633
?? (nto) ??? (ntou) -5.769 ?? (yuu) ?? (iu) -6.660
?(wa) ? (wa) -5.924 ?? (nan) ?? (nano) -6.706
Table 3: Example of character-level transformation table
posed rules. In addition, our method can learn more of the variational patterns that are difficult to create
manually.
4.2 Baseline and Evaluation Metrics
We compared the five methods listed in Table 4 in our experiments. Traditional means that which gen-
erates no normalization candidates and only uses the word cost and the cost for a pair of adjacent POS,
so we can consider it as a traditional Japanese morphological analysis. We compared three baselines,
Baseline1, Baseline2 and Baseline3. Baseline1 is the conventional rule-based method (considering in-
sertion of long sound symbols and lowercases, and substitution with long sound symbols and lower-
cases), which was proposed by Sasano et al. (2013). In Baseline2, 3, and Proposed, we basically use
the proposed discriminative model and features, but there are several differences. Baseline2 only gen-
erates character-level normalization candidates. Baseline3 uses our two-step normalization candidate
generation algorithms, but the character transformation cost of all the normalization candidates that are
generated by character normalization is the same. Proposed generates the character-level and Hiragana
and Katakana normalization candidates and use all features we proposed.
We evaluated each method on the basis of precision and recall and the F-value for the overall system
accuracy. Since Japanese morphological analysis simultaneously estimates the word segmentation and
POS tagging, we have to check whether or not our system is negatively affected by anything other than the
non-standard tokens. We also evaluated the recall with considering only normalized words. That value
directly reflects the performance of our normalization method. We registered emoticons that occurred in
the test data in the dictionary so that they would not negatively affect the systems? performance.
4.3 Results and Discussion
The results are classified in Table 4. As the table shows, the proposed methods performed statistically
significantly better than the baselines and the traditional method in both precision and recall (p < 0.01),
where the precision was greatly improved. This indicates that our method can not only correctly analyze
the non-standard tokens, but can also reduce the number of wrong words generated. Baseline1 also
improved the accuracy and recall compared to the traditional method, but the effect was limited. When
we compare Proposed with Baseline2, we find the F-value is improved when we take the Hiragana
and Katakana substitution into consideration. Baseline3 also improved the F-value but its performance is
inferior to proposed method.This proves that even if we can generate sufficient normalization candidates,
the results worsen if the weight parameter of each normalization candidate is not appropriately tuned. The
column of ?recall?? in Table 4 specifies the improvement rates of the non-standard tokens. The proposed
methods improve about seven times when using Baseline1 while preventing degradation. These results
prove that we have to generate appropriate and sufficient normalization candidates and appropriately tune
the cost of each candidate to improve both the precision and recall.
We show examples of the system output in Table 5. In the table, slashes indicate the position of the
estimated word segmentations and the words that were correctly analyzed are written in bold font. Exam-
ples (1) to (5) are examples improved by using the proposed method. Examples (6) to (7) are examples
that were not improved and example (8) is an example that was degraded. Examples (1) to (3) include
phonetic variations and example (4) is a Hiragana substitution. Example (5) is a combinational trans-
1780
word segmentation word segmentation and POS tag
method precision recall F-value precision recall F-value recall?
Traditional 0.716 0.826 0.767 0.683 0.788 0.732 -
Rule based (BL1??) 0.753 0.833 0.791 0.717 0.794 0.754 0.092
Proposed 0.856 0.883 0.869 0.822 0.849 0.835 0.667
- without Hiragana and Katakana normalization (BL2) 0.834 0.875 0.854 0.798 0.838 0.818 0.509
- character transformation cost is fixed (BL3) 0.838 0.865 0.851 0.807 0.834 0.821 0.533
? considering only normalized words, ?? BL:baseline
Table 4: Results of precision and recall of test data
input traditional proposed gold standard
(1)???(adii) ? (a)/? (di)/? ??? (atsui) ??? (atsui, ?hot?)
(2)???(sugee) ?? (suge)/? ??? (sugoi) ??? (sugoi, ?great?)
(3)????? (gommeen) ? (go)/?/? (me)/?/? (n)/ ??? (gomen) ??? (gomen, ?I?m sorry?)
(4)????(hitsuyou) ?? (hitsu)/?? (you) ?? (hitsuyou) ?? (hitsuyou, ?necessary?)
(5)?????(daichuki) ? (da)/?? (ichi)/?(yu)/? (ki)/ ??? (daisuki) ??? (daisuki, ?like very much?)
(6)?????(oseee) ?? (ose)/?? (ee)/? (e) ?? (ose) ??? (osoi, ?slow?)
(7)????? (kanwaii) ?? (kan)/? (wa)/?? (ii) ?? (kanwa)/?? (ii) ???? (kawaii, ?cute?)
(8)??? (inai) ? (i)/?? (nai) ?? (inai) ?/?? (i/nai, ?absent?)
Table 5: System output examples
formation pattern of a phonetic variation and Hiragana substitution. We can see our system can analyze
such variational non-standard tokens for all these examples. Two types of errors were identified. The first
occurred as the result of a lack of a character transformation pattern and the second was search errors.
Example (6) shows an example of a case in which our system couldn?t generate correct normalization
candidate because there was not corresponding character transformation pattern, even though there was
a similar phonetic transformation pattern. To ensure there will be no lack of transformation patterns,
we should either increase the parallel corpus size to enable the learning of more patterns or derive new
transformation patterns from the learned patterns. Example (7) shows an example of a case in which a
normalized candidate was generated but a search failed to locate it. Example (8) shows an example of a
case in which the result was degraded. Our system can control the degradation well, but there are several
degradation caused by normalization. We will need to develop a more complicated model or introduce
other features into the current model to reduce the number of search errors.
5 Conclusion and Future Work
We introduced a text normalization approach into joint Japanese morphological analysis and showed that
our two-step lattice generation algorithm and formulation using discriminative methods outperforms the
previous method. In future work, we plan to extend this approach by introducing an unsupervised or
semi-supervised parallel corpus extraction for learning character alignments to generate more patterns
at a reduced cost. We also plan to improve our model?s structure and features and implement it with a
decoding method to reduce the number of search errors. In addition, we should consider adding other
types of unknown words (such as named entities) to the morphological analysis system to improve its
overall performance.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for sms text normalization.
Proceedings of the COLING/ACL on Main Conference Poster Sessions, pages 33?40.
Maximilian Bisani and Hermann Ney. 2008. Joint-sequence models for grapheme-to-phoneme conversion.
Speech Commun., 50(5):434?451, May.
Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. Proceedings
of the Workshop on Computational Approaches to Linguistic Creativity, pages 71?78.
1781
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies - Volume 1, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for
microblogs. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, pages 421?432.
Hany Hassan and Arul Menezes. 2013. Social text normalization using contextual graph random walks. Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 1577?1586, August.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word lattice reranking for chinese word segmentation and part-of-
speech tagging. Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1,
pages 385?392.
Nobuhiro Kaji and Masaru Kitsuregawa. 2013. Efficient word lattice generation for joint word segmentation
and pos tagging in japanese. Proceedings of the Sixth International Joint Conference on Natural Language
Processing, pages 153?161.
Keigo Kubo, Hiromichi Kawanami, Hiroshi Saruwatari, and Kiyohiro Shikano. 2011. Unconstrained many-to-
many alignment for automatic pronunciation annotation. In Proc. of APSIPA ASC.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to japanese
morphological analysis. In Proc. of EMNLP, pages 230?237.
T. Kudo. 2005. Mecab : Yet another part-of-speech and morphological analyzer. http://mecab.sourceforge.net/.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of japanese
morphological analyzer juman. In Proc. of The International Workshop on Sharable Natural Language Re-
sources, page 22?38.
Chen Li and Yang Liu. 2012. Improving text normalization using character-blocks based models and system
combination. Proceedings of COLING 2012, pages 1587?1602.
Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2013. Paraphrasing 4 microblog normalization.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,
October.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011. Insertion, deletion, or substitution? normaliz-
ing text messages without pre-categorization nor supervision. Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, pages 71?76, June.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-coverage normalization system for social media language.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1035?1044.
W Machery, F J Och, and I Uszkoreit J Thayer. 2008. Lattice-based minimum error rate training for statistical
machine translation. In Proc. of EMNLP, 1:725?734.
Teruaki Oka, Mamoru Komachi, Toshinobu Ogiso, and Yuji Matsumoto. 2011. Handling orthographic variations
in morphological analysis for near-modern japanese (in japanese). In Proc. of The 27th Annual Conference of
the Japanese Society for Articial Intelligence.
Deana Pennell and Yang Liu. 2011. A character-level machine translation approach for normalization of sms
abbreviations. Proceedings of 5th International Joint Conference on Natural Language Processing, pages 974?
982, November.
Akira Sasaki, Junta Mizuno, Naoaki Okazaki, and Kentaro Inui. 2013. Normalization of text in microblogging
based on machine learning(in japanese) (in japanese). In Proc. of The 27th Annual Conference of the Japanese
Society for Articial Intelligence.
Ryohei Sasano, Sadao Kurohashi, and Manabu Okumura. 2013. A simple approach to unknown word process-
ing in japanese morphological analysis. Proceedings of the Sixth International Joint Conference on Natural
Language Processing, pages 162?170.
Jiampojamarn Sittichai, Kondrak Grzegorz, and Sherif Tarek. 2007. Applying many-to-many alignments and
hidden markov models to letter-to-phoneme conversion. In Proc. of The Conference of the North American
Chapter of the Association for Computational Linguistics, pages 372?379.
1782
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726?731,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Entity Set Expansion using Topic information
Kugatsu Sadamitsu, Kuniko Saito, Kenji Imamura and Genichiro Kikui?
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
{sadamitsu.kugatsu, saito.kuniko, imamura.kenji}@lab.ntt.co.jp
kikui@cse.oka-pu.ac.jp
Abstract
This paper proposes three modules based on
latent topics of documents for alleviating ?se-
mantic drift? in bootstrapping entity set ex-
pansion. These new modules are added to a
discriminative bootstrapping algorithm to re-
alize topic feature generation, negative exam-
ple selection and entity candidate pruning. In
this study, we model latent topics with LDA
(Latent Dirichlet Allocation) in an unsuper-
vised way. Experiments show that the accu-
racy of the extracted entities is improved by
6.7 to 28.2% depending on the domain.
1 Introduction
The task of this paper is entity set expansion in
which the lexicons are expanded from just a few
seed entities (Pantel et al, 2009). For example,
the user inputs a few words ?Apple?, ?Google? and
?IBM? , and the system outputs ?Microsoft?, ?Face-
book? and ?Intel?.
Many set expansion algorithms are based on boot-
strapping algorithms, which iteratively acquire new
entities. These algorithms suffer from the general
problem of ?semantic drift?. Semantic drift moves
the extraction criteria away from the initial criteria
demanded by the user and so reduces the accuracy
of extraction. Pantel and Pennacchiotti (2006) pro-
posed Espresso, a relation extraction method based
on the co-training bootstrapping algorithm with en-
tities and attributes. Espresso alleviates semantic-
drift by a sophisticated scoring system based on
? Presently with Okayama Prefectural University
pointwise mutual information (PMI). Thelen and
Riloff (2002), Ghahramani and Heller (2005) and
Sarmento et al (2007) also proposed original score
functions with the goal of reducing semantic-drift.
Our purpose is also to reduce semantic drift. For
achieving this goal, we use a discriminative method
instead of a scoring function and incorporate topic
information into it. Topic information means the
genre of each document as estimated by statisti-
cal topic models. In this paper, we effectively uti-
lize topic information in three modules: the first
generates the features of the discriminative mod-
els; the second selects negative examples; the third
prunes incorrect examples from candidate examples
for new entities. Our experiments show that the pro-
posal improves the accuracy of the extracted entities.
The remainder of this paper is organized as fol-
lows. In Section 2, we illustrate discriminative boot-
strapping algorithms and describe their problems.
Our proposal is described in Section 3 and experi-
mental results are shown in Section 4. Related works
are described in Section 5. Finally, Section 6 pro-
vides our conclusion and describes future works.
2 Problems of the previous Discriminative
Bootstrapping method
Some previous works introduced discriminative
methods based on the logistic sigmoid classifier,
which can utilize arbitrary features for the relation
extraction task instead of a scoring function such as
Espresso (Bellare et al, 2006; Mintz et al, 2009).
Bellare et al reported that the discriminative ap-
proach achieves better accuracy than Espresso when
the number of extracted pairs is increased because
726
multiple features are used to support the evidence.
However, three problems exist in their methods.
First, they use only local context features. The dis-
criminative approach is useful for using arbitrary
features, however, they did not identify which fea-
ture or features are effective for the methods. Al-
though the context features and attributes partly re-
duce entity word sense ambiguity, some ambiguous
entities remain. For example, consider the domain
broadcast program (PRG) and assume that PRG?s
attribute is advertisement. A false example is shown
here: ?Android ?s advertisement employs Japanese
popular actors. The attractive smartphone begins to
target new users who are ordinary people.? The en-
tity Android belongs to the cell-phone domain, not
PRG, but appears with positive attributes or contexts
because many cell-phones are introduced in adver-
tisements as same as broadcast program. By us-
ing topic, i.e. the genre of the document, we can
distinguish ?Android? from PRG and remove such
false examples even if the false entity appeared with
positive context strings or attributes. Second, they
did not solve the problem of negative example se-
lection. Because negative examples are necessary
for discriminative training, they used all remaining
examples, other than positive examples, as negative
examples. Although this is the simplest technique,
it is impossible to use all of the examples provided
by a large-scale corpus for discriminative training.
Third, their methods discriminate all candidates for
new entities. This principle increases the risk of gen-
erating many false-positive examples and is ineffi-
cient. We solve these three problems by using topic
information.
3 Set expansion using Topic information
3.1 Basic bootstrapping methods
In this section, we describe the basic method
adopted from Bellare (Bellare et al, 2006). Our
system?s configuration diagram is shown in Figure
1. In Figure 1, arrows with solid lines indicate the
basic process described in this section. The other
parts are described in the following sections. After
Ns positive seed entities are manually given, every
noun co-occurring with the seed entities is ranked
by PMI scores and then selected manually as Na
positive attributes. Ns and Na are predefined ba-
Figure 1: The structure of our system.
sic adjustment numbers. The entity-attribute pairs
are obtained by taking the cross product of seed en-
tity lists and attribute lists. The pairs are used as
queries for retrieving the positive documents, which
include positive pairs. The document set De,a in-
cluding same entity-attribute pair {e, a} is regarded
as one example Ee,a to alleviate over-fitting for con-
text features. These are called positive examples in
Figure 1. Once positive examples are constructed,
discriminative models can be trained by randomly
selecting negative examples.
Candidate entities are restricted to only the
Named Entities that lie in the close proximity to the
positive attributes. These candidates of documents,
including Named Entity and positive attribute pairs,
are regarded as one example the same as the train-
ing data. The discriminative models are used to cal-
culate the discriminative positive score, s(e, a), of
each candidate pair, {e, a}. Our system extracts Nn
types of new entities with high scores at each iter-
ation as defined by the summation of s(e, a) of all
positive attributes (AP );
?
a?AP s(e, a). Note that
we do not iteratively extract new attributes because
our purpose is entity set expansion.
3.2 Topic features and Topic models
In previous studies, context information is only used
as the features of discriminative models as we de-
scribed in Section 2. Our method utilizes not only
context features but also topic features. By utiliz-
ing topic information, our method can disambiguate
the entity word sense and alleviate semantic drift.
In order to derive the topic information, we utilize
statistical topic models, which represent the relation
727
between documents and words through hidden top-
ics. The topic models can calculate the posterior
probability p(z|d) of topic z in document d. For
example, the topic models give high probability to
topic z =?cell-phone? in the above example sen-
tences 1. This posterior probability is useful as a
global feature for discrimination. The topic feature
value ?t(z, e, a) is calculated as follows.
?t(z, e, a) =
?
d?De,a p(z|d)
?
z?
?
d?De,a p(z?|d)
.
In this paper, we use Latent Dirichlet Allocation
(LDA) as the topic models (Blei et al, 2003). LDA
represents the latent topics of the documents and the
co-occurrence between each topic.
In Figure 1, shaded part and the arrows with bro-
ken lines indicate our proposed method with its use
of topic information including the following sec-
tions.
3.3 Negative example selection
If we choose negative examples randomly, such ex-
amples are harmful for discrimination because some
examples include the same contexts or topics as the
positive examples. By contrast, negative examples
belonging to broad genres are needed to alleviate se-
mantic drift. We use topic information to efficiently
select such negative examples.
In our method, the negative examples are cho-
sen far from the positive examples according to the
measure of topic similarity. For calculating topic
similarity, we use a ranking score called ?positive
topic score?, PT (z), defined as follows, PT (z) =
?
d?DP p(z|d), where DP indicates the set of pos-
itive documents and p(z|d) is topic posterior prob-
ability for a given positive document. The bottom
50% of the topics sorted in decreasing order of pos-
itive topic score are used as the negative topics.
Our system picks up as many negative documents
as there are positive documents with each selected
negative topic being equally represented.
3.4 Candidate Pruning
Previous works discriminate all candidates for ex-
tracting new entities. Our basic system can constrain
1z is a random variable whose sample space is represented
as a discrete variable, not explicit words.
the candidate set by positive attributes, however, this
is not enough as described in Section 2. Our candi-
date pruning module, described below, uses the mea-
sure of topic similarity to remove obviously incor-
rect documents.
This pruning module is similar to negative exam-
ple selection described in the previous section. The
positive topic score, PT , is used as a candidate con-
straint. Taking all positive examples, we select the
positive topics, PZ, which including all topics z sat-
isfying the condition PT (z) > th. At least one
topic with the largest score is chosen as a positive
topic when PT (z) ? th about all topics. After se-
lecting this positive topic, the documents including
entity candidates are removed if the posterior prob-
ability satisfy p(z|d) ? th for all topics z. In this
paper, we set the threshold to th = 0.2. This con-
straint means that the topic of the document matches
that of the positive entities and can be regarded as a
hard constraint for topic features.
4 Experiments
4.1 Experimental Settings
We use 30M Japanese blog articles crawled in May
2008. The documents were tokenized by JTAG
(Fuchi and Takagi, 1998), chunked, and labeled with
IREX 8 Named Entity types by CRFs using Mini-
mum Classification Error rate (Suzuki et al, 2006),
and transformed into features. The context features
were defined using the template ?(head) entity (mid.)
attribute (tail)?. The words included in each part
were used as surface, part-of-speech and Named En-
tity label features added position information. Max-
imum word number of each part was set at 2 words.
The features have to appear in both the positive and
negative training data at least 5 times.
In the experiments, we used three domains, car
(?CAR?), broadcast program (?PRG?) and sports or-
ganization (?SPT?). The adjustment numbers for ba-
sic settings are Ns = 10, Na = 10, Nn = 100. Af-
ter running 10 iterations, we obtained 1000 entities
in total. SVM light (Joachims, 1999) with second
order polynomial kernel was used as the discrimina-
tive model. Parallel LDA, which is LDA with MPI
(Liu et al, 2011), was used for training 100 mix-
ture topic models and inference. Training corpus for
topic models consisted of the content gathered from
728
CAR PRG SPT
1. Baseline 0.249 0.717 0.781
2. Topic features + 1. 0.483 0.727 0.844
3. Negative selection + 2. 0.509 0.762 0.846
4. Candidate pruning + 3. 0.531 0.824 0.848
Table 1: The experimental results for the three domains.
Bold font indicates that the difference between accuracy
of the methods in the row and the previous row is signifi-
cant (P < 0.05 by binomial test) and italic font indicates
(P < 0.1).
14 days of blog articles. In the Markov-chain Monte
Carlo (MCMC) method, sampling was iterated 200
times for training with a burn-in taking 50 iterations.
These parameters were selected based on the results
of a preliminary experiment.
Four experimental settings were examined. First
is Baseline; it is described in Section 3.1. Second is
the first method with the addition of topic features.
Third is the second method with the addition of a
negative example selection module. Fourth is the
third method with the addition of a candidate prun-
ing module (equals the entire shaded part in Fig-
ure 1). Each extracted entity is labeled with cor-
rect or incorrect by two evaluators based on the re-
sults of a commercial search engine. The ? score for
agreement between evaluators was 0.895. Because
the third evaluator checked the two evaluations and
confirmed that the examples which were judged as
correct by either one of the evaluators were correct,
those examples were counted as correct.
4.2 Experimental Results
Table 1 shows the accuracy and significance for each
domain. Using topic features significantly improves
accuracy in the CAR and SPT domains. The nega-
tive example selection module improves accuracy in
the CAR and PRG domains. This means the method
could reduce the risk of selecting false-negative ex-
amples. Also, the candidate pruning method is ef-
fective for the CAR and PRG domains. The CAR
domain has lower accuracy than the others. This
is because similar entities such as motorcycles are
extracted; they have not only the same context but
also the same topic as the CAR domain. In the SPT
domain, the method with topic features offer signif-
icant improvements in accuracy and no further im-
provement was achieved by the other two modules.
To confirm whether our modules work properly,
we show some characteristic words belonging to
each topic that is similar and not similar to target do-
main in Table 2. Table 2 shows characteristic words
for one positive topic zh and two negative topics zl
and ze, defined as follow.
? zh (the second row) is the topic that maximizes
PT (z), which is used as a positive topic.
? zl (the fourth row) is the topic that minimizes
PT (z), which is used as a negative topic.
? ze (the fifth row) is a topic that, we consider, ef-
fectively eliminates ?drifted entities? extracted
by the baseline method. ze is eventually in-
cluded in the lower half of topic list sorted by
PT (z).
For a given topic, z, we chose topmost three words
in terms of topic-word score. The topic-word score
of a word, v, is defined as p(v|z)/p(v), where p(v)
is the unigram probability of v, which was estimated
by maximum likelihood estimation. For utilizing
candidate pruning, near topics including zh must be
similar to the domain. By contrast, for utilizing neg-
ative example selection, the lower half of topics, zl,
ze and other negative topics, must be far from the
domain. Our system succeeded in achieving this.
As shown in ?CAR? in Table 2, the nearest topic
includes ?shaken? (automobile inspection) and the
farthest topic includes ?naika? (internal medicine)
which satisfies our expectation. Furthermore, the ef-
fective negative topic is similar to the topic of drifted
entity sets (digital device). This indicates that our
method successfully eliminated drifted entities. We
can confirm that the other domains trend in the same
direction as ?CAR? domain.
5 Related Works
Some prior studies use every word in a docu-
ment/sentence as the features, such as the distribu-
tional approaches (Pantel et al, 2009). These meth-
ods are regarded as using global information, how-
ever, the space of word features are sparse, even if
the amount of data available is large. Our approach
can avoid this problem by using topic models which
729
domain CAR PRG SPT
words of the
nearest topic zh
(highest PT score)
shaken
(automobile inspection),
nosha (delivering a car),
daisha (loaner car)
Mari YAMADA,
Tohru KUSANO,
Reiko TOKITA
(Japanese stars)
toshu (pitcher),
senpatsu
(starting member),
shiai (game)
drifted entities
(using baseline)
iPod, mac
(digital device)
PS2, XBOX360
(video game)
B?z, CHAGE&ASKA
(music)
words of effective
negative topic ze
(Lower half of
PT score)
gaso (pixel),
kido (brightness),
mazabodo (mother board)
Lv. (level),
kariba (hunting area),
girumen (guild member)
sinpu (new release),
X JAPAN ,
Kazuyoshi Saito
(Japanese musicians)
words of
the farthest topic zl
(Lowest PT score)
naika (internal medicine),
hairan (ovulation),
shujii (attending doctor)
tsure (hook a fish),
choka (result of hooking),
choko (diary of hooking)
toritomento (treatment),
keana (pore),
hoshitsu (moisture retention)
Table 2: The characteristic words belonging to three topics, zh, zl and ze. zh is the nearest topic and zl is the farthest
topic for positive entity-attribute seed pairs. ze is an effective negative topic for eliminating ?drifted entities? extracted
by the baseline system.
are clustering methods based on probabilistic mea-
sures. By contrast, Pas?ca and Durme (2008) pro-
posed clustering methods that are effective in terms
of extraction, even though their clustering target is
only the surrounding context. Ritter and Etzioni
(2010) proposed a generative approach to use ex-
tended LDA to model selectional preferences. Al-
though their approach is similar to ours, our ap-
proach is discriminative and so can treat arbitrary
features; it is applicable to bootstrapping methods.
The accurate selection of negative examples is a
major problem for positive and unlabeled learning
methods or general bootstrapping methods and some
previous works have attempted to reach a solution
(Liu et al, 2002; Li et al, 2010). However, their
methods are hard to apply to the Bootstrapping al-
gorithms because the positive seed set is too small
to accurately select negative examples. Our method
uses topic information to efficiently solve both the
problem of extracting global information and the
problem of selecting negative examples.
6 Conclusion
We proposed an approach to set expansion that uses
topic information in three modules and showed that
it can improve expansion accuracy. The remaining
problem is that the grain size of topic models is not
always the same as the target domain. To resolve
this problem, we will incorporate the active learning
or the distributional approaches. Also, comparisons
with the previous works are remaining work. From
another perspective, we are considering the use of
graph-based approaches (Komachi et al, 2008) in-
corporated with the topic information using PHITS
(Cohn and Chang, 2000), to further enhance entity
extraction accuracy.
References
Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2006. Lightly-supervised at-
tribute extraction. In Proceedings of the Advances in
Neural Information Processing Systems Workshop on
Machine Learning for Web Search.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
David Cohn and Huau Chang. 2000. Learning to prob-
abilistically identify authoritative documents. In Pro-
ceedings of the 17th International Conference on Ma-
chine Learning, pages 167?174.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese
Morphological Analyzer using Word Co-occurrence-
JTAG. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, pages 409?413.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Proceedings of the Advances in Neu-
ral Information Processing Systems.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning. Software available at
http://svmlight.joachims.org/.
730
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1011?1020.
Xiao-Li Li, Bing Liu, and See-Kiong Ng. 2010. Neg-
ative Training Data can be Harmful to Text Classi-
fication. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 218?228.
Bing Liu, Wee S. Lee, Philip S. Yu, and Xiaoli Li. 2002.
Partially supervised classification of text documents.
In Proceedings of the 19th International Conference
on Machine Learning, pages 387?394.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. PLDA+: Parallel latent dirich-
let alocation with data placement and pipeline pro-
cessing. ACM Transactions on Intelligent Systems
and Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca and Benjamin Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, pages 19?27.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 113?120.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 938?
947.
Alan Ritter and Oren Etzioni. 2010. A Latent Dirich-
let Allocation method for Selectional Preferences. In
Proceedings of the 48th ACL Conference, pages 424?
434.
Luis Sarmento, Valentin Jijkuon, Maarten de Rijke, and
Eugenio Oliveira. 2007. More like these: grow-
ing entity classes from seeds. In Proceedings of the
16th ACM Conference on Information and Knowledge
Management, pages 959?962.
Jun Suzuki, Erik McDermott, and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multivari-
ate Evaluation Measures. In Proceedings of the 21st
COLING and 44th ACL Conference, pages 217?224.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using ex-
traction pattern contexts. In Proceedings of the 2002
conference on Empirical methods in natural language
processing, pages 214?221.
731
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 388?392,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Grammar Error Correction
Using Pseudo-Error Sentences and Domain Adaptation
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and Hitoshi Nishikawa
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan
{
imamura.kenji, saito.kuniko
sadamitsu.kugatsu, nishikawa.hitoshi
}
@lab.ntt.co.jp
Abstract
This paper presents grammar error correction
for Japanese particles that uses discrimina-
tive sequence conversion, which corrects erro-
neous particles by substitution, insertion, and
deletion. The error correction task is hindered
by the difficulty of collecting large error cor-
pora. We tackle this problem by using pseudo-
error sentences generated automatically. Fur-
thermore, we apply domain adaptation, the
pseudo-error sentences are from the source
domain, and the real-error sentences are from
the target domain. Experiments show that sta-
ble improvement is achieved by using domain
adaptation.
1 Introduction
Case marks of a sentence are represented by postpo-
sitional particles in Japanese. Incorrect usage of the
particles causes serious communication errors be-
cause the cases become unclear. For example, in
the following sentence, it is unclear what must be
deleted.
mail o todoi tara sakujo onegai-shi-masu
mail ACC. arrive when delete please
?When ? has arrived an e-mail, please delete it.?
If the accusative particle o is replaced by a nomi-
native one ga, it becomes clear that the writer wants
to delete the e-mail (?When the e-mail has arrived,
please delete it.?). Such particle errors frequently
occur in sentences written by non-native Japanese
speakers.
This paper presents a method that can automat-
ically correct Japanese particle errors. This task
corresponds to preposition/article error correction in
English. For English error correction, many stud-
ies employ classifiers, which select the appropriate
prepositions/articles, by restricting the error types
to articles and frequent prepositions (Gamon, 2010;
Han et al, 2010; Rozovskaya and Roth, 2011).
On the contrary, Mizumoto et al (2011) proposed
translator-based error correction. This approach can
handle all error types by converting the learner?s
sentences into the correct ones. Although the target
of this paper is particle error, we employ a similar
approach based on sequence conversion (Imamura
et al, 2011) since this offers excellent scalability.
The conversion approach requires pairs of the
learner?s and the correct sentences. However, col-
lecting a sufficient number of pairs is expensive. To
avoid this problem, we use additional corpus con-
sisting of pseudo-error sentences automatically gen-
erated from correct sentences that mimic the real-
errors (Rozovskaya and Roth, 2010b). Furthermore,
we apply a domain adaptation technique that re-
gards the pseudo-errors and the real-errors as the
source and the target domain, respectively, so that
the pseudo-errors better match the real-errors.
2 Error Correction by Discriminative
Sequence Conversion
We start by describing discriminative sequence con-
version. Our error correction method converts the
learner?s word sequences into the correct sequences.
Our method is similar to phrase-based statistical ma-
chine translation (PBSMT), but there are three dif-
ferences; 1) it adopts the conditional random fields,
2) it allows insertion and deletion, and 3) binary and
real features are combined. Unlike the classification
388
Incorrect Particle Correct Particle Note
? no/POSS. INS
? o/ACC. INS
ga/NOM. o/ACC. SUB
o/ACC. ni/DAT. SUB
o/ACC. ga/NOM. SUB
wa/TOP. o/ACC. SUB
no/POSS. ? DEL
: :
Table 1: Example of Phrase Table (partial)
approach, the conversion approach can correct mul-
tiple errors of all types in a sentence.
2.1 Basic Procedure
We apply the morpheme conversion approach that
converts the results of a speech recognizer into word
sequences for language analyzer processing (Ima-
mura et al, 2011). It corrects particle errors in the
input sentences as follows.
? First, all modification candidates are obtained by
referring to a phrase table. This table, called the
confusion set (Rozovskaya and Roth, 2010a) in
the error correction task, stores pairs of incorrect
and correct particles (Table 1). The candidates are
packed into a lattice structure, called the phrase
lattice (Figure 1). To deal with unchanged words,
it also copies the input words and inserts them into
the phrase lattice.
? Next, the best phrase sequence in the phrase lat-
tice is identified based on the conditional random
fields (CRFs (Lafferty et al, 2001)). The Viterbi
algorithm is applied to the decoding because error
correction does not change the word order.
? While training, word alignment is carried out by
dynamic programming matching. From the align-
ment results, the phrase table is constructed by ac-
quiring particle errors, and the CRF models are
trained using the alignment results as supervised
data.
2.2 Insertion / Deletion
Since an insertion can be regarded as replacing an
empty word with an actual word, and deletion is the
replacement of an actual word with an empty one,
we treat these operations as substitution without dis-
tinction while learning/applying the CRF models.
mailnounInput Words oACC. todoiverb taraPART ?
Phrase Lattice mail o todoi tara
copy INS copySUB copy copy
<s>
Incorrect Particle
noun
noPOSS.
ACC.
gaNOM.
niDAT.
verb PART
oACC.
Figure 1: Example of Phrase Lattice
However, insertion is a high cost operation be-
cause it may occur at any location and can cause
lattice size to explode. To avoid this problem, we
permit insertion only immediately after nouns.
2.3 Features
In this paper, we use mapping features and link fea-
tures. The former measure the correspondence be-
tween input and output words (similar to the trans-
lation models of PBSMT). The latter measure the
fluency of the output word sequence (similar to lan-
guage models).
The mapping features are all binary. The focusing
phrase and its two surrounding words of the input
are regarded as the window. The mapping features
are defined as the pairs of the output phrase and 1-,
2-, and 3-grams in the window.
The link features are important for the error cor-
rection task because the system has to judge output
correctness. Fortunately, CRF, which is a kind of
discriminative model, can handle features that de-
pend on each other; we mix two types of features
as follows and optimize their weights in the CRF
framework.
? N -gram features: N -grams of the output words,
from 1 to 3, are used as binary features. These
are obtained from a training corpus (paired sen-
tences). Since the feature weights are optimized
considering the entire feature space, fine-tuning
can be achieved. The accuracy becomes almost
perfect on the training corpus.
? Language model probability: This is a logarith-
mic value (real value) of the n-gram probability
of the output word sequence. One feature weight
is assigned. The n-gram language model can be
389
constructed from a large sentence set because it
does not need the learner?s sentences.
Incorporating binary and real features yields a
rough approximation of generative models in semi-
supervised CRFs (Suzuki and Isozaki, 2008). It can
appropriately correct new sentences while maintain-
ing high accuracy on the training corpus.
3 Pseudo-error Sentences and Domain
Adaptation
The error corrector described in Section 2 requires
paired sentences. However, it is expensive to col-
lect them. We resolve this problem by using pseudo-
error sentences and domain adaptation.
3.1 Pseudo-Error Generation
Correct sentences, which are halves of the paired
sentences, can be easily acquired from corpora such
as newspaper articles. Pseudo-errors are generated
from them by the substitution, insertion, and dele-
tion functions according to the desired error pat-
terns.
We utilize the method of Rozovskaya and Roth
(2010b). Namely, when particles appear in the cor-
rect sentence, they are replaced by incorrect ones in
a probabilistic manner by applying the phrase table
(which stores the error patterns) in the opposite di-
rection. The error generation probabilities are rel-
ative frequencies on the training corpus. The mod-
els are learnt using both the training corpus and the
pseudo-error sentences.
3.2 Adaptation by Feature Augmentation
Although the error generation probabilities are com-
puted from the real-error corpus, the error distribu-
tion that results may be inappropriate. To better fit
the pseudo-errors to the real-errors, we apply a do-
main adaptation technique. Namely, we regard the
pseudo-error corpus as the source domain and the
real-error corpus as the target domain, and models
are learnt that fit the target domain.
In this paper, we use Daume (2007)?s feature aug-
mentation method for the domain adaptation, which
eliminates the need to change the learning algo-
rithm. This method regards the models for the
source domain as the prior distribution and learns
the models for the target domain.
Common Source TargetFeature Space
Ds Ds 0Source Data
Dt 0 DtTarget Data
Figure 2: Feature Augmentation
We briefly review feature augmentation. The fea-
ture space is segmented into three parts: common,
source, and target. The features extracted from the
source domain data are deployed to the common
and the source spaces, and those from the target do-
main data are deployed to the common and the target
spaces. Namely, the feature space is tripled (Figure
2).
The parameter estimation is carried out in the
usual way on the above feature space. Consequently,
the weights of the common features are emphasized
if the features are consistent between the source and
the target. With regard to domain dependent fea-
tures, the weights in the source or the target space
are emphasized.
Error correction uses only the features in the com-
mon and target spaces. The error distribution ap-
proaches that of the real-errors because the weights
of features are optimized to the target domain. In ad-
dition, it becomes robust against new sentences be-
cause the common features acquired from the source
domain can be used even when they do not appear in
the target domain.
4 Experiments
4.1 Experimental Settings
Real-error Corpus: We collected learner?s sen-
tences written by Chinese native speakers. The sen-
tences were created from English Linux manuals
and figures, and Japanese native speakers revised
them. From these sentences, only particle errors
were retained; the other errors were corrected. As
a result, we obtained 2,770 paired sentences. The
number of incorrect particles was 1,087 (8.0%) of
13,534. Note that most particles did not need to be
revised. The number of pair types of incorrect parti-
cles and their correct ones was 132.
Language Model: It was constructed from
Japanese Wikipedia articles about computers and
390
0.5
0.6
0.7
0.8
0.9
1
Precision Rate
TRGSRCALLAUG
0.3
0.4
0 0.05 0.1 0.15 0.2 0.25
Precision Rate
Recall Rate
Figure 3: Recall/Precision Curve (Error Generation Mag-
nification is 1.0)
Japanese Linux manuals, 527,151 sentences in total.
SRILM (Stolcke et al, 2011) was used to train a
trigram model.
Pseudo-error Corpus: The pseudo-errors were
generated using 10,000 sentences randomly selected
from the corpus for the language model. The mag-
nification of the error generation probabilities was
changed from 0.0 (i.e., no errors) to 2.0 (the relative
frequency in the real-error corpus was taken as 1.0).
Evaluation Metrics: Five-fold cross-validation
on the real-error corpus was used. We used two met-
rics: 1) Precision and recall rates of the error correc-
tion by the systems, and 2) Relative improvement,
the number of differences between improved and de-
graded particles in the output sentences (no changes
were ignored). This is a practical metric because it
denotes the number of particles that human rewriters
do not need to revise after the system correction.
4.2 Results
Figure 3 plots the precision/recall curves for the fol-
lowing four combinations of training corpora and
method.
? TRG: The models were trained using only the
real-error corpus (baseline).
? SRC: Trained using only the pseudo-error corpus.
? ALL: Trained using the real-error and pseudo-
error corpora by simply adding them.
? AUG:
The proposed method. The feature augmentation
was realized by regarding the pseudo-errors as the
-50
0
+50
+100
0.0 0.5 1.0 1.5 2.0 
Relative Improvement
-150
-100
Relative Improvement
Error Generation Probability(Magnification)
TRGSRCALLAUG
Figure 4: Relative Improvement among Error Generation
Probabilities
source domain and the real-errors as the target do-
main.
The SRC case, which uses only the pseudo-error
sentences, did not match the precision of TRG. The
ALL case matched the precision of TRG at high
recall rates. AUG, the proposed method, achieved
higher precision than TRG at high recall rates. At
the recall rate of 18%, the precision rate of AUGwas
55.4%; in contrast, that of TRG was 50.5%. Fea-
ture augmentation effectively leverages the pseudo-
errors for error correction.
Figure 4 shows the relative improvement of each
method according to the error generation probabil-
ities. In this experiment, ALL achieved higher im-
provement than TRG at error generation probabili-
ties ranging from 0.0 to 0.6. Although the improve-
ments were high, we have to control the error gen-
eration probability because the improvements in the
SRC case fell as the magnification was raised. On
the other hand, AUG achieved stable improvement
regardless of the error generation probability. We
can conclude that domain adaptation to the pseudo-
error sentences is the preferred approach.
5 Conclusions
This paper presented an error correction method of
Japanese particles that uses pseudo-error generation.
We applied domain adaptation in which the pseudo-
errors are regarded as the source domain and the
real-errors as the target domain. In our experiments,
domain adaptation achieved stable improvement in
system performance regardless of the error genera-
tion probability.
391
References
Hal Daume, III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
pages 256?263, Prague, Czech Republic.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2010), pages
163?171, Los Angeles, California.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner
corpus to develop an ESL/EFL error correction sys-
tem. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta.
Kenji Imamura, Tomoko Izumi, Kugatsu Sadamitsu, Ku-
niko Saito, Satoshi Kobashikawa, and Hirokazu Masa-
taki. 2011. Morpheme conversion for connecting
speech recognizer and language analyzers in unseg-
mented languages. In Proceedings of Interspeech
2011, pages 1405?1408, Florence, Italy.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th International Conference
on Machine Learning (ICML-2001), pages 282?289,
Williamstown, Massachusetts.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2011. Mining revision log of
language learning SNS for automated Japanese error
correction of second language learners. In Proceed-
ings of 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 147?155,
Chiang Mai, Thailand.
Alla Rozovskaya and Dan Roth. 2010a. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), pages 961?970, Cambridge, Massachusetts.
Alla Rozovskaya and Dan Roth. 2010b. Training
paradigms for correcting errors in grammar and usage.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-HLT
2010), pages 154?162, Los Angeles, California.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for ESL correction tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Techologies (ACL-HLT 2011), pages 924?933,
Portland, Oregon.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU
2011), Waikoloa, Hawaii.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-08:
HLT), pages 665?673, Columbus, Ohio.
392
