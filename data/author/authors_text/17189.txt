Proceedings of the NAACL HLT 2013 Demonstration Session, pages 5?9,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
Topic Models and Metadata for Visualizing Text Corpora
Justin Snyder, Rebecca Knowles, Mark Dredze, Matthew R. Gormley, Travis Wolfe
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
{jsnyde32,mdredze,mgormley,twolfe3}@jhu.edu, rknowles@haverford.edu
Abstract
Effectively exploring and analyzing large text
corpora requires visualizations that provide a
high level summary. Past work has relied on
faceted browsing of document metadata or on
natural language processing of document text.
In this paper, we present a new web-based tool
that integrates topics learned from an unsuper-
vised topic model in a faceted browsing expe-
rience. The user can manage topics, filter doc-
uments by topic and summarize views with
metadata and topic graphs. We report a user
study of the usefulness of topics in our tool.
1 Introduction
When analyzing text corpora, such as newspaper ar-
ticles, research papers, or historical archives, users
need an intuitive way to understand and summa-
rize numerous documents. Exploratory search (Mar-
chionini, 2006) is critical for large corpora that can
easily overwhelm users. Corpus visualization tools
can provide a high-level view of the data and help di-
rect subsequent exploration. Broadly speaking, such
systems can be divided into two groups: those that
rely on structured metadata, and those that use infor-
mation derived from document content.
Metadata Approaches based on metadata include
visualizing document metadata alongside a domain
ontology (Seeling and Becks, 2003), providing tools
to select passages based on annotated words (Cor-
rell et al, 2011), and using images and metadata for
visualizing related documents (Cataldi et al, 2011).
A natural solution for exploring via metadata is
faceted browsing (English et al, 2002; Hearst, 2006;
Smith et al, 2006; Yee et al, 2003), a paradigm
for filtering commonly used in e-commerce stores.
This consists of filtering based on metadata like
?brand? or ?size?, which helps summarize the con-
tent of the current document set (Ka?ki, 2005). Stud-
ies have shown improved user experiences by facil-
itating user interactions through facets (Oren et al,
2006) and faceted browsing has been used for aid-
ing search (Fujimura et al, 2006) and exploration
(Collins et al, 2009) of text corpora.
However, facets require existing structured meta-
data fields, which may be limited or unavailable. An
alternative is to use NLP to show document content.
Content Topic modeling (Blei et al, 2003), has
become very popular for corpus and document un-
derstanding. Recent research has focused on aspects
highlighted by the topic model, such as topic distri-
butions across the corpus, topic distributions across
documents, related topics and words that make up
each topic (Chaney and Blei, 2012; Eisenstein et al,
2012), or document relations through topic compo-
sitions (Chuang et al, 2012; Gardner et al, 2010).
Newer work has begun to visualize documents in
the context of their topics and their metadata, such as
topics incorporated with keywords and events (Cui
et al, 2011). Other examples include displaying
topic prevalence over time (Liu et al, 2009) or help-
ing users understand how real events shape textual
trends (Dou et al, 2011). While interfaces may be
customized for specific metadata types, e.g. the top-
ical map of National Institutes of Health funding
agencies (Talley et al, 2011), these interfaces do not
incorporate arbitrary metadata.
5
2 Combining Metadata and Topics
We present MetaToMATo (Metadata and Topic
Model Analysis Toolkit), a visualization tool that
combines both metadata and topic models in a single
faceted browsing paradigm for exploration and anal-
ysis of document collections. While previous work
has shown the value of metadata facets, we show that
topic model output complements metadata. Provid-
ing both in a single interface yields a flexible tool.
We illustrate MetaToMATo with an example
adapted from our user study. Consider Sarah, a
hypothetical intern in the New York Times archive
room who is presented with the following task.
Your boss explains that although the New
York Times metadata fields are fairly compre-
hensive, sometimes human error leads to over-
sights or missing entries. Today you?ve been
asked to keep an eye out for documents that
mention the New York Marathon but do not
include descriptors linking them to that event.
This is corpus exploration: a user is asked to dis-
cover relevant information by exploring the corpus.
We illustrate the tool with a walk-through.
Corpus Selection The corpus selection page (tool
home page) provides information about all available
corpora, and allows for corpora upload and deletion.
Sarah selects the New York Times corpus.
Corpus Overview After selecting a corpus, the
user sees the corpus overview and configuration
page. Across four tabs, the user is presented with
more detailed corpus statistics and can customize
her visualization experience. The first tab shows
general corpus information. The second allows for
editing the inferred type (date, quantity, or string)
for each metadata attribute to change filtering be-
havior, hide unhelpful attributes, and choose which
attributes to ?quick display? in the document col-
lapsed view. On the remaining two tabs, the user can
customize date display formats and manage tags.
She selects attributes ?Date? and ?Byline? for
quick display, hides ?Series Name?, and formats
?Date? to show only the date (no times).
Topics View Each topic is displayed in a box con-
taining its name (initially set to its top 3 words) and a
list of the top 10 words. Top words within a topic are
words with the highest probability of appearing in
the corpus. Each topic word is highlighted to show a
Figure 1: Topics Page A view of the first row of top-
ics, and the sorting selector at the top of the page. The
left topic is being renamed. The second topic has been
marked as junk.
normalized probability of that word within the topic.
(Figure 1) Clicking a topic box provides more infor-
mation. Users can rename topics, label unhelpful or
low-quality topics as JUNK, or sort them in terms of
frequency in the corpus,1 predicted quality,2 or junk.
Sarah renames several topics, including the topic
?{running, athletes, race}? as SPORTS and marks
the ?{share, listed, bath}? topic as JUNK.
Documents View The document view provides a
faceted browsing interface of the corpus. (Figure 2)
The pane on the right side displays the set of docu-
ments returned by the current filters (search). Each
document is summarized by the first 100 words and
any quick view metadata. Users can expand doc-
uments to see all document metadata, a graph of
the distribution of the topics in this document, and
a graph of topics distinctive to this document com-
pared to corpus-wide averages.3
Sarah begins by looking at the types of documents
in the corpus, opening and closing a few documents
as she scrolls down the page.
The facets pane on the left side of the page dis-
plays the available facets given the current filters.
Topics in a drop-down menu can be used to filter
given a threshold.
Sarah selects the value ?New York City? for the
Location attribute and a threshold of 5% for the
SPORTS topic, filtering on both facets.
Values next to each metadata facet show the num-
ber of documents in the current view with those at-
tribute values, which helps tell the user what to ex-
1Frequency is computed using topic assignments from a
Gibbs sampler (Griffiths and Steyvers, 2004).
2Topic quality is given by the entropy of its word distribu-
tion. Other options include Mimno and Blei (2011).
3The difference of the probability of a topic in the current
document and the topic overall, divided by value overall.
6
Figure 2: Left: Documents Page. The left pane shows the available facets (topics and metadata) and the right pane
shows the matching documents (collapsed view.) Right: Expanded Document. An expanded collapsed document is
replaced with this more detailed view, showing the entire document as well as metadata and topic graphs.
pect if she refines her query.
Sarah notices that the News Desk value of
?Sports? matches a large number of documents in
the current view. She adds this filter to the current
facet query, updating the document view.
At the top of the document pane are the cur-
rent view?s ?Aggregate Statistics?, which shows how
many documents match the current query. An ex-
pandable box shows graphs for the current docu-
ments topic distribution and distinctive topics.4
Looking at the topic graph for the current query,
Sarah sees that another topic with sports related
words appears with high probability. She adds it to
the search and updates the document view.
Any document can be tagged with user-created
tags. Tags and their associated documents are dis-
played in the corpus overview on the configuration
page. If a user finds a search query of interest, she
can save and name the search to return to it later.
Sarah sees many documents relevant to the New
York City Marathon. She tags documents of interest
and saves the query for later reference.
2.1 Implementation Details
Our web based tool makes it easy for users to share
results, maintain the system, and make the tool
widely available. The application is built with a
JSP front-end, a Java back-end, and a MongoDB
database for storing the corpus and associated data.
To ensure a fast UI, filters use an in-memory meta-
data and topic index. Searches are cached so incre-
mental search queries are very fast. The UI uses
4Computed as above but with more topics displayed.
Ajax and JQuery UI for dynamic loading and inter-
active elements. We easily hosted more than a dozen
corpora on a single installation.
3 Evaluation
Our primary goal was to investigate whether incor-
porating topic model output along with document
metadata into a faceted browser provided an effec-
tive mechanism for filtering documents. Participants
were presented with four tasks consisting of a ques-
tion to answer using the tool and a paragraph provid-
ing context. The first three tasks tested exploration
(find documents) while the last tested analysis (learn
about article authors). At the end of each task, the
users were directed to a survey on the tool?s useful-
ness. We also logged user actions to further evaluate
how they used the tool.
3.1 Participants and Experimental Setup
Twelve participants (3 female, 9 male) volunteered
after receiving an email from a local mailing list.
They received no compensation for their participa-
tion and they were able to complete the experiment
in their preferred environment at a convenient time
by accessing the tool online. They were provided
with a tool guide and were encouraged to familiarize
themselves with the tool before beginning the tasks;
logs suggest 8 of 12 did exploration before starting.
The study required participants to find informa-
tion from a selection of 10,000 documents from
the New York Times Annotated Corpus (Sandhaus,
2008), which contains a range of metadata.5 All
5The full list of metadata fields that we allowed users to ac-
7
documents in the corpus were published in January
of 1995 and we made no effort at deduplication.
Topics were generated using the Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) implementation
in MALLET (McCallum, 2002). We used 100 top-
ics trained with 1500 Gibbs iterations and hyper-
parameter optimization.
3.2 Quantitative Results
The length of time required to complete individual
tasks ranged from 1 minute and 3 seconds to 24 min-
utes and 54 seconds (average 9 minutes.) 6
Within the scope of each task, each user initi-
ated on average 5.75 searches. The time between
searches was on average 1 minute and 53 seconds.
Of all the searches, 21.4% were new searches and
78.6% built on previous searches when users chose
to expand or narrow the scope of the search. When
users initiated new search queries, they began with
queries on topics 59.3% of the time, with queries on
metadata 37.3% of the time, and queries that used
both topics and metadata 3.4% of the time. This
lends credence to the claim that the ability to access
both metadata and topics is crucial.
We asked users to rate features in terms of their
usefulness on a Likert scale from 1 (not helpful at
all) to 5 (extremely helpful). The most preferred fea-
tures were filtering on topics (mean 4.217, median 5)
and compacted documents (mean 3.848, median 5)
The least preferred were document graphs of topic
usage (mean 1.848, median 1) and aggregate statis-
tics (mean 1.891, median 1).7 The fact that filtering
on topics was the most preferred feature validates
our approach of including topics as a facet. Addi-
tionally, topic names were critical to this success.
3.3 Surveys
Users provided qualitative feedback8 by describing
their approaches to the task, and offering sugges-
cess in the study was: online section, organization, news desk,
date, locations, series name, byline (author), people, title, fea-
ture page, and descriptors.
6These times do not include the 3 instances in which a user
felt unable to complete a task. Also omitted are 11 tasks (from
4 users) for which log files could not provide accurate times.
7Ratings are likely influenced by the specific nature of the
sample user tasks. In tasks that required seeking out metadata,
expanded document views rated higher than their average.
8The survey results presented here consist of one survey per
participant per task, with two exceptions where two participants
tions, the most common of which was an increase
in allowed query complexity, a feature we intend to
enhance. In the current version, all search terms are
combined using AND; 7 of the 12 participants made
requests for a NOT option.
Some users (6 of 12) admitted to using their
browser?s search feature to help complete the tasks.
We chose to forgo a keyword search capability in the
study-ready version of the tool because we wanted
to test the ability of topic information to provide a
way to navigate the content. Given the heavy us-
age of topic searches and the ability of users to com-
plete tasks with or without browser search, we have
demonstrated the usefulness of the topics as a win-
dow into the content. In future versions, we envision
incorporating keyword search capabilities, including
suggested topic filters for searched queries.
As users completed the tasks, their comfort with
the tool increased. One user wrote, ?After the last
task I knew exactly what to do to get my results. I
knew what information would help me find docu-
ments.? Users also began to suggest new ways that
they would like to see topics and metadata com-
bined. Task 4 led one user to say ?It would be in-
teresting to see a page on each author and what top-
ics they mostly covered.? We could provide this in a
general way by showing a page for each metadata at-
tribute that contains relevant topics and other meta-
data. We intend to implement such features.
4 Conclusion
A user evaluation of MetaToMATo, our toolkit for
visualizing text corpora that incorporates both topic
models and metadata, confirms the validity of our
approach to use topic models and metadata in a sin-
gle faceted browser. Users searched with topics a
majority of the time, but also made use of metadata.
This clearly demonstrates a reliance on both, sug-
gesting that users went back and forth as needed.
Additionally, while metadata is traditionally used for
facets, users ranked filtering by topic more highly
than metadata. This suggests a new direction in
which advances in topic models can be used to aid
corpus exploration.
each failed to record one of their four surveys.
8
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
M. Cataldi, L. Di Caro, and C. Schifanella. 2011. Im-
mex: Immersive text documents exploration system.
In Content-Based Multimedia Indexing (CBMI), 2011
9th International Workshop on, pages 1?6. IEEE.
A.J.B. Chaney and D.M. Blei. 2012. Visualizing topic
models. In AAAI.
J. Chuang, C.D. Manning, and J. Heer. 2012. Ter-
mite: visualization techniques for assessing textual
topic models. In Proceedings of the International
Working Conference on Advanced Visual Interfaces,
pages 74?77. ACM.
Christopher Collins, Fernanda B. Vie?gas, and Martin
Wattenberg. 2009. Parallel tag clouds to explore and
analyze faceted text corpora. In Proc. of the IEEE
Symp. on Visual Analytics Science and Technology
(VAST).
M. Correll, M. Witmore, and M. Gleicher. 2011. Explor-
ing collections of tagged text for literary scholarship.
Computer Graphics Forum, 30(3):731?740.
W. Cui, S. Liu, L. Tan, C. Shi, Y. Song, Z. Gao, H. Qu,
and X. Tong. 2011. Textflow: Towards better un-
derstanding of evolving topics in text. Visualiza-
tion and Computer Graphics, IEEE Transactions on,
17(12):2412?2421.
W. Dou, X. Wang, R. Chang, and W. Ribarsky. 2011.
Paralleltopics: A probabilistic approach to exploring
document collections. In Visual Analytics Science and
Technology (VAST), 2011 IEEE Conference on, pages
231?240. IEEE.
Jacob Eisenstein, Duen Horng ?Polo? Chau, Aniket Kit-
tur, and Eric P. Xing. 2012. Topicviz: Interactive topic
exploration in document collections. In CHI.
Jennifer English, Marti Hearst, Rashmi Sinha, Kirsten
Swearingen, and Ka-Ping Yee. 2002. Flexible search
and navigation using faceted metadata. In ACM SIGIR
Conference on Information Retrieval (SIGIR).
Ko Fujimura, Hiroyuki Toda, Takafumi Inoue, Nobuaki
Hiroshima, Ryoji Kataoka, and Masayuki Sugizaki.
2006. Blogranger - a multi-faceted blog search engine.
In World Wide Web (WWW).
Matthew J. Gardner, Joshua Lutes, Jeff Lund, Josh
Hansen, Dan Walker, Eric Ringger, and Kevin Seppi.
2010. The topic browser: An interactive tool for
browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.
T.L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228?5235.
Marti Hearst. 2006. Clustering versus faceted categories
for information exploration. Communications of the
ACM, 49(4).
Mika Ka?ki. 2005. Findex: search result categories help
users when document ranking fails. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, CHI ?05, pages 131?140, New York,
NY, USA. ACM.
S. Liu, M.X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian.
2009. Interactive, topic-based visual text summariza-
tion and analysis. In Proceedings of the 18th ACM
conference on Information and knowledge manage-
ment, pages 543?552. ACM.
G. Marchionini. 2006. Exploratory search: from find-
ing to understanding. Communications of the ACM,
49(4):41?46.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
D. Mimno and D. Blei. 2011. Bayesian checking for
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 227?237. Association for Computational Lin-
guistics.
Eyal Oren, Renaud Delbru, and Stefan Decker. 2006.
Extending faceted navigation for rdf data. In Interna-
tional Semantic Web Conference (ISWC).
Evan Sandhaus. 2008. The new york times annotated
corpus.
Christian Seeling and Andreas Becks. 2003. Exploit-
ing metadata for ontology-based visual exploration of
weakly structured text documents. In Proceedings of
the 7th International Conference on Information Visu-
alisation (IV03, pages 0?7695. IEEE Press, ISBN.
Greg Smith, Mary Czerwinski, Brian Meyers, Daniel
Robbins, George Robertson, and Desney S. Tan. 2006.
FacetMap: A Scalable Search and Browse Visualiza-
tion. IEEE Transactions on Visualization and Com-
puter Graphics, 12(5):797?804.
E.M. Talley, D. Newman, D. Mimno, B.W. Herr II,
H.M. Wallach, G.A.P.C. Burns, A.G.M. Leenders, and
A. McCallum. 2011. Database of nih grants using
machine-learned categories and graphical clustering.
Nature Methods, 8(6):443?444.
Ping Yee, Kirsten Swearingen, Kevin Li, and Marti
Hearst. 2003. Faceted metadata for image search and
browsing. In Computer-Human Interaction (CHI).
9
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181?186,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
I?m a Belieber:
Social Roles via Self-identification and Conceptual Attributes
Charley Beller, Rebecca Knowles, Craig Harman
Shane Bergsma
?
, Margaret Mitchell
?
, Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
?
University of Saskatchewan, Saskatoon, Saskatchewan Canada
?
Microsoft Research, Redmond, Washington USA
charleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,
shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.edu
Abstract
Motivated by work predicting coarse-
grained author categories in social me-
dia, such as gender or political preference,
we explore whether Twitter contains infor-
mation to support the prediction of fine-
grained categories, or social roles. We
find that the simple self-identification pat-
tern ?I am a ? supports significantly
richer classification than previously ex-
plored, successfully retrieving a variety of
fine-grained roles. For a given role (e.g.,
writer), we can further identify character-
istic attributes using a simple possessive
construction (e.g., writer?s ). Tweets
that incorporate the attribute terms in first
person possessives (my ) are confirmed
to be an indicator that the author holds the
associated social role.
1 Introduction
With the rise of social media, researchers have
sought to induce models for predicting latent au-
thor attributes such as gender, age, and politi-
cal preferences (Garera and Yarowsky, 2009; Rao
et al, 2010; Burger et al, 2011; Van Durme,
2012b; Zamal et al, 2012). Such models are
clearly in line with the goals of both computa-
tional advertising (Wortman, 2008) and the grow-
ing area of computational social science (Conover
et al, 2011; Nguyen et al, 2011; Paul and Dredze,
2011; Pennacchiotti and Popescu, 2011; Moham-
mad et al, 2013) where big data and computa-
tion supplement methods based on, e.g., direct hu-
man surveys. For example, Eisenstein et al (2010)
demonstrated a model that predicted where an au-
thor was located in order to analyze regional dis-
tinctions in communication. While some users ex-
plicitly share their GPS coordinates through their
Twitter clients, having a larger collection of au-
tomatically identified users within a region was
preferable even though the predictions for any
given user were uncertain.
We show that media such as Twitter can sup-
port classification that is more fine-grained than
gender or general location. Predicting social roles
such as doctor, teacher, vegetarian, christian,
may open the door to large-scale passive surveys
of public discourse that dwarf what has been pre-
viously available to social scientists. For exam-
ple, work on tracking the spread of flu infections
across Twitter (Lamb et al, 2013) might be en-
hanced with a factor based on aggregate predic-
tions of author occupation.
We present two studies showing that first-
person social content (tweets) contains intuitive
signals for such fine-grained roles. We argue that
non-trivial classifiers may be constructed based
purely on leveraging simple linguistic patterns.
These baselines suggest a wide range of author
categories to be explored further in future work.
Study 1 In the first study, we seek to determine
whether such a signal exists in self-identification:
we rely on variants of a single pattern, ?I am a ?,
to bootstrap data for training balanced-class binary
classifiers using unigrams observed in tweet con-
tent. As compared to prior research that required
actively polling users for ground truth in order to
construct predictive models for demographic in-
formation (Kosinski et al, 2013), we demonstrate
that some users specify such properties publicly
through direct natural language.
Many of the resultant models show intuitive
strongly-weighted features, such as a writer be-
ing likely to tweet about a story, or an ath-
lete discussing a game. This demonstrates self-
identification as a viable signal in building predic-
tive models of social roles.
181
Role Tweet
artist I?m an Artist..... the last of a dying breed
belieber @justinbieber I will support you in ev-
erything you do because I am a belieber
please follow me I love you 30
vegetarian So glad I?m a vegetarian.
Table 1: Examples of self-identifying tweets.
# Role # Role # Role
29,924 little 5,694 man 564 champion
21,822 big ... ... 559 teacher
18,957 good 4,007 belieber 556 writer
13,069 huge 3,997 celebrity 556 awful
13,020 bit 3,737 virgin ... ...
12,816 fan 3,682 pretty 100 cashier
10,832 bad ... ... 100 bro
10,604 girl 2,915 woman ... ...
9,981 very 2,851 beast 10 linguist
... ... ... ... ... ...
Table 2: Number of self-identifying users per ?role?. While
rich in interesting labels, cases such as very highlight the pur-
poseful simplicity of the current approach.
Study 2 In the second study we exploit a com-
plementary signal based on characteristic con-
ceptual attributes of a social role, or concept
class (Schubert, 2002; Almuhareb and Poesio,
2004; Pas?ca and Van Durme, 2008). We identify
typical attributes of a given social role by collect-
ing terms in the Google n-gram corpus that occur
frequently in a possessive construction with that
role. For example, with the role doctor we extract
terms matching the simple pattern ?doctor?s ?.
2 Self-identification
All role-representative users were drawn from
the free public 1% sample of the Twitter Fire-
hose, over the period 2011-2013, from the sub-
set that selected English as their native language
(85,387,204 unique users). To identify users of
a particular role, we performed a case-agnostic
search of variants of a single pattern: I am a(n)
, and I?m a(n) , where all single tokens filling
the slot were taken as evidence of the author self-
reporting for the given ?role?. Example tweets can
be seen in Table 1, examples of frequency per role
in Table 2. This resulted in 63,858 unique roles
identified, of which 44,260 appeared only once.
1
We manually selected a set of roles for fur-
ther exploration, aiming for a diverse sample
across: occupation (e.g., doctor, teacher), family
(mother), disposition (pessimist), religion (chris-
1
Future work should consider identifying multi-word role
labels (e.g., Doctor Who fan, or dog walker).
0.60
0.65
0.70
0.75
0.80
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
direc
tione
r
belie
ber
optim
ist
sold
ier
soph
omo
re
pess
imis
t
ran
dom
.0
danc
er
hips
ter
ran
dom
.2
sing
er
fresh
man
mo
ther
ran
dom
.1
chee
rlead
er
rapp
er
chris
tian
artis
t
sm
oker acto
r
vege
taria
n
wo
ma
n
athle
te
geek engi
neer
wa
itres
s
nur
se
ma
n
stud
ent doct
or poet writ
er
athe
ist
gran
dmalawy
er
teac
her
Role
Cha
nce 
of S
ucce
ss
Figure 1: Success rate for querying a user. Random.0,1,2
are background draws from the population, with the mean of
those three samples drawn horizontally. Tails capture 95%
confidence intervals.
tian), and ?followers? (belieber, directioner).
2
We filtered users via language ID (Bergsma et al,
2012) to better ensure English content.
3
For each selected role, we randomly sampled up
to 500 unique self-reporting users and then queried
Twitter for up to 200 of their recent publicly
posted tweets.
4
These tweets served as represen-
tative content for that role, with any tweet match-
ing the self-reporting patterns filtered. Three sets
of background populations were extracted based
on randomly sampling users that self-reported En-
glish (post-filtered via LID).
Twitter users are empowered to at any time
delete, rename or make private their accounts.
Any given user taken to be representative based on
a previously posted tweet may no longer be avail-
able to query on. As a hint of the sort of user stud-
ies one might explore given access to social role
prediction, we see in Figure 1 a correlation be-
tween self-reported role and the chance of an ac-
count still being publicly visible, with roles such
as belieber and directioner on the one hand, and
doctor and teacher on the other.
The authors examined the self-identifying tweet
of 20 random users per role. The accuracy of the
self-identification pattern varied across roles and
is attributable to various factors including quotes,
e.g. @StarTrek Jim, I?m a DOCTOR not a down-
load!. While these samples are small (and thus
estimates of quality come with wide variance), it
2
Those that follow the music/life of the singer Justin
Bieber and the band One Direction, respectively.
3
This removes users that selected English as their primary
language, used a self-identification phrase, e.g. I am a be-
lieber, but otherwise tended to communicate in non-English.
4
Roughly half of the classes had less than 500 self-
reporting users in total, in those cases we used all matches.
182
actorartistatheist
athletebeliebercheerleader
christiandancerdirectioner
doctorengineerfreshman
geekgrandmahipster
lawyermanmother
nurseoptimistpessimist
poetrappersinger
smokersoldiersophomore
studentteachervegetarian
waitresswomanwriter
0 5 10 15
Figure 2: Valid self-identifying tweets from sample of 20.
is noteworthy that a non-trivial number for each
were judged as actually self-identifying.
Indicative Language Most work in user clas-
sification relies on featurizing language use,
most simply through binary indicators recording
whether a user did or did not use a particular word
in a history of n tweets. To explore whether lan-
guage provides signal for future work in fine-grain
social role prediction, we constructed a set of ex-
periments, one per role, where training and test
sets were balanced between users from a random
background sample and self-reported users. Base-
line accuracy in these experiments was thus 50%.
Each training set had a target of 600 users (300
background, 300 self-identified); for those roles
with less than 300 users self-identifying, all users
were used, with an equal number background. We
used the Jerboa (Van Durme, 2012a) platform
to convert data to binary feature vectors over a un-
igram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al, 2008). We used the pos-
itively annotated data to form test sets, balanced
with data from the background set. Each test set
had a theoretical maximum size of 40, but for sev-
eral classes it was in the single digits (see Fig-
ure 2). Despite the varied noisiness of our simple
pattern-bootstrapped training data, and the small
size of our annotated test set, we see in Figure 3
that we are able to successfully achieve statisti-
cally significant predictions of social role for the
majority of our selected examples.
Table 3 highlights examples of language indica-
tive of role, as determined by the most positively
weighted unigrams in the classification experi-
0.2
0.4
0.6
0.8
1.0
l
l l l
l
ll
l ll
l
l l
l
ll
l ll
l
l
ll l
l
l
l
l l l
l
l
l
sold
ier
wo
ma
n
pess
imis
t
chris
tian
gran
dma
nur
se
rapp
er
ma
n poet
chee
rlead
er
stud
ent
engi
neer acto
r
teac
her
vege
taria
n
mo
ther sing
er
lawy
er
optim
ist
wa
itres
s
sm
oker hips
ter doct
or
danc
er
artis
t
fresh
man
direc
tione
r
geek
soph
omo
re
athe
ist
athle
te
writ
er
belie
ber
Role
Acc
urac
y
Figure 3: Accuracy in classifying social roles.
Role :: Feature ( Rank)
artist morning, summer, life, most, amp, studio
atheist fuck, fucking, shit, makes, dead, ..., religion
19
athlete lol, game, probably, life, into, ..., team
9
belieber justin, justinbeiber, believe, beliebers, bieber
cheerleader cheer, best, excited, hate, mom, ..., prom
16
christian lol, ..., god
12
, pray
13
, ..., bless
17
, ..., jesus
20
dancer dance, since, hey, never, been
directioner harry, d, follow, direction, never, liam, niall
doctor sweet, oh, or, life, nothing
engineer (, then, since, may, ), test
9
, -
17
, =
18
freshman summer, homework, na, ..., party
19
, school
20
geek trying, oh, different, dead, been
grandma morning, baby, around, night, excited
hipster fucking, actually, thing, fuck, song
lawyer did, never, his, may, pretty, law, even, office
man man, away, ai, young, since
mother morning, take, fuck, fucking, trying
nurse lol, been, morning, ..., night
10
, nursing
11
, shift
13
optimist morning, enough, those, everything, never
poet feel, song, even, say, yo
rapper fuck, morning, lol, ..., mixtape
8
, songs
15
singer sing, song, music, lol, never
smoker fuck, shit, fucking, since, ass, smoke, weed
20
solider ai, beautiful, lol, wan, trying
sophmore summer, >, ..., school
11
, homework
12
student anything, summer, morning, since, actually
teacher teacher, morning, teach, ..., students
7
, ..., school
20
vegetarian actually, dead, summer, oh, morning
waitress man, try, goes, hate, fat
woman lol, into, woman, morning, never
writer write, story, sweet, very, working
Table 3: Most-positively weighted features per role, along
with select features within the top 20. Surprising mother
features come from ambigious self-identification, as seen in
tweets such as: I?m a mother f!cking starrrrr.
ment. These results qualitatively suggest many
roles under consideration may be teased out from a
background population by focussing on language
that follows expected use patterns. For example
the use of the term game by athletes, studio by
artists, mixtape by rappers, or jesus by Christians.
3 Characteristic Attributes
Bergsma and Van Durme (2013) showed that the
183
task of mining attributes for conceptual classes can
relate straightforwardly to author attribute predic-
tion. If one views a role, in their case gender, as
two conceptual classes, male and female, then ex-
isting attribute extraction methods for third-person
content (e.g., news articles) can be cheaply used to
create a set of bootstrapping features for building
classifiers over first-person content (e.g., tweets).
For example, if we learn from news corpora that:
a man may have a wife, then a tweet saying: ...my
wife... can be taken as potential evidence of mem-
bership in the male conceptual class.
In our second study, we test whether this idea
extends to our wider set of fine-grained roles. For
example, we aimed to discover that a doctor may
have a patient, while a hairdresser may have a
salon; these properties can be expressed in first-
person content as possessives like my patient or my
salon. We approached this task by selecting target
roles from the first experiment and ranking charac-
teristic attributes for each using pointwise mutual
information (PMI) (Church and Hanks, 1990).
First, we counted all terms matching a target
social role?s possessive pattern (e.g., doctor?s )
in the web-scale n-gram corpus Google V2 (Lin
et al, 2010)
5
. We ranked the collected terms
by computing PMI between classes and attribute
terms. Probabilities were estimated from counts of
the class-attribute pairs along with counts match-
ing the generic possessive patterns his and
her which serve as general background cate-
gories. Following suggestions by Bergsma and
Van Durme, we manually filtered the ranked list.
6
We removed attributes that were either (a) not
nominal, or (b) not indicative of the social role.
This left fewer than 30 attribute terms per role,
with many roles having fewer than 10.
We next performed a precision test to identify
potentially useful attributes in these lists. We ex-
amined tweets with a first person possessive pat-
tern for each attribute term from a small corpus
of tweets collected over a single month in 2013,
discarding those attribute terms with no positive
matches. This precision test is useful regardless
of how attribute lists are generated. The attribute
5
In this corpus, follower-type roles like belieber and di-
rectioner are not at all prevalent. We therefore focused on
occupational and habitual roles (e.g., doctor, smoker).
6
Evidence from cognitive work on memory-dependent
tasks suggests that such relevance based filtering (recogni-
tion) involves less cognitive effort than generating relevant
attributes (recall) see (Jacoby et al, 1979). Indeed, this filter-
ing step generally took less than a minute per class.
term chart, for example, had high PMI with doc-
tor; but a precision test on the phrase my chart
yielded a single tweet which referred not to a med-
ical chart but to a top ten list (prompting removal
of this attribute). Using this smaller high-precision
set of attribute terms, we collected tweets from the
Twitter Firehose over the period 2011-2013.
4 Attribute-based Classification
Attribute terms are less indicative overall than
self-ID, e.g., the phrase I?m a barber is a clearer
signal than my scissors. We therefore include a
role verification step in curating a collection of
positively identified users. We use the crowd-
sourcing platform Mechanical Turk
7
to judge
whether the person tweeting held a given role
Tweets were judged 5-way redundantly. Me-
chanical Turk judges (?Turkers?) were presented
with a tweet and the prompt: Based on this
tweet, would you think this person is a BAR-
BER/HAIRDRESSER? along with four response
options: Yes, Maybe, Hard to tell, and No.
We piloted this labeling task on 10 tweets per
attribute term over a variety of classes. Each an-
swer was associated with a score (Yes = 1, Maybe
= .5, Hard to tell = No = 0) and aggregated across
the five judges. We found in development that an
aggregate score of 4.0 (out of 5.0) led to an ac-
ceptable agreement rate between the Turkers and
the experimenters, when the tweets were randomly
sampled and judged internally. We found that
making conceptual class assignments based on a
single tweet was often a subtle task. The results of
this labeling study are shown in Figure 4, which
gives the percent of tweets per attribute that were
4.0 or above. Attribute terms shown in red were
manually discarded as being inaccurate (low on
the y-axis) or non-prevalent (small shape).
From the remaining attribute terms, we identi-
fied users with tweets scoring 4.0 or better as posi-
tive examples of the associated roles. Tweets from
those users were scraped via the Twitter API to
construct corpora for each role. These were split
intro train and test, balanced with data from the
same background set used in the self-ID study.
Test sets were usually of size 40 (20 positive, 20
background), with a few classes being sparse (the
smallest had only 16 instances). Results are shown
in Figure 5. Several classes in this balanced setup
can be predicted with accuracies in the 70-90%
7
https://www.mturk.com/mturk/
184
l l l l ll lll l l l
l l l l l l
l
l l ll
l
l l l l l
l
l l l
Actor/Actress Athlete Barber/Hairdresser Bartender Blogger Cheerleader
Christian College Student Dancer Doctor/Nurse Drummer Hunter
Jew Mom Musician Photographer Professor Rapper/Songwriter
Reporter Sailor Skier Smoker Soldier Student
Swimmer Tattoo Artist Waiter/Waitress Writer
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
rehear
sal theater directo
r lines
conc
ussionplayin
gprotein sport squadcondit
ioningjerseypositioncoach calves clien
t
scissor
s
shears salon bar blog bloggi
ng pom
hope testimo
ny
church bible schola
rship
syllabu
s
adviso
r
tuition campu
s
univer
sity
college tu
tu
scrub patient stethos
cope drum stand
shul angel deliver
y kid parent
ing set alum guitar piano shoot shutter lecture faculty studen
t lyrics
cove
rage editor article shi
p
goggle
s pipe smokin
g
tobacc
o
smoke cigaret
te
billet comba
t duffel orders bunk deploy
mentbarrac
ks stats cap lab philoso
phy
pool ink station tip apron script memo
ir poemKeyword
Above
 Thres
hold
log10(Count)
l l l l1 2 3 4Keep
l FALSE TRUE
Figure 4: Turker judged quality of attributes selected as
candidate features for bootstrapping positive instances of the
given social role.
0.5
0.6
0.7
0.8
acto
r
athle
te
barbe
r
blogg
er
chee
rlead
er
chris
tian docto
r
drum
mer
mo
m
mu
sicia
n
photo
graph
er
profe
ssor
repor
ter
smo
ker
soldi
er
stude
nt
waite
r
write
r
Accu
racy
Figure 5: Classifier accuracy on balanced set contrasting
agreed upon Twitter users of a given role against users pulled
at random from the 1% stream.
range, supporting our claim that there is discrimi-
nating content for a variety of these social roles.
Conditional Classification How accurately we
can predict membership in a given class when a
Twitter user sends a tweet matching one of the tar-
geted attributes? For example, if one sends a tweet
saying my coach, then how likely is it that author
Figure 6: Results of positive vs negative by attribute term.
Given that a user tweets . . . my lines . . . we are nearly 80%
accurate in identifying whether or not the user is an actor.
is an athlete?
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given attribute term. Positive instances were taken
to be those with a score of 4.0 or higher, with neg-
ative instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in Figure 6.
5 Conclusion
We have shown that Twitter contains sufficiently
robust signal to support more fine-grained au-
thor attribute prediction tasks than have previously
been attempted. Our results are based on simple,
intuitive search patterns with minimal additional
filtering: this establishes the feasibility of the task,
but leaves wide room for future work, both in the
sophistication in methodology as well as the diver-
sity of roles to be targeted. We exploited two com-
plementary types of indicators: self-identification
and self-possession of conceptual class (role) at-
tributes. Those interested in identifying latent de-
mographics can extend and improve these indica-
tors in developing ways to identify groups of inter-
est within the general population of Twitter users.
Acknowledgements This material is partially
based on research sponsored by the NSF un-
der grants DGE-123285 and IIS-1249516 and by
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
185
References
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an eval-
uation. In Proceedings of EMNLP.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clay Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the NAACL Workshop
on Language and Social Media.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Jacob Eisenstein, Brendan O?Connor, Noah Smith, and
Eric P. Xing. 2010. A latent variable model of
geographical lexical variation. In Proceedings of
EMNLP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. Proceed-
ings of the National Academy of Sciences.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In Proceedings of NAACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Saif M. Mohammad, Svetlana Kiritchenko, and Joel
Martin. 2013. Identifying purpose behind elec-
toral tweets. In Proceedings of the Second Interna-
tional Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ?13, pages 1?9.
Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115?123. Association for Computational Lin-
guistics.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-Supervised Acquisition of Open-Domain
Classes and Class Attributes from Web Documents
and Query Logs. In Proceedings of ACL.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 430?
438. ACM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Lenhart K. Schubert. 2002. Can we derive general
world knowledge from texts? In Proceedings of
HLT.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Jennifer Wortman. 2008. Viral marketing and the
diffusion of trends on social networks. Technical
Report MS-CIS-08-19, University of Pennsylvania,
May.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
186
