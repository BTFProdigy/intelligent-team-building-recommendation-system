Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 688?695,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Unified Tagging Approach to Text Normalization 
Conghui Zhu 
Harbin Institute of Technology 
Harbin, China 
chzhu@mtlab.hit.edu.cn 
Jie Tang 
Department of Computer Science
Tsinghua University, China 
jietang@tsinghua.edu.cn
Hang Li 
Microsoft Research Asia  
Beijing, China 
hangli@microsoft.com
Hwee Tou Ng 
Department of Computer Science 
National University of Singapore, Singapore 
nght@comp.nus.edu.sg 
Tiejun Zhao 
Harbin Institute of Technology 
Harbin, China 
tjzhao@mtlab.hit.edu.cn 
 
 
Abstract 
This paper addresses the issue of text nor-
malization, an important yet often over-
looked problem in natural language proc-
essing. By text normalization, we mean 
converting ?informally inputted? text into 
the canonical form, by eliminating ?noises? 
in the text and detecting paragraph and sen-
tence boundaries in the text. Previously, 
text normalization issues were often under-
taken in an ad-hoc fashion or studied sepa-
rately. This paper first gives a formaliza-
tion of the entire problem. It then proposes 
a unified tagging approach to perform the 
task using Conditional Random Fields 
(CRF). The paper shows that with the in-
troduction of a small set of tags, most of 
the text normalization tasks can be per-
formed within the approach. The accuracy 
of the proposed method is high, because 
the subtasks of normalization are interde-
pendent and should be performed together. 
Experimental results on email data cleaning 
show that the proposed method signifi-
cantly outperforms the approach of using 
cascaded models and that of employing in-
dependent models. 
1 Introduction 
More and more ?informally inputted? text data be-
comes available to natural language processing, 
such as raw text data in emails, newsgroups, fo-
rums, and blogs. Consequently, how to effectively 
process the data and make it suitable for natural 
language processing becomes a challenging issue. 
This is because informally inputted text data is 
usually very noisy and is not properly segmented. 
For example, it may contain extra line breaks, extra 
spaces, and extra punctuation marks; and it may 
contain words badly cased. Moreover, the bounda-
ries between paragraphs and the boundaries be-
tween sentences are not clear. 
We have examined 5,000 randomly collected 
emails and found that 98.4% of the emails contain 
noises (based on the definition in Section 5.1). 
In order to perform high quality natural lan-
guage processing, it is necessary to perform ?nor-
malization? on informally inputted data first, spe-
cifically, to remove extra line breaks, segment the 
text into paragraphs, add missing spaces and miss-
ing punctuation marks, eliminate extra spaces and 
extra punctuation marks, delete unnecessary tokens, 
correct misused punctuation marks, restore badly 
cased words, correct misspelled words, and iden-
tify sentence boundaries. 
Traditionally, text normalization is viewed as an 
engineering issue and is conducted in a more or 
less ad-hoc manner. For example, it is done by us-
ing rules or machine learning models at different 
levels. In natural language processing, several is-
sues of text normalization were studied, but were 
only done separately. 
This paper aims to conduct a thorough investiga-
tion on the issue. First, it gives a formalization of 
688
the problem; specifically, it defines the subtasks of 
the problem. Next, it proposes a unified approach 
to the whole task on the basis of tagging. Specifi-
cally, it takes the problem as that of assigning tags 
to the input texts, with a tag representing deletion, 
preservation, or replacement of a token. As the 
tagging model, it employs Conditional Random 
Fields (CRF). The unified model can achieve better 
performances in text normalization, because the 
subtasks of text normalization are often interde-
pendent. Furthermore, there is no need to define 
specialized models and features to conduct differ-
ent types of cleaning; all the cleaning processes 
have been formalized and conducted as assign-
ments of the three types of tags. 
Experimental results indicate that our method 
significantly outperforms the methods using cas-
caded models or independent models on normali-
zation. Our experiments also indicate that with the 
use of the tags defined, we can conduct most of the 
text normalization in the unified framework. 
Our contributions in this paper include: (a) for-
malization of the text normalization problem, (b) 
proposal of a unified tagging approach, and (c) 
empirical verification of the effectiveness of the 
proposed approach. 
The rest of the paper is organized as follows. In 
Section 2, we introduce related work. In Section 3, 
we formalize the text normalization problem. In 
Section 4, we explain our approach to the problem 
and in Section 5 we give the experimental results. 
We conclude the paper in Section 6. 
2 Related Work 
Text normalization is usually viewed as an 
engineering issue and is addressed in an ad-hoc 
manner. Much of the previous work focuses on 
processing texts in clean form, not texts in 
informal form. Also, prior work mostly focuses on 
processing one type or a small number of types of 
errors, whereas this paper deals with many 
different types of errors. 
Clark (2003) has investigated the problem of 
preprocessing noisy texts for natural language 
processing. He proposes identifying token bounda-
ries and sentence boundaries, restoring cases of 
words, and correcting misspelled words by using a 
source channel model. 
Minkov et al (2005) have investigated the prob-
lem of named entity recognition in informally in-
putted texts. They propose improving the perform-
ance of personal name recognition in emails using 
two machine-learning based methods: Conditional 
Random Fields and Perceptron for learning HMMs. 
See also (Carvalho and Cohen, 2004). 
Tang et al (2005) propose a cascaded approach 
for email data cleaning by employing Support Vec-
tor Machines and rules. Their method can detect 
email headers, signatures, program codes, and ex-
tra line breaks in emails. See also (Wong et al, 
2007). 
Palmer and Hearst (1997) propose using a Neu-
ral Network model to determine whether a period 
in a sentence is the ending mark of the sentence, an 
abbreviation, or both. See also (Mikheev, 2000; 
Mikheev, 2002). 
Lita et al (2003) propose employing a language 
modeling approach to address the case restoration 
problem. They define four classes for word casing: 
all letters in lower case, first letter in uppercase, all 
letters in upper case, and mixed case, and formal-
ize the problem as assigning class labels to words 
in natural language texts. Mikheev (2002) proposes 
using not only local information but also global 
information in a document in case restoration. 
Spelling error correction can be formalized as a 
classification problem. Golding and Roth (1996) 
propose using the Winnow algorithm to address 
the issue. The problem can also be formalized as 
that of data conversion using the source channel 
model. The source model can be built as an n-gram 
language model and the channel model can be con-
structed with confusing words measured by edit 
distance. Brill and Moore, Church and Gale, and 
Mayes et al have developed different techniques 
for confusing words calculation (Brill and Moore, 
2000; Church and Gale, 1991; Mays et al, 1991). 
Sproat et al (1999) have investigated normaliza-
tion of non-standard words in texts, including 
numbers, abbreviations, dates, currency amounts, 
and acronyms. They propose a taxonomy of non-
standard words and apply n-gram language models, 
decision trees, and weighted finite-state transduc-
ers to the normalization. 
3 Text Normalization 
In this paper we define text normalization at three 
levels: paragraph, sentence, and word level. The 
subtasks at each level are listed in Table 1. For ex-
ample, at the paragraph level, there are two sub-
689
tasks: extra line-break deletion and paragraph 
boundary detection. Similarly, there are six (three) 
subtasks at the sentence (word) level, as shown in 
Table 1. Unnecessary token deletion refers to dele-
tion of tokens like ?-----? and ?====?, which are 
not needed in natural language processing. Note 
that most of the subtasks conduct ?cleaning? of 
noises, except paragraph boundary detection and 
sentence boundary detection. 
Level Task Percentages of Noises
Extra line break deletion 49.53 
Paragraph 
Paragraph boundary detection  
Extra space deletion 15.58 
Extra punctuation mark deletion 0.71 
Missing space insertion 1.55 
Missing punctuation mark insertion 3.85 
Misused punctuation mark correction 0.64 
Sentence 
Sentence boundary detection  
Case restoration 15.04 
Unnecessary token deletion 9.69 Word 
Misspelled word correction 3.41 
Table 1. Text Normalization Subtasks 
As a result of text normalization, a text is seg-
mented into paragraphs; each paragraph is seg-
mented into sentences with clear boundaries; and 
each word is converted into the canonical form. 
After normalization, most of the natural language 
processing tasks can be performed, for example, 
part-of-speech tagging and parsing. 
We have manually cleaned up some email data 
(cf., Section 5) and found that nearly all the noises 
can be eliminated by performing the subtasks de-
fined above. Table 1 gives the statistics. 
1.  i?m thinking about buying a pocket 
2.  pc    device for my wife this christmas,. 
3.  the worry that i have is that she won?t 
4.  be able to sync it to her outlook express  
5.  contacts? 
Figure 1. An example of informal text 
I?m thinking about buying a Pocket PC device for my 
wife this Christmas.// The worry that I have is that 
she won?t be able to sync it to her Outlook Express 
contacts.// 
Figure 2. Normalized text 
Figure 1 shows an example of informally input-
ted text data. It includes many typical noises. From 
line 1 to line 4, there are four extra line breaks at 
the end of each line. In line 2, there is an extra 
comma after the word ?Christmas?. The first word 
in each sentence and the proper nouns (e.g., 
?Pocket PC? and ?Outlook Express?) should be 
capitalized. The extra spaces between the words 
?PC? and ?device? should be removed. At the end 
of line 2, the line break should be removed and a 
space is needed after the period. The text should be 
segmented into two sentences. 
Figure 2 shows an ideal output of text normali-
zation on the input text in Figure 1. All the noises 
in Figure 1 have been cleaned and paragraph and 
sentence endings have been identified. 
We must note that dependencies (sometimes 
even strong dependencies) exist between different 
types of noises. For example, word case restoration 
needs help from sentence boundary detection, and 
vice versa. An ideal normalization method should 
consider processing all the tasks together. 
4 A Unified Tagging Approach 
4.1 Process 
In this paper, we formalize text normalization as a 
tagging problem and employ a unified approach to 
perform the task (no matter whether the processing 
is at paragraph level, sentence level, or word level). 
There are two steps in the method: preprocess-
ing and tagging. In preprocessing, (A) we separate 
the text into paragraphs (i.e., sequences of tokens), 
(B) we determine tokens in the paragraphs, and (C) 
we assign possible tags to each token. The tokens 
form the basic units and the paragraphs form the 
sequences of units in the tagging problem. In tag-
ging, given a sequence of units, we determine the 
most likely corresponding sequence of tags by us-
ing a trained tagging model. In this paper, as the 
tagging model, we make use of CRF. 
Next we describe the steps (A)-(C) in detail and 
explain why our method can accomplish many of 
the normalization subtasks in Table 1. 
(A). We separate the text into paragraphs by tak-
ing two or more consecutive line breaks as the end-
ings of paragraphs. 
(B). We identify tokens by using heuristics. 
There are five types of tokens: ?standard word?, 
?non-standard word?, punctuation mark, space, and 
line break. Standard words are words in natural 
language. Non-standard words include several 
general ?special words? (Sproat et al, 1999), email 
address, IP address, URL, date, number, money, 
percentage, unnecessary tokens (e.g., ?===? and 
690
?###?), etc. We identify non-standard words by 
using regular expressions. Punctuation marks in-
clude period, question mark, and exclamation mark. 
Words and punctuation marks are separated into 
different tokens if they are joined together. Natural 
spaces and line breaks are also regarded as tokens. 
(C). We assign tags to each token based on the 
type of the token. Table 2 summarizes the types of 
tags defined. 
Token Type Tag Description 
PRV Preserve line break 
RPA Replace line break by space Line break 
DEL Delete line break 
PRV Preserve space 
Space 
DEL Delete space 
PSB Preserve punctuation mark and view it as sentence ending 
PRV Preserve punctuation mark without viewing it as sentence ending 
Punctuation 
mark 
DEL Delete punctuation mark 
AUC Make all characters in uppercase 
ALC Make all characters in lowercase 
FUC Make the first character in uppercase
Word 
AMC Make characters in mixed case 
PRV Preserve the special token 
Special token 
DEL Delete the special token 
Table 2. Types of tags 
 
Figure 3. An example of tagging 
Figure 3 shows an example of the tagging proc-
ess. (The symbol ??? indicates a space). In the fig-
ure, a white circle denotes a token and a gray circle 
denotes a tag. Each token can be assigned several 
possible tags. 
Using the tags, we can perform most of the text 
normalization processing (conducting seven types 
of subtasks defined in Table 1 and cleaning 
90.55% of the noises). 
In this paper, we do not conduct three subtasks, 
although we could do them in principle. These in-
clude missing space insertion, missing punctuation 
mark insertion, and misspelled word correction. In 
our email data, it corresponds to 8.81% of the 
noises. Adding tags for insertions would increase 
the search space dramatically. We did not do that 
due to computation consideration. Misspelled word 
correction can be done in the same framework eas-
ily. We did not do that in this work, because the 
percentage of misspelling in the data is small. 
We do not conduct misused punctuation mark 
correction as well (e.g., correcting ?.? with ???). It 
consists of 0.64% of the noises in the email data. 
To handle it, one might need to parse the sentences. 
4.2 CRF Model 
We employ Conditional Random Fields (CRF) as 
the tagging model. CRF is a conditional probability 
distribution of a sequence of tags given a sequence 
of tokens, represented as P(Y|X) , where X denotes 
the token sequence and Y the tag sequence 
(Lafferty et al, 2001). 
In tagging, the CRF model is used to find the 
sequence of tags Y* having the highest likelihood 
Y* = maxYP(Y|X), with an efficient algorithm (the 
Viterbi algorithm). 
In training, the CRF model is built with labeled 
data and by means of an iterative algorithm based 
on Maximum Likelihood Estimation. 
Transition Features 
yi-1=y?, yi=y 
yi-1=y?, yi=y, wi=w 
yi-1=y?, yi=y, ti=t 
State Features 
wi=w, yi=y 
wi-1=w, yi=y 
wi-2=w, yi=y 
wi-3=w, yi=y 
wi-4=w, yi=y 
wi+1=w, yi=y 
wi+2=w, yi=y 
wi+3=w, yi=y 
wi+4=w, yi=y 
wi-1=w?, wi=w, yi=y
wi+1=w?, wi=w, yi=y 
ti=t, yi=y 
ti-1=t, yi=y 
ti-2=t, yi=y 
ti-3=t, yi=y 
ti-4=t, yi=y 
ti+1=t, yi=y 
ti+2=t, yi=y 
ti+3=t, yi=y 
ti+4=t, yi=y 
ti-2=t??, ti-1=t?, yi=y 
ti-1=t?, ti=t, yi=y 
ti=t, ti+1=t?, yi=y 
ti+1=t?, ti+2=t??, yi=y 
ti-2=t??, ti-1=t?, ti=t, yi=y 
ti-1=t??, ti=t, ti+1=t?, yi=y 
ti=t, ti+1=t?, ti+2=t??, yi=y 
Table 3. Features used in the unified CRF model 
691
4.3 Features 
Two sets of features are defined in the CRF model: 
transition features and state features. Table 3 
shows the features used in the model. 
Suppose that at position i in token sequence x, wi 
is the token, ti the type of token (see Table 2), and 
yi the possible tag. Binary features are defined as 
described in Table 3. For example, the transition 
feature yi-1=y?, yi=y implies that if the current tag is 
y and the previous tag is y?, then the feature value 
is true; otherwise false. The state feature wi=w, 
yi=y implies that if the current token is w and the 
current label is y, then the feature value is true; 
otherwise false. In our experiments, an actual fea-
ture might be the word at position 5 is ?PC? and the 
current tag is AUC. In total, 4,168,723 features 
were used in our experiments. 
4.4 Baseline Methods 
We can consider two baseline methods based on 
previous work, namely cascaded and independent 
approaches. The independent approach performs 
text normalization with several passes on the text. 
All of the processes take the raw text as input and 
output the normalized/cleaned result independently. 
The cascaded approach also performs normaliza-
tion in several passes on the text. Each process car-
ries out cleaning/normalization from the output of 
the previous process. 
4.5 Advantages 
Our method offers some advantages. 
(1) As indicated, the text normalization tasks are 
interdependent. The cascaded approach or the in-
dependent approach cannot simultaneously per-
form the tasks. In contrast, our method can effec-
tively overcome the drawback by employing a uni-
fied framework and achieve more accurate per-
formances. 
(2) There are many specific types of errors one 
must correct in text normalization. As shown in 
Figure 1, there exist four types of errors with each 
type having several correction results. If one de-
fines a specialized model or rule to handle each of 
the cases, the number of needed models will be 
extremely large and thus the text normalization 
processing will be impractical. In contrast, our 
method naturally formalizes all the tasks as as-
signments of different types of tags and trains a 
unified model to tackle all the problems at once. 
5 Experimental Results 
5.1 Experiment Setting 
Data Sets 
We used email data in our experiments. We ran-
domly chose in total 5,000 posts (i.e., emails) from 
12 newsgroups. DC, Ontology, NLP, and ML are 
from newsgroups at Google (http://groups-
beta.google.com/groups). Jena is a newsgroup at Ya-
hoo (http://groups.yahoo.com/group/jena-dev). Weka 
is a newsgroup at Waikato University (https://list. 
scms.waikato.ac.nz). Prot?g? and OWL are from a 
project at Stanford University 
(http://protege.stanford.edu/). Mobility, WinServer, 
Windows, and PSS are email collections from a 
company. 
Five human annotators conducted normalization 
on the emails. A spec was created to guide the an-
notation process. All the errors in the emails were 
labeled and corrected. For disagreements in the 
annotation, we conducted ?majority voting?.  For 
example, extra line breaks, extra spaces, and extra 
punctuation marks in the emails were labeled. Un-
necessary tokens were deleted. Missing spaces and 
missing punctuation marks were added and marked. 
Mistakenly cased words, misspelled words, and 
misused punctuation marks were corrected. Fur-
thermore, paragraph boundaries and sentence 
boundaries were also marked. The noises fell into 
the categories defined in Table 1. 
Table 4 shows the statistics in the data sets. 
From the table, we can see that a large number of 
noises (41,407) exist in the emails. We can also see 
that the major noise types are extra line breaks, 
extra spaces, casing errors, and unnecessary tokens. 
In the experiments, we conducted evaluations in 
terms of precision, recall, F1-measure, and accu-
racy (for definitions of the measures, see for ex-
ample (van Rijsbergen, 1979; Lita et al, 2003)). 
Implementation of Baseline Methods 
We used the cascaded approach and the independ-
ent approach as baselines. 
For the baseline methods, we defined several 
basic prediction subtasks: extra line break detec-
tion, extra space detection, extra punctuation mark 
detection, sentence boundary detection, unneces-
sary token detection, and case restoration. We 
compared the performances of our method with 
those of the baseline methods on the subtasks. 
692
Data Set 
Number 
of 
Email 
Number 
of 
Noises 
Extra 
Line 
Break 
Extra 
Space 
Extra
 Punc.
Missing
Space
Missing
Punc.
Casing
Error
Spelling
Error
Misused 
Punc.
Unnece-
ssary 
Token 
Number of 
Paragraph 
Boundary 
Number of 
Sentence 
Boundary
DC 100 702 476 31 8 3 24 53 14 2 91 457 291 
Ontology 100 2,731 2,132 24 3 10 68 205 79 15 195 677 1,132 
NLP 60 861 623 12 1 3 23 135 13 2 49 244 296 
ML 40 980 868 17 0 2 13 12 7 0 61 240 589 
Jena 700 5,833 3,066 117 42 38 234 888 288 59 1,101 2,999 1,836 
Weka 200 1,721 886 44 0 30 37 295 77 13 339 699 602 
Prot?g? 700 3,306 1,770 127 48 151 136 552 116 9 397 1,645 1,035 
OWL 300 1,232 680 43 24 47 41 152 44 3 198 578 424 
Mobility 400 2,296 1,292 64 22 35 87 495 92 8 201 891 892 
WinServer 400 3,487 2,029 59 26 57 142 822 121 21 210 1,232 1,151 
Windows 1,000 9,293 3,416 3,056 60 116 348 1,309 291 67 630 3,581 2,742 
PSS 1,000 8,965 3,348 2,880 59 153 296 1,331 276 66 556 3,411 2,590 
Total 5,000 41,407 20,586 6,474 293 645 1,449 6,249 1,418 265 4,028 16,654 13,580 
Table 4. Statistics on data sets 
For the case restoration subtask (processing on 
token sequence), we employed the TrueCasing 
method (Lita et al, 2003). The method estimates a 
tri-gram language model using a large data corpus 
with correctly cased words and then makes use of 
the model in case restoration. We also employed 
Conditional Random Fields to perform case 
restoration, for comparison purposes. The CRF 
based casing method estimates a conditional 
probabilistic model using the same data and the 
same tags defined in TrueCasing. 
For unnecessary token deletion, we used rules as 
follows. If a token consists of non-ASCII charac-
ters or consecutive duplicate characters, such as 
?===?, then we identify it as an unnecessary token. 
For each of the other subtasks, we exploited the 
classification approach. For example, in extra line 
break detection, we made use of a classification 
model to identify whether or not a line break is a 
paragraph ending. We employed Support Vector 
Machines (SVM) as the classification model (Vap-
nik, 1998). In the classification model we utilized 
the same features as those in our unified model 
(see Table 3 for details). 
In the cascaded approach, the prediction tasks 
are performed in sequence, where the output of 
each task becomes the input of each immediately 
following task. The order of the prediction tasks is: 
(1) Extra line break detection: Is a line break a 
paragraph ending? It then separates the text into 
paragraphs using the remaining line breaks. (2) 
Extra space detection: Is a space an extra space? (3) 
Extra punctuation mark detection: Is a punctuation 
mark a noise? (4) Sentence boundary detection: Is 
a punctuation mark a sentence boundary? (5) Un-
necessary token deletion: Is a token an unnecessary 
token? (6) Case restoration. Each of steps (1) to (4) 
uses a classification model (SVM), step (5) uses 
rules, whereas step (6) uses either a language 
model (TrueCasing) or a CRF model (CRF). 
In the independent approach, we perform the 
prediction tasks independently. When there is a 
conflict between the outcomes of two classifiers, 
we adopt the result of the latter classifier, as de-
termined by the order of classifiers in the cascaded 
approach. 
To test how dependencies between different 
types of noises affect the performance of normali-
zation, we also conducted experiments using the 
unified model by removing the transition features. 
Implementation of Our Method 
In the implementation of our method, we used the 
tool CRF++, available at http://chasen.org/~taku 
/software/CRF++/. We made use of all the default 
settings of the tool in the experiments. 
5.2 Text Normalization Experiments 
Results 
We evaluated the performances of our method 
(Unified) and the baseline methods (Cascaded and 
Independent) on the 12 data sets. Table 5 shows 
the five-fold cross-validation results. Our method 
outperforms the two baseline methods. 
Table 6 shows the overall performances of text 
normalization by our method and the two baseline 
methods. We see that our method outperforms the 
two baseline methods. It can also be seen that the 
performance of the unified method decreases when 
removing the transition features (Unified w/o 
Transition Features). 
693
We conducted sign tests for each subtask on the 
results, which indicate that all the improvements of 
Unified over Cascaded and Independent are statis-
tically significant (p << 0.01). 
Detection Task Prec. Rec. F1 Acc.
Independent 95.16 91.52 93.30 93.81
Cascaded 95.16 91.52 93.30 93.81Extra Line Break  
Unified 93.87 93.63 93.75 94.53
Independent 91.85 94.64 93.22 99.87
Cascaded 94.54 94.56 94.55 99.89Extra Space 
Unified 95.17 93.98 94.57 99.90
Independent 88.63 82.69 85.56 99.66
Cascaded 87.17 85.37 86.26 99.66
Extra 
 Punctuation 
Mark Unified 90.94 84.84 87.78 99.71
Independent 98.46 99.62 99.04 98.36
Cascaded 98.55 99.20 98.87 98.08Sentence Boundary  
Unified 98.76 99.61 99.18 98.61
Independent 72.51 100.0 84.06 84.27
Cascaded 72.51 100.0 84.06 84.27Unnecessary Token 
Unified 98.06 95.47 96.75 96.18
Independent 27.32 87.44 41.63 96.22Case  
Restoration 
(TrueCasing) Cascaded 28.04 88.21 42.55 96.35
Independent 84.96 62.79 72.21 99.01
Cascaded 85.85 63.99 73.33 99.07
Case  
Restoration 
(CRF) Unified 86.65 67.09 75.63 99.21
Table 5. Performances of text normalization (%) 
Text Normalization Prec. Rec. F1 Acc.
Independent (TrueCasing) 69.54 91.33 78.96 97.90
Independent (CRF) 85.05 92.52 88.63 98.91
Cascaded (TrueCasing) 70.29 92.07 79.72 97.88
Cascaded (CRF) 85.06 92.70 88.72 98.92
Unified w/o Transition 
Features 86.03 93.45 89.59 99.01
Unified 86.46 93.92 90.04 99.05
Table 6. Performances of text normalization (%) 
Discussions 
Our method outperforms the independent method 
and the cascaded method in all the subtasks, espe-
cially in the subtasks that have strong dependen-
cies with each other, for example, sentence bound-
ary detection, extra punctuation mark detection, 
and case restoration. 
The cascaded method suffered from ignorance 
of the dependencies between the subtasks. For ex-
ample, there were 3,314 cases in which sentence 
boundary detection needs to use the results of extra 
line break detection, extra punctuation mark detec-
tion, and case restoration. However, in the cas-
caded method, sentence boundary detection is con-
ducted after extra punctuation mark detection and 
before case restoration, and thus it cannot leverage 
the results of case restoration. Furthermore, errors 
of extra punctuation mark detection can lead to 
errors in sentence boundary detection. 
The independent method also cannot make use 
of dependencies across different subtasks, because 
it conducts all the subtasks from the raw input data. 
This is why for detection of extra space, extra 
punctuation mark, and casing error, the independ-
ent method cannot perform as well as our method. 
Our method benefits from the ability of model-
ing dependencies between subtasks. We see from 
Table 6 that by leveraging the dependencies, our 
method can outperform the method without using 
dependencies (Unified w/o Transition Features) by 
0.62% in terms of F1-measure. 
Here we use the example in Figure 1 to show the 
advantage of our method compared with the inde-
pendent and the cascaded methods. With normali-
zation by the independent method, we obtain: 
I?m thinking about buying a pocket PC   device for my wife 
this Christmas, The worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by the cascaded method, we 
obtain: 
I?m thinking about buying a pocket PC device for my wife 
this Christmas, the worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by our method, we obtain: 
I?m thinking about buying a Pocket PC device for my wife 
this Christmas.// The worry that I have is that she won?t be 
able to sync it to her Outlook Express contacts.// 
The independent method can correctly deal with 
some of the errors. For instance, it can capitalize 
the first word in the first and the third line, remove 
extra periods in the fifth line, and remove the four 
extra line breaks. However, it mistakenly removes 
the period in the second line and it cannot restore 
the cases of some words, for example ?pocket? and 
?outlook express?. 
In the cascaded method, each process carries out 
cleaning/normalization from the output of the pre-
vious process and thus can make use of the 
cleaned/normalized results from the previous proc-
ess. However, errors in the previous processes will 
also propagate to the later processes. For example, 
the cascaded method mistakenly removes the pe-
riod in the second line. The error allows case resto-
ration to make the error of keeping the word ?the? 
in lower case. 
694
TrueCasing-based methods for case restoration 
suffer from low precision (27.32% by Independent 
and 28.04% by Cascaded), although their recalls 
are high (87.44% and 88.21% respectively). There 
are two reasons: 1) About 10% of the errors in 
Cascaded are due to errors of sentence boundary 
detection and extra line break detection in previous 
steps; 2) The two baselines tend to restore cases of 
words to the forms having higher probabilities in 
the data set and cannot take advantage of the de-
pendencies with the other normalization subtasks. 
For example, ?outlook? was restored to first letter 
capitalized in both ?Outlook Express? and ?a pleas-
ant outlook?. Our method can take advantage of the 
dependencies with other subtasks and thus correct 
85.01% of the errors that the two baseline methods 
cannot handle. Cascaded and Independent methods 
employing CRF for case restoration improve the 
accuracies somewhat. However, they are still infe-
rior to our method. 
Although we have conducted error analysis on 
the results given by our method, we omit the de-
tails here due to space limitation and will report 
them in a future expanded version of this paper. 
We also compared the speed of our method with 
those of the independent and cascaded methods. 
We tested the three methods on a computer with 
two 2.8G Dual-Core CPUs and three Gigabyte 
memory. On average, it needs about 5 hours for 
training the normalization models using our 
method and 25 seconds for tagging in the cross-
validation experiments. The independent and the 
cascaded methods (with TrueCasing) require less 
time for training (about 2 minutes and 3 minutes 
respectively) and for tagging (several seconds). 
This indicates that the efficiency of our method 
still needs improvement. 
6 Conclusion 
In this paper, we have investigated the problem of 
text normalization, an important issue for natural 
language processing. We have first defined the 
problem as a task consisting of noise elimination 
and boundary detection subtasks. We have then 
proposed a unified tagging approach to perform the 
task, specifically to treat text normalization as as-
signing tags representing deletion, preservation, or 
replacement of the tokens in the text. Experiments 
show that our approach significantly outperforms 
the two baseline methods for text normalization. 
References 
E. Brill and R. C. Moore. 2000. An Improved Error 
Model for Noisy Channel Spelling Correction, Proc. 
of ACL 2000. 
V. R. Carvalho and W. W. Cohen. 2004. Learning to 
Extract Signature and Reply Lines from Email, Proc. 
of CEAS 2004. 
K. Church and W. Gale. 1991. Probability Scoring for 
Spelling Correction, Statistics and Computing, Vol. 1. 
A. Clark. 2003. Pre-processing Very Noisy Text, Proc. 
of Workshop on Shallow Processing of Large Cor-
pora. 
A. R. Golding and D. Roth. 1996. Applying Winnow to 
Context-Sensitive Spelling Correction, Proc. of 
ICML?1996. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data, Proc. of ICML 
2001. 
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 
2003. tRuEcasIng, Proc. of ACL 2003. 
E. Mays, F. J. Damerau, and R. L. Mercer. 1991. Con-
text Based Spelling Correction, Information Process-
ing and Management, Vol. 27, 1991. 
A. Mikheev. 2000. Document Centered Approach to 
Text Normalization, Proc. SIGIR 2000. 
A. Mikheev. 2002. Periods, Capitalized Words, etc. 
Computational Linguistics, Vol. 28, 2002. 
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting Personal Names from Email: Applying 
Named Entity Recognition to Informal Text, Proc. of 
EMNLP/HLT-2005. 
D. D. Palmer and M. A. Hearst. 1997. Adaptive Multi-
lingual Sentence Boundary Disambiguation, Compu-
tational Linguistics, Vol. 23. 
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London. 
R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf, 
and C. Richards. 1999. Normalization of non-
standard words, WS?99 Final Report. 
http://www.clsp.jhu.edu/ws99/projects/normal/. 
J. Tang, H. Li, Y. Cao, and Z. Tang. 2005. Email data 
cleaning, Proc. of SIGKDD?2005. 
V. Vapnik. 1998. Statistical Learning Theory, Springer. 
W. Wong, W. Liu, and M. Bennamoun. 2007. Enhanced 
Integrated Scoring for Cleaning Dirty Texts, Proc. of 
IJCAI-2007 Workshop on Analytics for Noisy Un-
structured Text Data. 
695
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 402?411, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Locally Training the Log-Linear Model for SMT
Lemao Liu1, Hailong Cao1, Taro Watanabe2, Tiejun Zhao1, Mo Yu1, CongHui Zhu1
1School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{lmliu,hailong,tjzhao,yumo,chzhu}@mtlab.hit.edu.cn
taro.watanabe@nict.go.jp
Abstract
In statistical machine translation, minimum
error rate training (MERT) is a standard
method for tuning a single weight with regard
to a given development data. However, due to
the diversity and uneven distribution of source
sentences, there are two problems suffered by
this method. First, its performance is highly
dependent on the choice of a development set,
which may lead to an unstable performance
for testing. Second, translations become in-
consistent at the sentence level since tuning is
performed globally on a document level. In
this paper, we propose a novel local training
method to address these two problems. Un-
like a global training method, such as MERT,
in which a single weight is learned and used
for all the input sentences, we perform training
and testing in one step by learning a sentence-
wise weight for each input sentence. We pro-
pose efficient incremental training methods to
put the local training into practice. In NIST
Chinese-to-English translation tasks, our lo-
cal training method significantly outperforms
MERT with the maximal improvements up to
2.0 BLEU points, meanwhile its efficiency is
comparable to that of the global method.
1 Introduction
Och and Ney (2002) introduced the log-linear model
for statistical machine translation (SMT), in which
translation is considered as the following optimiza-
tion problem:
e?(f ;W ) = arg max
e
P(e|f ;W )
= arg max
e
exp
{
W ? h(f, e)
}
?
e? exp
{
W ? h(f, e?)
}
= arg max
e
{
W ? h(f, e)
}
, (1)
where f and e (e?) are source and target sentences,
respectively. h is a feature vector which is scaled
by a weight W . Parameter estimation is one of
the most important components in SMT, and var-
ious training methods have been proposed to tune
W . Some methods are based on likelihood (Och and
Ney, 2002; Blunsom et al2008), error rate (Och,
2003; Zhao and Chen, 2009; Pauls et al2009; Gal-
ley and Quirk, 2011), margin (Watanabe et al2007;
Chiang et al2008) and ranking (Hopkins and May,
2011), and among which minimum error rate train-
ing (MERT) (Och, 2003) is the most popular one.
All these training methods follow the same
pipeline: they train only a single weight on a given
development set, and then use it to translate all the
sentences in a test set. We call them a global train-
ing method. One of its advantages is that it allows us
to train a single weight offline and thereby it is effi-
cient. However, due to the diversity and uneven dis-
tribution of source sentences(Li et al2010), there
are some shortcomings in this pipeline.
Firstly, on the document level, the performance of
these methods is dependent on the choice of a devel-
opment set, which may potentially lead to an unsta-
ble translation performance for testing. As referred
in our experiment, the BLEU points on NIST08 are
402
 Source  Candidate Translation   
i  
i
f  j  
ij
e  h  score  
1 ? ? ?? ? 1 I am students . <2, 1> 0.5 
  2 I was students . <1,1> 0.2 
2 ?? ?? ? ? 1 week several today ? <1,2> 0.3 
  2 today several weeks . <3,2> 0.1 
 
(a) (b)
2 21 2 222,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 212,0 ( , ) ( , )h f e h f e? ?? ?1 11 1 11, 0 ( , ) ( , )h f e h f e? ?? ?
1 12 1 111,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 21( , ) ( , )h f e h f e?
1 11 1 12( , ) ( , )h f e h f e?
<-2,0>
<-1,0>
<1,0>
<2,0>
0h1h
. .* *
2 21 2 22( , ) ( , )h f e h f e?
1 12 1 11( , ) ( , )h f e h f e?
Figure 1: (a). An Example candidate space of dimensionality two. score is a evaluation metric of e. (b). The non-
linearly separable classification problem transformed from (a) via tuning as ranking (Hopkins and May, 2011). Since
score of e11 is greater than that of e12, ?1, 0? corresponds to a possitive example denoted as ???, and ??1, 0? corre-
sponds to a negative example denoted as ?*?. Since the transformed classification problem is not linearly separable,
there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile. However, one can
obtain e11 and e21 with weights: ?1, 1? and ??1, 1?, respectively.
19.04 when the Moses system is tuned on NIST02
by MERT. However, its performance is improved to
21.28 points when tuned on NIST06. The automatic
selection of a development set may partially address
the problem. However it is inefficient since tuning
requires iteratively decoding an entire development
set, which is impractical for an online service.
Secondly, translation becomes inconsistent on the
sentence level (Ma et al2011). Global training
method such as MERT tries to optimize the weight
towards the best performance for the whole set, and
it can not necessarily always obtain good translation
for every sentence in the development set. The rea-
son is that different sentences may need different
optimal weights, and MERT can not find a single
weight to satisfy all of the sentences. Figure 1(a)
shows such an example, in which a development set
contains two sentences f1 and f2 with translations e
and feature vectors h. When we tune examples in
Figure 1(a) by MERT, it can be regarded as a non-
linearly separable classification problem illustrated
in Figure 1(b). Therefore, there exists no single
weightW which simultaneously obtains e11 and e21
as translation for f1 and f2 via Equation (1). How-
ever, we can achieve this with two weights: ?1, 1?
for f1 and ??1, 1? for f2.
In this paper, inspired by KNN-SVM (Zhang et
al., 2006), we propose a local training method,
which trains sentence-wise weights instead of a sin-
gle weight, to address the above two problems.
Compared with global training methods, such as
MERT, in which training and testing are separated,
our method works in an online fashion, in which
training is performed during testing. This online
fashion has an advantage in that it can adapt the
weights for each of the test sentences, by dynam-
ically tuning the weights on translation examples
which are similar to these test sentences. Similar
to the method of development set automatical selec-
tion, the local training method may also suffer the
problem of efficiency. To put it into practice, we
propose incremental training methods which avoid
retraining and iterative decoding on a development
set.
Our local training method has two advantages:
firstly, it significantly outperforms MERT, especially
when test set is different from the development set;
secondly, it improves the translation consistency.
Experiments on NIST Chinese-to-English transla-
tion tasks show that our local training method sig-
nificantly gains over MERT, with the maximum im-
provements up to 2.0 BLEU, and its efficiency is
comparable to that of the global training method.
2 Local Training and Testing
The local training method (Bottou and Vapnik,
1992) is widely employed in computer vision
(Zhang et al2006; Cheng et al2010). Compared
with the global training method which tries to fit
a single weight on the training data, the local one
learns weights based on the local neighborhood in-
formation for each test example. It is superior to
403
the global one when the data sets are not evenly
distributed (Bottou and Vapnik, 1992; Zhang et al
2006).
Algorithm 1 Naive Local Training Method
Input: T = {ti}Ni=1(test set), K (retrieval size),
Dev(development set), D(retrieval data)
Output: Translation results of T
1: for all sentence ti such that 1 ? i ? N do
2: Retrieve the training examples Di with size
K for ti from D according to a similarity;
3: Train a local weight W i based on Dev and
Di;
4: Decode ti with W i;
5: end for
Suppose T be a test set, Dev a development set,
and D a retrieval data. The local training in SMT
is described in the Algorithm 1. For each sentence
ti in test set, training examples Di is retrieved from
D using a similarity measure (line 2), a weight W i
is optimized on Dev and Di (line 3)1, and, finally,
ti is decoded with W i for testing (line 4). At the
end of this algorithm, it returns the translation re-
sults for T . Note that weights are adapted for each
test sentence ti in line 3 by utilizing the translation
examples Di which are similar to ti. Thus, our local
training method can be considered as an adaptation
of translation weights.
Algorithm 1 suffers a problem of training effi-
ciency in line 3. It is impractical to train a weight
W i on Dev and Di from scratch for every sen-
tence, since iteratively decodingDev andDi is time
consuming when we apply MERT. To address this
problem, we propose a novel incremental approach
which is based on a two-phase training.
On the first phase, we use a global training
method, like MERT, to tune a baseline weight on
the development set Dev in an offline manner. On
the second phase, we utilize the retrieved examples
to incrementally tune sentence-wise local weights
based on the baseline weight. This method can
not only consider the common characteristics learnt
from the Dev, but also take into account the knowl-
1Usually, the quality of development set Dev is high, since
it is manually produced with multiple references. This is the
main reason why Dev is used as a part of new development set
to train W i.
edge for each individual sentence learnt from sim-
ilar examples during testing. On the phase of in-
cremental training, we perform decoding only once
for retrieved examples Di, though several rounds of
decoding are possible and potentially better if one
does not seriously care about training speed. Fur-
thermore, instead of on-the-fly decoding, we decode
the retrieval data D offline using the parameter from
our baseline weight and its nbest translation candi-
dates are saved with training examples to increase
the training efficiency.
Algorithm 2 Local Training Method Based on In-
cremental Training
Input: T = {ti}Ni=1 (test set), K (retrieval size),
Dev (development set),
D = {?fs, rs?}s=Ss=1 (retrieval data),
Output: Translation results of T
1: Run global Training (such as MERT) on Dev to
get a baseline weight Wb; // Phase 1
2: Decode each sentence in D to get
D = {?fs, cs, rs?}s=Ss=1 ;
3: for all sentence ti such that 1 ? i ? N do
4: Retrieve K training examples Di =
{?f ij , c
i
j , r
i
j?}
j=K
j=1 for ti from D according to
a similarity;
5: Incrementally train a local weight W i based
on Wb and Di; // Phase 2
6: Decode ti with W i;
7: end for
The two-phase local training algorithm is de-
scribed in Algorithm 2, where cs and rs denote the
translation candidate set and reference set for each
sentence fs in retrieval data, respectively, and K is
the retrieval size. It globally trains a baseline weight
Wb (line 1), and decodes each sentence in retrieval
data D with the weight Wb (line 2). For each sen-
tence ti in test set T , it first retrieves training exam-
ples Di from D (line 4), and then it runs local train-
ing to tune a local weight W i (line 5) and performs
testing with W i for ti (line 6). Please note that the
two-phase training contains global training in line 1
and local training in line 5.
From Algorithm 2, one can see that our method is
effective even if the test set is unknow, for example,
in the scenario of online translation services, since
the global training on development set and decoding
404
on retrieval data can be performed offline.
In the next two sections, we will discuss the de-
tails about the similarity metric in line 4 and the in-
cremental training in line 5 of Algorithm 2.
3 Acquiring Training Examples
In line 4 of Algorithm 2, to retrieve training exam-
ples for the sentence ti , we first need a metric to
retrieve similar translation examples. We assume
that the metric satisfy the property: more similar the
test sentence and translation examples are, the better
translation result one obtains when decoding the test
sentence with the weight trained on the translation
examples.
The metric we consider here is derived from
an example-based machine translation. To retrieve
translation examples for a test sentence, (Watanabe
and Sumita, 2003) defined a metric based on the
combination of edit distance and TF-IDF (Manning
and Schu?tze, 1999) as follows:
dist(f1, f2) = ? ? edit-dist(f1, f2)+
(1? ?)? tf-idf(f1, f2), (2)
where ?(0 ? ? ? 1) is an interpolation weight,
fi(i = 1, 2) is a word sequence and can be also
considered as a document. In this paper, we extract
similar examples from training data. Like example-
based translation in which similar source sentences
have similar translations, we assume that the optimal
translation weights of the similar source sentences
are closer.
4 Incremental Training Based on
Ultraconservative Update
Compared with retraining mode, incremental train-
ing can improve the training efficiency. In the field
of machine learning research, incremental training
has been employed in the work (Cauwenberghs and
Poggio, 2001; Shilton et al2005), but there is lit-
tle work for tuning parameters of statistical machine
translation. The biggest difficulty lies in that the fea-
ture vector of a given training example, i.e. transla-
tion example, is unavailable until actually decoding
the example, since the derivation is a latent variable.
In this section, we will investigate the incremental
training methods in SMT scenario.
Following the notations in Algorithm 2, Wb is
the baseline weight, Di = {?f ij , c
i
j , r
i
j?}
K
j=1 denotes
training examples for ti. For the sake of brevity, we
will drop the index i, Di = {?fj , cj , rj?}Kj=1, in the
rest of this paper. Our goal is to find an optimal
weight, denoted by W i, which is a local weight and
used for decoding the sentence ti. Unlike the global
method which performs tuning on the whole devel-
opment set Dev +Di as in Algorithm 1, W i can be
incrementally learned by optimizing onDi based on
Wb. We employ the idea of ultraconservative update
(Crammer and Singer, 2003; Crammer et al2006)
to propose two incremental methods for local train-
ing in Algorithm 2 as follows.
Ultraconservative update is an efficient way to
consider the trade-off between the progress made on
development set Dev and the progress made on Di.
It desires that the optimal weight W i is not only
close to the baseline weight Wb, but also achieves
the low loss over the retrieved examples Di. The
idea of ultraconservative update can be formalized
as follows:
min
W
{
d(W,Wb) + ? ? Loss(D
i,W )
}
, (3)
where d(W,Wb) is a distance metric over a pair
of weights W and Wb. It penalizes the weights
far away from Wb and it is L2 norm in this paper.
Loss(Di,W ) is a loss function of W defined on Di
and it evaluates the performance of W over Di. ?
is a positive hyperparameter. If Di is more similar
to the test sentence ti, the better performance will be
achieved for the larger ?. In particular, ifDi consists
of only a single sentence ti, the best performance
will be obtained when ? goes to infinity.
4.1 Margin Based Ultraconservative Update
MIRA(Crammer and Singer, 2003; Crammer et al
2006) is a form of ultraconservative update in (3)
whoseLoss is defined as hinge loss based on margin
over the pairwise translation candiates in Di. It tries
to minimize the following quadratic program:
1
2
||W ?Wb||
2+
?
K
K?
j=1
max
1?n?|cj |
(
`jn?W ??h(fj , ejn)
)
with
?h(fj , ejn) = h(fj , ej?)? h(fj , ejn), (4)
405
where h(fj , e) is the feature vector of candidate e,
ejn is a translation member of fj in cj , ej? is the
oracle one in cj , `jn is a loss between ej? and ejn
and it is the same as referred in (Chiang et al2008),
and |cj | denotes the number of members in cj .
Different from (Watanabe et al2007; Chiang
et al2008) employing the MIRA to globally train
SMT, in this paper, we apply MIRA as one of local
training method for SMT and we call it as margin
based ultraconservative update (MBUU for shortly)
to highlight its advantage of incremental training in
line 5 of Algorithm 2.
Further, there is another difference between
MBUU and MIRA in (Watanabe et al2007; Chi-
ang et al2008). MBUU is a batch update mode
which updates the weight with all training examples,
but MIRA is an online one which updates with each
example (Watanabe et al2007) or part of examples
(Chiang et al2008). Therefore, MBUU is more ul-
traconservative.
4.2 Error Rate Based Ultraconservative
Update
Instead of taking into account the margin-based
hinge loss between a pair of translations as the Loss
in (3), we directly optimize the error rate of trans-
lation candidates with respect to their references in
Di. Formally, the objective function of error rate
based ultraconservative update (EBUU) is as fol-
lows:
1
2
?W ?Wb?
2 +
?
K
K?
j=1
Error(rj ; e?(fj ;W )), (5)
where e?(fj ;W ) is defined in Equation (1), and
Error(rj , e) is the sentence-wise minus BLEU (Pa-
pineni et al2002) of a candidate e with respect to
rj .
Due to the existence of L2 norm in objective
function (5), the optimization algorithm MERT can
not be applied for this question since the exact line
search routine does not hold here. Motivated by
(Och, 2003; Smith and Eisner, 2006), we approxi-
mate the Error in (5) by the expected loss, and then
derive the following function:
1
2
?W?Wb?
2+
?
K
K?
j=1
?
e
Error(rj ; e)P?(e|fj ;W ),
(6)
Systems NIST02 NIST05 NIST06 NIST08
Moses 30.39 26.31 25.34 19.07
Moses hier 33.68 26.94 26.28 18.65
In-Hiero 31.24 27.07 26.32 19.03
Table 1: The performance comparison of the baseline In-
Hiero VS Moses and Moses hier.
with
P?(e|fj ;W ) =
exp[?W ? h(fj , e)]
?
e??cj exp[?W ? h(fj , e
?)]
, (7)
where ? > 0 is a real number valued smoother. One
can see that, in the extreme case, for ? ? ?, (6)
converges to (5).
We apply the gradient decent method to minimize
the function (6), as it is smooth with respect to ?.
Since the function (6) is non-convex, the solution
obtained by gradient descent method may depend on
the initial point. In this paper, we set the initial point
as Wb in order to achieve a desirable solution.
5 Experiments and Results
5.1 Setting
We conduct our experiments on the Chinese-to-
English translation task. The training data is FBIS
corpus consisting of about 240k sentence pairs. The
development set is NIST02 evaluation data, and the
test datasets are NIST05, NIST06,and NIST08.
We run GIZA++ (Och and Ney, 2000) on the
training corpus in both directions (Koehn et al
2003) to obtain the word alignment for each sen-
tence pair. We train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus using the SRILM Toolkits (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). In our experiments the translation per-
formances are measured by case-insensitive BLEU4
metric (Papineni et al2002) and we use mteval-
v13a.pl as the evaluation tool. The significance test-
ing is performed by paired bootstrap re-sampling
(Koehn, 2004).
We use an in-house developed hierarchical
phrase-based translation (Chiang, 2005) as our base-
line system, and we denote it as In-Hiero. To ob-
tain satisfactory baseline performance, we tune In-
Hiero system for 5 times using MERT, and then se-
406
Methods Steps Seconds
Global method Decoding 2.0
Local method Retrieval +0.6
Local training +0.3
Table 2: The efficiency of the local training and testing
measured by sentence averaged runtime.
Methods NIST05 NIST06 NIST08
Global MERT 27.07 26.32 19.03
Local MBUU 27.75+ 27.88+ 20.84+
EBUU 27.85+ 27.99+ 21.08+
Table 3: The performance comparison of local train-
ing methods (MBUU and EBUU) and a global method
(MERT). NIST05 is the set used to tune ? for MBUU and
EBUU, and NIST06 and NIST08 are test sets. + means
the local method is significantly better than MERT with
p < 0.05.
lect the best-performing one as our baseline for the
following experiments. As Table 1 indicates, our
baseline In-Hiero is comparable to the phrase-based
MT (Moses) and the hierarchical phrase-based MT
(Moses hier) implemented in Moses, an open source
MT toolkit2 (Koehn et al2007). Both of these sys-
tems are with default setting. All three systems are
trained by MERT with 100 best candidates.
To compare the local training method in Algo-
rithm 2, we use a standard global training method,
MERT, as the baseline training method. We do not
compare with Algorithm 1, in which retraining is
performed for each input sentence, since retraining
for the whole test set is impractical given that each
sentence-wise retraining may take some hours or
even days. Therefore, we just compare Algorithm
2 with MERT.
5.2 Runtime Results
To run the Algorithm 2, we tune the baseline weight
Wb on NIST02 by MERT3. The retrieval data is set
as the training data, i.e. FBIS corpus, and the re-
trieval size is 100. We translate retrieval data with
Wb to obtain their 100 best translation candidates.
We use the simple linear interpolated TF-IDF met-
ric with ? = 0.1 in Section 3 as the retrieval metric.
2See web: http://www.statmt.org
3Wb is exactly the weight of In-Hiero in Table 1.
NIST05 NIST06 NIST08
NIST02 0.665 0.571 0.506
Table 4: The similarity of development and three test
datasets.
For an efficient tuning, the retrieval process is par-
allelized as follows: the examples are assigned to 4
CPUs so that each CPU accepts a query and returns
its top-100 results, then all these top-100 results are
merged into the final top-100 retrieved examples to-
gether with their translation candidates. In our ex-
periments, we employ the two incremental training
methods, i.e. MBUU and EBUU. Both of the hyper-
parameters ? are tuned on NIST05 and set as 0.018
and 0.06 for MBUU and EBUU, respectively. In
the incremental training step, only one CPU is em-
ployed.
Table 2 depicts that testing each sentence with lo-
cal training method takes 2.9 seconds, which is com-
parable to the testing time 2.0 seconds with global
training method4. This shows that the local method
is efficient. Further, compared to the retrieval, the
local training is not the bottleneck. Actually, if we
use LSH technique (Andoni and Indyk, 2008) in re-
trieval process, the local method can be easily scaled
to a larger training data.
5.3 Results and Analysis
Table 3 shows the main results of our local train-
ing methods. The EBUU training method signifi-
cantly outperforms the MERT baseline, and the im-
provement even achieves up to 2.0 BLEU points on
NIST08. We can also see that EBUU and MBUU are
comparable on these three test sets. Both of these
two local training methods achieve significant im-
provements over the MERT baseline, which proves
the effectiveness of our local training method over
global training method.
Although both local methods MBUU and EBUU
achieved improvements on all the datasets, their
gains on NIST06 and NIST08 are significantly
higher than those achieved on NIST05 test dataset.
We conjecture that, the more different a test set and
a development set are, the more potential improvem-
4The runtime excludes the time of tuning and decoding on D
in Algorithm 2, since both of them can be performanced offline.
407
0 . 0 0 0 . 0 2 0 . 0 4 0 . 0 6 0 . 0 8 0 . 1 01 82 02 2
2 42 62 8  
 
 N I S T 0 5 N I S T 0 6 N I S T 0 8BLEU l
Figure 2: The peformance of EBUU for different ? over
all the test datasets. The horizontal axis denotes the val-
ues of ? in function (6), and the vertical one denotes the
BLEU points.
Metthods Dev NIST08
NIST02 19.03
MERT NIST05 20.06
NIST06 21.28
EBUU NIST02 21.08
Table 5: The comparison of MERT with different de-
velopment datasets and local training method based on
EBUU.
nts local training has for the sentences in this test set.
To test our hypothesis, we measured the similarity
between the development set and a test set by the
average value5 of accumulated TF-IDF scores of de-
velopment dataset and each sentence in test datasets.
Table 4 shows that NIST06 and NIST08 are more
different from NIS02 than NIST05, thus, this is po-
tentially the reason why local training is more effec-
tive on NIST06 and NIST08.
As mentioned in Section 1, the global training
methods such as MERT are highly dependent on de-
velopment sets, which can be seen in Table 5. There-
fore, the translation performance will be degraded if
one chooses a development data which is not close
5Instead of using the similarity between two documents de-
velopment and test datasets, we define the similarity as the av-
erage similarity of the development set and the sentences in test
set. The reason is that it reduces its dependency on the number
of sentences in test dataset, which may cause a bias.
Methods Number Percents
MERT 1735 42.3%
EBUU 1606 39.1%
Table 6: The statistics of sentences with 0.0 sentence-
level BLEU points over three test datasets.
to the test data. We can see that, with the help of the
local training, we still gain much even if we selected
an unsatisfactory development data.
As also mentioned in Section 1, the global meth-
ods do not care about the sentence level perfor-
mance. Table 6 depicts that there are 1735 sentences
with zero BLEU points in all the three test datasets
for MERT. Besides obtaining improvements on doc-
ument level as referred in Table 3, the local training
methods can also achieve consistent improvements
on sentence level and thus can improve the users?
experiences.
The hyperparameters ? in both MBUU (4) and
EBUU (6) has an important influence on transla-
tion performance. Figure 2 shows such influence
for EBUU on the test datasets. We can see that, the
performances on all these datasets improve as ? be-
comes closer to 0.06 from 0, and the performance
continues improving when ? passes over 0.06 on
NIST08 test set, where the performance constantly
improves up to 2.6 BLEU points over baseline. As
mentioned in Section 4, if the retrieved examples are
very similar to the test sentence, the better perfor-
mance will be achieved with the larger ?. There-
fore, it is reasonable that the performances improved
when ? increased from 0 to 0.06. Further, the turn-
ing point appearing at 0.06 proves that the ultra-
conservative update is necessary. We can also see
that the performance on NIST08 consistently im-
proves and achieves the maximum gain when ? ar-
rives at 0.1, but those on both NIST05 and NIST06
achieves the best when it arrives at 0.06. This
phenomenon can also be interpreted in Table 4 as
the lowest similarity between the development and
NIST08 datasets.
Generally, the better performance may be
achieved when more examples are retrieved. Actu-
ally, in Table 7 there seems to be little dependency
between the numbers of examples retrieved and the
translation qualities, although they are positively re-
408
Retrieval Size NIST05 NIST06 NIST08
40 27.66 27.81 20.87
70 27.77 27.93 21.08
100 27.85 27.99 21.08
Table 7: The performance comparison by varying re-
trieval size in Algorithm 2 based on EBUU.
Methods NIST05 NIST06 NIST08
MERT 27.07 26.32 19.03
EBUU 27.85 27.99 21.08
Oracle 29.46 29.35 22.09
Table 8: The performance of Oracle of 2-best results
which consist of 1-best resluts of MERT and 1-best
resluts of EBUU.
lated approximately.
Table 8 presents the performance of the oracle
translations selected from the 1-best translation re-
sults of MERT and EBUU. Clearly, there exists more
potential improvement for local training method.
6 Related Work
Several works have proposed discriminative tech-
niques to train log-linear model for SMT. (Och and
Ney, 2002; Blunsom et al2008) used maximum
likelihood estimation to learn weights for MT. (Och,
2003; Moore and Quirk, 2008; Zhao and Chen,
2009; Galley and Quirk, 2011) employed an eval-
uation metric as a loss function and directly opti-
mized it. (Watanabe et al2007; Chiang et al2008;
Hopkins and May, 2011) proposed other optimiza-
tion objectives by introducing a margin-based and
ranking-based indirect loss functions.
All the methods mentioned above train a single
weight for the whole development set, whereas our
local training method learns a weight for each sen-
tence. Further, our translation framework integrates
the training and testing into one unit, instead of treat-
ing them separately. One of the advantages is that it
can adapt the weights for each of the test sentences.
Our method resorts to some translation exam-
ples, which is similar as example-based translation
or translation memory (Watanabe and Sumita, 2003;
He et al2010; Ma et al2011). Instead of using
translation examples to construct translation rules
for enlarging the decoding space, we employed them
to discriminatively learn local weights.
Similar to (Hildebrand et al2005; Lu? et al
2007), our method also employes IR methods to re-
trieve examples for a given test set. Their methods
utilize the retrieved examples to acquire translation
model and can be seen as the adaptation of trans-
lation model. However, ours uses the retrieved ex-
amples to tune the weights and thus can be consid-
ered as the adaptation of tuning. Furthermore, since
ours does not change the translation model which
needs to run GIZA++ and it incrementally trains lo-
cal weights, our method can be applied for online
translation service.
7 Conclusion and Future Work
This paper proposes a novel local training frame-
work for SMT. It has two characteristics, which
are different from global training methods such as
MERT. First, instead of training only one weight for
document level, it trains a single weight for sentence
level. Second, instead of considering the training
and testing as two separate units, we unify the train-
ing and testing into one unit, which can employ the
information of test sentences and perform sentence-
wise local adaptation of weights.
Local training can not only alleviate the prob-
lem of the development data selection, but also re-
duce the risk of sentence-wise bad translation re-
sults, thus consistently improve the translation per-
formance. Experiments show gains up to 2.0 BLEU
points compared with a MERT baseline. With the
help of incremental training methods, the time in-
curred by local training was negligible and the local
training and testing totally took 2.9 seconds for each
sentence.
In the future work, we will further investigate the
local training method, since there are more room for
improvements as observed in our experiments. We
will test our method on other translation models and
larger training data6.
Acknowledgments
We would like to thank Hongfei Jiang and Shujie
Liu for many valuable discussions and thank three
6Intuitionally, when the corpus of translation examples is
larger, the retrieval results in Algorithm 2 are much similar as
the test sentence. Therefore our method may favor this.
409
anonymous reviewers for many valuable comments
and helpful suggestions. This work was supported
by National Natural Science Foundation of China
(61173073,61100093), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207), and the
Fundamental Research Funds for Central Univer-
sites (HIT.NSRIF.2013065).
References
Alexandr Andoni and Piotr Indyk. 2008. Near-optimal
hashing algorithms for approximate nearest neighbor
in high dimensions. Commun. ACM, 51(1):117?122,
January.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200?208, Columbus, Ohio, June. Association
for Computational Linguistics.
Le?on Bottou and Vladimir Vapnik. 1992. Local learning
algorithms. Neural Comput., 4:888?900, November.
G. Cauwenberghs and T. Poggio. 2001. Incremental
and decremental support vector machine learning. In
Advances in Neural Information Processing Systems
(NIPS*2000), volume 13.
Stanley F Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. In Technical Report TR-10-98. Harvard Univer-
sity.
Haibin Cheng, Pang-Ning Tan, and Rong Jin. 2010. Ef-
ficient algorithm for localized support vector machine.
IEEE Trans. on Knowl. and Data Eng., 22:537?549,
April.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
S. Hildebrand, M. Eck, S. Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of EAMT. Association for Computational
Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP.
ACL.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2010. Adaptive development data selection for
log-linear model in statistical machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 662?
670, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
410
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrim-
inative learning - a translation memory-inspired ap-
proach. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1239?1248, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics -
Volume 1, COLING ?08, pages 585?592, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 440?447, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 295?302, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Adam Pauls, John Denero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427, Singapore, August. Association for
Computational Linguistics.
Alistair Shilton, Marimuthu Palaniswami, Daniel Ralph,
and Ah Chung Tsoi. 2005. Incremental training of
support vector machines. IEEE Transactions on Neu-
ral Networks, 16(1):114?131.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe and Eiichiro Sumita. 2003. Example-
based decoding for statistical machine translation. In
Proc. of MT Summit IX, pages 410?417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hao Zhang, Alexander C. Berg, Michael Maire, and Ji-
tendra Malik. 2006. Svm-knn: Discriminative near-
est neighbor classification for visual category recog-
nition. In Proceedings of the 2006 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ?06, pages 2126?2136,
Washington, DC, USA. IEEE Computer Society.
Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, Compan-
ion Volume: Short Papers, NAACL-Short ?09, pages
21?24, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
411
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 524?534,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation 
Using Random Walk 
 
Xiaoning Zhu1*
Conghui Zhu1, and Tiejun Zhao1 
, Zhongjun He2, Hua Wu2, Haifeng Wang2,  
Harbin Institute of Technology, Harbin, China1 
Baidu Inc., Beijing, China2 
{xnzhu, chzhu, tjzhao}@mtlab.hit.edu.cn 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
 
 
 
                                                          
* This work was done when the first author was visiting Baidu. 
Abstract 
This paper proposes a novel approach that uti-
lizes a machine learning method to improve 
pivot-based statistical machine translation 
(SMT). For language pairs with few bilingual 
data, a possible solution in pivot-based SMT 
using another language as a "bridge" to gen-
erate source-target translation. However, one 
of the weaknesses is that some useful source-
target translations cannot be generated if the 
corresponding source phrase and target phrase 
connect to different pivot phrases. To allevi-
ate the problem, we utilize Markov random 
walks to connect possible translation phrases 
between source and target language. Experi-
mental results on European Parliament data, 
spoken language data and web data show that 
our method leads to significant improvements 
on all the tasks over the baseline system. 
1 Introduction 
Statistical machine translation (SMT) uses bilin-
gual corpora to build translation models. The 
amount and the quality of the bilingual data 
strongly affect the performance of SMT systems. 
For resource-rich language pairs, such as Chinese-
English, it is easy to collect large amounts of bi-
lingual corpus. However, for resource-poor lan-
guage pairs, such as Chinese-Spanish, it is difficult 
to build a high-performance SMT system with the 
small scale bilingual data available.  
The pivot language approach, which performs 
translation through a third language, provides a 
possible solution to the problem. The triangulation 
method (Wu and Wang, 2007; Cohn and Lapata, 
2007) is a representative work for pivot-based ma-
chine translation. With a triangulation pivot ap-
proach, a source-target phrase table can be 
obtained by combining the source-pivot phrase 
table and the pivot-target phrase table. However, 
one of the weaknesses is that some corresponding 
source and target phrase pairs cannot be generated, 
because they are connected to different pivot 
phrases (Cui et al, 2013). As illustrated in Figure 
1, since there is no direct translation between ??
?? henkekou? and ?really delicious?, the trian-
gulation method is unable to establish a relation 
between ???? henkekou? and the two Spanish 
phrases. 
To solve this problem, we apply a Markov ran-
dom walk method to pivot-based SMT system. 
Random walk has been widely used. For example, 
Brin and Page (1998) used random walk to dis-
cover potential relations between queries and doc-
uments for link analysis in information retrieval. 
Analogous to link analysis, the aim of pivot-based 
translation is to discover potential translations be-
tween source and target language via the pivot 
language.  
524
The goal of this paper is to extend the previous 
triangulation approach by exploring implicit trans-
lation relations using random walk method. We 
evaluated our approach in several translation tasks, 
including translations between European lan-
guages; Chinese-Spanish spoken language transla-
tion and Chinese-Japanese translation with English 
as the pivot language. Experimental results show 
that our approach achieves significant improve-
ments over the conventional pivot-based method, 
triangulation method. 
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work. 
We review the triangulation method for pivot-
based machine translation in section 3. Section 4 
describes the random walk models. In section 5 
and section 6, we describe the experiments and 
analyze the performance, respectively. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classified 
into 3 kinds of methods: 
Transfer Method: Within the transfer frame-
work (Utiyama and Isahara, 2007; Wang et al, 
2008; Costa-juss? et al, 2011), a source sentence 
is first translated to n pivot sentences via a source-
pivot translation system, and then each pivot sen-
tence is translated to m target sentences via a piv-
ot-target translation system. At each step (source 
to pivot and pivot to target), multiple translation 
outputs will be generated, thus a minimum Bayes-
risk system combination method is often used to 
select the optimal sentence (Gonz?lez-Rubio et al, 
2011; Duh et al, 2011). A problem with the trans-
fer method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot translation 
system will be transferred to the pivot-target trans-
lation. 
Synthetic Method: A synthetic method creates 
a synthetic source-target corpus using source-pivot 
translation model or pivot-target translation model 
(Utiyama et al, 2008; Wu and Wang, 2009). For 
example, we can translate each pivot sentence in 
the pivot-target corpus to source language with a 
pivot-source model, and then combine the translat-
ed source sentence with the target sentence to ob-
tain a synthetic source-target corpus, and vice 
versa. However, it is difficult to build a high quali-
ty translation system with a corpus created by a 
machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target model by combining 
source-pivot and pivot-target translation models 
(Wu and Wang, 2007; Cohn and Lapata 2007), 
which has been shown to work better than the oth-
er pivot approaches (Utiyama and Isahara, 2007). 
As we mentioned earlier, the weakness of triangu-
lation is that the corresponding source and target 
phrase pairs cannot be connected in the case that 
they connect to different pivot phrases. 
3 The Triangulation Method 
In this section, we review the triangulation method 
for pivot-based translation. 
With the two additional bilingual corpora, the 
source-pivot and pivot-target translation models 
can be trained. Thus, a pivot model can be ob-
tained by merging these two models. In the trans-
lation model, the phrase translation probability and 
the lexical weight are language dependent, which 
will be introduced in the next two sub-sections. 
Figure 1: An example of random walk on phrase table. The dashed line indicates an implicit relation 
in the phrase table. 
???? 
feichanghaochi 
really delicious 
very tasty 
 
???
henkekou 
realmente delicioso 
 
Chinese English Spanish 
muy delicioso 
 
525
3.1 Phrase Translation Probability 
The triangulation method assumes that there exist 
translations between phrases s  and phrase p  in 
source and pivot languages, and between phrase 
p  and phrase t  in pivot and target languages. 
The phrase translation probability ?  between 
source and target languages is determined by the 
following model: 
( | ) ( | , ) ( | )
          ( | ) ( | )
p
p
s t s p t p t
s p p t
? ? ?
? ?
=
=
?
?
       (1) 
3.2 Lexical Weight 
Given a phrase pair ( , )s t and a word alignment 
a  between the source word positions 1, ,i n= ?  
and the target word positions 0,1, ,j m= ? , the 
lexical weight of phrase pair ( , )s t  can be calcu-
lated with the following formula (Koehn et al 
2003) : 
( , )1
1
( | , ) ( | )
{ | ( , ) }
n
i j
i j ai
p s t a s t
j i j a?
?
? ?=
=
? ?? (2) 
In formula 2, the lexical translation probability 
distribution ( | )s t?  between source word s  and 
target word t  can be estimated with formula 3. 
'
'
( , )
( | )
( , )
s
count s t
s t
count s t
? =
?
            (3) 
Thus the alignment a  between the source 
phrase s  and target phrase t  via pivot phrase p  
is needed for computing the lexical weight. The 
alignment a  can be obtained as follows: 
1 2{( , ) | : ( , ) & ( , ) }a s t p s p a p t a= ? ? ?    (4) 
where 1a  and 2a  indicate the word alignment be-
tween the phrase pair ( , )s p  and ( , )p t , respec-
tively. 
The triangulation method requires that both the 
source and target phrases connect to the same piv-
ot phrase. Otherwise, the source-target phrase pair 
cannot be discovered. As a result, some useful 
translation relations will be lost. In order to allevi-
ate this problem, we propose a random walk model, 
to discover the implicit relations among the source, 
pivot and target phrases. 
4 Random Walks on Translation Graph 
For phrase-based SMT, all source-target phrase 
pairs are stored in a phrase table. In our random 
walk approach, we first build a translation graph 
according to the phrase table. A translation graph 
contains two types of nodes: source phrase and 
target phrase. A source phrase s  and a target 
phrase t  are connected if exists a phrase pair 
( , )s t  in the phrase table. The edge can be 
weighted according to translation probabilities or 
alignments in the phrase table. For the pivot-based 
translation, the translation graph can be derived 
from the source-pivot phrase table and pivot-target 
phrase table.  
Our random walk model is inspired by two 
works (Szummer and Jaakkola, 2002; Craswell 
and Szummer,2007). The general process of ran-
dom walk can be described as follows: 
Let ( , )G V E= be a directed graph with n  ver-
tices and m  edges. For a vertex v V? , ( )v?  de-
notes the set of neighbors of v  in G . A random 
walk on G  follows the following process: start at 
a vertex 0v , chose and walk along a random 
neighbor 1v , with 1 0( )v v?? . At the second step, 
start from 1v  and chose a random neighbor 2v , and 
so on. 
Let S be the set of source phrases, and P be the 
set of pivot phrases. Then the nodes V are the un-
ion of S and P. The edges E correspond to the rela-
tions between phrase pairs.  
Let R represent the binary relations between 
source phrases and pivot phrases. Then the 1-step 
translation ikR from node i to node k can be direct-
ly obtained in the phrase table. 
Define operator ?  to denote the calculation of 
relation R. Then 2-step translation ijR  from node i 
to node j can be obtained with the following for-
mula.  
ij ik kjR R R= ?                           (4) 
We use |0 ( | )tR k i  to denote a t-step translation 
relation from node i to node k. In order to calculate 
the translation relations efficiently, we use a ma-
trix A to represent the graph. A t step translation 
probability can be denoted with the following for-
mula. 
526
|0 ( | ) [ ]
t
t ikP k i A=                         (5) 
where A is a matrix whose i,k-th element is ikR . 
4.1 Framework of Random Walk Approach 
The overall framework of random walk for pivot-
based machine translation is shown in Figure 2. 
Before using random walk model, we have two 
phrase tables: source-pivot phrase table (SP phrase 
table) and pivot-target phrase table (PT phrase ta-
ble). After applying the random walk approach, we 
can achieve two extended phrase table: extended 
source-pivot phrase table (S?P? phrase table) and 
extended pivot-target phrase table (P?T? phrase 
table). The goal of pivot-based SMT is to get a 
source-target phrase table (ST phrase table) via SP 
phrase table and PT phrase table.  
Our random walk was applied on SP phrase ta-
ble or PT phrase table separately. In next 2 sub-
sections, we will explain how the phrase transla-
tion probabilities and lexical weight are obtained 
with random walk model on the phrase table. 
Figure 3 shows some possible decoding pro-
cesses of random walk based pivot approach. In 
figure 3-a, the possible source-target phrase pair 
can be obtained directly via a pivot phrase, so it 
does not need a random walk model. In figure 3-b 
and figure 3-c, one candidate source-target phrase 
pair can be obtained by random walks on source-
pivot side or pivot-target side. Figure 3-d shows 
that the possible source-target can only by ob-
tained by random walks on source-pivot side and 
pivot-target side. 
4.2 Phrase Translation Probabilities 
For the translation probabilities, the binary relation 
R is the translation probabilities in the phrase table. 
The operator ?  is multiplication. According to 
formula 5, the random walk sums up the probabili-
ties of all paths of length t between the node i and 
k. 
Figure 2: Framework of random walk based pivot translation. The ST phrase table was generated by combin-
ing SP and PT phrase table through triangulation method. The phrase table with superscript ??? means that it 
was enlarged by random walk. 
 
S?P?
Phrase Table
P?T? 
Phrase Table
 SP 
Phrase Table
PT 
Phrase Table
ST 
Phrase Table
S?T?
Phrase Table
Pivot without 
random walk
Pivot with 
random walkrandom walk
random walk
Figure 3: Some possible decoding processes of random walk based pivot approach. The ? stands for the 
source phrase (S); the ? represents the pivot phrase (P) and the ? stands for the target phrase (T). 
 
(a) Pivot without  
       random walk 
S P T 
(d) Random walk on   
     both sides 
S P T 
(b) Random walk on  
      source-pivot side 
S P T 
(c) Random walk on 
      pivot-target side 
S P T 
527
Take source-to-pivot phrase graph as an exam-
ple; denote matrix A contains s+p nodes (s source 
phrases and p pivot phrases) to represent the trans-
lation graph.  
( ) ( )ij s p s p
A g
+ ? +
? ?= ? ?                         (6) 
where ijg  is the i,j-th elements of matrix A. 
We can split the matrix A into 4 sub-matrixes: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
                      (7) 
where the sub-matrix [ ]sp ik s pA p ?=  represents the 
translation probabilities from source to pivot lan-
guage, and psA  represents the similar meaning. 
Take 3 steps walks as an example: 
Step1: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
 
Step2: 
2
0
0
sp ps s p
p s ps sp
A A
A
A A
?
?
?? ?
= ? ??? ?
 
Step3: 
3
0
0
s s sp ps sp
ps sp ps p p
A A A
A
A A A
?
?
? ?? ?
= ? ?? ?? ?
 
For the 3 steps example, each step performs a 
translation process in the form of matrix?s self-
multiplication.  
1. The first step means the translation from 
source language to pivot language. The matrix 
A is derived from the phrase table directly and 
each element in the graph indicates a transla-
tion rule in the phrase table.  
2. The second step demonstrates a procedure: S-
P-S?. With 2 steps random walks, we can find 
the synonymous phrases, and this procedure is 
analogous to paraphrasing (Bannard and 
Callison-Burch, 2005). For the example shown 
in  figure 1 as an example, the hidden relation 
between ???? henkekou? and ?????
feichanghaochi? can be found through Step 2. 
3. The third step describes the following proce-
dure: S-P-S?-P?. An extended source-pivot 
phrase table is generated by 3-step random 
walks. Compared with the initial phrase table 
in Step1, although the number of phrases is 
not increased, the relations between phrase 
pairs are increased and more translation rules 
can be obtained. Still for the example in Fig-
ure 1 , the hidden relation between ????
henkekou? and ?really delicious? can be gen-
erated in Step 3. 
4.3 Lexical Weights 
To build a translation graph, the two sets of phrase 
translation probabilities are represented in the 
phrase tables. However, the two lexical weights 
are not presented in the graph directly. To deal 
with this, we should conduct a word alignment 
random walk model to obtain a new alignment a 
after t steps. For the computation of lexical 
weights, the relation R can be expressed as the 
word alignment in the phrase table. The operator 
?  can be induced with the following formula. 
1 2{( , ) | : ( , ) & ( , ) }a x y p x z a z y a= ? ? ?         (8) 
where a1 and a2 represent the word alignment 
information inside the phrase pairs ( , )x y  and 
( , )y z respectively. An example of word 
alignment inducing is shown in Figure 4. With a 
new word alignment, the two lexical weights can 
be calculated by formula 2 and formula 3. 
Figure 4: An example of word alignment induction with 3 steps random walks 
?   ??   ?   ?   ? 
could   you   fill   out   this   form ?   ?   ??   ??   ?? 
please   fill   out   this   form 
?   ??   ?   ?   ? 
could   you   fill   out   this   form 
step 1 
step 2 
step 3 
528
5 Experiments 
5.1 Translation System and Evaluation Met-
ric 
In our experiments, the word alignment was ob-
tained by GIZA++ (Och and Ney, 2000) and the 
heuristics ?grow-diag-final? refinement rule. 
(Koehn et al, 2003). Our translation system is an 
in-house phrase-based system using a log-linear 
framework including a phrase translation model, a 
language model, a lexicalized reordering model, a 
word penalty model and a phrase penalty model, 
which is analogous to Moses (Koehn et al, 2007). 
The baseline system is the triangulation method 
based pivot approach (Wu and Wang, 2007).  
To evaluate the translation quality, we used 
BLEU (Papineni et al, 2002) as our evaluation 
metric. The statistical significance using 95% con-
fidence intervals were measured with paired boot-
strap resampling (Koehn, 2004). 
5.2 Experiments on Europarl 
5.2.1. Data sets 
We mainly test our approach on Europarl1
We perform our experiments on different trans-
lation directions and via different pivot languages. 
As a most widely used language in the world 
(Mydans, 2011), English was used as the pivot 
language for granted when carrying out experi-
ments on different translation directions. For trans-
lating Portuguese to Swedish, we also tried to 
perform our experiments via different pivot lan-
 corpus, 
which is a multi-lingual corpus including 21 Euro-
pean languages. Due to the size of the data, we 
only select 11 languages which were added to 
Europarl from 04/1996 or 01/1997, including Dan-
ish (da), German (de), Greek (el), English (en), 
Spanish (es), Finnish (fi), French (fr), Italian (it) 
Dutch (nl) Portuguese (pt) and Swedish (sv). In 
order to avoid a trilingual scenario, we split the 
training corpus into 2 parts by the year of the data: 
the data released in odd years were used for train-
ing source-pivot model and the data released in 
even years were used for training pivot-target 
model.  
                                                          
1 http://www.statmt.org/europarl/ 
guages. Table 1 and Table 2 summarized the train-
ing data. 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
da-en 974,189 en-da 953,002 
de-en 983,411 en-de 905,167 
el-en 609,315 en-el 596,331 
es-en 968,527 en-es 961,782 
fi-en 998,429 en-fi 903,689 
fr-en 989,652 en-fr 974,637 
it-en 934,448 en-it 938,573 
nl-en 982,696 en-nl 971,379 
pt-en 967,816 en-pt 960,214 
sv-en 960,631 en-sv 869,254 
 
Table1. Training data for experiments using English as 
the pivot language. For source-pivot (src-pvt; xx-en) 
model training, the data of odd years were used. Instead 
the data of even years were used for pivot-target (pvt-
src; en-xx) model training. 
 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
pt-da 941,876 da-sv 865,020 
pt-de 939,932 de-sv 814,678 
pt-el 591,429 el-sv 558,765 
pt-es 934,783 es-sv 827,964 
pt-fi 950,588 fi-sv 872,182 
pt-fr 954,637 fr-sv 860,272 
pt-it 900,185 it-sv 813,000 
pt-nl 945,997 nl-sv 864,675 
 
Table2. Training data for experiments via different piv-
ot languages. For source-pivot (src-pvt; pt-xx) model 
training, the data of odd years were used. Instead the 
data of even years were used for pivot-target (pvt-src; 
xx-sv) model training. 
 
Test Set Sentence # Reference # 
WMT06 2,000 1 
WMT07 2,000 1 
WMT08 2,000 1 
 
Table3. Statistics of test sets. 
529
 
Several test sets have been released for the 
Europarl corpus. In our experiments, we used 
WMT20062, WMT20073 and WMT20084 as our 
test data. The original test data includes 4 lan-
guages and extended versions with 11 languages 
of these test sets are available by the EuroMatrix5
5.2.2. Experiments on Different Translation 
Directions 
  
project. Table 3 shows the test sets. 
We build 180 pivot translation systems6
The baseline system was built following the tra-
ditional triangulation pivot approach. Table 4 lists 
the results on Europarl training data. Limited by 
 (including 
90 baseline systems and 90 random walk based 
systems) using 10 source/target languages and 1 
pivot language (English).  
                                                          
2 http://www.statmt.org/wmt06/shared-task/ 
3 http://www.statmt.org/wmt07/shared-task.html 
4 http://www.statmt.org/wmt08/shared-task.html 
5 http://matrix.statmt.org/test_sets/list 
6 Given N languages, a total of N*(N-1) SMT systems should 
be build to cover the translation between each language.  
the length of the paper, we only show the results 
on WMT08, the tendency of the results on 
WMT06 and WMT07 is similar to WMT08. 
Several observations can be made from the table.  
1. In all 90 language pairs, our method achieves 
general improvements over the baseline system.  
2. Among 90 language pairs, random walk 
based approach is significantly better than the 
baseline system in 75 language pairs. 
3. The improvements of our approach are not 
equal in different translation directions. The im-
provement ranges from 0.06 (it-es) to 1.21 (pt-da). 
One possible reason is that the performance is re-
lated with the source and target language. For ex-
ample, when using Finnish as the target language, 
the improvement is significant over the baseline. 
This may be caused by the great divergence be-
tween Uralic language (Finnish) and Indo-
European language (the other European language 
in Table4). From the table we can find that the 
translation between languages in different lan-
guage family is worse than that in some language 
family. But our random walk approach can im-
 TGT 
SRC 
da de el es fi fr it nl pt sv 
Baseline 
RW 
da - 
19.83 
20.15* 
20.46 
21.02* 
27.59 
28.29* 
14.76 
15.63* 
24.11 
24.71* 
20.49 
20.82* 
22.26 
22.57* 
24.38 
24.88* 
28.33 
28.87* 
Baseline 
RW 
de 
23.35 
23.69* 
- 
19.83 
20.05 
26.21 
26.70* 
12.72 
13.57* 
22.43 
22.78* 
18.82 
19.32* 
23.74 
24.11* 
23.05 
23.35* 
21.17 
21.27 
Baseline 
RW 
el 
23.24 
23.82* 
18.12 
18.49* 
- 
32.28 
32.48 
13.31 
14.08* 
27.35 
27.67* 
23.19 
23.63* 
20.80 
21.26* 
27.62 
27.86 
22.70 
23.15* 
Baseline 
RW 
es 
25.34 
26.07* 
19.67 
20.17* 
27.24 
27.52 
- 
13.93 
14.61* 
32.91 
33.16 
27.67 
27.92 
22.37 
22.85* 
34.73 
34.93 
24.83 
25.50* 
Baseline 
RW 
fi 
18.29 
18.63* 
13.20 
13.40 
14.72 
15.00* 
20.17 
20.48* 
- 
17.52 
17.84* 
14.76 
15.01 
15.50 
16.04* 
17.30 
17.68* 
16.63 
16.79 
Baseline 
RW 
fr 
25.67 
26.51* 
20.02 
20.45* 
26.58 
26.75 
37.50 
37.80* 
13.90 
14.75* 
- 
28.51 
28.71 
22.65 
23.33* 
33.81 
33.93 
24.64 
25.59* 
Baseline 
RW 
it 
22.63 
23.27* 
17.81 
18.40* 
24.24 
24.66* 
34.36 
35.42* 
13.20 
14.11* 
30.16 
30.48* 
- 
21.37 
21.81* 
30.84 
30.92* 
22.12 
22.64* 
Baseline 
RW 
nl 
22.49 
22.76 
19.86 
20.45* 
18.56 
19.10* 
24.69 
25.19* 
11.96 
12.63* 
21.48 
22.05* 
18.36 
18.67* 
- 
21.71 
22.13* 
19.83 
22.17* 
Baseline 
RW 
pt 
24.08 
25.29* 
19.11 
19.83* 
25.30 
26.20* 
36.59 
37.13* 
13.33 
14.21* 
32.47 
32.78* 
28.08 
28.44* 
21.52 
22.46* 
- 
22.90 
23.90* 
Baseline 
RW 
sv 
31.24 
31.75* 
20.26 
20.74* 
22.06 
22.59* 
29.21 
29.87* 
15.39 
16.13* 
25.63 
26.18* 
21.25 
21.81* 
22.30 
22.62* 
25.60 
26.09* 
- 
Table4. Experimental results on Europarl with different translation directions (BLEU% on WMT08). 
RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05). 
530
prove the performance of translations between dif-
ferent language families. 
5.2.3. Experiments via Different Pivot Lan-
guages 
In addition to using English as the pivot language, 
we also try some other languages as the pivot 
language. In this sub-section, experiments were 
carried out from translating Portuguese to Swedish 
via different pivot languages.  
Table 5 summarizes the BLEU% scores of dif-
ferent pivot language when translating from Por-
tuguese to Swedish. Similar to Table 4, our 
approach still achieves general improvements over 
the baseline system even if the pivot language has 
been changed. From the table we can see that for 
most of the pivot language, the random walk based 
approach gains more than 1 BLEU score over the 
baseline. But when using Finnish as the pivot lan-
guage, the improvement is only 0.02 BLEU scores 
on WMT08. This phenomenon shows that the piv-
ot language can also influence the performance of 
random walk approach. One possible reason for 
the poor performance of using Finnish as the pivot 
language is that Finnish belongs to Uralic lan-
guage family, and the other languages belong to 
Indo-European family. The divergence between 
different language families led to a poor perfor-
mance. Thus how to select a best pivot language is 
our future work. 
The problem with random walk is that it will 
lead to a larger phrase table with noises. In this 
sub-section, a pre-pruning (before random walk) 
and a post-pruning (after random walk) method 
were introduced to deal with this problem.  
We used a naive pruning method which selects 
the top N phrase pairs in the phrase table. In our 
experiments, we set N to 20. For pre-pruning, we 
prune the SP phrase table and PT phrase table be-
fore applying random walks. Post-pruning means 
that we prune the ST phrase table after random 
walks. For the baseline system, we also apply a 
pruning method before combine the SP and PT 
phrase table. We test our pruning method on pt-en-
sv translation task. Table 6 shows the results. 
With a pre- and post-pruning method, the ran-
dom walk approach is able to achieve further im-
provements. Our approach achieved BLEU scores 
of 25.11, 24.69 and 24.34 on WMT06, WMT07 
and WMT08 respectively, which is much better 
than the baseline and the random walk approach 
with pruning.  Moreover, the size of the phrase 
table is about half of the no-pruning method. 
When adopting a post-pruning method, the per-
formance of translation did not improved signifi-
cantly over the pre-pruning, but the scale of the 
phrase table dropped to 69M, which is only about 
2 times larger than the triangulation method. 
Phrase table pruning is a key work to improve 
the performance of random walk. We plan to ex-
plore more approaches for phrase table pruning. 
E.g. using significance test (Johnson et al, 2007) 
or monolingual key phrases (He et al, 2009) to 
filter the phrase table. 
 
 
Table5. Experimental results on translating from Portu-
guese to Swedish via different pivot language. 
RW=Random Walk. * indicates the results are signifi-
cantly better than the baseline (p<0.05). 
 
 
Table6. Results of Phrase Table Filtering 
 
trans 
language 
WMT 
06 
WMT 
07 
WMT 
08 
Baseline 
RW 
pt-da-sv 
23.40 
24.47* 
22.80 
24.21* 
22.49 
23.75* 
Baseline 
RW 
pt-de-sv 
22.72 
23.12* 
22.21 
23.26* 
21.76 
22.35* 
Baseline 
RW 
pt-el-sv 
22.53 
23.75* 
22.19 
23.22* 
21.37 
22.40* 
Baseline 
RW 
pt-en-sv 
23.54 
24.66* 
23.24 
24.22* 
22.90 
23.90* 
Baseline 
RW 
pt-es-sv 
23.58 
24.65* 
23.37 
24.10* 
22.80 
23.77* 
Baseline 
RW 
pt-fi-sv 
21.06 
21.17 
20.06 
20.42* 
20.26 
20.28 
Baseline 
RW 
pt-fr-sv 
23.55 
24.75* 
23.09 
24.15* 
22.89 
23.96* 
Baseline 
RW 
pt-it-sv 
23.65 
24.74* 
22.96 
24.18* 
22.79 
24.02* 
Baseline 
RW 
pt-nl-sv 
21.87 
23.06* 
21.83 
22.76* 
21.36 
22.29* 
 WMT 
06 
WMT 
07 
WMT 
08 
Phrase 
Pairs # 
Baseline 
+pruning 
23.54 
24.05
* 
23.24 
23.70
* 
22.90 
23.59
* 
46M 
32M 
RW 
+pre-pruning 
+post-pruning 
24.66 
25.11 
25.19
* 
24.22 
24.69 
24.79
* 
23.90 
24.34 
24.41
* 
215M 
109M 
69M 
531
5.3 Experiments on Spoken Language 
The European languages show various degrees of 
similarity to one another. In this sub-section, we 
consider translation from Chinese to Spanish with 
English as the pivot language. Chinese belongs to 
Sino-Tibetan Languages and English/Spanish be-
longs to Indo-European Languages, the gap be-
tween two languages is wide. 
A pivot task was included in IWSLT 2008 in 
which the participants need to translate Chinese to 
Spanish via English. A Chinese-English and an 
English-Spanish data were supplied to carry out 
the experiments. The entire training corpus was 
tokenized and lowercased. Table 7 and Table 8 
summarize the training data and test data. 
Table 9 shows the similar tendency with Table 4. 
The random walk models achieved BLEU% scores 
32.09, which achieved an absolute improvement of 
2.08 percentages points on BLEU over the base-
line.   
 
Corpus 
Sentence 
pair # 
Source 
word # 
Target 
word # 
CE 20,000 135,518 182,793 
ES 19,972 153,178 147,560 
 
Table 7: Training Data of IWSLT2008 
 
Test Set Sentence # Reference # 
IWSLT08 507 16 
 
Table8. Test Data of IWSLT2008 
 
System BLEU% phrase pairs # 
Baseline 30.01 143,790 
+pruning 30.25 108,407 
RW 31.57 2,760,439 
+pre-pruning 31.99 1,845,648 
+post-pruning 32.09* 1,514,694 
 
Table9. Results on IWSLT2008 
5.4 Experiments on Web Data 
The setting with Europarl data is quite artificial as 
the training data for directly translating between 
source and target actually exists in the original 
data sets. The IWSLT data set is too small to rep-
resent the real scenario. Thus we try our experi-
ment on a more realistic scenario: translating from 
Chinese to Japanese via English with web crawled 
data. 
All the training data were crawled on the web. 
The scale of Chinese-English and English-
Japanese is 10 million respectively. The test set 
was built in house with 1,000 sentences and 4 ref-
erences. 
 
System BLEU% phrase pairs # 
Baseline 28.76 4.5G 
+pruning 28.90 273M 
RW 29.13 46G 
+pre-pruning 29.44 11G 
+post-pruning 29.51* 3.4G 
 
Table10. Results on Web Data 
 
Table 10 lists the results on web data. From the 
table we can find that the random walk model can 
achieve an absolute improvement of 0.75 percent-
ages points on BLEU over the baseline.  
In this subsection, the training data contains 
parallel sentences with different domains. And the 
two training corpora (Chinese-English and Eng-
lish-Japanese) are typically very different. It 
means that our random walk approach is robust in 
the realistic scenario. 
6 Discussions 
The random walk approach mainly improves the 
performance of pivot translation in two aspects: 
reduces the OOVs and provides more hypothesis 
phrases for decoding.  
6.1 OOV 
Out-of-vocabulary (OOV 7
We count the OOVs when decoding with trian-
gulation model and random walk model on 
IWSLT2008 data. The statistics shows that when 
using triangulation model, there are 11% OOVs 
when using triangulation model, compared with 
9.6% when using random walk model. Less OOV 
often lead to a better result. 
) terms cause serious 
problems for machine translation systems (Zhang 
et al, 2005). The random walk model can reduce 
the OOVs. As illustrated in Figure 1, the Chinese 
phrase ????henkekou? cannot be connected to 
any Spanish phrase, thus it is a OOV term.  
                                                          
7 OOV refer to phrases here. 
532
6.2 Hypothesis Phrases 
To illustrate how the random walk method helps 
improve the performance of machine translation, 
we illustrate an example as follows: 
 
- Source: ? ? ? ?? 
              wo xiang yao zhentou 
- Baseline trans: Quiero almohada 
- Random Walk trans: Quiero una almohada 
 
For translating a Chinese sentence ??????
wo xiang yao zhentou? to Spanish, we can get two 
candidate translations. In this case, the random 
walk translation is better than the baseline system. 
The key phrase in this sentence is ??? zhentou?, 
figure 5 shows the extension process. In this case, 
the article ?a? is hidden in the source-pivot phrase 
table. The same situation often occurs in articles 
and prepositions. Random walk is able to discover 
the hidden relations (hypothesis phrases) among 
source, pivot and target phrases. 
 
 
 
 
 
 
 
 
 
 
7 Conclusion and Future Work 
In this paper, we proposed a random walk method 
to improve pivot-based statistical machine transla-
tion. The random walk method can find implicit 
relations between phrases in the source and target 
languages. Therefore, more source-target phrase 
pairs can be obtained than conventional pivot-
based method. Experimental results show that our 
method achieves significant improvements over 
the baseline on Europarl corpus, spoken language 
data and the web data.  
A critical problem in the approach is the noise 
that may bring in. In this paper, we used a simple 
filtering to reduce the noise. Although the filtering 
method is effective, other method may work better. 
In the future, we plan to explore more approaches 
for phrase table pruning. 
Acknowledgments 
We would like to thank Jianyun Nie, Muyun Yang 
and Lemao Liu for insightful discussions, and 
three anonymous reviewers for many invaluable 
comments and suggestions to improve our paper. 
This work is supported by National Natural Sci-
ence Foundation of China (61100093), and the 
Key Project of the National High Technology Re-
search and Development Program of China 
(2011AA01A207). 
References  
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, pages 
597-604 
Sergey Brin and Lawrence Page. 1998. The Anatomy of 
a Large-Scale Hypertextual Web Search Engine. In 
Proceedings of the Seventh International World 
Wide Web Conference  
Trevor Cohn and Mirella Lapata. 2007. Machine Trans-
lation by Triangulation: Make Effective Use of Mul-
ti-Parallel Corpora. In Proceedings of 45th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Rafael E. 
Banchs. 2011. Enhancing Scarce-Resource Language 
Translation through Pivot Combinations. In Proceed-
ings of the 5th International Joint Conference on 
Natural Language Processing, pages 1361-1365 
Nick Craswell and Martin Szummer. 2007. Random 
Walks on the Click Graph. In Proceedings of the 
30th annual international ACM SIGIR conference on 
Research and development in information retrieval, 
pages 239-246 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao 
and Dequan Zheng. 2013. Phrase Table Combination 
Deficiency Analyses in Pivot-based SMT. In Pro-
ceedings of 18th International Conference on Appli-
cation of Natural Language to Information Systems, 
pages 355-358. 
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime 
Tsukada and Masaaki Nagata. 2011. Generalized 
Minimum Bayes Risk System Combination. In Pro-
ceedings of the 5th International Joint Conference 
on Natural Language Processing, pages 1356?1360 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francisco 
Casacuberta. 2011. Minimum Bayes-risk System 
Figure 5: Phrase extension process. The dotted line 
indicates an implicit relation in the phrase table. 
??? 
ge zhentou 
?? 
zhentou 
pillow 
a pillow 
almohada 
una 
almohada 
533
Combination. In Proceedings of the 49th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 1268?1277 
Zhongjun He, Yao Meng, Yajuan L?, Hao Yu and Qun 
Liu. 2009. Reducing SMT Rule Table with Mono-
lingual Key Phrase. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 121-
124 
Howard Johnson, Joel Martin, George Foster, and Ro-
land Kuhn. 2007. Improving  translation quality by 
discarding most of the phrase table. In Proceedings 
of the 2007 Joint Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pages 967?975. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In HLT-NAACL: 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, pages 127-133 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 388?395. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. In Proceedings of 
MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of the 45th Annual Meeting of the 
Association for Computational Linguistics, demon-
stration session, pages 177?180. 
Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine 
translation. In Proceedings of the 18th International 
Conference on Computational Linguistics, pages 
1086?1090 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computation Linguistics, pages 311-319 
Karl Pearson. 1905. The Problem of the Random Walk. 
Nature, 27(1865):294 
Mydans, Seth. 2011. Across cultures, English is the 
word. New York Times. 
Martin Szummer and Tommi Jaakkola. 2002. Partially 
Labeled Classification with Markov Random Walks. 
In Advances in Neural Information Processing Sys-
tems, pages 945-952 
Kristina Toutanova, Christopher D. Manning and An-
drew Y. Ng. 2004. Learning Random Walk Models 
for Inducting Word Dependency Distributions. In 
Proceedings of the 21st International Conference on 
Machine Learning.  
Masao Utiyama and Hitoshi Isahara. 2007. A Compari-
son of Pivot Methods for Phrase-Based Statistical 
Machine Translation. In Proceedings of Human 
Language Technology: the Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics, pages 484-491 
Masao Utiyama, Andrew Finch, Hideo Okuma, Michael 
Paul, Hailong Cao, Hirofumi Yamamoto, Keiji Ya-
suda, and Eiichiro Sumita. 2008. The NICT/ATR 
speech Translation System for IWSLT 2008. In Pro-
ceedings of the International Workshop on Spoken 
Language Translation, pages 77-84 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu, 
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008. 
The TCH Machine Translation System for IWSLT 
2008. In Proceedings of the International Workshop 
on Spoken Language Translation, pages 124-131 
Hua Wu and Haifeng Wang. 2007. Pivot Language Ap-
proach for Phrase-Based Statistical Machine Transla-
tion. In Proceedings of 45th Annual Meeting of the 
Association for Computational Linguistics, pages 
856-863.  
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot 
Language Approach for Machine Translation. In 
Proceedings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 4th 
IJCNLP of the AFNLP, pages 154-162 
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining 
translations of OOV terms from the web through 
cross-lingual query expansion. In Proceedings of the 
27th ACM SIGIR. pages 524-525 
 
 
534
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation by Pivoting 
the Co-occurrence Count of Phrase Pairs 
 
Xiaoning Zhu1*, Zhongjun He2, Hua Wu2, Conghui Zhu1,  
Haifeng Wang2, and Tiejun Zhao1 
Harbin Institute of Technology, Harbin, China1 
{xnzhu,chzhu,tjzhao}@mtlab.hit.edu.cn 
Baidu Inc., Beijing, China2 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
                                                 
* This work was done when the first author was visiting Baidu. 
Abstract 
To overcome the scarceness of bilingual 
corpora for some language pairs in ma-
chine translation, pivot-based SMT uses 
pivot language as a "bridge" to generate 
source-target translation from source-
pivot and pivot-target translation. One of 
the key issues is to estimate the probabili-
ties for the generated phrase pairs. In this 
paper, we present a novel approach to 
calculate the translation probability by 
pivoting the co-occurrence count of 
source-pivot and pivot-target phrase pairs. 
Experimental results on Europarl data 
and web data show that our method leads 
to significant improvements over the 
baseline systems. 
1 Introduction 
Statistical Machine Translation (SMT) relies on 
large bilingual parallel data to produce high qual-
ity translation results. Unfortunately, for some 
language pairs, large bilingual corpora are not 
readily available. To alleviate the parallel data 
scarceness, a conventional solution is to intro-
duce a ?bridge? language (named pivot language) 
to connect the source and target language (de 
Gispert and Marino, 2006; Utiyama and Isahara, 
2007; Wu and Wang, 2007; Bertoldi et al., 2008; 
Paul et al., 2011; El Kholy et al., 2013; Zahabi et 
al., 2013), where there are large amounts of 
source-pivot and pivot-target parallel corpora. 
Among various pivot-based approaches, the 
triangulation method (Cohn and Lapata, 2007; 
Wu and Wang, 2007) is a representative work in 
pivot-based machine translation. The approach 
proposes to build a source-target phrase table by 
merging the source-pivot and pivot-target phrase 
table. One of the key issues in this method is to 
estimate the translation probabilities for the gen-
erated source-target phrase pairs. Conventionally, 
the probabilities are estimated by multiplying the 
posterior probabilities of source-pivot and pivot-
target phrase pairs. However, it has been shown 
that the generated probabilities are not accurate 
enough (Cui et al., 2013). One possible reason 
may lie in the non-uniformity of the probability 
space. Through Figure 1. (a), we can see that the 
probability distributions of source-pivot and piv-
ot-target language are calculated separately, and 
the source-target probability distributions are 
induced from the source-pivot and pivot-target 
probability distributions. Because of the absence 
of the pivot language (e.g., p2 is in source-pivot 
probability space but not in pivot-target one), the 
induced source-target probability distribution is 
not complete, which will result in inaccurate 
probabilities.  
To solve this problem, we propose a novel ap-
proach that utilizes the co-occurrence count of 
source-target phrase pairs to estimate phrase 
translation probabilities more precisely. Different 
from the triangulation method, which merges the 
source-pivot and pivot-target phrase pairs after 
training the translation model, we propose to 
merge the source-pivot and pivot-target phrase 
pairs immediately after the phrase extraction step, 
and estimate the co-occurrence count of the 
source-pivot-target phrase pairs. Finally, we 
compute the translation probabilities according 
to the estimated co-occurrence counts, using the 
standard training method in phrase-based SMT 
(Koehn et al., 2003). As Figure 1. (b) shows, the 
1665
source-target probability distributions are calcu-
lated in a complete probability space. Thus, it 
will be more accurate than the traditional trian-
gulation method. Figure 2. (a) and (b) show the 
difference between the triangulation method and 
our co-occurrence count method. 
Furthermore, it is common that a small stand-
ard bilingual corpus can be available between the 
source and target language. The direct translation 
model trained with the standard bilingual corpus 
exceeds in translation performance, but its weak-
ness lies in low phrase coverage. However, the 
pivot model has characteristics characters. Thus, 
it is important to combine the direct and pivot 
translation model to compensate mutually and 
further improve the translation performance. To 
deal with this problem, we propose a mixed 
model by merging the phrase pairs extracted by 
pivot-based method and the phrase pairs extract-
ed from the standard bilingual corpus. Note that, 
this is different from the conventional interpola-
tion method, which interpolates the direct and 
pivot translation model. See Figure 2. (b) and (c) 
for further illustration. 
(a) the triangulation method                         (b) the co-occurrence count method 
 
Figure 1: An example of probability space evolution in pivot translation. 
 
 
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
SP model PT model
ST pivot 
model
Phrase Extraction Phrase Extraction
Train Train
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
ST pivot 
model
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Train
PT phrase 
pairs
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
ST mixed 
pairs
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Merge
Standard 
ST corpus
ST phrase 
pairs
Phrase Extraction
Train
ST mixed 
model
Mix
        (a) the triangulation method        (b) the co-occurrence count method            (c) the mixed model 
 
Figure 2: Framework of the triangulation method, the co-occurrence count method and the mixed 
model. The shaded box in (b) denotes difference between the co-occurrence count method and the 
triangulation method. The shaded box in (c) denotes the difference between the interpolation model 
and the mixed model. 
1666
The remainder of this paper is organized as 
follows. In Section 2, we describe the related 
work. We introduce the co-occurrence count 
method in Section 3, and the mixed model in 
Section 4. In Section 5 and Section 6, we de-
scribe and analyze the experiments. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classi-
fied into 3 kinds as follows: 
Transfer Method: The transfer method 
(Utiyama and Isahara, 2007; Wang et al., 2008; 
Costa-juss? et al., 2011) connects two translation 
systems: a source-pivot MT system and a pivot-
target MT system. Given a source sentence, (1) 
the source-pivot MT system translates it into the 
pivot language, (2) and the pivot-target MT sys-
tem translates the pivot sentence into the target 
sentence. During each step (source to pivot and 
pivot to target), multiple translation outputs will 
be generated, thus a minimum Bayes-risk system 
combination method is often used to select the 
optimal sentence (Gonz?lez-Rubio et al., 2011; 
Duh et al., 2011). The problem with the transfer 
method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot transla-
tion system will be transferred to the pivot-target 
translation. 
Synthetic Method: It aims to create a synthet-
ic source-target corpus by: (1) translate the pivot 
part in source-pivot corpus into target language 
with a pivot-target model; (2) translate the pivot 
part in pivot-target corpus into source language 
with a pivot-source model; (3) combine the 
source sentences with translated target sentences 
or/and combine the target sentences with trans-
lated source sentences (Utiyama et al., 2008; Wu 
and Wang, 2009). However, it is difficult to 
build a high quality translation system with a 
corpus created by a machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target phrase table by 
merging source-pivot and pivot-target phrase 
table entries with identical pivot language 
phrases and multiplying corresponding posterior 
probabilities (Wu and Wang, 2007; Cohn and 
Lapata, 2007), which has been shown to work 
better than the other pivot approaches (Utiyama 
and Isahara, 2007). A problem of this approach is 
that the probability space of the source-target 
phrase pairs is non-uniformity due to the mis-
matching of the pivot phrase.  
3 Our Approach 
In this section, we will introduce our method for 
learning a source-target phrase translation model 
with a pivot language as a bridge. We extract the 
co-occurrence count of phrase pairs for each lan-
guage pair with a source-pivot and a pivot-target 
corpus. Then we generate the source-target 
phrase pairs with induced co-occurrence infor-
mation. Finally, we compute translation proba-
bilities using the standard phrase-based SMT 
training method. 
3.1 Phrase Translation Probabilities 
Following the standard phrase extraction method 
(Koehn et al., 2003), we can extract phrase pairs 
???, ???  and ???, ???  from the corresponding word-
aligned source-pivot and pivot-target training 
corpus, where ?? , ??  and ??  denotes the phrase in 
source, pivot and target language respectively. 
Formally, given the co-occurrence count 
????, ??? and ????, ???, we can estimate  ????, ???  by 
Equation 1: 
????, ??? ? ???????, ???, ????, ????
??
 (1) 
where ????  is a function to merge the co-
occurrences count ????, ???  and ????, ??? . We pro-
pose four calculation methods for function ????. 
Given the co-occurrence count ????, ???  and 
????, ???, we first need to induce the co-occurrence 
count ????, ?,? ??? . The ????, ?,? ???  is counted when 
the source phrase, pivot phrase and target phrase 
occurred together, thus we can infer that 
????, ?,? ???  is smaller than ????, ???  and ????, ??? . In 
this circumstance, we consider that ????, ?,? ???  is 
approximately equal to the minimum value of 
????, ??? and ????, ???, as shown in Equation 2. 
????, ??, ??? ? ?min?????, ???, ????, ????
??
 (2) 
Because the co-occurrence count of source-
target phrase pairs needs the existence of pivot 
phrase ?? , we intuitively believe that the co-
occurrence count ????, ???  is equal to the co-
occurrence count ????, ?,? ???. Under this assump-
tion, we can obtain the co-occurrence count 
????, ??? as shown in Equation 3. Furthermore, to 
testify our assumption, we also try the maximum 
value (Equation 4) to infer the co-occurrence 
count of ???, ???  phrase pair. 
1667
????, ??? ? ?min?????, ???, ????, ????
??
 (3) 
????, ??? ? ?max?????, ???, ????, ????
??
 (4) 
In addition, if source-pivot and pivot-target 
parallel corpus greatly differ in quantities, then 
the minimum function would likely just take the 
counts from the smaller corpus. To deal with the 
problem of the imbalance of the parallel corpora, 
we also try the arithmetic mean (Equation 5) and 
geometric mean (Equation 6) function to infer 
the co-occurrence count of source-target phrase 
pairs. 
????, ??? ? ??????, ??? ? ????, ????/2
??
 (5) 
????, ??? ? ??????, ??? ? ????, ???
??
 (6) 
When the co-occurrence count of source-target 
language is calculated, we can estimate the 
phrase translation probabilities with the follow-
ing Equation 7 and Equation 8. 
?????|?? ? ????, ???? ????, ?????  (7) 
????|??? ? ????, ???? ????, ?????  (8) 
3.2 Lexical Weight 
Given a phrase pair ???, ??? and a word alignment 
a between the source word positions ? ? 1,? , ? 
and the target word positions ? ? 0,? ,? , the 
lexical weight of phrase pair ???, ??? can be calcu-
lated by the following Equation 9 (Koehn et al., 
2003). 
??????|?, ?? ??
1
|??|??, ?? ? ??| ? ????|??????,????
?
???
(9) 
The lexical translation probability distribution 
???|?? between source word s and target word t 
can be estimated with Equation 10. 
???|?? ? ???, ??? ????, ????  (10)
To compute the lexical weight for a phrase 
pair ???, ??? generated by ???, ??? and ???, ???, we need 
the alignment information ?, which can be ob-
tained as Equation 11 shows. 
? ? ???, ??|??: ??, ?? ? ??&??, ?? ? ??? (11)
where ??  and ??  indicate the word alignment 
information in the phrase pair ???, ???  and ???, ??? 
respectively. 
4 Integrate with Direct Translation 
If a standard source-target bilingual corpus is 
available, we can train a direct translation model. 
Thus we can integrate the direct model and the 
pivot model to obtain further improvements. We 
propose a mixed model by merging the co-
occurrence count in direct translation and pivot 
translation. Besides, we also employ an interpo-
lated model (Wu and Wang, 2007) by merging 
the direct translation model and pivot translation 
model using a linear interpolation. 
4.1 Mixed Model 
Given ?  pivot languages, the co-occurrence 
count can be estimated using the method de-
scribed in Section 3.1. Then the co-occurrence 
count and the lexical weight of the mixed model 
can be estimated with the following Equation 12 
and 13. 
???, ?? ??????, ??
?
???
 (12)
??????|?, ?? ????
?
???
??,?????|?, ?? (13)
where ????, ??  and ??,?????|?, ??  are the co-
occurrence count and lexical weight in the direct 
translation model respectively. ????, ??  and 
??,?????|?, ?? denote the co-occurrence count and 
lexical weight in the pivot translation model. ?? 
is the interpolation coefficient, requiring 
? ?????? ? 1. 
4.2 Interpolated Model 
Following Wu and Wang (2007), the interpolated 
model can be modelled with Equation 14. 
?????|?? ? ?????????|??
?
???
 (14)
where ??????|?? is the phrase translation probabil-
ity in direct translation model; ??????|??  is the 
phrase translation probability in pivot translation 
model. The lexical weight is obtained with Equa-
tion 13. ?? is the interpolation coefficient, requir-
ing ? ?? ? 1???? . 
1668
5 Experiments on Europarl Corpus 
Our first experiment is carried out on Europarl1 
corpus, which is a multi-lingual corpus including 
21 European languages (Koehn, 2005). In our 
work, we perform translations among French (fr), 
German (de) and Spanish (es). Due to the rich-
ness of available language resources, we choose 
English (en) as the pivot language. Table 1 
summarized the statistics of training data. For the 
language model, the same monolingual data ex-
tracted from the Europarl are used. 
The word alignment is obtained by GIZA++ 
(Och and Ney, 2000) and the heuristics ?grow-
diag-final? refinement rule (Koehn et al., 2003). 
Our translation system is an in-house phrase-
based system analogous to Moses (Koehn et al., 
2007). The baseline system is the triangulation 
method (Wu and Wang, 2007), including an in-
terpolated model which linearly interpolate the 
direct and pivot translation model. 
                                                 
1 http://www.statmt.org/europarl 
We use WMT082  as our test data, which con-
tains 2000 in-domain sentences and 2051 out-of-
domain sentences with single reference. The 
translation results are evaluated by case-
insensitive BLEU-4 metric (Papineni et al., 
2002). The statistical significance tests using 
95% confidence interval are measured with 
paired bootstrap resampling (Koehn, 2004). 
5.1 Results 
We compare 4 merging methods with the base-
line system. The results are shown in Table 2 and 
Table 3. We find that the minimum method out-
performs the others, achieving significant im-
provements over the baseline on all translation 
directions. The absolute improvements range 
from 0.61 (fr-de) to 1.54 (es-fr) in BLEU% score 
on in-domain test data, and range from 0.36 (fr-
de) to 2.05 (fr-es) in BLEU% score on out-of-
domain test data. This indicates that our method 
is effective and robust in general. 
                                                 
2 http://www.statmt.org/wmt08/shared-task.html 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
de-en 1.9M 48.5M 50.9M
es-en 1.9M 54M 51.7M
fr-en 2M 58.1M 52.4M
 
Table 1: Training data of Europarl corpus 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 27.04 23.01 20.65 33.84 20.87 38.31 
Minimum 27.93* 23.94* 21.52* 35.38* 21.48* 39.62* 
Maximum 25.70 21.59 20.26 32.58 20.50 37.30 
Arithmetic mean 26.01 22.24 20.13 33.38 20.37 37.37 
Geometric mean 27.31 23.49* 21.10* 34.76* 21.15* 39.19* 
 
Table 2: Comparison of different merging methods on in-domain test set. * indicates the results are 
significantly better than the baseline (p<0.05). 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 15.34 13.52 11.47 21.99 12.19 25.00 
Minimum 15.77* 14.08* 11.99* 23.90* 12.55* 27.05* 
Maximum 13.41 11.83 10.17 20.48 10.83 22.75 
Arithmetic mean 13.96 12.10 10.57 21.07 11.30 23.70 
Geometric mean 15.09 13.30 11.52 23.32* 12.46* 26.22* 
 
Table 3: Comparison of different merging methods on out-of-domain test set. 
 
1669
The geometric mean method also achieves im-
provement, but not as significant as the minimum 
method. However, the maximum and the arith-
metic mean methods show a decrement in BLEU 
scores. This reminds us that how to choose a 
proper merging function for the co-occurrence 
count is a key problem.  In the future, we will 
explore more sophisticated method to merge co-
occurrence count. 
5.2 Analysis 
The pivot-based translation is suitable for the 
scenario that there exists large amount of source-
pivot and pivot-target bilingual corpora and only 
a little source-target bilingual data. Thus, we 
randomly select 10K, 50K, 100K, 200K, 500K, 
1M, 1.5M sentence pairs from the source-target 
bilingual corpora to simulate the lack of source-
target data. With these corpora, we train several 
direct translation models with different scales of 
bilingual data. We interpolate each direct transla-
tion model with the pivot model (both triangula-
tion method and co-occurrence count method) to 
obtain the interpolated model respectively. We 
also mix the direct model and pivot model using 
the method described in Section 4.1.  Following 
 
(a) German-English-Spanish                                        (b) German-English-French 
 
 
(c) Spanish-English-German                                        (d) Spanish-English-French 
 
 
(e) French-English-German                                         (f) French-English-Spanish 
 
Figure 3: Comparisons of pivot-based methods on different scales of source-target standard corpora. 
(direct: direct model; tri: triangulation model; co: co-occurrence count model; tri+inter: triangulation 
model interpolated with direct model ; co+inter: co-occurrence count model interpolated with direct 
model; co+mix: mixed model). X-axis represents the scale of the standard training data. 
22.5
23
23.5
24
24.5
25
25.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
26.5
27
27.5
28
28.5
29
29.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
33.5
34
34.5
35
35.5
36
36.5
37
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
37.5
38
38.5
39
39.5
40
40.5
41
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
1670
Wu and Wang (2007), we set ?? ? 0.9, ?? ? 0.1, 
?? ? 0.9  and ?? ? 0.1  empirically. The experi-
ments are carried out on 6 translation directions: 
German-Spanish, German-French, Spanish-
German, Spanish-French, French-German and 
French-Spanish. The results are shown in Figure 
3. We only list the results on in-domain test sets. 
The trend of the results on out-of domain test 
sets is similar with in-domain test sets. 
The results are explained as follows: 
(1) Comparison of Pivot Translation and Di-
rect Translation 
The pivot translation models are better than 
the direct translation models trained on a small 
source-target bilingual corpus. With the incre-
ment of source-target corpus, the direct model 
first outperforms the triangulation model and 
then outperforms the co-occurrence count model 
consecutively. 
Taking Spanish-English-French translation as 
an example, the co-occurrence count model 
achieves BLEU% scores of 35.38, which is close 
to the direct translation model trained with 200K 
source-target bilingual data. Compared with the 
co-occurrence count model, the triangulation 
model only achieves BLEU% scores of 33.84, 
which is close to the direct translation model 
trained with 50K source-target bilingual data. 
(2) Comparison of Different Interpolated 
Models 
For the pivot model trained by triangulation 
method and co-occurrence count method, we 
interpolate them with the direct translation model 
trained with different scales of bilingual data. 
Figure 3 shows the translation results of the dif-
ferent interpolated models. For all the translation 
directions, our co-occurrence count method in-
terpolated with the direct model is better than the 
triangulation model interpolated with the direct 
model.  
The two interpolated model are all better than 
the direct translation model. With the increment 
of the source-target training corpus, the gap be-
comes smaller. This indicates that the pivot mod-
el and its affiliated interpolated model are suita-
ble for language pairs with small bilingual data. 
Even if the scale of source-pivot and pivot-target 
corpora is close to the scale of source-target bi-
lingual corpora, the pivot translation model can 
help the direct translation model to improve the 
translation performance. Take Spanish-English-
French translation as an issue, when the scale of 
Spanish-French parallel data is 1.5M sentences 
pairs, which is close to the Spanish-English and 
English-French parallel data, the performance of 
co+mix model is still outperforms the direct 
translation model. 
(3) Comparison of Interpolated Model and 
Mixed Model 
When only a small source-target bilingual 
corpus is available, the mix model outperforms 
the interpolated model. With the increasing of 
source-target corpus, the mix model is close to 
the interpolated model or worse than the interpo-
lated model. This indicates that the mix model 
has a better performance when the source-target 
corpus is small which is close to the realistic sce-
nario. 
5.3 Integrate the Co-occurrence Count 
Model and Triangulation Model 
Experimental results in the previous section 
show that, our co-occurrence count models gen-
erally outperform the baseline system. In this 
section, we carry out experiments that integrates 
co-occurrence count model into the triangulation 
model. 
For French-English-German translation, we 
apply a linear interpolation method to integrate 
the co-occurrence count model into triangulation 
model following the method described in Section 
4.2.  We set ? as the interpolation coefficient of 
triangulation model and 1 ? ? as the interpola-
tion coefficient of co-occurrence count model 
respectively. The experiments take 9 values for 
interpolation coefficient, from 0.1 to 0.9. The 
results are shown in Figure 4. 
 
 
Figure 4: Results of integrating the co-
occurrence count model and the triangulation 
model. 
 
When using interpolation coefficient ranging 
from 0.2 to 0.7, the integrated models outperform 
the triangulation and the co-occurrence count 
model. However, for the other intervals, the inte-
20.4
20.6
20.8
21
21.2
21.4
21.6
21.8
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
BL
EU
%
Interpolation Coefficient 
integrated triangulation
co-occurrence
1671
grated models perform slightly lower than the 
co-occurrence count model, but still show better 
results than the triangulation model. The trend of 
the curve infers that the integrated model synthe-
sizes the contributions of co-occurrence count 
model and triangulation model. Additionally, it 
also indicates that, the choice of the interpolation 
coefficient affects the translation performances. 
6 Experiments on Web Data 
The experimental on Europarl is artificial, as the 
training data for directly translating between 
source and target language actually exists in the 
original data sets. Thus, we conducted several 
experiments on a more realistic scenario: trans-
lating Chinese (zh) to Japanese (jp) via English 
(en) with web crawled data. 
As mentioned in Section 3.1, the source-pivot 
and pivot-target parallel corpora can be imbal-
anced in quantities. If one parallel corpus was 
much larger than another, then minimum heuris-
tic function would likely just take the counts 
from the smaller corpus.  
In order to analyze this issue, we manually set 
up imbalanced corpora. For source-pivot parallel 
corpora, we randomly select 1M, 2M, 3M, 4M 
and 5M Chinese-English sentence pairs. On the 
other hand, we randomly select 1M English-
Japanese sentence pairs as pivot-target parallel 
corpora. The training data of Chinese-English 
and English-Japanese language pairs are summa-
rized in Table 4. For the Chinese-Japanese direct 
corpus, we randomly select 5K, 10K, 20K, 30K, 
40K, 50K, 60K, 70K, 80K, 90K and 100K sen-
tence pairs to simulate the lack of bilingual data. 
We built a 1K in-house test set with four refer-
ences. For Japanese language model training, we 
used the monolingual part of English-Japanese 
corpus. 
Table 5 shows the results of different co-
occurrence count merging methods. First, the 
minimum method and the geometric mean meth-
od outperform the other two merging methods 
and the baseline system with different training 
corpus. When the scale of source-pivot and piv-
ot-target corpus is roughly balanced (zh-en-jp-1), 
the minimum method achieves an absolute im-
provement of 2.06 percentages points on BLEU 
over the baseline, which is also better than the 
other merging methods. While, with the growth 
of source-pivot corpus, the gap between source-
pivot corpus and pivot-target corpus becomes 
bigger. In this circumstance, the geometric mean 
method becomes better than the minimum meth-
od. Compared to the minimum method, the geo-
metric mean method considers both the source-
pivot and the pivot-target corpus, which may 
lead to a better result in the case of imbalanced 
training corpus. 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
zh-en-1 1M 18.1M 17.7M
zh-en-2 2M 36.2M 35.5M
zh-en-3 3M 54.2M 53.2M
zh-en-4 4M 72.3M 70.9M
zh-en-5 5M 90.4M 88.6M
en-jp 1M 9.2M 11.1M
 
Table 4: Training data of web corpus 
 
System 
BLEU% 
zh-en-jp-1* zh-en-jp-2 zh-en-jp-3 zh-en-jp-4 zh-en-jp-5
Baseline 29.07 29.39 29.44 29.67 29.80 
Minimum 31.13* 31.28* 31.43* 31.62* 32.02* 
Maximum 28.88 29.01 29.12 29.37 29.59 
Arithmetic mean 29.08 29.36 29.51 29.79 30.01 
Geometric mean 30.77* 31.30* 31.75* 32.07* 32.34* 
 
Table 5: Comparison of different merging methods on the imbalanced web data. ( zh-en-jp-1 means 
the translation system is trained with zh-en-1 as source-pivot corpus and en-jp as pivot-target corpus, 
and so on. ) 
1672
Furthermore, with the imbalanced corpus zh-
en-jp-5, we compared the translation perfor-
mance of our co-occurrence count model (with 
geometric mean merging method), triangulation 
model, interpolated model, mixed model and the 
direct translation models. Figure 5 summarized 
the results. 
The co-occurrence count model can achieve an 
absolute improvement of 2.54 percentages points 
on BLEU over the baseline. The triangulation 
method outperforms the direct translation when 
only 5K sentence pairs are available. Meanwhile, 
the number is 10K when using the co-occurrence 
count method. The co-occurrence count models 
interpolated with the direct model significantly 
outperform the other models. 
 
 
Figure 5: Results on Chinese-Japanese Web Data. 
X-axis represents the scale of the standard train-
ing data. 
 
In this experiment, the training data contains 
parallel sentences on various domains. And the 
training corpora (Chinese-English and English-
Japanese) are typically very different, since they 
are obtained on the web. It indicates that our co-
occurrence count method is robust in the realistic 
scenario. 
7 Conclusion 
This paper proposed a novel approach for pivot-
based SMT by pivoting the co-occurrence count 
of phrase pairs. Different from the triangulation 
method merging the source-pivot and pivot-
target language after training the translation 
model, our method merges the source-pivot and 
pivot-target language after extracting the phrase 
pairs, thus the computing for phrase translation 
probabilities is under the uniform probability 
space. The experimental results on Europarl data 
and web data show significant improvements 
over the baseline systems. We also proposed a 
mixed model to combine the direct translation 
and pivot translation, and the experimental re-
sults show that the mixed model has a better per-
formance when the source-target corpus is small 
which is close to the realistic scenario. 
A key problem in the approach is how to learn 
the co-occurrence count. In this paper, we use the 
minimum function on balanced corpora and the 
geometric mean function on imbalanced corpora 
to estimate the co-occurrence count intuitively. 
In the future, we plan to explore more effective 
approaches. 
Acknowledgments 
We would like to thank Yiming Cui for insight-
ful discussions, and three anonymous reviewers 
for many invaluable comments and suggestions 
to improve our paper. This work is supported by 
National Natural Science Foundation of China 
(61100093), and the State Key Development 
Program for Basic Research of China (973 Pro-
gram, 2014CB340505). 
Reference 
Nicola Bertoldi, Madalina Barbaiani, Marcello 
Federico, and Roldano Cattoni. 2008. Phrase-
Based statistical machine translation with Piv-
ot Languages. In Proceedings of the 5th Inter-
national Workshop on Spoken Language 
Translation (IWSLT), pages 143-149. 
Trevor Cohn and Mirella Lapata. 2007. Machine 
Translation by Triangulation: Make Effective 
Use of Multi-Parallel Corpora. In Proceedings 
of 45th Annual Meeting of the Association for 
Computational Linguistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Ra-
fael E. Banchs. 2011. Enhancing Scarce-
Resource Language Translation through Pivot 
Combinations. In Proceedings of the 5th In-
ternational Joint Conference on Natural Lan-
guage Processing, pages 1361-1365. 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun 
Zhao and Dequan Zheng. 2013. Phrase Table 
Combination Deficiency Analyses in Pivot-
based SMT. In Proceedings of 18th Interna-
tional Conference on Application of Natural 
Language to Information Systems, pages 355-
358. 
Adria de Gispert and Jose B. Marino. 2006. 
Catalan-English statistical machine translation 
without parallel corpus: bridging through 
Spanish. In Proceedings of 5th International 
Conference on Language Resources and Eval-
uation (LREC), pages 65-68. 
29
31
33
35
37
39
5K 20K 40K 60K 80K 100K
BL
EU
%
direct
tri
co-occur
tri+inter
co+inter
co+mix
1673
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, 
Hajime Tsukada and Masaaki Nagata. 2011. 
Generalized Minimum Bayes Risk System 
Combination. In Proceedings of the 5th Inter-
national Joint Conference on Natural Lan-
guage Processing, pages 1356-1360. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. 
Language Independent Connectivity Strength 
Features for Phrase Pivot Statistical Machine 
Translation. In Proceedings of the 51st Annual 
Meeting of the Association for Computational 
Linguistics, pages 412-418. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. Se-
lective Combination of Pivot and Direct Sta-
tistical Machine Translation Models. In Pro-
ceedings of the 6th International Joint Confer-
ence on Natural Language Processing, pages 
1174-1180. 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francis-
co Casacuberta. 2011. Minimum Bayes-risk 
System Combination. In Proceedings of the 
49th Annual Meeting of the Association for 
Computational Linguistics, pages 1268-1277. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
HLT-NAACL: Human Language Technology 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 127-133. 
Philipp Koehn. 2004. Statistical significance 
tests for machine translation evaluation. In 
Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Pro-
cessing (EMNLP), pages 388-395. 
Philipp Koehn. 2005. Europarl: A Parallel Cor-
pus for Statistical Machine Translation. In 
Proceedings of MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ondrej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In Proceed-
ings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, demon-
stration session, pages 177-180. 
Philipp Koehn, Alexandra Birch, and Ralf Stein-
berger. 2009. 462 Machine Translation Sys-
tems for Europe. In Proceedings of the MT 
Summit XII. 
Gregor Leusch, Aur?lien Max, Josep Maria 
Crego and Hermann Ney. 2010. Multi-Pivot 
Translation by System Combination. In Pro-
ceedings of the 7th International Workshop on 
Spoken Language Translation, pages 299-306. 
Franz Josef Och and Hermann Ney. 2000. A 
comparison of alignment models for statistical 
machine translation. In Proceedings of the 
18th International Conference on Computa-
tional Linguistics, pages 1086-1090. 
Michael Paul, Andrew Finch, Paul R. Dixon and 
Eiichiro Sumita. 2011. Dialect Translation: In-
tegrating Bayesian Co-segmentation Models 
with Pivot-based SMT. In Proceedings of the 
2011 Conference on Empirical Methods in 
Natural Language Processing, pages 1-9. 
Michael Paul and Eiichiro Sumita. 2011. Trans-
lation Quality Indicators for Pivot-based Sta-
tistical MT. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language 
Processing, pages 811-818. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computation Linguistics, pag-
es 311-319. 
Rie Tanaka, Yohei Murakami and Toru Ishida. 
2009. Context-Based Approach for Pivot 
Translation Services. In the Twenty-first In-
ternational Conference on Artificial Intelli-
gence, pages 1555-1561. 
J?rg Tiedemann. 2012. Character-Based Pivot 
Translation for Under-Resourced Languages 
and Domains. In Proceedings of the 13th Con-
ference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 
141-151. 
Masatoshi Tsuchiya, Ayu Purwarianti, Toshiyu-
kiWakita and Seiichi Nakagawa. 2007. Ex-
panding Indonesian-Japanese Small Transla-
tion Dictionary Using a Pivot Language. In 
Proceedings of the ACL 2007 Demo and Post-
er Sessions, pages 197-200. 
Takashi Tsunakawa, Naoaki Okazaki and 
Jun'ichi Tsujii. 2010. Building a Bilingual 
Lexicon Using Phrase-based Statistical Ma-
chine Translation via a Pivot Language. In 
1674
Proceedings of the 22th International Confer-
ence on Computational Linguistics (Coling), 
pages 127-130. 
Masao Utiyama and Hitoshi Isahara. 2007. A 
Comparison of Pivot Methods for Phrase-
Based Statistical Machine Translation. In Pro-
ceedings of Human Language Technology: the 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 484-491. 
Masao Utiyama, Andrew Finch, Hideo Okuma, 
Michael Paul, Hailong Cao, Hirofumi Yama-
moto, Keiji Yasuda,and Eiichiro Sumita. 2008. 
The NICT/ATR speech Translation System for 
IWSLT 2008. In Proceedings of the Interna-
tional Workshop on Spoken Language Trans-
lation, pages 77-84. 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi 
Liu, Jianfeng Li, Dengjun Ren, and Zhengyu 
Niu. 2008. The TCH Machine Translation 
System for IWSLT 2008. In Proceedings of 
the International Workshop on Spoken Lan-
guage Translation, pages 124-131. 
Hua Wu and Haifeng Wang. 2007. Pivot Lan-
guage Approach for Phrase-Based Statistical 
Machine Translation. In Proceedings of 45th 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 856-863. 
Hua Wu and Haifeng Wang. 2009. Revisiting 
Pivot Language Approach for Machine Trans-
lation. In Proceedings of the 47th Annual 
Meeting of the Association for Computational 
Linguistics and the 4th IJCNLP of the AFNLP, 
pages 154-162. 
Samira Tofighi Zahabi, Somayeh Bakhshaei and 
Shahram Khadivi. Using Context Vectors in 
Improving a Machine Translation System with 
Bridge Language. In Proceedings of the 51st 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 318-322. 
 
 
1675
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802?810,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Hierarchical Phrase Table Combination for Machine Translation
Conghui Zhu1 Taro Watanabe2 Eiichiro Sumita2 Tiejun Zhao1
1School of Computer Science and Technology
Harbin Institute of Technology (HIT), Harbin, China
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{chzhu,tjzhao}@mtlab.hit.edu.cn
{taro.watanabe,Sumita}@nict.go.jp
Abstract
Typical statistical machine translation sys-
tems are batch trained with a given train-
ing data and their performances are large-
ly influenced by the amount of data. With
the growth of the available data across
different domains, it is computationally
demanding to perform batch training ev-
ery time when new data comes. In face
of the problem, we propose an efficient
phrase table combination method. In par-
ticular, we train a Bayesian phrasal inver-
sion transduction grammars for each do-
main separately. The learned phrase ta-
bles are hierarchically combined as if they
are drawn from a hierarchical Pitman-Yor
process. The performance measured by
BLEU is at least as comparable to the tra-
ditional batch training method. Further-
more, each phrase table is trained sepa-
rately in each domain, and while compu-
tational overhead is significantly reduced
by training them in parallel.
1 Introduction
Statistical machine translation (SMT) system-
s usually achieve ?crowd-sourced? improvements
with batch training. Phrase pair extraction, the
key step to discover translation knowledge, heav-
ily relies on the scale of training data. Typi-
cally, the more parallel corpora used, the more
phrase pairs and more accurate parameters will
be learned, which can obviously be beneficial to
improving translation performances. Today, more
parallel sentences are drawn from divergent do-
mains, and the size keeps growing. Consequent-
ly, how to effectively use those data and improve
translation performance becomes a challenging is-
sue.
This joint work was done while the first author visited
NICT.
Batch retraining is not acceptable for this case,
since it demands serious computational overhead
when training on a large data set, and it requires
us to re-train every time new training data is avail-
able. Even if we can handle the large computation
cost, improvement is not guaranteed every time we
perform batch tuning on the newly updated train-
ing data obtained from divergent domains. Tradi-
tional domain adaption methods for SMT are also
not adequate in this scenario. Most of them have
been proposed in order to make translation sys-
tems perform better for resource-scarce domain-
s when most training data comes from resource-
rich domains, and ignore performance on a more
generic domain without domain bias (Wang et al,
2012). As an alternative, incremental learning
may resolve the gap by incrementally adding da-
ta sentence-by-sentence into the training data. S-
ince SMT systems trend to employ very large scale
training data for translation knowledge extraction,
updating several sentence pairs each time will be
annihilated in the existing corpus.
This paper proposes a new phrase table combi-
nation method. First, phrase pairs are extracted
from each domain without interfering with oth-
er domains. In particular, we employ the non-
parametric Bayesian phrasal inversion transduc-
tion grammar (ITG) of Neubig et al (2011) to per-
form phrase table extraction. Second, extracted
phrase tables are combined as if they are drawn
from a hierarchical Pitman-Yor process, in which
the phrase tables represented as tables in the Chi-
nese restaurant process (CRP) are hierarchically
chained by treating each of the previously learned
phrase tables as prior to the current one. Thus, we
can easily update the chain of phrase tables by ap-
pending the newly extracted phrase table and by
treating the chain of the previous ones as its prior.
Experiment results indicate that our method can
achieve better translation performance when there
exists a large divergence in domains, and can
802
achieve at least comparable results to batch train-
ing methods, with a significantly less computa-
tional overhead.
The rest of the paper is organized as follows.
In Section 2, we introduce related work. In sec-
tion 3, we briefly describe the translation mod-
el with phrasal ITGs and Pitman-Yor process. In
section 4, we explain our hierarchical combination
approach and give experiment results in section 5.
We conclude the paper in the last section.
2 Related Work
Bilingual phrases are cornerstones for phrase-
based SMT systems (Och and Ney, 2004; Koehn
et al, 2003; Chiang, 2005) and existing translation
systems often get ?crowd-sourced? improvements
(Levenberg et al, 2010). A number of approaches
have been proposed to make use of the full poten-
tial of the available parallel sentences from vari-
ous domains, such as domain adaptation and in-
cremental learning for SMT.
The translation model and language model
are primary components in SMT. Previous work
proved successful in the use of large-scale data for
language models from diverse domains (Brants et
al., 2007; Schwenk and Koehn, 2008). Alterna-
tively, the language model is incrementally up-
dated by using a succinct data structure with a
interpolation technique (Levenberg and Osborne,
2009; Levenberg et al, 2011).
In the case of the previous work on translation
modeling, mixed methods have been investigat-
ed for domain adaptation in SMT by adding do-
main information as additional labels to the orig-
inal phrase table (Foster and Kuhn, 2007). Un-
der this framework, the training data is first di-
vided into several parts, and phase pairs are ex-
tracted with some sub-domain features. Then al-
l the phrase pairs and features are tuned together
with different weights during decoding. As a way
to choose the right domain for the domain adap-
tion, a classifier-based method and a feature-based
method have been proposed. Classification-based
methods must at least add an explicit label to indi-
cate which domain the current phrase pair comes
from. This is traditionally done with an automat-
ic domain classifier, and each input sentence is
classified into its corresponding domain (Xu et al,
2007). As an alternative to the classification-based
approach, Wang et al (2012) employed a feature-
based approach, in which phrase pairs are enriched
by a feature set to potentially reflect the domain in-
formation. The similarity calculated by a informa-
tion retrieval system between the training subset
and the test set is used as a feature for each paral-
lel sentence (Lu et al, 2007). Monolingual topic
information is taken as a new feature for a domain
adaptive translation model and tuned on the devel-
opment set (Su et al, 2012). Regardless of under-
lying methods, either classifier-based or feature-
based method, the performance of current domain
adaptive phrase extraction methods is more sensi-
tive to the development set selection. Usually the
domain similar to a given development data is usu-
ally assigned higher weights.
Incremental learning in which new parallel sen-
tences are incrementally updated to the training
data is employed for SMT. Compared to tradi-
tional frequent batch oriented methods, an online
EM algorithm and active learning are applied to
phrase pair extraction and achieves almost compa-
rable translation performance with less computa-
tional overhead (Levenberg et al, 2010; Gonza?lez-
Rubio et al, 2011). However, their methods usu-
ally require numbers of hyperparameters, such as
mini-batch size, step size, or human judgment to
determine the quality of phrases, and still rely on a
heuristic phrase extraction method in each phrase
table update.
3 Phrase Pair Extraction with
Unsupervised Phrasal ITGs
Recently, phrase alignment with ITGs (Cherry
and Lin, 2007; Zhang et al, 2008; Blunsom et
al., 2008) and parameter estimation with Gibb-
s sampling (DeNero and Klein, 2008; Blunsom
and Cohn, 2010) are popular. Here, we em-
ploy a method proposed by Neubig et al (2011),
which uses parametric Bayesian inference with the
phrasal ITGs (Wu, 1997). It can achieve com-
parable translation accuracy with a much small-
er phrase table than the traditional GIZA++ and
heuristic phrase extraction methods. It has al-
so been proved successful in adjusting the phrase
length granularity by applying character-based
SMT with more sophisticated inference (Neubig
et al, 2012).
ITG is a synchronous grammar formalism
which analyzes bilingual text by introducing in-
verted rules, and each ITG derivation corresponds
to the alignment of a sentence pair (Wu, 1997).
Translation probabilities of ITG phrasal align-
803
ments can be estimated in polynomial time by s-
lightly limiting word reordering (DeNero and K-
lein, 2008).
More formally, P (?e, f?; ?x, ?t
) are the proba-
bility of phrase pairs ?e, f?, which is parameter-
ized by a phrase pair distribution ?t and a symbol
distribution ?x. ?x is a Dirichlet prior, and ?t is es-
timated with the Pitman-Yor process (Pitman and
Yor, 1997; Teh, 2006), which is expressed as
?t ? PY
(
d, s, Pdac
) (1)
where d is the discount parameter, s is the strength
parameter, and , and Pdac is a prior probability
which acts as a fallback probability when a phrase
pair is not in the model.
Under this model, the probability for a phrase
pair found in a bilingual corpus ?E,F ? can be rep-
resented by the following equation using the Chi-
nese restaurant process (Teh, 2006):
P
(
?ei, fi?; ?E,F ?
)
= 1C + s(ci ? d? ti)+
1
C + s(s+ d? T )? Pdac(?ei, fi?) (2)
where
1. ci and ti are the customer and table count of
the ith phrase pair ?ei, fi? found in a bilingual
corpus ?E,F ?;
2. C and T are the total customer and table count
in corpus ?E,F ?;
3. d and s are the discount and strengthen hyper-
parameters.
The prior probability Pdac is recursively defined
by breaking a longer phrase pair into two through
the recursive ITG?s generative story as follows
(Neubig et al, 2011):
1. Generate symbol x from Px(x; ?x) with three
possible values: Base, REG, or INV .
2. Depending on the value of x take the following
actions.
a. If x = Base, generate a new phrase pair
directly from Pbase.
b. If x = REG, generate ?e1, f1? and
?e2, f2? from P
(
?e, f?; ?x, ?t
), and con-
catenate them into a single phrase pair
?e1e2, f1f2?.
Figure 1: A word alignment (a), and its hierarchi-
cal derivation (b).
c. If x = INV , follow a similar process as b,
but concatenate f1 and f2 in reverse order
?e1e2, f2f1?.
Note that the Pdac is recursively defined through
the binary branched P , which in turns employs
Pdac as a prior probability. Pbase is a base measure
defined as a combination of the IBM Models in t-
wo directions and the unigram language models in
both sides. Inference is carried out by a heuristic
beam search based block sampling with an effi-
cient look ahead for a faster convergence (Neubig
et al, 2012).
Compared to GIZA++ with heuristic phrase ex-
traction, the Bayesian phrasal ITG can achieve
competitive accuracy under a smaller phrase ta-
ble size. Further, the fallback model can incor-
porate phrases of all granularity by following the
ITG?s recursive definition. Figure 1 (b) illustrates
an example of the phrasal ITG derivation for word
alignment in Figure 1 (a) in which a bilingual sen-
tence pair is recursively divided into two through
the recursively defined generative story.
4 Hierarchical Phrase Table
Combination
We propose a new phrase table combination
method, in which individually learned phrase ta-
ble are hierarchically chained through a hierarchi-
cal Pitman-Yor process.
Firstly, we assume that the whole train-
ing data ?E,F ? can be split into J domains,
{?E1, F 1?, . . . , ?EJ , F J?}. Then phrase pairs are
804
Figure 2: A hierarchical phrase table combination (a), and a basic unit of a Chinese restaurant process
with K tables and N customers.
extracted from each domain j (1 ? j ? J) sepa-
rately with the method introduced in Section 3. In
traditional domain adaptation approaches, phrase
pairs are extracted together with their probabili-
ties and/or frequencies so that the extracted phrase
pairs are merged uniformly or after scaling.
In this work, we extract the table counts for each
phrase pair under the Chinese restaurant process
given in Section 3. In Figure 2 (b), a CRP is illus-
trated which has K tables and N customers with
each chair representing a customer. Meanwhile
there are two parameters, discount and strength for
each domain similar to the ones in Equation (1).
Our proposed hierarchical phrase table combi-
nation can be formally expressed as following:
?1 ? PY (d1, s1, P 2)
? ? ? ? ? ?
?j ? PY (dj , sj , P j+1)
? ? ? ? ? ?
?J ? PY
(
dJ , sJ , P Jbase
) (3)
Here the (j + 1)th layer hierarchical Pitman-Yor
process is employed as a base measure for the
jth layer hierarchical Pitman-Yor process. The
hierarchical chain is terminated by the base mea-
sure from the J th domain P Jbase. The hierarchi-
cal structure is illustrated in Figure 2 (a) in which
the solid lines implies a fall back using the ta-
ble counts from the subsequent domains, and the
dotted lines means the final fallback to the base
measure P Jbase. When we query a probability of
a phrase pair ?e, f?, we first query the probabil-
ity of the first layer P 1(?e, f?). If ?e, f? is not
in the model, we will fallback to the next level of
P 2(?e, f?). This process continues until we reach
the Jth base measure of P J(?e, f?). Each fallback
can be viewed as a translation knowledge integra-
tion process between subsequent domains.
For example in Figure 2 (a), the ith phrase pair
?ei, fi? appears only in the domain 1 and domain
2, so its translation probability can be calculated
by substituting Equation (3) with Equation (2):
P
(
?ei, fi?; ?E,F ?
)
= 1C1 + s1 (c
1
i ? d1 ? t1i )
+ s
1 + d1 ? T 1
(C1 + s1)? (C2 + s2)(c
2
i ? d2 ? t2i )
+
J?
j=1
(sj + dj ? T j
Cj + sj
)
? P Jbase(?ei, fi?) (4)
where the superscript indicates the domain for the
corresponding counts, i.e. cji for the customer
count in the jth domain. The first term in Equa-
tion (4) is the phrase probability from the first do-
main, and the second one comes from the second
domain, but weighted by the fallback weight of the
1st domain. Since ?ei, fi? does not appear in the
rest of the layers, the last term is taken from al-
l the fallback weight from the second layer to the
J th layer with the final P Jbase. All the parameter-
s ?j and hyperparameters dj and sj , are obtained
by learning on the jth domain. Returning the hy-
perparameters again when cascading another do-
main may improve the performance of the combi-
nation weight, but we will leave it for future work.
The hierarchical process can be viewed as an in-
stance of adapted integration of translation knowl-
edge from each sub-domain.
805
Algorithm 1 Translation Probabilities Estima-
tion
Input: cji , tji , P jbase, Cj , T j , dj and sjOutput: The translation probabilities for each
pair
1: for all phrase pair ?ei, fi? do
2: Initialize the P (?ei, fi?) = 0 and wi = 1
3: for all domain ?Ej , Fj? such that 1 6 j 6
J ? 1 do
4: if ?ei, fi? ? ?Ej , Fj? then
5: P (?ei, fi?) += wi ? (Cji ? dj ?
tji )/(Cj + sj)
6: end if
7: wi = wi ? (sj + dj ? T j)/(Cj + sj)
8: end for
9: P (?ei, fi?) += wi? (CJi ?dJ ? tJi + (sJ +
dJ ? T J)? P Jbase(?ei, fi?))/(CJ + sJ)
10: end for
Our approach has several advantages. First,
each phrase pair extraction can concentrate on a s-
mall portion of domain-specific data without inter-
fering with other domains. Since no tuning stage
is involved in the hierarchical combination, we can
easily include a new phrase table from a new do-
main by simply chaining them together. Second,
phrase pair phrase extraction in each domain is
completely independent, so it is easy to parallelize
in a situation where the training data is too large
to fit into a small amount of memory. Finally, new
domains can be integrated incrementally. When
we encounter a new domain, and if a phrase pair is
completely new in terms of the model, the phrase
pair is simply appended to the current model, and
computed without the fallback probabilities, since
otherwise, the phrase pair would be boosted by the
fallback probabilities. Pitman-Yor process is also
employed in n-gram language models which are
hierarchically represented through the hierarchi-
cal Pitman-Yor process with switch priors to in-
tegrate different domains in all the levels (Wood
and Teh, 2009). Our work incrementally combines
the models from different domains by directly em-
ploying the hierarchical process through the base
measures.
5 Experiment
We evaluate the proposed approach on the
Chinese-to-English translation task with three data
sets with different scales.
Data set Corpus #sent. pairs
IWSLT HIT 52, 603
BTEC 19, 975
Domain 1 47, 993
Domain 2 30, 272
FBIS Domain 3 49, 509
Domain 4 38, 228
Domain 5 55, 913
News 221, 915
News 95, 593
LDC Magazine 98, 335
Magazine 254, 488
Finance 86, 112
Table 1: The sentence pairs used in each data set.
5.1 Experiment Setup
The first data set comes from the IWSLT2012
OLYMPICS task consisting of two training sets:
the HIT corpus, which is closely related to the Bei-
jing 2008 Olympic Games, and the BTEC corpus,
which is a multilingual speech corpus containing
tourism-related sentences. The second data set,
the FBIS corpus, is a collection of news articles
and does not have domain information itself, so a
Latent Dirichlet Allocation (LDA) tool, PLDA1,
is used to divide the whole corpus into 5 different
sub-domains according to the concatenation of the
source side and target side as a single sentence (Li-
u et al, 2011). The third data set is composed of 5
corpora2 from LDC with various domains, includ-
ing news, magazine, and finance. The details are
shown in Table 1.
In order to evaluate our approach, four phrase
pair extraction methods are performed:
1. GIZA-linear: Phase pairs are extracted in each
domain by GIZA++ (Och and Ney, 2003) and
the ?grow-diag-final-and? method with a max-
imum length 7. The phrase tables from vari-
ous domains are linearly combined by averag-
ing the feature values.
2. Pialign-linear: Similar to GIZA-linear, but we
employed the phrasal ITG method described in
Section 3 using the pialign toolkit 3 (Neubig et
1http://code.google.com/p/plda/
2In particular, they come from LDC catalog number:
LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47,
LDC2006E26, in this order.
3http://www.phontron.com/pialign/
806
Methods IWSLT FBIS LDCBLEU Size BLEU Size BLEU Size
GIZA-linear 19.222 1,200,877 29.342 15,369,028 30.67 77,927,347
Pialign-linear 19.534 876,059 29.858 7,235,342 31.12 28,877,149
GIZA-batch 19.616 1,185,255 31.38 13,737,258 32.06 63,606,056
Pialign-batch 19.506 841,931 31.104 6,459,200
Pialign-adaptive 19.624 841,931 30.926 6,459,200
Hier-combin 20.32 876,059 31.29 7,235,342 32.03 28,877,149
Table 2: BLEU scores and phrase table size by alignment method and probabilities estimation method.
Pialign was run with five samples. Because of computational overhead, the baseline Pialign-batch and
Pialign-adaptive were not run on the largest data set.
al., 2011). Extracted phrase pairs are linearly
combined by averaging the feature values.
3. GIZA-batch: Instead of splitting into each do-
main, the data set is merged as a single corpus
and then a heuristic GZA-based phrase extrac-
tion is performed, similar as GIZA-linear.
4. Pialign-batch: Similar to the GIZA-batch, a s-
ingle model is estimated from a single, merged
corpus. Since pialign cannot handle large data,
we did not experiment on the largest LDC data
set.
5. Pialign-adaptive: Alignment and phrase pairs
extraction are same to Pialign-batch, while
translation probabilities are estimated by the
adaptive method with monolingual topic in-
formation (Su et al, 2012). The method es-
tablished the relationship between the out-of-
domain bilingual corpus and in-domain mono-
lingual corpora via topic distribution to esti-
mate the translation probability.
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|tf , f?) ? P (tf |f?)
(5)
where ?(e?|tf , f?) is the probability of translating f?
into e? given the source-side topic f? , P (tf |f?) is
the phrase-topic distribution of f.
The method we proposed is named Hier-
combin. It extracts phrase pairs in the same way as
the Pialign-linear. In the phrase table combination
process, the translation probability of each phrase
pair is estimated by the Hier-combin and the other
features are also linearly combined by averaging
the feature values. Pialign is used with default pa-
rameters. The parameter ?samps? is set to 5, which
indicates 5 samples are generated for a sentence
pair.
The IWSLT data consists of roughly 2, 000 sen-
tences and 3, 000 sentences each from the HIT and
BTEC for development purposes, and the test da-
ta consists of 1, 000 sentences. For the FBIS and
LDC task, we used NIST MT 2002 and 2004 for
development and testing purposes, consisting of
878 and 1, 788 sentences respectively. We em-
ploy Moses, an open-source toolkit for our exper-
iment (Koehn et al, 2007). SRILM Toolkit (Stol-
cke, 2002) is employed to train 4-gram language
models on the Xinhua portion of Gigaword cor-
pus, while for the IWLST2012 data set, only its
training set is used. We use batch-MIRA (Cher-
ry and Foster, 2012) to tune the weight for each
feature and translation quality is evaluated by the
case-insensitive BLEU-4 metric (Papineni et al,
2002). The BLEU scores reported in this paper
are the average of 5 independent runs of indepen-
dent batch-MIRA weight training, as suggested by
(Clark et al, 2011).
5.2 Result and Analysis
5.2.1 Performances of various extraction
methods
We carry out a series of experiments to evaluate
translation performance. The results are listed in
Table 2. Our method significantly outperforms the
baseline Pialign-linear. Except for the translation
probabilities, the phrase pairs of two methods are
exactly same, so the number of phrase pairs are
equal in the two methods. Further more, the per-
formance of the baseline Pialign-adaptive is also
higher than the baseline Pialign-linear?s and lower
than ours. This proves that the adaptive method
807
Methods Task Time(minute)
Batch Retraining 536.9
Hierarchical Parallel Extraction 122.55
Combination Integrating 1.5
Total 124.05
Table 3: Minutes used for alignment and phase
pair extraction in the FBIS data set.
with monolingual topic information is useful in
the tasks, but our approach with the hierarchical
Pitman-Yor process can estimate more accurate
translation probabilities based on all the data from
various domains.
Compared with the GIZA-batch, our approach
achieves competitive performance with a much s-
maller phrase table. The number of phase pairs
generated by our method is only 73.9%, 52.7%,
and 45.4% of the GIZA-batch?s respectively. In
the IWLST2012 data set, there is a huge difference
gap between the HIT corpus and the BTEC corpus,
and our method gains 0.814 BLEU improvement.
While the FBIS data set is artificially divided and
no clear human assigned differences among sub-
domains, our method loses 0.09 BLEU.
In the framework we proposed, phrase pairs are
extracted from each domain completely indepen-
dent of each other, so those tasks can be executed
on different machines, at different times, and of
course in parallel when we assume that the do-
mains are not incrementally added in the train-
ing data. The runtime of our approach and the
batch-based ITGs sampling method in the FBIS
data set is listed in Table 3 measured on a 2.7 GHz
E5-2680 CPU and 128 Gigabyte memory. When
comparing the hier-combin with the pialign-batch,
the BLEU scores are a little higher while the time
spent for training is much lower, almost one quar-
ter of the pialign-batch.
Even the performance of the pialign-linear is
better than the Baseline GIZA-linear?s, which
means that phrase pair extraction with hierarchi-
cal phrasal ITGs and sampling is more suitable
for domain adaptation tasks than the combination
GIZA++ and a heuristic method.
Generally, the hierarchical combination method
exploits the nature of a hierarchical Pitman-Yor
process and gains the advantage of its smoothing
effect, and our approach can incrementally gener-
ate a succinct phrase table based on all the data
from various domains with more accurate prob-
abilities. Traditional SMT phrase pair extraction
is batch-based, while our method has no obvious
shortcomings in translation accuracy, not to men-
tion efficiency.
5.2.2 Effect of Integration Order
Here, we evaluate whether our hierarchical com-
bination is sensitive to the order of the domains
when forming a hierarchical structure. Through
Equation (3), in our experiments, we chained the
domains in the order listed in Table 1, which is
in almost chronological order. Table 4 shows the
BLEU scores for the three data sets, in which the
order of combining phrase tables from each do-
main is alternated in the ascending and descending
of the similarity to the test data. The similarity be-
tween the data from each domain and the test data
is calculated using the perplexity measure with 5-
gram language model. The model learned from
the domain more similar to the test data is placed
in the front so that it can largely influence the
parameter computation with less backoff effects.
There is a big difference between the two opposite
order in IWSLT 2012 data set, in which more than
one point of decline in BLEU score when taking
the BTEC corpus as the first layer. Note that the
perplexity of BTEC was 344.589 while that of HIT
was 107.788. The result may indicate that our hi-
erarchical phrase combination method is sensitive
to the integration order when the training data is
small and there exists large gap in the similarity.
However, if most domains are similar (FBIS data
set) or if there are enough parallel sentence pairs
(NIST data set) in each domain, then the transla-
tion performances are almost similar even with the
opposite integrating orders.
IWSLT FBIS LDC
Descending 20.154 30.491 31.268
Ascending 19.066 30.388 31.254
Difference 1.088 0.103 0.014
Table 4: BLEU scores for the hierarchical model
with different integrating orders. Here Pialign was
run without multi-samples.
6 Conclusion and Future Work
In this paper, we present a novel hierarchical
phrase table combination method for SMT, which
can exploit more of the potential from all of da-
ta coming from various fields and generate a suc-
808
cinct phrase table with more accurate translation
probabilities. The method assumes that a com-
bined model is derived from a hierarchical Pitman-
Yor process with each prior learned separately in
each domain, and achieves BLEU scores competi-
tive with traditional batch-based ones. Meanwhile,
the framework has natural characteristics for par-
allel and incremental phrase pair extraction. The
experiment results on three different data sets in-
dicate the effectiveness of our approach.
In future work, we will also introduce incre-
mental learning for phase pair extraction inside a
domain, which means using the current translation
probabilities already obtained as the base measure
of sampling parameters for the upcoming domain.
Furthermore, we will investigate any tradeoffs be-
tween the accuracy of the probability estimation
and the coverage of phrase pairs.
Acknowledgments
We would like to thank our colleagues in both
HIT and NICT for insightful discussions, and
three anonymous reviewers for many invaluable
comments and suggestions to improve our paper.
This work is supported by National Natural Sci-
ence Foundation of China (61100093, 61173073,
61073130, 61272384), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207).
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238?241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200?208, Columbus, Ohio, June. Association
for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 858?867, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montre?al, Canada, June. Association for
Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion
transduction grammar for joint phrasal translation
modeling. In Proceedings of SSST, NAACL-HLT
2007/AMTA Workshop on Syntax and Structure in
Statistical Translation, pages 17?24.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John DeNero and Dan Klein. 2008. The complexi-
ty of phrase alignment problems. In Proceedings of
ACL-08: HLT, Short Papers, pages 25?28, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 128?135.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Martinez, and
Francisco Casacuberta. 2011. Fast incremental ac-
tive learning for statistical machine translation. A-
VANCES EN INTELIGENCIA ARTIFICIAL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 45?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 756?764. Association for Compu-
tational Linguistics.
809
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 394?
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
177?186, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
Technology (TIST), 2(3):1?18.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 343?350, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistic-
s: Human Language Technologies, pages 632?641,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Graham Neubig, Taro Watanabe, Shinsuke Mori, and
Tatsuya Kawahara. 2012. Machine translation with-
out words through substring alignment. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 165?174, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In International Joint Conference on
Natural Language Processing, pages 661?668.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, X-
iaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 459?468.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 985?992. Association for Computa-
tional Linguistics.
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In Proceedings of the Conference of the Association
for Machine translation, Americas.
F. Wood and Y. W. Teh. 2009. A hierarchical non-
parametric Bayesian approach to statistical language
model domain adaptation. In Proceedings of the In-
ternational Conference on Artificial Intelligence and
Statistics, volume 12.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Jia Xu, Yonggang Deng, Yuqing Gao, and Hermann
Ney. 2007. Domain dependent statistical machine
translation. In Proceedings of the MT Summit XI.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
810
