Analysis of Link Grammar on Biomedical Dependency Corpus
Targeted at Protein-Protein Interactions
Sampo Pyysalo, Filip Ginter, Tapio Pahikkala,
Jorma Boberg, Jouni Ja?rvinen, Tapio Salakoski
Turku Centre for Computer Science (TUCS)
and Dept. of computer science, University of Turku
Lemminka?isenkatu 14A
20520 Turku, Finland,
first name.last name@it.utu.fi
Jeppe Koivula
MediCel Ltd.,
Haartmaninkatu 8
00290 Helsinki, Finland,
jeppe.koivula@medicel.com
Abstract
In this paper, we present an evaluation of the
Link Grammar parser on a corpus consisting
of sentences describing protein-protein interac-
tions. We introduce the notion of an interac-
tion subgraph, which is the subgraph of a de-
pendency graph expressing a protein-protein in-
teraction. We measure the performance of the
parser for recovery of dependencies, fully correct
linkages and interaction subgraphs. We analyze
the causes of parser failure and report specific
causes of error, and identify potential modifica-
tions to the grammar to address the identified
issues. We also report and discuss the effect of
an extension to the dictionary of the parser.
1 Introduction
The challenges of processing the vast amounts of
biomedical publications available in databases
such as MEDLINE have recently attracted a
considerable interest in the Natural Language
Processing (NLP) research community. The
task of information extraction, commonly tar-
geting entity relationships, such as protein-
protein interactions, is an often studied prob-
lem to which various NLP methods have been
applied, ranging from keyword-based methods
(see, e.g., Ginter et al (2004)) to full syntactic
analysis as employed, for example, by Craven
and Kumlien (1999), Temkin and Gilder (2003)
and Daraselia et al (2004).
In this paper, we focus on the syntactic anal-
ysis component of an information extraction
system targeted to find protein-protein inter-
actions from the dependency output produced
by the Link Grammar1 (LG) parser of Sleator
and Temperley (1991). Two recent papers study
LG in the context of biomedical NLP. The work
by Szolovits (2003) proposes a fully automated
method to extend the dictionary of the LG
1http://www.link.cs.cmu.edu/link/
parser with the UMLS Specialist2 lexicon, and
Ding et al (2003) perform a basic evaluation
of LG performance on biomedical text. As both
papers suggest, LG will require modifications in
order to provide a correct analysis of grammat-
ical phenomena that are rare in general English
text, but common in biomedical language. Im-
plementing such modifications is a major effort
that requires a careful analysis of the perfor-
mance of the LG parser to identify the most
common causes of parsing failures and to target
modification efforts.
While Szolovits (2003) does not attempt to
evaluate parser performance at all and Ding
et al (2003) provide only an informal evalua-
tion on manually simplified sentences, we focus
on a more formal evaluation of the LG parser.
For the purpose of this study and also for sub-
sequent research of biomedical information ex-
traction with the LG parser, we have developed
a hand-annotated corpus consisting of unmod-
ified sentences from publications. We use this
corpus to evaluate the performance of the LG
parser and to identify problems and potential
improvements to the grammar and parser.
2 Link Grammar and parser
The Link Grammar and its parser represent an
implementation of a dependency-based compu-
tational grammar. The result of LG analysis for
a sentence is a labeled undirected simple graph,
whose nodes represent the words of the sentence
and whose edges and their labels express the
grammatical relationships between the words.
In LG terminology, the graph is called a link-
age, and its edges are called links. The linkage
must be planar (i.e., links must not cross) when
drawn above the words of the sentence, and the
labels of the links must satisfy the linking con-
straints specified for each word in the grammar.
A connected linkage is termed complete.
2http://www.nlm.nih.gov/research/umls/
15
findings suggest that PIP2 binds to proteins such as profilin
Figure 1: Annotation example. The interaction of two proteins, PIP2 and profilin, is stated by
the words binds to. The links joining these words form the interaction subgraph (drawn with solid
lines).
Due to the structural ambiguity of natural
language, several linkages can typically be con-
structed for an input sentence. In such cases,
the LG parser enumerates all linkages allowed
by the grammar. A post-processing step is then
employed to enforce a number of additional con-
straints. The number of linkages for some sen-
tences can be very high, making post-processing
and storage prohibitively expensive. This prob-
lem is addressed in the LG parser by defining
kmax, the maximal number of linkages to be
post-processed. If the parsing algorithm pro-
duces more than kmax linkages, the output is
reduced to kmax linkages by random sampling.
The linkages are then ordered from best to worst
using heuristic goodness criteria.
In order to be usable in practice, a parser is
typically required to provide a partial analysis
of a sentence for which it cannot construct a full
analysis. If the LG parser cannot construct a
complete linkage for a sentence, the connected-
ness requirement is relaxed so that some words
do not belong to the linkage at all. The LG
parser is also time-limited. If the full set of
linkages cannot be constructed in a given time
tmax, the parser enters a panic mode, in which it
performs an efficient but considerably restricted
parse, resulting in reduced performance. The
parameters tmax and kmax set the trade-off be-
tween the qualitative performance and the re-
source efficiency of the parser.
3 Corpus annotation and interaction
subgraphs
To compile a corpus of sentences describing
protein-protein interactions, we first selected
pairs of proteins that are known to interact
from the Database of Interacting Proteins3. We
entered these pairs as search terms into the
PubMed retrieval system. We then split the
publication abstracts returned by the searches
into sentences and included titles. These were
again searched for the protein pairs. This gave
us a set of 1927 sentences that contain the
3http://dip.doe-mbi.ucla.edu/
names of at least two proteins that are known
to interact. A domain expert annotated these
sentences for protein names and for words stat-
ing their interactions. Of these sentences, 1114
described at least one protein-protein interac-
tion.
Thereafter, we performed a dependency anal-
ysis and produced annotation of dependencies.
To minimize the amount of mistakes, each sen-
tence was independently annotated by two an-
notators and differences were then resolved by
discussion. The assigned dependency structure
was produced according to the LG linkage con-
ventions. Link types were not included in the
annotation, and no cycles were introduced in
the dependency graphs. All ambiguities where
the LG parser is capable of at least enumerat-
ing all alternatives (such as prepositional phrase
attachment) were enforced in the annotation.
A random sample consisting of 300 sentences,
including 28 publication titles, has so far been
fully annotated, giving 7098 word-to-word de-
pendencies. This set of sentences is the corpus
we refer to in the following sections.
An information extraction system targeted
at protein-protein interactions and their types
needs to identify three constituents that express
an interaction in a sentence: the proteins in-
volved and the word or phrase that states their
interaction and suggests the type of this inter-
action. To extract this information from a LG
linkage, the links connecting these items must
be recovered correctly by the parser. The fol-
lowing definition formalizes this notion.
Definition 1 (Interaction subgraph) The
interaction subgraph for an interaction between
two proteins A and B in a linkage L is the
minimal connected subgraph of L that contains
A, B, and the word or phrase that states their
interaction.
The recovery of a connected component con-
taining the protein names and the interaction
word is not sufficient: by the definition of a
complete linkage, such a component is always
present. Consequently, the exact set of links
16
that forms the interaction subgraph must be re-
covered.
For each interaction stated in a sentence,
the corpus annotation specifies the proteins in-
volved and the interaction word. The interac-
tion subgraph for each interaction can thus be
extracted automatically from the corpus. Be-
cause the corpus does not contain cyclic depen-
dencies, the interaction subgraphs are unique.
366 interaction subgraphs were identified from
the corpus, one for each described interaction.
The interaction subgraphs can be partially over-
lapping, because a single link can be part of
more than one interaction subgraph. Figure 1
shows an example of an annotated text frag-
ment.
4 Evaluation criteria
We evaluated the performance of the LG parser
according to the following three quantitative cri-
teria:
? Number of dependencies recovered
? Number of fully correct linkages
? Number of interaction subgraphs recovered
The number of recovered dependencies gives an
estimate of the probability that a dependency
will be correctly identified by the LG parser
(this criterion is also employed by, e.g., Collins
et al (1999)). The number of fully correct link-
ages, i.e. linkages where all annotated depen-
dencies are recovered, measures the fraction of
sentences that are parsed without error. How-
ever, a fully correct linkage is not necessary to
extract protein-protein interactions from a sen-
tence; to estimate how many interactions can
potentially be recovered, we measure the num-
ber of interaction subgraphs for which all de-
pendencies were recovered.
For each criterion, we measure the perfor-
mance for the first linkage returned by the
parser. However, the first linkage as ordered
by the heuristics of the LG parser was often not
the best (according to the criteria above) of the
linkages returned by the parser. To separate
the effect of the heuristics from overall LG per-
formance, we identify separately for each of the
three criteria the best linkage among the link-
ages returned by the parser, and we also report
performance for the best linkages.
We further divide the parsed sentences into
three categories: (1) sentences for which the
time tmax for producing a normal parse was ex-
hausted and the parser entered panic mode, (2)
sentences where linkages were sampled because
more than kmax linkages were produced, and
(3) stable sentences for which neither of these
occurred. A full analysis of all linkages that
the grammar allows is only possible for stable
sentences. For sentences in the other two cat-
egories, random effects may affect the results:
sentences for which more than kmax linkages are
produced are subject to randomness in sam-
pling, and sentences where the parser enters
panic mode were always subject to subsequent
sampling in our experiments.
5 Evaluation
To evaluate the ability of the LG parser to pro-
duce correct linkages, we increased the number
of stable sentences by setting the tmax param-
eter to 10 minutes and the kmax parameter to
10000 instead of using the defaults tmax = 30
seconds and kmax = 1000. When parsing the
corpus using these parameters, 28 sentences fell
into the panic category, 61 into the sampled
category, and 211 were stable. The measured
parser performance for the corpus is presented
in Table 1.
While the fraction of sentences that have a
fully correct linkage as the first linkage is quite
low (approximately 7%), for 28% of sentences
the parser is capable of producing a fully cor-
rect linkage. Performance was especially poor
for the publication titles in the corpus. Because
titles are typically fragments not containing a
verb, and LG is designed to model full clauses,
the parser failed to produce a fully correct link-
age for any of the titles.
The performance for recovered interaction
subgraphs is more encouraging, as 25% of the
subgraphs were recovered in the first linkage and
more than half in the best linkage. Yet many in-
teraction subgraphs remain unrecovered by the
parser: the results suggest an upper limit of
approximately 60% to the fraction of protein-
protein interactions that can be recovered from
any linkage produced by the unmodified LG.
In the following sections we further analyze the
reasons why the parser fails to recover all de-
pendencies.
5.1 Panics
No fully correct linkages and very few interac-
tion subgraphs were found in the panic mode.
This effect may be partly due to the complex-
ity of the sentences for which the parser en-
17
Category
Criterion Linkage Stable Sampled Panic Overall
Dependency First linkage 3242 (80.0%) 1376 (74.3%) 569 (52.3%) 5187 (73.1%)
Best linkage 3601 (86.6%) 1576 (85.0%) 620 (57.0%) 5797 (81.7%)
Total 4157 1853 1088 7098
Fully correct First linkage 22 (10.4%) 0 (0.0%) 0 (0.0%) 22 (7.3%)
Best linkage 79 (37.4%) 6 (9.8%) 0 (0.0%) 85 (28.3%)
Total 211 61 28 300
Interaction First linkage 75 (30.5%) 16 (20.2%) 0 (0.0%) 91 (24.9%)
subgraph Best linkage 156 (63.4%) 49 (62.0%) 4 (9.8%) 209 (57.1%)
Total 246 79 41 366
Table 1: Parser performance. The fraction of fulfilled criteria is shown by category (the criteria and
categories are explained in Section 4). The total rows give the number of criteria for each category,
and the overall column gives combined results for all categories.
tered panic mode. The effect of panics can
be better estimated by forcing the parser to
bypass standard parsing and to directly apply
panic options. For the 272 sentences where the
parser did not enter the panic mode, 77% of
dependencies were recovered in the first link-
age. When these sentences were parsed in forced
panic mode, 67% of dependencies were recov-
ered, suggesting that on average parses in panic
mode recover approximately 10% fewer depen-
dencies than in standard parsing mode. Simi-
larly, the number of fully correct first linkages
decreased from 22 to 6 and the number of inter-
action subgraphs recovered in the first linkage
from 91 to 65. These numbers indicate that
panics are a significant cause of error.
Experiments indicate than on a 1GHz ma-
chine approximately 40% of sentences can be
fully parsed in under a second, 80% in under 10
seconds and 90% within 10 minutes; yet approx-
imately 5% of sentences take more than an hour
to fully parse. With tmax set to 10 minutes, the
total parsing time was 165 minutes.
Long parsing times are caused by ambiguous
sentences for which the parser creates thousands
or even millions of alternative linkages. In ad-
dition to simply increasing the time limit, the
fraction of sentences where the parser enters the
panic mode could therefore be reduced by re-
ducing the ambiguity of the sentences, for ex-
ample, by extending the dictionary of the parser
(see Section 7).
5.2 Heuristics
When several linkages are produced for a sen-
tence, the LG parser applies heuristics to or-
der the sentences so that linkages that are more
likely to be correct are presented first. The
heuristics are based on examination and intu-
itions on general English, and may not be opti-
mal for biomedical text. Note in Table 1 that
both for recovered full linkages and interaction
subgraphs, the number of items that were recov-
ered in the best linkage is more than twice the
number recovered in the first linkage, suggesting
that a better ordering heuristic could dramat-
ically improve the performance of the parser.
Such improvements could perhaps be achieved
by tuning the heuristics to the domain or by
adopting a probabilistic ordering model.
6 Failure analysis
A significant fraction of dependencies were not
recovered in any linkage, even in sentences
where resources were not exhausted. In order to
identify reasons for the parser failing to recover
the correct dependencies, we analyze sentences
for which it is certain that the grammar cannot
produce a fully correct linkage. We thus ana-
lyzed the 132 stable sentences for which some
dependencies were not recovered.
For each sentence, we attempt to identify the
reason for the failure of the parser. For each
identified reason, we manually edit the sentence
to remove the source of failure. We repeat this
procedure until the parser is capable of pro-
ducing a correct parse for the sentence. Note
that this implies that also the interaction sub-
graphs in the sentence are correctly recovered,
and therefore the reasons for failures to recover
interaction subgraphs are a subset of the iden-
tified issues. The results of the analysis are
18
Reason for failure Cases
Unknown grammatical structure 72 (34.4%)
Dictionary issue 54 (25.8%)
Unknown word handling 35 (16.7%)
Sentence fragment 27 (12.9%)
Ungrammatical sentence 17 (8.1%)
Other 4 (1.9%)
Table 2: Results of failure analysis
summarized in Table 2. In many of the sen-
tences, more than one reason for parser failure
was found; in total 209 issues were identified in
the 132 sentences. The results are described in
more detail in the following sections.
6.1 Fragments and ungrammatical
sentences
As some of the analyzed sentences were taken
from publication titles, not all of them were
full clauses. To identify further problems when
parsing fragments not containing a verb, the
phrase ?is explained? and required determiners
were added to these fragments, a technique used
also by Ding et al (2003). The completed frag-
ments were then analyzed for potential further
problems.
A number of other ungrammatical sentences
were also encountered. The most common
problem was the omission of determiners, but
some other issues such as missing possessive
markers and errors in agreement (e.g., ?expres-
sions. . . has?) were also encountered.
Ungrammatical sentences pose interesting
challenges for parsing. Because many authors
are not native English speakers, a greater toler-
ance for grammatical mistakes should allow the
parser to identify the intended parse for more
sentences. Similarly, the ability to parse publi-
cation titles would extend the applicability of
the parser; in some cases it may be possible
to extract information concerning the key find-
ings of a publication from the title. However,
while relaxing completeness and correctness re-
quirements, such as mandatory determiners and
subject-predicate agreement, would allow the
parser to create a complete linkage for more sen-
tences, it would also be expected to lead to in-
creased ambiguity for all sentences, and subse-
quent difficulties in identifying the correct link-
age. If the ability to parse titles is considered
important, a potential solution not incurring
this cost would be to develop a separate version
of the grammar for parsing titles.
capping protein and actin genes
capping protein and actin genes
Figure 2: Multiple modifier coordination prob-
lem. Above: correct linkage disallowed by the
LG parser. Below: solution by chaining modi-
fiers.
6.2 Unknown grammatical structures
The method of the LG implementation for pars-
ing coordinations was found to be a frequent
cause of failures. A specific coordination prob-
lem occurs with multiple noun-modifiers: the
parser assumes that coordinated constituents
can be connected to the rest of the sentence
through exactly one word, and the grammar at-
taches all noun-modifiers to the head. Biomed-
ical texts frequently contain phrases that cause
these requirements to conflict: for example, in
the phrase ?capping protein and actin genes?
(where ?capping protein genes? and ?actin
genes? is the intended parse), the parser allows
only one of the words ?capping? and ?protein?
to connect to the word ?genes?, and is thus un-
able to produce the correct linkage (for illustra-
tion, see Figure 2(a)).
This multiple modifier coordination issue
could be addressed by modifying the grammar
to chain modifiers (Figure 2(b)). This alterna-
tive model is adopted by another major depen-
dency grammar, the EngCG-based Connexor
Machinese. The problem could also be ad-
dressed by altering the coordination handling
system in the parser.
Other identified grammatical structures not
known to the parser were number postmodifiers
to nouns (e.g., ?serine 38?), specifiers in paren-
theses (e.g., ?profilin mutant (H119E)?), coor-
dination with the phrase ?but not?, and various
unknown uses of colons and quotes. Single in-
stances of several distinct unknown grammati-
cal structures were also noted (e.g., ?5 to 10?,
?as expected from?, ?most concentrated in?).
Most of these issues can be addressed by local
modifications to the grammar.
6.3 Unknown word handling
The LG parser assigns unknown words to cate-
gories based on morphological or other surface
clues when possible. For remaining unknown
19
words, parses are attempted by assigning the
words to the generic noun, verb and adjective
types in all possible combinations.
Some problems with the unknown word pro-
cessing method were encountered during analy-
sis; for example, the assumption that unknown
capitalized words are proper nouns often caused
failures, especially in sentences beginning with
an unknown word. Similarly, the assumption
that words containing a hyphen behave as ad-
jectives was violated by a number of unknown
verbs (e.g., ?cross-links?).
Another problem that was noted occurred
with lowercase unknown words that should be
treated as proper nouns: because LG does not
allow unknown lowercase words to act as proper
nouns, the parser assigns incorrect structure to
a number of phrases containing words such as
?actin?. Improving unknown word handling re-
quires some modifications to the LG parser.
6.4 Dictionary issues
Cases where the LG dictionary contains a word,
but not in the sense in which it appears in a
sentence, almost always lead to errors. For ex-
ample, the LG dictionary does not contain the
word ?assembly? in the sense ?construction?,
causing the parser to erroneously require a de-
terminer for ?protein assembly?4. A related
frequent problem occurred with proper names
headed by a common noun, where the parser ex-
pects a determiner for such names (e.g., ?myosin
heavy chain?), and fails when one is not present.
These issues are mostly straightforward to ad-
dress in the grammar, but difficult to identify
automatically.
6.5 Biomedical entity names
Many of the causes for parser failure discussed
above are related to the presence of biomed-
ical entity names. While the causes for fail-
ures relating to names can be addressed in the
grammar, the existence of biomedical named en-
tity (NE) recognition systems (for a recent sur-
vey, see, e.g., Bunescu et al (2004)) suggests
an alternative solution: NEs could be identified
in preprocessing, and treated as single (proper
noun) tokens during the parse. During failure
analysis, 59 cases (28% of all cases) were noted
where this procedure would have eliminated the
error, assuming that no errors are made in NE
430 distinct problematic word definitions were iden-
tified, including ?breakdown?, ?composed?, ?factor?,
?half?, ?independent?, ?localized?, ?parallel?, ?pro-
moter?, ?segment?, ?upstream? and ?via?.
recognition. However, the performance of cur-
rent NE recognition systems is not perfect, and
it is not clear what the effect of adopting such
a method would be on parser performance.
7 Dictionary extension
Szolovits (2003) describes an automatic method
for mapping lexical information from one lexi-
con to another, and applies this method to aug-
ment the LG dictionary with terms from the ex-
tensive UMLS Specialist lexicon. The extension
introduces more than 125,000 new words into
the LG dictionary, more than tripling its size.
We evaluated the effect of this dictionary exten-
sion on LG parser performance using the criteria
described above. The fraction of distinct tokens
in the corpus found in the parser dictionary in-
creased from 52% to 72% with the dictionary
extension, representing a significant reduction
in uncertainty. This decrease was coupled with
a 32% reduction in total parsing time.
Because the LG parser is unable to produce
any linkage for sentences where it cannot iden-
tify a verb (even incorrectly), extending the dic-
tionary significantly reduced the ability of LG
to extract dependencies in titles, where the frac-
tion of recovered dependencies fell from the al-
ready low value of 67% to 55%.
For the sentences excluding titles, the benefits
of the dictionary extension were most significant
for sentences that were in the panic category
when using the unextended LG dictionary; 12
of these 28 sentences could be parsed without
panic with the dictionary extension. In the first
linkage of these sentences, the fraction of re-
covered dependencies increased by 8%, and the
fraction of recovered interaction subgraphs in-
creased from zero to 15% with the dictionary
extension.
The overall effect of the dictionary extension
was positive but modest, with no more than
2.5% improvement for either the first or best
linkages for any criterion, despite the threefold
increase in dictionary size. This result agrees
with the failure analysis: most problems can-
not be removed by extending the dictionary and
must instead be addressed by modifications of
the grammar or parser.
8 Conclusion
We have presented an analysis of Link Gram-
mar performance using a custom dependency
corpus targeted at protein-protein interactions.
We introduced the concept of the interaction
20
subgraph and reported parser performance for
three criteria: recovery of dependencies, in-
teraction subgraphs and fully correct linkages.
While LG was able to recover 73% of dependen-
cies in the first linkage, only 7% of sentences had
a fully correct first linkage. However, fully cor-
rect linkages are not required for information ex-
traction, and we found that 25% of interaction
subgraphs were recovered in the first linkage.
Resource exhaustion was found to be a signif-
icant cause of poor performance. Furthermore,
an evaluation of performance in the case when
optimal heuristics for ordering linkages are ap-
plied indicated that the fraction of recovered in-
teraction subgraphs could be more than doubled
(to 57%) by optimal heuristics.
To further analyze the cases where the parser
cannot produce a correct linkage, we carefully
examined the sentences and were able to iden-
tify five problem types. For each identified
case, we discussed potential modifications for
addressing the problems. We also considered
the possibility of using a named entity recogni-
tion system to improve parser performance and
found that 28% of LG failures would be avoided
by a flawless named entity recognition system.
We evaluated the effect of the dictionary ex-
tension proposed by Szolovits (2003), and found
that while it significantly reduced ambiguity
and improved performance for the most ambigu-
ous sentences, overall improvement was only
2.5%. This indicates that extending the dic-
tionary is not sufficient to address the perfor-
mance problems and that modifications to the
grammar and parser are necessary.
The quantitative analysis of LG performance
confirms that, in its current state, LG is not well
suited to the IE task discussed. However, in the
failure analysis we have identified a number of
specific issues and problematic areas for LG in
parsing biomedical publications, and suggested
improvements for adapting the parser to this
domain. The examination and implementation
of these improvements is a natural follow-up of
this study. Our initial experiments suggest that
it is indeed possible to implement general so-
lutions to many of the discussed problems, and
such modifications would be expected to lead to
improved applicability of LG to the biomedical
domain.
9 Acknowledgments
This work has been supported by Tekes, the
Finnish National Technology Agency.
References
Razvan Bunescu, Ruifang Ge, Rohit J. Kate,
Edward M. Marcotte, Raymond J. Mooney,
Arun Kumar Ramani, and Yuk Wah Wong. 2004
(to appear). Comparative experiments on learn-
ing information extractors for proteins and their
interactions. Artificial Intelligence in Medicine.
Special Issue on Summarization and Information
Extraction from Medical Documents.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser
for Czech. In 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 505?
512. Association for Computational Linguistics,
Somerset, New Jersey.
Mark Craven and Johan Kumlien. 1999. Construct-
ing biological knowledge bases by extracting in-
formation from text sources. In T. Lengauer,
R. Schneider, P. Bork, D. Brutlag, J. Glasgow,
H HW Mewes, and Zimmer R., editors, Proceed-
ings of the 7th International Conference on Intel-
ligent Systems in Molecular Biology, pages 77?86.
AAAI Press, Menlo Park, CA.
Nikolai Daraselia, Anton Yuryev, Sergei Egorov,
Svetalana Novichkova, Alexander Nikitin, and
Ilya Mazo. 2004. Extracting human protein in-
teractions from MEDLINE using a full-sentence
parser. Bioinformatics, 20(5):604?611.
Jing Ding, Daniel Berleant, Jun Xu, and Andy W.
Fulmer. 2003. Extracting biochemical interac-
tions from medline using a link grammar parser.
In B. Werner, editor, Proceedings of the 15th
IEEE International Conference on Tools with Ar-
tificial Intelligence, pages 467?471. IEEE Com-
puter Society, Los Alamitos, CA.
Filip Ginter, Tapio Pahikkala, Sampo Pyysalo,
Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2004. Extracting protein-protein inter-
action sentences by applying rough set data anal-
ysis. In S. Tsumoto, R. Slowinski, J. Komorowski,
and J.W. Grzymala-Busse, editors, Lecture Notes
in Computer Science 3066. Springer, Heidelberg.
Daniel D. Sleator and Davy Temperley. 1991. Pars-
ing english with a link grammar. Technical Re-
port CMU-CS-91-196, Department of Computer
Science, Carnegie Mellon University, Pittsburgh,
PA.
Peter Szolovits. 2003. Adding a medical lexicon to
an english parser. In Mark Musen, editor, Pro-
ceedings of the 2003 AMIA Annual Symposium,
pages 639?643. American Medical Informatics As-
sociation, Bethesda, MD.
Joshua M. Temkin and Mark R. Gilder. 2003. Ex-
traction of protein interaction information from
unstructured text using a context-free grammar.
Bioinformatics, 19(16):2046?2053.
21
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 33?40,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
A Probabilistic Search for the Best Solution Among Partially Completed
Candidates
Filip Ginter, Aleksandr Mylla?ri, and Tapio Salakoski
Turku Centre for Computer Science (TUCS) and
Department of Information Technology
University of Turku
Lemminka?isenkatu 14 A
20520 Turku, Finland
first.last@it.utu.fi
Abstract
We consider the problem of identifying
among many candidates a single best so-
lution which jointly maximizes several
domain-specific target functions. Assum-
ing that the candidate solutions can be
generated incrementally, we model the er-
ror in prediction due to the incomplete-
ness of partial solutions as a normally
distributed random variable. Using this
model, we derive a probabilistic search al-
gorithm that aims at finding the best solu-
tion without the necessity to complete and
rank all candidate solutions. We do not as-
sume a Viterbi-type decoding, allowing a
wider range of target functions.
We evaluate the proposed algorithm on the
problem of best parse identification, com-
bining simple heuristic with more com-
plex machine-learning based target func-
tions. We show that the search algorithm
is capable of identifying candidates with a
very high score without completing a sig-
nificant proportion of the candidate solu-
tions.
1 Background
Most of the current NLP systems assume a pipeline
architecture, where each level of analysis is imple-
mented as a module that produces a single, locally
optimal solution that is passed to the next module in
the pipeline. There has recently been an increased
interest in the application of joint inference, which
identifies a solution that is globally optimal through-
out the system and avoids some of the problems of
the pipeline architecture, such as error propagation.
We assume, at least conceptually, a division of
the joint inference problem into two subproblems:
that of finding a set of solutions that are structurally
compatible with each of the modules, and that of se-
lecting the globally best of these structurally correct
solutions. Many of the modules define a target func-
tion that scores the solutions by some domain cri-
teria based on local knowledge. The globally best
solution maximizes some combination of the target
functions, for example a sum.
For illustration, consider a system comprising of
two modules: a POS tagger and a parser. The POS
tagger generates a set of tag sequences that are com-
patible with the sentence text. Further, it may im-
plement a target function, based, for instance, on
tag n-grams, that scores these sequences according
to POS-centric criteria. The parser produces a set
of candidate parses and typically also implements a
target function that scores the parses based on their
structural and lexical features. Each parse that is
compatible with both the POS tagger and the parser
is structurally correct. The best solution may be de-
fined, for instance, as such a solution that maximizes
the sum of the scores of the POS- and parser-centric
target functions.
In practice, the set of structurally correct solu-
tions may be computed, for example, through the
intersection or composition of finite-state automata
as in the formalism of finite-state intersection gram-
mars (Koskenniemi, 1990). Finding the best so-
33
lution may be implemented as a best-path search
through Viterbi decoding, given a target function
that satisfies the Viterbi condition.
Most of the recent approaches to NLP tasks like
parse re-ranking make, however, use of feature-
based representations and machine-learning induced
target functions, which do not allow efficient search
strategies that are guaranteed to find the global op-
timum. In general case, all structurally correct so-
lutions have to be generated and scored by the tar-
get functions in order to guarantee that the globally
optimal solution is found. Further, each of the vari-
ous problems in natural language processing is typ-
ically approached with a different class of models,
ranging from n-gram statistics to complex regressors
and classifiers such as the support vector machines.
These different approaches need to be combined in
order to find the globally optimal solution. There-
fore, in our study we aim to develop a search strat-
egy that allows to combine a wider range of target
functions.
An alternative approach is that of propagating n
best solutions through the pipeline system, where
each step re-ranks the solutions by local criteria
(Ji et al, 2005). Incorporating a wide range of
features representing information from all levels of
analysis into a single master classifier is other com-
monly used method (Kambhatla, 2004; Zelenko et
al., 2004).
In this paper, we assume the possibility of gen-
erating the structurally correct solutions incremen-
tally, through a sequence of partially completed so-
lutions. We then derive a probabilistic search algo-
rithm that attempts to identify the globally best solu-
tion, without fully completing all structurally correct
solutions. Further, we do not impose strong restric-
tions, such as the Viterbi assumption, on the target
functions.
To a certain extent, this approach is related to the
problem of cost-sensitive learning, where obtaining
a feature value is associated with a cost and the
objective is to minimize the cost of training data
acquisition and the cost of instance classification
(Melville et al, 2004). However, the crucial dif-
ference is that we do not assume the possibility to
influence when advancing a partial solution, which
feature will be obtained next.
2 Method
Let us consider a system in which there are N so-
lutions s1, . . . , sN ? S to a problem and M tar-
get functions f1, . . . , fM , where fk : S ? R, that
assign a score to each of the solutions. The score
fk(si) expresses the extent to which the solution
si satisfies the criterion implemented by the target
function fk. The overall score of a solution si
f(si) =
M
?
k=1
fk(si) (1)
is the sum of the scores given by the individual target
functions. The objective is to identify s?, the best
among the N possible solutions, that maximizes the
overall score:
s? = arg max
si
f(si) . (2)
Suppose that the solutions are generated in-
crementally so that each solution si can be
reached through a sequence of F partial solutions
si,1, si,2, . . . , si,F , where si,F = si. Let further
u : S ? (0, 1] be a measure of a degree of com-
pletion for a particular solution. For a complete so-
lution si, u(si) = 1, and for a partial solution si,n,
u(si) < 1. For instance, when assigning POS tags
to the words of a sentence, the degree of completion
could be defined as the number of words assigned
with a POS tag so far, divided by the total number of
words in the sentence.
The score of a partial solution si,n is, to a certain
extent, a prediction of the score of the correspond-
ing complete solution si. Intuitively, the accuracy of
this prediction depends on the degree of completion.
The score of a partial solution with a high degree
of completion is generally closer to the final score,
compared to a partial solution with a low degree of
completion.
Let
?k(si,n) = fk(si) ? fk(si,n) (3)
be the difference between the scores of si and si,n.
That is, ?k(si,n) is the error in score caused by the in-
completeness of the partial solution si,n. As the so-
lutions are generated incrementally, the exact value
of ?k(si,n) is not known at the moment of generating
si,n because the solution si has not been completed
34
yet. However, we can model the error based on the
knowledge of si,n. We assume that, for a given si,n,
the error ?k(si,n) is a random variable distributed ac-
cording to a probability distribution with a density
function ?k, denoted as
?k(si,n) ? ?k(?; si,n) . (4)
The partial solution si,n is a parameter to the distri-
bution and, in theory, each partial solution gives rise
to a different distribution of the same general shape.
We assume that the error ?(si,n) is distributed
around a mean value and for a ?reasonably behav-
ing? target function, the probability of a small error
is higher than the probability of a large error. Ideally,
the target function will not exhibit any systematic er-
ror, and the mean value would thus be zero1. For in-
stance, a positive mean error indicates a systematic
bias toward underestimating the score. The mean
error should approach 0 as the degree of completion
increases and the error of a complete solution is al-
ways 0. We have further argued that the reliability
of the prediction grows with the degree of comple-
tion. That is, the error of a partial solution with a
high degree of completion should exhibit a smaller
variance, compared to that of a largely incomplete
solution. The variance of the error for a complete
solution is always 0.
Knowing the distribution ?k of the error ?k, the
density of the distribution dk(f ; si,n) of the final
score fk(si) is obtained by shifting the density of
the error ?k(si,n) by fk(si,n), that is,
fk(si) ? dk(f ; si,n) , (5)
where
dk(f ; si,n) = ?k(f ? fk(si,n) ; si,n) . (6)
So far, we have discussed the case of a single tar-
get function fk. Let us now consider the general
case of M target functions. Knowing the final score
density dk for the individual target functions fk, it is
now necessary to find the density of the overall score
f(si). By Equation 1, it is distributed as the sum
1We will see in our evaluation experiments that this is not
the case, and the target functions may exhibit a systematic bias
in the error ?.
?(si,n)
d(f ; si,n)
?2(si,n)
?
?(si,n)f (si,n)0
Sys. bias in error ?
Figure 1: The probability density d(f ; si,n) of the
distribution of the final score f(si), given a partial
solution si,n. The density is assumed normally dis-
tributed, with mean ?(si,n) and variance ?2(si,n).
With probability 1 ? ?, the final score is less than
?(si,n).
of the random variables f1(si) , . . . , fM (si). There-
fore, assuming independence, its density is the con-
volution of the densities of these variables, that is,
given si,n,
d(f ; si,n) = (d1 ? . . . ? dM )(f ; si,n) , (7)
and
f(si) ? d(f ; si,n) . (8)
We have assumed the independence of the target
function scores. Further, we will make the assump-
tion that d takes the form of the normal distribution,
which is convolution-closed, a property necessary
for efficient calculation by Equation 7. We thus have
d(f ; si,n) = n
(
f ; ?(si,n) , ?2(si,n)
)
, (9)
where n is the normal density function. While it
is unlikely that independence and normality hold
strictly, it is a commonly used approximation, nec-
essary for an analytical solution of (7). The notions
introduced so far are illustrated in Figure 1.
2.1 The search algorithm
We will now apply the model introduced in the pre-
vious section to derive a probabilistic search algo-
rithm.
35
Let us consider two partial solutions si,n and sj,m
with the objective of deciding which one of them is
?more promising?, that is, more likely to lead to a
complete solution with a higher score. The condi-
tion of ?more promising? can be defined in several
ways. For instance, once again assuming indepen-
dence, it is possible to directly compute the proba-
bility P (f(si) < f(sj)):
P (f(si) < f(sj))
= P (f(si) ? f(sj) < 0)
=
? 0
??
(dsi,n ? (?dsj,m))(f) df ,
(10)
where dsi,n refers to the function d(f ; si,n). Since
d is the convolution-closed normal density, Equa-
tion 10 can be directly computed using the normal
cumulative distribution. The disadvantage of this
definition is that the cumulative distribution needs
to be evaluated separately for each pair of partial
solutions. Therefore, we assume an alternate defi-
nition of ?more promising? in which the cumulative
distribution is evaluated only once for each partial
solution.
Let ? ? [0, 1] be a probability value and ?(si,n)
be the score such that P (f(si) > ?(si,n)) = ?. The
value of ?(si,n) can easily be computed from the in-
verse cumulative distribution function correspond-
ing to the density function d(f ; si,n). The interpre-
tation of ?(si,n) is that with probability of 1 ? ?,
the partial solution si,n, once completed, will lead
to a score smaller than ?(si,n). The constant ? is
a parameter, set to an appropriate small value. See
Figure 1 for illustration.
We will refer to ?(si,n) as the maximal expected
score of si,n. Of the two partial solutions, we con-
sider as ?more promising? the one, whose maximal
expected score is higher. As illustrated in Figure 2,
it is possible for a partial solution si,n to be more
promising even though its score f(si,n) is lower than
that of some other partial solution sj,m.
Further, given a complete solution si and a partial
solution sj,m, a related question is whether sj,m is a
promising solution, that is, whether it is likely that
advancing it will lead to a score higher than f(si).
Using the notion of maximal expected score, we say
that a solution is promising if ?(sj,m) > f(si).
With the definitions introduced so far, we are
f (si,n) f (sj,m) ?(si,n)
?(sj,m)
d(f ; si,n)
d(f ; sj,m)
Figure 2: Although the score of si,n is lower than
the score of sj,m, the partial solution si,n is more
promising, since ?(si,n) > ?(sj,m). Note that for
the sake of simplicity, a zero systematic bias of the
error ? is assumed, that is, the densities are centered
around the partial solution scores.
now able to perform two basic operations: compare
two partial solutions, deciding which one of them is
more promising, and compare a partial solution with
some complete solution, deciding whether the par-
tial solution is still promising or can be disregarded.
These two basic operations are sufficient to devise
the following search algorithm.
? Maintain a priority queue of partial solutions,
ordered by their maximal expected score.
? In each step, remove from the queue the par-
tial solution with the highest maximal expected
score, advance it, and enqueue any resulting
partial solutions.
? Iterate while the maximal expected score of the
most promising partial solution remains higher
than the score of the best complete solution dis-
covered so far.
The parameter ? primarily affects how early the
algorithm stops, however, it influences the order in
which the solutions are considered as well. Low val-
ues of ? result in higher maximal expected scores
36
and therefore partial solutions need to be advanced
to a higher degree of completion before they can be
disregarded as unpromising.
While there are no particular theoretical restric-
tions on the target functions, there is an important
practical consideration. Since the target function is
evaluated every time a partial solution si,n is ad-
vanced into si,n+1, being able to use the informa-
tion about si,n to efficiently compute fk(si,n+1) is
necessary.
The algorithm is to a large extent related to the A?
search algorithm, which maintains a priority queue
of partial solutions, ordered according to a score
g(x) + h(x), where g(x) is the score of x and h(x)
is a heuristic overestimate2 of the final score of the
goal reached from x. Here, the maximal expected
score of a partial solution is an overestimate with
the probability of 1?? and can be viewed as a prob-
abilistic counterpart of the A? heuristic component
h(x). Note that A? only guarantees to find the best
solution if h(x) never underestimates, which is not
the case here.
2.2 Estimation of ?k(si,n) and ?2k(si,n)
So far, we have assumed that for each partial so-
lution si,n and each target function fk, the density
?k(?; si,n) is defined as a normal density specified
by the mean ?k(si,n) and variance ?2k(si,n). This
density models the error ?k(si,n) that arises due to
the incompleteness of si,n. The parameters ?k(si,n)
and ?2k(si,n) are, in theory, different for each si,n and
reflect the behavior of the target function fk as well
as the degree of completion and possibly other at-
tributes of si,n. It is thus necessary to estimate these
two parameters from data.
Let us, for each target function fk, consider a
training set of observations Tk ? S ? R. Each
training observation tj =
(
sj,nj , ?k
(
sj,nj
))
? Tk
corresponds to a solution sj,nj with a known error
?k
(
sj,nj
)
= fk(sj) ? fk
(
sj,nj
)
.
Before we introduce the method to estimate the
density ?k(?; si,n) for a particular si,n, we discuss
data normalization. The overall score f(si,n) is de-
fined as the sum of the scores assigned by the in-
dividual target functions fk. Naturally, it is desir-
2In the usual application of A? to shortest-path search, h(x)
is a heuristic underestimate since the objective is to minimize
the score.
able that these scores are of comparable magnitudes.
Therefore, we normalize the target functions using
the z-normalization
z(x) = x ? mean(x)stdev(x) . (11)
Each target function fk is normalized separately,
based on the data in the training set Tk. Throughout
our experiments, the values of the target function are
always z-normalized.
Let us now consider the estimation of the mean
?k(si,n) and variance ?2k(si,n) that define the den-
sity ?k(?; si,n). Naturally, it is not possible to es-
timate the distribution parameters for each solution
si,n separately. Instead, we approximate the parame-
ters based on two most salient characteristics of each
solution: the degree of completion u(si,n) and the
score fk(si,n). Thus,
?k(si,n) ? ?k(u(si,n) , fk(si,n)) (12)
?2k(si,n) ? ?2k(u(si,n) , fk(si,n)) . (13)
Let us assume the following notation: ui = u(si,n),
fi = fk(si,n), uj = u
(
sj,nj
)
, fj = fk
(
sj,nj
)
, and
?j = ?k
(
sj,nj
)
. The estimate is obtained from Tk
using kernel smoothing (Silverman, 1986):
?k(ui, fi) =
?
tj?T ?jK
?
tj?T K
(14)
and
?2k(ui, fi) =
?
tj?T (?j ? ?k(ui, fi))
2 K
?
tj?T K
, (15)
where K stands for the kernel value Kui,fi(uj , fj).
The kernel K is the product of two Gaussians, cen-
tered at ui and fi, respectively.
Kui,fi(uj , fj)
= n
(
uj ; ui, ?2u
)
? n
(
fj ; fi, ?2f
)
, (16)
where n
(
x; ?, ?2
)
is the normal density function.
The variances ?2u and ?2f control the degree of
smoothing along the u and f axes, respectively.
High variance results in stronger smoothing, com-
pared to low variance. In our evaluation, we set the
37
0.2 0.6 1.0
?
4
?
2
0
1
?A
0.2 0.6 1.0
?
4
?
2
0
1
?A
2
0.2 0.6 1.0
?
1
1
3
5
?B
0.2 0.6 1.0
?
1
1
3
5
?B
2
Figure 3: Mean and variance of the error ?(si,n).
By (12) and (13), the error is approximated as a
function of the degree of completion u(si,n) and the
score fk(si,n). The degree of completion is on the
horizontal and the score on the vertical axis. The
estimates (?A, ?2A) and (?B, ?2B) correspond to the
RLSC regressor and average link length target func-
tions, respectively.
variance such that ?u and ?f equal to 10% of the dis-
tance from min(uj) to max(uj) and from min(fj)
to max(fj), respectively.
The kernel-smoothed estimates of ? and ?2 for
two of the four target functions used in the evalua-
tion experiments are illustrated in Figure 3. While
both estimates demonstrate the decrease both in
mean and variance for u approaching 0, the tar-
get functions generally exhibit a different behav-
ior. Note that the values are clearly dependent on
both the score and the degree of completion, indi-
cating that the degree of completion alone is not suf-
ficiently representative of the partial solutions. Ide-
ally, the values of both the mean and variance should
be strictly 0 for u = 1, however, due to the effect of
smoothing, they remain non-zero.
3 Evaluation
We test the proposed search algorithm on the prob-
lem of dependency parsing. We have previously de-
veloped a finite-state implementation (Ginter et al,
2006) of the Link Grammar (LG) parser (Sleator and
Temperley, 1991) which generates the parse through
the intersection of several finite-state automata. The
resulting automaton encodes all candidate parses.
The parses are then generated from left to right, pro-
ceeding through the automaton from the initial to
the final state. A partial parse is a sequence of n
words from the beginning of the sentence, together
with string encoding of their dependencies. Advanc-
ing a partial parse corresponds to appending to it the
next word. The degree of completion is then defined
as the number of words currently generated in the
parse, divided by the total number of words in the
sentence.
To evaluate the ability of the proposed method to
combine diverse criteria in the search, we use four
target functions: a complex state-of-the-art parse re-
ranker based on a regularized least-squares (RLSC)
regressor (Tsivtsivadze et al, 2005), and three mea-
sures inspired by the simple heuristics applied by
the LG parser. The criteria are the average length of
a dependency, the average level of nesting of a de-
pendency, and the average number of dependencies
linking a word. The RLSC regressor, on the other
hand, employs complex features and word n-gram
statistics.
The dataset consists of 200 sentences ran-
domly selected from the BioInfer corpus of
dependency-parsed sentences extracted from ab-
stracts of biomedical research articles (Pyysalo et
al., 2006). For each sentence, we have randomly
selected a maximum of 100 parses. For sentences
with less than 100 parses, all parses were selected.
The average number of parses per sentence is 62.
Further, we perform 5 ? 2 cross-validation, that is,
in each of five replications, we divide the data ran-
domly to two sets of 100 sentences and use one set to
estimate the probability distributions and the other
set to measure the performance of the search algo-
rithm. The RLSC regressor is trained once, using a
different set of sentences from the BioInfer corpus.
The results presented here are averaged over the 10
folds. As a comparative baseline, we use a simple
38
greedy search algorithm that always advances the
partial solution with the highest score until all so-
lutions have been generated.
3.1 Results
For each sentence s with parses S {s1, . . . , sN}, let
SC ? S be the subset of parses fully completed be-
fore the algorithm stops and SN = S \ SC the sub-
set of parses not fully completed. Let further TC be
the number of iterations taken before the algorithm
stops, and T be the total number of steps needed to
generate all parses in S . Thus, |S| is the size of the
search space measured in the number of parses, and
T is the size of the search space measured in the
number of steps. For a single parse si, rank(si) is
the number of parses in S with a score higher than
f(si) plus 1. Thus, the rank of all solutions with
the maximal score is 1. Finally, ord(si) corresponds
to the order in which the parses were completed by
the algorithm (disregarding the stopping criterion).
For example, if the parses were completed in the
order s3, s8, s1, then ord(s3) = 1, ord(s8) = 2,
and ord(s1) = 3. While two solutions have the
same rank if their scores are equal, no two solutions
have the same order. The best completed solution
s?C ? SC is the solution with the highest rank in SC
and the lowest order among solutions with the same
rank. The best solution s? is the solution with rank 1
and the lowest order among solutions with rank 1. If
s? ? SC , then s?C = s? and the objective of the algo-
rithm to find the best solution was fulfilled. We use
the following measures of performance: rank(s?C),
ord(s?), |SC ||S| , and
TC
T . The most important criteria
are rank(s?C) which measures how good the best
found solution is, and TCT which measures the pro-
portion of steps actually taken by the algorithm of
the total number of steps needed to complete all the
candidate solutions. Further, ord(s?), the number
of parses completed before the global optimum was
reached regardless the stopping criterion, is indica-
tive about the ability of the search to reach the global
optimum early among the completed parses. Note
that all measures except for ord(s?) equal to 1 for
the baseline greedy search, since it lacks a stopping
criterion.
The average performance values for four settings
of the parameter ? are presented in Table 1. Clearly,
? rank(s?C) ord(s?) |SC ||S|
TC
T
0.01 1.6 8.8 0.78 0.94
0.05 2.8 11.2 0.62 0.85
0.10 4.0 12.2 0.53 0.79
0.20 6.0 13.5 0.41 0.73
Base 1.0 28.7 1.00 1.00
Table 1: Average results over all sentences.
the algorithm behaves as expected with respect to
the parameter ?. While with the strictest setting
? = 0.01, 94% of the search space is explored, with
the least strict setting of ? = 0.2, 73% is explored,
thus pruning one quarter of the search space. The
proportion of completed parses is generally consid-
erably lower than the proportion of explored search
space. This indicates that the parses are generally
advanced to a significant level of completion, but
then ruled out. The behavior of the algorithm is
thus closer to a breadth-first, rather than depth-first
search. We also notice that the average rank of the
best completed solution is very low, indicating that
although the algorithm does not necessarily identify
the best solution, it generally identifies a very good
solution. In addition, the order of the best solution is
low as well, suggesting that generally good solutions
are identified before low-score solutions. Further,
compared to the baseline, the globally optimal solu-
tion is reached earlier among the completed parses,
although this does not imply that it is reached earlier
in the number of steps. Apart from the overall aver-
ages, we also consider the performance with respect
to the number of alternative parses for each sentence
(Table 2). Here we see that even with the least strict
setting, the search finds a reasonably good solution
while being able to reduce the search space to 48%.
4 Conclusions and future work
We have considered the problem of identifying a
globally optimal solution among a set of candidate
solutions, jointly optimizing several target functions
that implement domain criteria. Assuming the solu-
tions are generated incrementally, we have derived
a probabilistic search algorithm that aims to identify
the globally optimal solution without completing all
of the candidate solutions. The algorithm is based on
a model of the error in prediction caused by the in-
39
? = 0.01 ? = 0.2 Base
|S| # rank(s?C) ord(s?) |SC ||S|
TC
T rank(s?C) ord(s?)
|SC |
|S|
TC
T ord(s?)
1-10 40 1.0 1.6 1.00 1.00 1.2 1.8 0.84 0.95 2.85
11-20 18 1.1 4.4 0.88 0.97 2.8 7.0 0.54 0.79 9.82
21-30 8 1.0 2.9 1.00 1.00 1.0 2.4 0.80 0.98 14.75
31-40 9 1.2 7.8 0.79 0.95 2.6 10.8 0.48 0.74 20.67
41-50 6 1.0 4.4 0.80 0.89 4.9 9.8 0.28 0.61 18.07
51-60 3 1.0 2.3 0.64 0.88 7.1 5.9 0.30 0.59 38.67
61-70 5 1.1 26.9 0.86 0.99 3.4 23.2 0.22 0.68 32.60
71-80 3 1.0 8.7 0.78 0.98 9.2 19.6 0.30 0.71 49.67
81-90 6 2.5 8.2 0.61 0.94 9.3 16.6 0.24 0.76 47.67
91-100 102 5.2 20.9 0.50 0.81 18.9 38.2 0.15 0.48 52.69
Table 2: Average results with respect to the number of alternative parses. The column # contains the number
of sentences in the dataset which have the given number of parses.
completeness of a partial solution. Using the model,
the order in which partial solutions are explored is
defined, as well as a stopping criterion for the algo-
rithm.
We have performed an evaluation using best parse
identification as the model problem. The results in-
dicate that the method is capable of combining sim-
ple heuristic criteria with a complex regressor, iden-
tifying solutions with a very low average rank.
The crucial component of the method is the model
of the error ?. Improving the accuracy of the model
may potentially further improve the performance of
the algorithm, allowing a more accurate stopping
criterion and better order in which the parses are
completed. We have assumed independence be-
tween the scores assigned by the target functions. As
a future work, a multivariate model will be consid-
ered that takes into account the mutual dependencies
of the target functions.
References
Filip Ginter, Sampo Pyysalo, Jorma Boberg, and Tapio
Salakoski. 2006. Regular approximation of Link
Grammar. Manuscript under review.
Heng Ji, David Westbrook, and Ralph Grishman. 2005.
Using semantic relations to refine coreference deci-
sions. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Methods
in Natural Language Processing (HLT/EMNLP?05),
Vancouver, Canada, pages 17?24. ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for information extraction. In The Companion Vol-
ume to the Proceedings of 42st Annual Meeting of the
Association for Computational Linguistics (ACL?04),
Barcelona, Spain, pages 178?181. ACL.
Kimmo Koskenniemi. 1990. Finite-state parsing and
disambiguation. In Proceedings of the 13th In-
ternational Conference on Computational Linguis-
tics (COLING?90), Helsinki, Finland, pages 229?232.
ACL.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost,
and Raymond Mooney. 2004. Active feature-value
acquisition for classifier induction. In Proceedings
of the Fourth IEEE International Conference on Data
Mining (ICDM?04), pages 483?486. IEEE Computer
Society.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2006. Bio Information Extraction Re-
source: A corpus for information extraction in the
biomedical domain. Manuscript under review.
Bernard W. Silverman. 1986. Density Estimation for
Statistics and Data Analysis. Chapman & Hall.
Daniel D. Sleator and Davy Temperley. 1991. Pars-
ing English with a link grammar. Technical Report
CMU-CS-91-196, Department of Computer Science,
Carnegie Mellon University, Pittsburgh, PA.
Evgeni Tsivtsivadze, Tapio Pahikkala, Sampo Pyysalo,
Jorma Boberg, Aleksandr Mylla?ri, and Tapio
Salakoski. 2005. Regularized least-squares for parse
ranking. In Proceedings of the 6th International
Symposium on Intelligent Data Analysis (IDA?05),
Madrid, Spain, pages 464?474. Springer, Heidelberg.
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbets.
2004. Binary integer programming for information ex-
traction. In Proceedings of the ACE Evaluation Meet-
ing.
40
BioNLP 2007: Biological, translational, and clinical language processing, pages 25?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
On the unification of syntactic annotations under the Stanford
dependency scheme: A case study on BioInfer and GENIA
Sampo Pyysalo, Filip Ginter, Katri Haverinen,
Juho Heimonen, Tapio Salakoski
Department of Information Technology
University of Turku,
Joukahaisenkatu 3-5
20014 Turku, Finland
first.last@utu.fi
Veronika Laippala
Department of French Studies
University of Turku,
Henrikinkatu 2
20014 Turku, Finland
veronika.laippala@utu.fi
Abstract
Several incompatible syntactic annotation
schemes are currently used by parsers and
corpora in biomedical information extrac-
tion. The recently introduced Stanford de-
pendency scheme has been suggested to be
a suitable unifying syntax formalism. In this
paper, we present a step towards such uni-
fication by creating a conversion from the
Link Grammar to the Stanford scheme. Fur-
ther, we create a version of the BioInfer cor-
pus with syntactic annotation in this scheme.
We present an application-oriented evalua-
tion of the transformation and assess the
suitability of the scheme and our conversion
to the unification of the syntactic annotations
of BioInfer and the GENIA Treebank.
We find that a highly reliable conversion is
both feasible to create and practical, increas-
ing the applicability of both the parser and
the corpus to information extraction.
1 Introduction
One of the main challenges in biomedical infor-
mation extraction (IE) targeting entity relationships
such as protein-protein interactions arises from the
complexity and variability of the natural language
statements used to express such relationships. To
address this complexity, many biomedical IE sys-
tems (Alphonse et al, 2004; Rinaldi et al, 2004;
Fundel et al, 2007) and annotated corpora (Kim et
al., 2003; Aubin, 2005; Pyysalo et al, 2007) incor-
porate full syntactic analysis. However, there are
significant differences between the syntactic anno-
tation schemes employed. This leads to difficulties
in sharing data between corpora and establishing the
relative performance of parsers as well as to a lack
of interchangeability of one parser for another in IE
systems, among other issues.
Syntax formalisms are broadly divided into con-
stituency and dependency. Constituency schemes
are dominant in many fields and are unified under
the established Penn Treebank (PTB) scheme (Bies
et al, 1995). However, dependency schemes have
been suggested to be preferable in IE, as they repre-
sent the semantic structure of the sentences more di-
rectly (see, e.g., de Marneffe et al (2006)). Further,
Lin (1998) argues for dependency-based evaluation
of both dependency and constituency parsers since
it allows evaluation metrics that are more relevant
to semantic interpretation as well as intuitively more
meaningful. Even though there is clearly a need for a
unifying scheme for dependency comparable to that
of PTB for constituency, no widely adopted standard
currently exists.
In this paper, we present a step towards unify-
ing the diverse syntax schemes in use in IE sys-
tems and corpora such as the GENIA Treebank1 and
the recently introduced BioInfer corpus (Pyysalo et
al., 2007). Clegg and Shepherd (2007) have re-
cently proposed to use the Stanford dependency
scheme (de Marneffe et al, 2006) as a common,
application-oriented syntax representation. To as-
sess this choice, we develop a set of conversion
rules for transforming the Link Grammar (LG) de-
pendency scheme (Sleator and Temperley, 1993) to
1http://www-tsujii.is.s.u-tokyo.ac.jp/ ?genia
25
the Stanford scheme and then create a version of
the BioInfer corpus in the Stanford scheme by ap-
plying the conversion rules and manually correcting
the errors. By making the BioInfer corpus available
in the Stanford scheme, we also increase the value
of the corpus for biomedical IE. The transforma-
tion has the further benefit of allowing Link Gram-
mar output to be normalized into a more application-
oriented form. Finally, to assess the practical value
of the conversion method and of the BioInfer syntac-
tic annotation in the Stanford scheme, we compare
the Charniak-Lease constituency parser2 (Charniak
and Lease, 2005) and BioLG,3 an adaptation of LG
(Pyysalo et al, 2006), on the newly unified dataset
combining the constituency-annotated GENIA Tree-
bank with the dependency-annotated BioInfer cor-
pus.
The transformation rules and software as well as
the Stanford annotation of the BioInfer corpus, the
main practical results of this work, are freely avail-
able at http://www.it.utu.fi/BioInfer.
2 Motivation
To support the development of IE systems, it is im-
portant for a corpus to provide three key types of
annotation capturing the named entities, their rela-
tionships and the syntax. To our knowledge, there
are only two corpora in the biomedical domain that
currently provide these three annotation types simul-
taneously, BioInfer and LLL (Aubin, 2005). In ad-
dition, GENIA, the de facto standard domain corpus
for named entity recognition and syntactic analysis,
is in the process of adding a relationship annota-
tion. The corpora have different strengths; BioInfer
provides a detailed relationship annotation, while
GENIA has a broader coverage of named entities
and a larger treebank. Unifying the syntactic anno-
tations of these two corpora allows these strengths
to be combined.
The BioInfer syntactic annotation follows the LG
dependency scheme, addressing the recent interest
in LG in the biomedical NLP community (Ding et
al., 2003; Alphonse et al, 2004; Aubin et al, 2005).
However, the LG scheme has been criticized for be-
ing oriented more towards structural than semantic
2http://nlp.stanford.edu/software/,
version 1.5.1
3http://www.it.utu.fi/BioLG, version 1.2.0
relations and having excessively detailed link types
whose functional meaning and value for semantic
analysis is questionable (Schneider, 1998; de Marn-
effe et al, 2006). Our experience with LG leads us
to largely agree with these criticisms.
De Marneffe et al (2006) have recently intro-
duced a transformation from PTB to the Stanford
scheme. Clegg and Shepherd (2007) have ap-
plied this transformation to perform a dependency-
based comparison of several statistical constituency
parsers on the GENIA Treebank and have argued for
the adoption of the Stanford scheme in biomedical
IE. Moreover, the IE system of Fundel et al (2007),
which employs the Stanford scheme, was shown to
notably outperform previously applied systems on
the LLL challenge dataset, finding an F-score of
72% against a previous best of 54%. This further
demonstrates the suitability of the Stanford scheme
to IE applications.
3 Dependency schemes
In this section, we present the Stanford and LG
dependency schemes and discuss their relative
strengths.
3.1 Stanford dependency scheme
A parse in the Stanford scheme (SF) is a directed
graph where the nodes correspond to the words and
the edges correspond to pairwise syntactic depen-
dencies between the words. The scheme defines
a hierarchy of 48 grammatical relations, or depen-
dency types. The most generic relation, dependent,
can be specialized as auxiliary, argument, or modi-
fier, which again have several subtypes (de Marneffe
et al, 2006).
The Stanford conversion transforms phrase struc-
ture parses into the Stanford scheme. First, the se-
mantic head of each constituent is identified using
head rules similar to those of Collins (1999) and un-
typed dependencies are then extracted and labeled
with the most specific grammatical relations possi-
ble using Tregex rules (Levy and Andrew, 2006).
The system additionally provides a set of collaps-
ing rules, suggested to be beneficial for IE appli-
cations (de Marneffe et al, 2006; Clegg and Shep-
herd, 2007). These rules collapse some dependen-
cies by incorporating certain parts of speech (mostly
26
Vimentin and actin were also up-regulated , whereas an isoform of myosin heavy chain was down-regulated .
A/ANPv Cs
Mp
Ss
A/AN PvDsuE
Js
MVsCC
Spx
CC
Vimentin and actin were also up-regulated , whereas an isoform of myosin heavy chain was down-regulated .
cc>
conj>
<nsubjpass
<auxpass
<advmod
advcl>
<mark
<det prep>
<nsubjpass
pobj>
<nmod
<nmod <auxpass
Vimentin and actin were also up-regulated , whereas an isoform of myosin heavy chain was down-regulated .
conj_and>
<nsubjpass
<nsubjpass
<auxpass
<advmod
advcl>
<mark
<det
prep_of>
<nsubjpass
<nmod
<nmod <auxpass
Figure 1: A sentence from the BioInfer corpus with its LG linkage (top), the Stanford parse (middle), and
the collapsed Stanford parse (bottom). The < and > symbols denote the direction of dependencies.
during incubation , actin suffered degradation
Jp
CO
Ss Os
actin suffered degradation during incubation
Jp
MVp
Ss Os
actin suffered degradation during incubation
JpMpSs Os
Figure 2: Variation in the link type connecting a
preposition: CO to the main noun in topicalized
prepositional phrases, MVp when modifying a verb,
and Mp when modifying a noun.
conjunctions and prepositions) in grammatical rela-
tions. This is realized by combining two relations
and denominating the resulting dependency with a
type based on the word to which the original two
relations were linked (see Figure 1).
In the LG-SF conversion, we target the uncol-
lapsed Stanford scheme, as the collapsing rules have
already been developed and reported by de Marn-
effe et al; reimplementing the collapsing would be
an unnecessary duplication of efforts. Also, the col-
lapsed relations can be easily created based on the
uncollapsed ones, whereas reversing the conversion
would be more complicated.
3.2 LG dependency scheme
Link Grammar (Sleator and Temperley, 1993) is
closely related to dependency formalisms. It is
based on the notion of typed links connecting words.
While links are not explicitly directional, the roles
of the words can be inferred from their left-to-right
order and the link type. An LG parse, termed link-
age, consists of a set of links that connect the words
so that no two links cross or connect the same two
words. When discussing LG, we will use the terms
dependency and link interchangeably.
Compared to the 48 dependency types of the Stan-
ford scheme, the LG English grammar defines over
100 main link types which are further divided into
400 subtypes. The unusually high number of dis-
tinct types is one of the properties of the LG English
grammar that complicate the application of LG in
information extraction. Consider, for instance, the
case of prepositional phrase attachment illustrated in
Figure 2, where all the alternative attachment struc-
tures receive different types. Arguably, this distinc-
tion is unimportant to current IE systems and there-
fore should be normalized. This normalization is in-
herent in the Stanford scheme, where the preposition
always attaches using a prep dependency.
In contrast to such unnecessarily detailed distinc-
tions, in certain cases LG types fail to make seman-
tically important distinctions. For instance, the CO
link type is used to mark almost all clause openers,
not distinguishing between, for example, adverbial
and prepositional openers.
4 Our contributions
In this section, we describe the LG-SF conversion
as well as SF BioInfer, the BioInfer corpus syntactic
27
annotation in the Stanford scheme. These are the
two primary contributions of this study.
4.1 LG-SF conversion
The LG-SF conversion transforms the undirected
LG links into directed dependencies that follow the
Stanford scheme. The transformation is based on
handwritten rules, each rule consisting of a pattern
that is matched in the LG linkage and generating a
single dependency in the Stanford parse. Since the
conversion rules only refer to the LG linkage, they
do not influence each other and are applied inde-
pendently in an arbitrary order. The pattern of each
rule is expressed as a set of positive or negative con-
straints on the presence of LG links. The constraints
typically restrict the link types and may also refer to
the lexical level, restricting only to links connecting
certain word forms. Since LG does not define link
directionality, the patterns refer to the left-to-right
order of tokens and the rules must explicitly specify
the directionality of the generated SF dependencies.
As an example, let us consider the rule
[X Pv? Y]? Y auxpass? X. The pattern matches two
tokens connected with an LG link of type Pv and
generates the corresponding directed auxpass de-
pendency. This rule applies twice in the linkage
in Figure 1. It is an example of a rare case of a
one-to-one correspondence between an LG and an
SF type. Many-to-many correspondences are much
more common: in these cases, rules specify multiple
restrictions and multiple rules are needed to gener-
ate all instances of a particular dependency type. As
a further example, we present the three rules below,
which together generate all left-to-right prep depen-
dencies. An exclamation mark in front of a restric-
tion denotes a negative restriction, i.e., the link must
not exist in order for the rule to apply. The link types
are specified as regular expressions.
[A Mp|MX[a-z]x? B]![B Cs? C]![A RS? D]? A prep? B
[A OF|MVx? B]![A RS? C]? A prep? B
[A MVp? B]![A RS? C]![C MVl? A]? A prep? B
The first of the above three rules generates the prep
dependency in the parse in Figure 1, with A=isoform
and B=of. The variables C and D are not bound to
any tokens in this sentence, as they only occur in
negative restrictions.
actin , profilin and cofilin
CC
CC CC
Figure 3: Example of a structure where the relative
order of the first two tokens cannot be resolved by
the rules.
To resolve coordination structures, it is crucial to
recognize the leftmost coordinated element, i.e. the
head of the coordination structure in the SF scheme.
However, the conversion rule patterns are unable to
capture general constraints on the relative order of
the tokens. For instance, in the linkage in Figure 3, it
is not possible to devise a pattern only matching one
of the tokens actin and profilin, while not matching
the other. Therefore, we perform a pre-processing
step to resolve the coordination structures prior to
the application of the conversion rules. After the
pre-processing, the conversion is performed with the
lp2lp software (Alphonse et al, 2004), previously
used to transform LG into the LLL competition for-
mat (Aubin, 2005).
In the development of the LG-SF conversion and
SF BioInfer, we make the following minor modifi-
cations to the Stanford scheme. The scheme dis-
tinguishes nominal and adjectival pre-modifiers of
nouns, a distinction that is not preserved in the
BioInfer corpus. Therefore, we merge the nom-
inal and adjectival pre-modifier grammatical rela-
tions into a single relation, nmod. For the same rea-
son, we do not distinguish between apposition and
abbreviation, and only use the appos dependency
type. Finally, we do not annotate punctuation.
Schneider (1998) has previously proposed a strat-
egy for identifying the head word for each LG link,
imposing directionality and thus obtaining a depen-
dency graph. Given the idiosyncrasies of the LG
linkage structures, this type of transformation into
dependency would clearly not have many of the nor-
malizing benefits of the LG-SF transformation.
4.2 SF BioInfer
For creating the BioInfer corpus syntactic annota-
tion in the Stanford scheme, the starting point of
the annotation process was the existing manual an-
notation of the corpus in the LG scheme to which
we applied the LG-SF conversion described in Sec-
tion 4.1. The resulting SF parses were then manu-
28
ally corrected by four annotators. In the manual cor-
rection phase, each sentence was double-annotated,
that is, two annotators corrected the converted out-
put independently. All disagreements were resolved
jointly by all annotators.
To estimate the annotation quality and the sta-
bility of the SF scheme, we determined annotator
agreement as precision and recall measured against
the final annotation. The average annotation preci-
sion and recall were 97.5% and 97.4%, respectively.
This high agreement rate suggests that the task was
well-defined and the annotation scheme is stable.
The BioInfer corpus consists of 1100 sentences
and, on average, the annotation consumed approxi-
mately 10 minutes per sentence in total.
5 Evaluation
In this section, we first evaluate the LG-SF conver-
sion. We then present an evaluation of the Charniak-
Lease constituency parser and the BioLG depen-
dency parser on BioInfer and GENIA.
5.1 Evaluation of the conversion rules
In the evaluation of the conversion rules against the
gold standard SF BioInfer annotation, we find a pre-
cision of 98.0% and a recall of 96.2%. Currently,
the LG-SF conversion consists of 114 rules, each
of which specifies, on average, 4.4 restrictions. Al-
together the rules currently generate 32 SF depen-
dency types, thus averaging 3.5 rules per SF type.
Only 9 of the SF types are generated by a single
rule, while the remaining require several rules. We
estimate that the current ruleset required about 100
hours to develop.
In Figure 4, we show the cumulative precision and
recall of the rules when added in the descending or-
der of their recall. Remarkably, we find that a recall
of 80% is reached with just 13 conversion rules, 90%
with 28 rules, and 95% with 56 rules. These fig-
ures demonstrate that while the SF and LG schemes
are substantially different, a high-recall conversion
can be obtained with approximately fifty carefully
crafted rules. Additionally, while precision is con-
sistently high, the highest-recall rules also have the
highest precision. This may be related to the fact
that the most common SF dependency types have a
straightforward correspondence in LG types.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Number of conversion rules
Recall
Precision
Figure 4: Cumulative precision and recall of the con-
version rules.
A common source of errors in the LG-SF conver-
sion are the Link Grammar idiomatic expressions,
which are analyzed as a chain of ID links (0.7% of
all links in the BioInfer corpus) and connected to
the linkage always through their last word. Some
examples of LG idiomatic expressions include each
other, no one, come of age, gotten rid of, for good,
and the like. These expressions are often problem-
atic in the SF conversion as well. We did not at-
tempt any wide-coverage systematic resolution of
the idiomatic expressions and, apart from the most
common cases such as in vitro, we preserve the LG
structure of connecting these expressions through
their last word. We note, however, that the list of
idiomatic LG expressions is closed and therefore a
case-by-case resolution leading to a full coverage is
possible, although not necessarily practical.
Similar to the LG idiomatic expressions are the
SF dep dependencies, generated when none of the
SF rules assigns a more specific type. In most cases,
dep is a result of a lack of coverage of the SF con-
version rules typically occurring in rare or idiomatic
expressions. We assume that many of the dep depen-
dencies will be resolved in the future, given that the
SF conversion and the SF dependency scheme itself
are presented by the authors as a work in progress.
Therefore, we do not attempt to replicate most of
the SF dep dependencies with the LG-SF conversion
rules; much of the effort would be obsoleted by the
progress of the SF conversion. The dep dependen-
cies account for 23% of the total 3.8% of dependen-
cies not recovered by the LG-SF conversion.
29
Charniak-Lease BioLG
corpus Prec. Rec. F Prec. Rec. F
GENIA 81.2 81.3 81.3 76.9 72.4 74.6
BioInfer 78.4 79.9 79.4 79.6 76.1 77.8
Table 1: Parser performance. Precision, recall and
F-measure for the two parsers on the two corpora.
5.2 Evaluated parsers and corpora
The Charniak-Lease parser is a statisti-
cal constituency parser developed by Char-
niak and Lease (2005). It is an adaptation of the
Charniak parser (Charniak, 1999) to the biomedical
domain. For example, it uses a POS-tagger trained
on the GENIA corpus, although the parser itself has
been trained on the Penn Treebank. The Charniak-
Lease parser is of particular interest, because in a
recent comparison performed by Clegg and Shep-
herd (2007) on the GENIA Treebank, it was the
best performing of several state-of-the-art statistical
constituency parsers.
The LG parser is a rule-based dependency parser
with a broad coverage grammar of newspaper-type
English. It has no probabilistic component and does
not perform pruning of ambiguous alternatives dur-
ing parsing. Instead, the parser generates all parses
accepted by the grammar. Simple heuristics are ap-
plied to rank the alternative parses.
Here, we evaluate a recently introduced adap-
tation of LG to the biomedical domain, BioLG
(Pyysalo et al, 2006), incorporating the GENIA
POS tagger (Tsuruoka et al, 2005) as well as a num-
ber of modifications to lexical processing and the
grammar.
To facilitate the comparison of results with those
of Clegg and Shepherd, we use their modified subset
of GENIA Treebank.4 As 600 of the 1100 BioInfer
sentences have previously been used in the develop-
ment of the BioLG parser, we only use the remaining
500 blind sentences of BioInfer in the evaluation.
5.3 Parser performance
To evaluate the performance of the parsers, we de-
termined the precision, recall and F-measure by
comparing the parser output against the corpus gold
4http://chomsky-ext.cryst.bbk.ac.uk/
andrew/downloads.html
BioLG
scheme Prec. Rec. F
LG 78.2 77.2 77.7
SF 79.6 76.1 77.8
Table 2: BioLG performance on the BioInfer corpus
with and without the LG-SF conversion.
standard dependencies. The matching criterion re-
quired that the correct words are connected and
that the direction and type of the dependency are
correct. The dependency-based evaluation results
for the Charniak-Lease and BioLG parsers on the
GENIA and BioInfer corpora are shown in Table 1.
We note that Clegg and Shepherd (2007) report
77% F-score performance of Charniak-Lease on the
GENIA corpus, using the collapsed variant of the SF
scheme. We replicated their experiment using the
uncollapsed variant and found an F-score of 80%.
Therefore, most of the approximately 4% difference
compared to our finding reported in Table 1 is due
to this difference in the use of collapsing, with our
modifications to the SF scheme having a lesser ef-
fect. The decrease in measured performance caused
by the collapsing is, however, mostly an artifact
caused by merging several dependencies into one; a
single mistake of the parser can have a larger effect
on the performance measurement.
We find that while the performance of the
Charniak-Lease parser is approximately 2 percent-
age units better on GENIA than on BioInfer, for
BioLG we find the opposite effect, with performance
approximately 3 percentage units better on BioInfer.
Thus, both parsers perform better on the corpora
closer to their native scheme. We estimate that this
total 5 percentage unit divergence represents an up-
per limit to the evaluation bias introduced by the two
sets of conversion rules. We discuss the possible
causes for this divergence in Section 5.4.
To determine whether the differences between the
two parsers on the two corpora were statistically
significant, we used the Wilcoxon signed-ranks test
for F-score performance using the Bonferroni cor-
rection for multiple comparisons (N = 2), follow-
ing the recent recommendation of Dems?ar (2006).
We find that the Charniak-Lease parser outperforms
BioLG statistically significantly on both the GENIA
corpus (p ? 0.01) and on the BioInfer corpus
30
  Z   protein  but  not  c-myb  protein 
<nmod <dep
cc>
<nmod
conj>
  Z   protein  but  not  c-myb  protein 
<nmod dep>
cc>
<nmod
conj>
Figure 5: Example of divergence on the interpreta-
tion of the Stanford scheme. Above: GENIA and
Stanford conversion interpretation. Below: BioInfer
and LG-SF rules interpretation.
(p < 0.01). Thus, the relative performance of the
parsers can, in this case, be established even in the
presence of opposing conversion biases on the two
corpora.
In Table 2, we present an evaluation of the BioLG
parser with and without the LG-SF conversion,
specifically evaluating the effect of the conversion
presented in this study. Here we find a substantially
more stable performance, including even an increase
in precision. This further validates the quality of the
conversion rules.
Finally, we note that the processing time required
to perform the conversions is insignificant compared
to the time consumed by the parsers.
5.4 Discussion
Evaluating BioLG on GENIA and the Charniak-
Lease parser on BioInfer includes multiple sources
of divergence. In addition to parser errors, differ-
ences can be created by the LG-SF conversion and
the Stanford conversion. Moreover, in examining
the outputs we identified that a further source of
divergence is due to differing interpretations of the
Stanford scheme. One such difference is illustrated
in Figure 5. Here the BioLG parser with the LG-
SF conversion produces an analysis that differs from
the result of converting the GENIA Treebank analy-
sis by the Stanford conversion. This is due to the
Stanford conversion producing an apparently flawed
analysis that is not replicated by the LG-SF con-
version. In certain cases of this type, the lack of a
detailed definition of the SF scheme prevents from
distinguishing between conversion errors and inten-
tional analyses. This will necessarily lead to differ-
ing interpretations, complicating precise evaluation.
6 Conclusions
We have presented a step towards unifying syntactic
annotations under the Stanford dependency scheme
and assessed the feasibility of this unification by
developing and evaluating a conversion from Link
Grammar to the Stanford scheme. We find that a
highly reliable transformation can be created, giv-
ing a precision and recall of 98.0% and 96.2%, re-
spectively, when compared against our manually an-
notated gold standard version of the BioInfer cor-
pus. We also find that the performance of the BioLG
parser is not adversely affected by the conversion.
Given the clear benefits that the Stanford scheme
has for domain analysis, the conversion increases the
overall suitability of the parser to IE applications.
Based on these results, we conclude that converting
to the Stanford scheme is both feasible and practical.
Further, we have developed a version of the
BioInfer corpus annotated with the Stanford scheme,
thereby increasing the usability of the corpus. We
applied the LG-SF conversion to the original LG
BioInfer annotation and manually corrected the er-
rors. The high annotator agreement of above 97%
precision and recall confirms the stability of the SF
scheme.
We have also demonstrated that the unification
permits direct parser comparison that was previously
impossible. However, we found that there is a cer-
tain accumulation of errors caused by the conver-
sion, particularly in a case when two distinct rule
sets are applied. In our case, we estimate this error
to be on the order of several percentage units, never-
theless, we were able to establish the relative perfor-
mance of the parses with a strong statistical signif-
icance. These results demonstrate the utility of the
Stanford scheme as a unifying representation of syn-
tax. We note that an authoritative definition of the
Stanford scheme would further increase its value.
Acknowledgments
We would like to thank Erick Alphonse, Sophie
Aubin and Adeline Nazarenko for providing us with
the lp2lp software and the LLL conversion rules. We
would also like to thank Andrew Brian Clegg and
Adrian Shepherd for making available the data and
evaluation tools used in their parser evaluation. This
work was supported by the Academy of Finland.
31
References
Erick Alphonse, Sophie Aubin, Philippe Bessie`res, Gilles
Bisson, Thierry Hamon, Sandrine Laguarigue, Ade-
line Nazarenko, Alain-Pierre Manine, Claire Ne?dellec,
Mohamed Ould Abdel Vetah, Thierry Poibeau, and
Davy Weissenbacher. 2004. Event-Based Information
Extraction for the biomedical domain: the Caderige
project. In N. Collier, P. Ruch, and A. Nazarenko, ed-
itors, COLING NLPBA/BioNLP Workshop, pages 43?
49, Geneva, Switzerland.
Sophie Aubin, Adeline Nazarenko, and Claire Ne?dellec.
2005. Adapting a general parser to a sublanguage. In
G. Angelova, K. Bontcheva, R. Mitkov, N. Nicolov,
and N. Nikolov, editors, Proceedings of the Interna-
tional Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 05), Borovets, Bulgaria,
pages 89?93. Incoma, Bulgaria.
Sophie Aubin. 2005. LLL challenge - syntactic analysis
guidelines. Technical report, LIPN, Universite? Paris
Nord, Villetaneuse.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank ii
style. Technical report, Penn Treebank Project, Uni-
versity of Pennsylvania.
Eugene Charniak and Matthew Lease. 2005. Parsing
biomedical literature. In R. Dale, K. F. Wong, J. Su,
and O. Y. Kwong, editors, Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
gage Processing, Jeju Island, Korea, pages 58?69.
Eugene Charniak. 1999. A maximum-entropy-inspired
parser. Technical report, Brown University.
Andrew Brian Clegg and Adrian Shepherd. 2007.
Benchmarking natural-language parsers for biological
applications using dependency graphs. BMC Bioinfor-
matics, 8(1):24.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
N. Calzolari, K. Choukri, A. Gangemi, B. Maegaard,
J. Mariani, J. Odijk, and D. Tapias, editors, Proceed-
ings of the 5th International Conference on Language
Resources and Evaluation (LREC 2006), pages 449?
454.
Janez Dems?ar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30.
Jing Ding, Daniel Berleant, Jun Xu, and Andy W. Fulmer.
2003. Extracting biochemical interactions from med-
line using a link grammar parser. In B. Werner, editor,
Proceedings of the 15th IEEE International Confer-
ence on Tools with Artificial Intelligence, pages 467?
471. IEEE Computer Society, Los Alamitos, CA.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2007.
RelEx?Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus?a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19:i180?182.
Roger Levy and Galen Andrew. 2006. Tregex and Tsur-
geon: tools for querying and manipulating tree data
structures. In N. Calzolari, K. Choukri, A. Gangemi,
B. Maegaard, J. Mariani, J. Odijk, and D. Tapias, ed-
itors, Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 2231?2234.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97?114.
Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and
Adeline Nazarenko. 2006. Lexical adaptation of link
grammar to the biomedical sublanguage: a compara-
tive evaluation of three approaches. BMC Bioinfor-
matics, 7(Suppl 3).
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
James Dowdall, Andreas Persidis, and Ourania Kon-
stanti. 2004. Mining relations in the genia corpus. In
Proceedings of the Workshop W9 on Data Mining and
Text Mining for Bioinformatics (ECML/PKDD?04),
pages 61?68, Pisa, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Master?s
thesis, University of Zu?rich.
Daniel D. Sleator and Davy Temperley. 1993. Parsing
English with a Link Grammar. In Third International
Workshop on Parsing Technologies.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In P. Bozanis and
E. N. Houstis, editors, 10th Panhellenic Conference on
Informatics, volume 3746, pages 382?392.
32
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Graph Kernel for Protein-Protein Interaction Extraction
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio Pahikkala, Filip Ginter and Tapio Salakoski
Turku Centre for Computer Science
and Department of IT, University of Turku
Joukahaisenkatu 3-5
20520 Turku, Finland
firstname.lastname@utu.fi
Abstract
In this paper, we propose a graph kernel
based approach for the automated extraction
of protein-protein interactions (PPI) from sci-
entific literature. In contrast to earlier ap-
proaches to PPI extraction, the introduced all-
dependency-paths kernel has the capability
to consider full, general dependency graphs.
We evaluate the proposed method across five
publicly available PPI corpora providing the
most comprehensive evaluation done for a ma-
chine learning based PPI-extraction system.
Our method is shown to achieve state-of-the-
art performance with respect to comparable
evaluations, achieving 56.4 F-score and 84.8
AUC on the AImed corpus. Further, we iden-
tify several pitfalls that can make evaluations
of PPI-extraction systems incomparable, or
even invalid. These include incorrect cross-
validation strategies and problems related to
comparing F-score results achieved on differ-
ent evaluation resources.
1 Introduction
Automated protein-protein interaction (PPI) extrac-
tion from scientific literature is a task of significant
interest in the BioNLP field. The most commonly
addressed problem has been the extraction of binary
interactions, where the system identifies which pro-
tein pairs in a sentence have a biologically relevant
relationship between them. Proposed solutions in-
clude both hand-crafted rule-based systems and ma-
chine learning approaches (see e.g. (Bunescu et al,
2005)). A wide range of results have been reported
for the systems, but as we will show, differences in
evaluation resources, metrics and strategies make di-
rect comparison of these numbers problematic. Fur-
ther, the results gained from the BioCreative II eval-
uation, where the best performing system achieved
a 29% F-score (Hunter et al, 2008), suggest that the
problem of extracting binary protein protein interac-
tions is far from solved.
The public availability of large annotated PPI-
corpora such as AImed (Bunescu et al, 2005),
BioInfer (Pyysalo et al, 2007a) and GENIA (Kim
et al, 2008), provides an opportunity for building
PPI extraction systems automatically using machine
learning. A major challenge is how to supply the
learner with the contextual and syntactic informa-
tion needed to distinguish between interactions and
non-interactions. To address the ambiguity and vari-
ability of the natural language expressions used to
state PPI, several recent studies have focused on
the development, adaptation and application of NLP
tools for the biomedical domain. Many high-quality
domain-specific tools are now freely available, in-
cluding full parsers such as that introduced by Char-
niak and Lease (2005). Additionally, a number
of conversions from phrase structure parses to de-
pendency structures that make the relationships be-
tween words more directly accessible have been in-
troduced. These include conversions into represen-
tations such as the Stanford dependency scheme (de
Marneffe et al, 2006) that are explicitly designed for
information extraction purposes. However, special-
ized feature representations and kernels are required
to make learning from such structures possible.
Approaches such as subsequence kernels
(Bunescu and Mooney, 2006), tree kernels (Zelenko
1
interaction of P1 and P2
prep_of> conj_and>prep_of>
P1 is a P2 binding protein
<nn<nn
<det<cop
<nsubj
P1 fails to bind P2
<nsubj <aux dobj>xcomp>
<xsubj
Figure 1: Stanford dependency parses (?collapsed? rep-
resentation) where the shortest path, shown in bold, ex-
cludes important words.
et al, 2003) and shortest path kernels (Bunescu
and Mooney, 2005) have been proposed and suc-
cessfully used for relation extraction. However,
these methods lack the expressive power to consider
representations derived from general, possibly
cyclic, dependency graph structures, such as those
generated by the Stanford tools. The subsequence
kernel approach does not consider parses at all, and
the shortest path approach is limited to representing
only a single path in the full dependency graph,
which excludes relevant words even in many simple
cases (Figure 1). Tree kernels can represent more
complex structures, but are still restricted to tree
representations.
Lately, in the framework of kernel-based machine
learning methods there has been an increased in-
terest in designing kernel functions for graph data.
Building on the work of Ga?rtner et al (2003),
graph representations tailored for the task of depen-
dency parse ranking were proposed by Pahikkala et
al. (2006b). Though the proposed representations
are not directly applicable to the task of PPI extrac-
tion, they offer insight in how to learn from depen-
dency graphs. We develop a graph kernel approach
for PPI extraction based on these ideas.
We next define a graph representation suitable for
describing potential interactions and introduce a ker-
nel which makes efficient learning from a general,
unrestricted graph representation possible. Then we
provide a short description of the sparse regular-
ized least squares (sparse RLS) kernel-based ma-
chine learning method we use for PPI-extraction.
Further, we rigorously assess our method on five
publicly available PPI corpora, providing the first
broad cross-corpus evaluation with a machine learn-
ing approach to PPI extraction. Finally, we discuss
the effects that different evaluation strategies, choice
of corpus and applied metrics have on measured per-
formance, and conclude.
2 Method
We next present our graph representation, formalize
the notion of graph kernels, and present our learning
method of choice, the sparse RLS.
2.1 Graph encoding of sentence structure
As in most recent work on machine learning for PPI
extraction, we cast the task as learning a decision
function that determines for each unordered candi-
date pair of protein names occurring together in a
sentence whether the two proteins interact. In the
following, we first define the graph representation
used to represent an interaction candidate pair. We
then proceed to derive the kernel used to measure
the similarities of these graphs.
We assume that the input of our learning method
is a dependency parse of a sentence where a pair of
protein names is marked as the candidate interac-
tion for which an extraction decision must be made.
Based on this, we form a weighted, directed graph
that consists of two unconnected subgraphs. One
represents the dependency structure of the sentence,
and the other the linear order of the words (see Fig-
ure 2).
The first subgraph is built from the dependency
analysis. One vertex and an associated set of labels
is created in the graph for each token and for each
dependency. The vertices that represent tokens have
as labels the text and part-of-speech (POS) of the
token. To ensure generalization of the learned ex-
traction model, the labels of vertices that correspond
to protein names are replaced with PROT1, PROT2
or PROT, where PROT1 and PROT2 are the pair of
interest. The vertices that represent dependencies
are labeled with the type of the dependency. The
edges in the subgraph are defined so that each de-
pendency vertex is connected by an incoming edge
from the vertex representing its governor token, and
by an outgoing edge to the vertex representing its de-
2
Figure 2: Graph representation generated from an example sentence. The candidate interaction pair is marked as
PROT1 and PROT2, the third protein is marked as PROT. The shortest path between the proteins is shown in bold. In
the dependency based subgraph all nodes in a shortest path are specialized using a post-tag (IP). In the linear order
subgraph possible tags are (B)efore, (M)iddle, and (A)fter. For the other two candidate pairs in the sentence, graphs
with the same structure but different weights and labels would be generated.
pendent token. The graph thus represents the entire
sentence structure.
It is widely acknowledged that the words between
the candidate entities or connecting them in a syn-
tactic representation are particularly likely to carry
information regarding their relationship; (Bunescu
and Mooney, 2005) formalize this intuition for de-
pendency graphs as the shortest path hypothesis. We
apply this insight in two ways in the graph repre-
sentation: the labels of the nodes on the shortest
undirected paths connecting PROT1 and PROT2 are
differentiated from the labels outside the paths us-
ing a special tag. Further, the edges are assigned
weights; after limited preliminary experiments, we
chose a simple weighting scheme where all edges
on the shortest paths receive a weight of 0.9 and
other edges receive a weight of 0.3. The represen-
tation thus allows us to emphasize the shortest path
without completely disregarding potentially relevant
words outside of the path.
The second subgraph is built from the linear struc-
ture of the sentence. For each token, a second ver-
tex is created and the labels for the vertices are de-
rived from the texts, POS-tags and named entity tag-
ging as above. The labels of each word are special-
ized to denote whether the word appears before, in-
between, or after the protein pair of interest. Each
word node is connected by an edge to its succeed-
ing word, as determined by sentence order the of the
words. Each edge is given the weight 0.9.
2.2 The all-dependency-paths graph kernel
We next formalize the graph representation and
present the all-dependency-paths kernel. This ker-
nel can be considered as a practical instantiation of
the theoretical graph kernel framework introduced
by Ga?rtner et al (2003). Let V be the set of ver-
tices in the graph and L be the set of possible labels
vertices can have. We represent the graph with an
adjacency matrix A ? R|V |?|V |, whose rows and
columns are indexed by the vertices, and [A]i,j con-
tains the weight of the edge connecting vi ? V and
vj ? V if such an edge exists, and zero otherwise.
Further, we represent the labels as a label allocation
matrix L ? R|L|?|V | so that Li,j = 1 if the j-th
vertex has the i-th label and Li,j = 0 otherwise. Be-
cause only a very small fraction of all the possible
labels are ever assigned to any single node, this ma-
trix is extremely sparse.
It is well known that when an adjacency matrix is
multiplied with itself, each element [A2]i,j contains
the summed weight of paths from vertex vi to vertex
vj through one intervening vertex, that is, paths of
length two. Similarly, for any length n, the summed
weights from vi to vj can be determined by calculat-
ing [An]i,j .
Since we are interested not only in paths of one
specific length, it is natural to combine the effect of
paths of different lengths by summing the powers
of the adjacency matrices. We calculate the infinite
sum of the weights of all possible paths connecting
3
the vertices using the Neumann Series, defined as
(I ?A)?1 = I + A + A2 + ... =
??
k=0
Ak
if |A| < 1 where |A| is the spectral radius of A
(Meyer, 2000). From this sum we can form a new
adjacency matrix
W = (I ?A)?1 ? I .
The final adjacency matrix contains the summed
weights of all possible paths connecting the ver-
tices. The identity matrix is subtracted to remove
the paths of length zero, which would correspond to
self-loops.
Next, we present the graph kernel that utilizes the
graph representation defined previously. We define
an instance G representing a candidate interaction
as G = LWLT, where L and W are the label al-
location matrix and the final adjacency matrix cor-
responding to the graph representation of the candi-
date interaction.
Following Ga?rtner et al (2003) the graph kernel
is defined as
k(G?, G??) =
|L|?
i=1
|L|?
j=1
G?i,jG
??
i,j ,
where G? and G?? are two instances formed as de-
fined previously. The features can be thought as
combinations of labels from connected pairs of ver-
tices, with a value that represents the strength of
their connection. In practical implementations, the
full G matrices, which consist mostly of zeroes, are
never explicitly formed. Rather, only the non-zero
elements are stored in memory and used when cal-
culating the kernels.
2.3 Scalable learning with Sparse RLS
RLS is a state-of-the-art kernel-based machine
learning method which has been shown to have
comparable performance to support vector machines
(Rifkin et al, 2003). We choose the sparse version
of the algorithm, also known as subset of regressors,
as it allows us to scale up the method to very large
training set sizes. Sparse RLS also has the property
that it is possible to perform cross-validation and
regularization parameter selection so that their time
complexities are negligible compared to the training
complexity. These efficient methods are analogous
to the ones proposed by Pahikkala et al (2006a) for
the basic RLS regression.
We now briefly present the basic sparse RLS al-
gorithm. Let m denote the training set size and
M = {1, . . . ,m} an index set in which the indices
refer to the examples in the training set. Instead of
allowing functions that can be expressed as a linear
combination over the whole training set, as in the
case of basic RLS regression, we only allow func-
tions of the following restricted type:
f(?) =
?
i?B
aik(?, xi), (1)
where k is the kernel function, xi are training data
points, ai ? R are weights, and the set indexing the
basis vectors B ? M is selected in advance. The co-
efficients ai that determine (1) are obtained by min-
imizing
m?
i=1
(yi ?
?
j?B
ajk(xi, xj))
2 + ?
?
i,j?B
aiajk(xi, xj),
where the first term is the squared loss function, the
second term is the regularizer, and ? ? R+ is a reg-
ularization parameter. Note that all the training in-
stances are used for determining the coefficient vec-
tor. The minimizer is obtained by solving the corre-
sponding system of linear equations, which can be
performed in O(m|B|2) time.
We set the maximum number of basis vectors to
4000 in all experiments in this study. The subset
is selected randomly when the training set size ex-
ceeds this number. Other methods for the selection
of the basis vectors were considered by Rifkin et
al. (2003), who however reported that the random
selection worked as well as the more sophisticated
approaches.
3 Experimental evaluation
We next describe the evaluation resources and met-
rics used, provide a comprehensive evaluation of our
method across five PPI corpora, and compare our re-
sults to earlier work. Further, we discuss the chal-
lenges inherent in providing a valid method evalua-
tion and propose solutions.
4
Statistics Graph Kernel Co-occ.
Corpus #POS. #NEG. P R F ?F AUC ?AUC P F
AIMed 1000 4834 0.529 0.618 0.564 0.050 0.848 0.023 0.178 0.301
BioInfer 1370 8924 0.477 0.599 0.529 0.053 0.849 0.065 0.135 0.237
HPRD50 163 270 0.643 0.658 0.634 0.114 0.797 0.063 0.389 0.554
IEPA 335 482 0.696 0.827 0.751 0.070 0.851 0.051 0.408 0.576
LLL 164 166 0.725 0.872 0.768 0.178 0.834 0.122 0.559 0.703
Table 1: Counts of positive and negative examples in the corpora and (P)recision, (R)ecall (F)-score and AUC for the
graph kernel, with standard deviations provided for F and AUC.
3.1 Corpora and evaluation criteria
We evaluate our method using five publicly avail-
able corpora that contain PPI interaction annotation:
AImed (Bunescu et al, 2005), BioInfer (Pyysalo et
al., 2007a), HPRD50 (Fundel et al, 2007), IEPA
(Ding et al, 2002) and LLL (Ne?dellec, 2005). All
the corpora were processed to a common format us-
ing transformations1 that we have introduced ear-
lier (Pyysalo et al, 2008). We parse these cor-
pora with the Charniak-Lease parser (Charniak and
Lease, 2005), which has been found to perform best
among a number of parsers tested in recent domain
evaluations (Clegg and Shepherd, 2007; Pyysalo et
al., 2007b). The Charniak-Lease phrase structure
parses are transformed into the collapsed Stanford
dependency scheme using the Stanford tools (de
Marneffe et al, 2006). We cast the PPI extraction
task as binary classification, where protein pairs that
are stated to interact are positive examples and other
co-occuring pairs negative. Thus, from each sen-
tence,
(n
2
)
examples are generated, where n is the
number of occurrences of protein names in the sen-
tence. Finally, we form the graph representation de-
scribed earlier for each candidate interaction.
We evaluate the method with 10-fold document-
level cross-validation on all of the corpora. This
guarantees the maximal use of the available data,
and also allows comparison to relevant earlier work.
In particular, on the AImed corpus we apply the ex-
act same 10-fold split that was used by Bunescu et
al. (2006) and Giuliano et al (2006). Performance
is measured according to the following criteria: in-
teractions are considered untyped, undirected pair-
wise relations between specific protein mentions,
that is, if the same protein name occurs multiple
1Available at http://mars.cs.utu.fi/PPICorpora.
times in a sentence, the correct interactions must be
extracted for each occurrence. Further, we do not
consider self-interactions as candidates and remove
them from the corpora prior to evaluation.
The majority of PPI extraction system evaluations
use the balanced F-score measure for quantifying the
performance of the systems. This metric is defined
as F = 2prp+r , where p is precision and r recall. Like-
wise, we provide F-score, precision, and recall val-
ues in our evaluation. It should be noted that F-score
is very sensitive to the underlying positive/negative
pair distribution of the corpus ? a property whose
impact on evaluation is discussed in detail below. As
an alternative to F-score, we also evaluate the per-
formance of our system using the area under the re-
ceiver operating characteristics curve (AUC) mea-
sure (Hanley and McNeil, 1982). AUC has the im-
portant property that it is invariant to the class dis-
tribution of the used dataset. Due to this and other
beneficial properties for comparative evaluation, the
usage of AUC for performance evaluation has been
recently advocated in the machine learning commu-
nity (see e.g. (Bradley, 1997)). Formally, AUC can
be defined as
AUC =
?m+
i=1
?m?
j=1H(xi ? yi)
m+m?
,
where m+ and m? are the numbers of positive
and negative examples, respectively, and x1,...,xm+
are the outputs of the system for the positive, and
y1,...,ym? for the negative examples, and
H(r) =
?
?
?
1, if r > 0
0.5, if r = 0
0, otherwise.
The measure corresponds to the probability that
given a randomly chosen positive and negative ex-
5
ample, the system will be able to correctly disin-
guish which one is which.
3.2 Performance across corpora
The performance of our method on the five corpora
for the various metrics is presented in Table 1. For
reference, we show also the performance of the co-
occurrence (or all-true) baseline, which simply as-
signs each candidate into the interaction class. The
recall of the co-occurrence method is trivially 100%,
and in terms of AUC it has a score of 0.5, the ran-
dom baseline. All the numbers in Table 1 are aver-
ages taken over the ten folds. One should note that
because of the non-linearity of the F-score measure,
the average precision and recall will not produce ex-
actly the average F.
The results hold several interesting findings. First,
we briefly observe that on the AImed corpus, which
has recently been applied in numerous evaluations
(S?tre et al, 2008) and can be seen as an emerging
de facto standard for PPI extraction method evalua-
tion, the method achieves an F-score performance of
56.4%. As we argue in more detail below, this level
of performance is comparable to the state-of-the-art
in machine learning based PPI extraction. For the
other large corpus, BioInfer, F-score performance is
slightly lower.
Second, we observe that the F-score performance
of the method varies strikingly between the differ-
ent corpora, with results on IEPA and LLL approx-
imately 20 percentage units higher than on AImed
and BioInfer, despite the larger size of the latter two.
In our previous work we have observed similar re-
sults with a rule-based extraction method (Pyysalo et
al., 2008). As the first broad cross-corpus evaluation
using a state-of-the-art machine learning method for
PPI extraction, our results support and extend the
key finding that F-score performance results mea-
sured on different corpora cannot, in general, be
meaningfully compared.
The co-occurrence baseline numbers indicate one
reason for the high F-score variance between the
corpora. The F-score metric is not invariant to the
distribution of positive and negative examples: for
example, halving the number of negative test exam-
ples is expected to approximately halve the number
of false positives at a given recall point. Thus, the
greater the fraction of true interactions in a corpus
is, the easier it is to reach high performance in terms
of F-score. This is reflected in co-occurrence re-
sults, which range from 24% to 70% depending on
the class distribution of the corpus.
This is a critical weakness of the F-score metric in
cross-corpus comparisons as, for example, the frac-
tion of true interactions out of all candidates is 50%
on the LLL corpus but only 17% on AImed. By
contrast to the large differences in performance mea-
sured using F-score, we find that for the distribution-
invariant AUC measure the performance for all of
the AImed, BioInfer, IEPA, and LLL corpora falls in
the narrow range of 83-85%. In terms of AUC, per-
formance on the HPRD50 corpus is an outlier, being
approximately three percentage units lower than for
any other corpus. Nevertheless, the results provide a
strong argument in favor of applying the AUC met-
ric instead of, or in addition to, F-score. AUC is also
more stable in terms of variance.
Finally, we note that the similar performance in
terms of AUC for corpora with as widely differing
sizes as LLL and BioInfer indicates that past a rel-
atively modest number of examples, increasing cor-
pus size has a surprisingly small effect on the perfor-
mance of the method. A similar finding can be seen,
for example, in the relatively flat learning curve of
Giuliano et al (2006). While the issue requires fur-
ther investigation, these results suggest that there
may be more value in investing effort in develop-
ing better learning methods as opposed to larger cor-
pora.
3.3 Performance compared to other methods
We next discuss the performance of our method
compared to other methods introduced in the liter-
ature and the challenges of meaningful comparison,
where we identify three major issues.
First, as indicated by the results above, differ-
ences in the makeup of different corpora render
cross-corpus comparisons in terms of F-score es-
sentially meaningless. As F-score is typically the
only metric for which results are reported in the PPI
extraction literature, we are limited to comparing
against results on single corpora. We consider the
AImed and BioInfer evaluations to be the most rele-
vant ones, as these corpora are sufficiently large for
training and reliably testing machine learning meth-
ods. As the present study is, to the best of our knowl-
6
P R F
(Giuliano et al, 2006) 60.9% 57.2% 59.0%
All-dependency-paths graph kernel 52.9% 61.8% 56.4%
(Bunescu and Mooney, 2006) 65.0% 46.4% 54.2%
(S?tre et al, 2008) 64.3% 44.1% 52.0%
(Mitsumori et al, 2006) 54.2% 42.6% 47.7%
(Yakushiji et al, 2005) 33.7% 33.1% 33.4%
Table 2: (P)recision, (R)ecall and (F)-score results for methods evaluated on AImed with the correct cross-validation
methodology.
edge, the first to report machine learning method
performance on BioInfer, we will focus on AImed
in the following comparison.
Second, the cross-validation strategy used in eval-
uation has a large impact on measured performance.
In earlier system evaluations, two major strategies
for defining the splits used in cross-validation can
be observed. The approach used by Bunescu and
Mooney (2006), which we consider the correct one,
is to split the data into folds on level of docu-
ments. This guarantees that all pairs generated from
the same document are always either in the train-
ing set or in the test set. Another approach is to
pool all the generated pairs together, and then ran-
domly split them to folds. To illustrate the signifi-
cance of this choice, consider two interaction candi-
dates extracted from the same sentence, e.g. from
a statement of the form ?P1 and P2 [. . . ] P3?,
where ?[. . . ]? is any statement of interaction or non-
interaction. Due to the near-identity of contexts, a
machine learning method will easily learn to predict
that the label of the pair (P1, P2) should match that
of (P1, P3). However, such ?learning? will clearly
not generalize. This approach must thus be consid-
ered invalid, because allowing pairs generated from
same sentences to appear in different folds leads to
an information leak between the training and test
sets. S?tre et al (2008) observed that adopting the
latter cross-validation strategy on AImed could lead
up to 18 F-score percentage unit overestimation of
performance. For this reason, we will not consider
results listed in the ?False 10-fold cross-validation?
table (2b) of S?tre et al (2008).
With these restrictions in place, we now turn to
comparison with relevant results reported in related
research, summarized in Table 2. We note that
Bunescu and Mooney (2006) only applied evalua-
tion criteria where it is enough to extract only one
occurrence of each mention of an interaction from
each abstract, while the other results shown were
evaluated using the same criteria as applied here.
The former approach can produce higher perfor-
mance: the evaluation of Giuliano et al (2006) in-
cludes both alternatives, and their method achieves
an F-score of 63.9% under the former criterion,
which they term One Answer per Relation in a
given Document (OARD). Our method outperforms
most studies using similar evaluation methodology,
with the exception being the approach of Giuliano
et al (2006). This result is somewhat surprising,
as the method proposed by Giuliano does not ap-
ply any form of parsing but relies instead only on
the sequential order of the words. This brings us
to our third point regarding comparability of meth-
ods. As pointed out by S?tre et al (2008), the
AImed corpus allows remarkably different ?inter-
pretations? regarding the number of interacting and
non-interacting pairs. For example, where we have
identified 1000 interacting and 4834 non-interacting
protein pairs in AImed, in the data used by Giuliano
there are eight more interacting and 200 fewer non-
interacting pairs. The corpus can also be prepro-
cessed in a number of ways. In particular we noticed
that whereas protein names are always blinded in our
data, in the data used by Giuliano protein names are
sometimes partly left visible. As Giuliano has gen-
erously made his method implementation available2,
we were able to test the performance of his system
on the data we used in our experiments. This re-
sulted in an F-score of 52.4%.
Finally, there remains an issue of parameter se-
lection. For sparse RLS the values of the regular-
2Available at http://tcc.itc.it/research/
textec/tools-resources/jsre.html.
7
ization parameter ? and the decision threshold sep-
arating the positive and negative classes must be
chosen, which can be problematic when no sepa-
rate data for choosing them is available. Choos-
ing from several parameter values the ones that give
best results in testing, or picking the best point
from a precision/recall curve when evaluating in
terms of F-score, will lead to an overoptimistic eval-
uation of performance. This issue has often not
been addressed in earlier evaluations that do cross-
validation on a whole corpus. We choose the pa-
rameters by doing further leave-one-document-out
cross-validation within each round of 10-fold-cross-
validation, on the nine folds that constitute the train-
ing set.
As a conclusion, we observe the results achieved
with the all-dependency-paths kernel to be state-of-
the-art level. However, differences in evaluation
strategies and the large variance exhibited in the re-
sults make it impossible to state which of the sys-
tems considered can be expected in general to per-
form best. We encourage future PPI-system evalua-
tions to report AUC and F-score results over mul-
tiple corpora, following clearly defined evaluation
strategies, to bring further clarity to this issue.
4 Conclusions and future work
In this paper we have proposed a graph kernel
approach to extracting protein-protein interactions,
which captures the information in unrestricted de-
pendency graphs to a format that kernel based learn-
ing algorithms can process. The method combines
syntactic analysis with a representation of the lin-
ear order of the sentence, and considers all possi-
ble paths connecting any two vertices in the result-
ing graph. We demonstrate state-of-the art perfor-
mance for the approach. All software developed in
the course of this study is made publicly available at
http://mars.cs.utu.fi/PPICorpora.
We identify a number of issues which make re-
sults achieved with different evaluation strategies
and resources incomparable, or even incorrect. In
our experimental design we consider the problems
related to differences across corpora, the effects dif-
ferent cross-validation strategies have, and how pa-
rameter selection can be done. Our recommendation
is to provide evaluations over different corpora, to
use document-level cross-validation and to always
selected parameters on the training set.
We draw attention to the behaviour of the F-score
metric over corpora with differing pair distributions.
The higher the relative frequency of interacting pairs
is, the higher the performance can be expected to
be. This is noticed both for the graph kernel method
and for the naive co-occurrence baseline. Indeed,
the strategy of just stating that all pairs interact leads
to as high result as 70% F-score on one of the cor-
pora. We consider AUC as an alternative measure
that does not exhibit such behaviour, as it is invari-
ant to the distribution of pairs. The AUC metric is
much more stable across all the corpora, and never
gives better results than random for approaches such
as the naive co-occurrence.
Though we only consider binary interactions in
this work, the graph representations have the prop-
erty that they could be used to represent more com-
plex structures than pairs. The availability of cor-
pora that annotate complex interactions, such as the
full BioInfer and GENIA, makes training a PPI ex-
traction system for extracting complex interactions
an important avenue of future research. However,
how to avoid the combinatorial explosion following
from considering triplets, quartets etc. remains an
open question. Also, the performance of the cur-
rent approaches may need to be yet improved before
extending them to recognize complex interactions.
Acknowledgements
We would like to thank Razvan Bunescu, Claudio
Giuliano and Rune S?tre for their generous assis-
tance in providing us with data, software and infor-
mation about their work on PPI extraction. Further,
we thank CSC, the Finnish IT center for science,
for providing us extensive computational resources.
This work has been supported by the Academy of
Finland and the Finnish Funding Agency for Tech-
nology and Innovation, Tekes.
References
Andrew P. Bradley. 1997. The use of the area under
the ROC curve in the evaluation of machine learning
algorithms. Pattern Recognition, 30(7):1145?1159.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT/EMNLP?05, pages 724?731.
8
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Proceedings
of NIPS?05, pages 171?178. MIT Press.
Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Ku-
mar Ramani, and Yuk Wah Wong. 2005. Compar-
ative experiments on learning information extractors
for proteins and their interactions. Artif Intell Med,
33(2):139?155.
Eugene Charniak and Matthew Lease. 2005. Parsing
biomedical literature. In Proceedings of IJCNLP?05,
pages 58?69.
Andrew Brian Clegg and Adrian Shepherd. 2007.
Benchmarking natural-language parsers for biological
applications using dependency graphs. BMC Bioinfor-
matics, 8(1):24.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC?06, pages 449?454.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele. 2002.
Mining MEDLINE: abstracts, sentences, or phrases?
In Proceedings of PSB?02, pages 326?337.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2007.
RelEx?Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Thomas Ga?rtner, Peter A. Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and efficient
alternatives. In COLT?03, pages 129?143. Springer.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of EACL?06.
James A. Hanley and B. J. McNeil. 1982. The meaning
and use of the area under a receiver operating charac-
teristic (roc) curve. Radiology, 143(1):29?36.
Lawrence Hunter, Zhiyong Lu, James Firby, William A.
Baumgartner, Helen L Johnson, Philip V. Ogren, and
K. Bretonnel Cohen. 2008. OpenDMAP: An open-
source, ontology-driven concept analysis engine, with
applications to capturing knowledge regarding protein
transport, protein interactions and cell-specific gene
expression. BMC Bioinformatics, 9(78).
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Carl D. Meyer. 2000. Matrix analysis and applied linear
algebra. Society for Industrial and Applied Mathe-
matics.
Tomohiro Mitsumori, Masaki Murata, Yasushi Fukuda,
Kouichi Doi, and Hirohumi Doi. 2006. Extracting
protein-protein interaction information from biomed-
ical text with svm. IEICE - Trans. Inf. Syst., E89-
D(8):2464?2466.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceedings
of LLL?05.
Tapio Pahikkala, Jorma Boberg, and Tapio Salakoski.
2006a. Fast n-fold cross-validation for regularized
least-squares. In Proceedings of SCAI?06, pages 83?
90.
Tapio Pahikkala, Evgeni Tsivtsivadze, Jorma Boberg, and
Tapio Salakoski. 2006b. Graph kernels versus graph
representations: a case study in parse ranking. In Pro-
ceedings of the ECML/PKDD?06 workshop on Mining
and Learning with Graphs.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007a. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007b. On the unification of syntactic annotations un-
der the stanford dependency scheme: A case study on
BioInfer and GENIA. In Proceedings of BioNLP?07,
pages 25?32.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC Bioinformatics, special issue,
9(Suppl 3):S6.
Ryan Rifkin, Gene Yeo, and Tomaso Poggio, 2003. Reg-
ularized Least-squares Classification, volume 190 of
NATO Science Series III: Computer and System Sci-
ences, chapter 7, pages 131?154. IOS Press.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2008. Syn-
tactic features for protein-protein interaction extrac-
tion. In Proceedings of LBM?07, volume 319, pages
6.1?6.14.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun?ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns. In
Proceedings of SMBM?05, pages 60?69.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. J. Mach. Learn. Res., 3:1083?1106.
9
Proceedings of the Workshop on BioNLP: Shared Task, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting Complex Biological Events with Rich Graph-Based Feature Sets
Jari Bjo?rne,1 Juho Heimonen,1,2 Filip Ginter,1 Antti Airola,1,2
Tapio Pahikkala1 and Tapio Salakoski1,2
1Department of Information Technology, University of Turku
2Turku Centre for Computer Science (TUCS)
Joukahaisenkatu 3-5, 20520 Turku, Finland
firstname.lastname@utu.fi
Abstract
We describe a system for extracting com-
plex events among genes and proteins from
biomedical literature, developed in context of
the BioNLP?09 Shared Task on Event Extrac-
tion. For each event, its text trigger, class, and
arguments are extracted. In contrast to the pre-
vailing approaches in the domain, events can
be arguments of other events, resulting in a
nested structure that better captures the under-
lying biological statements. We divide the task
into independent steps which we approach as
machine learning problems. We define a wide
array of features and in particular make ex-
tensive use of dependency parse graphs. A
rule-based post-processing step is used to re-
fine the output in accordance with the restric-
tions of the extraction task. In the shared task
evaluation, the system achieved an F-score of
51.95% on the primary task, the best perfor-
mance among the participants.
1 Introduction
In this paper, we present the best-performing system
in the primary task of the BioNLP?09 Shared Task
on Event Extraction (Kim et al, 2009).1 The pur-
pose of this shared task was to competitively eval-
uate information extraction systems targeting com-
plex events in the biomedical domain. Such an eval-
uation helps to establish the relative merits of com-
peting approaches, allowing direct comparability of
results in a controlled setting. The shared task was
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask
the first competitive evaluation of its kind in the
BioNLP field as the extraction of complex events
became possible only recently with the introduction
of corpora containing the necessary annotation: the
GENIA event corpus (Kim et al, 2008a) and the
BioInfer corpus (Pyysalo et al, 2007).
The objective of the primary task (Task 1) was
to detect biologically relevant events such as pro-
tein binding and phosphorylation, given only anno-
tation of named entities. For each event, its class,
trigger expression in the text, and arguments need to
be extracted. The task follows the recent movement
in BioNLP towards the extraction of semantically
typed, complex events the arguments of which can
also be other events. This results in a nested struc-
ture that captures the underlying biological state-
ments more accurately compared to the prevailing
approach of merely detecting binary interactions of
pairs of biological entities.
Our system is characterized by heavy reliance
on efficient, state-of-the-art machine learning tech-
niques and a wide array of features derived from
a full dependency analysis of each sentence. The
system is a pipeline of three major processing steps:
trigger recognition, argument detection and seman-
tic post-processing. By separating trigger recog-
nition from argument detection, we can use meth-
ods familiar from named entity recognition to tag
words as event triggers. Event argument detection
then becomes the task of predicting for each trigger?
trigger or trigger?named entity pair whether it cor-
responds to an actual instantiation of an event argu-
ment. Both steps can thus be approached as classi-
fication tasks. In contrast, semantic post-processing
10
Sentence?splitting
Tokenization
Parsing
Conversion?to?graph?
representation
Trigger?detection
(multi?class?SVM)
System?output
Semantic?post?processing
(rule?based)
Edge?detection
(multi?class?SVM)
Input?data
Figure 1: The main components of the system.
is rule-based, directly implementing argument type
constraints following from the definition of the task.
In the following sections, we present the imple-
mentation of the three stages of our information ex-
traction system in detail, and provide insights into
why we chose the approach we did. We also discuss
alternate directions we followed but that did not im-
prove performance. Finally, we analyze the overall
performance of our system in the shared task as well
as evaluate its components individually.
2 The system description
The overall architecture of the system is shown
in Figure 1. All steps in the system process one
sentence at a time. Since 95% of all annotated
events are fully contained within a single sentence,
this does not incur a large performance penalty but
greatly reduces the size and complexity of the ma-
chine learning problems.
2.1 Graph representation
We represent the extraction target in terms of seman-
tic networks, graphs where the nodes correspond
to named entities and events, and the edges corre-
spond to event arguments. The shared task can then
be viewed as the problem of finding the nodes and
edges of this graph. For instance, nested events are
naturally represented through edges connecting two
event nodes. The graph representation of an exam-
ple sentence is illustrated in Figure 2D.
We have previously used this graph representa-
tion for information extraction (Heimonen et al,
2008; Bjo?rne et al, 2009) as well as for establishing
the connection between events and syntactic depen-
dency parses in the Stanford scheme of de Marneffe
and Manning (2008) (Bjo?rne et al, 2008).
2.2 Trigger detection
We cast trigger detection as a token labeling prob-
lem, that is, each token is assigned to an event class,
or a negative class if it does not belong to a trig-
ger. Triggers are then formed based on the predicted
classes of the individual tokens. Since 92% of all
triggers in the data consist of a single token, adjacent
tokens with the same class prediction form a single
trigger only in case that the resulting string occurs
as a trigger in the training data. An event node is
created for each detected trigger (Figure 2B).
In rare cases, the triggers of events of different
class share a token, thus the token belongs to sev-
eral separate classes. To be able to approach trigger
detection as a multi-class classification task where
each token is given a single prediction, we intro-
duce combined classes as needed. For instance the
class gene expression/positive regulation denotes to-
kens that act as a trigger to two events of the two
respective classes. Note that this implies that the
trigger detection step produces at most one event
node per class for any detected trigger. In the shared
task, however, multiple events of the same class can
share the same trigger. For instance, the trigger in-
volves in Figure 2 corresponds to two separate regu-
lation events. A separate post-processing step is in-
troduced after event argument detection to duplicate
event nodes as necessary (see Section 2.4).
Due to the nature of the GENIA event annota-
tion principles, trigger detection cannot be easily re-
duced to a simple dictionary lookup of trigger ex-
pressions for two main reasons. First, a number of
common textual expressions act as event triggers in
some cases, but not in other cases. For example,
only 28% of the instances of the expression activates
are triggers for a positive regulation event while the
remaining 72% are not triggers for any event. Sec-
ond, a single expression may be associated with var-
ious event classes. For example, the instances of the
token overexpression are evenly distributed among
11
Regulation
Protein
IL-4 gene
Regulation
regulation
Regulation
involves
Protein
NFAT1 and
Protein
NFAT2 .
<Theme <Theme Cause> Cause>
NN NN NN VBZ NN CC .
<nn conj_and><nn dobj><nsubj
NN
Protein
IL-4 gene
Regulation
regulation
Regulation
involves
Protein
NFAT1 and
Protein
NFAT2 .
<Theme <Theme Cause>
Cause><Theme
node duplication
pars
e
T29   Regulation   regulationT30   Regulation   involvesE10   Regulation:T29   Theme:T7E11   Regulation:T30   Theme:E10   Cause:T9E12   Regulation:T30   Theme:E10   Cause:T8
equivalent
D
C
E
T7     Protein   IL-4T8     Protein   NFAT1T9     Protein   NFAT2
Protein
IL-4 gene
Regulation
regulation
Regulation
involves
Protein
NFAT1 and
Protein
NFAT2 .
Protein
IL-4 gene regulation involves
Protein
NFAT1 and
Protein
NFAT2 .
edge detection
trigger recognition
Trai
ning
 Dat
a Pr
epa
ratio
n
B
A
Eve
nt E
xtra
ction
dobj>
Figure 2: An example sentence from Shared Task document 10069428 (simplified). A) Named entities are given.
B) Triggers are detected and corresponding event nodes are created. C) Event argument edges are predicted between
nodes. The result is a sentence-level semantic network. D) One node may denote multiple events of the same class,
therefore nodes are duplicated in the semantic post-processing step. E) The resulting graph can be losslessly trans-
formed into the Shared Task event annotation. Training data for the trigger recognizer includes named entity annotation
(A) and for the edge detector the semantic network with no node duplication (C).
gene expression, positive regulation, and the nega-
tive class. In light of these properties, we address
trigger detection with a multi-class support vector
machine (SVM) classifier that assigns event classes
to individual tokens, one at a time. This is in con-
trast to sequence labeling problems such as named
entity recognition, where a sequential model is typ-
ically employed. The classifier is trained on gold-
standard triggers from the training data and incorpo-
rates a wide array of features capturing the proper-
ties of the token to be classified, both its linear and
dependency context, and the named entities within
the sentence.
Token features include binary tests for capital-
ization, presence of punctuation or numeric charac-
ters, stem using the Porter stemmer (Porter, 1980),
character bigrams and trigrams, and presence of the
token in a gazetteer of known trigger expressions
and their classes, extracted from the training data.
Token features are generated not only for the token
to be classified, but also for tokens in the immediate
linear context and dependency context (tokens that
govern or depend on the token to be classified).
Frequency features include the number of
named entities in the sentence and in a linear win-
dow around the token in question as well as bag-of-
word counts of token texts in the sentence.
Dependency chains up to depth of three are
constructed, starting from the token to be classified.
At each depth, both token features and dependency
type are included, as well as the sequence of depen-
dency types in the chain.
The trigger detector used in the shared task is
in fact a weighted combination of two indepen-
12
dent SVM trigger detectors, both based on the same
multi-class classification principle and somewhat
different feature sets.2 The predictions of the two
trigger detectors are combined as follows. For each
trigger detector and each token, the classifier confi-
dence scores of the top five classes are re-normalized
into the [0, 1] interval. The renormalized confidence
scores of the two detectors are then linearly inter-
polated using a parameter ?, 0 ? ? ? 1, whose
value is set experimentally on the development set,
as discussed below.
Setting the correct precision?recall trade-off in
trigger detection is very important. On one hand,
any trigger left undetected directly implies a false
negative event. On the other hand, the edge detec-
tor is trained on gold standard data where there are
no event nodes without arguments, which creates a
bias toward predicting edges for any event node the
edge detector is presented with. On the develop-
ment set, essentially all predicted event nodes are
given at least one argument edge. We optimize the
precision?recall trade-off explicitly by introducing a
parameter ?, 0 ? ?, that multiplies the classifier
confidence score given to the negative class, that is,
the ?no trigger? class. When ? < 1, the confidence
of the negative class is decreased, thus increasing
the possibility of a given token forming a trigger,
and consequently increasing the recall of the trigger
detector (naturally, at the expense of its precision).
Both trigger detection parameters, the interpola-
tion weight ? and the precision?recall trade-off pa-
rameter ?, are set experimentally using a grid search
to find the globally optimal performance of the en-
tire system on the development set, using the shared
task performance metric. The parameters are thus
not set to optimize the performance of trigger detec-
tion in isolation; they are rather set to optimize the
performance of the whole system.
2.3 Edge detection
After trigger detection, edge detection is used to pre-
dict the edges of the semantic graph, thus extracting
event arguments. Like the trigger detector, the edge
detector is based on a multi-class SVM classifier.
We generate examples for all potential edges, which
2This design should be considered an artifact of the time-
constrained, experiment-driven development of the system
rather than a principled design choice.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
0 1 2 3 4 5 6 7 8 9 10 >10
Pro
por
tion
 of 
edg
es [%
]
Edge length
dependency distancelinear distance
Figure 3: The distribution of event argument edge lengths
measured as the number of dependencies on the shortest
dependency path between the edge terminal nodes, con-
trasted with edge lengths measured as the linear token
distance.
are always directed from an event node to another
event node (event nesting) or from an event node to
a named entity node. Each example is then classified
as theme, cause, or a negative denoting the absence
of an edge between the two nodes in the given di-
rection. It should be noted that even though event
nodes often require multiple outgoing edges corre-
sponding to multiple event arguments, all edges are
predicted independently and are not affected by pos-
itive or negative classifications of other edges.
The feature set makes extensive use of syntac-
tic dependencies, in line with many recent stud-
ies in biomedical information extraction (see, e.g.
(Kim et al, 2008b; Miwa et al, 2008; Airola et al,
2008; Van Landeghem et al, 2008; Katrenko and
Adriaans, 2008)). The central concept in generat-
ing features of potential event argument edges is the
shortest undirected path of syntactic dependencies
in the Stanford scheme parse of the sentence which
we assume to accurately capture the relationship ex-
pressed by the edge. In Figure 3, we show that the
distances among event and named entity nodes in
terms of shortest dependency path length are con-
siderably shorter than in terms of their linear order in
the sentence. The end points of the path are the syn-
tactic head tokens of the two named entities or event
triggers. The head tokens are identified using a sim-
ple heuristic. Where multiple shortest paths exist,
all are considered. Most features are built by com-
bining the attributes of multiple tokens (token text,
13
POS tag and entity or event class, such as protein or
binding) or dependencies (type such as subject and
direction relative to surrounding tokens).
N-grams are generated by merging the at-
tributes of 2?4 consecutive tokens. N-grams are also
built for consecutive dependencies. Additional tri-
grams are built for each token and its two flank-
ing dependencies, as well as for each dependency
and its two flanking tokens. These N-grams are de-
fined in the direction of the potential event argument
edge. To take into account the varying directions
of the dependencies, each pair of consecutive tokens
forms an additional bigram defining their governor-
dependent relationship.
Individual component features are defined for
each token and edge in a path based on their
attributes which are also combined with the to-
ken/edge position at either the interior or the end of
the path. Edge attributes are combined with their di-
rection relative to the path.
Semantic node features are built by directly
combining the attributes of the two terminal
event/entity nodes of the potential event argument
edge. These features concatenate both the specific
types of the nodes (e.g. protein or binding) as well
as their categories (event or named entity). Finally,
if the events/entities have the same head token, this
self-loop is explicitly defined as a feature.
Frequency features include the length of the
shortest path as an integer-valued feature as well as
an explicit binary feature for each length. The num-
ber of named entities and event nodes, per type, in
the sentence are defined for each example.
We have used this type of edge detector with a
largely similar feature set previously (Bjo?rne et al,
2009). Also, many of these features are standard
in relation extraction studies (see, e.g., Buyko et al
(2008)).
2.4 Semantic post-processing
The semantic graph produced by the trigger and
edge detection steps is not final. In particular, it
may contain event nodes with an improper combi-
nation of arguments, or no arguments whatsoever.
Additionally, as discussed in Section 2.2, if there are
events of the same class with the same trigger, they
are represented by a single node. Therefore, we in-
troduce a rule-based post-processing step to refine
Figure 4: Example of event duplication. A) All theme?
cause combinations are generated for regulation events.
B) A heuristic is applied to decide how theme arguments
of binding events should be grouped.
the graph, using the restrictions on event argument
types and combinations defined in the shared task.
In Task 1, the allowed argument edges in the
graph are 1) theme from an event to a named en-
tity, 2) theme or cause from a regulation event (or its
subclasses) to an event or a named entity. Edges cor-
responding to invalid arguments are removed. Also,
directed cycles are broken by removing the edge
with the weakest classification confidence score.
After pruning invalid edges, event nodes are du-
plicated so that all events have a valid combination
of arguments. For example, the regulation event in-
volves in Figure 2C has two cause arguments and
therefore represents two distinct events. We thus
duplicate the event node, obtaining one regulation
event for each of the cause arguments (Figure 2D).
Events of type gene expression, transcription,
translation, protein catabolism, localization, and
phosphorylation must have exactly one theme argu-
ment, which makes the duplication process trivial:
duplicate events are created, one for each of the ar-
guments. Regulation events must have one theme
and can additionally have one cause argument. For
these classes we use a heuristic, generating a new
event for each theme?cause combination of outgo-
ing edges (Figure 4A). Binding is the only event
class that can have multiple theme arguments. There
is thus no simple way of determining how multi-
ple outgoing theme edges should be grouped (Fig-
ure 4B). We apply a heuristic that first groups the ar-
guments by their syntactic role, defined here as xthe
first dependency in the shortest path from the event
14
to the argument. It then generates an event for each
pair of arguments that are in different groups. In the
case of only one group, all single-argument events
are generated.
Finally, all events with no arguments as well as
regulation events without a theme argument are iter-
atively removed until no such event remains. The
resulting graph is the output of our event extrac-
tion system and can be losslessly converted into the
shared task format (Figure 2D&E).
2.5 Alternative directions
We now briefly describe some of the alternative di-
rections explored during the system development,
which however did not result in increased perfor-
mance, and were thus not included in the final sys-
tem. Whether the reason was due to the considered
approaches being inadequate for the extraction task,
or simply a result of the tight time constraints en-
forced by the shared task is a question only further
research can shed light on.
For the purpose of dividing the extraction prob-
lem into manageable subproblems, we make strong
independence assumptions. This is particularly the
case in the edge detection phase where each edge
is considered in isolation from other edges, some
of which may actually be associated with the same
event. Similar assumptions are made in the trigger
detection phase, where the classifications of individ-
ual tokens are independent.
A common way to relax independence assump-
tions is to use N -best re-ranking where N most-
likely candidates are re-ranked using global features
that model data dependencies that could not be mod-
elled in the candidate generation step. The best can-
didate with respect to this re-ranked order is then
the final prediction of the system. N -best re-ranking
has been successfully applied for example in statisti-
cal parsing (Charniak and Johnson, 2005). We gen-
erated the ten most likely candidate graphs, as de-
termined by the confidence scores of the individual
edges given by the multi-class SVM. A perfect re-
ranking of these ten candidates would lead to 11.5
percentage point improvement in the overall system
F-score on the development set. While we were un-
able to produce a re-ranker sufficiently accurate to
improve the system performance in the time given,
the large potential gain warrants further research.
In trigger word detection, we experimented with
a structural SVM incorporating Hidden Markov
Model type of sequential dependencies (Altun et al,
2003; Tsochantaridis et al, 2004), which allow con-
ditioning classification decisions on decisions made
for previous tokens as well as with a conditional ran-
dom field (CRF) sequence classifier (Lafferty et al,
2001). Neither of these experiments led to a perfor-
mance gain over the multiclass SVM classifier.
As discussed previously, 4.8% of all annotated
events cross sentence boundaries. This problem
could be approached using coreference resolution
techniques, however, the necessary explicit corefer-
ence annotation to train a coreference resolution sys-
tem is not present in the data. Instead, we attempted
to build a machine-learning based system to detect
cross-sentence event arguments directly, rather than
via their referring expression, but were unable to im-
prove the system performance.
3 Tools and resources
3.1 Multi-class SVM
We use a support vector machine (SVM) multi-class
classifier which has been shown to have state-of-
the-art classification performance (see e.g. (Cram-
mer and Singer, 2002; Tsochantaridis et al, 2004)).
Namely, we use the SVMmulticlass implementa-
tion3 which is one of the fastest multi-class SVM
implementations currently available. Analogously
to the binary SVMs, multi-class SVMs have a reg-
ularization parameter that determines the trade-off
between the training error and the complexity of the
learned concept. We select the value of the parame-
ter on the development set. Multi-class SVMs scale
linearly with respect to both the amount of training
data and the average number of nonzero features per
training example, making them an especially suit-
able learning method for our purposes. They also
provide a real-valued prediction for each example
to be classified which is used as a confidence score
in trigger detection precision?recall trade-off adjust-
ment and event argument edge cycle breaking in se-
mantic post-processing. We use the linear kernel,
the only practical choice to train the classifier with
the large training sets available. For example, the
3http://svmlight.joachims.org/svm_
multiclass.html
15
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  10  20  30  40  50  60  70  80
Re
cal
l [%
]
Precision [%]
Figure 5: Performance of the 24 systems that participated
in Task 1, together with an F-score contour plot for refer-
ence. Our system is marked with a full circle.
final training data of the edge detector (8932 sen-
tences) consists of 31792 training examples with
295034 unique features. Training with even this
amount of data is computationally feasible, typically
taking less than an hour.
All classifiers used in the system are trained as
follows. First we optimize the regularization param-
eter C by training on the shared task training set and
testing on the shared task development set. We then
re-train the final classifier on the union of the train-
ing and development sets, using the best value of C
in the previous step. The same protocol is followed
for the ? and ? parameters in trigger detection.
3.2 Dependency parses
Both trigger detection and edge prediction rely on
a wide array of features derived from full depen-
dency parses of the sentence. We use the McClosky-
Charniak domain-adapted parser (McClosky and
Charniak, 2008) which is among the best perform-
ing parsers trained on the GENIA Treebank corpus.
The native constituency output of the parser is trans-
formed to the ?collapsed? form of the Stanford de-
pendency scheme (de Marneffe and Manning, 2008)
using the Stanford parser tools.4 The parses were
provided by the shared task organizers.
4 Results and discussion
The final evaluation of the system was performed by
the shared task organizers using a test set whose an-
4http://nlp.stanford.edu/software/
notation was at no point available to the task partici-
pants. By the main criterion of Task 1, approximate
span matching with approximate recursive match-
ing, our system achieved an F-score of 51.95%. Fig-
ure 5 shows the performance of all systems partic-
ipating in Task 1. The per-class results in Table 1
show that regulation events (including positive and
negative regulation) as well as binding events are the
hardest to extract. These classes have F-scores in
the 31?44% range, while the other classes fall into
the 50?78% range. This is not particularly surpris-
ing since binding and regulation are the only classes
in which events can have multiple arguments, which
means that for an event to be detected correctly, the
edge detector often must make several correct pre-
dictions. Additionally, these classes have the lowest
trigger recognition performance on the development
set. It is interesting to note that the per-class perfor-
mance in Table 1 shows no clear correlation between
the number of events of a class and its F-score.
Table 2 shows the performance of the system us-
ing various other evaluation criteria defined in the
shared task. The most interesting of these is the
strict matching criterion, which, in order to consider
an event correctly extracted, requires exact trigger
span as well as all its nested events to be recursively
correct. The performance of the system with respect
to the strict criterion is 47.41% F-score, only 4.5 per-
centage points lower than the relaxed primary mea-
sure. As seen in Table 2, this difference is almost
exclusively due to triggers with incorrect span.
To evaluate the performance impact of each sys-
tem component individually, we report in Table 3
overall system performance on the development set,
obtained by progressively replacing the processing
steps with gold-standard data. The results show that
the errors of the system are almost evenly distributed
between the trigger and edge detectors. For instance,
a perfect trigger detector would decrease the overall
system error of 46.5% by 18.58 percentage points,
a relative decrease of 40%. A perfect edge detec-
tor would, in combination with a perfect trigger de-
tector, lead to system performance of 94.69%. The
improvement that could be gained by further devel-
opment of the semantic post-processing step is thus
limited, indicating that the strict argument combina-
tion restrictions of Task 1 are sufficient to resolve the
majority of post-processing cases.
16
Event Class # R P F
Protein catabolism 14 42.86 66.67 52.17
Phosphorylation 135 80.74 74.66 77.58
Transcription 137 39.42 69.23 50.23
Localization 174 49.43 81.90 61.65
Regulation 291 25.43 38.14 30.52
Binding 347 40.06 49.82 44.41
Negative regulation 379 35.36 43.46 38.99
Gene expression 722 69.81 78.50 73.90
Positive regulation 983 38.76 48.72 43.17
Total 3182 46.73 58.48 51.95
Table 1: Per-class performance in terms of Recall, Preci-
sion, and F-score on the test set (3182 events) using ap-
proximate span and recursive matching, the primary eval-
uation criterion of Task 1.
Matching R P F
Strict 42.65 53.38 47.41
Approx. Span 46.51 58.24 51.72
Approx. Span&Recursive 46.73 58.48 51.95
Table 2: Performance of our system on the test set (3182
events) with respect to other evaluation measures in the
shared task.
5 Conclusions
We have described a system for extracting complex,
typed events from biomedical literature, only assum-
ing named entities as given knowledge. The high
rank achieved in the BioNLP?09 Shared Task com-
petitive evaluation validates the approach taken in
building the system. While the performance is cur-
rently the highest achieved on this data, the F-score
of 51.95% indicates that there remains considerable
room for further development and improvement.
We use a unified graph representation of the data
in which the individual processing steps can be for-
mulated as simple graph transformations: adding or
removing nodes and edges. It is our experience that
such a representation makes handling the data fast,
easy and consistent. The choice of graph representa-
tion is further motivated by the close correlation of
these graphs with dependency parses. As we are go-
ing to explore the interpretation and applications of
these graphs in the future, the graph representation
will likely provide a flexible base to build on.
Dividing the task of event extraction into multi-
ple subtasks that can be approached by well-studied
Trig Edge PP R P F ?F
pred pred pred 51.54 55.62 53.50
GS pred pred 71.66 72.51 72.08 18.58
GS GS pred 97.21 92.30 94.69 22.61
GS GS GS 100.0 100.0 100.0 5.31
Table 3: Effect of the trigger detector (Trig), edge detec-
tor (Edge), and post-processing (PP) on performance on
the development set (1789 events). The ?F column in-
dicates the effect of replacing the predictions (pred) of
a component with the corresponding gold standard data
(GS), i.e. the maximal possible performance gain obtain-
able from further development of that component.
methods proved to be an effective approach in de-
veloping our system. We relied on state-of-the-art
machine learning techniques that scale up to the task
and allow the use of a considerable number of fea-
tures. We also carefully optimized the various pa-
rameters, a vital step when using machine learning
methods, to fine-tune the performance of the system.
In Section 2.5, we discussed alternative directions
pursued during the development of the current sys-
tem, indicating possible future research directions.
To support this future work as well as complement
the description of the system in this paper we intend
to publish our system under an open-source license.
This shared task represents the first competi-
tive evaluation of complex event extraction in the
biomedical domain. The prior research has largely
focused on binary interaction extraction, achieving
after a substantial research effort F-scores of slightly
over 60% (see, e.g., Miwa et al (2008)) on AIMed,
the de facto standard corpus for this task. Even if
a direct comparison of these results is difficult, they
suggest that 52% F-score in complex event extrac-
tion is a non-trivial achievement, especially consid-
ering the more detailed semantics of the extracted
events. Further, complex event extraction is still a
new problem ? relevant corpora having been avail-
able for only a few years.
Acknowledgments
This research was funded by the Academy of Fin-
land. Computational resources were provided by
CSC ? IT Center for Science Ltd. We thank the
shared task organizers for their efforts in data prepa-
ration and system evaluation.
17
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9(Suppl 11):S2.
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden Markov support vector ma-
chines. In Proceedings of the Twentieth International
Conference on Machine Learning (ICML?03), pages
3?10. AAAI Press.
Jari Bjo?rne, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. How complex are complex
protein-protein interactions? In Proceedings of the
Third International Symposium on Semantic Mining in
Biomedicine (SMBM?08), pages 125?128. TUCS.
Jari Bjo?rne, Filip Ginter, Juho Heimonen, Sampo
Pyysalo, and Tapio Salakoski. 2009. Learning to ex-
tract biological event and relation graphs. In Proceed-
ings of the 17th Nordic Conference on Computational
Linguistics (NODALIDA?09).
Ekaterina Buyko, Elena Beisswanger, and Udo Hahn.
2008. Testing different ACE-style feature sets for
the extraction of gene regulation relations from MED-
LINE abstracts. In Proceedings of the Third Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM?08), pages 21?28. TUCS.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180. ACL.
Koby Crammer and Yoram Singer. 2002. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search, 2:265?292.
Marie-Catherine de Marneffe and Christopher Manning.
2008. Stanford typed hierarchies representation. In
Proceedings of the COLING?08 Workshop on Cross-
Framework and Cross-Domain Parser Evaluation,
pages 1?8.
Juho Heimonen, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. Complex-to-pairwise mapping of
biological relationships using a semantic network rep-
resentation. In Proceedings of the Third Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM?08), pages 45?52. TUCS.
Sophia Katrenko and Pieter Adriaans. 2008. A local
alignment kernel in the context of NLP. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (Coling?08).
Jin-Dong Kim, Tomoko Ohta, and Tsujii Jun?ichi. 2008a.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1):10.
Seonho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
Proceedings of the NAACL-HLT 2009 Workshop
on Natural Language Processing in Biomedicine
(BioNLP?09). ACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning (ICML?01), pages 282?289.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
ACL-08: HLT, Short Papers, pages 101?104. Associa-
tion for Computational Linguistics.
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining
multiple layers of syntactic information for protein-
protein interaction extraction. In Proceedings of the
Third International Symposium on Semantic Mining in
Biomedicine (SMBM?08), pages 101?108. TUCS.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(1):50.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the Twenty-first Inter-
national Conference on Machine Learning (ICML?04),
pages 104?111. ACM.
Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,
and Yves Van de Peer. 2008. Extracting protein-
protein interactions from text using rich feature vec-
tors and feature selection. In Proceedings of the
Third International Symposium on Semantic Mining in
Biomedicine (SMBM?08), pages 77?84. TUCS.
18
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 678?682,
Dublin, Ireland, August 23-24, 2014.
Turku: Broad-Coverage Semantic Parsing with Rich Features
Jenna Kanerva
?
Department of Information
Technology
University of Turku
Finland
jmnybl@utu.fi
Juhani Luotolahti
?
Department of Information
Technology
University of Turku
Finland
mjluot@utu.fi
Filip Ginter
Department of Information
Technology
University of Turku
Finland
figint@utu.fi
Abstract
In this paper we introduce our system ca-
pable of producing semantic parses of sen-
tences using three different annotation for-
mats. The system was used to partic-
ipate in the SemEval-2014 Shared Task
on broad-coverage semantic dependency
parsing and it was ranked third with an
overall F
1
-score of 80.49%. The sys-
tem has a pipeline architecture, consisting
of three separate supervised classification
steps.
1 Introduction
In the SemEval-2014 Task 8 on semantic parsing,
the objective is to extract for each sentence a rich
set of typed semantic dependencies in three differ-
ent formats: DM, PAS and PCEDT. These formats
differ substantially both in the assignment of se-
mantic heads as well as in the lexicon of seman-
tic dependency types. In the open track of the
shared task, participants were encouraged to use
all resources and tools also beyond the provided
training data. To improve the comparability of the
systems, the organizers provided ready-to-use de-
pendency parses produced using the state-of-the-
art parser of Bohnet and Nivre (2012).
In this paper we describe our entry in the open
track of the shared task. Our system is a pipeline
of three support vector machine classifiers trained
separately for detecting semantic dependencies,
assigning their roles, and selecting the top nodes
of semantic graphs. In this, we loosely follow
the architecture of e.g. the TEES (Bj?orne et al.,
2012) and EventMine (Miwa et al., 2012) systems,
which were found to be effective in the structurally
?
These authors contributed equally.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
related task of biomedical event extraction. Sim-
ilar classification approach is shown to be effec-
tive also in semantic parsing by e.g. Zhao et al.
(2009), the winner of the CoNLL?09 Shared Task
on Syntactic and Semantic Dependencies in Mul-
tiple Languages (SRL-only subtask) (Haji?c et al.,
2009), where semantic parsing is approached as a
word-pair classification problem and semantic ar-
guments and their roles are predicted simultane-
ously. In preliminary experiments, we also de-
veloped a joint approach to simultaneously iden-
tify semantic dependencies and assign their roles,
but found that the performance of the joint predic-
tion was substantially worse than for the current
pipeline approach. As the source of features, we
rely heavily on the syntactic parses as well as other
external resources such as vector space represen-
tations of words and large-scale syntactic n-gram
statistics.
In the following sections, we describe the three
individual classification steps of our semantic
parsing pipeline.
2 Detecting Semantic Dependencies
The first step of our semantic parsing pipeline
is to detect semantic dependencies, i.e. governor-
dependent pairs which has a semantic relation be-
tween them. The first stage covers only the identi-
fication of such dependencies; the labels describ-
ing the semantic roles of the dependents are as-
signed in a later stage.
The semantic dependencies are identified using
a binary support vector machine classifier from the
LIBSVM package (Chang and Lin, 2011). Each
possible combination of two tokens in the sen-
tence is considered to be a candidate for a seman-
tic dependency in both directions, and thus also
included as a training example. No beforehand
pruning of possible candidates is performed dur-
ing training. However, we correct for the over-
whelming number of negative training examples
678
by setting the weights of positive and negative ex-
amples used during training, so as to maximize the
unlabeled F
1
-score on the development set.
Increasing the recall of semantic dependency
detection can be beneficial for the overall perfor-
mance of the pipeline system, since a candidate
lost in the dependency detection stage cannot be
recovered later. We therefore tested the approach
applied, among others by Bj?orne et al. (2012),
whereby the dependency detection stage heavily
overgenerates candidates and the next stage in the
pipeline is given the option to predict a nega-
tive label, thus removing a candidate dependency.
In preliminary experiments we tried to explicitly
overgenerate the dependency candidates by alter-
ing the classifier threshold, but noticed that heavy
overgeneration of positive examples leads to a de-
creased performance in the role assigning stage.
Instead, the above-mentioned optimization of the
example weights during training results in a clas-
sifier which overgenerates positive examples by
4.4%, achieving the same objective and improving
the overall performance of the system.
Features used during the dependency identifi-
cation are derived from tokens and the syntactic
parse trees provided by the organizers. Our pri-
mary source of features are the syntactic trees,
since 73.2% of semantic dependencies have a cor-
responding undirected syntactic dependency in the
parse tree. Further, the syntactic dependency path
between the governor and the dependent is shorter
than their linear distance in 48.8% of cases (in
43.4% of cases the distance is the same). The final
feature set used in the identification is optimized
by training models with different combinations of
features and selecting the best combination based
on performance on the held-out development set.
Interestingly, the highest performance is achieved
with a rather small set of features, whose full list-
ing is shown in Table 1. The feature vectors are
normalized to unit length prior to classification
and the SVM regularization parameter c is opti-
mized separately for each annotation format.
3 Role Assignment
After the semantic governor-dependent pairs are
identified, the next step is to assign a role for
each pair to constitute a full semantic dependency.
This is done by training a multiclass support vec-
tor machine classifier implemented in the SVM-
multiclass package by Joachims (1999). We it-
Feature D R T
arg.pos X X
arg.deptype X X
arg.lemma X X
pred.pos X X X
pred.deptype X X X
pred.lemma X X X
pred.is predicate X X
arg.issyntaxdep X
arg.issyntaxgov X
arg.issyntaxsibling X
path.length X X
undirected path.deptype X X
directed path.deptype X X
undirected path.pos X X
extended path.deptype X X
simplified path.deptype with len X
simplified path.deptype wo len X
splitted undirected path.deptype X
arg.prev.pos X X
arg.next.pos X X
arg.prev+arg.pos X X
arg.next+arg.pos X X
arg.next+arg+arg.prev.pos X X
pred.prev.pos X X
pred.next.pos X X
pred.prev+pred.pos X X
pred.next+pred.pos X X
pred.next+pred+pred.prev.pos X X
linear route.pos X
arg.child.pos X
arg.child.deptype X
arg.child.lemma X
pred.child.pos X
pred.child.deptype X X
pred.child.lemma X
syntaxgov.child.deptype X
vector similarities X
n-gram frequencies X
pred.sem role X
pred.child.sem role X
pred.syntaxsibling.deptype X
pred.semanticsibling.sem role X
Table 1: Features used in the detection of semantic
dependencies (D), assigning their roles (R) and top
node detection (T). path refers to syntactic depen-
dencies between the argument and the predicate,
and linear route refers to all tokens between the
argument and the predicate. In top node detection,
where only one token is considered at a time, the
pred is used to represent that token.
679
erate through all identified dependencies, and for
each assign a role, or alternatively classify it as a
negative example. This is to account for the 4.4%
of overgenerated dependencies. However, the pro-
portion of negative classifications should stay rel-
atively low and to ensure this, we downsample the
number of negative examples used in training to
contain only 5% of all negative examples. The
downsampling ratio is optimized on the develop-
ment set using grid search and downsampled train-
ing instances are chosen randomly.
The basic features, shown in Table 1, follow the
same style as in dependency identification. We
also combine some of the basic features by creat-
ing all possible feature pairs in a given set, but do
not perform this with the full set of features. In the
open track, participants are also allowed to use ad-
ditional data and tools beyond the official training
data. In addition to the parse trees, we include also
features utilizing syntactic n-gram frequencies and
vector space similarities.
Google has recently released a large corpus
of syntactic n-grams, a collection of depen-
dency subtrees with frequency counts (Goldberg
and Orwant, 2013). The syntactic n-grams are
induced from the Google Books collection, a
350B token corpus of syntactically parsed text.
In this work we are interested in arcs, which
are (governor, dependent, syntactic relation)
triplets associated with their count.
For each governor-dependent pair, we generate
a set of n-gram features by iterating through all
known dependency types and searching from the
syntactic n-grams how many times (if any) the
governor-dependent pair with the particular de-
pendency type is seen. A separate feature is then
created for each dependency type and the counts
are encoded in feature weights compressed using
w = log
10
(count). This approach gives us an op-
portunity to include statistical information about
word relations induced from a very large corpus.
Information is captured also outside the particular
syntactic context, as we iterate through all known
dependency types during the process.
Another source of additional data used in role
classification is a publicly available Google News
vector space model
1
representing word similari-
ties. The vector space model is induced from the
Google News corpus with the word2vec software
(Mikolov et al., 2013) and negative sampling ar-
1
https://code.google.com/p/word2vec/
chitecture, and each vector have 300 dimensions.
The vector space representation gives us an oppor-
tunity to measure word similarities using the stan-
dard cosine similarity function.
The approach to transforming the vector repre-
sentations into features varies with the three dif-
ferent annotation formats. On DM and PAS, we
follow the method of Kanerva and Ginter (2014),
where for each role an average argument vector
is calculated. This is done by averaging all word
vectors seen in the training data as arguments for
the given predicate with a particular role. For each
candidate argument, we can then establish a set of
similarity values to each possible role by taking
the cosine similarity of the argument vector to the
role-wise average vectors. These similarities are
then turned into separate features, where the simi-
larity values are encoded as feature weights.
On PCEDT, preliminary experiments showed
that the best strategy to include word vectors into
classification is by turning them directly into fea-
tures, so that each dimension of the word vector
is represented as a separate feature. Thus, we it-
erate through all 300 vector dimensions and cre-
ate a separate feature representing the position and
value of a particular dimension. Values are again
encoded in feature weights. These features are cre-
ated separately for both the argument and the pred-
icate. The word vectors are pre-normalized to unit
length, so no additional normalization of feature
weights is needed.
Both the n-gram? and vector similarities?based
features give a modest improvement to the classi-
fication performance.
4 Detecting Top Nodes
The last step in the pipeline is the detection of
top nodes. A top node is the semantic head or
the structural root of the sentence. Typically each
sentence annotated in the DM and PAS formats
contains one top node, whereas PCEDT sentences
have on average 1.12 top nodes per sentence.
As in the two previous stages, we predict top
nodes by training a support vector machine clas-
sifier, with each token being considered a candi-
date. Because the top node prediction is the last
step performed, in addition to the basic informa-
tion available in the two previous steps, we are
able to use also predicted arguments as features.
Otherwise, the feature set used in top node detec-
tion follows the same style as in the two previous
680
LP LR LF UF
DM 80.94 82.14 81.53 83.48
PAS 87.33 87.76 87.54 88.97
PCEDT 72.42 72.37 72.40 85.86
Overall 80.23 80.76 80.49 86.10
Table 2: Overall scores of whole task as well as
separately for each annotation format in terms of
labeled precision (LP), recall (LR) and F
1
-score
(LF) as well as unlabeled F
1
-score (UF).
tasks, but is substantially smaller (see Table 1). We
also create all possible feature pairs prior to clas-
sification to simulate the use of a second-degree
polynomial kernel.
For each token in the sentence, we predict
whether it is a top node or not. However, in DM
and PAS, where typically only one top node is al-
lowed, we choose only the token with the maxi-
mum positive value to be the final top node. In
PCEDT, we simply let all positive predictions act
as top nodes.
5 Results
The primary evaluation measure is the labeled F
1
-
score of the predicted dependencies, where the
identification of top nodes is incorporated as an
additional dummy dependency. The overall se-
mantic F
1
-score of our system is 80.49%. The
prediction performance in DM is 81.53%, in PAS
87.54% and in PCEDT 72.40%. The top nodes
are identified with an overall F
1
-score of 87.05%.
The unlabeled F
1
-score reflects the performance
of the dependency detection in isolation from la-
beling task and by comparing the labeled and un-
labeled F
1
-scores from Table 2 we can see that the
most common mistake relates to the identification
of correct governor-dependent pairs. This is espe-
cially true with the DM and PAS formats where the
difference between labeled and unlabeled scores
is very small (1.9pp and 1.4pp), reflecting high
performance in assigning the roles. Instead, in
PCEDT the role assignment accuracy is substan-
tially below the other two and the difference be-
tween unlabeled and labeled F
1
-score is as much
as 13.5pp. One likely reason is the higher number
of possible roles defined in the PCEDT format.
5.1 Discussion
Naturally, our system generally performs better
with frequently seen semantic roles than roles that
are seen rarely. In the case of DM, the 4 most
common semantic roles cover over 87% of the
gold standard dependencies and are predicted with
a macro F
1
-score of 85.3%, while the remaining
35 dependency labels found in the gold standard
are predicted at an average rate of 49.4%. To
give this a perspective, the most common 4 roles
have on average 121K training instances, while the
remaining 35 roles have on average about 2000
training instances. For PAS, the 9 most common
labels, which comprise over 80% of all depen-
dencies in the gold standard data and have on av-
erage about 66K training instances per role, are
predicted with an F
1
-score of 87.6%, while the
remaining 32 labels have on average 4200 train-
ing instance and are predicted with an F
1
-score of
57.8%. The PCEDT format has the highest num-
ber of possible semantic roles and also lowest cor-
relation between the frequency in training data and
F
1
-score. For PCEDT, the 11 most common la-
bels, which cover over 80% of all dependencies in
the gold standard, are predicted with an F
1
-score
of 69.6%, while the remaining 53 roles are pre-
dicted at an average rate of 46.6%. The higher
number of roles also naturally affects the number
of training instances and the 11 most common la-
bels in PCEDT have on average 35K training in-
stances, while the remaining 53 roles have on av-
erage 1600 instances per role.
Similarly, the system performs better with se-
mantic arguments which are nearby the governor.
This is true for both linear distance between the
two tokens and especially for distance measured
by syntactic dependency steps. For example in the
case of DM, semantic dependencies shorter than
3 steps in the syntactic tree cover more than 95%
of the semantic dependencies in the gold standard
and have an F
1
-score of 75.1%, while the rest have
only 32.6%. The same general pattern is also evi-
dent in the other formats.
6 Conclusion
In this paper we presented our system used to
participate in the SemEval-2014 Shared Task on
broad-coverage semantic dependency parsing. We
built a pipeline of three supervised classifiers to
identify semantic dependencies, assign a role for
each dependency and finally, detect the top nodes.
In addition to basic features used in classifica-
tion we have shown that additional information,
such as frequencies of syntactic n-grams and word
681
similarities derived from vector space representa-
tions, can also positively contribute to the classifi-
cation performance.
The overall F
1
-score of our system is 80.49%
and it was ranked third in the open track of the
shared task.
Acknowledgments
This work was supported by the Emil Aaltonen
Foundation and the Kone Foundation. Computa-
tional resources were provided by CSC ? IT Cen-
ter for Science.
References
Jari Bj?orne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1455?1465.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
Syntactic-Ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity, pages
241?247.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Meth-
ods - Support Vector Learning, pages 169?184. MIT
Press.
Jenna Kanerva and Filip Ginter. 2014. Post-hoc ma-
nipulations of vector space models with application
to semantic role labeling. In Proceedings of the 2nd
Workshop on Continuous Vector Space Models and
their Compositionality (CVSC)@ EACL, pages 1?
10.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Workshop Proceedings of
International Conference on Learning Representa-
tions.
Makoto Miwa, Paul Thompson, John McNaught, Dou-
glas Kell, and Sophia Ananiadou. 2012. Extracting
semantically enriched events from biomedical liter-
ature. BMC Bioinformatics, 13(1):108.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning:
a huge feature engineering method to semantic de-
pendency parsing. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 55?60.
682
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 807?811,
Dublin, Ireland, August 23-24, 2014.
UTU: Disease Mention Recognition and Normalization with CRFs and
Vector Space Representations
Suwisa Kaewphan
1,2,3?
, Kai Hakaka
1?
, Filip Ginter
1
1
Dept. of Information Technology, University of Turku, Finland
2
Turku Centre for Computer Science (TUCS), Turku, Finland
3
The University of Turku Graduate School (UTUGS), University of Turku, Finland
sukaew@utu.fi, kahaka@utu.fi, ginter@cs.utu.fi
Abstract
In this paper we present our system par-
ticipating in the SemEval-2014 Task 7
in both subtasks A and B, aiming at
recognizing and normalizing disease and
symptom mentions from electronic medi-
cal records respectively. In subtask A, we
used an existing NER system, NERsuite,
with our own feature set tailored for this
task. For subtask B, we combined word
vector representations and supervised ma-
chine learning to map the recognized men-
tions to the corresponding UMLS con-
cepts. Our system was placed 2nd and 5th
out of 21 participants on subtasks A and B
respectively showing competitive perfor-
mance.
1 Introduction
The SemEval 2014 task 7 aims to advance the de-
velopment of tools for analyzing clinical text. The
task is organized by providing the researchers an-
notated clinical records to develop systems that
can detect the mentions of diseases and symptoms
in medical records. In particular, the SemEval task
7 comprises two subtasks, recognizing the men-
tions of diseases and symptoms (task A) and map-
ping the mentions to unique concept identifiers
that belong to the semantic group of disorders in
the Unified Medical Language System (UMLS).
Our team participated in both of these sub-
tasks. In subtask A, we used an existing named
entity recognition (NER) system, NERsuite, sup-
plemented with UMLS dictionary and normaliza-
tion similarity features. In subtask B, we com-
bined compositional word vector representations
?
These authors contributed equally.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
with supervised machine learning to map the rec-
ognized mentions from task A to the UMLS con-
cepts. Our best systems, evaluated on strict match-
ing criteria, achieved F-score of 76.6% for the sub-
task A and accuracy of 60.1% for the subtask B,
showing competitive performance in both tasks.
2 Task A: Named Entity Recognition
with NERSuite
The ML approach based on conditional random
fields (CRFs) has shown to have state-of-the-art
performance in recognizing the biological entities.
We thus performed task A by using NERsuite,
an existing NER toolkit with competitive perfor-
mance on biological entity recognition (Campos
et al., 2013).
NERsuite is a NER system that is built on
top of the CRFsuite (Okazaki, 2007). It con-
sists of three language processing modules: a to-
kenizer, a modified version of the GENIA tagger
and a named entity recognizer. NERsuite allows
user-implemented features in addition to dictio-
nary matching and features shown to benefit the
systems such as raw token, lemma, part-of-speech
(POS) and text chunk.
Prior to detecting the disease mentions by the
recognizer module of NERsuite, the clinical text
is split into sentences by using GENIA Sentence
Splitter, a supervised ML system that is known to
be well optimized for biomedical texts (S?tre et
al., 2007). The sentences are subsequently tok-
enized and POS tagged.
To represent the positive entities, the ?BIO?
model was used in our system. The first tokens
of positive mentions are labeled with ?B? and the
rest with ?I?. Negative examples, non-entities, are
thus labeled with ?O?. This model was used for
both contiguous and discontiguous entities.
The features include the normalization similar-
ity (see Section 3.3), types of medical records (dis-
charge, echo, radiology and ecg), and UMLS dic-
807
Trained Model Precision Recall F-score
train + positive samples 77.3% 72.4% 74.8%
train + development 76.7 % 76.5% 76.6%
Table 1: The results of our different NERsuite
models, announced by the organizers.
tionary matching in addition to NERsuite?s own
feature generation.
The UMLS dictionary is prepared by extract-
ing the UMLS database for the semantic types de-
manded by the task. In addition to those 11 seman-
tic types, ?Finding? was also included in our dic-
tionary since, according to its definition, the con-
cept is also deemed relevant for the task. Due to
the common use of acronyms, which are not ex-
tensively provided by UMLS, we also extended
the coverage of our prepared UMLS dictionary
by extracting medical acronyms from the UMLS
database using regular expression.
We assessed the effect of dictionary matching
by training the models with and without the com-
piled UMLS dictionary and evaluating against the
development set. The model trained with dictio-
nary features outperformed the one without. The
best model was obtained by training the NERsuite
with UMLS dictionary in case-number-symbol
normalization mode. In this mode, all letters,
numbers and symbols are converted to lower case,
zero (0) and underscore ( ) respectively.
The regularization parameter (C2) was selected
by using development set to evaluate the best
model. The default parameter (C2 = 1.0) gave
the best performing system and thus was used
throughout the work.
Finally, for the NER task, we submitted two
models. The first model was trained with the orig-
inal training data and duplicates of sentences with
at least one entity mention. The second model
was trained by using the combination of the first
model?s training data and development set.
2.1 Results and Discussions
Our NER system from both submissions benefited
from the increased number of training examples
while the more diverse training data set gave a bet-
ter performance. The official results are shown in
table 1.
The analysis of our best performing NER sys-
tem is not possible since the gold standard of the
test data is not publicly available. We thus simply
analyze our second NER system based on the eval-
uation on the development data. The F-score of the
system was 75.1% and 88.0% for the strict and re-
laxed evaluation criteria respectively. Among all
the mistakes made by the system, the discontigu-
ous entities were the most challenging ones for the
NERsuite. In development data, the discontiguous
entities contribute about 10% of all entities, how-
ever, only 2% were recognized correctly. On the
contrary, the system did well for the other types as
73% were correctly recognized under strict crite-
ria. This demonstrates that the ?BIO? model has
limitations in representing the discontiguous enti-
ties. Improving the model to better represent the
discontiguous entities can possibly boost the per-
formance of the NER system significantly.
3 Task B: Normalization with
Compositional Vector Representations
Our normalization approach is based on con-
tinuous distributed word vector representations,
namely the state-of-the-art method word2vec
(Mikolov et al., 2013a). Our word2vec model
was trained on a subset of abstracts and full ar-
ticles from the PubMed and PubMed Central re-
sources. This data was used as it was readily
available to us from the EVEX resource (Van Lan-
deghem et al., 2013). Before training, all non-
alphanumeric characters were removed and all to-
kens were lower-cased. Even though a set of unan-
notated clinical reports was provided in the task
to support unsupervised learning methods, our ex-
periments on the development set showed better
performance with the model trained with PubMed
articles. This might be due to the size of the cor-
pora, as the PubMed data included billions of to-
kens whereas the provided clinical reports totaled
in over 200 million tokens.
The dimensionality of the word vectors was set
to 300 and we used the continuous skip-gram ap-
proach. For other word2vec parameters default
values were used.
One interesting feature demonstrated by
Mikolov et al. (2013b; 2013c) is that the vectors
conserve some of the semantic characteristics in
element-wise addition and subtraction. In this task
we used the same approach of simply summing
the word-level vectors to create compositional
vectors for multi-word entities and concepts, i.e.
we looked up the vectors for every token appear-
ing in a concept name or entity and summed them
to form a vector to represent the whole phrase.
808
We then formed a lexicon including all preferred
terms and synonyms of all the concepts in the
subset of UMLS defined in the task guidelines.
This lexicon is a mapping from the compositional
vector representations of the concept names into
the corresponding UMLS identifiers. To select the
best concept for a recognized entity we calculated
cosine similarity between the vector representa-
tion of the given entity and all the concept vectors
in the lexicon and the concept with the highest
similarity was chosen.
Word2vec is generally able to relate different
forms of the same word to each other, but we no-
ticed a small improvement in accuracy when pos-
sessive suffixes were removed and all tokens were
lemmatized.
3.1 Detecting CUI-less Mentions
As some of the mentions in the training data do not
have corresponding concepts in the semantic cat-
egories listed in the task guidelines, they are an-
notated as ?CUI-less?. However, our normaliza-
tion approach will always find the nearest match-
ing concept, thus getting penalized for wrong pre-
dictions in the official evaluation. To overcome
this problem, we implemented three separate steps
for detecting the ?CUI-less? mentions. As the
simplest approach we set a fixed cosine similarity
threshold and if the maximal similarity falls below
it, the mention is normalized to ?CUI-less?. The
threshold value was selected using a grid search to
optimize the performance on the official develop-
ment set. Although this method resulted in decent
performance, it is not capable of coping with cases
where the mention has very high similarity or even
exact match with a concept name. For instance
our system normalized ?aspiration? mentions into
UMLS concept ?Pulmonary aspiration? which has
a synonym ?Aspiration?, thus resulting in an exact
match. To resolve this kind of cases, we used sim-
ilar approach as in the DNorm system (Leaman et
al., 2013b), where the ?CUI-less? mentions occur-
ring several times in the training data were added
to the concept lexicon with concept ID ?CUI-less?.
As the final step we trained a binary SVM classi-
fier to distinguish the ?CUI-less? mentions. The
classifier utilized bag-of-word features as well as
the compositional vectors. The performance im-
provement provided by each of these steps is pre-
sented in table 2. This evaluation shows that each
step increases the performance considerably, but
Method Strict accuracy
B 43.6
T 48.4
T + L 53.5
T + L + C 55.4
O 59.3
Table 2: Evaluation of the different approaches
to detect CUI-less entities on the official develop-
ment set compared to a baseline without CUI-less
detection and an oracle method with perfect de-
tection. This evaluation was done with the entities
recognized by our NER system instead of the gold
standard entities. B = baseline without CUI-less
detection, T = similarity threshold, L = Lexicon-
based method, C = classifier, O = Oracle.
the overall performance is still 3.9pp below per-
fect detection.
3.2 Acronym Resolution
Abbreviations, especially acronyms, form a con-
siderable portion of the entity mentions in clini-
cal reports. One of the problems in normalizing
the acronyms is disambiguation as one acronym
can be associated with multiple diseases. Previ-
ous normalization systems (Leaman et al., 2013b)
handle this by selecting the matching concept with
most occurrences in the training data. However,
this approach does not resolve the problem of
non-standard acronyms, i.e. acronyms that are not
known in the UMLS vocabulary or in other medi-
cal acronym dictionaries. Our goal was to resolve
both of these problems by looking at the other enti-
ties found in the same document instead of match-
ing the acronym against the concept lexicon. With
this approach for instance entity mention ?CP?
was on multiple occasions correctly normalized
into the concept ?Chest Pain?, even though UMLS
is not aware of this acronym for the given concept
and in fact associates it with several other con-
cepts such as ?Chronic Pancreatitis? and ?Cerebral
Palsy?. However, the overall gain in accuracy ob-
tained from this method was only minor.
3.3 Normalization Feedback to Named
Entity Recognition
While basic exact match dictionary features pro-
vide usually a large improvement in NER perfor-
mance, they are prone to bias the system to high
precision and low recall. As both noun and ad-
jective forms of medical concepts, e.g. ?atrium?
and ?atrial?, are commonly used in clinical texts,
809
the entities may not have exact dictionary matches.
Moreover the different forms of medical terms
may not share a common morphological root dis-
covered by simple stemming methods, thus com-
plicating approximate matching. In this task we
tried to boost the recall of our entity recognition by
feeding back the normalization similarity informa-
tion as features. These features included the max-
imum similarity between the token and the UMLS
concepts as a numerical value as well as a boolean
feature describing whether the similarity exceeded
a certain threshold.
In addition we experimented by calculating the
similarities for bigrams and trigrams in a slid-
ing window around the tokens, but these features
did not provide any further performance improve-
ments.
3.4 Other Directions Explored
The DNorm system utilizes TF-IDF vectors to rep-
resent the entities and concepts but instead of cal-
culating cosine similarity, the system trains a rank-
ing algorithm to measure the maximal similarity
(Leaman et al., 2013a). Their evaluation, carried
out on the NCBI disease corpus (Do?gan et al.,
2014), showed a notable improvement in perfor-
mance compared to cosine similarity. In our anal-
ysis we noticed that in 39% of the false predic-
tions made by our normalization system, the cor-
rect concept was in the top 10 most similar con-
cepts. This strongly suggested that a similar rank-
ing method might be beneficial with our system as
well. To test this we trained a linear SVM to rerank
the top 10 concepts with highest cosine similarity,
but we were not able to increase the overall per-
formance of the system. However, due to the strict
time constraints of the task, we cannot conclude
whether this approach is feasible or not.
As our compositional vectors are formed by
summing the word vectors, each word has an equal
weight in the sum. Due to this our system made
various errors where the entity was a single word
matching closely to several concepts with longer
names. For instance entity ?hypertensive? was
falsely normalized to concept ?Hypertensive car-
diopathy? whereas the correct concept was ?Hy-
pertensive disorder?. These mistakes could have
been prevented to some extent if the more impor-
tant words had had a larger weight in the sum, e.g.
word ?disorder? is of low significance when try-
ing to distinguish different disorders. However,
Team Strict accuracy Relaxed accuracy
UTH CCB 74.1 87.3
UWM 66.0 90.9
RelAgent 63.9 91.2
IxaMed 60.4 86.2
UTU 60.1 78.3
Table 3: Official evaluation results for the top 5
teams in the normalization task.
weighting the word vectors with their IDF values,
document in this case being an UMLS concept, did
not improve the performance.
3.5 Results
The official results for the normalization task are
shown in table 3. Our system achieved accuracy
of 60.1% when evaluated with the official strict
evaluation metric. This result suggests that com-
positional vector representations are a competitive
approach for entity normalization. However, the
best performing team surpassed our performance
by 14.0pp, showing that there is plenty of room for
other teams to improve. It is worth noting though
that their recall in the NER task tops ours by 8.2pp
thus drastically influencing the normalization re-
sults as well. To evaluate the normalization sys-
tems in isolation from the NER task, a separate
evaluation set with gold standard entities should
be provided.
4 Conclusions
Overall, our NER system can perform well with
the same default settings of NERsuite for gene
name recognition. The performance improves
when relevant features, such as UMLS dictionary
matching and word2vec similarity are added. We
speculated that representing the nature of the data
with more suitable model can improve the system
performance further. As a part of a combined sys-
tem, the improvement on NER system can result
in the increased performance of normalization sys-
tem.
Our normalization system showed competitive
results as well, indicating that word2vec-based
vector representations are a feasible way of solv-
ing the normalization task. As future work we
would like to explore different methods for cre-
ating the compositional vectors and reassess the
applicability of the reranking approach described
in section 3.4.
810
Acknowledgements
Computational resources were provided by CSC
? IT Center for Science Ltd, Espoo, Finland. This
work was supported by the Academy of Finland.
References
David Campos, S?ergio Matos, and Jos?e Lu??s Oliveira.
2013. Gimli: open source and high-performance
biomedical name recognition. BMC bioinformatics,
14(1):54.
Rezarta Islamaj Do?gan, Robert Leaman, and Zhiyong
Lu. 2014. NCBI disease corpus: a resource for dis-
ease name recognition and concept normalization.
Journal of Biomedical Informatics, 47:1?10, Feb.
Robert Leaman, Rezarta Islamaj Do?gan, and Zhiyong
Lu. 2013a. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909?2917.
Robert Leaman, Ritu Khare, and Zhiyong Lu.
2013b. NCBI at 2013 ShARe/CLEF eHealth Shared
Task: Disorder normalization in clinical notes with
DNorm. In Proceedings of the Conference and Labs
of the Evaluation Forum.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In ICLR Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji,
Yusuke Miyao, Y Matsubyashi, and Tomoko Ohta.
2007. AKANE system: protein-protein interaction
pairs in BioCreAtIvE2 challenge, PPI-IPS subtask.
In Proceedings of the BioCreative II, pages 209?
212.
Sofie Van Landeghem, Jari Bj?orne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.
811
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 137?141,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Dependency-based PropBanking of clinical Finnish
Katri Haverinen,1,3 Filip Ginter,1 Timo Viljanen,1
Veronika Laippala2 and Tapio Salakoski1,3
1Department of Information Technology
2Department of French Studies
3Turku Centre for Computer Science, TUCS
20014 University of Turku, Finland
first.last@utu.fi
Abstract
In this paper, we present a PropBank of
clinical Finnish, an annotated corpus of
verbal propositions and arguments. The
clinical PropBank is created on top of a
previously existing dependency treebank
annotated in the Stanford Dependency
(SD) scheme and covers 90% of all verb
occurrences in the treebank.
We establish that the PropBank scheme
is applicable to clinical Finnish as well
as compatible with the SD scheme, with
an overwhelming proportion of arguments
being governed by the verb. This allows
argument candidates to be restricted to di-
rect verb dependents, substantially simpli-
fying the PropBank construction.
The clinical Finnish PropBank
is freely available at the address
http://bionlp.utu.fi.
1 Introduction
Natural language processing (NLP) in the clini-
cal domain has received substantial interest, with
applications in decision support, patient man-
aging and profiling, mining trends, and others
(see the extensive review by Friedman and John-
son (2006)). While some of these applications,
such as document retrieval and trend mining,
can rely solely on word-frequency-based methods,
others, such as information extraction and summa-
rization require a detailed linguistic analysis cap-
turing some of the sentence semantics. Among the
most important steps in this direction is an analysis
of verbs and their argument structures.
In this work, we focus on the Finnish lan-
guage in the clinical domain, analyzing its verbs
and their argument structures using the PropBank
scheme (Palmer et al, 2005). The choice of this
particular scheme is motivated by its practical,
application-oriented nature. We build the clinical
Finnish PropBank on top of the existing depen-
dency treebank of Haverinen et al (2009).
The primary outcome of this study is the
PropBank of clinical Finnish itself, consisting of
the analyses for 157 verbs with 2,382 occurrences
and 4,763 arguments, and covering 90% of all
verb occurrences in the underlying treebank. This
PropBank, together with the treebank, is an impor-
tant resource for the further development of clini-
cal NLP applications for the Finnish language.
We also establish the applicability of the
PropBank scheme to the clinical sublanguage with
its many atypical characteristics, and finally, we
find that the PropBank scheme is compatible with
the Stanford Dependency scheme of de Marneffe
and Manning (2008a; 2008b) in which the under-
lying treebank is annotated.
2 The PropBank scheme
Our annotation work is based on the PropBank se-
mantic annotation scheme of Palmer et al (2005).
For each verb, PropBank defines a number of
framesets, each frameset corresponding to a
coarse-grained sense. A frameset consists of a
roleset which defines a set of roles (arguments
numbered from Arg0 onwards) and their descrip-
tions, and a set of syntactic frames. Any element
that occurs together with a given verb sufficiently
frequently is taken to be its argument. Arg0 is gen-
erally a prototypical Agent argument and Arg1 is
a prototypical Patient or Theme argument. The
remaining numbered arguments have no consis-
tent overall meanings: they are defined on a verb-
by-verb basis. An illustration of a verb with two
framesets is given in Figure 1. In addition to the
numbered arguments, a verb occurrence can have
a number of modifiers, labeled ArgM, each modi-
fier being categorized as one of 14 subtypes, such
as temporal, cause and location.
137
kesta?a?.0: ?tolerate? kesta?a?.1: ?last?
Arg0: the one who tolerates Arg1: the thing that lasts
Arg1: what is being tolerated Arg2: how long it lasts
Figure 1: The PropBank framesets for kesta?a?
(translated to English from the original frames file)
correspond to two different uses of the verb.
Pitka? yo?vuoro Long nightshift
Jouduttu laittamaan Had to put to
illala bipap:lle, bipap in the evning,
nyt hapettuu hyvin. now oxidizes well.
DIUREESI: riitta?va?a? DIURESIS: sufficient
Tajunta: rauhallinen Consciousness: calm
hrhoja ei ena?a? ole there are no more hllucinations
Figure 2: Example of clinical Finnish (left col-
umn) and its exact translation (right column), with
typical features such as spelling errors preserved.
3 Clinical Finnish and the clinical
Finnish treebank
This study is based on the clinical Finnish tree-
bank of Haverinen et al (2009), which consists
of 2,081 sentences with 15,335 tokens and 13,457
dependencies. The text of the treebank comprises
eight complete patient reports from an intensive
care unit in a Finnish hospital. An intensive care
patient report describes the condition of the pa-
tient and its development in time. The clinical
Finnish in these reports has many characteristics
typical of clinical languages, including frequent
misspellings, abbreviations, domain terms, tele-
graphic style and non-standard syntactic structures
(see Figure 2 for an illustration). For a detailed
analysis, we refer the reader to the studies by Laip-
pala et al (2009) and Haverinen et al (2009).
The treebank of Haverinen et al is annotated
in the Stanford Dependency (SD) scheme of de
Marneffe and Manning (2008a; 2008b). This
scheme is layered, and the annotation variant of
the treebank of Haverinen et. al is the basic vari-
ant of the scheme, in which the analysis forms a
tree.
The SD scheme also defines a collapsed de-
pendencies with propagation of conjunct depen-
dencies variant (referred to as the extended vari-
ant of the SD scheme throughout this paper). It
adds on top of the basic variant a second layer
of dependencies which are not part of the strict,
syntactic tree. In particular, the xsubj dependency
marks external subjects, and dependencies involv-
ing the heads of coordinations are explicitly dupli-
PatientPotilas allowedsaanut to_haveottaa juicemehua andja breadleip?? ..
<nsubj xcomp> dobj> cc><xsubj conj>
dobj>punct>
Figure 3: The extended SD scheme. The dashed
dependencies denote the external subjects and
propagated conjunct dependencies that are only
part of the extended variant of the scheme. The
example can be translated as Patient [has been]
allowed to have juice and bread.
In_morningAamulla patientpotilas drankjuonut.0 littlev?h?n juicemehua ..
<nsubj:Arg0 <advmod<nommod:ArgM?tmp dobj:Arg1>
punct>
Figure 4: The PropBank annotation scheme on
top of the treebank syntactic annotation. The verb
juonut (drank) is marked with its frameset, in this
case the frameset number 0. This frameset spec-
ifies that Arg0 marks the agent doing the drink-
ing and Arg1 the liquid being consumed. The
ArgM-tmp label specifies that Aamulla is a tem-
poral modifier. The example can be translated as
In the morning patient drank a little juice.
cated also for the remaining coordinated elements
where appropriate. The extended variant of the SD
scheme is illustrated in Figure 3.
Due to the importance of the additional depen-
dencies for PropBanking (see Section 5 for discus-
sion), we augment the annotation of the underly-
ing treebank to conform to the extended variant of
the SD scheme by manual annotation, adding a to-
tal of 520 dependencies.
The PropBank was originally developed on top
of the constituency scheme of the Penn Tree-
bank and requires arguments to correspond to con-
stituents. In a dependency scheme, where there is
no explicit notion of constituents, we associate ar-
guments of a verb with dependencies governed by
it. The argument can then be understood as the
entire subtree headed by the dependent. The an-
notation is illustrated in Figure 4.
4 PropBanking clinical Finnish
When annotating the clinical Finnish PropBank,
we consider all verbs with at least three occur-
rences in the underlying treebank. In total, we
analyze 157 verbs with 192 framesets. Since the
treebank does not have gold-standard POS infor-
138
FuresisFuresis notei helpedauttanut.0 ,, stoppedlopetettu.0 for_nowtoistaiseksi ..
<neg:ArgM punct> advmod:ArgM?tmp><subj:Arg1 sdep:ArgM?csq>
<xarg:ArgM?cau<xarg:Arg1
punct>
Figure 5: The simplified PropBank annotation strategy. The dashed dependencies labeled with the tech-
nical dependency type xarg signify arguments and modifiers not in a syntactic relationship to the verb.
These arguments and modifiers, as well as those associated with a conj or sdep dependency (ArgM-csq
in this Figure), are only marked in the 100 sentence sample for quantifying unannotated arguments and
modifiers. The sentence can be translated as Furesis did not help, stopped for now.
mation, we identify all verbs and verbal participles
using the FinCG1 analyzer, which gives a verbal
reading to 2,816 tokens. With POS tagging er-
rors taken into account, we estimate the treebank
to contain 2,655 occurrences of verbs and verb
participles. Of these, 2,382 (90%) correspond to
verbs with at least three occurrences and are thus
annotated. In total, these verbs have 4,763 argu-
ments and modifiers.
Due to the telegraphic nature of clinical Finnish,
omissions of different sentence elements, even
main verbs, are very frequent. In order to be able
to analyze the syntax of sentences with a missing
main verb, Haverinen et al have added a so called
null verb to these sentences in the treebank. For
instance, the clinical Finnish sentence Putkesta
nestetta? (Liquid from the drain) lacks a main verb,
and the insertion of one produces Putkesta *null*
nestetta?. In total, there are 428 null verb occur-
rences, making the null verb the most common
verb in the treebank.
In the clinical PropBank annotation, we treat the
null verb in principle as if it was a regular verb,
and give it framesets accordingly. For each null
verb occurrence, we have determined which reg-
ular verb frameset it stands for, and found that,
somewhat surprisingly, there were only four com-
mon coarse senses of the null verb, roughly cor-
responding to four framesets of the verbs olla (to
be), tulla (to come), tehda? (to do) and laittaa (to
put). The 26 (6%) null verb occurrences that did
not correspond to any of these four framesets were
assigned to a ?leftover frameset?, for which no ar-
guments were marked.
1http://www.lingsoft.fi
5 Annotating the arguments on top of
the SD scheme
In contrast to the original PropBank, where any
syntactic constituent could be marked as an argu-
ment, we require arguments to be directly depen-
dent on the verb in the SD scheme (for an illustra-
tion, see Figure 5). This restriction is to consider-
ably simplify the annotation process ? instead of
all possible subtrees, the annotator only needs to
look for direct dependents of the verb. In addition,
this constraint should naturally also simplify pos-
sible automatic identification and classification of
the arguments.
In addition to restricting arguments to direct de-
pendents of the verb, coordination dependencies
conj and sdep (implicit coordination of top level
independent clauses, see Figure 5) are left outside
the annotation scope. This is due to the nature of
the clinical language, which places on these de-
pendencies cause-consequence relationships that
require strong inference. For instance, sentences
such as Patient restless, given tranquilizers where
there is clearly a causal relationship but no explicit
marker such as thus or because, are common.
Naturally, it is necessary to estimate the effect
of these restrictions, which can be justified only
if the number of lost arguments is minimal. We
have conducted a small-scale experiment on 100
randomly selected sentences with at least one verb
that has a frameset assigned. We have provided
this portion of the clinical PropBank with a full an-
notation, including the arguments not governed by
the verb and those associated with conj and sdep
dependencies. For an illustration, see Figure 5.
There are in total 326 arguments and modifiers
(169 arguments and 157 modifiers) in the 100 sen-
tence sample. Of these, 278 (85%) are governed
by the verb in the basic SD scheme and are thus in
a direct syntactic relationship with the verb. Fur-
139
ther 19 (6%) arguments and modifiers are gov-
erned by the verb in the extended SD scheme. Out
of the remaining 29 (9%), 23 are in fact modi-
fiers, leaving only 6 numbered arguments not ac-
counted for in the extended SD scheme. Thus,
96% (163/169) of arguments and 85% (134/157)
of modifiers are directly governed by the verb.
Of the 23 ungoverned modifiers, all are either
cause (CAU) or consequence (CSQ)2. Of the sdep
and conj dependencies only a small portion (9/68)
were associated with an argument or a modifier,
all of which were in fact CAU or CSQ modifiers.
Both these and the CAU and CSQ modifiers not
governed by the verb reflect strongly inferred rela-
tionships between clauses.
Based on these figures, we conclude that an
overwhelming majority of arguments and modi-
fiers is governed by the verb in the extended SD
scheme and restricting the annotation to depen-
dents of the verb as well as leaving sdep and conj
outside the annotation scope seems justified. Ad-
ditionally, we demonstrate the utility of the con-
junct dependency propagation and external subject
marking in the extended SD scheme.
6 Related work
Many efforts have been made to capture meanings
and arguments of verbs. For instance, the VerbNet
project (Kipper et al, 2000) strives to create a
broad on-line verb lexicon, and FrameNet (Rup-
penhofer et al, 2005) aims to document the range
of valences of each verb in each of its senses. The
PropBank project (Palmer et al, 2005) strives for
a practical approach to semantic representation,
adding a layer of semantic role labels to the Penn
Treebank (Marcus et al, 1993).
In addition to the original PropBank by Palmer
et al, numerous PropBanks have been devel-
oped for languages other than English (e.g. Chi-
nese (Xue and Palmer, 2003) and Arabic (Diab
et al, 2008)). Also applications attempting to
automatically recover PropBank-style arguments
have been proposed. For example, the CoNLL
shared task has focused on semantic role labeling
four times, twice as a separate task (Carreras and
Ma`rquez, 2004; Carreras and Ma`rquez, 2005), and
twice in conjunction with syntactic parsing (Sur-
deanu et al, 2008; Hajic? et al, 2009).
2CSQ is a new modifier subtype added by us, due to
the restriction of only annotating direct syntactic dependents,
which does not allow the annotation of all causal relation-
ships with the type CAU.
In semantic analysis of clinical language, Paek
et al (2006) have experimented on PropBank-
based machine learning on abstracts of Random-
ized Controlled Trials (RCTs), and Savova et
al. (2009) have presented work on temporal rela-
tion discovery from clinical narratives.
7 Conclusion
In this paper, we have presented a PropBank of
clinical Finnish, building a new layer of annotation
on top of the existing clinical treebank of Haver-
inen et al (2009). This PropBank covers all 157
verbs occurring at least three times in the treebank
and accounts for 90% of all verb occurrences.
This work has also served as a test case for the
PropBank annotation scheme in two senses. First,
the scheme has been tested on a highly specialized
language, clinical Finnish, and second, its compa-
tibility with the SD syntactic scheme has been ex-
amined. On both accounts, we find the PropBank
scheme a suitable choice.
In general, the specialized language did not
seem to cause problems for the scheme. For in-
stance, the frequent null verbs could be analyzed
similarly to regular verbs, with full 94% belonging
to one of only four framesets. This is likely due to
the very restricted clinical domain of the corpus.
We also find a strong correspondence between
the PropBank arguments and the verb dependents
in the extended SD scheme, with 96% of argu-
ments and 85% of modifiers being directly gov-
erned by the verb. The 15% ungoverned modifiers
are cause-consequence relationships that require
strong inference. This correspondence allowed us
to simplify the annotation task by only considering
direct verb dependents as argument candidates.
The new version of the treebank, manually
anonymized, including the enhanced SD scheme
annotation and the PropBank annotation, is freely
available at http://bionlp.utu.fi.
Acknowledgments
We are grateful to Helja? Lundgren-Laine, Riitta
Danielsson-Ojala and prof. Sanna Salantera? for
their assistance in the anonymization of the cor-
pus. We would also like to thank Lingsoft Ltd.
for making FinTWOL and FinCG available to us.
This work was supported by the Academy of Fin-
land.
140
References
Xavier Carreras and Llu??s Ma`rquez. 2004. In-
troduction to the CoNLL-2004 shared task: Se-
mantic role labeling. In HLT-NAACL 2004 Work-
shop: Eighth Conference on Computational Natu-
ral Language Learning (CoNLL-2004), pages 89?
97, Boston, Massachusetts, USA, May 6 - May 7.
Association for Computational Linguistics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Intro-
duction to the CoNLL-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 152?164, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Marie-Catherine de Marneffe and Christopher Man-
ning. 2008a. Stanford typed dependencies manual.
Technical report, Stanford University, September.
Marie-Catherine de Marneffe and Christopher Man-
ning. 2008b. Stanford typed dependencies repre-
sentation. In Proceedings of COLING?08, Workshop
on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8.
Mona Diab, Mansouri Aous, Martha Palmer, Babko-
Malaya Olga, Wadji Zaghouani, Ann Bies, and
Mohammed Maamouri. 2008. A pilot Arabic
PropBank. In Proceedings of LREC?08, pages
3467?3472. Association for Computational Linguis-
tics.
Carol Friedman and Stephen Johnson. 2006. Natu-
ral language and text processing in biomedicine. In
Biomedical Informatics, pages 312?343. Springer.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria A. Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian Pado?, Jan
S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu, Niawen
Xue, and Yi Zhang. 2009. The CoNLL-2008
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of CoNLL?09:
Shared Task, pages 1?18. Association for Computa-
tional Linguistics.
Katri Haverinen, Filip Ginter, Veronika Laippala, and
Tapio Salakoski. 2009. Parsing clinical Finnish:
Experiments with rule-based and statistical depen-
dency parsers. In Proceedings of NODALIDA?09,
Odense, Denmark, pages 65?72.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of the Seventeenth National Confer-
ence on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 691?696. AAAI Press / The MIT Press.
Veronika Laippala, Filip Ginter, Sampo Pyysalo, and
Tapio Salakoski. 2009. Towards automatic process-
ing of clinical Finnish: A sublanguage analysis and
a rule-based parser. International Journal of Medi-
cal Informatics, Special Issue on Mining of Clinical
and Biomedical Text and Data, 78:7?12.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Hyung Paek, Yacov Kogan, Prem Thomas, Seymor
Codish, and Michael Krauthammer. 2006. Shallow
semantic parsing of randomized controlled trial re-
ports. In Proceedings of AMIA?06, pages 604?608.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: an annotated corpus of se-
mantic roles. Computational Linguistics, 31(1).
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2005. FrameNet II: Extended theory and
practice. Technical report, ICSI.
Guergana Savova, Steven Bethard, Will Styler, James
Martin, Martha Palmer, James Masanz, and Wayne
Ward. 2009. Towards temporal relation discov-
ety from the clinical narrative. In Proceedings of
AMIA?09, pages 568?572.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing on syn-
tactic and semantic dependencies. In Proceedings of
CoNLL?08, pages 159?177. Association for Compu-
tational Linguistics.
Nianwen Xue and Martha Palmer. 2003. Annotating
the propositions in the Penn Chinese Treebank. In
Proceedings of the 2nd SIGHAN Workshop on Chi-
nese Language Processing, pages 47?54, Sapporo,
Japan. Association for Computational Linguistics.
141
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 28?36,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Scaling up Biomedical Event Extraction to the Entire PubMed
Jari Bjo?rne?, ,1,2 Filip Ginter,?,1 Sampo Pyysalo,?,3 Jun?ichi Tsujii,3,4 Tapio Salakoski1,2
1Department of Information Technology, University of Turku, Turku, Finland
2Turku Centre for Computer Science (TUCS), Turku, Finland
3Department of Computer Science, University of Tokyo, Tokyo, Japan
4National Centre for Text Mining, University of Manchester, Manchester, UK
jari.bjorne@utu.fi,ginter@cs.utu.fi,smp@is.s.u-tokyo.ac.jp
tsujii@is.s.u-tokyo.ac.jp,tapio.salakoski@it.utu.fi
Abstract
We present the first full-scale event extrac-
tion experiment covering the titles and ab-
stracts of all PubMed citations. Extraction
is performed using a pipeline composed
of state-of-the-art methods: the BANNER
named entity recognizer, the McClosky-
Charniak domain-adapted parser, and the
Turku Event Extraction System. We an-
alyze the statistical properties of the re-
sulting dataset and present evaluations of
the core event extraction as well as nega-
tion and speculation detection components
of the system. Further, we study in de-
tail the set of extracted events relevant
to the apoptosis pathway to gain insight
into the biological relevance of the result.
The dataset, consisting of 19.2 million oc-
currences of 4.5 million unique events,
is freely available for use in research at
http://bionlp.utu.fi/.
1 Introduction
There has recently been substantial interest in
event models in biomedical information extraction
(IE). The expressive event representation captures
extracted knowledge as structured, recursively
nested, typed associations of arbitrarily many par-
ticipants in specific roles. The BioNLP?09 Shared
Task on Event Extraction (Kim et al, 2009), the
first large scale evaluation of biomedical event
extraction systems, drew the participation of 24
groups and established a standard event represen-
tation scheme and datasets. The training and test
data of the Shared Task comprised 13,623 manu-
ally annotated events in 1,210 PubMed citation ab-
stracts, and on this data the top performing system
of Bjo?rne et al (2009; 2010b) achieved an overall
F-score of 51.95% (Kim et al, 2009).
?Equal contribution by first three authors.
The issue of the scalability and generalization
ability of the introduced event extraction systems
beyond the domain of the GENIA corpus on which
the Shared Task was based has remained largely
an open question. In a prior study, we have es-
tablished on a 1% random sample of PubMed ti-
tles and abstracts that the event extraction system
of Bjo?rne et al is able to scale up to PubMed-
wide extraction without prohibitive computational
time requirements, however, the actual extraction
from the entire PubMed was left as a future work
(Bjo?rne et al, 2010a). Thus, the top-ranking event
extraction systems in the Shared Task have, in fact,
not been used so far for actual mass-scale event ex-
traction beyond the carefully controlled setting of
the Shared Task itself. Further, since an automated
named entity recognition step was not part of the
Shared Task, the interaction of the event extrac-
tion systems with gene/protein name recognizers
remains largely unexplored as well.
In this study, we address some of these ques-
tions by performing a mass-scale event extraction
experiment using the best performing system1 of
the Shared Task (Bjo?rne et al, 2009; Bjo?rne et al,
2010b), and applying it to the entire set of titles
and abstracts of the nearly 18 million citations in
the 2009 distribution of PubMed. The extraction
result, containing 19.2 million event occurrences,
is the largest dataset of its type by several orders
of magnitude and arguably represents the state-of-
the-art in automatic event extraction with respect
to both accuracy and size.
To support emerging community efforts in tasks
that build on event extraction output, such as event
network refinement, hypothesis generation, path-
way extraction, and others, we make the entire
resulting dataset freely available for research pur-
poses. This allows researchers interested in ques-
tions involving text mining, rather than initial in-
1Available at http://bionlp.utu.fi/
28
Event type Example
Gene expression 5-LOX is expressed in leukocytes
Transcription promoter associated with IL-4 gene
transcription
Localization phosphorylation and nuclear translo-
cation of STAT6
Protein catabolism I kappa B-alpha proteolysis by
phosphorylation.
Phosphorylation BCL-2 was phosphorylated at the
G(2)/M phase
Binding Bcl-w forms complexes with Bax and
Bak
Regulation c-Met expression is regulated by Mitf
Positive regulation IL-12 induced STAT4 binding
Negative regulation DN-Rac suppressed NFAT activation
Table 1: Targeted event types with brief example
statements expressing an event of each type. In the
examples, the word or words marked as triggering
the presence of the event are shown in italics and
event participants underlined. The event types are
grouped by event participants, with the first five
types taking one theme, binding events taking mul-
tiple themes and the regulation types theme and
cause participants. Adapted from (Bjo?rne et al,
2009).
formation extraction, to make use of the many fa-
vorable statistical properties of the massive dataset
without having to execute the laborious and time-
consuming event extraction pipeline.
In the following, we describe the Shared Task
event representation applied throughout this study,
the event extraction pipeline itself, and a first set
of analyzes of multiple aspects of the resulting
dataset.
2 Event extraction
The event extraction pipeline follows the model of
the BioNLP?09 Shared Task in its representation
of extracted information. The primary extraction
targets are gene or gene product-related entities
and nine fundamental biomolecular event types in-
volving these entities (see Table 1 for illustration).
Several aspects of the event representation, as
defined in the context of the Shared Task, differ-
entiate the event extraction task from the body of
domain IE studies targeting e.g. protein?protein
interactions and gene?disease relations, including
previous domain shared tasks (Ne?dellec, 2005;
Krallinger et al, 2008). Events can have an ar-
bitrary number of participants with specified roles
(e.g. theme or cause), making it possible to cap-
ture n-ary associations and statements where some
participants occur in varying roles or are only oc-
Regulation
NNP NN VB NNP CC .conj_and>
<nn dobj><nsubj NNP
ProteinSTAT3 Phosphorylationphosphorylation Regulationinvolve ProteinVav and ProteinRac-1 .
Cause>
Cause><Theme
event detectionC
B
A
dobj>
named entity recognition
ProteinSTAT3 phosphorylation involve ProteinVav and ProteinRac-1 .
STAT3 phosphorylation involve Vav and Rac-1 .Ser(727) mayNNP
appos> <auxMD
Ser(727) may
Ser(727) mayEntity
<Theme<Site <Theme
Regulation
ProteinSTAT3 Phosphorylationphosphorylation involve ProteinVav and ProteinRac-1 .
Cause>
Cause><Theme
speculation and negation detectionD
Ser(727) mayEntity
<Theme<Site <Theme
RegulationSpec
Spec
STAT3 phosphorylation involve Vav and Rac-1 .Ser(727) may
parsing
Figure 1: Event extraction. A multi-stage sys-
tem produces an event graph for each sentence.
Named entities are detected (A) using BANNER.
Independently of named entity detection, sen-
tences are parsed (B) to produce a dependency
parse. Event detection (C) uses the named entities
and the parse in predicting the trigger nodes and
argument edges that form the events. Finally, po-
larity and certainty (D) are predicted for the gen-
erated events. Adapted from (Bjo?rne et al, 2009).
casionally mentioned. A further important prop-
erty is that event participants can be other events,
resulting in expressive, recursively nested struc-
tures. Finally, events are given GENIA Event on-
tology types drawn from the community-standard
Gene Ontology (The Gene Ontology Consortium,
2000), giving each event well-defined semantics.
2.1 Event Extraction Pipeline
The event extraction pipeline applied in this work
consists of three main processing steps: named en-
tity recognition, syntactic parsing, and event ex-
traction. The process is illustrated in Figure 1.
For named entity recognition, we use the BAN-
NER system of Leaman and Gonzales (2008),
which in its current release achieves results close
to the best published on the standard GENETAG
dataset and was reported to have the best perfor-
mance in a recent study comparing publicly avail-
able taggers (Kabiljo et al, 2009). Titles and ab-
stracts of all 17.8M citations in the 2009 distribu-
tion of PubMed are processed through the BAN-
NER system.
Titles and abstracts of PubMed citations in
which at least one named entity was identified, and
29
which therefore contain a possible target for event
extraction, are subsequently split into sentences
using a maximum-entropy based sentence splitter
trained on the GENIA corpus (Kazama and Tsujii,
2003) with limited rule-based post-processing for
some common errors.
All sentences containing at least one named
entity are then parsed with the domain-adapted
McClosky-Charniak parser (McClosky and Char-
niak, 2008; McClosky, 2009), which has achieved
the currently best published performance on the
GENIA Treebank (Tateisi et al, 2005). The con-
stituency parse trees are then transformed to the
collapsed-ccprocessed variant of the Stanford De-
pendency scheme using the conversion tool2 intro-
duced by de Marneffe et al (2006).
Finally, events are extracted using the Turku
Event Extraction System of Bjo?rne et al which
achieved the best performance in the BioNLP?09
Shared Task and remains fully competitive with
even the most recent advances (Miwa et al, 2010).
We use a recent publicly available revision of the
event extraction system that performs also extrac-
tion of Shared Task subtask 2 and 3 information,
providing additional event arguments relevant to
event sites and localization (site, atLoc, and toLoc
role types in the Shared Task) as well as informa-
tion on event polarity and certainty (Bjo?rne et al,
2010b).
2.2 Extraction result and computational
requirements
Named entity recognition using the BANNER sys-
tem required in total roughly 1,800 CPU-hours
and resulted in 36,454,930 named entities identi-
fied in 5,394,350 distinct PubMed citations.
Parsing all 20,037,896 sentences with at least
one named entity using the McClosky-Charniak
parser and transforming the resulting constituency
trees into dependency analyzes using the Stanford
conversion tool required about 5,000 CPU-hours,
thus averaging 0.9 sec per sentence. Even though
various stability and scalability related problems
were met during the parsing process, we were able
to successfully parse 20,020,266 (99.91%) of all
sentences.
Finally, the event extraction step required ap-
proximately 1,500 CPU-hours and resulted in
19,180,827 event instances. In total, the entire cor-
2http://www-nlp.stanford.edu/
downloads/lex-parser.shtml
pus of PubMed titles and abstracts was thus pro-
cessed in roughly 8,300 CPU-hours, or, 346 CPU-
days, the most time-consuming step by far being
the syntactic parsing.
We note that, even though the components used
in the pipeline are largely well-documented and
mature, a number of technical issues directly re-
lated to, or at least magnified by, the untypi-
cally large dataset were met at every point of the
pipeline. Executing the pipeline was thus far from
a trivial undertaking. Due to the computational re-
quirements of the pipeline, cluster computing sys-
tems were employed at every stage of the process.
2.3 Evaluation
We have previously evaluated the Turku Event
Extraction System on a random 1% sample of
PubMed citations, estimating a precision of 64%
for event types and arguments pertaining to sub-
task 1 of the Shared Task (Bjo?rne et al, 2010a),
which compares favorably to the 58% precision
the system achieves on the Shared Task dataset it-
self (Bjo?rne et al, 2009).
To determine precision on subtasks 2 and 3
on PubMed citations, we manually evaluate 100
events with site and location arguments (sub-
task 2) and 100 each of events predicted to be
speculated or negated (subtask 3).
Subtask 2 site and location arguments are
mostly external to the events they pertain to and
therefore were evaluated independently of their
parent event. Their precision is 53% (53/100),
comparable to the 58% precision established on
the BioNLP?09 Shared Task development set, us-
ing the same parent-independent criterion.
To estimate the precision of the negation detec-
tion (subtask 3), we randomly select 100 events
predicted to be negated. Of these, 9 were incor-
rect as events to such an extent that the correct-
ness of the predicted negation could not be judged
and, among the remaining 91 events, the negation
was correctly predicted in 82% of the cases. Sim-
ilarly, to estimate the precision of speculation de-
tection, we randomly select 100 events predicted
to be speculated, of which 20 could not be judged
for correctness of speculation. Among the remain-
ing 80, 88% were correctly predicted as specula-
tive events. The negations were mostly signalled
by explicit statements such as is not regulated, and
speculation by statements, such as was studied,
that defined the events as experimental questions.
30
For comparison, on the BioNLP?09 Shared Task
development set, for correctly predicted events,
precision for negation examples was 83% (with
recall of 53%) and for speculation examples 77%
(with recall of 51%).
In the rest of this paper, we turn our attention to
the extraction result.
3 Term-NE mapping
As the event types are drawn from the Gene On-
tology and the original data on which the system
is trained has been annotated with reference to the
GO definitions, the events targeted by the extrac-
tion system have well-defined biological interpre-
tations. The meaning of complete event struc-
tures depends also on the participating entities,
which are in the primary event extraction task con-
strained to be of gene/gene product (GGP) types,
as annotated in the GENIA GGP corpus (Ohta et
al., 2009a). The simple and uniform nature of
these entities makes the interpretation of complete
events straightforward.
However, the semantics of the entities au-
tomatically tagged in this work are somewhat
more openly defined. The BANNER system was
trained on the GENETAG corpus, annotated for
?gene/protein entities? without differentiating be-
tween different entity types and marking entities
under a broad definition that not only includes
genes and gene products but also related entities
such as gene promoters and protein complexes,
only requiring that the tagged entities be specific
(Tanabe et al, 2005). The annotation criteria of
the entities used to train the BANNER system as
well as the event extraction system also differ in
the extent of the marked spans, with GENIA GGP
marking the minimal name and GENETAG allow-
ing also the inclusion of head nouns when a name
occurs in modifier position. Thus, for example, the
latter may annotate the spans p53 gene, p53 pro-
tein, p53 promoter and p53 mutations in contexts
where the former would in each case mark only
the substring p53.
One promising future direction for the present
effort is to refine the automatically extracted data
into an event network connected to specific entries
in gene/protein databases such as Entrez Gene and
UniProt. To achieve this goal, the resolution of
the tagged entities can be seen to involve two re-
lated but separate challenges. First, identifying
the specific database entries that are referred to
Relation Examples
Equivalent GGP gene, wild-type GGP
Class-Subclass human GGP, HIV-1 GGP
Object-Variant
GGP-Isoform GGP isoform
GGP-Mutant dominant-negative GGP
GGP-Recombinant GGP expression plasmid
GGP-Precursor GGP precursor, pro-GGP
Component-Object
GGP-Amino acid GGP-Ile 729
GGP-AA motif GGP NH2-terminal
GGP-Reg. element GGP proximal promoter
GGP-Flanking region GGP 5? upstream sequence
Object-Component
GGP-Protein Complex GGP homodimers
Place-Area
GGP-Locus GGP loci
Member-Collection
GGP-Group GGP family members
Table 2: Gene/gene product NE-term relation
types with examples. Top-level relations in the re-
lation type hierarchy shown in bold, specific NE
names in examples replaced with GGP. Intermedi-
ate levels in the hierarchy and a number of minor
relations omitted. Relation types judged to allow
remapping (see text) underlined.
by the genes/proteins named in the tagged enti-
ties, and second, mapping from the events involv-
ing automatically extracted terms to ones involv-
ing the associated genes/proteins. The first chal-
lenge, gene/protein name normalization, is a well-
studied task in biomedical NLP for which a num-
ber of systems with promising performance have
been proposed (Morgan and Hirschman, 2007).
The second we believe to be novel. In the follow-
ing, we propose a method for resolving this task.
We base the decision on how to map events ref-
erencing broadly defined terms to ones referencing
associated gene/protein names in part on a recently
introduced dataset of ?static relations? (Pyysalo et
al., 2009) between named entities and terms (Ohta
et al, 2009b). This dataset was created based on
approximately 10,000 cases where GGP NEs, as
annotated in the GENIA GGP corpus (Ohta et al,
2009a), were embedded in terms, as annotated in
the GENIA term corpus (Ohta et al, 2002). For
each such case, the relation between the NE and
the term was annotated using a set of introduced
relation types whose granularity was defined with
reference to MeSH terms (see Table 2, Ohta et al,
2009b). From this data, we extracted prefix and
suffix strings that, when affixed to a GGP name,
produced a term with a predictable relation (within
the dataset) to the GGP. Thus, for example, the
31
term GGP
p53 protein p53
p53 gene p53
human serum albumin serum albumin
wild-type p53 p53
c-fos mRNA c-fos
endothelial NO synthase NO synthase
MHC cl. II molecules MHC cl. II
human insulin insulin
HIV-1 rev.transcriptase rev.transcriptase
hepatic lipase lipase
p24 antigen p24
tr. factor NF-kappaB NF-kappaB
MHC molecules MHC
PKC isoforms PKC
HLA alleles HLA
RET proto-oncogene RET
ras oncogene ras
SV40 DNA SV40
EGFR tyrosine kinase EGFR
Table 3: Examples of frequently applied map-
pings. Most frequent term for each mapping is
shown. Some mention strings are abbreviated for
space.
Mentions Types
Total 36454930 4747770
Mapped 2212357 (6.07%) 547920 (11.54%)
Prefix 430737 (1.18%) 129536 (2.73%)
Suffix 1838646 (5.04%) 445531 (9.38%)
Table 4: Statistics for applied term-GGP map-
pings. Tagged mentions and types (unique men-
tions) shown separately. Overall total given for
reference, for mappings overall for any mapping
shown and further broken down into prefix-string
and suffix-string based.
prefix string ?wild-type? was associated with the
Equivalent relation type and the suffix string ?ac-
tivation sequence? with the GGP-Regulatory ele-
ment type. After filtering out candidates shorter
than 3 characters as unreliable (based on prelim-
inary experiments), this procedure produced a set
of 68 prefix and 291 suffix strings.
To make use of the data for predicting relations
between GGP names and the terms formed by af-
fixing a prefix or suffix string, it is necessary to
first identify name-term pairs. Candidates can be
generated simply by determining the prefix/suffix
strings occurring in each automatically tagged en-
tity and assuming that what remains after remov-
ing the prefixes and suffixes is a GGP name. How-
ever, this naive strategy often fails: while remov-
ing ?protein? from ?p53 protein? correctly identi-
fies ?p53? as the equivalent GGP name, for ?cap-
sid protein? the result, ?capsid? refers not to a
GGP but to the shell of a virus ? ?protein? is prop-
erly part of the protein name. To resolve this is-
sue, we drew on the statistics of the automatically
tagged entities, assuming that if a prefix/suffix
string is not a fixed part of a name, the name will
appear tagged also without that string. As the tag-
ging covers the entire PubMed, this is likely to
hold for all but the very rarest GGP names. To
compensate for spurious hits introduced by tag-
ging errors, we specifically required that to accept
a candidate prefix/suffix string-name pair, the can-
didate name should occur more frequently without
the prefix/suffix than with it. As the dataset is very
large, this simple heuristic often gives the right de-
cision with secure margins: for example, ?p53?
was tagged 117,835 times but ?p53 protein? only
11,677, while ?capsid? was (erroneously) tagged
7 times and ?capsid protein? tagged 1939 times.
A final element of the method is the definition
of a mapping to events referencing GGP NEs from
the given events referencing terms, the NEs con-
tained in the terms, and the NE-term relations. In
this work, we apply independently for each term a
simple mapping based only on the relation types,
deciding for each type whether replacing refer-
ence to a term with reference to a GGP holding
the given relation to the term preserves event se-
mantics (to an acceptable approximation) or not.
For the Equivalent relation this holds by defini-
tion. We additionally judged all Class-Subclass
and Component-Object relations to allow remap-
ping (accepting e.g. P1 binds part of P2 ? P1
binds P2) as well as selected Object-Variant rela-
tions (see Table 2). For cases judged not to allow
remapping, we simply left the event unmodified.
Examples of frequently applied term-GGP map-
pings are shown in Table 3, and Table 4 shows
the statistics of the applied mappings. We find
that suffix-based mappings apply much more fre-
quently than prefix-based, perhaps reflecting also
the properties of the source dataset. Overall, the
number of unique tagged types is reduced by over
10% by this procedure. It should be noted that the
applicability of the method could likely be consid-
erably extended by further annotation of NE-term
relations in the dataset of Ohta et al (2009b): the
current data is all drawn from the GENIA corpus,
drawn from the subdomain of transcription factors
in human blood cells, and its coverage of PubMed
is thus far from exhaustive.
32
4 Event recurrence
Given a dataset of events extracted from the en-
tire PubMed, we can study whether, and to what
extent, events are re-stated in multiple PubMed ci-
tations. This analysis may shed some light ? nat-
urally within the constraints of an automatically
extracted dataset rather than gold-standard anno-
tation ? on the often (informally) discussed hy-
pothesis that a high-precision, low recall system
might be a preferred choice for large-scale extrac-
tion as the lower recall would be compensated by
the redundancy of event statements in PubMed.
In order to establish event recurrence statistics,
that is, the number of times a given event is re-
peated in the corpus, we perform a limited normal-
ization of tagged entities consisting of the Term-
NE mapping presented in Section 3 followed
by lowercasing and removal of non-alphanumeric
characters. Two named entities are then consid-
ered equal if their normalized string representa-
tions are equal. For instance, the two names IL-
2 gene and IL2 would share the same normalized
form il2 and would thus be considered equal.
For the purpose of recurrence statistics, two
events are considered equal if their types are equal,
and all their Theme and Cause arguments, which
can be other events, are recursively equal as well.
A canonical order of arguments is used in the com-
parison, thus e.g. the following events are consid-
ered equal:
regulation(Cause:A, Theme:binding(Theme:B, Theme:C))
regulation(Theme:binding(Theme:C, Theme:B), Cause:A)
In total, the system extracted 19,180,827 instances
of 4,501,883 unique events. On average, an
event is thus stated 4.2 times. The distribution
is, however, far from uniform and exhibits the
?long tail? typical of natural language phenom-
ena, with 3,484,550 (77%) of events being single-
ton occurrences. On the other hand, the most fre-
quent event, localization(Theme:insulin), occurs
as many as 59,821 times. The histogram of the
number of unique events with respect to their oc-
currence count is shown in Figure 2.
The total event count consists mostly of sim-
ple one-argument events. The arguably more
interesting category of events that involve at
least two different named entities constitutes
2,064,278 instances (11% of the 19.2M total)
of 1,565,881 unique events (35% of the 4.5M
total). Among these complex events, recur-
 10
 100
1K
10K
100K
1M
 1  10  100 1K 10KUn
iqu
e e
ven
ts w
ith 
giv
en 
occ
urre
nce
 co
unt
Event occurrence count
Figure 2: Number of unique events (y-axis) with a
given occurrence count (x-axis).
R P N L B E T C H
R 561 173 128 42 63 83 30 16 17
P 173 1227 192 58 99 143 39 20 23
N 128 192 668 46 73 98 31 17 18
L 42 58 46 147 57 75 25 15 15
B 63 99 73 57 1023 134 35 20 21
E 83 143 98 75 134 705 49 22 24
T 30 39 31 25 35 49 79 11 11
C 16 20 17 15 20 22 11 39 7
H 17 23 18 15 21 24 11 7 49
Table 5: Event type confusion matrix. Each el-
ement contains the number of unique events, in
thousands, that are equal except for their type.
The matrix is symmetric and its diagonal sums to
4,5M, the total number of extracted unique events.
The event types are (R)egulation, (P)ositive
regulation, (N)egative regulation, (L)ocalization,
(B)inding, gene (E)xpression, (T)ranscription,
protein (C)atabolism, and p(H)osphorylation.
rence is thus considerably lower, an event be-
ing stated on average 1.3 times. The most
frequent complex event, with 699 occurrences,
is positive-regulation(Cause:GnRG,Theme:local-
ization(Theme:LH)), reflecting the well-known
fact that GnRG causes the release of LH, a hor-
mone important in human reproduction.
To gain an additional broad overview of the
characteristics of the extracted events, we com-
pute an event type confusion matrix, shown in Ta-
ble 5. In this matrix, we record for each pair of
event types T1 and T2 the number of unique events
of type T1 for which an event of type T2 can be
found such that, apart for the type difference, the
events are otherwise equal. While e.g. a posi-
tive regulation-negative regulation pair is at least
unusual, in general these event pairs do not sug-
gest extraction errors: for instance the existence
33
of the event expression(Theme:A) does not in any
way prevent the existence of the event localiza-
tion(Theme:A), and regulation subsumes positive-
regulation. Nevertheless, Table 5 shows a clear
preference for a single type for the events.
5 Case Study: The apoptosis pathway
In this section, we will complement the preceding
broad statistical overview of the extracted events
with a detailed study of a specific pathway, the
apoptosis pathway, determining how well the ex-
tracted events cover its interactions (Figure 3).
To create an event network, the events must be
linked through their protein arguments. In addi-
tion to the limited named entity normalization in-
troduced in Section 4, we make use of a list of syn-
onyms for each protein name in the apoptosis path-
way, obtained manually from protein databases,
such as UniProt. Events whose protein arguments
correspond to any of these known synonyms are
then used for reconstructing the pathway.
The apoptosis pathway consists of several over-
lapping signaling routes and can be defined on
different levels of detail. To have a single, ac-
curate and reasonably high-level definition, we
based our pathway on a concisely presentable sub-
set of the KEGG human apoptosis pathway (entry
hsa04210) (Kanehisa and Goto, 2000). As seen
in Figure 3, the extracted dataset contains events
between most interaction partners in the pathway.
The constructed pathway also shows that the ex-
tracted events are not necessarily interactions in
the physical sense. Many ?higher level? events
are extracted as well. For example, the extracel-
lular signaling molecule TNF? can trigger path-
ways leading to the activation of Nf-?B. Although
the two proteins are not likely to interact directly,
it can be said that TNF? upregulates NF-?B, an
event actually extracted by the system. Such state-
ments of indirect interaction co-exist with state-
ments of actual, physical interactions in the event
data.
6 Conclusions
In this paper, we have presented the result of pro-
cessing the entire, unabridged set of PubMed titles
and abstracts with a state-of-the-art event extrac-
tion pipeline as a new resource for text mining in
the biomedical domain. The extraction result ar-
guably represents the best event extraction output
achievable with currently available tools.
The primary contribution of this work is the set
of over 19M extracted event instances of 4.5M
unique events. Of these, 2.1M instances of 1.6M
unique events involve at least two different named
entities. These form an event network several
orders of magnitude larger than those previously
available. The data is intended to support re-
search in biological hypothesis generation, path-
way extraction, and similar higher-level text min-
ing tasks. With the network readily available in an
easy-to-process format under an open license, re-
searchers can focus on the core tasks of text min-
ing without the need to perform the tedious and
computationally very intensive task of event ex-
traction with a complex IE pipeline.
In addition to the extracted events, we make
readily available the output of the BANNER sys-
tem on the entire set of PubMed titles and abstracts
as well as the parser output of the McClosky-
Charniak domain-adapted parser (McClosky and
Charniak, 2008; McClosky, 2009) further trans-
formed to the Stanford Dependency representa-
tion using the tools of de Marneffe et al (2006)
for nearly all (99.91%) sentences with at least one
named entity identified. We expect this data to be
of use for the development and application of sys-
tems for event extraction and other BioNLP tasks,
many of which currently make extensive use of
dependency syntactic analysis. The generation of
this data having been far from a trivial technical
undertaking, its availability as-is can be expected
to save substantial duplication of efforts in further
research.
A manual analysis of extracted events relevant
to the apoptosis pathway demonstrates that the
event data can be used to construct detailed bio-
logical interaction networks with reasonable accu-
racy. However, accurate entity normalization, in
particular taking into account synonymous names,
seems to be a necessary prerequisite and remains
among the most important future work directions.
In the current study, we take first steps in this di-
rection in the form of a term-NE mapping method
in event context. The next step will be the applica-
tion of a state-of-the-art named entity normaliza-
tion system to obtain biological database identities
for a number of the named entities in the extracted
event network, opening possibilities for combin-
ing the data in the network with other biological
information. A further practical problem to ad-
dress will be that of visualizing the network and
34
ev
n
nv
v
v v
n e t
n n v
v nv
v
vn
n
n
n
n
n
n
n n
n
n
n
n n
n
nn
 
n
d
n
e
n
c
v
n
v
t
n
nvnv
n
n
n
v
t
en
ct
nncv
n
n
n e n v
n
n
vnd
n
 
n
v
n t
n
vv
v
n
v
t
n
t
e t
e n v
e
t
n
v
v
n
v
v
nv
n
n v
v
n v
n
c
n
n
v
n v
nn
c
v
nn
vn
v
v
n
v
v
n
e v
vn
IL-1 TNF? TRAIL Fas-L
IL-1R TNF-R1 TRAIL-R Fas
FADDTRADDRIP1MyD88IRAK
NIK
IKK
I?B? NF-?B
CASP10CASP8
FLIP
CASP3CASP7
dioamayorgsrdapuaporrlpaop
ugugpp
TRAF2
IAP
doiiraii
Figure 3: Extracted apoptosis event network. Events shown in the figure are selected on their
prominence in the data or correspondence to known apoptosis interactions. Events corresponding
to KEGG apoptosis pathway interaction partners are highlighted with a light grey background. The
event types are (P)ositive regulation, (N)egative regulation, (R)egulation, gene (E)xpression, (B)inding,
p(H)osphorylation, (L)ocalization and protein (C)atabolism.
presenting the information in a biologically mean-
ingful manner.
The introduced dataset is freely available for
research purposes at http://bionlp.utu.
fi/.
Acknowledgments
This work was supported by the Academy of
Finland and by Grant-in-Aid for Specially Pro-
moted Research (MEXT, Japan). Computational
resources were provided by CSC ? IT Center for
Science, Ltd., a joint computing center for Finnish
academia and industry. We thank Robert Leaman
for advance access and assistance with the newest
release of BANNER.
35
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado. Association for
Computational Linguistics.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010a. Complex event ex-
traction at PubMed scale. In Proceedings of the 18th
Annual International Conference on Intelligent Sys-
tems for Molecular Biology (ISMB 2010). In press.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2010b. Ex-
tracting contextualized complex biological events
with rich graph-based feature sets. Computational
Intelligence. In press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Renata Kabiljo, Andrew Clegg, and Adrian Shepherd.
2009. A realistic assessment of methods for extract-
ing gene/protein interactions from free text. BMC
Bioinformatics, 10(1):233.
M. Kanehisa and S. Goto. 2000. KEGG: kyoto ency-
clopedia of genes and genomes. Nucleic Acids Res.,
28:27?30, Jan.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 137?144.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado. ACL.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. 2008. Overview of
the protein-protein interaction annotation extraction
task of BioCreative II. Genome Biology, 9(Suppl
2):S4.
R. Leaman and G. Gonzalez. 2008. BANNER: an exe-
cutable survey of advances in biomedical named en-
tity recognition. Pacific Symposium on Biocomput-
ing, pages 652?663.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
David McClosky. 2009. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer Sci-
ence, Brown University.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010. Event Extraction With Com-
plex Event Classification Using Rich Features. J
Bioinform Comput Biol, 8:131?146.
Alexander A. Morgan and Lynette Hirschman. 2007.
Overview of BioCreative II gene normalization. In
Proceedings of BioCreative II, pages 101?103.
Claire Ne?dellec. 2005. Learning Language in
Logic - Genic Interaction Extraction Challenge. In
J. Cussens and C. Ne?dellec, editors, Proceedings
of the 4th Learning Language in Logic Workshop
(LLL05), pages 31?37.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA corpus: An annotated re-
search abstract corpus in molecular biology domain.
In Proceedings of the Human Language Technology
Conference (HLT?02), pages 73?77.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009a. Incorporating
genetag-style annotation to genia corpus. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 106?
107, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Tomoko Ohta, Sampo Pyysalo, Kim Jin-Dong, and
Jun?ichi Tsujii. 2009b. A re-evaluation of biomedi-
cal named entity - term relations. In Proceedings of
LBM?09.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece in the
biomedical information extraction puzzle. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 1?9,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Lorraine Tanabe, Natalie Xie, Lynne H Thom, Wayne
Matten, and W John Wilbur. 2005. GENETAG: A
tagged corpus for gene/protein named entity recog-
nition. BMC Bioinformatics, 6(Suppl. 1):S3.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proceedings of the IJCNLP
2005, Companion volume, pages 222?227.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature ge-
netics, 25:25?29.
36
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 28?37,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
EVEX: A PubMed-Scale Resource for Homology-Based Generalization
of Text Mining Predictions
Sofie Van Landeghem1,2, Filip Ginter3, Yves Van de Peer1,2 and Tapio Salakoski3,4
1. Dept. of Plant Systems Biology, VIB, Belgium
2. Dept. of Plant Biotechnology and Genetics, Ghent University, Belgium
3. Dept. of Information Technology, University of Turku, Finland
4. Turku Centre for Computer Science (TUCS), Finland
solan@psb.ugent.be, ginter@cs.utu.fi
yvpee@psb.ugent.be, tapio.salakoski@utu.fi
Abstract
In comparative genomics, functional annota-
tions are transferred from one organism to an-
other relying on sequence similarity. With
more than 20 million citations in PubMed, text
mining provides the ideal tool for generating
additional large-scale homology-based predic-
tions. To this end, we have refined a recent
dataset of biomolecular events extracted from
text, and integrated these predictions with
records from public gene databases. Account-
ing for lexical variation of gene symbols, we
have implemented a disambiguation algorithm
that uniquely links the arguments of 11.2 mil-
lion biomolecular events to well-defined gene
families, providing interesting opportunities
for query expansion and hypothesis genera-
tion. The resulting MySQL database, includ-
ing all 19.2 million original events as well
as their homology-based variants, is publicly
available at http://bionlp.utu.fi/.
1 Introduction
Owing to recent advances in high-throughput se-
quencing technologies, whole genomes are being se-
quenced at an ever increasing rate (Metzker, 2010).
However, for the DNA sequence to truly unravel its
secrets, structural annotation is necessary to identify
important elements on the genome, such as coding
regions and regulatory motifs. Subsequently, func-
tional annotation is crucial to link these structural
elements to their biological function.
Functional annotation of genomes often requires
extensive in vivo experiments. This time-consuming
procedure can be expedited by integrating knowl-
edge from closely related species (Fulton et al,
2002; Proost et al, 2009). Over the past few
years, homology-based functional annotation has
become a widely used technique in the bioinformat-
ics field (Loewenstein et al, 2009).
Unfortunately, many known genotype-phenotype
links are still buried in research articles: The largest
biomolecular literature database, PubMed, consists
of more than 20 million citations1. Due to its expo-
nential growth, automated tools have become a ne-
cessity to uncover all relevant information.
There exist several text mining efforts focusing
on pairwise interactions and co-occurrence links of
genes and proteins (Hoffmann and Valencia, 2004;
Ohta et al, 2006; Szklarczyk et al, 2011). In
this paper, we present the first large-scale text min-
ing resource which both utilizes a detailed event-
based representation of biological statements and
provides homology-based generalization of the text
mining predictions. This resource results from the
integration of text mining predictions from nearly
18M PubMed citations with records from public
gene databases (Section 2). To enable such inte-
gration, it is crucial to first produce canonical forms
of the automatically tagged biological entities (Sec-
tion 3.1). A gene symbol disambiguation algorithm
then links these canonical forms to gene families and
gene identifiers (Section 3.2). Finally, a MySQL-
driven framework aggregates the text-bound event
occurrences into generalized events, creating a rich
resource of homology-based predictions extracted
from text (Section 3.3).
1http://www.ncbi.nlm.nih.gov/pubmed/
28
IL-2
NF-kappa B
Pos. regulation
Binding
p55
ca th
th th
Figure 1: Event representation of the statement IL-2 acts
by enhancing binding activity of NF-kappa B to p55, il-
lustrating recursive nesting of events where the (th)eme
of the Positive regulation event is the Binding event. The
(ca)use argument is the gene symbol IL-2.
2 Data
Our integrative approach is based on two types
of data: text mining predictions generated for the
whole of PubMed (Section 2.1) and publicly avail-
able gene database records (Section 2.2).
2.1 Text mining predictions
Bjo?rne et al (2010) have applied to all PubMed ab-
stracts an event extraction pipeline comprising of
the BANNER named entity recognizer (Leaman and
Gonzalez, 2008) and the Turku Event Extraction
System (Bjo?rne et al, 2009). The resulting dataset
contains 36.5M occurrences of gene / gene product
(GGP) entities and 19.2M occurrences of events per-
taining to these entities.
The file format and information scheme of
the resource correspond to the definition of the
BioNLP?09 Shared Task on Event Extraction (Kim
et al, 2009). Events are defined as typed relations
between arguments that are either entity occurrences
or, recursively, other events. There are nine possi-
ble event types: Localization, Binding, Gene expres-
sion, Transcription, Protein catabolism, Phosphory-
lation, Regulation, Positive regulation, and Negative
regulation. Further, arguments are assigned a role:
Theme or Cause for the core arguments and AtLoc,
ToLoc, Site, and CSite for auxiliary arguments that
define additional information such as cellular loca-
tion of the event. In addition, each event occurrence
may be marked as negative and/or speculative. Fig-
ure 1 depicts an example event.
2.2 Database records
During the last few decades, several large-scale
databases have been designed to deal with the abun-
dance of data in the field of life sciences. In this
study, we are particularly interested in databases of
gene symbols and homologous gene groups or gene
families. These families are composed by clustering
pairwise orthologs, which are genes sharing com-
mon ancestry evolved through speciation, often hav-
ing a similar biological function.
Entrez Gene2 is the default cross-species gene
nomenclature authority, hosted by NCBI (Sayers et
al., 2009). It bundles information from species-
specific resources as well as from RefSeq records3.
More than 8M Entrez Gene identifiers were col-
lected from over 8,000 different taxa, all together
referring to more than 10M distinct gene symbols,
descriptions, abbreviations and synonyms. While
Entrez Gene IDs are unique across taxa, gene sym-
bols are highly ambiguous. Section 3 describes how
we tackle gene symbol ambiguity across and within
species.
The HomoloGene4 database is also hosted at
NCBI and provides the results of automated de-
tection of orthologs in 20 completely sequenced
eukaryotic genomes. From this resource, around
43,700 HomoloGene families were extracted, con-
taining about 242,000 distinct genes. A second set
of gene families was retrieved from Ensembl (Flicek
et al, 2011). More than 13,000 Ensembl clusters
were assembled comprising about 220,000 genes.
As a general rule, the functional similarity scores
per homologous pair in a gene family are higher
when more stringent criteria are used to define the
families (Hulsen et al, 2006). While HomoloGene
consists of many strict clusters containing true or-
thologs, bigger Ensembl clusters were obtained by
assembling all pairwise orthologous mappings be-
tween genes. Ultimately, such clusters may also in-
clude paralogs, genes originated by duplication. As
an example, consider the nhr-35 gene from C. el-
egans, which has both Esr-1 and Esr-2 as known
orthologs, resulting in the two paralogs being as-
signed to the same final Ensembl cluster. The En-
sembl clustering algorithm can thus be seen as a
more coarse-grained method while the HomoloGene
mapping results in more strictly defined gene fami-
lies. The implications are discussed on a specific
use-case in Section 4.3.1.
2http://www.ncbi.nlm.nih.gov/gene
3http://www.ncbi.nlm.nih.gov/refseq
4http://www.ncbi.nlm.nih.gov/homologene
29
3 Methods
Widely known biomolecular events occur in many
different articles, often mentioning a different gene
synonym or lexical variant. Canonicalization of the
entity occurrences deals with these lexical variants
(Section 3.1), while the disambiguation algorithm
then uniquely links canonical forms to a gene fam-
ilies (Section 3.2). In a final step, these links can
be used to generalize the text mining events to their
homology-based variants (Section 3.3).
3.1 Canonicalization of the entity occurrences
The entity occurrences predicted by BANNER (Sec-
tion 2.1) follow the guidelines of GENETAG (Tan-
abe et al, 2005), the corpus it was trained on. These
guidelines allow not only gene and gene products,
but also related entities such as protein complexes
and gene promoters. Furthermore, BANNER fre-
quently tags noun phrases such as human Esr-1 gene
rather than only the minimal symbol Esr-1.
To enable integration of text mining predictions
with external databases, it is necessary to refine the
entity occurrences to canonical forms that can be
linked to gene records such as those in Entrez Gene.
To this end, common prefixes and suffixes such as
gene and wild-type should be removed.
In a first step towards canonicalization of the en-
tities, a mapping table was assembled containing
common contexts in which a gene symbol appears
and where the full noun phrase can be reduced to
that embedded symbol for the sake of information
retrieval (Table 1). This mapping table was created
by matching5 a list of candidate minimal gene sym-
bols to the extracted BANNER entities.
To define the list of candidate minimal gene sym-
bols, two approaches have been combined. First,
a set of around 15,000 likely gene symbols is ex-
tracted by looking for single token strings that were
tagged by BANNER at least 50% of the times they
occur in a PubMed abstract. Secondly, all official
gene names are extracted from Entrez Gene. As this
latter list also contains common English words such
as was and protein, we have only selected those that
were likely to be standalone gene symbols. We cal-
culate this likelihood by Cs/(Cs + Cn) where Cs
5All string matching steps have been implemented using the
SimString string retrieval library (Okazaki and Tsujii, 2010).
GGP contexts
-ORG- -GGP- gene
-GGP- sequences
mutant -GGP- proteins
-GGP- homologs
cytoplasmic wild-type -GGP-
Table 1: This table lists a few examples of entity occur-
rences extracted with BANNER that are resolved to the
embedded minimal gene symbol (marked as -GGP-).
is the number of times a string is tagged standalone
and Cn is the number of times the string occurs in
PubMed but is not tagged (neither as standalone,
nor as part of a larger entity). This likelihood rep-
resents the proportion of standalone occurrences of
the string that are tagged. We experimentally set a
threshold on this value to be higher than 0.01, ex-
cluding a list of 2,865 common English words.
Subsequently, all BANNER entity occurrences
are screened and likely minimal gene symbols sub-
stituted with -GGP-, resulting in generalized con-
texts. Then, we have matched these contexts with an
extensive list of organism names from the Linneaus
distribution (Gerner et al, 2010) and a small col-
lection of miscellaneous non-formal organism terms
(e.g. monkey), replacing all known organisms with
an -ORG- placeholder. Finally, we have excluded
all contexts where the embedded GGP is likely to
be functionally too far removed from the embed-
ding noun phrase (e.g. ?-GGP- inhibitor?), rely-
ing on a corpus defining and categorizing such re-
lationships (Ohta et al, 2009). Some of the contexts
that were retained after this step, such as ?-GGP-
mutant? or ?-GGP- promoter? still refer to entities
that are distinctly different from the embedded GGP.
These results are considered valid, as the goal of the
affix stripping algorithm is to increase recall and of-
fer explorative results involving various types of in-
formation on gene symbols.
The final list of contexts, generalized with -GGP-
and -ORG- placeholders, is split into two separate
lists of prefixes and suffixes, ranked by frequency.
Also, numerical affixes as well as those shorter than
3 characters are discarded from these lists.
30
Each text-bound entity occurrence can then be
canonicalized by applying the following algorithm:
1. Replace all organism names with the place-
holder -ORG-
2. If the string can be matched6 to a known sym-
bol in Entrez Gene, stop the algorithm
3. Find all occurring affixes and strip the one as-
sociated with the highest count
4. Repeat (2-3) until no more affixes match
5. Strip remaining -ORG- placeholders and all
whitespace and non-alphanumeric characters
For example, the canonicalization of human anti-
inflammatory il-10 gene proceeds as -ORG- anti-
inflamatory il-10 gene ? anti-inflammatory il-10
gene ? anti-inflammatory il-10 ? il-10, at which
point the string il10 is matched in Entrez Gene, be-
coming the final canonical form. In the following
section, we describe how these canonical forms are
assigned to unique gene families.
3.2 Disambiguation of gene symbols
Gene name ambiguity is caused by the lack of
community-wide approved standards for assigning
gene symbols (Chen et al, 2005). Furthermore, au-
thors often introduce their own lexical variants or ab-
breviations for specific genes.
From Entrez Gene, we have retrieved 8,034,512
gene identifiers that link to 10,177,542 unique sym-
bols. Some of these symbols are highly ambiguous
and uninformative, such as NEWENTRY. Others are
ambiguous because they are abbreviations. Finally,
many symbols can not be linked to one unique gene,
but do represent a homologous group of genes shar-
ing a similar function. Often, orthologs with similar
functions are assigned similar official gene names.
The first step towards gene symbol disambigua-
tion involves collecting all possible synonyms for
each gene family from either Ensembl or Homolo-
Gene. We strip these symbols of all whitespace and
non-alphanumeric characters to match the final step
in the canonicalization algorithm.
The disambiguation pipeline then synthesizes the
ambiguity for all gene symbols by counting their oc-
currences in the gene families. Each such relation
6The comparison is done ignoring whitespace and non-
alphanumeric characters.
Family Type of symbol Count
HG:47906 Default symbol 7
HG:99739 Synonym 1
HG:3740 Synonym 1
ECL:10415 Default symbol 12
ECL:8731 Synonym 1
ECL:8226 Synonym 1
Table 2: Intrinsic ambiguity of esr1, analysed in both Ho-
moloGene (HG) and Ensembl clusters (ECL).
records whether the symbol is registered as an offi-
cial or default gene symbol, as the gene description,
an abbreviation, or a synonym. As an example, Ta-
ble 2 depicts the intrinsic ambiguity of esr1.
In a subsequent step, the ambiguity is reduced by
applying the following set of rules, relying on a pri-
ority list imposed on the type of the symbol, ensur-
ing we choose an official or default symbol over a
description or synonym.
1. If one family has the most (or all) hits for a
certain symbol and these hits refer to a sym-
bol type having priority over other possibilities,
this family is uniquely assigned to that symbol.
2. If a conflict exists between one family having
the highest linkage count for a certain sym-
bol, and another family linking that symbol to
a higher priority type, the latter is chosen.
3. If two families have equal counts and type pri-
orities for a certain symbol, this symbol can
not be unambiguously resolved and is removed
from further processing.
4. If the ambiguity is still not resolved, all fami-
lies with only one hit for a certain symbol are
removed, and steps 1-3 repeated.
The above disambiguation rules were applied to
the 458,505 gene symbols in HomoloGene. In the
third step, 6,891 symbols were deleted, and when
the algorithm ends, 555 symbols remained ambigu-
ous. In total, 451,059 gene symbols could thus be
uniquely linked to a HomoloGene family (98%). In
the esr1 example depicted in Table 2, only the link to
HG:47906 will be retained. The results for Ensembl
were very similar, with 342,252 out of 345,906 sym-
bols uniquely resolved (99%).
31
All Ensembl HomoloGene
No stripping 39.9 / 67.5 / 50.2 62.8 / 70.0 / 66.2 64.2 / 69.2 / 66.6
Affix stripping 48.7 / 82.3 / 61.1 61.7 / 88.0 / 72.5 62.8 / 87.9 / 73.3
Table 3: Influence on precision, recall and F-measure (given as P/R/F) of the affix stripping algorithm on the entity
recognition module, as measured across all BioNLP?09 ST entity occurrences and also separately on the subsets which
can be uniquely mapped to Ensembl and HomoloGene (77.3% and 75.5% of all occurrences, respectively).
3.3 Homology-based generalization of the text
mining events
In order to gain a broader insight into the 19.2M
event occurrences obtained by Bjo?rne et al (2010),
it is necessary to identify and aggregate multiple oc-
currences of the same underlying event. This gen-
eralization also notably simplifies working with the
data, as the number of generalized events is an or-
der of magnitude smaller than the number of event
occurrences.
To aggregate event occurrences into generalized
events, it is necessary to first define equivalence
of two event occurrences: Two event occurrences
are equivalent, if they have the same event type,
and their core arguments are equivalent and have
the same roles. For arguments that are themselves
events, the equivalence is applied recursively. The
equivalence of arguments that are entities can be es-
tablished in a number of different ways, affecting
the granularity of the event generalization. One ap-
proach is to use the string canonicalization described
in Section 3.1; two entities are then equivalent if
their canonical forms are equal. This, however, does
not take symbol synonymy into account. A differ-
ent approach which we believe to be more power-
ful, is to disambiguate gene symbols to gene fam-
ilies, as described in Section 3.2. In this latter ap-
proach, two entity occurrences are deemed equiv-
alent if their canonical forms can be uniquely re-
solved to the same gene family. Consequently, two
event occurrences are considered equivalent if they
pertain to the same gene families.
As both approaches have their merits, three dis-
tinct generalization procedures have been imple-
mented: one on top of the canonical gene symbols,
and one on top of the gene families defined by Ho-
moloGene and Ensembl, respectively.
4 Results and discussion
4.1 Evaluation of entity canonicalization
The affix stripping step of the canonicalization al-
gorithm described in Section 3.1 often substantially
shortens the entity strings and an evaluation of its
impact is thus necessary. One of the primary objec-
tives of the canonicalization is to increase the pro-
portion of entity occurrences that can be matched
to Entrez Gene identifiers. We evaluate its im-
pact using manually tagged entities from the pub-
licly available BioNLP?09 Shared Task (ST) train-
ing set, which specifically aims at identifying enti-
ties that are likely to match gene and protein sym-
bol databases (Kim et al, 2009). Further, the ST set
comprises of PubMed abstracts and its underlying
text is thus covered in our data. Consequently, the
ST training set forms a very suitable gold standard
for the evaluation.
First, we compare7 the precision and recall of
the BANNER output before and after affix stripping
(Table 3, first column). The affix stripping results in
a notable gain in both precision and recall. In partic-
ular, the nearly 15pp gain on recall clearly demon-
strates that the affix stripping results in entity strings
more likely to match existing resources.
Second, the effect of affix stripping is evaluated
on the subset of entity strings that can be uniquely
mapped into Ensembl and HomoloGene (77.3% and
75.5% of the ST entity strings, respectively). This
subset is of particular interest, since the generalized
events are built on top of the entities that can be
found in these resources and any gain on this par-
ticular subset is thus likely to be beneficial for the
overall quality of the generalized events. Here, af-
fix stripping leads to a substantial increase in re-
call when compared to no stripping being applied
7The comparison is performed on the level of bags of strings
from each PubMed abstract, avoiding the complexity of align-
ing character offsets across different resources.
32
Entities Ent. occ.
Canonical 1.6M (100%) 36.4M (100%)
HomoloGene 64.0K (3.9%) 18.8M (51.7%)
Ensembl 54.6K (3.3%) 18.7M (51.2%)
Table 4: Entity coverage comparison. The entities col-
umn gives the number of canonical entities, also shown
as a percentage of all unique, canonical BANNER entities
(1.6M). The entity occurrences column shows the num-
ber of occurrences for which the generalization could be
established, out of the total number of 36.4M extracted
BANNER entities.
(around 18pp), which is offset by a comparatively
smaller drop in precision (less than 2pp). Global
performance increases with about 6.5pp in F-score
for both the Ensembl and HomoloGene subsets.
Bjo?rne et al (2010) used a simpler, domain-
restricted affix stripping algorithm whereby candi-
date affixes were extracted only from NP-internal
relations in the GENIA corpus (Ohta et al, 2009).
This original algorithm affects 11.5% unique en-
tity strings and results in 3.5M unique canonical
forms and 4.5M unique events. In comparison,
our current affix stripping algorithm results in 1.6M
unique canonical forms and 3.2M unique events,
thus demonstrating the improved generalization ca-
pability of the current affix stripping algorithm.
4.2 Evaluation of homology-based
disambiguation
The symbol to gene family disambiguation algo-
rithm succesfully resolves almost all gene symbols
in HomoloGene or Ensembl (Section 3.2). However,
not all genes are a member of a known gene family,
and the event generalization on top of the gene fam-
ilies will thus inevitably discard a significant portion
of the text mining predictions.
Table 4 shows that only a small fraction of all
unique canonical entities matches the gene families
from HomoloGene or Ensembl (3.9% and 3.3%, re-
spectively). However, this small fraction of symbols
accounts for approximately half of all entity occur-
rences in the text mining data (51.7% and 51.2%).
The algorithm thus discards a long tail of very in-
frequent entities. Table 5 shows a similar result for
the events and event occurrences. We find that map-
ping to HomoloGene and Ensembl results in a con-
siderably smaller number of generalized events, yet
Events Ev. occ.
Canonical 3223K 19.2M (100%)
HomoloGene 614K 10.2M (53%)
Ensembl 505K 10.2M (52.9%)
Table 5: Comparison of the three event generalization
methods. The events column gives the number of gen-
eralized events and the event occurrences column shows
the number of occurrences for which the generalization
could be established, out of the total number of 19.2M
text-bound event occurrences.
accounts for more than half of all event occurrences
(53% and 52.9%, respectively).
Finally, merging the canonical entities and the
corresponding generalized events for both Homolo-
Gene and Ensembl, we can assess the percentage of
all text mining predictions that can be linked to at
least one homology-based variant: 21.8M (59.8%)
of all entity occurrences and 11.2M (58.4%) of all
event occurrences can be resolved. Nearly 60% of
entity and event occurrences in the original text min-
ing data could thus be uniquely linked to well de-
fined gene families. Also, as shown in Section 4.1,
the 60% entities retained are expected to contain
proportionally more true positives, compared to the
40% entities that could not be mapped. One might
speculate that a similar effect will be seen also
among events.
4.3 MySQL database and Use-cases
As the PubMed events extracted by Bjo?rne et
al. (2010) are purely text-bound and distributed as
text files, they can not easily be searched. One im-
portant contribution of this paper is the release of all
text mining predictions as a MySQL database. Dur-
ing the conversion, all original information is kept,
including links to the PubMed IDs and the offsets
in text for all entities and triggers, referring to the
original strings as they were obtained by BANNER
and the event extraction system. This allows for fast
retrieval of text mining data on a PubMed-scale.
As described in Section 3.3, three distinct gener-
alization methods have been applied to the original
events. On the database level, each generalization is
represented by a separate set of tables for the gen-
eralized events and their arguments, aggregating im-
portant event statistics such as occurrence count and
33
Figure 2: Database scheme of the generalized events. Three instantiations of the general scheme (i.e. the three leftmost
tables) exist in the database. Following the dotted lines, each instance links to a different table in which the canonical
forms and the gene identifiers can be looked up.
negation/speculation information (Figure 2). Table 5
states general statistics for the three different sets.
Finally, a mapping table is provided that links the
generalized events to the event occurrences from
which they were abstracted. More technical details
on the MySQL scheme and example queries can be
found at http://bionlp.utu.fi/.
4.3.1 Use case: Query expansion
The MySQL database is the ideal resource to re-
trieve information on a PubMed-scale for a certain
gene or set of genes. Suppose there would be an in-
terest in Esr-1, then all abstract events on top of the
canonical form esr1 can be retrieved. However, re-
sults will display events for both the Estrogen recep-
tor as well as for the much less common Enhancer of
shoot regeneration. Furthermore, it makes no sense
to add known synonyms of both genes to the query,
as this will generate an incoherent list of synonyms
and even more false positive hits.
In such a case, it is to be recommended to use
the homology-based generalization of the events.
For example, esr1 hits the HomoloGene family
HG:47906, which contains all Estrogen receptor-
alpha genes across eukaryotic species. Canonical
symbols linked to this family include era, estra,
nr3a1 and estrogenreceptor1alpha.
A similar analysis can be done for the Ensembl
clustering, where esr1 links to ECL:10415. How-
ever, this more coarse-grained Ensembl family con-
tains all genes from the two closely related sub-
groups Estrogen receptor and Estrogen related re-
ceptor, both belonging to the Estrogen Receptor-
like group of the superfamily of nuclear recep-
tors (Zhang et al, 2004). On top of the synonyms
mentioned previously, this family thus also includes
erb, esr2b, errbetagamma and similartoesrrbpro-
tein. By using this list for query expansion, more
general text mining predictions can be retrieved.
It is to be noted that both homology-based ap-
proaches will also include events mentioning Esr-1
as the abbreviation for Enhancer of shoot regener-
ation. While this usage is much less common, it
will result in a few false positive hits. These false
positives may be prevented by taking into account
local context such as organism mentions, as the En-
hancer of shoot regeneration gene is only present
in A. thaliana. We believe our current homology-
based approach could be integrated with existing
or future normalization algorithms (Krallinger and
Hirschman, 2007; Wermter et al, 2009) to provide
such fine-grained resolution. This is regarded as in-
teresting future work.
4.3.2 Use case: Homology-based hypotheses
Consider a newly annotated, protein-coding gene
for which no database information currently ex-
ists. To generate homology-based text mining hy-
potheses, the orthologs of this gene first have to
be defined by assessing sequence similarity through
BLAST (Altschul et al, 1997).
Imagine for example a newly sequenced genome
X for which a gene similar to the mouse gene Esr-
1 is identified. This gene will soon be known as
?genome X Esr-1? and thus related to the Esr-1 gene
family. As described in Section 4.3.1, homology-
34
based query expansion can then be used to retrieve
all events involving lexical variants and synonyms
of the canonical string esr1.
5 Conclusions
We present a large-scale resource for research and
application of text mining from biomedical litera-
ture. The resource is obtained by integrating text
mining predictions in the dataset of Bjo?rne et al
(2010) with public databases of gene symbols and
gene families: Entrez Gene, Ensembl, and Homolo-
Gene. The integration is performed on the level of
gene families, allowing for a number of novel use
cases for both text mining and exploratory analysis
of the biological statements in PubMed literature. To
achieve the linking between text-based event predic-
tions and gene databases, several algorithms are in-
troduced to solve the problems involved.
First, we propose an algorithm for stripping af-
fixes in entity occurrences tagged by the BAN-
NER named entity recognizer, addressing the prob-
lem of such entities often including wider context
which prevents direct matching against gene symbol
databases. Using the BioNLP?09 Shared Task data
as gold standard, we show that the algorithm sub-
stantially increases both precision and recall of the
resulting canonical entities, the gain in recall being
particularly pronounced.
Second, we propose an algorithm which assigns
to the vast majority of gene symbols found in Ho-
moloGene and Ensembl a single unique gene fam-
ily, resolving the present intra-organism ambiguity
based on symbol occurrence statistics and symbol
type information. Matching these disambiguated
symbols with the affix-stripped canonical forms of
entity occurrences, we were able to assign a unique
gene family from either HomoloGene or Ensembl to
nearly 60% of all entities in the text, thus linking the
text-bound predictions with gene databases.
Finally, we use the resolution of entity occur-
rences to unique gene families to generalize the
events in the text mining data, aggregating together
event occurrences whose arguments are equivalent
with respect to their gene family. Depending on
whether HomoloGene or Ensembl is used for the
gene family definition, this generalization process
results in 500K-600K generalized events, which to-
gether aggregate over 11.2M (58.4%) of all event
occurrences in the text mining data. Being able
to link the literature-based events with well-defined
gene families opens a number of interesting new
use-cases for biomedical text mining, such as the
ability to use the homology information to search for
events relevant to newly discovered sequences. The
remaining 41.6% of event occurrences not general-
izable to gene families can still be retrieved through
an additional generalization on the level of entity
canonical forms.
All relevant data, namely all original events and
entities together with their canonical forms, the
generalizations of events based on canonical entity
forms and gene families, as well as the gene symbol
to unique family mapping are made publicly avail-
able as records in a MySQL database. We also pro-
vide detailed online documentation of the database
scheme and example queries. Finally, we release the
affix lists used in the canonicalization algorithm.
We believe this resource to be very valuable
for explorative analysis of text mining results and
homology-based hypothesis generation, as well as
for supporting future research on data integration
and biomedical text mining.
One important future work direction is a further
disambiguation of canonical gene symbols to unique
gene identifiers rather than entire gene families,
which would allow for more fine-grained event gen-
eralization. There is an ongoing active, community-
wide research focusing on this challenge and the cur-
rent resource could be integrated as an additional
source of information. Another future work direc-
tion is to create a visualization method and a web
interface which would allow simple, user-friendly
access to the data for researchers outside of the
BioNLP research community itself.
Acknowledgments
The authors would like to thank Sampo Pyysalo
(University of Tokyo) and Jari Bjo?rne (University
of Turku) for their contribution. SVL would like
to thank the Research Foundation Flanders (FWO)
for funding her research and a travel grant to Turku.
This work was partly funded by the Academy of Fin-
land and the computational resources were provided
by CSC IT Center for Science Ltd., Espoo, Finland.
35
References
Stephen F. Altschul, Thomas L. Madden, Alejandro A.
Scha?ffer, Jinghui Zhang, Zheng Zhang, Webb Miller,
and David J. Lipman. 1997. Gapped BLAST and PSI-
BLAST: a new generation of protein database search
programs. Nucleic acids research, 25(17):3389?3402,
September.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In BioNLP ?09: Proceedings of the Work-
shop on BioNLP, pages 10?18, Morristown, NJ, USA.
Association for Computational Linguistics.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Scaling up biomed-
ical event extraction to the entire PubMed. In Pro-
ceedings of the 2010 Workshop on Biomedical Natu-
ral Language Processing, BioNLP ?10, pages 28?36,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Lifeng Chen, Hongfang Liu, and Carol Friedman. 2005.
Gene name ambiguity of eukaryotic nomenclatures.
Bioinformatics, 21:248?256, January.
Paul Flicek, M. Ridwan Amode, Daniel Barrell, Kathryn
Beal, Simon Brent, Yuan Chen, Peter Clapham, Guy
Coates, Susan Fairley, Stephen Fitzgerald, Leo Gor-
don, Maurice Hendrix, Thibaut Hourlier, Nathan John-
son, Andreas Ka?ha?ri, Damian Keefe, Stephen Keenan,
Rhoda Kinsella, Felix Kokocinski, Eugene Kulesha,
Pontus Larsson, Ian Longden, William McLaren, Bert
Overduin, Bethan Pritchard, Harpreet Singh S. Riat,
Daniel Rios, Graham R. Ritchie, Magali Ruffier,
Michael Schuster, Daniel Sobral, Giulietta Spudich,
Y. Amy Tang, Stephen Trevanion, Jana Vandrov-
cova, Albert J. Vilella, Simon White, Steven P.
Wilder, Amonida Zadissa, Jorge Zamora, Bronwen L.
Aken, Ewan Birney, Fiona Cunningham, Ian Dunham,
Richard Durbin, Xose? M. Ferna?ndez-Suarez, Javier
Herrero, Tim J. Hubbard, Anne Parker, Glenn Proc-
tor, Jan Vogel, and Stephen M. Searle. 2011. Ensembl
2011. Nucleic acids research, 39(Database issue), Jan-
uary.
Theresa M. Fulton, Rutger Van der Hoeven, Nancy T.
Eannetta, and Steven D. Tanksley. 2002. Identifica-
tion, analysis, and utilization of conserved ortholog
set markers for comparative genomics in higher plants.
Plant Cell, 14(5):1457?1467.
Martin Gerner, Goran Nenadic, and Casey M. Bergman.
2010. LINNAEUS: a species name identification sys-
tem for biomedical literature. BMC bioinformatics,
11(1):85+, February.
Robert Hoffmann and Alfonso Valencia. 2004. A gene
network for navigating the literature. Nat Genet,
36(7):664, Jul.
Tim Hulsen, Martijn Huynen, Jacob de Vlieg, and Peter
Groenen. 2006. Benchmarking ortholog identification
methods using functional genomics data. Genome Bi-
ology, 7(4):R31+, April.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Martin Krallinger and Lynette Hirschman, editors. 2007.
Proceedings of the Second BioCreative Challenge
Evaluation Workshop, Madrid, April.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedi-
cal named entity recognition. Pacific Symposium on
Biocomputing. Pacific Symposium on Biocomputing,
pages 652?663.
Yaniv Loewenstein, Domenico Raimondo, Oliver C.
Redfern, James Watson, Dmitrij Frishman, Michal
Linial, Christine Orengo, Janet Thornton, and Anna
Tramontano. 2009. Protein function annotation
by homology-based inference. Genome biology,
10(2):207, February.
Michael L. Metzker. 2010. Sequencing technolo-
gies - the next generation. Nature reviews. Genetics,
11(1):31?46, January.
Tomoko Ohta, Yusuke Miyao, Takashi Ninomiya, Yoshi-
masa Tsuruoka, Akane Yakushiji, Katsuya Masuda,
Jumpei Takeuchi, Kazuhiro Yoshida, Tadayoshi Hara,
Jin-Dong Kim, Yuka Tateisi, and Jun?ichi Tsujii.
2006. An intelligent search engine and GUI-based ef-
ficient MEDLINE search tool based on deep syntactic
parsing. In Proceedings of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 17?20, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Tomoko Ohta, Sampo Pyysalo, Kim Jin-Dong, and
Jun?ichi Tsujii. 2009. A re-evaluation of biomedi-
cal named entity - term relations. In Proceedings of
LBM?09.
Naoaki Okazaki and Jun?ichi Tsujii. 2010. Simple and
efficient algorithm for approximate dictionary match-
ing. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 851?859, Beijing, China, August.
Sebastian Proost, Michiel Van Bel, Lieven Sterck, Kenny
Billiau, Thomas Van Parys, Yves Van de Peer, and
Klaas Vandepoele. 2009. PLAZA: A comparative ge-
nomics resource to study gene and genome evolution
in plants. Plant Cell, 21(12):3718?3731, December.
36
Eric W. W. Sayers, Tanya Barrett, Dennis A. A. Ben-
son, Stephen H. H. Bryant, Kathi Canese, Vyacheslav
Chetvernin, Deanna M. M. Church, Michael Dicuc-
cio, Ron Edgar, Scott Federhen, Michael Feolo, Lewis
Y. Y. Geer, Wolfgang Helmberg, Yuri Kapustin, David
Landsman, David J. J. Lipman, Thomas L. L. Madden,
Donna R. R. Maglott, Vadim Miller, Ilene Mizrachi,
James Ostell, Kim D. D. Pruitt, Gregory D. D.
Schuler, Edwin Sequeira, Stephen T. T. Sherry, Martin
Shumway, Karl Sirotkin, Alexandre Souvorov, Grig-
ory Starchenko, Tatiana A. A. Tatusova, Lukas Wag-
ner, Eugene Yaschenko, and Jian Ye. 2009. Database
resources of the National Center for Biotechnology
Information. Nucleic Acids Research, 37(Database
issue):D5?15, January.
Damian Szklarczyk, Andrea Franceschini, Michael
Kuhn, Milan Simonovic, Alexander Roth, Pablo
Minguez, Tobias Doerks, Manuel Stark, Jean Muller,
Peer Bork, Lars J. Jensen, and Christian von Mer-
ing. 2011. The STRING database in 2011: functional
interaction networks of proteins, globally integrated
and scored. Nucleic acids research, 39(Database
issue):D561?D568, January.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC bioinformatics, 6 Suppl 1.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GENO. Bioinformatics, 25(6):815?821.
Zhengdong Zhang, Paula E. Burch, Austin J. Cooney,
Rainer B. Lanz, Fred A. Pereira, Jiaqian Wu,
Richard A. Gibbs, George Weinstock, and David A.
Wheeler. 2004. Genomic analysis of the nuclear re-
ceptor family: New insights into structure, regulation,
and evolution from the rat genome. Genome Research,
14(4):580?590, April.
37
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 82?90,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
PubMed-Scale Event Extraction for Post-Translational Modifications,
Epigenetics and Protein Structural Relations
Jari Bjo?rne 1,2, Sofie Van Landeghem 3,4, Sampo Pyysalo 5, Tomoko Ohta 5,
Filip Ginter 2, Yves Van de Peer 3,4, Sophia Ananiadou 5 and Tapio Salakoski 1,2
1Turku Centre for Computer Science (TUCS), Joukahaisenkatu 3-5B, 20520 Turku, Finland
2Department of Information Technology, 20014 University of Turku, Finland
3Department of Plant Systems Biology, VIB, Technologiepark 927, 9052 Gent, Belgium
4Department of Plant Biotechnology and Bioinformatics, Ghent University, Gent, Belgium
5National Centre for Text Mining and University of Manchester,
Manchester Interdisciplinary Biocentre,131 Princess Street, Manchester, UK
Abstract
Recent efforts in biomolecular event extrac-
tion have mainly focused on core event types
involving genes and proteins, such as gene
expression, protein-protein interactions, and
protein catabolism. The BioNLP?11 Shared
Task extended the event extraction approach
to sub-protein events and relations in the Epi-
genetics and Post-translational Modifications
(EPI) and Protein Relations (REL) tasks. In
this study, we apply the Turku Event Ex-
traction System, the best-performing system
for these tasks, to all PubMed abstracts and
all available PMC full-text articles, extract-
ing 1.4M EPI events and 2.2M REL relations
from 21M abstracts and 372K articles. We
introduce several entity normalization algo-
rithms for genes, proteins, protein complexes
and protein components, aiming to uniquely
identify these biological entities. This nor-
malization effort allows direct mapping of
the extracted events and relations with post-
translational modifications from UniProt, epi-
genetics from PubMeth, functional domains
from InterPro and macromolecular structures
from PDB. The extraction of such detailed
protein information provides a unique text
mining dataset, offering the opportunity to fur-
ther deepen the information provided by ex-
isting PubMed-scale event extraction efforts.
The methods and data introduced in this study
are freely available from bionlp.utu.fi.
1 Introduction
Biomedical domain information extraction has in re-
cent years seen a shift from focus on the extraction
of simple pairwise relations (Pyysalo et al, 2008;
Tikk et al, 2010) towards the extraction of events,
represented as structured associations of arbitrary
numbers of participants in specific roles (Ananiadou
et al, 2010). Domain event extraction has been pop-
ularized in particular by the BioNLP Shared Task
(ST) challenges in 2009 and 2011 (Kim et al, 2009;
Kim et al, 2011). While the BioNLP ST?09 em-
phasized protein interactions and regulatory rela-
tionships, the expressive event formalism can also
be applied to the extraction of statements regarding
the properties of individual proteins. Accordingly,
the EPI (Epigenetics and Post-Translational Modi-
fications) subchallenge of the BioNLP ST?11 pro-
vided corpora and competitive evaluations for the
detection of epigenetics and post-translational mod-
ification (PTM) events, while the REL (Entity Re-
lations) subchallenge covers structural and complex
membership relations of proteins (Ohta et al, 2011b;
Pyysalo et al, 2011). The complex memberships
and domains define the physical nature of an indi-
vidual protein, which is closely linked to its func-
tion and biological activity. Post-translational mod-
ifications alter and regulate this activity via struc-
tural or chemical changes induced by the covalent
attachment of small molecules to the protein. In
epigenetic regulation, gene expression is controlled
by the chemical modification of DNA and the his-
tone proteins supporting chromosomal DNA. All of
these aspects are important for defining the biologi-
cal role of a protein, and thus the EPI and REL tasks
enable the development of text mining systems that
can extract a more complete picture of the biomolec-
ular reactions and relations than previously possible
(cf. Table 1). Furthermore, previous work has shown
promising results for improving event extraction by
82
integration of ?static? entity relations (Pyysalo et al,
2009), in particular for the previously only available
PTM event, phosphorylation (Van Landeghem et al,
2010).
Information on protein modifications is avail-
able in general-purpose protein databases such as
UniProt, and there are also a number of dedicated
database resources covering such protein modifica-
tions (Wu and others, 2003; Lee et al, 2006; Li et
al., 2009). While the automatic extraction of PTMs
from text has also been considered in a number of
earlier studies, these have primarily involved single
PTM reactions extracted with special-purpose meth-
ods (Hu et al, 2005; Yuan et al, 2006; Lee et al,
2008). The EPI task and associated work (Ohta et
al., 2010) were the first to target numerous PTM re-
actions in a general framework using retrainable ex-
traction methods. The automatic detection of mod-
ification statements using keyword matching-based
methods has been applied also in support of DNA
methylation DB curation (Ongenaert et al, 2008;
Fang et al, 2011). However, as for PTM, the EPI
task and its preparatory efforts (Ohta et al, 2011a)
were the first to consider DNA methylation using the
general event extraction approach. To the best of our
knowledge, the present study is the first to extend the
event extraction approach to PTM and DNA methy-
lation event extraction to the scale of the entire avail-
able literature.
The Turku Event Extraction System (TEES), first
introduced for the BioNLP ST?09 (Bjo?rne et al,
2009), was updated and generalized for participa-
tion in the BioNLP ST?11, in which it had the best
performance on both the EPI and REL challenges
(Bjo?rne and Salakoski, 2011). With an F-score of
53.33% for the EPI and 57.7% for the REL task, it
performed over 16 pp better than the next best sys-
tems, making it well suited for our study. We apply
this system to the extraction of EPI events and REL
relations from all PubMed abstracts and all PMC
open access articles, using a pipeline of open source
text mining tools introduced in Bjo?rne et al (2010).
We further process the result using a recently
created bibliome-scale gene normalization dataset1.
This normalization effort connects protein and gene
mentions in text to their database IDs, a prerequi-
1Data currently under review.
site for effective use of text mining results for most
bioinformatics applications. In addition to protein
names, the EPI and REL challenges refer to the
protein substructures, modifications and complexes,
which we also need to normalize in order to deter-
mine the biological context of these events. In this
work, we develop a number of rule-based algorithms
for the normalization of such non-protein entities.
With both proteins and other entities normalized,
we can align the set of events extracted from the
literature with biological databases containing an-
notations on protein features, such as UniProt. We
can determine how many known and unknown fea-
tures we have extracted from text, and what percent-
age of various protein feature annotations our text
mining results cover. This association naturally also
works in the other direction, as we can take a gene or
protein and find yet unannotated post-translational
modifications, domains, or other features from sci-
entific articles, a promising use case for supporting
biomedical database curation.
2 Methods
2.1 PMC preprocessing
PMC full texts are distributed in an XML format that
TEES cannot use directly for event extraction. We
convert this XML into a flat ASCII text format with
a pipeline built on top of BioNLP ST?11 supporting
resource tools (Stenetorp et al, 2011). This process-
ing resolves embedded LATEX expressions, separates
blocks of text content (titles, sections, etc.) from
others, maps non-ASCII characters to corresponding
ASCII sequences, and normalizes whitespace. Re-
solving non-ASCII characters avoids increased error
rates from NLP tools trained on ASCII-only data.
2.2 Event Extraction
We use the Turku Event Extraction System for ex-
tracting both REL relations and EPI events. TEES is
a modular event extraction pipeline, that has recently
been extended for all the subtasks of the BioNLP?11
ST, including EPI and REL (Bjo?rne and Salakoski,
2011). TEES performs all supported tasks using
a shared graph scheme, which can represent both
events and relations (Figure 1 D). The system also
provides confidence scores enabling selection of the
most likely correct predictions. Before event extrac-
83
Event/relation type Example
Hydroxylation HIF-alpha proline hydroxylation
Phosphorylation (D) siRNA-mediated ATM depletion blocks p53 Serine-15 phosphorylation.
Ubiquitination K5 ubiquitinates BMPR-II on a Membrane-proximal Lysine
DNA methylation RUNX3 is frequently inactivated by P2 methylation in solid tumors.
Glycosylation Also, two asparagine residues in alpha-hCG were glycosylated.
Acetylation This interaction was regulated by Tat acetylation at lysine 50.
Methylation Methylation of lysine 37 of histone H2B is conserved.
Catalysis GRK2 catalyzed modest phosphorylation of BAC1.
Protein-Component Three enhancer elements are located in the 40 kb intron of the GDEP gene.
Subunit-Complex The most common form is a heterodimer composed of the p65/p50 subunits.
Table 1: Sentences with examples of the eight EPI event and two REL relation types, with highlighted triggers, and
protein and site arguments. Relations have no trigger and Catalysis takes as an argument another event.
Protein
Serine
Phosphorylation
of
Catalysis
is
Protein
mediated by CKI .
Cause>
REL detectionD
C
B
parsing
phosphorylation T-bet
Entity
<Theme
<Site
E
named entity recognition and normalization BANNER + GenNorm
McCJ-parser + Stanford Conversion
TEES
sentence splitting GENIA Sentence Splitter
PubMed Article Data
conversion to ST format and database import
A
Theme>
Serine of is mediated by CKI .phosphorylation T-betProteinEntity
<Protein-Component
Serine of is mediated by CKI .phosphorylation T-betNN VBN NN .NNNN VBZIN IN
<nn prep_of>
<nsubjpass
<auxpass agent>
Serine of is mediated by CKI .phosphorylation T-betProtein Protein
57765 27373
Serine of is mediated by CKI .phosphorylation T-bet
EPI detection
REL EPI
Figure 1: Event and relation extraction. Article text is
split into sentences (A), where gene/protein entities are
detected and normalized to their Entrez Gene IDs (B).
Each sentence with at least one entity is then parsed
(C). EPI events and REL relations are extracted from
the parsed sentences (D) and following conversion to
the BioNLP ST format are imported into a database (E).
(Adapted from Bjo?rne and Salakoski (2011)).
tion, protein/gene names are detected and sentences
are parsed. TEES handles all these preprocessing
steps via a pipeline of tool wrappers for the GE-
NIA Sentence Splitter (Kazama and Tsujii, 2003),
the BANNER named entity recognizer (Leaman and
Gonzalez, 2008), the McClosky-Charniak-Johnson
(McCCJ) parser (Charniak and Johnson, 2005; Mc-
Closky, 2010) and the Stanford tools (de Marneffe
et al, 2006). For a detailed description of TEES
we refer to Bjo?rne and Salakoski (2011) and for the
computational requirements of PubMed-scale event
extraction to Bjo?rne et al (2010).
2.3 Entity normalization
The extraction of events and relations as described in
the previous sections is purely text-based and does
not rely on any domain information from external
resources. This ensures generalizability of the meth-
ods to new articles possibly describing novel inter-
actions. However, practical use cases often require
integration of text mining results with external re-
sources. To enable such an integration, it is crucial to
link the retrieved information to known gene/protein
identifiers. In this section, we describe how we link
text mining data to biomolecular databases by pro-
viding integration with Entrez Gene, UniProt, Inter-
Pro and the Protein Data Bank.
2.3.1 Protein annotations
A crucial step for integrating statements in do-
main text with data records is gene name normaliza-
tion As part of a recent PubMed-scale effort,2 gene
2Data currently under review.
84
normalizations were produced by the GenNorm sys-
tem (Wei and Kao, 2011), assigning unique Entrez
Gene identifiers (Sayers and others, 2010) to am-
biguous gene/protein symbols. The GenNorm sys-
tem represents the state-of-the-art in gene normal-
ization, having achieved first rank by several evalua-
tion criteria in the BioCreative III Challenge (Lu and
others, 2011).
For practical applications, the Entrez Gene iden-
tifiers have been mapped to UniProt (The UniProt
Consortium, 2011) through conversion tables pro-
vided by the NCBI. As Entrez Gene and UniProt
are two of the most authoritative resources for gene
and protein identification, these annotations ensure
straightforward integration with other databases.
2.3.2 Complex annotations
The REL task Subunit-Complex relations all in-
volve exactly one protein complex and one of its
subunits, but the same complex may be involved in
many different Subunit-Complex relations (Pyysalo
et al, 2011). A key challenge for making use
of these relations thus involves retrieving a unique
identification of the correct complex. To identify
protein complexes, we use the Protein Data Bank
(PDB), an archive of structural data of biological
macromolecules (Berman et al, 2000). This re-
source currently contains more than 80,000 3-D
structures, and each polymer of a structure is anno-
tated with its respective UniProt ID.
To assign a unique PDB ID to an entity involved
in one or more Subunit-Complex relations, there
is usually no other lexical context than the protein
names in the sentence, e.g. ?the Rad9-Hus1-Rad1
complex?. Consequently, we rely on the normal-
ized protein names (Section 2.3.1) to retrieve a list
of plausible complexes, using data downloaded from
UniProt to link proteins to PDB entries. Ambiguity
is resolved by selecting the complex with the high-
est number of normalized proteins and giving pref-
erence to so-called representative chains. A list of
representative chains is available at the PDB web-
site, and they are determined by clustering similar
protein chains3 and taking the most confident ones
based on resolution quality.
Each assignment of a PDB identifier is annotated
with a confidence value between 0 and 1, express-
3Requiring at least 40% sequence similarity.
ing the percentage of proteins in the complex that
could be retrieved and normalized in text. For ex-
ample, even if one out of three UniProt identifiers is
wrongly assigned for a mention, the correct complex
might still be assigned with 0.66 confidence.
2.3.3 Domain annotations
Protein-Component relations define a relation be-
tween a gene/protein and one of its components,
such as a gene promoter or a protein domain. To
identify at least a substantial subset of these di-
verse relations, we have integrated domain knowl-
edge extracted from InterPro. InterPro is a rich re-
source on protein families, domains and functional
sites, integrating data from databases like PROSITE,
PANTHER, Pfam, ProDom, SMART and TIGR-
FAMs (Hunter and others, 2012). Over 23,000 dis-
tinct InterPro entries were retrieved, linking to more
than 16.5 million protein identifiers.
To assign an InterPro ID to an entity involved in
one or more Protein-Component relations, a set of
candidates is generated by inspecting the InterPro
associations of each of the proteins annotated with
that domain in text. For each such candidate, the
description of the InterPro entry is matched against
the lexical context around the entity by comparing
the number of overlapping tokens, excluding gen-
eral words, such as domain, and prepositions. The
amount of overlap is normalized against the length
of the InterPro description and expressed as a per-
centage, creating confidence values between 0 and 1.
Additionally, a simple pattern matching algorithm
recognizes statements expressing an amino acid in-
terval, e.g. ?aristaless domain (aa 527-542)?. When
such expressions are found, the intervals as anno-
tated in InterPro are matched against the retrieved
interval from text, and the confidence values express
the amount of overlap between the two intervals.
2.3.4 PTM site normalization
Six of the eight4 EPI event types refer to
post-translational modification of proteins. These
events are Hydroxylation, Phosphorylation, Ubiq-
uitination, Glycosylation, Acetylation and (Protein)
Methylation. To evaluate the events predicted
4As we are interested in PTM sites, we make no distinc-
tion between ?additive? PTMs such as Acetylation and their ?re-
verse? reactions such as Deacetylation.
85
from text, we compare these to annotated post-
translational modifications in UniProt. UniProt is
one of the largest manually curated databases for
protein knowledge, and contains annotations corre-
sponding to each of the EPI PTM event types.
We use the reviewed and manually annotated
UniProtKB/Swiss-Prot dataset (release 2012 02) in
XML format. We take for each protein all feature
elements of types modified residue, cross-link and
glycosylation site. Each of these feature elements
defines the site of the modification, either a single
amino acid, or a sequence of amino acids. We select
only annotations based on experimental findings,
that is, features that do not have a non-experimental
status (potential, probable or by similarity) to avoid
e.g. features only inferred from the sequence.
The modified residue feature type covers the event
types Hydroxylation, Phosphorylation, Acetylation
and Methylation. We determine the class of the mod-
ification with the UniProt controlled vocabulary of
post-translational modifications5. The description
attribute is the ID attribute of an entry in the vocabu-
lary, through which we can determine the more gen-
eral keyword (KW) for that description, if defined.
These keywords can then be connected to the corre-
sponding event types in the case of Hydroxylation,
Phosphorylation, Acetylation and Methylation. For
Ubiquitination events, we look for the presence of
the string ?ubiquitin? in the description attribute of
cross-link features. Finally, features corresponding
to Glycosylation events are determined by their fea-
ture element having the type glycosylation site.
The result of this selection process is a list of in-
dividual modification features, which contain a type
corresponding to one of the EPI PTM event types,
the UniProt ID of the protein, and the position and
amino acid(s) of the modification site. This data can
be compared with extracted events, using their type,
normalized protein arguments and modification site
arguments. However, we also need to normalize the
modification site arguments.
PTM sites are defined with a modification type
and the numbered target amino acid residue. In EPI
events, these residues are defined in the site argu-
ment target entities. To convert these into a form
that can be aligned with UniProt, we apply a set
5http://www.uniprot.org/docs/ptmlist/
Event Type Extracted PMC (%)
Hydroxylation 14,555 34.17
Phosphorylation 726,757 44.00
Ubiquitination 74,027 70.46
DNA methylation 140,531 52.27
Glycosylation 154,523 42.31
Acetylation 114,585 69.40
Methylation 122,015 74.86
Catalysis 45,763 67.86
Total EPI 1,392,756 51.53
Protein-Component 1,613,170 52.59
Subunit-Complex 537,577 51.18
Total REL 2,150,747 52.23
Table 2: Total number of EPI events and REL relations
extracted from PubMed abstracts and PMC full-text arti-
cles, with the fractions extracted from PMC.
of rules that try to determine whether a site is an
amino acid. We start from the main site token, and
check whether it is of the form AA#, where AA is an
amino acid name, or a one or three letter code, and
# an optional site number, which can also be in a to-
ken following the amino acid. For cases where the
site entity is the word ?residue? or ?residues?, we
look for the amino acid definition in the preceding
and following tokens. All strings are canonicalized
with removal of punctuation, hyphens and parenthe-
sis before applying the rules. In total, of the 177,994
events with a site argument, 75,131 could be nor-
malized to an amino acid, and 60,622 of these to a
specific residue number.
3 Results
The source for extraction in this work is the set of 21
million PubMed abstracts and 372 thousand PMC
open-access full-text articles. From this dataset,
1.4M EPI events and 2.2M REL relations were ex-
tracted (Table 2). For both tasks, about half of the
results were extracted from PMC, confirming that
full-text articles are an important source of infor-
mation for these extraction targets. The total num-
bers of events and relations are considerably lower
than e.g. the 21.3M events extracted for the GENIA
task from PubMed abstracts (Bjo?rne et al, 2010;
Van Landeghem et al, 2012), likely relating to the
comparatively low frequency with which EPI and
REL extraction targets are discussed with respect to
the basic GENIA biomolecular reactions.
86
Event type UniProt Events Match Coverage Events (site) Match Coverage
Hydroxylation 1,587 14,555 1,526 19 4,298 130 5
Phosphorylation 57,059 726,757 286,978 4,795 86,974 9,732 748
Ubiquitination 792 74,027 4,994 143 10,562 54 20
Glycosylation 6,708 154,523 18,592 897 22,846 68 31
Acetylation 6,522 114,585 15,470 764 25,689 158 30
Methylation 1,135 122,015 2,178 113 27,625 36 10
Total 73,803 1,206,462 329,738 6,731 177,994 10,178 844
Table 3: PTM events. PTMs that are not marked with non-experimental qualifiers are taken from UniProt. The
Events column lists the total number of predicted events, and the Events (site) the number of events that also have a
predicted site-argument. For these groups, Match is the number of events that matches a known PTM from UniProt,
and Coverage the number of UniProt PTMs for which at least one match exists. For Events matching takes into account
the PTM type and protein id, for Events (site) also the amino acid and position of the modified residue.
Event type AA UP # Highest confidence event Article ID
Phosphorylation S9 ? 2 p53 isolated from ML1, HCT116 and RKO cells, after short
term genotoxic stress, were phosphorylated on Ser 6, Ser 9
PMC:2777442
Acetylation S15 4 phosphorylated (Ser15), acetylated p53(Lys382) PMC:2557062
Methylation S15 1 phosphorylation of p53 at serine 15 and acetylation PM:10749144
Phosphorylation S15 ? 238 Chk2, as well as p53 Ser(15) phosphorylation and its PM:16731759
Phosphorylation T18 ? 12 p53 stabilization and its phosphorylation in Thr18 PMC:3046209
Phosphorylation S20 ? 45 that phosphorylation of p53 at Ser20 leads to PMC:3050855
Phosphorylation S33 ? 14 phosphorylation of p53 at serine 33 may be part of PMC:35361
Phosphorylation S37 ? 20 serine 33 of p53 in vitro when serine 37 is already PMC:35361
Phosphorylation S46 ? 55 phosphorylation of p53, especially at Serine 46 by PMC:2634840
Phosphorylation T55 ? 7 that phosphorylation of p53 at Thr55 inhibits its PMC:3050855
Phosphorylation S99 ? 0
Phosphorylation S183 ? 0
Phosphorylation S269 ? 0
Phosphorylation T284 ? 0
Ubiquitination K291 ? 0
Acetylation K292 ? 0
Ubiquitination K292 ? 0
Acetylation K305 ? 0
Phosphorylation S313 ? 1 hyperphosphorylation of p53, particularly of Ser313 PM:8649812
Phosphorylation S314 ? 0
Phosphorylation S315 ? 6 to require phosphorylation of p53 at serine 315 (35) PMC:2532731
Methylation K370 ? 6 by methylating lysine 370 of p53 PMC:1636665
Acetylation K372 1 for lysine 372 and 383 acetylated p53 (Upstate, PMC:1315280
Methylation K372 ? 5 methylation of p53 by the KMT7(SET7/9) methyltransferase
enzyme on Lys372
PMC:2794343
Acetylation K373 ? 16 p53 and acetylated p53 (lysine-373 and lysine-382) PMC:1208859
Methylation K373 ? 4 EHMT1-mediated p53 methylation at K373 PM:20588255
Acetylation K381 ? 0
Acetylation K382 ? 82 p53 acetylation at lysine 382 was found not PM:17898049
Methylation K382 ? 6 SET8 specifically monomethylates p53 at lysine 382 PM:17707234
Methylation K386 ? 1 that sumoylation of p53 at K386 blocks subsequent PM:19339993
Phosphorylation S392 ? 35 and phosphorylation of p53 at S392 PM:17237827
Table 4: Extracted and known PTM sites of p53. The type and site of the modification are in the first two columns.
UP indicates whether the PTM is present in the UniProt annotation for p53. Column # shows the number of extracted
events, followed by the event with the highest confidence score and the PubMed abstract or PMC full-text article it has
been extracted from.
87
3.1 Extracted PTMs compared to UniProt
The EPI PTM events were compared to annotated
PTMs from UniProt (Table 3). The majority of ex-
tracted PTM events (85%) have only a protein ar-
gument, and no information about the modification
site, so these can only be compared by the protein
id and PTM type. For the subset of proteins that
also have a site, which can be normalized to an
amino acid position, we can make a detailed com-
parison with UniProt. Finding a match for these
normalized amino acids is more difficult, and for
both categories, only a small fraction of proteins
from UniProt is covered. In part this may be due
to the limitations of the gene name normalization, as
finding the exact species-specific protein ID remains
a challenging task (Lu and others, 2011). How-
ever, even if the overall coverage is limited, well-
known protein modifications can be assigned to spe-
cific residues, as we show in the next section.
3.2 Extracted PTMs for a single protein
For an in-depth example of PTM modifications, we
study the protein p53, a central tumor suppressor
protein that is the subject of many studies. p53 is
also among the proteins with the most UniProt PTM
sites for which EPI events were predicted, making it
a good example for a case study (see Table 4).
We take from UniProt all known p53 PTMs corre-
sponding to our EPI event types and list the number
of predicted events for them (see Table 4). When
the number of predicted events is high, the most
confident prediction is usually a correctly extracted,
clear statement about the PTM. All events for PTMs
known in UniProt are correct except for the type
of K386. For events not in UniProt, the two S15
ones are false positives, and K372 acetylation, while
correctly extracted, is most likely a typo in the arti-
cle. For the PTMs for which no event was extracted,
we checked the reference article from UniProt an-
notation. K291, K292 ubiquitination, and K305 are
from abstracts, and thus missed events. S183, S269
and T284 are from a non-open access PMC article,
while S99, K292 acetylation, K305, S314 and K381
are from Excel or PDF format supplementary tables,
sources outside our extraction input.
In total, we have extracted 561 PTM events re-
lated to p53, 554 of which correspond to a PTM an-
Item PubMeth Extracted Recall
PMID+UPID 2776 1698 61.2%
UPID 392 363 92.6%
PMID 1163 1120 96.3%
Table 5: Evaluation of DNA methylation event extraction
recall against PubMeth.
notated in UniProt. Of the 28 EPI-relevant PTMs on
p53, 17 have at least one predicted event. The high-
est confidence events are about equally often from
abstracts as from full texts.
3.3 DNA methylation analysis
Two recently introduced databases, PubMeth (On-
genaert et al, 2008) and MeInfoText (Fang et al,
2011) provide manually curated information on
DNA methylation, primarily as it relates to cancer.
To evaluate the coverage of DNA methylation event
extraction, we focus here on PubMeth, as the full
content of this database could be directly used. Each
PubMeth DB record provides the primary name of
the methylated gene and the PMID of the publica-
tion supporting the curation of the record. We used
these two pieces of information to evaluate the recall
6 of DNA methylation event extraction.
We mapped PubMeth entries to UniProt iden-
tifiers (UPIDs), and extracted all unique (PMID,
UPID) pairs from both PubMeth and the automat-
ically extracted DNA methylation/demethylation
events. The results of comparison of these sets of
ID pairs are given in Table 5. We find that for over
60% of PubMeth entries, the system is able to re-
cover the specific (document, gene) pair. This result
is broadly in line with the recall of the system as
evaluated in the BioNLP ST. However, if the match-
ing constraint is relaxed, asking either 1) can the sys-
tem extract the methylation of each gene in PubMeth
somewhere in the literature or, inversely, 2) can the
system detect some DNA methylation event in each
document included in PubMeth as evidence, recall
is over 90%. In particular, the evaluation indicates
that the system shows very high recall for identify-
ing documents discussing DNA methylation.
6As PubMeth does not aim for exhaustive coverage, preci-
sion cannot be directly estimated in this way. For example, Pub-
Meth covers fewer than 2,000 documents and DNA methylation
events were extracted from over 20,000, but due to differences
in scope, this does not suggest precision is below 10%.
88
REL Type Extracted Match (p) Match (e)
Prot-Cmp 1613.1K 561.8K 150.7K
SU-Cmplx 537.6K 226.5K 99.6K
Table 6: Numbers of extracted entity relations, with the
protein (p) or both protein and entity (e) identified.
3.4 REL statistics
Table 6 presents the amount of extracted entity re-
lations and the coverage of the normalization algo-
rithms assigning protein, domain and complex iden-
tifiers. From a total of 537.6K Subunit-Complex re-
lations, 226.5K (42%) involve a protein that could be
unambiguously identified (Section 2.3.1). From this
subset, 99.6K relations (44%) could be assigned to a
PDB complex identifier (Section 2.3.2), accounting
for 3800 representative 3D protein structures.
The Protein-Component relations are much more
frequent in the data (1.6M relations) and here 35%
of the relations (561.8K) involve a normalized pro-
tein mention. The assignment of InterPro domains
to these Protein-Component relations (Section 2.3.3)
further covers 150.7K relations in this subset (27%),
identifying 5500 distinct functional domains. The
vast majority of these annotations (99%) are pro-
duced by matching the lexical context against the
InterPro descriptions, and only a few cases (200)
matched against the amino-acid pattern.
4 Conclusions
We have combined state-of-the-art methods for
gene/protein name normalization together with the
best available methods for event-based extraction
of protein post-translational modifications, reactions
relating to the epigenetic control of gene expres-
sion, and part-of relations between genes/proteins,
their components, and complexes. These methods
were jointly applied to the entire available litera-
ture, both PubMed abstracts and PMC full-text doc-
uments, creating a text mining dataset unique in both
scope and breadth of analysis. We further performed
a comprehensive analysis of the results of this au-
tomatic extraction process against major biological
database resources covering various aspects of the
extracted information. This analysis indicated that
text mining results for protein complexes, substruc-
tures and epigenetic DNA methylation provides al-
ready quite extensive coverage of relevant proteins.
For post-translational modifications, we note that
coverage still needs to be improved, but conclude
that the extracted events already provide a valuable
link to PTM related literature. In future work we
hope to further extend the event types extracted by
our PubMed-scale approach. The extraction meth-
ods as well as all data introduced in this study are
freely available from bionlp.utu.fi.
Acknowledgments
We thank the Academy of Finland, the Research
Foundation Flanders (FWO) and the UK BBSRC
(reference number: BB/G013160/1) for funding,
and CSC ? IT Center for Science Ltd for compu-
tational resources.
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
Helen M. Berman, John Westbrook, Zukang Feng,
Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N.
Shindyalov, and Philip E. Bourne. 2000. The protein
data bank. Nucleic Acids Research, 28(1):235?242.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 183?191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009 Work-
shop, pages 10?18.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Scaling up biomedical
event extraction to the entire PubMed. In Proceedings
of the BioNLP 2010 Workshop, pages 28?36.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
ACL, pages 173?180.
Y.C. Fang, P.T. Lai, H.J. Dai, and W.L. Hsu. 2011. Me-
infotext 2.0: gene methylation and cancer relation ex-
traction from biomedical literature. BMC bioinformat-
ics, 12(1):471.
Z. Z. Hu, M. Narayanaswamy, K. E. Ravikumar,
K. Vijay-Shanker, and C. H. Wu. 2005. Literature
mining and database annotation of protein phospho-
rylation using a rule-based system. Bioinformatics,
21(11):2759?2765.
89
Sarah Hunter et al 2012. Interpro in 2011: new devel-
opments in the family and domain prediction database.
Nucleic Acids Research, 40(D1):D306?D312.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP 2003,
pages 137?144.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP 2009, pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP Shared Task 2011. In Proceed-
ings of the BioNLP Shared Task 2011, pages 1?6.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedical
named entity recognition. Pacific Symposium on Bio-
computing, pages 652?663.
Tzong-Yi Lee, Hsien-Da Huang, Jui-Hung Hung, Hsi-
Yuan Huang, Yuh-Shyong Yang, and Tzu-Hao Wang.
2006. dbPTM: an information repository of pro-
tein post-translational modification. Nucleic acids re-
search, 34(suppl 1):D622?D627.
Hodong Lee, Gwan-Su Yi, and Jong C. Park. 2008.
E3Miner: a text mining tool for ubiquitin-protein lig-
ases. Nucl. Acids Res., 36(suppl.2):W416?422.
Hong Li, Xiaobin Xing, Guohui Ding, Qingrun Li, Chuan
Wang, Lu Xie, Rong Zeng, and Yixue Li. 2009.
SysPTM: A Systematic Resource for Proteomic Re-
search on Post-translational Modifications. Molecular
& Cellular Proteomics, 8(8):1839?1849.
Zhiyong Lu et al 2011. The gene normalization task
in BioCreative III. BMC Bioinformatics, 12(Suppl
8):S2+.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of LREC-06, pages 449?454.
David McClosky. 2010. Any domain parsing: auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2011a. Event extraction for
DNA methylation. Journal of Biomedical Semantics,
2(Suppl 5):S2.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii.
2011b. Overview of the epigenetics and post-
translational modifications (EPI) task of BioNLP
Shared Task 2011. In Proceedings of BioNLP Shared
Task 2011 Workshop, pages 16?25.
Mate? Ongenaert, Leander Van Neste, Tim De Meyer,
Gerben Menschaert, Sofie Bekaert, and Wim
Van Criekinge. 2008. PubMeth: a cancer methy-
lation database combining text-mining and expert
annotation. Nucl. Acids Res., 36(suppl 1):D842?846.
Sampo Pyysalo, Antti Airola, Juho Heimonen, and Jari
Bjo?rne. 2008. Comparative analysis of five protein-
protein interaction corpora. BMC Bioinformatics,
9(Suppl. 3):S6.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece in the
biomedical information extraction puzzle. In Proceed-
ings of the BioNLP 2009 Workshop, pages 1?9.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii. 2011.
Overview of the entity relations (REL) supporting task
of BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 83?88.
Eric W. Sayers et al 2010. Database resources of the na-
tional center for biotechnology information. Nucleic
Acids Research, 38(suppl 1):D5?D16.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Bionlp shared task 2011: Supporting resources. In
Proceedings of BioNLP Shared Task 2011 Workshop,
pages 112?120.
The UniProt Consortium. 2011. Ongoing and future de-
velopments at the universal protein resource. Nucleic
Acids Research, 39(suppl 1):D214?D219.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehen-
sive benchmark of kernel methods to extract protein-
protein interactions from literature. PLoS Comput
Biol, 6(7):e1000837, 07.
Sofie Van Landeghem, Sampo Pyysalo, Tomoko Ohta,
and Yves Van de Peer. 2010. Integration of static re-
lations to enhance event extraction from text. In Pro-
ceedings of BioNLP?10, pages 144?152.
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Ginter.
2012. Exploring biomolecular literature with EVEX:
Connecting genes through events, homology and indi-
rect associations. Advances in Bioinformatics.
Chih-Hsuan Wei and Hung-Yu Kao. 2011. Cross-species
gene normalization by species inference. BMC bioin-
formatics, 12(Suppl 8):S5.
Cathy H. Wu et al 2003. The Protein Information Re-
source. Nucl. Acids Res., 31(1):345?347.
X. Yuan, ZZ Hu, HT Wu, M. Torii, M. Narayanaswamy,
KE Ravikumar, K. Vijay-Shanker, and CH Wu. 2006.
An online literature mining tool for protein phospho-
rylation. Bioinformatics, 22(13):1668.
90
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 63?71,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evaluating large-scale text mining applications
beyond the traditional numeric performance measures
Sofie Van Landeghem1,2, Suwisa Kaewphan3,4, Filip Ginter3, Yves Van de Peer1,2
1. Dept. of Plant Systems Biology, VIB, Belgium
2. Dept. of Plant Biotechnology and Bioinformatics, Ghent University, Belgium
3. Dept. of Information Technology, University of Turku, Finland
4. Turku Centre for Computer Science (TUCS), Turku, Finland
solan@psb.ugent.be, sukaew@utu.fi
ginter@cs.utu.fi, yvpee@psb.ugent.be
Abstract
Text mining methods for the biomedical
domain have matured substantially and
are currently being applied on a large
scale to support a variety of applica-
tions in systems biology, pathway cura-
tion, data integration and gene summa-
rization. Community-wide challenges in
the BioNLP research field provide gold-
standard datasets and rigorous evaluation
criteria, allowing for a meaningful com-
parison between techniques as well as
measuring progress within the field. How-
ever, such evaluations are typically con-
ducted on relatively small training and
test datasets. On a larger scale, sys-
tematic erratic behaviour may occur that
severely influences hundreds of thousands
of predictions. In this work, we per-
form a critical assessment of a large-scale
text mining resource, identifying system-
atic errors and determining their underly-
ing causes through semi-automated analy-
ses and manual evaluations1.
1 Introduction
The development and adaptation of natural lan-
guage processing (NLP) techniques for the
biomedical domain are of crucial importance to
manage the abundance of available literature. The
inherent ambiguity of gene names and complex-
ity of biomolecular interactions present an intrigu-
ing challenge both for BioNLP researchers as well
as their targeted audience of biologists, geneticists
and bioinformaticians. Stimulating such research,
various community-wide challenges have been or-
ganised and received international participation.
1The supplementary data of this study is freely avail-
able from http://bioinformatics.psb.ugent.
be/supplementary_data/solan/bionlp13/
The BioCreative (BC) challenge (Hirschman et
al., 2005; Krallinger et al, 2008; Leitner et al,
2010; Arighi et al, 2011) touches upon a variety of
extraction targets. The identification of gene and
protein mentions (?named entity recognition?) is a
central task and a prerequisite for any follow-up
work in BioNLP. Linking these mentions to their
respective gene database identifiers, ?gene normal-
ization?, is a crucial step to allow for integration
of textual information with authoritative databases
and experimental results. Other BC tasks are en-
gaged in finding functional and physical relations
between gene products, including Gene Ontology
annotations and protein-protein interactions.
Focusing more specifically on the molecu-
lar interactions between genes and proteins, the
BioNLP Shared Task on Event Extraction (Kim et
al., 2009; Kim et al, 2011b; Nedellec and others,
2013) covers a number of detailed molecular event
types, including binding and transcription, regula-
tory control and post-translational modifications.
Additionally, separate tracks involve specific ap-
plications of event extraction, including infectious
diseases, bacterial biotopes and cancer genetics.
Performance of the participants in each of these
challenges is measured using numeric metrics
such as precision, recall, F-measure, slot error
rate, MAP and TAP scores. While such rig-
urous evaluations allow for a meaningful compar-
ison between different systems, it is often difficult
to translate these numeric values into a measure-
ment of practical utility when applied on a large
scale. Additionally, infrequent but consistent er-
rors are often not identified through small-scale
evaluations, though they may result in hundreds of
thousands of wrongly predicted interactions on a
larger scale. In this work, we perform an in-depth
study of an open-source state-of-the-art event ex-
traction system which was previously applied to
the whole of PubMed. Moving beyond the tra-
ditional numeric evaluations, we identify a num-
63
Figure 1: Example event and relation represen-
tations, depicted in solid and dotted lines respec-
tively. Picture by Kim et al (2011a).
ber of systematic errors in the large-scale data,
analyse their underlying causes, and design post-
processing rules to resolve these errors. We be-
lieve these findings to be highly relevant for any
practical large-scale implementation of BioNLP
techniques, as the presence of obvious mistakes in
a text mining resource might undermine the credi-
bility of text mining techniques in general.
2 Data and methods
In this section, we first describe the data and meth-
ods used in previous work for the construction
of the large-scale text mining resource that is the
topic of our error analyses (Section 3).
2.1 Event extraction
Event extraction has become a widely studied
topic within the field of BioNLP following the
first Shared Task (ST) in 2009. The ST?09 in-
troduced the event formalism as a more detailed
representation of the common binary relation an-
notation (Figure 1). Each event occurrence con-
sists of an event trigger; i.e. one or more con-
secutive tokens that are linked to a specific event
type. While the ST?09 included only 9 event types,
among which 3 regulatory event types, the ST?11
further broadened the coverage of event extraction
to post-translational modifications and epigenetics
(EPI).
To compose a fully correct event, an event trig-
ger needs to be connected to its correct arguments.
Within the ST, these arguments are selected from a
set of gold-standard gene and gene product anno-
tations (GGPs). The ST guidelines determine an
unambiguous formalism to which correct events
must adhere: most event types only take one theme
argument, while Binding events can be connected
to more than one theme. Regulation events further
have an optional cause slot (Figure 1). Connecting
the correct arguments to the correct trigger words
is denoted as ?edge detection?.
To perform event extraction, we rely on the
publicly available Turku Event Extraction System
(TEES) (Bjo?rne et al, 2012), which was origi-
nally developed for the ST?09. The TEES mod-
ules for trigger and edge detection are based upon
supervised learning principles, employing support
vector machines (SVMs) for multi-label classifi-
cation. TEES has been shown to obtain state-of-
the-art performance when measured on the gold-
standard datasets of the Shared Tasks of 2009,
2011 and 2013.
2.2 Large-scale processing
Previously, the whole of PubMed has been anal-
ysed using a large-scale event extraction pipeline
composed of the BANNER named entity rec-
ognizer, the McClosky-Charniak parser, and the
Turku Event Extraction System (Bjo?rne et al,
2010). BANNER identifies gene and protein sym-
bols in text through a machine learning approach
based on conditional random fields (Leaman and
Gonzalez, 2008). While the resulting large-scale
text mining resource EVEX was focused only on
abstracts and ST?09 event types (Van Landeghem
et al, 2011), it has matured substantially during
the past few years and now includes ST?11 EPI
event types, full-text processing and gene normal-
ization (Van Landeghem et al, 2013a). In this
work, we use the version of EVEX as publicly
available on 16 March 2013, containing 40 million
event occurrences among 122 thousand gene and
protein symbols in 22 million PubMed abstracts
and 460 thousand PubMed Central full-text arti-
cles. Each event occurrence is linked to a normal-
ized confidence value, automatically derived from
the original TEES SVM classification step and the
distance to the hyperplane of each prediction.
While this study focuses on the EVEX resource
as primary dataset, the findings are also highly rel-
evant for other large-scale text mining resources,
especially those based on supervised learning,
such as the BioContext (Gerner et al, 2012).
2.3 Cross-domain evaluation
Recently, a plant-specific, application-oriented as-
sessment of the EVEX text mining resource has
been conducted by manually evaluating 1,800
event occurrences (Van Landeghem et al, 2013b).
In that study, it was established that the general
performance rates as measured previously on the
ST, are transferrable also to other domains and or-
ganisms. Specifically, the 58.5% TEES precision
64
Event type Five most frequent trigger words
Binding binding interaction associated bind association
Gene expression expression expressed production expressing levels
Localization secretion release localization secreted localized
Protein catabolism degradation degraded cleavage proteolysis degrade
Transcription transcription expression levels transcribed detected
Acetylation acetylation acetylated deacetylation hyperacetylation activation
Glycosylation glycosylated glycosylation attached N-linked absence
Hydroxylation hydroxylation hydroxylated hydroxylate beta-hydroxylation hydroxylations
Methylation radiation methylation methylated diffractometer trimethylation
DNA methylation methylation hypermethylation methylated hypermethylated unmethylated
Phosphorylation phosphorylation phosphorylated dephosphorylation phosphorylates phosphorylate
Ubiquitination ubiquitination ubiquitinated ubiquitylation ubiquitous polyubiquitination
Regulation effect regulation effects regulated control
Positive regulation increased activation increase induced induction
Negative regulation reduced inhibition decreased inhibited inhibitor
Catalysis mediated dependent mediates removes induced
Table 1: The top-5 most frequently tagged trigger words per event type in EVEX. The first 5 rows
represent fundamental event types, the next 7 post-translational modifications (PTMs), and the last 4
rows are regulatory event types. In this analysis, the PTMs and their reverse types are pooled together.
Trigger words that refer to systematic errors are in italic and are discussed further in the text.
rate measured in the ST?09, with the literature data
concerning human blood cell transcription factors,
corresponded with a 58.6% precision rate for the
plant-specific evaluation dataset (?PLEV?). This
encouraging result supports the general applicabil-
ity of large-scale text mining methods trained on
relatively small corpora. The findings of this pre-
vious study and the resulting data are further inter-
preted and analysed in more detail in this study.
3 Results
While the text mining pipeline underlying the
EVEX resource has been shown to produce state-
of-the-art results which are transferrable across
domains and organisms, it is conceivable that the
mere scale of the resource allows the accumula-
tion of systematic errors. In this section, we per-
form several targeted semi-automated evaluations
to identify, explain and resolve such cases. It is
important to note that our main focus is on im-
proving the precision rate of the resource, rather
than the recall, aiming to increase the credibility
of large-scale text mining resources in general.
3.1 Most common triggers
The trigger detection algorithm of the TEES soft-
ware is based upon SVM classifiers (Section 2.1),
and has been shown to outperform dictionary-
based approaches (Kim et al, 2009; Kim et al,
2011c). To investigate its performance in a large-
scale application, we first analyse the most fre-
quent trigger words of each event type in EVEX
(Table 1). We notice the presence of different in-
flections of the same word as well as related verbs
and nouns, such as ?inhibition?, ?inhibited? and
?inhibitor?. The trigger recognition module suc-
cessfully uses character bigrams and trigrams in
its SVM classification algorithm to allow for the
identification of such related concepts, even when
some of these trigger words were not encountered
in the training phase (Bjo?rne et al, 2009).
However, occasionally this approach results in
confusion between words with small edit dis-
tances, such as the trigger word ?ubiquitous? for
Ubiquitination events. Similarly, the Acetylation
trigger ?activation? is found within the context of
a correct event structure in most cases, but should
actually be of the type Positive regulation. The
implementation of custom post-processing rules
to automatically detect and resolve these specific
cases would ultimately deal with more than 6,000
false-positive event predictions.
Further, the trigger ?radiation? seems to occur
frequently for a Methylation event, of which 82%
of the instances can be identified in the ?Exper-
imental? subsection of the article. The majority
of these articles relate to protein crystallography,
and that subsection describes the data from the ex-
perimental set-up. Within such sections, phrases
like ?Mo Kalpha radiation? are wrongly tagged as
Methylation events. Similarly, many false-positive
Methylation predictions refer to the trigger word
?diffractometer?. Removing these instances from
the resource would result in the deletion of more
65
Trigger word s Most frequent type t2 Count Frequency Infrequent type t1 Count Frequency
acetylation Acetylation 40,291 0.298383 Binding 1,332 0.000216
Phosphorylation 1,050 0.001045
Gene expression 969 0.000093
Localization 1,045 0.000579
secretion Localization 376,976 0.208888 Acetylation 243 0.001800
glycosylation Glycosylation 24,226 0.141052 Phosphorylation 389 0.000387
Gene expression 214 0.000020
phosphorylation Phosphorylation 589,681 0.586772 Binding 454 0.000074
DNA methylation 225 0.001297
ubiquitylation Ubiquitination 4961 0.055976 Binding 128 0.000021
hypermethylation Methylation 19,501 0.112434 Phosphorylation 365 0.000363
cleavage Protein catabolism 20,552 0.073728 Gene expression 2,451 0.000234
Binding 3,011 0.000489
decreased Negative regulation 374,859 0.062372 Positive regulation 1,721 0.000173
Binding 855 0.000139
Gene expression 2,928 0.000280
reduced Negative regulation 442,400 0.073610 Positive regulation 1,091 0.000110
reduction Negative regulation 164,736 0.027410 Positive regulation 389 0.000039
absence Negative regulation 65,180 0.010845 Positive regulation 226 0.000071
Table 2: Examples of trigger words that correspond to the type which has the highest relative frequency
(left), but are also found with much lower frequencies in other types (right). The instances corresponding
to the right-most column can thus be interpreted as wrong predictions. The full list is available as a
machine readible translation table in the supplementary data.
than 82,000 false-positive event predictions.
Finally, we notice that the trigger word ?ab-
sence? for Glycosylation usually refers to a Neg-
ative regulation. Similarly, some words appear as
most frequent for more than one event type, such
as ?levels? (Gene expression and Transcription).
This type of error in trigger type disambiguation
is analysed in more detail in the next section.
3.2 Event type disambiguation
While previous work has focused on the disam-
biguation of event types on a small, gold-standard
dataset (Martinez and Baldwin, 2011), the rich-
ness of a large-scale text mining resource provides
additional opportunities to detect plausible errors.
To exploit this large-scale information, we anal-
yse all EVEX trigger words and their correspond-
ing event types, summarizing their raw event oc-
currence counts as Occ(t, s) where t denotes the
trigger type and s the trigger string. As some
event types are more abundantly described in lit-
erature, we normalize these counts to frequen-
cies (Freq(t, s)) depending on the total number
of event occurrences per type (Tot(t)):
Freq(t, s) =
Occ(t, s)
Tot(t)
with
Tot(t) =
n?
i=1
Occ(t, si)
and n the number of different triggers for event
type t. We then compare all trigger words and their
relative frequencies across different event types.
First, we inspect those cases where a trigger
word appears with comparable frequencies for two
event types t1 and t2:
Freq(t1, s) ? Freq(t2, s) ? 10? Freq(t1, s)
(1)
A first broad category of these cases are trig-
ger words that refer to both regulatory and non-
regulatory events at the same time, such as ?over-
expression? (Gene expression and Positive regula-
tion), or ?ubiquitinates? (Ubiquitination and Catal-
ysis). The majority of these cases are perfectly
valid and are in fact modeled explicitly by the
TEES software (Bjo?rne et al, 2009).
Further, we find that two broad groups of non-
regulatory event types are semantically similar and
share common trigger words: Methylation and
DNA methylation (e.g. ?methylation?, ?unmethy-
lated?, ?hypomethylation?), as well as Gene ex-
pression and Transcription (?expression?, ?synthe-
sis?, ?levels?), with occasional overlap also with
Localization (?abundance?, ?found?). Similarly,
trigger words are often shared among the four
regulatory event types (?dependent?, ?role?, ?regu-
late?), as the exact type may depend on the broader
context within the sentence.
While the previous findings do not necessar-
66
Predicted event type
Curated event type Localization Transcription Expression
Localization 15 0 3
Transcription 0 12 1
Expression 0 2 12
No event 0 2 3
Total 15 16 19
Table 3: Targeted evaluation of 50 mixed events of type Localization, Transcription and Gene expression.
The curated event type is compared to the original (hidden) predicted type.
ily refer to wrong predictions, we also notice the
usage of punctuation marks as trigger words for
various event types. This option was specifically
provided in the TEES trigger detection algorithm
as the ST?09 training data contains Binding in-
stances with ?-? as trigger word. However, these
punctuation triggers are found to be largely false
positives in the PubMed-scale event dataset. Re-
moving them in an additional post-processing step
would result in the filtering of more than 130,000
event occurrences, of which the largest part is ex-
pected to be incorrect predictions. Similarly, we
can easily remove 25,000 events that are related to
trigger words that are numeric values.
In a second step, we analyse those cases where
k ? Freq(t1, s) ? Freq(t2, s). (2)
When this condition holds, it can be hypothesized
that trigger predictions of the word s as type t1
are false positives and should have instead been of
type t2. Automatically generating such lists from
the data, we have experimentally determined an
optimal value of k = 100 that represents a reason-
able trade-off between the amount of false posi-
tives that can be identified and the manual work
needed for this.
From the resulting list, we can easily identify a
number of such cases that are clearly incorrect (Ta-
ble 2, right column). Specifically, a large number
of Positive regulation events actually refer to Neg-
ative regulation, providing an explanation of the
lower precision rate of Positive regulation predic-
tions in the previous PLEV evaluation (Van Lan-
deghem et al, 2013b). This semi-automated de-
tection procedure can ultimately result in the cor-
rection of more than 242,000 events.
The remaining cases for which condition (2)
holds are more ambiguous and can not be au-
tomatically corrected. However, these cases are
more likely to be incorrect and their confidence
values could thus be automatically decreased de-
pending on the ratio between Freq(t1, s) and
Freq(t2, s). A general exception to this rule is
formed by the broad groups of semantically simi-
lar events, such as Transcription-Gene expression-
Localization, which we analyse in more detail in
the next section.
3.3 Gene expression, Transcription and
Localization
Transcription is a sub-process of Gene expression,
with both event types relating to protein produc-
tion. However, the distinction between the two in
text may not always be straightforward. Addition-
ally, the ST training data for Transcription events
is significantly smaller than for Gene expression
events, which may be the reason why not only the
TEES performance, but also those of other sys-
tems, is considerably lower for Transcription than
for Gene expression (Kim et al, 2011c). Further,
cell-type specific gene expression should be cap-
tured by additional site arguments connected to a
Localization event, which represents the presence
or a change in the location of a protein.
To gain a deeper insight into the interplay be-
tween these three different event types, we have
performed a manual curation of 50 event occur-
rences, sampled at random from the Gene expres-
sion, Transcription and Localization events avail-
able in EVEX. For each event, the trigger word
and the corresponding sentence was extracted, but
the predicted event type was hidden. An expert an-
notator subsequently decided on the correct event
type of the trigger. Within this evaluation we fol-
lowed the ST guidelines to only annotate Gene ex-
pression when there is no evidence for the more
detailed Transcription type.
Table 3 shows the results. All 15 predicted
Localization triggers are recorded to be correct.
From the 16 predicted Transcription events, two
involve incorrect event triggers, and two other
events refer to the more general Gene expression
type (75% overall precision). Likewise, only one
Gene expression event should be of the more spe-
67
Curated event type Error type Instances (%)
1 Single-argument Binding No error 5 10%
2 Single-argument Binding Edge detection error 0 0%
3 Multiple-argument Binding Edge detection error 4 8%
4 Single-argument Binding Entity recognition error 1 2%
5 Multiple-argument Binding Entity recognition error 19 38%
6 Other Trigger detection error 21 42%
Table 4: Targeted evaluation of 50 single-argument Binding event triggers. Row 1: Fully correct event.
Row 2: The correct argument was annotated but not linked. Row 3: At least one correct multiple-
argument Binding event could have been extracted using the annotated entities in the sentence. Row 4:
The correct argument was not annotated. Row 5: No event could be extracted due to missing argument
annotations. Row 6: The trigger did not refer to a Binding event.
Unannotated entity type Entity occurrence count Examples
GGP 10 SPF30, spinal muscular atrophy gene
Generic GGP 9 primary antibodies, peptides, RNA
Chemical compound 10 Ca(2+), iron, manganese(II)
Table 5: Manual inspection of the textual entity types for those Binding events where a relevant theme
argument was not annotated in the entity recognition step.
cific Transcription type, three instances should be
Localization, and three more are considered not to
be correct events at all (63% overall precision). In
general, we remark that the predicted event type
largely corresponds to the curated type (78% of
all predictions and 87% of all otherwise correct
events).
3.4 Binding
Moving beyond the event type specification as
determined by the ST guidelines, the previous
PLEV analysis (Section 2.3) has established a re-
markable difference between single-argument and
multiple-argument Binding. In contrast to the reg-
ular ST evaluations, this work considered single-
and multiple-argument Binding as two separate
event types, resulting in a precision rate of 93% for
multiple-argument Binding triggers and only 8%
precision for single-argument Binding triggers.
As the PLEV study only focused on textual
network data, single-argument Bindings were not
analysed further. In this work however, we fur-
ther investigate this performance discrepancy and
perform an in-depth manual evaluation to try and
detect the main causes of this systematic error.
Several hypotheses can be postulated to explain
the low precision rate of single-argument Binding
events. Firstly, a false negative instance of the
entity recognition module might result in the ab-
sence of annotation for a relevant second interac-
tion partner. Another plausible explanation is an
error by the edge detection module of the event
extraction mechanism, which would occasionally
decide to produce one or several single-argument
Binding events rather than one multiple-argument
Binding, even when all involved entities are cor-
rectly annotated. Finally, it is conceivable that
predicted single-argument triggers simply do not
refer to Binding events, i.e. they contain false pos-
itive predictions of the trigger detection module of
the event extraction system.
In some cases, one trigger leads to many dif-
ferent Binding events, such as the trigger ?bind?
in the sentence ?Sir3 and Sir4 bind preferentially
to deacetylated tails of histones H3 and H4?. In
these cases, error types may accumulate: some
events could be missed due to unannotated enti-
ties, while others may be due to errors in the edge
detection step. However, multiple events with the
same trigger word are often represented by very
similar feature vectors in the classification step,
and consequently have almost identical final con-
fidence values. For this reason, we summarize the
error as ?Edge detection error? as soon as one pair
of entities was correctly annotated but not linked,
and as ?Entity recognition error? otherwise.
Table 4 summarizes the results of a curation
effort of 50 event triggers linked to a single-
argument Binding event in EVEX. We notice
that in fact, 46% should have been multiple-
argument Binding events. The main underlying
reason for the prediction of an incorrect single-
argument Binding event, when it should have been
a multiple-argument one, is apparently caused by
68
Curated event type Error type Instances (%)
1 Phosphorylation No error 34 68%
2 Phosphorylation Edge detection error 4 8%
3 Invalid Phosphorylation Edge detection error 2 4%
4 Phosphorylation Edge directionality detection error 4 8%
5 Invalid Phosphorylation Edge directionality detection error 1 2%
6 Phosphorylation Entity recognition error 3 6%
7 Other Trigger detection error 2 4%
Table 6: Targeted evaluation of 50 Phosphorylation event triggers and their theme arguments. Row 1:
Fully correct event. Row 2: The correct argument was annotated but not linked. Row 3: An argument
was linked but should not have been. Row 4: A causal argument was wrongly annotated as the theme
argument. Row 5: A causal argument was wrongly annotated as the theme argument. Row 6: The correct
argument was not annotated. Row 7: The trigger did not refer to a Phosphorylation event.
an entity recognition error (19/23 or 83%), while
an edge detection error is much less frequent
(17%). When we examine these entity recogni-
tion errors in more detail, we find that 10 rele-
vant entities are true GGPs in the sense of the
Shared Task annotation. However, 9 entities refer
to generic GGPs, and 10 instances relate to chemi-
cal compounds (Table 5). As these type of entities
can not be unambiguously normalized to unique
gene identifiers, they fall out-of-scope of the orig-
inal ST challenge. However, we feel this practice
introduces an artificial bias on the classifier and
the evaluation. Additionally, this information can
prove to be of value within a large-scale text min-
ing resource geared towards practical applications
and explorative browsing of textual information.
Finally, we notice that a remarkable 42% of all
predicted events contain trigger detection errors.
Analysing this subclass in more detail, we found
that 5 cases are invalid event triggers, 6 cases re-
fer to other event types such as Localization and
Gene expression, and 10 more cases were consid-
ered to be out-of-scope of the ST challenge, such
as a factor-disease association.
3.5 Phosphorylation
Within the PLEV evaluation (Section 2.3), it be-
came apparent that Phosphorylation is easy to
recognise from the sentence (98%) but the full cor-
rect event has a much lower precision rate (65%).
As we have seen in the previous section, even
when a trigger word is correctly predicted, errors
may still be generated by the edge detection or en-
tity recognition step. For instance, we might hy-
pothesize that the main underlying reason for the
reduced final performance is an error by the en-
tity recognition step, forcing the edge detection
mechanism to link an incorrect theme due to lack
of other options. Other plausible explanations in-
volve genuine errors by the edge detection algo-
rithm when the correct argument is annotated, as
well as problems with the identification of causal-
ity. As the TEES version applied in this work was
developed for the Shared Task 2009 and 2011, it
does not predict causal arguments for a Phospho-
rylation event directly, but instead adds Regulation
events on top of the Phosphorylations. Occasion-
ally, we have noticed that the theme of a Phospho-
rylation event should in fact have been the cause
of the embedding Regulation association, resulting
in a wrongly directed causal relationship.
To investigate these possibilities, we have man-
ually inspected 50 Phosphorylation events picked
at random from the EVEX resource. Table 6 sum-
marizes the results of this effort. Only two events
are found not to be Phosphorylation events: one
is in fact a Gene expression mention, the other
involves an incorrect trigger. Additionally, three
more events can semantically be regarded as Phos-
phorylations, but do not follow the ST specifica-
tions (?Invalid Phosphorylation?), for instance be-
cause they only mention causal arguments (?an
inhibition of Ca2+/calmodulin-dependent protein
phosphorylation?). Among the 45 cases which
correctly refer to the Phosphorylation type, 34
events are fully correct (68% of the total). Four
cases are wrongly extracted by misinterpreting the
causal relationship (?Edge directionality detection
error?) and four more instances refer to genuine
mistakes of the edge detection algorithm. Only
three other cases can be attributed to a missing en-
tity annotation. In contrast to the previous find-
ings on single-argument Bindings, we thus es-
tablish that the incorrect Phosphorylation events
are mainly caused by errors in the edge detection
mechanism, which either picks the wrong theme
69
from the set of annotated GGPs, or misinterprets
the causality direction.
4 Discussion and conclusion
We have performed several semi-automated eval-
uations and targeted manual curation experiments,
identifying and explaining systematic errors in a
large-scale event dataset. As a first observation,
we notice that a few frequent trigger words are
almost always associated to incorrect event pre-
dictions, such as the trigger words ?ubiquitous?
and ?radiation?, or a punctuation symbol. These
cases were identified through a large-scale auto-
matic analysis in combination with a limited man-
ual evaluation effort. The results are distributed as
a blacklist of event triggers for the implementation
or filtering of future large-scale event predictions
efforts.
Further, a semi-automated procedure has iden-
tified a list of likely incorrect predictions, by
comparing the type-specific frequencies of trigger
words across all event types. Manual inspection of
the most frequent cases allowed us to determine a
number of trigger words for which the event type
can automatically be corrected. These results are
also made publicly available.
Additionally, after removal of the most obvi-
ous and frequent errors, a fully automated script
can automatically reduce the confidence scores of
those event occurrences where the trigger words
are found to be much more frequent for another
event type. We have established that this proce-
dure should disregard triggers identified within a
few specific semantically similar clusters: DNA
methylation/Methylation, Regulation/Positive reg-
ulation/Negative regulation/Catalysis and Gene
expression-Transcription/Localization. An addi-
tional targeted evaluation of these last three types
revealed that, despite their semantic overlap, the
largest fraction of these predictions refers to the
correct event type (78? 11.5%).
Finally, we note that trigger detection (47 ?
14.6%) and entity recognition errors (44?14.6%)
are the main causes of wrongly predicted Bind-
ing events. The latter causes the event extraction
mechanism to artificially produce single-argument
Bindings instead of multiple-argument Bindings.
We believe this issue can be resolved by broaden-
ing the scope of the entity recognition module to
generic GGPs and chemical compounds, and re-
applying the TEES algorithm to these entities as
if they were normal GGPs as defined in the ST
formalism. In contrast, edge detection errors are
much more frequently the cause of a wrongly pre-
dicted Phosphorylation event (statistically signifi-
cant difference with p < 0.05), caused by wrongly
identifying the thematic object or the causality of
the event. To resolve this issue, we propose fu-
ture annotation efforts to specifically annotate the
protein adding the phosphate group to the target
protein as a separate class than the regulation of
such a phosphorylation process by other cellular
machineries and components (Kim et al, 2013).
In conclusion, we have performed several
statistical analyses and targeted manual eval-
uations on a large-scale event dataset. As a
result, we were able to identify a set of rules
to automatically delete or correct a number
of false positive predictions (supplementary
material at http://bioinformatics.
psb.ugent.be/supplementary_data/
solan/bionlp13/). When applying these
rules to the winning submission of the recent
ST?13 (GE subchallenge), which was based
on the TEES classifier (Hakala et al, 2013),
3 false positive predictions could be identified
and removed. Even though this procedure only
marginally improves the classification results
(50.97% to 50.99% F-score), we believe the
cleaning procedure to be crucial specifically for
the credibility of any large-scale text mining
application. For example, applied on the EVEX
resource, it would ultimately result in the removal
of 242,000 instances and a corrected event type of
230,000 more cases (1.2% of all EVEX events in
total). These corrections will be implemented as
part of the next big EVEX release. Additionally,
the confidence score of more than 120,000 am-
biguous cases could be automatically decreased.
Alternatively, these cases could be the target of
a large-scale re-annotation, for instance using
the brat annotation tool (Stenetorp et al, 2012).
The resulting dataset could then serve as a new
training set to enable active learning on top of
existing event extraction approaches.
Acknowledgments
The authors thank Cindy Martens and the anony-
mous reviewers for a critical reading of the
manuscript and constructive feedback. SVL
thanks the Research Foundation Flanders (FWO)
for funding her research.
70
References
Cecilia Arighi, Zhiyong Lu, Martin Krallinger, Kevin
Cohen, J. Wilbur, Alfonso Valencia, Lynette
Hirschman, and Cathy Wu. 2011. Overview of
the BioCreative III workshop. BMC Bioinformatics,
12(Suppl 8):S1.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop, pages 10?18.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Scaling up biomed-
ical event extraction to the entire PubMed. In Pro-
ceedings of the BioNLP 2010 Workshop, pages 28?
36.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
Generalizing biomedical event extraction. BMC
Bioinformatics, 13(suppl. 8):S4.
Martin Gerner, Farzaneh Sarafraz, Casey M. Bergman,
and Goran Nenadic. 2012. BioContext: an in-
tegrated text mining system for large-scale extrac-
tion and contextualization of biomolecular events.
Bioinformatics, 28(16):2154?2161.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,
Yves Van de Peer, and Filip Ginter. 2013. EVEX
in ST13: Application of a large-scale text mining
resource to event extraction and network construc-
tion. In Proceedings of the BioNLP Shared Task
2013 Workshop (in press).
Lynette Hirschman, Alexander Yeh, Christian
Blaschke, and Alfonso Valencia. 2005. Overview
of BioCreAtIvE: critical assessment of informa-
tion extraction for biology. BMC Bioinformatics,
6(Suppl 1):S1.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on event extraction. In
Proceedings of the BioNLP 2009 Workshop, pages
1?9.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2011a. Ex-
tracting bio-molecular events from literature - the
BioNLP?09 Shared Task. Computational Intelli-
gence, 27(4):513?540.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011b.
Overview of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011c. Overview of Genia event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP Shared Task 2011 Workshop, BioNLP
Shared Task ?11, pages 7?15.
Jin-Dong Kim, Yue Wang, Yamamoto Yasunori,
Sabine Bergler, Roser Morante, and Kevin Cohen.
2013. The Genia Event Extraction Shared Task,
2013 edition - overview. In Proceedings of the
BioNLP Shared Task 2013 Workshop (in press).
Martin Krallinger, Alexander Morgan, Larry Smith,
Florian Leitner, Lorraine Tanabe, John Wilbur,
Lynette Hirschman, and Alfonso Valencia. 2008.
Evaluation of text-mining systems for biology:
overview of the second BioCreative community
challenge. Genome Biology, 9(Suppl 2):S1.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedi-
cal named entity recognition. Pacific Symposium on
Biocomputing. Pacific Symposium on Biocomputing,
pages 652?663.
F. Leitner, S.A. Mardis, M. Krallinger, G. Cesareni,
L.A. Hirschman, and A. Valencia. 2010. An
overview of BioCreative II.5. Computational Bi-
ology and Bioinformatics, IEEE/ACM Transactions
on, 7(3):385?399.
David Martinez and Timothy Baldwin. 2011. Word
sense disambiguation for event trigger word detec-
tion in biomedicine. BMC Bioinformatics, 12(Suppl
2):S4.
Claire Nedellec et al 2013. Overview of BioNLP
Shared Task 2013. In Proceedings of the BioNLP
Shared Task 2013 Workshop (in press).
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011. EVEX: a PubMed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of the BioNLP
2011 Workshop, pages 28?37.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013a. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.
Sofie Van Landeghem, Stefanie De Bodt, Zuzanna J.
Drebert, Dirk Inz, and Yves Van de Peer. 2013b.
The potential of text mining in data integration and
network biology for plant research: A case study on
arabidopsis. The Plant Cell, 25(3):794?807.
71
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 26?34,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
EVEX in ST?13: Application of a large-scale text mining resource
to event extraction and network construction
Kai Hakala1, Sofie Van Landeghem3,4, Tapio Salakoski1,2,
Yves Van de Peer3,4 and Filip Ginter1
1. Dept. of Information Technology, University of Turku, Finland
2. Turku Centre for Computer Science (TUCS), Finland
3. Dept. of Plant Systems Biology, VIB, Belgium
4. Dept. of Plant Biotechnology and Bioinformatics, Ghent University, Belgium
kahaka@utu.fi, solan@psb.ugent.be, yvpee@psb.ugent.be,
ginter@cs.utu.fi, tapio.salakoski@utu.fi
Abstract
During the past few years, several novel
text mining algorithms have been de-
veloped in the context of the BioNLP
Shared Tasks on Event Extraction. These
algorithms typically aim at extracting
biomolecular interactions from text by in-
specting only the context of one sen-
tence. However, when humans inter-
pret biomolecular research articles, they
usually build upon extensive background
knowledge of their favorite genes and
pathways. To make such world knowl-
edge available to a text mining algorithm,
it could first be applied to all available lit-
erature to subsequently make a more in-
formed decision on which predictions are
consistent with the current known data. In
this paper, we introduce our participation
in the latest Shared Task using the large-
scale text mining resource EVEX which
we previously implemented using state-of-
the-art algorithms, and which was applied
to the whole of PubMed and PubMed Cen-
tral. We participated in the Genia Event
Extraction (GE) and Gene Regulation Net-
work (GRN) tasks, ranking first in the for-
mer and fifth in the latter.
1 Introduction
The main objective of our entry was to test the
usability of the large-scale text mining resource
EVEX to provide supporting information to an
existing state-of-the-art event extraction system.
In the GE task, EVEX is used to extract addi-
tional features for event extraction, capturing the
occurrence of relevant events in other documents
across PubMed and PubMed Central. In the GRN
task, EVEX is the sole source of information, i.e.
our entry consists of a modified subset of EVEX,
rather than a new text mining system specifically
trained for the task.
In the 2011 GE task, the majority of partici-
pating systems used features solely extracted from
the immediate textual context of the event candi-
date, typically restricted to a single sentence (Kim
et al, 2012; McClosky et al, 2012; Bjo?rne et al,
2012b; Vlachos and Craven, 2012). Several stud-
ies have subsequently incorporated coreference re-
lations, capturing information also from surround-
ing sentences (Yoshikawa et al, 2011; Miwa et al,
2012). However, no prior work exists on extend-
ing the event context to the information extracted
from other documents on a large scale. The moti-
vation for this entry is thus to test whether a gain
can be obtained by aggregating information across
documents with mutually supporting evidence.
In the following sections, we first introduce
EVEX as the underlying text mining resource, and
then describe the methods developed specifically
for the GRN and GE task entries. Finally, a de-
tailed error analysis of the results offers insight
into the performance of our systems and provides
possible directions of future development.
2 EVEX
EVEX1 is a text mining resource built on top
of events extracted from all PubMed abstracts
and PubMed Central Open-Access full-text doc-
uments (Van Landeghem et al, 2013a). The ex-
traction was carried out using a combination of
the BANNER named entity detector (Leaman and
Gonzalez, 2008) and the TEES event extraction
system as made publicly available subsequent to
the last Shared Task (ST) of 2011 (Bjo?rne et al,
2012a). Specifically, this version of TEES was
trained on the ST?11 GE data.
1http://www.evexdb.org
26
On top of the individual event occurrences,
EVEX provides event generalizations, allowing
the integration and summarization of knowledge
across different articles (Van Landeghem et al,
2011). For instance, the canonicalization algo-
rithm deals with small lexical variations by re-
moving non-alphanumerical characters (e.g. ?Esr-
1? to ?esr1?). The canonical generalization then
groups those events together with the same event
type and the same canonicalized arguments. Addi-
tionally, gene normalization data has recently been
integrated within the EVEX resource, assigning
taxonomic classification and database identifiers
to gene mentions in text using the GenNorm sys-
tem (Wei and Kao, 2011). Finally, the assignment
of genes to homologous families allows a more
coarse-grained generalization of the textual data.
For each generalized event, a confidence score is
automatically calculated based upon the original
TEES classification procedure, with higher values
representing more confident predictions.
Finally, the EVEX resource provides a network
interpretation which transforms events into pair-
wise gene/protein relations to represent a typed,
directed network. The primary advantage of such
a network, as compared to the complex, recursive
event structures, is that a network is more eas-
ily analysed and integrated with other external re-
sources (Kaewphan et al, 2012; Van Landeghem
et al, 2013b).
3 GRN Task
The Gene Regulatory Network subtask of the
ST?13 aims at evaluating the ability of text min-
ing systems to automatically compile a gene regu-
lation network from the literature. The task is fo-
cused specifically on sporulation in Bacillus sub-
tilis, a thoroughly studied process.
3.1 Challenge definition
The primary goal of our participation in this task
was assessing the ability to reconstruct regulatory
networks directly from the EVEX resource. Con-
sequently, we have applied the EVEX data as it
is publicly available. This decision has two major
consequences. First, we have used the predicted
BANNER entities rather than the gold-standard
entity annotation, artificially rendering the chal-
lenge more difficult. Second, we did not adapt the
EVEX events, which follow the ST?11 GE formal-
ism, to the novel annotation scheme of the GRN
EVEX type GRN type
Binding Binding
Regulation* of Transcription Transcription
Regulation* of Gene expression Transcription
Positive regulation of Any* Activation
Negative regulation of Any* Inhibition
Regulation of Any* Regulation
Table 1: Conversion of EVEX event types to the
GRN types. The table is traversed from top to
bottom, and the first rule that matches is applied.
Regulation* refers to any type of regulatory event,
and Any* refers to any other non-regulatory event
type.
challenge, but rather derived the network data di-
rectly from the EVEX interactions. For example,
given these trigger annotations
T1 Protein 37 43 sigmaB
T2 Gene 54 58 katX
T3 Transcription 59 69 expression
a GE Transcription event looks like
E1 Transcription:T3 Theme:T2 Cause:T1
while the GRN annotation is given by
R1 Transcription Target:E1 Agent:T1
E1 Action_Target:T3 Target:T2
However, both formalisms can easily be trans-
lated into the required GRN network format:
sigB Interaction.Transcription katX
where ?sigB? is annotated as the Gene identifier
of ?sigmaB?. These gene identifiers are provided
in the gold-standard entity annotations. Note that
in this context, ?gene identifiers? are standardized
gene symbols rather than numeric identifiers, and
full gene normalization is thus not required.
3.2 From EVEX to GRN data
As a first step towards creating a gene regula-
tory network directly from EVEX, we have down-
loaded all pairwise relations of the canonical gen-
eralization (Section 2). For each such relation,
we also obtain important meta-data, including the
confidence value, the PubMed IDs in which a re-
lation was found, whether or not those articles
describe Bacillus subtilis research, and whether
or not those articles are part of the GRN train-
ing or test set. In the most stringent setting, we
could then limit the EVEX results only to those
relations found in the articles of the GRN dataset
(72 in training, 45 in the development set, 55 in
the test set). Additionally, we could test whether
performance can be improved by also adding all
Bacillus subtilis articles (17,065 articles) or even
27
GRN event type Possible target types Possible agent types
Interaction.Binding Protein Gene
Interaction.Transcription Protein, PolymeraseComplex Gene, Operon
Interaction.Regulation
Protein, PolymeraseComplex Gene, Operon, Protein, ProteinComplexInteraction.Activation
Interaction.Inhibition
Table 2: Entity-type filtering of event predictions. Only those events for which the arguments (the target
as well as the agent) have the correct entity types, are retained in the result set.
all EVEX articles in which at least one event was
found (4,107,953 articles).
To match the canonicalized BANNER entities
from EVEX to the standardized gene symbols
required for the GRN challenge, we have con-
structed a mapping based on the GRN data. First,
we have scanned all gold-standard entities and
removed non-alphanumerical characters from the
gene symbols as tagged in text. Next, these canon-
ical forms were linked to the corresponding stan-
dardized gene symbols in the gold-standard anno-
tations. From the EVEX data, we then only re-
tained those relations that could be linked to two
gene symbols occurring together in a sentence.
Finally, it was necessary to convert the origi-
nal EVEX event types to the GRN relation types.
This mapping is summarized in Table 1. Because
EVEX Binding events are symmetrical and GRN
Bindings are not, we add both possible directions
to the result set. Note that some GRN types could
not be mapped because they have no equivalent
within the EVEX resource, such as the GRN type
?Requirement? or ?Promoter?.
3.3 Filtering the data
After converting the EVEX pairwise relations to
the GRN network format, it is necessary to fur-
ther process the set of predictions to obtain a co-
herent network. One additional filtering step con-
cerns the entity types of the arguments of a specific
event type. From the GRN data, we can retrieve
a symbol-to-type mapping, recording whether a
specific symbol referred to e.g. a gene, protein
or operon in a certain article. After careful in-
spection of the GRN guidelines and the training
data, we enforced the filtering rules as listed in
Table 2. For example, this procedure success-
fully removes protein-protein interactions from
the dataset, which are excluded according to the
GRN guidelines. Even though these rules are oc-
casionally more restrictive than the original GRN
guidelines, their effectiveness to prune the data
was confirmed on the training set.
Further, the GRN guidelines specify that a set
of edges with the same Agent and Target should
be resolved into a single edge, giving preference
to a more specialized type, such as Transcription
in favour of Regulation. Further, contradictory
types between a specific entity pair (e.g. Inhibition
and Activation) may occur simultaneously in the
GRN data. For the EVEX data however, it is more
beneficial to try and pick one single correct event
type from the set of predictions, effectively reduc-
ing the false positive rate. To this end, the EVEX
confidence values are used to determine the single
most plausible candidate. Further analyses on the
training data suggested that the best performance
could be achieved when only retaining the ?Mech-
anism? edges (Transcription and Binding) in cases
when no regulatory edge was found. Finally, we
noted that the EVEX Binding events more often
correspond to the GRN Transcription type, and
they were thus systematically refactored as such
(after entity-type filtering). We believe this shift
in semantics is caused by the fact that a promoter
binding is usually extracted as a binding event by
the TEES classifier, while it can semantically be
seen as a Transcription event, especially in those
cases where the Theme is a protein name, and the
Cause a gene symbol (Table 2).
3.4 Results
Table 3 lists the results of our method on the GRN
training data, which was primarily used for tun-
ing the parameters described in Section 3.3. The
highest recall (42%) could be obtained when using
all EVEX data, without restrictions on entity types
and without restricting to Bacillus subtilis articles.
As a result, this set of predictions may contain re-
lations between homologs in related species which
have the same name. While the relaxed F-score
(41%) is quite high, the Slot Error Rate (SER)
score (1.56) is unsatisfying, as SER scores should
be below 1 for decent predictions.
When applying entity type restrictions to the
prediction set, relaxed precision rises from 39%
28
Dataset ETF SER F Rel. P Rel. R Rel. F Rel. SER
All EVEX data no 1.56 8.86 39.29% 41.98% 40.59% 1.23
All EVEX data yes 1.15 11.53 59.74% 35.11% 44.23% 0.89
B. subtilis PMIDs yes 0.954 20.81 71.43% 22.90% 34.68% 0.86
GRN PMIDs yes 0.939 17.39 80.00% 18.32% 29.81% 0.86
Table 3: Performance measurement of a few different system settings, applied on the training data. The
SER score is the main evaluation criterion of the GRN challenge. The relaxed precision, recall, F and
SER scores are produced by scoring the predictions regardless of the specific event types. ETF refers to
entity type filtering.
to 60%, the relaxed F-score obtains a maximum
score of 44%, and the SER score improves to
1.15. The SER score can further be improved
when restricting the data to Bacillus subtilis arti-
cles (0.954). The optimal SER score is obtained by
further limiting the prediction set to only those re-
lations found in the articles from the GRN dataset
(0.939), maximizing at the same time the relaxed
precision rate (80%).
The final run which obtained the best SER score
on the training data was subsequently applied on
the GRN test data. It is important to note that the
parameter selection of our system was not overfit-
ted on the training data, as the SER score of our
final submission on the test data is 0.92, i.e. higher
than the best run on the training data.
Table 4 summarizes the official results of all
participants to the GRN challenge. Interestingly,
the TEES classifier has been modified to retrain
itself on the GRN data and to produce event
annotations in the GRN formalism (Bjo?rne and
Salakoski, 2013), obtaining a final SER score of
0.86. It is remarkable that this score is only 0.06
points better than our system which needed no re-
training, and which was based upon the original
GE annotation format and predicted gene/protein
symbols rather than gold-standard ones. Addition-
ally, the events in EVEX have been produced by a
version of TEES which was maximized on F-score
rather than SER score, and these measurements
are not mutually interchangeable (Table 3). We
conclude that even though our GRN system ob-
tained last place out of 5 participants, we believe
that its relative close performance to the TEES
submission demonstrates that large-scale text min-
ing resources can be used for gene regulatory net-
work construction without the need for retraining
the text mining component.
3.5 Error analysis
To determine the underlying reasons of our rela-
tively low recall rate, we have analysed the 117
SER Relaxed SER
University of Ljubljana 0.73 0.64
K.U.Leuven 0.83 0.66
TEES-2.1 0.86 0.76
IRISA-TexMex 0.91 0.60
EVEX 0.92 0.81
Table 4: Official GRN performance rates.
false negative predictions of our final run on the
training dataset. We found that 23% could be at-
tributed to a missing or incompatible BANNER
entity, 59% to a false negative TEES prediction,
15% to a wrong GRN event type and 3% to incor-
rectly mapping the gene symbol to the standard-
ized GRN format. Analysing the 16 false positives
in the same dataset, 25% could be attributed to an
incorrectly predicted event structure, and 62.5% to
a wrongly predicted event type. One case was cor-
rectly predicted but from a sentence outside the
GRN data, and in one case a correctly predicted
negation context was not taken into account. In
conclusion, future work on the GRN conversion of
TEES output should mainly focus on refining the
event type prediction, while general performance
could be enhanced by further improving the TEES
classification system.
4 GE Task
Our GE submission builds on top of the TEES 2.1
system2 as available just prior to the ST?13 test pe-
riod. First applying the unmodified TEES system,
we subsequently re-ranked its output and enforced
a cut-off threshold with the objective of removing
false positives from the TEES output (Section 4.1).
In the official evaluation, this step results in a mi-
nor 0.23pp increase of F-score compared to unpro-
cessed TEES output (Table 5). This yields the first
rank in the primary measure of the task with TEES
ranking second.
The main motivation for the re-ranking ap-
2https://github.com/jbjorne/TEES/wiki/
TEES-2.1
29
P R F
EVEX 58.03 45.44 50.97
TEES-2.1 56.32 46.17 50.74
BioSEM 62.83 42.47 50.68
NCBI 61.72 40.53 48.93
DlutNLP 57.00 40.81 47.56
Table 5: Official precision, recall and F-score rates
of the top-5 GE participants, in percentages.
proach was the ability to incorporate external in-
formation from EVEX to compare the TEES event
predictions and identify the most reliable ones.
Further, such a re-ranking approach leads to an in-
dependent component which is in no way bound to
TEES as the underlying event extraction system.
The component can be combined with any system
with sufficient recall to justify output re-ranking.
4.1 Event re-ranking
The output of TEES is re-ranked using SVMrank,
a formulation of Support Vector Machines which
is trained to optimize ranking, rather than classifi-
cation (Joachims, 2006). It differs from the basic
linear SVM classifier in the training phase, when a
query structure is defined as a subset of instances
which can be meaningfully compared among each
other ? in our case all events from a single sen-
tence. During training, only instances within a
single query are compared and the SVM does not
aim to learn a global ranking across sentences and
documents. We also experimented with polyno-
mial and radial basis kernels, feature vector nor-
malization and broadening the ranking query sets
to whole sections or narrowing them to only events
with shared triggers, but none of these settings
were found to further enhance the performance.
The re-ranker assigns a numerical score to each
event produced by TEES, and all events below
a certain threshold score are removed. To set
this threshold, a linear SVM regressor is applied
with the SVMlight package (Joachims, 1999) to
each sentence individually, i.e. we do not apply a
data-wide, pre-set threshold. Unlike the re-ranker
which receives features from a single event at a
time, the regressor receives features describing the
set of events in a single sentence.
Re-ranker features
Each event is described using a number of fea-
tures, including the TEES prediction scores for
triggers and arguments, the event structure, and
the EVEX information about this as well as simi-
lar events. Events can be recursively nested, with
the root event containing other events as its ar-
guments. The root event is of particular impor-
tance as the top-most event. A number of fea-
tures are thus dedicated specifically to this root
event, while other features capture properties of
the nested events.
Features derived from TEES confidence scores:
? TEES trigger detector confidence of the root
event and its difference from the confidence
of the negative class, i.e. the margin by which
the event was predicted by TEES.
? Minimum and maximum argument confi-
dences of the root event.
? Minimum and maximum argument confi-
dences, including recursively nested events
(if any).
? Minimum and maximum trigger confidences,
including recursively nested events (if any).
? Difference between the minimum and max-
imum argument confidences compared to
other events sharing the same trigger word.
Features describing the structure of the event:
? Event type of the root trigger.
? For each path in the event from the root to
a leaf argument, the concatenation of event
types along the path.
? For each path in the event from a leaf argu-
ment to another leaf argument, the concate-
nation of event types along the path.
? The event structure encoded in the bracketed
notation with leaf (T)heme and (C)ause argu-
ments replaced by a placeholder string, e.g.
Regulation(C:_, T:Acetylation(T:_)).
Features describing other events in the same sen-
tence:
? Event counts for each event type.
? Event counts for each unique event structure
given by the bracketed structure notation.
All event counts extracted from EVEX are rep-
resented as their base-10 logarithm to compress
the range and suppress differences in counts of
very common events.
The following features are generated in two ver-
sions, one by grouping the events according to the
EVEX canonical generalization and one for the
Entrez Gene generalization (Section 2)3.
3The generalizations based on gene families were evalu-
ated as well, but did not result in a positive performance gain.
30
? All occurrences of the given event in EVEX.
? For each path from root to a leaf gene/protein,
all occurrences of that exact path in EVEX.
? For each pair of genes/proteins in the event,
all occurrences of that pair in the network in-
terpretation of EVEX.
? For each pair of genes/proteins in the event,
all occurrences of that pair with a different
event type in the network interpretation of
EVEX.
For each event, path, or pair under considera-
tion, features are created for the base-10 logarithm
of the count in EVEX and of the number of unique
articles in which it was identified, as well as for
the minimum, maximum, and average confidence
values, discretized into six unique categories.
Regressor features
While the re-ranker features capture a single event
at a time, the threshold regressor features aggre-
gate information about events extracted within one
sentence. The features include:
? For each event type, the average and mini-
mum re-ranker confidence score, as well as
the count of events of that type.
? For each event type, the count of events shar-
ing the same trigger.
? For each event type, the count of events shar-
ing the same arguments.
? Minimum and maximum confidence values
of triggers and arguments in the TEES out-
put for the sentence.
? The section in the article in which the sen-
tence appears, as given in the ST data.
4.2 Training phase
To train the re-ranker and the regressor, false pos-
itive events are needed in addition to the true pos-
itive events in the training data. We thus apply
TEES to the training data and train the re-ranker
using the correct ranking of the extracted events.
A true positive event is given the rank 1 and a false
positive event gets the rank -1. A query structure
is then defined, grouping all events from a sin-
gle sentence to avoid mutual comparison of events
across sentences and documents during the train-
ing phase.
The trained re-ranker is then again applied to
the training data. For every sentence, the optimal
threshold is set to be the re-ranker score of the last
event which should be retained so as to maximize
# P R F
Simple events 833 -0.08 -0.36 -0.23
Protein mod. 191 +0.09 -2.09 -1.12
Binding 333 +0.43 -1.20 -0.44
Regulation 1944 +2.38 -0.67 +0.36
All 3301 +1.71 -0.73 +0.23
Table 6: Performance difference in percentage
points against the TEES system in the official test
set results, shown for different event types.
the F-score. In case the sentence only contains
false positives, the highest score is used, increased
by an empirically established value of 0.2. A sim-
ilar strategy is applied for sentences only contain-
ing true positives by using the lowest score, de-
creased by 0.2.
In both steps, the SVM regularization parameter
C is set by a grid search on the development set.
Applying TEES and the re-ranker back to the
training set results in a notably smaller propor-
tion of false positives than would be expected on
a novel input. To obtain a fully realistic train-
ing dataset for the re-ranker and threshold regres-
sor would involve re-training TEES in a cross-
validation setting, but this was not feasible due to
the tight schedule constraints of the shared task,
and is thus left as future work.
4.3 Error analysis
Although the re-ranking approach resulted in a
consistent gain over the state-of-the-art TEES sys-
tem on both the development and the test sets,
the overall improvement is only modest. As sum-
marized in Table 6, the gain over the TEES sys-
tem can be largely attributed to regulation events
which exhibit a 2.38pp gain in precision for a
0.67pp loss in recall. Regulation events are at the
same time by far the largest class of events, thus
affecting the overall score the most.
In this section, we analyse the re-ranker and
threshold regressor in isolation to understand their
individual contributions to the overall result and to
identify interesting directions for future research.
To isolate the re-ranker from the threshold re-
gressor and to identify the maximal attainable per-
formance, we set an oracle threshold in every sen-
tence so as to maximize the sentence F-score and
inspect the performance at this threshold, effec-
tively bypassing the threshold regressor. This,
however, provides a very optimistic estimate for
sentences where all predicted events are false pos-
itives, because the oracle then simply obtains the
31
All events P R F
B-C oracle (re-ranked) 81.32 39.61 53.27
W-C oracle (re-ranked) 54.92 39.61 46.02
W-C oracle (random) 51.06 39.19 44.34
Current system 47.15 39.61 43.05
TEES 45.46 40.39 42.77
Single-arg. events
B-C oracle (re-ranked) 81.37 50.58 62.38
W-C oracle (re-ranked) 56.09 50.58 53.19
W-C oracle (random) 52.73 50.00 51.33
Current system 48.66 50.44 49.53
TEES 47.16 51.09 49.04
Multiple-arg. events
B-C oracle (re-ranked) 81.02 16.83 27.87
W-C oracle (re-ranked) 48.61 16.83 25.00
W-C oracle (random) 42.66 16.75 24.05
Current system 39.64 17.12 23.91
TEES 37.57 18.17 24.50
Table 7: Performance comparison of the best case
(B-C) and worst case (W-C) oracles, the current
system with the re-ranker and threshold regressor,
and TEES. As an additional baseline, the worst
case oracle is also calculated for randomly ranked
output. All results are reported also separately for
single and multiple-argument events.
decisions from the gold standard and the rank-
ing itself is irrelevant. This effect is particu-
larly pronounced in sentences where only a sin-
gle, false positive event is predicted (15.9% of all
sentences with at least one event). Therefore, in
addition to this best case oracle score, we also de-
fine a worst case oracle score, where no events
are removed from sentences containing only false-
positives. This error analysis is carried out on the
development set using our own implementation of
the performance measure to obtain per-event cor-
rectness judgments.
The results are shown in Table 7. Even for the
worst case oracle, the re-ranked output has the po-
tential to provide a 9.5pp increase in precision for
a 0.8pp loss in recall over the baseline TEES sys-
tem. How much of this potential gain is realized
depends on the accuracy of the threshold regres-
sor. In the current system, only a 1.7pp precision
increase for a 0.8pp recall loss is attained, demon-
strating that the threshold regressor leaves much
room for improvement.
The best case oracle precision is 26.4pp higher
than the worst case oracle, indicating that substan-
tial performance losses can be attributed to sen-
tences with purely false positive events. Indeed,
sentences only containing one or two incorrect
events account for 26% of all sentences with at
least one predicted event. Due to their large impact
TEES 1-arg N-arg Full
Simple events 64.43 +0.07 ?0.00 +0.07
Protein mod. 40.47 +0.06 ?0.00 +0.06
Binding 82.03 ?0.00 ?0.00 ?0.00
Regulation 30.34 +0.70 -0.14 +0.53
All events 45.04 +0.66 ?0.00 +0.64
Table 8: Performance of the system on the de-
velopment set when applied to single-argument
events only (1-arg), to multiple-argument events
only (N-arg), and to all events (Full).
on the overall system performance, these cases
may justify a focused effort in future research.
To establish the relative merit of the re-ranker,
we compare the worst-case oracle scores of the re-
ranked output against random ranking, averaged
over 10 randomization runs. While the difference
between TEES output and the random ranking re-
flects the effect of using an oracle to optimize per-
sentence score, the difference between the ran-
dom ranking and the re-ranker output shows an
actual added value of the re-ranker, not attained
from the use of oracle thresholds. Here it is of
particular interest to note that this difference is
more pronounced for events with multiple argu-
ments (5.95pp of precision) as opposed to single-
argument events (3.36pp of precision), possibly
due to the fact that such events have a much richer
feature representation and also employ the EVEX
resource. To assess the contribution of EVEX
data, a re-ranker was trained solely on features de-
rived from EVEX. This re-ranker achieved an F-
score of 1.26pp higher than randomized ranking,
thus suggesting that these features have a positive
influence on the overall score.
To verify these results and measure their im-
pact on the official evaluation, Table 8 summa-
rizes the performance on the development set us-
ing the official evaluation service. To study the
effect on single-argument events (column 1-arg),
the re-ranker score for multiple-argument events
is artificially increased to always fall above the
threshold. A similar strategy is used to study
the effect on multiple-argument events (column
N-arg). These results confirm that the overall
performance gain of our system on top of TEES
is obtained on single-argument events. Further,
multiple-argument events have only a negligible
effect on the overall score, demonstrating that, due
to their low frequency, little can be gained or lost
purely on multiple-argument events.
To summarize the error analysis, the results in
32
Table 7 suggest that the re-ranker is more effec-
tive on multiple-argument events where it receives
more features including external information from
EVEX. On the other hand, the results in Table 8
clearly demonstrate that the system is overall more
effective on single-argument events. This would
suggest a ?mismatch? between the re-ranker and
the threshold regressor, each being more effective
on a different class of events. One possible expla-
nation is the fact that the threshold regressor pre-
dicts a single threshold for all events in a sentence,
regardless of their type and number of arguments.
If these cannot be distinguished by one threshold,
it is clear that the threshold regressor will optimize
for the largest event type, i.e. a single-theme regu-
lation. Studying ways to allow the regressor to act
separately on various event types will be important
future work.
4.4 Discussion and future work
One of the main limitations of our approach is
that it can only increase precision, but not recall,
since it removes events from the TEES output, but
is not able to introduce new events. As TEES
utilizes separate processing stages for predicting
event triggers and argument edges, recall can be
adjusted by altering either of these steps. We
have briefly experimented with modifying TEES
to over-generate events by artificially lowering the
prediction threshold for event triggers. However,
this simple strategy of over-generating triggers
leads to a number of clearly incorrect events and
did not provide any performance gain. As future
work, we thus hope to explore effective ways to
over-generate events in a more controlled and ef-
fective fashion. In particular, a more detailed eval-
uation is needed to assess whether the rate of trig-
ger over-generation should be adjusted separately
for each event type. Another direction to explore
is to over-generate argument edges. This will en-
tail a detailed analysis of partially correct events
with a missing argument in TEES output. As in
the case of triggers, it is likely that each event type
will need to be optimized separately.
A notable amount of sentences include only
false positive predictions, severely complicating
the threshold regression. In an attempt to over-
come this issue, we trained a sentence classifier
for excluding sentences that should not contain
any events. This classifier partially utilized the
same features as the threshold regressor, as well
as bag of words and bag of POS tags. This
method showed some promise when used together
with trigger over-generation, but the gain was not
enough to surpass the lost precision caused by the
over-generation. If the event over-generation can
be improved, the feasibility of this method should
be re-evaluated.
5 Conclusions
We have presented our participation in the latest
BioNLP Shared Task by mainly relying on the
large-scale text mining resource EVEX. For the
GRN task, we were able to produce a gene reg-
ulatory network from the EVEX data without re-
training specific text mining algorithms. Using
predicted gene/protein symbols and the GE for-
malism, rather than gold standard entities and the
GRN annotation scheme, our final result on the
test set only performed 0.06 SER points worse
as compared to the corresponding TEES submis-
sion. This encouraging result warrants the use of
generic large-scale text mining data in network bi-
ology settings. As future work, we will extend the
EVEX dataset with information on the entity types
to enable pruning of false-positive events and
more fine-grained classification of event types,
such as the distinction between promoter binding
(Protein-Gene Binding) and protein-protein inter-
actions (Protein-Protein Binding).
In the GE task, we explored a re-ranking ap-
proach to improve the precision of the TEES
event extraction system, also incorporating fea-
tures from the EVEX resource. This approach
led to a modest increase in the overall F-score
of TEES and resulted in the first rank on the GE
task. In the subsequent error analysis, we have
demonstrated that the re-ranker provides an oppor-
tunity for a substantial increase of performance,
only partially realized by the regressor which sets
a per-sentence threshold. The analysis has identi-
fied numerous future research directions.
Acknowledgments
Computational resources were provided by CSC
IT Center for Science Ltd., Espoo, Finland. The
work of KH and FG was supported by the
Academy of Finland, and of SVL by the Research
Foundation Flanders (FWO). YVdP and SVL ac-
knowledge the support from Ghent University
(Multidisciplinary Research Partnership Bioinfor-
matics: from nucleotides to networks).
33
References
Jari Bjo?rne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the BioNLP
2013 Shared Task. In Proceedings of BioNLP
Shared Task 2013 Workshop. In press.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012a.
Generalizing biomedical event extraction. BMC
Bioinformatics, 13(suppl. 8):S4.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012b.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD).
Suwisa Kaewphan, Sanna Kreula, Sofie Van Lan-
deghem, Yves Van de Peer, Patrik Jones, and Filip
Ginter. 2012. Integrating large-scale text mining
and co-expression networks: Targeting NADP(H)
metabolism in E. coli with event extraction. In Pro-
ceedings of the Third Workshop on Building and
Evaluating Resources for Biomedical Text Mining
(BioTxtM 2012).
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The Genia event and protein coreference tasks of the
BioNLP Shared Task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedi-
cal named entity recognition. Pacific Symposium on
Biocomputing. Pacific Symposium on Biocomputing,
pages 652?663.
David McClosky, Sebastian Riedel, Mihai Surdeanu,
Andrew McCallum, and Christopher Manning.
2012. Combining joint models for biomedical event
extraction. BMC Bioinformatics, 13(Suppl 11):S9.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011. EVEX: a PubMed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of the BioNLP
2011 Workshop, pages 28?37.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013a. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.
Sofie Van Landeghem, Stefanie De Bodt, Zuzanna J.
Drebert, Dirk Inze?, and Yves Van de Peer. 2013b.
The potential of text mining in data integration and
network biology for plant research: A case study on
Arabidopsis. The Plant Cell, 25(3):794?807.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Chih-Hsuan Wei and Hung-Yu Kao. 2011. Cross-
species gene normalization by species inference.
BMC Bioinformatics, 12(Suppl 8):S5.
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hi-
rao, Masayuki Asahara, and Yuji Matsumoto. 2011.
Coreference based event-argument relation extrac-
tion on biomedical text. Journal of Biomedical Se-
mantics, 2(Suppl 5):S6.
34
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 116?124,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Care Episode Retrieval
Hans Moen
1
, Erwin Marsi
1
, Filip Ginter
2
,
Laura-Maria Murtola
3,4
, Tapio Salakoski
2
, Sanna Salanter
?
a
3,4
,
1
Dept. of Computer and Information Science,
Norwegian University of Science and Technology, Norway
2
Dept. of Information Technology, University of Turku, Finland
3
Dept. of Nursing Science, University of Turku, Finland
4
Turku University Hospital, Finland
{hans.moen,emarsi}@idi.ntnu.no, ginter@cs.utu.fi,
{lmemur,tapio.salakoski,sansala}@utu.fi
Abstract
The documentation of a care episode con-
sists of clinical notes concerning patient
care, concluded with a discharge sum-
mary. Care episodes are stored electron-
ically and used throughout the health care
sector by patients, administrators and pro-
fessionals from different areas, primarily
for clinical purposes, but also for sec-
ondary purposes such as decision support
and research. A common use case is, given
a ? possibly unfinished ? care episode,
to retrieve the most similar care episodes
among the records. This paper presents
several methods for information retrieval,
focusing on care episode retrieval, based
on textual similarity, where similarity is
measured through domain-specific mod-
elling of the distributional semantics of
words. Models include variants of random
indexing and a semantic neural network
model called word2vec. A novel method is
introduced that utilizes the ICD-10 codes
attached to care episodes to better induce
domain-specificity in the semantic model.
We report on an experimental evaluation
of care episode retrieval that circumvents
the lack of human judgements regarding
episode relevance by exploiting (1) ICD-
10 codes of care episodes and (2) seman-
tic similarity between their discharge sum-
maries. Results suggest that several of the
methods proposed outperform a state-of-
the art search engine (Lucene) on the re-
trieval task.
1 Introduction
Information retrieval (IR) aims at retrieving and
ranking documents relative to a textual query ex-
pressing the information need of a user (Manning
et al., 2008). IR has become a crucial technology
for many organisations that deal with vast amounts
of partly structured and unstructured (free text)
data stored in electronic format, including hospi-
tals and other health care providers. IR is an es-
sential part of the clinical practice; e.g., on-line IR
systems are associated with substantial improve-
ments in clinicians decision-making concerning
clinical problems (Westbrook et al., 2005).
The different stages of the clinical care of a pa-
tient are documented in clinical care notes, con-
sisting mainly of free text. A care episode consists
of a sequence of individual clinical care notes,
concluded by a discharge summary, as illustrated
in Figure 1. Care episodes are stored in elec-
tronic format in electronic health record (EHR)
systems. These systems are used throughout the
health care sector by patients, administrators and
professionals from different areas, primarily for
clinical purposes, but also for secondary purposes
such as decision support and research (H?ayrinen et
al., 2008). IR from EHR in general is therefore a
common and important task.
This paper focuses on the particular task of re-
trieving those care episodes that are most similar
to the sequence of clinical notes for a given pa-
tient, which we will call care episode retrieval.
In conventional IR, the query typically consists of
several keywords or a short phrase, while the re-
trievable units are typically documents. In con-
trast, in care episode retrieval, the query consist of
the clinical notes contained in a care episode. The
discharge summary is used separately for evalu-
116
A
BTime
Clinical notes Dischargesummary
Figure 1: Illustration of care episode retrieval. The
two care episodes (A and B) are composed of
a number of individual clinical notes and a sin-
gle discharge summary. Given an ongoing care
episode (minus the discharge summary), the task
is to retrieve other, similar care episodes.
ation purposes, and is assumed to be unavailable
for constructing a query at retrieval time. Retriev-
able units are thus complete care episodes without
summaries.
We envision a number of different use cases for
a care episode retrieval system. Firstly, it could fa-
cilitate clinicians in decision-making. For exam-
ple, given a patient that is being treated in a hos-
pital, an involved clinician may want to find previ-
ous patients that are similar in terms of their health
history, symptoms or received treatments. Supple-
mentary input from the clinician would enable the
system to give heightened weight to keywords of
particular interest within the care episodes, which
would further be emphasized in the semantic sim-
ilarity calculation during IR. It may help consider-
ably to see what similar patients have received in
terms of medication and further treatment, what
related issues such as bi-conditions or risks oc-
curred, how other clinicians have described cer-
tain aspects, what clinical practice guidelines have
been utilized, and so on. This relates to the un-
derlying principle in textual case-based reasoning
(Lenz et al., 1998). Secondly, it could help man-
agement to get almost real time information con-
cerning the overall situation on the unit for a spe-
cific follow-up period. Such a system could for ex-
ample support managerial decision-making with
statistical information concerning care trends on
the unit, adverse events or infections. Thirdly, it
could facilitate knowledge discovery and research.
For instance, it could enable researchers to map
or cluster similar care episodes to find common
symptoms or conditions. In sum, care episode re-
trieval is likely to improve care quality and consis-
tency in hospitals.
From the perspective of NLP, care episode re-
trieval ? and IR from EHRs in general ? is a
challenging task. It differs from general-purpose
web search in that the vocabulary, the informa-
tion needs and the queries of clinicians are highly
specialised (Yang et al., 2011). Clinical notes
contain highly domain-specific terminology (Rec-
tor, 1999; Friedman et al., 2002; Allvin et al.,
2010) and generic text processing resources are
therefore often suboptimal or inadequate (Shatkay,
2005). At the same time, development of dedi-
cated clinical NLP tools and resources is often dif-
ficult and costly. For example, popular data-driven
approaches to NLP are based on supervised learn-
ing, which requires substantial amounts of tailored
training data, typically built through manual anno-
tation by annotators who need both linguistic and
clinical knowledge. Additionally, variations in the
language and terminology used in sub-domains
within and across health care organisations greatly
limit the scope of applicability of such training
data (Rector, 1999).
Recent work has shown that distributional mod-
els of semantics, induced in an unsupervised man-
ner from large corpora of clinical and/or medical
text, are well suited as a resource-light approach
to capturing and representing domain-specific ter-
minology (Pedersen et al., 2007; Koopman et al.,
2012; Henriksson et al., 2014). This raises the
question to what extent distributional models of
semantics can alleviate the aforementioned prob-
lems of NLP in the clinical domain. The work
reported here investigates to what extent distribu-
tional models of semantics, built from a corpus of
clinical text in an fully unsupervised manner, can
be used for care episode retrieval. Models include
several variants of random indexing and a seman-
tic neural network model called word2vec, which
will be described in more detail in Section 4.
It has been argued that clinical NLP should ex-
ploit existing knowledge resources such as knowl-
edge bases about medications, treatments, dis-
eases, symptoms and care plans, despite these not
having been explicitly built for doing clinical NLP
(Friedman et al., 2013). Along these lines, a novel
method is proposed here that utilizes the ICD-10
codes ? diagnostic labels attached to care episodes
by clinicians ? to better induce domain-specificity
in the semantic model. Experimental results sug-
gest that this method outperforms a state-of-the art
search engine (Lucene) on the task of care episode
117
retrieval.
Apart from issues related to clinical terminol-
ogy, another problem in care episode retrieval is
the lack of benchmark data, such as the relevance
scores produced by human judges commonly used
for evaluation of IR systems. Although collec-
tions of care episodes may be available, producing
gold standard similarity scores required for evalu-
ation is costly. Another contribution of this paper
is the proposal of evaluation procedures that cir-
cumvent the lack of human judgements regarding
episode similarity. This is accomplished by ex-
ploiting either (1) ICD-10 codes of care episodes
or (2) semantic similarity between their discharge
summaries. Despite our focus on the specific task
of care episode retrieval, we hypothesize that the
methods and models proposed here have the po-
tential to increase performance of IR on clinical
text in general.
2 Data
The data set used in this study consists of the elec-
tronic health records from patients with any type
of heart related problem that were admitted to one
particular university hospital in Finland between
the years 2005-2009. Of these, only the clini-
cal notes written by physician are used. A sup-
porting statement for the research was obtained
from the Ethics Committee of the Hospital District
(17.2.2009 ?67) and permission to conduct the re-
search was obtained from the Medical Director of
the Hospital District (2/2009). The total set consist
of 66884 care episodes, which amounts to 398040
notes and 64 million words in total. This full set
was used for training of the semantic models. To
make the experimentation more convenient, we
chose to use a subset for evaluation. This com-
prises 26530 care episodes, amounting to 155562
notes and 25.7 million words in total.
Notes are mostly unstructured, consisting of
free text in Finnish. Some meta-data ? such as
names of the authors, dates, wards, and so on ? is
present, but is not used for retrieval.
Care episodes have been manually labeled ac-
cording to the 10th revision of the International
Classification of Diseases (ICD-10) (World Health
Organization and others, 2013), a standardised
tool of diagnostic codes for classifying diseases.
Codes are normally applied at the end of the pa-
tient?s stay, or even after the patient has been dis-
charged from the hospital. Care episodes have
one primary ICD-10 code attached and optionally
a number of additionally relevant codes. In this
study, only the primary one is used, because ex-
traction of the secondary codes is non-trivial.
ICD-10 codes have an internal structure that re-
flects the classification system ranging from broad
categories down to fine-grained subjects. For ex-
ample, the first character (J) of the code J21.1
signals that it belongs to the broad category Dis-
eases of the respiratory system. The next two
digits (21) classify the subject as belonging to
the subcategory Acute bronchiolitis. Finally, the
last digit after the dot (1) means that it belongs
to the sub-subclass Acute bronchiolitis due to hu-
man metapneumovirus. There are 356 unique ?pri-
mary? ICD-10 codes in the evaluation data set.
3 Task
The task addressed in this study is retrieval of care
episodes that are similar to each other. In con-
trast to the normal IR setting, where the search
query is derived from a text stating the user?s in-
formation need, here the query is based on an-
other care episode, which we refer to as the query
episode. As the query episode may document on-
going treatment, and thus lack a discharge sum-
mary and ICD-10 code, neither of these informa-
tion sources can be relied upon for constructing
the query. The task is therefore to retrieve the most
similar care episodes using only the information
contained in the free text of the clinical notes in
the query episode.
Evaluation of retrieval results generally re-
quires an assessment of their relevancy to the
query. Since similarity judgements by humans
are currently lacking, and obtaining these is time-
consuming and costly, we explored alternative
ways of evaluating performance on the task. The
first alternative is to assume that care episodes are
similar if they have the same ICD-10 code. That is,
a retrieved care episode is considered correct if its
ICD-10 code is identical to the code of the query
episode. It should be noted that ICD-10 codes are
not used in the query in any of the experiments.
Closer inspection shows that the free text con-
tent in care episodes with the same ICD-10 code
is indeed quite similar in many cases, but not al-
ways. Considering all of them equally similar
amounts to an arguably coarse approximation of
relevance. The second alternative tries to remedy
this issue by measuring the similarity between dis-
118
charge summaries. That is, if the discharge sum-
mary of a retrieved episode is semantically simi-
lar to the discharge summary of the query episode,
the retrieved episode is assumed to be correct.
In practice, textual similarity between discharge
summaries, and therefore the relevance score, is
continuous rather than binary. It is measured using
the same models of distributional semantics used
for retrieval, which will be described in Section 4.
It should be stressed that the discharge summaries
are not taken into consideration during retrieval in
any of the experiments and are only used for eval-
uation.
4 Method
4.1 Semantic models
A crucial part in retrieving similar care episodes
is having a good similarity measure. Here similar-
ity between care episodes is measured as the sim-
ilarity between the words they contain (see Sec-
tion 4.2). Semantic similarity between words is in
turn measured through the use of word space mod-
els (WSM), without performing an explicit query
expansion step. Several variants of these models
were tested, utilizing different techniques and pa-
rameters for building them. The models trained
and tested in this paper are: (1) classic random
indexing with a sliding window using term in-
dex vectors and term context vectors (RI-Word);
(2) random indexing with index vectors for doc-
uments (RI-Doc); (3) random indexing with in-
dex vectors for ICD-10 codes (RI-ICD); (4) a ver-
sion of random indexing where only the term in-
dex vectors are used (RI-Index); and (5) a seman-
tic neural network model, using word2vec to build
word context vectors (Word2vec).
RI-Word
Random Indexing (RI) (Kanerva et al., 2000) is
a method for building a (pre) compressed WSM
with a fixed dimensionality, done in an incremen-
tal fashion. RI consist of the following two steps:
First, instead of allocating one dimension in the
multidimensional vector space to a single word,
each word is assigned an ?index vector? as its
unique signature in the vector space. Index vectors
are generated vectors consisting of mostly zeros
together with a randomly distributed set of several
1?s and -1?s, uniquely distributed for each unique
word; The second step is to induce ?context vec-
tors? for each word. A context vector represents
the contextual meaning of a word in the WSM.
This is done using a sliding window of a fixed size
to traverse a training corpus, inducing context vec-
tors for the center/target word of the sliding win-
dow by summing the index vectors of the neigh-
bouring words in the window.
As the dimensionality of the index vectors is
fixed, the dimensionality of the vector space will
not grow beyond the size W ?Dim, where W is
the number of unique words in the vocabulary, and
Dim being the pre-selected dimensionality to use
for the index vectors. As a result, RI models are
significantly smaller than plain word space mod-
els, making them a lot less computationally expen-
sive. Additionally, the method is fully incremental
(additional training data can be added at any given
time without having to retrain the existing model),
easy to parallelize, and scalable, meaning that it is
fast and can be trained on large amounts of text in
an on-line fashion.
RI-Doc
Contrary to sliding window approach used in RI-
Word, a RI model built with document index vec-
tors first assigns unique index vectors to every
document in the training corpus. In the training
phase, each word in a document get the respective
document vector added to its context vector. The
resulting WSM is thus a compressed version of a
term-by-document matrix.
RI-ICD
Based on the principle of RI with document index
vectors, we here explore a novel way of construct-
ing a WSM by exploiting the ICD-10 code classi-
fication done by clinicians. Instead of using doc-
ument index vectors, we here use ICD-code index
vectors. First, a unique index vector is assigned to
each chapter and sub-chapter in the ICD-10 taxon-
omy. This means assigning a unique index vector
to each ?node? in the ICD-10 taxonomy, as illus-
trated in Figure 2. For each clinical note in the
training corpus, the index vector of the their pri-
mary ICD-10 code is added to all words within it.
In addition, all the index vectors for the ICD-codes
higher in the taxonomy are added, each weighted
according to their position in the hierarchy. A
weight of 1 is given to the full code, while the
weight is halved for each step upwards in the hi-
erarchy. The motivation for the latter is to capture
a certain degree of similarity between codes that
share an initial path in the taxonomy. As a result,
119
J	 ?
0.125	 ? 0.25	 ? 0.5	 ? 1	 ? Weight	 ?
2	 ?
1	 ?
0	 ?
1	 ?
1	 ?
0	 ?
J21.1	 ?
0	 ?
Figure 2: Weighting applied to ICD-code index
vectors when training WSMs based on ICD-10
codes (RI-ICD).
this similarity is encoded in the resulting WSM.
As a example: for a clinical note labelled with the
code J21.1, we add the following index vectors
to the context vectors of all its constituting words:
iv(J)? 0.125, iv(J2)? 0.25, iv(J21)? 0.5 and
iv(J21.1) ? 1.0. The underlying hypothesis for
building a WSM in this way is that it may cap-
ture relations between words in a way that bet-
ter reflects the clinical domain, compared to the
other domain-independent methods for construct-
ing a WSM.
RI-Index
As an alternative to using word?s (semantic) con-
text vectors, we simply only use their index vec-
tors as their ?contextual meaning?. When con-
structing document vectors directly from word in-
dex vectors (see Section 4.2), the resulting docu-
ment vectors represent a compressed version of a
document-by-term matrix.
Word2vec
Recently, a novel method for inducing WSMs was
introduced by Mikolov et al. (2013a), stemming
from the research in deep learning and neural net-
work language models. While the overall objec-
tive of learning a continuous vector space repre-
sentation for each word based on its textual con-
text remains, the underlying algorithms are sub-
stantially different from traditional methods such
as Latent Semantic Analysis and RI. Considering,
in turn, every word in the training data as a target
word, the method induces the representations by
training a simplified neural network to predict the
nearby context words of each target word (skip-
gram architecture), or alternatively the target word
based on all words in its immediate context (BoW
architecture). The vector space representation is
subsequently extracted from the learned weights
within the neural network. One of the main prac-
tical advantages of the word2vec method lies in
its scalability, allowing quick training on large
amounts of text, setting it apart from the majority
of other methods of distributional semantics. Ad-
ditionally, the word2vec method has been shown
to produce representations that surpass in quality
traditional methods such as Latent Semantic Anal-
ysis, especially on tasks measuring the preserva-
tion of important linguistic regularities (Mikolov
et al., 2013b).
4.2 Computing care episode similarity
After having computed a WSM, the next step is
to build episode vectors to use for the actual re-
trieval task. This is done by first normalizing the
word vectors and multiplying them with a word?s
TF*IDF weight. An episode vector is then ob-
tained by summing the word vectors of all its
words and dividing the result by the total num-
ber of words in the episode. Similarity between
episodes is determined by computing the cosine
similarity between their vectors.
4.3 Baselines
Two baselines were used in this study. The first
one is random retrieval of care episodes, which
can be expected to give very low scores and serves
merely as a sanity check. The second one is
Apache Lucene (Cutting, 1999), a state-of-the-art
search engine based on look-up of similar docu-
ments through a reverse index and relevance rank-
ing based on a TF*IDF-weighted vector space
model. Care episodes were indexed using Lucene.
Similar to the other models/methods, all of the free
text in the query episode, excluding the discharge
summary, served as the query string provided to
Lucene. Being a state-of-the-art IR system, the
scores achieved by Lucene in these experiments
should indicate the difficulty of the task.
5 Experiments
In these experiments we strove to have a setup
that was as comparable as possible for all models
and systems, both in terms of text pre-processing
and in terms of the target model dimensionality
when inducing the vector space models. The clin-
120
ical notes are split into sentences, tokenized, and
lemmatized using a Constraint-Grammar based
morphological analyzer and tagger extended with
clinical vocabulary (Karlsson, 1995). After stop
words were removed
1
, the total training corpus
contained 39 million words (minus the query
episodes), while the evaluation subset contained
18.5 million words. The vocabulary consisted of
0.6 million unique terms. Twenty care episodes
were randomly selected to serve as the query
episodes during testing, with the requirement that
each had different ICD-10 codes and consisted of a
minimum of six clinical notes. The average num-
ber of words per query episode is 830.
RI-based and word2vec models have a prede-
fined dimensionality of 800. For RI-based mod-
els, 4 non-zeros were used in the index vectors.
For the RI-Word model, a narrow context win-
dow was employed (5 left + 5 right), weighting
index vectors according to their distance to the tar-
get word (weight
i
= 2
1?dist
it
). In addition, the
index vectors were shifted once left or right de-
pending on what side of the target word they were
located, similar to direction vectors as described
in (Sahlgren et al., 2008) These parameters for RI
were chosen based on previous work on semantic
textual similarity (Moen et al., 2013). Also a much
larger window of 20+20 was tested, but without
noteworthy improvements. The word2vec model
is trained with the BoW architecture and otherwise
default parameters. In addition to Apache Lucene
(version 4.2.0)
2
, the word2vec tool
3
was used to
train the word2vec model, and the RI-based meth-
ods utilized the JavaSDM package
4
. Scores were
calculated using the trec eval tool
5
.
5.1 Experiment 1: ICD-10 code overlap
In this experiment retrieved episodes with a pri-
mary ICD-10 code identical to that of the query
episode were considered to be correct. The num-
ber of correct episodes varies between 49 and
1654. The total is 7721, and the average is
386. The high total is mainly due to three query
episodes with ICD-10 codes that occur very fre-
quently in the episode collection (896, 1590, and
1
http://www.nettiapina.fi/
finnish-stopword-list/
2
http://archive.apache.org/dist/
lucene/java/
3
https://code.google.com/p/word2vec/
4
http://www.nada.kth.se/
?
xmartin/java/
5
http://trec.nist.gov/trec_eval/
IR model MAP P@10
Lucene 0.1379 0.3000
RI-Word 0.0911 0.2650
RI-Doc 0.1015 0.3300
RI-ICD 0.3261 0.5150
RI-Index 0.1187 0.3200
Word2vec 0.1768 0.3350
Random 0.0154 0.0200
Table 1: Mean average precision and precision at
10 for retrieval of care episodes with the same pri-
mary ICD-10 code as the query episode
1654 times). When conducting the experiment all
care episodes were retrieved for each of the 20
query episodes.
Performance was measured in terms of mean
average precision (MAP) and precision among
the top-10 results (P@10), averaged over all 20
queries, as shown in in Table 1. The best MAP
score is achieved by RI-ICD, almost twice that of
word2vec, which achieved the second best MAP
score, whereas RI-Word performed worst of all.
All models score well above the random baseline,
whereas RI-ICD outperforms Lucene by a large
margin. P@10 scores follow the same ranking.
The latter scores are more representative for most
use cases where users will only inspect the top-n
retrieval results.
5.2 Experiment 2: Discharge summary
overlap
In this experiment retrieved episodes with a dis-
charge summary similar to that of the query
episode were considered to be correct. Using the
discharge summaries of the query episodes, the
top 100 care episodes with the most similar dis-
charge summary were selected as the most simi-
lar care episodes (disregarding the query episode).
This was repeated for each of the methods ? i.e.
the five different semantic models and Lucene ?
resulting in six different tests. The top 100 was
used rather than a threshold on the similarity score,
because otherwise six different thresholds would
have to be chosen. This procedure thus resulted in
six different test collections, each consisting of 20
query episodes with their corresponding 100 most
similar collection episodes.
Subsequently a 6-by-6 experimental design was
followed where each retrieval method was tested
against each test set construction method. At re-
trieval time, for each query episode, the system re-
trieves and ranks 1000 care episodes. It can be ex-
pected that when identical methods are used for re-
121
trieval and test set construction, the resulting bias
gives rise to relatively high scores. In contrast,
averaging over the scores for all six construction
methods is assumed to be a less biased indicator
of performance.
Table 2 shows the number of correctly retrieved
episodes by the different models, with the maxi-
mum being 2000 (20 queries times 100 most sim-
ilar episodes). This gives an indication of the re-
call among a 1000 retrieved episodes per query,
but without caring about precision or ranking. In
general, the numbers are relatively good when the
same model is used for both retrieval and construc-
tion of the test set (cf. values on the diagonal), al-
though in a couple of cases (e.g. with word2vec)
results are better with different models. The RI-
ICD model performs best when used for both re-
trieval and test construction. Looking at the av-
erages, which presumably are less biased indica-
tors, RI-ICD and word2vec seem to have compa-
rable performance, with both of them outperform-
ing Lucene. Other models are less successful, al-
though still much better than the random baseline.
The MAP scores in Table 3 show similar re-
sults, although here RI-ICD yields the best aver-
age score. Both models RI-ICD and word2vec
outperform Lucene. Again the RI-ICD model per-
forms exceptionally well when used for both re-
trieval and test construction.
Finally Table 4 presents precision for top-10 re-
trieved care episodes. Here RI-Doc yields the best
average scores, while RI-ICD and word2vec both
perform slightly worse.
6 Discussion
The goal of the experiments was primarily to
determine which distributional semantic models
work best for care episode retrieval. The exper-
imental results show that several models outper-
form Lucene at the care episode retrieval task.
This suggests that models of higher order seman-
tics contribute positively to calculating document
similarities in the clinical domain, compared with
straight forward boolean word matching (cf. RI-
Index and Lucene).
The relatively good performance of the RI-ICD
model, particularly in Experiment 1, suggests that
exploiting structured or encoded information in
building semantic models for clinical NLP is a
promising direction that calls for further investi-
gation. This approach concurs with the arguments
in favor of reuse of existing information sources
in Friedman et al. (2013). On the one hand, it
may not be surprising that the RI-ICD model is
performing well on Experiment 1, given how it in-
duces semantic relations between words occurring
in episodes with the same ICD-10 code. On the
other hand, being able to accurately retrieve care
episodes with similar ICD-10 codes evidently has
practical value from a clinical perspective.
The different ranking of models in experiments
1 versus 2 confirms that there is a difference be-
tween the two indicators of episode similarity,
i.e. similarity in terms of their ICD-10 codes
versus similarity with regard to their discharge
summaries. In our data a single care episode
can potentially span across several hospital wards.
A better correlation between the similarity mea-
sures is to be expected when narrowing the def-
inition of a care episode to only a single ward.
Also, taking into consideration all ICD-10 codes
for care episodes ? not only the primary one ?
could potentially improve discrimination among
care episodes. This could be useful in two ways:
(1) to create more precise test sets of the type used
in Experiment 1; (2) to extend RI-ICD models
with index vectors also for the secondary ICD-10
codes.
Input to the models for training was limited to
the free text in the clinical notes, with the ex-
ception of the use of ICD-10 codes in the RI-
ICD model. Other sources of information could,
and probably should, be utilized in a practical
care episode retrieval system applied in a hospi-
tal, such as the structured and coded information
commonly found in EHR systems. Another po-
tential information source is the internal structure
of the care episodes, as episodes containing sim-
ilar notes in the same sequential order are intu-
itively more likely to be similar. We tried comput-
ing exhaustive pairwise similarities between the
individual notes from two episodes and then tak-
ing the average of these as a similarity measure
for the episodes. However, this did not improve
performance on any measure. An alternative ap-
proach may be to apply sequence alignment algo-
rithms, as commonly used in bioinformatics (Gus-
field, 1997), in order to detect if both episodes
contain similar notes in the same temporal order.
We leave this to future work.
122
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 889 700 670 687 484 920 725 2
RI-Word 643 800 586 600 384 849 644 5
RI-Doc 665 630 859 697 436 795 680 4
RI-ICD 635 459 659 1191 490 813 707 3
RI-Index 690 491 607 654 576 758 629 6
Word2vec 789 703 702 870 516 1113 782 1
Random 74 83 86 67 84 85 79 7
Table 2: Number of correctly retrieved episodes (max 2000) for different IR models (rows) when using
different models for measuring discharge summary similarity (columns)
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 0.0856 0.0357 0.0405 0.0578 0.0269 0.0833 0.0550 3
RI-Word 0.0392 0.0492 0.0312 0.0412 0.0151 0.0735 0.0416 6
RI-Doc 0.0493 0.0302 0.0677 0.0610 0.0220 0.0698 0.0500 4
RI-ICD 0.0497 0.0202 0.0416 0.1704 0.0261 0.0712 0.0632 1
RI-Index 0.0655 0.0230 0.0401 0.0504 0.0399 0.0652 0.0473 5
Word2vec 0.0667 0.0357 0.0404 0.0818 0.0293 0.1193 0.0622 2
Random 0.0003 0.0003 0.0005 0.0002 0.0003 0.0004 0.0003 7
Table 3: Mean average precision for different IR models (rows) when using different models for measur-
ing discharge summary similarity (columns)
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 0.2450 0.1350 0.1200 0.1650 0.0950 0.1900 0.1583 5
RI-Word 0.1350 0.1500 0.1000 0.1350 0.0600 0.2100 0.1316 6
RI-Doc 0.2000 0.1250 0.2050 0.2200 0.0900 0.2400 0.1800 1
RI-ICD 0.1700 0.0650 0.1350 0.3400 0.0950 0.2050 0.1683 2
RI-Index 0.2000 0.1250 0.1550 0.1250 0.1700 0.2050 0.1633 3
Word2vec 0.1800 0.1200 0.1150 0.2100 0.0850 0.2650 0.1625 4
Random 0.0000 0.0000 0.0050 0.0000 0.0000 0.0000 0.0008 7
Table 4: Precision at top-10 retrieved episodes for different IR models (rows) when using different
models for measuring discharge summary similarity (columns)
7 Conclusion and future work
In this paper we proposed the task of care episode
retrieval as a way of evaluating several distribu-
tional semantic models in their performance at IR.
As manually constructing a proper test set of clas-
sified care episodes is costly, we experimented
with building test sets by exploiting either ICD-10
code overlap or semantic similarity of discharge
summaries. A novel method for generating se-
mantic models utilizing the ICD-10 codes of care
episodes in the training corpus was presented (RI-
ICD). The models, as well as the Lucene search
engine, were applied to the care episode retrieval
task and their performance was evaluated against
the test sets using different evaluation measures.
The results suggest that the RI-ICD model is bet-
ter suited to IR tasks in the clinical domain com-
pared with models trained on local distributions of
words, or those relying on direct word matching.
The word2vec model performed relatively well
and outperformed Lucene in both experiments.
In the results reported here, the internal se-
quence of clinical notes is ignored. Future work
should focus on exploring the temporal (sub-) se-
quence similarities between care episode pairs for
doing care episode retrieval. Further work should
also focus on expanding on the RI-ICD method
by exploiting other types of structured and/or en-
coded information related to clinical notes for
training semantic models tailored for NLP in the
clinical domain.
Acknowledgments
This study was partly supported by the Research
Council of Norway through the EviCare project
(NFR project no. 193022), the Turku University
Hospital (EVO 2014), and the Academy of Fin-
land (project no. 140323). The study is a part
of the research projects of the Ikitik consortium
(http://www.ikitik.fi). We would like
to thank Juho Heimonen for assisting us in pre-
processing the data and the reviewers for their
helpful comments.
123
References
Helen Allvin, Elin Carlsson, Hercules Dalianis, Ri-
itta Danielsson-Ojala, Vidas Daudaravi?cius, Mar-
tin Hassel, Dimitrios Kokkinakis, Helj?a Lundgren-
Laine, Gunnar Nilsson, ?ystein Nytr?, et al. 2010.
Characteristics and analysis of finnish and swedish
clinical intensive care nursing narratives. In Pro-
ceedings of the NAACL HLT 2010 Second Louhi
Workshop on Text and Data Mining of Health Docu-
ments, pages 53?60. Association for Computational
Linguistics.
Doug Cutting. 1999. Apache Lucene open source
package.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of zellig harris. Journal of
biomedical informatics, 35(4):222?235.
Carol Friedman, Thomas C Rindflesch, and Mil-
ton Corn. 2013. Natural language process-
ing: State of the art and prospects for significant
progress, a workshop sponsored by the national li-
brary of medicine. Journal of biomedical informat-
ics, 46(5):765?773.
Dan Gusfield. 1997. Algorithms on strings, trees and
sequences: computer science and computational bi-
ology. Cambridge University Press.
Kristiina H?ayrinen, Kaija Saranto, and Pirkko
Nyk?anen. 2008. Definition, structure, content, use
and impacts of electronic health records: a review
of the research literature. International journal of
medical informatics, 77(5):291?304.
Aron Henriksson, Hans Moen, Maria Skeppstedt, Vi-
das Daudaravi, Martin Duneld, et al. 2014. Syn-
onym extraction and abbreviation expansion with
ensembles of semantic spaces. Journal of biomed-
ical semantics, 5(1):6.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of 22nd Annual
Conference of the Cognitive Science Society, page
1036.
Fred Karlsson. 1995. Constraint grammar: a
language-independent system for parsing unre-
stricted text. Mouton de Gruyter, Berlin and New
York.
Bevan Koopman, Guido Zuccon, Peter Bruza, Lauri-
anne Sitbon, and Michael Lawley. 2012. An evalu-
ation of corpus-driven measures of medical concept
similarity for information retrieval. In Proceedings
of the 21st ACM international conference on Infor-
mation and knowledge management, pages 2439?
2442. ACM.
Mario Lenz, Andr?e H?ubner, and Mirjam Kunze. 1998.
Textual cbr. In Case-based reasoning technology,
pages 115?137. Springer.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751. Associa-
tion for Computational Linguistics, June.
Hans Moen, Erwin Marsi, and Bj?orn Gamb?ack. 2013.
Towards dynamic word sense discrimination with
random indexing. ACL 2013, page 83.
Ted Pedersen, Serguei VS Pakhomov, Siddharth Pat-
wardhan, and Christopher G Chute. 2007. Mea-
sures of semantic similarity and relatedness in the
biomedical domain. Journal of biomedical infor-
matics, 40(3):288?299.
Alan L Rector. 1999. Clinical terminology: why is
it so hard? Methods of information in medicine,
38(4/5):239?252.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In Proceedings of the Annual Meeting
of the Cognitive Science Society.
Hagit Shatkay. 2005. Hairpins in bookstacks: infor-
mation retrieval from biomedical text. Briefings in
Bioinformatics, 6(3):222?238.
Johanna I Westbrook, Enrico W Coiera, and A So-
phie Gosling. 2005. Do online information retrieval
systems help experienced clinicians answer clinical
questions? Journal of the American Medical Infor-
matics Association, 12(3):315?321.
World Health Organization and others. 2013. Interna-
tional classification of diseases (icd).
Lei Yang, Qiaozhu Mei, Kai Zheng, and David A
Hanauer. 2011. Query log analysis of an electronic
health record search engine. In AMIA Annual Sym-
posium Proceedings, volume 2011, page 915. Amer-
ican Medical Informatics Association.
124
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 1?10,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Post-hoc Manipulations of Vector Space Models
with Application to Semantic Role Labeling
Jenna Kanerva and Filip Ginter
Department of Information Technology
University of Turku, Finland
jmnybl@utu.fi, figint@utu.fi
Abstract
In this paper, we introduce several vector
space manipulation methods that are ap-
plied to trained vector space models in a
post-hoc fashion, and present an applica-
tion of these techniques in semantic role
labeling for Finnish and English. Specifi-
cally, we show that the vectors can be cir-
cularly shifted to encode syntactic infor-
mation and subsequently averaged to pro-
duce representations of predicate senses
and arguments. Further, we show that it is
possible to effectively learn a linear trans-
formation between the vector representa-
tions of predicates and their arguments,
within the same vector space.
1 Introduction
Recently, there has been much progress in the de-
velopment of highly scalable methods for induc-
ing vector space representations of language. In
particular, the word2vec method (Mikolov et al.,
2013b) is capable of training on billions of tokens
in a matter of hours, producing high quality rep-
resentations. An exciting property exhibited by
the vector spaces induced using word2vec is that
they preserve a number of linguistic regularities,
lending themselves to simple algebraic operations
with the vectors (Mikolov et al., 2013c) and linear
mapping between different spaces (Mikolov et al.,
2013a). These can be seen as post-hoc operations
manipulating the vector space with the significant
advantage of not requiring a new task-specific rep-
resentation to be induced, as is customary.
In this paper, we will investigate several addi-
tional such methods. Firstly, we will show how
syntax information can be encoded by the circular
shift operation and demonstrate that such shifted
vectors can be averaged in a meaningful manner to
represent predicate arguments. And secondly, we
will show that linear transformations of the vec-
tor spaces can be successfully applied also within
a single vector space, to tasks such as transform-
ing the vector of a predicate into the vector of its
argument with a particular role.
To test the above-mentioned operations in an
extrinsic setting, we will develop these methods
within the context of the Semantic Role Label-
ing (SRL) task. Automatic Semantic Role Label-
ing is the process of identifying the semantic ar-
guments of predicates, and assigning them labels
describing their roles. A predicate and its argu-
ments form a predicate-argument structure, which
describes events such as who does what to whom,
when and where.
The SRL task is ?semantic? in its nature and
therefore suitable for the application and testing
of vector space representations and methods for
their manipulation. However, rather than merely
adding features derived from vector spaces into
an existing system, we will approach the develop-
ment from a different angle and test whether these
representations of words and the similarities they
induce can be used for predicate argument role as-
signment and predicate sense disambiguation as
the primary source of information, with little ad-
ditional features.
In addition to the standard English CoNLL?09
dataset, we will apply the methods also to Finnish
SRL, testing the applicability of word2vec and the
overall methodology that we will develop in this
paper to this highly inflective language. With its
considerably larger and sparser surface form lex-
icon, Finnish poses interesting challenges of its
own, and only little attention has been dedicated
to the application of distributional semantics meth-
ods specifically to Finnish. This is also partly due
to the lack of sufficiently sized corpora, which we
address in this work by using a 1.5B token corpus
of Internet Finnish.
In order to be able to test the proposed meth-
1
ods on SRL, we need to carry out not only role
labeling and predicate sense disambiguation, but
also argument detection. As a secondary theme,
we thus test whether dependency parse graphs in
the semantically motivated Stanford Dependen-
cies (SD) scheme can be used as-is to perform ar-
gument identification. We are especially interested
in this scheme as it is designed to capture seman-
tically contentful relations (de Marneffe and Man-
ning, 2008) and would thus appear to be the ideal
choice as the underlying syntactic representation
for SRL.
2 Data and Task Setting
Throughout the paper, we will use the exact same
task setting as in the CoNLL?09 Shared Task on
Syntactic and Semantic Dependencies in Multiple
Languages (Haji?c et al., 2009). The input of the
SRL system are automatically generated syntactic
parses and the list of predicate tokens to be con-
sidered in each sentence. For each of the predi-
cates, the SRL system is expected to predict the
sense of the predicate, identify all tokens which
are its arguments, and for each argument, iden-
tify its role. As the primary measure of perfor-
mance, we will use the semantic F-score defined
in the CoNLL shared task. This F-score is cal-
culated from the precision and recall of argument
identification (calculated in the obvious manner)
and also incorporates the sense of the predicate via
an additional ?dummy? argument. We use the of-
ficial implementation of the metric distributed on
the Shared Task site.
1
We will report our results on two SRL datasets:
the Finnish PropBank (Haverinen et al., 2013a)
and the English SRL dataset from the CoNLL?09
Shared Task. The Finnish PropBank is built on
top of the Turku Dependency Treebank (TDT), a
205K token corpus of general Finnish (Haverinen
et al., 2013b) annotated using the SD scheme, in-
cluding manually annotated conjunct propagation
and other dependency relations from the non-basic
layer of the scheme. These extended SD analyzes
are thus not strictly trees, rather they are directed
labeled graphs (see Figure 1). The Finnish Prop-
Bank has 22 different argument roles of which 7
are numbered core roles and 15 are modifier roles.
The Finnish data has 164,530 training tokens with
27,603 occurrences of 2,826 unique predicate-
1
http://ufal.mff.cuni.cz/conll2009-st/
scorer.html
He ate.01 lunch and then washed.01 dishes .<nsubj:A0 dobj:A1> <advmod:AM-TMP dobj:A1>
cc> conj>
<nsubj:A0 punct>
Figure 1: Extended Stanford Dependencies
scheme combined with PropBank annotation.
sense combinations. The English CoNLL data is
derived from the PropBank and NomBank corpora
(Palmer et al., 2005; Meyers et al., 2004) and it has
a total of 54 different argument roles. In addition
to the same 22 roles as Finnish, English also has
discontinuous variants for each role. The English
data has 958,024 training tokens with 178,988 oc-
currences of 15,880 unique predicate-sense com-
binations.
All Finnish results are reported on the test sub-
set of the Finnish PropBank, and have no previ-
ously published baseline to compare with. The
results we report for English are produced on the
official test section of the CoNLL?09 data and are
thus directly comparable to the official results re-
ported in the Shared Task.
In the test phase, we follow the Shared Task set-
ting whereby morphological and syntactic analy-
sis is predicted as well, i.e., no gold standard data
enters the system other than the tokenization and
the information of which tokens constitute predi-
cates. We produce the Finnish morphological and
syntactic analyses for the test set with the parsing
pipeline of Haverinen et al. (2013b), composed of
a morphological analyzer and tagger (Hal?acsy et
al., 2007; Pirinen, 2008; Lind?en et al., 2009), de-
pendency parser (Bohnet, 2010) and a machine-
learning based component for predicting the ex-
tended SD dependencies (Nyblom et al., 2013).
While the English data is provided with automati-
cally produced dependency parses, we are specifi-
cally interested in the SD scheme and therefore we
re-parse the corpus with the Stanford parser
2
tak-
ing a union of the base and collapsed dependency
outputs to match the Finnish data.
The vector space models used throughout this
paper are induced using the word2vec software
(skip-gram architecture with default parameters).
For Finnish, the model is trained on 1.5 billion to-
kens of Finnish Internet texts gathered from the
Common Crawl dataset.
3
The data was sentence-
2
Version 3.3.1, October 2013
3
http://commoncrawl.org/
2
split and tokenized using the OpenNLP
4
toolchain
trained on TDT, and processed in the same man-
ner as the above-mentioned test set. This gives us
the opportunity to build two models, one for the
word forms and the other for the lemmas. Both
Finnish models have 300 dimensions. For En-
glish, the vector representation is induced on the
union of the English Wikipedia (1.7B tokens) and
the English Gigaword corpus (4B tokens), the to-
tal training data size thus being 5.7 billion tokens.
5
Sentence splitting and tokenization was carried out
using the relevant modules from the BRAT pack-
age (Stenetorp et al., 2012).
6
The English model
has 200 dimensions.
3 Method
In this section, we will describe the methods de-
veloped for argument identification, argument role
labeling and predicate sense disambiguation, the
three steps that must be implemented to obtain a
full SRL system.
3.1 Argument identification
In a semantically-oriented dependency scheme,
such as SD, it can be expected that a notable
proportion of arguments (in the SRL sense) are
directly attached to the predicate, and argument
identification can be reduced to assuming that ?
with a limited number of systematic exceptions ?
every argument is a dependent of the predicate.
The most frequent case where the assumption does
not hold in Finnish are the copula verbs, which are
not analyzed as heads in the SD scheme. For En-
glish, a common case are the auxiliaries, which
govern the main verb in the CoNLL data and are
thus marked as arguments for other higher-level
predicates as well. In the SD scheme, on the other
hand, the main verb governs the auxiliary taking
also its place in the syntactic tree. Since the fo-
cus of this paper lies in role assignment, we do
not go beyond developing a simple rule set to deal
with a limited number of such cases. In Section 6,
we will contrast this simple argument identifica-
tion method to that of the winning CoNLL?09 sys-
tem and we will show that while for Finnish the
above holds surprisingly well, the performance on
the English data is clearly sub-optimal.
4
http://opennlp.apache.org/
5
We are thankful to Sampo Pyysalo for providing us with
the English word2vec model.
6
http://brat.nlplab.org
Finnish
eat + A1 AM-TMP
salted fish not until
eggs now
wheat bread again
nuts when
pickled cucumbers then
English
drive + A1 drive + AM-TMP
car immediately
truck morning
cars now
vehicle afternoon
tires finally
Table 1: Five most similar words for the given
average argument vectors. AM-TMP refers to the
temporal modifier role. Note that the average vec-
tors for Finnish modifier roles are estimated inde-
pendently from the predicates (see Section 3.4).
3.2 Role Classification
Our initial role classification algorithm is based on
calculating the vector representation of an ?aver-
age argument? with a given role. For every predi-
cate x and every role r, we calculate the represen-
tation of the average argument with the role as
A(x, r) =
?
(r,x,y)
y?
count
, (1)
where y? refers to the L2 normalized version of y,
and count to the number of training pairs that are
summed over. We are thus averaging the normal-
ized vectors of all words y seen in the training data
as an argument of the predicate x with the role r.
To establish the role for some argument y during
testing, we can simply choose the role whose av-
erage argument vector has the maximal similarity
to y, i.e.
argmax
r
sim(A(x, r), y), (2)
where sim(a, b) is the standard cosine similarity.
To gain an intuitive insight into whether the av-
erage argument vectors behave as expected, we
show in Table 1 the top five most similar words
to the average argument vectors for several roles
and predicates. When evaluated with the data sets
described in Section 2, this initial method leads to
61.32% semantic F-score for Finnish and 65.05%
for English.
3
3.3 Incorporating syntax
As we will demonstrate shortly, incorporating in-
formation about dependency relations can lead to
a substantial performance gain. To incorporate the
dependency relation information into the role clas-
sification method introduced above, we apply the
technique of circular shifting of vectors. This tech-
nique was previously used in the context of Ran-
dom Indexing (RI) to derive new vectors from ex-
isting ones in a deterministic fashion (Basile and
Caputo, 2012). In RI, the shift operation is how-
ever not used on the final vectors, but rather al-
ready during the induction of the vector represen-
tation.
Given a vector representation of an argument y,
we can encode the dependency relation of y and
its predicate by circularly shifting the vector of
y by an offset assigned separately to each possi-
ble dependency relation. The assignment is arbi-
trary, but such that no two relations are assigned
the same offset. We will denote this operation as
yd, meaning the vector y circularly shifted to
the right by the offset assigned to the dependency
relation d. For instance, circularly shifting a vec-
tor a = (1, 2, 3, 4, 5) to the right by an offset of 2
results in a2 = (4, 5, 1, 2, 3).
We can incorporate the dependency relations
when calculating the average vectors representing
arguments as follows:
A(x, r) =
?
(r,d,x,y)
y?d
count
, (3)
where (r, d, x, y) iterates over all predicate-
argument pairs (x, y) where y has the dependency
relation d and role r. The role of an argument in
the test phase is established as before, by taking
the role which maximizes the similarity to the av-
erage vector:
argmax
r
sim(A(x, r), yd) (4)
In the cases, where arguments are not direct de-
pendents of the predicate, we use zero as the shift
offset.
To motivate this approach and illustrate its im-
plications, consider the two sentences (1) The cat
chases the dog. (2) The dog chases the cat. In
the first sentence the dog is an object which cor-
responds to the theme role A1, whereas in the
second sentence it is a subject with the agent
role A0. The role labeling decision is, how-
ever, in both cases based on the similarity value
sim(A(chases, r), dog), predicting A1, which is
incorrect in the latter case. When we incorporate
the syntactic information by shifting the vector ac-
cording to its syntactic relation to the predicate,
we obtain two diverging similarity values because
dog  nsubj and dog  dobj are essentially two
different vectors. This leads to the correct predic-
tion in both cases.
Relative to the base method, incorporating
the syntax improves the semantic F-score from
61.32% to 66.23% for Finnish and from 65.05%
to 66.55% for English. For Finnish, the gain is
rather substantial, while for English we see only
a moderate but nevertheless positive effect. This
demonstrates that, indeed, the circular shifting op-
eration successfully encodes syntactic information
both into the average vectors A and the candidate
argument vectors y.
3.4 Core arguments vs. modifiers
In comparison to modifier roles, the assignment
of core (numbered) argument roles is consider-
ably more influenced by the predicate sense and
therefore must be learned separately, which we
also confirmed in initial experiments. The modi-
fier roles, on the other hand, are global in the sense
that they are not tied to any particular predicate.
This brings out an interesting question of whether
the modifier roles should be learned independently
of the predicate or not. We find that the best strat-
egy is to learn predicate-specific modifier vectors
in English and global modifier vectors in Finnish.
Another problem, particularly common in the
Finnish PropBank stems from the distinction be-
tween core roles and modifier roles. For instance,
for the predicate to move the argument meaning
the destination of the moving action has the core
role A2, while for a number of other predicates
which may optionally take a destination argument,
the directional modifier role AM-DIR would be
used. This leads to a situation where core argu-
ments receive a high score for a modifier role, and
modifier roles are over-predicted at the expense
of core argument roles. To account for this, we
introduce the following simple heuristics. If the
predicate lacks a core role r after prediction, iter-
ate through predicted modifier roles p
1
. . .p
n
and
change the prediction from p
i
to r if r has the max-
imum similarity among the core roles and the dif-
ference sim(p
i
, y) ? sim(r, y) is smaller than a
threshold value optimized on a held-out develop-
4
ment set distinct from the test set.
We observe a 2.05pp gain in Finnish when using
this method, whereas in English this feature is less
significant with an improvement of only 0.3pp.
3.5 Fall-back for unknown words
The above-mentioned techniques based purely on
vector representations with no additional features
fail if the vector space model lacks the argument
token which prevents the calculation of the nec-
essary similarities. To address this problem, we
build separately for each POS a ?generic? repre-
sentation by averaging the vectors of all training
data tokens that have the POS and occurred only
once. These vectors, representing a typical rare
word of a given POS, are then used in place of
words missing from the vector space model.
Another solution taking advantage from the
vector space representation is used in cases where
a predicate is not seen in the training data and
therefore we have no information about its argu-
ment structure. We query for predicates closest
to the unseen predicate and take the average argu-
ment vectors from the most similar predicate that
was seen during the training.
Together, these two techniques result in a mod-
est gain of approximately 1pp for both languages.
3.6 Sense classification
One final step required in SRL is the disambigua-
tion of the sense of the predicate. Here we ap-
ply an approach very similar to that used for role
classification, whereby for every sense of every
predicate, we calculate an average vector repre-
senting that sense. This is done as follows: For
every predicate sense, we average the vector rep-
resentations of all dependents and governors
7
of
all occurrences of that sense in the training data,
circularly shifted to encode their syntactic relation
to the predicate. To assign a sense to a predicate
during testing, we average the shifted vectors cor-
responding to its dependents and governors in the
sentence, and choose the sense whose average vec-
tor is the nearest. Using this approach, we obtain a
84.18% accuracy for Finnish and 92.68% for En-
glish, compared to 79.89% and 92.88% without
the syntax information. This corresponds to a sub-
stantial gain for Finnish but, surprisingly, a small
drop for English. For the rare predicates that are
7
Recall we use the extended SD scheme where a word can
have several governors in various situations.
not seen in the training data, we have no informa-
tion about their sense inventory and therefore we
simply predict the sense ?.01? which is the cor-
rect choice in 79.56% of the cases in Finnish and
86.64% in English.
4 Role Labeling with Linear
Transformations
As we discussed earlier, it was recently shown that
the word2vec spaces preserve a number of lin-
guistic regularities, and an accurate mapping be-
tween two word2vec-induced vector spaces can
be achieved using a simple linear transformation.
Mikolov et al. (2013a) have demonstrated that a
linear transformation trained on source-target lan-
guage word pairs obtained from Google Translate
can surprisingly accurately map word vectors from
one language to another, with obvious applications
to machine translation. It is also worth noting that
this is not universally true of all vector space rep-
resentation methods, as Mikolov et al. have shown
for example for Latent Semantic Analysis, which
exhibits this property to a considerably lesser ex-
tent. In addition to testing the applicability of the
word2vec method in general, we are specifically
interested whether these additional properties can
be exploited in the context of SRL. In particular,
we will test whether a similar linear vector space
transformation can be used to map the vectors of
the predicates onto those of their arguments.
More formally, for each role r, we will learn a
transformation matrix W
r
such that for a vector
representation x of some predicate, W
r
x will be
close to the vector representation of its arguments
with the role r. For instance, if x is the represen-
tation of the predicate (to) eat, we aim for W
A1
x
to be a vector similar to the vectors representing
edible items (role A1). The transformation can be
trained using the tuples (r, x, y) of predicate x and
its argument y with the role r gathered from the
training data, minimizing the error
?
(x,y)
?W
r
x? y?
2
(5)
over all training pairs (separately for each r). We
minimize the error using the standard stochastic
gradient descent method, whereby the transfor-
mation matrix is updated separately for each pair
(x, y) using the equation
W
r
?W
r
? (W
r
x? y)x
T
(6)
5
where  is the learning rate whose suitable value is
selected on the development set. The whole proce-
dure is repeated until convergence is reached, ran-
domly shuffling the training data after each round
of training.
Using the transformation, we can establish the
most likely role for the argument y of a predicate
x as
argmax
r
sim(W
r
x, y) (7)
where sim is the cosine similarity function, i.e. in
the exact same manner as for the average argument
method described in the previous section, with the
difference that the vector for the average argument
is not calculated directly from the training data,
but rather obtained through the linear transforma-
tion of the predicate vector.
As an alternative approach, we can also learn
the reverse transformation RW
r
such that RW
r
y
is close to x, i.e. the transformation of the argu-
ment y onto the predicate x. Note that hereRW
r
is
not the same asW
T
r
; we train this reverse transfor-
mation separately using the same gradient descent
method. We then modify the method for finding
the most likely role for an argument by taking the
average of the forward and reverse transformation
similarities:
argmax
r
sim(W
r
x, y) + sim(x,RW
r
y)
2
(8)
Note that we make no assumptions about the
vector spaces where x and y are drawn from; they
may be different spaces and they do not need to be
matched in their dimensionality either, as there is
no requirement that W and RW be square matri-
ces. In practice, we find that the best strategy for
both Finnish and English is to represent both the
predicates and arguments using the space induced
from word forms, however, we have also tested on
Finnish representing the predicates using the space
induced from lemmas and the arguments using a
space induced from word forms, with only mini-
mally worse results. This shows that the transfor-
mation does not degrade substantially even when
mapping between two different spaces.
With this strategy, we reach an F-score of
62.71% in Finnish and 63.01% in English. These
results are on par with the scores obtained with
the average argument method, showing that a lin-
ear transformation is effective also in this kind of
problems.
To incorporate syntax information, we train
transformation matrices W
r,d
and RW
r,d
for each
dependency relation d rather than relying on the
circular shift operation which cannot be captured
by linear transformations.
8
As some combinations
of r and d may occur only in the test data, we use
the matrices W
r
and RW
r
as a fall-back strategy.
In testing, we found that even if the (r, d) com-
bination is known from the training data, a small
improvement can be obtained by taking the aver-
age of the similarities with and without syntactic
information as the final similarity. Incorporating
these techniques into the basic linear transforma-
tion improves the semantic F-score from 62.71%
to 65.88% for Finnish and from 63.01% to 67.04%
for English. The improvement for both languages
is substantial.
5 Supervised classification approach
In the previous sections, we have studied an ap-
proach to SRL based purely on the vector space
representations with no additional features. We
have addressed the choice of the argument role by
simply assigning the role with the maximum sim-
ilarity to the argument. To test the gain that could
be obtained by employing a more advanced tech-
nique for aggregating the scores and incorporating
additional features, we train a linear multi-class
support vector machine to assign the role to every
detected argument. As features, we use the simi-
larity values for each possible role using the best
performing method for each language,
9
the sense
of the predicate, and ? separately for the predi-
cate and the argument ? the token itself, its POS,
morphological tags, every dependency relation to
its governors, and every dependency relation to
its dependents. The similarities are encoded as
feature weights, while all other features are bi-
nary. We use the multi-class SVM implementa-
tion from the SVM-multiclass package (Joachims,
1999), setting the regularization parameter on the
development set using a grid search.
For both languages, we observe a notable gain
in performance, leading to the best scores so far.
In Finnish the improvement in F-score is from
66.23% to 73.83% and in English from 67.04%
to 70.38%. However, as we will further discuss in
Section 6, in Finnish the contribution of the simi-
larity features is modest.
8
Note that this does not affect the overall computational
cost, as the total number of training examples remains un-
changed and the transformation matrices are small in size.
9
Average vectors for Finnish and transformation for En-
glish (Table 2).
6
Finnish English
Average vectors
full method 66.23 66.55
?modifier vs. core role 64.18 66.25
?syntax 61.32 65.05
Linear transformation
full method 65.88 67.04
?syntax 62.71 63.01
Supervised classification
full method 73.83 70.38
only similarity features 64.21 65.89
?similarity features 73.54 67.51
?lexical features 65.51 58.42
Table 2: Overview of main results and a feature
ablation study. Modifier vs. core role refers to the
algorithm presented in Section 3.4. In the super-
vised classification part, -lexical features refers to
the removal of features based on word forms, pred-
icate sense and role similarities.
6 Results and discussion
All results discussed throughout Sections 3 to 5
are summarized in Table 2, which also serves as
a coarse feature ablation study. Overall, we see
that the average vector and linear transformation
methods perform roughly on par, with the aver-
age vector method being slightly better for Finnish
and slightly worse for English. Both vector space-
based methods gain notably from syntax informa-
tion, confirming that the manner in which this in-
formation is incorporated is indeed meaningful.
Adding the SVM classifier on top of these two
methods results in a substantial further raise in per-
formance, demonstrating that to be competitive on
SRL, it is necessary to explicitly model also ad-
ditional information besides the semantic similar-
ity between the predicate and the argument. This
is particularly pronounced for Finnish where the
present SVM method does not gain substantially
from the similarity-based features, while English
clearly benefits. To shed some light on this differ-
ence, we show in Table 3 the oracle accuracy of
role labeling for top-1 through top-10 roles as or-
dered by their similarity scores. The performance
on English is clearly superior to that on Finnish.
An important factor may be the fact that ? in
terms of token count ? the CoNLL?09 English
training size is nearly six times that of the Finnish
PropBank and the English vector space model was
induced on a nearly four times larger text corpus.
Finnish English
n Recall n Recall
1 58.23 1 67.39
2 68.29 2 82.82
3 74.30 3 88.49
4 78.71 4 91.58
5 82.21 5 93.20
6 84.74 6 94.29
7 87.04 7 94.91
8 88.98 8 95.31
9 90.76 9 95.65
10 92.05 10 95.86
Table 3: A study of how many times (%) the cor-
rect role is among the top n most similar roles
when the arguments are known in advance. Left:
Similarities taken from the average vector method
on Finnish. Right: Similarities from the linear
transformation method on English.
Finnish English
Average vectors 66.23 / 89.89 66.55 / 79.57
Linear transf. 65.88 / 89.92 67.04 / 80.85
Supervised 73.83 / 89.29 70.38 / 78.71
Table 4: Overall results separately for all main
methods (labeled/unlabeled semantic F-score).
Returning to our original question of whether
the SD scheme can be used as-is for argument
identification, we show in Table 4 the unlabeled
F-scores for the main methods. These scores re-
flect the performance of the argument identifica-
tion step in isolation. While Finnish approaches
90% which is comparable to the best systems in
the CoNLL?09 task, English lags behind by over
10pp. To test to what extent the results would be
affected if a more accurate argument identification
system was applied, we used the output of the win-
ning (for English) CoNLL?09 Shared Task system
(Zhao et al., 2009a) as the argument identifica-
tion component, while predicting the roles with
the methods introduced so far. The results are
summarized in Table 5, where we see a substan-
tial gain for all the methods presented in this pa-
per, achieving an F-score of 82.33%, only 3.82pp
lower than best CoNLL?09 system. These results
give a mixed signal as to whether the extended SD
scheme is usable nearly as-is for argument identi-
fication (Finnish) or not (English). Despite our ef-
forts, we were unable to pinpoint the cause for this
difference, beyond the fact that the Finnish Prop-
7
Semantic F-score
CoNLL?09 best 86.15 / 91.97
Average vectors 73.12 / 91.97
Linear transformation 74.41 / 91.97
Supervised classif. 82.33 / 91.97
Table 5: Performance of suggested methods with
argument identification from the top-performing
CoNLL?09 system (labeled/unlabeled F-score).
Bank was originally developed specifically on top
of the SD scheme, while the English PropBank
and NomBank corpora were not.
7 Related work
While different methods have been studied to
build task specific vector space representations,
post-hoc methods to manipulate the vector spaces
without retraining are rare. Current SRL systems
utilize supervised machine learning approaches,
and typically a large set of features. For instance,
the winning system in the CoNLL?09 shared task
(SRL-only) introduces a heavy feature engineer-
ing system, which has about 1000 potential fea-
ture templates from which the system discovers
the best set to be used (Zhao et al., 2009b). Word
similarities are usually introduced to SRL as a
part of unsupervised or semi-supervised meth-
ods. For example, Titov and Klementiev (2012)
present an unsupervised clustering method ap-
plying word representation techniques, and De-
schacht and Moens (2009) used vector similarities
to automatically expand the small training set to
build semi-supervised SRL system. Additionally,
Turian et al. (2010) have shown that word repre-
sentations can be included among the features to
improve the performance of named entity recogni-
tion and chunking systems.
8 Conclusions
We set out to test two post-hoc vector space ma-
nipulation techniques in the context of semantic
role labeling. We found that the circular shift op-
eration can indeed be applied also to other vector
representations as a way to encode syntactic infor-
mation. Importantly, the circular shift is applied to
a pre-existing vector space representation, rather
than during its induction, and is therefore task-
independent. Further, we find that such shifted
vectors can be meaningfully averaged to represent
predicate senses and arguments.
We also extended the study of the linear trans-
formation between two vector spaces and show
that the same technique can be used also within
a single space, mapping the vectors of predicates
onto the vectors of their arguments. This map-
ping produces results that are performance-wise
on par with the average vectors method, demon-
strating a good generalization ability of the lin-
ear mapping and the underlying word2vec vector
space representation. Here it is worth noting that
? if we gloss over some obvious issues of am-
biguity ? the mapping between two languages
demonstrated by Mikolov et al. is conceptually a
one-to-one mapping, at least in contrast to the one-
to-many nature of the mapping between predicates
and their arguments. These results hint at the pos-
sibility that a number of problems which can be re-
duced to the ?predict a word given a word? pattern
may be addressable with this simple technique.
With respect to the application to SRL, we have
shown that it is possible to carry out SRL based
purely on the vector space manipulation meth-
ods introduced in this paper, outperforming sev-
eral entries in the CoNLL-09 Shared Task. How-
ever, it is perhaps not too surprising that much
more is needed to build a competitive SRL sys-
tem. Adding an SVM classifier with few relatively
simple features derived from the syntactic analy-
ses in addition to features based on vector similar-
ities, and especially adding a well-performing ar-
gument identification method, can result in a sys-
tem close to approaching state-of-the-art perfor-
mance, which is encouraging.
As future work, it will be interesting to study to
which extent SRL, and similar applications would
benefit from addressing the one-to-many nature of
the underlying problem. While for some predi-
cates the arguments likely form a cluster that can
be represented as a single average vector, for other
predicates, such as to see, it is not the case. Find-
ing methods which allow us to model this property
of the problem will constitute an interesting direc-
tion with broader applications beyond SRL.
Acknowledgements
This work has been supported by the Emil Aal-
tonen Foundation and Kone Foundation. Com-
putational resources were provided by CSC ? IT
Center for Science. We would also like to thank
Sampo Pyysalo and Hans Moen for comments and
general discussion.
8
References
Pierpaolo Basile and Annalina Caputo. 2012. Encod-
ing syntactic dependencies using Random Indexing
and Wikipedia as a corpus. In Proceedings of the
3rd Italian Information Retrieval (IIR) Workshop,
volume 835, pages 144?154.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97. Association for
Computational Linguistics.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1-Volume 1, pages
21?29. Association for Computational Linguistics.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Associa-
tion for Computational Linguistics.
P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz.
2007. HunPos: an open source trigram tagger. In
Proceedings of the 45th annual meeting of the ACL
on interactive poster and demonstration sessions,
pages 209?212. Association for Computational Lin-
guistics.
Katri Haverinen, Veronika Laippala, Samuel Kohonen,
Anna Missil?a, Jenna Nyblom, Stina Ojala, Timo Vil-
janen, Tapio Salakoski, and Filip Ginter. 2013a.
Towards a dependency-based PropBank of general
Finnish. In Proceedings of the 19th Nordic Confer-
ence on Computational Linguistics (NoDaLiDa?13),
pages 41?57.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Mis-
sil?a, Stina Ojala, Tapio Salakoski, and Filip Ginter.
2013b. Building the essential resources for Finnish:
the Turku Dependency Treebank. Language Re-
sources and Evaluation, pages 1?39.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Meth-
ods - Support Vector Learning, pages 169?184. MIT
Press.
Krister Lind?en, Miikka Silfverberg, and Tommi Piri-
nen. 2009. HFST tools for morphology ? an effi-
cient open-source package for construction of mor-
phological analyzers. In State of the Art in Com-
putational Morphology, volume 41 of Communica-
tions in Computer and Information Science, pages
28?47.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8. Coling 2008 Organiz-
ing Committee.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, pages 24?31.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a. Exploiting similarities among languages
for machine translation. CoRR (arxiv.org),
abs/1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751. Associa-
tion for Computational Linguistics, June.
Jenna Nyblom, Samuel Kohonen, Katri Haverinen,
Tapio Salakoski, and Filip Ginter. 2013. Pre-
dicting conjunct propagation and other extended
Stanford Dependencies. In Proceedings of the In-
ternational Conference on Dependency Linguistics
(Depling 2013), pages 252?261.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Tommi Pirinen. 2008. Suomen kielen ?a?arellistilainen
automaattinen morfologinen j?asennin avoimen
l?ahdekoodin resurssein. Master?s thesis, University
of Helsinki.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. BRAT: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107. Association for Computational Lin-
guistics.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12?22. Association for
Computational Linguistics.
9
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multilin-
gual dependency learning: Exploiting rich features
for tagging syntactic and semantic dependencies.
In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 61?66. Association for Computational
Linguistics.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009b. Multilingual dependency learning:
a huge feature engineering method to semantic de-
pendency parsing. In Proceedings of the Thirteenth
Conference on Computational Natural Language
Learning: Shared Task, pages 55?60. Association
for Computational Linguistics.
10
