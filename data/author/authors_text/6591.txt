Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Refining the most frequent sense baseline
Judita Preiss
Department of Linguistics
The Ohio State University
judita@ling.ohio-state.edu
Jon Dehdari
Department of Linguistics
The Ohio State University
jonsafari@ling.ohio-state.edu
Josh King
Computer Science and Engineering
The Ohio State University
kingjo@cse.ohio-state.edu
Dennis Mehay
Department of Linguistics
The Ohio State University
mehay@ling.ohio-state.edu
Abstract
We refine the most frequent sense baseline
for word sense disambiguation using a num-
ber of novel word sense disambiguation tech-
niques. Evaluating on the S???????-3 English
all words task, our combined system focuses
on improving every stage of word sense dis-
ambiguation: starting with the lemmatization
and part of speech tags used, through the ac-
curacy of the most frequent sense baseline, to
highly targeted individual systems. Our super-
vised systems include a ranking algorithm and
a Wikipedia similarity measure.
1 Introduction
The difficulty of outperforming the most frequent
sense baseline, the assignment of the sense which
appears most often in a given annotated corpus, in
word sense disambiguation (WSD) has been brought
to light by the recent S??????? WSD system evalu-
ation exercises. In this work, we present a combi-
nation system, which, rather than designing a single
approach to all words, enriches the most frequent
sense baseline when there is high confidence for an
alternative sense to be chosen.
WSD, the task of assigning a sense to a given
word from a sense inventory is clearly necessary
for other natural language processing tasks. For ex-
ample, when performing machine translation, it is
necessary to distinguish between word senses in the
original language if the different senses have differ-
ent possible translations in the target language (Yn-
gve, 1955). A number of different approaches to
WSD have been explored in recent years, with two
distinct approaches: techniques which require anno-
tated training data (supervised techniques) and tech-
niques which do not (unsupervised methods).
It has long been believed that supervised systems,
which can be tuned to a word?s context, greatly out-
perform unsupervised systems. This theory was sup-
ported in the S??????? WSD system evaluation exer-
cises, where the performance gap between the best
supervised system and the best unsupervised sys-
tem is large. Unsupervised systems were found to
never outperform the most frequent sense (MFS)
baseline (a sense assignment made on the basis of
the most frequent sense in an annotated corpus),
while supervised systems occasionally perform bet-
ter than the MFS baseline, though rarely by more
than 5%. However, recent work by McCarthy et al
(2007) shows that acquiring a predominant sense
from an unannotated corpus can outperform many
supervised systems, and under certain conditions
will also outperform the MFS baseline.
Rather than proposing a new algorithm which will
tackle all words, we focus on improving upon the
MFS baseline system when an alternative system
proposes a high confidence answer. An MFS refin-
ing system can therefore benefit from answers sug-
gested by a very low recall (but high precision) WSD
system. We propose a number of novel approaches
to WSD, but also demonstrate the importance of a
highly accurate lemmatizer and part of speech tag-
ger to the English all words task of S???????-3.1
We present our enriched most frequent sense
1Unless specified otherwise, we use WordNet 1.7.1 (Miller
et al, 1990) and the associated sense annotated SemCor cor-
pus (Miller et al, 1993) (translated to WordNet 1.7.1 by Rada
Mihalcea).
10
baseline in Section 2, which motivates the lemma-
tizer and part of speech tagger refinements presented
in Section 3. Our novel high precision WSD al-
gorithms include a reranking algorithm (Section 4),
and a Wikipedia-based similarity measure (Sec-
tion 5). The individual systems are combined in
Section 6, and we close with our conclusions in Sec-
tion 7.
2 Most frequent sense baseline
The most frequent sense (MFS) baseline assumes
a sense annotated corpus from which the frequen-
cies of individual senses are learnt. For each tar-
get word, a part of speech tagger is used to deter-
mine the word?s part of speech, and the MFS for
that part of speech is selected. Although this is a
fairly naive baseline, it has been shown to be diffi-
cult to beat, with only 5 systems of the 26 submitted
to the S???????-3 English all words task outperform-
ing the reported 62.5% MFS baseline. The success
of the MFS baseline is mainly due to the frequency
distribution of senses, with the shape of the sense
rank versus frequency graph being a Zipfian curve
(i.e., the top-ranked sense being much more likely
than any other sense).
However, two different MFS baseline perfor-
mance results are reported in Snyder and Palmer
(2004), with further implementations being differ-
ent still. The differences in performance of the MFS
baseline can be attributed to a number of factors:
the English all words task is run on natural text and
therefore performance greatly depends on the accu-
racy of the lemmatizer and the part of speech tag-
ger employed.2 If the lemmatizer incorrectly iden-
tifies the stem of the word, the MFS will be looked
up for the wrong word and the resulting sense as-
signment will be incorrect. The performance of the
MFS given the correct lemma and part of speech
information is 66%, while the performance of the
MFS with a Port Stemmer without any POS infor-
mation is 32%. With a TreeTagger (Schmidt, 1994),
and a sophisticated lemma back-off strategy, the per-
formance increases to 56%. It is this difference in
2Other possible factors include: 1) The sense distribution in
the corpus which the MFS baseline is drawn from, 2) If SemCor
is used as the underlying sense annotated corpus, the accuracy
of the mapping from WordNet 1.6 (with which SemCor was
initially annotated) to WordNet 1.7.1 could also have an effect
on the performance).
performance which motivates refining the most fre-
quent sense baseline, and our work on improving
the underlying lemmatizer and part of speech tagger
presented in Section 3.
Our initial investigation refines the SemCor based
MFS baseline using the automatic method of de-
termining the predominant sense presented in Mc-
Carthy et al (2007).
1. For nouns and adjectives which appear in Sem-
Cor fewer than 5 times, we employ the auto-
matically determined predominant sense.
2. For verbs which appear in SemCor fewer than 5
times, we employ subcategorization frame sim-
ilarity rather than Lesk similarity to give us a
verb?s predominant sense.
2.1 Predominant sense
McCarthy et al (2007) demonstrate that it is possi-
ble to acquire the predominant sense for a word in
a corpus without having access to annotated data.
They employ an automatically created thesaurus
(Lin, 1998), and a sense?word similarity metric to
assign to each sense si of a word w a score corre-
sponding to
?
n j?Nw
dss(w, n j) ? sss(si, n j)?
s?i?senses(w) sss(s?i , n j)
where dss(w, n j) reflects the distributional simi-
larity of word w to n j, w?s thesaural neighbour, and
sss(si, n j) = maxsx?senses(n j) sss?(si, sx) is the max-
imum similarity3 between w?s sense si and a sense
sx of w?s thesaural neighbour n j. The authors show
that although this method does not always outper-
form the MFS baseline based on SemCor, it does
outperform it when the word?s SemCor frequency is
below 5. We therefore switch our MFS baseline to
this value for such words. This result is represented
as ?McCarthy? in Table 1, which contains the results
of the techniques presented in this Section evaluated
on the S???????-3 English all words task.
2.2 Verb predominant sense
McCarthy et al (2007) observe that their predom-
inant sense method is not performing as well for
3We use the Lesk (overlap) similarity as implemented by the
WordNet::similarity package (Pedersen et al, 2004).
11
System Precision Recall F-measure
MFS 58.4% 58.4% 58.4%
McCarthy 58.5% 58.5% 58.5%
Verbs 58.5% 58.5% 58.5%
All 58.6% 58.6% 58.6%
Table 1: Refining the MFS baseline with predominant
sense
verbs as it does for nouns and adjectives. We hy-
pothesize that this is due to the thesaural neighbours
obtained from Lin?s thesaurus, and we group verbs
according to the subcategorization frame (SCF) dis-
tributions they present in the ????? (Korhonen et al,
2006) lexicon. A word w1 is grouped with word w2
if the Bhattacharyya coefficient
BC(w1, w2) =
?
x?X
?p(x)q(x)
where p(x) and q(x) represent the probability val-
ues for subcategorization class x, is above a cer-
tain threshold. The BC coefficient then replaces the
dss value in the original formula and the predomi-
nant senses are obtained. Again, this system is only
used for words with frequency lower than 5 in Sem-
Cor. The great advantage of the Bhattacharyya co-
efficient over various entropy based similarity mea-
sures which are usually used to compare SCF distri-
butions (Korhonen and Krymolowski, 2002), is that
it is guaranteed to lie between 0 and 1, unlike the
entropy based measures which are not easily com-
parable between different word pairs. This result is
represented by ?Verbs? in Table 1.
Table 1 displays the results for the MFS, the MFS
combined with the two approaches described above,
and the MFS combining MFS with verbs and Mc-
Carthy.
3 Lemmatization and Part of Speech
Tagging
We made use of several lemmatizers and part-of-
speech taggers, in order to give the other WSD com-
ponents the best starting point possible.
3.1 Lemmatization
Lemmatization, the process of obtaining the canon-
ical form of a word, was the first step for us to
ultimately identify the correct WordNet sense of
a given word in the English all words task. We
found that without any lemmatizing of the test input,
the maximum f -score possible was in the mid-50?s.
Conversely, we found that a basic most-frequent-
sense system that had a perfectly-lemmatized input
achieved an f -score in the mid-60?s. This large dif-
ference in the ceiling of a non-lemmatized system
and the floor of a perfectly-lemmatized system mo-
tivated us to focus on this task.
We looked at three different lemmatizers: the lem-
matizing backend of the XTAG project (XTAG Re-
search Group, 2001)4, Celex (Baayen et al, 1995),
and the lemmatizing component of an enhanced
TBL tagger (Brill, 1992).5 We then employed a vot-
ing system on these three components, taking the
lemma from the most individual lemmatizers. If all
three differ, we take the lemma from the most accu-
rate individual system, namely the TBL tagger.
3.1.1 Lemmatizer Evaluation
We evaluated the lemmatizers against the lem-
mas found in the S???????-3 gold standard.6 Even
the lowest performing system improved accuracy
by 31.74% over the baseline, which baseline sim-
ply equates the given token with the lemma. Ta-
ble 2 shows the results of evaluating the lemmatizers
against the EAW key.
While the simple voting system performed bet-
ter than any of the individual lemmatizers, hyphen-
ated words proved problematic for all of the sys-
tems. Some hyphenated words in the test set re-
mained hyphenated in the gold standard, and some
others were separated. However, evaluation results
show that splitting hyphenated words increases lem-
matizing accuracy by 0.9% .
3.2 Part of Speech Tagging
We also investigated the contribution of part of
speech taggers to the task of word sense disam-
biguation. We considered three taggers: the El-
worthy bigram tagger (Elworthy, 1994) within the
RASP parser (Briscoe et al, 2006), an enhanced
4http://www.cis.upenn.edu/?xtag
5http://gposttl.sourceforge.net
6We removed those lines from both the test input and the
gold standard which were marked U (= unknown, 34 lines), and
we removed the 40 lines from the test input that were missing
from the gold standard. This gave us 2007 words in both the
test set and the gold standard.
12
Lemmatizer Accuracy
Baseline 57.50%
XTAG 89.24%
Celex 91.58%
TBL 92.38%
Voting {XTAG,Celex,TBL} 93.77%
Voting, no hyphen {XTAG,Celex,TBL} 94.67%
Table 2: Accuracy of several lemmatizers on <head>
words of EAW task.
TBL tagger (Brill, 1992)7, and a TnT-style trigram
tagger (Hala?csy et al, 2007).8 The baseline was a
unigram tagger which selects the most frequently-
occurring tag of singletons when dealing with un-
seen words.
All three of the main taggers performed compa-
rably, although only the Elworthy tagger provides
probabilities associated with tags, rather than get-
ting a single tag as output. This additional infor-
mation can be useful, since we can employ differ-
ent strategies for a word with one single tag with a
probability of 1, versus a word with multiple tags,
the most probable of which might only have a prob-
ability of 0.3 for example. For comparative pur-
poses, we mapped the various instantiations of tags
for nouns, verbs, adjectives, and adverbs to these
four basic tags, and evaluated the taggers? results
against the EAW key. Table 3 shows the results of
this evaluation.
The performance of these taggers on the EAW
<head>-words is lower than results reported on
other datasets. This can explained by the lack of
frequently-occurring function words, which are easy
to tag and raise overall accuracy. Also, the words
in the test set are often highly ambiguous not only
with respect to their word sense, but also their part
of speech.
4 Supervised Learning of Sparse Category
Indices for WSD
In this component of our refinement of the base-
line, we train a supervised system that performs
higher-precision classification, only returning an an-
swer when a predictive feature that strongly pre-
dicts a particular sense is observed. To achieve this,
7http://gposttl.sourceforge.net
8http://code.google.com/p/hunpos
POS Tagger Accuracy
Baseline 84.10%
TBL 90.48%
Elworthy 90.58%
TnT 91.13%
Voting {TBL,Elw.,TnT} 91.88%
Table 3: Accuracy of several POS taggers on <head>
words of EAW task.
we implemented a ?feature focus? classifier (sparse
weighted index) as described in (Madani and Con-
nor, 2008, henceforth, MC08). MC08?s methods
for restricting and pruning the number of feature-to-
class associations are useful for finding and retain-
ing only strong predictive features. Moreover, this
allowed us to use a rich feature set (more than 1.6
million features) without an unwieldy explosion in
the number of parameters, as feature-class associa-
tions that are not strong enough are simply dropped.
4.1 Sparse Category Indices
MC08 describe a space and time efficient method
for learning discriminative classifiers that rank large
numbers of output classes using potentially millions
of features for many instances in potentially tera-
scale data sets. The authors describe a method for
learning ?category indices? ? i.e., weighted bipar-
tite graphs G ? F ?W ?C, where F is the set of fea-
tures, C is the set of output classes and all weights
(or ?associations?) w ? W between features and the
output classes they predict are real-valued and in
[0.0, 1.0]. The space and time efficiency of MC08?s
approach stems chiefly from three (parameterisable)
restrictions on category indices and how they are up-
dated. First, at any time in the learning process, only
those edges ( fi, w j, ck) ? G whose associations w j
are a large enough proportion of the sum of all class
associations for fi are retained: that is, only retain
w j s.t. w j ? wmin.9 Second, by setting an upper
bound dmax on the number of associations that a
feature fi is allowed to have, only the largest fea-
ture associations are retained. Setting dmax to a low
number (? 25) makes each feature a high-precision,
low-recall predictor of output classes. Further, the
dmax and wmin restrictions on parameter reten-
9Recall that w j ? W are all between 0.0 and 1.0 and sum to
1.0.
13
tion allow efficient retrieval and update of feature
weights, as only a small number of feature weights
need be consulted for predicting output classes or
learning from prediction mistakes in an online learn-
ing setting.10 Finally, in the online learning algo-
rithm,11 in addition to the small number of features
that need be consulted or updated, an error margin
marg can be set so that parameter update only oc-
curs when the score(c)? score(c?) ? marg, where c
is the correct output class and c? , c is the most con-
fident incorrect prediction of the classifier. Setting
marg = 0.0 leads to purely error-driven learning,
while marg = 1.0 always updates on every learning
instance. Values of marg ? (0.0, 1.0) will bias the
category index learner to update at different levels
of separation of the correct class from the most con-
fident incorrect class, ranging from almost always
error driven (near 0.0) to almost error-insensitive
learning (near 1.0).
4.2 Integration into the WSD Task
Using both the Semcor-3 and English Lexical Sam-
ple training data sets (a total of ?45,000 sentences,
each with one or more labeled instances), we trained
a sparse category index classifier as in MC08 with
the following features: using words, lemmas and
parts of speech (POSs) as tokens, we define fea-
tures for (1) preceding and following unigrams and
bigrams over tokens, as well as (2) the conjunc-
tion of the preceding unigrams (i.e., a 3-word win-
dow minus the current token) and (3) the conjunc-
tion of the preceding and following bigrams (5-
word window minus the current token). Finally
all surrounding lemmas in the sentence are treated
as left- or right-oriented slot-independent features
with an exponentially decaying level of activation
act(li) = 0.5 ? exp
(
0.5 ? ? dist(li , targ wd)
)
? where dist(li, targ wd) is simply the word dis-
tance from the target word to the contextual lemma
li.12 Although WSD is not a many-class, large-
10dmax bounds the number of feature-class associations (pa-
rameters) must be consulted in prediction and updating, but,
because of the wmin restriction, MC08 found that, on aver-
age, many fewer feature associations ? ? 16 ? were ever
touched per training or testing instance in their classification
experiments. See Madani and Connor (2008) for more details.
11Again, see Madani and Connor (2008) for more details.
12The value 0.5 is also a parameter that we have fixed, but it
could in principle be tuned to a particular data set. In the interest
of simplicity, we have not done this.
scale classification task,13 we nevertheless found
MC08?s pruning mechanisms useful for removing
weak feature-word associations. Due to the ag-
gressive pruning of feature-class associations, our
model only has ?1.9M parameters out of a potential
1, 600, 000 ? 200, 000 = 320 billion (the number of
features times the number of WordNet 3.0 senses).
4.3 Individual System Results
To integrate the predictions of the classifier into the
EAW task, we looked up all senses for each lemma-
POS pairing, backing off to looking up the words
themselves by the same POS, and finally resorting
to splitting hyphenated words and rejoining multi-
word units (as marked up in the EAW test set). Be-
ing high precision, the classifier does not return a
valid answer for every lemma, so we report results
with and without backing off to the most frequent
sense baseline to fill in these gaps.
Individual system scores are listed in Table 4. The
classifier on its own returns very few answers (with a
coverage ? as distinct from recall ? of only 10.4%
of the test set items). Although the classifier-only
performance does not have broad enough coverage
for stand-alone use, its predictions are nonetheless
useful in combination with the baseline. Further, we
expect coverage to grow when trained over a larger
corpus (such as the very large web-extracted corpus
of Agirre et al (2004), which this learning method
is well suited for).
5 Wikipedia for Word Sense
Disambiguation
Wikipedia, an online, user-created encyclopedia,
can be considered a collection of articles which link
to each other. While much information exists within
the textual content of Wikipedia that may assist in
WSD, the approach presented here instead uses the
article names and link structure within Wikipedia to
find articles which are most related to a WordNet
sense or context. We use the Green method to find a
relatedness metric for articles from Wikipedia14 (Ol-
13Large-scale data sets are available, but this does not change
the level of polysemy in WordNet, which is not in the thousands
for any given lemma.
14Computations were performed using a January 3rd 2008
download of the English Wikipedia.
14
Back-off Precision Recall Prec. (n-best) Rec. (n-best)
Y?? 0.592 0.589 0.594 0.589
N? 0.622 0.065 0.694 0.070
Table 4: Precision and recall of sparse category index classifier ? both ?soft? scores of standard Senseval script and
scores where any correct answer in list returned by the classifier is counted as a correct answer (?n-best?). ?Back-off?
signals whether the system backs off to the most frequent sense baseline.
livier and Senellart, 2007) based on each sense or
context of interest.
Advantages of this method over alternative meth-
ods that attempt to incorporate Wikipedia into WSD
is that our system is unsupervised and that no man-
ual mapping needs to take place between WordNet
and Wikipedia. Mihalcea (2007) demonstrates that
manual mappings can be created for a small num-
ber of words with relative ease, but for a very large
number of words the effort involved in mapping
would approach presented involves no be consider-
able. The approach presented here involves no map-
ping between WordNet and Wikipedia but human ef-
fort in mapping between WordNet and Wikipedia,
but instead initializes the Green method with a vec-
tor based only on the article names (as described in
Section 5.2).
5.1 Green Method
The Green method (Ollivier and Senellart, 2007) is
used to determine the importance of one node in a
directed graph with respect to other nodes.15 In the
context of Wikipedia the method finds the articles
which are most likely to be frequented if a random
walk were used to traverse the articles, starting with
a specific article and returning to that article if the
random walk either strays too far off topic or to an
article which is generally popular even without the
context of the initial article. One of the features of
the Green method is that it does not simply repro-
duce the global PageRank (Brin and Page, 1998),
instead determining the related pages nearby due to
relevance to the initial node.
The probability that the random walker of
Wikipedia will transfer to an article is defined as a
uniform distribution over the outlinks of the page
where the random walker is currently located. As
an approximation to the method described by Ol-
15In subsequent sections we give a high-level description of
using the Green method with Wikipedia, however see Ollivier
and Senellart (2007) for a much more detailed explanation.
livier and Senellart (2007), we create a subgraph of
Wikipedia for every computation, comprised of the
articles within a distance of 2 outlink traversals from
the initial articles. Since Wikipedia is very highly
connected, this constructed subgraph still contains
a large number of articles and performance of the
Green method on this subgraph is similar to that on
the whole connectivity graph.
5.2 Green Method for Contexts
To use the Green method to find Wikipedia arti-
cles which correspond to a given word to be dis-
ambiguated, articles which may discuss that word
and the context surrounding that word are found in
Wikipedia as an initial set of locations for the ran-
dom walker to start. This is done by looking for the
word itself as the name of an article. If there is not
an article whose name corresponds to the word in
question, then articles with the word as a substring
of the article name are found.
Since the goal of WSD is to choose the best word
sense within the context of other words, we use a
given word?s context to select a set of Wikipedia ar-
ticles which may discuss the content of the word in
question. The expectation is that the context words
will aid in disambiguation and that the context words
will together be associated with an appropriate sense
of the word being disambiguated. For this method
we defined a word?s context as the word itself, the
content words in the sentence the word occurs in,
and those occurring in the sentences before and af-
ter that sentence.
5.3 Green Method for Senses
Every sense of a word to be disambiguated also
needs to be represented as corresponding articles
in Wikipedia before using the Green method. The
words that we search for in the titles of Wikipedia
articles include the word itself, and, for every sense,
the content words of the sense?s WordNet gloss, as
well as the content of the sense?s hypernym gloss
15
and the synonyms of the hypernym. Exploring this
particular aspect of this module ? which informa-
tion about a sense to extract before using the Green
Method ? is a point for further exploration.
5.4 Interpreting Projections
The Green method as described by Ollivier and
Senellart (2007) uses, as the initial set of articles,
the vector containing only one article: that article
for which related articles are being searched. We
use as the initial set of articles the collection of ar-
ticles in Wikipedia corresponding to either the con-
text for the word to be disambiguated or the sense of
a word. The random walker is modeled as starting
in any of the articles in this set with uniform proba-
bility. Within the context of the Green method, this
means that this initial set of articles corresponds to
what would be linked to from a new Wikipedia arti-
cle about the sense or context. Each of the content
words in this new article (which is not in Wikipedia)
would link to one of the articles in the set found by
the methods described above. In this way the results
of the Green method computation can be interpreted
as a relatedness metric for the sense or context itself
and the articles which are in Wikipedia.
5.5 Analysis
The process of finding the sense of a word to be dis-
ambiguated is as follows: the vector output from the
Green method (a relatedness measure between the
initial seed and each article in Wikipedia) for the
context of the word is compared against the vector
output from using the Green method on each sense
that the word could have. The comparison is done
using the cosine of the angle between the two vec-
tors.
To determine for which instances in S??????? this
method may perform well, an analysis was per-
formed on a small development set (15 sentences)
from SemCor. A simple heuristic was formulated,
selecting the sense with the nearest Green method
output to the sentence?s Green method output when
the ratio between the first and second highest ranked
senses? cosine angle scores was above a threshold.
Applying this heuristic to the EAW task yielded
an expectedly low recall of 11% but a precision of
81% on all the words that this heuristic could apply,
but only a precision of 25% (recall 0.5%) for non-
monosemous words (which were the desired targets
MFS Rerank Wiki
MFS ? 94% 97%
Rerank 23% ? 99%
Wiki 45% 98% ?
Table 5: Complementarity between modules
of the method). Of 37 instances where this method
differs from the MFS baseline in the EAW task, 8 in-
stances are correctly disambiguated by this module.
6 Results
Although the individual systems have fairly low re-
call, we can calculate pairwise complementarity be-
tween systems si and s j by evaluating
(
1 ? |wrong in si and s j|
|wrong in si|
)
The results, presented in Table 5, indicate that the
systems complement each other well, and suggest
that a combination system could have a higher per-
formance than the individual systems.
We investigate a number of techniques to combine
the results ? while the integration of the lemma / part
of speech refinement is done by all modules as a pre-
processing step, the method of combination of the
resulting modules is less clear. As shown in Florian
et al (2002), a simple voting mechanism achieves
comparable performance to a stacking mechanism.
We present our results in Table 6, DT gives the re-
sult of a 10-fold cross-validation of WEKA stacked
decision trees and nearest neighbours built from the
individual system results (Witten and Frank, 2000).
Very few decisions are changed with the voting
method of combination, and the overall result does
not outperform the best MFS baseline (presented in
the table as ?All MFS?). This combination method
may be more useful with a greater number of sys-
tems being combined ? our system only combines
three systems (thus only one non-MFS system has to
suggest the MFS for this to be selected), and backs
off to the MFS sense in case all three disagree. The
degree of complementarity between the Wiki system
and the MFS system indicates that these will over-
ride the Rerank system in many cases.
Better results are seen with the simple stacking
result: in this case, systems are ordered and thus
16
System Precision Recall F-measure
All MFS 58.6% 58.6% 58.6%
Voting 58.6% 58.6% 58.6%
Stacking 58.9% 58.9% 58.9%
Stacked DT/NN 58.7% 58.7% 58.7%
Table 6: Resulting refined system (forced-choice)
are not being subjected to overriding by other MFS
skewed systems.
7 Conclusion
We have presented a refinement of the most fre-
quent sense baseline system, which incorporates a
number of novel approaches to word sense disam-
biguation methods. We demonstrate the need for
accurate lemmatization and part of speech tagging,
showing that that is probably the area where the
biggest boost in performance can currently be ob-
tained. We would also argue that examining the ab-
solute performance in a task where the baseline is so
exceedingly variable (ourselves, we have found the
baseline to be as low as 56% with restricted lemma
backoff, 58.4% with a fairly sophisticated lemma /
PoS module, against published baselines of 61.5%
in McCarthy et al, 62.5% reported in Snyder, or the
upper bound baseline of 66% using correct lemmas
and parts of speech), the performance difference be-
tween the baseline used and the resulting system is
interesting in itself.
Acknowledgments
We would like to thank DJ Hovermale for his input
throughout this project.
References
Agirre, E., , and de Lacalle Lekuona, O. L. (2004).
Publicly Available Topic Signatures for all Word-
Net Nominal Senses. In Proceedings of the 4th In-
ternational Conference on Languages Resources
and Evaluations (LREC), Lisbon, Portugal.
Baayen, H., Piepenbrock, R., and Gulikers, L.
(1995). The CELEX lexical database (release 2).
CD-ROM. Centre for Lexical Information, Max
Planck Institute for Psycholinguistics, Nijmegen;
Linguistic Data Consortium, University of Penn-
sylvania.
Brill, E. (1992). A simple rule-based part of speech
tagger. In Proceedings of the Third Conference
on Applied Natural Language Processing, pages
152?155, Trento, Italy.
Brin, S. and Page, L. (1998). The anatomy of a large-
scale hypertextual web search engine. In Com-
puter Networks and ISDN Systems, pages 107?
117.
Briscoe, E., Carroll, J., and Watson, R. (2006). The
second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, Sydney, Australia.
Elworthy, D. (1994). Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ACL Conference on Applied NLP, pages 53?
58, Stuttgart, Germany.
Florian, R., Cucerzan, S., Schafer, C., and
Yarowsky, D. (2002). Combining classifiers for
word sense disambiguation. Journal of Natural
Language Engineering, 8(4):327?342.
Hala?csy, P., Kornai, A., and Oravecz, C. (2007).
HunPos ? an open source trigram tagger. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 209?212, Prague, Czech Republic.
Association for Computational Linguistics.
Korhonen, A., Krymolovski, Y., and Briscoe, T.
(2006). A large subcategorization lexicon for
natural language processing applications. In
Proceedings of the 5th international conference
on Language Resources and Evaluation, pages
1015?1020.
Korhonen, A. and Krymolowski, Y. (2002). On the
robustness of entropy-based similarity measures
in evaluation of subcategorization acquisition sys-
tems. In Proceedings of the 6th Conference on
Natural Language Learning, pages 91?97.
Lin, D. (1998). Automatic retrieval and clustering
of similar words. In Proceedings of the COLING-
ACL?98, pages 768?773.
Madani, O. and Connor, M. (2008). Large-Scale
17
Many-Class Learning. In Proceedins of the SIAM
Conference on Data Mining (SDM-08).
McCarthy, D., Koeling, R., Weeds, J., and Carroll,
J. (2007). Unsupervised acquisition of predom-
inant word senses. Computational Linguistics,
33(4):553?590.
Mihalcea, R. (2007). Using Wikipedia for automatic
word sense disambiguation. In Human Language
Technologies 2007: The Conferece of the North
Americ an Chapter of the Association for Compu-
tational Linguistics, Rochester, New York.
Miller, G., Beckwith, R., Felbaum, C., Gross, D.,
and Miller, K. (1990). Introduction to WordNet:
An on-line lexical database. Journal of Lexicog-
raphy, 3(4):235?244.
Miller, G., Leacock, C., Ranee, T., and Bunker, R.
(1993). A semantic concordance. In Proceedings
of the 3rd DARPA Workshop on Human Language
Technology, pages 232?235.
Ollivier, Y. and Senellart, P. (2007). Finding related
pages using Green measures: An illustration with
Wikipedia. In Association for the Advancement
of Artificial Intelligence Conference on Artificial
Intelligence (AAAI 2007).
Pedersen, T., Patwardhan, S., and Michelizzi, J.
(2004). Wordnet::similarity - measuring the re-
latedness of concepts. In Proceedings of Fifth
Annual Meeting of the North American Chapter
of the Association for Computational Linguistics,
pages 38?41.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49.
Snyder, B. and Palmer, M. (2004). The english all-
words task. In Mihalcea, R. and Chklowski, T.,
editors, Proceedings of SENSEVAL-3: Third In-
ternational Workshop on Evaluating Word Sense
Disambiguating Systems, pages 41?43.
Witten, I. H. and Frank, E. (2000). Data min-
ing: Practical Machine Learning Tools and Tech-
niques with Java Implementations, chapter 8.
Morgan Kaufmann Publishers.
XTAG Research Group (2001). A lexicalized tree
adjoining grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
Yarowsky, D. (1993). One Sense Per Collocation. In
Proceedings of the Human Language Technology
Conference, Princeton, NJ, USA.
Yngve, V. H. (1955). Syntax and the problem of
multiple meaning. In Locke, W. N. and Booth,
A. D., editors, Machine translation of languages,
pages 208?226. John Wiley and Sons, New York.
18
291
292
293
294
295
296
297
298
 
	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 912?919,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival
Subcategorization Frames from Corpora
Judita Preiss, Ted Briscoe, and Anna Korhonen
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
Judita.Preiss, Ted.Briscoe, Anna.Korhonen@cl.cam.ac.uk
Abstract
This paper describes the first system for
large-scale acquisition of subcategorization
frames (SCFs) from English corpus data
which can be used to acquire comprehen-
sive lexicons for verbs, nouns and adjectives.
The system incorporates an extensive rule-
based classifier which identifies 168 verbal,
37 adjectival and 31 nominal frames from
grammatical relations (GRs) output by a ro-
bust parser. The system achieves state-of-
the-art performance on all three sets.
1 Introduction
Research into automatic acquisition of lexical in-
formation from large repositories of unannotated
text (such as the web, corpora of published text,
etc.) is starting to produce large scale lexical re-
sources which include frequency and usage infor-
mation tuned to genres and sublanguages. Such
resources are critical for natural language process-
ing (NLP), both for enhancing the performance of
state-of-art statistical systems and for improving the
portability of these systems between domains.
One type of lexical information with particular
importance for NLP is subcategorization. Access
to an accurate and comprehensive subcategoriza-
tion lexicon is vital for the development of success-
ful parsing technology (e.g. (Carroll et al, 1998),
important for many NLP tasks (e.g. automatic verb
classification (Schulte im Walde and Brew, 2002))
and useful for any application which can benefit
from information about predicate-argument struc-
ture (e.g. Information Extraction (IE) ((Surdeanu et
al., 2003)).
The first systems capable of automatically learn-
ing a small number of verbal subcategorization
frames (SCFs) from unannotated English corpora
emerged over a decade ago (Brent, 1991; Manning,
1993). Subsequent research has yielded systems for
English (Carroll and Rooth, 1998; Briscoe and Car-
roll, 1997; Korhonen, 2002) capable of detecting
comprehensive sets of SCFs with promising accu-
racy and demonstrated success in application tasks
(e.g. (Carroll et al, 1998; Korhonen et al, 2003)).
Recently, a large publicly available subcategoriza-
tion lexicon was produced using such technology
which contains frame and frequency information for
over 6,300 English verbs ? the VALEX lexicon (Ko-
rhonen et al, 2006).
While there has been considerable work in the
area, most of it has focussed on verbs. Although
verbs are the richest words in terms of subcatego-
rization and although verb SCF distribution data is
likely to offer the greatest boost in parser perfor-
mance, accurate and comprehensive knowledge of
the many noun and adjective SCFs in English could
improve the accuracy of parsing at several levels
(from tagging to syntactic and semantic analysis).
Furthermore the selection of the correct analysis
from the set returned by a parser which does not ini-
tially utilize fine-grained lexico-syntactic informa-
tion can depend on the interaction of conditional
probabilities of lemmas of different classes occur-
912
ring with specific SCFs. For example, a) and b) be-
low indicate the most plausible analyses in which the
sentential complement attaches to the noun and verb
respectively
a) Kim (VP believes (NP the evidence (Scomp that
Sandy was present)))
b) Kim (VP persuaded (NP the judge) (Scomp that
Sandy was present))
However, both a) and b) consist of an identical
sequence of coarse-grained lexical syntactic cate-
gories, so correctly ranking them requires learn-
ing that P (NP | believe).P (Scomp | evidence) >
P (NP&Scomp | believe).P (None | evidence)
and P (NP | persuade).P (Scomp | judge) <
P (NP&Scomp | persuade).P (None | judge). If
we acquired frames and frame frequencies for all
open-class predicates taking SCFs using a single sys-
tem applied to similar data, we would have a better
chance of modeling such interactions accurately.
In this paper we present the first system for large-
scale acquisition of SCFs from English corpus data
which can be used to acquire comprehensive lexi-
cons for verbs, nouns and adjectives. The classifier
incorporates 168 verbal, 37 adjectival and 31 nomi-
nal SCF distinctions. An improved acquisition tech-
nique is used which expands on the ideas Yallop et
al. (2005) recently explored for a small experiment
on adjectival SCF acquisition. It involves identifying
SCFs on the basis of grammatical relations (GRs) in
the output of the RASP (Robust Accurate Statistical
Parsing) system (Briscoe et al, 2006).
As detailed later, the system performs better with
verbs than previous comparable state-of-art systems,
achieving 68.9 F-measure in detecting SCF types. It
achieves similarly good performance with nouns and
adjectives (62.2 and 71.9 F-measure, respectively).
Additionally, we have developed a tool for lin-
guistic annotation of SCFs in corpus data aimed at
alleviating the process of obtaining training and test
data for subcategorization acquisition. The tool in-
corporates an intuitive interface with the ability to
significantly reduce the number of frames presented
to the user for each sentence.
We introduce the new system for SCF acquisition
in section 2. Details of the experimental evaluation
are supplied in section 3. Section 4 provides discus-
sion of our results and future work, and section 5
concludes.
2 Description of the System
A common strategy in existing large-scale SCF ac-
quisition systems (e.g. (Briscoe and Carroll, 1997))
is to extract SCFs from parse trees, introducing an
unnecessary dependence on the details of a particu-
lar parser. In our approach SCFs are extracted from
GRs ? representations of head-dependent relations
which are more parser/grammar independent but at
the appropriate level of abstraction for extraction of
SCFs.
A similar approach was recently motivated and
explored by Yallop et al (2005). A decision-tree
classifier was developed for 30 adjectival SCF types
which tests for the presence of GRs in the GR out-
put of the RASP (Robust Accurate Statistical Pars-
ing) system (Briscoe and Carroll, 2002). The results
reported with 9 test adjectives were promising (68.9
F-measure in detecting SCF types).
Our acquisition process consists of four main
steps: 1) extracting GRs from corpus data, 2) feeding
the GR sets as input to a rule-based classifier which
incrementally matches them with the corresponding
SCFs, 3) building lexical entries from the classified
data, and 4) filtering those entries to obtain a more
accurate lexicon. The details of these steps are pro-
vided in the subsequent sections.
2.1 Obtaining Grammatical Relations
We obtain the GRs using the recent, second release
of the RASP toolkit (Briscoe et al, 2006). RASP is a
modular statistical parsing system which includes a
tokenizer, tagger, lemmatizer, and a wide-coverage
unification-based tag-sequence parser. We use the
standard scripts supplied with RASP to output the set
of GRs for the most probable analysis returned by the
parser or, in the case of parse failures, the GRs for
the most likely sequence of subanalyses. The GRs
are organized as a subsumption hierarchy as shown
in Figure 1.
The dependency relationships which the GRs em-
body correspond closely to the head-complement
structure which subcategorization acquisition at-
tempts to recover, which makes GRs ideal input to
the SCF classifier. Consider the arguments of easy
913
dependent
ta arg mod det aux conj
mod arg
ncmod xmod cmod pmod
subj dobj
subj comp
ncsubj xsubj csubj obj pcomp clausal
dobj obj2 iobj xcomp ccomp
Figure 1: The GR hierarchy used by RASP
?
?
?
?
?
?
?
?
SUBJECT NP 1 ,
ADJ-COMPS
?
PP
[
PVAL for
NP 3
]
,
VP
?
?
?
?
MOOD to-infinitive
SUBJECT 3
OMISSION 1
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Feature structure for SCF
adj-obj-for-to-inf
(|These:1_DD2| |example+s:2_NN2| |of:3_IO|
|animal:4_JJ| |senses:5_NN2| |be+:6_VBR|
|relatively:7_RR| |easy:8_JJ| |for:9_IF|
|we+:10_PPIO2| |to:11_TO| |comprehend:12_VV0|)
...
xcomp(_ be+[6] easy:[8])
xcomp(to[11] be+[6] comprehend:[12])
ncsubj(be+[6] example+s[2] _)
ncmod(for[9] easy[8] we+[10])
ncsubj(comprehend[12] we+[10], _)
...
Figure 3: GRs from RASP for adj-obj-for-to-inf
in the sentence: These examples of animal senses
are relatively easy for us to comprehend as they are
not too far removed from our own experience. Ac-
cording to the COMLEX classification, this is an ex-
ample of the frame adj-obj-for-to-inf, shown in
Figure 2, (using AVM notation in place of COMLEX
s-expressions). Part of the output of RASP for this
sentence is shown in Figure 3.
Each instantiated GR in Figure 3 corresponds to
one or more parts of the feature structure in Fig-
ure 2. xcomp( be[6] easy[8]) establishes be[6]
as the head of the VP in which easy[8] occurs as
a complement. The first (PP)-complement is for us,
as indicated by ncmod(for[9] easy[8] we+[10]),
with for as PFORM and we+ (us) as NP. The sec-
ond complement is represented by xcomp(to[11]
be+[6] comprehend[12]): a to-infinitive VP. The
xcomp ?Y : pos=vb,val=be ?X : pos=adj
xcomp ?S : val=to ?Y : pos=vb,val=be ?W : pos=VV0
ncsubj ?Y : pos=vb,val=be ?Z : pos=noun
ncmod ?T : val=for ?X : pos=adj ?Y: pos=pron
ncsubj ?W : pos=VV0 ?V : pos=pron
Figure 4: Pattern for frame adj-obj-for-to-inf
NP headed by examples is marked as the subject
of the frame by ncsubj(be[6] examples[2]), and
ncsubj(comprehend[12] we+[10]) corresponds to
the coindexation marked by 3 : the subject of the
VP is the NP of the PP. The only part of the feature
structure which is not represented by the GRs is coin-
dexation between the omitted direct object 1 of the
VP-complement and the subject of the whole clause.
2.2 SCF Classifier
SCF Frames
The SCFs recognized by the classifier were ob-
tained by manually merging the frames exempli-
fied in the COMLEX Syntax (Grishman et al, 1994),
ANLT (Boguraev et al, 1987) and/or NOMLEX
(Macleod et al, 1997) dictionaries and including
additional frames found by manual inspection of
unclassifiable examples during development of the
classifier. These consisted of e.g. some occurrences
of phrasal verbs with complex complementation and
with flexible ordering of the preposition/particle,
some non-passivizable words with a surface direct
object, and some rarer combinations of governed
preposition and complementizer combinations.
The frames were created so that they abstract
over specific lexically-governed particles and prepo-
sitions and specific predicate selectional preferences
914
but include some derived semi-predictable bounded
dependency constructions.
Classifier
The classifier operates by attempting to match the
set of GRs associated with each sentence against one
or more rules which express the possible mappings
from GRs to SCFs. The rules were manually devel-
oped by examining a set of development sentences
to determine which relations were actually emitted
by the parser for each SCF.
In our rule representation, a GR pattern is a set of
partially instantiated GRs with variables in place of
heads and dependents, augmented with constraints
that restrict the possible instantiations of the vari-
ables. A match is successful if the set of GRs for
a sentence can be unified with any rule. Unifica-
tion of sentence GRs and a rule GR pattern occurs
when there is a one-to-one correspondence between
sentence elements and rule elements that includes a
consistent mapping from variables to values.
A sample pattern for matching
adj-obj-for-to-inf can be seen in Fig-
ure 4. Each element matches either an empty GR
slot ( ), a variable with possible constraints on part
of speech (pos) and word value (val), or an already
instantiated variable. Unlike in Yallop?s work (Yal-
lop et al, 2005), our rules are declarative rather than
procedural and these rules, written independently
of the acquisition system, are expanded by the
system in a number of ways prior to execution. For
example, the verb rules which contain an ncsubj
relation will not contain one inside an embedded
clause. For verbs, the basic rule set contains 248
rules but automatic expansion gives rise to 1088
classifier rules for verbs.
Numerous approaches were investigated to allow
an efficient execution of the system: for example, for
each target word in a sentence, we initially find the
number of ARGument GRs (see Figure 1) containing
it in head position, as the word must appear in ex-
actly the same set in a matching rule. This allows
us to discard all patterns which specify a different
number of GRs: for example, for verbs each group
only contains an average of 109 patterns.
For a further increase in speed, both the sentence
GRs and the GRs within the patterns are ordered (ac-
cording to frequency) and matching is performed us-
ing a backing off strategy allowing us to exploit the
relatively low number of possible GRs (compared
to the number of possible rules). The system exe-
cutes on 3500 sentences in approx. 1.5 seconds of
real time on a machine with a 3.2 GHz Intel Xenon
processor and 4GB of RAM.
Lexicon Creation and Filtering
Lexical entries are constructed for each word and
SCF combination found in the corpus data. Each lex-
ical entry includes the raw and relative frequency of
the SCF with the word in question, and includes var-
ious additional information e.g. about the syntax of
detected arguments and the argument heads in dif-
ferent argument positions1.
Finally the entries are filtered to obtain a more
accurate lexicon. A way to maximise the accu-
racy of the lexicon would be to smooth (correct) the
acquired SCF distributions with back-off estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002) (see section 4) before filtering them.
However, in this first experiment with the new sys-
tem we filtered the entries directly so that we could
evaluate the performance of the new classifier with-
out any additional modules. For the same reason, the
filtering was done by using a very simple method:
by setting empirically determined thresholds on the
relative frequencies of SCFs.
3 Experimental Evaluation
3.1 Data
In order to test the accuracy of our system, we se-
lected a set of 183 verbs, 30 nouns and 30 adjec-
tives for experimentation. The words were selected
at random, subject to the constraint that they exhib-
ited multiple complementation patterns and had a
sufficient number of corpus occurrences (> 150) for
experimentation. We took the 100M-word British
National Corpus (BNC) (Burnard, 1995), and ex-
tracted all sentences containing an occurrence of one
of the test words. The sentences were processed us-
ing the SCF acquisition system described in the pre-
vious section. The citations from which entries were
derived totaled approximately 744K for verbs and
219K for nouns and adjectives, respectively.
1The lexical entries are similar to those in the VALEX lexi-
con. See (Korhonen et al, 2006) for a sample entry.
915
3.2 Gold Standard
Our gold standard was based on a manual analysis
of some of the test corpus data, supplemented with
additional frames from the ANLT, COMLEX, and/or
NOMLEX dictionaries. The gold standard for verbs
was available, but it was extended to include addi-
tional SCFs missing from the old system. For nouns
and adjectives the gold standard was created. For
each noun and adjective, 100-300 sentences from the
BNC (an average of 267 per word) were randomly
extracted. The resulting c. 16K sentences were then
manually associated with appropriate SCFs, and the
SCF frequency counts were recorded.
To alleviate the manual analysis we developed
a tool which first uses the RASP parser with some
heuristics to reduce the number of SCF presented,
and then allows an annotator to select the preferred
choice in a window. The heuristics reduced the av-
erage number of SCFs presented alongside each sen-
tence from 52 to 7. The annotator was also presented
with an example sentence of each SCF and an intu-
itive name for the frame, such as PRED (e.g. Kim
is silly). The program includes an option to record
that particular sentences could not (initially) be clas-
sified. A screenshot of the tool is shown in Figure 5.
The manual analysis was done by two linguists;
one who did the first annotation for the whole data,
and another who re-evaluated and corrected some of
the initial frame assignments, and classified most of
the data left unclassified by the first annotator2). A
total of 27 SCF types were found for the nouns and
30 for the adjectives in the annotated data. The av-
erage number of SCFs taken by nouns was 9 (with
the average of 2 added from dictionaries to supple-
ment the manual annotation) and by adjectives 11
(3 of which were from dictionaries). The latter are
rare and may not be exemplified in the data given the
extraction system.
3.3 Evaluation Measures
We used the standard evaluation metrics to evaluate
the accuracy of the SCF lexicons: type precision (the
percentage of SCF types that the system proposes
2The process precluded measurements of inter-annotator
agreement, but this was judged less important than the enhanced
accuracy of the gold standard data.
Figure 5: Sample screen of the annotation tool
which are correct), type recall (the percentage of SCF
types in the gold standard that the system proposes)
and the F-measure which is the harmonic mean of
type precision and recall.
We also compared the similarity between the ac-
quired unfiltered3 SCF distributions and gold stan-
dard SCF distributions using various measures of
distributional similarity: the Spearman rank corre-
lation (RC), Kullback-Leibler distance (KL), Jensen-
Shannon divergence (JS), cross entropy (CE), skew
divergence (SD) and intersection (IS). The details of
these measures and their application to subcatego-
rization acquisition can be found in (Korhonen and
Krymolowski, 2002).
Finally, we recorded the total number of gold
standard SCFs unseen in the system output, i.e. the
type of false negatives which were never detected
by the classifier.
3.4 Results
Table 1 includes the average results for the 183
verbs. The first column shows the results for Briscoe
and Carroll?s (1997) (B&C) system when this sys-
tem is run with the original classifier but a more
recent version of the parser (Briscoe and Carroll,
2002) and the same filtering technique as our new
system (thresholding based on the relative frequen-
cies of SCFs). The classifier of B&C system is com-
parable to our classifier in the sense that it targets al-
most the same set of verbal SCFs (165 out of the 168;
the 3 additional ones are infrequent in language and
thus unlikely to affect the comparison). The second
column shows the results for our new system (New).
3No threshold was applied to remove the noisy SCFs from
the distributions.
916
Verbs - Method
Measures B&C New
Precision (%) 47.3 81.8
Recall (%) 40.4 59.5
F-measure 43.6 68.9
KL 3.24 1.57
JS 0.20 0.11
CE 4.85 3.10
SD 1.39 0.74
RC 0.33 0.66
IS 0.49 0.76
Unseen SCFs 28 17
Table 1: Average results for verbs
The figures show that the new system clearly per-
forms better than the B&C system. It yields 68.9 F-
measure which is a 25.3 absolute improvement over
the B&C system. The better performance can be ob-
served on all measures, but particularly on SCF type
precision (81.8% with our system vs. 47.3% with the
B&C system) and on measures of distributional sim-
ilarity. The clearly higher IS (0.76 vs. 0.49) and the
fewer gold standard SCFs unseen in the output of the
classifier (17 vs. 28) indicate that the new system is
capable of detecting a higher number of SCFs.
The main reason for better performance is the
ability of the new system to detect a number of chal-
lenging or complex SCFs which the B&C system
could not detect4. The improvement is partly at-
tributable to more accurate parses produced by the
second release of RASP and partly to the improved
SCF classifier developed here. For example, the new
system is now able to distinguish predicative PP ar-
guments, such as I sent him as a messenger from the
wider class of referential PP arguments, supporting
discrimination of several syntactically similar SCFs
with distinct semantics.
Running our system on the adjective and noun test
data yielded the results summarized in Table 2. The
F-measure is lower for nouns (62.2) than for verbs
(68.9); for adjectives it is slightly better (71.9).5
4The results reported here for the B&C system are lower
than those recently reported in (Korhonen et al, 2006) for the
same set of 183 test verbs. This is because we use an improved
gold standard. However, the results for the B&C system re-
ported using the less ambitious gold standard are still less ac-
curate (58.6 F-measure) than the ones reported here for the new
system.
5The results for different word classes are not directly com-
parable because they are affected by the total number of SCFs
evaluated for each word class, which is higher for verbs and
Measures Nouns Adjectives
Precision (%) 91.2 95.5
Recall (%) 47.2 57.6
F-measure 62.2 71.9
KL 0.91 0.69
JS 0.09 0.05
CE 2.03 2.01
SD 0.48 0.36
RC 0.70 0.77
IS 0.62 0.72
Unseen SCFs 15 7
Table 2: Average results for nouns and adjectives
The noun and adjective classifiers yield very high
precision compared to recall. The lower recall fig-
ures are mostly due to the higher number of gold
standard SCFs unseen in the classifier output (rather
than, for example, the filtering step). This is par-
ticularly evident for nouns for which 15 of the 27
frames exemplified in the gold standard are missing
in the classifier output. For adjectives only 7 of the
30 gold standard SCFs are unseen, resulting in better
recall (57.6% vs. 47.2% for nouns).
For verbs, subcategorization acquisition perfor-
mance often correlates with the size of the input
data to acquisition (the more data, the better perfor-
mance). When considering the F-measure results for
the individual words shown in Table 3 there appears
to be little such correlation for nouns and adjectives.
For example, although there are individual high fre-
quency nouns with high performance (e.g. plan,
freq. 5046, F 90.9) and low frequency nouns with
low performance (e.g. characterisation, freq. 91, F
40.0), there are also many nouns which contradict
the trend (compare e.g. answer, freq. 2510, F 50.0
with fondness, freq. 71, F 85.7).6
Although the SCF distributions for nouns and ad-
jectives appear Zipfian (i.e. the most frequent frames
are highly probable, but most frames are infre-
quent), the total number of SCFs per word is typi-
cally smaller than for verbs, resulting in better resis-
tance to sparse data problems.
There is, however, a clear correlation between
the performance and the type of gold standard SCFs
taken by individual words. Many of the gold stan-
lower for nouns and adjectives. This particularly applies to the
sensitive measures of distributional similarity.
6The frequencies here refer to the number of citations suc-
cessfully processed by the parser and the classifier.
917
Noun F Adjective F
abundance 75.0 able 66.7
acknowledgement 47.1 angry 62.5
answer 50.0 anxious 82.4
anxiety 53.3 aware 87.5
apology 50.0 certain 73.7
appearance 46.2 clear 77.8
appointment 66.7 curious 57.1
belief 76.9 desperate 83.3
call 58.8 difficult 77.8
characterisation 40.0 doubtful 63.6
communication 40.0 eager 83.3
condition 66.7 easy 66.7
danger 76.9 generous 57.1
decision 70.6 imperative 81.8
definition 42.8 important 60.9
demand 66.7 impractical 71.4
desire 71.4 improbable 54.6
doubt 66.7 insistent 80.0
evidence 66.7 kind 66.7
examination 54.6 likely 66.7
experimentation 60.0 practical 88.9
fondness 85.7 probable 80.0
message 66.7 sure 84.2
obsession 54.6 unaware 85.7
plan 90.9 uncertain 60.0
provision 70.6 unclear 63.2
reminder 63.2 unimportant 61.5
rumour 61.5 unlikely 69.6
temptation 71.4 unspecified 50.0
use 60.0 unsure 90.0
Table 3: System performance for each test noun and
adjective
dard nominal and adjectival SCFs unseen by the
classifier involve complex complementation patterns
which are challenging to extract, e.g. those exem-
plified in The argument of Jo with Kim about Fido
surfaced, Jo?s preference that Kim be sacked sur-
faced, and that Sandy came is certain. In addition,
many of these SCFs unseen in the data are also very
low in frequency, and some may even be true nega-
tives (recall that the gold standard was supplemented
with additional SCFs from dictionaries, which may
not necessarily appear in the test data).
The main problem is that the RASP parser system-
atically fails to select the correct analysis for some
SCFs with nouns and adjectives regardless of their
context of occurrence. In future work, we hope to al-
leviate this problem by using the weighted GR output
from the top n-ranked parses returned by the parser
as input to the SCF classifier.
4 Discussion
The current system needs refinement to alleviate the
bias against some SCFs introduced by the parser?s
unlexicalized parse selection model. We plan to in-
vestigate using weighted GR output with the clas-
sifier rather than just the GR set from the highest
ranked parse. Some SCF classes also need to be fur-
ther resolved mainly to differentiate control options
with predicative complementation. This requires a
lexico-semantic classification of predicate classes.
Experiments with Briscoe and Carroll?s system
have shown that it is possible to incorporate some
semantic information in the acquisition process us-
ing a technique that smooths the acquired SCF dis-
tributions using back-off (i.e. probability) estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002). The estimates help to correct the ac-
quired SCF distributions and predict SCFs which are
rare or unseen e.g. due to sparse data. They could
also form the basis for predicting control of predica-
tive complements.
We plan to modify and extend this technique for
the new system and use it to improve the perfor-
mance further. The technique has so far been applied
to verbs only, but it can also be applied to nouns
and adjectives because they can also be classified on
lexical-semantic grounds. For example, the adjec-
tive simple belongs to the class of EASY adjectives,
and this knowledge can help to predict that it takes
similar SCFs to the other class members and that
control of ?understood? arguments will pattern with
easy (e.g. easy, difficult, convenient): The problem
will be simple for John to solve, For John to solve
the problem will be simple, The problem will be sim-
ple to solve, etc.
Further research is needed before highly accurate
lexicons encoding information also about semantic
aspects of subcategorization (e.g. different predicate
senses, the mapping from syntactic arguments to
semantic representation of argument structure, se-
lectional preferences on argument heads, diathesis
alternations, etc.) can be obtained automatically.
However, with the extensions suggested above, the
system presented here is sufficiently accurate for
building an extensive SCF lexicon capable of sup-
porting various NLP application tasks. Such a lex-
icon will be built and distributed for research pur-
918
poses along with the gold standard described here.
5 Conclusion
We have described the first system for automatically
acquiring verbal, nominal and adjectival subcat-
egorization and associated frequency information
from English corpora, which can be used to build
large-scale lexicons for NLP purposes. We have
also described a new annotation tool for producing
training and test data for the task. The acquisition
system, which is capable of distinguishing 168
verbal, 37 adjectival and 31 nominal frames, clas-
sifies corpus occurrences to SCFs on the basis of
GRs produced by a robust statistical parser. The
information provided by GRs closely matches the
structure that subcategorization acquisition seeks
to recover. Our experiment shows that the system
achieves state-of-the-art performance with each
word class. The discussion suggests ways in which
we could improve the system further before using it
to build a large subcategorization lexicon capable of
supporting various NLP application tasks.
Acknowledgements
This work was supported by the Royal Society and
UK EPSRC project ?Accurate and Comprehensive
Lexical Classification for Natural Language Pro-
cessing Applications? (ACLEX). We would like to
thank Diane Nicholls for her help during this work.
References
B. Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover.
1987. The derivation of a grammatically-indexed lexicon
from the Longman Dictionary of Contemporary English. In
Proc. of the 25th Annual Meeting of ACL, pages 193?200,
Stanford, CA.
M. Brent. 1991. Automatic acquisition of subcategorization
frames from untagged text. In Proc. of the 29th Meeting of
ACL, pages 209?214.
E. J. Briscoe and J. Carroll. 1997. Automatic Extraction of
Subcategorization from Corpora. In Proc. of the 5th ANLP,
Washington DC, USA.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proc. of the 3rd LREC, pages
1499?1504, Las Palmas, Canary Islands, May.
E. J. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the rasp system. In Proc. of the COLING/ACL
2006 Interactive Presentation Sessions, Sydney, Australia.
L. Burnard, 1995. The BNC Users Reference Guide. British
National Corpus Consortium, Oxford, May.
G. Carroll and M. Rooth. 1998. Valence induction with a head-
lexicalized pcfg. In Proc. of the 3rd Conference on EMNLP,
Granada, Spain.
J. Carroll, G. Minnen, and E. J. Briscoe. 1998. Can Subcat-
egorisation Probabilities Help a Statistical Parser? In Pro-
ceedings of the 6th ACL/SIGDAT Workshop on Very Large
Corpora, pages 118?126, Montreal, Canada.
R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX
Syntax: Building a Computational Lexicon. In COLING,
Kyoto.
A. Korhonen and Y. Krymolowski. 2002. On the Robustness
of Entropy-Based Similarity Measures in Evaluation of Sub-
categorization Acquisition Systems. In Proc. of the Sixth
CoNLL, pages 91?97, Taipei, Taiwan.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clustering
Polysemic Subcategorization Frame Distributions Semanti-
cally. In Proc. of the 41st Annual Meeting of ACL, pages
64?71, Sapporo, Japan.
A. Korhonen, Y. Krymolowski, and E. J. Briscoe. 2006. A
large subcategorization lexicon for natural language process-
ing applications. In Proc. of the 5th LREC, Genova, Italy.
A. Korhonen. 2002. Subcategorization acquisition. Ph.D. the-
sis, University of Cambridge Computer Laboratory.
C. Macleod, A. Meyers, R. Grishman, L. Barrett, and R. Reeves.
1997. Designing a dictionary of derived nominals. In Proc.
of RANLP, Tzigov Chark, Bulgaria.
C. Manning. 1993. Automatic Acquisition of a Large Subcat-
egorization Dictionary from Corpora. In Proc. of the 31st
Meeting of ACL, pages 235?242.
S. Schulte im Walde and C. Brew. 2002. Inducing german se-
mantic verb classes from purely syntactic subcategorisation
information. In Proc. of the 40th Annual Meeting of ACL,
Philadephia, USA.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proc. of the 41st Annual Meeting of ACL, Sapporo.
J. Yallop, A. Korhonen, and E. J. Briscoe. 2005. Auto-
matic acquisition of adjectival subcategorization from cor-
pora. In Proc. of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 614?621, Ann Arbor,
Michigan.
919
 
	   	
 	


  
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 558?562,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying Comparable Corpora Using LDA
Judita Preiss
j.preiss@sheffield.ac.uk
Department of Computer Science, University of Sheffield, Regent Court
211 Portobello, Sheffield, S1 4DP, United Kingdom
Abstract
Parallel corpora have applications in many ar-
eas of Natural Language Processing, but are
very expensive to produce. Much information
can be gained from comparable texts, and we
present an algorithm which, given any bod-
ies of text in multiple languages, uses ex-
isting named entity recognition software and
topic detection algorithm to generate pairs of
comparable texts without requiring a paral-
lel corpus training phase. We evaluate the
system?s performance firstly on data from the
online newspaper domain, and secondly on
Wikipedia cross-language links.
1 Introduction
Manual alignment or creation of parallel corpora is
exceedingly expensive, requiring highly skilled an-
notators or professional translators. Methods exist
for aligning parallel corpora, and extracted parallel
segments can be used to, for example, augment ma-
chine translation phrase tables, but the amount of
genuinely parallel data is limited. However, paral-
lel segments can also be extracted from comparable
corpora (a comparable corpus is one which contains
similar texts in more than one language). Compara-
ble documents, if produced with a confidence value,
could also be used to prioritize translation (manual
or automatic) when one is searching for further in-
formation (which may only be available in a foreign
language) to augment information given in an arti-
cle in the source language. We present a technique
to automatically detect comparable corpora in exist-
ing data, and we demonstrate the applicability of our
method to any genre by evaluating on crawled online
newspaper text, as well as Wikipedia articles.
Clearly, texts need to contain some of the same
data in order to be comparable (Harris, 1954), and
we assume:
? To be similar, texts need to share some named
entities, e.g., To?th et al, (2008).
? Comparable texts need to be on the same topic.
Construction of multilingual topic models usu-
ally requires either parallel data or some number of
aligned documents across multiple languages. Zhao
and Xing (2007) create bilingual topic models from
(at least 25%) of parallel data. Mimno et al, (2009)
start from tuples of equivalent documents to build
models, and then the same distribution over topics
holds in both source and target languages.
While Zhao and Xing (2007) used their topic
models for word alignment from comparable cor-
pora (combined with underlying parallel data), mul-
tilingual topic models are usually applied to data to
automatically detect word translations based on par-
allel data, e.g., Vulic? et al, (2011) exploit a shared
language independent topic distribution to measure
the similarity between topics pertaining to words.
The novelty of our work is the transformation of a
source language topic model rather than the creation
of a language independent model from parallel data.
Transforming the source language model to the tar-
get language allows the classification of the target
language documents to source language topics. The
translated model is applied to two document collec-
tions to demonstrate its ability to detect comparable
558
corpora. Our system can be applied to any pair of
languages for which there is a dictionary.
Section 2 describes the tools we employ. Sec-
tion 3 contains a description of our system: the
method for employing NE recognition across lan-
guages is presented in Section 3.1, while Section 3.2
outlines our technique for employing LDA across
languages. Our experiments and their results are de-
scribed in Section 4. Section 5 draws our conclu-
sions and indicates avenues for future work.
2 Tools
2.1 Named entity recognition
The Stanford named entity recognition (NER) soft-
ware1 (Finkel et al, 2005) is an implementation of
linear chain Conditional Random Field (CRF) se-
quence models, which includes a three class (per-
son, organization, location and other) named entity
recognizer for English.
2.2 Topic detection
LDA (Blei et al, 2003) is a generative probabilistic
model where documents are viewed as mixtures over
underlying topics, and each topic is a distribution
over words. Both the document-topic and the topic-
word distributions are assumed to have a Dirichlet
prior. Given a set of documents and a number of
topics, the model returns ?i, the topic distribution
for each document i, and ?ik, the word distribution
for topic k. We employ the publicly available imple-
mentation of LDA, JGibbLDA2 (Phan et al, 2008),
which has two main execution methods: parameter
estimation (model building) and inference for new
data (classification of a new document). Both invo-
cations produce the following:
?ij : p(wordi|topicj)
?jk: p(topicj |documentk)
tassign: a deterministic topic-word assignment for
each word in every document
The LDA topic models are created from a ran-
domly selected tenth of the Reuters corpus (Rose
et al, 2002).3
1http://nlp.stanford.edu/ner/index.shtml
2http://jgibblda.sourceforge.net/
3LDA modeling can abstract a model from a relatively small
corpus and a tenth of the original Reuters corpus is much more
2.3 Indexing
To provide quick searching access to the large text
collections, we utilize the high-performance search
engine library Lucene.4 The stemmed and stoplisted
documents are stored along with the frequency of
occurrence of each word within a document.
2.4 Lemmatization / stemming
English text is lemmatized using the lemmatizer
available within RASP5 (Briscoe et al, 2006). Stem-
ming is provided for all the non-English languages
included in our work within Lucene.
3 Identifying comparable corpora
3.1 Cross language NER
NEs extracted from the English text collections are
automatically translated into the target languages us-
ing the BING Translation API6 yielding a single
translation, which is retained. The stemmed, trans-
lated version of each NE in the source text is sought
in the indexed form of the target language document
collection, and the frequency of occurrence of the
NE is returned.
Filtering is applied based on the proportion of
source language document?s NEs found in the target
document (we do not expect all the NEs to be present
in the target language: NEs could be mis-translated,
and not all NEs would necessarily be mentioned
even in a comparable document). The proportions
of all types of NEs required were optimized over a
small manually created set. While we could assign a
weight and not filter documents, this is not believed
to be adequate: e.g., a newspaper article containing
all the source location mentions (and thus having a
high weight), but none of the same people, is likely
to be a news story about the same area but a different
event.
manageable in terms of memory and time requirements.
4http://lucene.apache.org
5http://ilexir.co.uk/applications/rasp/
download
6The translations could also be retrieved from NE mapping
lists, dictionaries (if these are available) or manually translated
? we therefore do not see this step as violating the lack of need
for a parallel corpus.
559
3.2 Cross language topic identification
Being non-deterministic, multiple executions of the
LDA algorithm are not guaranteed to (and do not)
give rise to identical topics (even within one lan-
guage). It is therefore not possible to build a topic
model in the source language and the target lan-
guage separately, as there is no clear alignment be-
tween their respective topics. Traditionally, par-
allel corpora are used to generate a language in-
dependent topic-document distribution, from which
polylingual topic models can be created so the un-
derlying topics are shared.
We propose to translate each word from the
source language topic model using the BING API
and substitute the new wordmap thus creating a tar-
get language topic model. While word distributions
are clearly different across languages, and building a
shared topic-document distribution to sample words
from allows words to retain their language specific
distributions, our technique completely avoids the
need for parallel corpora, and merely requires the
translation of the words in the LDA model (which
can be performed using dictionary lookup, or NE
lists instead of the BING API).
3.3 Selecting comparable corpora
Target language candidate documents found to share
sufficient proportions of NEs are classified using the
translated target language LDA model. This yields
?jk (the probability distribution of topic given doc-
ument) and classifying the original document using
the source language LDA model gives ??jk. The can-
didate documents are ranked according to the cosine
similarity between the two vectors:
similarity =
?jk ? ??jk
??jk????jk?
By definition, cosine similarity ranges between -1
and 1. Similarity of 1 indicates two documents with
? = ??, and thus the higher the similarity, the higher
we rank the document.
4 Experiments
We present two evaluations: firstly, we manu-
ally evaluate the comparable documents generated
from online newspaper text in two languages, while
the second evaluation finds comparable articles in
source and target versions of Wikipedia with results
evaluated against the cross-language links present in
Wikipedia.
4.1 Online newspaper documents
Simple Google search yields a number of links to
online newspapers in any language, these lists (auto-
matically retrieved) are used to seed a crawler. Doc-
uments from newspaper sites which allow crawling
are retrieved and only well formed HTML docu-
ments are retained,7 and the language of the docu-
ments is verified using a Perl implementation of Lin-
gua::Ident (Dunning, 1994), an n-gram based model
for language identification.8
A single annotator evaluated 10 randomly se-
lected English documents and the comparable doc-
uments returned for them from 40,528 Czech news-
paper articles (total retrieved within a 24 hour pe-
riod). Since there is no current scheme available for
judging comparability, we employed a four category
scale:
Strong: The documents are about the same news
event, in a similar style. (Articles about the
same news event, but elaborating, would be in-
cluded here.)
Medium: The documents are about related news
events.
Weak: The documents refers to similar events.
None: No overlap in topic in the two documents.
Results of the evaluation are presented in Table 1;
the top document is scored for each pair, showing
the high precision of the technique. The 10 English
documents were selected subject to the constraint
that a comparable corpus was retrieved for them: the
imposed constraints on NEs make this a high preci-
sion / low recall technique. Many articles found us-
ing the crawling approach on news sites (rather than
an RSS feed gathering approach) were discussions,
7Note that the crawler is not permitted to leave the domain
of the newspaper.
8The Lingua::Ident Perl module is available from http:
//search.cpan.org/?mpiotr/Lingua-Ident-1.
7/Ident.pm. We build the models for the language identi-
fication system from downloaded Wikipedia content for each
language.
560
Strong Medium Weak None
4 4 1 1
Table 1: Results for English-Czech documents
for example discussions of strategies in sports, inter-
views with actors, rather than topical news stories.
From a manual inspection of the target language ar-
ticles, many of these articles do not appear to have
comparable equivalents. Also, enforcing a high pro-
portion of NEs shared between the source and target
languages frequently rules out documents which are
subsets of each other (this was also apparent in the
second evaluation).
4.2 Wikipedia
Information within Wikipedia is connected across
languages using cross-language links. While the
lists of links are not necessarily complete, and the
articles they link may not contain large parallel seg-
ments, the linked documents should be comparable
(under the definition), and thus provide an empirical
measure of the utility of our method.
The top comparable articles in Czech were gener-
ated for 100 randomly selected English Wikipedia
articles (subject to the constraint that they have
cross-language links). As in our first evaluation, the
system had a low recall (35%), however precision
was 83%. By the design of the experiment, an arti-
cle about the same subject has to exist in both lan-
guages, and therefore the low recall value is surpris-
ing. Rather than a low cosine value, the low recall
is mainly due to the NE filtering step removing the
?correct? article from consideration. A brief inspec-
tion of a small number of articles which had been fil-
tered out was performed and substantial differences
between the pages were found ? for example, a sig-
nificant portion of the Wikipedia page for Equinox in
English contains descriptions of Equinox commem-
orations all over the world, which are missing in the
Czech version of the Wikipedia article (leading to a
large number of missing NEs). Similar length of ar-
ticles appeared to be a good indicator of both articles
containing similar data, and our system detecting the
two texts to be comparable.
Please note that while the NE filtering step is re-
moving texts from consideration, it is not possible
to compute cosines of the topic vectors of all docu-
ments and thus some candidate selection step is nec-
essary.
4.3 Baseline
There are no standard baselines for the task of cre-
ating comparable corpora. It is possible to trans-
late the source language text into the target language
using BING, however, a cosine comparison of the
stemmed, automatically translated document with
all documents in the target language collection is
extremely time consuming. Applying NE filtering,
automatically translating the remaining target lan-
guage candidate texts into the source language us-
ing BING, and ranking according to cosine similar-
ity gives a precision of 69% for the collection dis-
cussed in Section 4.2.
5 Conclusion
We have presented an LDA based algorithm applica-
ble to large document collections to find comparable
documents across multi-lingual corpora without the
needing to train with parallel data. We show, using
a human judge as well as Wikipedia cross-language
links, that the system achieves high precision in find-
ing comparable documents.
The technique strongly relies on the named en-
tity method selected, and another technique may be
more suitable. A comparison with a bilingual topic
model created from parallel data would also prove
interesting.
Acknowledgments
This research was supported by EU grant 248347
on Analysis and Evaluation of Comparable Corpora
for Under-Resourced Areas of Machine Translation
(ACCURAT). My thanks also go to the three review-
ers whose comments strengthened the findings of
this work.
References
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent
Dirichlet alocation. Journal of Machine Learning Re-
search, 3:993?1022.
Briscoe, T., Carroll, J., and Watson, R. (2006). The sec-
ond release of the RASP system. In Proceedings of the
COLING/ACL 2006 Interactive Presentation Sessions.
561
Dunning, T. (1994). Statistical identification of language.
Technical Report CRL MCCS-94-273, Computing Re-
search Lab, New Mexico State University.
Finkel, J. R., Grenager, T., and Manning, C. (2005). In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proceedings of
the 43nd Annual Meeting of the Association for Com-
putational Linguistics, pages 363?370.
Harris, Z. S. (1954). Distributional structure. Word,
10(23):146162.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith,
D. A., and McCallum, A. (2009). Polylingual topic
models. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, page
880889.
Phan, X.-H., Nguyen, L.-M., and Horiguchi, S. (2008).
Learning to classify short and sparse text & web with
hidden topics from large-scale data collections. In
Proceedings of The 17th International World Wide
Web Conference (WWW 2008), pages 91?100.
Rose, T. G., Stevenson, M., and Whitehead, M. (2002).
The Reuters corpus volume 1 - from yesterday?s news
to tomorrow?s language resources. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, pages 827?832.
To?th, K., Farkas, R., and Kocsor, A. (2008). Sentence
alignment of Hungarian-English parallel corpora using
a hybrid algorithm. Acta Cybernetica, pages 463?478.
Vulic, I., Smet, W. D., and Moens, M.-F. (2011). Iden-
tifying word translations from comparable corpora us-
ing latent topic models. In Proceedings of ACL, pages
479?484.
Zhao, B. and Xing, E. P. (2007). HM-BiTAM: Bilingual
topic exploration, word alignment, and translation. In
NIPS.
562
Proceedings of NAACL-HLT 2013, pages 680?684,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Unsupervised Domain Tuning to Improve Word Sense Disambiguation
Judita Preiss and Mark Stevenson
j.preiss@sheffield.ac.uk and m.stevenson@dcs.shef.ac.uk
Department of Computer Science, University of Sheffield
211 Portobello, Sheffield, S1 4DP, UK
Abstract
The topic of a document can prove to be use-
ful information for Word Sense Disambigua-
tion (WSD) since certain meanings tend to be
associated with particular topics. This paper
presents an LDA-based approach for WSD,
which is trained using any available WSD sys-
tem to establish a sense per (Latent Dirich-
let alocation based) topic. The technique is
tested using three unsupervised and one su-
pervised WSD algorithms within the SPORT
and FINANCE domains giving a performance
increase each time, suggesting that the tech-
nique may be useful to improve the perfor-
mance of any available WSD system.
1 Introduction
Assigning each word its most frequent sense (MFS)
is commonly used as a baseline in Word Sense Dis-
ambiguation (WSD). This baseline can be difficult to
beat, particularly for unsupervised systems which do
not have access to the annotated training data used to
determine the MFS. However, it has also been shown
that unsupervised methods can be used to identify
the most likely sense for each ambiguous word type
and this approach can be effective for disambigua-
tion (McCarthy et al, 2004).
Knowledge of the domain of a document has been
shown to be useful information for WSD. For ex-
ample, Khapra et al (2010) improve the perfor-
mance of a graph-based WSD system using a small
number of hand-tagged examples, but further ex-
amples would be required for each new domain.
Agirre et al (2009) automatically construct a the-
saurus from texts in a domain which they use for
WSD. Unfortunately, performance drops when the
thesaurus is combined with information from local
context. Stevenson et al (2011) showed that per-
formance of an unsupervised WSD algorithm can
be improved by supplementing the context with do-
main information. Cai et al (2007) use LDA to
create an additional feature for a supervised WSD
algorithm, by inferring topics for labeled training
data. Boyd-Graber et al (2007) integrate a topic
model with WordNet and use it to carry out dis-
ambiguation and learn topics simultaneously. Li et
al. (2010) use sense paraphrases to estimate prob-
abilities of senses and carry out WSD. Koeling et
al. (2005) showed that automatically acquiring the
predominant sense of a word from a corpus from
the same domain increases performance (over using
a predominant sense acquired from a balanced cor-
pus), but their work requires a separate thesaurus to
be built for each domain under investigation. Nav-
igli et al (2011) extracted relevant terms from texts
in a domain and used them to initialize a random
walk over the WordNet graph.
Our approaches rely on a one sense per topic
hypothesis (Gale et al, 1992), making use of top-
ics induced using LDA ? we present three novel
techniques for exploiting domain information that
are employable with any WSD algorithm (unsuper-
vised or supervised). Using any WSD algorithm, we
create a sense per topic distribution for each LDA
topic, and the classification of a new document into a
topic determines the sense distribution of the words
within. Once a sense per topic distribution is ob-
tained, no further WSD annotation of new texts is
required. Instead of fixing domains, our technique
680
allows these to be dynamically created (using LDA)
and we using four existing publicly available WSD
algorithms (three unsupervised and one supervised)
to show that our technique increases their perfor-
mance with no changes to the original algorithm.
Section 2 briefly introduces LDA, while Section 3
describes our three techniques for adding domain
information to a WSD algorithm. The WSD algo-
rithms employed in the evaluation of our techniques
are described in Section 4 with experiments and re-
sults in Section 5. Section 6 draws our conclusions
and presents avenues for future work.
2 Latent Dirichlet alocation
LDA (Blei et al, 2003) is a widely used topic model,
which views the underlying document distribution
as having a Dirichlet prior. We employ a pub-
licly available implementation of LDA1 which has
two main execution methods: parameter estimation
(model building) and inference for new data (classi-
fication of a new document). Both invocation meth-
ods produce ? distributions (the topic-document dis-
tributions, i.e., p(ti|d) for ti topics and d document),
and ? distributions (word-topic distributions, i.e.,
p(wj |ti) for words wj). The parameter estimation
phase also creates a list of n words most likely to be
associated with each topic.
3 Using LDA for WSD
The underlying idea of our approach lies in deriv-
ing a document invariant sense distribution for each
topic, p(w, s|t). Once this word sense distribution
is obtained, the underlying WSD algorithm is never
needed again. We make the assumption that while
the WSD algorithm may not be able to select the
correct sense within an individual text due to insuf-
ficient domain information, the topic specific sense
will be selected with a greater frequency over all
documents pertaining to a topic, and thus the prob-
ability distributions over senses generated in this
fashion should be more accurate.
Only the distribution p(w, s|t) is dependent on an
underlying WSD algorithm ? once this distribution
is obtained, it can be combined with the LDA de-
rived ? distribution, p(t|dnew), to compute the de-
1http://jgibblda.sourceforge.net/.
sired word sense distribution within the new docu-
ment dnew:
p(w, s|dnew) =
?
t
p(w, s|t)p(t|dnew)
Sections 3.1, 3.2 and 3.3 describe three different
methods for deriving p(w, s|t), and we investigate
the performance changes with different WSD algo-
rithms: two versions of Personalized PageRank, de-
scribed in Section 4.1, a similarity based WSD sys-
tem outlined in Section 4.2, and a supervised graph
based algorithm (Section 4.3).
3.1 Sense-based topic model (SBTM)
In its usual form, the ? distribution generated
by LDA merely provides a word-topic distribution
(p(w|t)). However, we modify the approach to di-
rectly output p(w, s|t), but we remain able to clas-
sify (non WSD annotated) new text. The topic
model is built from documents annotated with word
senses using the chosen WSD algorithm.2 The topic
model created from this data is based on word-sense
combinations and thus ? represents p(w, s|t).
To classify new (non sense disambiguated) doc-
uments, the model is transformed to a word (rather
than word-sense) based for: i.e., the p(w, s|t) prob-
abilities are summed over all senses of w to give re-
sulting probabilities for the wordform. A new docu-
ment, dnew, classified using this system gives rise to
a number of distributions, including the probability
of a topic given a document distribution (p(t|dnew)).
3.2 Linear equations (LinEq)
If the topic model is created directly from word-
forms, we can use the known probabilities p(s|w, d)
(obtained from the WSD algorithm), and p(t|d)
(from the LDA classifier) to yield an overdetermined
system of linear equations of the form
p(s|w, d) =
?
t
p(s|w, t)p(t|d)
We use an existing implementation of linear least
squares to find a solution (i.e. p(s|w, t) for each t)
2It is not crucial to word sense disambiguate all words in the
text ? a word can be passed to LDA in either its word-sense, dis-
ambiguated, form or in its raw form. While we do not attempt
this in our work, it would be possible to build a model specifi-
cally for noun senses of a word, by including noun senses of the
word and leaving the raw form for any non-noun occurrences.
681
by minimizing the sum of squared differences be-
tween the data values and their corresponding mod-
eled values, i.e., minimizing:
?
d
(
p(s|w, d)?
?
t
p(s|w, t)p(t|d)
)2
3.3 Topic words (TopicWord)
The techniques presented in Sections 3.1 and 3.2
both require the WSD algorithm to annotate a rea-
sonably high proportion of the data used to build the
topic model. For systems which do not rely on word
order, an alternative based on the most likely words
per topic is possible: the LDA algorithm generates
?, a word-topic distribution. It is therefore possible
to extract the most likely words per topic.
To acquire a sense-topic distribution for a topic t,
each target word w is included in a bag of words
which includes the most likely words for t and
the unsupervised WSD algorithm is executed (w is
added to the list if t does not already contain it).
This technique is not applicable to non bag-of-words
WSD algorithms, as structure is absent.
4 Word Sense Disambiguation
Only the topic model documents need to be auto-
matically annotated with the chosen WSD system,
after this, the WSD system is never applied again
(an LDA classification determines the sense distri-
bution) ? this is particularly useful for supervised
system which frequently have a long execution time.
We explore three different types of WSD system:
two versions of a knowledge base based system
(Section 4.1), an unsupervised system (Section 4.2)
and a supervised system (Section 4.3).
4.1 Personalized PageRank (ppr and w2w)
We use the freely available3 Personalized PageRank
algorithm (Agirre and Soroa, 2009) with WordNet
3.0. In Section 5 we present results from two options
of the Personalized PageRank algorithm: ppr, which
performs one PageRank calculation for a whole con-
tent, and w2w, which performs one PageRank cal-
culation for every word in the context to be disam-
biguated.
3Available from http://ixa2.si.ehu.es/ukb/
4.2 WordNet similarity (sim)
We also evaluated another unsupervised approach,
the Perl package WordNet::SenseRelate::AllWords
(Pedersen and Kolhatkar, 2009), which finds senses
of each word in a text based on senses of the sur-
rounding words. The algorithm is invoked with Lesk
similarity (Banerjee and Pedersen, 2002).
4.3 Vector space model (vsm)
An existing vector space model (VSM) based state-
of-the-art supervised WSD system with features de-
rived from the text surrounding the ambiguous word
(Stevenson et al, 2008) is trained on Semcor (Miller
et al, 1993).4
5 Experiments
5.1 Data
The approach is evaluated using a domain-specific
WSD corpus (Koeling et al, 2005) which includes
articles from the FINANCE and SPORTS domains
taken from the Reuters corpus (Rose et al, 2002).
This corpus contains 100 manually annotated in-
stances (from each domain) for 41 words.5
The word-sense LDA topic models are created
from 80,128 documents randomly selected from the
Reuters corpus (this corresponds to a tenth of the en-
tire Reuters corpus). LDA can abstract a model from
a relatively small corpus and a tenth of the Reuters
corpus is much more manageable in terms of mem-
ory and time requirements, particularly given the
need to word sense disambiguate (some part of) each
document in this dataset.6
4A version of Semcor automatically transformed to
WordNet 3.0 available from http://www.cse.unt.edu/
?rada/downloads.html#semcor was used in this work.
5Unfortunately, the entire domain-specific sense disam-
biguated corpus could not be used in the evaluation of
our system, as the released corpus does not link each
annotated sentence to its source document, and it is
not always possible to recover these; approximately 87%
of the data could be used. This dataset is available
at http://staffwww.dcs.shef.ac.uk/people/J.
Preiss/downloads/source_texts.tgz
6In this work, all 80,128 documents were word sense disam-
biguated. However, it would be possible to restrict this set to a
smaller number, as long as a reliable distribution of word senses
per topic could be obtained.
682
ppr w2w sim vsm
Baseline 36 41 23 27
SBTM model 39 43 30 31
LinEq 41 44 ? 33
TopicWord 38 41 ? ?
Table 1: Summary of results based on 150 topics
5.2 Results
Table 1 presents the performance results for the four
WSD algorithms based on 150 topics. A range of
topic values was explored, and 150 topics yielded
highest performance, though the variance between
the performance based on different topics (ranging
from 50 to 250) was very small (0.4% difference to
the average performance with 250 topics, and 3%
with 50). The performance shown indicates the pre-
cision (number correct / number attempted). Recall
is 100% in all cases.
The similarity algorithm (sim) fails on certain
documents and therefore the linear equations tech-
nique could not be applied. The topic word tech-
nique (TopicWord) could not be evaluated using the
similarity algorithm, due to the high sensitivity to
word order within the test paragraph. In addition,
the topic words technique is not applicable to su-
pervised systems, due to its reliance on structured
sentences. The best results with this technique were
obtained with including all likely words with proba-
bilities exceeding 0.001 and smoothing of 0.1 of the
topic document distribution.
Using a Wilcoxon signed-rank test, the results
were found to be significantly better over the orig-
inal algorithms in every case (apart from Topic-
Words). Both the WordNet similarity (sim) and
the VSM approach (vsm) have a lower performance
than the two PPR based WSD algorithms (ppt and
w2w). For example, sim assigns the same (usually
incorrect) sense to all occurrences of the word tie,
while both PPR based algorithms detect an obvious
domain change. The vsm approach suffers from a
lack of training data (only a small number of exam-
ples of each word appear in Semcor), while sim does
not get enough information from the context.
As an interesting aside, the topic models based on
word-sense combinations, as opposed to wordforms
only, are more informative with less overlap. Exam-
ining the word stake annotated with the w2w WSD
algorithm: only topic 1 contains stake among the top
12 terms associated with a topic in the word-sense
model, while 10 topics are found in the wordform
topic model. Table 2 shows the top 12 terms associ-
ated with topics containing the word stake.
Topic Word-based model
39 say, will, company, share, deal, euro-
pean, buy, agreement, stake, new, hun-
gary, oil
63 say, share, united, market, offer, stock,
union, percent, stake, will, point, new
90 say, will, fund, price, london, sell,
stake, indonesia, court, investment,
share, buy
91 say, market, bond, russia, press, party,
stake, russian, country, indonesia, new,
election
97 say, million, bank, uk, percent, share,
stake, world, will, year, central, british
113 say, will, percent, week, billion, last,
italy, plan, stake, year, budget, czech
134 say, china, percent, hong, kong, offi-
cial, stake, billion, report, buy, group,
year
142 say, percent, market, first, bank, rate,
year, dealer, million, money, close,
stake
145 say, will, new, brazil, dollar, group,
percent, stake, year, one, make, do
147 say, yen, forecast, million, parent, mar-
ket, share, will, profit, percent, stake,
group
Sense-based model
1 stake*13286801-n, share*13285176-
n, sell*02242464-v, buy*02207206-v,
have*02204692-v, group*00031264-
n, company*08058098-n,
percent*13817526-n, hold*02203362-
v, deal*01110274-n, shareholder,
interest*13286801-n
Table 2: The presence of stake within the word- and
sense-based topic models
683
6 Conclusion
We present three unsupervised techniques based on
acquiring LDA topics which can be used to improve
the performance of a number of WSD algorithms.
All approaches make use of topic information ob-
tained using LDA and do not require any modifi-
cation of the underlying WSD system. While the
technique is dependent on the accuracy of the WSD
algorithm, it consistently outperforms the baselines
for all four different algorithms.
Acknowledgments
This research was supported by a Google Research
Award. Our thanks also go to the two anonymous
reviewers whose comments have made this paper
much clearer.
References
Agirre, E., de Lacalle, O. L., and Soroa, A. (2009).
Knowledge-based WSD on specific domains: per-
forming better than generic supervised WSD. In Pro-
ceedings of the 21st International Joint Conference on
Artificial Intelligence, pages 1501?1506.
Agirre, E. and Soroa, A. (2009). Personalizing pager-
ank for word sense disambiguation. In Proceedings of
EACL.
Banerjee, S. and Pedersen, T. (2002). An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In Proceedings of the Third International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 135?145.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent
Dirichlet alocation. Journal of Machine Learning Re-
search, 3:993?1022.
Boyd-Graber, J., Blei, D., and Zhu, X. (2007). A topic
model for word sense disambiguation. In Proceedings
of the EMNLP-CoNLL, pages 1024?1033.
Cai, J. F., Lee, W. S., and Teh, Y. W. (2007). Nus-ml:
Improving word sense disambiguation using topic fea-
tures. In Proceedings of SEMEVAL.
Gale, W. A., Church, K. W., and Yarowsky, D. (1992).
One sense per discourse. In Proceedings of the
4th DARPA Speech and Natural Language Workshop,
pages 233?237.
Khapra, M., Kulkarni, A., Sohoney, S., and Bhat-
tacharyya, P. (2010). All words domain adapted WSD:
Finding a middle ground between supervision and
unsupervision. In Proceedings of ACL 2010, pages
1532?1541, Uppsala, Sweden.
Koeling, R., Mccarthy, D., and Carroll, J. (2005). Do-
main specific sense distributions and predominant
sense acquisition. In Proceedings of Joint HLT-
EMNLP05, pages 419?426.
Li, L., Roth, B., and Sporleder, C. (2010). Topic models
for word sense disambiguation and token-based idiom
detection. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1138?1147.
McCarthy, D., Koeling, R., Weeds, J., and Carroll, J.
(2004). Finding predominant senses in untagged text.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 280?
287.
Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.
(1993). A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technology,
pages 303?308.
Navigli, R., Faralli, S., Soroa, A., de Lacalle, O. L., and
Agirre, E. (2011). Two birds with one stone: learn-
ing semantic models for text categorization and word
sense disambiguation. In CIKM, pages 2317?2320.
Pedersen, T. and Kolhatkar, V. (2009). Word-
net::senserelate::allwords - a broad coverageword
sense tagger that maximizes semantic relatedness
(demonstration system). In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
Conference, pages 17?20.
Rose, T. G., Stevenson, M., and Whitehead, M. (2002).
The Reuters corpus volume 1 - from yesterday?s news
to tomorrow?s language resources. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, pages 827?832.
Stevenson, M., Agirre, E., and Soroa, A. (2012). Exploit-
ing domain information for word sense disambigua-
tion of medical documents. Journal of the American
Medical Informatics Association, 19(2):235?240.
Stevenson, M., Guo, Y., Gaizauskas, R., and Martinez,
D. (2008). Knowledge sources for word sense disam-
biguation of biomedical text. In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processing at ACL, pages 80?87.
684
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 1?4,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
DALE: A Word Sense Disambiguation System for Biomedical Documents
Trained using Automatically Labeled Examples
Judita Preiss and Mark Stevenson
Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP, United Kingdom
j.preiss,m.stevenson@dcs.shef.ac.uk
Abstract
Automatic interpretation of documents is
hampered by the fact that language contains
terms which have multiple meanings. These
ambiguities can still be found when language
is restricted to a particular domain, such as
biomedicine. Word Sense Disambiguation
(WSD) systems attempt to resolve these am-
biguities but are often only able to identify the
meanings for a small set of ambiguous terms.
DALE (Disambiguation using Automatically
Labeled Examples) is a supervised WSD sys-
tem that can disambiguate a wide range of
ambiguities found in biomedical documents.
DALE uses the UMLS Metathesaurus as both
a sense inventory and as a source of infor-
mation for automatically generating labeled
training examples. DALE is able to disam-
biguate biomedical documents with the cover-
age of unsupervised approaches and accuracy
of supervised methods.
1 Introduction
Word Sense Disambiguation (WSD) is an impor-
tant challenge for any automatic text processing sys-
tem since language contains ambiguous terms which
can be difficult to interpret. Ambiguous terms that
are found in biomedical documents include words,
phrases and abbreviations (Schuemie et al, 2005).
Identifying the correct interpretation of ambiguous
terms is important to ensure that the text can be pro-
cessed appropriately.
Many WSD systems developed for biomedical
documents are based on supervised learning, for ex-
ample (McInnes et al, 2007; Martinez and Baldwin,
2011); these have the advantage of being more accu-
rate than unsupervised approaches. However, WSD
systems based on supervised learning rely on man-
ually labeled examples consisting of instances of an
ambiguous term marked with their correct interpre-
tations. Manually labeled examples are very expen-
sive to create and are consequently only available for
a few hundred terms, with each new domain (with
its specialist vocabulary) needing new examples la-
beled. The majority of supervised WSD systems are
limited to resolving a small number of ambiguous
terms and, despite their accuracy, are not suitable for
use within applications.
An alternative approach is to use automatically
labeled examples which can be generated without
manual annotation (Leacock et al, 1998). These
have been used to generate an all-words WSD sys-
tem that assigns senses from WordNet (Zhong and
Ng, 2010). For biomedical documents the UMLS
Metathesaurus (Humphreys et al, 1998b) is a more
suitable lexical resource than WordNet and tech-
niques have been developed to create automatically
labeled examples for this resource (Stevenson and
Guo, 2010). However, to date, automatically labeled
examples have only been used as substitutes for am-
biguous terms for which manually labeled examples
are not available, rather than using them to create a
WSD system that can resolve a wider range of am-
biguities in biomedical documents.
DALE (Disambiguation using Automatically La-
beled Examples) is an online WSD system for
biomedical documents that was developed by creat-
ing automatically labeled examples for all ambigu-
ous terms in the UMLS Metathesaurus. DALE is
1
able to identify a meaning for any term that is am-
biguous in the Metathesaurus and therefore has far
greater coverage of ambiguous terms than other su-
pervised WSD systems. Other all-words WSD sys-
tems for biomedical documents are unsupervised
and do not have as high accuracy as supervised ap-
proaches, e.g. (McInnes, 2008; Agirre et al, 2010).
An unsupervised WSD algorithm (Humphreys et al,
1998a) is included in MetaMap (Aronson and Lang,
2010) but is unable to resolve all types of sense dis-
tinction.
2 The DALE System
2.1 Automatically Labeling Examples
DALE assigns Concept Unique Identifiers (CUIs)
from the UMLS Metathesaurus. The WSD algo-
rithm in DALE is based around a supervised algo-
rithm (Stevenson et al, 2008) trained using automat-
ically labeled examples. The examples are gener-
ated using two methods: Monosemous relatives and
Co-occurring concepts (Stevenson and Guo, 2010).
Both approaches take a single CUI, c, as input and
use information from the UMLS Metathesaurus to
search Medline and identify instances of c that can
be used as labeled examples. The difference be-
tween the two approaches is that they make use of
different information from the Metathesaurus.
Both approaches are provided with a set of
ambiguous CUIs from the UMLS Metathesaurus,
which represent the possible meanings of an am-
biguous term, and a target number of training ex-
amples to be generated for each CUI. The UMLS
Metathesaurus contains a number of data files which
are exploited within these techniques, including: 1.
AMBIGLUI: a list of cases where a LUI, a particular
lexical variant of a term, is linked to multiple CUIs;
2. MRCON: list of all strings and concept names in
the Metathesaurus; 3. MRCOC: co-occurring con-
cepts.
For the monosemous relatives approach, the
strings of monosemous LUIs of the target CUI and
its relatives are used to search Medline to retrieve
training examples. The monosemous LUIs related
to a CUI are defined as any LUIs associated with the
CUI in MRCON table and not listed in AMBIGLUI
table. For example, one of the LUIs associated with
CUI ?C0028707? is L0875433 ?Nutrition Science?
in MRCON table. It is not listed in AMBIGLUI ta-
ble and therefore considered to be a monosemous
LUI of CUI ?C0028707?. The string ?Nutrition
Science? can be used to identify examples of CUI
?C0028707?.
The co-occurring concept approach works differ-
ently: instead of using strings of monosemous LUIs
of the target CUI and its relatives, the strings associ-
ated with LUIs of a number of co-occurring CUIs
of the target CUI and its relatives found in MR-
COC table are used. For instance, ?C0025520?,
?C1524024? and ?C0079107? are the top three co-
occurring CUIs of CUI ?C0015677? in MRCOC ta-
ble. The strings associated with LUIs of these
three CUIs can be used to retrieve examples of CUI
?C0015677? by searching for abstracts containing
all the LUIs of the co-occurring CUIs.
These approaches were used to create labeled
examples for ambiguous CUIs in the 2010AB,
2011AA, 2011AB and 2012AA versions of the
UMLS Metathesaurus. Examples could be gener-
ated for 95.2%, 96.2%, 96.2% and 98% of the CUIs
in each version of the Metathesaurus respectively.
Neither technique was able to generate examples for
the remaining CUIs, however none of these CUIs ap-
pear in the corresponding MetaMapped version of
the Medline Baseline Repository (http://mbr.
nlm.nih.gov), suggesting these CUIs do not tend
to be mentioned within documents. 100 examples
were generated for each CUI since using an equal
number of examples for each CUI produces the best
WSD performance in the absence of other informa-
tion about the likelihood of each CUI (Cheng et al,
2012).
The labeled examples are converted into feature
vectors consisting of lemmas of all content words in
the same sentence as the ambiguous word and, in
addition, the lemmas of all content words in a ?4-
word window around it. A single feature vector is
created for each CUI by taking the centroid of the
feature vectors created from the labeled examples of
that CUI. These vectors are stored in the Centroid
Database for later use.
2.2 Word Sense Disambiguation
WSD of an ambiguous term is carried out by com-
piling a list of all its possible CUIs and comparing
their centroids against a feature vector created from
2
Figure 1: DALE system diagram showing the stages in
the WSD process
the sentence containing the ambiguous term. Pro-
cessing is carried out in multiple stages (see Fig.
1). MetaMap (Aronson and Lang, 2010) is applied
to the text to identify ambiguous terms (identify-
ing terms includes some level of multiword detec-
tion) and their possible CUIs (UMLS lookup of the
identified terms). The input text is also fed into a
pipeline to carry out sentence splitting, tokenization,
part-of-speech tagging and morphological analysis.
Information added by this pipeline is used to cre-
ate a feature vector for each ambiguous term identi-
fied by MetaMap. Finally, the Word Sense Disam-
biguation module uses cosine similarity to compare
the centroid of each possible CUI of the ambiguous
term (retrieved from the Centroid Database) with the
ambiguous term?s feature vector (Stevenson et al,
2008). The most similar CUI is selected for each
ambiguous term.
2.3 Online System
DALE is available as a web service with multiple
interfaces:
The Interactive interface enables a user to submit
a piece of text to the system and view the result in an
intuitive way. Terms in the result are marked accord-
ing to their polysemy: blue denotes that it has only
one meaning in Metathesaurus (i.e. is not ambigu-
ous) while green means that it has multiple mean-
ings. Rolling the mouse over the highlighted items
provides access to additional information in a tooltip
style window, including the set of possible CUIs
and their preferred names. Clicking on one of these
CUIs links to the appropriate page from the UMLS
Terminology Services (http://uts.nlm.nih.
gov/). The CUI chosen by the WSD process is
shown underlined at the bottom of the window. The
result is also available in XML format which can be
downloaded by clicking a link in the result page.
The Batch interface is more suitable for disam-
biguating large amounts of texts. A user can upload
plain text files to be processed by DALE using the
batch interface. The results will be sent to user?s
email address in XML format as soon as the system
finishes processing the file. This interface is sup-
ported by a Job management interface. A job is cre-
ated every time a user uploads a file and each job as-
signed the status of being either ?Waiting? or ?Run-
ning?. The user is also emailed a pin code allowing
them to access this interface to check the status of
their jobs and cancel any waiting jobs.
3 Conclusion
This paper describes DALE, a WSD system for
the biomedical domain based on automatically la-
beled examples. The system is able to disambiguate
all ambiguous terms found in the UMLS Metathe-
saurus. A freely accessible web service is available
and offers a set of easy to use interfaces. We intend
to update DALE with new versions of the UMLS
Metathesaurus as they become available.
The DALE system is available at http://kta.
rcweb.dcs.shef.ac.uk/dale/
Acknowledgments
The authors are grateful to Weiwei Cheng for his
work on the development of the original version of
the DALE system. The development of DALE was
funded by the UK Engineering and Physical Sci-
ences Research Council (grants EP/H500170/1 and
EP/J008427/1) and by a Google Research Award.
We would also like to thank the three reviewers
whose feedback has improved the clarity of this pa-
per.
References
E. Agirre, A. Sora, and M. Stevenson. 2010. Graph-
based word sense disambiguation of biomedical docu-
ments. Bioinformatics, 26(22):2889?2896.
A. Aronson and F. Lang. 2010. An overview
of MetaMap: historical perspective and recent ad-
3
Figure 2: Disambiguation results shown in DALE?s Interactive Interface with the ambiguous term ?cold? selected.
DALE shows the three possible CUIs for ?cold? identified by MetaMap with the selected CUI (C0009443) highlighted
vances. Journal of the American Medical Association,
17(3):229?236.
W. Cheng, J. Preiss, and M. Stevenson. 2012. Scal-
ing up WSD with Automatically Generated Examples.
In BioNLP: Proceedings of the 2012 Workshop on
Biomedical Natural Language Processing, pages 231?
239, Montre?al, Canada.
K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks. 1998a.
Description of the LaSIE-II System used in MUC-7.
In Proceedings of the Seventh Message Understanding
Conference (MUC-7).
L. Humphreys, D. Lindberg, H. Schoolman, and G. Bar-
nett. 1998b. The Unified Medical Language System:
An Informatics Research Collaboration. Journal of the
AmericanMedical Informatics Association, 1(5):1?11.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
165.
D. Martinez and T. Baldwin. 2011. Word sense
disambiguation for event trigger word detection in
biomedicine. BMC Bioinformatics, 12(Suppl 2):S4.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
Bridget McInnes. 2008. An unsupervised vector ap-
proach to biomedical term disambiguation: Integrat-
ing UMLS and Medline. In Proceedings of the ACL-
08: HLT Student Research Workshop, pages 49?54,
Columbus, Ohio, June. Association for Computational
Linguistics.
M. Schuemie, J. Kors, and B. Mons. 2005. Word Sense
Disambiguation in the Biomedical Domain. Journal
of Computational Biology, 12, 5:554?565.
M. Stevenson and Y. Guo. 2010. Disambiguation of Am-
biguous Biomedical Terms using Examples Generated
from the UMLS Metathesaurus. Journal of Biomedi-
cal Informatics, 43(5):762?773.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
Z. Zhong and H. Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free
text. In Proceedings of the ACL 2010 System Demon-
strations, pages 78?83, Uppsala, Sweden.
4
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 655?661,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
University Of Sheffield: Two Approaches to Semantic Text Similarity
Sam Biggins, Shaabi Mohammed, Sam Oakley,
Luke Stringer, Mark Stevenson and Judita Priess
Department of Computer Science
University of Sheffield
Sheffield
S1 4DP, UK
{aca08sb, aca08sm, coa07so, aca08ls,
r.m.stevenson, j.preiss}@shef.ac.uk
Abstract
This paper describes the University of
Sheffield?s submission to SemEval-2012 Task
6: Semantic Text Similarity. Two approaches
were developed. The first is an unsupervised
technique based on the widely used vector
space model and information from WordNet.
The second method relies on supervised ma-
chine learning and represents each sentence as
a set of n-grams. This approach also makes
use of information from WordNet. Results
from the formal evaluation show that both ap-
proaches are useful for determining the simi-
larity in meaning between pairs of sentences
with the best performance being obtained by
the supervised approach. Incorporating infor-
mation from WordNet alo improves perfor-
mance for both approaches.
1 Introduction
This paper describes the University of Sheffield?s
submission to SemEval-2012 Task 6: Semantic Text
Similarity (Agirre et al, 2012). The task is con-
cerned with determining the degree of semantic
equivalence between a pair of sentences.
Measuring the similarity between sentences is an
important problem that is relevant to many areas
of language processing, including the identification
of text reuse (Seo and Croft, 2008; Bendersky and
Croft, 2009), textual entailment (Szpektor et al,
2004; Zanzotto et al, 2009), paraphrase detection
(Barzilay and Lee, 2003; Dolan et al, 2004), In-
formation Extraction/Question Answering (Lin and
Pantel, 2001; Stevenson and Greenwood, 2005), In-
formation Retrieval (Baeza-Yates and Ribeiro-Neto,
1999), short answer grading (Pulman and Sukkarieh,
2005; Mohler and Mihalcea, 2009), recommenda-
tion (Tintarev and Masthoff, 2006) and evaluation
(Papineni et al, 2002; Lin, 2004).
Many of the previous approaches to measuring the
similarity between texts have relied purely on lexi-
cal matching techniques, for example (Baeza-Yates
and Ribeiro-Neto, 1999; Papineni et al, 2002; Lin,
2004). In these approaches the similarity of texts is
computed as a function of the number of matching
tokens, or sequences of tokens, they contain. How-
ever, this approach fails to identify similarities when
the same meaning is conveyed using synonymous
terms or phrases (for example, ?The dog sat on the
mat? and ?The hound sat on the mat?) or when the
meanings of the texts are similar but not identical
(for example, ?The cat sat on the mat? and ?A dog
sat on the chair?).
Significant amounts of previous work on text
similarity have focussed on comparing the mean-
ings of texts longer than a single sentence, such as
paragraphs or documents (Baeza-Yates and Ribeiro-
Neto, 1999; Seo and Croft, 2008; Bendersky and
Croft, 2009). The size of these texts means that
there is a reasonable amount of lexical items in each
document that can be used to determine similarity
and failing to identify connections between related
terms may not be problematic. The situation is dif-
ferent for the problem of semantic text similarity
where the texts are short (single sentences). There
are fewer lexical items to match in this case, making
it more important that connections between related
terms are identified. One way in which this infor-
mation has been incorporated in NLP systems has
655
been to make use of WordNet to provide informa-
tion about similarity between word meanings, and
this approach has been shown to be useful for com-
puting text similarity (Mihalcea and Corley, 2006;
Mohler and Mihalcea, 2009).
This paper describes two approaches to the se-
mantic text similarity problem that use WordNet
(Miller et al, 1990) to provide information about
relations between word meanings. The two ap-
proaches are based on commonly used techniques
for computing semantic similarity based on lexical
matching. The first is unsupervised while the other
requires annotated data to train a learning algorithm.
Results of the SemEval evaluation show that the su-
pervised approach produces the best overall results
and that using the information provided by WordNet
leads to an improvement in performance.
The remainder of this paper is organised as fol-
lows. The next section describes the two approaches
for computing semantic similarity between pairs of
sentences that were developed. The system submit-
ted for the task is described in Section 3 and its per-
formance in the official evaluation in Section 4. Sec-
tion 5 contains the conclusions and suggestions for
future work.
2 Computing Semantic Text Similarity
Two approaches for computing semantic similar-
ity between sentences were developed. The first
method, described in Section 2.1, is unsupervised. It
uses an enhanced version of the vector space model
by calculating the similarity between word senses,
and then finding the distances between vectors con-
structed using these distances. The second method,
described in Section 2.2, is based on supervised ma-
chine learning and compares sentences based on the
overlap of the n-grams they contain.
2.1 Vector Space Model
The first approach is inspired by the vector space
model (Salton et al, 1975) commonly used to com-
pare texts in Information Retrieval and Natural Lan-
guage Processing (Baeza-Yates and Ribeiro-Neto,
1999; Manning and Schu?tze, 1999; Jurafsky and
Martin, 2009).
2.1.1 Creating vectors
Each sentence is tokenised, stop words removed
and the remaining words lemmatised using NLTK
(Bird et al, 2009). (The WordPunctTokenizer
and WordNetLemmatizer are applied.) Binary
vectors are then created for each sentence.
The similarity between sentences can be com-
puted by comparing these vectors using the cosine
metric. However, this does not take account of
words with similar meanings, such as ?dog? and
?hound? in the sentences ?The dog sat on the mat?
and ?The hound sat on the mat?. To take account
of these similarities WordNet-based similarity mea-
sures are used (Patwardhan and Pedersen, 2006).
Any terms that occur in only one of the sentences
do not contribute to the similarity score since they
will have a 0 value in the binary vector. Any words
with a 0 value in one of the binary vectors are com-
pared with all of the words in the other sentence and
the similarity values computed. The highest similar-
ity value is selected and use to replace the 0 value
in that vector, see Figure 1. (If the similarity score
is below the set threshold of 0.5 then the similarity
value is not used and in these cases the 0 value re-
mains unaltered.) This substitution of 0 values in the
vectors ensures that similarity between words can be
taken account of when computing sentence similar-
ity.
Figure 1: Determining word similarity values for
vectors
Various techniques were explored for determining
the similarity values between words. These are de-
scribed and evaluated in Section 2.1.3.
2.1.2 Computing Sentence Similarity
The similarity between two sentences is com-
puted using the cosine metric. Since the cosine met-
ric is a distance measure, which returns a score of 0
for identical vectors, its complement is used to pro-
656
duce the similarity score. This score is multiplied by
5 in order to generate a score in the range required
for the task.
2.1.3 Computing Word Similarity
The similarity values for the vectors are computed
by first disambiguating each sentence and then ap-
plying a similarity measure. Various approaches for
carrying out these tasks were explored.
Word Sense Disambiguation Two simple and
commonly used techniques for Word Sense
Disambiguation were applied.
Most Frequent Sense (MFS) simply selects
the first sense in WordNet, i.e., the most
common occurring sense for the word.
This approach is commonly used as a
baseline for word sense disambiguation
(McCarthy et al, 2004).
Lesk (1986) chooses a synset by comparing its
definition against the sentence and select-
ing the one with the highest number of
words in common.
Similarity measures WordNet-based similarity
measures have been found to perform well
when used in combination with text similarity
measures (Mihalcea and Corley, 2006) and
several of these were compared. Implementa-
tions of these measures from the NLTK (Bird
et al, 2009) were used.
Path Distance uses the length of the shortest
path between two senses to determine the
similarity between them.
Leacock and Chodorow (1998) expand upon
the path distance similarity measure by
scaling the path length by the maximum
depth of the WordNet taxonomy.
Resnik (1995) makes use of techniques from
Information Theory. The measure of re-
latedness between two concepts is based
on the Information Content of the Least
Common Subsumer.
Jiang and Conrath (1997) also uses the In-
formation Content of the two input
synsets.
Lin (1998) uses the same values as Jiang and
Conrath (1997) but takes the ratio of the
shared information content to that of the
individual concepts.
Results produced by the various combinations of
word sense disambiguation strategy and similarity
measures are shown in Table 1. This table shows
the Pearson correlation of the system output with the
gold standard over all of the SemEval training data.
The row labelled ?Binary? shows the results using
binary vectors which are not augmented with any
similarity values. The remainder of the table shows
the performance of each of the similarity measures
when the senses are selected using the two word
sense disambiguation algorithms.
Metric MFS Lesk
Binary 0.657
Path Distance 0.675 0.669
Leacock and Chodorow (1998) 0.087 0.138
Resnik (1995) 0.158 0.153
Jiang and Conrath (1997) 0.435 0.474
Lin (1998) 0.521 0.631
Table 1: Performance of Vector Space Model us-
ing various disambiguation strategies and similarity
measures
The results in this table show that the only simi-
larity measure that leads to improvement above the
baseline is the path measure. When this is applied
there is a modest improvement over the baseline for
each of the word sense disambiguation algorithms.
However, all other similarity measures lead to a drop
in performance. Overall there seems to be little dif-
ference between the performance of the two word
sense disambiguation algorithms. The best perfor-
mance is obtained using the paths distance and MFS
disambiguation.
Table 2 shows the results of the highest scoring
method broken down by the individual corpora used
for the evaluation. There is a wide range between the
highest (0.726) and lowest (0.485) correlation scores
with the best performance being obtained for the
MSRvid corpus which contains short, simple sen-
tences.
657
Metric Correlation
MSRpar 0.591
MSRvid 0.726
SMTeuroparl 0.485
Table 2: Correlation scores across individual cor-
pora using Path Distance and Most Frequent Sense.
2.2 Supervised Machine Learning
For the second approach the sentences are repre-
sented as sets of n-grams of varying length, a com-
mon approach in text comparison applications which
preserves some information about the structure of
the document. However, like the standard vector
space model (Section 2.1) this technique also fails to
identify similarity between texts when an alternative
choice of lexical item is used to express the same,
or similar, meaning. To avoid this problem Word-
Net is used to generate sets of alternative n-grams.
After the n-grams have been generated for each sen-
tence they are augmented with semantic alternatives
created using WordNet (Section 2.2.1). The overlap
scores between the n-grams from the two sentences
are used as features for a supervised learning algo-
rithm (Section 2.2.2).
2.2.1 Generating n-grams
Preprocessing is carried out using NLTK. Each
sentence is tokenised, lemmatised and stop words
removed. A set of n-grams are then extracted from
each sentence. The set of n-grams for the sentence
S is referred to as So.
For every n-gram in So a list of alternative n-
grams is generated using WordNet. Each item in
the n-gram is considered in turn and checked to de-
termine whether it occurs in WordNet. If it does
then a set of alternative lexical items is constructed
by combining all terms that are found in all synsets
containing that item as well as their immediate hy-
pernyms and hyponyms of the terms. An additional
n-gram is created for each item in this set of alterna-
tive lexical items by substituting each for the origi-
nal term. This set of expanded n-grams is referred to
as Sa.
2.2.2 Sentence Comparison
Overlap metrics to determine the similarity be-
tween the sets of n-grams are used to create features
for the learning algorithm. For two sentences, S1
and S2, four sets of n-grams are compared: S1o,
S2o, S1a and S2a (i.e., the n-grams extracted di-
rectly from sentences S1 and S2 as well as the mod-
ified versions created using WordNet).
The n-grams that are generated using WordNet
(Sa) are not as important as the original n-grams
(So) for determining the similarity between sen-
tences and this is accounted for by generating three
different scores reflecting the overlap between the
two sets of n-grams for each sentence. These scores
can be expressed using the following equations:
|S1o ? S2o|
?
|S1o| ? |S2o|
(1)
|(S1o ? S2a)?(S2o ? S1a)|
?
|(S1o ? S2a)| ? |(S2o ? S1a)|
(2)
|S1a ? S2a|
?
|S1a| ? |S2a|
(3)
Equation 1 is the cosine measure applied to the
two sets of original n-grams, equation 2 compares
the original n-grams in each sentence with the alter-
native n-grams in the other while equation 3 com-
pares the alternative n-grams with each other.
Other features are used in addition to these sim-
ilarity scores: the mean length of S1 and S2, the
difference between the lengths of S1 and S2 and the
corpus label (indicating which part of the SemEval
training data the sentence pair was drawn from). We
found that these additional features substantially in-
crease the performance of our system, particularly
the corpus label.
3 University of Sheffield?s entry for Task 6
Our entry for this task consisted of three runs using
the two approaches described in Section 2.
Run 1: Vector Space Model (VS) The first run
used the unsupervised vector space approach (Sec-
tion 2.1). Comparison of word sense disambiguation
strategies and semantic similarity measures on the
training data showed that the best results were ob-
tained using the Path Distance Measure combined
658
with the Most Frequent Sense approach (see Ta-
bles 1 and 2) and these were used for the official
run. Post evaluation analysis also showed that this
strategy produced the best performance on the test
data.
Run 2: Machine Learning (NG) The second
run used the supervised machine learning approach
(Section 2.2.2). The various parameters used by
this approach were explored using 10-fold cross-
validation applied to the SemEval training data. We
varied the lengths of the n-grams generated, exper-
imented with various pre-processing strategies and
machine learning algorithms. The best performance
was obtained using short n-grams, unigrams and bi-
grams, and these were used for the official run. In-
cluding longer n-grams did not lead to any improve-
ment in performance but created significant com-
putational cost due to the number of alternative n-
grams that were created using WordNet. When
the pre-processing strategies were compared it was
found that the best performance was obtained by ap-
plying both stemming and stop word removal before
creating n-grams and this approach was used in the
official run. The Weka1 LinearRegression al-
gorithm was used for the official run and a single
model was created by training on all of the data pro-
vided for the task.
Run 3: Hybrid (VS + NG) The third run is a
hybrid combination of the two methods. The su-
pervised approach (NG) was used for the three data
sets that had been made available in the training data
(MSRpar, MSRvid and SMT-eur) while the vector
space model (VS) was used for the other two data
sets. This strategy was based on analysis of perfor-
mance of the two approaches on the training data.
The NG approach was found to provide the best
performance. However it was sensitive to the data
set from which the training data was obtained from
while VS, which does not require training data, is
more robust.
A diagram depicting the various components of
the submitted entry is shown in Figure 2.
4 Evaluation
The overall performance (ALLnrm) of NG, VG and
the hybrid systems is significantly higher than the
1http://www.cs.waikato.ac.nz/ml/weka/
Figure 2: System Digram for entry
official baseline (see Table 3). The table also in-
cludes separate results for each of the evaluation
corpora (rows three to seven): the unsupervised VS
model performance is significantly higher than the
baseline (p-value of 0.06) over all corpus types, as is
that of the hybrid model.
However, the performance of the supervised NG
model is below the baseline for the (unseen in train-
ing data) SMT-news corpus. Given a pair of sen-
tences from an unknown source, the algorithm em-
ploys a model trained on all data combined (i.e.,
omits the corpus information), which may resemble
the input (On-WN) or it may not (SMT-news).
After stoplist removal, the average sentence
length within MSRvid is 4.5, whereas it is 6.0 and
6.9 in MSRpar and SMT-eur respectively, and thus
the last two corpora are expected to form better train-
ing data for each other. The overall performance on
the MSRvid data is higher than for the other cor-
pora, which may be due to the small number of ad-
jectives and the simpler structure of the shorter sen-
tences within the corpus.
The hybrid system, which selects the supervised
system (NG)?s output when the test sentence pair
is drawn from a corpus within the training data
659
Corpus Baseline Vector Space (VS) Machine Learning (NG) Hybrid (NG+VS)
ALL .3110 .6054 .7241 .6485
ALLnrm .6732 .7946 .8169 .8238
MSRpar .4334 .5460 .5166 .5166
MSRvid .2996 .7241 .8187 .8187
SMT-eur .4542 .4858 .4859 .4859
On-WN .5864 .6676 .6390 .6676
SMT-news .3908 .4280 .2089 .4280
Table 3: Correlation scores from official SemEval results
Rank (/89) Rank Ranknrm RankMean
Baseline 87 85 70
Vector Space (VS) 48 44 29
Machine Learning (NG) 17 18 37
Hybrid 34 15 20
Table 4: Ranks from official SemEval results
and selects the unsupervised system (VS)?s answer
otherwise, outperforms both systems in combina-
tion. Contrary to expectations, the supervised sys-
tem did not always outperform VS on phrases based
on training data ? the performance of VS on MSR-
par, with its long and complex sentences, proved
to be slightly higher than that of NG. However, the
unsupervised system was clearly the correct choice
when the source was unknown.
5 Conclusion and Future Work
Two approaches for computing semantic similarity
between sentences were explored. The first, unsu-
pervised approach, uses a vector space model and
computes similarity between sentences by compar-
ing vectors while the second is supervised and rep-
resents the sentences as sets of n-grams. Both
approaches used WordNet to provide information
about similarity between lexical items. Results from
evaluation show that the supervised approach pro-
vides the best results on average but also that per-
formance of the unsupervised approach is better for
some data sets. The best overall results for the Se-
mEval evaluation were obtained using a hybrid sys-
tem that attempts to choose the most suitable ap-
proach for each data set.
The results reported here show that the semantic
text similarity task can be successfully approached
using lexical overlap techniques augmented with
limited semantic information derived from Word-
Net. In future, we would like to explore whether
performance can be improved by applying deeper
analysis to provide information about the structure
and semantics of the sentences being compared. For
example, parsing the input sentences would provide
more information about their structure than can be
obtained by representing them as a bag of words or
set of n-grams. We would also like to explore meth-
ods for improving performance of the n-gram over-
lap approach and making it more robust to different
data sets.
Acknowledgements
This research has been supported by a Google Re-
search Award.
References
E. Agirre, D. Cer, M Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012), in
conjunction with the First Joint Conference on Lexical
and Computational Semantics (*SEM 2012).
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley Longman Lim-
ited, Essex.
660
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics.
M. Bendersky and W.B. Croft. 2009. Finding text reuse
on the web. In Proceedings of the Second ACM Inter-
national Conference on Web Search and Data Mining,
pages 262?271. ACM.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Proceed-
ings of Coling 2004, pages 350?356, Geneva, Switzer-
land.
J.J. Jiang and D.W. Conrath. 1997. Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X).
D. Jurafsky and J. Martin. 2009. Speech and Language
Processing. Pearson, second edition.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332. In C. Fellbaum (Ed.), MIT
Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings of ACM SIG-
DOC Conference, pages 24?26, Toronto, Canada.
D. Lin and P. Pantel. 2001. Discovery of interence rules
for question answering. Natural Language Engineer-
ing, 7(4):343?360.
D. Lin. 1998. An information-theoretic definition of
similarity. In In Proceedings of the 15th International
Conference on Machine Learning, pages 296?304.
C. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July.
C. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press,
Cambridge, MA.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Lingusitics (ACL-2004), pages
280?287, Barcelona, Spain.
R. Mihalcea and C. Corley. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In In AAAI06, pages 775?780.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. WordNet: An On-line Lexi-
cal Database. International Journal of Lexicography,
3(4):235?312.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 567?575,
Athens, Greece.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
311?318, Philadelphia, Pennsylvania, USA.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concept. In Proceedings of the workshop on
?Making Sense of Sense: Bringing Psycholinguistics
and Computational Linguistics Together? held in con-
junction with the EACL 2006, pages 1?8.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic
short answer marking. In Proceedings of the Second
Workshop on Building Educational Applications Us-
ing NLP, pages 9?16, Ann Arbor, Michigan.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613?620.
J. Seo and W.B. Croft. 2008. Local text reuse detection.
In Proceedings of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 571?578.
M. Stevenson and M. Greenwood. 2005. A Semantic
Approach to IE Pattern Induction. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL-05), pages 379?386, Ann
Arbour, MI.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment relations.
In Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing, pages 41?
48, Barcelona, Spain.
N. Tintarev and J. Masthoff. 2006. Similarity for
news recommender systems. In In Proceedings of the
AH?06 Workshop on Recommender Systems and Intel-
ligent User Interfaces.
F.M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine learning approach to textual entail-
ment recognition. Natural Language Engineering, 15-
04:551?582.
661
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 80?84, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Distinguishing Common and Proper Nouns
Judita Preiss and Mark Stevenson
{j.preiss, r.m.stevenson}@sheffield.ac.uk
Department of Computer Science,
University of Sheffield
211 Portobello, Sheffield S1 4DP
United Kingdom
Abstract
We describe a number of techniques for auto-
matically deriving lists of common and proper
nouns, and show that the distinction between
the two can be made automatically using a
vector space model learning algorithm. We
present a direct evaluation on the British Na-
tional Corpus, and application based evalua-
tions on Twitter messages and on automatic
speech recognition (where the system could be
employed to restore case).
1 Introduction
Some nouns are homographs (they have the same
written form, but different meaning) which can be
used to denote either a common or proper noun, for
example the word apple in the following examples:
(1) Apple designs and creates iPod (2) The Apple II
series is a set of 8-bit home computers (3) The apple
is the pomaceous fruit of the apple tree (4) For apple
enthusiasts ? tasting notes and apple identification.
The common and proper uses are not always as
clearly distinct as in this example; for example, a
specific instance of a common noun, e.g., District
Court turns court into a proper noun.
While heuristically, proper nouns often start with
a capital letter in English, capitalization can be in-
consistent, incorrect or omitted, and the presence or
absence of an article cannot be relied on.
The problem of distinguishing between common
and proper usages of nouns has not received much
attention within language processing, despite being
an important component for many tasks including
machine translation (Lopez, 2008; Hermjakob et al,
2008), sentiment analysis (Pang and Lee, 2008; Wil-
son et al, 2009) and topic tracking (Petrovic? et al,
2010). Approaches to the problem also have appli-
cations to tasks such as web search (Chen et al,
1998; Baeza-Yates and Ribeiro-Neto, 2011), and
case restoration (e.g., in automatic speech recogni-
tion output) (Baldwin et al, 2009), but frequently
involve the manual creation of a gazeteer (a list of
proper nouns), which suffer not only from omissions
but also often do not allow the listed words to as-
sume their common role in text.
This paper presents methods for generating lists
of nouns that have both common and proper usages
(Section 2) and methods for identifying the type of
usage (Section 3) which are evaluated using data de-
rived automatically from the BNC (Section 4) and
on two applications (Section 5). It shows that it is
difficult to automatically construct lists of ambigu-
ous nouns but also that they can be distinguished ef-
fectively using standard features from Word Sense
Disambiguation.
2 Generating Lists of Nouns
To our knowledge, no comprehensive list of com-
mon nouns with proper noun usage is available. We
develop a number of heuristics to generate such lists
automatically.
Part of speech tags A number of part of speech
(PoS) taggers assign different tags to common and
proper nouns. Ambiguous nouns are identified by
tagging a corpus and extracting those that have
had both tags assigned, together with the frequency
of occurrence of the common/proper usage. The
CLAWS (Garside, 1987) and the RASP taggers
80
(Briscoe et al, 2006) were applied to the British Na-
tional Corpus (BNC) (Leech, 1992) to generate the
lists BNCclaws and BNCrasp respectively. In addi-
tion the RASP tagger was also run over the 1.75 bil-
lion word Gigaword corpus (Graff, 2003) to extract
the list Gigaword.
Capitalization Nouns appearing intra-
sententially with both lower and upper case
first letters are assumed to be ambiguous. This
technique is applied to the 5-grams from the Google
corpus (Brants and Franz, 2006) and the BNC
(creating the lists 5-grams and BNCcaps).
Wikipedia includes disambiguation pages for
ambiguous words which provide information about
their potential usage. Wikipedia pages for nouns
with senses (according to the disambiguation page)
in a set of predefined categories were identified to
form the list Wikipedia.
Named entity recognition The Stanford Named
Entity Recogniser (Finkel et al, 2005) was run over
the BNC and any nouns that occur in the corpus with
both named entity and non-named entity tags are ex-
tracted to form the list Stanford.
WordNet The final heuristic makes use of Word-
Net (Fellbaum, 1998) which lists nouns that are of-
ten used as proper nouns with capitalisation. Nouns
which appeared in both a capitalized and lowercased
form were extracted to create the list WordNet.
Table 1 shows the number of nouns identified by
each technique in the column labeled words which
demonstrates that the number of nouns identified
varies significantly depending upon which heuris-
tic is used. A pairwise score is also shown to in-
dicate the consistency between each list and two ex-
ample lists, BNCclaws and Gigaword. It can be seen
that the level of overlap is quite low and the various
heuristics generate quite different lists of nouns. In
particular the recall is low, in almost all cases less
than a third of nouns in one list appear in the other.
One possible reason for the low overlap between
the noun lists is mistakes by the heuristics used to
extract them. For example, if a PoS tagger mistak-
enly tags just one instance of a common noun as
proper then that noun will be added to the list ex-
tracted by the part of speech heuristic. Two filter-
ing schemes were applied to improve the accuracy of
the lists: (1) minimum frequency of occurrence, the
noun must appear more than a set number of times
words BNCclaws Gigaword
P R P R
BNCclaws 41,110 100 100 31 2
BNCrasp 20,901 52 27 45 17
BNCcaps 18,524 56 26 66 21
5-grams 27,170 45 29 59 28
Gigaword 57,196 22 31 100 100
Wikipedia 7,351 49 9 59 8
WordNet 798 75 1 68 1
Stanford 64,875 43 67 26 29
Table 1: Pairwise comparison of lists. The nouns in each
list are compared against the BNCclaws and Gigaword
lists. Results are computed for P(recision) and R(ecall).
in the corpus and (2) bias, the least common type of
noun usage (i.e., common or proper) must account
for more than a set percentage of all usages.
We experimented with various values for these fil-
ters and a selection of results is shown in Table 2,
where freq is the minimum frequency of occurrence
filter and bias indicates the percentage of the less
frequent noun type.
bias freq words BNCclaws Gigaword
P R P R
BNCclaws 40 100 274 100 1 53 1
BNCrasp 30 100 253 94 1 85 0
5-grams 40 150 305 80 1 67 0
Stanford 40 200 260 87 1 47 0
Table 2: Pairwise comparison of lists with filtering
Precision (against BNCclaws) increased as the fil-
ters become more aggressive. However comparison
with Gigaword does not show such high precision
and recall is extremely low in all cases.
These experiments demonstrate that it is difficult
to automatically generate a list of nouns that exhibit
both common and proper usages. Manual analy-
sis of the lists generated suggest that the heuristics
can identify ambiguous nouns but intersecting the
lists results in the loss of some obviously ambigu-
ous nouns (however, their union introduces a large
amount of noise). We select nouns from the lists
created by these heuristics (such that the distribu-
tion of either the common or proper noun sense in
the data was not less than 45%) for experiments in
the following sections.1
1The 100 words selected for our evaluation are available at
http://pastehtml.com/view/cjsbs4xvl.txt
81
3 Identifying Noun Types
We cast the problem of distinguishing between com-
mon and proper usages of nouns as a classification
task and develop the following approaches.
3.1 Most frequent usage
A naive baseline is supplied by assigning each word
its most frequent usage form (common or proper
noun). The most frequent usage is derived from the
training portion of labeled data.
3.2 n-gram system
A system based on n-grams was implemented using
NLTK (Bird et al, 2009). Five-grams, four-grams,
trigrams and bigrams from the training corpus are
matched against a test corpus sentence, and results
of each match are summed to yield a preferred use in
the given context with a higher weight (experimen-
tally determined) being assigned to longer n-grams.
The system backs off to the most frequent usage (as
derived from the training data).
3.3 Vector Space Model (VSM)
Distinguishing between common and proper nouns
can be viewed as a classification problem. Treating
the problem in this manner is reminiscent of tech-
niques commonly employed in Word Sense Disam-
biguation (WSD). Our supervised approach is based
on an existing WSD system (Agirre and Martinez,
2004) that uses a wide range of features:
? Word form, lemma or PoS bigrams and tri-
grams containing the target word.
? Preceding or following lemma (or word form)
content word appearing in the same sentence as
the target word.
? High-likelihood, salient, bigrams.
? Lemmas of all content words in the same sen-
tence as the target word.
? Lemmas of all content words within a?4 word
window of the target word.
? Non stopword lemmas which appear more than
twice throughout the corpus.
Each occurrence of a common / proper noun is
represented as a binary vector in which each position
indicates the presence or absence of a feature. A
centroid vector is created during the training phase
for the common noun and the proper noun instances
of a word. During the test phase, the centroids are
compared to the vector of each test instance using
the cosine metric, and the word is assigned the type
of the closest centroid.
4 Evaluation
The approaches described in the previous section are
evaluated on two data sets extracted automatically
from the BNC. The BNC-PoS data set is created
using the output from the CLAWS tagger. Nouns
assigned the tag NP0 are treated as proper nouns
and those assigned any other nominal tag as com-
mon nouns. (According to the BNC manual the
NP0 tag has a precision 83.99% and recall 97.76%.2)
This data set consists of all sentences in the BNC in
which the target word appears. The second data set,
BNC-Capital, is created using capitalisation infor-
mation and consists of instances of the target noun
that do not appear sentence-initially. Any instances
that are capitalised are treated as proper nouns and
those which are non-capitalised as common nouns.
Experiments were carried out using capitalised
and decapitalized versions of the two test corpora.
The decapitalised versions by lowercasing each cor-
pus and using it for training and testing. Results are
presented in Table 3. Ten fold cross validation is
used for all experiments: i.e. 9/10th of the corpus
were used to acquire the training data centroids and
1/10th was used for evaluation. The average perfor-
mance over the 10 experiments is reported.
The vector space model (VSM) outperforms other
approaches on both corpora. Performance is partic-
ularly high when capitalisation is included (VSM w
caps). However, this approach still outperforms the
baseline without case information (VSM w/o caps),
demonstrating that using this simple approach is less
effective than making use of local context.
2No manual annotation of common and proper nouns in this
corpus exists and thus an exact accuracy figure for this corpus
cannot be obtained.
82
Gold standard
BNC-PoS BNC-Capital
Most frequent 79% 67%
n-gram w caps 80% 77%
n-gram w/o caps 68% 56%
VSM w caps 90% 100%
VSM w/o caps 86% 80%
Table 3: BNC evaluation results
5 Applications
We also carried out experiments on two types of
text in which capitalization information may not be
available: social media and ASR output.
5.1 Twitter
As demonstrated in the BNC based evaluations, the
system can be applied to text which does not contain
capitalization information to identify proper nouns
(and, as a side effect, enable the correction of capi-
talization). An example of such a dataset are the (up
to) 140 character messages posted on Twitter.
There are some interesting observations to be
made on messages downloaded from Twitter. Al-
though some users choose to always tweet in lower
case, the overall distribution of capitalization in
tweets is high for the 100 words selected in Section 2
and only 3.7% of the downloaded tweets are entirely
lower case. It also appeared that users who capital-
ize, do so fairly consistently.
This allows the creation of a dataset based on
downloaded Twitter data3:
1. Identify purely lower case tweets containing
the target word. These will form the test data
(and are manually assigned usage).
2. Any non-sentence initial occurrences of the tar-
get word are used as training instances: lower
case indicating a common instance, upper case
indicating a proper instance.
14 words4 were randomly selected from the list
used in Section 4 and their lowercase tweet instances
were manually annotated by a single annotator. The
3http://search.twitter.com/api
4abbot, bull, cathedral, dawn, herald, justice, knight, lily,
lodge, manor, park, president, raven and windows
Training corpus MF n-grams VSM
Twitter 59% 40% 60%
BNCclaw decap 59% 44% 79%
Table 4: Results on the Twitter data
average proportion of proper nouns in the test data
was 59%.
The results for the three systems are presented in
Table 4. As the length of the average sentence in the
Twitter data is only 15 words (compared to 27 words
in the BNCclaws data for the same target words),
the Twitter data is likely to be suffering sparseness
issues. This hypothesis is partly supported by the in-
crease in performance when the BNCclaws decapi-
talized data is added to the training data, however,
the performance of the n-gram system remains be-
low the most frequent use. On closer examination,
this is likely due to the skew in the data ? there are
many more examples for the common use of each
noun, and thus each context is much more likely to
have been seen in this setting.
5.2 Automatic speech recognition
Most automatic speech recognition (ASR) systems
do not provide capitalization. However, our sys-
tem does not rely on capitalization information, and
therefore can identify proper / common nouns even
if capitalization is absent. Also, once proper nouns
are identified, the system can be used to restore case
? a feature which allows an evaluation to take place
on this dataset. We use the TDT2 Test and Speech
corpus (Cieri et al, 1999), which contains ASR and
a manually transcribed version of news texts from
six different sources, to demonstrate the usefulness
of this system for this task.
The ASR corpus is restricted to those segments
which contain an equal number of target word oc-
currences in the ASR text and the manually tran-
scribed version, and all such segments are extracted.
The gold standard, and the most frequent usage, are
drawn from the manually transcribed data.
Again, results are based on an average perfor-
mance obtained using a ten fold cross validation.
Three versions of training data are used: the 9/10 of
ASR data (with labels provided by the manual tran-
scription), the equivalent 9/10 of lowercased manu-
83
Training corpus MF n-grams VSM
Manual 66% 42% 73%
ASR 63% 41% 79%
Table 5: Results on the ASR data
ally transcribed data, and a combination of the two.
The results can be seen in Table 5. The perfor-
mance rise obtained with the VSM model when the
ASR data is used is likely due to the repeated errors
within this, which will not be appearing in the man-
ually transcribed texts. The n-gram performance is
greatly affected by the low volume of training data
available, and again, a large skew within this.
6 Conclusion
We automatically generate lists of common and
proper nouns using a number of different techniques.
A vector space model technique for distinguish-
ing common and proper nouns is found to achieve
high performance when evaluated on the BNC. This
greatly outperforms a simple n-gram based system,
due to its better adaptability to sparse training data.
Two application based evaluations also demonstrate
the system?s performance and as a side effect the
system could serve as a technique for automatic case
restoration.
Acknowledgments
The authors are grateful to the funding for this
research received from Google (Google Research
Award) and the UK Engineering and Physical Sci-
ences Research Council (EP/J008427/1).
References
Agirre, E. and Martinez, D. (2004). The Basque Coun-
try University system: English and Basque tasks.
In Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, pages 44?48.
Baeza-Yates, R. and Ribeiro-Neto, B. (2011). Modern
Information Retrieval: The Concepts and Technology
Behind Search. Addison Wesley Longman Limited,
Essex.
Baldwin, T., Paul, M., and Joseph, A. (2009). Restoring
punctuation and casing in English text. In Proceedings
of the 22nd Australian Joint Conference on Artificial
Intelligence (AI09), pages 547?556.
Bird, S., Klein, E., and Loper, E. (2009). Natural Lan-
guage Processing with Python ? Analyzing Text with
the Natural Language Toolkit. O?Reilly.
Brants, T. and Franz, A. (2006). Web 1T 5-gram v1.
Briscoe, T., Carroll, J., and Watson, R. (2006). The sec-
ond release of the RASP system. In Proceedings of the
COLING/ACL 2006 Interactive Presentation Sessions.
Chen, H., Huang, S., Ding, Y., and Tsai, S. (1998).
Proper name translation in cross-language information
retrieval. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, Volume 1, pages 232?236, Montreal, Canada.
Cieri, C., Graff, D., Liberman, M., Martey, N., and
Strassel, S. (1999). The TDT-2 text and speech cor-
pus. In Proceedings of DARPA Broadcast News Work-
shop, pages 57?60.
Fellbaum, C., editor (1998). WordNet: An Electronic
Lexical Database and some of its Applications. MIT
Press, Cambridge, MA.
Finkel, J. R., Grenager, T., and Manning, C. (2005). In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proceedings of
the 43nd Annual Meeting of the Association for Com-
putational Linguistics, pages 363?370.
Garside, R. (1987). The CLAWS word-tagging system.
In Garside, R., Leech, G., and Sampson, G., editors,
The Computational Analysis of English: A Corpus-
based Approach. London: Longman.
Graff, D. (2003). English Gigaword. Technical report,
Linguistic Data Consortium.
Hermjakob, U., Knight, K., and Daume? III, H. (2008).
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio.
Leech, G. (1992). 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
Lopez, A. (2008). Statistical machine translation. ACM
Computing Surveys, 40(3):1?49.
Pang, B. and Lee, L. (2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, Vol. 2(1-2):pp. 1?135.
Petrovic?, S., Osborne, M., and Lavrenko, V. (2010).
Streaming first story detection with application to twit-
ter. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
181?189, Los Angeles, California.
Wilson, T., Wiebe, J., and Hoffman, P. (2009). Recogniz-
ing contextual polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(5).
84
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 231?239,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Scaling up WSD with Automatically Generated Examples
Weiwei Cheng, Judita Preiss and Mark Stevenson
Department of Computer Science,
Sheffield University,
Regent Court, 211 Portobello,
Sheffield, S1 4DP
United Kingdom
{W.Cheng, J.Preiss, M.Stevenson}@dcs.shef.ac.uk
Abstract
The most accurate approaches to Word Sense
Disambiguation (WSD) for biomedical docu-
ments are based on supervised learning. How-
ever, these require manually labeled training
examples which are expensive to create and
consequently supervised WSD systems are
normally limited to disambiguating a small set
of ambiguous terms. An alternative approach
is to create labeled training examples automat-
ically and use them as a substitute for manu-
ally labeled ones. This paper describes a large
scale WSD system based on automatically la-
beled examples generated using information
from the UMLS Metathesaurus. The labeled
examples are generated without any use of la-
beled training data whatsoever and is therefore
completely unsupervised (unlike some previ-
ous approaches). The system is evaluated on
two widely used data sets and found to outper-
form a state-of-the-art unsupervised approach
which also uses information from the UMLS
Metathesaurus.
1 Introduction
The information contained in the biomedical liter-
ature that is available in electronic formats is use-
ful for health professionals and researchers (West-
brook et al, 2005). The amount is so vast that
it is difficult for researchers to identify informa-
tion of interest without the assistance of automated
tools (Krallinger and Valencia, 2005). However,
processing these documents automatically is made
difficult by the fact that they contain terms that
are ambiguous. For example, ?culture? can mean
?laboratory procedure? (e.g. ?In peripheral blood
mononuclear cell culture?) or ?anthropological cul-
ture? (e.g. ?main accomplishments of introducing a
quality management culture?). These lexical ambi-
guities are problematic for language understanding
systems.
Word sense disambiguation (WSD) is the process
of automatically identifying the meanings of am-
biguous terms. Some WSD systems for the biomed-
ical domain are only able to disambiguate a small
number of ambiguous terms (see Section 2). How-
ever, for WSD systems to be useful in applications
they should be able to disambiguate all ambiguous
terms. One way to create such a WSD system is to
automatically create the labeled data that is used to
train supervised WSD systems. Several approaches
(Liu et al, 2002; Stevenson and Guo, 2010; Jimeno-
Yepes and Aronson, 2010) have used information
from the UMLS Metathesaurus1 to create labeled
training data that have successfully been used to cre-
ate WSD systems.
A key decision for any system that automatically
generates labeled examples is the number of exam-
ples of each sense to create, known as the bias of the
data set. It has been shown that the bias of a set of la-
beled examples affects the performance of the WSD
system it is used to train (Mooney, 1996; Agirre and
Mart??nez, 2004b). Some of the previous approaches
to generating labeled data relied on manually anno-
tated examples to determine the bias of the data sets
and were therefore not completely unsupervised.
This paper describes the development of a large
scale WSD system that is able to disambiguate all
1http://www.nlm.nih.gov/research/umls/
231
terms that are ambiguous in the UMLS Metathe-
saurus. The system relies on labeled examples that
are created using information from UMLS. Various
bias options are explored, including ones that do not
make use of information from manually labeled ex-
amples, and thus we can create a completely unsu-
pervised system. Evaluation is carried out on two
standard datasets (the NLM-WSD and MSH-WSD
corpora). We find that WSD systems can be cre-
ated without using any information from manually
labeled examples and that their performance is bet-
ter than a state-of-the-art unsupervised approach.
The remainder of this paper is organized as fol-
lows. Previous approaches to WSD in biomedical
documents are described in the next Section. Section
3 presents the methods used to identify bias in the
labeled examples and WSD system. Experiments in
which these approaches are compared are described
in Section 4 and their results in Section 5.
2 Background
Many WSD systems for the biomedical domain are
based on supervised learning (McInnes et al, 2007;
Xu et al, 2007; Stevenson et al, 2008; Yepes and
Aronson, 2011). These systems require labeled
training data, examples of an ambiguous term la-
beled with the correct meaning. Some sets of labeled
data have been developed for the biomedical domain
(Weeber et al, 2001; Savova et al, 2008; Jimeno-
Yepes et al, 2011). However, these data sets only
contain examples for a few hundred terms and can
only be used to develop WSD systems to identify
the meanings of those terms. The process of creat-
ing labeled examples is extremely time-consuming
and difficult (Artstein and Poesio, 2008), making it
impractical to create labeled examples of all possible
ambiguous terms found in biomedical documents.
Two alternative approaches have been explored to
develop systems which are able to disambiguate all
ambiguous terms in biomedical documents. The first
makes use of unsupervised WSD algorithms (see
Section 2.1) and the second creates labeled data au-
tomatically and uses it to train a supervised WSD
system (see Section 2.2).
2.1 Unsupervised WSD
Unsupervised WSD algorithms make use of infor-
mation from some knowledge source, rather than re-
lying on training data.
Humphrey et al (2006) describe an unsupervised
system which uses semantic types in UMLS to dis-
tinguish between the possible meanings of ambigu-
ous words. However, it cannot disambiguate be-
tween senses with the same semantic type, i.e., it
is not possible for the system to recognise all sense
distinctions.
The Personalised Page Rank (PPR) system
(Agirre et al, 2010; Jimeno-Yepes and Aronson,
2010) relies on a a graph-based algorithm similar
to the Page Rank algorithm originally developed for
use in search engines (Brin, 1998). It performs
WSD by converting the UMLS Metathesaurus into
a graph in which the possible meanings of ambigu-
ous words are nodes and relations between them are
edges. Disambiguation is carried out by providing
the algorithm with a list of senses that appear in the
text that is being disambiguated. This information is
then combined with the graph and a ranked list of the
possible senses for each ambiguous word generated.
Unsupervised systems have the advantage of be-
ing able to disambiguate all ambiguous terms. How-
ever, the performance of unsupervised systems that
have been developed for biomedical documents is
lower than that of supervised ones.
2.2 Automatic Generation of Labeled Data
Automatic generation of labeled data for WSD com-
bines the accuracy of supervised approaches with
the ability of unsupervised approaches to disam-
biguate all ambiguous terms. It was first suggested
by Leacock et al (1998). Their approach is based
on the observation that some terms in a lexicon oc-
cur only once and, consequently, there is no doubt
about their meaning. These are referred to as being
monosemous. Examples for each possible meaning
of an ambiguous term are generated by identifying
the closest monosemous term (the monosemous rel-
ative) in the lexicon and using examples of that term.
Variants of the approach have been applied to the
biomedical domain using the UMLS Metathesaurus
as the sense inventory.
232
Liu et al (2002) were the first to apply the
monosemous relatives approach to biomedical WSD
and use it to disambiguate a set of 35 abbreviations.
They reported high precision but low recall, indicat-
ing that labeled examples could not be created for
many of the abbreviations. Jimeno-Yepes and Aron-
son (2010) applied a similar approach and found
that it performed better than a number of alternative
approaches on a standard evaluation resource (the
NLM-WSD corpus) but did not perform as well as
supervised WSD. Stevenson and Guo (2010) com-
pared two techniques for automatically creating la-
beled data, including the monosemous relatives ap-
proach. They found that the examples which were
generated were as good as manually labeled exam-
ples when used to train a supervised WSD system.
However, Stevenson and Guo (2010) relied on la-
beled data to determine the number of examples of
each sense to create, and therefore the bias of the
data set. Consequently their approach is not com-
pletely unsupervised since it could not be applied to
ambiguous terms that do not have labeled training
data available.
3 Approach
3.1 WSD System
The WSD system is based on a supervised approach
that has been adapted for the biomedical domain
(Stevenson et al, 2008). The system was tested on
the NLM-WSD corpus (see Section 4.1) and found
to outperform alternative approaches.
The system can exploit a wide range of fea-
tures, including several types of linguistic informa-
tion from the context of an ambiguous term, MeSH
codes and Concept Unique Identifiers (CUIs) from
the UMLS Metathesaurus. However, computing
these features for every example is a time consum-
ing process and to make the system suitable for large
scale WSD it was restricted to using a smaller set
of features. Previous experiments (Stevenson et al,
2008) showed that this only leads to a small drop in
disambiguation accuracy while significantly reduc-
ing the computational cost of generating features.
3.1.1 Features
Two types of context words are used as features:
the lemmas of all content words in the same sen-
tence as the ambiguous word and the lemmas of all
content words in a?4-word window around the am-
biguous term. A list of corpus-specific stopwords
was created containing terms that appear frequently
in Medline abstracts but which are not useful for dis-
ambiguation (e.g. ?abstract?, ?conclusion?). Any
lemmas found in this list were not used as features.
3.1.2 Learning algorithm
Disambiguation is carried out using the Vector
Space Model, a memory-based learning algorithm
in which each occurrence of an ambiguous word is
represented as a vector created using the features ex-
tracted to represent it (Agirre and Mart??nez, 2004a).
The Vector Space Model was found to outperform
other learning algorithms when evaluated using the
NLM-WSD corpus (Stevenson et al, 2008).
During the algorithm?s training phase a single
centroid vector, ~Csj , is generated for each possible
sense, sj . This is shown in equation 1 where T is
the set of training examples for a particular term and
sense(~t) is the sense associated with the vector ~t.
~Csj =
?
~ti  T :sense(~ti)=sj
~ti
|~ti  T : sense(~ti) = sj |
(1)
Disambiguation is carried out by comparing the
vector representing the ambiguous word, ~a, against
the centroid of each sense using the cosine metric,
shown in equation 2, and choosing the one with the
highest score.
score(sj ,~a) = cos( ~Csj ,~a) =
~Csj .~a
| ~Csj ||~a|
(2)
Note that the learning algorithm does not ex-
plicitly model the prior probability of each possi-
ble sense, unlike alternative approaches (e.g. Naive
Bayes), since it was found that including this infor-
mation did not improve performance.
3.2 Automatically generating training
examples
The approaches used for generating training exam-
ples used here are based on the work of Stevenson
and Guo (2010), who describe two approaches:
1. Monosemous relatives
2. Co-occurring concepts
233
Both approaches are provided with a set of ambigu-
ous CUIs from the UMLS Metathesaurus, which
represent the possible meanings of an ambiguous
term, and a target number of training examples to be
generated for each CUI. Each CUI is associated with
at least one term and each term is labeled with a lex-
ical unique identifier (LUI) which represents a range
of lexical variants for a particular term. The UMLS
Metathesaurus contains a number of data files which
are exploited within these techniques, including:
AMBIGLUI: a list of cases where a LUI is linked
to multiple CUIs.
MRCON: every string or concept name in the
Metathesaurus appears in this file.
MRCOC: co-occuring concepts.
For the monosemous relatives approach, the
strings of monosemous LUIs of the target CUI
and its relatives are used to search Medline to re-
trieve training examples. The monosemous LUIs re-
lated to a CUI are defined as any LUIs associated
with the CUI in the MRCON table and not listed in
AMBIGLUI table.
The co-occurring concept approach works differ-
ently. Instead of using strings of monosemous LUIs
of the target CUI and its relatives, the strings associ-
ated with LUIs of a number of co-occurring CUIs of
the target CUI and its relatives found in MRCOC ta-
ble are used. The process starts by finding the LUIs
of the top n co-occurring CUIs of the target CUI.
These LUIs are then used to form search queries.
The query is quite restrictive in the beginning and re-
quires all terms appear in the Medline citations files.
Subsequently queries are made less restrictive by re-
ducing the number of required terms in the query.
These techniques were used to generate labeled
examples for all terms that are ambiguous in the
2010 AB version of the UMLS Metathesaurus.2 The
set of all ambiguous terms was created by analysing
the AMBIGLUI table, to identify CUIs that are asso-
ciated with multiple LUIs. The Medline Baseline
Repository (MBR)3 was also analysed and it was
found that some terms were ambiguous in this re-
source, in the sense that more than one CUI had been
2Stevenson and Guo (2010) applied them to a small set of
examples from the NLM-WSD data set (see Section 4.1).
3http://mbr.nlm.nih.gov
assigned to an instance of a term, but could not be
identified from the AMBIGLUI table. The final list
of ambiguous CUIs was created by combining those
identified from the AMBIGLUI table and those find
in the MBR. This list contained a total of 103,929
CUIs.
Both techniques require large number of searches
over the Medline database and to carry this out ef-
ficiently the MBR was indexed using the Lucene
Information Retrieval system4 and all searches ex-
ecuted locally.
Examples were generated using both approaches.
The monosemous relatives approach generated ex-
amples for 98,462 CUIs and the co-occurring con-
cepts for 98,540. (Examples generated using the
monosemous relatives approach were preferred for
the experiments reported later.) However, neither
technique was able to generate examples for 5,497
CUIs, around 5% of the total. This happened when
none of the terms associated with a CUI returned
any documents when queried against the MBR and
that CUI does not have any monosemous relatives.
An example is C1281723 ?Entire nucleus pulpo-
sus of intervertebral disc of third lumbar vertebra?.
The lengthy terms associated with this CUI do not
return any documents when used as search terms
and, in addition, it is only related to one other CUI
(C0223534 ?Structure of nucleus pulposus of inter-
vertebral disc of third lumbar vertebra?) which is it-
self only connected to C1281723. Fortunately there
are relatively few CUIs for which no examples could
be generated and none of them appear in the MBR,
suggesting they refer to UMLS concepts that do not
tend to be mentioned in documents.
3.3 Generating Bias
Three different techniques for deciding the number
of training examples to be generated for each CUI
(i.e. the bias) were explored.
Uniform Bias (UB) uses an equal number of
training examples to generate centroid vectors for
each of the possible senses of the ambiguous term.
Gold standard bias (GSB) is similar to the uni-
form bias but instead of being the same for all pos-
sible CUIs the number of training examples for each
CUI is determined by the number of times it appears
4http://lucene.apache.org/
234
in a manually labeled gold standard corpus. Assume
t is an ambiguous term and Ct is the set of possible
meanings (CUIs). The number of training examples
used to generate the centroid for that CUI, Ec, is
computed according to equation 3 where Gc is the
number of instances in the gold standard corpus an-
notated with CUI c and n is a constant which is set
to 100 for these experiments.5
Ec =
Gc
?
ci  Ct
Gci,t
.n (3)
The final technique, Metamap Baseline Repos-
itory Bias (MBB), is based on the distribution of
CUIs in the MBR. The number of training examples
are generated in a similar way to the gold standard
bias with MBR being used instead of a manually la-
beled corpus and is shown in equation 4 whereMc is
the number of times the CUI c appears in the MBR.
Ec =
Mc
?
ci  Ct
Mci
.n (4)
For example, consider the three possible CUIs as-
sociated with term ?adjustment? in the NLM-WSD
corpus: C0376209, C0456081 and C06832696.
The corpus contains 18 examples of C0376209,
62 examples of C0456081 and 13 of C0683269.
Using equation 3, the number of training exam-
ples when GSB is applied for C0376209 is 20,
67 for C0456081 and 14 for C0683269. In the
Metamap Baseline Repository files, C0376209 has
a frequency count of 98046, C0456081 a count of
292809 and C0683269 a count of 83530. Therefore
the number of training examples used for the three
senses when applying MBB is: 21 for C0376209, 62
for C0456081 and 18 for C0683269.
4 Evaluation
4.1 Data sets
We evaluate our system on two datasets: the NLM-
WSD and MSH-WSD corpora.
5Small values for Ec are rounded up to ensure that any rare
CUIs have at least one training example.
6These CUIs are obtained using the mappings from NLM-
WSD senses to CUIs available on the NLM website: http:
//wsd.nlm.nih.gov/collaboration.shtml
The NLM-WSD corpus7 (Weeber et al, 2001) has
been widely used for experiments on WSD in the
biomedical domain, for example (Joshi et al, 2005;
Leroy and Rindflesch, 2005; McInnes et al, 2007;
Savova et al, 2008). It contains 50 ambiguous terms
found in Medline with 100 examples of each. These
examples were manually disambiguated by 11 an-
notators. The guidelines provided to the annotators
allowed them to label a senses as ?None? if none
of the concepts in the UMLS Metathesaurus seemed
appropriate. These instances could not be mapped
onto UMLS Metathesaurus and were ignored for our
experiments.
The larger MSH-WSD corpus (Jimeno-Yepes et
al., 2011) contains 203 strings that are associated
with more than one possible MeSH code in the
UMLS Metathesaurus. 106 of these are ambiguous
abbreviations, 88 ambiguous terms and 9 a combi-
nation of both. The corpus contains up to 100 ex-
amples for each possible sense and a total of 37,888
examples of ambiguous strings taken from Medline.
Unlike the NLM-WSD corpus, all of the instances
can be mapped to the UMLS Metathesaurus and
none was removed from the dataset for our exper-
iments.
The two data sets differ in the way the number
of instances of each sense was determined. For
the NLM-WSD corpus manual annotation is used to
decide the number of instances that are annotated
with each sense of an ambiguous term. However,
the NLM-MSH corpus was constructed automati-
cally and each ambiguous term has roughly the same
number of examples of each possible sense.
4.2 Experiments
The WSD system described in Section 3 was tested
using each of the three techniques for determining
the bias, i.e. number of examples generated for each
CUI. Performance is compared against various alter-
native approaches.
Two supervised approaches are included. The
first, most frequent sense (MFS) (McCarthy et al,
2004), is widely used baseline for supervised WSD
systems. It consists of assigning each ambiguous
term the meaning that is more frequently observed
in the training data. The second supervised approach
7http://wsd.nlm.nih.gov
235
is to train the WSD system using manually labeled
examples from the NLM-WSD and MSH-WSD cor-
pora. 10-fold cross validation is applied to evaluate
this approach.
Performance of the Personalised Page Rank ap-
proach described in Section 2.1 is also provided to
allow comparison with an unsupervised algorithm.
Both Personalised Page Rank and the techniques
we employ to generate labeled data, base disam-
biguation decisions on information from the UMLS
Metathesaurus.
The performance of all approaches is measured
in terms of the percentage of instances which are
correctly disambiguated for each term with the av-
erage across all terms reported. Confidence inter-
vals (95%) computed using bootstrap resampling
(Noreen, 1989) are also shown.
5 Results
Results of the experiments are shown in Table 1
where the first three rows show performance of the
approach described in Section 3 using the three
methods for computing the bias (UB, MMB and
GSB). MFS and Sup refer to the Most Frequent
Sense supervised baseline and using manually la-
beled examples, respectively, and PPR to the Per-
sonalised PageRank approach.
When the performance of the approaches us-
ing automatically labeled examples (UB, MMB and
GSB) is compared it is not surprising that the best re-
sults are obtained using the gold standard bias since
this is obtained from manually labeled data. Results
using this technique for computing bias always out-
perform the other two, which are completely unsu-
pervised and do not make use of any information
from manually labeled data. However, the improve-
ment in performance varies according to the corpus,
for the NLM-WSD corpus there is an improvement
of over 10% in comparison to UB while the corre-
sponding improvement for the MSH-WSD corpus is
less than 0.5%.
A surprising result is that performance obtained
using the uniform bias (UB) is consistently better
than using the bias obtained by analysis of the MBR
(MMB). It would be reasonable to expect that in-
formation about the distribution of CUIs in this cor-
pus would be helpful for WSD but it turns out that
making no assumptions whatsoever about their rel-
ative frequency, i.e., assigning a uniform baseline,
produces better results.
The relative performance of the supervised (MFS,
Sup and GSB) and unsupervised approaches (UB,
MMB and PPR) varies according to the corpus. Un-
surprisingly using manually labeled data (Sup) out-
performs all other approaches on both corpora. The
supervised approaches also outperform the unsuper-
vised ones on the NLM-WSD corpus. However, for
the MSH-WSD corpus all of the unsupervised ap-
proaches outperform the MFS baseline.
A key reason for the differences in these results is
the different distributions of senses in the two cor-
pora, as shown by the very different performance of
the MFS approach on the two corpora. This is dis-
cussed in more detail later (Section 5.2).
Comparison of the relative performance of the un-
supervised approaches (UB, MMB and PPR) shows
that training a supervised system with the automat-
ically labeled examples using a uniform bias (UB)
always outperforms PPR. This demonstrates that
this approach outperforms a state-of-the-art unsu-
pervised algorithm that relies on the same infor-
mation used to generate the examples (the UMLS
Metathesaurus).
5.1 Performance by Ambiguity Type
The MSH-WSD corpus contains both ambiguous
terms and abbreviations (see Section 4.1). Perfor-
mance of the approaches on both types of ambiguity
are shown in Table 2.
MSH-WSD Ambiguity Type
Approach Abbreviation Term
UB 91.40 [91.00, 91.75] 72.68 [72.06, 73.32]
MMB 84.43 [83.97, 84.89] 69.45 [68.86, 70.10]
GSB 90.82 [90.45, 91.22] 73.96 [73.40, 74.62]
MFS 52.43 [51.73, 53.05] 51.76 [51.11, 52.36]
Sup. 97.41 [97.19, 97.62] 91.54 [91.18, 91.94]
PPR 86.40 [86.00, 86.85] 68.40 [67.80, 69.14]
Table 2: WSD evaluation results for abbreviations and
terms in the MSH-WSD data set.
The relative performance of the different ap-
proaches on the terms and abbreviations is similar to
the entire MSH-WSD data set (see Table 1). In par-
236
Corpus
Approach Type NLM-WSD MSH-WSD
UB Unsup. 74.00 [72.80, 75.29] 83.19 [82.87, 83.54]
MMB Unsup. 71.18 [69.94, 72.38] 78.09 [77.70, 78.46]
GSB Sup. 84.28 [83.12, 85.36] 83.39 [83.08, 83.67]
MFS Sup. 84.70 [83.67, 85.81] 52.01 [51.50, 52.45]
Sup Sup. 90.69 [89.87, 91.52] 94.83 [94.63, 95.02]
PPR Unsup. 68.10 [66.80, 69.23] 78.60 [78.23, 78.90]
Table 1: WSD evaluation results on NLM-WSD and MSH-WSD data sets.
ticular using automatically generated examples with
a uniform bias (UB) outperforms using the bias de-
rived from the Medline Baseline Repository (MBR)
while using the gold standard baseline (GSB) im-
proves results slightly for terms and actually reduces
them for abbreviations.
Results for all approaches are higher when disam-
biguating abbreviations than terms which is consis-
tent with previous studies that have suggested that
in biomedical text abbreviations are easier to disam-
biguate than terms.
5.2 Analysis
An explanation of the reason for some of the re-
sults can be gained by looking at the distributions
of senses in the various data sets used for the ex-
periments. Kullback-Leibler divergence (or KL di-
vergence) (Kullback and Leibler, 1951) is a com-
monly used measure for determining the difference
between two probability distributions. For each term
t, we define S as the set of possible senses of t,
the sense probability distributions of t as D and D?.
Then the KL divergence between the sense probabil-
ity distributions D and D? can be calculated accord-
ing to equation 5.
KL(D||D?) =
?
s  S
D(s). log
D(s)
D?(s)
(5)
The three techniques for determining the bias de-
scribed in Section 3.3 each generate a probability
distribution over senses. Table 2 shows the average
KL divergence when the gold standard distribution
obtained from the manually labeled data (GSB) is
compared with the uniform bias (UB) and bias ob-
tained by analysing the Medline Baseline Reposi-
tory (MMB).
Corpus
Avg. KL Divergence NLM-WSD MSH-WSD
KL(GSB||MMB) 0.5649 0.4822
KL(GSB||UB) 0.4600 0.0406
Table 3: Average KL divergence of sense probability dis-
tributions in the NLM-WSD and MSH-WSD data sets.
The average KL divergence scores in the table
are roughly similar with the exception of the much
lower score obtained for the gold-standard and uni-
form bias for the MSH-WSD corpus (0.0406). This
is due to the fact that the MSH-WSD corpus was
designed to have roughly the same number of ex-
amples for each sense, making the sense distribu-
tion close to uniform (Jimeno-Yepes et al, 2011).
This is evident from the MFS scores for the MSH-
WSD corpus which are always close to 50%. This
also provides as explanation of why performance us-
ing automatically generated examples on the MSH-
WSD corpus only improves by a small amount when
the gold standard bias is used (see Table 1). The gold
standard bias simply does not provide much addi-
tional information to the WSD system. The situa-
tion is different in the NLM-WSD corpus, where the
MFS score is much higher. In this case the additional
information available in the gold standard sense dis-
tribution is useful for the WSD system and leads to
a large improvement in performance.
In addition, this analysis demonstrates why per-
formance does not improve when the bias gener-
ated from the MBR is used. The distributions which
are obtained are different from the gold standard
and are therefore mislead the WSD system rather
than providing useful information. The difference
between these distributions would be expected for
237
the MSH-WSD corpus, since it contains roughly the
same number of examples for each possible sense
and does not attempt to represent the relative fre-
quency of the different senses. However, it is sur-
prising to observe a similar difference for the NLM-
WSD corpus, which does not have this constraint.
The difference suggests the information about CUIs
in the MBR, which is generated automatically, has
some limitations.
Table 4 shows a similar analysis for the MSH-
WSD corpus when abbreviations and terms are con-
sidered separately and supports this analysis. The
figures in this table show that the gold standard and
uniform distributions are very similar for both ab-
breviations and terms, which explains the similar re-
sults for UB and GSB in Table 2. However, the gold
standard distribution is different from the one ob-
tained from the MBR. The drop in performance of
MMB compared with GBS in Table 2 is a conse-
quence of this.
Ambiguity Type
Avg. KL Divergence Abbreviation Term
KL(GSB||MMB) 0.4554 0.4603
KL(GSB||UB) 0.0544 0.0241
Table 4: Average KL divergence for abbreviations and
terms in the MSH-WSD data set.
6 Conclusion
This paper describes the development of a large
scale WSD system based on automatically labeled
examples. We find that these examples can be gener-
ated for the majority of CUIs in the UMLS Metathe-
saurus. Evaluation on the NLM-WSD and MSH-
WSD data sets demonstrates that the WSD system
outperforms the PPR approach without making any
use of labeled data.
Three techniques for determining the number of
examples to use for training are explored. It is
found that a supervised approach (which makes use
of manually labeled data) provides the best results.
Surprisingly it was also found that using information
from the MBR did not improve performance. Anal-
ysis showed that the sense distributions extracted
from the MBR were different from those observed
in the evaluation data, providing an explanation for
this result.
Evaluation showed that accurate information
about the bias of training examples is useful for
WSD systems and future work will explore other un-
supervised ways of obtaining this information. Al-
ternative techniques for generating labeled examples
will also be explored. In addition, further evaluation
of the WSD system will be carried out, such as ap-
plying it to an all words task and within applications.
Acknowledgements
This research has been supported by the Engineer-
ing and Physical Sciences Research Council and a
Google Research Award.
References
E. Agirre and D. Mart??nez. 2004a. The Basque Country
University system: English and Basque tasks. In Rada
Mihalcea and Phil Edmonds, editors, Proceedings of
Senseval-3, pages 44?48, Barcelona, Spain.
E. Agirre and D. Mart??nez. 2004b. Unsupervised WSD
Based on Automatically Retrieved Examples: The
Importance of Bias. In Proceedings of EMNLP-04,
Barcelona, Spain.
E. Agirre, A. Sora, and M. Stevenson. 2010. Graph-
based word sense disambiguation of biomedical docu-
ments. Bioinformatics, 26(22):2889?2896.
R. Artstein and M. Poesio. 2008. Inter-Coder Agree-
ment for Computational Linguistics. Computational
Linguistics, 34(4):555?596.
S. Brin. 1998. Extracting Patterns and relations from the
Word-Wide Web. In Proceedings of WebDB?98.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense
Disambiguation by Selecting the Best Semantic Type
Based on Journal Descriptor Indexing: Preliminary ex-
periment. Journal of the American Society for Infor-
mation Science and Technology, 57(5):96?113.
A. Jimeno-Yepes and A. Aronson. 2010. Knowledge-
based biomedical word sense disambiguation: com-
parison of approaches. BMC Bioinformatics,
11(1):569.
A. Jimeno-Yepes, B. McInnes, and A. Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to generate
a data set for word sense disambiguation. BMC Bioin-
formatics, 12(1):223.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Com-
parative Study of Support Vector Machines Applied to
the Word Sense Disambiguation Problem for the Med-
ical Domain. In Proceedings of IICAI-05, pages 3449?
3468, Pune, India.
238
M. Krallinger and A. Valencia. 2005. Text mining and
information retrieval services for molecular biology.
Genome Biology, 6(7):224.
S. Kullback and R. A. Leibler. 1951. On Information and
Sufficiency. The Annals of Mathematical Statistics,
22(1):79?86.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
165.
G. Leroy and T. Rindflesch. 2005. Effects of Information
and Machine Learning algorithms on Word Sense Dis-
ambiguation with Small Datasets. International Jour-
nal of Medical Informatics, 74(7-8):573?585.
H. Liu, S. Johnson, and C. Friedman. 2002. Au-
tomatic Resolution of Ambiguous Terms Based on
Machine Learning and Conceptual Relations in the
UMLS. Journal of the American Medical Informatics
Association, 9(6):621?636.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding Predominant Word Senses in Untagged
Text. In Proceedings of ACL-2004, pages 280?287,
Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the AMIA Symposium, pages 533?537,
Chicago, IL.
R. Mooney. 1996. Comparative Experiments on Disam-
biguating Word Senses: An Illustration of the Role of
Bias in Machine Learning. In Proceedings of EMNLP-
96, pages 82?91, Philadelphia, PA.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
G. Savova, A. Coden, I. Sominsky, R. Johnson, P. Ogren,
C. de Groen, and C. Chute. 2008. Word Sense Disam-
biguation across Two Domains: Biomedical Literature
and Clinical Notes. Journal of Biomedical Informat-
ics, 41(6):1088?1100.
M. Stevenson and Y. Guo. 2010. Disambiguation of Am-
biguous Biomedical Terms using Examples Generated
from the UMLS Metathesaurus. Journal of Biomedi-
cal Informatics, 43(5):762?773.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMIA Symposium, pages
746?50, Washington, DC.
J. Westbrook, E. Coiera, and A. Gosling. 2005. Do On-
line Information Retrieval Systems Help Experienced
Clinicians Answer Clinical Questions? Journal of the
American Medical Informatics Association, 12:315?
321.
H. Xu, J. Fan, G. Hripcsak, E. Mendonc?a, Markatou M.,
and Friedman C. 2007. Gene symbol disambigua-
tion using knowledge-based profiles. Bioinformatics,
23(8):1015?22.
A. Jimeno Yepes and A. Aronson. 2011. Self-training
and co-training in biomedical word sense disambigua-
tion. In Proceedings of BioNLP 2011 Workshop, pages
182?183, Portland, Oregon, USA, June.
239
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 112?117,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Seeking Informativeness in Literature Based Discovery
Judita Preiss
University of Sheffield, Department of Computer Science
Regent Court, 211 Portobello
Sheffield S1 4DP, United Kingdom
j.preiss@sheffield.ac.uk
Abstract
The continuously increasing number of
publications within the biomedical domain
has fuelled the creation of literature based
discovery (LBD) systems which identify
unconnected pieces of knowledge appear-
ing in separate literatures which can be
combined to make new discoveries. With-
out filtering, the amount of hidden knowl-
edge found is vast due to noise, making it
impractical for a researcher to examine, or
clinically evaluate, the potential discover-
ies. We present a number of filtering tech-
niques, including two which exploit the
LBD system itself rather than being based
on a statistical or manual examination of
document collections, and we demonstrate
usefulness via replication of known dis-
coveries.
1 Introduction and background
The number of publications in the biomedical do-
main has been observed to increase at a great rate,
making it impossible for one person to read all,
and thus potentially leaving knowledge hidden:
for example, Swanson (1986) found one publica-
tion mentioning a connection between Raynaud?s
Disease and blood viscosity while another pointed
out the effect of fish oil on blood viscosity, but
there was no publication making the connection
between fish oil and Raynaud?s Disease. Auto-
mated approaches to knowledge discovery often
set up the problem as outlined by Swanson; A be-
ing the source term (in this case Raynaud?s Dis-
ease), with a possible target term, C, being speci-
fied (fish oil) and any connections between them
form the linking, B, terms. If C is not speci-
fied, all possible hidden links from A are explored
and discovery is classified as open. If both A and
C terms are supplied, the discovery is closed and
only any linking, B, terms are being sought.
Independent of how a connection between an A
term and a B is defined (whether this is based on A
and B co-occurring in the same title, in the same
sentence or the same document, or some other
relation), an obvious difficulty is the amount of
data generated by a technique along these lines:
with no filtering, a great number of connections
will be made through terms such as clinical study
or patient, and, if not also linked through other
terms, these should be discarded. A number of
approaches to term reduction have been explored.
Swanson and Smalheiser (1999)?s knowledge
discovery system, Arrowsmith,
1
contains an in-
creasing, currently 9,500 term (Swanson et al.,
2006), stoplist, created semi-automatically.
2
Such
a stoplist is unlikely to be complete ? the list
has grown from 5,000 (Swanson and Smalheiser,
1997) to 9,500 words (Swanson et al., 2006) and
is likely to keep increasing. Over fitting is po-
tentially an issue, in this case the list generated
has been criticized for being tuned for the original
Raynaud?fish oil discovery (Weeber et al., 2001).
A word based stoplist also does not take into ac-
count the potential ambiguity of terms: one sense
may be highly frequent and uninformative, guar-
anteeing it an appearance in the stoplist, while an-
other sense may be rare but highly informative.
Instead of using words directly, it is possible to
employ a (much smaller) controlled vocabulary:
Medical Subject Headings (MeSH), consisting of
22,500 codes, are (mostly) manually assigned to
each document indexed in Medline ? even though
multiple MeSH codes for a document are allowed,
restricting to this set greatly reduces dimensional-
ity. For example, Srinivasan (2004) uses MeSH
based topic profiles to connect A to topics C via
the most likely MeSH terms.
1
Available at http://arrowsmith.psych.uic.
edu/arrowsmith_uic/index.html
2
Note that only 365 words of this stoplist are publicly
available.
112
Keeping entire vocabularies is possible if topics
are limited, for example, Fleuren et al (2011) ex-
tract statistics regarding gene co-occurrence, and
restricts their hidden knowledge generation to bi-
ological mechanisms related to them.
Another difficulty in using word vocabularies is
the necessary identification of multiwords, Wee-
ber at al. (2001) avoid previously tried n-gram
techniques (e.g. (Gordon and Lindsay, 1996)) by
switching knowledge discovery to UMLS Concept
Unique Identifiers (CUIs). Using MetaMap (Rind-
flesch and Aronson, 1994) to assign CUIs to texts
discards non content words (CUIs only exist for
concepts), resolves ambiguity and deals with mul-
tiwords in one, thus reducing the number of terms
considered in later stages. Weeber et al. also ex-
ploit the broad subject categories that UMLS as-
signs to each CUI, which allow the authors to per-
form domain specific filtering to reduce dimen-
sionality. This they do on a per search basis, tun-
ing the filtering to the replication experiments pre-
sented.
Dimensionality reduction can also be performed
at the relation level. Swanson?s (1997) original
work deemed two terms connected if they both
appeared in the title of an abstract ? titles were
thought to be the most informative, and descrip-
tive, part of each article. As the number of ab-
stracts explored during the knowledge discovery
process increased, and connections were extended
to whole abstracts (rather than titles only), the
amount of hidden knowledge generated increased
dramatically and with it did the need for term and
connection filtering.
Hristovski et al (2006) argue for filtering within
the relation definition ? co-occurrence does not
provide any basis for a relation between two
terms, no underlying semantic reason, and thus, as
well as leading to many spurious links, it yields
no justification for a hidden connection that is
found. They extract subject-relation-object triples,
with relations such as treats or affects forming
their UMLS concept relations, leading to a much
smaller number of (more accurate) relations to de-
rive hidden knowledge from.
While re-ranking (placing the most ?useful?
links at the top of the list) the resulting hidden
knowledge is clearly valuable, removing terms
from consideration prior to identifying hidden
knowledge will reduce the computational load
as well as avoid noisy hidden knowledge being
produced and possibly accidentally being highly
ranked.
We explore a number of filtering approaches in-
cluding two novel techniques which can be inte-
grated into any method designed using the Swan-
son framework, and we compare these against
previously explored filtering methods. Section 2
outlines our knowledge discovery approach, Sec-
tion 3 presents a number of filtering approaches
with Section 4 discussing results based on repli-
cation of existing knowledge and Section 5 draws
our conclusions.
2 Knowledge discovery system
There are two main components which define
an LBD system created following the Swanson
framework: the terms and the relations. Based on
arguments presented in Section 1, our system em-
ploys UMLS CUIs as produced by SemRep (Rind-
flesch and Fiszman, 2003), a natural language pro-
cessing system which identifies semantic relations
in biomedical text.
3
SemRep extracts relation triples from text by
running a set of rules over the output of an under-
specified parser. The rules, such as the mapping
of treatment to TREATS, map syntactic indicators
to predicates in the Semantic Network. Further re-
strictions are imposed regarding the permissibility
of arguments, the viability of the given proposi-
tions, and other syntactic constraints, resulting in
relations such as
? Epoprostenol TREATS Raynaud Phenomenon
? blood rheology DIAGNOSES Raynaud Disease
Each triple is also output with the corresponding
CUIs.
All 29 non negative relations were extracted
(such as AFFECTS, ASSOCIATED WITH, INTER-
ACTS WITH, . . . ), while negative relations (such
as NEG AFFECTS, NEG ASSOCIATED WITH,
NEG INTERACTS WITH, . . . ) were dropped. The
extracted relations form the connections between
CUIs: i.e., the set of linking CUIs B is created
by following all SemRep links from the CUI A,
which lead to C through another SemRep relation.
3
In this work, the SemRep annotated Medline data,
database semmedVER24 (processed up to November 2013)
run over 23,319,737 citations to yield 68,000,470 predica-
tions, was downloaded from http://skr3.nlm.nih.
gov and used throughout.
113
3 Filtering approaches
While employing CUIs (rather than words) elimi-
nates non content words (thus immediately reduc-
ing noise), it does not eliminate CUIs correspond-
ing to patient, week, statement . . . We present, and
in Section 4 evaluate (individually and in combi-
nation), four filtering approaches of which two are,
to our knowledge, completely novel.
3.1 Synonyms
While not a filtering method under the usual def-
inition, the identification of synonym CUIs and
collapsing thereof results in the reduction of the
number of CUIs being used (i.e. the technique fil-
ters out some CUIs).
A manual examination of the documents con-
taining CUI C0034734, Raynaud Disease, re-
vealed that some of the expected connections were
missing and were linked to CUI C0034735, Ray-
naud Phenomenon, instead. The resulting hid-
den knowledge is greatly affected by the particu-
lar CUI chosen as the source term A, yet in this
case, the two CUIs are synonymous. The MR-
REL related concepts file within UMLS contains
pairs of CUIs within related relationships, includ-
ing the SY (source asserted synonymy) relation-
ship
4
, and CUIs C0034734 and C0034735 appear
in the SY relationship in this list. Identifying con-
cepts within the SY relationship has the following
advantages:
? Merging such synonyms into classes will al-
low the retrieval of more hidden knowledge if
the multiple synonymous CUIs correspond to
the start point, A (as in the case of Raynaud
Disease).
? There will be potentially more hidden knowl-
edge created if a multiclass CUI is a linking
term (as A connected to C0034734 and C
connected to C0034735 would not have been
found to be connected if these were the only
potential overlap).
? Synonymous hidden knowledge (and linking
terms) will merge, reducing the amount of
knowledge (and terms) to manually explore.
Merging synonyms into single CUI classes re-
duces the 561,155 CUIs present in UMLS to
540,440 CUI classes.
5
4
Due to the version of SemRep files used, UMLS 2013AA
is employed throughout.
5
Note that other MRREL related relationships were ex-
3.2 Semantic types
The UMLS Semantic Network consists of 133 se-
mantic types, a type of subject category, which is
assigned to each CUI. Many of these categories
are clearly unhelpful for knowledge discovery (for
example, geographic area or language), and 70
semantic types are manually selected for removal
(by examining the basic information about the re-
lation, as well as the structure of the network and
the CUIs assigned each semantic type). This re-
moves a further 121,284 CUIs.
3.3 Discarding common linking terms
In some cases, a given CUI is clearly too general
to be a useful linking term, but its semantic type
contains more specific CUIs which should not be
removed. Restricting semantic type filtering based
on the depth within the hierarchy is also not a vi-
able option, as UMLS is composed of different
hierarchies, each with a different level of granu-
larity and establishing an overall threshold would
likely include general terms for some while dis-
carding crucial terms for others. Therefore another
approach is needed for these CUIs.
Along the lines of Swanson et al (2006), a sto-
plist can be built to contain such terms, without
over-training for a particular discovery and with-
out the need for manual intervention: we hypoth-
esize that any CUIs which are linking terms more
often than others can effectively form a stoplist.
The creation of this stoplist can be performed
iteratively:
1. Start with an empty stoplist set S.
2. Create hidden knowledge based on SemRep
connections between CUIs, removing any
connections to CUIs in set S (the hidden
knowledge is acquired from Medline articles
published between 1865 and 2000).
3. Randomly select 10,000 hidden knowledge
pairs, identify their linking CUIs, and add
any linking CUIs appearing in more than
threshold of pairs to S (the value of thresh-
old needs to be empirically determined).
4. If Step 3 increased the size of S, return to
Step 2.
Note that since the training set is not designed for
any particular discovery, this should not result in
an over trained stoplist.
plored, but completing cycles lead to multiple extremely large
equivalence classes.
114
3.4 Breaking high frequency connections
The creation of a stoplist will always suffer from
omissions and inclusions of CUIs that should not
be filtered out in every instance. The last approach
is based on a slightly different underlying idea: in-
stead of finding frequently appearing terms, this
approach bases its decisions on the number of
terms a given term is connected to.
Two CUIs A and B are deemed connected if a
(non negative) SemRep relation exists which links
them. If A corresponds to a term such as study
or patient, it is expected to be connected to a large
number of CUIs. We hypothesize that terms which
are so highly connected are likely to be relatively
general terms, and so uninformative linking terms.
This gives rise to the following filtering options:
1. Break (discard) all connections to CUI A
when the C(A) > threshold.
2. Discard the connection between CUIs A and
B when min(C(A), C(B)) > threshold.
(Where C(A) represents the number of CUIs
linked to A, and the threshold needs to be empiri-
cally determined.)
Method 1 effectively forms a stoplist of highly
connected CUIs, but method 2 is different: only
connections satisfying the condition are broken
while A remains under consideration. This allows
filtering method 2 to leave a frequently connected
term to be a linking term for a rare term (unlike
method 1, which would discard such a term).
4 Results
Swanson?s original discoveries (Swanson, 1986;
Swanson, 1988) were verified through clinical tri-
als and evaluation of LBD systems often involves
replication of these discoveries (Gordon and Lind-
say, 1996; Weeber et al., 2001). From literature,
we identify seven separate discoveries to replicate
(presented with the labels used in Table 1):
RD: Raynaud disease and fish oil (Swanson,
1986).
Arg: Somatomedin C and arginine (Swanson,
1990).
Mg: Migraine disorders and magnesium (Hu et
al., 2006).
ND: Magnesium deficiency and neurologic dis-
ease (Smalheiser and Swanson, 1994).
INN: Alzheimer?s and indomethacin (Smalheiser
and Swanson, 1996a).
estrogen: Alzheimer?s disease and estrogen
(Smalheiser and Swanson, 1996b).
Ca
2+
iPLA2: Schizophrenia and Calcium-
Independent Phospholipase A2 (Smalheiser
and Swanson, 1997).
The same subset of Medline as in each origi-
nal discovery is employed for replication, and
any abstracts containing a direct link between the
two terms are removed (note that including these
would not have affected the original discoveries as
these only used titles) ? thus any connections be-
tween A and C are necessarily hidden and require
at least one linking term.
The Raynaud-fish oil and migraine-magnesium
connections are the most commonly replicated
discoveries, while the remaining discoveries are
rarely explored. For CUI based investigations, this
is likely due to the difficulty of selecting a repre-
sentative CUI for the sought concepts. The sec-
ond concept in the Schizophrenia and Calcium-
Independent Phospholipase A2 connection is par-
ticularly tricky: UMLS suggests CUI C1418624
(PLA2G6 gene) as the most likely match, followed
by CUI C2830173 (Calcium-Independent Phos-
pholipase A2) as the second most likely. How-
ever, neither CUI is found in any relations in the
given date range by SemRep. Closer examina-
tion reveals that the Ca
2+
iPLA2 connections in
the 1960-1997 Medline range are between CUI
C0538273 (PLA2G6 protein, human). Not only
does this highlight the difficulty of the replication
task, it further motivates the need for a ?synonym?
(or related concept) list.
The number of linking terms found between
each pair of sought terms is presented in Table 1
(zero linking terms means the connection was not
found) for a subset of the filtering results. ST rep-
resents semantic type filtering, HF the breaking of
high frequency connections (a min subscript de-
noting the version which takes into account con-
nectivity of both CUIs), together with the thresh-
old value, and LT elimination of common linking
terms, again with the relevant threshold value.
While the Raynaud-fish oil connection appears
to be consistently produced by the system, Table 2
reveals the value of filtering: with no filtering, the
two linking terms are pure noise and the connec-
tion should not be made. Employing UMLS syn-
115
RD Arg Mg ND INN estrogen Ca
2+
iPLA2
No filtering 2 235 78 98 370 500 7
Synonyms (Sy) 6 173 58 65 296 415 16
Sy & LT-200 3 145 48 56 265 0 16
Sy & HF-2900 6 149 56 52 243 0 14
Sy & HF
min
-900 6 73 22 27 82 164 9
Sy & HF
min
-400 6 25 5 8 25 65 8
Sy & ST 4 130 47 43 234 331 13
Sy & ST & LT-200 3 108 41 38 207 0 13
Sy & ST & HF-2500 4 120 47 38 205 0 13
Sy & ST & HF
min
-900 6 73 22 27 82 164 9
Sy & ST & HF
min
-400 4 28 6 12 30 73 6
Table 1: Number of hidden links found during replication
onyms adds genuine linking terms,
6
and restrict-
ing by semantic types drops the remaining general
terms. Discarding common linking terms finds an-
timicrobial susceptibility to be a frequently used
linking term, and it is also dropped. A great ad-
vantage of the technique can be seen when con-
nections are made through hundreds of terms ?
in this case, higher thresholds (and thus more ag-
gressive filtering) can be employed to reduce the
number of linking terms to the most promising set.
Should these not be sufficient, the threshold can
be increased to produce more linking terms and as
such, the burden on the user in checking a large
number of linking terms when a hidden connec-
tion is suspected can be greatly reduced, without
sacrificing connections should more be needed.
Term NF Sy Sy ST LT-200
acetylsalicyclic acid ? X X X
antimicrobial susceptibility ? X X ?
blood viscosity ? X X X
brain infarction ? X X X
patient X X ? ?
volunteer helper X X ? ?
Table 2: Linking term analysis for RD
For example, common linking term filtering re-
moves the term estrogen from consideration as
therapeutic estrogen is a commonly used linking
term, making the estrogen-AD link impossible to
find. Linking term frequencies (on a 10,000 pair
sample) exceeding values from 50 to 200 (in incre-
ments of 50) were tested resulting in the removal
6
Note that the merging of synonyms is achieved with-
out the need to back off to general classes (e.g. (Srinivasan,
2004)), which have been observed to lead to connections
based on ?aboutness? rather than producing genuine hidden
knowledge (Beresi et al., 2008).
of between 1,902 and 227 CUIs. Therapeutic es-
trogen appears in all the lists. Similarly, the CUI
is dropped when high frequency connections are
broken using the first technique, which is based on
stoplists. This highlights the value of the second
high frequency connection technique, which only
discards particular connections (rather than CUIs)
and therapeutic estrogen CUI remains a search-
able CUI.
As shown, the system replicates most of the
previously published discoveries with its main as-
set being noise reduction: the number of linking
terms for a suspected connection (closed discov-
ery) can be greatly reduced to remove spurious
connections, with backoffs available to yield more
connections should more be required. For novel
applications (i.e. open discovery), the technique
greatly reduces the amount of hidden knowledge
generated from a source term A. For example, the
amount of hidden knowledge generated from so-
matomedin C drops from 82,601 CUIs when no
filtering is performed, to 3,005 CUIs with syn-
onym, semantic type and breaking connections
with frequency more than 200, which represents
a great reduction for a user who is likely looking
for a particular type of C term.
5 Conclusions and future work
We present and demonstrate the effectiveness of a
number of filtering methods, including two novel
techniques based on any LBD system built accord-
ing to the Swanson framework ? one approach
based on stoplist methods, but requiring no man-
ual intervention except for a user?s selection of a
threshold, and the second based on removing con-
nections when these are deemed to be likely to
116
contribute mainly noise. A great advantage of the
second approach is shown to be the fact that terms
are not directly discarded, as with a stoplist, and
thus a fairly common term can remain a source
term when required.
While the method is evaluated by replicating
known discoveries, we suggest that the noise re-
duction performed is ultimately leading to a much
more user friendly LBD system, and plan to inves-
tigate other evaluation approaches, such as times-
licing (Yetisgen-Yildiz and Pratt, 2009), as part of
future work.
Acknowledgements
Judita Preiss was supported by the EPSRC grant
EP/J008427/1: Language Processing for Litera-
ture Based Discovery in Medicine.
References
Ulises Cervino Beresi, Mark Baillie, and Ian Ruthven.
2008. Towards the evaluation of literature based dis-
covery. In Proceedings of the Workshop on Novel
Evaluation Methodologies (at ECIR 2008), pages 5?
13.
Wilco W. M. Fleuren, Stefan Verhoeven, Raoul Frijters,
Bart Heupers, Jan Polman, Ren?e van Schaik, Jacob
de Vlieg, and Wynand Alkema. 2011. Copub up-
date: Copub 5.0 a text mining system to answer bio-
logical questions. Nuclear Acids Research, 39 (Web
Server issue). doi:10.1093/nar/gkr310.
Michael D. Gordon and Robert K. Lindsay. 1996. To-
ward discovery support systems: a replication, re-
examination, and extension of Swanson?s work on
literature-based discovery of a connection between
Raynaud?s and fish oil. Journal of the American So-
ciety for Information Science, 47(2):116?128.
Thomas C. Rindflesch Hristovski D, Friedman C and
Peterlin B. 2006. Exploiting semantic relations
for literature-based discovery. In Proceedings of the
2006 AMIA Annual Symposium, pages 349?353.
Xiaohua Hu, Xiaodan Zhang, Illhoi Yoo, and Yanquing
Zang. 2006. A semantic approach for mining hid-
den links from complementary and non-interactive
biomedical literature. In SDM.
Thomas C. Rindflesch and Alan R. Aronson. 1994.
Ambiguity resolution while mapping free text to
the UMLS Metathesaurus. In J. G. Ozbolt, edi-
tor, Proceedings of the Eigheeth Annual Symposium
on Computer Appplications in Medical Care, pages
240?244.
Thomas C. Rindflesch and Marcelo Fiszman. 2003.
The interaction of domain knowledge and linguis-
tic structure in natural language processing: inter-
preting hypernymic propositions in biomedical text.
Journal of Biomedical Informatics, 36(6):462?477.
Neil R. Smalheiser and Don R. Swanson. 1994. As-
sessing a gap in the biomedical literature: Mag-
nesium deficiency and neurologic disease. Neuro-
science Research Communications, 15(1):1?9.
Neil R. Smalheiser and Don R. Swanson. 1996a. In-
domethacin and Alzheimer?s disease. Neurology,
46:583.
Neil R. Smalheiser and Don R. Swanson. 1996b. Link-
ing estrogen to Alzheimer?s disease. Neurology,
47:809?810.
Neil R. Smalheiser and Don R. Swanson. 1997.
Calcium-independent phospholipase a2 and
schizophrenia. Arch Gen Psychiatry, 55(8):752?
753.
Padmini Srinivasan. 2004. Text mining generating
hypotheses from medline. Journal of the Ameri-
can Society for Information Science and Technology,
55(5):396?413.
Don R. Swanson and Neil R. Smalheiser. 1997. An
interactive system for finding complementary litera-
tures: A stimulus to scientific discovery. Artificial
Intelligence, 91:183?203.
Don R. Swanson and Neil R. Smalheiser. 1999. Link
analysis of MEDLINE titles as an aid to scientific
discovery: Using Arrowsmith as an aid to scientific
discovery. Library Trends, 48:48?59.
Don R. Swanson, Neil R. Smalheiser, and Vetle I.
Torvik. 2006. Ranking indirect connnections in
literature-based discovery: The role of medical sub-
ject headings. Journal of the American Society for
Information Science and Technology, 57(11):1427?
1439.
Don R. Swanson. 1986. Fish oil, Raynaud?s syndrome,
and undiscovered public knowledge. Perspectives in
Biology and Medicine, 30:7?18.
Don R. Swanson. 1988. Migraine and magnesium ? 11
neglected connections. Perpectives in Biology and
Medicine, 31(4):526?557.
Don R. Swanson. 1990. Somatomedin c and arginine:
Implicit connections between mutually isolated lit-
eratures. Perspectives in Biology and Medicine,
33(2):157?186.
Marc Weeber, Rein Vos, Henny Klein, and Lolkje T. W.
de Jong-van den Berg. 2001. Using concepts in
literature-based discovery: Simulating Swanson?s
Reynaud ? fish oil and migraine ? magnesium dis-
coveries. Journal of the American Society for Infor-
mation Science and Technology, 52(7):548?557.
M. Yetisgen-Yildiz and W. Pratt. 2009. A new eval-
uation methodology for literature-based discovery.
Journal of Biomedical Informatics, 42(4):633?643.
117
