Proceedings of the EACL 2009 Student Research Workshop, pages 37?45,
Athens, Greece, 2 April 2009. c?2009 Association for Computational Linguistics
Structural Correspondence Learning for Parse Disambiguation
Barbara Plank
Alfa-informatica
University of Groningen, The Netherlands
b.plank@rug.nl
Abstract
The paper presents an application of
Structural Correspondence Learning
(SCL) (Blitzer et al, 2006) for domain
adaptation of a stochastic attribute-value
grammar (SAVG). So far, SCL has
been applied successfully in NLP for
Part-of-Speech tagging and Sentiment
Analysis (Blitzer et al, 2006; Blitzer
et al, 2007). An attempt was made
in the CoNLL 2007 shared task to ap-
ply SCL to non-projective dependency
parsing (Shimizu and Nakagawa, 2007),
however, without any clear conclusions.
We report on our exploration of applying
SCL to adapt a syntactic disambiguation
model and show promising initial results
on Wikipedia domains.
1 Introduction
Many current, effective natural language process-
ing systems are based on supervised Machine
Learning techniques. The parameters of such sys-
tems are estimated to best reflect the character-
istics of the training data, at the cost of porta-
bility: a system will be successful only as long
as the training material resembles the input that
the model gets. Therefore, whenever we have ac-
cess to a large amount of labeled data from some
?source? (out-of-domain), but we would like a
model that performs well on some new ?target?
domain (Gildea, 2001; Daume? III, 2007), we face
the problem of domain adaptation.
The need for domain adaptation arises in many
NLP tasks: Part-of-Speech tagging, Sentiment
Analysis, Semantic Role Labeling or Statistical
Parsing, to name but a few. For example, the per-
formance of a statistical parsing system drops in
an appalling way when a model trained on the Wall
Street Journal is applied to the more varied Brown
corpus (Gildea, 2001).
The problem itself has started to get attention
only recently (Roark and Bacchiani, 2003; Hara et
al., 2005; Daume? III and Marcu, 2006; Daume? III,
2007; Blitzer et al, 2006; McClosky et al, 2006;
Dredze et al, 2007). We distinguish two main ap-
proaches to domain adaptation that have been ad-
dressed in the literature (Daume? III, 2007): super-
vised and semi-supervised.
In supervised domain adaptation (Gildea, 2001;
Roark and Bacchiani, 2003; Hara et al, 2005;
Daume? III, 2007), besides the labeled source data,
we have access to a comparably small, but labeled
amount of target data. In contrast, semi-supervised
domain adaptation (Blitzer et al, 2006; McClosky
et al, 2006; Dredze et al, 2007) is the scenario in
which, in addition to the labeled source data, we
only have unlabeled and no labeled target domain
data. Semi-supervised adaptation is a much more
realistic situation, while at the same time also con-
siderably more difficult.
Studies on the supervised task have shown that
straightforward baselines (e.g. models based on
source only, target only, or the union of the data)
achieve a relatively high performance level and are
?surprisingly difficult to beat? (Daume? III, 2007).
Thus, one conclusion from that line of work is that
as soon as there is a reasonable (often even small)
amount of labeled target data, it is often more fruit-
ful to either just use that, or to apply simple adap-
tation techniques (Daume? III, 2007; Plank and van
Noord, 2008).
2 Motivation and Prior Work
While several authors have looked at the super-
vised adaptation case, there are less (and espe-
cially less successful) studies on semi-supervised
domain adaptation (McClosky et al, 2006; Blitzer
et al, 2006; Dredze et al, 2007). Of these, Mc-
Closky et al (2006) deal specifically with self-
training for data-driven statistical parsing. They
show that together with a re-ranker, improvements
37
are obtained. Similarly, Structural Correspon-
dence Learning (Blitzer et al, 2006; Blitzer et
al., 2007; Blitzer, 2008) has proven to be suc-
cessful for the two tasks examined, PoS tagging
and Sentiment Classification. In contrast, Dredze
et al (2007) report on ?frustrating? results on
the CoNLL 2007 semi-supervised adaptation task
for dependency parsing, i.e. ?no team was able
to improve target domain performance substan-
tially over a state of the art baseline?. In the
same shared task, an attempt was made to ap-
ply SCL to domain adaptation for data-driven de-
pendency parsing (Shimizu and Nakagawa, 2007).
The system just ended up at rank 7 out of 8 teams.
However, based on annotation differences in the
datasets (Dredze et al, 2007) and a bug in their
system (Shimizu and Nakagawa, 2007), their re-
sults are inconclusive.1 Thus, the effectiveness of
SCL is rather unexplored for parsing.
So far, most previous work on domain adapta-
tion for parsing has focused on data-driven sys-
tems (Gildea, 2001; Roark and Bacchiani, 2003;
McClosky et al, 2006; Shimizu and Nakagawa,
2007), i.e. systems employing (constituent or de-
pendency based) treebank grammars (Charniak,
1996). Parse selection constitutes an important
part of many parsing systems (Johnson et al,
1999; Hara et al, 2005; van Noord and Malouf,
2005; McClosky et al, 2006). Yet, the adaptation
of parse selection models to novel domains is a far
less studied area. This may be motivated by the
fact that potential gains for this task are inherently
bounded by the underlying grammar. The few
studies on adapting disambiguation models (Hara
et al, 2005; Plank and van Noord, 2008) have fo-
cused exclusively on the supervised scenario.
Therefore, the direction we explore in this
study is semi-supervised domain adaptation for
parse disambiguation. We examine the effec-
tiveness of Structural Correspondence Learning
(SCL) (Blitzer et al, 2006) for this task, a re-
cently proposed adaptation technique shown to be
effective for PoS tagging and Sentiment Analy-
sis. The system used in this study is Alpino, a
wide-coverage Stochastic Attribute Value Gram-
mar (SAVG) for Dutch (van Noord and Malouf,
2005; van Noord, 2006). For our empirical eval-
1As shown in Dredze et al (2007), the biggest problem
for the shared task was that the provided datasets were an-
notated with different annotation guidelines, thus the gen-
eral conclusion was that the task was ill-defined (Nobuyuki
Shimizu, personal communication).
uation we explore Wikipedia as primary test and
training collection.
In the sequel, we first introduce the parsing sys-
tem. Section 4 reviews Structural Correspondence
Learning and shows our application of SCL to
parse selection, including all our design choices.
In Section 5 we present the datasets, introduce the
process of constructing target domain data from
Wikipedia, and discuss interesting initial empiri-
cal results of this ongoing study.
3 Background: Alpino parser
Alpino (van Noord and Malouf, 2005; van Noord,
2006) is a robust computational analyzer for Dutch
that implements the conceptual two-stage parsing
approach. The system consists of approximately
800 grammar rules in the tradition of HPSG, and
a large hand-crafted lexicon, that together with a
left-corner parser constitutes the generation com-
ponent. For parse selection, Alpino employs a dis-
criminative approach based on Maximum Entropy
(MaxEnt). The output of the parser is dependency
structure based on the guidelines of CGN (Oost-
dijk, 2000).
The Maximum Entropy model (Berger et al,
1996; Ratnaparkhi, 1997; Abney, 1997) is a con-
ditional model that assigns a probability to every
possible parse ? for a given sentence s. The model
consists of a set of m feature functions fj(?) that
describe properties of parses, together with their
associated weights ?j . The denominator is a nor-
malization term where Y (s) is the set of parses
with yield s:
p?(?|s; ?) =
exp(
?m
j=1 ?jfj(?))
?
y?Y (s) exp(
?m
j=1 ?jfj(y)))
(1)
The parameters (weights) ?j can be estimated
efficiently by maximizing the regularized condi-
tional likelihood of a training corpus (Johnson et
al., 1999; van Noord and Malouf, 2005):
?? = arg max
?
logL(?) ?
?m
j=1 ?2j
2?2 (2)
where L(?) is the likelihood of the training data.
The second term is a regularization term (Gaus-
sian prior on the feature weights with mean zero
and variance ?). The estimated weights determine
the contribution of each feature. Features appear-
ing in correct parses are given increasing (posi-
tive) weight, while features in incorrect parses are
38
given decreasing (negative) weight. Once a model
is trained, it can be applied to choose the parse
with the highest sum of feature weights.
The MaxEnt model consists of a large set of
features, corresponding to instantiations of feature
templates that model various properties of parses.
For instance, Part-of-Speech tags, dependency re-
lations, grammar rule applications, etc. The cur-
rent standard model uses about 11,000 features.
We will refer to this set of features as original fea-
tures. They are used to train the baseline model on
the given labeled source data.
4 Structural Correspondence Learning
SCL (Structural Correspondence Learn-
ing) (Blitzer et al, 2006; Blitzer et al, 2007;
Blitzer, 2008) is a recently proposed domain
adaptation technique which uses unlabeled data
from both source and target domain to learn
correspondences between features from different
domains.
Before describing the algorithm in detail, let us
illustrate the intuition behind SCL with an exam-
ple, borrowed from Blitzer et al (2007). Suppose
we have a Sentiment Analysis system trained on
book reviews (domain A), and we would like to
adapt it to kitchen appliances (domain B). Fea-
tures such as ?boring? and ?repetitive? are com-
mon ways to express negative sentiment in A,
while ?not working? or ?defective? are specific to
B. If there are features across the domains, e.g.
?don?t buy?, with which the domain specific fea-
tures are highly correlated with, then we might
tentatively align those features.
Therefore, the key idea of SCL is to identify au-
tomatically correspondences among features from
different domains by modeling their correlations
with pivot features. Pivots are features occur-
ring frequently and behaving similarly in both do-
mains (Blitzer et al, 2006). They are inspired by
auxiliary problems from Ando and Zhang (2005).
Non-pivot features that correspond with many of
the same pivot-features are assumed to corre-
spond. Intuitively, if we are able to find good cor-
respondences among features, then the augmented
labeled source domain data should transfer better
to a target domain (where no labeled data is avail-
able) (Blitzer et al, 2006).
The outline of the algorithm is given in Figure 1.
The first step is to identify m pivot features oc-
curring frequently in the unlabeled data of both
Input: - labeled source data {(xs, ys)Nss=1}
- unlabeled data from both source and
target domain xul = xs, xt
1. Select m pivot features
2. Train m binary classifiers (pivot predictors)
3. Create matrix Wn?m of binary predictor
weight vectors W = [w1, .., wm], where n
is the number of nonpivot features in xul
4. Apply SVD to W : Wn?m =
Un?nDn?mV Tm?m where ? = UT[1:h,:]
are the h top left singular vectors of W .
5. Apply projection xs? and train a predictor
on the original and new features obtained
through the projection.
Figure 1: SCL algorithm (Blitzer et al, 2006).
domains. Then, a binary classifier is trained for
each pivot feature (pivot predictor) of the form:
?Does pivot feature l occur in this instance??. The
pivots are masked in the unlabeled data and the
aim is to predict them using non-pivot features.
In this way, we obtain a weight vector w for each
pivot predictor. Positive entries in the weight vec-
tor indicate that a non-pivot is highly correlated
with the respective pivot feature. Step 3 is to ar-
range the m weight vectors in a matrix W , where
a column corresponds to a pivot predictor weight
vector. Applying the projection W Tx (where x
is a training instance) would give us m new fea-
tures, however, for ?both computational and sta-
tistical reasons? (Blitzer et al, 2006; Ando and
Zhang, 2005) a low-dimensional approximation of
the original feature space is computed by applying
Singular Value Decomposition (SVD) on W (step
4). Let ? = UTh?n be the top h left singular vec-
tors of W (with h a dimension parameter and n
the number of non-pivot features). The resulting ?
is a projection onto a lower dimensional space Rh,
parameterized by h.
The final step of SCL is to train a linear predic-
tor on the augmented labeled source data ?x, ?x?.
In more detail, the original feature space x is aug-
mented with h new features obtained by apply-
ing the projection ?x. In this way, we can learn
weights for domain-specific features, which oth-
erwise would not have been observed. If ? con-
tains meaningful correspondences, then the pre-
39
dictor trained on the augmented data should trans-
fer well to the new domain.
4.1 SCL for Parse Disambiguation
A property of the pivot predictors is that they can
be trained from unlabeled data, as they represent
properties of the input. So far, pivot features on the
word level were used (Blitzer et al, 2006; Blitzer
et al, 2007; Blitzer, 2008), e.g. ?Does the bigram
not buy occur in this document?? (Blitzer, 2008).
Pivot features are the key ingredient for SCL,
and they should align well with the NLP task. For
PoS tagging and Sentiment Analysis, features on
the word level are intuitively well-related to the
problem at hand. For the task of parse disambigua-
tion based on a conditional model this is not the
case.
Hence, we actually introduce an additional and
new layer of abstraction, which, we hypothesize,
aligns well with the task of parse disambiguation:
we first parse the unlabeled data. In this way we
obtain full parses for given sentences as produced
by the grammar, allowing access to more abstract
representations of the underlying pivot predictor
training data (for reasons of efficiency, we here use
only the first generated parse as training data for
the pivot predictors, rather than n-best).
Thus, instead of using word-level features, our
features correspond to properties of the gener-
ated parses: application of grammar rules (r1,r2
features), dependency relations (dep), PoS tags
(f1,f2), syntactic features (s1), precedence (mf ),
bilexical preferences (z), apposition (appos) and
further features for unknown words, temporal
phrases, coordination (h,in year and p1, respec-
tively). This allows us to get a possibly noisy,
but more abstract representation of the underlying
data. The set of features used in Alpino is further
described in van Noord and Malouf (2005).
Selection of pivot features As pivot features
should be common across domains, here we re-
strict our pivots to be of the type r1,p1,s1 (the most
frequently occurring feature types). In more de-
tail, r1 indicates which grammar rule applied, p1
whether coordination conjuncts are parallel, and
s1 whether topicalization or long-distance depen-
dencies occurred. We count how often each fea-
ture appears in the parsed source and target do-
main data, and select those r1,p1,s1 features as
pivot features, whose count is > t, where t is a
specified threshold. In all our experiments, we set
t = 5000. In this way we obtained on average 360
pivot features, on the datasets described in Sec-
tion 5.
Predictive features As pointed out by Blitzer et
al. (2006), each instance will actually contain fea-
tures which are totally predictive of the pivot fea-
tures (i.e. the pivot itself). In our case, we ad-
ditionally have to pay attention to ?more specific?
features, e.g. r2 is a feature that extends r1, in the
sense that it incorporates more information than
its parent (i.e. which grammar rules applied in the
construction of daughter nodes). It is crucial to re-
move these predictive features when creating the
training data for the pivot predictors.
Matrix and SVD Following Blitzer et al (2006)
(which follow Ando and Zhang (2005)), we only
use positive entries in the pivot predictors weight
vectors to compute the SVD. Thus, when con-
structing the matrix W , we disregard all nega-
tive entries in W and compute the SVD (W =
UDV T ) on the resulting non-negative sparse ma-
trix. This sparse representation saves both time
and space.
4.2 Further practical issues of SCL
In practice, there are more free parameters and
model choices (Ando and Zhang, 2005; Ando,
2006; Blitzer et al, 2006; Blitzer, 2008) besides
the ones discussed above.
Feature normalization and feature scaling.
Blitzer et al (2006) found it necessary to normal-
ize and scale the new features obtained by the pro-
jection ?, in order to ?allow them to receive more
weight from a regularized discriminative learner?.
For each of the features, they centered them by
subtracting out the mean and normalized them to
unit variance (i.e. x ? mean/sd). They then
rescaled the features by a factor ? found on held-
out data: ??x.
Restricted Regularization. When training the
supervised model on the augmented feature space
?x, ?x?, Blitzer et al (2006) only regularize the
weight vector of the original features, but not
the one for the new low-dimensional features.
This was done to encourage the model to use
the new low-dimensional representation rather
than the higher-dimensional original representa-
tion (Blitzer, 2008).
Dimensionality reduction by feature type. An
extension suggested in Ando and Zhang (2005) is
40
to compute separate SVDs for blocks of the matrix
W corresponding to feature types (as illustrated in
Figure 2), and then to apply separate projection
for every type. Due to the positive results in Ando
(2006), Blitzer et al (2006) include this in their
standard setting of SCL and report results using
block SVDs only.
Figure 2: Illustration of dimensionality reduction
by feature type (Ando and Zhang, 2005). The grey
area corresponds to a feature type (submatrix of
W ) on which the SVD is computed (block SVD);
the white area is regarded as fixed to zero matrices.
5 Experiments and Results
5.1 Experimental design
The base (source domain) disambiguation model
is trained on the Alpino Treebank (van Noord,
2006) (newspaper text), which consists of ap-
proximately 7,000 sentences and 145,000 tokens.
For parameter estimation of the disambiguation
model, in all reported experiments we use the
TADM2 toolkit (toolkit for advanced discrimina-
tive training), with a Gaussian prior (?2=1000)
and the (default) limited memory variable metric
estimation technique (Malouf, 2002).
For training the binary pivot predictors, we use
the MegaM3 Optimization Package with the so-
called ?bernoulli implicit? input format. To com-
pute the SVD, we use SVDLIBC.4
The output of the parser is dependency struc-
ture. A standard evaluation metric is to measure
the amount of generated dependencies that are
identical to the stored dependencies (correct la-
beled dependencies), expressed as f-score. An al-
ternative measure is concept accuracy (CA), which
is similar to f-score, but allows possible discrep-
ancy between the number of returned dependen-
cies (van Noord, 2006; Plank and van Noord,
2http://tadm.sourceforge.net/
3http://www.cs.utah.edu/?hal/megam/
4http://tedlab.mit.edu/?dr/svdlibc/
2008). CA is usually slightly lower than f-score.
Let Dip be the number of dependencies produced
by the parser for sentence i. Dig is the number of
dependencies in the treebank parse, and Dio is the
number of correct dependencies produced by the
parser. Then,
CA = Do?
i max(Dig,Dip)
If we want to compare the performance of dis-
ambiguation models, we can employ the ? mea-
sure (van Noord and Malouf, 2005; van Noord,
2007). Intuitively, it tells us how much of the dis-
ambiguation problem has been solved.
? = CA? baseoracle ? base ? 100
In more detail, the ? measure incorporates an up-
per and lower bound: base measures the accu-
racy of a model that simply selects the first parse
for each sentence; oracle represents the accuracy
achieved by a model that always selects the best
parse from the set of potential parses (within the
coverage of the parser). In addition, we also re-
port relative error reduction (rel.er), which is the
relative difference in ? scores for two models.
As target domain, we consider the Dutch part
of Wikipedia as data collection, described in the
following.
5.2 Wikipedia as resource
In our experiments, we exploit Wikipedia both as
testset and as unlabeled data source. We assume
that in order to parse data from a very specific do-
main, say about the artist Prince, then data related
to that domain, like information about the New
Power Generation, the Purple rain movie, or other
American singers and artists, should be of help.
Thus, we exploit Wikipedia and its category sys-
tem to gather domain-specific target data.
Construction of target domain data In more
detail, we use the Dutch part of Wikipedia pro-
vided by WikiXML,5 a collection of Wikipedia ar-
ticles converted to XML format. As the corpus is
encoded in XML, we can exploit general purpose
XML Query Languages, such as XQuery, Xslt and
XPath, to extract relevant information from the
Wikipedia corpus.
Given a wikipage p, with c ? categories(p),
we can identify pages related to p of various
5http://ilps.science.uva.nl/WikiXML/
41
types of ?relatedness?: directly related pages (those
that share a category, i.e. all p? where ?c? ?
categories(p?) such that c = c?), or alterna-
tively, pages that share a sub- or supercategory
of p, i.e. p? where c? ? categories(p?) and c? ?
sub categories(p) or c? ? super categories(p).
For example, Figure 3 shows the categories ex-
tracted for the Wikipedia article about pope Jo-
hannes Paulus II.
<wikipage id="6677">
<cat t="direct" n="Categorie:Paus"/>
<cat t="direct" n="Categorie:Pools_theoloog"/>
<cat t="super" n="Categorie:Religieus leider"/>
<cat t="super" n="Categorie:Rooms-katholiek persoon"/>
<cat t="super" n="Categorie:Vaticaanstad"/>
<cat t="super" n="Categorie:Bisschop"/>
<cat t="super" n="Categorie:Kerkgeschiedenis"/>
<cat t="sub" n="Categorie:Tegenpaus"/>
<cat t="super" n="Categorie:Pools persoon"/>
</wikipage>
Figure 3: Example of extracted Wikipedia cate-
gories for a given article (direct, sup- and subcats).
To create the set of related pages for a given ar-
ticle p, we proceed as follows:
1. Find sub- and supercategories of p
2. Extract all pages that are related to p (through
sharing a direct, sub or super category)
3. Optionally, filter out certain pages
In our empirical setup, we followed Blitzer et al
(2006) and tried to balance the size of source and
target data. Thus, depending on the size of the re-
sulting target domain dataset, and the ?broadness?
of the categories involved in creating it, we might
wish to filter out certain pages. We implemented
a filter mechanism that excludes pages of a cer-
tain category (e.g. a supercategory that is hypoth-
esized to be ?too broad?). Alternatively, we might
have used a filter mechanism that excludes certain
pages directly.
In our experiments, we always included pages
that are directly related to a page of inter-
est, and those that shared a subcategory. Of
course, the page itself is not included in that
dataset. With regard to supercategories, we usu-
ally included all pages having a category c ?
super categories(p), unless stated otherwise.
Test collection Our testset consists of a selection
of Wikipedia articles that have been manually cor-
rected in the course of the D-Coi/LASSY project.6
6Ongoing project, see http://www.let.rug.nl/
?vannoord/Lassy/
An overview of the testset including size indica-
tions is given in Table 1. Table 2 provides infor-
mation on the target domain datasets constructed
from Wikipedia.
Wiki/DCOI ID Title Sents
6677/026563 Prince (musician) 358
6729/036834 Paus Johannes Paulus II 232
182654/041235 Augustus De Morgan 259
Table 1: Size of test datasets.
Related to Articles Sents Tokens Relationship
Prince 290 9,772 145,504 filtered super
Paus 445 8,832 134,451 all
De Morgan 394 8,466 132,948 all
Table 2: Size of related unlabeled data; relation-
ship indicates whether all related pages are used
or some are filtered out (see section 5.2).
5.3 Empirical Results
For all reported results, we randomly select n =
200 maximum number of parses per sentence for
evaluation.
Baseline accuracies Table 3 shows the baseline
performance (of the standard Alpino model) on the
various Wikipedia testsets (CA, f-score). The third
and fourth column indicate the upper- and lower
bound measures (defined in section 5.1).
Title CA f-score base oracle
Prince (musician) 85.03 85.38 71.95 88.70
Paus Johannes Paulus II 85.72 86.32 74.30 89.09
Augustus De Morgan 80.09 80.61 70.08 83.52
Table 3: Baseline results.
While the parser normally operates on an accu-
racy level of roughly 88-89% (van Noord, 2007)
on its own domain (newspaper text), the accu-
racy on these subdomains drops to around 85%.
The biggest performance decrease (to 80%) was
on the article about the British logician and math-
ematician De Morgan. This confirms the intu-
ition that this specific subdomain is the ?hardest?,
given that mathematical expressions might emerge
in the data (e.g. ?Wet der distributiviteit : a(b+c)
= ab+ac? - distributivity law).
SCL results Table 4 shows the results of our in-
stantiation of SCL for parse disambiguation, with
varying h parameter (dimensionality parameter;
42
h = 25 means that applying the projection x? re-
sulted in adding 25 new features to every source
domain instance).
CA f-score ? rel.er.
baseline Prince 85.03 85.38 78.06 0.00
SCL[+/-], h = 25 85.12 85.46 78.64 2.64
SCL[+/-], h = 50 85.29 85.63 79.66 7.29
SCL[+/-], h = 100 85.19 85.53 79.04 4.47
SCL[+/-], h = 200 85.21 85.54 79.18 5.10
baseline Paus 85.72 86.32 77.23 0.00
SCL[+/-], h = 25 85.87 86.48 78.26 4.52
SCL[+/-], h = 50 85.82 86.43 77.87 2.81
SCL[+/-], h = 100 85.87 86.49 78.26 4.52
SCL[+/-], h = 200 85.87 86.48 78.26 4.52
baseline DeMorgan 80.09 80.61 74.44 0.00
SCL[+/-], h = 25 80.15 80.67 74.92 1.88
SCL[+/-], h = 50 80.12 80.64 74.68 0.94
SCL[+/-], h = 100 80.12 80.64 74.68 0.94
SCL[+/-], h = 200 80.15 80.67 74.91 1.88
Table 4: Results of our instantiation of SCL (with
varying h parameter and no feature normaliza-
tion).
The results show a (sometimes) small but con-
sistent increase in absolute performance on all
testsets over the baseline system (up to +0.26
absolute CA score), as well as an increase in ?
measure (absolute error reduction). This corre-
sponds to a relative error reduction of up to 7.29%.
Thus, our first instantiation of SCL for parse dis-
ambiguation indeed shows promising results.
We can confirm that changing the dimensional-
ity parameter h has rather little effect (Table 4),
which is in line with previous findings (Ando and
Zhang, 2005; Blitzer et al, 2006). Thus we might
fix the parameter and prefer smaller dimensionali-
ties, which saves space and time.
Note that these results were obtained without
any of the additional normalization, rescaling,
feature-specific regularization, or block SVD is-
sues, etc. (discussed in section 4.2). We used the
same Gaussian regularization term (?2=1000) for
all features (original and new features), and did
not perform any feature normalization or rescal-
ing. This means our current instantiation of SCL
is an actually simplified version of the original
SCL algorithm, applied to parse disambiguation.
Of course, our results are preliminary and, rather
than warranting many definite conclusions, en-
courage further exploration of SCL and related
semi-supervised adaptation techniques.
5.4 Additional Empirical Results
In the following, we describe additional results ob-
tained by extensions and/or refinements of our cur-
rent SCL instantiation.
Feature normalization. We also tested fea-
ture normalization (as described in Section 4.2).
While Blitzer et al (2006) found it necessary to
normalize (and scale) the projection features, we
did not observe any improvement by normalizing
them (actually, it slightly degraded performance in
our case). Thus, we found this step unnecessary,
and currently did not look at this issue any further.
A look at ? To gain some insight of which kind
of correspondences SCL learned in our case, we
started to examine the rows of ?. Recall that ap-
plying a row of the projection matrix ?i to a train-
ing instance x gives us a new real-valued fea-
ture. If features from different domains have sim-
ilar entries (scores) in the projection row, they
are assumed to correspond (Blitzer, 2008). Fig-
ure 4 shows example of correspondences that SCL
found in the Prince dataset. The first column rep-
resents the score of a feature. The labels wiki
and alp indicate the domain of the features, re-
spectively. For readability, we here grouped the
features obtaining similar scores.
0.00010248|dep35(?Chaka Khan?,name(?PER?),hd/su,verb,ben)|wiki
0.00010248|dep35(de,det,hd/det,adj,?Afro-Amerikaanse?)|wiki
0.00010248|dep35(?Yvette Marie Stevens?,name(?PER?),hd/app,
noun,zangeres)|wiki
0.000102772|dep34(leraar,noun,hd/su,verb)|alp
0.000161095|dep34(commissie,noun,hd/obj1,prep)|16|alp
0.00016113|dep34(?Confessions Tour?,name,hd/obj1,prep)|2|wiki
0.000161241|dep34(orgel,noun,hd/obj1,prep)|1|wiki
0.000217698|dep34(tournee,noun,hd/su,verb)|1|wiki
0.000223301|dep34(regisseur,noun,hd/su,verb)|15|wiki
0.000224517|dep34(voorsprong,noun,hd/su,verb)|2|alp
0.000224684|dep34(wetenschap,noun,hd/su,verb)|2|alp
0.000226617|dep34(pop_rock,noun,hd/su,verb)|1|wiki
0.000228918|dep34(plan,noun,hd/su,verb)|9|alp
Figure 4: Example projection from ? (row 2).
SCL clustered information about ?Chaka Khan?,
an ?Afro-Amerikaanse? ?zangeres? (afro-american
singer) whose real name is ?Yvette Marie
Stevens?. She had close connections to Prince,
who even wrote one of her singles. These features
got aligned to the Alpino feature ?leraar? (teacher).
Moreover, SCL finds that ?tournee?, ?regisseur?
and ?pop rock? in the Prince domain behave like
?voorsprong? (advance), ?wetenschap? (research)
and ?plan? as possible heads in a subject relation
in the newspaper domain. Similarly, correspon-
43
dences between the direct object features ?Con-
fessions Tour? and ?orgel? (pipe organ) to ?com-
missie? (commission) are discovered.
More unlabeled data In the experiments so far,
we balanced the amount of source and target data.
We started to examine the effect of more unla-
beled target domain data. For the Prince dataset,
we included all supercategories in constructing
the related target domain data. The so obtained
dataset contains: 859 articles, 29,186 sentences
and 385,289 tokens; hence, the size approximately
tripled (w.r.t. Table 2). Table 5 shows the effect of
using this larger dataset for SCL with h = 25. The
accuracy increases (from 85.12 to 85.25). Thus,
there seems to be a positive effect (to be investi-
gated further).
CA f-score ? rel.er.
baseline Prince 85.03 85.38 78.06 0.00
SCL[+/-], h = 25, all 85.25 85.58 79.42 6.20
Table 5: First result on increasing unlabeled data.
Dimensionality reduction by feature type We
have started to implement the extension discussed
in section 4.2, i.e. perform separate dimension-
ality reductions based on blocks of nonpivot fea-
tures. We clustered nonpivots (see section 4.1 for
a description) into 9 types (ordered in terms of
decreasing cluster size): dep, f1/f2 (pos), r1/r2
(rules), appos person, mf, z, h1, in year, dist. For
each type, a separate SVD was computed on sub-
matrix Wt (illustrated in Figure 2). Then, sepa-
rate projections were applied to every training in-
stance.
The results of these experiments on the Prince
dataset are shown in Figure 5. Applying SCL with
dimensionality reduction by feature type (SCL
block) results in a model that performs better (CA
85.27, ? 79.52, rel.er. 6.65%) than the model with
no feature split (no block SVDs), thus obtaining a
relative error reduction of 6.65% over the baseline.
The same figure also shows what happens if we
remove a specific feature type at a time; the appo-
sition features contribute the most on this Prince
domain. As a fact, one third of the sentences in
the Prince testset contain constructions with appo-
sitions (e.g. about film-, album- and song titles).
6 Conclusions and Future Work
The paper presents an application of Structural
Correspondence Learning (SCL) to parse disam-
Figure 5: Results of dimensionality reduction by
feature type, h = 25; block SVD included all 9
feature types; the right part shows the accuracy
when one feature type was removed.
biguation. While SCL has been successfully
applied to PoS tagging and Sentiment Analy-
sis (Blitzer et al, 2006; Blitzer et al, 2007), its
effectiveness for parsing was rather unexplored.
The empirical results show that our instantiation
of SCL to parse disambiguation gives promising
initial results, even without the many additional
extensions on the feature level as done in Blitzer
et al (2006). We exploited Wikipedia as pri-
mary resource, both for collecting unlabeled tar-
get domain data, as well as test suite for empirical
evaluation. On the three examined datasets, SCL
slightly but constantly outperformed the baseline.
Applying SCL involves many design choices and
practical issues, which we tried to depict here in
detail. A novelty in our application is that we
first actually parse the unlabeled data from both
domains. This allows us to get a possibly noisy,
but more abstract representation of the underlying
data on which the pivot predictors are trained.
In the near future, we plan to extend the work on
semi-supervised domain adaptation for parse dis-
ambiguation, viz. (1) further explore/refine SCL
(block SVDs, varying amount of target domain
data, other testsets, etc.), and (2) examine self-
training. Studies on the latter have focused mainly
on generative, constituent based, i.e. data-driven
parsing systems. Furthermore, from a machine
learning point of view, it would be interesting to
know a measure of corpus similarity to estimate
the success of porting an NLP system from one do-
main to another. This relates to the general ques-
tion of what is meant by domain.
44
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23:597?618.
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817?1853.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL).
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?72.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney, Aus-
tralia.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.
John Blitzer. 2008. Domain Adaptation of Natural
Language Processing Systems. Ph.D. thesis, Uni-
versity of Pennsylvania.
Eugene Charniak. 1996. Tree-bank grammars. In In
Proceedings of the Thirteenth National Conference
on Artificial Intelligence, pages 1031?1036.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Conference of the Association for Compu-
tational Linguistics (ACL), Prague, Czech Republic.
Mark Dredze, John Blitzer, Pratha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adap-
tation for parsing. In Proceedings of the CoNLL
Shared Task Session - Conference on Natural Lan-
guage Learning, Prague, Czech Republic.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Tadayoshi Hara, Miyao Yusuke, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an hpsg parser to a new domain. In Pro-
ceedings of the International Joint Conference on
Natural Language Processing.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of the 37th Annual Meeting of the ACL.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL-2002), Taipei.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Nelleke Oostdijk. 2000. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of
Second International Conference on Language Re-
sources and Evaluation (LREC), pages 887?894.
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach to
domain adaptation of a syntactic disambiguation
model. In Proceedings of the Workshop on Cross-
Framework and Cross-Domain Parser Evaluation
(PE), Manchester, August.
A. Ratnaparkhi. 1997. A simple introduction to max-
imum entropy models for natural language process-
ing. Technical report, Institute for Research in Cog-
nitive Science, University of Pennsylvania.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised pcfg adaptation to novel domains.
In In Proceedings of the Human Language Technol-
ogy Conference and Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Nobuyuki Shimizu and Hiroshi Nakagawa. 2007.
Structural correspondence learning for dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007.
Gertjan van Noord and Robert Malouf. 2005.
Wide coverage parsing with stochastic at-
tribute value grammars. Draft available from
http://www.let.rug.nl/?vannoord. A preliminary ver-
sion of this paper was published in the Proceedings
of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20?42,
Leuven.
Gertjan van Noord. 2007. Using self-trained bilexi-
cal preferences to improve disambiguation accuracy.
In Proceedings of the Tenth International Confer-
ence on Parsing Technologies. IWPT 2007, Prague.,
pages 1?10, Prague.
45
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Comparison of Structural Correspondence Learning and Self-training for
Discriminative Parse Selection
Barbara Plank
University of Groningen, The Netherlands
b.plank@rug.nl
Abstract
This paper evaluates two semi-supervised
techniques for the adaptation of a parse selec-
tion model to Wikipedia domains. The tech-
niques examined are Structural Correspon-
dence Learning (SCL) (Blitzer et al, 2006)
and Self-training (Abney, 2007; McClosky et
al., 2006). A preliminary evaluation favors the
use of SCL over the simpler self-training tech-
niques.
1 Introduction and Motivation
Parse selection constitutes an important part of many
parsing systems (Hara et al, 2005; van Noord and
Malouf, 2005; McClosky et al, 2006). Yet, there
is little to no work focusing on the adaptation of
parse selection models to novel domains. This is
most probably due to the fact that potential gains
for this task are inherently bounded by the under-
lying grammar. The few studies on adapting parse
disambiguation models, like Hara et al (2005), have
focused exclusively on supervised domain adapta-
tion, i.e. one has access to a comparably small, but
labeled amount of target data. In contrast, in semi-
supervised domain adaptation one has only unla-
beled target data. It is a more realistic situation, but
at the same time also considerably more difficult.
In this paper we evaluate two semi-supervised
approaches to domain adaptation of a discrimina-
tive parse selection model. We examine Struc-
tural Correspondence Learning (SCL) (Blitzer et
al., 2006) for this task, and compare it to several
variants of Self-training (Abney, 2007; McClosky et
al., 2006). For empirical evaluation (section 4) we
use the Alpino parsing system for Dutch (van Noord
and Malouf, 2005). As target domain, we exploit
Wikipedia as primary test and training collection.
2 Previous Work
So far, Structural Correspondence Learning has
been applied successfully to PoS tagging and Sen-
timent Analysis (Blitzer et al, 2006; Blitzer et
al., 2007). An attempt was made in the CoNLL
2007 shared task to apply SCL to non-projective de-
pendency parsing (Shimizu and Nakagawa, 2007).
However, the system just ended up at rank 7 out
of 8 teams. Based on annotation differences in the
datasets (Dredze et al, 2007) and a bug in their sys-
tem (Shimizu and Nakagawa, 2007), their results are
inconclusive. A recent attempt (Plank, 2009) shows
promising results on applying SCL to parse disam-
biguation. In this paper, we extend that line of work
and compare SCL to bootstrapping approaches such
as self-training.
Studies on self-training have focused mainly on
generative, constituent based parsing (Steedman et
al., 2003; McClosky et al, 2006; Reichart and Rap-
poport, 2007). Steedman et al (2003) as well as Re-
ichart and Rappoport (2007) examine self-training
for PCFG parsing in the small seed case (< 1k la-
beled data), with different results. In contrast, Mc-
Closky et al (2006) focus on large seeds and exploit
a reranking-parser. Improvements are obtained (Mc-
Closky et al, 2006; McClosky and Charniak, 2008),
showing that a reranker is necessary for successful
self-training in such a high-resource scenario. While
they self-trained a generative model, we examine
self-training and SCL for semi-supervised adapta-
tion of a discriminative parse selection system.
37
3 Semi-supervised Domain Adaptation
3.1 Structural Correspondence Learning
Structural Correspondence Learning (Blitzer et al,
2006) exploits unlabeled data from both source and
target domain to find correspondences among fea-
tures from different domains. These correspon-
dences are then integrated as new features in the la-
beled data of the source domain. The outline of SCL
is given in Algorithm 1.
The key to SCL is to exploit pivot features to au-
tomatically identify feature correspondences. Piv-
ots are features occurring frequently and behaving
similarly in both domains (Blitzer et al, 2006).
They correspond to auxiliary problems in Ando and
Zhang (2005). For every such pivot feature, a binary
classifier is trained (step 2 of Algorithm 1) by mask-
ing the pivot feature in the data and trying to predict
it with the remaining non-pivot features. Non-pivots
that correlate with many of the same pivots are as-
sumed to correspond. These pivot predictor weight
vectors thus implicitly align non-pivot features from
source and target domain. Intuitively, if we are able
to find good correspondences through ?linking? piv-
ots, then the augmented source data should transfer
better to a target domain (Blitzer et al, 2006).
Algorithm 1 SCL (Blitzer et al, 2006)
1: Select m pivot features.
2: Train m binary classifiers (pivot predictors). Cre-
ate matrix Wn?m of binary predictor weight vectors
W = [w1, .., wm], with n number of nonpivots.
3: Dimensionality Reduction. Apply SVD to W :
Wn?m = Un?nDn?mV Tm?m and select ? = UT[1:h,:]
(the h top left singular vectors of W ).
4: Train a new model on the original and new features
obtained by applying the projection x ? ?.
SCL for Discriminative Parse Selection So far,
pivot features on the word level were used (Blitzer
et al, 2006; Blitzer et al, 2007). However, for parse
disambiguation based on a conditional model they
are irrelevant. Hence, we follow Plank (2009) and
actually first parse the unlabeled data. This allows
a possibly noisy, but more abstract representation
of the underlying data. Features thus correspond to
properties of parses: application of grammar rules
(r1,r2 features), dependency relations (dep), PoS
tags (f1,f2), syntactic features (s1), precedence (mf ),
bilexical preferences (z), apposition (appos) and fur-
ther features for unknown words, temporal phrases,
coordination (h,in year and p1, respectively). These
features are further described in van Noord and Mal-
ouf (2005).
Selection of pivot features As pivot features
should be common across domains, here we restrict
our pivots to be of the type r1,p1,s1 (the most fre-
quently occurring feature types). In more detail, r1
indicates which grammar rule applied, p1 whether
coordination conjuncts are parallel, and s1 whether
local/non-local extraction occurred. We count how
often each feature appears in the parsed source and
target domain data, and select those r1,p1,s1 fea-
tures as pivot features, whose count is > t, where
t is a specified threshold. In all our experiments, we
set t = 5000. In this way we obtained on average
360 pivot features, on the datasets described in Sec-
tion 4.
3.2 Self-training
Self-training (Algorithm 2) is a simple single-view
bootstrapping algorithm. In self-training, the newly
labeled instances are taken at face value and added
to the training data.
There are many possible ways to instantiate self-
training (Abney, 2007). One variant, introduced in
Abney (2007) is the notion of ?(in)delibility?: in the
delible case the classifier relabels all of the unla-
beled data from scratch in every iteration. The clas-
sifier may become unconfident about previously se-
lected instances and they may drop out (Steven Ab-
ney, personal communication). In contrast, in the
indelible case, labels once assigned do not change
again (Abney, 2007).
In this paper we look at the following variants of
self-training:
? single versus multiple iterations,
? selection versus no selection (taking all self-
labeled data or selecting presumably higher
quality instances); different scoring functions
for selection,
? delibility versus indelibility for multiple itera-
tions.
38
Algorithm 2 Self-training (indelible) (Abney, 2007).
1: L0 is labeled [seed] data, U is unlabeled data
2: c? train(L0)
3: repeat
4: L? L + select(label(U ? L, c))
5: c? train(L)
6: until stopping criterion is met
Scoring methods We examine three simple scor-
ing functions for instance selection: i) Entropy
(??y?Y (s) p(?|s, ?) log p(?|s, ?)). ii) Number of
parses (|Y (s)|); and iii) Sentence Length (|s|).
4 Experiments and Results
Experimental Design The system used in this
study is Alpino, a two-stage dependency parser for
Dutch (van Noord and Malouf, 2005). The first
stage consists of a HPSG-like grammar that consti-
tutes the parse generation component. The second
stage is a Maximum Entropy (MaxEnt) parse selec-
tion model. To train the MaxEnt model, parame-
ters are estimated based on informative samples (Os-
borne, 2000). A parse is added to the training data
with a score indicating its ?goodness? (van Noord
and Malouf, 2005). The score is obtained by com-
paring it with the gold standard (if available; other-
wise the score is approximated through parse proba-
bility).
The source domain is the Alpino Treebank (van
Noord and Malouf, 2005) (newspaper text; approx.
7,000 sentences; 145k tokens). We use Wikipedia
both as testset and as unlabeled target data source.
We assume that in order to parse data from a very
specific domain, say about the artist Prince, then
data related to that domain, like information about
the New Power Generation, the Purple rain movie,
or other American singers and artists, should be of
help. Thus, we exploit Wikipedia?s category system
to gather domain-specific target data. In our empiri-
cal setup, we follow Blitzer et al (2006) and balance
the size of source and target data. Thus, depending
on the size of the resulting target domain dataset, and
the ?broadness? of the categories involved in creat-
ing it, we might wish to filter out certain pages. We
implemented a filter mechanism that excludes pages
of a certain category (e.g. a supercategory that is hy-
pothesized to be ?too broad?). Further details about
the dataset construction are given in (Plank, 2009).
Table 1 provides information on the target domain
datasets constructed from Wikipedia.
Related to Articles Sents Tokens Relationship
Prince 290 9,772 145,504 filtered super
Paus 445 8,832 134,451 all
DeMorgan 394 8,466 132,948 all
Table 1: Size of related unlabeled data; relationship in-
dicates whether all related pages are used or some are
filtered out.
The size of the target domain testsets is given in
Table 2. As evaluation measure concept accuracy
(CA) (van Noord and Malouf, 2005) is used (similar
to labeled dependency accuracy).
The training data for the pivot predictors are the
1-best parses of source and target domain data as
selected by the original Alpino model. We report
on results of SCL with dimensionality parameter set
to h = 25, and remaining settings identical to Plank
(2009) (i.e., no feature-specific regularization and no
feature normalization and rescaling).
Baseline Table 2 shows the baseline accuracies
(model trained on labeled out-of-domain data) on
the Wikipedia testsets (last column: size in number
of sentences). The second and third column indicate
lower (first parse) and upper- (oracle) bounds.
Wikipedia article baseline first oracle sent
Prince (musician) 85.03 71.95 88.70 357
Paus Johannes Paulus II 85.72 74.30 89.09 232
Augustus De Morgan 80.09 70.08 83.52 254
Table 2: Supervised Baseline results.
SCL and Self-training results The results for
SCL (Table 3) show a small, but consistent increase
in absolute performance on all testsets over the base-
lines (up to +0.27 absolute CA or 7.34% relative
error reduction, which is significant at p < 0.05 ac-
cording to sign test).
In contrast, basic self-training (Table 3) achieves
roughly only baseline accuracy and lower perfor-
mance than SCL, with one exception. On the De-
Morgan testset, self-training scores slightly higher
than SCL. However, the improvements of both SCL
and self-training are not significant on this rather
39
small testset. Indeed, self-training scores better than
the baseline on only 5 parses out of 254, while its
performance is lower on 2, leaving only 3 parses that
account for the difference.
CA ? Rel.ER
Prince baseline 85.03 78.06 0.00
SCL ? 85.30 79.67 7.34
Self-train (all-at-once) 85.08 78.38 1.46
Paus baseline 85.72 77.23 0.00
SCL 85.82 77.87 2.81
Self-train (all-at-once) 85.78 77.62 1.71
DeMorgan baseline 80.09 74.44 0.00
SCL 80.15 74.92 1.88
Self-train (all-at-once) 80.24 75.63 4.65
Table 3: Results of SCL and self-training (single itera-
tion, no selection). Entries marked with ? are statistically
significant at p < 0.05. The ? score incorporates upper-
and lower-bounds.
To gauge whether other instantiations of self-
training are more effective, we evaluated the self-
training variants introduced in section 3.2 on the
Prince dataset. In the iterative setting, we fol-
low Steedman et al (2003) and parse 30 sentences
from which 20 are selected in every iteration.
With regard to the comparison of delible versus
indelible self-training (whether labels may change),
our empirical findings shows that the two cases
achieve very similar performance; the two curves
highly overlap (Figure 1). The accuracies of both
curves fluctuate around 85.13, showing no upward
or downward trend. In general, however, indelibility
is preferred since it takes considerably less time (the
classifier does not have to relabel U from scratch in
every iteration). In addition, we tested EM (which
uses all unlabeled data in each iteration). Its per-
formance is consistently lower, varying around the
baseline.
Figure 2 compares several self-training variants
with the supervised baseline and SCL. It summa-
rizes the effect of i) selection versus no selection
(and various selection techniques) as well as ii) sin-
gle versus multiple iterations of self-training. For
clarity, the figure shows the learning curve of the
best selection technique only, but depicts the perfor-
mance of the various selection techniques in a single
iteration (non-solid lines).
In the iterative setting, taking the whole self-
labeled data and not selecting certain instances (grey
curve in Figure 2) degrades performance. In con-
trast, selecting shorter sentences slightly improves
accuracy, and is the best selection method among
the ones tested (shorter sentences, entropy, fewer
parses).
For all self-training instantiations, running multi-
ple iterations is on average just the same as running
a single iteration (the non-solid lines are roughly the
average of the learning curves). Thus there is no real
need to run several iterations of self-training.
The main conclusion is that in contrast to SCL,
none of the self-training instantiations achieves a
significant improvement over the baseline.
5 Conclusions and Future Work
The paper compares Structural Correspondence
Learning (Blitzer et al, 2006) with (various in-
stances of) self-training (Abney, 2007; McClosky
et al, 2006) for the adaptation of a parse selection
model to Wikipedia domains.
The empirical findings show that none of the eval-
uated self-training variants (delible/indelible, single
versus multiple iterations, various selection tech-
niques) achieves a significant improvement over the
baseline. The more ?indirect? exploitation of unla-
beled data through SCL is more fruitful than pure
self-training. Thus, favoring the use of the more
complex method, although the findings are not con-
firmed on all testsets.
Of course, our results are preliminary and, rather
than warranting yet many definite conclusions, en-
courage further investigation of SCL (varying size
of target data, pivots selection, bigger testsets as
well as other domains etc.) as well as related semi-
supervised adaptation techniques.
Acknowledgments
Thanks to Gertjan van Noord and the anonymous re-
viewers for their comments. The Linux cluster of the
High-Performance Computing Center of the Univer-
sity of Groningen was used in this work.
40
0 50 100 150 200
85
.00
85
.05
85
.10
85
.15
85
.20
85
.25
85
.30
number of iterations
ac
cu
ra
cy
Indelibility versus delibility
baseline
SCL
Indelible SelfTrain
Delible SelfTrain
EM
Figure 1: Delible versus Indelible self-training and EM. Delible and indelible self-training achieve very similar per-
formance. However, indelibility is preferred over delibility since it is considerably faster.
0 50 100 150 200
85
.00
85
.05
85
.10
85
.15
85
.20
85
.25
85
.30
number of iterations
ac
cu
ra
cy
shorter sent
entropy
fewer parses / no selection
baseline
SCL
Indelibility with different selection techniques
select shorter sent
no selection
Figure 2: Self-training variants compared to supervised baseline and SCL. The effect of various selection techniques
(Sec. 3.2) in a single iteration is depicted (non-solid lines; fewer parses and no selection achieve identical results). For
clarity, the figure shows the learning curve for the best selection technique only (shorter sent) versus no selection. On
average running multiple iterations is just the same as a single iteration. In all cases SCL still performs best.
41
References
Steven Abney. 2007. Semi-supervised Learning for
Computational Linguistics. Chapman & Hall.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning Re-
search, 6:1817?1853.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Methods
in Natural Language Processing, Sydney, Australia.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Association for Computational Linguistics, Prague,
Czech Republic.
Mark Dredze, John Blitzer, Pratha Pratim Talukdar, Kuz-
man Ganchev, Joao Graca, and Fernando Pereira.
2007. Frustratingly hard domain adaptation for pars-
ing. In Proceedings of the CoNLL Shared Task Session
- Conference on Natural Language Learning, Prague,
Czech Republic.
Tadayoshi Hara, Miyao Yusuke, and Jun?ichi Tsujii.
2005. Adapting a probabilistic disambiguation model
of an hpsg parser to a new domain. In Proceedings
of the International Joint Conference on Natural Lan-
guage Processing.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
ACL-08: HLT, Short Papers, pages 101?104, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City. Association for Computational Linguistics.
Miles Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the Eighteenth International Conference
on Computational Linguistics (COLING 2000).
Barbara Plank. 2009. Structural correspondence learn-
ing for parse disambiguation. In Proceedings of the
Student Research Workshop at EACL 2009, Athens,
Greece, April.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
Association for Computational Linguistics, Prague.
Nobuyuki Shimizu and Hiroshi Nakagawa. 2007. Struc-
tural correspondence learning for dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In In Pro-
ceedings of the EACL, pages 331?338.
Gertjan van Noord and Robert Malouf. 2005. Wide
coverage parsing with stochastic attribute value gram-
mars. Draft. A preliminary version of this paper was
published in the Proceedings of the IJCNLP workshop
Beyond Shallow Analyses, Hainan China, 2004.
42
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1783?1792, Dublin, Ireland, August 23-29 2014.
Adapting taggers to Twitter with not-so-distant supervision
Barbara Plank
1
, Dirk Hovy
1
, Ryan McDonald
2
and Anders S?gaard
1
Center for Language Technology, University of Copenhagen
1
Google Inc.
2
{bplank,dirkh}@cst.dk,ryanmcd@google.com,soegaard@hum.ku.dk
Abstract
We experiment with using different sources of distant supervision to guide unsupervised and
semi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter.
We show that a particularly good source of not-so-distant supervision is linked websites. Specif-
ically, with this source of supervision we are able to improve over the state-of-the-art for Twitter
POS tagging (89.76% accuracy, 8% error reduction) and NER (F1=79.4%, 10% error reduction).
1 Introduction
Twitter contains a vast amount of information, including first stories and breaking news (Petrovic et al.,
2010), fingerprints of public opinions (Jiang et al., 2011) and recommendations of relevance to poten-
tially very small target groups (Benson et al., 2011). In order to automatically extract this information,
we need to be able to analyze tweets, e.g., determine the part-of-speech (POS) of words and recognize
named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013;
Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions
for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, frag-
mented or mixed language, etc.
Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample
Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts
of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski
et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective
samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing
even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due
to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems
perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and
suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning
from additional unlabeled tweets. This is the hypothesis we explore in this paper.
We present a semi-supervised learning method that does not require additional labeled in-domain data
to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers
trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the
unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data.
Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited
to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic
analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We
explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose
to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites
to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs
provide a one-to-one map between an unlabeled instance and the source of supervision, making this
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1783
1: X = {?x
i
, y
i
?}
N
i=1
labeled tweets
2: U = {?x
i
, w
i
?}
M
i=1
unlabeled tweet-website pairs
3: I iterations
4: k = 1000 pool size
5: v=train(X) base model
6: for i ? I do
7: for ?x, w? ? pool
k
(U) do
8: y?=predict(?x, w?;v)
9: X ? X ? {?y?,x?}
10: end for
11: v=train(X)
12: end for
13: return v
Figure 1: Semi-supervised learning with not-so-distant supervision, i.e. tweet-website pairs {?x
i
, w
i
?}.
SELF-TRAINING, WEB, DICT, DICT?WEB and WEB?DICT differ only in how predict() (line 8) is
implemented (cf. Section 2).
less distant supervision. Note that we use linked websites only for semi-supervised learning, but do not
require them at test time.
Our semi-supervised learning method enables us to learn POS tagging and NER models that perform
more robustly across different samples of tweets than existing approaches. We consider both the scenario
where a small sample of labeled Twitter data is available, and the scenario where only newswire data is
available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unla-
beled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our
tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd
2 Tagging with not-so-distant supervision
We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), popula-
tion drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use
unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training
sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper
presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and
not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging deci-
sions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised
approach gives state-of-the-art performance across available Twitter POS and NER data sets.
The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model
bias by predicting tag sequences on small pools of unlabeled tweets, and re-training the model across
several iterations to gradually correct model bias. Since information from hyperlinks will be important,
the unlabeled data U is a corpus of tweets containing URLs. We present a baseline and four system
proposals that only differ in their treatment of the predict() function.
In the SELF-TRAINING baseline, predict() corresponds to standard Viterbi inference on the unlabeled
Twitter data. This means, the current model v is applied to the tweets by disregarding the websites in
the tweet-website pairs, i.e., tagging x without considering w. Then the automatically tagged tweets are
added to the current pool of labeled data and the procedure is iterated (line 7-11 in Figure 1).
In the WEB method, we additionally use the information from the websites. The current model v
is used to predict tags for the pooled tweets and the website they linked to. For all the words that
occur both in a tweet and on the corresponding website, we then project the tag most frequently
assigned to those words on the website to their occurrences in the tweet. This enables us to basically
condition the tag decision for each such word on its accumulated context on the website. The assumption
of course being that the word in the tweet has the part-of-speech it most often has on the website linked to.
1784
Example Here is an example of a tweet that contains a URL:
(1) #Localization #job: Supplier / Project Manager - Localisation Vendor - NY, NY, United States
http://bit.ly/16KigBg #nlppeople
The words in the tweet are all common words, but they occur without linguistic context that could
help a tagging model to infer whether these words are nouns, verbs, named entities, etc. However, on the
website that the tweet refers to, all of these words occur in context:
(2) The Supplier/Project Manager performs the selection and maintenance . . .
For illustration, the Urbana-Champaign POS tagger
1
incorrectly tags Supplier in (1) as an adjective.
In (2), however, it gets the same word right and tags it as a noun. The tagging of (2) could potentially
help us infer that Supplier is also a noun in (1).
Obviously, the superimposition of tags in the WEB method may change the tag of a tweet word such
that it results in an unlikely tag sequence, as we will discuss later. Therefore we also implemented
type-constrained decoding (T?ackstr?om et al., 2013), i.e., prune the lattice such that the tweet words ob-
served on the website have one of the tags they were labeled with on the website (soft constraints), or,
alternatively, were forced during decoding to have the most frequent tags they were labeled with (hard
constraint decoding), thereby focusing on licensed sequences. However, none of these approaches per-
formed significantly better than the simple WEB approach on held-out data. This suggests that sequential
dependencies are less important for tagging Twitter data, which is of rather fragmented nature. Also, the
WEB approach allows us to override transitional probabilities that are biased by the observations we
made about the distribution of tags in our out-of-domain data.
Furthermore, we combine the not-so-distant supervision from linked websites (WEB) with supervision
from dictionaries (DICT). The idea here is to exploit the fact that many word types in a dictionary are
actually unambiguous, i.e., contain only a single tag. In particular, 93% of the word types in Wiktionary
2
are unambiguous. Wiktionary is a crowdsourced tag dictionary that has previously been used for mini-
mally supervised POS tagging (Li et al., 2012; T?ackstr?om et al., 2013). In the case of NER, we use a
gazetteer that combines information on PER, LOC and ORG from the KnownLists of the Illinois tagger.
3
For this gazetteer, 79% of the word types contained only a single named entity tag.
We experiment with a model that uses the dictionary only (DICT) and two ways to combine the two
sources. In the former setup, the current model is first applied to tag the tweets, then any token that
appears in the dictionary and is unambiguous is projected back to the tweet. The next two methods are
combinations of WEB and DICT: either first project the predicted tags from the website and then, in case
of conflicts, overrule predictions by the dictionary (WEB?DICT), or the other way around (DICT?WEB).
The intuition behind the idea of using linked websites as not-so-distant supervision is that while tweets
are hard to analyze (even for humans) because of the limited context available in 140 character messages,
tweets relate to real-world events, and Twitter users often use hyperlinks to websites to indicate what
real-world events their comments address. In fact, we observed that about 20% of tweets contain URLs.
The websites they link to are often newswire sites that provide more context and are written in a more
canonical language, and are therefore easier to process. Our analysis of the websites can then potentially
inform our analysis of the tweets. The tweets with the improved analyses can then be used to bootstrap
our tagging models using a self-training mechanism. Note that our method does not require tweets to
contain URLs at test time, but rather uses unlabeled tweets with URLs during training to build better
tagging models for tweets in general. At test time, these models can be applied to any tweet.
1
http://cogcomp.cs.illinois.edu/demo/pos/
2
http://en.wiktionary.org/ - We used the Wiktionary version derived by Li et al. (2012).
3
http://cogcomp.cs.illinois.edu/page/software_view/NETagger
1785
3 Experiments
3.1 Model
In our experiments we use a publicly available implementation of conditional random fields (CRF) (Laf-
ferty et al., 2001).
4
We use the features proposed by Gimpel et al. (2011), in particular features for word
tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase,
3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2
i
for
i ? 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is pub-
licly available.
5
We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000}
showing similar performance. The number of iterations i is set on the development data.
For NER on websites, we use the Stanford NER system (Finkel et al., 2005)
6
with POS tags from the
LAPOS tagger (Tsuruoka et al., 2011).
7
For POS we found it to be superior to use the current POS model
for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line
NER tagging rather than retagging the websites in every iteration.
3.2 Data
In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semi-
supervised domain adaptation (DA), respectively (Daum?e et al., 2010; Plank, 2011). In unsupervised
DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both
domains, besides unlabeled target data, but the amount of labeled target data is much smaller than the
labeled source data. Most annotated corpora for English are newswire corpora. Some annotated Twitter
data sets have been made available recently, described next.
POS NER
train
WSJ (700k) REUTER-CONLL (Tjong Kim Sang and De Meulder, 2003) (200k)
GIMPEL-TRAIN (Owoputi et al., 2013) (14k) FININ-TRAIN (Finin et al., 2010) (170k)
dev
FOSTER-DEV (Foster et al., 2011) (3k) n/a
RITTER-DEV (Ritter et al., 2011) (2k) n/a
test
FOSTER-TEST (Foster et al., 2011) (2.8k) RITTER-TEST (Ritter et al., 2011) (46k)
GIMPEL-TEST (Gimpel et al., 2011) (7k) FININ-TEST (Finin et al., 2010) (51k)
HOVY-TEST (Hovy et al., 2014) FROMREIDE-TEST (Fromreide et al., 2014) (20k)
Table 1: Overview of data sets. Number in parenthesis: size in number of tokens.
Training data. An overview of the different data sets is given in Table 3.2. In our experiments, we
use the SANCL shared task
8
splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations
as newswire training data for POS tagging.
9
For NER, we use the CoNLL 2003 data sets of annotated
newswire from the Reuters corpus.
10
The in-domain training POS data comes from Gimpel et al. (2011),
and the in-domain NER data comes from Finin et al. (2010) (FININ-TRAIN). These data sets are added
to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expert-
annotated Twitter data, but rely on crowdsourced annotations. We use MACE
11
(Hovy et al., 2013) to
resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We
believe relying on crowdsourced annotations makes our set-up more robust across different samples of
Twitter data.
Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a
specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster
4
http://www.chokkan.org/software/crfsuite/
5
http://www.ark.cs.cmu.edu/TweetNLP/
6
http://http://nlp.stanford.edu/software/CRF-NER.shtml
7
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/
8
https://sites.google.com/site/sancl2012/home/shared-task
9
LDC2011T03.
10
http://www.clips.ua.ac.be/conll2003/ner/
11
http://www.isi.edu/publications/licensed-sw/mace/
1786
et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do
not assume to have access to further development data. For both POS tagging and NER, we have three
test sets. For POS tagging, the ones used in Foster et al. (2011) (FOSTER-TEST) and Ritter et al. (2011)
(RITTER-TEST),
12
as well as the one presented in Hovy et al. (2014) (HOVY-TEST). For NER, we use
the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets.
One is a manual correction of a held-out portion of FININ-TRAIN, named FININ-TEST; the other one
is referred to as FROMREIDE-TEST. Since the different POS corpora use different tag sets, we map all
of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a
few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire
tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong.
Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a
few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The
major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We
follow Finin et al. (2010) in doing so.
Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period
of one week in August 2013 by searching for tweets that contain the string http and downloading the
content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites
that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).
13
We also
require website and tweet to have at least one matching word that is not a stopword (as defined by the
NLTK stopword list).
14
Finally we restrict ourselves to pairs where the website is a subsite, because
website head pages tend to contain mixed content that is constantly updated. The resulting files are all
tokenized using the Twokenize tool.
15
Tweets were treated as one sentence, similar to the approaches in
Gimpel et al. (2011) and Owoputi et al. (2013); websites were processed by applying the Moses sentence
splitter.
16
The out-of-vocabulary (OOV) rates in Figure 2 show that in-domain training data reduces the number
of unseen words considerably, especially in the NER data sets. They also suggest that some evaluation
data sets share more vocabulary with our training data than others. In particular, we would expect better
performance on FOSTER-TEST than on RITTER-TEST and HOVY-TEST in POS tagging, as well as better
performance on FININ-TEST than on the other two NER test sets. In POS tagging, we actually do see
better results with FOSTER-TEST across the board, but in NER, FININ-TEST actually turns out to be the
hardest data set.
4 Results
4.1 POS results
Baselines We use three supervised CRF models as baselines (cf. the first part of Table 2). The first
supervised model is trained only on WSJ. This model does very well on FOSTER-DEV and FOSTER-
TEST, presumably because of the low OOV rates (Figure 2). The second supervised model is trained
only on GIMPEL-TRAIN; the third on the concatenation of WSJ and GIMPEL-TRAIN. While the second
baseline performs well on held-out data from its own sample (90.3% on GIMPEL-DEV), it performs
poorly across our out-of-sample test and development sets. Thus, it seems to overfit the sample of
tweets described in Gimpel et al. (2011). The third model trained on the concatenation of WSJ and
GIMPEL-TRAIN achieves the overall best baseline performance (88.4% macro-average accuracy). We
note that this is around one percentage point better than the best available off-the-shelf system for Twitter
(Owoputi et al., 2013) with an average accuracy of 87.5%.
12
Actually (Ritter et al., 2011) do cross-validation over this data, but we use the splits of Derczynski et al. (2013) for POS.
13
Using https://github.com/miso-belica/jusText
14
ftp://ftp.cs.cornell.edu/pub/smart/english.stop
15
https://github.com/brendano/ark-tweet-nlp
16
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/
split-sentences.perl
1787
Figure 2: Test set (type-level) OOV rates for POS (left) and NER (right).
l l
l
l l l
l l l l l l l
l l l l l l l l l l l l l l l l l l l l l l
0 5 10 15 20 25 30
88.5
89.0
89.5
90.0
DEV?avg wsj
iteration
accu
racy
l self?trainingWebDictWeb<DictDict<Web
l l
l l l
l l l l l l l
l l l l l l l l l l l l
l l
l l l l
0 5 10 15 20 25 30
88.8
89.0
89.2
89.4
89.6
89.8
90.0
90.2
DEV?avg wsj+gimpel
iteration
accu
racy
Figure 3: Learning curves on DEV-avg for systems trained on WSJ (left) and WSJ+GIMPEL (right) used
to set the hyperparameter i.
Learning with URLs The results of our approaches are presented in Table 2. The hyperparameter i
was set on the development data (cf. Figure 3). Note, again, that they do not require the test data to
contain URLs. First of all, naive self-training does not work: accuracy declines or is just around baseline
performance (Table 2 and Figure 3). In contrast, our augmented self-training methods with WEB or
DICT reach large improvements. In case we assume no target training data (train on WSJ only, i.e.
unsupervised DA), we obtain improvements of up to 9.1% error reduction. Overall the system improves
from 88.42% to 89.07%. This also holds for the second scenario, i.e. training on WSJ+GIMPEL-TRAIN
(semi-supervised DA, i.e., the case where we have some labeled target data, besides the pool of unlabeled
tweets) where we reach error reductions of up to 10%. Our technique, in other words, improves the
robustness of taggers, leading to much better performance on new samples of tweets.
4.2 NER results
For our NER results, cf. Table 3, we used the same feature models and parameter settings as those used for
POS tagging, except conditioning also on POS information. It is conceivable that other parameter settings
would have led to better results, but we did not want to assume the existence of in-domain development
data for this task. Our baselines are again supervised systems, as well as off-the-shelf systems. Our in-
1788
DEV-avg TEST TEST-avg
FOSTER HOVY RITTER
Baselines trained on
WSJ 88.82 91.87 87.01 86.38 88.42
GIMPEL-TRAIN 83.32 84.86 86.03 81.67 84.19
WSJ+GIMPEL-TRAIN 89.07 91.59 87.50 87.39 88.83
Systems trained on WSJ
SELF-TRAINING i = 25 85.52 91.80 86.72 85.90 88.14
DICT i = 25 85.61 92.08 87.63 85.68 88.46
WEB i = 25 85.27 92.47 87.30 86.60 88.79
DICT?WEB i = 25 86.11 92.61 87.70 86.69 89.00
WEB?DICT i = 25 86.15 92.57 88.12 86.51 89.07
max err.red 4.7% 9.1% 8.6% 2.3% 4.2%
Systems trained on WSJ+GIMPEL-TRAIN
SELF-TRAINING i = 27 89.12 91.83 86.88 87.43 88.71
DICT i = 27 89.43 92.22 88.38 87.69 89.43
WEB i = 27 89.82 92.43 87.43 88.21 89.36
DICT?WEB i = 27 90.04 92.43 88.38 88.48 89.76
WEB?DICT i = 27 90.04 92.40 87.99 88.39 89.59
max err.red 8.9% 10% 7.1% 8.6% 8.4%
Table 2: POS results.
house supervised baselines perform better than the available off-the-shelf systems, including the system
provided by Ritter et al. (2011) (TEST-avg of 54.2%). We report micro-average F
1
-scores over entity
types, computed using the publicly available evaluation script.
17
Our approaches again lead to substantial
error reductions of 8?13% across our NER evaluation data sets.
TEST TEST-avg
RITTER FROMREIDE FININ
Baseline trained on
CONLL+FININ-TRAIN 77.44 82.13 74.02 77.86
Systems trained on CONLL+FININ-TRAIN
SELF-TRAINING i = 27 78.63 82.88 74.89 78.80
DICT i = 27 65.24 69.1 65.45 66.60
WEB i = 27 78.29 83.82 74.99 79.03
DICT?WEB i = 27 78.53 83.91 75.83 79.42
WEB?DICT i = 27 65.97 69.92 65.86 67.25
err.red 9.1% 13.3% 8.0% 9.8%
Table 3: NER results.
5 Error analysis
The majority of cases where our taggers improve on the ARK tagger (Owoputi et al., 2013) seem to
relate to richer linguistic context. The ARK tagger incorrectly tags the sequence Man Utd as PRT-
NOUN, whereas our taggers correctly predict NOUN-NOUN. In a similar vein, our taggers correctly
predict the tag sequence NOUN-NOUN for Radio Edit, while the ARK tagger predicts NOUN-VERB.
However, some differences seem arbitrary. For example, the ARK tagger tags the sequence Nokia
17
http://www.cnts.ua.ac.be/conll2000/chunking/
1789
D5000 in FOSTER-TEST as NOUN-NUM. Our systems correctly predict NOUN-NOUN, but it is not
clear which analysis is better in linguistic terms. Our systems predict a sequence such as Love his version
to be VERB-PRON-NOUN, whereas the ARK tagger predicts VERB-DET-NOUN. Both choices seem
linguistically motivated.
Finally, some errors are made by all systems. For example, the word please in please, do that, for
example, is tagged as VERB by all systems. In FOSTER-TEST, this is annotated as X (which in the PTB
style was tagged as interjection UH). Obviously, please often acts as a verb, and while its part-of-speech
in this case may be debatable, we see please annotated as a verb in similar contexts in the PTB, e.g.:
(3) Please/VERB make/VERB me/PRON . . .
It is interesting to look at the tags that are projected from the websites to the tweets. Several of the
observed projections support the intuition that coupling tweets and the websites they link to enables us
to condition our tagging decisions on a richer linguistic context. Consider, for example Salmon-Safe,
initially predicted to be a NOUN, but after projection correctly analyzed as an ADJ:
Word Context Initial tag Projected tag
Salmon-Safe . . . parks NOUN ADJ
Snohomish . . . Bakery ADJ NOUN
toxic ppl r . . . NOUN ADJ
One of the most frequent projections is analyzing you?re, correctly, as a VERB rather than an ADV (if
the string is not split by tokenization).
One obvious limitation of the WEB-based models is that the projections apply to all occurrences of a
word. In rare cases, some words occur with different parts of speech in a single tweet, e.g., wish in:
(4) If I gave you one wish that will become true . What?s your wish ?... ? i wish i?ll get <num> wishes
from you :p <url>
In this case, our models enforce all occurrences of wish to, incorrectly, be verbs.
6 Related work
Previous work on tagging tweets has assumed labeled training data (Ritter et al., 2011; Gimpel et al.,
2011; Owoputi et al., 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter
has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter.
This paper presents results using no in-domain labeled data that are significantly better than several off-
the-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data
to reach new highs across several data sets.
Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings
(Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T?ackstr?om et al., 2013).
Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea
of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et
al. (2012) for search query tagging.
Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and
the associated snippets provided by the search engine, projecting tags from the snippets to the queries,
guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more
advanced matching of snippets and search queries, giving priority to n-gram matches with larger n.
Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less
spelling variation than tweets.
In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and
Torisawa, 2007; Cucerzan, 2007). R?ud et al. (2011) consider using search engines for distant supervision
of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use
click-through data. They use the search engine snippets to generate feature representations rather than
projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts,
1790
applying their model to Twitter data, but their results are considerably below the state of the art. Also,
their source of supervision is not linked to the individual tweets in the way mentioned websites are.
In sum, our method is the first successful application of distant supervision to POS tagging and NER
for Twitter. Moreover, it is, to the best of our knowledge, the first paper that addresses both problems
using the same technique. Finally, our results are significantly better than state-of-the-art results in both
POS tagging and NER.
7 Conclusion
We presented a semi-supervised approach to POS tagging and NER for Twitter data that uses dictionaries
and linked websites as a source of not-so-distant (or linked) supervision to guide the bootstrapping. Our
approach outperforms off-the-shelf taggers when evaluated across various data sets, achieving average
error reductions across data sets of 5% on POS tagging and 10% on NER over state-of-the-art baselines.
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text,
how diffrnt social media sources? In IJCNLP.
Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In ACL.
Silvia Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLP-CoNLL.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In ACL.
Hal Daum?e, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation.
In ACL Workshop on Domain Adaptation for NLP.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all:
overcoming sparse and noisy data. In RANLP.
Jacob Eisenstein. 2013. What to do about bad language on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Anno-
tating named entities in Twitter data with crowdsourcing. In NAACL-HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Jenny Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In ACL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van
Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In
IJCNLP.
Hege Fromreide, Dirk Hovy, and Anders S?gaard. 2014. Crowdsourcing and annotating ner for twitter #drift. In
LREC.
Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav Petrov. 2012. Using search-logs to improve query
tagging. In ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-Speech Tagging for Twitter:
Annotation, Features, and Experiments. In ACL.
David Hand. 2006. Classifier technology and illusion of progress. Statistical Science, 21(1):1?15.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014. When pos datasets don?t add up: Combatting sample bias.
In LREC.
1791
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2009. Self-training with products of latent variable grammars.
In EMNLP.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.
Long Jiang, Mo Yo, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment
classification. In ACL.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploiting wikipedia as external knowledge for named entity
recognition. In EMNLP-CoNLL.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In HLT-NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In ACL.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to
Twitter. In NAACL.
Barbara Plank. 2011. Domain Adaptation for Parsing. Ph.D. thesis, University of Groningen.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In
EMNLP.
Stefan R?ud, Massimiliano Ciaramita, Jens M?uller, and Hinrich Sch?utze. 2011. Piggyback: Using search engines
for robust cross-domain named entity recognition. In ACL.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. TACL, 1:1?12.
Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In In CoNLL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Kazama. 2011. Learning with lookahead: can history-based
models rival globally optimized models? In CoNLL.
Chun-Kai Wang, Bo-June Hsu, Ming-Wei Chang, and Emre Kiciman. 2013. Simple and knowledge-intensive
generative model for named entity recognition. Technical report, Microsoft Research.
1792
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 11?13, Dublin, Ireland, August 23-29 2014.
Selection Bias, Label Bias, and Bias in Ground Truth
Anders S?gaard, Barbara Plank, and Dirk Hovy
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk, {bplank|dhovy}@cst.dk
Introduction
Language technology is biased toward English newswire. In POS tagging, we get 97?98 words right out
of a 100 in English newswire, but results drop to about 8 out of 10 when running the same technology
on Twitter data. In dependency parsing, we are able to identify the syntactic head of 9 out of 10 words
in English newswire, but only 6?7 out of 10 in tweets. Replace references to Twitter with references to a
low-resource language of your choice, and the above sentence is still likely to hold true.
The reason for this bias is obviously that mainstream language technology is data-driven, based on
supervised statistical learning techniques, and annotated data resources are widely available for English
newswire. The situation that arises when applying off-the-shelf language technology, induced from
annotated newswire corpora, to something like Twitter, is a bit like when trying to predict elections from
Xbox surveys (Wang et al., 2013). Our induced models suffer from a data selection bias.
This is actually not the only way our data is biased. The available resources for English newswire
are the result of human annotators following specific guidelines. Humans err, leading to label bias, but
more importantly, annotation guidelines typically make debatable linguistic choices. Linguistics is not
an exact science, and we call the influence of annotation guidelines bias in ground truth.
In the tutorial, we present various case studies for each kind of bias, and show several methods that
can be used to deal with bias. This results in improved performance of NLP systems.
Selection Bias
The situation that arises when applying off-the-shelf language technology, induced from annotated
newswire corpora, to something like Twitter, is, as mentioned, a bit like when trying to predict elec-
tions from Xbox surveys. In the case of elections, however, we can correct most of the selection bias by
post-stratification or instance weighting (Wang et al., 2013). In language technology, the bias correction
problem is harder.
In the case of elections, you have a single output variable and various demographic observed variables.
All values taken by discrete variables at test time can be assumed to have been observed, and all values
observed at training time can be assumed to be seen at test time. In language technology, we typically
have several features only seen in training data and several features only seen in test data.
The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al.,
2006; Turian et al., 2010), while the former has led to the development of learning algorithms that
prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associ-
ated with less predictive, correlated features from being updated. Note, however, that post-stratification
(Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Roy-
all, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization
of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster
et al., 2011), but most work on domain adaptation in language technology has focused on predictive
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
11
approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Mc-
Closky et al., 2010; Chen et al., 2011).
Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algo-
rithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive
bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny,
2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training
sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the
distribution observed in the population. As mentioned, this will never solve the problem with unseen
features, since you cannot up-weigh a null feature.
Semi-supervised learning can correct modest selection bias, but if the domain gap is too wide, our
initial predictions in the target domain will be poor, and semi-supervised learning is likely to increase
bias rather than decrease it. However, recent work has shown that semi-supervised learning can be
combined with distant supervision and correct bias in cases where semi-supervised learning algorithms
typically fail (Plank et al., 2014).
In the tutorial we illustrate these different approaches to selection bias correction, with discriminative
learning of POS taggers for English Twitter as our running example.
Label Bias
In most annotation projects, there is an initial stage, where the project managers compare annotators?
performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on anno-
tation guidelines, if necessary. Such procedures are considered necessary to correct for the individual
biases of the annotators (label bias). However, this is typically only for the first batches of data, and it
is well-known that even some of the most widely used annotated corpora (such as the Penn Treebank)
contain many errors (Dickinson and Meurers, 2003) in the form of inconsistent annotations of the same
n-grams.
Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label
bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint
and averaging over them, which is often feasible because of the low cost of non-expert annotation. This
is called majority voting and is analogous to using ensembles of models to obtain more robust systems.
In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate anno-
tator confidence (Hovy et al., 2013), and joint learning of annotator competence and model parameters
(Raykar and Yu, 2012).
Bias in Ground Truth
In annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure
consistent annotations. However, annotation guidelines often make linguistically debatable and even
somewhat arbitrary decisions, and inter-annotator agreement is often less than perfect. Some annotators,
for example, may annotate socialin social media as a noun, others may annotate it as an adjective. In
this part of the tutorial, we discuss how to correct for the bias introduced by annotation guidelines. For
both label bias and bias in ground truth, we, again, use POS tagging for English Twitter as our running
example.
Evaluation
Once we accept our data is biased in different ways, we need to reconsider model evaluation. If our data
was selected in a biased way, say from a few editions of the Wall Street Journal, does significance over
data points make much sense? If our annotators have individual biases, can we no longer evaluate our
models on the data of one or two annotators? If the annotation guidelines introduce biases in ground truth,
can we somehow correct for that? In practice we typically do not have hundreds of datasets annotated by
different annotators using different annotation guidelines, but in the tutorial we present various ways of,
nevertheless, correcting for some of these biases.
12
Acknowledgements
This research is funded by the ERC Starting Grant LOWLANDS No. 313695.
References
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence
learning. In EMNLP.
Minmin Chen, Killiang Weinberger, and John Blitzer. 2011. Co-training for domain adaptation. In NIPS.
Markus Dickinson and Detmar Meurers. 2003. Detecting errors in part-of-speech annotation. In EACL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van
Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In
IJCNLP.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
MACE. In NAACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.
David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In
NAACL-HLT.
Barbara Plank, Dirk Hovy, Ryan McDonald, and Anders S?gaard. 2014. Adapting taggers to Twitter with not-so-
distant supervision. In COLING.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating Spammers and Ranking Annotators for Crowdsourced
Labeling Tasks. Journal of Machine Learning Research, 13:491?518.
Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers
trained on small datasets. In ACL.
R Royall. 1988. The prediction approach to sampling theory. In Rao Krishnaiah, editor, Handbook of Statistics.
North-Holland.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In EMNLP-CoNLL.
Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 90:227?244.
T Smith. 1988. Post-stratification. The Statistician, 40.
Charles Sutton, Michael Sindelar, and Andrew McCallum. 2006. Reducing weight undertraining in structured
discriminative learning. In NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In ACL.
Wei Wang, David Rotschild, Sharad Goel, and Andrew Gelman. 2013. Forecasting elections with non-
representative polls. Forthcoming in International Journal of Forecasting.
Bianca Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In ICML.
13
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968?973,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Importance weighting and unsupervised domain adaptation
of POS taggers: a negative result
Barbara Plank, Anders Johannsen and Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,ajohannsen@hum.ku.dk,soegaard@hum.ku.dk
Abstract
Importance weighting is a generalization
of various statistical bias correction tech-
niques. While our labeled data in NLP is
heavily biased, importance weighting has
seen only few applications in NLP, most of
them relying on a small amount of labeled
target data. The publication bias toward
reporting positive results makes it hard to
say whether researchers have tried. This
paper presents a negative result on unsu-
pervised domain adaptation for POS tag-
ging. In this setup, we only have unlabeled
data and thus only indirect access to the
bias in emission and transition probabili-
ties. Moreover, most errors in POS tag-
ging are due to unseen words, and there,
importance weighting cannot help. We
present experiments with a wide variety of
weight functions, quantilizations, as well
as with randomly generated weights, to
support these claims.
1 Introduction
Many NLP tasks rely on the availability of anno-
tated data. The majority of annotated data, how-
ever, is sampled from newswire corpora. The
performance of NLP systems, e.g., part-of-speech
(POS) tagger, parsers, relation extraction sys-
tems, etc., drops significantly when they are ap-
plied to data that departs from newswire conven-
tions. So while we can extract information, trans-
late and summarize newswire in major languages
with some success, we are much less successful
processing microblogs, chat, weblogs, answers,
emails or literature in a robust way. The main rea-
sons for the drops in accuracy have been attributed
to factors such as previously unseen words and bi-
grams, missing punctuation and capitalization, as
well as differences in the marginal distribution of
data (Blitzer et al., 2006; McClosky et al., 2008;
S?gaard and Haulrich, 2011).
The move from one domain to another (from a
source to a new target domain), say from newspa-
per articles to weblogs, results in a sample selec-
tion bias. Our training data is now biased, since
it is sampled from a related, but nevertheless dif-
ferent distribution. The problem of automatically
adjusting the model induced from source to a dif-
ferent target is referred to as domain adaptation.
Some researchers have studied domain adap-
tation scenarios, where small samples of labeled
data have been assumed to be available for the
target domains. This is usually an unrealistic as-
sumption, since even for major languages, small
samples are only available from a limited number
of domains, and in this work we focus on unsuper-
vised domain adaptation, assuming only unlabeled
target data is available.
Jiang and Zhai (2007), Foster et al. (2010; Plank
and Moschitti (2013) and S?gaard and Haulrich
(2011) have previously tried to use importance
weighting to correct sample bias in NLP. Im-
portance weighting means assigning a weight
to each training instance, reflecting its impor-
tance for modeling the target distribution. Im-
portance weighting is a generalization over post-
stratification (Smith, 1991) and importance sam-
pling (Smith et al., 1997) and can be used to cor-
rect bias in the labeled data.
Out of the four papers mentioned, only S?gaard
and Haulrich (2011) and Plank and Moschitti
(2013) considered an unsupervised domain adap-
tation scenario, obtaining mixed results. These
two papers assume covariate shift (Shimodaira,
2000), i.e., that there is only a bias in the marginal
distribution of the training data. Under this as-
sumption, we can correct the bias by applying a
weight function
P
t
(x)
P
s
(x)
to our training data points
(labeled sentences) and learn from the weighted
data. Of course this weight function cannot be
968
computed in general, but we can approximate it
in different ways.
In POS tagging, we typically factorize se-
quences into emission and transition probabilities.
Importance weighting can change emission prob-
abilities and transition probabilities by assigning
weights to sentences. For instance, if our corpus
consisted of three sequences: 1) a/A b/A, 2) a/A
b/B, and 3) a/A b/B, then P (B|A) = 2/3. If se-
quences two and three were down-weighted to 0.5,
then P (B|A) = 1/2.
However, this paper argues that importance
weighting cannot help adapting POS taggers to
new domains using only unlabeled target data. We
present three sources of evidence: (a) negative
results with the most obvious weight functions
across various English datasets, (b) negative re-
sults with randomly sampled weights, as well as
(c) an analysis of annotated data indicating that
there is little variation in emission and transition
probabilities across the various domains.
2 Related work
Most prior work on importance weighting use a
domain classifier, i.e., train a classifier to discrimi-
nate between source and target instances (S?gaard
and Haulrich, 2011; Plank and Moschitti, 2013)
(y ? {s, t}). For instance, S?gaard and Haulrich
(2011) train a n-gram text classifier and Plank
and Moschitti (2013) a tree-kernel based clas-
sifier on relation extraction instances. In these
studies,
?
P (t|x) is used as an approximation of
P
t
(x)
P
s
(x)
, following Zadrozny (2004). In ?3, we fol-
low the approach of S?gaard and Haulrich (2011),
but consider a wider range of weight functions.
Others have proposed to use kernel mean match-
ing (Huang et al., 2007) or minimizing KL-
divergence (Sugiyama et al., 2007).
Jiang and Zhai (2007) use importance weight-
ing to select a subsample of the source data by
subsequently setting the weight of all selected data
points to 1, and 0 otherwise. However, they do
so by relying on a sequential model trained on
labeled target data. Our results indicate that the
covariate shift assumption fails to hold for cross-
domain POS tagging. While the marginal distri-
butions obviously do differ (since we can tell do-
mains apart without POS analysis), this is most
likely not the only difference. This might explain
the positive results obtained by Jiang and Zhai
(2007). We will come back to this in ?4.
Cortes et al. (2010) show that importance
weighting potentially leads to over-fitting, but pro-
pose to use quantiles to obtain more robust weight
functions. The idea is to rank all weights and ob-
tain q quantiles. If a data point x is weighted by
w, and w lies in the ith quantile of the ranking
(i ? q), x is weighted by the average weight of
data points in the ith quantile.
The weighted structured perceptron (?3) used in
the experiments below was recently used for a dif-
ferent problem, namely for correcting for bias in
annotations (Plank et al., 2014).
l l l l
l l l l l l l l l l l l l l l l
0 5 10 15 20
92
93
94
95
96
97
98
99 l wsjanswersreviews
emailsweblogsnewsgroups
Figure 1: Training epochs vs tagging accuracy for
the baseline model on the dev data.
3 Experiments
3.1 Data
We use the data made available in the SANCL
2012 Shared Task (Petrov and McDonald, 2012).
The training data is the OntoNotes 4.0 release
of the Wall Street Journal section of the Penn
Treebank, while the target domain evaluation data
comes from various sources, incl. Yahoo Answers,
user reviews, emails, weblogs and newsgroups.
For each target domain, we have both development
and test data.
3.2 Model
In the weighted perceptron (Cavallanti et al.,
2006), we make the learning rate dependent on the
current instance x
n
, using the following update:
w
i+1
? w
i
+ ?
n
?(y
n
? sign(w
i
? x
n
))x
n
(1)
where ?
n
is the weight associated with x
n
. See
Huang et al. (2007) for similar notation.
We extend this idea straightforwardly to the
structured perceptron (Collins, 2002), for which
969
System Answers Newsgroups Reviews Avg Emails Weblogs WSJ
Our system 91.08 91.57 91.59 91.41 87.97 92.19 97.32
SANCL12-2nd 90.99 92.32 90.65 91.32 ? ? 97.76
SANCL12-best 91.79 93.81 93.11 92.90 ? ? 97.29
SANCL12-last 88.24 89.70 88.15 88.70 ? ? 95.14
FLORS basic 91.17 92.41 92.25 88.67 91.37 97.11 91.94
Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS).
we use an in-house implementation. We use
commonly used features, i.e., w,w
?1
, w
?2
,
w
+1
, w
+2
, digit, hyphen, capitalization, pre-
/suffix features, and Brown word clusters. The
model seems robust with respect to number
of training epochs, cf. Figure 1. Therefore
we fix the number of epochs to five and use
this setting in all our experiments. Our code
is available at: https://bitbucket.org/
bplank/importance-weighting-exp.
3.3 Importance weighting
In our first set of experiments, we follow S?gaard
and Haulrich (2011) in using document classifiers
to obtain weights for the source instances. We
train a text classifier that discriminates the two
domains (source and target). For each sentence
in the source and target domain (the unlabeled
text that comes with the SANCL data), we mark
whether it comes from the source or target do-
main and train a binary classifier (logistic regres-
sion) to discriminate between the two. For ev-
ery sentence in the source we obtain its probabil-
ity for the target domain by doing 5-fold cross-
validation. While S?gaard and Haulrich (2011)
use only token-based features (word n-grams ?
3), we here exploit a variety of features: word
token n-grams, and two generalizations: using
Brown clusters (estimated from the union of the
5 target domains), and Wiktionary tags (if a word
has multiple tags, we assign it the union of tags as
single tag; OOV words are marked as such).
The distributions of weights can be seen in the
upper half of Figure 2.
3.3.1 Results
Table 1 shows that our baseline model achieves
state-of-the-art performance compared to
SANCL (Petrov and McDonald, 2012)
1
and
FLORS (Schnabel and Sch?utze, 2014). Our
results align well with the second best POS
tagger in the SANCL 2012 Shared Task. Note
1
https://sites.google.com/site/sancl2012/home/
shared-task/results
Figure 2: Histogram of different weight functions.
that the best tagger in the shared task explicitly
used normalization and various other heuristics
to achieve better performance. In the rest of the
paper, we use the universal tag set part of the
SANCL data (Petrov et al., 2012).
Figure 3 presents our results on development
data for different importance weighting setups.
None of the above weight functions lead to signifi-
cant improvements on any of the datasets. We also
tried scaling and binning the weights, as suggested
by Cortes et al. (2010), but results kept fluctuating
around baseline performance, with no significant
improvements.
3.4 Random weighting
Obviously, weight functions based on document
classifiers may simply not characterize the rele-
vant properties of the instances and hence lead to
bad re-weighting of the data. We consider three
random sampling strategies, namely sampling ran-
dom uniforms, random exponentials, and random
970
Figure 3: Results on development data for different weight functions, i.e., document classifiers trained
on a) raw tokens; b) tokens replaced by Wiktionary tags; c) tokens replaced by Brown cluster ids. The
weight was the raw p
t
(y|x) value, no scaling, no quantiles. Replacing only open-class tokens for b) and
c) gave similar or lower performance.
Zipfians and ran 500 samples for each. For these
experiments, we estimate significance cut-off lev-
els of tagging accuracies using the approximate
randomization test. To find the cut-off levels,
we randomly replace labels with gold labels until
the achieved accuracy significantly improves over
the baseline for more than 50% of the samples.
For each accuracy level, 50 random samples were
taken.
llll
l
l
llll
lll
ll
llll
l
ll
l
l
llll
l
l
ll
lll
llllll
l
llll
llllll
llllll
l
lll
lllllll
l
ll
l
llllll
l
l
l
lll
llllll
l
l
lllll
ll
l
l
lllllll
llllll
l
l
l
llll
llll
l
lll
l
ll
ll
ll
ll
ll
l
llll
ll
l
ll
ll
ll
ll
l
ll
lllllll
l
l
l
lll
llllll
l
l
ll
ll
ll
ll
l
l
lllll
ll
l
llll
lllllll
llllll
l
ll
ll
ll
l
l
lll
lll
ll
lll
l
l
l
l
lllll
l
lll
l
l
lllll
l
ll
0 200 400
93.2
93.6
94.0
answers
Index
l randomexpzipf
ll
ll
l
lllll
ll
l
l
ll
l
l
l
l
l
l
ll
l
ll
ll
ll
ll
ll
llll
ll
l
ll
l
l
ll
ll
l
ll
ll
llll
lllll
l
ll
l
l
l
l
ll
l
ll
l
lllll
l
l
ll
l
ll
lll
ll
l
l
lll
lll
l
ll
ll
l
llll
ll
l
l
l
l
l
l
l
ll
l
l
l
l
lllll
lllllll
l
ll
ll
l
l
l
llll
ll
l
l
ll
ll
ll
l
ll
ll
l
l
l
lll
l
l
l
ll
l
ll
lll
lll
ll
lll
l
ll
ll
llll
ll
l
ll
l
l
llllll
lll
l
l
lll
llll
l
lll
ll
l
l
l
ll
l
l
lll
l
l
l
l
l
llll
lllll
0 200 4009
4.2
94.4
94.6
94.8
reviews
Index
TA
l
lll
l
llll
l
ll
llll
ll
llll
ll
llllll
ll
ll
llll
l
l
l
ll
llllll
lll
ll
lll
l
ll
l
llll
l
ll
l
ll
lll
ll
l
l
l
lll
ll
l
lll
l
l
l
ll
ll
ll
lll
llll
l
ll
lllll
l
llll
lll
l
l
lll
ll
l
l
l
llll
l
llll
l
l
l
lll
ll
ll
l
llllll
ll
l
llll
ll
lllll
lll
l
l
lllll
l
l
ll
l
l
l
l
lllll
ll
ll
l
lll
lll
ll
l
l
l
l
ll
lllll
lll
llll
l
l
llllllll
l
ll
llll
lll
l
l
l
l
lll
l
l
l
l
l
ll
ll
lll
l
lll
ll
l
ll
l
l
l
ll
ll
llll
llll
0 200 400
93.4
93.8
emails
Index
TA
l
ll
lll
ll
l
lll
ll
ll
l
l
ll
ll
llllllll
ll
lllll
l
l
l
ll
l
l
ll
l
lll
lll
llll
l
lll
l
l
lllll
ll
lll
lllllll
llll
ll
l
l
ll
ll
l
l
lllll
lllll
llllll
lllll
lllllll
l
ll
lllll
l
ll
l
l
ll
l
l
llllll
l
l
l
l
llll
llllll
ll
ll
lllll
llllllll
l
lll
l
ll
l
l
lllllll
l
ll
l
l
lllll
lll
ll
ll
llll
llllll
l
llll
ll
l
l
l
ll
lll
l
llll
l
lllllll
llll
l
l
lll
l
l
ll
l
l
l
lll
l
l
l
0 200 400
94.4
94.8
95.2
weblogs
Index
TA
lll
lll
lll
ll
ll
lll
lll
l
ll
ll
l
lllllllll
lllll
l
l
lll
l
ll
ll
ll
llll
l
llll
llll
l
l
lll
l
lllll
lllll
llllllll
llll
llll
l
ll
l
lll
ll
ll
lllll
lllllllllll
llll
llll
l
ll
lll
ll
lllllll
lllll
l
llllll
l
l
lll
ll
l
lllll
l
ll
ll
ll
lll
ll
llll
lll
l
llll
llllll
lll
lll
llll
lllll
ll
ll
l
llllllll
ll
ll
llll
lll
l
l
llll
l
ll
l
llll
llll
l
ll
l
lll
ll
l
l
lll
llllll
ll
l
0 200 400
94.2
94.6
95.0
newsgroups
Index
TA
0 200 400
93.0
93.4
93.8
answers
Index
0 200 400
94.2
94.6
reviews
Index
TA
0 200 400
93.2
93.6
94.0
emails
Index
TA
0 200 400
94.2
94.6
95.0
95.4
weblogs
Index
TA
0 200 400
94.0
94.4
94.8
newsgroups
Index
TA
0 50 100 150
92.5
93.0
93.5
94.0
answers
0 50 100 150
93.5
94.0
94.5
reviews
TA
0 50 100 150
92.5
93.0
93.5
94.0
emails
TA
0 50 100 150
94.0
94.5
95.0
weblogs
TA
0 50 100 1509
3.5
94.0
94.5
95.0
newsgroups
TA
Figure 4: Random weight functions (500 runs
each) on test sets. Solid line is the baseline per-
formance, while the dashed line is the p-value cut-
off. From top: random, exponential and Zipfian
weighting. All runs fall below the cut-off.
3.4.1 Results
The dashed lines in Figure 4 show the p-value cut-
offs for positive results. We see that most random
weightings of data lead to slight drops in perfor-
mance or are around baseline performance, and no
weightings lead to significant improvements. Ran-
dom uniforms seem slightly better than exponen-
tials and Zipfians.
domain (tokens) avg tag ambiguity OOV KL ?
type token
wsj (train/test: 731k/39k) 1.09 1.41 11.5 0.0006 0.99
answers (28k) 1.09 1.22 27.7 0.048 0.77
reviews (28k) 1.07 1.19 29.5 0.040 0.82
emails (28k) 1.07 1.19 29.9 0.027 0.92
weblogs (20k) 1.05 1.11 22.1 0.010 0.96
newsgroups (20k) 1.05 1.14 23.1 0.011 0.96
Table 2: Relevant statistics for our analysis (?4)
on the test sets: average tag ambiguity, out-of-
vocabulary rate, and KL-divergence and Pearson
correlation coefficient (?) on POS bigrams.
4 Analysis
Some differences between the gold-annotated
source domain data and the gold-annotated tar-
get data used for evaluation are presented in Ta-
ble 2. One important observation is the low ambi-
guity of word forms in the data. This makes the
room for improvement with importance weight-
ing smaller. Moreover, the KL divergencies over
POS bigrams are also very low. This tells us that
transition probabilities are also relatively constant
across domains, again suggesting limited room for
improvement for importance weighting.
Compared to this, we see much bigger differ-
ences in OOV rates. OOV rates do seem to explain
most of the performance drop across domains.
In order to verify this, we implemented a ver-
sion of our structured perceptron tagger with type-
constrained inference (T?ackstr?om et al., 2013).
This technique only improves performance on un-
seen words, but nevertheless we saw significant
improvements across all five domains (cf. Ta-
ble 3). This suggests that unseen words are a
more important problem than the marginal distri-
bution of data for unsupervised domain adaptation
of POS taggers.
971
ans rev email webl newsg
base 93.41 94.44 93.54 94.81 94.55
+type constr. 94.09? 94.85? 94.31? 95.99? 95.97?
p-val cut-off 93.90 94.85 94.10 95.3 95.10
Table 3: Results on the test sets by adding Wik-
tionary type constraints. ?=p-value < 0.001.
We also tried Jiang and Zhai?s subset selection
technique (?3.1 in Jiang and Zhai (2007)), which
assumes labeled training material for the target
domain. However, we did not see any improve-
ments. A possible explanation for these different
findings might be the following. Jiang and Zhai
(2007) use labeled target data to learn their weight-
ing model, i.e., in a supervised domain adaptation
scenario. This potentially leads to very different
weight functions. For example, let the source do-
main be 100 instances of a/A b/B and 100 in-
stances of b/B b/B, and the target domain be 100
instances of a/B a/B. Note that a domain classi-
fier would favor the first 100 sentences, but in an
HMM model induced from the labeled target data,
things look very different. If we apply Laplace
smoothing, the probability of a/A b/B accord-
ing to the target domain HMM model would be
? 8.9e
?7
, and the probability of b/B b/B would
be ? 9e
?5
. Note also that this set-up does not as-
sume covariate shift.
5 Conclusions and Future Work
Importance weighting, a generalization of various
statistical bias correction techniques, can poten-
tially correct bias in our labeled training data, but
this paper presented a negative result about impor-
tance weighting for unsupervised domain adapta-
tion of POS taggers. We first presented exper-
iments with a wide variety of weight functions,
quantilizations, as well as with randomly gener-
ated weights, none of which lead to significant im-
provements. Our analysis indicates that most er-
rors in POS tagging are due to unseen words, and
what remains seem to not be captured adequately
by unsupervised weight functions.
For future work we plan to extend this work to
further weight functions, data sets and NLP tasks.
Acknowledgements
This research is funded by the ERC Starting Grant
LOWLANDS No. 313695.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Clau-
dio Gentile. 2006. Tracking the best hyperplane
with a simple budget perceptron. In COLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri.
2010. Learning bounds for importance weighting.
In NIPS.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Jiayuan Huang, Alexander Smola, Arthur Gretton,
Karsten Borgwardt, and Bernhard Sch?olkopf. 2007.
Correcting sample bias by unlabeled data. In NIPS.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2008. When is self-training effective for pars-
ing? In COLING.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Tobias Schnabel and Hinrich Sch?utze. 2014. Flors:
Fast and simple domain adaptation for part-of-
speech tagging. TACL, 2:15?16.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
Peter Smith, Mansoor Shafi, and Hongsheng Gao.
1997. Quick simulation: A review of importance
sampling techniques in communications systems.
IEEE Journal on Selected Areas in Communica-
tions, 15(4):597?613.
T.M.F. Smith. 1991. Post-stratification. The Statisti-
cian, 40:315?323.
972
Anders S?gaard and Martin Haulrich. 2011.
Sentence-level instance-weighting for graph-based
and transition-based dependency parsing. In IWPT.
Masashi Sugiyama, Shinichi Nakajima, Hisashi
Kashima, Paul von B?unau, and Motoaki Kawanabe.
2007. Direct importance estimation with model se-
lection and its application to covariate shift adapta-
tion. In NIPS.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1?12.
Bianca Zadrozny. 2004. Learning and evaluating clas-
sifiers under sample selection bias. In ICML.
973
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742?751,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning part-of-speech taggers with inter-annotator agreement loss
Barbara Plank, Dirk Hovy, Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
Abstract
In natural language processing (NLP) an-
notation projects, we use inter-annotator
agreement measures and annotation guide-
lines to ensure consistent annotations.
However, annotation guidelines often
make linguistically debatable and even
somewhat arbitrary decisions, and inter-
annotator agreement is often less than
perfect. While annotation projects usu-
ally specify how to deal with linguisti-
cally debatable phenomena, annotator dis-
agreements typically still stem from these
?hard? cases. This indicates that some er-
rors are more debatable than others. In this
paper, we use small samples of doubly-
annotated part-of-speech (POS) data for
Twitter to estimate annotation reliability
and show how those metrics of likely inter-
annotator agreement can be implemented
in the loss functions of POS taggers. We
find that these cost-sensitive algorithms
perform better across annotation projects
and, more surprisingly, even on data an-
notated according to the same guidelines.
Finally, we show that POS tagging mod-
els sensitive to inter-annotator agreement
perform better on the downstream task of
chunking.
1 Introduction
POS-annotated corpora and treebanks are collec-
tions of sentences analyzed by linguists accord-
ing to some linguistic theory. The specific choice
of linguistic theory has dramatic effects on down-
stream performance in NLP tasks that rely on syn-
tactic features (Elming et al., 2013). Variation
across annotated corpora in linguistic theory also
poses challenges to intrinsic evaluation (Schwartz
et al., 2011; Tsarfaty et al., 2012), as well as
for languages where available resources are mu-
tually inconsistent (Johansson, 2013). Unfortu-
nately, there is no grand unifying linguistic the-
ory of how to analyze the structure of sentences.
While linguists agree on certain things, there is
still a wide range of unresolved questions. Con-
sider the following sentence:
(1) @GaryMurphyDCU of @DemMattersIRL
will take part in a panel discussion on Octo-
ber 10th re the aftermath of #seanref . . .
While linguists will agree that in is a preposi-
tion, and panel discussion a compound noun, they
are likely to disagree whether will is heading the
main verb take or vice versa. Even at a more basic
level of analysis, it is not completely clear how to
assign POS tags to each word in this sentence: is
part a particle or a noun; is 10th a numeral or a
noun?
Some linguistic controversies may be resolved
by changing the vocabulary of linguistic theory,
e.g., by leaving out numerals or introducing ad
hoc parts of speech, e.g. for English to (Marcus
et al., 1993) or words ending in -ing (Manning,
2011). However, standardized label sets have
practical advantages in NLP (Zeman and Resnik,
2008; Zeman, 2010; Das and Petrov, 2011; Petrov
et al., 2012; McDonald et al., 2013).
For these and other reasons, our annotators
(even when they are trained linguists) often dis-
agree on how to analyze sentences. The strategy
in most previous work in NLP has been to monitor
and later resolve disagreements, so that the final
labels are assumed to be reliable when used as in-
put to machine learning models.
Our approach
Instead of glossing over those annotation disagree-
ments, we consider what happens if we embrace
the uncertainty exhibited by human annotators
742
when learning predictive models from the anno-
tated data.
To achieve this, we incorporate the uncertainty
exhibited by annotators in the training of our
model. We measure inter-annotator agreement on
small samples of data, then incorporate this in the
loss function of a structured learner to reflect the
confidence we can put in the annotations. This
provides us with cost-sensitive online learning al-
gorithms for inducing models from annotated data
that take inter-annotator agreement into consider-
ation.
Specifically, we use online structured percep-
tron with drop-out, which has previously been ap-
plied to POS tagging and is known to be robust
across samples and domains (S?gaard, 2013a). We
incorporate the inter-annotator agreement in the
loss function either as inter-annotator F1-scores
or as the confusion probability between annota-
tors (see Section 3 below for a more detailed de-
scription). We use a small amounts of doubly-
annotated Twitter data to estimate F1-scores and
confusion probabilities, and incorporate them dur-
ing training via a modified loss function. Specif-
ically, we use POS annotations made by two an-
notators on a set of 500 newly sampled tweets
to estimate our agreement scores, and train mod-
els on existing Twitter data sets (described be-
low). We evaluate the effect of our modified
training by measuring intrinsic as well as down-
stream performance of the resulting models on two
tasks, namely named entity recognition (NER) and
chunking, which both use POS tags as input fea-
tures.
2 POS-annotated Twitter data sets
The vast majority of POS-annotated resources
across languages contain mostly newswire text.
Some annotated Twitter data sets do exist for En-
glish. Ritter et al. (2011) present a manually an-
notated data set of 16 thousand tokens. They
do not report inter-annotator agreement. Gimpel
et al. (2011) annotated about 26 thousand tokens
and report a raw agreement of 92%. Foster et
al. (2011) annotated smaller portions of data for
cross-domain evaluation purposes. We refer to the
data as RITTER, GIMPEL and FOSTER below.
In our experiments, we use the RITTER splits
provided by Derczynski et al. (2013), and the
October splits of the GIMPEL data set, version
0.3. We train our models on the concatenation of
RITTER-TRAIN and GIMPEL-TRAIN and evaluate
them on the remaining data, the dev and test set
provided by Foster et al. (2011) as well as an in-
house annotated data set of 3k tokens (see below).
The three annotation efforts (Ritter et al., 2011;
Gimpel et al., 2011; Foster et al., 2011) all used
different tagsets, however, and they also differ in
tokenization, as well as a wide range of linguistic
decisions. We mapped all the three corpora to the
universal tagset provided by Petrov et al. (2012)
and used the same dummy symbols for numbers,
URLs, etc., in all the data sets. Following (Fos-
ter et al., 2011), we consider URLs, usernames
and hashtags as NOUN. We did not change the tok-
enization.
The data sets differ in how they analyze many of
the linguistically hard cases. Consider, for exam-
ple, the analysis of will you come out to in GIM-
PEL and RITTER (Figure 1, top). While Gimpel
et al. (2011) tag out and to as adpositions, Ritter
et al. (2011) consider them particles. What is the
right analysis depends on the compositionality of
the construction and the linguistic theory one sub-
scribes to.
Other differences include the analysis of abbre-
viations (PRT in GIMPEL; X in RITTER and FOS-
TER), colon (X in GIMPEL; punctuation in RIT-
TER and FOSTER), and emoticons, which can take
multiple parts of speech in GIMPEL, but are al-
ways X in RITTER, while they are absent in FOS-
TER. GIMPEL-TRAIN and RITTER-TRAIN are
also internally inconsistent. See the bottom of Fig-
ure 1 for examples and Hovy et al. (2014) for a
more detailed discussion on differences between
the data sets.
Since the mapping to universal tags could
potentially introduce errors, we also annotated
a data set directly using universal tags. We
randomly selected 200 tweets collected over the
span of one day, and had three annotators tag
this set. We split the data in such a way that
each annotator had 100 tweets: two annotators
had disjoint sets, the third overlapped 50 items
with each of the two others. In this way, we
obtained an initial set of 100 doubly-annotated
tweets. The annotators were not provided with
annotation guidelines. After the first round of
annotations, we achieved a raw agreement of
0.9, a Cohen?s ? of 0.87, and a Krippendorff?s
? of 0.87. We did one pass over the data to
adjudicate the cases where annotators disagreed,
743
. . .
will you come out to the
. . .GIMPEL VERB PRON VERB ADP ADP DET
RITTER VERB PRON VERB PRT PRT DET
RITTER
. . .
you/PRON come/VERB out/PRT to/PRT
. . .
it/PRON comes/VERB out/ADP nov/NOUN
GIMPEL
. . .
Advances/NOUN and/CONJ Social/NOUN Media/NOUN .../X
. . .
Journalists/NOUN and/CONJ Social/ADJ Media/NOUN experts/NOUN
Figure 1: Annotation differences between (top) and within (bottom) two available Twitter POS data sets.
or where they had flagged their choice as debat-
able. The final data set (lowlands.test),
referred below to as INHOUSE, contained 3,064
tokens (200 tweets) and is publicly available
at http://bitbucket.org/lowlands/
costsensitive-data/, along with the data
used to compute inter-annotator agreement scores
for learning cost-sensitive taggers, described in
the next section.
3 Computing agreement scores
Gimpel et al. (2011) used 72 doubly-annotated
tweets to estimate inter-annotator agreement, and
we also use doubly-annotated data to compute
agreement scores. We randomly sampled 500
tweets for this purpose. Each tweet was anno-
tated by two annotators, again using the univer-
sal tag set (Petrov et al., 2012). All annotators
were encouraged to use their own best judgment
rather than following guidelines or discussing dif-
ficult cases with each other. This is in contrast to
Gimpel et al. (2011), who used annotation guide-
lines. The average inter-annotator agreement was
0.88 for raw agreement, and 0.84 for Cohen?s ?.
Gimpel et al. (2011) report a raw agreement of
0.92.
We use two metrics to provide a more detailed
picture of inter-annotator agreement, namely
F1-scores between annotators on individual parts
of speech, and tag confusion probabilities, which
we derive from confusion matrices.
The F1-score relates to precision and recall
in the usual way, i.e, as the harmonic mean
between those two measure. In more detail, given
two annotators A
1
and A
2
, we say the precision
Figure 2: Inter-annotator F1-scores estimated
from 500 tweets.
of A
1
relative to A
2
with respect to POS tag T in
some data setX , denoted Prec
T
(A
1
(X), A
2
(X)),
is the number of tokens both A
1
and A
2
predict to
be T over the number of times A
1
predicts a token
to be T . Similarly, we define the recall with re-
spect to some tag T , i.e., Rec
T
(A
1
(X), A
2
(X)),
as the number of tokens both A
1
and A
2
predict
to be T over the number of times A
2
predicts
a token to be T . The only difference with
respect to standard precision and recall is that
the gold standard is replaced by a second anno-
tator, A
2
. Note that Prec
T
(A
1
(X), A
2
(X)) =
Rec
T
(A
2
(X), A
1
(X)). It follows from all of
the above that the F1-score is symmetrical, i.e.,
F1
T
(A
1
(X), A
2
(X)) = F1
T
(A
2
(X), A
1
(X)).
The inter-annotator F1-scores over the 12
POS tags in the universal tagset are presented in
Figure 2. It shows that there is a high agreement
for nouns, verbs and punctuation, while the agree-
744
Figure 3: Confusion matrix of POS tags obtained
from 500 doubly-annotated tweets.
ment is low, for instance, for particles, numerals
and the X tag.
We compute tag confusion probabilities
from a confusion matrix over POS tags like
the one in Figure 3. From such a matrix,
we compute the probability of confusing
two tags t
1
and t
2
for some data point x,
i.e. P ({A
1
(x), A
2
(x)} = {t
1
, t
2
}) as the
mean of P (A
1
(x) = t
1
, A
2
(x) = t
2
) and
P (A
1
(x) = t
2
, A
2
(x) = t
1
), e.g., the confusion
probability of two tags is the mean of the prob-
ability that annotator A
1
assigns one tag and A
2
another, and vice versa.
We experiment with both agreement scores (F1
and confusion matrix probabilities) to augment the
loss function in our learner. The next section de-
scribes this modification in detail.
4 Inter-annotator agreement loss
We briefly introduce the cost-sensitive perceptron
classifier. Consider the weighted perceptron loss
on our ith example ?x
i
, y
i
? (with learning rate ? =
1), L
w
(?x
i
, y
i
?):
?(sign(w ? x
i
), y
i
) max(0,?y
i
w ? x
i
)
In a non-cost-sensitive classifier, the weight
function ?(y
j
, y
i
) = 1 for 1 ? i ? N . The
1: X = {?x
i
, y
i
?}
N
i=1
with x
i
= ?x
1
i
, . . . , x
m
i
?
2: I iterations
3: w = ?0?
m
4: for iter ? I do
5: for 1 ? i ? N do
6: y? = arg max
y?Y
w ? ?(x
i
, y)
7: w? w+ ?(y?, y
i
)[?(x
i
, y
i
)??(x
i
, y?)]
8: w
?
+ = w
9: end for
10: end for
11: return w
?
/ = (N ? I)
Figure 4: Cost-sensitive structured perceptron (see
Section 3 for weight functions ?).
two cost-sensitive systems proposed only differ in
how we formulate ?(?, ?). In one model, the loss is
weighted by the inter-annotator F1 of the gold tag
in question. This boils down to
?(y
j
, y
i
) = F1
y
i
(A
1
(X), A
2
(X))
where X is the small sample of held-out data used
to estimate inter-annotator agreement. Note that
in this formulation, the predicted label is not taken
into consideration.
The second model is slightly more expressive
and takes both the gold and predicted tags into ac-
count. It basically weights the loss by how likely
the gold and predicted tag are to be mistaken for
each other, i.e., (the inverse of) their confusion
probability:
?(y
j
, y
i
)) = 1?P ({A
1
(X), A
2
(X)} = {y
j
, y
i
})
In both loss functions, a lower gamma value
means that the tags are more likely to be confused
by a pair of annotators. In this case, the update is
smaller. In contrast, the learner incurs greater loss
when easy tags are confused.
It is straight-forward to extend these cost-
sensitive loss functions to the structured percep-
tron (Collins, 2002). In Figure 4, we provide the
pseudocode for the cost-sensitive structured online
learning algorithm. We refer to the cost-sensitive
structured learners as F1- and CM-weighted be-
low.
5 Experiments
In our main experiments, we use structured per-
ceptron (Collins, 2002) with random corruptions
745
using a drop-out rate of 0.1 for regularization, fol-
lowing S?gaard (2013a). We use the LXMLS
toolkit implementation
1
with default parameters.
We present learning curves across iterations, and
only set parameters using held-out data for our
downstream experiments.
2
5.1 Results
Our results are presented in Figure 5. The top left
graph plots accuracy on the training data per iter-
ation. We see that CM-weighting does not hurt
training data accuracy. The reason may be that
the cost-sensitive learner does not try (as hard) to
optimize performance on inconsistent annotations.
The next two plots (upper mid and upper right)
show accuracy over epochs on in-sample evalua-
tion data, i.e., GIMPEL-DEV and RITTER-TEST.
Again, the CM-weighted learner performs better
than our baseline model, while the F1-weighted
learner performs much worse.
The interesting results are the evaluations on
out-of-sample evaluation data sets (FOSTER and
IN-HOUSE) - lower part of Figure 5. Here, both
our learners are competitive, but overall it is clear
that the CM-weighted learner performs best. It
consistently improves over the baseline and F1-
weighting. The former is much more expressive
as it takes confusion probabilities into account and
does not only update based on gold-label uncer-
tainty, as is the case with the F1-weighted learner.
5.2 Robustness across regularizers
Discriminative learning typically benefits from
regularization to prevent overfitting. The simplest
is the averaged perceptron, but various other meth-
ods have been suggested in the literature.
We use structured perceptron with drop-out, but
results are relatively robust across other regular-
ization methods. Drop-out works by randomly
dropping a fraction of the active features in each
iteration, thus preventing overfitting. Table 1
shows the results for using different regularizers,
in particular, Zipfian corruptions (S?gaard, 2013b)
and averaging. While there are minor differences
across data sets and regularizers, we observe that
the corresponding cell using the loss function sug-
gested in this paper (CM) always performs better
than the baseline method.
1
https://github.com/gracaninja/
lxmls-toolkit/
2
In this case, we use FOSTER-DEV as our development
data to avoid in-sample bias.
6 Downstream evaluation
We have seen that our POS tagging model im-
proves over the baseline model on three out-of-
sample test sets. The question remains whether
training a POS tagger that takes inter-annotator
agreement scores into consideration is also effec-
tive on downstream tasks. Therefore, we eval-
uate our best model, the CM-weighted learner,
in two downstream tasks: shallow parsing?also
known as chunking?and named entity recogni-
tion (NER).
For the downstream evaluation, we used the
baseline and CM models trained over 13 epochs,
as they performed best on FOSTER-DEV (cf. Fig-
ure 5). Thus, parameters were optimized only on
POS tagging data, not on the downstream evalu-
ation tasks. We use a publicly available imple-
mentation of conditional random fields (Lafferty
et al., 2001)
3
for the chunking and NER exper-
iments, and provide the POS tags from our CM
learner as features.
6.1 Chunking
The set of features for chunking include informa-
tion from tokens and POS tags, following Sha and
Pereira (2003).
We train the chunker on Twitter data (Ritter et
al., 2011), more specifically, the 70/30 train/test
split provided by Derczynski et al. (2013) for POS
tagging, as the original authors performed cross
validation. We train on the 70% Twitter data (11k
tokens) and evaluate on the remaining 30%, as
well as on the test data from Foster et al. (2011).
The FOSTER data was originally annotated for
POS and constituency tree information. We con-
verted it to chunks using publicly available conver-
sion software.
4
Part-of-speech tags are the ones
assigned by our cost-sensitive (CM) POS model
trained on Twitter data, the concatenation of Gim-
pel and 70% Ritter training data. We did not in-
clude the CoNLL 2000 training data (newswire
text), since adding it did not substantially improve
chunking performance on tweets, as also shown
in (Ritter et al., 2011).
The results for chunking are given in Ta-
ble 2. They show that using the POS tagging
model (CM) trained to be more sensitive to inter-
annotator agreement improves performance over
3
http://crfpp.googlecode.com
4
http://ilk.uvt.nl/team/sabine/
homepage/software.html
746
5 10 15 20 25
Epochs
74
75
76
77
78
79
80
81
82
Ac
cu
rac
y(
%)
TRAINING
BASELINE
F1
CM
5 10 15 20 25
Epochs
77.5
78.0
78.5
79.0
79.5
80.0
80.5
Ac
cu
rac
y(
%)
GIMPEL-DEV
BASELINE
F1
CM
5 10 15 20 25
Epochs
83.5
84.0
84.5
85.0
85.5
86.0
86.5
87.0
Ac
cu
rac
y(
%)
RITTER-TEST
BASELINE
F1
CM
5 10 15 20 25
Epochs
81.0
81.5
82.0
82.5
83.0
83.5
84.0
Ac
cu
rac
y(
%)
FOSTER-DEV
BASELINE
F1
CM
5 10 15 20 25
Epochs
82.5
83.0
83.5
84.0
84.5
85.0
Ac
cu
rac
y(
%)
FOSTER-TEST
BASELINE
F1
CM
5 10 15 20 25
Epochs
82.2
82.4
82.6
82.8
83.0
83.2
83.4
83.6
83.8
84.0
Ac
cu
rac
y(
%)
IN-HOUSE
BASELINE
F1
CM
Figure 5: POS accuracy for the three models: baseline, confusion matrix loss (CM) and F1-weighted
(F1) loss for increased number of training epochs. Top row: in-sample accuracy on training (left) and
in-sample evaluation datasets (center, right). Bottom row: out-of-sample accuracy on various data sets.
CM is robust on both in-sample and out-of-sample data.
RITTER-TEST
F1: All NP VP PP
BL 76.20 78.61 74.25 86.79
CM 76.42 79.07 74.98 86.19
FOSTER-TEST
F1: All NP VP PP
BL 68.49 70.73 60.56 86.50
CM 68.97 71.25 61.97 87.24
Table 2: Downstream results on chunking. Overall
F1 score (All) as well as F1 for NP, VP and PP.
the baseline (BL) for the downstream task of
chunking. Overall chunking F1 score improves.
More importantly, we report on individual scores
for NP, VP and PP chunks, where we see consis-
tent improvements for NPs and VPs (since both
nouns and verbs have high inter-annotator agree-
ment), while results on PP are mixed. This is to
be expected, since PP phrases involve adposition-
als (ADP) that are often confused with particles
(PRT), cf. Figure 3. Our tagger has been trained
to deliberately abstract away from such uncertain
cases. The results show that taking uncertainty in
POS annotations into consideration during train-
ing has a positive effect in downstream results. It
is thus better if we do not try to urge our models
to make a firm decision on phenomena that neither
747
BASELINE CM
Regularizer FOSTER-DEV FOSTER-TEST IN-HOUSE FOSTER-DEV FOSTER-TEST IN-HOUSE
Averaging 0.827 0.837 0.830 0.831 0.844 0.833
Drop-out 0.827 0.838 0.827 0.836 0.843 0.833
Zipfian 0.821 0.835 0.833 0.825 0.838 0.836
Table 1: Results across regularizers (after 13 epochs).
linguistic theories nor annotators do agree upon.
6.2 NER
In the previous section, we saw positive effects of
cost-sensitive POS tagging for chunking, and here
we evaluate it on another downstream task, NER.
For the named entity recognition setup, we use
commonly used features, in particular features
for word tokens, orthographic features like the
presence of hyphens, digits, single quotes, up-
per/lowercase, 3 character prefix and suffix infor-
mation. Moreover, we add Brown word cluster
features that use 2,4,6,8,..,16 bitstring prefixes es-
timated from a large Twitter corpus (Owoputi et
al., 2013).
5
For NER, we do not have access to carefully
annotated Twitter data for training, but rely on
the crowdsourced annotations described in Finin
et al. (2010). We use the concatenation of the
CoNLL 2003 training split of annotated data from
the Reuters corpus and the Finin data for training,
as in this case training on the union resulted in a
model that is substantially better than training on
any of the individual data sets. For evaluation, we
have three Twitter data set. We use the recently
published data set from the MSM 2013 challenge
(29k tokens)
6
, the data set of Ritter et al. (2011)
used also by Fromheide et al. (2014) (46k tokens),
as well as an in-house annotated data set (20k to-
kens) (Fromheide et al., 2014).
F1: RITTER MSM IN-HOUSE
BL 78.20 82.25 82.58
CM 78.30 82.00 82.77
Table 3: Downstream results for named entity
recognition (F1 scores).
Table 3 shows the result of using our POS mod-
els in downstream NER evaluation. Here we ob-
serve mixed results. The cost-sensitive model is
5
http://www.ark.cs.cmu.edu/TweetNLP/
6
http://oak.dcs.shef.ac.uk/msm2013/ie_
challenge/
able to improve performance on two out of the
three test sets, while being slightly below baseline
performance on the MSM challenge data. Note
that in contrast to chunking, POS tags are just one
of the many features used for NER (albeit an im-
portant one), which might be part of the reason
why the picture looks slightly different from what
we observed above on chunking.
7 Related work
Cost-sensitive learning takes costs, such as mis-
classification cost, into consideration. That is,
each instance that is not classified correctly during
the learning process may contribute differently to
the overall error. Geibel and Wysotzki (2003) in-
troduce instance-dependent cost values for the per-
ceptron algorithm and apply it to a set of binary
classification problems. We focus here on struc-
tured problems and propose cost-sensitive learn-
ing for POS tagging using the structured percep-
tron algorithm. In a similar spirit, Higashiyama
et al. (2013) applied cost-sensitive learning to the
structured perceptron for an entity recognition task
in the medical domain. They consider the dis-
tance between the predicted and true label se-
quence smoothed by a parameter that they esti-
mate on a development set. This means that the
entire sequence is scored at once, while we update
on a per-label basis.
The work most related to ours is the recent study
of Song et al. (2012). They suggest that some er-
rors made by a POS tagger are more serious than
others, especially for downstream tasks. They de-
vise a hierarchy of POS tags for the Penn tree-
bank tag set (e.g. the class NOUN contains NN,
NNS, NNP, NNPS and CD) and use that in an
SVM learner. They modify the Hinge loss that
can take on three values: 0, ?, 1. If an error oc-
curred and the predicted tag is in the same class as
the gold tag, a loss ? occurred, otherwise it counts
as full cost. In contrast to our approach, they let
the learner focus on the more difficult cases by oc-
curring a bigger loss when the predicted POS tag
748
is in a different category. Their approach is thus
suitable for a fine-grained tagging scheme and re-
quires tuning of the cost parameter ?. We tackle
the problem from a different angle by letting the
learner abstract away from difficult, inconsistent
cases as estimated from inter-annotator scores.
Our approach is also related to the literature
on regularization, since our cost-sensitive loss
functions are aimed at preventing over-fitting to
low-confidence annotations. S?gaard (2013b;
2013a) presented two theories of linguistic varia-
tion and perceptron learning algorithms that reg-
ularize models to minimize loss under expected
variation. Our work is related, but models varia-
tions in annotation rather than variations in input.
There is a large literature related to the issue of
learning from annotator bias. Reidsma and op den
Akker (2008) show that differences between anno-
tators are not random slips of attention but rather
different biases annotators might have, i.e. differ-
ent mental conceptions. They show that a classi-
fier trained on data from one annotator performed
much better on in-sample (same annotator) data
than on data of any other annotator. They propose
two ways to address this problem: i) to identify
subsets of the data that show higher inter-annotator
agreement and use only that for training (e.g. for
speaker address identification they restrict the data
to instances where at least one person is in the
focus of attention); ii) if available, to train sepa-
rate models on data annotated by different anno-
tators and combine them through voting. The lat-
ter comes at the cost of recall, because they de-
liberately chose the classifier to abstain in non-
consensus cases.
In a similar vein, Klebanov and Beigman (2009)
divide the instance space into easy and hard cases,
i.e. easy cases are reliably annotated, whereas
items that are hard show confusion and disagree-
ment. Hard cases are assumed to be annotated
by individual annotator?s coin-flips, and thus can-
not be assumed to be uniformly distributed (Kle-
banov and Beigman, 2009). They show that learn-
ing with annotator noise can have deteriorating ef-
fect at test time, and thus propose to remove hard
cases, both at test time (Klebanov and Beigman,
2009) and training time (Beigman and Klebanov,
2009).
In general, it is important to analyze the data
and check for label biases, as a machine learner is
greatly affected by annotator noise that is not ran-
dom but systematic (Reidsma and Carletta, 2008).
However, rather than training on subsets of data or
training separate models ? which all implicitly as-
sume that there is a large amount of training data
available ? we propose to integrate inter-annotator
biases directly into the loss function.
Regarding measurements for agreements, sev-
eral scores have been suggested in the literature.
Apart from the simple agreement measure, which
records how often annotators choose the same
value for an item, there are several statistics that
qualify this measure by adjusting for other fac-
tors, such as Cohen?s ? (Cohen and others, 1960),
the G-index score (Holley and Guilford, 1964), or
Krippendorff?s ? (Krippendorf, 2004). However,
most of these scores are sensitive to the label dis-
tribution, missing values, and other circumstances.
The measure used in this paper is less affected by
these factors, but manages to give us a good un-
derstanding of the agreement.
8 Conclusion
In NLP, we use a variety of measures to assess
and control annotator disagreement to produce ho-
mogenous final annotations. This masks the fact
that some annotations are more reliable than oth-
ers, and which is thus not reflected in learned pre-
dictors. We incorporate the annotator uncertainty
on certain labels by measuring annotator agree-
ment and use it in the modified loss function of
a structured perceptron. We show that this ap-
proach works well independent of regularization,
both on in-sample and out-of-sample data. More-
over, when evaluating the models trained with our
loss function on downstream tasks, we observe im-
provements on two different tasks. Our results
suggest that we need to pay more attention to an-
notator confidence when training predictors.
Acknowledgements
We would like to thank the anonymous review-
ers and Nathan Schneider for valuable comments
and feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Eyal Beigman and Beata Klebanov. 2009. Learning
with annotation noise. In ACL.
Jacob Cohen et al. 1960. A coefficient of agreement
749
for nominal scales. Educational and psychological
measurement, 20(1):37?46.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In ACL.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez, and Anders
S?gaard. 2013. Down-stream effects of tree-to-
dependency conversions. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Hege Fromheide, Dirk Hovy, and Anders S?gaard.
2014. Crowdsourcing and annotating NER for Twit-
ter #drift. In Proceedings of LREC 2014.
Peter Geibel and Fritz Wysotzki. 2003. Perceptron
based learning with example dependent and noisy
costs. In ICML.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In ACL.
Shohei Higashiyama, Kazuhiro Seki, and Kuniaki Ue-
hara. 2013. Clinical entity recognition using
cost-sensitive structured perceptron for NTCIR-10
MedNLP. In NTCIR.
Jasper Wilson Holley and Joy Paul Guilford. 1964.
A Note on the G-Index of Agreement. Educational
and Psychological Measurement, 24(4):749.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
When POS datasets don?t add up: Combatting sam-
ple bias. In Proceedings of LREC 2014.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In NAACL.
Beata Klebanov and Eyal Beigman. 2009. From an-
notator agreement to noise models. Computational
Linguistics, 35(4):495?503.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Computational Linguistics and Intelligent
Text Processing, pages 171?189. Springer.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In ACL.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Dennis Reidsma and Jean Carletta. 2008. Reliabil-
ity measurement without limits. Computational Lin-
guistics, 34(3):319?326.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ?subjective? annotations. In Workshop on
Human Judgements in Computational Linguistics,
COLING.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In ACL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Anders S?gaard. 2013a. Part-of-speech tagging with
antagonistic adversaries. In ACL.
Anders S?gaard. 2013b. Zipfian corruptions for robust
pos tagging. In NAACL.
750
Hyun-Je Song, Jeong-Woo Son, Tae-Gil Noh, Seong-
Bae Park, and Sang-Jo Lee. 2012. A cost sensitive
part-of-speech tagging: differentiating serious errors
from minor errors. In ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
751
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1566?1576,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Effective Measures of Domain Similarity for Parsing
Barbara Plank
University of Groningen
The Netherlands
b.plank@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
G.J.M.van.Noord@rug.nl
Abstract
It is well known that parsing accuracy suf-
fers when a model is applied to out-of-domain
data. It is also known that the most benefi-
cial data to parse a given domain is data that
matches the domain (Sekine, 1997; Gildea,
2001). Hence, an important task is to select
appropriate domains. However, most previ-
ous work on domain adaptation relied on the
implicit assumption that domains are some-
how given. As more and more data becomes
available, automatic ways to select data that is
beneficial for a new (unknown) target domain
are becoming attractive. This paper evaluates
various ways to automatically acquire related
training data for a given test set. The results
show that an unsupervised technique based on
topic models is effective ? it outperforms ran-
dom data selection on both languages exam-
ined, English and Dutch. Moreover, the tech-
nique works better than manually assigned la-
bels gathered from meta-data that is available
for English.
1 Introduction and Motivation
Previous research on domain adaptation has focused
on the task of adapting a system trained on one do-
main, say newspaper text, to a particular new do-
main, say biomedical data. Usually, some amount
of (labeled or unlabeled) data from the new domain
was given ? which has been determined by a human.
However, with the growth of the web, more and
more data is becoming available, where each doc-
ument ?is potentially its own domain? (McClosky
et al, 2010). It is not straightforward to determine
which data or model (in case we have several source
domain models) will perform best on a new (un-
known) target domain. Therefore, an important is-
sue that arises is how to measure domain similar-
ity, i.e. whether we can find a simple yet effective
method to determine which model or data is most
beneficial for an arbitrary piece of new text. More-
over, if we had such a measure, a related question is
whether it can tell us something more about what is
actually meant by ?domain?. So far, it was mostly
arbitrarily used to refer to some kind of coherent
unit (related to topic, style or genre), e.g.: newspa-
per text, biomedical abstracts, questions, fiction.
Most previous work on domain adaptation, for in-
stance Hara et al (2005), McClosky et al (2006),
Blitzer et al (2006), Daume? III (2007), sidestepped
this problem of automatic domain selection and
adaptation. For parsing, to our knowledge only one
recent study has started to examine this issue (Mc-
Closky et al, 2010) ? we will discuss their approach
in Section 2. Rather, an implicit assumption of all of
these studies is that domains are given, i.e. that they
are represented by the respective corpora. Thus, a
corpus has been considered a homogeneous unit. As
more data is becoming available, it is unlikely that
domains will be ?given?. Moreover, a given corpus
might not always be as homogeneous as originally
thought (Webber, 2009; Lippincott et al, 2010). For
instance, recent work has shown that the well-known
Penn Treebank (PT) Wall Street Journal (WSJ) ac-
tually contains a variety of genres, including letters,
wit and short verse (Webber, 2009).
In this study we take a different approach. Rather
than viewing a given corpus as a monolithic entity,
1566
we break it down to the article-level and disregard
corpora boundaries. Given the resulting set of doc-
uments (articles), we evaluate various ways to au-
tomatically acquire related training data for a given
test set, to find answers to the following questions:
? Given a pool of data (a collection of articles
from unknown domains) and a test article, is
there a way to automatically select data that is
relevant for the new domain? If so:
? Which similarity measure is good for parsing?
? How does it compare to human-annotated data?
? Is the measure also useful for other languages
and/or tasks?
To this end, we evaluate measures of domain sim-
ilarity and feature representations and their impact
on dependency parsing accuracy. Given a collection
of annotated articles, and a new article that we want
to parse, we want to select the most similar articles
to train the best parser for that new article.
In the following, we will first compare automatic
measures to human-annotated labels by examining
parsing performance within subdomains of the Penn
Treebank WSJ. Then, we extend the experiments to
the domain adaptation scenario. Experiments were
performed on two languages: English and Dutch.
The empirical results show that a simple measure
based on topic distributions is effective for both lan-
guages and works well also for Part-of-Speech tag-
ging. As the approach is based on plain surface-
level information (words) and it finds related data in
a completely unsupervised fashion, it can be easily
applied to other tasks or languages for which anno-
tated (or automatically annotated) data is available.
2 Related Work
The work most related to ours is McClosky et al
(2010). They try to find the best combination of
source models to parse data from a new domain,
which is related to Plank and Sima?an (2008). In
the latter, unlabeled data was used to create sev-
eral parsers by weighting trees in the WSJ accord-
ing to their similarity to the subdomain. McClosky
et al (2010) coined the term multiple source domain
adaptation. Inspired by work on parsing accuracy
prediction (Ravi et al, 2008), they train a linear re-
gression model to predict the best (linear interpola-
tion) of source domain models. Similar to us, Mc-
Closky et al (2010) regard a target domain as mix-
ture of source domains, but they focus on phrase-
structure parsing. Furthermore, our approach differs
from theirs in two respects: we do not treat source
corpora as one entity and try to mix models, but
rather consider articles as base units and try to find
subsets of related articles (the most similar articles);
moreover, instead of creating a supervised model (in
their case to predict parsing accuracy), our approach
is ?simplistic?: we apply measures of domain simi-
larity directly (in an unsupervised fashion), without
the necessity to train a supervised model.
Two other related studies are (Lippincott et al,
2010; Van Asch and Daelemans, 2010). Van Asch
and Daelemans (2010) explore a measure of domain
difference (Renyi divergence) between pairs of do-
mains and its correlation to Part-of-Speech tagging
accuracy. Their empirical results show a linear cor-
relation between the measure and the performance
loss. Their goal is different, but related: rather than
finding related data for a new domain, they want to
estimate the loss in accuracy of a PoS tagger when
applied to a new domain. We will briefly discuss
results obtained with the Renyi divergence in Sec-
tion 5.1. Lippincott et al (2010) examine subdomain
variation in biomedicine corpora and propose aware-
ness of NLP tools to such variation. However, they
did not yet evaluate the effect on a practical task,
thus our study is somewhat complementary to theirs.
The issue of data selection has recently been ex-
amined for Language Modeling (Moore and Lewis,
2010). A subset of the available data is automati-
cally selected as training data for a Language Model
based on a scoring mechanism that compares cross-
entropy scores. Their approach considerably outper-
formed random selection and two previous proposed
approaches both based on perplexity scoring.1
3 Measures of Domain Similarity
3.1 Measuring Similarity Automatically
Feature Representations A similarity function
may be defined over any set of events that are con-
1We tested data selection by perplexity scoring, but found
the Language Models too small to be useful in our setting.
1567
sidered to be relevant for the task at hand. For
parsing, these might be words, characters, n-grams
(of words or characters), Part-of-Speech (PoS) tags,
bilexical dependencies, syntactic rules, etc. How-
ever, to obtain more abstract types such as PoS tags
or dependency relations, one would first need to
gather respective labels. The necessary tools for this
are again trained on particular corpora, and will suf-
fer from domain shifts, rendering labels noisy.
Therefore, we want to gauge the effect of the sim-
plest representation possible: plain surface charac-
teristics (unlabeled text). This has the advantage
that we do not need to rely on additional supervised
tools; moreover, it is interesting to know how far we
can get with this level of information only.
We examine the following feature representa-
tions: relative frequencies of words, relative fre-
quencies of character tetragrams, and topic mod-
els. Our motivation was as follows. Relative fre-
quencies of words are a simple and effective rep-
resentation used e.g. in text classification (Manning
and Schu?tze, 1999), while character n-grams have
proven successful in genre classification (Wu et al,
2010). Topic models (Blei et al, 2003; Steyvers
and Griffiths, 2007) can be considered an advanced
model over word distributions: every article is repre-
sented by a topic distribution, which in turn is a dis-
tribution over words. Similarity between documents
can be measured by comparing topic distributions.
Similarity Functions There are many possible
similarity (or distance) functions. They fall broadly
into two categories: probabilistically-motivated and
geometrically-motivated functions. The similarity
functions examined in this study will be described
in the following.
The Kullback-Leibler (KL) divergence D(q||r) is
a classical measure of ?distance?2 between two prob-
ability distributions, and is defined as: D(q||r) =
?
y q(y) log
q(y)
r(y) . It is a non-negative, additive,
asymmetric measure, and 0 iff the two distributions
are identical. However, the KL-divergence is unde-
fined if there exists an event y such that q(y) > 0
but r(y) = 0, which is a property that ?makes it
unsuitable for distributions derived via maximum-
likelihood estimates? (Lee, 2001).
2It is not a proper distance metric since it is asymmetric.
One option to overcome this limitation is to apply
smoothing techniques to gather non-zero estimates
for all y. The alternative, examined in this paper, is
to consider an approximation to the KL divergence,
such as the Jensen-Shannon (JS) divergence (Lin,
1991) and the skew divergence (Lee, 2001).
The Jensen-Shannon divergence, which is sym-
metric, computes the KL-divergence between q, r,
and the average between the two. We use the JS
divergence as defined in Lee (2001): JS(q, r) =
1
2 [D(q||avg(q, r)) + D(r||avg(q, r))]. The asym-
metric skew divergence s?, proposed by Lee (2001),
mixes one distribution with the other by a degree de-
fined by ? ? [0, 1): s?(q, r, ?) = D(q||?r + (1 ?
?)q). As ? approaches 1, the skew divergence ap-
proximates the KL-divergence.
An alternative way to measure similarity is to
consider the distributions as vectors and apply
geometrically-motivated distance functions. This
family of similarity functions includes the cosine
cos(q, r) = q(y) ? r(y)/||q(y)||||r(y)||, euclidean
euc(q, r) =
??
y(q(y)? r(y))
2 and variational
(also known as L1 or Manhattan) distance function,
defined as var(q, r) =
?
y |q(y)? r(y)|.
3.2 Human-annotated data
In contrast to the automatic measures devised in the
previous section, we might have access to human an-
notated data. That is, use label information such as
topic or genre to define the set of similar articles.
Genre For the Penn Treebank (PT) Wall Street
Journal (WSJ) section, more specifically, the subset
available in the Penn Discourse Treebank, there ex-
ists a partition of the data by genre (Webber, 2009).
Every article is assigned one of the following genre
labels: news, letters, highlights, essays, errata, wit
and short verse, quarterly progress reports, notable
and quotable. This classification has been made on
the basis of meta-data (Webber, 2009). It is well-
known that there is no meta-data directly associated
with the individual WSJ files in the Penn Treebank.
However, meta-data can be obtained by looking at
the articles in the ACL/DCI corpus (LDC99T42),
and a mapping file that aligns document numbers of
DCI (DOCNO) to WSJ keys (Webber, 2009). An
example document is given in Figure 1. The meta-
data field HL contains headlines, SO source info, and
1568
the IN field includes topic markers.
<DOC><DOCNO> 891102-0186. </DOCNO>
<WSJKEY> wsj_0008 </WSJKEY>
<AN> 891102-0186. </AN>
<HL> U.S. Savings Bonds Sales
@ Suspended by Debt Limit </HL>
<DD> 11/02/89 </DD>
<SO> WALL STREET JOURNAL (J) </SO>
<IN> FINANCIAL, ACCOUNTING, LEASING (FIN)
BOND MARKET NEWS (BON) </IN>
<GV> TREASURY DEPARTMENT (TRE) </GV>
<DATELINE> WASHINGTON </DATELINE>
<TXT>
<p><s>
The federal government suspended sales of U.S.
savings bonds because Congress hasn?t lifted
the ceiling on government debt.</s></p> [...]
Figure 1: Example of ACL/DCI article. We have aug-
mented it with the WSJ filename (WSJKEY).
Topic On the basis of the same meta-data, we
devised a classification of the Penn Treebank WSJ
by topic. That is, while the genre division has been
mostly made on the basis of headlines, we use the
information of the IN field. Every article is assigned
one, more than one or none of a predefined set of
keywords. While their origin remains unclear,3
these keywords seem to come from a controlled
vocabulary. There are 76 distinct topic markers.
The three most frequent keywords are: TENDER
OFFERS, MERGERS, ACQUISITIONS (TNM),
EARNINGS (ERN), STOCK MARKET, OFFERINGS
(STK). This reflects the fact that a lot of arti-
cles come from the financial domain. But the
corpus also contains articles from more distant do-
mains, like MARKETING, ADVERTISING (MKT),
COMPUTERS AND INFORMATION TECHNOLOGY
(CPR), HEALTH CARE PROVIDERS, MEDICINE,
DENTISTRY (HEA), PETROLEUM (PET).
4 Experimental Setup
4.1 Tools & Evaluation
The parsing system used in this study is the MST
parser (McDonald et al, 2005), a state-of-the-art
data-driven graph-based dependency parser. It is
3It is not known what IN stands for, as also stated in Mark
Liberman?s notes in the readme of the ACL/DCI corpus. How-
ever, a reviewer suggested that IN might stand for ?index terms?
which seems plausible.
a system that can be trained on a variety of lan-
guages given training data in CoNLL format (Buch-
holz and Marsi, 2006). Additionally, the parser im-
plements both projective and non-projective pars-
ing algorithms. The projective algorithm is used for
the experiments on English, while the non-projective
variant is used for Dutch. We train the parser using
default settings. MST takes PoS-tagged data as in-
put; we use gold-standard tags in the experiments.
We estimate topic models using Latent Dirichlet
Allocation (Blei et al, 2003) implemented in the
MALLET4 toolkit. Like Lippincott et al (2010),
we set the number of topics to 100, and otherwise
use standard settings (no further optimization). We
experimented with the removal of stopwords, but
found no deteriorating effect while keeping them.
Thus, all experiments are carried out on data where
stopwords were not removed.
We implemented the similarity measures pre-
sented in Section 3.1. For skew divergence, that re-
quires parameter ?, we set ? = .99 (close to KL
divergence) since that has shown previously to work
best (Lee, 2001). Additionally, we evaluate the ap-
proach on English PoS tagging using two different
taggers: MXPOST, the MaxEnt tagger of Ratna-
parkhi5 and Citar,6 a trigram HMM tagger.
In all experiments, parsing performance is mea-
sured as Labeled Attachment Score (LAS), the per-
centage of tokens with correct dependency edge and
label. To compute LAS, we use the CoNLL 2007
evaluation script7 with punctuation tokens excluded
from scoring (as was the default setting in CoNLL
2006). PoS tagging accuracy is measured as the per-
centage of correctly labeled words out of all words.
Statistical significance is determined by Approxi-
mate Randomization Test (Noreen, 1989; Yeh, 2000)
with 10,000 iterations.
4.2 Data
English - WSJ For English, we use the portion of
the Penn Treebank Wall Street Journal (WSJ) that
has been made available in the CoNLL 2008 shared
4http://mallet.cs.umass.edu/
5ftp://ftp.cis.upenn.edu/pub/adwait/jmx/
6Citar has been implemented by Danie?l de Kok and is avail-
able at: https://github.com/danieldk/citar
7http://nextens.uvt.nl/depparse-wiki/
1569
task. This data has been automatically converted8
into dependency structure, and contains three files:
the training set (sections 02-21), development set
(section 24) and test set (section 23).
Since we use articles as basic units, we actually
split the data to get back original article boundaries.9
This led to a total of 2,034 articles (1 million words).
Further statistics on the datasets are given in Ta-
ble 1. In the first set of experiments on WSJ subdo-
mains, we consider articles from section 23 and 24
that contain at least 50 sentences as test sets (target
domains). This amounted to 22 test articles.
EN: WSJ WSJ+G+B Dutch
articles 2,034 3,776 51,454
sentences 43,117 77,422 1,663,032
words 1,051,997 1,784,543 20,953,850
Table 1: Overview of the datasets for English and Dutch.
To test whether we have a reasonable system,
we performed a sanity check and trained the MST
parser on the training section (02-21). The result
on the standard test set (section 23) is identical to
previously reported results (excluding punctuation
tokens: LAS 87.50, Unlabeled Attachment Score
(UAS) 90.75; with punctuation tokens: LAS 87.07,
UAS 89.95). The latter has been reported in (Sur-
deanu and Manning, 2010).
English - Genia (G) & Brown (B) For the Do-
main Adaptation experiments, we added 1,552 ar-
ticles from the GENIA10 treebank (biomedical ab-
stracts from Medline) and 190 files from the Brown
corpus to the pool of data. We converted the data
to CoNLL format with the LTH converter (Johans-
son and Nugues, 2007). The size of the test files is,
respectively: Genia 1,360 sentences with an aver-
age number of 26.20 words per sentence; the Brown
test set is the same as used in the CoNLL 2008
shared task and contains 426 sentences with a mean
of 16.80 words.
8Using the LTH converter: http://nlp.cs.lth.se/
software/treebank_converter/
9This was a non-trivial task, as we actually noticed that some
sentences have been omitted from the CoNLL 2008 shared task.
10We use the GENIA distribution in Penn Treebank for-
mat available at http://bllip.cs.brown.edu/download/
genia1.0-division-rel1.tar.gz
5 Experiments on English
5.1 Experiments within the WSJ
In the first set of experiments, we focus on the WSJ
and evaluate the similarity functions to gather re-
lated data for a given test article. We have 22 WSJ
articles as test set, sampled from sections 23 and
24. Regarding feature representations, we examined
three possibilities: relative frequencies of words, rel-
ative frequencies of character tetragrams (both un-
smoothed) and document topic distributions.
In the following, we only discuss representations
based on words or topic models as we found charac-
ter tetragrams less stable; they performed sometimes
like their word-based counterparts but other times,
considerably worse.
Results of Similarity Measures Table 2 com-
pares the effect of the different ways to select re-
lated data in comparison to the random baseline for
increasing amounts of training data. The table gives
the average over 22 test articles (rather than show-
ing individual tables for the 22 articles). We select
articles up to various thresholds that specify the to-
tal number of sentences selected in each round (e.g.
0.3k, 1.2k, etc.).11 In more detail, Table 2 shows the
result of applying various similarity functions (intro-
duced in Section 3.1) over the two different feature
representations (w: words; tm: topic model) for in-
creasing amounts of data. We additionally provide
results of using the Renyi divergence.12
Clearly, as more and more data is selected, the
differences become smaller, because we are close
to the data limit. However, for all data points less
than 38k (97%), selection by jensen-shannon, varia-
tional and cosine similarity outperform random data
selection significantly for both types of feature rep-
resentations (words and topic model). For selection
by topic models, this additionally holds for the eu-
clidean measure.
From the various measures we can see that se-
lection by jensen-shannon divergence and varia-
tional distance perform best, followed by cosine
similarity, skew divergence, euclidean and renyi.
11Rather than choosing k articles, as article length may differ.
12The Renyi divergence (Re?nyi, 1961), also used by Van
Asch and Daelemans (2010), is defined as D?(q, r) = 1/(??
1) log(
?
q?r1??).
1570
1% 3% 25% 49% 97%
(0.3k) (1.2k) (9.6k) (19.2k) (38k)
random 70.61 77.21 82.98 84.48 85.51
w-js 74.07? 79.41? 83.98? 84.94? 85.68
w-var 74.07? 79.60? 83.82? 84.94? 85.45
w-skw 74.20? 78.95? 83.68? 84.60 85.55
w-cos 73.77? 79.30? 83.87? 84.96? 85.59
w-euc 73.85? 78.90? 83.52? 84.68 85.57
w-ryi 73.41? 78.31 83.76? 84.46 85.46
tm-js 74.23? 79.49? 84.04? 85.01? 85.45
tm-var 74.29? 79.59? 83.93? 84.94? 85.43
tm-skw 74.13? 79.42? 84.13? 84.82 85.73
tm-cos 74.04? 79.27? 84.14? 84.99? 85.42
tm-euc 74.27? 79.53? 83.93? 85.15? 85.62
tm-ryi 71.26 78.64? 83.79? 84.85 85.58
Table 2: Comparison of similarity measures based
on words (w) and topic model (tm): parsing accu-
racy for increasing amounts of training data as average
over 22 WSJ articles (js=jensen-shannon; cos=cosine;
skw=skew; var=variational; euc=euclidean; ryi=renyi).
Best score (per representation) underlined, best overall
score bold; ? indicates significantly better (p < 0.05)
than random.
Renyi divergence does not perform as well as other
probabilistically-motivated functions. Regarding
feature representations, the representation based on
topic models works slightly better than the respec-
tive word-based measure (cf. Table 2) and often
achieves the overall best score (boldface).
Overall, the differences in accuracy between the
various similarity measures are small; but interest-
ingly, the overlap between them is not that large.
Table 3 and Table 4 show the overlap (in terms of
proportion of identically selected articles) between
pairs of similarity measures. As shown in Table 3,
for all measures there is only a small overlap with
the random baseline (around 10%-14%). Despite
similar performance, topic model selection has inter-
estingly no substantial overlap with any other word-
based similarity measures: their overlap is at most
41.6%. Moreover, Table 4 compares the overlap of
the various similarity functions within a certain fea-
ture representation (here x stands for either topic
model ? left value ? or words ? right value). The
table shows that there is quite some overlap be-
tween jensen-shannon, variational and skew diver-
gence on one side, and cosine and euclidean on
the other side, i.e. between probabilistically- and
geometrically-motivated functions. Variational has
a higher overlap with the probabilistic functions. In-
terestingly, the ?peaks? in Table 4 (underlined, i.e.
the highest pair-wise overlaps) are the same for the
different feature representations.
In the following we analyze selection by topic
model and words, as they are relatively different
from each other, despite similar performance. For
the word-based model, we use jensen-shannon as
similarity function, as it turned out to be the best
measure. For topic model, we use the simpler vari-
ational metric. However, very similar results were
achieved using jensen-shannon. Cosine and eu-
clidean did not perform as well.
ran w-js w-var w-skw w-cos w-euc
ran ? 10.3 10.4 10.0 10.4 10.2
tm-js 12.1 41.6 39.6 36.0 29.3 28.6
tm-var 12.3 40.8 39.3 34.9 29.3 28.5
tm-skw 11.8 40.9 39.7 36.8 30.0 30.1
tm-cos 14.0 31.7 30.7 27.3 24.1 23.2
tm-euc 14.6 27.5 27.2 23.4 22.6 22.1
Table 3: Average overlap (in %) of similarity measure:
random selection (ran) vs. measures based on words (w)
and topic model (tm).
x=tm/w x-js x-var x-skw x-cos x-euc
tm/w-var 76/74 ? 60/63 55/48 49/47
tm/w-skw 69/72 60/63 ? 48/41 42/42
tm/w-cos 57/42 55/48 48/41 ? 62/71
tm/w-euc 47/41 49/47 42/42 62/71 ?
Table 4: Average overlap (in %) for different feature
representations x as tm/w, where tm=topic model and
w=words. Highest pair-wise overlap is underlined.
Automatic Measure vs. Human labels The next
question is how these automatic measures compare
to human-annotated data. We compare word-based
and topic model selection (by using jensen-shannon
and variational, respectively) to selection based on
human-given labels: genre and topic. For genre, we
randomly select larger amounts of training data for
a given test article from the same genre. For topic,
the approach is similar, but as an article might have
1571
several topic markers (keywords in the IN field), we
rank articles by proportion of overlapping keywords.
l
l
l
l
l
l
0 5000 10000 15000 20000
76
78
80
82
84
86
Average
number of sentences
Accu
racy
l randomwords?jstopic model?vargenretopic (IN fields)
Figure 2: Comparison of automatic measures (words us-
ing jensen-shannon and topic model using variational)
with human-annotated labels (genre/topic). Automatic
measures outperform human labels (p < 0.05).
Figure 2 shows that human-labels do actually not
perform better than the automatic measures. Both
are close to random selection. Moreover, the line
of selection by topic marker (IN fields) stops early
? we believe the reason for this is that the IN fields
are too fine-grained, which limits the number of ar-
ticles that are considered relevant for a given test
article. However, manually aggregating articles on
similar topics did not improve topic-based selection
either. We conclude that the automatic selection
techniques perform significantly better than human-
annotated data, at least within the WSJ domain con-
sidered here.
5.2 Domain Adaptation Results
Until now, we compared similarity measures by re-
stricting ourselves to articles from the WSJ. In this
section, we extend the experiments to the domain
adaptation scenario. We augment the pool of WSJ
articles with articles coming from two other corpora:
Genia and Brown. We want to gauge the effective-
ness of the domain similarity measures in the multi-
domain setting, where articles are selected from the
pool of data without knowing their identity (which
corpus the articles came from).
The test sets are the standard evaluation sets from
the three corpora: the standard WSJ (section 23)
and Brown test set from CoNLL 2008 (they contain
2,399 and 426 sentences, respectively) and the Ge-
nia test set (1,370 sentences). As a reference, we
give results of models trained on the respective cor-
pora (per-corpus models; i.e. if we consider corpora
boundaries and train a model on the respective do-
main ? this model is ?supervised? in the sense that it
knows from which corpus the test article came from)
as well as a baseline model trained on all data, i.e.
the union of all three corpora (wsj+genia+brown),
which is a standard baseline in domain adapta-
tion (Daume? III, 2007; McClosky et al, 2010).
WSJ Brown Genia
(38k) (28k) (19k)
random 86.58 73.81 83.77
per-corpus 87.50 81.55 86.63
union 87.05 79.12 81.57
topic model (var) 87.11? 81.76? 86.77?
words (js) 86.30 81.47? 86.44?
Table 5: Domain Adaptation Results on English (signifi-
cantly better: ? than random; ? than random and union).
The learning curves are shown in Figure 3, the
scores for a specific amount of data are given in
Table 5. The performance of the reference mod-
els (per-corpus and union in Table 5) are indicated
in Figure 3 with horizontal lines: the dashed line
represents the per-corpus performance (?supervised?
model); the solid line shows the performance of the
union baseline trained on all available data (77k sen-
tences). For the former, the vertical dashed lines in-
dicate the amount of data the model was trained on
(e.g. 23k sentences for Brown).
Simply taking all available data has a deteriorat-
ing effect: on all three test sets, the performance of
the union model is below the presumably best per-
formance of a model trained on the respective corpus
(per-corpus model).
The empirical results show that automatic data se-
lection by topic model outperforms random selec-
tion on all three test sets and the union baseline in
two out of three cases. More specifically, selection
by topic model outperforms random selection sig-
nificantly on all three test sets and all points in the
graph (p < 0.001). Selection by the word-based
measure (words-js) achieves a significant improve-
1572
ll
l
l
l l
0 10000 20000 30000 400008
0
82
84
86
88
wsj23all
number of sentences
Accu
racy
l
l
l
l
l l
l
0 10000 20000 30000 40000
70
75
80
brown
number of sentences
Accu
racy
l
l
l
l
l l
l
0 10000 20000 30000 40000
76
78
80
82
84
86
88
genia
number of sentences
Accu
racy
l randomwords?jstopic model?varper?corpus modelunion (wsj+genia+brown)
Figure 3: Domain Adaptation Results for English Parsing with Increasing Amounts of Training Data. The vertical line
represents the amount of data the per-corpus model is trained on.
ment over the random baseline on two out of the
three test sets ? it falls below the random baseline on
the WSJ test set. Thus, selection by topic model per-
forms best ? it achieves better performance than the
union baseline with comparatively little data (Genia:
4k; Brown: 19k ? in comparison: union has 77k).
Moreover, it comes very close to the supervised per-
corpus model performance13 with a similar amount
of data (cf. vertical dashed line). This is a very good
result, given that the technique disregards the origin
of the articles and just uses plain words as informa-
tion. It automatically finds data that is beneficial for
an unknown target domain.
So far we examined domain similarity measures
for parsing, and concluded that selection by topic
model performs best, closely followed by word-
based selection using the jensen-shannon diver-
gence. The question that remains is whether the
measure is more widely applicable: How does it per-
form on another language and task?
PoS tagging We perform similar Domain Adap-
tation experiments on WSJ, Genia and Brown for
PoS tagging. We use two taggers (HMM and Max-
Ent) and the same three test articles as before. The
results are shown in Figure 4 (it depicts the aver-
age over the three test sets, WSJ, Genia, Brown, for
space reasons). The left figure shows the perfor-
mance of the HMM tagger; on the right is the Max-
Ent tagger. The graphs show that automatic train-
ing data selection outperforms random data selec-
13On Genia and Brown (cf. Table 5) there is no significant
difference between topic model and per-corpus model.
tion, and again topic model selection performs best,
closely followed by words-js. This confirms previ-
ous findings and shows that the domain similarity
measures are effective also for this task.
l
l
l
l l
l l l
0 10000 20000 30000 400000.9
00.
920
.94
0.96
0.98
Average HMM tagger
number of sentences
Accura
cy
l randomwords?jstopic model?var
l
l
l
l l
l l
0 10000 20000 30000 400000.9
00.
920
.94
0.96
0.98
Average MXPOST tagger
number of sentences
Accura
cy
l randomwords?jstopic model?var
Figure 4: PoS tagging results, average over 3 test sets.
6 Experiments on Dutch
For Dutch, we evaluate the approach on a bigger and
more varied dataset. It contains in total over 50k ar-
ticles and 20 million words (cf. Table 1). In con-
trast to the English data, only a small portion of the
dataset is manually annotated: 281 articles.14
Since we want to evaluate the performance of
different similarity measures, we want to keep the
influence of noise as low as possible. Therefore,
we annotated the remaining articles with a parsing
system that is more accurate (Plank and van No-
ord, 2010), the Alpino parser (van Noord, 2006).
Note that using a more accurate parsing system to
train another parser has recently also been proposed
by Petrov et al (2010) as uptraining. Alpino is a
14http://www.let.rug.nl/vannoord/Lassy/
1573
parser tailored to Dutch, that has been developed
over the last ten years, and reaches an accuracy level
of 90% on general newspaper text. It uses a condi-
tional MaxEnt model as parse selection component.
Details of the parser are given in (van Noord, 2006).
l
l
l l
l
l
l
0 5000 10000 15000 20000 25000 30000
74
76
78
80
82
84
86
Average
number of sentences
Accu
racy
l randomtopic model?varwords?js
Figure 5: Result on Dutch; average over 30 articles.
Data and Results The Dutch dataset contains
articles from a variety of sources: Wikipedia15,
EMEA16 (documents from the European Medicines
Agency) and the Dutch parallel corpus17 (DPC), that
covers a variety of subdomains. The Dutch arti-
cles were parsed with Alpino and automatically con-
verted to CoNLL format with the treebank conver-
sion software from CoNLL 2006, where PoS tags
have been replaced with more fine-grained Alpino
tags as that had a positive effect on MST. The 281
annotated articles come from all three sources. As
with English, we consider as test set articles with
at least 50 sentences, from which 30 are randomly
sampled.
The results on Dutch are shown in Figure 5. Do-
main similarity measures clearly outperform random
data selection also in this setting with another lan-
guage and a considerably larger pool of data (20 mil-
lion words; 51k articles).
7 Discussion
In this paper we have shown the effectiveness of a
simple technique that considers only plain words as
domain selection measure for two tasks, dependency
15http://ilps.science.uva.nl/WikiXML/
16http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
17http://www.kuleuven-kortrijk.be/DPC
parsing and PoS tagging. Interestingly, human-
annotated labels did not perform better than the au-
tomatic measures. The best technique is based on
topic models, and compares document topic distri-
butions estimated by LDA (Blei et al, 2003) using
the variational metric (very similar results were ob-
tained using jensen-shannon). Topic model selec-
tion significantly outperforms random data selection
on both examined languages, English and Dutch,
and has a positive effect on PoS tagging. More-
over, it outperformed a standard Domain Adapta-
tion baseline (union) on two out of three test sets.
Topic model is closely followed by the word-based
measure using jensen-shannon divergence. By ex-
amining the overlap between word-based and topic
model-based techniques, we found that despite sim-
ilar performance their overlap is rather small. Given
these results and the fact that no optimization has
been done for the topic model itself, results are en-
couraging: there might be an even better measure
that exploits the information from both techniques.
So far, we tested a simple combination of the two by
selecting half of the articles by a measure based on
words and the other half by a measure based on topic
models (by testing different metrics). However, this
simple combination technique did not improve re-
sults yet ? topic model alone still performed best.
Overall, plain surface characteristics seem to
carry important information of what kind of data is
relevant for a given domain. Undoubtedly, parsing
accuracy will be influenced by more factors than lex-
ical information. Nevertheless, as we have seen, lex-
ical differences constitute an important factor.
Applying divergence measures over syntactic pat-
terns, adding additional articles to the pool of
data (by uptraining (Petrov et al, 2010), selftrain-
ing (McClosky et al, 2006) or active learning (Hwa,
2004)), gauging the effect of weighting instances
according to their similarity to the test data (Jiang
and Zhai, 2007; Plank and Sima?an, 2008), as well
as analyzing differences between gathered data are
venues for further research.
Acknowledgments
The authors would like to thank Bonnie Webber and
the three anonymous reviewers for their valuable
comments on earlier drafts of this paper.
1574
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Sydney, Australia.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL-X), pages 149?
164, New York City.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
Pittsburgh, PA.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Adapting a Probabilistic Disambiguation Model
of an HPSG Parser to a New Domain. In Robert Dale,
Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors,
Natural Language Processing IJCNLP 2005, volume
3651 of Lecture Notes in Computer Science, pages
199?210. Springer Berlin / Heidelberg.
Rebecca Hwa. 2004. Sample Selection for Statistical
Parsing. Compututational Linguistics, 30:253?276,
September.
Jing Jiang and ChengXiang Zhai. 2007. Instance
Weighting for Domain Adaptation in NLP. In Pro-
ceedings of the 45th Meeting of the Association for
Computational Linguistics, pages 264?271, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
Constituent-to-dependency Conversion for English. In
Proceedings of NODALIDA, Tartu, Estonia.
Lillian Lee. 2001. On the Effectiveness of the Skew Di-
vergence for Statistical Language Analysis. In In Ar-
tificial Intelligence and Statistics 2001, pages 65?72,
Key West, Florida.
J. Lin. 1991. Divergence measures based on the Shannon
entropy. Information Theory, IEEE Transactions on,
37(1):145 ?151, January.
Tom Lippincott, Diarmuid O? Se?aghdha, Lin Sun, and
Anna Korhonen. 2010. Exploring variation across
biomedical subdomains. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 689?697, Beijing, China, August.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge Mass.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Pro-
ceedings of Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 152?159, Brooklyn,
New York. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic Domain Adaptation for Parsing. In
Proceedings of Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 28?36, Los An-
geles, California, June. Association for Computational
Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective Dependency Parsing
using Spanning Tree Algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for Accurate Deter-
ministic Question Parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Barbara Plank and Khalil Sima?an. 2008. Subdomain
Sensitive Statistical Parsing using Raw Corpora. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation, Marrakech, Mo-
rocco, May.
Barbara Plank and Gertjan van Noord. 2010. Grammar-
Driven versus Data-Driven: Which Parsing System Is
More Affected by Domain Shifts? In Proceedings of
the 2010 Workshop on NLP and Linguistics: Finding
the Common Ground, pages 25?33, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic Prediction of Parser Accuracy. In EMNLP
?08: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 887?
1575
896, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
A. Re?nyi. 1961. On measures of information and en-
tropy. In Proceedings of the 4th Berkeley Sympo-
sium on Mathematics, Statistics and Probability, pages
547?561, Berkeley.
Satoshi Sekine. 1997. The Domain Dependence of Pars-
ing. In In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 96?102,
Washington D.C.
Mark Steyvers and Tom Griffiths, 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble Models for Dependency Parsing: Cheap and
Good? In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
649?652, Los Angeles, California, June. Association
for Computational Linguistics.
Vincent Van Asch and Walter Daelemans. 2010. Us-
ing Domain Similarity for Performance Estimation. In
Proceedings of the 2010 Workshop on Domain Adap-
tation for Natural Language Processing, pages 31?36,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Gertjan van Noord. 2006. At Last Parsing Is Now Oper-
ational. In TALN 2006 Verbum Ex Machina, Actes De
La 13e Conference sur Le Traitement Automatique des
Langues naturelles, pages 20?42, Leuven.
Bonnie Webber. 2009. Genre distinctions for Discourse
in the Penn TreeBank. In Proceedings of the 47th
Meeting of the Association for Computational Linguis-
tics, pages 674?682, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Zhili Wu, Katja Markert, and Serge Sharoff. 2010. Fine-
Grained Genre Classification Using Structural Learn-
ing Algorithms. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 749?759, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Morristown, NJ, USA. Association for
Computational Linguistics.
1576
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 194?199,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reversible Stochastic Attribute-Value Grammars
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Barbara Plank
University of Groningen
b.plank@rug.nl
Gertjan van Noord
University of Groningen
g.j.m.van.noord@rug.nl
Abstract
An attractive property of attribute-value gram-
mars is their reversibility. Attribute-value
grammars are usually coupled with sepa-
rate statistical components for parse selection
and fluency ranking. We propose reversible
stochastic attribute-value grammars, in which
a single statistical model is employed both for
parse selection and fluency ranking.
1 Introduction
Reversible grammars were introduced as early as
1975 by Martin Kay (1975). In the eighties, the
popularity of attribute-value grammars (AVG) was
in part motivated by their inherent reversible na-
ture. Later, AVG were enriched with a statistical
component (Abney, 1997): stochastic AVG (SAVG).
Training a SAVG is feasible if a stochastic model
is assumed which is conditioned on the input sen-
tences (Johnson et al, 1999). Various parsers based
on this approach now exist for various languages
(Toutanova et al, 2002; Riezler et al, 2002; van
Noord and Malouf, 2005; Miyao and Tsujii, 2005;
Clark and Curran, 2004; Forst, 2007). SAVG can be
applied for generation to select the most fluent real-
ization from the set of possible realizations (Velldal
et al, 2004). In this case, the stochastic model is
conditioned on the input logical forms. Such gener-
ators exist for various languages as well (Velldal and
Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et
al., 2007; de Kok and van Noord, 2010).
If an AVG is applied both to parsing and gen-
eration, two distinct stochastic components are re-
quired, one for parsing, and one for generation. To
some extent this is reasonable, because some fea-
tures are only relevant in a certain direction. For
instance, features that represent aspects of the sur-
face word order are important for generation, but ir-
relevant for parsing. Similarly, features which de-
scribe aspects of the logical form are important for
parsing, but irrelevant for generation. Yet, there are
also many features that are relevant in both direc-
tions. For instance, for Dutch, a very effective fea-
ture signals a direct object NP in fronted position in
main clauses. If a main clause is parsed which starts
with a NP, the disambiguation component will fa-
vor a subject reading of that NP. In generation, the
fluency component will favor subject fronting over
object fronting. Clearly, such shared preferences are
not accidental.
In this paper we propose reversible SAVG in
which a single stochastic component is applied both
in parsing and generation. We provide experimen-
tal evidence that such reversible SAVG achieve sim-
ilar performance as their directional counterparts.
A single, reversible model is to be preferred over
two distinct models because it explains why pref-
erences in a disambiguation component and a flu-
ency component, such as the preference for subject
fronting over object fronting, are shared. A single,
reversible model is furthermore of practical inter-
est for its simplicity, compactness, and maintainabil-
ity. As an important additional advantage, reversible
models are applicable for tasks which combine as-
pects of parsing and generation, such as word-graph
parsing and paraphrasing. In situations where only a
small amount of training data is available for parsing
or generation, cross-pollination improves the perfor-
194
mance of a model. If preferences are shared between
parsing and generation, it follows that a generator
could benefit from parsing data and vice versa. We
present experimental results indicating that in such a
bootstrap scenario a reversible model achieves better
performance.
2 Reversible SAVG
As Abney (1997) shows, we cannot use relatively
simple techniques such as relative frequencies to
obtain a model for estimating derivation probabili-
ties in attribute-value grammars. As an alternative,
he proposes a maximum entropy model, where the
probability of a derivation d is defined as:
p(d) =
1
Z
exp
?
i
?ifi(d) (1)
fi(d) is the frequency of feature fi in derivation
d. A weight ?i is associated with each feature fi.
In (1), Z is a normalizer which is defined as fol-
lows, where ? is the set of derivations defined by
the grammar:
Z =
?
d???
exp
?
i
?ifi(d
?) (2)
Training this model requires access to all derivations
? allowed by the grammar, which makes it hard to
implement the model in practice.
Johnson et al (1999) alleviate this problem by
proposing a model which conditions on the input
sentence s: p(d|s). Since the number of derivations
for a given sentence s is usually finite, the calcula-
tion of the normalizer is much more practical. Con-
versely, in generation the model is conditioned on
the input logical form l, p(d|l) (Velldal et al, 2004).
In such directional stochastic attribute-value gram-
mars, the probability of a derivation d given an input
x (a sentence or a logical form) is defined as:
p(d|x) =
1
Z(x)
exp
?
i
?ifi(x, d) (3)
with Z(x) as (?(x) are all derivations for input x):
Z(x) =
?
d???(x)
exp
?
i
?ifi(x, d
?) (4)
Consequently, the constraint put on feature values
during training only refers to derivations with the
same input. If X is the set of inputs (for parsing,
all sentences in the treebank; for generation, all log-
ical forms), then we have:
Ep(fi)? Ep?(fi) = 0 ? (5)
?
x?X
?
d??(x)
p?(x)p(d|x)fi(x, d)? p?(x, d)fi(x, d) = 0
Here we assume a uniform distribution for p?(x).
Let j(d) be a function which returns 0 if the deriva-
tion d is inconsistent with the treebank, and 1 in case
the derivation is correct. p?(x, d) is now defined in
such a way that it is 0 for incorrect derivations, and
uniform for correct derivations for a given input:
p?(x, d) = p?(x)
j(d)
?d???(x)j(d?)
(6)
Directional SAVG make parsing and generation
practically feasible, but require separate models for
parse disambiguation and fluency ranking.
Since parsing and generation both create deriva-
tions that are in agreement with the constraints im-
plied by the input, a single model can accompany
the attribute-value grammar. Such a model estimates
the probability of a derivation d given a set of con-
straints c, p(d|c). We use conditional maximum en-
tropy models to estimate p(d|c):
p(d|c) =
1
Z(c)
exp
?
i
?ifi(c, d) (7)
Z(c) =
?
d???(c)
exp
?
i
?ifi(c, d
?) (8)
We derive a reversible model by training on data
for parse disambiguation and fluency ranking simul-
taneously. In contrast to directional models, we im-
pose the two constraints per feature given in figure 1:
one on the feature value with respect to the sentences
S in the parse disambiguation treebank and the other
on the feature value with respect to logical forms L
in the fluency ranking treebank. As a result of the
constraints on training defined in figure 1, the fea-
ture weights in the reversible model distinguish, at
the same time, good parses from bad parses as well
as good realizations from bad realizations.
3 Experimental setup and evaluation
To evaluate reversible SAVG, we conduct experi-
ments in the context of the Alpino system for Dutch.
195
?s?S
?
d??(s)
p?(s)p(d|c = s)fi(s, d)? p?(c = s, d)fi(s, d) = 0
?
l?L
?
d??(l)
p?(l)p(d|c = l)fi(l, d)? p?(c = l, d)fi(l, d) = 0
Figure 1: Constraints imposed on feature values for training reversible models p(d|c).
Alpino provides a wide-coverage grammar, lexicon
and parser (van Noord, 2006). Recently, a sentence
realizer has been added that uses the same grammar
and lexicon (de Kok and van Noord, 2010).
In the experiments, the cdbl part of the Alpino
Treebank (van der Beek et al, 2002) is used as train-
ing data (7,154 sentences). The WR-P-P-H part
(2,267 sentences) of the LASSY corpus (van Noord
et al, 2010), which consists of text from the Trouw
2001 newspaper, is used for testing.
3.1 Features
The features that we use in the experiment are the
same features which are available in the Alpino
parser and generator. In the following section, these
features are described in some detail.
Word adjacency. Two word adjacency features
are used as auxiliary distributions (Johnson and Rie-
zler, 2000). The first feature is the probability of the
sentence according to a word trigram model. The
second feature is the probability of the sentence ac-
cording to a tag trigram model that uses the part-
of-speech tags assigned by the Alpino system. In
both models, linear interpolation smoothing for un-
known trigrams, and Laplacian smoothing for un-
known words and tags is applied. The trigram mod-
els have been trained on the Twente Nieuws Corpus
corpus (approximately 110 million words), exclud-
ing the Trouw 2001 corpus. In conventional pars-
ing tasks, the value of the word trigram model is the
same for all derivations of a given input sentence.
Lexical frames. Lexical analysis is applied dur-
ing parsing to find all possible subcategorization
frames for the tokens in the input sentence. Since
some frames occur more frequently in good parses
than others, we use feature templates that record the
frames that were used in a parse. An example of
such a feature is: ??to play? serves as an intransi-
tive verb?. We also use an auxiliary distribution of
word and frame combinations that was trained on
a large corpus of automatically annotated sentences
(436 million words). The values of lexical frame
features are constant for all derivations in sentence
realization, unless the frame is not specified in the
logical form.
Dependency relations. There are also feature
templates which describe aspects of the dependency
structure. For each dependency, three types of de-
pendency features are extracted. Examples of such
features are ?a pronoun is used as the subject of
a verb?, ?the pronoun ?she? is used as the sub-
ject of a verb?, ?the noun ?beer? is used as the
object of the verb ?drink??. In addition, features
are used which implement auxiliary distributions
for selectional preferences, as described in Van No-
ord (2007). In conventional realization tasks, the
values of these features are constant for all deriva-
tions for a given input representation.
Syntactic features. Syntactic features include fea-
tures which record the application of each grammar
rule, as well as features which record the application
of a rule in the context of another rule. An exam-
ple of the latter is ?rule 167 is used to construct the
second daughter of a derivation constructed by rule
233?. In addition, there are features describing more
complex syntactic patterns such as: fronting of sub-
jects and other noun phrases, orderings in the middle
field, long-distance dependencies, and parallelism of
conjuncts in coordination.
3.2 Parse disambiguation
Earlier we assumed that a treebank is a set of cor-
rect derivations. In practice, however, a treebank
only contains an abstraction of such derivations (in
196
our case sentences with corresponding dependency
structures), thus abstracting away from syntactic de-
tails needed in a parse disambiguation model. As in
Osborne (2000), the derivations for the parse disam-
biguation model are created by parsing the training
corpus. In the current setting, up to at most 3000
derivations are created for every sentence. These
derivations are then compared to the gold standard
dependency structure to judge the quality of the
parses. For a given sentence, the parses with the
highest concept accuracy (van Noord, 2006) are con-
sidered correct, the rest is treated as incorrect.
3.3 Fluency ranking
For fluency ranking we also need access to full
derivations. To ensure that the system is able to
generate from the dependency structures in the tree-
bank, we parse the corresponding sentence, and se-
lect the parse with the dependency structure that
corresponds most closely to the dependency struc-
ture in the treebank. The resulting dependency
structures are fed into the Alpino chart generator
to construct derivations for each dependency struc-
ture. The derivations for which the corresponding
sentences are closest to the original sentence in the
treebank are marked correct. Due to a limit on gen-
eration time, some longer sentences and correspond-
ing dependency structures were excluded from the
data. As a result, the average sentence length was
15.7 tokens, with a maximum of 26 tokens. To com-
pare a realization to the correct sentence, we use the
General Text Matcher (GTM) method (Melamed et
al., 2003; Cahill, 2009).
3.4 Training the models
Models are trained by taking an informative sam-
ple of ?(c) for each c in the training data (Osborne,
2000). This sample consists of at most 100 ran-
domly selected derivations. Frequency-based fea-
ture selection is applied (Ratnaparkhi, 1999). A fea-
ture f partitions ?(c), if there are derivations d and
d? in ?(c) such that f(c, d) 6= f(c, d?). A feature is
used if it partitions the informative sample of ?(c)
for at least two c. Table 1 lists the resulting charac-
teristics of the training data for each model.
We estimate the parameters of the conditional
Features Inputs Derivations
Generation 1727 3688 141808
Parse 25299 7133 376420
Reversible 25578 10811 518228
Table 1: Size of the training data for each model
maximum entropy models using TinyEst,1 with a
Gaussian (`2) prior distribution (? = 0, ?2 = 1000)
to reduce overfitting (Chen and Rosenfeld, 1999).
4 Results
4.1 Parse disambiguation
Table 2 shows the results for parse disambiguation.
The table also provides lower and upper bounds: the
baseline model selects an arbitrary parse per sen-
tence; the oracle chooses the best available parse.
Figure 2 shows the learning curves for the direc-
tional parsing model and the reversible model.
Model CA (%) f-score (%)
Baseline 75.88 76.28
Oracle 94.86 95.09
Parse model 90.93 91.28
Reversible 90.87 91.21
Table 2: Concept Accuracy scores and f-scores in terms
of named dependency relations for the parsing-specific
model versus the reversible model.
The results show that the general, reversible,
model comes very close to the accuracy obtained
by the dedicated, parsing specific, model. Indeed,
the tiny difference is not statistically significant. We
compute statistical significance using the Approxi-
mate Randomization Test (Noreen, 1989).
4.2 Fluency ranking
Table 3 compares the reversible model with a di-
rectional fluency ranking model. Figure 3 shows
the learning curves for the directional generation
model and the reversible model. The reversible
model achieves similar performance as the direc-
tional model (the difference is not significant).
To show that a reversible model can actually profit
from mutually shared features, we report on an ex-
periment where only a small amount of generation
1http://github.com/danieldk/tinyest
197
0.0 0.1 0.2 0.3 0.4 0.57
67
88
08
28
48
68
89
0
Proportion parse training data
CA (%
)
parse modelreversible model
Figure 2: Learning curve for directional and reversible
models for parsing. The reversible model uses all training
data for generation.
Model GTM
Random 55.72
Oracle 86.63
Fluency 71.82
Reversible 71.69
Table 3: General Text Matcher scores for fluency ranking
using various models.
training data is available. In this experiment, we
manually annotated 234 dependency structures from
the cdbl part of the Alpino Treebank, by adding cor-
rect realizations. In many instances, there is more
than one fluent realization. We then used this data to
train a directional fluency ranking model and a re-
versible model. The results for this experiment are
shown in Table 4. Since the reversible model outper-
forms the directional model we conclude that indeed
fluency ranking benefits from parse disambiguation
data.
Model GTM
Fluency 70.54
Reversible 71.20
Table 4: Fluency ranking using a small amount of anno-
tated fluency ranking training data (difference is signifi-
cant at p < 0.05).
0.0 0.1 0.2 0.3 0.4 0.5
60
65
70
Proportion generation training data
GTM
 scor
e
generation modelreversible model
Figure 3: Learning curves for directional and reversible
models for generation. The reversible models uses all
training data for parsing.
5 Conclusion
We proposed reversible SAVG as an alternative to
directional SAVG, based on the observation that
syntactic preferences are shared between parse dis-
ambiguation and fluency ranking. This framework
is not purely of theoretical interest, since the exper-
iments show that reversible models achieve accura-
cies that are similar to those of directional models.
Moreover, we showed that a fluency ranking model
trained on a small data set can be improved by com-
plementing it with parse disambiguation data.
The integration of knowledge from parse disam-
biguation and fluency ranking could be beneficial for
tasks which combine aspects of parsing and genera-
tion, such as word-graph parsing or paraphrasing.
198
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In ENLG ?07: Proceedings of the Eleventh
European Workshop on Natural Language Genera-
tion, pages 17?24, Morristown, NJ, USA.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference - Short Pa-
pers, pages 97?100.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University, Pittsburg.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of the 42nd Annual Meeting of the ACL, pages
103?110, Morristown, NJ, USA.
Danie?l de Kok and Gertjan van Noord. 2010. A sentence
generator for Dutch. In Proceedings of the 20th Com-
putational Linguistics in the Netherlands conference
(CLIN).
Martin Forst. 2007. Filling statistics with linguistics:
property design for the disambiguation of german lfg
parses. In DeepLP ?07: Proceedings of the Workshop
on Deep Linguistic Processing, pages 17?24, Morris-
town, NJ, USA.
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proceedings of the 1st Meeting of the
NAACL, pages 154?161, Seattle, Washington.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the ACL.
Martin Kay. 1975. Syntactic processing and functional
sentence perspective. In TINLAP ?75: Proceedings of
the 1975 workshop on Theoretical issues in natural
language processing, pages 12?15, Morristown, NJ,
USA.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In HLT-
NAACL.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 83?90, Morristown, NJ, USA.
Hiroko Nakanishi and Yusuke Miyao. 2005. Probabilis-
tic models for disambiguation of an hpsg-based chart
generator. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT), pages 93?
102.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Miles Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics (COLING), pages 586?592.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151?175.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the wall street journal using a
lexical-functional grammar and discriminative estima-
tion techniques. In Proceedings of the 40th Annual
Meeting of the ACL, pages 271?278, Morristown, NJ,
USA.
Kristina Toutanova, Christopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich hpsg grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT), pages 253?263, Sozopol.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
Gertjan van Noord and Robert Malouf. 2005. Wide
coverage parsing with stochastic attribute value gram-
mars. Draft available from the authors. A preliminary
version of this paper was published in the Proceedings
of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
Gertjan van Noord, Ineke Schuurman, and Gosse Bouma.
2010. Lassy syntactische annotatie, revision 19053.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement Au-
tomatique des Langues naturelles, pages 20?42, Leu-
ven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Parsing
Technology (IWPT), ACL 2007 Workshop, pages 1?
10, Prague.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 517?525, Sydney,
Australia, July. ACL.
Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004.
Paraphrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Treebanks
and Linguistic Theories (TLT), pages 149?160.
199
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498?1507,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Embedding Semantic Similarity in Tree Kernels for Domain Adaptation
of Relation Extraction
Barbara Plank?
Center for Language Technology
University of Copenhagen, Denmark
bplank@gmail.com
Alessandro Moschitti
QCRI - Qatar Foundation &
DISI - University of Trento, Italy
amoschitti@qf.org.qa
Abstract
Relation Extraction (RE) is the task of
extracting semantic relationships between
entities in text. Recent studies on rela-
tion extraction are mostly supervised. The
clear drawback of supervised methods is
the need of training data: labeled data is
expensive to obtain, and there is often a
mismatch between the training data and
the data the system will be applied to.
This is the problem of domain adapta-
tion. In this paper, we propose to combine
(i) term generalization approaches such as
word clustering and latent semantic anal-
ysis (LSA) and (ii) structured kernels to
improve the adaptability of relation ex-
tractors to new text genres/domains. The
empirical evaluation on ACE 2005 do-
mains shows that a suitable combination
of syntax and lexical generalization is very
promising for domain adaptation.
1 Introduction
Relation extraction is the task of extracting se-
mantic relationships between entities in text, e.g.
to detect an employment relationship between the
person Larry Page and the company Google in
the following text snippet: Google CEO Larry
Page holds a press announcement at its headquar-
ters in New York on May 21, 2012. Recent stud-
ies on relation extraction have shown that super-
vised approaches based on either feature or ker-
nel methods achieve state-of-the-art accuracy (Ze-
lenko et al, 2002; Culotta and Sorensen, 2004;
? The first author was affiliated with the Department of
Computer Science and Information Engineering of the Uni-
versity of Trento (Povo, Italy) during the design of the mod-
els, experiments and writing of the paper.
Zhang et al, 2005; Zhou et al, 2005; Zhang et
al., 2006; Bunescu, 2007; Nguyen et al, 2009;
Chan and Roth, 2010; Sun et al, 2011). How-
ever, the clear drawback of supervised methods is
the need of training data, which can slow down
the delivery of commercial applications in new
domains: labeled data is expensive to obtain, and
there is often a mismatch between the training data
and the data the system will be applied to. Ap-
proaches that can cope with domain changes are
essential. This is the problem of domain adapta-
tion (DA) or transfer learning (TL). Technically,
domain adaptation addresses the problem of learn-
ing when the assumption of independent and iden-
tically distributed (i.i.d.) samples is violated. Do-
main adaptation has been studied extensively dur-
ing the last couple of years for various NLP tasks,
e.g. two shared tasks have been organized on do-
main adaptation for dependency parsing (Nivre et
al., 2007; Petrov and McDonald, 2012). Results
were mixed, thus it is still a very active research
area.
However, to the best of our knowledge, there
is almost no work on adapting relation extraction
(RE) systems to new domains.1 There are some
prior studies on the related tasks of multi-task
transfer learning (Xu et al, 2008; Jiang, 2009)
and distant supervision (Mintz et al, 2009), which
are clearly related but different: the former is the
problem of how to transfer knowledge from old
to new relation types, while distant supervision
tries to learn new relations from unlabeled text
by exploiting weak-supervision in the form of a
knowledge resource (e.g. Freebase). We assume
the same relation types but a shift in the underlying
1Besides an unpublished manuscript of a student project,
but it is not clear what data was used. http://tinyurl.com/
bn2hdwk
1498
data distribution. Weak supervision is a promis-
ing approach to improve a relation extraction sys-
tem, especially to increase its coverage in terms of
types of relations covered. In this paper we ex-
amine the related issue of changes in the underly-
ing data distribution, while keeping the relations
fixed. Even a weakly supervised system is ex-
pected to perform well when applied to any kind of
text (other domain/genre), thus ideally, we believe
that combining domain adaptation with weak su-
pervision is the way to go in the future. This study
is a first step towards this.
We focus on unsupervised domain adaptation,
i.e. no labeled target data. Moreover, we consider
a particular domain adaptation setting: single-
system DA, i.e. learning a single system able to
cope with different but related domains. Most
studies on DA so far have focused on building
a specialized system for every specific target do-
main, e.g. Blitzer et al (2006). In contrast, the
goal here is to build a single system that can ro-
bustly handle several domains, which is in line
with the setup of the recent shared task on pars-
ing the web (Petrov and McDonald, 2012). Par-
ticipants were asked to build a single system that
can robustly parse all domains (reviews, weblogs,
answers, emails, newsgroups), rather than to build
several domain-specific systems. We consider this
as a shift in what was considered domain adapta-
tion in the past (adapt from source to a specific tar-
get) and what can be considered a somewhat dif-
ferent recent view of DA, that became widespread
since 2011/2012. The latter assumes that the tar-
get domain(s) is/are not really known in advance.
In this setup, the domain adaptation problem boils
down to finding a more robust system (S?gaard
and Johannsen, 2012), i.e. one wants to build a
system that can robustly handle any kind of data.
We propose to combine (i) term generalization
approaches and (ii) structured kernels to improve
the performance of a relation extractor on new
domains. Previous studies have shown that lexi-
cal and syntactic features are both very important
(Zhang et al, 2006). We combine structural fea-
tures with lexical information generalized by clus-
ters or similarity. Given the complexity of feature
engineering, we exploit kernel methods (Shawe-
Taylor and Cristianini, 2004). We encode word
clusters or similarity in tree kernels, which, in
turn, produce spaces of tree fragments. For ex-
ample, ?president?, ?vice-president? and ?Texas?,
?US?, are terms indicating an employment rela-
tion between a person and a location. Rather than
only matching the surface string of words, lexi-
cal similarity enables soft matches between similar
words in convolution tree kernels. In the empir-
ical evaluation on Automatic Content Extraction
(ACE) data, we evaluate the impact of convolu-
tion tree kernels embedding lexical semantic sim-
ilarities. The latter is derived in two ways with:
(a) Brown word clustering (Brown et al, 1992);
and (b) Latent Semantic Analysis (LSA). We first
show that our system aligns well with the state of
the art on the ACE 2004 benchmark. Then, we
test our RE system on the ACE 2005 data, which
exploits kernels, structures and similarities for do-
main adaptation. The results show that combining
the huge space of tree fragments generalized at the
lexical level provides an effective model for adapt-
ing RE systems to new domains.
2 Semantic Syntactic Tree Kernels
In kernel-based methods, both learning and classi-
fication only depend on the inner product between
instances. Kernel functions can be efficiently and
implicitly computed by exploiting the dual formu-
lation: ?i=1..l yi?i?(oi)?(o) + b = 0, where oi
and o are two objects, ? is a mapping from an ob-
ject to a feature vector ~xi and ?(oi)?(o) =K(oi, o)
is a kernel function implicitly defining such a map-
ping. In case of structural kernels, K determines
the shape of the substructures describing the ob-
jects. Commonly used kernels in NLP are string
kernels (Lodhi et al, 2002) and tree kernels (Mos-
chitti, 2006; Moschitti, 2008).
NP
PP
NP
E2
NNP
Texas
IN
from
NP
E1
NNP
governor
?
NP
PPNP
NP
PPNP
E1
NP
PPNP
E1
NNP
governor
E1
NNP
governor
. . .
NNP
Texas
Figure 1: Syntactic tree kernel (STK).
Syntactic tree kernels (Collins and Duffy, 2001)
compute the similarity between two trees T1
and T2 by counting common sub-trees (cf. Fig-
ure 1), without enumerating the whole fragment
space. However, if two trees have similar sub-
structures that employ different though related ter-
minal nodes, they will not be matched. This is
1499
clearly a limitation. For instance, the fragments
corresponding to governor from Texas and
head of Maryland are intuitively semanti-
cally related and should obtain a higher match
when compared to mother of them.
Semantic syntactic tree kernels (Bloehdorn
and Moschitti, 2007a; Bloehdorn and Moschitti,
2007b; Croce et al, 2011) provide one way to ad-
dress this problem by introducing similarity ? that
allows soft matches between words and, conse-
quently, between fragments containing them. Let
N1 and N2 be the set of nodes in T1 and T2, re-
spectively. Moreover, let Ii(n) be an indicator
variable that is 1 if subtree i is rooted at n and
0 otherwise. The syntactic semantic convolution
kernel TK? (Bloehdorn and Moschitti, 2007b)
over T1 and T2 is computed as TK?(T1, T2) =?
n1?N1,n2?N2 ??(n1, n2) where ??(n1, n2) =?
n1?N1
?
n2?N2
?
i Ii(n1)Ii(n2) is computed ef-
ficiently using the following recursive defini-
tion: i) If the nodes n1 and n2 are ei-
ther different or have different number of chil-
dren then ??(n1, n2) = 0; else ii) If
n1 and n2 are pre-terminals then ??(n1, n2)
= ??nc(n1)j=1 ??(ch(n1, j), ch(n2, j)), where ?
measures the similarity between the correspond-
ing children of n1 and n2; iii) If n1 and n2 have
identical children: ??(n1, n2) = ??nc(n1)j=1 (1 +
??(ch(n1, j)), ch(n2, j)); else ??(n1, n2) = 0.
TK? combines generalized lexical with structural
information: it allows matching tree fragments
that have the same syntactic structure but differ in
their terminals. After introducing related work, we
will discuss computational structures for RE and
their extension with semantic similarity.
3 Related Work
Semantic syntactic tree kernels have been previ-
ously used for question classification (Bloehdorn
and Moschitti, 2007a; Bloehdorn and Moschitti,
2007b; Croce et al, 2011). These kernels have
not yet been studied for either domain adaptation
or RE. Brown clusters were studied previously for
feature-based approaches to RE (Sun et al, 2011;
Chan and Roth, 2010), but they were not yet eval-
uated in kernels. Thus, we present a novel applica-
tion of semantic syntactic tree kernels and Brown
clusters for domain adaptation of tree-kernel based
relation extraction.
Regarding domain adaptation, several meth-
ods have been proposed, ranging from instance
weighting (Jiang and Zhai, 2007) to approaches
that change the feature representation (Daume? III,
2007) or try to exploit pivot features to find
a generalized shared representation between do-
mains (Blitzer et al, 2006). The easy-adapt ap-
proach presented in Daume? III (2007) assumes the
supervised adaptation setting and is thus not ap-
plicable here. Structural correspondence learn-
ing (Blitzer et al, 2006) exploits unlabeled data
from both source and target domain to find cor-
respondences among features from different do-
mains. These correspondences are then integrated
as new features in the labeled data of the source
domain. The key to SCL is to exploit pivot fea-
tures to automatically identify feature correspon-
dences, and as such is applicable to feature-based
approaches but not in our case since we do not as-
sume availability of target domain data. Instead,
we apply a similar idea where we exploit an en-
tire unlabeled corpus as pivot, and compare our
approach to instance weighting (Jiang and Zhai,
2007).
Instance weighting is a method for domain
adaptation in which instance-dependent weights
are assigned to the loss function that is mini-
mized during the training process. Let l(x, y, ?)
be some loss function. Then, as shown in Jiang
and Zhai (2007), the loss function can be weighted
by ?il(x, y, ?), such that ?i = Pt(xi)Ps(xi) , where Psand Pt are the source and target distributions, re-
spectively. Huang et al (2007) present an appli-
cation of instance weighting to support vector ma-
chines by minimizing the following re-weighted
function: min?,? 12 ||?||2 + C
?m
i=1 ?i?i. Finding
a good weight function is non-trivial (Jiang and
Zhai, 2007) and several approximations have been
evaluated in the past, e.g. S?gaard and Haulrich
(2011) use a bigram-based text classifier to dis-
criminate between domains. We will use a binary
classifier trained on RE instance representations.
4 Computational Structures for RE
A common way to represent a constituency-based
relation instance is the PET (path-enclosed-tree),
the smallest subtree including the two target enti-
ties (Zhang et al, 2006). This is basically the for-
mer structure PAF2 (predicate argument feature)
defined in Moschitti (2004) for the extraction of
predicate argument relations. The syntactic rep-
2It is the smallest subtree enclosing the predicate and one
of its argument node.
1500
resentation used by Zhang et al (2006) (we will
refer to it as PET Zhang) is the PET with enriched
entity information: e.g. E1-NAM-PER, including
entity type (PER, GPE, LOC, ORG) and mention
type (NAM, NOM, PRO, PRE: name, nominal,
pronominal or premodifier). An alternative ker-
nel that does not use syntactic information is the
Bag-of-Words (BOW) kernel, where a single root
node is added above the terminals. Note that in
this BOW kernel we actually mark target entities
with E1/E2. Therefore, our BOW kernel can be
considered an enriched BOW model. If we do not
mark target entities, performance drops consider-
ably, as discussed later.
As shown by Zhang et al (2006), includ-
ing gold-standard information on entity and men-
tion type substantially improves relation extrac-
tion performance. We will use this gold infor-
mation also in Section 6.1 to show that our sys-
tem aligns well to the state of the art on the ACE
2004 benchmark. However, in a realistic setting
this information is not available or noisy. In fact,
as we discuss later, excluding gold entity informa-
tion decreases system performance considerably.
In the case of porting a system to new domains
entity information will be unreliable or missing.
Therefore, in our domain adaptation experiments
on the ACE 2005 data (Section 6.3) we will not
rely on this gold information but rather train a sys-
tem using PET (target mentions only marked with
E1/E2 and no gold entity label).3
4.1 Syntactic Semantic Structures
Combining syntax with semantics has a clear ad-
vantage: it generalizes lexical information encap-
sulated in syntactic parse trees, while at the same
time syntax guides semantics in order to obtain an
effective semantic similarity. In fact, lexical infor-
mation is highly affected by data-sparseness, thus
tree kernels combined with semantic information
created from additional resources should provide
a way to obtain a more robust system.
We exploit this idea here for domain adaptation
(DA): if words are generalized by semantic simi-
larity LS, then in a hypothetical world changing
LS such that it reflects the target domain would
3In a setup where gold label info is included, the impact
of similarity-based methods is limited ? gold information
seems to predominate. We argue that whenever gold data is
not available, distributional semantics paired with kernels can
be useful to improve generalization and complement missing
gold info.
allow the system to perform better in the target
domain. The question remains how to establish a
link between the semantic similarity in the source
and target domain. We propose to use an entire
unlabeled corpus as pivot: this corpus must be
general enough to encapsulate the source and tar-
get domains of interest. The idea is to (i) learn
semantic similarity between words on the pivot
corpus and (ii) use tree kernels embedding such
a similarity to learn a RE system on the source,
which allows to generalize to the new target do-
main. This reasoning is related to Structural Cor-
respondence Learning (SCL) (Blitzer et al, 2006).
In SCL, a representation shared across domains is
learned by exploiting pivot features, where a set
of pivot features has to be selected (usually a few
thousands). In our case pivots are words that co-
occur with the target words in a large unlabeled
corpus and are thus implicitly represented in the
similarity matrix. Thus, in contrast to SCL, we do
not need to select a set of pivot features but rather
rely on the distributional hypothesis to infer a se-
mantic similarity from a large unlabeled corpus.
Then, this similarity is incorporated into the tree
kernel that provides the necessary restriction for
an effective semantic similarity calculation. One
peculiarity of our work is that we exploit a large
amount of general data, i.e. data gathered from the
web, which is a different but also more challeng-
ing scenario than the general unsupervised DA set-
ting where domain specific data is available. We
study two ways for term generalization in tree ker-
nels: Brown words clusters and Latent Semantic
Analysis (LSA), both briefly described next.
a) replace pos
NP
PP
NP
E2
1111100110
Seoul
10001110
from
NP
E1
1101100011
officials
b) replace word
..
NP
E2
NNP
1111100110
c) above pos
..
NP
E2
1111100110
NNP
Seoul
Figure 2: Integrating Brown cluster information
The Brown algorithm (Brown et al, 1992) is
a hierarchical agglomerative hard-clustering algo-
rithm. The path from the root of the tree down to
a leaf node is represented compactly as a bitstring.
By cutting the hierarchy at different levels one can
obtain different granularities of word clusters. We
1501
evaluate different ways to integrate cluster infor-
mation into tree kernels, some of which are illus-
trated in Figure 2.
For LSA, we compute term similarity functions
following the distributional hypothesis (Harris,
1964), i.e. the meaning of a word can be described
by the set of textual contexts in which it appears.
The original word-by-word context matrix M is
decomposed through Singular Value Decomposi-
tion (SVD) (Golub and Kahan, 1965), where M
is approximated by UlSlV Tl . This approxima-
tion supplies a way to project a generic term wi
into the l-dimensional space using W = UlS1/2l ,
where each row corresponds to the vectors ~wi.
Given two words w1 and w2, the term similarity
function ? is estimated as the cosine similarity be-
tween the corresponding projections ~w1, ~w2 and
used in the kernel as described in Section 2.
5 Experimental Setup
We treat relation extraction as a multi-class classi-
fication problem and use SVM-light-TK4 to train
the binary classifiers. The output of the classifiers
is combined using the one-vs-all approach. We
modified the SVM-light-TK package to include
the semantic tree kernels and instance weight-
ing. The entire software package is publicly avail-
able.5 For the SVMs, we use the same parameters
as Zhang et al (2006): ? = 0.4, c = 2.4 using the
Collins Kernel (Collins and Duffy, 2001). The pre-
cision/recall trade-off parameter for the none class
was found on held-out data: j = 0.2. Evalua-
tion metrics are standard micro average Precision,
Recall and balanced Fscore (F1). To compute sta-
tistical significance, we use the approximate ran-
domization test (Noreen, 1989).6 In all our exper-
iments, we model argument order of the relations
explicitly. Thus, for instance for the 7 coarse ACE
2004 relations, we build 14 coarse-grained classi-
fiers (two for each coarse ACE 2004 relation type
except for PER-SOC, which is symmetric, and one
classifier for the none relation).
Data We use two datasets. To compare our
model against the state of the art we use the ACE
2004 data. It contains 348 documents and 4,374
positive relation instances. To generate the train-
ing data, we follow prior studies and extract an
instance for every pair of mentions in the same
4http://disi.unitn.it/moschitti/Tree-Kernel.htm
5http://disi.unitn.it/ikernels/RelationExtraction
6http://www.nlpado.de/?sebastian/software/sigf.shtml
sentence, which are separated by no more than
three other mentions (Zhang et al, 2006; Sun et
al., 2011). After data preprocessing, we obtained
4,327 positive and 39,120 negative instances.
ACE 2005 docs sents ASL relations
nw+bn 298 5029 18.8 3562
bc 52 2267 16.3 1297
cts 34 2696 15.3 603
wl 114 1697 22.6 677
Table 1: Overview of the ACE 2005 data.
For the domain adaptation experiments we use
the ACE 2005 corpus. An overview of the data
is given in Table 1. Note that this data is dif-
ferent from ACE 2004: it covers different years
(ACE 2004: texts from 2001-2002; ACE 2005:
2003-2005). Moreover, the annotation guidelines
have changed (for example, ACE 2005 contains no
discourse relation, some relation (sub)types have
changed/moved, and care must be taken for differ-
ences in SGM markup, etc.).
More importantly, the ACE 2005 corpus cov-
ers additional domains: weblogs, telephone con-
versation, usenet and broadcast conversation. In
the experiments, we use news (the union of nw
and bn) as source domain, and weblogs (wl), tele-
phone conversations (cts) and broadcast conversa-
tion (bc) as target domains.7 We take half of bc
as only target development set, and leave the re-
maining data and domains for final testing (since
they are already small, cf. Table 1). To get a feel-
ing of how these domains differ, Figure 3 depicts
the distribution of relations in each domain and Ta-
ble 2 provides the most frequent out-of-vocabulary
words together with their percentage.
Lexical Similarity and Clustering We applied
LSA to ukWaC (Baroni et al, 2009), a 2 billion
word corpus constructed from the Web8 using the
s-space toolkit.9 Dimensionality reduction was
performed using SVD with 250 dimensions, fol-
lowing (Croce et al, 2011). The co-occurrence
matrix was transformed by tfidf. For the Brown
word clusters, we used Percy Liang?s implemen-
tation10 of the Brown clustering algorithm (Liang,
2005). We incorporate cluster information by us-
7We did not consider the usenet subpart, since it is among
the smaller domains and data-preprocessing was difficult.
8http://wacky.sslmit.unibo.it/
9http://code.google.com/p/airhead-research/
10https://github.com/percyliang/brown-cluster
1502
nw_bn bc cts wl
ARTGEN?AFFORG?AFFPART?WHOLEPER?SOCPHYS
Distribution of relations across domains (normalized)
Domain
Pro
por
tion
0.0
0.1
0.2
0.3
0.4
Figure 3: Distribution of relations in ACE 2005.
Dom Most frequent OOV words
bc
(24%)
insurance, unintelligible, malprac-
tice, ph, clip, colonel, crosstalk
cts
(34%)
uh, Yeah, um, eh, mhm, uh-huh, ?,
ah, mm, th, plo, topic, y, workplace
wl
(49%)
title, Starbucks, Well, blog, !!,
werkheiser, undefeated, poor, shit
Table 2: For each domain the percentage of target
domain words (types) that are unseen in the source
together with the most frequent OOV words.
ing the 10-bit cluster prefix (Sun et al, 2011; Chan
and Roth, 2010). For the domain adaptation exper-
iments, we use ukWaC corpus-induced clusters as
bridge between domains. We limited the vocabu-
lary to that in ACE 2005, which are approximately
16k words. Following previous work, we left case
intact in the corpus and induced 1,000 word clus-
ters from words appearing at least 100 times.11
DA baseline We compare our approach to in-
stance weighting (Jiang and Zhai, 2007). We mod-
ified SVM-light-TK such that it takes a parameter
vector ?i, .., ?m as input, where each ?i represents
the relative importance of example i with respect
to the target domain (Huang et al, 2007; Wid-
mer, 2008). To estimate the importance weights,
we train a binary classifier that distinguishes be-
tween source and target domain instances. We
consider the union of the three target domains as
target data. To train the classifier, the source in-
stances are marked as negative and the target in-
stances are marked as positive. Then, this classi-
11Clusters are available at http://disi.unitn.it/ikernels/
RelationExtraction
Prior Work: Type P R F1
Zhang (2006), tree only K,yes 74.1 62.4 67.7
Zhang (2006), linear K,yes 73.5 67.0 70.1
Zhang (2006), poly K,yes 76.1 68.4 72.1
Sun & Grishman (2011) F,yes 73.4 67.7 70.4
Jiang & Zhai (2007) F,no 73.4 70.2 71.3
Our re-implementation: Type P R F1
Tree only (PET Zhang) K,yes 70.7 62.5 66.3
Linear composite K,yes 71.3 66.6 68.9
Polynomial composite K,yes 72.6 67.7 70.1
Table 3: Comparison to previous work on the 7 re-
lations of ACE 2004. K: kernel-based; F: feature-
based; yes/no: models argument order explicitly.
fier is applied to the source data. To obtain the
weights ?i, we convert the SVM scores into pos-
terior probabilities by training a sigmoid using the
modified Platt algorithm (Lin et al, 2007).12
6 Results
6.1 Alignment to Prior Work
Although most prior studies performed 5-fold
cross-validation on ACE 2004, it is often not clear
whether the partitioning has been done on the in-
stance or on the document level. Moreover, it is
often not stated whether argument order is mod-
eled explicitly, making it difficult to compare sys-
tem performance. Citing Wang (2008), ?We feel
that there is a sense of increasing confusion down
this line of research?. To ease comparison for fu-
ture research we use the same 5-fold split on the
document level as Sun et al (2011)13 and make
our system publicly available (see Section 5).
Table 3 shows that our system (bottom) aligns
well with the state of the art. Our best sys-
tem (composite kernel with polynomial expan-
sion) reaches an F1 of 70.1, which aligns well to
the 70.4 of Sun et al (2011) that use the same data-
split. This is slightly behind that of Zhang (2006);
the reason might be threefold: i) different data par-
titioning; ii) different pre-processing; iii) they in-
corporate features from additional sources, i.e. a
phrase chunker, dependency parser and semantic
resources (Zhou et al, 2005) (we have on aver-
age 9 features/instance, they use 40). Since we
focus on evaluating the impact of semantic simi-
larity in tree kernels, we think our system is very
competitive. Removing gold entity and mention
12Other weightings/normalizations (like LDA) didn?t im-
prove the results; best was to take the posteriors and add c.
13http://cs.nyu.edu/?asun/pub/ACL11_CVFileList.txt
1503
information results in a significant F1 drop from
66.3% to 54.2%. However, in a realistic setting
we do not have gold entity info available, espe-
cially not in the case when we apply the system
to any kind of text. Thus, in the domain adapta-
tion setup we assume entity boundaries given but
not their label. Clearly, evaluating the approach on
predicted mentions, e.g. Giuliano et al (2007), is
another important dimension, however, out of the
scope of the current paper.
6.2 Tree Kernels with Brown Word Clusters
To evaluate the effectiveness of Brown word clus-
ters in tree kernels, we evaluated different instance
representations (cf. Figure 2) on the ACE 2005 de-
velopment set. Table 4 shows the results.
bc-dev P R F1
baseline 52.2 41.7 46.4
replace word 49.7 38.6 43.4
replace pos 56.3 41.9 48.0
replace pos only mentions 55.3 41.6 47.5
above word 54.5 42.2 47.6
above pos 55.8 41.1 47.3
Table 4: Brown clusters in tree kernels (cf. Fig 2).
To summarize, we found: i) it is generally a bad
idea to dismiss lexical information completely,
i.e. replacing or ignoring terminals harms perfor-
mance; ii) the best way to incorporate Brown clus-
ters is to replace the Pos tag with the cluster bit-
string; iii) marking all words is generally better
than only mentions; this is in contrast to Sun et
al. (2011) who found that in their feature-based
system it was better to add cluster information
to entity mentions only. As we will discuss, the
combination of syntax and semantics exploited in
this novel kernel avoids the necessity of restricting
cluster information to mentions only.
6.3 Semantic Tree Kernels for DA
To evaluate the effectiveness of the proposed ker-
nels across domains, we use the ACE 2005 data
as testbed. Following standard practices on ACE
2004, the newswire (nw) and broadcast news (bn)
data from ACE 2005 are considered training data
(labeled source domain). The test data consists
of three targets: broadcast conversation, telephone
conversation, weblogs. As we want to build a sin-
gle system that is able to handle heterogeneous
data, we do not assume that there is further unla-
beled domain-specific data, but we assume to have
a large unlabeled corpus (ukWaC) at our disposal
to improve the generalizability of our models.
Table 5 presents the results. In the first three
rows we see the performance of the baseline
models (PET, BOW and BOW without mark-
ing). In-domain (col 1): when evaluated on the
same domain the system was trained on (nw+bn,
5-fold cross-validation). Out-of-domain perfor-
mance (cols 2-4): the system evaluated on the
targets, namely broadcast conversation (bc), tele-
phone conversation (cts) and weblogs (wl). While
the system achieves a performance of 46.0 F1
within its own domain, the performance drops to
45.3, 43.4 and 34.0 F1 on the target domains, re-
spectively. The BOW kernel that disregards syn-
tax is often less effective (row 2). We see also
the effect of target entity marking: the BOW ker-
nel without entity marking performs substantially
worse (row 3). For the remaining experiments we
use the BOW kernel with entity marking.
Rows 4 and 5 of Table 5 show the effect of
using instance weighting for the PET baseline.
Two models are shown: they differ in whether
PET or BOW was used as instance representa-
tion for training the discriminative classifier. In-
stance weighting shows mixed results: it helps
slightly on the weblogs domain, but does not help
on broadcast conversation and telephone conversa-
tions. Interestingly, the two models used to obtain
the weights perform similarly, despite the fact that
their performance differs (F1: 70.5 BOW, 73.5
PET); it turns out that the correlation between the
weights is high (+0.82).
The next part (rows 6-9) shows the effect of en-
riching the syntactic structures with either Brown
word clusters or LSA. The Brown cluster ker-
nel applied to PET (P WC) improves performance
over the baseline over all target domains. The
same holds also for the lexical semantic kernel
based on LSA (P LSA), however, to only two out
of three domains. This suggests that the two ker-
nels capture different information and a combined
kernel might be effective. More importantly, the
table shows the effect of adding Brown clusters or
LSA semantics to the BOW kernel: it can actually
hurt performance, sometimes to a small but other
times to a considerably degree. For instance, WC
applied to PET achieves an F1 of 47.0 (baseline:
45.3) on the bc domain, while applied to BOW it
hurts performance significantly, i.e. it drops from
1504
nw+bn (in-dom.) bc cts wl
Baseline: P: R: F1: P: R: F1: P: R: F1: P: R: F1:
PET 50.6 42.1 46.0 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0
BOW 55.1 37.3 44.5 57.2 37.1 45.0 57.5 31.8 41.0 41.1 27.2 32.7
BOW no marking 49.6 34.6 40.7 51.5 34.7 41.4 54.6 30.7 39.3 37.6 25.7 30.6
PET adapted: P: R: F: P: R: F: P: R: F: P: R: F:
IW1 (using PET) 51.4 44.1 47.4 49.1 41.1 44.7 50.8 37.5 43.1 35.5 33.9 34.7
IW2 (using BOW) 51.2 43.6 47.1 49.1 41.3 44.9 51.2 37.8 43.5 35.6 33.8 34.7
With Similarity: P: R: F1: P: R: F1: P: R: F1: P: R: F1:
P WC 55.4 44.6 49.4 54.3 41.4 47.0 55.9 37.1 44.6 40.0 32.7 36.0
B WC 47.9 36.4 41.4 49.5 35.2 41.2 53.3 33.2 40.9 31.7 24.1 27.4
P LSA 52.3 44.1 47.9 51.4 41.7 46.0 49.7 36.5 42.1 38.1 36.5 37.3
B LSA 53.7 37.8 44.4 55.1 33.8 41.9 54.9 32.3 40.7 39.2 28.6 33.0
P+P WC 55.0 46.5 50.4 54.4 43.4 48.3 54.1 38.1 44.7 38.4 34.5 36.3
P+P LSA 52.7 46.6 49.5 53.9 45.2 49.2 49.9 37.6 42.9 37.9 38.3 38.1
P+P WC+P LSA 55.1 45.9 50.1 55.3 43.1 48.5? 53.1 37.0 43.6 39.9 35.8 37.8?
Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005.
PET and BOW are abbreviated by P and B, respectively. If not specified BOW is marked.
45.0 to 41.2. This is also the case for LSA ap-
plied to the BOW kernel, which drops to 41.9. On
the cts domain this is less pronounced. Only on
the weblogs domain B LSA achieves a minor im-
provement (from 32.7 to 33.0). In general, dis-
tributional semantics constrained by syntax (i.e.
combined with PET) can be effectively exploited,
while if applied ?blindly? ? without the guide of
syntax (i.e. BOW) ? performance might drop, of-
ten considerably. We believe that the semantic in-
formation does not help the BOW kernel as there is
no syntactic information that constrains the appli-
cation of the noisy source, as opposed to the case
with the PET kernel.
As the two semantically enriched kernels,
PET LSA and PET WC, seem to capture different
information we use composite kernels (rows 10-
11): the baseline kernel (PET) summed with the
lexical semantic kernels. As we can see, results
improve further: for instance on the bc test set,
PET WC reaches an F1 of 47.0, while combined
with PET (PET+PET WC) this improves to 48.3.
Adding also PET LSA results in the best perfor-
mance and our final system (last row): the com-
posite kernel (PET+PET WC+PET LSA) reaches
an F1 of 48.5, 43.6 and 37.8 on the target domains,
respectively, i.e. with an absolute improvement of:
+3.2%, +0.2% and +3.8%, respectively. Two out
of three improvements are significant at p < 0.05
(indicated by ? in Table 5). Moreover, the system
also improved in its own domain (first column),
therefore having achieved robustness.
By performing an error analysis we found that,
for instance, the Brown clusters help to general-
ize locations and professions. For example, the
baseline incorrectly considered ?Dutch filmmaker?
in a PART-WHOLE relation, while our system
correctly predicted GEN-AFF(filmmaker,Dutch).
?Filmmaker? does not appear in the source, how-
ever ?Dutch citizen? does. Both ?citizen? and ?film-
maker? appear in the same cluster, thereby helping
the system to recover the correct relation.
bc cts wl
Relation: BL SYS BL SYS BL SYS
PART-WHOLE 37.8 43.1 59.3 52.3 30.5 36.3
ORG-AFF 60.7 62.9 35.5 42.3 41.0 42.0
PHYS 35.3 37.6 25.4 28.7 25.2 26.9
ART 20.8 37.9 34.5 43.5 26.5 40.3
GEN-AFF 30.1 33.0 16.8 18.6 21.6 28.1
PER-SOC 74.1 74.2 66.3 63.1 42.6 48.0
? average 45.3 48.5 43.4 43.6 34.0 37.8
Table 6: F1 per coarse relation type (ACE
2005). SYS is the final model, i.e. last row
(PET+PET WC+PET LSA) of Table 5.
Furthermore, Table 6 provides the performance
breakdown per relation for the baseline (BL) and
our best system (SYS). The table shows that our
system is able to improve F1 on all relations for
the broadcast and weblogs data. On most rela-
tions, this is also the case for the telephone (cts)
data, although the overall improvement is not sig-
nificant. Most errors were made on the PER-SOC
1505
relation, which constitutes the largest portion of
cts (cf. Figure 3). As shown in the same figure,
the relation distribution of the cts domain is also
rather different from the source. This conversation
data is a very hard domain, with a lot of disflu-
encies and spoken language patterns. We believe
it is more distant from the other domains, espe-
cially from the unlabeled collection, thus other ap-
proaches might be more appropriate, e.g. domain
identification (Dredze et al, 2010).
7 Conclusions and Future Work
We proposed syntactic tree kernels enriched by
lexical semantic similarity to tackle the portabil-
ity of a relation extractor to different domains.
The results of diverse kernels exploiting (i) Brown
clustering and (ii) LSA show that a suitable com-
bination of syntax and lexical generalization is
very promising for domain adaptation. The pro-
posed system is able to improve performance sig-
nificantly on two out of three target domains (up
to 8% relative improvement). We compared it to
instance weighting, which gave only modest or
no improvements. Brown clusters remained un-
explored for kernel-based approaches. We saw
that adding cluster information blindly might ac-
tually hurt performance. In contrast, adding lex-
ical information combined with syntax can help
to improve performance: the syntactic structure
enriched with lexical information provides a fea-
ture space where syntax constrains lexical similar-
ity obtained from unlabeled data. Thus, seman-
tic syntactic tree kernels appear to be a suitable
mechanism to adequately trade off the two kinds
of information. In future we plan to extend the
evaluation to predicted mentions, which necessar-
ily includes a careful evaluation of pre-processing
components, as well as evaluating the approach on
other semantic tasks.
Acknowledgments
We would like to thank Min Zhang for discus-
sions on his prior work as well as the anony-
mous reviewers for their valuable feedback. The
research described in this paper has been sup-
ported by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
the grant #288024: LIMOSINE ? Linguistically
Motivated Semantic aggregation engiNes.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, pages 209?226.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Corre-
spondence Learning. In Conference on Empirical
Methods in Natural Language Processing, Sydney,
Australia.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In ECIR, pages 307?318.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Exploiting Structure and Semantics for Expressive
Text Kernels. In Conference on Information Knowl-
edge and Management, Lisbon, Portugal.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-Based n-gram Models of Natural Language.
Computational Linguistics, 18:467?479.
Razvan C. Bunescu. 2007. Learning to extract rela-
tions from the web using minimal supervision. In
Proceedings of ACL.
Yee Seng Chan and Dan Roth. 2010. Exploiting
background knowledge for relation extraction. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
152?160, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion Kernels for Natural Language. In Proceedings
of Neural Information Processing Systems (NIPS
2001).
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Semantic convolution kernels over
dependency trees: smoothed partial tree kernel. In
CIKM, pages 2013?2016.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting on ACL, Barcelona,
Spain.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
ACL, pages 256?263, Prague, Czech Republic, June.
Mark Dredze, Tim Oates, and Christine Piatko. 2010.
We?re not in kansas anymore: Detecting domain
changes in streams. In Proceedings of EMNLP,
pages 585?595, Cambridge, MA.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2007. Relation extraction and the influence
of automatic named-entity recognition. ACM Trans.
Speech Lang. Process., 5(1):2:1?2:26, December.
1506
G. Golub and W. Kahan. 1965. Calculating the singu-
lar values and pseudo-inverse of a matrix. Journal of
the Society for Industrial and Applied Mathematics:
Series B, Numerical Analysis, 2(2):pp. 205?224.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics. Oxford University Press.
Jiayuan Huang, Arthur Gretton, Bernhard Scho?lkopf,
Alexander J. Smola, and Karsten M. Borgwardt.
2007. Correcting sample selection bias by unlabeled
data. In In NIPS. MIT Press.
Jing Jiang and Chengxiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In In ACL
2007, pages 264?271.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th IJCNLP, pages
1012?1020, Suntec, Singapore.
Percy Liang. 2005. Semi-Supervised Learning for
Natural Language. Master?s thesis, Massachusetts
Institute of Technology.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007. A note on platt?s probabilistic outputs for
support vector machines. Mach. Learn., 68(3):267?
276.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419?444.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
ACL-IJCNLP, pages 1003?1011, Suntec, Singapore,
August.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Meeting of the ACL, Barcelona,
Spain.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th ECML, Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM, pages 253?262.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In Proceedings of EMNLP
?09, pages 1378?1387, Stroudsburg, PA, USA.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL, pages 915?932.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Anders S?gaard and Martin Haulrich. 2011.
Sentence-level instance-weighting for graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, IWPT ?11, pages 43?47,
Stroudsburg, PA, USA.
Anders S?gaard and Anders Johannsen. 2012. Robust
learning in random subspaces: equipping NLP for
OOV effects. In Proceedings of Coling.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of ACL-HLT, pages
521?529, Portland, Oregon, USA.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In Pro-
ceedings of the 3rd International Joint Conference
on Natural Language Processing-IJCNLP.
Christian Widmer. 2008. Domain adaptation in
sequence analysis. Diplomarbeit, University of
Tu?bingen.
Feiyu Xu, Hans Uszkoreit, Hond Li, and Niko Felger.
2008. Adaptation of relation extraction rules to new
domains. In Proceedings of LREC?08, Marrakech,
Morocco.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations
between named entities from a large raw corpus us-
ing tree similarity-based clustering. In Proceedings
of IJCNLP?2005, pages 378?389, Jeju Island, South
Korea.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of COLING-ACL 2006, pages 825?
832.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
of ACL), pages 427?434, Ann Arbor, Michigan.
1507
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1252?1261,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Opinion Mining on YouTube
Aliaksei Severyn
1
, Alessandro Moschitti
3,1
,
Olga Uryupina
1
, Barbara Plank
2
, Katja Filippova
4
1
DISI - University of Trento,
2
CLT - University of Copenhagen,
3
Qatar Computing Research Institute,
4
Google Inc.
severyn@disi.unitn.it, amoschitti@qf.org.qa,
uryupina@gmail.com, bplank@cst.dk, katjaf@google.com
Abstract
This paper defines a systematic approach
to Opinion Mining (OM) on YouTube
comments by (i) modeling classifiers for
predicting the opinion polarity and the
type of comment and (ii) proposing ro-
bust shallow syntactic structures for im-
proving model adaptability. We rely on the
tree kernel technology to automatically ex-
tract and learn features with better gener-
alization power than bag-of-words. An ex-
tensive empirical evaluation on our manu-
ally annotated YouTube comments corpus
shows a high classification accuracy and
highlights the benefits of structural mod-
els in a cross-domain setting.
1 Introduction
Social media such as Twitter, Facebook or
YouTube contain rapidly changing information
generated by millions of users that can dramati-
cally affect the reputation of a person or an orga-
nization. This raises the importance of automatic
extraction of sentiments and opinions expressed in
social media.
YouTube is a unique environment, just like
Twitter, but probably even richer: multi-modal,
with a social graph, and discussions between peo-
ple sharing an interest. Hence, doing sentiment
research in such an environment is highly relevant
for the community. While the linguistic conven-
tions used on Twitter and YouTube indeed show
similarities (Baldwin et al, 2013), focusing on
YouTube allows to exploit context information,
possibly also multi-modal information, not avail-
able in isolated tweets, thus rendering it a valuable
resource for the future research.
Nevertheless, there is almost no work showing
effective OM on YouTube comments. To the best
of our knowledge, the only exception is given by
the classification system of YouTube comments
proposed by Siersdorfer et al (2010).
While previous state-of-the-art models for opin-
ion classification have been successfully applied
to traditional corpora (Pang and Lee, 2008),
YouTube comments pose additional challenges:
(i) polarity words can refer to either video or prod-
uct while expressing contrasting sentiments; (ii)
many comments are unrelated or contain spam;
and (iii) learning supervised models requires train-
ing data for each different YouTube domain, e.g.,
tablets, automobiles, etc. For example, consider a
typical comment on a YouTube review video about
a Motorola Xoom tablet:
this guy really puts a negative spin on
this , and I ?m not sure why , this seems
crazy fast , and I ?m not entirely sure
why his pinch to zoom his laggy all the
other xoom reviews
The comment contains a product name xoom and
some negative expressions, thus, a bag-of-words
model would derive a negative polarity for this
product. In contrast, the opinion towards the prod-
uct is neutral as the negative sentiment is ex-
pressed towards the video. Similarly, the follow-
ing comment:
iPad 2 is better. the superior apps just
destroy the xoom.
contains two positive and one negative word, yet
the sentiment towards the product is negative (the
negative word destroy refers to Xoom). Clearly,
the bag-of-words lacks the structural information
linking the sentiment with the target product.
In this paper, we carry out a systematic study on
OM targeting YouTube comments; its contribution
is three-fold: firstly, to solve the problems outlined
above, we define a classification schema, which
separates spam and not related comments from the
informative ones, which are, in turn, further cate-
gorized into video- or product-related comments
1252
(type classification). At the final stage, differ-
ent classifiers assign polarity (positive, negative or
neutral) to each type of a meaningful comment.
This allows us to filter out irrelevant comments,
providing accurate OM distinguishing comments
about the video and the target product.
The second contribution of the paper is the cre-
ation and annotation (by an expert coder) of a
comment corpus containing 35k manually labeled
comments for two product YouTube domains:
tablets and automobiles.
1
It is the first manu-
ally annotated corpus that enables researchers to
use supervised methods on YouTube for comment
classification and opinion analysis. The comments
from different product domains exhibit different
properties (cf. Sec. 5.2), which give the possibility
to study the domain adaptability of the supervised
models by training on one category and testing on
the other (and vice versa).
The third contribution of the paper is a novel
structural representation, based on shallow syn-
tactic trees enriched with conceptual information,
i.e., tags generalizing the specific topic of the
video, e.g., iPad, Kindle, Toyota Camry. Given the
complexity and the novelty of the task, we exploit
structural kernels to automatically engineer novel
features. In particular, we define an efficient tree
kernel derived from the Partial Tree Kernel, (Mos-
chitti, 2006a), suitable for encoding structural rep-
resentation of comments into Support Vector Ma-
chines (SVMs). Finally, our results show that our
models are adaptable, especially when the struc-
tural information is used. Structural models gen-
erally improve on both tasks ? polarity and type
classification ? yielding up to 30% of relative im-
provement, when little data is available. Hence,
the impractical task of annotating data for each
YouTube category can be mitigated by the use of
models that adapt better across domains.
2 Related work
Most prior work on more general OM has been
carried out on more standardized forms of text,
such as consumer reviews or newswire. The most
commonly used datasets include: the MPQA cor-
pus of news documents (Wilson et al, 2005), web
customer review data (Hu and Liu, 2004), Ama-
zon review data (Blitzer et al, 2007), the JDPA
1
The corpus and the annotation guidelines are pub-
licly available at: http://projects.disi.unitn.
it/iKernels/projects/sentube/
corpus of blogs (Kessler et al, 2010), etc. The
aforementioned corpora are, however, only par-
tially suitable for developing models on social
media, since the informal text poses additional
challenges for Information Extraction and Natu-
ral Language Processing. Similar to Twitter, most
YouTube comments are very short, the language
is informal with numerous accidental and deliber-
ate errors and grammatical inconsistencies, which
makes previous corpora less suitable to train mod-
els for OM on YouTube. A recent study focuses on
sentiment analysis for Twitter (Pak and Paroubek,
2010), however, their corpus was compiled auto-
matically by searching for emoticons expressing
positive and negative sentiment only.
Siersdorfer et al (2010) focus on exploiting user
ratings (counts of ?thumbs up/down? as flagged by
other users) of YouTube video comments to train
classifiers to predict the community acceptance of
new comments. Hence, their goal is different: pre-
dicting comment ratings, rather than predicting the
sentiment expressed in a YouTube comment or its
information content. Exploiting the information
from user ratings is a feature that we have not ex-
ploited thus far, but we believe that it is a valuable
feature to use in future work.
Most of the previous work on supervised senti-
ment analysis use feature vectors to encode doc-
uments. While a few successful attempts have
been made to use more involved linguistic anal-
ysis for opinion mining, such as dependency
trees with latent nodes (T?ackstr?om and McDonald,
2011) and syntactic parse trees with vectorized
nodes (Socher et al, 2011), recently, a comprehen-
sive study by Wang and Manning (2012) showed
that a simple model using bigrams and SVMs per-
forms on par with more complex models.
In contrast, we show that adding structural fea-
tures from syntactic trees is particularly useful for
the cross-domain setting. They help to build a sys-
tem that is more robust across domains. Therefore,
rather than trying to build a specialized system
for every new target domain, as it has been done
in most prior work on domain adaptation (Blitzer
et al, 2007; Daum?e, 2007), the domain adapta-
tion problem boils down to finding a more robust
system (S?gaard and Johannsen, 2012; Plank and
Moschitti, 2013). This is in line with recent ad-
vances in parsing the web (Petrov and McDonald,
2012), where participants where asked to build a
single system able to cope with different yet re-
1253
lated domains.
Our approach relies on robust syntactic struc-
tures to automatically generate patterns that adapt
better. These representations have been inspired
by the semantic models developed for Ques-
tion Answering (Moschitti, 2008; Severyn and
Moschitti, 2012; Severyn and Moschitti, 2013)
and Semantic Textual Similarity (Severyn et al,
2013). Moreover, we introduce additional tags,
e.g., video concepts, polarity and negation words,
to achieve better generalization across different
domains where the word distribution and vocab-
ulary changes.
3 Representations and models
Our approach to OM on YouTube relies on the
design of classifiers to predict comment type and
opinion polarity. Such classifiers are traditionally
based on bag-of-words and more advanced fea-
tures. In the next sections, we define a baseline
feature vector model and a novel structural model
based on kernel methods.
3.1 Feature Set
We enrich the traditional bag-of-word representa-
tion with features from a sentiment lexicon and
features quantifying the negation present in the
comment. Our model (FVEC) encodes each docu-
ment using the following feature groups:
- word n-grams: we compute unigrams and
bigrams over lower-cased word lemmas where
binary values are used to indicate the pres-
ence/absence of a given item.
- lexicon: a sentiment lexicon is a collection of
words associated with a positive or negative senti-
ment. We use two manually constructed sentiment
lexicons that are freely available: the MPQA Lex-
icon (Wilson et al, 2005) and the lexicon of Hu
and Liu (2004). For each of the lexicons, we use
the number of words found in the comment that
have positive and negative sentiment as a feature.
- negation: the count of negation words, e.g.,
{don?t, never, not, etc.}, found in a comment.
2
Our structural representation (defined next) en-
ables a more involved treatment of negation.
- video concept: cosine similarity between a com-
ment and the title/description of the video. Most
of the videos come with a title and a short descrip-
tion, which can be used to encode the topicality of
2
The list of negation words is adopted from
http://sentiment.christopherpotts.net/lingstruc.html
each comment by looking at their overlap.
3.2 Structural model
We go beyond traditional feature vectors by em-
ploying structural models (STRUCT), which en-
code each comment into a shallow syntactic tree.
These trees are input to tree kernel functions
for generating structural features. Our struc-
tures are specifically adapted to the noisy user-
generated texts and encode important aspects of
the comments, e.g., words from the sentiment lexi-
cons, product concepts and negation words, which
specifically targets the sentiment and comment
type classification tasks.
In particular, our shallow tree structure is a
two-level syntactic hierarchy built from word lem-
mas (leaves) and part-of-speech tags that are fur-
ther grouped into chunks (Fig. 1). As full syn-
tactic parsers such as constituency or dependency
tree parsers would significantly degrade in perfor-
mance on noisy texts, e.g., Twitter or YouTube
comments, we opted for shallow structures, which
rely on simpler and more robust components: a
part-of-speech tagger and a chunker. Moreover,
such taggers have been recently updated with
models (Ritter et al, 2011; Gimpel et al, 2011)
trained specifically to process noisy texts show-
ing significant reductions in the error rate on user-
generated texts, e.g., Twitter. Hence, we use the
CMU Twitter pos-tagger (Gimpel et al, 2011;
Owoputi et al, 2013) to obtain the part-of-speech
tags. Our second component ? chunker ? is taken
from (Ritter et al, 2011), which also comes with a
model trained on Twitter data
3
and shown to per-
form better on noisy data such as user comments.
To address the specifics of OM tasks on
YouTube comments, we enrich syntactic trees
with semantic tags to encode: (i) central con-
cepts of the video, (ii) sentiment-bearing words
expressing positive or negative sentiment and (iii)
negation words. To automatically identify con-
cept words of the video we use context words (to-
kens detected as nouns by the part-of-speech tag-
ger) from the video title and video description and
match them in the tree. For the matched words,
we enrich labels of their parent nodes (part-of-
speech and chunk) with the PRODUCT tag. Sim-
ilarly, the nodes associated with words found in
3
The chunker from (Ritter et al, 2011) relies on its own
POS tagger, however, in our structural representations we fa-
vor the POS tags from the CMU Twitter tagger and take only
the chunk tags from the chunker.
1254
Figure 1: Shallow tree representation of the example comment (labeled with product type and
negative sentiment): ?iPad 2 is better. the superior apps just destroy the xoom.? (lemmas are replaced
with words for readability) taken from the video ?Motorola Xoom Review?. We introduce additional tags
in the tree nodes to encode the central concept of the video (motorola xoom) and sentiment-bearing words
(better, superior, destroy) directly in the tree nodes. For the former we add a PRODUCT tag on the chunk
and part-of-speech nodes of the word xoom) and polarity tags (positive and negative) for the latter. Two
sentences are split into separate root nodes S.
the sentiment lexicon are enriched with a polar-
ity tag (either positive or negative), while nega-
tion words are labeled with the NEG tag. It should
be noted that vector-based (FVEC) model relies
only on feature counts whereas the proposed tree
encodes powerful contextual syntactic features in
terms of tree fragments. The latter are automati-
cally generated and learned by SVMs with expres-
sive tree kernels.
For example, the comment in Figure 1 shows
two positive and one negative word from the senti-
ment lexicon. This would strongly bias the FVEC
sentiment classifier to assign a positive label
to the comment. In contrast, the STRUCT model
relies on the fact that the negative word, destroy,
refers to the PRODUCT (xoom) since they form a
verbal phase (VP). In other words, the tree frag-
ment: [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP [PRODUCT-N
[xoom]]]] is a strong feature (induced
by tree kernels) to help the classifier to dis-
criminate such hard cases. Moreover, tree
kernels generate all possible subtrees, thus
producing generalized (back-off) features,
e.g., [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] or [S
[negative-VP [PRODUCT-NP]]]].
3.3 Learning
We perform OM on YouTube using supervised
methods, e.g., SVM. Our goal is to learn a model
to automatically detect the sentiment and type of
each comment. For this purpose, we build a multi-
class classifier using the one-vs-all scheme. A bi-
nary classifier is trained for each of the classes
and the predicted class is obtained by taking a
class from the classifier with a maximum predic-
tion score. Our back-end binary classifier is SVM-
light-TK
4
, which encodes structural kernels in the
SVM-light (Joachims, 2002) solver. We define a
novel and efficient tree kernel function, namely,
Shallow syntactic Tree Kernel (SHTK), which is
as expressive as the Partial Tree Kernel (PTK)
(Moschitti, 2006a) to handle feature engineering
over the structural representations of the STRUCT
model. A polynomial kernel of degree 3 is applied
to feature vectors (FVEC).
Combining structural and vector models. A
typical kernel machine, e.g., SVM, classifies a
test input x using the following prediction func-
tion: h(x) =
?
i
?
i
y
i
K(x,x
i
), where ?
i
are
the model parameters estimated from the training
data, y
i
are target variables, x
i
are support vec-
tors, and K(?, ?) is a kernel function. The latter
computes the similarity between two comments.
The STRUCT model treats each comment as a tu-
ple x = ?T ,v? composed of a shallow syntactic
tree T and a feature vector v . Hence, for each pair
of comments x
1
and x
2
, we define the following
comment similarity kernel:
K(x
1
,x
2
) = K
TK
(T
1
,T
2
) +K
v
(v
1
, v
2
), (1)
where K
TK
computes SHTK (defined next), and
K
v
is a kernel over feature vectors, e.g., linear,
polynomial, Gaussian, etc.
Shallow syntactic tree kernel. Following the
convolution kernel framework, we define the new
4
http://disi.unitn.it/moschitti/Tree-Kernel.htm
1255
SHTK function from Eq. 1 to compute the similar-
ity between tree structures. It counts the number of
common substructures between two trees T
1
and
T
2
without explicitly considering the whole frag-
ment space. The general equations for Convolu-
tion Tree Kernels is:
TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
), (2)
where N
T
1
and N
T
2
are the sets of the T
1
?s and
T
2
?s nodes, respectively and ?(n
1
, n
2
) is equal to
the number of common fragments rooted in the n
1
and n
2
nodes, according to several possible defini-
tion of the atomic fragments.
To improve the speed computation of TK, we
consider pairs of nodes (n
1
, n
2
) belonging to the
same tree level. Thus, given H , the height of the
STRUCT trees, where each level h contains nodes
of the same type, i.e., chunk, POS, and lexical
nodes, we define SHTK as the following
5
:
SHTK(T
1
, T
2
) =
H
?
h=1
?
n
1
?N
h
T
1
?
n
2
?N
h
T
2
?(n
1
, n
2
), (3)
where N
h
T
1
and N
h
T
2
are sets of nodes at height h.
The above equation can be applied with any ?
function. To have a more general and expressive
kernel, we use ? previously defined for PTK.
More formally: if n
1
and n
2
are leaves then
?(n
1
, n
2
) = ??(n
1
, n
2
); else ?(n
1
, n
2
) =
?
(
?
2
+
?
~
I
1
,
~
I
2
,|
~
I
1
|=|
~
I
2
|
?
d(
~
I
1
)+d(
~
I
2
)
|
~
I
1
|
?
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
)
,
where ?, ? ? [0, 1] are decay factors; the large
sum is adopted from a definition of the sub-
sequence kernel (Shawe-Taylor and Cristianini,
2004) to generate children subsets with gaps,
which are then used in a recursive call to ?. Here,
c
n
1
(i) is the i
th
child of the node n
1
;
~
I
1
and
~
I
2
are
two sequences of indexes that enumerate subsets
of children with gaps, i.e.,
~
I = (i
1
, i
2
, .., |I|), with
1 ? i
1
< i
2
< .. < i
|I|
; and d(
~
I
1
) =
~
I
1l(
~
I
1
)
?
~
I
11
+ 1
and d(
~
I
2
) =
~
I
2l(
~
I
2
)
?
~
I
21
+ 1, which penalizes
subsequences with larger gaps.
It should be noted that: firstly, the use of a
subsequence kernel makes it possible to generate
child subsets of the two nodes, i.e., it allows for
gaps, which makes matching of syntactic patterns
5
To have a similarity score between 0 and 1, a normaliza-
tion in the kernel space, i.e.
SHTK(T
1
,T
2
)
?
SHTK(T
1
,T
1
)?SHTK(T
2
,T
2
)
is
applied.
less rigid. Secondly, the resulting SHTK is essen-
tially a special case of PTK (Moschitti, 2006a),
adapted to the shallow structural representation
STRUCT (see Sec. 3.2). When applied to STRUCT
trees, SHTK exactly computes the same feature
space as PTK, but in faster time (on average). In-
deed, SHTK required to be only applied to node
pairs from the same level (see Eq. 3), where the
node labels can match ? chunk, POS or lexicals.
This reduces the time for selecting the matching-
node pairs carried out in PTK (Moschitti, 2006a;
Moschitti, 2006b). The fragment space is obvi-
ously the same, as the node labels of different
levels in STRUCT are different and will not be
matched by PTK either.
Finally, given its recursive definition in Eq. 3
and the use of subsequence (with gaps), SHTK can
derive useful dependencies between its elements.
For example, it will generate the following subtree
fragments: [positive-NP [positive-A
N]], [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] and so on.
4 YouTube comments corpus
To build a corpus of YouTube comments, we fo-
cus on a particular set of videos (technical reviews
and advertisings) featuring commercial products.
In particular, we chose two product categories:
automobiles (AUTO) and tablets (TABLETS). To
collect the videos, we compiled a list of prod-
ucts and queried the YouTube gData API
6
to re-
trieve the videos. We then manually excluded
irrelevant videos. For each video, we extracted
all available comments (limited to maximum 1k
comments per video) and manually annotated each
comment with its type and polarity. We distin-
guish between the following types:
product: discuss the topic product in general or
some features of the product;
video: discuss the video or some of its details;
spam: provide advertising and malicious links; and
off-topic: comments that have almost no content
(?lmao?) or content that is not related to the video
(?Thank you!?).
Regarding the polarity, we distinguish between
{positive, negative, neutral} sentiments with re-
spect to the product and the video. If the comment
contains several statements of different polarities,
it is annotated as both positive and negative: ?Love
the video but waiting for iPad 4?. In total we have
6
https://developers.google.com/youtube/v3/
1256
annotated 208 videos with around 35k comments
(128 videos TABLETS and 80 for AUTO).
To evaluate the quality of the produced labels,
we asked 5 annotators to label a sample set of one
hundred comments and measured the agreement.
The resulting annotator agreement ? value (Krip-
pendorf, 2004; Artstein and Poesio, 2008) scores
are 60.6 (AUTO), 72.1 (TABLETS) for the senti-
ment task and 64.1 (AUTO), 79.3 (TABLETS) for
the type classification task. For the rest of the
comments, we assigned the entire annotation task
to a single coder. Further details on the corpus can
be found in Uryupina et al (2014).
5 Experiments
This section reports: (i) experiments on individ-
ual subtasks of opinion and type classification; (ii)
the full task of predicting type and sentiment; (iii)
study on the adaptability of our system by learn-
ing on one domain and testing on the other; (iv)
learning curves that provide an indication on the
required amount and type of data and the scalabil-
ity to other domains.
5.1 Task description
Sentiment classification. We treat each com-
ment as expressing positive, negative or
neutral sentiment. Hence, the task is a three-
way classification.
Type classification. One of the challenging as-
pects of sentiment analysis of YouTube data is that
the comments may express the sentiment not only
towards the product shown in the video, but
also the video itself, i.e., users may post posi-
tive comments to the video while being generally
negative about the product and vice versa. Hence,
it is of crucial importance to distinguish between
these two types of comments. Additionally, many
comments are irrelevant for both the product and
the video (off-topic) or may even contain
spam. Given that the main goal of sentiment
analysis is to select sentiment-bearing comments
and identify their polarity, distinguishing between
off-topic and spam categories is not critical.
Thus, we merge the spam and off-topic into
a single uninformative category. Similar to
the opinion classification task, comment type clas-
sification is a multi-class classification with three
classes: video, product and uninform.
Full task. While the previously discussed sen-
timent and type identification tasks are useful to
Task class
AUTO TABLETS
TRAIN TEST TRAIN TEST
Sentiment
positive 2005 (36%) 807 (27%) 2393 (27%) 1872 (27%)
neutral 2649 (48%) 1413 (47%) 4683 (53%) 3617 (52%)
negative 878 (16%) 760 (26%) 1698 (19%) 1471 (21%)
total 5532 2980 8774 6960
Type
product 2733 (33%) 1761 (34%) 7180 (59%) 5731 (61%)
video 3008 (36%) 1369 (26%) 2088 (17%) 1674 (18%)
off-topic 2638 (31%) 2045 (39%) 2334 (19%) 1606 (17%)
spam 26 (>1%) 17 (>1%) 658 (5%) 361 (4%)
total 8405 5192 12260 9372
Full
product-pos. 1096 (13%) 517 (10%) 1648 (14%) 1278 (14%)
product-neu. 908 (11%) 729 (14%) 3681 (31%) 2844 (32%)
product-neg. 554 (7%) 370 (7%) 1404 (12%) 1209 (14%)
video-pos. 909 (11%) 290 (6%) 745 (6%) 594 (7%)
video-neu. 1741 (21%) 683 (14%) 1002 (9%) 773 (9%)
video-neg. 324 (4%) 390 (8%) 294 (2%) 262 (3%)
off-topic 2638 (32%) 2045 (41%) 2334 (20%) 1606 (18%)
spam 26 (>1%) 17 (>1%) 658 (6%) 361 (4%)
total 8196 5041 11766 8927
Table 1: Summary of YouTube comments data
used in the sentiment, type and full classification
tasks. The comments come from two product cate-
gories: AUTO and TABLETS. Numbers in paren-
thesis show proportion w.r.t. to the total number of
comments used in a task.
model and study in their own right, our end goal is:
given a stream of comments, to jointly predict both
the type and the sentiment of each comment. We
cast this problem as a single multi-class classifica-
tion task with seven classes: the Cartesian product
between {product, video} type labels and
{positive, neutral, negative} senti-
ment labels plus the uninformative category
(spam and off-topic). Considering a real-life ap-
plication, it is important not only to detect the po-
larity of the comment, but to also identify if it is
expressed towards the product or the video.
7
5.2 Data
We split all the videos 50% between training
set (TRAIN) and test set (TEST), where each
video contains all its comments. This ensures
that all comments from the same video appear
either in TRAIN or in TEST. Since the number
of comments per video varies, the resulting sizes
of each set are different (we use the larger split
for TRAIN). Table 1 shows the data distribution
across the task-specific classes ? sentiment and
type classification. For the sentiment task we ex-
clude off-topic and spam comments as well
as comments with ambiguous sentiment, i.e., an-
7
We exclude comments annotated as both video and
product. This enables the use of a simple flat multi-
classifiers with seven categories for the full task, instead of
a hierarchical multi-label classifiers (i.e., type classification
first and then opinion polarity). The number of comments as-
signed to both product and video is relatively small (8%
for TABLETS and 4% for AUTO).
1257
notated as both positive and negative.
For the sentiment task about 50% of the
comments have neutral polarity, while the
negative class is much less frequent. Inter-
estingly, the ratios between polarities expressed
in comments from AUTO and TABLETS are very
similar across both TRAIN and TEST. Conversely,
for the type task, we observe that comments from
AUTO are uniformly distributed among the three
classes, while for the TABLETS the majority of
comments are product related. It is likely due
to the nature of the TABLETS videos, that are
more geek-oriented, where users are more prone
to share their opinions and enter involved discus-
sions about a product. Additionally, videos from
the AUTO category (both commercials and user
reviews) are more visually captivating and, be-
ing generally oriented towards a larger audience,
generate more video-related comments. Regard-
ing the full setting, where the goal is to have
a joint prediction of the comment sentiment and
type, we observe that video-negative and
video-positive are the most scarce classes,
which makes them the most difficult to predict.
5.3 Results
We start off by presenting the results for the tradi-
tional in-domain setting, where both TRAIN and
TEST come from the same domain, e.g., AUTO or
TABLETS. Next, we show the learning curves to
analyze the behavior of FVEC and STRUCT mod-
els according to the training size. Finally, we per-
form a set of cross-domain experiments that de-
scribe the enhanced adaptability of the patterns
generated by the STRUCT model.
5.3.1 In-domain experiments
We compare FVEC and STRUCT models on three
tasks described in Sec. 5.1: sentiment, type and
full. Table 2 reports the per-class performance
and the overall accuracy of the multi-class clas-
sifier. Firstly, we note that the performance on
TABLETS is much higher than on AUTO across
all tasks. This can be explained by the follow-
ing: (i) TABLETS contains more training data and
(ii) videos from AUTO and TABLETS categories
draw different types of audiences ? well-informed
users and geeks expressing better-motivated opin-
ions about a product for the former vs. more gen-
eral audience for the latter. This results in the
different quality of comments with the AUTO be-
ing more challenging to analyze. Secondly, we
observe that the STRUCT model provides 1-3%
of absolute improvement in accuracy over FVEC
for every task. For individual categories the F1
scores are also improved by the STRUCT model
(except for the negative classes for AUTO, where
we see a small drop). We conjecture that sentiment
prediction for AUTO category is largely driven
by one-shot phrases and statements where it is
hard to improve upon the bag-of-words and senti-
ment lexicon features. In contrast, comments from
TABLETS category tend to be more elaborated
and well-argumented, thus, benefiting from the ex-
pressiveness of the structural representations.
Considering per-class performance, correctly
predicting negative sentiment is most difficult
for both AUTO and TABLETS, which is proba-
bly caused by the smaller proportion of the neg-
ative comments in the training set. For the type
task, video-related class is substantially more dif-
ficult than product-related for both categories. For
the full task, the class video-negative ac-
counts for the largest error. This is confirmed by
the results from the previous sentiment and type
tasks, where we saw that handling negative sen-
timent and detecting video-related comments are
most difficult.
5.3.2 Learning curves
The learning curves depict the behavior of FVEC
and STRUCT models as we increase the size of
the training set. Intuitively, the STRUCT model
relies on more general syntactic patterns and may
overcome the sparseness problems incurred by the
FVEC model when little training data is available.
Nevertheless, as we see in Figure 2, the learning
curves for sentiment and type classification tasks
across both product categories do not confirm this
intuition. The STRUCT model consistently outper-
forms the FVEC across all training sizes, but the
gap in the performance does not increase when we
move to smaller training sets. As we will see next,
this picture changes when we perform the cross-
domain study.
5.3.3 Cross-domain experiments
To understand the performance of our classifiers
on other YouTube domains, we perform a set of
cross-domain experiments by training on the data
from one product category and testing on the other.
Table 3 reports the accuracy for three tasks
when we use all comments (TRAIN + TEST) from
AUTO to predict on the TEST from TABLETS
1258
Task class
AUTO TABLETS
FVEC STRUCT FVEC STRUCT
P R F1 P R F1 P R F1 P R F1
Sent
positive 49.1 72.1 58.4 50.1 73.9 59.0 67.5 70.3 69.9 71.2 71.3 71.3
neutral 68.2 55.0 61.4 70.1 57.6 63.1 81.3 71.4 76.9 81.1 73.1 77.8
negative 42.0 36.9 39.6 41.3 35.8 38.8 48.3 60.0 54.8 50.2 62.6 56.5
Acc 54.7 55.7 68.6 70.5
Type
product 66.8 73.3 69.4 68.8 75.5 71.7 78.2 95.3 86.4 80.1 95.5 87.6
video 45.0 52.8 48.2 47.8 49.9 48.7 83.6 45.7 58.9 83.5 46.7 59.4
uninform 59.3 48.2 53.1 60.6 53.0 56.4 70.2 52.5 60.7 72.9 58.6 65.0
Acc 57.4 59.4 77.2 78.6
Full
product-pos 34.0 49.6 39.2 36.5 51.2 43.0 48.4 56.8 52.0 52.4 59.3 56.4
product-neu 43.4 31.1 36.1 41.4 36.1 38.4 68.0 67.5 68.1 59.7 83.4 70.0
product-neg 26.3 29.5 28.8 26.3 25.3 25.6 43.0 49.9 45.4 44.7 53.7 48.4
video-pos 23.2 47.1 31.9 26.1 54.5 35.5 69.1 60.0 64.7 64.9 68.8 66.4
video-neu 26.1 30.0 29.0 26.5 31.6 28.8 56.4 32.1 40.0 55.1 35.7 43.3
video-neg 21.9 3.7 6.0 17.7 2.3 4.8 39.0 17.5 23.9 39.5 6.1 11.5
uninform 56.5 52.4 54.9 60.0 53.3 56.3 60.0 65.5 62.2 63.3 68.4 66.9
Acc 40.0 41.5 57.6 60.3
Table 2: In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT. The
results are reported for sentiment, type and full classification tasks. The metrics used are precision (P),
recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc).
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVECAccuracy
55
60
65
70
training size1k 2k 3k 4k 5k ALL
(a) Sentiment classification
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVEC
Accurac
y
40
45
50
55
60
65
70
75
80
training size1k 2k 3k 4k 5k ALL
(b) Type classification
Figure 2: In-domain learning curves. ALL refers
to the entire TRAIN set for a given product cate-
gory, i.e., AUTO and TABLETS (see Table 1)
and in the opposite direction (TABLETS?AUTO).
When using AUTO as a source domain, STRUCT
model provides additional 1-3% of absolute im-
Source Target Task FVEC STRUCT
AUTO TABLETS
Sent 66.1 66.6
Type 59.9 64.1
?
Full 35.6 38.3
?
TABLETS AUTO
Sent 60.4 61.9
?
Type 54.2 55.6
?
Full 43.4 44.7
?
Table 3: Cross-domain experiment. Ac-
curacy using FVEC and STRUCT models
when trained/tested in both directions, i.e.
AUTO?TABLETS and TABLETS?AUTO.
?
de-
notes results statistically significant at 95% level
(via pairwise t-test).
provement, except for the sentiment task.
Similar to the in-domain experiments, we stud-
ied the effect of the source domain size on the tar-
get test performance. This is useful to assess the
adaptability of features exploited by the FVEC and
STRUCT models with the change in the number
of labeled examples available for training. Addi-
tionally, we considered a setting including a small
amount of training data from the target data (i.e.,
supervised domain adaptation).
For this purpose, we drew the learning curves of
the FVEC and STRUCT models applied to the sen-
timent and type tasks (Figure 3): AUTO is used
as the source domain to train models, which are
tested on TABLETS.
8
The plot shows that when
8
The results for the other direction (TABLETS?AUTO)
show similar behavior.
1259
STRUCTFVEC
Source +Target
Accurac
y
62
63
64
65
66
67
68
training size1k 2k 3k 4k 5k 8.5k(ALL) 100 500 1k
(a) Sentiment classification
STRUCTFVEC
Source +Target
Accurac
y
30
35
40
45
50
55
60
65
70
training size1k 2k 3k 4k 5k 8.5k(TRAIN) 13k(ALL) 100 500 1k
(b) Type classification
Figure 3: Learning curves for the cross-domain
setting (AUTO?TABLETS). Shaded area refers to
adding a small portion of comments from the same
domain as the target test data to the training.
little training data is available, the features gener-
ated by the STRUCT model exhibit better adapt-
ability (up to 10% of improvement over FVEC).
The bag-of-words model seems to be affected by
the data sparsity problem which becomes a crucial
issue when only a small training set is available.
This difference becomes smaller as we add data
from the same domain. This is an important ad-
vantage of our structural approach, since we can-
not realistically expect to obtain manual annota-
tions for 10k+ comments for each (of many thou-
sands) product domains present on YouTube.
5.4 Discussion
Our STRUCT model is more accurate since it is
able to induce structural patterns of sentiment.
Consider the following comment: optimus pad
is better. this xoom is just to bulky but optimus
pad offers better functionality. The FVEC bag-
of-words model misclassifies it to be positive,
since it contains two positive expressions (better,
better functionality) that outweigh a single nega-
tive expression (bulky). The structural model, in
contrast, is able to identify the product of interest
(xoom) and associate it with the negative expres-
sion through a structural feature and thus correctly
classify the comment as negative.
Some issues remain problematic even for the
structural model. The largest group of errors are
implicit sentiments. Thus, some comments do not
contain any explicit positive or negative opinions,
but provide detailed and well-argumented criti-
cism, for example, this phone is heavy. Such com-
ments might also include irony. To account for
these cases, a deep understanding of the product
domain is necessary.
6 Conclusions and Future Work
We carried out a systematic study on OM from
YouTube comments by training a set of su-
pervised multi-class classifiers distinguishing be-
tween video and product related opinions. We
use standard feature vectors augmented by shallow
syntactic trees enriched with additional conceptual
information.
This paper makes several contributions: (i) it
shows that effective OM can be carried out with
supervised models trained on high quality annota-
tions; (ii) it introduces a novel annotated corpus
of YouTube comments, which we make available
for the research community; (iii) it defines novel
structural models and kernels, which can improve
on feature vectors, e.g., up to 30% of relative im-
provement in type classification, when little data
is available, and demonstrates that the structural
model scales well to other domains.
In the future, we plan to work on a joint model
to classify all the comments of a given video, s.t. it
is possible to exploit latent dependencies between
entities and the sentiments of the comment thread.
Additionally, we plan to experiment with hierar-
chical multi-label classifiers for the full task (in
place of a flat multi-class learner).
Acknowledgments
The authors are supported by a Google Fac-
ulty Award 2011, the Google Europe Fellowship
Award 2013 and the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der the grant #288024: LIMOSINE.
1260
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources? In
IJCNLP.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Hal Daum?e, III. 2007. Frustratingly easy domain
adaptation. ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Alessandro Moschitti. 2006a. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML.
Alessandro Moschitti. 2006b. Making tree kernels
practical for natural language learning. In EACL,
pages 113?120.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In EMNLP.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual simi-
larity with structural representations. In ACL.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,
and Jose San Pedro. 2010. How useful are your
comments?: Analyzing and predicting YouTube
comments and comment ratings. In WWW.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In EMNLP.
Anders S?gaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Oscar T?ackstr?om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In ACL.
Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
SenTube: A corpus for sentiment analysis on
YouTube social media. In LREC.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
1261
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377?382,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Experiments with crowdsourced re-annotation of a POS tagging data set
Dirk Hovy, Barbara Plank, and Anders S?gaard
Center for Language Technology
University of Copenhagen
Njalsgade 140, 2300 Copenhagen
{dirk|bplank}@cst.dk, soegaard@hum.ku.dk
Abstract
Crowdsourcing lets us collect multiple an-
notations for an item from several annota-
tors. Typically, these are annotations for
non-sequential classification tasks. While
there has been some work on crowdsourc-
ing named entity annotations, researchers
have largely assumed that syntactic tasks
such as part-of-speech (POS) tagging can-
not be crowdsourced. This paper shows
that workers can actually annotate sequen-
tial data almost as well as experts. Fur-
ther, we show that the models learned from
crowdsourced annotations fare as well as
the models learned from expert annota-
tions in downstream tasks.
1 Introduction
Training good predictive NLP models typically re-
quires annotated data, but getting professional an-
notators to build useful data sets is often time-
consuming and expensive. Snow et al (2008)
showed, however, that crowdsourced annotations
can produce similar results to annotations made
by experts. Crowdsourcing services such as Ama-
zon?s Mechanical Turk has since been successfully
used for various annotation tasks in NLP (Jha et
al., 2010; Callison-Burch and Dredze, 2010).
However, most applications of crowdsourcing
in NLP have been concerned with classification
problems, such as document classification and
constructing lexica (Callison-Burch and Dredze,
2010). A large part of NLP problems, however, are
structured prediction tasks. Typically, sequence
labeling tasks employ a larger set of labels than
classification problems, as well as complex inter-
actions between the annotations. Disagreement
among annotators is therefore potentially higher,
and the task of annotating structured data thus
harder.
Only a few recent studies have investi-
gated crowdsourcing sequential tasks; specifically,
named entity recognition (Finin et al, 2010; Ro-
drigues et al, 2013). Results for this are good.
However, named entities typically use only few la-
bels (LOC, ORG, and PER), and the data contains
mostly non-entities, so the complexity is manage-
able. The question of whether a more linguisti-
cally involved structured task like part-of-speech
(POS) tagging can be crowdsourced has remained
largely unaddressed.
1
In this paper, we investigate how well lay anno-
tators can produce POS labels for Twitter data. In
our setup, we present annotators with one word at
a time, with a minimal surrounding context (two
words to each side). Our choice of annotating
Twitter data is not coincidental: with the short-
lived nature of Twitter messages, models quickly
lose predictive power (Eisenstein, 2013), and re-
training models on new samples of more represen-
tative data becomes necessary. Expensive profes-
sional annotation may be prohibitive for keeping
NLP models up-to-date with linguistic and topical
changes on Twitter. We use a minimum of instruc-
tions and require few qualifications.
Obviously, lay annotation is generally less re-
liable than professional annotation. It is there-
fore common to aggregate over multiple annota-
tions for the same item to get more robust anno-
tations. In this paper we compare two aggrega-
tion schemes, namely majority voting (MV) and
MACE (Hovy et al, 2013). We also show how we
can use Wiktionary, a crowdsourced lexicon, to fil-
ter crowdsourced annotations. We evaluate the an-
notations in several ways: (a) by testing their ac-
curacy with respect to a gold standard, (b) by eval-
uating the performance of POS models trained on
1
One of the reviewers alerted us to an unpublished mas-
ters thesis, which uses pre-annotation to reduce tagging to
fewer multiple-choice questions. See Related Work section
for details.
377
the annotations across several existing data sets,
as well as (c) by applying our models in down-
stream tasks. We show that with minimal context
and annotation effort, we can produce structured
annotations of near-expert quality. We also show
that these annotations lead to better POS tagging
models than previous models learned from crowd-
sourced lexicons (Li et al, 2012). Finally, we
show that models learned from these annotations
are competitive with models learned from expert
annotations on various downstream tasks.
2 Our Approach
We crowdsource the training section of the data
from Gimpel et al (2011)
2
with POS tags. We use
Crowdflower,
3
to collect five annotations for each
word, and then find the most likely label for each
word among the possible annotations. See Figure
1 for an example. If the correct label is not among
the annotations, we are unable to recover the cor-
rect answer. This was the case for 1497 instances
in our data (cf. the token ?:? in the example).
We thus report on oracle score, i.e., the best label
sequence that could possibly be found, which is
correct except for the missing tokens. Note that
while we report agreement between the crowd-
sourced annotations and the crowdsourced anno-
tations, our main evaluations are based on models
learned from expert vs. crowdsourced annotations
and downstream applications thereof (chunking
and NER). We take care in evaluating our models
across different data sets to avoid biasing our
evaluations to particular annotations. All the data
sets used in our experiments are publicly available
at http://lowlands.ku.dk/results/.
x Z y
@USER NOUN,NOUN,X,NOUN,-,NOUN NOUN
: .,.,-,.,.,. X
I PRON,NOUN,PRON,NOUN,PRON,- PRON
owe VERB,VERB,-,VERB,VERB,VERB VERB
U PRON,X,-,NOUN,NOUN,PRON PRON
? = 0.9, 0.4, 0.2, 0.8, 0.8, 0.9
Figure 1: Five annotations per token, supplied by 6
different annotators (- = missing annotation), gold
label y. ? = competence values for each annotator.
2
http://www.ark.cs.cmu.edu/TweetNLP/
3
http://crowdflower.com
3 Crowdsourcing Sequential Annotation
In order to use the annotations to train models that
can be applied across various data sets, i.e., mak-
ing out-of-sample evaluation possible (see Section
5), we follow Hovy et al (2014) in using the uni-
versal tag set (Petrov et al, 2012) with 12 labels.
Figure 2: Screen shot of the annotation interface
on Crowdflower
Annotators were given a bold-faced word with
two words on either side and asked to select the
most appropriate tag from a drop down menu. For
each tag, we spell out the name of the syntactic
category, and provide a few example words.
See Figure 2 for a screenshot of the interface.
Annotators were also told that words can belong
to several classes, depending on the context. No
additional guidelines were given.
Only trusted annotators (in Crowdflower:
Bronze skills) that had answered correctly on 4
gold tokens (randomly chosen from a set of 20
gold tokens provided by the authors) were allowed
to submit annotations. In total, 177 individual
annotators supplied answers. We paid annotators
a reward of $0.05 for 10 tokens. The full data set
contains 14,619 tokens. Completion of the task
took slightly less than 10 days. Contributors were
very satisfied with the task (4.5 on a scale from 1
to 5). In particular, they felt instructions were clear
(4.4/5), and that the pay was reasonable (4.1/5).
4 Label Aggregation
After collecting the annotations, we need to aggre-
gate the annotations to derive a single answer for
each token. In the simplest scheme, we choose the
majority label, i.e., the label picked by most an-
notators. In case of ties, we select the final label
at random. Since this is a stochastic process, we
average results over 100 runs. We refer to this as
MAJORITY VOTING (MV). Note that in MV we
trust all annotators to the same degree. However,
crowdsourcing attracts people with different mo-
378
tives, and not all of them are equally reliable?
even the ones with Bronze level. Ideally, we would
like to factor this into our decision process.
We use MACE
4
(Hovy et al, 2013) as our sec-
ond scheme to learn both the most likely answer
and a competence estimate for each of the annota-
tors. MACE treats annotator competence and the
correct answer as hidden variables and estimates
their parameters via EM (Dempster et al, 1977).
We use MACE with default parameter settings to
give us the weighted average for each annotated
example.
Finally, we also tried applying the joint learn-
ing scheme in Rodrigues et al (2013), but their
scheme requires that entire sequences are anno-
tated by the same annotators, which we don?t have,
and it expects BIO sequences, rather than POS
tags.
Dictionaries Decoding tasks profit from the use
of dictionaries (Merialdo, 1994; Johnson, 2007;
Ravi and Knight, 2009) by restricting the number
of tags that need to be considered for each word,
also known as type constraints (T?ackstr?om et al,
2013). We follow Li et al (2012) in including
Wiktionary information as type constraints into
our decoding: if a word is found in Wiktionary,
we disregard all annotations that are not licensed
by the dictionary entry. If the word is not found in
Wiktionary, or if none of its annotations is licensed
by Wiktionary, we keep the original annotations.
Since we aggregate annotations independently
(unlike Viterbi decoding), we basically use Wik-
tionary as a pre-filtering step, such that MV and
MACE only operate on the reduced annotations.
5 Experiments
Each of the two aggregation schemes above pro-
duces a final label sequence y? for our training cor-
pus. We evaluate the resulting annotated data in
three ways.
1. We compare y? to the available expert annota-
tion on the training data. This tells us how similar
lay annotation is to professional annotation.
2. Ultimately, we want to use structured anno-
tations for supervised training, where annotation
quality influences model performance on held-out
test data. To test this, we train a CRF model
(Lafferty et al, 2001) with simple orthographic
features and word clusters (Owoputi et al, 2013)
4
http://www.isi.edu/publications/
licensed-sw/mace/
on the annotated Twitter data described in Gim-
pel et al (2011). Leaving out the dedicated test
set to avoid in-sample bias, we evaluate our mod-
els across three data sets: RITTER (the 10% test
split of the data in Ritter et al (2011) used in Der-
czynski et al (2013)), the test set from Foster et
al. (2011), and the data set described in Hovy et
al. (2014).
We will make the preprocessed data sets avail-
able to the public to facilitate comparison. In ad-
dition to a supervised model trained on expert an-
notations, we compare our tagging accuracy with
that of a weakly supervised system (Li et al, 2012)
re-trained on 400,000 unlabeled tweets to adapt to
Twitter, but using a crowdsourced lexicon, namely
Wiktionary, to constrain inference. We use param-
eter settings from Li et al (2012), as well as their
Wikipedia dump, available from their project web-
site.
5
3. POS tagging is often the first step for further
analysis, such as chunking, parsing, etc. We
test the downstream performance of the POS
models from the previous step on chunking and
NER. We use the models to annotate the training
data portion of each task with POS tags, and
use them as features in a chunking and NER
model. For both tasks, we train a CRF model
on the respective (POS-augmented) training set,
and evaluate it on several held-out test sets. For
chunking, we use the test sets from Foster et al
(2011) and Ritter et al (2011) (with the splits
from Derczynski et al (2013)). For NER, we use
data from Finin et al (2010) and again Ritter et al
(2011). For chunking, we follow Sha and Pereira
(2003) for the set of features, including token
and POS information. For NER, we use standard
features, including POS tags (from the previous
experiments), indicators for hyphens, digits,
single quotes, upper/lowercase, 3-character prefix
and suffix information, and Brown word cluster
features
6
with 2,4,8,16 bitstring prefixes estimated
from a large Twitter corpus (Owoputi et al, 2013).
We report macro-averages over all these data sets.
6 Results
Agreement with expert annotators Table 1
shows the accuracy of each aggregation compared
to the gold labels. The crowdsourced annotations
5
https://code.google.com/p/
wikily-supervised-pos-tagger/
6
http://www.ark.cs.cmu.edu/TweetNLP/
379
majority 79.54
MACE-EM 79.89
majority+Wiktionary 80.58
MACE-EM+Wiktionary 80.75
oracle 89.63
Table 1: Accuracy (%) of different annotations wrt
gold data
aggregated using MV agree with the expert anno-
tations in 79.54% of the cases. If we pre-filter the
data using Wiktionary, the agreement becomes
80.58%. MACE leads to higher agreement with
expert annotations under both conditions (79.89
and 80.75). The small difference indicates that
annotators are consistent and largely reliable,
thus confirming the Bronze-level qualification
we required. Both schemes cannot recover the
correct answer for the 1497 cases where none of
the crowdsourced labels matched the gold label,
i.e. y /? Z
i
. The best possible result either of them
could achieve (the oracle) would be matching all
but the missing labels, an agreement of 89.63%.
Most of the cases where the correct label was
not among the annotations belong to a small set
of confusions. The most frequent was mislabeling
?:? and ?. . .?, both mapped to X. Annotators
mostly decided to label these tokens as punctu-
ation (.). They also predominantly labeled your,
my and this as PRON (for the former two), and a
variety of labels for the latter, when the gold label
is DET.
RITTER FOSTER HOVY
Li et al (2012) 73.8 77.4 79.7
MV 80.5 81.6 83.7
MACE 80.4 81.7 82.6
MV+Wik 80.4 82.1 83.7
MACE+Wik 80.5 81.9 83.7
Upper bounds
oracle 82.4 83.7 85.1
gold 82.6 84.7 86.8
Table 2: POS tagging accuracies (%).
Effect on POS Tagging Accuracy Usually, we
don?t want to match a gold standard, but we
rather want to create new annotated training
data. Crowdsourcing matches our gold standard
to about 80%, but the question remains how useful
this data is when training models on it. After all,
inter-annotator agreement among professional an-
notators on this task is only around 90% (Gimpel
et al, 2011; Hovy et al, 2014). In order to evalu-
ate how much each aggregation scheme influences
tagging performance of the resulting model, we
train separate models on each scheme?s annota-
tions and test on the same four data sets. Table
2 shows the results. Note that the differences be-
tween the four schemes are insignificant. More
importantly, however, POS tagging accuracy us-
ing crowdsourced annotations are on average only
2.6% worse than gold using professional annota-
tions. On the other hand, performance is much
better than the weakly supervised approach by Li
et al (2012), which only relies on a crowdsourced
POS lexicon.
POS model from CHUNKING NER
MV 74.80 75.74
MACE 75.04 75.83
MV+Wik 75.86 76.08
MACE+Wik 75.86 76.15
Upper bounds
oracle 76.22 75.85
gold 79.97 75.81
Table 3: Downstream accuracy for chunking (l)
and NER (r) of models using POS.
Downstream Performance Table 3 shows the
accuracy when using the POS models trained
in the previous evaluation step. Note that we
present the average over the two data sets used
for each task. Note also how the Wiktionary con-
straints lead to improvements in downstream per-
formance. In chunking, we see that using the
crowdsourced annotations leads to worse perfor-
mance than using the professional annotations.
For NER, however, we find that some of the POS
taggers trained on aggregated data produce bet-
ter NER performance than POS taggers trained on
expert-annotated gold data. Since the only dif-
ference between models are the respective POS
features, the results suggest that at least for some
tasks, POS taggers learned from crowdsourced an-
notations may be as good as those learned from
expert annotations.
7 Related Work
There is considerable work in the literature on
modeling answer correctness and annotator com-
petence as latent variables (Dawid and Skene,
380
1979; Smyth et al, 1995; Carpenter, 2008; White-
hill et al, 2009; Welinder et al, 2010; Yan et al,
2010; Raykar and Yu, 2012). Rodrigues et al
(2013) recently presented a sequential model for
this. They estimate annotator competence as la-
tent variables in a CRF model using EM. They
evaluate their approach on synthetic and NER data
annotated on Mechanical Turk, showing improve-
ments over the MV baselines and the multi-label
model by Dredze et al (2009). The latter do not
model annotator reliability but rather model label
priors by integrating them into the CRF objective,
and re-estimating them during learning. Both re-
quire annotators to supply a full sentence, while
we use minimal context, which requires less anno-
tator commitment and makes the task more flexi-
ble. Unfortunately, we could not run those mod-
els on our data due to label incompatibility and
the fact that we typically do not have complete se-
quences annotated by the same annotators.
Mainzer (2011) actually presents an earlier pa-
per on crowdsourcing POS tagging. However, it
differs from our approach in several ways. It uses
the Penn Treebank tag set to annotate Wikipedia
data (which is much more canonical than Twitter)
via a Java applet. The applet automatically labels
certain categories, and only presents the users with
a series of multiple choice questions for the re-
mainder. This is highly effective, as it eliminates
some sources of possible disagreement. In con-
trast, we do not pre-label any tokens, but always
present the annotators with all labels.
8 Conclusion
We use crowdsourcing to collect POS annotations
with minimal context (five-word windows). While
the performance of POS models learned from
this data is still slightly below that of models
trained on expert annotations, models learned
from aggregations approach oracle performance
for POS tagging. In general, we find that the
use of a dictionary tends to make aggregations
more useful, irrespective of aggregation method.
For some downstream tasks, models using the
aggregated POS tags perform even better than
models using expert-annotated tags.
Acknowledgments
We would like to thank the anonymous review-
ers for valuable comments and feedback. This re-
search is funded by the ERC Starting Grant LOW-
LANDS No. 313695.
References
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20?28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39(1):1?
38.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multi-
ple labels. In ECML/PKDD Workshop on Learning
from Multi-Label Data.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
When pos datasets don t add up: Combatting sample
bias. In LREC.
381
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara
Rosenthal, and Kathleen McKeown. 2010. Corpus
creation for new genres: A crowdsourced approach
to pp attachment. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jacob Emil Mainzer. 2011. Labeling parts of
speech using untrained annotators on mechanical
turk. Master?s thesis, The Ohio State University.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP. Association for Computational Lin-
guistics.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491?518.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Filipe Rodrigues, Francisco Pereira, and Bernardete
Ribeiro. 2013. Sequence labeling with multiple an-
notators. Machine Learning, pages 1?17.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Ad-
vances in neural information processing systems,
pages 1085?1092.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, Mar(1):1?12.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. Advances in
Neural Information Processing Systems, 22:2035?
2043.
Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator exper-
tise: Learning when everybody knows a bit of some-
thing. In International Conference on Artificial In-
telligence and Statistics.
382
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 507?511,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Linguistically debatable or just plain wrong?
Barbara Plank, Dirk Hovy and Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
Abstract
In linguistic annotation projects, we typ-
ically develop annotation guidelines to
minimize disagreement. However, in this
position paper we question whether we
should actually limit the disagreements
between annotators, rather than embracing
them. We present an empirical analysis
of part-of-speech annotated data sets that
suggests that disagreements are systematic
across domains and to a certain extend also
across languages. This points to an un-
derlying ambiguity rather than random er-
rors. Moreover, a quantitative analysis of
tag confusions reveals that the majority of
disagreements are due to linguistically de-
batable cases rather than annotation errors.
Specifically, we show that even in the ab-
sence of annotation guidelines only 2% of
annotator choices are linguistically unmo-
tivated.
1 Introduction
In NLP, we often model annotation as if it re-
flected a single ground truth that was guided by
an underlying linguistic theory. If this was true,
the specific theory should be learnable from the
annotated data. However, it is well known that
there are linguistically hard cases (Zeman, 2010),
where no theory provides a clear answer, so an-
notation schemes commit to more or less arbi-
trary decisions. For example, in parsing auxil-
iary verbs may head main verbs, or vice versa,
and in part-of-speech (POS) tagging, possessive
pronouns may belong to the category of deter-
miners or the category of pronouns. This posi-
tion paper argues that annotation projects should
embrace these hard cases rather than pretend they
can be unambiguously resolved. Instead of using
overly specific annotation guidelines, designed to
minimize inter-annotator disagreement (Duffield
et al, 2007), and adjudicating between annotators
of different opinions, we should embrace system-
atic inter-annotator disagreements. To motivate
this, we present an empirical analysis showing
1. that certain inter-annotator disagreements are
systematic, and
2. that actual errors are in fact so infrequent as
to be negligible, even when linguists annotate
without guidelines.
The empirical analysis presented below relies
on text corpora annotated with syntactic cate-
gories or parts-of-speech (POS). POS is part of
most linguistic theories, but nevertheless, there
are still many linguistic constructions ? even very
frequent ones ? whose POS analysis is widely
debated. The following sentences exemplify some
of these hard cases that annotators frequently
disagree on. Note that we do not claim that both
analyses in each of these cases (1?3) are equally
good, but that there is some linguistic motivation
for either analysis in each case.
(1) Noam goes out tonight
NOUN VERB ADP/PRT ADV/NOUN
(2) Noam likes social media
NOUN VERB ADJ/NOUN NOUN
(3) Noam likes his car
NOUN VERB DET/PRON NOUN
To substantiate our claims, we first compare
the distribution of inter-annotator disagreements
across domains and languages, showing that most
disagreements are systematic (Section 2). This
suggests that most annotation differences derive
from hard cases, rather than random errors.
We then collect a corpus of such disagreements
and have experts mark which ones are due to ac-
tual annotation errors, and which ones reflect lin-
guistically hard cases (Section 3). The results
show that the majority of disagreements are due
507
to hard cases, and only about 20% of conflict-
ing annotations are actual errors. This suggests
that inter-annotator agreement scores often hide
the fact that the vast majority of annotations are
actually linguistically motivated. In our case, less
than 2% of the overall annotations are linguisti-
cally unmotivated.
Finally, in Section 4, we present an experiment
trying to learn a model to distinguish between hard
cases and annotation errors.
2 Annotator disagreements across
domains and languages
In this study, we had between 2-10 individual an-
notators with degrees in linguistics annotate dif-
ferent kinds of English text with POS tags, e.g.,
newswire text (PTB WSJ Section 00), transcripts
of spoken language (from a database containing
transcripts of conversations, Talkbank
1
), as well
as Twitter posts. Annotators were specifically not
presented with guidelines that would help them re-
solve hard cases. Moreover, we compare profes-
sional annotation to that of lay people. We in-
structed annotators to use the 12 universal POS
tags of Petrov et al (2012). We did so in or-
der to make comparison between existing data
sets possible. Moreover, this allows us to fo-
cus on really hard cases, as any debatable case in
the coarse-grained tag set is necessarily also part
of the finer-grained tag set.
2
For each domain,
we collected exactly 500 doubly-annotated sen-
tences/tweets. Besides these English data sets, we
also obtained doubly-annotated POS data from the
French Social Media Bank project (Seddah et al,
2012).
3
All data sets, except the French one, are
publicly available at http://lowlands.ku.
dk/.
We present disagreements as Hinton diagrams
in Figure 1a?c. Note that the spoken language data
does not include punctuation. The correlations
between the disagreements are highly significant,
with Spearman coefficients ranging from 0.644
1
http://talkbank.org/
2
Experiments with variation n-grams on WSJ (Dickinson
and Meurers, 2003) and the French data lead us to estimate
that the fine-to-coarse mapping of POS tags disregards about
20% of observed tag-pair confusion types, most of which re-
late to fine-grained verb and noun distinctions, e.g. past par-
ticiple versus past in ?[..] criminal lawyers speculated/VBD
vs. VBN that [..]?.
3
We mapped POS tags into the universal POS tags using
the mappings available here: https://code.google.
com/p/universal-pos-tags/
(spoken and WSJ) to 0.869 (spoken and Twit-
ter). Kendall?s ? ranges from 0.498 (Twitter and
WSJ) to 0.659 (spoken and Twitter). All diagrams
have a vaguely ?dagger?-like shape, with the blade
going down the diagonal from top left to bot-
tom right, and a slightly curved ?hilt? across the
counter-diagonal, ending in the more pronounced
ADP/PRT confusion cells.
Disagreements are very similar across all three
domains. In particular, adpositions (ADP) are con-
fused with particles (PRT) (as in the case of ?get
out?); adjectives (ADJ) are confused with nouns
(as in ?stone lion?); pronouns (PRON) are con-
fused with determiners (DET) (?my house?); nu-
merals are confused with adjectives, determiners,
and nouns (?2nd time?); and adjectives are con-
fused with adverbs (ADV) (?see you later?). In
Twitter, the X category is often confused with
punctuations, e.g., when annotating punctuation
acting as discourse continuation marker.
Our analyses show that a) experts disagree on
the known hard cases when freely annotating text,
and b) that these disagreements are the same
across text types. More surprisingly, though, we
also find that, as discussed next, c) roughly the
same disagreements are also observed when com-
paring the linguistic intuitions of lay people.
More specifically, we had lay annotators on the
crowdsourcing platform Crowdflower re-annotate
the training section of Gimpel et al (2011). They
collected five annotations per word. Only annota-
tors that had answered correctly on 4 gold items
(randomly chosen from a set of 20 gold items
provided by the authors) were allowed to submit
annotations. In total, 177 individual annotators
supplied answers. We paid annotators a reward
of $0.05 for 10 items. The full data set con-
tains 14,619 items and is described in further de-
tail in Hovy et al (2014). Annotators were satis-
fied with the task (4.5 on a scale from 1 to 5) and
felt that instructions were clear (4.4/5), and the pay
reasonable (4.1/5). The crowdsourced annotations
aggregated using majority voting agree with the
expert annotations in 79.54% of the cases. If we
pre-filter the data via Wiktionary and use an item-
response model (Hovy et al, 2013) rather than ma-
jority voting, the agreement rises to 80.58%.
Figure 2 presents the Hinton diagram of the dis-
agreements of lay people. Disagreements are very
similar to the disagreements between expert an-
notators, especially on Twitter data (Figure 1b).
508
a) b) c)
Figure 1: Hinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ (Marcus et al,
1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org
One difference is that lay people do not confuse
numerals very often, probably because they rely
more on orthographic cues than on distributional
evidence. The disagreements are still strongly cor-
related with the ones observed with expert anno-
tators, but at a slightly lower coefficient (with a
Spearman?s ? of 0.493 and Kendall?s ? of 0.366
for WSJ).
Figure 2: Disagreement between lay annotators
Lastly, we compare the disagreements of anno-
tators on a French social media data set (Seddah et
al., 2012), which we mapped to the universal POS
tag set. Again, we see the familiar dagger shape.
The Spearman coefficient with English Twitter is
0.288; Kendall?s ? is 0.204. While the correlation
is weaker across languages than across domains, it
remains statistically significant (p < 0.001).
3 Hard cases and annotation errors
In the previous section, we demonstrated that
some disagreements are consistent across domains
and languages. We noted earlier, though, that dis-
agreements can arise both from hard cases and
from annotation errors. This can explain some
Figure 3: Disagreement on French social media
of the variation. In this section, we investigate
what happens if we weed out obvious errors by
detecting annotation inconsistencies across a cor-
pus. The disagreements that remain are the truly
hard cases.
We use a modified version of the a priori algo-
rithm introduced in Dickinson and Meurers (2003)
to identify annotation inconsistencies. It works
by collecting ?variation n-grams?, i.e. the longest
sequence of words (n-gram) in a corpus that has
been observed with a token being tagged differ-
ently in another occurence of the same n-gram in
the same corpus. The algorithm starts off by look-
ing for unigrams and expands them until no longer
n-grams are found.
For each variation n-gram that we found in
WSJ-00, i.e, a word in various contexts and the
possible tags associated with it, we present anno-
tators with the cross product of contexts and tags.
Essentially, we ask for a binary decision: Is the tag
plausible for the given context?
We used 3 annotators with PhD degrees in lin-
guistics. In total, our data set contains 880 items,
509
i.e. 440 annotated confusion tag pairs. The raw
agreement was 86%. Figure 4 shows how truly
hard cases are distributed over tag pairs (dark gray
bars), as well as the proportion of confusions with
respect to a given tag pair that are truly hard cases
(light gray bars). The figure shows, for instance,
that the variation n-gram regarding ADP-ADV is
the second most frequent one (dark gray), and
approximately 70% of ADP-ADV disagreements
are linguistically hard cases (light gray). NOUN-
PRON disagreements are always linguistically de-
batable cases, while they are less frequent.
Figure 4: Relative frequency of hard cases
A survey of hard cases. To further test the idea
of there being truly hard cases that probably can-
not be resolved by linguistic theory, we presented
nine linguistics faculty members with 10 of the
above examples and asked them to pick their fa-
vorite analyses. In 8/10 cases, the faculty mem-
bers disagreed on the right analysis.
4 Learning to detect annotation errors
In this section, we examine whether we can learn
a classifier to distinguish between hard cases and
annotation errors. In order to do so, we train a clas-
sifier on the annotated data set containing 440 tag-
confusion pairs by relying only on surface form
features. If we balance the data set and perform 3-
fold cross-validation, a L2-regularized logistic re-
gression (L2-LR) model achieves an f
1
-score for
detecting errors at 70% (cf. Table 1), which is
above average, but not very impressive.
The two classes are apparently not easily sepa-
rable using surface form features, as illustrated in
f
1
HARD CASES ERRORS
L2-LR 73%(71-77) 70%(65-75)
NN 76%(76-77) 71%(68-72)
Table 1: Classification results
Figure 5: Hard cases and errors in 2d-PCA
the two-dimensional plot in Figure 5, obtained us-
ing PCA. The logistic regression decision bound-
ary is plotted as a solid, black line. This is prob-
ably also why the nearest neighbor (NN) classi-
fier does slightly better, but again, performance is
rather low. While other features may reveal that
the problem is in fact learnable, our initial experi-
ments lead us to conclude that, given the low ratio
of errors over truly hard cases, learning to detect
errors is often not worthwhile.
5 Related work
Juergens (2014) presents work on detecting lin-
guistically hard cases in the context of word
sense annotations, e.g., cases where expert an-
notators will disagree, as well as differentiat-
ing between underspecified, overspecified and
metaphoric cases. This work is similar to ours in
spirit, but considers a very different task. While
we also quantify the proportion of hard cases and
present an analysis of these cases, we also show
that disagreements are systematic.
Our work also relates to work on automatically
correcting expert annotations for inconsistencies
(Dickinson and Meurers, 2003). This work is
very different in spirit from our work, but shares
an interest in reconsidering expert annotations,
and we made use of their mining algorithm here.
There has also been recent work on adjudicat-
510
ing noisy crowdsourced annotations (Dawid and
Skene, 1979; Smyth et al, 1995; Carpenter, 2008;
Whitehill et al, 2009; Welinder et al, 2010; Yan
et al, 2010; Raykar and Yu, 2012; Hovy et al,
2013). Again, their objective is orthogonal to
ours, namely to collapse multiple annotations into
a gold standard rather than embracing disagree-
ments.
Finally, Plank et al (2014) use small samples of
doubly-annotated POS data to estimate annotator
reliability and show how those metrics can be im-
plemented in the loss function when inducing POS
taggers to reflect confidence we can put in annota-
tions. They show that not biasing the theory to-
wards a single annotator but using a cost-sensitive
learning scheme makes POS taggers more robust
and more applicable for downstream tasks.
6 Conclusion
In this paper, we show that disagreements between
professional or lay annotators are systematic and
consistent across domains and some of them are
systematic also across languages. In addition, we
present an empirical analysis of POS annotations
showing that the vast majority of inter-annotator
disagreements are competing, but valid, linguis-
tic interpretations. We propose to embrace such
disagreements rather than using annotation guide-
lines to optimize inter-annotator agreement, which
would bias our models in favor of some linguistic
theory.
Acknowledgements
We would like to thank the anonymous reviewers
for their feedback, as well as Djam?e Seddah for the
French data. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20?28.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing errors in part-of-speech annotation. In EACL.
Cecily Duffield, Jena Hwang, Susan Brown, Dmitriy
Dligach, Sarah Vieweg, Jenny Davis, and Martha
Palmer. 2007. Criteria for the manual grouping of
verb senses. In LAW.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
Experiments with crowdsourced re-annotation of a
POS tagging data set. In ACL.
David Juergens. 2014. An analysis of ambiguity in
word sense annotations. In LREC.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491?518.
Djam?e Seddah, Benoit Sagot, Marie Candito, Virginie
Mouilleron, and Vanessa Combet. 2012. The
French Social Media Bank: a treebank of noisy user
generated content. In COLING.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. In NIPS.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In NIPS.
Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In AIStats.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
511
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1?11,
Dublin, Ireland, August 23-24 2014.
More or less supervised supersense tagging of Twitter
Anders Johannsen, Dirk Hovy, H
?
ector Mart??nez Alonso, Barbara Plank, Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140
ajohannsen@hum.ku.dk, dirk@cst.dk, alonso@hum.ku.dk
plank@cst.dk, soegaard@hum.ku.dk
Abstract
We present two Twitter datasets annotated
with coarse-grained word senses (super-
senses), as well as a series of experiments
with three learning scenarios for super-
sense tagging: weakly supervised learn-
ing, as well as unsupervised and super-
vised domain adaptation. We show that
(a) off-the-shelf tools perform poorly on
Twitter, (b) models augmented with em-
beddings learned from Twitter data per-
form much better, and (c) errors can be
reduced using type-constrained inference
with distant supervision from WordNet.
1 Introduction
Supersense tagging (SST, Ciaramita and Altun,
2006) is the task of assigning high-level ontolog-
ical classes to open-class words (here, nouns and
verbs). It is thus a coarse-grained word sense dis-
ambiguation task. The labels are based on the lexi-
cographer file names for Princeton WordNet (Fell-
baum, 1998). They include 15 senses for verbs
and 26 for nouns (see Table 1). While WordNet
also provides catch-all supersenses for adjectives
and adverbs, these are grammatically, not seman-
tically motivated, and do not provide any higher-
level abstraction (recently, however, Tsvetkov et
al. (2014) proposed a semantic taxonomy for ad-
jectives). They will not be considered in this paper.
Coarse-grained categories such as supersenses
are useful for downstream tasks such as question-
answering (QA) and open relation extraction (RE).
SST is different from NER in that it has a larger set
of labels and in the absence of strong orthographic
cues (capitalization, quotation marks, etc.). More-
over, supersenses can be applied to any of the lex-
ical parts of speech and not only proper names.
Also, while high-coverage gazetteers can be found
for named entity recognition, the lexical resources
available for SST are very limited in coverage.
Twitter is a popular micro-blogging service,
which, among other things, is used for knowledge
sharing among friends and peers. Twitter posts
(tweets) announce local events, say talks or con-
certs, present facts about pop stars or program-
ming languages, or simply express the opinions of
the author on some subject matter.
Supersense tagging is relevant for Twitter, be-
cause it can aid e.g. QA and open RE. If someone
posts a message saying that some LaTeX module
now supports ?drawing trees?, it is important to
know whether the post is about drawing natural
objects such as oaks or pines, or about drawing
tree-shaped data representations.
This paper is, to the best of our knowledge, the
first work to address the problem of SST for Twit-
ter. While there exist corpora of newswire and
literary texts that are annotated with supersenses,
e.g., SEMCOR (Miller et al., 1994), no data is
available for microblogs or related domains. This
paper introduces two new data sets.
Furthermore, most, if not all, of previous work
on SST has relied on gold standard part-of-speech
(POS) tags as input. However, in a domain such
as Twitter, which has proven to be challenging
for POS tagging (Foster et al., 2011; Ritter et
al., 2011), results obtained under the assumption
of available perfect POS information are almost
meaningless for any real-life application.
In this paper, we instead use predicted POS tags
and investigate experimental settings in which one
or more of the following resources are available to
us:
? a large corpus of unlabeled Twitter data;
? Princeton WordNet (Fellbaum, 1998);
? SEMCOR (Miller et al., 1994); and
? a small corpus of Twitter data annotated with
supersenses.
We approach SST of Twitter using various de-
grees of supervision for both learning and domain
adaptation (here, from newswire to Twitter). In
1
weakly supervised learning, only unlabeled data
and the lexical resource WordNet are available to
us. While the quality of lexical resources varies,
this is the scenario for most languages. We present
an approach to weakly supervised SST based on
type-constrained EM-trained second-order HMMs
(HMM2s) with continuous word representations.
In contrast, when using supervised learning, we
can distinguish between two degrees of supervi-
sion for domain adaptation. For some languages,
e.g., Basque, English, Swedish, sense-annotated
resources exist, but these corpora are all limited
to newswire or similar domains. In such lan-
guages, unsupervised domain adaptation (DA)
techniques can be used to exploit these resources.
The setting does not presume labeled data from
the target domain. We use discriminative mod-
els for unsupervised domain adaptation, training
on SEMCOR and testing on Twitter.
Finally, we annotated data sets for Twitter, mak-
ing supervised domain adaptation (SU) exper-
iments possible. For supervised domain adapta-
tion, we use the annotated training data sets from
both the newswire and the Twitter domain, as well
as WordNet.
For both unsupervised domain adaptation and
supervised domain adaptation, we use structured
perceptron (Collins, 2002), i.e., a discriminative
HMM model, and search-based structured predic-
tion (SEARN) (Daume et al., 2009). We aug-
ment both the EM-trained HMM2, discrimina-
tive HMMs and SEARN with type constraints and
continuous word representations. We also exper-
imented with conditional random fields (Lafferty
et al., 2001), but obtained worse or similar results
than with the other models.
Contributions In this paper, we present two
Twitter data sets with manually annotated su-
persenses, as well as a series of experiments
with these data sets. These experiments cover
existing approaches to related tasks, as well as
some new methods. In particular, we present
type-constrained extensions of discriminative
HMMs and SEARN sequence models with con-
tinuous word representations that perform well.
We show that when no in-domain labeled data
is available, type constraints improve model
performance considerably. Our best models
achieve a weighted average F1 score of 57.1 over
nouns and verbs on our main evaluation data
set, i.e., a 20% error reduction over the most
frequent sense baseline. The two annotated Twit-
ter data sets are publicly released for download
at https://github.com/coastalcph/
supersense-data-twitter.
n.Tops n.object v.cognition
n.act n.person v.communication
n.animal n.phenomenon v.competition
n.artifact n.plant v.consumption
n.attribute n.possession v.contact
n.body n.process v.creation
n.cognition n.quantity v.emotion
n.communication n.relation v.motion
n.event n.shape v.perception
n.feeling n.state v.possession
n.food n.substance v.social
n.group n.time v.stative
n.location v.body v.weather
n.motive v.change
Table 1: The 41 noun and verb supersenses in
WordNet
2 More or less supervised models
This sections covers the varying degree of super-
vision of our systems as well as the usage of type
constraints as distant supervision.
2.1 Distant supervision
Distant supervision in these experiments was im-
plemented by only allowing a system to predict
a certain supersense for a given word if that su-
persense had either been observed in the training
data, or, for unobserved words, if the sense was
the most frequent sense in WordNet. If the word
did not appear in the training data nor in WordNet,
no filtering was applied. We refer to the distant-
supervision strategy as type constraints.
Distant supervision was implemented differ-
ently in SEARN and the HMM model. SEARN
decomposes sequential labelling into a series of
binary classifications. To constrain the labels we
simply pick the top-scoring sense for each token
from the allowed set. Structured perceptron uses
Viterbi decoding. Here we set the emission prob-
abilities for disallowed senses to negative infinity
and decode as usual.
2.2 Weakly supervised HMMs
The HMM2 model is a second-order hidden
Markov model (Mari et al., 1997; Thede and
Harper, 1999) using logistic regression to estimate
emission probabilities. In addition we constrain
2
w1
t
1
t
2
P(t
2
|t
1
)
P(w
1
|t
1
)
t
3
w
2
w
3
Figure 1: HMM2 with continuous word represen-
tations
the inference space of the HMM2 tagger using
type-level tag constraints derived from WordNet,
leading to roughly the model proposed by Li et
al. (2012), who used Wiktionary as a (part-of-
speech) tag dictionary. The basic feature model
of Li et al. (2012) is augmented with continuous
word representation features as shown in Figure 1,
and our logistic regression model thus works over
a combination of discrete and continuous variables
when estimating emission probabilities. We do 50
passes over the data as in Li et al. (2012).
We introduce two simplifications for the HMM2
model. First, we only use the most frequent senses
(k = 1) in WordNet as type constraints. The
most frequent senses seem to better direct the EM
search for a local optimum, and we see dramatic
drops in performance on held-out data when we
include more senses for the words covered by
WordNet. Second, motivated by computational
concerns, we only train and test on sequences of
(predicted) nouns and verbs, leaving out all other
word classes. Our supervised models performed
slightly worse on shortened sequences, and it is an
open question whether the HMM2 models would
perform better if we could train them on full sen-
tences.
2.3 Structured perceptron and SEARN
We use two approaches to supervised sequen-
tial labeling, structured perceptron (Collins, 2002)
and search-based structured prediction (SEARN)
(Daume et al., 2009). The structured perceptron
is a in-house reimplementation of Ciaramita and
Altun (2006).
1
SEARN performed slightly better
than structured perceptron, so we use it as our in-
house baseline in the experiments below. In this
section, we briefly explain the two approaches.
1
https://github.com/coastalcph/
rungsted
2.3.1 Structured perceptron (HMM)
Structured perceptron learning was introduced in
Collins (2002) and is an extension of the online
perceptron learning algorithm (Rosenblatt, 1958)
with averaging (Freund and Schapire, 1999) to
structured learning problems such as sequence la-
beling.
In structured perceptron for sequential labeling,
where we learn a function from sequences of data
points x
1
. . . x
n
to sequences of labels y
1
. . . y
n
,
we begin with a random weight vector w
0
initial-
ized to all zeros. This weight vector is used to
assign weights to transitions between labels, i.e.,
the discriminative counterpart of P (y
i+1
| y
i
), and
emissions of tokens given labels, i.e., the counter-
part of P (x
i
| y
i
). We use Viterbi decoding to de-
rive a best path
?
y through the correspondingm?n
lattice (with m the number of labels). Let the fea-
ture mapping ?(x,y) be a function from a pair
of sequences ?x,y? to all the features that fired
to make y the best path through the lattice for x.
Now the structured update for a sequence of data
points is simply ?(?(x,y)??(x,
?
y)), i.e., a fixed
positive update of features that fired to produce the
correct sequence of labels, and a fixed negative up-
date of features that fired to produce the best path
under the model. Note that if y =
?
y, no features
are updated.
2.3.2 SEARN
SEARN is a way of decomposing structured pre-
diction problems into search and history-based
classification. In sequential labeling, we decom-
pose the sequence of m tokens into m classifica-
tion problems, conditioning our labeling of the ith
token on the history of i ? 1 previous decisions.
The cost of a mislabeling at training time is de-
fined by a cost function over output structures. We
use Hamming loss rather than F
1
as our cost func-
tion, and we then use stochastic gradient descent
with quantile loss as a our cost-sensitive learning
algorithm. We use a publicly available implemen-
tation.
2
3 Experiments
We experiment with weakly supervised learning,
unsupervised domain adaptation, as well as su-
pervised domain adaptation, i.e., where our mod-
els are induced from hand-annotated newswire
and Twitter data. Note that in all our experiments,
2
http://hunch.net/
?
vw/
3
we use predicted POS tags as input to the system,
in order to produce a realistic estimate of SST per-
formance.
3.1 Data
Our experiments rely on combinations of available
resources and newly annotated Twitter data sets
made publicly available with this paper.
3.1.1 Available resources
Princeton WordNet (Fellbaum, 1998) is the main
resource for SST. The lexicographer file names
provide the label alphabet of the task, and the tax-
onomy defined therein is used not only in the base-
lines, but also as a feature in the discriminative
models. We use the WordNet 3.0 distribution.
SEMCOR (Miller et al., 1994) is a sense-
annotated corpus composed of 80% newswire and
20% literary text, using the sense inventory from
WordNet. SEMCOR comprises 23k distinct lem-
mas in 234k instances. We use the texts which
have full annotations, leaving aside the verb-only
texts (see Section 6).
We use a distributional semantic model in order
to incorporate distributional information as fea-
tures in our system. In particular, we use the
neural-network based models from (Mikolov et
al., 2013), also referred as word embeddings. This
model makes use of skip-grams (n-grams that do
not need to be consecutive) within a word window
to calculate continuous-valued vector representa-
tions from a recurrent neural network. These dis-
tributional models have been able to outperform
state of the art in the SemEval-2012 Task 2 (Mea-
suring degrees of relational similarity). We calcu-
late the embeddings from an in-house corpus of
57m English tweets using a window size 5 and
yielding vectors of 100 dimensions.
We also use the first 20k tweets of the 57m
tweets to train our HMM2 models.
3.1.2 Annotation
While an annotated newswire corpus and a high-
quality lexical resource already enable us to train,
we also need at least a small sample of anno-
tated tweets data to evaluate SST for Twitter. Fur-
thermore, if we want to experiment with super-
vised SST, we also need sufficient annotated Twit-
ter data to learn the distribution of sense tags.
This paper presents two data sets: (a) super-
sense annotations for the POS+NER-annotated
data set described in Ritter et al. (2011), which we
use for training, development and evaluation, us-
ing the splits proposed in Derczynski et al. (2013),
and (b) supersense annotations for a sample of 200
tweets, which we use for additional, out-of-sample
evaluation. We call these data sets RITTER-
{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, re-
spectively. The IN-HOUSE-EVAL dataset was
downloaded in 2013 and is a sample of tweets that
contain links to external homepages but are other-
wise unbiased. It was previously used (with part-
of-speech annotation) in (Plank et al., 2014). Both
data sets are made publicly available with this pa-
per.
Supersenses are annotated with in spans defined
by the BIO (Begin-Inside-Other) notation. To ob-
tain the Twitter data sets, we carried out an an-
notation task. We first pre-annotated all data sets
with WordNet?s most frequent senses. If the word
was not in WordNet and a noun, we assigned it the
sense n.person. All other words were labeled O.
Chains of nouns were altered to give every ele-
ment the sense of the head noun, and the BI tags
adjusted, i.e.:
Empire/B-n.loc State/B-n.loc Building/B-n.artifact
was changed to
Empire/B-n.artifact State/I-n.artifact Building/I-
n.artifact
For the RITTER data, three paid student an-
notators worked on different subsets of the pre-
annotated data. They were asked to correct mis-
takes in both the BIO notation and the assigned
supersenses. They were free to chose from the full
label set, regardless of the pre-annotation. While
the three annotators worked on separate parts, they
overlapped on a small part of RITTER-TRAIN (841
tokens). On this subset, we computed agreement
scores and annotation difficulties. The average
raw agreement was 0.86 and Cohen?s ? 0.77. The
majority of tokens received the O label by all an-
notators; this happended in 515 out of 841 cases.
Excluding these instances to evaluate the perfor-
mance on the more difficult content words, raw
agreement dropped to 0.69 and Cohen?s ? to 0.69.
The IN-HOUSE-EVAL data set was annotated
by two different annotators, namely two of the au-
thors of this article. Again, for efficiency reasons
they worked on different subsets of the data, with
an overlapping portion. Their average raw agree-
ment was 0.65 and their Cohen?s ? 0.62. For this
data set, we also compute F
1
, defined as usual as
the harmonic mean of recall and precision. To
4
compute this, we set one of the annotators as gold
data and the other as predicted data. However,
since F
1
is symmetrical, the order does not mat-
ter. The annotation F
1
gives us another estimate
of annotation difficulty. We present the figures in
Table 3.
3.2 Baselines
For most word sense disambiguation studies, pre-
dicting the most frequent sense (MFS) of a word
has been proven to be a strong baseline. Follow-
ing this, our MFS baseline simply predicts the su-
persense of the most frequent WordNet sense for
a tuple of a word and a part of speech. We use
the part of speech predicted by the LAPOS tagger
(Tsuruoka et al., 2011). Any word not in Word-
Net is labeled as noun.person, which is the most
frequent sense overall in the training data. After
tagging, we run a script to correct the BI tag pre-
fixes, as described above for the annotation ask.
We also compare to the performance of exist-
ing SST systems. In particular we use Sense-
Learner (Mihalcea and Csomai, 2005) as a base-
line, which produces estimates of the WordNet
sense for each word. For these predictions, we
retrieve the corresponding supersense. Finally,
we use a publicly available reimplementation of
Ciaramita and Altun (2006) by Michael Heilman,
which reaches comparable performance on gold-
tagged SEMCOR.
3
3.3 Model parameters
We use the feature model of Paa? and Reichartz
(2009) in all our models, except the weakly su-
pervised models. For the structured perceptron we
set the number of passes over the training data on
the held-out development data. The weakly super-
vised models use the default setting proposed in
Li et al. (2012). We have used the standard online
setup for SEARN, which only takes one pass over
the data.
The type of embedding is the same in all our
experiments. For a given word the embedding fea-
ture is a 100 dimensional vector, which combines
the embedding of the word with the embedding of
adjacent words. The feature combination f
e
for a
word w
t
is calculated as:
f
e
(w
t
) =
1
2
(e(w
t?1
) + e(w
t+1
))? 2e(w
t
),
3
http://www.ark.cs.cmu.edu/mheilman/
questions/SupersenseTagger-10-01-12.tar.
gz
where the factor of two is chosen heurestically to
give more weight to the current word.
We also set a parameter k on development data
for using the k-most frequent senses inWordNet
as type constraints. Our supervised models are
trained on SEMCOR+RITTER-TRAIN or simply
RITTER-TRAIN, depending on what gave us the
best performance on the held-out data.
4 Results
The results are presented in Table 2. We dis-
tinguish between three settings with various de-
grees of supervision: weakly supervised, which
uses no domain annotated information, but solely
relies on embeddings trained on unlabeled Twit-
ter data; unsupervised domain adaptation (DA),
which uses SemCor for supervised training; and
supervised domain adaptation (SU), which uses
annotated Twitter data in addition to the SemCor
data for training.
In each of the two domain adaptation settings,
SEARN and HMM are evaluated with type con-
straints as distant supervision, and without for
comparison. SEARN without embeddings or dis-
tant supervision serves as an in-house baseline.
In Table 3 we present the WordNet token cov-
erage of predicted nouns and verbs in the devel-
opment and evaluation data, as well as the inter-
annotator agreement F
1
scores.
All the results presented in Table 2 are
(weighted averaged) F
1
measures obtained on pre-
dicted POS tags. Note that these results are con-
siderably lower than results on supersense tagging
newswire (up to 80 F
1
) that assume gold standard
POS tags (Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009).
The re-implementation of the state-of-the-art
system improves slightly upon the most frequent
sense baseline. SenseLearner does not seem to
capture the relevant information and does not
reach baseline performance. In other words, there
is no off-the-shelf tool for supersense tagging of
Twitter that does much better than assigning the
most frequent sense to predicted nouns and verbs.
Our weakly supervised model performs worse
than the most frequent sense baseline. This is a
negative result. It is, however, well-known from
the word sense disambiguation literature that the
MFS is a very strong baseline. Moreover, the EM
learning problem is hard because of the large la-
bel set and weak distributional evidence for super-
5
RITTER IN-HOUSE
DEV EVAL EVAL
Wordnet noun-verb
token coverage 83.72 70.22 41.18
Inter-annotator
agreement (F1) 81.01 69.15 61.57
Table 3: Properties of dataset.
senses.
The unsupervised domain adaptation and fully
supervised systems perform considerably better
than this baseline across the board. In the unsuper-
vised domain adaptation setup, we see huge im-
provements from using type constraints as distant
supervision. In the supervised setup, we only see
significant improvements adding type constraints
for the structured perceptron (HMM), but not for
search-based structured prediction (SEARN).
For all the data sets, there is still a gap between
model performance and human inter-annotator
agreement levels (see Table 3), leaving some room
for improvements. We hope that the release of the
data sets will help further research into this.
4.1 Coarse-grained evaluation
We also experimented with the more coarse-
grained classes proposed by Yuret and Yatbaz
(2010). Here our best model obtained an F
1
score
for mental concepts (nouns) of 72.3%, and 62.6%
for physical concepts, on RITTER-DEV. The over-
all F
1
score for verbs is 85.6%. The overall F
1
is
75.5%. Note that this result is not directly com-
parable to the figure (72.9%) reported in Yuret
and Yatbaz (2010), since they use different data
sets, exclude verbs and make different assump-
tions, e.g., relying on gold POS tags.
5 Error analysis
We have seen that inter-annotator agreements on
supersense annotation are reliable at above .60
but far from perfect. The Hinton diagram in Ta-
ble 2 presents the confusion matrix between our
annotators on IN-HOUSE-EVAL.
Errors in the prediction primarily stem from
two sources: out-of-vocabulary words and incor-
rect POS tags. Figure 3 shows the distribution of
senses over the words that were not contained in
either the training data, WordNet, or the Twitter
data used to learn the embeddings. The distribu-
tion follows a power law, with the most frequent
sense being noun.person, followed by noun.group,
and noun.artifact. The first two are related to NER
categories, namely PER and ORG, and can be ex-
pected, since Twitter users frequently talk about
new actors, musicians, and bands. Nouns of com-
munication are largely related to films, but also in-
clude Twitter, Facebook, and other forms of social
media. Note that verbs occur only towards the tail
end of the distribution, i.e., there are very few un-
known verbs, even in Twitter.
Overall, our models perform best on labels with
low lexical variability, such as quantities, states
and times for nouns, as well as consumption, pos-
session and stative for verbs. This is unsurprising,
since these classes have lower out-of-vocabulary
rates.
With regards to the differences between source
(SEMCOR) and target (Twitter) domains, we ob-
serve that the distribution of supersenses is al-
ways headed by the same noun categories like
noun.person or noun.group, but the frequency of
out-of-vocabulary stative verbs plummets in the
target domain, as some semantic types are more
closed class than others. There are for instance
fewer possibilities for creating new time units
(noun.time) or stative verbs like be than people or
company names (noun.person or noun.group, re-
spectively).
The weakly supervised model HMM2 has
higher precision (57% on RITTER-DEV) than re-
call (48.7%), which means that it often predicts
words to not belong to a semantic class. This
suggests an alternative strategy, which is to train
a model on sequences of purely non-O instances.
This would force the model to only predict O on
words that do not appear in the reduced sequences.
One important source of error seems to be un-
reliable part-of-speech tagging. In particular we
predict the wrong POS for 20-35% of the verbs
across the data sets, and for 4-6.5% of the nouns.
In the SEMCOR data, for comparability, we have
wrongly predicted tags for 6-8% of the anno-
tated tokens. Nevertheless, the error propaga-
tion of wrongly predicted nouns and verbs is par-
tially compensated by our systems, since they are
trained on imperfect input, and thus it becomes
possible for the systems to predict a noun super-
sense for a verb and viceversa. In our data we have
found e.g. that the noun Thanksgiving was incor-
rectly tagged as a verb, but its supersense was cor-
rectly predicted to be noun.time, and that the verb
guess had been mistagged as noun but the system
6
Resources Results
Token-level Type-level RITTER IN-HOUSE
SemCor Twitter Embeddings Type constraints DEV EVAL EVAL
General baselines
MFS - - - + 47.54 44.98 38.65
SENSELEARNER + - - - 14.61 26.24 22.81
HEILMAN + - - - 48.96 45.03 39.65
Weakly supervised systems
HMM2 - - - + 47.09 42.12 26.99
Unsupervised domain adaptation systems (DA)
SEARN (Baseline) + - - - 48.31 42.34 34.30
SEARN + - + - 52.45 48.30 40.22
SEARN + - + + 56.59 50.89 40.50
HMM + - + - 52.40 47.90 40.51
HMM + - + + 57.14 50.98 41.84
Supervised domain adaptation systems (SU)
SEARN (Baseline) + + - - 58.30 52.12 36.86
SEARN + + + - 63.05 57.09 42.37
SEARN + + + + 62.72 57.14 42.42
HMM + + + - 57.20 49.26 39.88
HMM + + + + 60.66 51.40 41.60
Table 2: Weighted F1 average over 41 supersenses.
7
Figure 2: Inter-annotator confusion matrix on TWITTER-EVAL.
0
0.1
0.2
0.3
0.4
noun.
person noun.
group
noun.
artifac
t
noun.
comm
unicat
ion
noun.
event
noun.
locatio
n
noun.
time noun.
act
noun.
food
noun.
attribu
te
noun.
relatio
n
verb.c
ogniti
on
verb.c
reatio
n
verb.e
motio
n
verb.m
otion
verb.p
ercept
ion
verb.s
tative
Figure 3: Sense distribution of OOV words.
8
still predicted the correct verb.cognition as super-
sense.
6 Related Work
There has been relatively little previous work on
supersense tagging, and to the best of our knowl-
edge, all of it has been limited to English newswire
and literature (SEMCOR and SENSEVAL).
The task of supersense tagging was first intro-
duced by Ciaramita and Altun (2006), who used
a structured perceptron trained and evaluated on
SEMCOR via 5-fold cross validation. Their eval-
uation included a held-out development set on
each fold that was used to estimate the number of
epochs. They used additional training data con-
taining only verbs. More importantly, they relied
on gold standard POS tags. Their overall F
1
score
on SEMCOR was 77.1. Reichartz and Paa? (Re-
ichartz and Paa?, 2008; Paa? and Reichartz, 2009)
extended this work, using a CRF model as well
as LDA topic features. They report an F
1
score
of 80.2, again relying on gold standard POS fea-
tures. Our implementation follows their setup and
feature model, but we rely on predicted POS fea-
tures, not gold standard features.
Supersenses provide information similar to
higher-level distributional clusters, but more in-
terpretable, and have thus been used as high-
level features in various tasks, such as preposi-
tion sense disambiguation, noun compound inter-
pretation, and metaphor detection (Ye and Bald-
win, 2007; Tratz and Hovy, 2010; Tsvetkov et al.,
2013). Princeton WordNet only provides a fully
developed taxonomy of supersenses for verbs and
nouns, but Tsvetkov et al. (2014) have recently
proposed an extension of the taxonomy to cover
adjectives. Outside of English, supersenses have
been annotated for Arabic Wikipedia articles by
Schneider et al. (2012).
In addition, a few researchers have tried to
solve coarse-grained word sense disambiguation
problems that are very similar to supersense tag-
ging. Kohomban and Lee (2005) and Kohom-
ban and Lee (2007) also propose to use lexicogra-
pher file identifers from Princeton WordNet senses
(supersenses) and, in addition, discuss how to re-
trieve fine-grained senses from those predictions.
They evaluate their model on all-words data from
SENSEEVAL-2 and SENSEEVAL-3. They use a
classification approach rather than structured pre-
diction.
Yuret and Yatbaz (2010) present a weakly unsu-
pervised approach to this problem, still evaluating
on SENSEVAL-2 and SENSEVAL-3. They focus
only on nouns, relying on gold part-of-speech, but
also experiment with a coarse-grained mapping,
using only three high level classes.
For Twitter, we are aware of little previous work
on word sense disambiguation. Gella et al. (2014)
present lexical sample word sense disambiguation
annotation of 20 target nouns on Twitter, but no
experimental results with this data. There has also
been related work on disambiguation to Wikipedia
for Twitter (Cassidy et al., 2012).
In sum, existing work on supersense tagging
and coarse-grained word sense disambiguation for
English has to the best of our knowledge all fo-
cused on newswire and literature. Moreover, they
all rely on gold standard POS information, making
previous performance estimates rather optimistic.
7 Conclusion
In this paper, we present two Twitter data sets with
manually annotated supersenses, as well as a se-
ries of experiments with these data sets. The data
is publicly available for download.
In this article we have provided, to the best
of our knowledge, the first supersense tagger for
Twitter. We have shown that off-the-shelf tools
perform poorly on Twitter, and we offer two
strategies?namely distant supervision and the us-
age of embeddings as features?that can be com-
bined to improve SST for Twitter.
We propose that distant supervision imple-
mented as type constraints during decoding is a
viable method to limit the mispredictions of su-
persenses by our systems, thereby enforcing pre-
dicted senses that a word has in WordNet. This ap-
proach compensates for the size limitations of the
training data and mitigates the out-of-vocabulary
effect, but is still subject to the coverage of Word-
Net; which is far from perfect for words coming
from high-variability sources such as Twitter.
Using distributional semantics as features in
form of word embeddings also improves the pre-
diction of supersenses, because it provides seman-
tic information for words, regardless of whether
they have been observed the training data. This
method does not require a hand-created knowl-
edge base like WordNet, and is a promising tech-
nique for domain adaptation of supersense tag-
ging.
9
References
Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-
biaga, and Hongzhao Huang. 2012. Analysis and
enhancement of wikification for microblogs with
context expansion. In COLING, volume 12, pages
441?456.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602, Sydney, Australia,
July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Hal Daume, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, pages 297?325.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press USA.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Spandana Gella, Paul Cook, and Timothy Baldwin.
2014. One sense per tweeter and other lexical se-
mantic tales of Twitter. In EACL.
Upali Kohomban and Wee Lee. 2005. Learning se-
mantic classes for word sense disambiguation. In
ACL.
Upali Kohomban and Wee Lee. 2007. Optimizing
classifier performance in word sense disambiguation
by redefining word sense classes. In IJCAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jean-Francois Mari, Jean-Paul Haton, and Abdelaziz
Kriouile. 1997. Automatic word recognition based
on second-order hidden Markov models. IEEE
Transactions on Speech and Audio Processing,
5(1):22?25.
Rada Mihalcea and Andras Csomai. 2005. Sense-
learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the ACL 2005
on Interactive poster and demonstration sessions,
pages 53?56. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994.
Using a semantic concordance for sense identifica-
tion. In Proceedings of the workshop on Human
Language Technology, pages 240?243. Association
for Computational Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploit-
ing semantic constraints for estimating supersenses
with CRFs. In Proc. of the Ninth SIAM Interna-
tional Conference on Data Mining, pages 485?496,
Sparks, Nevada, May.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of EACL.
Frank Reichartz and Gerhard Paa?. 2008. Estimating
Supersenses with Conditional Random Fields. In
Proceedings of ECMLPKDD.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In EMNLP.
Frank Rosenblatt. 1958. The perceptron: a probabilis-
tic model for information storage and organization
in the brain. Psychological Review, 65(6):386?408.
Nathan Schneider, Behrang Mohit, Kemal Oflazer, and
Noah A Smith. 2012. Coarse lexical semantic an-
notation with supersenses: an arabic case study. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 253?
258. Association for Computational Linguistics.
Scott Thede and Mary Harper. 1999. A second-order
hidden Markov model for part-of-speech tagging. In
ACL.
Stephen Tratz and Eduard Hovy. 2010. Isi: automatic
classification of relations between nominals using a
maximum entropy classifier. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 222?225. Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
10
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection us-
ing common semantic features. Meta4NLP 2013,
page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting english adjective senses with super-
senses. In Proc. of LREC.
Patrick Ye and Timothy Baldwin. 2007. Melb-yb:
Preposition sense disambiguation using rich seman-
tic features. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 241?244.
Association for Computational Linguistics.
Deniz Yuret and Mehmet Yatbaz. 2010. The noisy
channel model for unsupervised word sense disam-
biguation. Computational Linguistics, 36:111?127.
11
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 213?217,
Dublin, Ireland, August 23-24, 2014.
Copenhagen-Malm
?
o: Tree Approximations of Semantic Parsing Problems
Natalie Schluter
?
, Jakob Elming, Sigrid Klerke, H
?
ector Mart??nez Alonso, Dirk Hovy
Barbara Plank, Anders Johannsen, and Anders S?gaard
?
Dpt. of Computer Science Center for Language Technology
Malm?o University University of Copenhagen
natalie.schluter@mah.se {zmk867,skl,alonso}@hum.ku.dk
{dirk,bplank}@cst.dk,
{ajohannsen,soegaard}@hum.ku.dk
Abstract
In this shared task paper for SemEval-
2014 Task 8, we show that most se-
mantic structures can be approximated by
trees through a series of almost bijective
graph transformations. We transform in-
put graphs, apply off-the-shelf methods
from syntactic parsing on the resulting
trees, and retrieve output graphs. Us-
ing tree approximations, we obtain good
results across three semantic formalisms,
with a 15.9% error reduction over a state-
of-the-art semantic role labeling system on
development data. Our system came in 3/6
in the shared task closed track.
1 Introduction
Semantic analyses often go beyond tree-
structured representations, assigning multiple se-
mantic heads to nodes, some semantic formalisms
even tolerating directed cycles.
1
At the same
time, syntactic parsing is a mature field with effi-
cient, highly optimised decoding and learning al-
gorithms for tree-structured representations. We
present tree approximation algorithms that in com-
bination with a state-of-the-art syntactic parser
achieve competitive performance in semantic di-
graph parsing.
We investigate two kinds of tree approximation
algorithms that we will refer to as pruning algo-
rithms and packing algorithms. Our pruning al-
gorithms simply remove and reverse edges until
the graph is a tree; edge reversals are then undone
as a postprocessing step. Our packing algorithms,
on the other hand, carry out two bijective graph
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
1
For example, HPSG predicate-argument structures (Pol-
lard and Sag, 1994).
transformations to pack structural information into
new edge labels, making it possible to reconstruct
most of the structural complexity as a postprocess-
ing step. Specifically, we present a packing al-
gorithm that consists of two fully bijective graph
transformations, in addition to a further transfor-
mation that incurs only a small information loss.
We carry out experiments across three seman-
tic annotations of the Wall Street Journal section
of the Penn Treebank (Marcus et al., 1993), cor-
responding to simplified versions of the semantic
formalisms minimal recursion semantics (MRS)
(Copestake et al., 2005), Enju-style predicate-
argument structures (Miyao and Tsujii, 2003), and
Prague-style tectogrammar semantics (B?ohmov?a
et al., 2003). We show that pruning and pack-
ing algorithms lead to state-of-the-art performance
across these semantic formalisms using an off-the-
shelf syntactic dependency parser.
2 Related work
Sagae and Tsujii (2008) present a pruning algo-
rithm in their paper on transition-based parsing of
directed acyclic graphs (DAGs), which discards
the edges of longest span entering nodes. They
apply the dependency parser described in Sagae
and Tsujii (2007) to the tree representations. We
note that this algorithm is not sufficient to produce
trees in our case, where the input graphs are not
necessarily acyclic. It does correspond roughly to
our LONGEST-EDGE baseline, which removes the
longest edge in cycles, in addition to flow reversal.
Sagae and Tsujii (2008) also present a shift-
reduce automaton approach to parsing DAGs. In
their paper, they report a labeled F1-score of
88.7% on the PAS dataset (see Section 3), while
we obtain 89.1%, however the results are thus not
directly comparable due to different data splits.
2
2
We obtained code to run this as a baseline, but were un-
able to, due to memory leaks, caused by subsets of our data,
and on the subsets of data that actually parsed, recall was very
213
The shared task organizers of the Broad-
coverage Semantic Dependency Parsing task at
SemEval-2014
3
also presented a pruning-based
baseline system. They eliminate re-entrancies in
the graph by removing dependencies to nodes with
multiple incoming edges. Of these edges, they
again keep the shortest. They incorporate all sin-
gleton nodes by attaching nodes to the immedi-
ately following node or to a virtual root - in case
the singleton is sentence-final. Finally, they inte-
grate fragments by subordinating remaining nodes
with in-degree 0 to the root node. They apply the
parser described in Bohnet (2010), also used be-
low, to the resulting trees. This system obtained
a labeled F1-score of 54.7% on the PAS dataset.
The performance of their pruning algorithm was
also considerably lower than our algorithms on the
other datasets considered below.
3 Tree approximations
This section describes two approaches to approxi-
mating graphs by trees, namely pruning and pack-
ing. Pruning optimizes the number of ?good?
edges in trees (Section 3.1), while packing trans-
forms graphs into trees by means of a pipeline of
operations which are 99.6% reversible (see Fig-
ure 1); that is, almost no information from the
original graphs is lost in the trees (Section 3.2).
Under both approaches, we first introduce arti-
ficial root nodes to the graphs and append them
to the word list. Graphs may initially be discon-
nected. We connect all weakly connected com-
ponents as follows. We first identify a most im-
portant node in each weakly connected compo-
nent, which we will refer to as the root. This root
is taken to be the first node with the ?top? fea-
ture from the data, if one exists. If none exists,
then the node with highest degree is chosen as the
?root?. (Note that the ?root? of each non-singleton
connected component is marked as a ?top? node
in the inverse transformation.) The root of each
non-singleton weakly connected component is at-
tached as a dependent of the artificial root node
with a special new label for the corresponding
edge. Also, each disconnected node is attached
as a dependent of the node to the right of it, with
a distinct special new label. It is these connected
graphs that we take to be the input in the following
low, suggesting that maybe the decoding algorithm was tuned
to a specific planarization of the complex graphs.
3
http://alt.qcri.org/semeval2014/task8/
two subsections describing our graph pruning and
packing algorithms.
3.1 Graph pruning
Our PRUNING algorithm removes a small number
of edges in the semantic graphs to be able to repre-
sent them as trees. The average edge counts from
the training data (see Section 4.1) indicate that the
potential edge loss in pruning is relatively small
(5.7% in the worst case). In this approach, two
transformations on the connected semantic graphs
are carried out: pruning and flow reversal.
Pruning. The input digraph may contain under-
lying undirected cycles. We break these cycles
by iteratively removing the longest edge from the
node with the fewest predecessors (lowest depth)
in the digraph. The resulting underlying undi-
rected graph is a tree.
Depth-first flow reversal. We then carry out
depth-first traversal of the resulting underlying
undirected tree, reversing the direction of edges
from the leaves upwards, as needed, until reach-
ing the root. Any reversed edge?s label is given a
special prefix, so that this reversal can be undone
in a post-processing step.
Following the above two transformations, we train
our parsers on the transformed semantic annota-
tions and output graphs such as the one in Fig-
ure 1a.
3.2 Graph packing
Our PACKING algorithm consists of a pipeline of
four graph transformations. The two major trans-
formations are for coordination and generalised
long-distance dependencies, being both parallel
path inducing constructions. The transformations
are both linguistically and topologically inspired
by the f-structure annotated c-structures in Lex-
ical Functional Grammar and f-structure parsing
via off-the-shelf dependency parsers (Schluter and
Van Genabith, 2009). We further ensure the defin-
ing tree property that every node is connected by a
unique path from the root, by carrying out flow re-
versal when necessary. Finally remaining parallel
paths are broken according to an heuristic on path
locality.
Coordination. In some semantic representa-
tions of coordination, individual conjunct nodes
may all dominate a same argument, or be domi-
nated by a same head. In both these cases, paral-
lel paths are generated. The same structures may
214
a)
b)
c)
Figure 1: Example of pruned (top), packed (middle), and original (bottom) semantic graph. (Sentence
22002004 from the PAS dataset.)
be represented if the head or arguments are ?fac-
tored out?. To do this, we remove all edges from
conjuncts towards a same argument (resp. from
a shared head to each conjunct), and introduce a
new edge from the root of the coordination sub-
tree towards this argument (resp. from a shared
head to the root of the coordination subtree). The
new edges receive a special prefix to facilitate ap-
plying the inverse transformation.
Breadth-first flow reversal. Unlike our pruning
algorithm, there is not yet any clear distinct path
from the root to the all nodes (as there are not
leaves yet). After carrying out the coordination
transformation, we carry out a breadth-first search
on the graph to direct flow away from the root, and
again, reversed edges? labels are given a special
prefix. As we do this, we test resulting nodes to
see if there are any parallel paths leading to them.
If so, these paths may be transformed immediately
according to the following transformation.
Generalized long-distance dependencies.
Long-distance dependencies are represented
in f-structure annotated c-structures by path
equations. This gives a tree representation of
parallel paths, at least one of which is exactly
one edge long. Given two parallel paths p
1
and
p
2
in the graph, where p
1
= (v
1
, l, v
n
) and p
2
=
(v
1
, l
1
, v
2
), (v
2
, l
2
, v
3
), . . . , (v
n?1
, l
n?1
, v
n
), we
remove the last edge of p
2
and augment p
1
?s label
with the representation l
1
: l
2
: ? ? ? : l
n?1
of p
2
. p
1
becomes (v
1
, l and l
1
: l
2
: ? ? ? : l
n?1
, v
n
), indi-
cating that v
n
is also the child (with dependency
label l
n?1
) of the node found by travelling (from
v
1
) down an l
1
labelled edge, followed by an l
2
labelled edge, and so on until the child of the l
n?2
labelled edge is found.
Maximum average locality heuristic. Follow-
ing these transformations, there may still be paral-
lel paths in the graph: those not parallel to a single
edge. We remove ?worst? re-entrant edges using
the simple heuristic that the path with the lowest
average edge span should be conserved entirely.
These removed edges clearly cannot be recovered
after transformation.
Our parsers are trained on the output graphs of
these four transformations such as the one in Fig-
ure 1b. We observe the main difference between
PRUNING and PACKING: coordination and long-
distance dependencies. For example, PACKING
keeps the edge between the conjunction and the
first conjunct, which is pruned away in PRUNING.
Such a difference provides a partial explanation
for the lower recall of PRUNING vis-`a-vis PACK-
ING (see Section 4.5).
4 Experiments
4.1 Data
The three datasets are semantic annotations of the
WSJ section of the Penn Treebank of English. The
average sentence length, which is also the aver-
age number of dependency edges in the tree ap-
proximations that we use to induce our semantic
parsers, is 22.93. The three semantic formalisms
are slightly richer, and the average number of
edges in the PAS-annotated treebank is 24.32. For
DM, the average number of edges is 23.77, and
for DM it is 23.33. While the pruning-based ap-
proaches thus suffers from a modest information
loss, throwing out 5.7% of the edges in the worst
215
case, this is not the case for packing. The re-
versibility of the packed representations is given
by the score upper bound in the last row in Ta-
ble 1. We use the dataset splits of the SemEval
2014 shared task.
4.2 Model
For both our pruning and packing models, we use
the Mate parser (Bohnet, 2010)
4
with default pa-
rameters to learn our parsing models. The Mate
parser is trained on the output of the transforma-
tion pipeline on Sections 00-19 of the three se-
mantically annotated WSJ datasets. Some models
use Brown clusters generated from Sections 00-
19 only. This does not solve OOV problems, but
allows of slightly better generalisation across dis-
tributionally similar words in the training data.
4.3 Baselines
We use the SemEval 2014 shared task baseline
(SIMPLE-PRUNE; see Section 2), as well as the
LONGEST-EDGE baseline, also mentioned above.
The latter is our strongest baseline system. It is
very similar to PRUNING, in doing both edge prun-
ing and flow reversal, but the pruning step only
removes the longest edge rather than considering
node depth. Our third baseline is the Mate seman-
tic role labeling learner (SRL-DEP) (Bj?orkelund
et al., 2009), which uses predicted syntactic parses
as input; for this, we use the syntactic parses made
available in the SemEval 2014 shared task for
replicability.
Approach Cl DM PAS PCEDT Av
Systems
PRUNING
NO 86.6 88.8 72.7 82.7
YES 86.9 89.1 72.5 82.8
PACKING
NO 85.8 88.7 71.8 82.1
YES 86.1 88.7 72.9 82.6
Baselines
SIMPLE-PRUNE 54.7 50.9 67.8 57.8
LONGEST-EDGE 83.8 88.9 66.1 79.6
SRL-DEP 79.5 82.4 70.1 77.4
Upper bound
PACKING 99.9 99.5 99.5 99.6
Table 1: Labelled F1-score results on development
data, with and without use of Brown clusters (Cl).
4.4 Results
The results are presented in Tables 1 through 3,
where the system evaluations for the SemEval task
are marked with asterisks in Table 2. We note that
all our approaches do considerably better than our
4
https://code.google.com/p/mate-tools/
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 84.8 87.7 71.2 81.2
(W/ TOP) REC 84.0 88.4 68.6 80.3
F1 84.4 88.0 69.9 80.8
?
PREC 85.4 87.9 70.8 81.4
(W/O TOP) REC 84.6 88.6 68.8 80.7
F1 85.0 88.3 69.9 81.1
PRUNING PREC 87.2 91.3 72.8 83.8
(W/ TOP) REC 80.2 81.3 62.8 74.8
F1 83.6 86.0 67.4 79.0
?
PREC 87.2 91.3 72.8 83.8
(W/O TOP) REC 85.1 85.1 68.0 79.4
F1 86.2 88.1 70.3 81.5
Table 2: Labelled results on test data, with and
without evaluation of top nodes. The scores with
asterisks correspond to the output evaluated in the
SemEval task.
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 86.8 89.1 84.8 86.9
(W/ TOP) REC 86.0 89.8 81.8 85.9
F1 86.4 89.4 83.2 86.3
PREC 87.5 89.4 85.4 87.4
(W/O TOP) REC 86.7 90.1 83.0 86.6
F1 87.1 89.7 84.2 87.0
PRUNING PREC 89.2 92.6 88.2 90.0
(W/ TOP) REC 82.0 82.5 76.1 80.2
F1 85.4 87.3 81.7 84.8
PREC 89.2 92.6 88.2 90.0
(W/O TOP) REC 87.1 86.3 82.4 85.3
F1 88.1 89.3 85.2 87.5
Table 3: Unlabelled results on test data, with and
without evaluation of top nodes.
three baselines. The error reduction of our best
system over the SRL system across all three for-
malisms is 24.2%, and the error reduction over
the more competitive pruning baseline LONGEST-
EDGE is 15.9%. As mentioned in Section 2, these
results seem to promise better performance than
current DAG parsing models. Note from the re-
sults in Table 2 that, as expected, PRUNING leads
to higher precision than PACKING at the expense
of recall.
4.5 Error Analysis
We observe that pruning leads to high precision,
while our packing algorithm gives us much bet-
ter recall. This is not surprising, since our packed
representations introduce new labels, making it
harder to generalize at training time. On the other
hand, pruning approaches suffer in recall, simply
because edges are thrown away in preprocessing
the data.
216
5 Conclusions
In this paper, we experimented with using tree ap-
proximation algorithms to reduce semantic struc-
tures to trees and use off-the-shelf structured pre-
diction techniques to train semantic parsers. Our
approximation algorithms include both pruning
and packing algorithms, i.e., algorithms that try
to reduce graphs to trees optimally, as well as al-
gorithms that pack information about graphs into
trees from which we later recover the richer struc-
tures. Using these tree approximation algorithms,
we obtain 15.9% error reductions over a state-of-
the-art SRL system.
References
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Proc.
of CoNLL: Shared Task, pages 43?48, Boulder, CO,
USA.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill?e,
editor, Treebanks: Building and Using Syntacti-
cally Annotated Corpora, pages 103?127. Kluwer,
Netherlands.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89?97, Beijing, China.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. Research
on Language and Computation, 3:281?332.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2003. Probabilis-
tic modeling of argument structures including non-
local dependencies. In Proc. of RANLP, pages 79?
85, Borovets, Bulgaria.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. University of Chicago Press.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proc. of CoNLL Shared
task session of EMNLP-CoNLL, pages 1044?1050,
Prague, Czech Republic.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proc. of COLING,
pages 753?760, Manchester, UK.
Natalie Schluter and Josef Van Genabith. 2009. De-
pendency parsing resources for French. In Proc. of
NODALIDA, pages 166?173, Odense, Denmark.
217
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 9?16
Manchester, August 2008
Exploring an Auxiliary Distribution based approach to
Domain Adaptation of a Syntactic Disambiguation Model
Barbara Plank
University of Groningen
The Netherlands
B.Plank@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
G.J.M.van.Noord@rug.nl
Abstract
We investigate auxiliary distribu-
tions (Johnson and Riezler, 2000) for
domain adaptation of a supervised parsing
system of Dutch. To overcome the limited
target domain training data, we exploit an
original and larger out-of-domain model
as auxiliary distribution. However, our
empirical results exhibit that the auxiliary
distribution does not help: even when very
little target training data is available the
incorporation of the out-of-domain model
does not contribute to parsing accuracy on
the target domain; instead, better results
are achieved either without adaptation or
by simple model combination.
1 Introduction
Modern statistical parsers are trained on large an-
notated corpora (treebanks) and their parameters
are estimated to reflect properties of the training
data. Therefore, a disambiguation component will
be successful as long as the treebank it was trained
on is representative for the input the model gets.
However, as soon as the model is applied to an-
other domain, or text genre (Lease et al, 2006),
accuracy degrades considerably. For example, the
performance of a parser trained on the Wall Street
Journal (newspaper text) significantly drops when
evaluated on the more varied Brown (fiction/non-
fiction) corpus (Gildea, 2001).
A simple solution to improve performance on
a new domain is to construct a parser specifically
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
for that domain. However, this amounts to hand-
labeling a considerable amount of training data
which is clearly very expensive and leads to an un-
satisfactory solution. In alternative, techniques for
domain adaptation, also known as parser adap-
tation (McClosky et al, 2006) or genre porta-
bility (Lease et al, 2006), try to leverage ei-
ther a small amount of already existing annotated
data (Hara et al, 2005) or unlabeled data (Mc-
Closky et al, 2006) of one domain to parse data
from a different domain. In this study we examine
an approach that assumes a limited amount of al-
ready annotated in-domain data.
We explore auxiliary distributions (Johnson and
Riezler, 2000) for domain adaptation, originally
suggested for the incorporation of lexical selec-
tional preferences into a parsing system. We gauge
the effect of exploiting a more general, out-of-
domain model for parser adaptation to overcome
the limited amount of in-domain training data. The
approach is examined on two application domains,
question answering and spoken data.
For the empirical trials, we use Alpino (van No-
ord and Malouf, 2005; van Noord, 2006), a ro-
bust computational analyzer for Dutch. Alpino
employs a discriminative approach to parse selec-
tion that bases its decision on a Maximum Entropy
(MaxEnt) model. Section 2 introduces the MaxEnt
framework. Section 3 describes our approach of
exploring auxiliary distributions for domain adap-
tation. In section 4 the experimental design and
empirical results are presented and discussed.
2 Background: MaxEnt Models
Maximum Entropy (MaxEnt) models are widely
used in Natural Language Processing (Berger et
al., 1996; Ratnaparkhi, 1997; Abney, 1997). In
this framework, a disambiguation model is speci-
9
fied by a set of feature functions describing prop-
erties of the data, together with their associated
weights. The weights are learned during the train-
ing procedure so that their estimated value deter-
mines the contribution of each feature. In the task
of parsing, features appearing in correct parses are
given increasing weight, while features in incorrect
parses are given decreasing weight. Once a model
is trained, it can be applied to parse selection that
chooses the parse with the highest sum of feature
weights.
During the training procedure, the weights vec-
tor is estimated to best fit the training data. In
more detail, given m features with their corre-
sponding empirical expectation E
p?
[f
j
] and a de-
fault model q
0
, we seek a model p that has mini-
mum Kullback-Leibler (KL) divergence from the
default model q
0
, subject to the expected-value
constraints: E
p
[f
j
] = E
p?
[f
j
], where j ? 1, ...,m.
In MaxEnt estimation, the default model q
0
is
often only implicit (Velldal and Oepen, 2005) and
not stated in the model equation, since the model
is assumed to be uniform (e.g. the constant func-
tion 1
?(s)
for sentence s, where ?(s) is the set of
parse trees associated with s). Thus, we seek the
model with minimum KL divergence from the uni-
form distribution, which means we search model
p with maximum entropy (uncertainty) subject to
given constraints (Abney, 1997).
In alternative, if q
0
is not uniform then p is
called a minimum divergence model (according
to (Berger and Printz, 1998)). In the statistical
parsing literature, the default model q
0
that can
be used to incorporate prior knowledge is also re-
ferred to as base model (Berger and Printz, 1998),
default or reference distribution (Hara et al, 2005;
Johnson et al, 1999; Velldal and Oepen, 2005).
The solution to the estimation problem of find-
ing distribution p, that satisfies the expected-
value constraints and minimally diverges from
q
0
, has been shown to take a specific parametric
form (Berger and Printz, 1998):
p
?
(?, s) =
1
Z
?
q
0
exp
P
m
j=1
?
j
f
j
(?) (1)
with m feature functions, s being the input sen-
tence, ? a corresponding parse tree, and Z
?
the
normalization equation:
Z
?
=
?
?
?
??
q
0
exp
P
m
j=1
?
j
f
j
(?
?
) (2)
Since the sum in equation 2 ranges over all pos-
sible parse trees ?? ? ? admitted by the gram-
mar, calculating the normalization constant ren-
ders the estimation process expensive or even in-
tractable (Johnson et al, 1999). To tackle this
problem, Johnson et al (1999) redefine the esti-
mation procedure by considering the conditional
rather than the joint probability.
P
?
(?|s) =
1
Z
?
q
0
exp
P
m
j=1
?
j
f
j
(?) (3)
with Z
?
as in equation 2, but instead, summing
over ?? ? ?(s), where ?(s) is the set of parse
trees associated with sentence s. Thus, the proba-
bility of a parse tree is estimated by summing only
over the possible parses of a specific sentence.
Still, calculating ?(s) is computationally very
expensive (Osborne, 2000), because the number of
parses is in the worst case exponential with respect
to sentence length. Therefore, Osborne (2000) pro-
poses a solution based on informative samples. He
shows that is suffices to train on an informative
subset of available training data to accurately es-
timate the model parameters. Alpino implements
the Osborne-style approach to Maximum Entropy
parsing. The standard version of the Alpino parser
is trained on the Alpino newspaper Treebank (van
Noord, 2006).
3 Exploring auxiliary distributions for
domain adaptation
3.1 Auxiliary distributions
Auxiliary distributions (Johnson and Riezler,
2000) offer the possibility to incorporate informa-
tion from additional sources into a MaxEnt Model.
In more detail, auxiliary distributions are inte-
grated by considering the logarithm of the proba-
bility given by an auxiliary distribution as an addi-
tional, real-valued feature. More formally, given k
auxiliary distributions Q
i
(?), then k new auxiliary
features f
m+1
, ..., f
m+k
are added such that
f
m+i
(?) = logQ
i
(?) (4)
where Q
i
(?) do not need to be proper probability
distributions, however they must strictly be posi-
tive ?? ? ? (Johnson and Riezler, 2000).
The auxiliary distributions resemble a reference
distribution, but instead of considering a single
reference distribution they have the advantage
that several auxiliary distributions can be inte-
grated and weighted against each other. John-
10
son establishes the following equivalence between
the two (Johnson and Riezler, 2000; Velldal and
Oepen, 2005):
Q(?) =
k
?
i=1
Q
i
(?)
?
m+i (5)
where Q(?) is the reference distribution and
Q
i
(?) is an auxiliary distribution. Hence, the con-
tribution of each auxiliary distribution is regulated
through the estimated feature weight. In general,
a model that includes k auxiliary features as given
in equation (4) takes the following form (Johnson
and Riezler, 2000):
P
?
(?|s) =
?
k
i=1
Q
i
(?)
?
m+i
Z
?
exp
P
m
j=1
?
j
f
j
(?) (6)
Due to the equivalence relation in equation (5)
we can restate the equation to explicitly show that
auxiliary distributions are additional features1.
P
?
(?|s)
=
Q
k
i=1
[exp
f
m+i(?)
]
?
m+i
Z
?
exp
P
m
j=1
?
j
f
j
(?)
(7)
=
1
Z
?
k
Y
i=1
exp
f
m+i(?)
??
m+i
exp
P
m
j=1
?
j
f
j
(?) (8)
=
1
Z
?
exp
P
k
i=1
f
m+i(?)
??
m+i
exp
P
m
j=1
?
j
f
j
(?)
(9)
=
1
Z
?
exp
P
m+k
j=1
?
j
f
j
(?)
with f
j
(?) = logQ(?) for m < j ? (m + k)
(10)
3.2 Auxiliary distributions for adaptation
While (Johnson and Riezler, 2000; van Noord,
2007) focus on incorporating several auxiliary dis-
tributions for lexical selectional preferences, in
this study we explore auxiliary distributions for do-
main adaptation.
We exploit the information of the more gen-
eral model, estimated from a larger, out-of-domain
treebank, for parsing data from a particular tar-
get domain, where only a small amount of train-
ing data is available. A related study is Hara
et al (2005). While they also assume a limited
amount of in-domain training data, their approach
1Note that the step from equation (6) to (7) holds by re-
stating equation (4) as Q
i
(?) = exp
f
m+i
(?)
differs from ours in that they incorporate an origi-
nal model as a reference distribution, and their es-
timation procedure is based on parse forests (Hara
et al, 2005; van Noord, 2006), rather than infor-
mative samples. In this study, we want to gauge
the effect of auxiliary distributions, which have the
advantage that the contribution of the additional
source is regulated.
More specifically, we extend the target model
to include (besides the original integer-valued fea-
tures) one additional real-valued feature (k=1)2.
Its value is defined to be the negative logarithm
of the conditional probability given by OUT , the
original, out-of-domain, Alpino model. Hence, the
general model is ?merged? into a single auxiliary
feature:
f
m+1
= ?logP
OUT
(?|s) (11)
The parameter of the new feature is estimated us-
ing the same estimation procedure as for the re-
maining model parameters. Intuitively, our auxil-
iary feature models dispreferences of the general
model for certain parse trees. When the Alpino
model assigns a high probability to a parse candi-
date, the auxiliary feature value will be small, close
to zero. In contrast, a low probability parse tree in
the general model gets a higher feature value. To-
gether with the estimated feature weight expected
to be negative, this has the effect that a low prob-
ability parse in the Alpino model will reduce the
probability of a parse in the target domain.
3.3 Model combination
In this section we sketch an alternative approach
where we keep only two features under the Max-
Ent framework: one is the log probability assigned
by the out-domain model, the other the log proba-
bility assigned by the in-domain model:
f
1
= ?logP
OUT
(?|s), f
2
= ?logP
IN
(?|s)
The contribution of each feature is again scaled
through the estimated feature weights ?
1
, ?
2
.
We can see this as a simple instantiation of model
combination. In alternative, data combination is
a domain adaptation method where IN and OUT-
domain data is simply concatenated and a new
model trained on the union of data. A potential and
well known disadvantage of data combination is
that the usually larger amount of out-domain data
2Or alternatively, k ? 1 (see section 4.3.1).
11
?overwhelms? the small amount of in-domain data.
Instead, Model combination interpolates the two
models in a linear fashion by scaling their contri-
bution. Note that if we skip the parameter esti-
mation step and simply assign the two parameters
equal values (equal weights), the method reduces
to P
OUT
(?|s) ? P
IN
(?|s), i.e. just multiplying
the respective model probabilities.
4 Experiments and Results
4.1 Experimental design
The general model is trained on the Alpino Tree-
bank (van Noord, 2006) (newspaper text; approx-
imately 7,000 sentences). For the domain-specific
corpora, in the first set of experiments (section 4.3)
we consider the Alpino CLEF Treebank (ques-
tions; approximately 1,800 sentences). In the sec-
ond part (section 4.4) we evaluate the approach
on the Spoken Dutch corpus (Oostdijk, 2000)
(CGN, ?Corpus Gesproken Nederlands?; spoken
data; size varies, ranging from 17 to 1,193 sen-
tences). The CGN corpus contains a variety of
components/subdomains to account for the various
dimensions of language use (Oostdijk, 2000).
4.2 Evaluation metric
The output of the parser is evaluated by comparing
the generated dependency structure for a corpus
sentence to the gold standard dependency structure
in a treebank. For this comparison, we represent
the dependency structure (a directed acyclic graph)
as a set of named dependency relations. To com-
pare such sets of dependency relations, we count
the number of dependencies that are identical in
the generated parse and the stored structure, which
is expressed traditionally using precision, recall
and f-score (Briscoe et al, 2002).
Let Di
p
be the number of dependencies produced
by the parser for sentence i, Di
g
is the number of
dependencies in the treebank parse, and Di
o
is the
number of correct dependencies produced by the
parser. If no superscript is used, we aggregate over
all sentences of the test set, i.e.,:
D
p
=
?
i
D
i
p
D
o
=
?
i
D
i
o
D
g
=
?
i
D
i
g
Precision is the total number of correct dependen-
cies returned by the parser, divided by the over-
all number of dependencies returned by the parser
(precision = D
o
/D
p
); recall is the number of
correct system dependencies divided by the total
number of dependencies in the treebank (recall =
D
o
/D
g
). As usual, precision and recall can be
combined in a single f-score metric.
An alternative similarity score for dependency
structures is based on the observation that for a
given sentence of n words, a parser would be ex-
pected to return n dependencies. In such cases,
we can simply use the percentage of correct de-
pendencies as a measure of accuracy. Such a la-
beled dependency accuracy is used, for instance,
in the CoNLL shared task on dependency parsing
(?labeled attachment score?).
Our evaluation metric is a variant of labeled
dependency accuracy, in which we do allow for
some discrepancy between the number of returned
dependencies. Such a discrepancy can occur,
for instance, because in the syntactic annotations
of Alpino (inherited from the CGN) words can
sometimes be dependent on more than a single
head (called ?secondary edges? in CGN). A fur-
ther cause is parsing failure, in which case a parser
might not produce any dependencies. We argue
elsewhere (van Noord, In preparation) that a metric
based on f-score can be misleading in such cases.
The resulting metric is called concept accuracy, in,
for instance, Boros et al (1996).3
CA = Do?
i
max(D
i
g
,D
i
p
)
The concept accuracy metric can be characterized
as the mean of a per-sentence minimum of recall
and precision. The resulting CA score therefore
is typically slightly lower than the corresponding
f-score, and, for the purposes of this paper, equiv-
alent to labeled dependency accuracy.
4.3 Experiments with the QA data
In the first set of experiments we focus on the
Question Answering (QA) domain (CLEF corpus).
Besides evaluating our auxiliary based approach
(section 3), we conduct separate baseline experi-
ments:
? In-domain (CLEF): train on CLEF (baseline)
? Out-domain (Alpino): train on Alpino
? Data Combination (CLEF+Alpino): train a model on
the combination of data, CLEF ? Alpino
3In previous publications and implementations defini-
tions were sometimes used that are equivalent to: CA =
D
o
max(D
g
,D
p
)
which is slightly different; in practice the dif-
ferences can be ignored.
12
Dataset In-dom. Out-dom. Data Combination Aux.distribution Model Combination
size (#sents) CLEF Alpino CLEF+Alpino CLEF+Alpino aux CLEF aux+Alpino aux equal weights
CLEF 2003 (446) 97.01 94.02 97.21 97.01 97.14 97.46
CLEF 2004 (700) 96.60 89.88 95.14 96.60 97.12 97.23
CLEF 2005 (200) 97.65 87.98 93.62 97.72 97.99 98.19
CLEF 2006 (200) 97.06 88.92 95.16 97.06 97.00 96.45
CLEF 2007 (200) 96.20 92.48 97.30 96.33 96.33 96.46
Table 1: Results on the CLEF test data; underlined scores indicate results > in-domain baseline (CLEF)
? Auxiliary distribution (CLEF+Alpino aux): adding
the original Alpino model as auxiliary feature to CLEF
? Model Combination: keep only two features
P
OUT
(?|s) and P
IN
(?|s). Two variants: i) estimate
the parameters ?
1
, ?
2
(CLEF aux+Alpino aux); ii)
give them equal values, i.e. ?
1
=?
2
=?1 (equal weights)
We assess the performance of all of these mod-
els on the CLEF data by using 5-fold cross-
validation. The results are given in table 1.
The CLEF model performs significantly better
than the out-of-domain (Alpino) model, despite of
the smaller size of the in-domain training data.
In contrast, the simple data combination results
in a model (CLEF+Alpino) whose performance is
somewhere in between. It is able to contribute in
some cases to disambiguate questions, while lead-
ing to wrong decisions in other cases.
However, for our auxiliary based approach
(CLEF+Alpino aux) with its regulated contribu-
tion of the general model, the results show that
adding the feature does not help. On most datasets
the same performance was achieved as by the in-
domain model, while on only two datasets (CLEF
2005, 2007) the use of the auxiliary feature results
in an insignificant improvement.
In contrast, simple model combination works
surprisingly well. On two datasets (CLEF 2004
and 2005) this simple technique reaches a sub-
stantial improvement over all other models. On
only one dataset (CLEF 2006) it falls slightly off
the in-domain baseline, but still considerably out-
performs data combination. This is true for both
model combination methods, with estimated and
equal weights. In general, the results show that
model combination usually outperforms data com-
bination (with the exception of one dataset, CLEF
2007), where, interestingly, the simplest model
combination (equal weights) often performs best.
Contrary to expectations, the auxiliary based ap-
proach performs poorly and could often not even
come close to the results obtained by simple model
combination. In the following we will explore pos-
sible reasons for this result.
Examining possible causes One possible point
of failure could be that the auxiliary feature was
simply ignored. If the estimated weight would be
close to zero the feature would indeed not con-
tribute to the disambiguation task. Therefore, we
examined the estimated weights for that feature.
From that analysis we saw that, compared to the
other features, the auxiliary feature got a weight
relatively far from zero. It got on average a weight
of ?0.0905 in our datasets and as such is among
the most influential weights, suggesting it to be im-
portant for disambiguation.
Another question that needs to be asked, how-
ever, is whether the feature is modeling properly
the original Alpino model. For this sanity check,
we create a model that contains only the single
auxiliary feature and no other features. The fea-
ture?s weight is set to a constant negative value4.
The resulting model?s performance is assessed on
the complete CLEF data. The results (0% column
in table 3) show that the auxiliary feature is indeed
properly modeling the general Alpino model, as
the two result in identical performance.
4.3.1 Feature template class models
In the experiments so far the general model was
?packed? into a single feature value. To check
whether the feature alone is too weak, we exam-
ine the inclusion of several auxiliary distributions
(k > 1). Each auxiliary feature we add represents
a ?submodel? corresponding to an actual feature
template class used in the original model. The fea-
ture?s value is the negative log-probability as de-
fined in equation 11, where OUT corresponds to
the respective Alpino submodel.
The current Disambiguation Model of Alpino
uses the 21 feature templates (van Noord and Mal-
ouf, 2005). Out of this given feature templates,
we create two models that vary in the number of
classes used. In the first model (?5 class?), we cre-
ate five (k = 5) auxiliary distributions correspond-
ing to five clusters of feature templates. They are
4Alternatively, we may estimate its weight, but as it does
not have competing features we are safe to assume it constant.
13
defined manually and correspond to submodels for
Part-of-Speech, dependencies, grammar rule ap-
plications, bilexical preferences and the remaining
Alpino features. In the second model (?21 class?),
we simply take every single feature template as its
own cluster (k = 21).
We test the two models and compare them to
our baseline. The results of this experiment are
given in table 2. We see that both the 5 class and
the 21 class model do not achieve any considerable
improvement over the baseline (CLEF), nor over
the single auxiliary model (CLEF+Alpino aux).
Dataset (#sents) 5class 21class CLEF+Alpino aux CLEF
CLEF2003 (446) 97.01 97.04 97.01 97.01
CLEF2004 (700) 96.57 96.60 96.60 96.60
CLEF2005 (200) 97.72 97.72 97.72 97.65
CLEF2006 (200) 97.06 97.06 97.06 97.06
CLEF2007 (200) 96.20 96.27 96.33 96.20
Table 2: Results on CLEF including several auxil-
iary features corresponding to Alpino submodels
4.3.2 Varying amount of training data
Our expectation is that the auxiliary feature is at
least helpful in the case very little in-domain train-
ing data is available. Therefore, we evaluate the
approach with smaller amounts of training data.
We sample (without replacement) a specific
amount of training instances from the original QA
data files and train models on the reduced train-
ing data. The resulting models are tested with and
without the additional feature as well as model
combination on the complete data set by using
cross validation. Table 3 reports the results of these
experiments for models trained on a proportion of
up to 10% CLEF data. Figure 1 illustrates the over-
all change in performance.
Obviously, an increasing amount of in-domain
training data improves the accuracy of the models.
However, for our auxiliary feature, the results in
table 3 show that the models with and without the
auxiliary feature result in an overall almost iden-
tical performance (thus in figure 1 we depict only
one of the lines). Hence, the inclusion of the aux-
iliary feature does not help in this case either. The
models achieve similar performance even indepen-
dently of the available amount of in-domain train-
ing data.
Thus, even on models trained on very little in-
domain training data (e.g. 1% CLEF training data)
the auxiliary based approach does not work. It
even hurts performance, i.e. depending on the spe-
cific dataset, the inclusion of the auxiliary feature
 86
 88
 90
 92
 94
 96
 98
 0  10  20  30  40  50  60
CA
% training data
Varying amount of training data (CLEF 2004)
Aux.distr. (CLEF+Alp_aux)
Out-dom (Alpino)
Mod.Comb. (CLEF_aux+Alpino_aux)
Figure 1: Amount of in-domain training data ver-
sus concept accuracy (Similar figures result from
the other CLEF datasets) - note that we depict only
aux.distr. as its performance is nearly indistin-
guishable from the in-domain (CLEF) baseline
results in a model whose performance lies even be-
low the original Alpino model accuracy, for up to a
certain percentage of training data (varying on the
dataset from 1% up to 10%).
In contrast, simple model combination is much
more beneficial. It is able to outperform almost
constantly the in-domain baseline (CLEF) and
our auxiliary based approach (CLEF+Alpino aux).
Furthermore, in contrast to the auxiliary based ap-
proach, model combination never falls below the
out-of-domain (Alpino) baseline, not even in the
case a tiny amount of training data is available.
This is true for both model combinations (esti-
mated versus equal weights).
We would have expected the auxiliary feature to
be useful at least when very little in-domain train-
ing data is available. However, the empirical re-
sults reveal the contrary5. We believe the reason
for this drop in performance is the amount of avail-
able in-domain training data and the corresponding
scaling of the auxiliary feature?s weight. When
little training data is available, the weight cannot
be estimated reliably and hence is not contributing
enough compared to the other features (exempli-
fied in the drop of performance from 0% to 1%
5As suspected by a reviewer, the (non-auxiliary) features
may overwhelm the single auxiliary feature, such that possi-
ble improvements by increasing the feature space on such a
small scale might be invisible. We believe this is not the case.
Other studies have shown that including just a few features
might indeed help (Johnson and Riezler, 2000; van Noord,
2007). (e.g., the former just added 3 features).
14
0% 1% 5% 10%
Dataset no aux = Alp. no aux +aux m.c. eq.w. no aux +aux m.c. eq.w. no aux +aux m.c. eq.w.
CLEF2003 94.02 94.02 91.93 91.93 95.59 93.65 93.83 93.83 95.74 95.17 94.80 94.77 95.72 95.72
CLEF2004 89.88 89.88 86.59 86.59 90.97 91.06 93.62 93.62 93.42 92.95 94.79 94.82 96.26 95.85
CLEF2005 87.98 87.98 87.34 87.41 91.35 89.15 95.90 95.90 97.92 97.52 96.31 96.37 98.19 97.25
CLEF2006 88.92 88.92 89.64 89.64 92.16 91.17 92.77 92.77 94.98 94.55 95.04 95.04 95.04 95.47
CLEF2007 92.48 92.48 91.07 91.13 95.44 93.32 94.60 94.60 95.63 95.69 94.21 94.21 95.95 95.43
Table 3: Results on the CLEF data with varying amount of training data
training data in table 3). In such cases it is more
beneficial to just apply the original Alpino model
or the simple model combination technique.
4.4 Experiments with CGN
One might argue that the question domain is
rather ?easy?, given the already high baseline per-
formance and the fact that few hand-annotated
questions are enough to obtain a reasonable
model. Therefore, we examine our approach on
CGN (Oostdijk, 2000).
The empirical results of testing using cross-
validation within a subset of CGN subdomains
are given in table 4. The baseline accuracies
are much lower on this more heterogeneous, spo-
ken, data, leaving more room for potential im-
provements over the in-domain model. How-
ever, the results show that the auxiliary based ap-
proach does not work on the CGN subdomains ei-
ther. The approach is not able to improve even on
datasets where very little training data is available
(e.g. comp-l), thus confirming our previous find-
ing. Moreover, in some cases the auxiliary fea-
ture rather, although only slightly, degrades perfor-
mance (indicated in italic in table 4) and performs
worse than the counterpart model without the ad-
ditional feature.
Depending on the different characteristics of
data/domain and its size, the best model adapta-
tion method varies on CGN. On some subdomains
simple model combination performs best, while on
others it is more beneficial to just apply the origi-
nal, out-of-domain Alpino model.
To conclude, model combination achieves in most
cases a modest improvement, while we have
shown empirically that our domain adaptation
method based on auxiliary distributions performs
just similar to a model trained on in-domain data.
5 Conclusions
We examined auxiliary distributions (Johnson and
Riezler, 2000) for domain adaptation. While
the auxiliary approach has been successfully ap-
plied to lexical selectional preferences (Johnson
and Riezler, 2000; van Noord, 2007), our empir-
ical results show that integrating a more general
into a domain-specific model through the auxil-
iary feature approach does not help. The auxil-
iary approach needs training data to estimate the
weight(s) of the auxiliary feature(s). When little
training data is available, the weight cannot be es-
timated appropriately and hence is not contributing
enough compared to the other features. This re-
sult was confirmed on both examined domains. We
conclude that the auxiliary feature approach is not
appropriate for integrating information of a more
general model to leverage limited in-domain data.
Better results were achieved either without adapta-
tion or by simple model combination.
Future work will consist in investigating other pos-
sibilities for parser adaptation, especially semi-
supervised domain adaptation, where no labeled
in-domain data is available.
References
Abney, Steven P. 1997. Stochastic attribute-value grammars.
Computational Linguistics, 23:597?618.
Berger, A. and H. Printz. 1998. A comparison of criteria
for maximum entropy / minimum divergence feature selec-
tion. In In Proceedings of the 3nd Conference on Empir-
ical Methods in Natural Language Processing (EMNLP),
pages 97?106, Granada, Spain.
Berger, Adam, Stephen Della Pietra, and Vincent Della Pietra.
1996. A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?72.
Boros, M., W. Eckert, F. Gallwitz, G. Go?rz, G. Hanrieder, and
H. Niemann. 1996. Towards understanding spontaneous
speech: Word accuracy vs. concept accuracy. In Pro-
ceedings of the Fourth International Conference on Spoken
Language Processing (ICSLP 96), Philadelphia.
Briscoe, Ted, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes. In Pro-
ceedings of the Beyond PARSEVAL Workshop at the 3rd In-
ternational Conference on Language Resources and Eval-
uation, pages 4?8, Las Palmas, Gran Canaria.
Gildea, Daniel. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the 2001 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP).
Hara, Tadayoshi, Miyao Yusuke, and Jun?ichi Tsujii. 2005.
Adapting a probabilistic disambiguation model of an hpsg
15
comp-a (1,193) - Spontaneous conversations (?face-to-face?) comp-b (525) - Interviews with teachers of Dutch
DataSet no aux + aux Alpino Mod.Comb. Mod.Comb. Dataset no aux + aux Alpino Mod.Comb. Mod.Comb
eq.weights eq.weights
fn000250 63.20 63.28 62.90 63.91 63.99 fn000081 66.20 66.39 66.45 67.26 66.85
fn000252 64.74 64.74 64.06 64.87 64.96 fn000089 62.41 62.41 63.88 64.35 64.01
fn000254 66.03 66.00 65.78 66.39 66.44 fn000086 62.60 62.76 63.17 63.59 63.77
comp-l (116) - Commentaries/columns/reviews (broadcast) comp-m (267) - Ceremonious speeches/sermons
DataSet no aux + aux Alpino Mod.Comb. Model.Comb. Dataset no aux + aux Alpino Mod.Comb. Mod.Comb
eq.weights eq.weights
fn000002 67.63 67.63 77.30 76.96 72.40 fn000271 59.25 59.25 63.78 64.94 61.76
fn000017 64.51 64.33 66.42 66.30 65.74 fn000298 70.33 70.19 74.55 74.83 72.70
fn000021 61.54 61.54 64.30 64.10 63.24 fn000781 72.26 72.37 73.55 73.55 73.04
Table 4: Excerpt of results on various CGN subdomains (# of sentences in parenthesis).
parser to a new domain. In Proceedings of the Interna-
tional Joint Conference on Natural Language Processing.
Johnson, Mark and Stefan Riezler. 2000. Exploiting auxiliary
distributions in stochastic unification-based grammars. In
Proceedings of the first conference on North American
chapter of the Association for Computational Linguistics,
pages 154?161, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the 37th
Annual Meeting of the ACL.
Lease, Matthew, Eugene Charniak, Mark Johnson, and David
McClosky. 2006. A look at parsing and its applications.
In Proceedings of the Twenty-First National Conference on
Artificial Intelligence (AAAI-06), Boston, Massachusetts,
16?20 July.
McClosky, David, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference of
the NAACL, Main Conference, pages 152?159, New York
City, USA, June. Association for Computational Linguis-
tics.
Oostdijk, Nelleke. 2000. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of Sec-
ond International Conference on Language Resources and
Evaluation (LREC), pages 887?894.
Osborne, Miles. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Proceed-
ings of the Eighteenth International Conference on Com-
putational Linguistics (COLING 2000).
Ratnaparkhi, A. 1997. A simple introduction to maximum
entropy models for natural language processing. Technical
report, Institute for Research in Cognitive Science, Univer-
sity of Pennsylvania.
van Noord, Gertjan and Robert Malouf. 2005. Wide coverage
parsing with stochastic attribute value grammars. Draft
available from http://www.let.rug.nl/?vannoord. A prelim-
inary version of this paper was published in the Proceed-
ings of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
van Noord, Gertjan. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina, Actes
De La 13e Conference sur Le Traitement Automatique des
Langues naturelles, pages 20?42, Leuven.
van Noord, Gertjan. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In Pro-
ceedings of the Tenth International Conference on Parsing
Technologies. IWPT 2007, Prague., pages 1?10, Prague.
van Noord, Gertjan. In preparation. Learning efficient pars-
ing.
Velldal, E. and S. Oepen. 2005. Maximum entropy mod-
els for realization ranking. In Proceedings of MT-Summit,
Phuket, Thailand.
16
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 25?33,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Grammar-driven versus Data-driven: Which Parsing System is More
Affected by Domain Shifts?
Barbara Plank
University of Groningen
The Netherlands
b.plank@rug.nl
Gertjan van Noord
University of Groningen
The Netherlands
G.J.M.van.Noord@rug.nl
Abstract
In the past decade several parsing systems
for natural language have emerged, which
use different methods and formalisms. For
instance, systems that employ a hand-
crafted grammar and a statistical disam-
biguation component versus purely sta-
tistical data-driven systems. What they
have in common is the lack of portabil-
ity to new domains: their performance
might decrease substantially as the dis-
tance between test and training domain in-
creases. Yet, to which degree do they suf-
fer from this problem, i.e. which kind of
parsing system is more affected by domain
shifts? Intuitively, grammar-driven sys-
tems should be less affected by domain
changes. To investigate this hypothesis,
an empirical investigation on Dutch is car-
ried out. The performance variation of
a grammar-driven versus two data-driven
systems across domains is evaluated, and a
simple measure to quantify domain sensi-
tivity proposed. This will give an estimate
of which parsing system is more affected
by domain shifts, and thus more in need
for adaptation techniques.
1 Introduction
Most modern Natural Language Processing (NLP)
systems are subject to the wellknown problem of
lack of portability to new domains: there is a sub-
stantial drop in their performance when the sys-
tem gets input from another text domain (Gildea,
2001). This is the problem of domain adapta-
tion. Although the problem exists ever since the
emergence of supervised Machine Learning, it has
started to get attention only in recent years.
Studies on supervised domain adaptation
(where there are limited amounts of annotated
resources in the new domain) have shown that
straightforward baselines (e.g. models based on
source only, target only, or the union of the data)
achieve a relatively high performance level and are
?surprisingly difficult to beat? (Daume? III, 2007).
In contrast, semi-supervised adaptation (i.e. no
annotated resources in the new domain) is a much
more realistic situation but is clearly also consid-
erably more difficult. Current studies on semi-
supervised approaches show very mixed results.
Dredze et al (2007) report on ?frustrating? re-
sults on the CoNLL 2007 semi-supervised adap-
tation task for dependency parsing, i.e. ?no team
was able to improve target domain performance
substantially over a state-of-the-art baseline?. On
the other hand, there have been positive results as
well. For instance, McClosky et al (2006) im-
proved a statistical parser by self-training. Struc-
tural Correspondence Learning (Blitzer et al,
2006) was effective for PoS tagging and Sentiment
Analysis (Blitzer et al, 2006; Blitzer et al, 2007),
while only modest gains were obtained for struc-
tured output tasks like parsing.
For parsing, most previous work on do-
main adaptation has focused on data-driven sys-
tems (Gildea, 2001; McClosky et al, 2006;
Dredze et al, 2007), i.e. systems employing (con-
stituent or dependency based) treebank gram-
mars. Only few studies examined the adaptation of
grammar-based systems (Hara et al, 2005; Plank
and van Noord, 2008), i.e. systems employing
a hand-crafted grammar with a statistical disam-
biguation component. This may be motivated by
the fact that potential gains for this task are inher-
ently bound by the grammar. Yet, domain adap-
tation poses a challenge for both kinds of pars-
ing systems. But to what extent do these differ-
ent kinds of systems suffer from the problem? We
test the hypothesis that grammar-driven systems
are less affected by domain changes. We empir-
ically investigate this in a case-study on Dutch.
25
2 Related work
Most previous work has focused on a single pars-
ing system in isolation (Gildea, 2001; Hara et
al., 2005; McClosky et al, 2006). However,
there is an observable trend towards combining
different parsing systems to exploit complemen-
tary strengths. For instance, Nivre and McDon-
ald (2008) combine two data-driven systems to im-
prove dependency accuracy. Similarly, two studies
successfully combined grammar-based and data-
driven systems: Sagae et al (2007) incorporate
data-driven dependencies as soft-constraint in a
HPSG-based system for parsing the Wallstreet
Journal. In the same spirit (but the other di-
rection), Zhang and Wang (2009) use a deep-
grammar based backbone to improve data-driven
parsing accuracy. They incorporate features from
the grammar-based backbone into the data-driven
system to achieve better generalization across do-
mains. This is the work most closest to ours.
However, which kind of system (hand-crafted
versus purely statistical) is more affected by the
domain, and thus more sensitive to domain shifts?
To the best of our knowledge, no study has yet ad-
dressed this issue. We thus assess the performance
variation of three dependency parsing systems for
Dutch across domains, and propose a simple mea-
sure to quantify domain sensitivity.
3 Parsing Systems
The parsing systems used in this study are: a
grammar-based system for Dutch (Alpino) and
two data-driven systems (MST and Malt), all de-
scribed next.
(1) Alpino is a parser for Dutch which has
been developed over the last ten years, on the ba-
sis of a domain-specific HPSG-grammar that was
used in the OVIS spoken dialogue system. The
OVIS parser was shown to out-perform a statisti-
cal (DOP) parser, in a contrastive formal evalua-
tion (van Zanten et al, 1999). In the ten years af-
ter this evaluation, the system has developed into a
generic parser for Dutch. Alpino consists of more
than 800 grammar rules in the tradition of HPSG,
and a large hand-crafted lexicon. It produces de-
pendency structures as ouput, where more than a
single head per token is allowed. For words that
are not in the lexicon, the system applies a large
variety of unknown word heuristics (van Noord,
2006), which deal with number-like expressions,
compounds, proper names, etc. Coverage of the
grammar and lexicon has been extended over the
years by paying careful attention to the results of
parsing large corpora, by means of error mining
techniques (van Noord, 2004; de Kok et al, 2009).
Lexical ambiguity is reduced by means of a
POS-tagger, described in (Prins and van No-
ord, 2003). This POS-tagger is trained on large
amounts of parser output, and removes unlikely
lexical categories. Some amount of lexical am-
biguity remains. A left-corner parser constructs
a parse-forest for an input sentence. Based on
large amounts of parsed data, the parser considers
only promising parse step sequences, by filtering
out sequences of parse steps which were not pre-
viously used to construct a best parse for a given
sentence. The parse step filter improves efficiency
considerably (van Noord, 2009).
A best-first beam-search algorithm retrieves the
best parse(s) from that forest by consulting a Max-
imum Entropy disambiguation component. Fea-
tures for the disambiguation component include
non-local features. For instance, there are features
that can be used to learn a preference for local ex-
traction over long-distance extraction, and a pref-
erence for subject fronting rather than direct ob-
ject fronting, and a preference for certain types of
orderings in the ?mittelfeld? of a Dutch sentence.
The various features that we use for disambigua-
tion, as well as the best-first algorithm is described
in (van Noord, 2006). The model now also con-
tains features which implement selection restric-
tions, trained on the basis of large parsed corpora
(van Noord, 2007). The maximum entropy dis-
ambiguation component is trained on the Alpino
treebank, described below.
To illustrate the role of the disambiguation com-
ponent, we provide some results for the first 536
sentences of one of the folds of the training data
(of course, the model used in this experiment is
trained on the remaining folds of training data).
In this setup, the POS-tagger and parse step filter
already filter out many, presumably bad, parses.
This table indicates that a very large amount of
parses can be constructed for some sentences. Fur-
thermore, the maximum entropy disambiguation
component does a good job in selecting good
parses from those. Accuracy is given here in terms
of f-score of named dependencies.
sents parses oracle arbitrary model
536 45011 95.74 76.56 89.39
(2) MST Parser (McDonald et al, 2005) is a
26
data-driven graph-based dependency parser. The
system couples a minimum spanning tree search
procedure with a separate second stage classifier
to label the dependency edges.
(3) MALT Parser (Nivre et al, 2007) is a data-
driven transition-based dependency parser. Malt
parser uses SVMs to learn a classifier that predicts
the next parsing action. Instances represent parser
configurations and the label to predict determines
the next parser action.
Both data-driven parsers (MST and Malt) are
thus not specific for the Dutch Language, however,
they can be trained on a variety of languages given
that the training corpus complies with the column-
based format introduced in the 2006 CoNLL
shared task (Buchholz and Marsi, 2006). Ad-
ditionally, both parsers implement projective and
non-projective parsing algorithms, where the latter
will be used in our experiments on the relatively
free word order language Dutch. Despite that, we
train the data-driven parsers using their default set-
tings (e.g. first order features for MST, SVM with
polynomial kernel for Malt).
4 Datasets and experimental setup
The source domain on which all parsers are trained
is cdb, the Alpino Treebank (van Noord, 2006).
For our cross-domain evaluation, we consider
Wikipedia and DPC (Dutch Parallel Corpus) as
target data. All datasets are described next.
Source: Cdb The cdb (Alpino Treebank) con-
sists of 140,000 words (7,136 sentences) from the
Eindhoven corpus (newspaper text). It is a col-
lection of text fragments from 6 Dutch newspa-
pers. The collection has been annotated accord-
ing to the guidelines of CGN (Oostdijk, 2000) and
stored in XML format. It is the standard treebank
used to train the disambiguation component of the
Alpino parser. Note that cdb is a subset of the
training corpus used in the CoNLL 2006 shared
task (Buchholz and Marsi, 2006). The CoNLL
training data additionally contained a mix of non-
newspaper text,1 which we exclude here on pur-
pose to keep a clean baseline.
Target: Wikipedia and DPC We use the
Wikipedia and DPC subpart of the LASSY cor-
1Namely, a large amount of questions (from CLEF,
roughly 4k sentences) and hand-crafted sentences used dur-
ing the development of the grammar (1.5k).
Wikipedia Example articles #a #w ASL
LOC (location) Belgium, Antwerp (city) 31 25259 11.5
KUN (arts) Tervuren school 11 17073 17.1
POL (politics) Belgium elections 2003 16 15107 15.4
SPO (sports) Kim Clijsters 9 9713 11.1
HIS (history) History of Belgium 3 8396 17.9
BUS (business) Belgium Labour Federation 9 4440 11.0
NOB (nobility) Albert II 6 4179 15.1
COM (comics) Suske and Wiske 3 4000 10.5
MUS (music) Sandra Kim, Urbanus 3 1296 14.6
HOL (holidays) Flemish Community Day 4 524 12.2
Total 95 89987 13.4
DPC Description/Example #a #words ASL
Science medicine, oeanography 69 60787 19.2
Institutions political speeches 21 28646 16.1
Communication ICT/Internet 29 26640 17.5
Welfare state pensions 22 20198 17.9
Culture darwinism 11 16237 20.5
Economy inflation 9 14722 18.5
Education education in Flancers 2 11980 16.3
Home affairs presentation (Brussel) 1 9340 17.3
Foreign affairs European Union 7 9007 24.2
Environment threats/nature 6 8534 20.4
Finance banks (education banker) 6 6127 22.3
Leisure various (drugscandal) 2 2843 20.3
Consumption toys from China 1 1310 22.6
Total 186 216371 18.5
Table 1: Overview Wikipedia and DPC corpus (#a
articles, #w words, ASL average sentence length)
pus2 as target domains. These corpora contain sev-
eral domains, e.g. sports, locations, science. On
overview of the corpora is given in Table 1. Note
that both consist of hand-corrected data labeled by
Alpino, thus all domains employ the same anno-
tation scheme. This might introduce a slight bias
towards Alpino, however it has the advantage that
all domains employ the same annotation scheme ?
which was the major source of error in the CoNLL
task on domain adaptation (Dredze et al, 2007).
CoNLL2006 This is the testfile for Dutch that
was used in the CoNLL 2006 shared task on multi-
lingual dependency parsing. The file consists
of 386 sentences from an institutional brochure
(about youth healthcare). We use this file to check
our data-driven models against state-of-the-art.
Alpino to CoNLL format In order to train the
MST and Malt parser and evaluate it on the var-
ious Wikipedia and DPC articles, we needed to
convert the Alpino Treebank format into the tab-
ular CoNLL format. To this end, we adapted the
treebank conversion software developed by Erwin
Marsi for the CoNLL 2006 shared task on multi-
lingual dependency parsing. Instead of using the
PoS tagger and tagset used in the shared task (to
which we did not have access to), we replaced the
PoS tags with more fine-grained tags obtained by
2LASSY (Large Scale Syntactic Annotation of written
Dutch), ongoing project. Corpus version 17905, obtained
from http://www.let.rug.nl/vannoord/Lassy/corpus/
27
parsing the data with the Alpino parser.3 At testing
time, the data-driven parsers are given PoS tagged
input, while Alpino gets plain sentences.
Evaluation In all experiments, unless otherwise
specified, performance is measured as Labeled
Attachment Score (LAS), the percentage of to-
kens with the correct dependency edge and label.
To compute LAS, we use the CoNLL 2007 eval-
uation script4 with punctuation tokens excluded
from scoring (as was the default setting in CoNLL
2006). We thus evaluate all parsers using the same
evaluation metric. Note that the standard metric
for Alpino would be a variant of LAS, which al-
lows for a discrepancy between expected and re-
turned dependencies. Such a discrepancy can oc-
cur, for instance, because the syntactic annotation
of Alpino allows words to be dependent on more
than a single head (?secondary edges?) (van No-
ord, 2006). However, such edges are ignored in
the CoNLL format; just a single head per token
is allowed. Furthermore, there is another simpli-
fication. As the Dutch tagger used in the CoNLL
2006 shared task did not have the concept of multi-
words, the organizers chose to treat them as a sin-
gle token (Buchholz and Marsi, 2006). We here
follow the CoNLL 2006 task setup. To determine
whether results are significant, we us the Approx-
imate Randomization Test (see Yeh (2000)) with
1000 random shuffles.
5 Domain sensitivity
The problem of domain dependence poses a chal-
lenge for both kinds of parsing systems, data-
driven and grammar-driven. However, to what ex-
tent? Which kind of parsing system is more af-
fected by domain shifts? We may rephrase our
question as: Which parsing system is more robust
to different input texts? To answer this question,
we will examine the robustness of the different
parsing systems in terms of variation of accuracy
on a variety of domains.
A measure of domain sensitivity Given a pars-
ing system (p) trained on some source domain
and evaluated on a set of N target domains, the
most intuitive measure would be to simply calcu-
3As discussed later (Section 6, cf. Table 2), using Alpino
tags actually improves the performance of the data-driven
parsers. We could perform this check as we recently got ac-
cess to the tagger and tagset used in the CoNLL shared task
(Mbt with wotan tagset; thanks to Erwin Marsi).
4
http://nextens.uvt.nl/depparse-wiki/SoftwarePage
late mean (?) and standard deviation (sd) of the
performance on the target domains:
LASip = accuracy of parser p on target domain i
?targetp =
?N
i=1 LAS
i
p
N
, sdtargetp =
?
?N
i=1(LAS
i
p ? ?
target
p )2
N ? 1
However, standard deviation is highly influenced
by outliers. Furthermore, this measure does not
take the source domain performance (baseline)
into consideration nor the size of the target domain
itself. We thus propose to measure the domain
sensitivity of a system, i.e. its average domain
variation (adv), as weighted average difference
from the baseline (source) mean, where weights
represents the size of the various domains:
adv =
?N
i=1w
i ??ip
?N
i=1wi
, with
?ip = LAS
i
p?LAS
baseline
p and w
i =
size(wi)
?N
i=1 size(wi)
In more detail, we measure average domain
variation (adv) relative to the baseline (source do-
main) performance by considering non-squared
differences from the out-of-domain mean and
weigh it by domain size. The adv measure can
thus take on positive or negative values. Intu-
itively, it will indicate the average weighted gain
or loss in performance, relative to the source do-
main. As alternative, we may want to just cal-
culate a straight, unweighted average: uadv =
?N
i=1 ?
i
p/N . However, this assumes that domains
have a representative size, and a threshold might
be needed to disregard domains that are presum-
ably too small.
We will use adv in the empirical result section
to evaluate the domain sensitivity of the parsers,
where sizewill be measured in terms of number of
words. We additionally provide values for the un-
weighted version using domains with at least 4000
words (cf. Table 1).
6 Empirical results
First of all, we performed several sanity checks.
We trained the MST parser on the entire original
CoNLL training data as well as the cdb subpart
only, and evaluated it on the original CoNLL test
data. As shown in Table 2 (row 1-2) the accura-
cies of both models falls slightly below state-of-
the-art performance (row 5), most probably due to
the fact that we used standard parsing settings (e.g.
28
no second-order features for MST). More impor-
tantly, there was basically no difference in perfor-
mance when trained on the entire data or cdb only.
Model LAS UAS
MST (original CoNLL) 78.35 82.89
MST (original CoNLL, cdb subpart) 78.37 82.71
MST (cdb retagged with Alpino) 82.14 85.51
Malt (cdb retagged with Alpino) 80.64 82.66
MST (Nivre and McDonald, 2008) 79.19 83.6
Malt (Nivre and McDonald, 2008) 78.59 n/a
MST (cdb retagged with Mbt) 78.73 82.66
Malt (cdb retagged with Mbt) 75.34 78.29
Table 2: Performance of data-driven parsers ver-
sus state-of-the-art on the CoNLL 2006 testset (in
Labeled/Unlabeled Attachment Score).
We then trained the MST and Malt parser on
the cdb corpus converted into the retagged CoNLL
format, and tested on CoNLL 2006 test data (also
retagged with Alpino). As seen in Table 2, by
using Alpino tags the performance level signifi-
cantly improves (with p < 0.002 using Approx-
imate Randomization Test with 1000 iterations).
This increase in performance can be attributed to
two sources: (a) improvements in the Alpino tree-
bank itself over the course of the years, and (b) the
more fine-grained PoS tagset obtained by parsing
the data with the deep grammar. To examine the
contribution of each source, we trained an addi-
tional MST model on the cdb data but tagged with
the same tagger as in the CoNLL shared task (Mbt,
cf. Table 2 last row): the results show that the
major source of improvement actually comes from
using the more fine-grained Alpino tags (78.73?
82.14 = +3.41 LAS), rather than the changes in
the treebank (78.37 ? 78.73 = +0.36 LAS).
Thus, despite the rather limited training data and
use of standard training settings, we are in line
with, and actually above, current results of data-
driven parsing for Dutch.
Baselines To establish our baselines, we per-
form 5-fold cross validation for each parser on the
source domain (cdb corpus, newspaper text). The
baselines for each parser are given in Table 3. The
grammar-driven parser Alpino achieves a baseline
that is significantly higher (90.75% LAS) com-
pared to the baselines of the data-driven systems
(around 80-83% LAS).
Cross-domain results As our goal is to assess
performance variation across domains, we evalu-
ate each parser on the Wikipedia and DPC corpora
Model Alpino MST Malt
Baseline (LAS) 90.76 83.63 79.95
Baseline (UAS) 92.47 88.12 83.31
Table 3: Baseline (5-fold cross-validation). All
differences are significant at p < 0.001.
that cover a variety of domains (described in Ta-
ble 1). Figure 1 and Figure 2 summarizes the re-
sults for each corpus, respectively. In more detail,
the figures depict for each parser the baseline per-
formance as given in Table 3 (straight lines) and
the performance on every domain (bars). Note that
domains are ordered by size (number of words), so
that the largest domains appear as bars on the left.
Similar graphs come up if we replace labeled at-
tachment score with its unlabeled variant.
Figure 1 depicts parser performance on the
Wikipedia domains with respect to the source
domain baseline. The figure indicates that the
grammar-driven parser does not suffer much from
domain shifts. Its performance falls even above
baseline for several Wikipedia domains. In con-
trast, the MST parser suffers the most from the
domain changes; on most domains a substantial
performance drop can be observed. The transition-
based parser scores on average significantly lower
than the graph-based counterpart and Alpino, but
seems to be less affected by the domain shifts.
We can summarize this findings by our pro-
posed average domain variation measure (un-
weighted scores are given in the Figure): On av-
erage (over all Wikipedia domains), Alpino suf-
fers the least (adv = +0.81), followed by Malt
(+0.59) and MST (?2.2), which on average loses
2.2 absolute LAS. Thus, the graph-based data-
driven dependency parser MST suffers the most.
We evaluate the parsers also on the more var-
ied DPC corpus. It contains a broader set of do-
mains, amongst others science texts (medical texts
from the European Medicines Agency as well as
texts about oceanography) and articles with more
technical vocabulary (Communication, i.e. Inter-
net/ICT texts). The results are depicted in Fig-
ure 2. Both Malt (adv = 0.4) and Alpino (adv =
0.22) achieve on average a gain over the baseline,
with this time Malt being slightly less domain af-
fected than Alpino (most probably because Malt
scores above average on the more influential/larger
domains). Nevertheless, Alpino?s performance
level is significantly higher compared to both data-
driven counterparts. The graph-based data-driven
29
La
be
led
 At
tac
hm
en
t S
cor
e (L
AS)
Alpino
adv= 0.81 (+/? 3.7 )
uadv (>4k)= 2 (+/? 2.1 )
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
MST
adv = ?2.2 (+/? 9 )
uadv (>4k)= ?1.8 (+/? 4 )
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
Malt
adv = 0.59 (+/? 9.4 )
uadv(>4k)= 1.3 (+/? 3 )
Alpino
MST
Malt
Figure 1: Performance on Wikipedia domains with respect to the source baseline (newspaper text) in-
cluding average domain variation (adv) score and its unweighted alternative (uadv). Domains are ordered
by size (largest on left). Full-colored bars indicate domains where performance lies below the baseline.
parser MST is the most domain-sensitive parser
also on DPC (adv = ?0.27).
In contrast, if we would take only the deviation
on the target domains into consideration (with-
out considering the baseline, cf. Section 5), we
would get a completely opposite ranking on DPC,
where the Malt parser would actually be consid-
ered the most domain-sensitive (here higher sd
means higher sensitivity): Malt (sd = 1.20), MST
(sd = 1.14), Alpino (sd = 1.05). However, by
looking at Figure 2, intuitively, MST suffers more
from the domain shifts than Malt, as most bars lie
below the baseline. Moreover, the standard devia-
tion measure neither gives a sense of whether the
parser on average suffers a loss or gain over the
new domains, nor incorporates the information of
domain size. We thus believe our proposed aver-
age domain variation is a better suited measure.
To check whether the differences in perfor-
mance variation are statistically significant, we
performed an Approximate Randomization Test
over the performance differences (deltas) on the
23 domains (DPC and Wikipedia). The results
show that the difference between Alpino and MST
is significant. The same goes for the difference
between MST and Malt. Thus Alpino is signifi-
cantly more robust than MST. However, the dif-
ference between Alpino and Malt is not signif-
icant. These findings hold for differences mea-
sured in both labeled and unlabeled attachments
scores. Furthermore, all differences in absolute
performance across domains are significant.
To summarize, our empirical evaluation shows
that the grammar-driven system Alpino is rather
robust across domains. It is the best perform-
ing system and it is significantly more robust than
MST. In constrast, the transition-based parser Malt
scores the lowest across all domains, but its vari-
ation turned out not to be different from Alpino.
Over all domains, MST is the most domain-
sensitive parser.
30
La
be
led
 A
tta
ch
me
nt 
Sc
ore
 (LA
S)
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
Sc
ien
ce
Ins
titu
tio
ns
Co
mm
un
ica
tio
n
W
elf
are
_s
tat
e
Cu
ltu
re
Ec
on
om
y
Ed
uc
ati
on
Ho
me
_a
ffa
irs
Fo
rei
gn
_a
ffa
irs
En
vir
on
me
nt
Fin
an
ce
Le
isu
re
Co
ns
um
pti
on
Alpino
adv = 0.22 (+/? 0.823 )
uadv (>4k)= 0.4 (+/? 0.8 )
Sc
ien
ce
Ins
titu
tio
ns
Co
mm
un
ica
tio
n
W
elf
are
_s
tat
e
Cu
ltu
re
Ec
on
om
y
Ed
uc
ati
on
Ho
me
_a
ffa
irs
Fo
rei
gn
_a
ffa
irs
En
vir
on
me
nt
Fin
an
ce
Le
isu
re
Co
ns
um
pti
on
MST
adv = ?0.27 (+/? 0.56 )
uadv (>4k)= ?0.21 (+/? 1 )
Sc
ien
ce
Ins
titu
tio
ns
Co
mm
un
ica
tio
n
W
elf
are
_s
tat
e
Cu
ltu
re
Ec
on
om
y
Ed
uc
ati
on
Ho
me
_a
ffa
irs
Fo
rei
gn
_a
ffa
irs
En
vir
on
me
nt
Fin
an
ce
Le
isu
re
Co
ns
um
pti
on
Malt
adv = 0.4 (+/? 0.54 )
uadv (>4k)= 0.41 (+/? 0.9 )
Alpino
MST
Malt
Figure 2: Performance on DPC domains with respect to the source baseline (newspaper text).
Excursion: Lexical information Both kinds
of parsing systems rely on lexical information
(words/stems) when learning their parsing (or
parse disambiguation) model. However, how
much influence does lexical information have?
To examine this issue, we retrain all parsing sys-
tems by excluding lexical information. As all pars-
ing systems rely on a feature-based representa-
tion, we remove all feature templates that include
words and thus train models on a reduced fea-
ture space (original versus reduced space: Alpino
24k/7k features; MST 14M/1.9M features; Malt
17/13 templates). The result of evaluating the
unlexicaled models on Wikipedia are shown in
Figure 3. Clearly, performance drops for for all
parsers in all domains. However, for the data-
driven parsers to a much higher degree. For in-
stance, MST loses on average 11 absolute points
in performance (adv = ?11) and scores below
baseline on all Wikipedia domains. In contrast,
the grammar-driven parser Alpino suffers far less,
still scores above baseline on some domains.5 The
Malt parser lies somewhere in between, also suf-
fers from the missing lexical information, but to a
lesser degree than the graph-based parser MST.
7 Conclusions and Future work
We examined a grammar-based system cou-
pled with a statistical disambiguation component
(Alpino) and two data-driven statistical parsing
systems (MST and Malt) for dependency parsing
of Dutch. By looking at the performance variation
across a large variety of domains, we addressed
the question of how sensitive the parsing systems
are to the text domain. This, to gauge which kind
5Note that the parser has still access to its lexicon here;
for now we removed lexicalized features from the trainable
part of Alpino, the statistical disambiguation component.
31
La
bel
ed 
Att
ach
me
nt S
cor
e (L
AS)
Alpino
adv= ?0.63 (+/? 3.6 )
uadv (>4k)= 0.1 (+/? 2 )
66
68
70
72
74
76
78
80
82
84
86
88
90
92
94
96
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
MST
adv = ?11 (+/? 11 )
uadv (>4k)= ?11 (+/? 2.1 )
LO
C
KU
N
PO
L
SP
O HIS BU
S
NO
B
CO
M
MU
S
HO
L
Malt
adv = ?4.9 (+/? 9 )
uadv (>4k)= ?4.8 (+/? 3 )
Alpino
MST
Malt
Figure 3: Performance of unlexical parsers on Wikipedia domains with respect to the source baseline.
of system (data-driven versus grammar-driven) is
more affected by domain shifts, and thus more in
need for adaptation techniques. We also proposed
a simple measure to quantify domain sensitivity.
The results show that the grammar-based sys-
tem Alpino is the best performing system, and it
is robust across domains. In contrast, MST, the
graph-based approach to data-driven parsing is the
most domain-sensitive parser. The results for Malt
indicate that its variation across domains is lim-
ited, but this parser is outperformed by both other
systems on all domains. In general, data-driven
systems heavily rely on the training data to esti-
mate their models. This becomes apparent when
we exclude lexical information from the train-
ing process, which results in a substantial perfor-
mance drop for the data-driven systems, MST and
Malt. The grammar-driven model was more robust
against the missing lexical information. Grammar-
driven systems try to encode domain independent
linguistic knowledge, but usually suffer from cov-
erage problems. The Alpino parser successfully
implements a set of unknown word heuristics and
a partial parsing strategy (in case no full parse can
be found) to overcome this problem. This makes
the system rather robust across domains, and, as
shown in this study, significantly more robust than
MST. This is not to say that domain dependence
does not consitute a problem for grammar-driven
parsers at all. As also noted by Zhang and Wang
(2009), the disambiguation component and lexi-
cal coverage of grammar-based systems are still
domain-dependent. Thus, domain dependence is a
problem for both types of parsing systems, though,
as shown in this study, to a lesser extent for the
grammar-based system Alpino. Of course, these
results are specific for Dutch; however, it?s a first
step. As the proposed methods are indepedent of
language and parsing system, they can be applied
to another system or language.
In future, we would like to (a) perform an error
analysis (e.g. why for some domains the parsers
outperform their baseline; what are typical in-
domain and out-domain errors), (a) examine why
there is such a difference in performance variation
between Malt and MST, and (c) investigate what
part(s) of the Alpino parser are responsible for the
differences with the data-driven parsers.
32
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL, Prague, Czech Republic.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
In Proc. of CoNLL, pages 149?164.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL, Prague, Czech Republic.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error min-
ing in parsing results. In Proceedings of the 2009
Workshop on Grammar Engineering Across Frame-
works (GEAF 2009), pages 71?79, Suntec, Singa-
pore, August.
Mark Dredze, John Blitzer, Pratha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adaptation
for parsing. In Proceedings of the CoNLL Shared
Task Session, Prague, Czech Republic.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Tadayoshi Hara, Miyao Yusuke, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an hpsg parser to a new domain. In Pro-
ceedings of the International Joint Conference on
Natural Language Processing.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13:95?135.
Nelleke Oostdijk. 2000. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of
LREC, pages 887?894.
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach to
domain adaptation of a syntactic disambiguation
model. In Proceedings of the Workshop on Cross-
Framework and Cross-Domain Parser Evaluation
(PE), Manchester, August.
Robbert Prins and Gertjan van Noord. 2003. Reinforc-
ing parser preferences through tagging. Traitement
Automatique des Langues, 44(3):121?139.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 624?
631, Prague, Czech Republic, June.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL2004,
Barcelona. ACL.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20?42,
Leuven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Pars-
ing Technology (IWPT), ACL 2007 Workshop, pages
1?10, Prague. ACL.
Gertjan van Noord. 2009. Learning efficient parsing.
In EACL 2009, The 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 817?825, Athens, Greece.
Gert Veldhuijzen van Zanten, Gosse Bouma, Khalil
Sima?an, Gertjan van Noord, and Remko Bonnema.
1999. Evaluation of the NLP components of the
OVIS2 spoken dialogue system. In Frank van
Eynde, Ineke Schuurman, and Ness Schelkens, ed-
itors, Computational Linguistics in the Netherlands
1998, pages 213?229. Rodopi Amsterdam.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL,
pages 947?953, Morristown, NJ, USA.
Yi Zhang and Rui Wang. 2009. Cross-domain depen-
dency parsing using a deep linguistic grammar. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 378?386, Suntec, Singapore,
August.
33
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1?10,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
What?s in a p-value in NLP?
Anders S?gaard, Anders Johannsen, Barbara Plank, Dirk Hovy and Hector Martinez
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
In NLP, we need to document that our pro-
posed methods perform significantly bet-
ter with respect to standard metrics than
previous approaches, typically by re-
porting p-values obtained by rank- or
randomization-based tests. We show that
significance results following current re-
search standards are unreliable and, in ad-
dition, very sensitive to sample size, co-
variates such as sentence length, as well as
to the existence of multiple metrics. We
estimate that under the assumption of per-
fect metrics and unbiased data, we need a
significance cut-off at ?0.0025 to reduce
the risk of false positive results to <5%.
Since in practice we often have consider-
able selection bias and poor metrics, this,
however, will not do alone.
1 Introduction
In NLP, we try to improve upon state of the art
language technologies, guided by experience and
intuition, as well as error analysis from previous
experiments, and research findings often consist in
system comparisons showing that System A is bet-
ter than System B.
Effect size, i.e., one system?s improvements
over another, can be seen as a random variable.
If the random variable follows a known distribu-
tion, e.g., a normal distribution, we can use para-
metric tests to estimate whether System A is bet-
ter than System B. If it follows a normal dis-
tribution, we can use Student?s t-test, for exam-
ple. Effect sizes in NLP are generally not nor-
mally distributed or follow any of the other well-
studied distributions (Yeh, 2000; S?gaard, 2013).
The standard significance testing methods in NLP
are therefore rank- or randomization-based non-
parametric tests (Yeh, 2000; Riezler and Maxwell,
2005; Berg-Kirkpatrick et al., 2012). Specifi-
cally, most system comparisons across words, sen-
tences or documents use bootstrap tests (Efron and
Tibshirani, 1993) or approximate randomization
(Noreen, 1989), while studies that compare perfor-
mance across data sets use rank-based tests such as
Wilcoxon?s test.
The question we wish to address here is: how
likely is a research finding in NLP to be false?
Naively, we would expect all reported findings to
be true, but significance tests have their weak-
nesses, and sometimes researchers are forced
to violate test assumptions and basic statistical
methodology, e.g., when there is no one estab-
lished metric, when we can?t run our models on
full-length sentences, or when data is biased. For
example, one such well-known bias from the tag-
ging and parsing literature is what we may refer to
as the WSJ FALLACY. This is the false belief that
performance on the test section of the Wall Street
Journal (WSJ) part of the English Penn treebank
is representative for performance on other texts in
English. In other words, it is the belief that our
samples are always representative. However, (the
unawareness of) selection bias is not the only rea-
son research findings in NLP may be false.
In this paper, we critically examine significance
results in NLP by simulations, as well as running
a series of experiments comparing state-of-the-art
POS taggers, dependency parsers, and NER sys-
tems, focusing on the sensitivity of p-values to var-
ious factors.
Specifically, we address three important factors:
Sample size. When system A is reported to be
better than system B, this may not hold across do-
mains (cf. WSJ FALLACY). More importantly,
though, it may not even hold on a sub-sample of
the test data, or if we added more data points to
the test set. Below, we show that in 6/10 of our
POS tagger evaluations, significant effects become
insignificant by (randomly) adding more test data.
1
Covariates. Sometimes we may bin our results
by variables that are actually predictive of the out-
come (covariates) (Simmons et al., 2011). In some
subfields of NLP, such as machine translation or
(unsupervised) syntactic parsing, for example, it
is common to report results that only hold for sen-
tences up to some length. If a system A is reported
to be better than a system B on sentences up to
some length, A need not be better than B, neither
for a different length nor in general, since sentence
length may actually be predictive of A being better
than B.
Multiple metrics. In several subfields of NLP,
we have various evaluation metrics. However, if
a system A is reported to be better than a system
B with respect to some metric M
1
, it need not be
better with respect to some other metric M
2
. We
show that even in POS tagging it is sometimes the
case that results are significant with respect to one
metric, but not with respect to others.
While these caveats should ideally be avoided
by reporting significance over varying sample
sizes and multiple metrics, some of these effects
also stem from the p-value cut-off chosen in the
NLP literature. In some fields, p-values are re-
quired to be much smaller, e.g., in physics, where
the 5   criterion is used, and maybe we should also
be more conservative in NLP?
We address this question by a simulation of the
interaction of type 1 and type 2 error in NLP and
arrive at an estimate that more than half of research
findings in NLP with p < 0.05 are likely to be
false, even with a valid metric and in the absence
of selection bias. From the same simulations, we
propose a new cut-off level at 0.0025 or smaller
for cases where the metric can be assumed to be
valid, and where there is no selection bias.
1
We
briefly discuss what to do in case of selection bias
or imperfect metrics.
Note that we do not discuss false discovery rate
control or family wise error rate procedures here.
While testing with different sample sizes could
be be considered multiple hypothesis testing, as
pointed out by one of our anonymous reviewers,
NLP results should be robust across sample sizes.
Note that the p < 0.0025 cut-off level corresponds
1
In many fields, including NLP, it has become good prac-
tice to report actual p-values, but we still need to understand
how significance levels relate to the probability that research
findings are false, to interpret such values. The fact that we
propose a new cut-off level for the ideal case with perfect
metrics and no bias does not mean that we do not recommend
reporting actual p-values.
to a Bonferroni correction for a family of m = 20
hypotheses.
Our contributions
Several authors have discussed significance test-
ing in NLP before us (Yeh, 2000; Riezler and
Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but
while our discussion touches on many of the same
topics, this paper is to the best of our knowledge
the first to:
a) show experimentally how sensitive p-values
are to sample size, i.e., that in standard NLP
experiments, significant effects may actually
disappear by adding more data.
b) show experimentally that multiple metrics
and the use of covariates in evaluation in-
crease the probability of positive test results.
c) show that even under the assumption of per-
fect metrics and unbiased data, as well as our
estimates of type 1 and 2 error in NLP, you
need at least p < 0.0025 to reduce the prob-
ability of a research finding being false to be
< 5%.
2 Significance testing in NLP
Most NLP metric for comparing system outputs
can be shown to be non-normally distributed
(S?gaard, 2013) and hence, we generally cannot
use statistical tests that rely on such an assump-
tion, e.g., Student?s t-test. One alternative to such
tests are non-parametric rank-based tests such as
Wilcoxon?s test. Rank-based tests are sometimes
used in NLP, and especially when the number of
observations is low, e.g., when evaluating perfor-
mance across data sets, such tests seem to be the
right choice (Demsar, 2006; S?gaard, 2013). The
draw-back of rank-based tests is their relatively
weak statistical power. When we reduce scores to
ranks, we throw away information, and rank-based
tests are therefore relatively conservative, poten-
tially leading to high type 2 error rate ( , i.e., the
number of false negatives over trials). An alterna-
tive, however, are randomization-based tests such
as the bootstrap test (Efron and Tibshirani, 1993)
and approximate randomization (Noreen, 1989),
which are the de facto standards in NLP. In this
paper, we follow Berg-Kirkpatrick et al. (2012) in
focusing on the bootstrap test. The bootstrap test is
non-parametric and stronger than rank-based test-
ing, i.e., introduces fewer type 2 errors. For small
samples, however, it does so at the expense of a
2
higher type 1 error (?, i.e., the number of false
positives). The reason for this is that for the boot-
strap test to work, the original sample has to cap-
ture most of the variation in the population. If the
sample is very small, though, this is likely not the
case. Consequently, with small sample sizes, there
is a risk that the calculated p-value will be arti-
ficially low?simply because the bootstrap sam-
ples are too similar. In our experiments below, we
make sure only to use bootstrap when sample size
is > 200, unless otherwise stated. In our experi-
ments, we average across 3 runs for POS and NER
and 10 runs for dependency parsing.
DOMAIN #WORDS TASKS
POS Dep. NER
CONLL 2007
Bio 4k ?
Chem 5k ?
SWITCHBOARD 4
Spoken 162k ?
ENGLISH WEB TREEBANK
Answers 29k ? ?
Emails 28k ? ?
Newsgrs 21k ? ?
Reviews 28k ? ?
Weblogs 20k ? ?
WSJ 40k ? ?
FOSTER
Twitter 3k ?
CONLL 2003
News 50k ?
Table 1: Evaluation data.
3 Experiments
Throughout the rest of the paper, we use four run-
ning examples: a synthetic toy example and three
standard experimental NLP tasks, namely POS
tagging, dependency parsing and NER. The toy
example is supposed to illustrate the logic behind
our reasoning and is not specific to NLP. It shows
how likely we are to obtain a low p-value for the
difference in means when sampling from exactly
the same (Gaussian) distributions. For the NLP
setups (2-4), we use off-the-shelf models or avail-
able runs, as described next.
3.1 Models and data
We use pre-trained models for POS tagging and
dependency parsing. For NER, we use the output
of the best performing systems from the CoNLL
2003 shared task. In all three NLP setups, we
compare the outcome of pairs of systems. The
data sets we use for each of the NLP tasks are
listed in Table 1 (Nivre et al., 2007a; Foster et
Figure 1: Accuracies of LAPOS VS. STANFORD
across 10 data sets.
al., 2011; Tjong Kim Sang and De Meulder, 2003,
LDC99T42; LDC2012T13).
POS tagging. We compare the performance
of two state-of-the-art newswire taggers across 10
evaluation data sets (see Table 1), namely the LA-
POS tagger (Tsuruoka et al., 2011) and the STAN-
FORD tagger (Toutanova et al., 2003), both trained
on WSJ00?18. We use the publicly available pre-
trained models from the associated websites.
2
Dependency parsing. Here we compare the
pre-trained linear SVM MaltParser model for En-
glish (Nivre et al., 2007b) to the compositional
vector grammar model for the Stanford parser
(Socher et al., 2013). For this task, we use the sub-
set of the POS data sets that comes with Stanford-
style syntactic dependencies (cf. Table 1), exclud-
ing the Twitter data set which we found too small
to produce reliable results.
NER. We use the publicly available runs of
the two best systems from the CoNLL 2003
shared task, namely FLORIAN (Florian et al.,
2003) and CHIEU-NG (Chieu and Ng, 2003).
3
3.2 Standard comparisons
POS tagging. Figure 1 shows that the LAPOS
tagger is marginally better than STANFORD on
macro-average, but it is also significantly better? If
we use the bootstrap test over tagging accuracies,
the difference between the two taggers is only sig-
nificant (p < 0.05) in 3/10 cases (see Table 2),
namely SPOKEN, ANSWERS and REVIEWS. In
two of these cases, LAPOS is significantly better
2
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/ and http://nlp.stanford.
edu/software/tagger.shtml
3
http://www.cnts.ua.ac.be/conll2003/
ner/
3
TA (b) UA (b) SA (b) SA(w)
Bio 0.3445 0.0430 0.3788 0.9270
Chem 0.3569 0.2566 0.4515 0.9941
Spoken <0.001 <0.001 <0.001 <0.001
Answers <0.001 0.0143 <0.001 <0.001
Emails 0.2020 <0.001 0.1622 0.0324
Newsgrs 0.3965 0.0210 0.1238 0.6602
Reviews 0.0020 0.0543 0.0585 0.0562
Weblogs 0.2480 0.0024 0.2435 0.9390
WSJ 0.4497 0.0024 0.2435 0.9390
Twitter 0.4497 0.0924 0.1111 0.7853
Table 2: POS tagging p-values across tagging ac-
curacy (TA), accuracy for unseen words (UA) and
sentence-level accuracy (SA) with bootstrap (b)
and Wilcoxon (w) (p < 0.05 gray-shaded).
LAS UAS
Answers 0.020 <0.001
Emails 0.083 <0.001
Newsgroups 0.049 <0.001
Reviews <0.001 <0.001
Weblogs <0.001 <0.001
WSJ <0.001 <0.001
Table 3: Parsing p-values (MALT-LIN
VS. STANFORD-RNN) across LAS and UAS
(p < 0.05 gray-shaded).
than STANFORD, but in one case it is the other way
around. If we do a Wilcoxon test over the results
on the 10 data sets, following the methodology
in Demsar (2006) and S?gaard (2013), the differ-
ence, which is ?0.12% on macro-average, is not
significant (p ? 0.1394). LAPOS is thus not sig-
nificantly better than STANFORD across data sets,
but as we have already seen, it is significantly bet-
ter on some data sets. So if we allow ourselves
to cherry-pick our data sets and report significance
over word-level tagging accuracies, we can at least
report significant improvements across a few data
sets.
Dependency parsing. Using the bootstrap test
over sentences, we get the p-values in Table 3.
We see that differences are always significant
wrt. UAS, and in most cases wrt. LAS.
NER. Here we use the macro-f
1
as our stan-
dard metric. FLORIAN is not significantly bet-
ter than CHIEU-NG with p < 0.05 as our cut-
off (p ? 0.15). The two systems were also re-
ported to have overlapping confidence intervals in
the shared task.
3.3 p-values across metrics
In several NLP subfields, multiple metrics are in
use. This happens in dependency parsing where
multiple metrics (Schwartz et al., 2011; Tsarfaty
et al., 2012) have been proposed in addition to un-
labeled and labeled attachment scores, as well as
exact matches. Perhaps more famously, in ma-
chine translation and summarization it is com-
mon practice to use multiple metrics, and there
exists a considerable literature on that topic (Pa-
pineni et al., 2002; Lin, 2004; Banerjee and Lavie,
2005; Clark et al., 2011; Rankel et al., 2011).
Even in POS tagging, some report tagging ac-
curacies, tagging accuracies over unseen words,
macro-averages over sentence-level accuracies, or
number of exact matches.
The existence of several metrics is not in it-
self a problem, but if researchers can cherry-pick
their favorite metric when reporting results, this
increases the a priori chance of establishing sig-
nificance. In POS tagging, most papers report sig-
nificant improvements over tagging accuracy, but
some report significant improvements over tag-
ging accuracy of unknown words, e.g., Denis and
Sagot (2009) and Umansky-Pesin et al. (2010).
This corresponds to the situation in psychology
where researchers cherry-pick between several de-
pendent variables (Simmons et al., 2011), which
also increases the chance of finding a significant
correlation.
Toy example. We draw two times 100 val-
ues from identical (0, 1)-Gaussians 1000 times
and calculate a t-test for two independent sam-
ples. This corresponds to testing the effect size
between two systems on a 1000 randomly cho-
sen test sets with N = 100. Since we are sam-
pling from the same distribution, the chance of
p < ? should be smaller than ?. In our simula-
tion, the empirical chance of obtaining p < 0.01
is .8%, and the chance of obtaining p < 0.05 is
4.8%, as expected. If we simulate a free choice
between two metrics by introducing choice be-
tween a pair of samples and a distorted copy of
that pair (inducing random noise at 10%), simu-
lating the scenario where we have a perfect metric
and a suboptimal metric, the chance of obtaining
p < 0.05 is 10.0%. We see a significant correla-
tion (p < 0.0001) between Pearson?s ? between
the two metrics, and the p-value. The less the two
metrics are correlated, the more likely we are to
obtain p < 0.05. If we allow for a choice between
two metrics, the chance of finding a significant dif-
ference increases considerably. If the two metrics
are identical, but independent (introducing a free
choice between two pairs of samples), we have
4
P (A_B) = P (A) + P (B)  P (A)P (B), hence
the chance of obtaining p < 0.01 is 1.9%, and the
chance of obtaining p < 0.05 is 9.75%.
POS tagging. In our POS-tagging experiments,
we saw a significant improvement in 3/10 cases
following the standard evaluation methodology
(see Table 2). If we allow for a choice between
tagging accuracy and sentence-level accuracy, we
see a significant improvement in 4/10 cases, i.e.,
for 4/10 data sets the effect is significance wrt. at
least one metric. If we allow for a free choice be-
tween all three metrics (TA, UA, and SA), we ob-
serve significance in 9/10 cases. This way the ex-
istence of multiple metrics almost guarantees sig-
nificant differences. Note that there are only two
data sets (Answers and Spoken), where all metric
differences appear significant.
Dependency parsing. While there are multi-
ple metrics in dependency parsing (Schwartz et
al., 2011; Tsarfaty et al., 2012), we focus on
the two standard metrics: labeled (LAS) and un-
labeled attachment score (UAS) (Buchholz and
Marsi, 2006). If we just consider the results in
Table 3, i.e., only the comparison of MALT-LIN
VS. STANFORD-RNN, we observe significant im-
provements in all cases, if we allow for a free
choice between metrics. Bod (2000) provides a
good example of a parsing paper evaluating mod-
els using different metrics on different test sets.
Chen et al. (2008), similarly, only report UAS.
NER. While macro-f
1
is fairly standard in
NER, we do have several available multiple met-
rics, including the unlabeled f
1
score (collapsing
all entity types), as well as the f
1
scores for each
of the individual entity types (see Derczynski and
Bontcheva (2014) for an example of only report-
ing f
1
for one entity type). With macro-f
1
and
f
1
for the individual entity types, we observe that,
while the average p-value for bootstrap tests over
five runs is around 0.15, the average p-value with a
free choice of metrics is 0.02. Hence, if we allow
for a free choice of metrics, FLORIAN comes out
significantly better than CHIEU-NG.
3.4 p-values across sample size
We now show that p-values are sensitive to sam-
ple size. While it is well-known that studies with
low statistical power have a reduced chance of
detecting true effects, studies with low statistical
power are also more likely to introduce false pos-
itives (Button et al., 2013). This, combined with
the fact that free choice between different sample
Figure 2: The distribution of p-values with (above)
and without (below) multiple metrics.
Figure 3: POS tagging p-values varying sample
sizes (p < 0.05 shaded).
sizes also increases the chance of false positives
(Simmons et al., 2011), is a potential source of er-
ror in NLP.
Toy example. The plot in Figure 2 shows the
distribution of p-values across 1000 bootstrap tests
(above), compared to the distribution of p-values
with a free choice of four sample sizes. It is clear
that the existence of multiple metrics makes the
probability of a positive result much higher.
POS tagging. The same holds for POS tag-
ging. We plot the p-values across various sample
sizes in Figure 3. Note that even when we ignore
the smallest sample size (500 words), where re-
sults may be rather unreliable, it still holds that for
Twitter, Answers, Newsgrs, Reviews, Weblogs and
WSJ, i.e., more than half of the data sets, a sig-
nificant result (p < 0.05) becomes insignificant
by increasing the sample size. This shows how
unreliable significance results in NLP with cut-off
p < 0.05 are.
5
Figure 4: Parsing p-values varying sample sizes
(p < 0.05 shaded)
Figure 5: NER p-values varying sample sizes (p <
0.05 shaded)
Dependency parsing. We performed simi-
lar experiments with dependency parsers, seeing
much the same picture. Our plots are presented in
Figure 4. We see that while effect sizes are al-
ways significant wrt. UAS, LAS differences be-
come significant when adding more data in 4/6
cases. An alternative experiment is to see how
often a bootstrap test at a particular sample size
comes out significant. The idea is to sample, say,
10% of the test data 100 times and report the ra-
tio of positive results. We only present the results
for MALT-LIN VS. STANFORD-RNN in Table 4,
but the full set of results (including comparisons of
more MaltParser and Stanford parser models) are
made available at http://lowlands.ku.dk.
For MALT-LIN VS. STANFORD-RNN differ-
ences on the full Emails data set are consistently
insignificant, but on small sample sizes we do get
significant test results in more than 1/10 cases. We
see the same picture with Newsgrs and Reviews.
On Weblogs and WSJ, the differences on the full
data sets are consistently significant, but here we
see that the test is underpowered at small sam-
ple sizes. Note that we use bootstrap tests over
sentences, so results with small samples may be
somewhat unreliable. In sum, these experiments
show how small sample sizes not only increase the
chance of false negatives, but also the chance of
false positives (Button et al., 2013).
NER. Our plots for NER are presented in Fig-
ure 5. Here, we see significance at small sam-
ple sizes, but the effect disappears with more data.
This is an example of how underpowered studies
may introduce false positives (Button et al., 2013).
3.5 p-values across covariates
Toy example. If we allow for a choice between
two subsamples, using a covariate to single out a
subset of the data, the chance of finding a signifi-
cant difference increases. Even if we let the subset
be a random 50-50 split, the chance of obtaining
p < 0.01 becomes 2.7%, and the chance of obtain-
ing p < 0.05 is 9.5%. If we allow for both a choice
of dependent variables and a random covariate, the
chance of obtaining p < 0.01 is 3.7%, and the
chance of obtaining p < 0.05 is 16.2%. So iden-
tical Gaussian variables will appear significantly
different in 1/6 cases, if our sample size is 100,
and if we are allowed a choice between two iden-
tical, but independent dependent variables, and a
choice between two subsamples provided by a ran-
dom covariate.
POS We see from Figure 6 that p-values are
also very sensitive to sentence length cut-offs. For
instance, LAPOS is significantly (p < 0.05) bet-
ter than STANFORD on sentences shorter than 16
words in EMAILS, but not on sentences shorter
than 14 words. On the other hand, when longer
sentences are included, e.g., up to 22 words, the
effect no longer appears significant. On full sen-
tence length, four differences seem significant, but
if we allow ourselves to cherry-pick a maximum
sentence length, we can observe significant differ-
ences in 8/10 cases.
Figure 6: POS tagging p-values varying sentence
length (p < 0.05 shaded)
We observe similar results in Dependency
parsing and NER when varying sentence length,
but do not include them here for space rea-
sons. The results are available at http://
lowlands.ku.dk. We also found that other
covariates are used in evaluations of dependency
parsers and NER systems. In dependency pars-
ing, for example, parsers can either be evaluated
6
N Emails Newsgrs Reviews Weblogs WSJ
LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
10% 14 % 100 % 9 % 100 % 33% 100 % 42 % 99 % 28 % 75 %
25% 15 % 100 % 23 % 100 % 52% 100 % 68 % 100 % 27 % 98 %
50% 19 % 100 % 25 % 100 % 78% 100 % 100 % 100 % 60 % 100 %
75% 22 % 100 % 41 % 100 % 97% 100 % 100 % 100 % 80 % 100 %
100% 0 % 100 % 36 % 100 % 100% 100 % 100 % 100 % 100 % 100 %
Table 4: Ratio of positive results (p < 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N )
.
on naturally occurring text such as in our experi-
ments or at tailored test suites, typically focusing
on hard phenomena (Rimell et al., 2009). While
such test suites are valuable resources, cf. Man-
ning (2011), they do introduce free choices for re-
searchers, increasing the a priori chance of posi-
tive results. In NER, it is not uncommon to leave
out sentences without any entity types from eval-
uation data. This biases evaluation toward high
recall systems, and the choice between including
them or not increases chances of positive results.
4 How likely are NLP findings to be
false?
The previous sections have demonstrated how
many factors can contribute to reporting an erro-
neously significant result. Given those risks, it is
natural to wonder how likely we are as a field to
report false positives. This can be quantified by
the positive predictive value (PPV), or probability
that a research finding is true. PPV is defined as
(1  )R
R  R+?
(1)
The PPV depends on the type 1 and 2 error rates
(? and  ) and the ratio of true relations over null
relations in the field (R) (Ioannidis, 2005).
R. The likelihood that a research finding is true
depends on the ratio of true relations over null re-
lations in the field, usually denoted R (Ioannidis,
2005). Out of the systems that researchers in the
field would test out (not rejecting them a priori),
how many of them are better than the current state
of the art? The a priori likelihood of a relation be-
ing true, i.e., a new system being better than state
of the art, is R/(R+1). Note that while the space
of reasonably motivated methods may seem big to
researchers in the field, there is often more than
one method that is better than the current state of
the art. Obviously, as the state of the art improves,
R drops. On the other hand, if R becomes very
low, researchers are likely to move on to new ap-
plications where R is higher.
The type 1 error rate (?) is also known as the
false positive rate, or the likelihood to accept a
non-significant result. Since our experiments are
fully automated and deterministic, and precision
usually high, the type 1 error rate is low in NLP.
What is not always appreciated in the field is that
this should lead us to expect true effects to be
highly significant with very low p-values, much
like in physics. The type 2 error rate ( ) is the
false negative rate, i.e., the likelihood that a true
relation is never found. This factors into the recall
of our experimental set-ups.
So what values should we use to estimate PPV?
Our estimate for R (how often reasonable hy-
potheses lead to improvements over state of the
art) is around 0.1. This is based on a sociolog-
ical rather than an ontological argument. With
? = 0.05 and R = 0.1, researchers get positive
results inR+(1 R)? cases, i.e.,? 1/7 cases. If
researchers needed to test more than 7 approaches
to ?hit the nail?, they would never get to write pa-
pers. With ? = 0.05, and   set to 0.5, we find that
the probability of a research finding being true ?
given there is no selection bias and with perfectly
valid metrics ? is just 50%:
PPV =
(1  )R
R  R+?
=
0.5?0.1
0.1 0.05+0.05
=
0.05
0.1
= 0.5
(2)
In other words, if researchers do a perfect experi-
ment and report p < 0.05, the chance of that find-
ing being true is the chance of seeing tail when
flipping a coin. With p < 0.01, the chance is 5/6,
i.e., the chance of not getting a 3 when rolling a
die. Of course these parameters are somewhat ar-
bitrary. Figure 7 shows PPV for various values of
?.
In the experiments in Section 3, we consistently
used the standard p-value cut-off of 0.05. How-
ever, our experiments have shown that significance
results at this threshold are unreliable and very
sensitive to the choice of sample size, covariates,
or metrics. Based on the curves in Figure 7, we
7
Figure 7: PPV for different ? (horizontal line is PPV for p = 0.05, vertical line is ? for PPV=0.95).
could propose a p-value cut-off at p < 0.0025.
This is the cut-off that ? in the absence of bias and
with perfect metrics ? gives us the level of con-
fidence we expect as a research community, i.e.,
PPV = 0.95. Significance results would thus be
more reliable and reduce type 1 error.
5 Discussion
Incidentally, the p < 0.0025 cut-off also leads to
a 95% chance of seeing the same effect on held-
out test data in Berg-Kirkpatrick et al. (2012) (see
their Table 1, first row). The caveat is that this
holds only in the absence of bias and with perfect
metrics. In reality, though, our data sets are of-
ten severely biased (Berg-Kirkpatrick et al., 2012;
S?gaard, 2013), and our metrics are far from per-
fect (Papineni et al., 2002; Lin, 2004; Banerjee
and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et
al., 2012). Here, we discuss how to address these
challenges.
Selection bias. The WSJ FALLACY (Section
1) has been widely discussed in the NLP litera-
ture (Blitzer et al., 2006; Daume III, 2007; Jiang
and Zhai, 2007; Plank and van Noord, 2011). But
if our test data is biased, how do we test whether
System A performs better than System B in gen-
eral? S?gaard (2013) suggests to predict signif-
icance across data sets. This only assumes that
data sets are randomly chosen, e.g., not all from
newswire corpora. This is also standard practice in
the machine learning community (Demsar, 2006).
Poor metrics. For tasks such as POS tagging
and dependency parsing, our metrics are subopti-
mal (Manning, 2011; Schwartz et al., 2011; Tsar-
faty et al., 2012). System A and System B may
perform equally well as measured by some met-
ric, but contribute very differently to downstream
tasks. Elming et al. (2013) show how parsers
trained on different annotation schemes lead to
very different downstream results. This suggests
that being wrong with respect to a gold standard,
e.g., choosing NP analysis over a ?correct? DP
analysis, may in some cases lead to better down-
stream performance. See the discussion in Man-
ning (2011) for POS tagging. One simple ap-
proach to this problem is to report results across
available metrics. If System A improves over Sys-
tem B wrt. most metrics, we obtain significance
against the odds. POS taggers and dependency
parsers should also be evaluated by their impact
on downstream performance, but of course down-
stream tasks may also introduce multiple metrics.
6 Conclusion
In sum, we have shown that significance results
with current research standards are unreliable, and
we have provided a more adequate p-value cut-off
under the assumption of perfect metrics and unbi-
8
ased data. In the cases where these assumptions
cannot be met, we suggest reporting significance
results across datasets wrt. all available metrics.
Acknowledgements
We would like to thank the anonymous review-
ers, as well as Jakob Elming, Matthias Gondan,
and Natalie Schluter for invaluable comments and
feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: an automatic metric for MT evaluation with
improved correlation with human judgments. In
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In EMNLP.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Rens Bod. 2000. Parsing with the shortest derivation.
In COLING.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In CoNLL.
Katherine Button, John Ioannidis, Claire Mokrysz,
Brian Nosek, Jonathan Flint, Emma Robinson, and
Marcus Munafo. 2013. Power failure: why small
sample size undermines the reliability of neuro-
science. Nature Reviews Neuroscience, 14:365?376.
Wenliang Chen, Youzheng Wu, and Hitoshi Isahara.
2008. Learning Reliable Information for Depen-
dency Parsing Adaptation. In COLING.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In CoNLL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In ACL.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30.
Pascal Denis and Beno??t Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC.
Leon Derczynski and Kalina Bontcheva. 2014.
Passive-aggressive sequence labeling with discrim-
inative post-editing for recognising person entities
in tweets. In EACL.
Bradley Efron and Robert Tibshirani. 1993. An intro-
duction to the bootstrap. Chapman & Hall, Boca
Raton, FL.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez Alonso, and
Anders S?gaard. 2013. Down-stream effects of
tree-to-dependency conversions. In NAACL.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
John Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2(8):696?701.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In WAS.
Chris Manning. 2011. Part-of-speech tagging from
97% to 100%: Is it time for some linguistics? In
CICLing.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In EMNLP-CoNLL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser:
a language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Eric Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley.
Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311?318, Philadelphia, Pennsylvania.
Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In ACL.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine sum-
marization systems. In EMNLP.
9
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance test-
ing for MT. In ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In EMNLP.
Roy Schwartz, and Omri Abend, Roi Reichart, and
Ari Rappoport. 2011. Neutralizing linguisti-
cally problematic annotations in unsupervised de-
pendency parsing evaluation. In ACL.
Joseph Simmons, Leif Nelson, and Uri Simonsohn.
2011. False-positive psychology: undisclosed flexi-
bility in data collection and analysis allows present-
ing anything as significant. Psychological Science,
22(11):1359?1366.
Richard Socher, John Bauer, Chris Manning, and An-
drew Ng. 2013. Parsing with compositional vector
grammars. In ACL.
Anders S?gaard. 2013. Estimating effect size across
datasets. In NAACL.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
In CoNLL.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for POS tagging of unknown words. In COLING.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL.
10
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2?7,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Robust Cross-Domain Sentiment Analysis for Low-Resource Languages
Jakob Elming Dirk Hovy Barbara Plank
Centre for Language Technology
University of Copenhagen
zmk867@hum.ku.dk,{dirk,bplank}@cst.dk
Abstract
While various approaches to domain adap-
tation exist, the majority of them requires
knowledge of the target domain, and ad-
ditional data, preferably labeled. For a
language like English, it is often feasible
to match most of those conditions, but in
low-resource languages, it presents a prob-
lem. We explore the situation when nei-
ther data nor other information about the
target domain is available. We use two
samples of Danish, a low-resource lan-
guage, from the consumer review domain
(film vs. company reviews) in a sentiment
analysis task. We observe dramatic perfor-
mance drops when moving from one do-
main to the other. We then introduce a
simple offline method that makes models
more robust towards unseen domains, and
observe relative improvements of more
than 50%.
1 Introduction
Sentiment analysis, the task of determining the
polarity of a text, is a valuable tool for gather-
ing information from the vast amount of opin-
ionated text produced today. It is actively used
in reputation management and consumer assess-
ment (Amig?o et al., 2012; Amig?o et al., 2013).
While supervised approaches achieve reasonable
performance (Mohammad et al., 2013), they are
typically highly domain-dependent. In fact, mov-
ing from one (source) domain to a different (tar-
get) domain will often lead to severe performance
drops (Blitzer et al., 2007; Daum?e et al., 2010).
This is mainly due to the models overfitting the
source (training) data, both in terms of its la-
bel and word distribution. The task of overcom-
ing this tendency is known as domain adaptation
(DA) (Blitzer et al., 2007; Daum?e et al., 2010).
There are three different approaches to DA: in
Supervised DA, labeled training data for the target
domain exists, in Unsupervised DA, data for the
target domain exists, but it is unlabeled. A third,
less investigated scenario is Blind DA: the target
domain is not known at all in advance. Super-
vised DA effectively counteracts domain-bias by
including labeled data from the target domain dur-
ing training, thus preventing overfitting to both the
label and the word distribution of the source. Un-
supervised methods usually rely either on external
data, in the form of gazetteers, dictionaries, or on
unlabeled data from the target domain. While they
do not prevent overfitting to the source domain?s
label distribution, the additional data acts as a reg-
ularizer by introducing a larger vocabulary.
However, both cases presuppose that we already
know the target domain and have data from it. In
many real-world settings, these conditions are not
met, especially when dealing with low-resource
languages. We thus need to regularize our models
independent of the possible target domains. Ef-
fectively, this means that we need to prevent our
models from memorizing the observed label distri-
bution, and from putting too much weight on fea-
tures that are predictive in the source domain, but
might not even be present in the target domain.
In this paper, we investigate sentiment analysis
for Danish, a low-resource language, and therefore
approach it as a Blind DA problem. We perform
experiments on two types of domains, namely re-
views for movies and companies. The challenge
lies in the fact that the label distribution (posi-
tive, negative, neutral) changes dramatically when
moving from one domain to the other, and many
highly predictive words in the company domain
(e.g., ?reliable?) are unlikely to carry over to the
movie domain, and vice versa. To the best of our
knowledge, this is the first study to perform senti-
ment analysis for Danish, a low-resource language
where relevant resources like polarity dictionaries
2
are hard to come by.
We present a simple offline-learning version in-
spired by previous work on corruptions (S?gaard,
2013), which also addresses the sparsity of avail-
able training data. Our method introduces a rela-
tive improvement on out-of-domain performance
by up to 54%.
2 Robust Learning
The main idea behind robust learning is to steer the
model away from overfitting the source domain.
Overfitting can occur either by
1. putting too much weight on certain features
(which might not be present in the target do-
main), or
2. over-using certain labels (since the label dis-
tribution on the target domain might differ).
One approach that has been proven to re-
duce overfitting is data corruption, also known as
dropout training (S?gaard and Johannsen, 2012;
S?gaard, 2013), which is a way of regularizing
the model by randomly leaving out features. In-
tuitively, this approach can be viewed as coercing
the learning algorithm to rely on more general, but
less consistent features. Rather than learning to
mainly trust the features that are highly predictive
for the given training data, the algorithm is encour-
aged to use the less predictive features, since the
highly predictive features might be deleted by the
corruption. Most prior work on dropout regular-
ization (S?gaard and Johannsen, 2012; Wang and
Manning, 2012; S?gaard, 2013) has used online
corruptions, i.e., the specific dropout function is
integrated into the learning objective and thus tied
to the specific learner. Here, we propose a simple
approximation, i.e., a wrapper function that cor-
rupts instances in an off-line fashion based on the
weights learned from a base model. The advan-
tage is that it can be used for any learning func-
tion, thereby abstracting away from the underlying
learner.
2.1 Our approach
Our off-line feature corruption algorithm works as
follows:
1. train an uncorrupted (base) model,
2. create k copies of the training data instances,
3. corrupt copies based on the feature weights of
the base model and an exponential function
(described below), and
4. train a new model on the corrupted training
data.
The advantages of this algorithm compared to
online corruption are
1. it is a wrapper method, so it becomes very
easy to move to a different learning algo-
rithm, and
2. corruption is done based on knowledge from
a full, uncorrupted model, which provides a
better picture of the overfitting.
This comes, however, at the cost of longer training
times, but in a low-resource language training time
is less of an issue.
Specifically, multiple copies of the training data
are used in the corrupted training stage. This re-
sults in each data point appearing in different, cor-
rupted versions, as visualized in Figure 1. The
copying process retains more of the information in
the training data, since it is unlikely that the same
feature is deleted in each copy. In our experiments,
we used k=5. Larger values of k resulted in longer
training times without improving performance.
1 11 1Original
1 1 11 1 111
1
1
1
Corrupted Copies?!?!?111
Figure 1: Example of an original feature vector
and its multiple corrupted copies.
We experiment with a random and a biased
corruption approach. The first approach (S?gaard
and Johannsen, 2012) does not utilize the feature
weight information from the base model, but ran-
domly deletes 10% of the features. We use this
approach to test whether an effect is merely the
result of deleting features.
The biased approach, on the other hand, tar-
gets the most predictive features in the base model
for deletion. We use a function that increases
the probability of deleting a feature exponentially
3
0	 ?
25	 ?
50	 ?
75	 ?
100	 ?
-??0.33	 ? -??0.23	 ? -??0.13	 ? -??0.03	 ? 0.07	 ? 0.17	 ? 0.27	 ? 0.37	 ?
%	 ?
Feature	 ?weight	 ?
Figure 2: The corruption function conditioning the
probability of deleting a feature in a positive in-
stance on its weight in the Scope baseline model.
with its model weight. That is, a highly predic-
tive feature (with a high weight in the model) will
be more likely to be deleted. A feature with a
low weight, on the other hand, has a much lower
chance of being deleted. Figure 2 visualizes the
exponential corruption function used. The func-
tion assigns the lowest weighted feature of the
model zero likelihood of deletion, and the highest
weighted feature a 0.9 likelihood of deletion. In
order to mainly corrupt the highly predictive fea-
tures, the exponential function is shifted to an area
with a steeper gradient. That is, instead of scal-
ing to the exponential function between 0 and 1, it
is scaled to the area between -3 and 2 (parameters
set experimentally on the development set). The
corruption probability p
cor
of deleting a feature f
given a category c is defined as
p
cor
(f |c) =
exp(
w(f |c)?w
min
(c)
w
max
(c)?w
min
(c)
?5?3)?exp(?3)
exp(2)?exp(?3)
? 0.9
(1)
with w(f |c) being the weight of f given the in-
stance category c in the model, and w
min
(c) and
w
max
(c) being the lowest and highest weights of
the model respectively for category c.
3 Experiments
Our experiments use Danish reviews from two do-
mains: movies and companies. The specifications
of the data sets are listed in Table 1 and Figure 3.
The two data sets differ considerably in data size
and label distribution.
DOMAIN SPLIT REVIEWS WORDS
Scope Train 8,718 749,952
Dev 1,198 107,351
Test 2,454 210,367
Total 12,370 1,067,670
Trustpilot Train 170,137 7,180,160
Dev 23,958 1,000,443
Test 48,252 2,040,956
Total 242,347 10,221,559
Table 1: Overview of data set and split sizes in
number of reviews and number of words.
3.1 Data preparation
The movie reviews are downloaded from a Dan-
ish movie website, www.scope.dk. They con-
tain reviews of 829 movies, each rated on a scale
from 1 to 6 stars. The company reviews are
downloaded from a Danish consumer review web-
site, www.trustpilot.dk. They consist of re-
views of 19k companies, each rated between 1 and
5 stars.
Similar to prior work on sentiment analy-
sis (Blitzer et al., 2007), the star ratings are binned
into the three standard categories; positive, neu-
tral, and negative. For the Scope data, a 6 star rat-
ing is considered positive, a 3 or 4 rating neutral,
and a 1 star rating negative. 2 and 5 star ratings are
excluded to retain more distinct categories. For the
Trustpilot data, 5 star reviews are categorized as
positive, 3 stars as neutral, and 1 star as negative.
Similar to Scope data, 2 and 4 stars are excluded.
0%
25%
50%
75%
100%
scope trustpilot
84.85%
27.36%
5.40%
60.85%
9.75%11.79%
negative neutral positive
Figure 3: Label distribution in the two data sets.
Apart from the difference in size, the two data
sets also differ in the distribution of categories (see
Figure 3). This means that a majority label base-
line estimated from one would perform horribly on
4
- N-gram presence for token lengths 1 to 4
- Skip-grams (n-gram with one middle word replaced by *) presence for token lengths 3 to 4
- Character n-gram presence for entire document string for token lengths 1 to 5
- Brown clusters (Brown et al., 1992; Liang, 2005) estimated on the source training data
- Number of words with only upper case characters
- Number of contiguous sequences of question marks, exclamation marks, or both
- Presence of question mark or exclamation mark in last word
- Number of words with characters repeated more than two times e.g. ?sooooo?
- Number of negated contexts using algorithm described in the text
- Most positive, most negative, or same amount of polar words according to a sentiment lexicon
Table 2: Feature set description.
the other domain. For instance, the majority base-
line on Scope (assigning neutral to all instances)
achieves a 5% accuracy on Trustpilot data. Sim-
ilarly, the Trustpilot majority baseline obtains an
accuracy of 27% on Scope data by always assign-
ing positive.
We choose not to balance the data sets, in keep-
ing with the blind DA setup. Knowing the target
label distribution can help greatly, but we can as-
sume no prior knowledge about that. In fact, the
difference in label distribution is one of the ma-
jor challenges when predicting on out-of-domain
data.
3.2 Features
The features we use (described in Table 2) are
inspired by the top performing system from the
SemEval-2013 task on Twitter sentiment analy-
sis (Mohammad et al., 2013).
One main difference is that Mohammad et al.
(2013) had several high-quality sentiment lexicons
at their disposal, shown to be effective. Working
with a low-resource language, we only have ac-
cess to a single lexicon created by an MA student
(containing 2248 positive and 4736 negative word
forms). Our lexicon features are therefore simpler,
i.e., based on whether words are considered pos-
itive or negative in the lexicon, as opposed to the
score-based features in Mohammad et al. (2013).
We adopted the simple negation scope reso-
lution algorithm directly from Mohammad et al.
(2013). Anything appearing in-between a negation
token
1
and the first following punctuation mark is
considered a negated context. This works well for
English, but Danish has different sentence adver-
bial placement, so the negation may also appear
1
We use the following negation markers: ikke, ingen, in-
tet, ingenting, aldrig, hverken. n?ppe.
after the negated constituent. This simple algo-
rithm is therefore less likely to be beneficial in a
Danish system. We plan to extend the system for
better negation handling in future work.
3.3 Corruption
The corruption happens at the feature-instance
level. When we refer to the deletion of a feature
in the following, it does not mean the deletion of
this feature throughout the training data, but the
deletion of a single instance in a feature vector (cf.
Figure 1).
Corrupting the Scope data deleted 9.24% of all
feature instances in the training data. Most fea-
tures are deleted from positive instances (16.7%
of all features) and least from the majority neutral
instances (6.5% of all features). Only 9.4% of the
minority class negative are deleted.
For Trustpilot, the corruption deleted 11.73%
of the feature instances. The pattern is the same
here, though more extreme. The majority positive
class has the fewest features removed (2.2%), the
minority class neutral has 22.8% of its features
deleted, and the negative class has an overwhelm-
ing 35.6% of its features deleted.
The fact that the corruption function does not
take the weight distribution of the individual la-
bels into account, and therefore corrupts the data
of some labels much more than others, does prove
to be a problem. We will get back to this in the
results section.
4 Results
Table 3 shows the results of the experiments. We
report both accuracy and the average f-score for
positive and negative instances (AF).
AF is the official SemEval-2013 metric (Nakov
et al., 2013). It offers a more detailed insight into
5
In-domain Out-of-domain
System Dev set Test set Dev set Test set
Acc. AF Acc. AF Acc. AF Acc. AF
Scope baseline 84.2 75.6 82.4 72.1 35.5 43.3 36.0 44.3
Scope random corrupt 83.1 72.9 82.7 72.8 35.7 43.9 36.2 44.5
Scope biased corrupt 82.7 72.6 81.5 70.6 55.5 48.6 55.5 44.9
Trustpilot baseline 94.8 91.8 94.3 91.2 39.9 45.0 39.9 46.2
Trustpilot random corrupt 94.8 91.7 94.4 91.4 39.8 45.6 40.0 46.0
Trustpilot biased corrupt 93.7 89.0 93.4 89.5 43.6 45.7 43.4 44.7
Table 3: Evaluation on development and test sets measured in accuracy (Acc.) and the average f-score
for positive and negative instances (AF).
the model?s performance on the two ?extreme?
classes, but it is highly skewed, since it ignores the
neutral label. As we have seen in our data, this
can make up the majority of the instances. Ac-
curacy has the advantage that it provides a clear
picture of how often the system makes a correct
prediction, but can be harder to interpret when the
data sets are highly skewed in favor of one class.
The results show that randomly corrupting the
data (cf. S?gaard and Johannsen (2012), Sec. 5)
does not have much influence on the model. Per-
formance on in- and out-of-domain data is similar
to the baseline system. This indicates that we can
not just delete any features to help domain adapta-
tion.
The biased corruption model, on the other hand,
makes informed choices about deleting features.
As expected, this leads to a drop on in-domain
data, since we are underfitting the model. Con-
sidering that the algorithm is targeting the most
important features for this particular domain, the
drop is relatively small, though. The percentage
of features deleted is roughly the same as the 10%
for the random system (see section 3.3).
With the exception of AF on Trustpilot test,
our biased corruption approach always increases
out-of-domain performance. The increase is es-
pecially notable when the model is trained on the
small domain, Scope. On both test and develop-
ment, the corruption approach increases accuracy
more than 50%. On the AF measure, the increase
is smaller, which indicates that most of the in-
crease stems from the neutral category. On the
test set, the f-score for positive labels increases
from 49.1% to 71.2%, neutral increases from
13.5% to 18.4%, but negative decreases from
39.4% to 27.5%. The fact that f-score decreases on
negative indicates that the corruption algorithm
is too aggressive for this category. We previously
saw that this was the category where 35% of the
features are deleted.
The lower degree of overfitting in the corrupted
model is also reflected in the overall label distri-
bution. For the Scope system, the training data
has a negative/neutral/positive distribution (in per-
centages) of 27/61/12. The baseline predictions
on the Trustpilot data has a very similar distribu-
tion of 30/63/7, while the corrupted system results
in a very different distribution of 52/35/13, which
is more similar to the Trustpilot gold distribution
of 85/5/10. The KL divergence between the base-
line system and the Trustpilot data is 1.26, while
for the corrupted system it is 0.46.
5 Related Work
There is a large body of prior work on sen-
timent analysis (Pang and Lee, 2008), ranging
from work on well-edited newswire data using
the MPQA corpus (Wilson et al., 2005), to Ama-
zon reviews (Blitzer et al., 2007), blogs (Kessler
et al., 2010) and user-generated content such as
tweets (Mohammad et al., 2013). All of these
studies worked with English, while this study ? to
the best of our knowledge ? is the first to present
results for Danish.
As far as we are aware of, the only related work
on Danish is Hardt and Wulff (2012). In their ex-
ploratory paper, they investigate whether user pop-
ulations differ systematically in the way they ex-
press sentiment, finding that positive ratings are
far more common in U.S. reviews than in Danish
ones. However, their paper focuses on a quantita-
tive analysis and a single domain (movie reviews),
while we build an actual sentiment classification
system that performs well across domains.
Data corruption has been used for other NLP
6
tasks (S?gaard and Johannsen, 2012; S?gaard,
2013). Our random removal setup is basi-
cally an offline version of the approach presented
in (S?gaard and Johannsen, 2012). Their online
algorithm removes a random subset of the features
in each iteration and was successfully applied to
cross-domain experiments on part-of-speech tag-
ging and document classification. S?gaard (2013)
presents a follow-up online approach that takes
the weights of the current model into considera-
tion, regularizing the most predictive features. Our
biased approach is inspired by this, but has the ad-
vantage that it abstracts away from the underlying
learner.
6 Discussion and Future Work
We investigate cross-domain sentiment analysis
for a low-resource language, Danish. We observe
that performance drops precipitously when train-
ing on one domain and evaluating on the other. We
presented a robust offline-learning approach that
deletes features proportionate to their predictive-
ness. Applied to blind domain adaptation, this cor-
ruption method prevents overfitting to the source
domain, and results in relative improvements of
more than 50%.
In the future, we plan to experiment with in-
tegrating the weight distribution of a label into
the corruption function in order to prevent over-
corrupting of certain labels.
Acknowledgments
We would like to thank Daniel Hardt for host-
ing the Copenhagen Sentiment Analysis Work-
shop and making the data sets available. The last
two authors are supported by the ERC Starting
Grant LOWLANDS No. 313695.
References
Enrique Amig?o, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Maarten de Rijke. 2012. Overview of
RepLab 2012: Evaluating Online Reputation Man-
agement Systems. In CLEF.
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Mart??n, Edgar Meij, Maarten de Rijke, and Dami-
ano Spina. 2013. Overview of RepLab 2013: Eval-
uating Online Reputation Monitoring Systems. In
CLEF.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J. DellaPi-
etra, and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467?479.
Hal Daum?e, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In ACL Workshop on Domain Adapta-
tion for NLP.
Daniel Hardt and Julie Wulff. 2012. What is the mean-
ing of 5 *?s? An investigation of the expression and
rating of sentiment. In Proceedings of KONVENS
2012.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. In
SemEval-2013.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Anders S?gaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Anders S?gaard. 2013. Part-of-speech tagging with
antagonistic adversaries. In ACL.
Sida Wang and Christopher D Manning. 2012. Fast
dropout training for logistic regression. In NIPS
workshop on log-linear models.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
7
