Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 292?295,
New York City, June 2006. c?2006 Association for Computational Linguistics
AUTOMATED QUALITY MONITORING FOR CALL CENTERS USING SPEECH AND NLP
TECHNOLOGIES
G. Zweig, O. Siohan, G. Saon, B. Ramabhadran, D. Povey, L. Mangu and B. Kingsbury
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598
ABSTRACT
This paper describes an automated system for assigning qual-
ity scores to recorded call center conversations. The system com-
bines speech recognition, pattern matching, and maximum entropy
classification to rank calls according to their measured quality.
Calls at both ends of the spectrum are flagged as ?interesting? and
made available for further human monitoring. In this process, the
ASR transcript is used to answer a set of standard quality control
questions such as ?did the agent use courteous words and phrases,?
and to generate a question-based score. This is interpolated with
the probability of a call being ?bad,? as determined by maximum
entropy operating on a set of ASR-derived features such as ?max-
imum silence length? and the occurrence of selected n-gram word
sequences. The system is trained on a set of calls with associated
manual evaluation forms. We present precision and recall results
from IBM?s North American Help Desk indicating that for a given
amount of listening effort, this system triples the number of bad
calls that are identified, over the current policy of randomly sam-
pling calls. The application that will be demonstrated is a research
prototype that was built in conjunction with IBM?s North Ameri-
can call centers.
1. INTRODUCTION
Every day, tens of millions of help-desk calls are recorded at call
centers around the world. As part of a typical call center operation
a random sample of these calls is normally re-played to human
monitors who score the calls with respect to a variety of quality
related questions, e.g.
? Was the account successfully identified by the agent?
? Did the agent request error codes/messages to help deter-
mine the problem?
? Was the problem resolved?
? Did the agent maintain appropriate tone, pitch, volume and
pace?
This process suffers from a number of important problems: first,
the monitoring at least doubles the cost of each call (first an opera-
tor is paid to take it, then a monitor to evaluate it). This causes the
second problem, which is that therefore only a very small sample
of calls, e.g. a fraction of a percent, is typically evaluated. The
third problem arises from the fact that most calls are ordinary and
uninteresting; with random sampling, the human monitors spend
most of their time listening to uninteresting calls.
This work describes an automated quality-monitoring system
that addresses these problems. Automatic speech recognition is
used to transcribe 100% of the calls coming in to a call center,
and default quality scores are assigned based on features such as
key-words, key-phrases, the number and type of hesitations, and
the average silence durations. The default score is used to rank
the calls from worst-to-best, and this sorted list is made available
to the human evaluators, who can thus spend their time listening
only to calls for which there is some a-priori reason to expect that
there is something interesting.
The automatic quality-monitoring problem is interesting in
part because of the variability in how hard it is to answer the ques-
tions. Some questions, for example, ?Did the agent use courteous
words and phrases?? are relatively straightforward to answer by
looking for key words and phrases. Others, however, require es-
sentially human-level knowledge to answer; for example one com-
pany?s monitors are asked to answer the question ?Did the agent
take ownership of the problem?? Our work focuses on calls from
IBM?s North American call centers, where there is a set of 31 ques-
tions that are used to evaluate call-quality. Because of the high de-
gree of variability found in these calls, we have investigated two
approaches:
1. Use a partial score based only on the subset of questions
that can be reliably answered.
2. Use a maximum entropy classifier to map directly from
ASR-generated features to the probability that a call is bad
(defined as belonging to the bottom 20% of calls).
We have found that both approaches are workable, and we present
final results based on an interpolation between the two scores.
These results indicate that for a fixed amount of listening effort,
the number of bad calls that are identified approximately triples
with our call-ranking approach. Surprisingly, while there has been
significant previous scholarly research in automated call-routing
and classification in the call center , e.g. [1, 2, 3, 4, 5], there has
been much less in automated quality monitoring per se.
2. ASR FOR CALL CENTER TRANSCRIPTION
2.1. Data
The speech recognition systems were trained on approximately
300 hours of 6kHz, mono audio data collected at one of the IBM
call centers located in Raleigh, NC. The audio was manually tran-
scribed and speaker turns were explicitly marked in the word tran-
scriptions but not the corresponding times. In order to detect
speaker changes in the training data, we did a forced-alignment of
the data and chopped it at speaker boundaries. The test set consists
of 50 calls with 113 speakers totaling about 3 hours of speech.
2.2. Speaker Independent System
The raw acoustic features used for segmentation and recognition
are perceptual linear prediction (PLP) features. The features are
292
Segmentation/clustering Adaptation WER
Manual Off-line 30.2%
Manual Incremental 31.3%
Manual No Adaptation 35.9%
Automatic Off-line 33.0%
Automatic Incremental 35.1%
Table 1. ASR results depending on segmentation/clustering and
adaptation type.
Accuracy Top 20% Bottom 20%
Random 20% 20%
QA 41% 30%
Table 2. Accuracy for the Question Answering system.
mean-normalized 40-dimensional LDA+MLLT features. The SI
acoustic model consists of 50K Gaussians trained with MPE and
uses a quinphone cross-word acoustic context. The techniques are
the same as those described in [6].
2.3. Incremental Speaker Adaptation
In the context of speaker-adaptive training, we use two forms
of feature-space normalization: vocal tract length normalization
(VTLN) and feature-space MLLR (fMLLR, also known as con-
strained MLLR) to produce canonical acoustic models in which
some of the non-linguistic sources of speech variability have been
reduced. To this canonical feature space, we then apply a discrim-
inatively trained transform called fMPE [7]. The speaker adapted
recognition model is trained in this resulting feature space using
MPE.
We distinguish between two forms of adaptation: off-line and
incremental adaptation. For the former, the transformations are
computed per conversation-side using the full output of a speaker
independent system. For the latter, the transformations are updated
incrementally using the decoded output of the speaker adapted sys-
tem up to the current time. The speaker adaptive transforms are
then applied to the future sentences. The advantage of incremental
adaptation is that it only requires a single decoding pass (as op-
posed to two passes for off-line adaptation) resulting in a decoding
process which is twice as fast. In Table 1, we compare the per-
formance of the two approaches. Most of the gain of full offline
adaptation is retained in the incremental version.
2.3.1. Segmentation and Speaker Clustering
We use an HMM-based segmentation procedure for segmenting
the audio into speech and non-speech prior to decoding. The rea-
son is that we want to eliminate the non-speech segments in order
to reduce the computational load during recognition. The speech
segments are clustered together in order to identify segments com-
ing from the same speaker which is crucial for speaker adaptation.
The clustering is done via k-means, each segment being modeled
by a single diagonal covariance Gaussian. The metric is given by
the symmetric K-L divergence between two Gaussians. The im-
pact of the automatic segmentation and clustering on the error rate
is indicated in Table 1.
Accuracy Top 20% Bottom 20%
Random 20% 20%
ME 49% 36%
Table 3. Accuracy for the Maximum Entropy system.
Accuracy Top 20% Bottom 20%
Random 20% 20%
ME + QA 53% 44%
Table 4. Accuracy for the combined system.
3. CALL RANKING
3.1. Question Answering
This section presents automated techniques for evaluating call
quality. These techniques were developed using a train-
ing/development set of 676 calls with associated manually gen-
erated quality evaluations. The test set consists of 195 calls.
The quality of the service provided by the help-desk represen-
tatives is commonly assessed by having human monitors listen to
a random sample of the calls and then fill in evaluation forms. The
form for IBM?s North American Help Desk contains 31 questions.
A subset of the questions can be answered easily using automatic
methods, among those the ones that check that the agent followed
the guidelines e.g.
? Did the agent follow the appropriate closing script?
? Did the agent identify herself to the customer?
But some of the questions require human-level knowledge of the
world to answer, e.g.
? Did the agent ask pertinent questions to gain clarity of the
problem?
? Were all available resources used to solve the problem?
We were able to answer 21 out of the 31 questions using pat-
tern matching techniques. For example, if the question is ?Did
the agent follow the appropriate closing script??, we search for
?THANK YOU FOR CALLING?, ?ANYTHING ELSE? and
?SERVICE REQUEST?. Any of these is a good partial match for
the full script, ?Thank you for calling, is there anything else I can
help you with before closing this service request?? Based on the
answer to each of the 21 questions, we compute a score for each
call and use it to rank them. We label a call in the test set as being
bad/good if it has been placed in the bottom/top 20% by human
evaluators. We report the accuracy of our scoring system on the
test set by computing the number of bad calls that occur in the
bottom 20% of our sorted list and the number of good calls found
in the top 20% of our list. The accuracy numbers can be found in
Table 2.
3.2. Maximum Entropy Ranking
Another alternative for scoring calls is to find arbitrary features in
the speech recognition output that correlate with the outcome of a
call being in the bottom 20% or not. The goal is to estimate the
probability of a call being bad based on features extracted from
the automatic transcription. To achieve this we build a maximum
293
Fig. 1. Display of selected calls.
entropy based system which is trained on a set of calls with asso-
ciated transcriptions and manual evaluations. The following equa-
tion is used to determine the score of a call C using a set of N
predefined features:
P (class/C) = 1Z exp(
N
X
i=1
?ifi(class, C)) (1)
where class ? {bad, not ? bad}, Z is a normalizing factor, fi()
are indicator functions and {?i}{i=1,N} are the parameters of the
model estimated via iterative scaling [8].
Due to the fact that our training set contained under 700 calls,
we used a hand-guided method for defining features. Specifi-
cally, we generated a list of VIP phrases as candidate features,
e.g. ?THANK YOU FOR CALLING?, and ?HELP YOU?. We
also created a pool of generic ASR features, e.g. ?number of hes-
itations?, ?total silence duration?, and ?longest silence duration?.
A decision tree was then used to select the most relevant features
and the threshold associated with each feature. The final set of fea-
tures contained 5 generic features and 25 VIP phrases. If we take a
look at the weights learned for different features, we can see that if
a call has many hesitations and long silences then most likely the
call is bad.
We use P (bad|C) as shown in Equation 1 to rank all the calls.
Table 3 shows the accuracy of this system for the bottom and top
20% of the test calls.
At this point we have two scoring mechanisms for each call:
one that relies on answering a fixed number of evaluation ques-
tions and a more global one that looks across the entire call for
hints. These two scores are both between 0 and 1, and therefore
can be interpolated to generate one unique score. After optimizing
the interpolation weights on a held-out set we obtained a slightly
higher weight (0.6) for the maximum entropy model. It can be
seen in Table 4 that the accuracy of the combined system is greater
that the accuracy of each individual system, suggesting the com-
plementarity of the two initial systems.
4. END-TO-END SYSTEM PERFORMANCE
4.1. Application
This section describes the user interface of the automated quality
monitoring application. As explained in Section 1, the evalua-
Fig. 2. Interface to listen to audio and update the evaluation form.
tor scores calls with respect to a set of quality-related questions
after listening to the calls. To aid this process, the user interface
provides an efficient mechanism for the human evaluator to select
calls, e.g.
? All calls from a specific agent sorted by score
? The top 20% or the bottom 20% of the calls from a specific
agent ranked by score
? The top 20% or the bottom 20% of all calls from all agents
The automated quality monitoring user interface is a J2EE web
application that is supported by back-end databases and content
management systems 1 The displayed list of calls provides a link
to the audio, the automatically filled evaluation form, the overall
score for this call, the agent?s name, server location, call id, date
and duration of the call (see Figure 1). This interface now gives
the agent the ability to listen to interesting calls and update the
answers in the evaluation form if necessary (audio and evaluation
form illustrated in 2). In addition, this interface provides the eval-
uator with the ability to view summary statistics (average score)
and additional information about the quality of the calls. The over-
all system is designed to automatically download calls from mul-
tiple locations on a daily-basis, transcribe and index them, thereby
making them available to the supervisors for monitoring. Calls
spanning a month are available at any given time for monitoring
purposes.
4.2. Precision and Recall
This section presents precision and recall numbers for the
identification of ?bad? calls. The test set consists of 195 calls that
were manually evaluated by call center personnel. Based on these
manual scores, the calls were ordered by quality, and the bottom
20% were deemed to be ?bad.? To retrieve calls for monitoring,
we sort the calls based on the automatically assigned quality score
and return the worst. In our summary figures, precision and recall
are plotted as a function of the number of calls that are selected
for monitoring. This is important because in reality only a small
number of calls can receive human attention. Precision is the ratio
1In our case, the backend consists of DB2 and IBM?s Websphere Infor-
mation Integrator for Content and the application is hosted on Websphere
5.1.)
294
020
40
60
80
100
0 20 40 60 80 100
Observed
Ideal
Random
Fig. 3. Precision for the bottom 20% of the calls as a function of
the number of calls retrieved.
0
20
40
60
80
100
0 20 40 60 80 100
Observed
Ideal
Random
Fig. 4. Recall for the bottom 20% of the calls.
of bad calls retrieved to the total number of calls monitored, and
recall is the ratio of the number of bad calls retrieved to the total
number of bad calls in the test set. Three curves are shown in each
plot: the actually observed performance, performance of random
selection, and oracle or ideal performance. Oracle performance
shows what would happen if a perfect automatic ordering of the
calls was achieved.
Figure 3 shows precision performance. We see that in the
monitoring regime where only a small fraction of the calls are
monitored, we achieve over 60% precision. (Further, if 20% of
the calls are monitored, we still attain over 40% precision.)
Figure 4 shows the recall performance. In the regime of low-
volume monitoring, the recall is midway between what could be
achieved with an oracle, and the performance of random-selection.
Figure 5 shows the ratio of the number of bad calls found with
our automated ranking to the number found with random selection.
This indicates that in the low-monitoring regime, our automated
technique triples efficiency.
4.3. Human vs. Computer Rankings
As a final measure of performance, in Figure 6 we present a
scatterplot comparing human to computer rankings. We do not
have calls that are scored by two humans, so we cannot present a
human-human scatterplot for comparison.
5. CONCLUSION
This paper has presented an automated system for quality moni-
toring in the call center. We propose a combination of maximum-
entropy classification based on ASR-derived features, and question
answering based on simple pattern-matching. The system can ei-
ther be used to replace human monitors, or to make them more
1
1.5
2
2.5
3
3.5
4
4.5
5
0 20 40 60 80 100
Observed
Ideal
Fig. 5. Ratio of bad calls found with QTM to Random selection as
a function of the number of bad calls retrieved.
0
20
40
60
80
100
120
140
160
180
200
0 20 40 60 80 100 120 140 160 180 200
Fig. 6. Scatter plot of Human vs. Computer Rank.
efficient. Our results show that we can triple the efficiency of hu-
man monitors in the sense of identifying three times as many bad
calls for the same amount of listening effort.
6. REFERENCES
[1] J. Chu-Carroll and B. Carpenter, ?Vector-based natural lan-
guage call routing,? Computational Linguistics, 1999.
[2] P. Haffner, G. Tur, and J. Wright, ?Optimizing svms for com-
plex call classification,? 2003.
[3] M. Tang, B. Pellom, and K. Hacioglu, ?Call-type classifica-
tion and unsupervised training for the call center domain,? in
ARSU-2003, 2003.
[4] D. Hakkani-Tur, G. Tur, M. Rahim, and G. Riccardi, ?Unsu-
pervised and active learning in automatic speech recognition
for call classification,? in ICASSP-04, 2004.
[5] C. Wu, J. Kuo, E.E. Jan, V. Goel, and D. Lubensky, ?Improv-
ing end-to-end performance of call classification through data
confusion reduction and model tolerance enhancement,? in
Interspeech-05, 2005.
[6] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and
G. Zweig, ?The ibm 2004 conversational telephony system
for rich transcription,? in Eurospeech-2005, 2005.
[7] D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau,
and G. Zweig, ?fMPE: Discriminatively trained features for
speech recognition,? in ICASSP-2005, 2004.
[8] A. Berger, S. Della Pietra, and V. Della Pietra, ?A maximum
entropy approach to natural language processing,? Computa-
tional Linguistics, vol. 22, no. 1, 1996.
295
Information Extraction From Voicemail
Jing Huang and Geoffrey Zweig and Mukund Padmanabhan
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
USA
jhuang, gzweig, mukund@watson.ibm.com
Abstract
In this paper we address the problem
of extracting key pieces of information
from voicemail messages, such as the
identity and phone number of the caller.
This task differs from the named entity
task in that the information we are inter-
ested in is a subset of the named entities
in the message, and consequently, the
need to pick the correct subset makes
the problem more difficult. Also, the
caller?s identity may include informa-
tion that is not typically associated with
a named entity. In this work, we present
three information extraction methods,
one based on hand-crafted rules, one
based on maximum entropy tagging,
and one based on probabilistic trans-
ducer induction. We evaluate their per-
formance on both manually transcribed
messages and on the output of a speech
recognition system.
1 Introduction
In recent years, the task of automatically extract-
ing information from data has grown in impor-
tance, as a result of an increase in the number of
publicly available archives and a realization of the
commercial value of the available data. One as-
pect of information extraction (IE) is the retrieval
of documents. Another aspect is that of identify-
ing words from a stream of text that belong in pre-
defined categories, for instance, ?named entities?
such as proper names, organizations, or numerics.
Though most of the earlier IE work was done in
the context of text sources, recently a great deal of
work has also focused on extracting information
from speech sources. Examples of this are the
Spoken Document Retrieval (SDR) task (NIST,
1999), named entity (NE) extraction (DARPA,
1999; Miller et al, 2000; Kim and Woodland,
2000). The SDR task focused on Broadcast News
and the NE task focused on both Broadcast News
and telephone conversations.
In this paper, we focus on a source of con-
versational speech data, voicemail, that is found
in relatively large volumes in the real-world, and
that could benefit greatly from the use of IE tech-
niques. The goal here is to query one?s personal
voicemail for items of information, without hav-
ing to listen to the entire message. For instance,
?who called today??, or ?what is X?s phone num-
ber??. Because of the importance of these key
pieces of information, in this paper, we focus pre-
cisely on extracting the identity and the phone
number of the caller. Other attempts at sum-
marizing voicemail have been made in the past
(Koumpis and Renals, 2000), however the goal
there was to compress a voicemail message by
summarizing it, and not to extract the answers to
specific questions.
An interesting aspect of this research is that be-
cause a transcription of the voicemail is not avail-
able, speech recognition algorithms have to be
used to convert the speech to text and the sub-
sequent IE algorithms must operate on the tran-
scription. One of the complications that we have
to deal with is the fact that the state-of-the-art ac-
curacy of speech recognition algorithms on this
type of data 1 is only in the neighborhood of 60-
70% (Huang et al, 2000).
The task that is most similar to our work
is named entity extraction from speech data
(DARPA, 1999). Although the goal of the named
entity task is similar - to identify the names of per-
sons, locations, organizations, and temporal and
numeric expressions - our task is different, and
in some ways more difficult. There are two main
reasons for this: first, caller and number informa-
tion constitute a small fraction of all named enti-
ties. Not all person-names belong to callers, and
not all digit strings specify phone-numbers. In
this sense, the algorithms we use must be more
precise than those for named entity detection.
Second, the caller?s identity may include infor-
mation that is not typically found in a named en-
tity, for example, ?Joe on the third floor?, rather
than simply ?Joe?. We discuss our definitions of
?caller? and ?number? in Section 2.
To extract caller information from transcribed
speech text, we implemented three different sys-
tems, spanning both statistical and non-statistical
approaches. We evaluate these systems on man-
ual voicemail transcriptions as well as the out-
put of a speech recognizer. The first system is a
simple rule-based system that uses trigger phrases
to identify the information-bearing words. The
second system is a maximum entropy model that
tags the words in the transcription as belong-
ing to one of the categories, ?caller?s identity?,
?phone number? or ?other?. The third system is
a novel technique based on automatic stochastic-
transducer induction. It aims to learn rules auto-
matically from training data instead of requiring
hand-crafted rules from experts. Although the re-
sults with this system are not yet as good as the
other two, we consider it highly interesting be-
cause the technology is new and still open to sig-
nificant advances.
The rest of the paper is organized as follows:
Section 2 describes the database we are using;
Section 3 contains a description of the baseline
system; Section 4 describes the maximum en-
tropy model and the associated features; Section
1The large word error rate is due to the fact that the
speech is spontaneous, and characterized by poor grammar,
false starts, pauses, hesitations, etc. While this does not pose
a problem for a human listener, it causes significant prob-
lems for speech recognition algorithms.
5 discusses the transducer induction technique;
Section 6 contains our experimental results and
Section 7 concludes our discussions.
2 The Database
Our work focuses on a database of voicemail mes-
sages gathered at IBM, and made publicly avail-
able through the LDC. This database and related
speech recognition work is described fully by
(Huang et al, 2000). We worked with approx-
imately   messages, which we divided into
	
messages for training,  
 for develop-
ment test set, and   for evaluation test set. The
messages were manually transcribed 2, and then
a human tagger identified the portions of each
message that specified the caller and any return
numbers that were left. In this work, we take a
broad view of what constitutes a caller or num-
ber. The caller was defined to be the consecutive
sequence of words that best answered the ques-
tion ?who called??. The definition of a number
we used is a sequence of consecutive words that
enables a return call to be placed. Thus, for ex-
ample, a caller might be ?Angela from P.C. Labs,?
or ?Peggy Cole Reed Balla?s secretary?. Simi-
larly, a number may not be a digit string, for ex-
ample: ?tieline eight oh five six,? or ?pager one
three five?. No more than one caller was identi-
fied for a single message, though there could be
multiple numbers. The training of the maximum
entropy model and statistical transducer are done
on these annotated scripts.
3 A Baseline Rule-Based System
In voicemail messages, people often identify
themselves and give their phone numbers in
highly stereotyped ways. So for example, some-
one might say, ?Hi Joe it?s Harry...? or ?Give
me a call back at extension one one eight four.?
Our baseline system takes advantage of this fact
by enumerating a set of transduction rules - in the
form of a flex program - that transduce out the key
information in a call.
The baseline system is built around the notion
of ?trigger phrases?. These hand-crafted phases
are patterns that are used in the flex program to
recognize caller?s identity and phone numbers.
2The manual transcription has a  word error rate
Examples of trigger phrases are ?Hi this is?, and
?Give me a call back at?. In order to identify
names and phone numbers as generally as pos-
sible, our baseline system has defined classes for
person-names and numbers.
In addition to trigger phrases, ?trigger suf-
fixes? proved to be useful for identifying phone
numbers. For example, the phrase ?thanks bye?
frequently occurs immediately after the caller?s
phone number. In general, a random sequence of
digits cannot be labeled as a phone number; but,
a sequence of digits followed by ?thanks bye? is
almost certainly the caller?s phone number. So
when the flex program matches a sequence of dig-
its, it stores it; then it tries to match a trigger suf-
fix. If this is successful, the digit string is recog-
nized a phone number string. Otherwise the digit
string is ignored.
Our baseline system has about 200 rules. Its
creation was aided by an automatically generated
list of short, commonly occurring phrases that
were then manually scanned, generalized, and
added to the flex program. It is the simplest of
the systems presented, and achieves a good per-
formance level, but suffers from the fact that a
skilled person is required to identify the rules.
4 Maximum Entropy Model
Maximum entropy modeling is a powerful frame-
work for constructing statistical models from
data. It has been used in a variety of difficult
classification tasks such as part-of-speech tagging
(Ratnaparkhi, 1996), prepositional phrase attach-
ment (Ratnaparkhi et al, 1994) and named en-
tity tagging (Borthwick et al, 1998), and achieves
state of the art performance. In the following, we
briefly describe the application of these models
to extracting caller?s information from voicemail
messages.
The problem of extracting the information per-
taining to the callers identity and phone number
can be thought of as a tagging problem, where the
tags are ?caller?s identity,? ?caller?s phone num-
ber? and ?other.? The objective is to tag each
word in a message into one of these categories.
The information that can be used to predict a
word?s tag is the identity of the surrounding words
and their associated tags. Let  denote the set
of possible word and tag contexts, called ?histo-
ries?, and  denote the set of tags. The maxent
model is then defined over  ,and predicts
the conditional probability NAACL HLT Demonstration Program, pages 31?32,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Voice-Rate: A Dialog System for Consumer Ratings 
 
Geoffrey Zweig, Y.C. Ju, Patrick Nguyen, Dong Yu, 
Ye-Yi Wang and Alex Acero 
Speech Research Group 
Microsoft Corp. 
Redmond, WA 98052 
{gzweig,yuncj,panguyen,dongyu, yeyi-
wang,alexac}@microsoft.com 
 
  
Abstract 
Voice-Rate is an automated dialog system 
which provides access to over one million 
ratings of products and businesses. By 
calling a toll-free number, consumers can 
access ratings for products, national busi-
nesses such as airlines, and local busi-
nesses such as restaurants. Voice-Rate 
also has a facility for recording and ana-
lyzing ratings that are given over the 
phone. The service has been primed with 
ratings taken from a variety of web 
sources, and we are augmenting these 
with user ratings. Voice-Rate can be ac-
cessed by dialing 1-877-456-DATA. 
1 Overview 
 Voice-Rate is an automated dialog system de-
signed to help consumers while they are shopping. 
The target user is a consumer who is considering 
making an impulse purchase and would like to get 
more information. He or she can take out a cell-
phone, call Voice-Rate, and get rating information 
to help decide whether to buy the item. Here are 
three sample scenarios: 
 
 Sally has gone to Home Depot to buy 
some paint to touch-up scratches on the 
wall at home. She?ll use exactly the same 
color and brand as when she first painted 
the wall, so she knows what she wants. 
While at Home Depot, however, Sally sees 
some hand-held vacuum cleaners and de-
cides it might be nice to have one. But, she 
is unsure whether which of the available 
models is better: The ?Black & Decker 
CHV1400 Cyclonic DustBuster,? the 
?Shark SV736? or the ?Eureka 71A.? Sally 
calls Voice-Rate and gets the ratings and 
makes an informed purchase. 
 John is on vacation with his family in Seat-
tle. After going up in the Space Needle, 
they walk by ?Abbondanza Pizzeria? and 
are considering lunch there. While it looks 
good, there are almost no diners inside, 
and John is suspicious. He calls Voice-
Rate and discovers that in fact the restau-
rant is highly rated, and decides to go 
there. 
 Returning from his vacation, John drops 
his rental car off at the airport. The rental 
company incorrectly asserts that he has 
scratched the car, and causes a big hassle, 
until they finally realize that they already 
charged the last customer for the same 
scratch. Unhappy with the surly service, 
John calls Voice-Rate and leaves a warn-
ing for others.  
 
Currently, Voice-Rate can deliver ratings for over 
one million products, two hundred thousand res-
taurants in over sixteen hundred cities; and about 
three thousand national businesses.  
2 Technical Challenges 
To make Voice-Rate operational, it was necessary 
to solve the key challenges of name resolution and 
disambiguation. Users rarely make an exactly cor-
rect specification of a product or business, and it is 
necessary both to utilize a ?fuzzy-match? for name 
lookup, and to deploy a carefully designed disam-
biguation strategy.  
31
Voice-Rate solves the fuzzy-matching process by 
treating spoken queries as well as business and 
product names as documents, and then performing 
TF-IDF based lookup. For a review of name 
matching methods, see e.g. Cohen et al, 2003. In 
the ideal case, after a user asks for a particular 
product or business, the best-matching item as 
measured by TF-IDF would be the one intended by 
the user. In reality, of course, this is often not the 
case, and further dialog is necessary to determine 
the user?s intent. For concreteness, we will illu-
strate the disambiguation process in the context of 
product identification. 
 
When a user calls Voice-Rate and asks for a prod-
uct review, the system solicits the user for the 
product name, does TF-IDF lookup, and presents 
the highest-scoring match for user confirmation. If 
the user does not accept the retrieved item, Voice-
Rate initiates a disambiguation dialog.  
 
Aside from inadequate product coverage, which 
cannot be fixed at runtime, there are two possible 
sources for error: automatic speech recognition 
(ASR) errors, and TF-IDF lookup errors.  The dis-
ambiguation process begins by eliminating the 
first. To do this, it asks the user if his or her exact 
words were the recognized text, and if not to repeat 
the request. This loop iterates twice, and if the us-
er?s exact words still have not been identified, 
Voice-Rate apologizes and hangs up. 
 
Once the user?s exact words have been validated, 
Voice-Rate gets a positive identification on the 
product category. From the set of high-scoring TF-
IDF items, a list of possible categories is compiled. 
For example, for ?The Lord of the Rings The Two 
Towers,? there are items in Video Games, DVDs, 
Music, VHS, Software, Books, Websites, and Toys 
and Games. These categories are read to the user, 
who is asked to select one. All the close-matching 
product names in the selected category are then 
read to the user, until one is selected or the list is 
exhausted.  
3 Related Work 
To our knowledge, Voice-Rate is the first large 
scale ratings dialog system. However, the technol-
ogy behind it is closely related to previous dialog 
systems, especially directory assistance or ?411? 
systems (e.g. Kamm et al, 1994, Natarajan et al, 
2002, Levin et al, 2005, Jan et al, 2003).  A gen-
eral discussion of name-matching techniques such 
as TF-IDF can be found in (Cohen et al, 2003, 
Bilenko et al, 2003). 
 
The second area of related research has to do with 
web rating systems. Interesting work on extracting 
information from such ratings can be found in, e.g. 
(Linden et al, 2003, Hu et al, 2004, Gammon et 
al., 2005). Work has also been done using text-
based input to determine relevant products (Chai et 
al., 2002).  Our own work differs from this in that 
it focuses on spoken input, and in its breadth ? 
covering both products and businesses. 
References  
M. Bilenko, R. Mooney, W. W. Cohen, P. Ravikumar and S. 
Fienberg. 2003. Adaptive Name-Matching in Information 
Integration. IEEE Intelligent Systems 18(5): 16-23 (2003).  
J. Chai, V. Horvath, N. Nicolov, M. Stys, N. Kambhatla, W. 
Zadrozny and P. Melville.  2002. Natural Language Assis-
tant- A Dialog System for Online Product Recommenda-
tion. AI Magazine (23), 2002 
 
W. W. Cohen, P Ravikumar and S. E. Fienberg . 2003. A 
comparison of string distance metrics for name-matching 
tasks. Proceedings of the IJCAI-2003 Workshop on Infor-
mation, 2003 
M.  Gamon, A. Aue, S. Corston-Oliver and E. Ringger. 2005. 
Pulse: Mining Customer Opinions from Free Text. In Lec-
ture Notes in Computer Science. Vol. 3646. Springer Ver-
lag. (IDA 2005)., pages 121-132. 
M. Hu and B. Liu. 2004. Mining and summarizing customer 
reviews. Proceedings of the 2004 ACM SIGKDD interna-
tional conference. 
 
E. E. Jan, B. Maison, L. Mangu and G. Zweig. 2003. Auto-
matic construction of Unique Signatures and Confusable 
sets for Natural Language Directory Assistance Applica-
tion.  Eurospeech 2003 
C. A. Kamm, K. M. Yang, C. R. Shamieh and S. Singhal. 
1994. Speech recognition issues for directory assistance 
applications. Second IEEE Workshop on Interactive Voice 
Technology for Telecommunications Applications. 
 
E. Levin and A. M. Man?. 2005. Voice User Interface Design 
for Automated Directory Assistance Eurospeech 2005. 
G. Linden, B. Smith and J. York. Amazon.com recommenda-
tions: item-to-item collaborative filtering. 2003.  Internet 
Computing, IEEE , vol.7, no.1pp. 76- 80. 
 
P. Natarajan, R. Prasad, R. Schwartz and J. Makhoul. 2002. A 
Scalable Architecture for Directory Assistance Automation, 
ICASSP 2002, Orlando, Florida. 
32
Proceedings of NAACL HLT 2009: Short Papers, pages 101?104,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multi-scale Personalization for Voice Search Applications
Daniel Bolan?os
Center for Spoken Language Research
University of Colorado at Boulder, USA
bolanos@cslr.colorado.edu
Geoffrey Zweig
Microsoft Research
One Microsoft Way, Redmond, WA 98052
gzweig@microsoft.com
Patrick Nguyen
Microsoft Research
One Microsoft Way, Redmond, WA 98052
panguyen@microsoft.com
Abstract
Voice Search applications provide a very con-
venient and direct access to a broad variety
of services and information. However, due to
the vast amount of information available and
the open nature of the spoken queries, these
applications still suffer from recognition er-
rors. This paper explores the utilization of per-
sonalization features for the post-processing
of recognition results in the form of n-best
lists. Personalization is carried out from three
different angles: short-term, long-term and
Web-based, and a large variety of features are
proposed for use in a log-linear classification
framework.
Experimental results on data obtained from a
commercially deployed Voice Search system
show that the combination of the proposed
features leads to a substantial sentence error
rate reduction. In addition, it is shown that
personalization features which are very dif-
ferent in nature can successfully complement
each other.
1 Introduction
Search engines are a powerful mechanism to find
specific content through the use of queries. In recent
years, due to the vast amount of information avail-
able, there has been significant research on the use of
recommender algorithms to select what information
will be presented to the user. These systems try to
predict what content a user may want based not only
on the user?s query but on the user?s past queries,
history of clicked results, and preferences. In (Tee-
van et al, 1996) it was observed that a significant
percent of the queries made by a user in a search
engine are associated to a repeated search. Recom-
mender systems like (Das et al, 2007) and (Dou et
al., 2007) take advantage of this fact to refine the
search results and improve the search experience.
In this paper, we explore the use of personaliza-
tion in the context of voice searches rather than web
queries. Specifically, we focus on data from a multi-
modal cellphone-based business search application
(Acero et al, 2008). In such an application, repeated
queries can be a powerful tool for personalization.
These can be classified into short and long-term rep-
etitions. Short-term repetitions are typically caused
by a speech recognition error, which produces an in-
correct search result and makes the user repeat or
reformulate the query. On the other hand, long-term
repetitions, as in text-based search applications, oc-
cur when the user needs to access some information
that was accessed previously, for example, the exact
location of a pet clinic.
This paper proposes several different user per-
sonalization methods for increasing the recognition
accuracy in Voice Search applications. The pro-
posed personalization methods are based on extract-
ing short-term, long-term and Web-based features
from the user?s history. In recent years, other user
personalization methods like deriving personalized
pronunciations have proven successful in the context
of mobile applications (Deligne et al, 2002).
The rest of this paper is organized as follows: Sec-
tion 2 describes the classification method used for
rescoring the recognition hypotheses. Section 3 de-
scribes the proposed personalization methods. Sec-
tion 4 describes the experiments carried out. Finally,
101
conclusions from this work are drawn in section 5.
2 Rescoring procedure
2.1 Log linear classification
Our work will proceed by using a log-linear clas-
sifier similar to the maximum entropy approach of
(Berger and Della Pietra, 1996) to predict which
word sequence W appearing on an n-best list N is
most likely to be correct. This is estimated as
P (W |N) = exp(
?
i ?ifi(W,N))?
W ??N exp(?i ?ifi(W ?, N)) . (1)
The feature functions fi(W,N) can represent ar-
bitrary attributes of W and N . This can be seen
to be the same as a maximum entropy formulation
where the class is defined as the word sequence (thus
allowing potentially infinite values) but with sums
restricted as a computational convenience to only
those class values (word strings) appearing on the n-
best list. The models were estimated with a widely
available toolkit (Mahajan, 2007).
2.2 Feature extraction
Given the use of a log-linear classifier, the crux of
our work lies in the specific features used. As a base-
line, we take the hypothesis rank, which results in
the 1-best accuracy of the decoder. Additional fea-
tures were obtained from the personalization meth-
ods described in the following section.
3 Personalization methods
3.1 Short-term personalization
Short-term personalization aims at modeling the re-
pair/repetition behavior of the user. Short-term fea-
tures are a mechanism suitable for representing neg-
ative evidence: if the user repeats a utterance it nor-
mally means that the hypotheses in the previous n-
best lists are not correct. For this reason, if a hy-
pothesis is contained in a preceding n-best list, that
hypothesis should be weighted negatively during the
rescoring.
A straightforward method for identifying likely
repetitions consists of using a fixed size time win-
dow and considering all the user queries within that
window as part of the same repetition round. Once
an appropriate window size has been determined,
the proposed short-term features can be extracted for
each hypothesis using a binary tree like the one de-
picted in figure 1, where feature values are in the
leaves of the tree.
Does a recent (60s) n-best
list contain the hypothesis
we are scoring?
seen = 1
seen & clicked = 0
seen & clicked = 0
No
Did the user click
on that hypothesis?
Yes
seen = 0
seen & clicked = 1
seen & clicked = 0
No
seen = 0
seen & clicked = 0
seen & clicked = 1
Yes
Figure 1: Short-term feature extraction (note that over-
lines mean ?do not?).
Given these features, we expect ?seen and not
clicked? to have a negative weight while ?seen and
clicked? should have a positive weight.
3.2 Long-term personalization
Long-term personalization consists of using the user
history (i.e. recognition hypotheses that were con-
firmed by the user in the past) to predict which
recognition results are more likely. The assumption
here is that recognition hypotheses in the n-best list
that match or ?resemble? those in the user history are
more likely to be correct. The following list enumer-
ates the long-term features proposed in this work:
? User history (occurrences): number of times
the hypothesis appears in the user history.
? User history (alone): 1 if the hypothesis ap-
pears in the user history and no other compet-
ing hypothesis does, otherwise 0.
? User history (most clicked): 1 if the hypothe-
sis appears in the user history and was clicked
more times than any other competing hypothe-
sis.
? User history (most recent): 1 if the hypothe-
sis appears in the user history and was clicked
102
more recently than any other competing hy-
pothesis.
? User history (edit distance): minimum edit dis-
tance between the hypothesis and the closest
query in the user history, normalized by the
number of words.
? User history (words in common): maximum
number of words in common between the hy-
pothesis and each of the queries in the user his-
tory, normalized by the number of words in the
hypothesis.
? User history (plural/singular): 1 if either the
plural or singular version of the hypothesis ap-
pears in the user history, otherwise 0.
? Global history: 1 if the hypothesis has ever
been clicked by any user, otherwise 0.
? Global history (alone): 1 if the hypothesis is the
only one in the n-best that has ever been clicked
by any user, otherwise 0.
Note that the last two features proposed make
use of the ?global history? which comprises all the
queries made by any user.
3.3 LiveSearch-based features
Typically, users ask for businesses that exist, and if
a business exists it probably appears in a Web docu-
ment indexed by Live Search (Live Search, 2006). It
is reasonable to assume that the relevance of a given
business is connected to the number of times it ap-
pears in the indexed Web documents, and in this sec-
tion we derive such features.
For the scoring process, an application has been
built that makes automated queries to Live Search,
and for each hypothesis in the n-best list obtains the
number of Web documents in which it appears. De-
noting by x the number of Web documents in which
the hypothesis (the exact sequence of words, e.g.
?tandoor indian restaurant?) appears, the following
features are proposed:
? Logarithm of the absolute count: log(x).
? Search results rank: sort the hypotheses in the
n-best list by their relative value of x and use
the rank as a feature.
? Relative relevance (I): 1 if the hypothesis was
not found and there is another hypothesis in the
n-best list that was found more than 100 times,
otherwise 0.
? Relative relevance (II): 1 if the the hypothesis
appears fewer than 10 times and there is an-
other hypothesis in the n-best list that appears
more than 100 times, otherwise 0.
4 Experiments
4.1 Data
The data used for the experiments comprises 22473
orthographically transcribed business utterances ex-
tracted from a commercially deployed large vocabu-
lary directory assistance system.
For each of the transcribed utterances two n-best
lists were produced, one from the commercially de-
ployed system and other from an enhanced decoder
with a lower sentence error rate (SER). In the exper-
iments, due to their lower oracle error rate, n-bests
from the enhanced decoder were used for doing the
rescoring. However, these n-bests do not correspond
to the listings shown in the user?s device screen (i.e.
do not match the user interaction) so are not suit-
able for identifying repetitions. For this reason, the
short term features were computed by comparing a
hypothesis from the enhanced decoder with the orig-
inal n-best list from the immediate past. Note that all
other features were computed solely with reference
to the n-bests from the enhanced decoder.
A rescoring subset was made from the original
dataset using only those utterances in which the n-
best lists contain the correct hypothesis (in any po-
sition) and have more than one hypothesis. For all
other utterances, rescoring cannot have any effect.
The size of the rescoring subset is 43.86% the size
of the original dataset for a total of 9858 utterances.
These utterances were chronologically partitioned
into a training set containing two thirds and a test
set with the rest.
4.2 Results
The baseline system for the evaluation of the pro-
posed features consist of a ME classifier trained on
only one feature, the hypothesis rank. The resulting
sentence error rate (SER) of this classifier is that of
the best single path, and it is 24.73%. To evaluate
103
the contribution of each of the features proposed in
section 3, a different ME classifier was trained us-
ing that feature in addition to the baseline feature.
Finally, another ME classifier was trained on all the
features together.
Table 1 summarizes the Sentence Error Rate
(SER) for each of the proposed features in isolation
and all together respect to the baseline. ?UH? stands
for user history.
Features SER
Hypothesis rank (baseline) 24.73%
base + repet. (seen) 24.48%
base + repet. (seen & clicked) 24.32%
base + repet. (seen & clicked) 24.73%
base + UH (occurrences) 23.76%
base + UH (alone) 23.79%
base + UH (most clicked) 23.73%
base + UH (most recent) 23.88%
base + UH (edit distance) 23.76%
base + UH (words in common) 24.60%
base + UH (plural/singular) 24.76%
base + GH 24.63%
base + GH (alone) 24.66%
base + Live Search (absolute count) 24.35%
base + Live Search (rank) 24.85%
base + Live Search (relative I) 23.51%
base + Live Search (relative II) 23.69%
base + all 21.54%
Table 1: Sentence Error Rate (SER) for each of the fea-
tures in isolation and for the combination of all of them.
5 Conclusions
The proposed features reduce the SER of the base-
line system by 3.19% absolute on the rescoring set,
and by 1.40% absolute on the whole set of tran-
scribed utterances.
Repetition based features are moderately useful;
by incorporating them into the rescoring it is possi-
ble to reduce the SER from 24.73% to 24.32%. Al-
though repetitions cover a large percentage of the
data, it is believed that inconsistencies in the user
interaction (the right listing is displayed but not con-
firmed by the user) prevented further improvement.
As expected, long-term personalization based fea-
tures contribute to improve the classification accu-
racy. The UH (occurrences) feature by itself is able
to reduce the SER in about a 1%.
Live Search has shown a very good potential for
feature extraction. In this respect it is interesting to
note that a right design of the features seems critical
to take full advantage of it. The relative number of
counts of one hypothesis respect to other hypotheses
in the n-best list is more informative than an absolute
or ranked count. A simple feature using this kind of
information, like Live Search (relative I), can reduce
the SER in more than 1% respect to the baseline.
Finally, it has been shown that personalization
based features can complement each other very well.
References
Alex Acero, Neal Bernstein, Rob Chambers, Yun-Cheng
Ju, Xiao Li, Julian Odell, Patrick Nguyen, Oliver
Scholtz and Geoffrey Zweig. 2008. Live Search
for Mobile: Web Services by Voice on the Cellphone.
ICASSP 2008, March 31 2008-April 4 2008. Las Ve-
gas, NV, USA.
Adam L. Berger; Vincent J. Della Pietra; Stephen A.
Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 1996. 22(1): p. 39-72.
Abhinandan Das, Mayur Datar and Ashutosh Garg.
2007. Google News Personalization: Scalable Online
Collaborative Filtering. WWW 2007 / Track: Indus-
trial Practice and Experience May 8-12, 2007. Banff,
Alberta, Canada.
Sabine Deligne, Satya Dharanipragada, Ramesh
Gopinath, Benoit Maison, Peder Olsen and Harry
Printz. 2002. A robust high accuracy speech recog-
nition system for mobile applications. Speech and
Audio Processing, IEEE Transactions on, Nov 2002,
Volume: 10, Issue: 8, On page(s): 551- 561.
Zhicheng Dou, Ruihua Song, and Ji-Rong Wen. 2007.
A large-scale evaluation and analysis of personalized
search strategies. In WWW ?07: Proceedings of the
16th international conference on World Wide Web,
pages 581 - 590, New York, NY, USA, 2007. ACM
Press.
Live Search. ?http://www.live.com,?.
Milind Mahajan. 2007. Conditional Maximum-Entropy
Training Tool http://research.microsoft.com/en-
us/downloads/9f199826-49d5-48b6-ba1b-
f623ecf36432/.
Jaime Teevan, Eytan Adar, Rosie Jones and Michael A.
S. Potts. 2007. Information Re-Retrieval: Repeat
Queries in Yahoos Logs. SIGIR, 2007.
104
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 21?28,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Learning N-Best Correction Models from Implicit User Feedback  
in a Multi-Modal Local Search Application 
 
 
Dan Bohus, Xiao Li, Patrick Nguyen, Geoffrey Zweig 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
{dbohus, xiaol, panguyen, gzweig}@microsoft.com 
 
 
 
 
 
 
Abstract 
We describe a novel n-best correction model 
that can leverage implicit user feedback (in 
the form of clicks) to improve performance in 
a multi-modal speech-search application. The 
proposed model works in two stages. First, the 
n-best list generated by the speech recognizer 
is expanded with additional candidates, based 
on confusability information captured via user 
click statistics. In the second stage, this ex-
panded list is rescored and pruned to produce 
a more accurate and compact n-best list. Re-
sults indicate that the proposed n-best correc-
tion model leads to significant improvements 
over the existing baseline, as well as other tra-
ditional n-best rescoring approaches.  
1 Introduction 
Supported by years of research in speech recogni-
tion and related technologies, as well as advances 
in mobile devices, speech-enabled mobile applica-
tions are finally transitioning into day-to-day use. 
One example is Live Search for Windows Mobile 
(2008), a speech-enabled application that allows 
users to get access to local information by speaking 
a query into their device. Several other systems 
operating in similar domains have recently become 
available (TellMeByMobile, 2008; Nuance Mobile 
Search, 2008; V-Lingo Mobile, 2008; VoiceSignal 
Search, 2008.) 
Traditionally, multi-modal systems leverage the 
additional input channels such as text or buttons to 
compensate for the current shortcomings of speech 
recognition technology. For instance, after the user 
speaks a query, the Live Search for Windows Mo-
bile application displays a confirmation screen that 
contains the n-best recognition results. The user 
selects the correct hypothesis using the buttons on 
the device, and only then the system displays the 
corresponding search results (see Figure 1.) 
We argue that ideally multi-modal systems 
could use the additional, more accurate input chan-
nels not only for confirmation or immediate cor-
rection, but also to learn from the interaction and 
improve their performance over time, without ex-
plicit human supervision. For example, in the inte-
raction paradigm described above, apart from 
providing the means for selecting the correct rec-
ognition result from an n-best list, the user click on 
a hypothesis can provide valuable information 
about the errors made by system, which could be 
exploited to further improve performance.  
Consider for instance the following numbers 
from an analysis of logged click data in the Live 
Search for Windows Mobile system. Over a certain 
period of time, the results Beer and Gear were dis-
played together in an n-best list 122 times. Out of 
these cases, Beer was clicked 67% of the time, and 
Gear was never clicked. In 25% of the cases when 
Beer was selected, Gear was incorrectly presented 
above (i.e. higher than) Beer in the n-best list. 
More importantly, there are also 82 cases in which 
Gear appears in an n-best list, but Beer does not. A 
manual inspection reveals that, in 22% of these 
cases, the actual spoken utterance was indeed Beer. 
The clicks therefore indicate that the engine often 
misrecognizes Gear instead of Beer.  
21
Ideally, the system should be able to take advan-
tage of this information and use the clicks to create 
an automatic positive feedback loop. We can envi-
sion several ways in which this could be accom-
plished. A possible approach would be to use all 
the clicked results to adapt the existing language or 
acoustic models. Another, higher-level approach is 
to treat the recognition process as a black-box, and 
use the click feedback (perhaps also in conjunction 
with other high-level information) to post-process 
the results recognition results. 
While both approaches have their merits, in this 
work we concentrate on the latter paradigm. We 
introduce a novel n-best correction model that le-
verages the click data to improve performance in a 
speech-enabled multi-modal application. The pro-
posed model works in two stages. First, the n-best 
list generated by the speech recognizer is expanded 
with additional candidates, based on results confu-
sability information captured by the click statistics. 
For instance, in the 82 cases we mentioned above 
when Gear was present in the n-best list but Beer 
was not, Beer (as well as potentially other results) 
would also be added to form an expanded n-best 
list. The expanded list is then rescored and pruned 
to construct a corrected, more accurate n-best list.  
The proposed approach, described in detail in 
Section 3, draws inspiration from earlier work in 
post-recognition error-correction models (Ringger 
and Allen, 1996; Ringger and Allen, 1997) and n-
best rescoring (Chotimongkol and Rudnicky, 2001; 
Birkenes et al, 2007). The novelty of our approach 
lies in: (1) the use of user click data in a deployed 
multi-modal system for creating a positive feed-
back loop, and (2) the development of an n-best 
correction model based on implicit feedback that 
outperforms traditional rescoring-only approaches. 
Later on, in Section 5, we will discuss in more de-
tail the relationship of the proposed approach to 
these and other works previously reported in the 
literature.  
Before moving on to describe the n-best correc-
tion model in more detail, we give a high-level 
overview of Live Search for Windows Mobile, the 
multi-modal, mobile local search application that 
provided the test-bed for evaluating this work.  
2 Live Search for Windows Mobile  
Live Search for Windows Mobile is an application 
that enables local web-search on mobile devices. In 
its current version, it allows users to find informa-
tion about local businesses and restaurants, to ob-
tain driving directions, explore maps, view current 
traffic, get movie show-times, etc. A number of 
screen-shots are illustrated in Figure 1. 
Recently, Live Search for Windows Mobile has 
been extended with a speech interface (notice the 
Speak button assigned to the left soft-key in Figure 
1.a.) The speech-based interaction with the system 
proceeds as follows: the user clicks the Speak but-
ton and speaks the name of a local business, for 
instance A-B-C Hauling, or a general category such 
as Vietnamese Restaurants. The application end-
points the audio and forwards it over the data 
channel to a server (Figure 1.b.) Recognition is 
performed on the server side, and the resulting n-
best list is sent back to the client application, where 
it is displayed to the user (Figure 1.c.) The user can 
select the correct item from the n-best list, re-speak 
the request, or abandon the interaction altogether 
by pressing Cancel. Once the user selects an item in 
the n-best list, the corresponding search results are 
displayed (Figure 1.d.) 
(a) (b) (c) (d) 
Figure 1. Windows Live Search for Mobile. (a) initial screen; (b) user is speaking a request; (c) n-best list 
is presented; (d) final search results are displayed 
22
Apart from business names, the system also 
handles speech input for addresses, as well as 
compound requests, such as Shamiana Restaurant 
in Kirkland, Washington. For the latter cases, a 
two-tier recognition and confirmation process is 
used. In the first stage a location n-best list is gen-
erated and sent to the client for confirmation. After 
the user selects the location, a second recognition 
stage uses a grammar tailored to that specific loca-
tion to re-recognize the utterance. The client then 
displays the final n-best list from which the user 
can select the correct result. 
Several details about the system architecture and 
the structure of the recognition process have been 
omitted here due to space considerations. For the 
interested reader, a more in-depth description of 
this system is available in (Acero et al, 2008).  
3 Approach 
We now turn our attention to the proposed n-best 
correction model 
3.1 Overview 
The model works in two stages, illustrated in Fig-
ure 2. In the first stage the n-best list produced by 
the speech recognizer is expanded with several 
alternative hypotheses. In the second stage, the 
expanded n-best list is rescored to construct the 
final, corrected n-best list.  
The n-best expansion step relies on a result con-
fusion matrix, constructed from click information. 
The matrix, which we will describe in more detail 
in the following subsection, contains information 
about which result was selected (clicked) by the 
user when a certain result was displayed. For in-
stance, in the example from Figure 2, the matrix 
indicates that when Burlington appeared in the n-
best list, Bar was clicked once, Bowling was 
clicked 13 times, Burger King was clicked twice, 
and Burlington was clicked 15 times (see hashed 
row in matrix.) The last element in the row indi-
cates that there were 7 cases in which Burlington 
was decoded, but nothing (?) was clicked. Essen-
tially, the matrix captures information about the 
confusability of different recognition results.  
The expansion step adds to an n-best list gener-
ated by the recognizer all the results that were pre-
viously clicked in conjunction with any one of the 
items in the given n-best list. For instance, in the 
example from Figure 2, the n-best list contains 
Sterling, Stirling, Burlington and Cooling. Based 
on the confusion matrix, this list will be expanded 
to also include Bar, Bowling, Burger King, Tow-
ing, and Turley. In this particular case, the correct 
recognition result, Bowling, is added in the ex-
panded n-best list.  
In the final step, the expanded list is rescored. In 
the previous example, for simplicity of explana-
tion, a simple heuristic for re-scoring was used: 
add all the counts on the columns corresponding to 
each expanded result. As a consequence, the cor-
Burlington 
Cooling 
Sterling 
Stirling 
0  ?   7    0     0    ?     0    0  ?   1   0    9 
0  ?   4    0     0    ?   10    1  ?   2   2    5 
0  ?   4    0     0    ?     4    1  ?   0   0    9 
B
u
rl
in
g
to
n
 
B
o
w
li
n
g
 
B
u
rg
er
 K
in
g
 
T
o
w
in
g
 
T
u
rl
ey
 
S
ti
rl
in
g
 
B
ar
 
S
te
rl
in
g
 
Sterling 
Stirling 
Burlington 
Cooling + ? 
Bowling  28 
Burlington  15 
Sterling  14 
Towing  3 
Burger King  2 
Stirling  2 
Turley  2 
Bar  1 
 
Bar 
Bowling 
Burger King 
Burlington 
Sterling  
Stirling 
Towing  
Turley 
 
? 
Result Confusion Matrix 
Initial  
N-Best 
Expanded 
N-Best 
Corrected 
(expanded & 
rescored) 
N-Best 
Figure 2. A confusion-based n-best correction model 
1  ? 13    2   15    ?     0    0  ?   0   0    7 
?
 
Stage 1: Expansion Stage 2: Rescoring 
23
rect recognition result, Bowling, was pushed to the 
top of the n-best list.  
We begin by formally describing the construc-
tion of the results confusability matrix and the ex-
pansion process in the next two sub-sections. Then, 
we describe three rescoring approaches. The first 
one is based on an error-correction model con-
structed from the confusion matrix. The other two, 
are more traditional rescoring approaches, based 
on language model adaptation.  
3.2 The Result Confusion Matrix 
The result confusion matrix is computed in a sim-
ple traversal of the click logs. The rows in the ma-
trix correspond to decoded results, i.e. results that 
have appeared in an n-best list. The columns in the 
matrix correspond to clicked (or intended) results, 
i.e. results that the user has clicked on in the n-best 
list. The entries at the intersection of row ? and 
column ? correspond to the number of times result 
? was clicked when result ? was decoded: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
In addition, the last column in the matrix, de-
noted ? contains the number of times no result was 
clicked when result ? was displayed: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
The rows in the matrix can therefore be used to 
compute the maximum likelihood estimate for the 
conditional probability distribution: 
 
???(?|?) =
?? ,?
 ?? ,??
 . 
 
The full dimensions of the result confusion ma-
trix can grow very large since the matrix is con-
structed at the result level (the average number of 
words per displayed result is 2.01). The number of 
rows equals the number of previously decoded re-
sults, and the number of columns equals the num-
ber of previously clicked results. However, the 
matrix is very sparse and can be stored efficiently 
using a sparse matrix representation. 
3.3 N-Best Expansion 
The first step in the proposed n-best correction 
model is to expand the initial n-best list with all 
results that have been previously clicked in con-
junction with the items in the current n-best list. 
Let?s denote by ? = {??}?=1..?  the initial n-best 
list produced by the speech recognizer. Then, the 
expanded n-best list ?? will contain all ?? , as well 
as all previously clicked results ? such that there 
exists ? with ??? ,? > 0. 
3.4 Confusion Matrix Based Rescoring  
Ideally, we would like to rank the hypotheses in 
the expanded list ?? according to ?(?|?), where ? 
represents the intended result and ? represents the 
acoustics of the spoken utterance. This can be re-
written as follows: 
 
                       ? ? ? =  ?(?|?) ? ?(?|?)? .             [1] 
 
The first component in this model is an error-
correction model ?(?|?). This model describes the 
conditional probability that the correct (or in-
tended) result is ? given that result ? has been de-
coded. While this conditional model cannot be 
constructed directly, we can replace it by a proxy - 
?(?|?), which models the probability that the re-
sult ? will be clicked, given that result ? was de-
coded. As mentioned earlier in subsection 3.2, this 
conditional probability distribution can be com-
puted from the result confusion matrix. In replac-
ing ? ? ?  with ?(?|?), we are making the 
assumption that the clicks correspond indeed to the 
correct, intended results, and to nothing else1. 
Notice that the result confusion matrix is gener-
ally very sparse. The maximum likelihood estima-
tor ???(?|?) will therefore often be inappropriate. 
To address this data sparsity issue, we linearly in-
terpolate the maximum likelihood estimator with 
an overall model ??(?|?): 
 
? ? ? =  ???? ? ? + (1? ?)?? ? ? . 
 
The overall model is defined in terms of two 
constants, ? and ?, as follows: 
 
?? ? ? =  
?, ?? ? = ?
?, ?? ? ? ?
  
 
where ? is the overall probability in the whole 
dataset of clicking on a given decoded result, and 
? is computed such that ?? ? ?  normalizes to 1. 
                                                          
1 While this assumption generally holds, we have also ob-
served cases where it is violated: sometimes users (perhaps 
accidentally) click on an incorrect result; other times the cor-
rect result is in the list but nothing is clicked (perhaps the user 
was simply testing out the recognition capabilities of the sys-
tem, without having an actual information need) 
24
Finally, the ? interpolation parameter is determined 
empirically on the development set.  
The second component in the confusion based 
rescoring model from equation [1] is ?(?|?). This 
is the recognition score for hypothesis ?. The n-
best rescoring model from [1] becomes: 
 
? ? ? =   ???? ? ?? + (1? ?)?? ? ??  ? ?(?? |?)
????
 
3.5 Language Model Based Rescoring 
A more traditional alternative for n-best rescoring 
is to adapt the bigram language model used by the 
system in light of the user click data, and re-rank 
the decoded results by: 
 
? ? ? ? ? ??  ? ? ? ? ?? ?(??) 
 
Here ? ? ??  is the acoustic score assigned by 
the recognizer to hypothesis ?? , and ?(??) is the 
adapted language model score for this hypothesis.  
A simple approach for adapting the system?s 
language model is to add the word sequences of 
the user-clicked results to the original training sen-
tences and to re-estimate the language model ?(?). 
We will refer to this method as maximum likelih-
ood (ML) estimation. A second approach, referred 
to as conditional maximum likelihood (CML) es-
timation, is to adapt the language model such as to 
directly maximize the conditional likelihood of the 
correct result given acoustics, i.e., 
 
? ? ? =
? ? ? ?(?)
 ? ? ?? ?(??)????
 
 
Note that this is the same objective function as 
the one used in Section 3.4, except that here the 
click data is used to estimate the language model 
instead of the error correction model. Again, in 
practice we assume that users click on correct re-
sults, i.e. ? = ?. 
4 Experiments  
We now discuss a number of experiments and the 
results obtained using the proposed n-best correc-
tion approach.  
4.1 Data 
For the purposes of the experiments described be-
low we extracted just over 800,000 queries from 
the server logs in which the recognizer had gener-
ated a simple n-best list2. For each recognition 
event, we collected from the system logs the n-best 
list, and the result clicked by the user (if the user 
clicked on any result).  
In addition, for testing purposes, we also make 
use of 11529 orthographically transcribed user re-
quests. The transcribed set was further divided into 
a development set containing 5680 utterances and 
a test set containing 5849 utterances.  
4.2 Initial N-Best Rescoring 
To tease apart the effects of expansion and rescor-
ing in the proposed n-best correction model, we 
began by using the rescoring techniques on the 
initial n-best lists, without first expanding them. 
Since the actual recognition confidence scores 
?(?? |?) were not available in the system logs, we 
replaced them with an exponential probability den-
sity function based on the rank of the hypothesis:  
 
? ??  ? = 2
??  
 
We then rescored the n-best lists from the test 
set according to the three rescoring models de-
scribed earlier: confusion matrix, maximum like-
lihood (ML), and conditional maximum likelihood 
(CML). We computed the sentence level accuracy 
for the rescored n-best list, at different cutoffs. The 
accuracy was measured by comparing the rescored 
hypotheses against the available transcripts. 
Note that the maximum depth of the n-best lists 
generated by the recognizer is 10; this is the max-
imum number of hypotheses that can be displayed 
on the mobile device. However, the system may 
generate fewer than 10 hypotheses. The observed 
average n-best list size in the test set was 4.2.  
The rescoring results are illustrated in Figure 3 
and reported in Table 1. The X axis in Figure 3 
shows the cutoff at which the n-best accuracy was 
computed. For instance in the baseline system, the 
correct hypothesis was contained in the top result 
in 46.2% of cases, in the top-2 results in 50.5% of 
the cases and in the top-3 results in 51.5% of the 
cases. The results indicate that all the rescoring 
models improve performance relative to the base-
                                                          
2 We did not consider cases where a false-recognition event 
was fired (e.g. if no speech was detected in the audio signal) ? 
in these cases no n-best list is generated. We also did not con-
sider cases where a compound n-best was generated (e.g. for 
compound requests like Shamiana in Kirkland, Washington) 
25
line. The improvement is smallest for the maxi-
mum likelihood (ML) language model rescoring 
approach, but is still statistically significant 
(? = 0.008 in a Wilcoxon sign-rank test.) The con-
fusion-matrix based rescoring and the CML rescor-
ing models perform similarly well, leading to a 1% 
absolute improvement in 1-best and 2-best sen-
tence-level accuracy from the baseline (? < 10?5). 
No statistically significant difference can be de-
tected between these two models. At the same 
time, they both outperform the maximum likelih-
ood rescoring model (? < 0.03). 
4.3 N-Best Correction 
Next, we evaluated the end-to-end n-best correc-
tion approach. The n-best lists were first expanded, 
as described in section 3.3, and the expanded lists 
were ranked using the confusion matrix based res-
coring model described in Section 3.4.  
The expansion process enlarges the original n-
best lists. Immediately after expansion, the average 
n-best size grows from 4.2 to 96.9. The oracle per-
formance for the expanded n-best lists increases to 
59.8% (versus 53.5% in the initial n-best lists.) 
After rescoring, we trimmed the expanded n-best 
lists to a maximum of 10 hypotheses: we still want 
to obey the mobile device display constraint. The 
resulting average n-best size was 7.09 (this is low-
er than 10 since there are cases when the system 
cannot generate enough expansion hypotheses.) 
The sentence-level accuracy of the corrected n-
best lists is displayed in line 4 from Table 1. A di-
rect comparison with the rescoring-only models or 
with the baseline is however unfair, due to the 
larger average size of the corrected n-best lists. To 
create a fair comparison and to better understand 
the performance of the n-best correction process, 
we pruned the corrected n-best lists by eliminating 
all hypotheses with a score below a certain thre-
shold. By varying this rejection threshold, we can 
therefore control the average depth of the resulting 
corrected n-best lists. At a rejection threshold of 
0.004, the average corrected n-best size is 4.15, 
comparable to the baseline of 4.2 .  
The performance for the corresponding cor-
rected (and pruned) n-best lists is shown in line 5 
from Table 1 and illustrated in Figure 4. In contrast 
to a rescoring-only approach, the expansion pro-
cess allows for improved performance at higher 
depths in the n-best list. The maximum n-best per-
formance (while keeping the average n-best size at 
4.15), is 56.5%, a 3% absolute improvement over 
the baseline (? < 10?5).  
Figure 5 provides more insight into the relation-
ship between the sentence-level accuracy of the 
corrected (and pruned) n-best lists and the average 
n-best size (the plot was generated by varying the 
rejection threshold.) The result we discussed above 
can also be observed here: at the same average n-
best size, the n-best correction model significantly 
outperforms the baseline. Furthermore, we can see 
that we can attain the same level of accuracy as the 
baseline system while cutting the average n-best 
size by more than 50%, from 4.22 to 2. In the op-
posite direction, if we are less sensitive to the 
number of items displayed in the n-best list (except 
for the 10-maximum constraint we already obey), 
we can further increase the overall performance by 
another 0.8% absolute to 57.3%; this overall accu-
racy is attained at an average n-best size of 7.09.  
Figure 3. Initial n-best rescoring (test-set) 
Table 1. Test-set sentence-level n-best accuracy; 
(0) baseline; (1)-(3) initial n-best rescoring;  
(4)-(5) expansion + rescoring 
 Model 1-
Best 
2-
Best 
3-
Best 
10-
Best 
0 Baseline 46.2 50.5 51.5 53.5 
1 ML Rescoring  46.8 50.9 52.1 53.5 
2 CML Rescoring 47.4 51.4 52.6 53.5 
3 Confusion Matrix Resc. 47.3 51.5 52.5 53.5 
4 Expansion + Rescoring 
(size=7.09) 
46.8 52.3 54.5 57.3 
5 Expansion + Rescoring 
(size=4.15) 
46.8 52.3 54.4 56.5 
 
26
Finally, we also investigated rescoring the ex-
panded n-best lists using the CML approach. To 
apply CML, an initial ranking of the expanded n-
best lists is however needed. If we use the ranking 
produced by the confusion-matrix based model 
discussed above, no further performance improve-
ments can be observed.  
5 Related work 
The n-best correction model we have described in 
this paper draws inspiration from earlier works on 
post-recognition error correction models, n-best 
rescoring and implicitly supervised learning. In 
this section we discuss some of the similarities and 
differences between the proposed approach and 
previous work. 
The idea of correcting speech recognition errors 
in a post-processing step has been proposed earlier 
by (Ringger and Allen, 1996; Ringger and Allen, 
1997). The authors showed that, in the presence of 
transcribed data, a translation-based post-processor 
can be trained to correct the results of a speech 
recognizer, leading to a 15% relative WER im-
provement in a corpus of TRAINS-95 dialogues.  
The n-best correction approach described here is 
different in two important aspects. First, instead of 
making use of transcripts, the proposed error-
correction model is trained using implicit user 
feedback obtained in a multi-modal interface (in 
this case user clicks in the n-best list.) This is a less 
costly endeavor, as the system automatically ob-
tains the supervision signal directly from the inte-
raction; no transcripts are necessary. Second, the 
approach operates on the entire n-best list, rather 
than only on the top hypothesis; as such, it has ad-
ditional information that can be helpful in making 
corrections. At Figure 2 illustrates, there is a poten-
tial for multiple incorrect hypotheses to point to-
wards and reinforce the same correction 
hypothesis, leading to improved performance (in 
this example, Burlington, Cooling, Sterling and 
Stirling were all highly confusable with Bowling, 
which was the correct hypothesis). 
The n-best correction model we have described 
includes a rescoring step. N-best rescoring ap-
proaches have been investigated extensively in the 
speech recognition community. In the dialog 
community, n-best rescoring techniques that use 
higher-level, dialog features have also been pro-
posed and evaluated (Chotimongkol and Rudnicky, 
2001). Apart from using the click feedback, the 
novelty in our approach lies in the added expansion 
step and in the use of an error-correction model for 
rescoring. We have seen that the confusability-
based n-best expansion process leads to signifi-
cantly improved performance, even if we force the 
model to keep the same average n-best size. 
Finally, the work discussed in this paper has 
commonalities with previous works on lightly su-
pervised learning in the speech community, e.g. 
(Lamel and Gauvain, 2002) and leveraging implicit 
feedback for learning from interaction, e.g. (Baner-
jee and Rudnicky, 2007; Bohus and Rudnicky, 
2007). In all these cases, the goal is to minimize 
the need for manually-labeled data, and learn di-
Figure 5. Overall n-best accuracy as a function of 
the average n-best size  
53.5% 
56.5% 
57.3% 
Figure 4. N-Best correction (test-set) 
27
rectly from the interaction. We believe that in the 
long term this family of learning techniques will 
play a key role towards building autonomous, self-
improving systems. 
6 Conclusion and future work 
We have proposed and evaluated a novel n-best 
correction model that leverages implicit user feed-
back in a multi-modal interface to create a positive 
feedback loop. While the experiments reported 
here were conducted in the context of a local 
search application, the approach is applicable in 
any multi-modal interface that elicits selection in 
an n-best list from the user.  
The proposed n-best correction model works in 
two stages. First, the n-best list generated by the 
speech recognizer is expanded with additional hy-
potheses based on confusability information cap-
tured from previous user clicks. This expanded list 
is then rescored and pruned to create a more accu-
rate and more compact n-best list. Our experiments 
show that the proposed n-best correction approach 
significantly outperforms both the baseline and 
other traditional n-best rescoring approaches, with-
out increasing the average length of the n-best lists.  
Several issues remain to be investigated. The 
models discussed in this paper focus on post-
recognition processing. Other ways of using the 
click data can also be envisioned. For instance, one 
approach would be to add all the clicked results to 
the existing language model training data and 
create an updated recognition language model. In 
the future, we plan to investigate the relationship 
between these two approaches, and to whether they 
can be used in conjunction. Earlier related work 
(Ringger and Allen, 1997) suggests that this should 
indeed be the case. 
Second, the click-based error-correction model 
we have described in section 3.4 operates at the 
result level. The proposed model is essentially a 
sentence level, memory-based translation model. 
In the future, we also plan to investigate word-
level error-correction models, using machine trans-
lation techniques like the ones discussed in (Ring-
ger and Allen, 1997; Li et al, 2008). 
Finally, we plan to investigate how this process 
of learning from implicit feedback in a multi-
modal interface can be streamlined, such that the 
system continuously learns online, with a minimal 
amount of human intervention.  
Acknowledgments 
This work would have not been possible without 
the help of a number of other people. We would 
like to especially thank Oliver Scholz, Julian 
Odell, Christopher Dac, Tim Paek, Y.C. Ju, Paul 
Bennett, Eric Horvitz and Alex Acero for their 
help and for useful conversations and feedback. 
References  
Acero, A., N. Bernstein, et al (2008). "Live Search for 
Mobile: Web Services by Voice on the Cellphone". 
ICASSP'08. Las Vegas, NV. 
Banerjee, S. and A. Rudnicky (2007). "Segmenting 
Meetings into Agenda Items by Extracting Implicit 
Supervision from Human Note-Taking". IUI'2007. 
Honolulu, Hawaii. 
Birkenes, O., T. Matsui, et al (2007). "N-Best Rescor-
ing for Speech Recognition using Penalized Logis-
tic Regression Machines with Garbage Class". 
ICASSP'2007, Honolulu, Hawaii. 
Bohus, D. and A. Rudnicky (2007). "Implicitly-
supervised learning in spoken language interfaces: 
an application to the confidence annotation prob-
lem". SIGdial 2007, Antwerp, Belgium. 
Chotimongkol, A. and A. Rudnicky (2001). "N-best 
Speech Hypotheses Reordering Using Linear Re-
gression". Eurospeech'2001, Aalborg, Denmark. 
Lamel, L. and J.-L. Gauvain (2002). "Lightly Super-
vised and Unsupervised Acoustic Model Training." 
Computer Speech and Language 16: 115-129. 
Li, X., Y.-C. Ju, et al (2008). "Language Modeling for 
Voice Search: a Machine Translation Approach". 
ICASSP'08, Las Vegas, NV. 
Live Search for Windows Mobile (2008): 
 http://mobile.search.live.com 
Nuance Mobile Search (2008): 
http://www.nuance.com/mobilesearch. 
Ringger, E. and J. Allen (1996). "Error Correction via 
Post-Processor for Continuous Speech Recogni-
tion". ICASSP'96, Atlanta, GA. 
Ringger, E. and J. Allen (1997). "Robust Error Correc-
tion of Continuous Speech Recognition". ESCA-
NATO Workshop on Robust Speech Recognition 
for Unknown Communication Channels, Pont-a-
Mousson, France. 
TellMeByMobile (2008): 
http://www.tellme.com/products/tellmebymobile. 
V-Lingo Mobile. (2008): 
http://www.vlingomobile.com/downloads.html. 
VoiceSignal Search. (2008): 
http://www.voicesignal.com/solutions/vsearch.php. 
 
28
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 104?111,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Optimal Dialog in Consumer-Rating Systems using a POMDP Framework
Zhifei Li
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
zhifei.work@gmail.com
Patrick Nguyen, Geoffrey Zweig
Microsoft Corporation
1 Microsoft Way,
Redmond, WA 98052, USA
{panguyen,gzweig}@microsoft.com
Abstract
Voice-Rate is an experimental dialog system
through which a user can call to get prod-
uct information. In this paper, we describe
an optimal dialog management algorithm for
Voice-Rate. Our algorithm uses a POMDP
framework, which is probabilistic and cap-
tures uncertainty in speech recognition and
user knowledge. We propose a novel method
to learn a user knowledge model from a review
database. Simulation results show that the
POMDP system performs significantly better
than a deterministic baseline system in terms
of both dialog failure rate and dialog interac-
tion time. To the best of our knowledge, our
work is the first to show that a POMDP can
be successfully used for disambiguation in a
complex voice search domain like Voice-Rate.
1 Introduction
In recent years, web-based shopping and rating sys-
tems have provided a valuable service to consumers
by allowing them to shop products and share their
assessments of products online. The use of these
systems, however, requires access to a web interface,
typically through a laptop or desktop computer, and
this restricts their usefulness. While mobile phones
also provide some web access, their small screens
make them inconvenient to use. Therefore, there
arises great interests in having a spoken dialog in-
terface through which a user can call to get product
information (e.g., price, rating, review, etc.) on the
fly. Voice-Rate (Zweig et al, 2007) is such a sys-
tem. Here is a typical scenario under which shows
the usefulness of the Voice-Rate system. A user en-
ters a store and finds that a digital camera he has
not planned to buy is on sale. Before he decides
to buy the camera, he takes out his cell phone and
calls Voice-Rate to see whether the price is really
a bargain and what other people have said about
the camera. This helps him to make a wise deci-
sion. The Voice-Rate system (Zweig et al, 2007) in-
volves many techniques, e.g., information retrieval,
review summarization, speech recognition, speech
synthesis, dialog management, etc. In this paper, we
mainly focus on the dialog management component.
When a user calls Voice-Rate for the information
of a specific product, the system needs to identify,
from a database containing millions of products, the
exact product the user intends. To achieve this, the
system first solicits the user for the product name.
Using the product name as a query, the system then
retrieves from its database a list of products related
to the query. Ideally, the highest-ranked product
should be the one intended by the user. In reality,
this is often not the case due to various reasons. For
example, there might be a speech recognition error
or an information retrieval ranking error. Moreover,
the product name is usually very ambiguous in iden-
tifying an exact product. The product name that the
user says may not be exactly the same as the name
in the product database. For example, while the user
says ?Canon Powershot SD750?, the exact name
in the product database may be ?Canon Powershot
SD750 Digital Camera?. Even the user says the ex-
act name, it is possible that the same name may be
corresponding to different products in different cat-
egories, for instance books and movies.
Due to the above reasons, whenever the Voice-
Rate system finds multiple products matching the
user?s initial speech query, it initiates a dialog proce-
dure to identify the intended product by asking ques-
tions about the products. In the product database,
104
many attributes can be used to identify a product.
For example, a digital camera has the product name,
category, brand, resolution, zoom, etc. Given a list
of products, different attributes may have different
ability to distinguish the products. For example, if
the products belong to many categories, the category
attribute is very useful to distinguish the products. In
contrast, if all the products belong to a single cate-
gory, it makes no sense to ask a question on the cat-
egory. In addition to the variability in distinguishing
products, different attributes may require different
knowledge from the user in order for them to an-
swer questions about these attributes. For example,
while most users can easily answer a question on
category, they may not be able to answer a question
on the part number of a product, though the part
number is unique and perfect to distinguish prod-
ucts. Other variabilities are in the difficulty that the
attributes impose on speech recognition and speech
synthesis. Clearly, given a list of products and a set
of attributes, what questions and in what order to ask
is essential to make the dialog successful. Our goal
is to dynamically find such important attributes at
each stage/turn.
The baseline system (Zweig et al, 2007) asks
questions only on product name and category. The
order of questions is fixed: first ask questions on
product category, and then on name. Moreover, it
is deterministic and does not model uncertainly in
speech recognition and user knowledge. Partially
observable Markov decision process (POMDP) has
been shown to be a general framework to capture the
uncertainty in spoken dialog systems. In this paper,
we present a POMDP-based probabilistic system,
which utilizes rich product information and captures
uncertainty in speech recognition and user knowl-
edge. We propose a novel method to learn a user
knowledge model from a review database. Our sim-
ulation results show that the POMDP-based system
improves the baseline significantly.
To the best of our knowledge, our work is the first
to show that a POMDP can be successfully used for
disambiguation in a complex voice search domain
like Voice-Rate.
2 Voice-Rate Dialog System Overview
Figure 1 shows the main flow in the Voice-Rate sys-
tem with simplification. Specifically, when a user
calls Voice-Rate for the information of a specific
 
 
Yes 
Begin 
Information Retrieval 
Dialog Manager 
End 
Initial Speech Query 
List of Products 
Corrupted User Action 
Human 
Speech recognizer 
User Action 
Found 
product? No  
Play Rating 
Question 
? Intended product  
Figure 1: Flow Chart of Voice-Rate System
Step-1: remove products that do not match
the user action
Step-2: any category question to ask?
yes: ask the question and return
no: go to step-3
Step-3: ask a product name question
Table 1: Baseline Dialog Manager Algorithm
product, the system first solicits the user for the
product name. Treating the user input as a query
and the product names in the product database as
documents, the system retrieves a list of products
that match the user input based on TF-IDF mea-
sure. Then, the dialog manager dynamically gener-
ates questions to identify the specific intended prod-
uct. Once the product is found, the system plays
back its rating information. In this paper, we mainly
focus on the dialog manager component.
Baseline Dialog Manager: Table 1 shows the
baseline dialog manager. In Step-1, it removes all
the products that are not consistent with the user re-
sponse. For example, if the user answers ?camera?
when given a question on category, the system re-
moves all the products that do not belong to category
?camera?. In Step-2 and Step-3, the baseline system
asks questions about product name and product cat-
egory, and product category has a higher priority.
3 Overview of POMDP
3.1 Basic Definitions
A Partially Observable Markov Decision Process
(POMDP) is a general framework to handle uncer-
tainty in a spoken dialog system. Following nota-
105
tions in Williams and Young (2007), a POMDP is
defined as a tuple {S,A, T,R,O,Z, ?,~b0} where S
is a set of states s describing the environment; A is
a set of machine actions a operating on the environ-
ment; T defines a transition probability P (s? |s, a);
R defines a reward function r(s, a); O is a set of ob-
servations o, and an observation can be thought as
a corrupted version of a user action; Z defines an
observation probability P (o? |s? , a); ? is a geometric
discount factor; and~b0 is an initial belief vector.
The POMDP operates as follows. At each time-
step (a.k.a. stage), the environment is in some unob-
served state s. Since s is not known exactly, a distri-
bution (called a belief vector ~b) over possible states
is maintained where~b(s) indicates the probability of
being in a particular state s. Based on the current be-
lief vector ~b, an optimal action selection algorithm
selects a machine action a, receives a reward r, and
the environment transits to a new unobserved state
s? . The environment then generates an observation
o? (i.e., a user action), after which the system update
the belief vector ~b. We call the process of adjusting
the belief vector~b at each stage ?belief update?.
3.2 Applying POMDP in Practice
As mentioned in Williams and Young (2007), it is
not trivial to apply the POMDP framework to a
specific application. To achieve this, one normally
needs to design the following three components:
? State Diagram Modeling
? Belief Update
? Optimal Action Selection
The state diagram defines the topology of the
graph, which contains three kinds of elements: sys-
tem state, machine action, and user action. To drive
the transitions, one also needs to define a set of
models (e.g., user goal model, user action model,
etc.). The modeling assumptions are application-
dependent. The state diagram, together with the
models, determines the dynamics of the system.
In general, the belief update depends on the ob-
servation probability and the transition probability,
while the transition probability itself depends on the
modeling assumptions the system makes. Thus, the
exact belief update formula is application-specific.
Optimal action selection is essentially an opti-
mization algorithm, which can be defined as,
a? = argmax
a?A
G(P (a)), (1)
where A refers to a set of machine actions a.
Clearly, the optimal action selection requires three
sub-components: a goodness measure function G, a
prediction algorithm P , and a search algorithm (i.e.,
the argmax operator). The prediction algorithm is
used to predict the behavior of the system in the
future if a given machine action a was taken. The
search algorithm can use an exhaustive linear search
or an approximated greedy search depending on the
size of A (Murphy, 2000; Spaan and Vlassis, 2005).
4 POMDP Framework in Voice-Rate
In this section, we present our instantiation of
POMDP in the Voice-Rate system.
4.1 State Diagram Modeling
4.1.1 State Diagram Design
Table 2 summarizes the main design choices in
the state diagram for our application, i.e., identifying
the intended product from a large list of products.
As in Williams and Young (2007), we incorporate
both the user goal (i.e., the intended product) and
the user action in the system state. Moreover, to ef-
ficiently update belief vector and compute optimal
action, the state space is dynamically generated and
pruned. In particular, instead of listing all the possi-
ble combinations between the products and the user
actions, at each stage, we only generate states con-
taining the products and the user actions that are rel-
evant to the last machine action. Moreover, at each
stage, if the belief probability of a product is smaller
than a threshold, we prune out this product and all
its associated system states. Note that the intended
product may be pruned away due to an overly large
threshold. In the simulation, we will use a develop-
ment set to tune this threshold.
As shown in Table 2, five kinds of machine ac-
tions are defined. The questions on product names
are usually long, imposing difficulty in speech syn-
thesis/recgonition and user input. Thus, short ques-
tions (e.g., questions on category or simple at-
tributes) are preferable. This partly motivate us to
exploit rich product information to help the dialog.
Seven kinds of user actions are defined as shown
in Table 2. Among them, the user actions ?others?,
?not related?, and ?not known? are special. Specif-
ically, to limit the question length and to ensure the
106
Component Design Comments
System State (Product, User action) e.g., (HP Computer, Category: computer)
Machine Action Question on Category e.g., choose category: Electronics, Movie, Book
Question on Product name e.g., choose product name: Canon SD750 digital cam-
era, Canon Powershot A40 digital camera, Canon
SD950 digital camera, Others
Question on Attribute e.g., choose memory size: 64M, 128M, 256M
Confirmation question e.g., you want Canon SD750 camera, yes or no?
Play Rating e.g., I think you want Canon SD750 digital camera,
here is the rating!
User Action Category e.g., Movie
Product name e.g., Canon SD750 digital camera
Attribute value e.g., memory size: 64M
Others used when a question has too many possible options
Yes/No used for a confirmation question
Not related used if the intended product is unrelated to the question
Not known used if the user does not have required knowledge to
answer the question
Table 2: State Diagram Design in Voice-Rate
human is able to memorize all the options, we re-
strict the number of options in a single question to a
threshold N (e.g., 5). Clearly, given a list of prod-
ucts and a question, there might be more than N pos-
sible options. In such a case, we need to merge some
options into the ?others? class. The third example in
Table 2 shows an example with the ?others? option.
One may exploit a clustering algorithm (e.g., an it-
erative greedy search algorithm) to find an optimal
merge. In our system, we simply take the top-(N -1)
options (ranked by the belief probabilities) and treat
all the remaining options as ?others?.
The ?not related? option is required when some
candidate products are irrelevant to the question. For
example, when the system asks a question regarding
the attribute ?cpu speed? while the products contain
both books and computers, the ?not related? option
is required in case the intended product is a book.
Lastly, while some attributes are very useful to
distinguish the products, a user may not have enough
knowledge to answer a question on these attributes.
For example, while there is a unique part number for
each product, however, the user may not know the
exact part number for the intended product. Thus,
?not known? option is required whenever the system
expects the user is unable to answer the question.
4.1.2 Models
We assume that the user does not change his goal
(i.e., the intended product) along the dialog. We
also assume that the user rationally answers the
question to achieve his goal. Additionally, we as-
sume that the speech synthesis is good enough such
that the user always gets the right information that
the system intends to convey. The two main mod-
els that we consider include an observation model
that captures speech recognition uncertainty, and a
user knowledge model that captures the variability
of user knowledge required for answering questions
on different attributes.
Observation Model: Since the speech recogni-
tion engine we are using returns only a one-best and
its confidence value C ? [0, 1]. We define the obser-
vation function as follows,
P (a?u|au) =
{
C if a?u = au,
1?C
|Au|?1 otherwise.
(2)
where au is the true user action, a?u is the speech
recognition output (i.e., corrupted user action), and
Au is the set of user actions related to the last ma-
chine action.
User Knowledge Model: In most of the appli-
cations (Roy et al, 2000; Williams, 2007) where
107
the POMDP framework got applied, it is normally
assumed that the user needs only common sense to
answer the questions asked by the dialog system.
Our application is more complex as the product in-
formation is very rich. A user may have different
difficulty in answering different questions. For ex-
ample, while a user can easily answer a question on
category, he may not be able to answer a question
on the part number. Thus, we define a user knowl-
edge model to capture such uncertainty. Specifically,
given a question (say am) and an intended product
(say gu) in the user?s mind, we want to know how
likely the user has required knowledge to answer the
question. Formally, the user knowledge model is,
P (au|gu, am) =
?
??
??
P (unk|gu, am) if au=unk,
1? P (unk|gu, am) if au=truth,
0 otherwise.
(3)
where unk represents the user action ?not known?.
Clearly, given a specific product gu and a specific
question am, there is exactly one correct user ac-
tion (represented by truth in Equation 3), and its
probability is 1 ? P (unk|gu, am). Now, to obtain
a user knowledge model, we only need to obtain
P (unk|gu, am). As shown in Table 2, there are four
kinds of question-type machine actions am. We as-
sume that the user always has knowledge to answer
a question regarding the category and product name,
and thus P (unk|gu, am) for these types of machine
actions are zero regardless of what the specific prod-
uct gu is. Therefore, we only need to consider
P (unk|gu, am) when am is a question about an at-
tribute (say attr). Moreover, since there are millions
of products, to deal with the data sparsity issue, we
assume P (unk|gu, am) does not depends on a spe-
cific product gu, instead it depends on only the cate-
gory (say cat) of the product gu. Therefore,
P (unk|gu, am) ? P (unk|cat,attr). (4)
Now, we only need to get the probability
P (unk|cat,attr) for each attribute attr in each cate-
gory cat. To learn P (unk|cat,attr), one may collect
data from human, which is very expensive. Instead,
we learn this model from a database of online re-
views for the products. Our method is based on the
following intuition: if a user cares/knows about an
attribute of a product, he will mention either the at-
tribute name, or the attribute value, or both in his
review of this product. With this intuition, the occur-
rence frequency of a given attr in a given category
cat is collected from the review database, followed
by proper weighting, scaling and normalization, and
thus P (unk|cat,attr) is obtained.
4.2 Belief Update
Based on the model assumptions in Section 4.1.2,
the belief update formula for the state (gu, a?u) is,
~b(gu, a?u) = (5)
k ? P (a??u|a
?
u)P (a
?
u|gu, am)
?
au?A(gu)
~b(gu, au)
where k is a normalization constant. The P (a??u|a
?
u)
is the observation function as defined in Equation 2,
while P (a?u|gu, am) is the user knowledge model as
defined in Equation 3. The A(gu) represents the set
of user actions au related to the system states for
which the intended product is gu.
In our state representation, a single product gu
is associated with several states which differ in the
user action au, and the belief probability of gu is the
sum of the probabilities of these states. Therefore,
even there is a speech recognition error or an un-
intentional user mistake, the true product still gets
a non-zero belief probability (though the true/ideal
user action au gets a zero probability). Moreover,
the probability of the true product will get promoted
through later iterations. Therefore, our system has
error-handling capability, which is one of the major
advantages over the deterministic baseline system.
4.3 Optimal Action Selection
As mentioned in Section 3.2, the optimal action se-
lection involves three sub-components: a prediction
algorithm, a goodness measure, and a search algo-
rithm. Ideally, in our application, we should mini-
mize the time required to successfully identify the
intended product. Clearly, this is too difficult as
it needs to predict the infinite future and needs to
encode the time into a reward function. Therefore,
for simplicity, we predict only one-step forward, and
use the entropy as a goodness measure1. Formally,
1Due to this approximation, one may argue that our model
is more like the greedy information theoretic model in Paek and
Chickering (2005), instead of a POMDP model. However, we
believe that our model follows the POMDP modeling frame-
work in general, though it does not involve reinforcement learn-
ing currently.
108
the optimization function is as follows:
a? = argmin
a?A
H(Products | a), (6)
where H(Products | a) is the entropy over the belief
probabilities of the products if the machine action
a was taken. When predicting the belief vector us-
ing Equation 5, we consider only the user knowledge
model and ignore the observation function2.
In the above, we consider only the question-type
machine actions. We also need to decide when
to take the play rating action such that the dialog
will terminate. Specifically, we take the play rating
action whenever the belief probability of the most
probable product is greater than a threshold. More-
over, the threshold should depend on the number of
surviving products. For example, if there are fifty
surviving products and the most probable product
has a belief probability greater than 0.3, it is reason-
able to take the play rating action. This is not true
if there are only four surviving products. Also note
that if we set the thresholds to too small values, the
system may play the rating for a wrong product. We
will use a development set to tune these thresholds.
4.3.1 Machine Action Filtering during Search
We use an exhaustive linear search for the opera-
tor argmin in Equation 6. However, additional filter-
ing during the search is required.
Repeated Question: Since the speech response
from the user to a question is probabilistic, it is quite
possible that the system will choose the same ques-
tion that has been asked in previous stages3. Since
our product information is very rich, many differ-
ent questions have the similar capability to reduce
entropy. Therefore, during the search, we simply ig-
nore all the questions asked in previous stages.
?Not Related? Option: While reducing entropy
helps to reduce the confusion at the machine side, it
does not measure the ?weirdness? of a question to
the human. For example, when the intended product
is a book and the candidate products contain both
books and computers, it is quite possible that the
optimal action, based solely on entropy reduction,
2Note that we ignore the observation function only in the
prediction, not in real belief update.
3In a regular decision tree, the answer to a question is deter-
ministic. It never asks the same question as that does not lead to
any additional reduction of entropy. This problem is also due to
the fact we do not have an explicit reward function.
is a question on the attribute ?cpu speed?. Clearly,
such a question is very weird to the human as he is
looking for a book that has nothing related to ?cpu
speed?. Though the user may be able to choose the
?not related? option correctly after thinking for a
while, it degrades the dialog quality. Therefore, for
a given question, whenever the system predicts that
the user will have to choose the ?not related? option
with a probability greater than a threshold, we sim-
ply ignore such questions in the search. Clearly, if
we set the threshold as zero, we essentially elimi-
nates the ?not related? option. That is, at each stage,
we generate questions only on attributes that apply
to all the candidate products. Since we dynamically
remove products whose probability is smaller than
a threshold at each stage, the valid question set dy-
namically expands. Specifically, at the beginning,
only very general questions (e.g., questions on cate-
gory) are valid, then more refined questions become
valid (e.g., questions on product brand), and finally
very specific questions are valid (e.g, questions on
product model). This leads to very natural behav-
ior in identifying a product, i.e., coarse to fine4. It
also makes the system adapt to the user knowledge.
Specifically, as the user demonstrates deeper knowl-
edge of the products by answering the questions cor-
rectly, it makes sense to ask more refined questions
about the products.
5 Simulation Results
To evaluate system performance, ideally one should
ask people to call the system, and manually collect
the performance data. This is very expensive. Al-
ternatively, we develop a simulation method, which
is automatic and thus allow fast evaluation of the
system during development5. In fact, many design
choices in Section 4 are inspired by the simulation.
5.1 Simulation Model
Figure 2 illustrates the general framework for the
simulation. The process is very similar to that in
Figure 1 except that the human user and the speech
4While the baseline dialog manager achieves the similar be-
havior by manually enforcing the order of questions, the sys-
tem here automatically discovers the order of questions and the
question set is much more richer than that in the baseline.
5However, we agree that simulation is not without its limi-
tations and the results may not precisely reflect real scenarios.
109
  
Yes 
Begin 
Information Retrieval 
Dialog Manager 
? Baseline 
? POMDP 
End 
Initial Query 
List of Products 
Corrupted User Action 
Simulated User 
? Intended product  
? User knowledge model 
Simulated  
Speech Recognizer 
User Action 
Found 
product? No  
Play Rating 
Question 
Figure 2: Flow Chart in Simulation
recognizer are replaced with a simulated compo-
nent, and that the simulated user has access to a user
knowledge model. In particular, we generate the
user action and its corrupted version using random
number generators by following the models defined
in Equations 3 and 2, respectively. We use a fixed
value (e.g., 0.9) for C in Equation 2.
Clearly, our goal here is not to evaluate the good-
ness of the user knowledge model or the speech rec-
ognizer. Instead, we want to see how the probabilis-
tic dialog manger (i.e., POMDP) performs compared
with the deterministic baseline dialog manager, and
to see whether the richer attribute information helps
to reduce the dialog interaction time.
5.2 Data Resources
In the system, we use three data resources: a prod-
uct database, a review database, and a query-click
database. The product database contains detailed in-
formation for 0.2 million electronics and computer
related products. The review database is used for
learning the user knowledge model. The query-
click database contains 2289 pairs in the format (text
query, product clicked). One example pair is (Canon
Powershot A700, Canon Powershot A700 6.2MP
digital camera). We divide it into a development set
(1308 pairs) and a test set (981 pairs).
5.3 Results on Information Retrieval
For each initial query, the information retrieval
(IR) engine returns a list of top-ranked products.
Whether the intended product is in the returned list
depends on the size of the list. If the intended prod-
uct is in the list, the IR successfully recalled the
product. Table 3 shows the correlation between the
recall rate and the size of the returned list. Clearly,
the larger the list size is, the larger the recall rate is.
One may notice that the IR recall rate is low. This
is because the query-click data set is very noisy, that
is, the clicked product may be nothing to do with
the query. For example, (msn shopping, Handspring
Treo 270) is one of the pairs in our data set.
List Size Recall Rate (%)
50 38.36
100 41.46
150 43.5
Table 3: Information Retrieval Recall Rates on Test set
5.4 Dialog System Configuration and Tuning
As mentioned in Section 4, several parameters in the
system are configurable and tunable. Specifically,
we set the max number of options in a question as
5, and the threshold for ?not related? option as zero.
We use the development set to tune the following pa-
rameters: the threshold of the belief probability be-
low which the product is pruned, and the thresholds
above which the most probable product is played.
The parameters are tuned in a way such that no dia-
log error is made on the development set.
5.5 Results on Error Handling
Even the IR succeeds, the dialog system may not
find the intended product successfully. In particu-
lar, the baseline system does not have error handling
capability. Whenever the system makes a speech
recognition error or the user mistakenly answers a
question, the dialog system fails (either plays the rat-
ing for a wrong product or fails to find any product).
On the contrary, our POMDP framework has error
handling functionality due to its probabilistic na-
ture. Table 5 compares the dialog error rate between
the baseline and the POMDP systems. Clearly,
the POMDP system performs much better to han-
dle errors. Note that the POMDP system does not
eliminate dialog failures on the test set because the
thresholds are not perfect for the test set6. This is
due to two reasons: the system may prune the in-
tended product (reason-1), and the system may play
the rating for a wrong product (reason-2).
6Note that the POMDP system does not have dialog failures
on the development set as we tune the system in this way.
110
System Size Average MaxStages Characters Words Stages Characters Words
Baseline
50 2.44 524.0 82.3 11 2927 546
100 3.37 765.4 120.4 25 7762 1369
150 3.90 906.4 143.0 30 9345 1668
POMDP
50 1.57 342.8 54.3 4 2659 466
100 2.36 487.9 76.6 18 3575 597
150 2.59 541.3 85.0 19 4898 767
Table 4: Interaction Time Results on Test Set
Size Baseline POMDP (%)(%) Total Reason-1 Reason-2
50 13.8 8.2 4.2 4.0
100 17.7 2.7 1.2 1.5
150 19.3 4.7 0.7 4.0
Table 5: Dialog Failure Rate on Test Set
5.6 Results on Interaction Time
It is quite difficult to measure the exact interaction
time, so instead we measure it through the number of
stages/characters/words required during the dialog
process. Clearly, the number of characters is the one
that matches most closely to the true time. Table 4
reports the average and maximum numbers. In gen-
eral, the POMDP system performs much better than
the baseline system. One may notice the difference
in the number of stages between the baseline and
the POMDP systems is not as significant as in the
number of characters. This is because the POMDP
system is able to exploit very short questions while
the baseline system mainly uses the product name
question, which is normally very long. The long
question on product name also imposes difficulty in
speech synthesis, user input, and speech recognition,
though this is not reflected in the simulation.
6 Conclusions
In this paper, we have applied the POMDP frame-
work into Voice-Rate, a system through which a
user can call to get product information (e.g., price,
rating, review, etc.). We have proposed a novel
method to learn a user knowledge model from a re-
view database. Compared with a deterministic base-
line system (Zweig et al, 2007), the POMDP system
is probabilistic and is able to handle speech recogni-
tion errors and user mistakes, in which case the de-
terministic baseline system is doomed to fail. More-
over, the POMDP system exploits richer product in-
formation to reduce the interaction time required to
complete a dialog. We have developed a simulation
model, and shown that the POMDP system improves
the baseline system significantly in terms of both di-
alog failure rate and dialog interaction time. We also
implement our POMDP system into a speech demo
and plan to carry out tests through humans.
Acknowledgement
This work was conducted during the first author?s
internship at Microsoft Research; thanks to Dan Bo-
hus, Ghinwa Choueiter, Yun-Cheng Ju, Xiao Li,
Milind Mahajan, Tim Paek, Yeyi Wang, and Dong
Yu for helpful discussions.
References
K. Murphy. 2000. A survey of POMDP solution tech-
niques. Technical Report, U. C. Berkeley.
T. Paek and D. Chickering. 2005. The Markov assump-
tion in spoken dialogue management. In Proc of SIG-
dial 2005.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog
management for robots. In Proc of ACL 2000.
M. Spaan and N. Vlassis. 2005. Perseus: randomized
point-based value iteration for POMDPs. Journal of
Artificial Intelligence Research, 24:195-220.
J. Williams. 2007. Applying POMDPs to Dialog
Systems in the Troubleshooting Domain. In Proc
HLT/NAACL Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technology.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language 21(2): 231-
422.
G. Zweig, P. Nguyen, Y.C. Ju, Y.Y. Wang, D. Yu, and
A. Acero. 2007. The Voice-Rate Dialog System for
Consumer Ratings. In Proc of Interspeech 2007.
111
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212?1222, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Polarity Inducing Latent Semantic Analysis
Wen-tau Yih Geoffrey Zweig John C. Platt
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{scottyih,gzweig,jplatt}@microsoft.com
Abstract
Existing vector space models typically map
synonyms and antonyms to similar word vec-
tors, and thus fail to represent antonymy. We
introduce a new vector space representation
where antonyms lie on opposite sides of a
sphere: in the word vector space, synonyms
have cosine similarities close to one, while
antonyms are close to minus one.
We derive this representation with the aid of a
thesaurus and latent semantic analysis (LSA).
Each entry in the thesaurus ? a word sense
along with its synonyms and antonyms ? is
treated as a ?document,? and the resulting doc-
ument collection is subjected to LSA. The key
contribution of this work is to show how to as-
sign signs to the entries in the co-occurrence
matrix on which LSA operates, so as to induce
a subspace with the desired property.
We evaluate this procedure with the Grad-
uate Record Examination questions of (Mo-
hammed et al 2008) and find that the method
improves on the results of that study. Further
improvements result from refining the sub-
space representation with discriminative train-
ing, and augmenting the training data with
general newspaper text. Altogether, we im-
prove on the best previous results by 11 points
absolute in F measure.
1 Introduction
Vector space representations have proven useful
across a wide variety of text processing applications
ranging from document clustering to search rele-
vance measurement. In these applications, text is
represented as a vector in a multi-dimensional con-
tinuous space, and a similarity metric such as co-
sine similarity can be used to measure the related-
ness of different items. Vector space representations
have been used both at the document and word lev-
els. At the document level, they are effective for
applications including information retrieval (Salton
and McGill, 1983; Deerwester et al 1990), docu-
ment clustering (Deerwester et al 1990; Xu et al
2003), search relevance measurement (Baeza-Yates
and Ribiero-Neto, 1999) and cross-lingual docu-
ment retrieval (Platt et al 2010). At the word level,
vector representations have been used to measure
word similarity (Deerwester et al 1990; Turney and
Littman, 2005; Turney, 2006; Turney, 2001; Lin,
1998; Agirre et al 2009; Reisinger and Mooney,
2010) and for language modeling (Bellegarda, 2000;
Coccaro and Jurafsky, 1998). While quite success-
ful, these applications have typically been consistent
with a very general notion of similarity in which
basic association is measured, and finer shades of
meaning need not be distinguished. For example,
latent semantic analysis might assign a high degree
of similarity to opposites as well as synonyms (Lan-
dauer and Laham, 1998; Landauer, 2002).
Independent of vector-space representations, a
number of authors have focused on identifying dif-
ferent kinds of relatedness. At the simplest level,
we may wish to distinguish between synonyms and
antonyms, which can be further differentiated. For
example, in synonymy, we may wish to distinguish
hyponyms and hypernyms. Moreover, Cruse (1986)
notes that numerous kinds of antonymy are possible,
for example antipodal pairs like ?top-bottom? or
1212
gradable opposites like ?light-heavy.? Work in this
area includes (Turney, 2001; Lin et al 2003; Tur-
ney and Littman, 2005; Turney, 2006; Curran and
Moens, 2002; van der Plas and Tiedemann, 2006;
Mohammed et al 2008; Mohammed et al 2011).
Despite the existence of a large amount of related
work in the literature, distinguishing synonyms and
antonyms is still considered as a difficult open prob-
lem in general (Poon and Domingos, 2009).
In this paper, we fuse these two strands of re-
search in an attempt to develop a vector space rep-
resentation in which the synonymy and antonymy
are naturally differentiated. We follow Schwab et
al. (2002) in requiring a representation in which
two lexical items in an antonymy relation should lie
at opposite ends of an axis. However, in contrast
to the logical axes used previously, we desire that
antonyms should lie at the opposite ends of a sphere
lying in a continuous and automatically induced vec-
tor space. To generate this vector space, we present
a novel method for assigning both negative and pos-
itive values to the TF-IDF weights used in latent se-
mantic analysis.
To determine these signed values, we exploit the
information present in a thesaurus. The result is a
vector space representation in which synonyms clus-
ter together, and the opposites of a word tend to clus-
ter together at the opposite end of a sphere.
This representation provides several advantages
over the raw thesaurus. First, by finding the items
most and least similar to a word, we are able to dis-
cover new synonyms and antonyms. Second, as dis-
cussed in Section 5, the representation provides a
natural starting point for gradient-descent based op-
timization. Thirdly, as we discuss in Section 6, it is
straightforward to embed new words into the derived
subspace by using information from a large unsuper-
vised text corpus such as Wikipedia.
The remainder of this paper is organized as fol-
lows. Section 2 describes previous work. Section 3
presents the classical LSA approach and analyzes
some of its limitations. In Section 4 we present our
polarity inducing extension to LSA. Section 5 fur-
ther extends the approach by optimizing the vector
space representation with supervised discriminative
training. Section 6 describes the proposed method of
embedding new words in the thesaurus-derived sub-
space. The experimental results of Section 7 indi-
cate that the proposed method outperforms previous
approaches on a GRE test of closest-opposites (Mo-
hammed et al 2008). Finally, Section 8 concludes
the paper.
2 Related Work
The detection of antonymy has been studied in a
number of previous papers. Mohammed et al(2008)
approach the problem by combining information
from a published thesaurus with corpus statistics de-
rived from the Google n-gram corpus (Brants and
Franz, 2006). Their method consists of two main
steps: first, detecting contrasting word categories
(e.g. ?WORK? vs. ?ACTIVITY FOR FUN?) and
then determining the degree of antonymy. Cate-
gories are defined by a thesaurus; contrasting cat-
egories are found by using affix rules (e.g., un- &
dis-) and WordNet antonymy links. Words belong-
ing to contrasting categories are treated as antonyms
and the degree of contrast is determined by distri-
butional similarity. Mohammed et al(2008) also
provides a publicly available dataset for detection of
antonymy, which we have adopted. This work has
been extended in (Mohammed et al 2011) to in-
clude a study of antonymy based on crowd-sourcing
experiments.
Turney (2008) proposes a unified approach to
handling analogies, synonyms, antonyms and asso-
ciations by transforming the last three cases into
cases of analogy. A supervised learning method
is then used to solve the resulting analogical prob-
lems. This is evaluated on a set of 136 ESL ques-
tions. Lin et al(2003) builds on (Lin, 1998) and
identifies antonyms as semantically related words
which also happen to be found together in a database
in pre-identified phrases indicating opposition. Lin
et al(2003) further note that whereas synonyms
will tend to translate to the same word in another
language, antonyms will not. This observation is
used to select antonyms from amongst distribution-
ally similar words. Antonymy is used in (de Si-
mone and Kazakov, 2005) for document clustering
and (Harabagiu et al 2006) to find contradiction.
The automatic detection of synonyms has been
more extensively studied. Lin (1998) presents
a thorough comparison of word-similarity metrics
based on distributional similarity, where this is de-
1213
termined from co-occurrence statistics in depen-
dency triples extracted by parsing a large dataset.
Related studies are described in (Curran and Moens,
2002; van der Plas and Bouma, 2005). Later, van
der Plas and Tiedemann (2006) extend the use of
multilingual data present in Lin et al(2003) by mea-
suring distributional similarity based on the contexts
that a word occurs in once translated into a new lan-
guage. This is used to improve the precision/recall
characteristics on synonym pairs. Structured infor-
mation can be important in determining relatedness,
and thesauri and Wikipedia links have been studied
in (Milne and Witten, 2008; Jarmasz and Szpakow-
icz, 2003). Combinations of approaches are studied
in (Turney et al 2003).
Vector-space models and latent semantic analysis
in particular have a long history of use in synonym
detection, which in fact was suggested in some of
the earliest LSA papers. Deerwester et al(1990)
defines a metric for measuring word similarity based
on LSA, and it has been used in (Landauer and Du-
mais, 1997; Landauer et al 1998) to answer word
similarity questions derived from the Test of English
as a Foreign Language (TOEFL). Turney (2001)
proposes the use of point-wise mutual information in
conjunction with LSA, and again presents results on
synonym questions derived from the TOEFL. Vari-
ants of vector space models are further analyzed
in (Turney and Littman, 2005; Turney, 2006; Tur-
ney and Pantel, 2010).
3 Latent Semantic Analysis
Latent Semantic Analysis (Deerwester et al 1990)
is a widely used method for representing words and
documents in a low dimensional vector space. The
method is based on applying singular value decom-
position (SVD) to a matrix W which indicates the
occurrence of words in documents. To perform
LSA, one proceeds as follows. The input is a col-
lection of d documents which are expressed in terms
of words from a vocabulary of size n. These docu-
ments may be actual documents such as newspaper
articles, or simply notional documents such as sen-
tences, or any other collection in which words are
grouped together. Next, a d?n document-term ma-
trix W is formed1. At its simplest form, the ijth
entry contains the number of times word j has oc-
curred in document i ? its term frequency or TF
value. More conventionally, the entry is weighted
by some notion of the importance of word j, for ex-
ample the negative logarithm of the fraction of doc-
uments that contain it, resulting in a TF-IDF weight-
ing (Salton et al 1975). The similarity between two
documents can be computed using the cosine simi-
larity of their corresponding row vectors:
sim(x,y) =
x ? y
? x ?? y ?
Similarly, the cosine similarity of two column vec-
tors can be used to judge the similarity of the corre-
sponding words. Finally, to obtain a subspace repre-
sentation of dimension k, W is decomposed as
W ? USV T
where U is d ? k, V T is k ? n, and S is a k ? k
diagonal matrix. In applications, k  n and k  d;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the columns
of SV T ? which now represent the words ? behave
similarly to the original columns of W , in the sense
that the cosine similarity between two columns in
SV T approximates the cosine similarity between the
corresponding columns in W . This follows from
the observation that W TW = V S2V T , and the fact
that the ijth entry of W TW is the dot product of
the ith and jth columns (words) in W . We will
use this observation subsequently in the derivation
of polarity-inducing LSA. For efficiency, we nor-
malize the columns of SV T to unit length, allow-
ing the cosine similarity between two words to be
computed with a single dot-product; this also has the
property of mapping each word to a point on a multi-
dimensional sphere.
A second important property of LSA is that in the
word representations which result can by viewed as
the result of applying a projection matrix U to the
original vectors as:
UTW = SV T
1(Bellegarda, 2000) constructs the transpose of this, but we
have found it convenient in data processing for documents to
represent rows.
1214
In Section 5, we will viewU simply as a d?k matrix
learned through gradient descent so as to optimize
an objective function.
3.1 Limitation of LSA
Word similarity as determined by LSA assigns high
values to words which tend to co-occur in doc-
uments. However, as noted by (Landauer and
Laham, 1998; Landauer, 2002), there is no no-
tion of antonymy; words with low or negative co-
sine scores are simply unrelated. In comparison,
words with high cosine similarity scores are typi-
cally semantically related, which includes both syn-
onyms and antonyms, as contrasting words often co-
occur (Murphy and Andrew, 1993; Mohammed et
al., 2008). To illustrate this, we have performed
SVD with the aid of the Encarta thesaurus developed
by Bloomsbury Publishing Plc. This thesaurus con-
tains approximately 47k word senses and a vocab-
ulary of 50k words and phrases. Each ?document?
is taken to be the thesaurus entry for a word-sense,
including synonyms and antonyms. For example,
the word ?admirable? induces a document consist-
ing of {admirable, estimable, commendable, vener-
able, good, splendid, worthy, marvelous, excellent,
unworthy}. Note that the last word in this set is its
antonym. Performing SVD on this set of thesaurus
derived ?meaning-documents? results in a subspace
representation for each word. This form of LSA is
similar to the use of Wikipedia in (Gabrilovich and
Markovitch, 2007).
Table 1 shows some words, their original the-
saurus documents, and the most and least similar
words in the LSA subspace. Several properties are
apparent:
? The vector-space representation of words is
able to identify related words that are not ex-
plicitly present in the original thesaurus. For
example, ?meritorious? for ?admirable? ? ar-
guably better than any of the words given in the
thesaurus itself.
? Similarity is based on co-occurrence, so the
co-occurrence of antonyms in the thesaurus-
derived documents induces their presence as
LSA-similar words. For example, ?con-
temptible? is identified as similar to ?ad-
mirable.? In the case of ?mourning,? opposites
acrimony rancor goodwill affection
acrimony 1 1 1 1
affection 1 1 1 1
Table 2: The W matrix for two thesaurus entries in
its original form. Rows represent documents; columns
words.
acrimony rancor goodwill affection
acrimony 1 1 -1 -1
affection -1 -1 1 1
Table 3: The W matrix for two thesaurus entries in its
polarity-inducing form.
such as ?joy? and ?elation? actually dominate
the list of LSA-similar words.
? The LSA-least-similar words have no relation-
ship at all to the word they are least-similar to.
For example, the least-similar word to ?consid-
ered? is ?ready-made-meal.?
In the next section, we will present a method for
inducing polarity in LSA subspaces, where opposite
words will tend to have negative cosine similarities,
analogous to the positive similarities of synonyms.
Thus, the least-similar words to a given word will be
its opposites.
4 Polarity Inducing LSA
We modify LSA so that we may exploit a thesaurus
to embed meaningful axes in the induced subspace
representation. Words with opposite meaning will
lie at opposite positions on a sphere. Recall that the
cosine similarity between word-vectors in the orig-
inal matrix W are preserved in the subspace repre-
sentation of words. Thus, if we construct the original
matrix so that the columns representing antonyms
will tend to have negative cosine similarities while
columns representing synonyms will tend to have
positive similarities, we will achieve the desired be-
havior.
This can be achieved by negating the TF-IDF en-
tries for the antonyms of a word when constructing
W from the thesaurus, which is illustrated in Ta-
bles 2 and 3. The two rows in these tables corre-
spond to thesaurus entries for the sense-categories
1215
Word Thesaurus Entry LSA Most-Similar Words LSA Least-Similar Words
admirable estimable, commendable,
venerable, good, splen-
did, worthy, marvelous,
excellent, unworthy
commendable, creditable,
laudable, praiseworthy,
worthy, meritorious,
scurvy, contemptible,
despicable, estimable
easy-on-the-eye, peace-
keeper, peace-lover,
conscientious-objector,
uninviting, dishy, dessert,
pudding, seductive
considered careful, measured, well-
thought-out, painstaking,
rash
calculated, premeditated,
planned, tactical, strate-
gic, thought-through, in-
tentional, fortuitous, pur-
poseful, unpremeditated
ready-made-meal, ready-
meal, disposed-to, apt-to,
wild-animals, big-game,
game-birds, game-fish,
rugger, rugby
mourning grief, bereavement, sor-
row, sadness, lamenta-
tion, woe, grieving, exul-
tation
sorrowfulness, anguish,
exultation, rejoicing, ju-
bilation, glee, heartache,
travail, joy, elation
muckiness, turn-the-
corner, impassibility,
filminess, pellucidity,
limpidity, sheerness
Table 1: LSA on a thesaurus. Thesaurus entries include antonyms in italics.
?acrimony,? and ?affection.? The thesaurus entries
induce two ?documents? containing the words and
their synonyms and antonyms. The complete set of
words is acrimony, rancor, goodwill, affection. For
simplicity, we assume that all TF-IDF weights are
1. In the original LSA formulation, we have the rep-
resentation of Table 2. ?Rancor? is listed as a syn-
onym of ?acrimony,? which has ?goodwill? and ?af-
fection? as its antonyms. This results in the first row.
Note that the cosine similarity between every pair of
words (columns) is 1.
Table 3 shows the polarity-inducing representa-
tion. Here, the cosine similarity between synony-
mous words (columns) is 1, and the cosine similarity
between antonymous words is -1. Since LSA tends
to preserve cosine similarities between words, in the
resulting subspace we may expect to find meaning-
ful axes, where opposite senses map to opposite ex-
tremes. We refer to this as polarity-inducing LSA or
PILSA.
In Table 4, we show the PILSA-similar and
PILSA-least-similar words for the same words as in
Table 1. We see now that words which are least
similar in the sense of having the lowest cosine-
similarity are indeed opposites. In this table gen-
erally the most similar words have similarities in the
range of 0.7 to 1.0 and the least similar in the range
of -0.7 to -1.0.
5 Discriminative Training
Although the cosine similarity of LSA-derived word
vectors are generally very effective in applications
such as judging the relevance of words or docu-
ments, or detecting antonyms as in our construction,
the process of singular value decomposition in LSA
does not explicitly try to achieve such goals. In this
section, we see that when supervised training data is
available, the projection matrix of LSA can be en-
hanced through a discriminative training technique
explicitly designed to create a representation suited
to a specific task.
Because LSA is closely related to principle com-
ponent analysis (PCA), extensions of PCA such as
canonical correlation analysis (CCA) and oriented
principle component analysis (OPCA) can leverage
the labeled data and produce the projection matrix
through general eigen-decomposition (Platt et al
2010). Along this line of work, Yih et al(2011)
proposed a Siamese neural network approach called
S2Net, which tunes the projection matrix directly
through gradient descent, and has shown to outper-
form other methods in several tasks. Below we de-
scribe briefly this technique and explain how we
adopt it for the task of antonym detection.
The goal of S2Net is to learn a concept vector
representation of the original sparse term vectors.
Although such transformation can be non-linear in
general, its current design chooses the model form
to be a linear projection matrix, which is identical to
1216
Word PILSA-Similar Words PILSA-Least-Similar Words
admirable commendable, creditable, laudable,
praiseworthy, worthy, meritorious, es-
timable, deserving, tiptop, valued
scurvy, contemptible, despicable,
lamentable, shameful, reprehensible,
unworthy, disgraceful, discreditable,
undeserving
considered calculated, premeditated, planned, tac-
tical, strategic, thought-through, inten-
tional, purposeful, intended, psycho-
logical
fortuitous, unpremeditated, unconsid-
ered, off-your-own-bat, unintended,
undirected, objectiveless, hit-and-miss,
unforced, involuntary
mourning sorrowful, doleful, sad, miserable,
wistful, pitiful, wailing, sobbing,
heavy-hearted, forlorn
smiley, happy, blissful, wooden, mirth-
ful, joyful, deadpan, fulfilled, straight-
faced, content
Table 4: PILSA on a thesaurus. Thesaurus entries are as in Table 1.
that of LSA, PCA, OPCA or CCA. Given a d-by-1
input vector f , the model of S2Net is a d-by-k ma-
trix A = [aij ]d?k, which maps f to a k-by-1 output
vector g = AT f . The fact that the transformation
can be viewed as a two-layer neural network leads
to the method?s name.
What differentiates S2Net from other approaches
is its loss function and optimization process. In
the ?parallel text? setting, the labeled data con-
sists of pairs of similar text objects such as doc-
uments. The objective of the training process is
to assign higher cosine similarities to these pairs
compared to others. More specifically, suppose the
training set consists of m pairs of raw input vectors
{(fp1 , fq1), (fp2 , fq2), ? ? ? , (fpm , fqm)}. Given a pro-
jection matrix A, the similarity score of any pair of
objects is simA(fpi , fqj ) = cosine(A
T fpi ,A
T fqj ).
Let ?ij = simA(fpi , fqi) ? simA(fpi , fqj ) be the
difference of the similarity scores of (fpi , fqi) and
(fpi , fqj ). The learning procedure tries to increase
?ij by using the following logistic loss:
L(?ij ;A) = log(1 + exp(???ij)),
where ? is a scaling factor that adjusts the loss func-
tion2. The loss of the whole training set is thus:
1
m(m? 1)
?
1?i,j?m,i 6=j
L(?ij ;A)
Parameter learning (i.e., tuning A) can be done
2As suggested in (Yih et al 2011), ? is set to 10 in our
experiments.
by standard gradient-based methods, such as L-
BFGS (Nocedal and Wright, 2006).
The original setting of S2Net can be directly ap-
plied to finding synonymous words, where the train-
ing data consists of pairs of vectors representing
two synonyms. It is also easy to modify the loss
function to apply it to the antonym detection prob-
lem. We first sample pairs of antonyms from the
thesaurus to create the training data. The raw input
vector f of a selected word is its corresponding col-
umn vector of the document-term matrix W (Sec-
tion 3) after inducing polarity (Section 4). When
each pair of vectors in the training data represents
two antonyms, we can redefine ?ij by flipping the
sign: ?ij = simA(fpi , fqj ) ? simA(fpi , fqi), and
leave others unchanged. As the loss function encour-
ages ?ij to be larger, an antonym pair will tend to
have a smaller cosine similarity than other pairs. Be-
cause S2Net uses a gradient descent technique and a
non-convex objective function, it is sensitive to ini-
tialization, and we have found that the PILSA pro-
jection matrix U (Section 3) provides an excellent
starting point. As illustrated in Section 7, learning
the word vectors with S2Net produces a significant
improvement over PILSA alone.
6 Extending PILSA to Out-of-thesaurus
Words
While PILSA is effective at representing synonym
and antonym information, in its pure form, it is lim-
ited to the vocabulary of the thesaurus. In order to
extend PILSA to operate on out-of-thesaurus words,
1217
we employ a two-stage strategy. We first conduct
some lexical analysis and try to match an unknown
word to one or more in-thesaurus words in their lem-
matized forms. If no such match can be found,
we then attempt to find semantically related in-
thesaurus words by leveraging co-occurrence statis-
tics from general text data. These two steps are de-
scribed in detail below.
6.1 Matching via Lexical Analysis
When a target word is not included in a thesaurus, it
is quite often that some of its morphological varia-
tions are covered. For example, although the Encarta
thesaurus does not have the word ?corruptibility,?
it does contain other forms like ?corruptible? and
?corruption.? Replacing the out-of-thesaurus target
word with these morphological variations may alter
the part-of-speech but typically does not change the
meaning3.
Given an out-of-thesaurus target word, we first
apply a morphological analyzer for English devel-
oped by Minnen et al(2001), which removes the
inflectional affixes and returns the lemma. If the
lemma still does not exist in the thesaurus, we then
apply Porter?s stemmer (Porter, 1980) and check
whether the target word can match any of the in-
thesaurus words in their stemmed forms. A sim-
ple rule that checks whether removing hyphens from
words can lead to a match and whether the target
word occurs as part of a compound word in the the-
saurus is applied when both morphological analysis
and stemming fail to find a match. When there are
more than one matched words, the centroid of their
PILSA vectors is used to represent the target word.
When there is only one matched word, the matched
word is treated as the target word.
6.2 Leveraging General Text Data
If no words in the thesaurus can be linked to the
target word through the simple lexical analysis pro-
cedure, we try to find matched words by creating
a context vector space model from a large docu-
ment collection, and then mapping from this space
to the PILSA space. We use contexts because of the
distributional hypothesis ? words that occur in the
same contexts tend to have similar meaning (Harris,
3The rules we use based on lexical analysis are moderately
conservative to avoid mistakes like mapping hopeless to hope.
1954). When a word is not in the thesaurus but ap-
pears in the corpus, we predict its PILSA vector rep-
resentation from the context vector space model by
using its k-nearest neighbors which are in the the-
saurus and consistent with each other.
6.2.1 Context Vector Space Model
Given a corpus of documents, we construct the
raw context vectors as follows. For each target word,
we first create a bag of words by collecting all the
terms within a window of [-10,+10] centered at each
occurrence of the target word in the corpus. The
non-identical terms form a term-vector, where each
term is weighted using its TF-IDF value. We then
perform LSA on the context-word matrix. The se-
mantic similarity/relatedness of two words can then
be determined using the cosine similarity of their
corresponding LSA word vectors. In the following
text, we refer to this LSA context vector space model
as the corpus space, in contrast to the PILSA the-
saurus space.
6.2.2 Embedding Out-of-Vocabulary Words
Given the context space model, we may use a
linear regression or a k-nearest neighbors approach
to embed out-of-thesaurus words into the thesaurus-
space representation. However, as near words in the
context space may be synonyms in addition to other
semantically related words (including antonyms),
such approaches can potentially be noisy. For ex-
ample, words like ?hot? and ?cold? may be close
to each other in the context space due to their sim-
ilar usage in text. An affine transform cannot ?tear
space? and map them to opposite poles in the the-
saurus space.
Therefore, we propose a revised k-nearest neigh-
bors approach. Suppose we are interested in an out-
of-thesaurus word w. We first find K-nearest in-
thesaurus neighbors to w in the context space. We
then select a subset of k members of these K words
such that the pairwise similarity of each of the k
members with every other member is positive. The
thesaurus-space centroid of these k items is com-
puted as w?s representation. This procedure has the
property that the k nearby words used to form the
embedding of a non-thesaurus word are selected to
be consistent with each other. In practice, we used
K = 10 and k = 3, which requires only around
1218
1000 pairwise computations even done in a brute-
force way. To provide a concrete example, if we
had the out-of-thesaurus word ?sweltering? with in-
thesaurus neighbors ?hot, cold, burning, scorching,
...? the procedure would return the centroid of ?hot,
burning, scorching? and exclude ?cold.?
7 Experimental Validation
In this section, we present our experimental results
on applying PILSA and its extensions to answering
the closest-opposite GRE questions.
7.1 Data Resources
The primary thesaurus we use is the Encarta The-
saurus developed by Bloomsbury Publishing Plc4.
Our version of this has approximately 47k word
senses and a vocabulary of 50k words, and con-
tains 125,724 pairs of antonyms. To experiment with
the effect of using a different thesaurus, we used
WordNet as an information source. Each synset in
WordNet maps to a row in the document-term ma-
trix; synonyms in a synset are weighted with posi-
tive TFIDF values, and antonyms are weighted neg-
ative TFIDF values. Entries corresponding to other
words in the vocabulary are 0. WordNet provides
significantly greater coverage with approximately
227k synsets involving multiple words, and a vo-
cabulary of about 190k words. However, it is also
much sparser, with 5.3 words per sense on average
as opposed to 10.3 in the thesaurus, and has only
62,821 pairs of antonyms. As general text data for
use in embedding out-of-vocabulary words, we used
a Nov-2010 dump of English Wikipedia, which con-
tains approximately 917M words.
7.2 Development and Test Data
For testing, we use the closest-opposite questions
from GRE tests provided by (Mohammed et al
2008). Each question contains a target word and
five choices, and asks which of the choice words has
the most opposite meaning to the target word. Two
datasets are made publicly available by Mohammad
et al(2008): the development set, which consists of
162 questions, and the test set, which has 950 ques-
tions5. We considered making our own, more exten-
4http://www.bloomsbury.com/
5http://www.umiacs.umd.edu/?saif/WebDocs/LC-
data/{devset,testset}.txt
Dimensions Bloomsbury Prec. WordNet Prec.
50 0.778 0.475
100 0.850 0.563
200 0.856 0.569
300 0.863 0.625
400 0.843 0.625
500 0.843 0.613
750 0.830 0.613
1000 0.837 0.544
2000 0.784 0.519
3000 0.778 0.494
Table 5: The performance of PILSA vs. the number of di-
mensions when applied to the closest-opposite questions
from the GRE development set. Out of the 162 ques-
tions, using the Bloomsbury thesaurus data we are able
to answer 153 of them. Using 300 dimensions gives the
best precision (132/153 = 0.863). This dimension set-
ting is also optimal when using the WordNet data, which
answers 100 questions correctly out of the 160 attempts
(100/160 = 0.625).
sive, test ? for example one which would require the
use of sentence context to choose between related
yet distributionally different antonyms (e.g. ?little,
small? as antonyms of ?big?) but chose to stick to a
previously used benchmark. This allows the direct
comparison with previously reported methods.
Some of these questions contain very rarely used
target or choice words, which are not included in
the thesaurus vocabulary. In order to provide a fair
comparison to existing methods, we do not try to
randomly answer these questions. Instead, when the
target word is out of vocabulary, we skip the whole
question. When the target word is in vocabulary but
one or more choices are unknown words, we ignore
those unknown words and pick the word with the
lowest cosine similarity from the rest as the answer.
The results of our methods are reported in precision
(the number of questions answered correctly divided
by the number of questions attempted), recall (the
number of questions answered correctly divided by
the number of all questions) and F1 (the harmonic
mean of precision and recall)6. We now turn to an
in-depth evaluation.
6Precision/recall/F1 were used in (Mohammed et al 2008)
as when their system ?did not find any evidence of antonymy
between the target and any of its alternatives, then it refrained
from attempting that question.? We adopt this convention to
provide a fair comparison to their system.
1219
Dev. Set Test Set
Prec Rec F1 Prec Rec F1
WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet signed-TFIDF w/o LSA 0.41 0.41 0.41 0.43 0.42 0.43
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
Bloomsbury lookup 0.65 0.61 0.63 0.61 0.56 0.59
Bloomsbury signed TFIDF w/o LSA 0.68 0.64 0.66 0.63 0.57 0.60
Bloomsbury PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Bloomsbury PILSA + S2Net 0.89 0.84 0.86 0.84 0.77 0.80
Bloomsbury PILSA + S2Net + Embedding 0.88 0.87 0.87 0.81 0.80 0.81
(Mohammed et al 2008) 0.76 0.66 0.70 0.76 0.64 0.70
Table 6: The overall results. PILSA performs LSA on the signed TF-IDF vectors.
7.3 Basic PILSA
When applying PILSA, we need to determine the
number of dimensions in the projected space. Eval-
uated on the GRE development set, Table 5 shows
the precision of PILSA, using two different training
datasets, Bloomsbury and WordNet, at different di-
mensions.
The Bloomsbury-based system is able to answer
153 questions, and the best dimension setting is
300, which answers 132 questions correctly and thus
archives 0.863 in precision. In contrast, the larger
vocabulary in WordNet helps the system answer 160
questions but the quality is not as good. We find
dimensions 300 and 400 are equally good, where
both answer 100 questions correctly (0.625 in pre-
cision)7. Because a lower number of dimensions
is preferred for saving storage space and computing
time, we choose 300 as the number of dimensions in
PILSA.
We now compare the proposed methods. All re-
sults are summarized in Table 6. When evaluated on
the GRE test set, the Bloomsbury thesaurus-based
methods (Lines 4?7) attempted 865 questions. The
precision, recall and F1 of the Bloomsbury-based
PILSA model (Line 6) are 0.81, 0.74 and 0.77,
which are all better than the best reported method
in (Mohammed et al 2008)8. In contrast, the
WordNet-based methods (Lines 1?3) attempted 936
7Note that the number of questions attempted is not a func-
tion of the number of dimensions.
8We take a conservative approach and assume that skipped
questions are answered incorrectly. The difference is statisti-
cally significant at 99% confidence level using a binomial test.
questions. However, consistent with what we ob-
served on the development set, the WordNet-based
model is inferior. Its precision, recall and F1 on
the test set are 0.60, 0.60 and 0.60 (Line 3). Al-
though the quality of the data source plays an im-
portant role, we need to emphasize that performing
LSA using our polarity inducing construction is in
fact a critical step in enhancing the model perfor-
mance. For example, directly using the antonym sets
in the Bloomsbury thesaurus gives 0.59 in F1 (Line
4), while using cosine similarity on the signed vec-
tors prior to LSA only reaches 0.60 in F1 (Line 5).
7.4 Improving Precision with Discriminative
Training
Building on the success of the unsupervised PILSA
model, we refine the projection matrix. As described
in Section 5, we take the PILSA projection matrix
as the initial model in S2Net and train the model
using 20,517 pairs of antonyms sampled from the
Bloomsbury thesaurus. A separate sample of 5,000
antonym pairs is used as the validation set for hyper-
parameter tuning in regularization. Encouragingly,
we found that the already strong results of PILSA
can indeed be improved, which gives 3 more points
in both precision (0.84), recall (0.77) and F1 (0.80).
7.5 Improving Recall with Unsupervised Data
We next evaluate our approach of extending the
word coverage with the help of an external text cor-
pus, as well as the lexical analysis procedure. Using
the Bloomsbury PILSA-S2Net thesaurus space and
the Wikipedia corpus space, our method increases
1220
recall by 3 points on the test set. Compared to the in-
vocabulary only setting, it attempted 75 more ques-
tions (865? 940) and had 33 of them correctly an-
swered.
While the accuracy on these questions is much
higher than random, the fact that it is substantially
below the precision of the original indicates some
room for improvement. We notice that the out-of-
thesaurus words are either offensive words excluded
in the thesaurus (e.g., moronic) or some very rarely
used words (e.g., froward). When the lexical analy-
sis procedure fails to match the target word to some
in-thesaurus words, the context vector embedding
approach solves the former case, but has difficulty
in handling the latter. The main reason is that such
words occur very infrequently in a general corpus,
which result in significant uncertainty in their se-
mantic vectors. Other than using a much larger
corpus, approaches that leverage character n-grams
may help. We leave this as future work.
8 Conclusion
In this paper we have tackled the problem of find-
ing a vector-space representation of words where,
by construction, synonyms and antonyms are easy
to distinguish. Specifically, we have defined a way
of assigning sign to the entries in the co-occurrence
matrix on which LSA operates, such that synonyms
will tend to have positive cosine similarity, and
antonyms will tend to have negative similarities. To
the best of our knowledge, our method of inducing
polarity to the document-term matrix before apply-
ing LSA is novel and has shown to effectively pre-
serve and generalize the synonymous/antonymous
information in the projected space. With this vector
space representation, we were able to bring to bear
the machinery of discriminative training in order to
further optimize the word representations. Finally,
by using the notion of closeness in this space, we
were able to embed new out-of-vocabulary words
into the space. On a standard test set, the proposed
methods improved the F measure by 11 points abso-
lute over previous results.
Acknowledgments
We thank Susan Dumais for her thoughtful com-
ments, Silviu-Petru Cucerzan for preparing the
Wikipedia data, and Saif Mohammed for sharing the
GRE datasets. We are also grateful to the anony-
mous reviewers for their useful suggestions.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceedings
of HLT-NAACL, pages 19?27.
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999.
Modern Information Retrieval. Addison-Wesley.
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, International Conference
on Spoken Language Processing (ICSLP-98).
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press.
James R. Curran and Marc Moens. 2002. Improvements
in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acqui-
sition - Volume 9, pages 59?66. Association for Com-
putational Linguistics.
Thomas de Simone and Dimitar Kazakov. 2005. Using
wordnet similarity and antonymy relations to aid doc-
ument retrieval. In Recent Advances in Natural Lan-
guage Processing (RANLP).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using wikipedia-based explicit se-
mantic analysis. In AAAI Conference on Artificial In-
telligence (AAAI).
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI Conference on Artificial Intelligence
(AAAI).
Zelig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Mario Jarmasz and Stan Szpakowicz. 2003. Rogets the-
saurus and semantic similarity. In Proceedings of the
International Conference on Recent Advances in Nat-
ural Language Processing (RANLP-2003).
1221
Thomas Landauer and Susan Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
T.K. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In Neural Information Processing
Systems (NIPS).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
T.K. Landauer. 2002. On the computational basis of
learning and cognition: Arguments from lsa. Psychol-
ogy of Learning and Motivation, 41:43?84.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In International Joint Conference on
Artificial Intelligence (IJCAI).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Milne and Ian H. Witten. 2008. An effective low-
cost measure of semantic relatedness obtained from
wikipedia links. In Proceedings of the AAAI 2008
Workshop on Wikipedia and Artificial Intelligence.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of english. Natural Lan-
guage Engineering, 7(3):207?223.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Gregory L. Murphy and Jane M. Andrew. 1993. The
conceptual basis of antonymy and synonymy in adjec-
tives. Journal of Memory and Language, 32(3):1?19.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251?261.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109?117.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw Hill.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
D. Schwab, M. Lafourcade, and V. Prince. 2002.
Antonymy and conceptual vectors. In International
Conference on Computational Linguistics (COLING).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, (37).
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In European Conference on
Machine Learning (ECML).
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Lonneke van der Plas and Gosse Bouma. 2005. Syntac-
tic contexts for finding semantically similar words. In
Proceedings of the Meeting of Computational Linguis-
tics in the Netherlands 2004 (CLIN).
Lonneke van der Plas and Jo?rg Tiedemann. 2006. Find-
ing synonyms using automatic word alignment and
measures of distributional similarity. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, COLING-ACL ?06, pages 866?873. Associa-
tion for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267?273, New York, NY,
USA. ACM.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteen Conference on Computational Nat-
ural Language Learning (CoNLL), pages 247?256,
Portland, Oregon, USA.
1222
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044?1054,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Language and Translation Modeling with Recurrent Neural Networks
Michael Auli, Michel Galley, Chris Quirk, Geoffrey Zweig
Microsoft Research
Redmond, WA, USA
{michael.auli,mgalley,chrisq,gzweig}@microsoft.com
Abstract
We present a joint language and transla-
tion model based on a recurrent neural net-
work which predicts target words based on
an unbounded history of both source and tar-
get words. The weaker independence as-
sumptions of this model result in a vastly
larger search space compared to related feed-
forward-based language or translation models.
We tackle this issue with a new lattice rescor-
ing algorithm and demonstrate its effective-
ness empirically. Our joint model builds on a
well known recurrent neural network language
model (Mikolov, 2012) augmented by a layer
of additional inputs from the source language.
We show competitive accuracy compared to
the traditional channel model features. Our
best results improve the output of a system
trained on WMT 2012 French-English data by
up to 1.5 BLEU, and by 1.1 BLEU on average
across several test sets.
1 Introduction
Recently, several feed-forward neural network-
based language and translation models have
achieved impressive accuracy improvements on sta-
tistical machine translation tasks (Allauzen et al,
2011; Le et al, 2012b; Schwenk et al, 2012). In this
paper we focus on recurrent neural network archi-
tectures, which have recently advanced the state of
the art in language modeling (Mikolov et al, 2010;
Mikolov et al, 2011a; Mikolov, 2012), outperform-
ing multi-layer feed-forward based networks in both
perplexity and word error rate in speech recognition
(Arisoy et al, 2012; Sundermeyer et al, 2013). The
major attraction of recurrent architectures is their
potential to capture long-span dependencies since
predictions are based on an unbounded history of
previous words. This is in contrast to feed-forward
networks as well as conventional n-gram models,
both of which are limited to fixed-length contexts.
Building on the success of recurrent architectures,
we base our joint language and translation model
on an extension of the recurrent neural network lan-
guage model (Mikolov and Zweig, 2012) that intro-
duces a layer of additional inputs (?2).
Most previous work on neural networks for
speech recognition or machine translation used a
rescoring setup based on n-best lists (Arisoy et al,
2012; Mikolov, 2012) for evaluation, thereby side
stepping the algorithmic and engineering challenges
of direct decoder-integration.1 Instead, we exploit
lattices, which offer a much richer representation
of the decoder output, since they compactly encode
an exponential number of translation hypotheses in
polynomial space. In contrast, n-best lists are typi-
cally very redundant, representing only a few com-
binations of top scoring arcs in the lattice. A major
challenge in lattice rescoring with a recurrent neural
network model is the effect of the unbounded history
on search since the usual dynamic programming as-
sumptions which are exploited for efficiency do not
hold up anymore. We apply a novel algorithm to the
task of rescoring with an unbounded language model
and empirically demonstrate its effectiveness (?3).
The algorithm proves robust, leading to signif-
icant improvements with the recurrent neural net-
work language model over a competitive n-gram
baseline across several language pairs. We even ob-
serve consistent gains when pairing the model with a
large n-gram model trained on up to 575 times more
1One notable exception is Le et al (2012a) who rescore reorder-
ing lattices with a feed-forward network-based model.
1044
data, demonstrating that the model provides comple-
mentary information (?4).
Our joint modeling approach is based on adding a
continuous space representation of the foreign sen-
tence as an additional input to the recurrent neu-
ral network language model. With this extension,
the language model can measure the consistency
between the source and target words in a context-
sensitive way. The model effectively combines the
functionality of both the traditional channel and lan-
guage model features. We test the power of this
new model by using it as the only source of tradi-
tional channel information. Overall, we find that the
model achieves accuracy competitive with the older
channel model features and that it can improve over
the gains observed with the recurrent neural network
language model (?5).
2 Model Structure
We base our model on the recurrent neural network
language model of Mikolov et al (2010) which is
factored into an input layer, a hidden layer with re-
current connections, and an output layer (Figure 1).
The input layer encodes the target language word at
time t as a 1-of-N vector et, where |V | is the size
of the vocabulary, and the output layer yt represents
a probability distribution over target words; both of
size |V |. The hidden layer state ht encodes the his-
tory of all words observed in the sequence up to time
step t. This model is extended by an auxiliary input
layer ft which provides complementary information
to the input layer (Mikolov and Zweig, 2012). While
the auxiliary input layer can be used to feed in arbi-
trary additional information, we focus on encodings
of the foreign sentence (?5).
The state of the hidden layer is determined by the
input layer, the auxiliary input layer and the hidden
layer configuration of the previous time step ht?1.
The weights of the connections between the layers
are summarized in a number of matrices: U, F and
W, represent weights from the input layer to the hid-
den layer, from the auxiliary input layer to the hid-
den layer, and from the previous hidden layer to the
current hidden layer, respectively. Matrix V repre-
sents connections between the current hidden layer
and the output layer; G represents direct weights be-
tween the auxiliary input and output layers.
et
ht-1
ft
ht
yt
V
G
F
W
U
D
Figure 1: Structure of the recurrent neural network
model, including the auxiliary input layer ft.
The hidden and output layers are computed via a
series of matrix-vector products and non-linearities:
ht = s(Uet +Wht?1 + Ff t)
yt = g(Vht +Gf t)
where
s(z) =
1
1 + exp {?z}
, g(zm) =
exp {zm}
?
k exp {zk}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram features
over input words (Mikolov et al, 2011a).2 The max-
imum entropy weights are added to the output acti-
vations before computing the softmax.
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the back propaga-
tion through time algorithm, which unrolls the net-
work and then computes error gradients over mul-
tiple time steps (Rumelhart et al, 1986). Af-
ter training, the output layer represents posteriors
p(et+1|ett?n+1,ht, ft); the probabilities of words in
the output vocabulary given the n previous input
words ett?n+1, the hidden layer configuration ht as
well as the auxiliary input layer configuration ft.
2While these features depend on multiple input words, we de-
picted them for simplicity as a connection between the current
input word vector et and the output layer (D).
1045
Na??ve computation of the probability distribution
over the next word is very expensive for large vo-
cabularies. A well established efficiency trick uses
word-classing to create a more efficient two-step
process (Goodman, 2001; Emami and Jelinek, 2005;
Mikolov et al, 2011b) where each word is assigned
a unique class. To compute the probability of a
word, we first compute the probability of its class,
and then multiply it by the probability of the word
conditioned on the class:
p(et+1|e
t
t?n+1,ht, ft) =
p(ci|e
t
t?n+1,ht, ft)? p(et+1|ci, e
t
t?n+1,ht, ft)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + maxi |ci|) where |C| is the number of
classes and |ci| is the number of words in class
ci. The best case complexity O(
?
|V |) requires the
number of classes and words to be evenly balanced,
i.e., each class contains exactly as many words as
there are classes.
3 Lattice Rescoring with an Unbounded
Language Model
We evaluate our joint language and translation
model in a lattice rescoring setup, allowing us to
search over a much larger space of translations than
would be possible with n-best lists. While very
space efficient, lattices also impose restrictions on
the context available to features, a particularly chal-
lenging setting for our model which depends on the
entire prefix of a translation. In the ensuing de-
scription we introduce a new algorithm to efficiently
tackle this issue.
Phrase-based decoders operate by maintaining a
set of states representing competing translations, ei-
ther partial or complete. Each state is scored by a
number of features including the n-gram language
model. The independence assumptions of the fea-
tures determine the amount of context each state
needs to maintain in order for it to be possible to
assign a score to it. For example, a trigram language
model is indifferent to any context other than the
two immediately preceding words. Assuming the
trigram model dominates the Markov assumptions
of all other features, which is typically the case, then
we have to maintain at least two words at each state,
also known as the n-gram context.
1: function RESCORELATTICE(k, V , E, s, T )
2: Q? TOPOLOGICALLY-SORT(V )
3: for all v in V do . Heaps of split-states
4: Hv ? MINHEAP()
5: end for
6: h0 ? ~0 . Initialize start-state
7: Hs.ADD(h0)
8: for all v in Q do . Examine outgoing arcs
9: for ?v, x? in E do
10: for h in Hv do . Extend LM states
11: h? ? SCORERNN(h, phrase(h))
12: parent(h?)? h . Backpointers
13: if Hx.size() ? k? . Beam width
14: Hx.MIN()<score(h?) then
15: Hx.REMOVEMIN()
16: if Hx.size()<k then
17: Hx.ADD(h?)
18: end for
19: end for
20: end for
21: I = MAXHEAP()
22: for all t in T do . Find best final split-state
23: I.MERGE(Ht)
24: end for
25: return I.MAX()
26: end function
Figure 2: Push-forward rescoring with a recurrent neu-
ral network language model given a beam-width for lan-
guage model split-states k, decoder states V , edges E, a
start state s and final states T .
However, a recurrent neural network language
model makes much weaker independence assump-
tions. In fact, the predictions of such a model depend
on all previous words in the sentence, which would
imply a potentially very large context. But storing
all words is an inefficient solution from a dynamic
programming point of view. Fortunately, we do not
need to maintain entire translations as context in the
states: the recurrent model compactly encodes the
entire history of previous words in the hidden layer
configuration hi. It is therefore sufficient to add hi
as context, instead of the entire translation. The lan-
guage model can then simply score any new words
1046
based on hi from the previous state when a new state
is created.
A much larger problem is that items, that were
previously equivalent from a dynamic programming
perspective, may now be different. Standard phrase-
based decoders (Koehn et al, 2007) recombine de-
coder states with the same context into a single
state because they are equivalent to the model fea-
tures; usually recombination retains only the high-
est scoring candidate.3 However, if the context is
large, then the amount of recombination will de-
crease significantly, leading to less variety in the de-
coder beam. This was confirmed in preliminary ex-
periments where we simulated context sizes of up to
100 words but found that accuracy dropped by be-
tween 0.5-1.0 BLEU.
Integrating a long-span language model na??vely
requires to keep context equivalent to the entire left
prefix of the translation, a setting which would per-
mit very little recombination. Instead of using ineffi-
cient long-span contexts, we propose to maintain the
usual n-gram context and to keep a fixed number of
hidden layer configurations k at each decoder state.
This leads to a new split-state dynamic program
which splits each decoder state into at most k new
items, each with a separate hidden layer configura-
tion representing an unbounded history (Figure 2).
This maintains diversity in the explored translation
hypothesis space and preserves high-scoring hidden
layer configurations.
What is the effect of this strategy? To answer
this question we measured translation accuracy for
various settings of k on our lattice rescoring setup
(see ?4 for details). In the same experiment, we
compare lattices to n-best lists in terms of accuracy,
model score and wall time impact.4 The results (Ta-
ble 1 and Figure 3) show that reranking accuracy on
lattices is not significantly better, however, rescor-
ing lattices with k = 1 is much faster than n-best
lists. Similar observations have been made in previ-
ous work on minimum error-rate training (Macherey
3Assuming a max-translation decision rule. In a minimum-risk
setting, we may assign the sum of the scores of all candidates
to the retained item.
4We measured running times on an HP z800 workstation
equipped with 24 GB main memory and two Xeon E5640
CPUs with four cores each, clocked at 2.66 GHz. All experi-
ments were run single-threaded.
BLEU oracle sec/sent
Baseline 28.25 - 0.173
100-best 28.90 37.22 0.470
1000-best 28.99 40.06 3.920
lattice (k = 1) 29.00 43.50 0.093
lattice (k = 10) 29.04 43.50 0.599
lattice (k = 100) 29.03 43.50 4.531
Table 1: Rescoring n-best lists and lattices with various
language model beam widths k. Accuracy is based on
the news2011 French-English task. Timing results are in
addition to the baseline.
Figure 3: BLEU vs. log probabilities of 1-best transla-
tions when rescoring n-best lists and lattices (cf. Table 1).
et al, 2008). The recurrent language model adds an
overhead of about 54% at k = 1 on top of the time
to produce the baseline 1-best output, a consider-
able but not necessarily prohibitive overhead. Larger
values of k return higher probability solutions, but
there is little impact on accuracy: the BLEU score
is nearly identical when retaining up to 100 histories
compared to keeping only the highest scoring.
While surprising at first, we believe that this ef-
fect is due to the high similarity of the translations
represented by the histories in the beam. Each his-
tory represents a different translation but all transla-
tion hypothesis share the same n-gram context, and,
more importantly, they are translations of the same
foreign words, since they have exactly the same cov-
erage vector. These commonalities are likely to re-
sult in similar recurrent histories, which in turn re-
duces the effect of aggressive pruning.
4 Language Model Experiments
Recurrent neural network language models have
previously only been used in n-best rescoring
1047
settings and on small-scale tasks with baseline
language models trained on only 17.5m words
(Mikolov, 2012). We extend this work by experi-
menting on lattices using strong baselines with n-
gram models trained on over one billion words and
by evaluating on a number of language pairs.
4.1 Experimental Setup
Baseline. We experiment with an in-house phrase-
based system similar to Moses (Koehn et al,
2003), scoring translations by a set of common fea-
tures including maximum likelihood estimates of
source given target mappings pMLE(e|f) and vice
versa pMLE(f |e), as well as lexical weighting es-
timates pLW (e|f) and pLW (f |e), word and phrase-
penalties, a linear distortion feature and a lexicalized
reordering feature. Log-linear weights are estimated
with minimum error rate training (Och, 2003).
Evaluation. We use training and test data
from the WMT 2012 campaign and report results
on French-English, German-English and English-
German. Translation models are estimated on 102m
words of parallel data for French-English, 91m
words for German-English and English-German; be-
tween 3.5-5m words are newswire, depending on the
language pair, and the remainder are parliamentary
proceedings. The baseline systems use two 5-gram
modified Kneser-Ney language models; the first is
estimated on the target-side of the parallel data,
while the second is based on a large newswire corpus
released as part of the WMT campaign. For French-
English and German-English we use a language
model based on 1.15bn words, and for English-
German we train a model on 327m words. We eval-
uate on the newswire test sets from 2010-2011 con-
taining between 2034-3003 sentences. Log-linear
weights are estimated on the 2009 data set compris-
ing 2525 sentences. We rescore the lattices produced
by the baseline systems with an aggressive but effec-
tive context beam of k = 1 that did not harm accu-
racy in preliminary experiments (?3).
Neural Network Language Model. The vocab-
ularies of the language models are comprised of
the words in the training set after removing single-
tons. We obtain word-classes using a version of
Brown-Clustering with an additional regularization
term to optimize the runtime of the language model
(Brown et al, 1992; Zweig and Makarychev, 2013).
Direct connections use maximum entropy features
over unigrams, bigrams and trigrams (Mikolov et al,
2011a). We use the standard settings for the model
with the default learning rate ? = 0.1 that decays
exponentially if the validation set entropy does not
increase after each epoch. Back propagation through
time computes error gradients over the past twenty
time steps. Training is stopped after 20 epochs or
when the validation entropy does not decrease over
two epochs. We experiment with varying training
data sizes and randomly draw the data from the same
corpora used for the baseline systems. Throughout,
we use a hidden layer size of 100 which provided a
good trade-off between time and accuracy in initial
experiments.
4.2 Results
Training times for neural networks can be a major
bottleneck. Recurrent architectures are particularly
hard to parallelize due to their inherent dependence
on the previous hidden layer configuration. One
straightforward way to influence training time is to
change the size of the training corpus.
Our results (Table 2, Table 3 and Table 4) show
that even small models trained on only two million
words significantly improve over the 1-best decoder
output (Baseline); this represents only 0.6 percent
of the data available to the n-gram model used by
the baseline. Models of this size can be trained in
only about 3.5 hours. A model trained on 50m words
took 63 hours to train. When paired with an n-gram
model trained on 25 times more data, accuracy im-
proved by up to 0.7 BLEU on French-English.
5 Joint Model Experiments
In the next set of experiments, we turn to the joint
language and translation model, an extension of the
recurrent neural network language model with ad-
ditional inputs for the foreign sentence. We first
introduce two continuous space representations of
the foreign sentence (?5.1). Using these represen-
tations we evaluate the accuracy of the joint model
in the lattice rescoring setup and compare against the
traditional translation channel model features (?5.2).
Next, we establish an upper bound on accuracy for
the joint model via an oracle experiment (?5.3). In-
spired by the results of the oracle experiment we
1048
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 26.6 27.6 28.3 27.5 27.8
+RNNLM (2m) 27.5 28.1 28.6 28.1 28.3
+RNNLM (50m) 27.7 28.2 29.0 28.1 28.5
Table 2: French-English results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 1.15bn words.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 21.2 20.7 19.2 20.6 20.0
+RNNLM (2m) 21.8 20.9 19.4 20.9 20.3
+RNNLM (50m) 22.1 21.1 19.7 21.0 20.5
Table 3: German-English results when rescoring with the recurrent neural network language model.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 15.2 15.6 14.3 15.7 15.1
+RNNLM (2m) 15.7 15.9 14.6 16.0 15.4
+RNNLM (50m) 15.8 15.9 14.7 16.1 15.5
Table 4: English-German results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 327m words.
train a transform between the source words and the
reference representations. This leads to the best re-
sults improving 1.5 BLEU over the 1-best decoder
output and adding 0.2 BLEU on average to the gains
achieved by the recurrent language model (?5.4).
Setup. Conventional language models can be
trained on monolingual or bilingual data; however,
the joint model can only be trained on the latter.
In order to control for data size effects, we restrict
training of all models, including the baseline n-gram
model, to the target side of the parallel corpus, about
102m words for French-English. Furthermore we
train recurrent models only on the newswire portion
(about 3.5m words for training and 250k words for
validation) since initial experiments showed compa-
rable results to using the full parallel corpus, avail-
able to the baseline. This is reasonable since the test
data is newswire. Also, it allows for more rapid ex-
perimentation.
5.1 Foreign Sentence Representations
We represent foreign sentences either by latent se-
mantic analysis (LSA; Deerwester et al 1990) or by
word encodings produced as a by-product of train-
ing the recurrent neural network language model on
the source words.
LSA is widely used for representing words and
documents in low-dimensional vector space. The
method applies reduced singular value decomposi-
tion (SVD) to a matrix M of word counts; in our
setting, rows represent sentences and columns rep-
resent foreign words. SVD reduces the number
of columns while preserving similarity among the
rows, effectively mapping from a high-dimensional
representation of a sentence, as a set of words, to a
low-dimensional set of concepts. The output of SVD
is an approximation of M by three matrices: T con-
tains single word representations, R represents full
sentences, and S is a diagonal scaling matrix:
M ? TSRT
Given vocabulary V and n sentences, we construct
M as a matrix of size |V ?n|. The ij-th entry is the
number of times word i occurs in sentence j, also
known as the term frequency value; the entry is also
weighted by the inverse document frequency, the rel-
ative importance of word i among all sentences, ex-
pressed as the negative logarithm of the fraction of
sentences in which word i occurs.
As a second representation we use single word
1049
embeddings implicitly learned by the input layer
weights U of the recurrent neural network language
model (?2), denoted as RNN. Each word is repre-
sented by a vector of size |hi|, the number of neu-
rons in the hidden layer; in our experiments, we
consider concatenations of individual word vectors
to represent foreign word contexts. These encodings
have previously been found to capture syntactic and
semantic regularities (Mikolov et al, 2013) and are
readily available in our experimental framework via
training a recurrent neural network language model
on the source-side of the parallel corpus.
5.2 Results
We first experiment with the two previously intro-
duced representations of the source-side sentence.
Table 5 shows the results compared to the 1-best de-
coder output and an RNN language model (target-
only). We first try LSA encodings of the entire
foreign sentence as 80 or 240 dimensional vectors
(sent-lsa-dim80, sent-lsa-dim240). Next, we experi-
ment with single-word RNN representations of slid-
ing word-windows in the hope of representing rel-
evant context more precisely. Word-windows are
constructed relative to the source words aligned to
the current target word, and individual word vec-
tors are concatenated into a single vector. We
first try contexts which do not include the aligned
source words, in the hope of capturing information
not already modeled by the channel models, start-
ing with the next five words (ww-rnn-dim50.n5),
the five previous and the next five words (ww-rnn-
dim50.p5n5) as well as the previous three words
(ww-rnn-dim50.p3). Next, we experiment with
word-windows of up to five aligned source words
(ww-rnn-dim50.c5). Finally, we try contexts based
on LSA word vectors (ww-lsa-dim50.n5, ww-lsa-
dim50.p3).5
While all models improve over the baseline, none
significantly outperforms the recurrent neural net-
work language model in terms of BLEU. However,
the perplexity results suggest that the models uti-
lize the foreign representations since all joint mod-
els improve vastly over the target-only language
5We ignore the coverage vector when determining word-
windows which risks including already translated words.
Building word-windows based on the coverage vector requires
additional state in a rescoring setting meant to be light-weight.
?p(f |e)
?p(e|f) ?p(e|f)
Baseline without CM 24.0 22.5
+ target-only 24.5 22.6
+ sent-lsa-dim240 24.9 23.3
+ ww-rnn-dim50.n5 24.9 24.0
+ ww-rnn-dim50.p5n5 24.6 23.7
+ ww-rnn-dim50.p3 24.6 22.3
+ ww-rnn-dim50.c5 24.9 24.0
+ ww-lsa-dim50.n5 24.8 23.9
+ ww-lsa-dim50.p3 23.8 23.2
Table 6: Comparison of the joint model and the chan-
nel model features (CM) by removing channel features
corresponding to ?p(e|f) from the lattices, or both di-
rections ?p(e|f),?p(f |e) and replacing them by vari-
ous joint models. We re-tuned the log-linear weights for
different feature-sets. Accuracy is based on the average
BLEU over news2010, newssyscomb2010, news2011.
model. The lowest perplexity is achieved by the
context covering the aligned source words (ww-rnn-
dim50.c5) since the source words are a better pre-
dictor of the target words than outside context.
The experiments so far measured if the joint
model can improve in addition to the four channel
model features used by the baseline, that is, the max-
imum likelihood and lexical translation features in
both translation directions. The joint model clearly
overlaps with these features, but how well does
the recurrent model perform compared against the
channel model features? To answer this question,
we removed channel model features corresponding
to the same translation direction as the joint model,
specifically pMLE(e|f) and pLW (e|f), from the lat-
tices and measured the effect of adding the joint
models.
The results (Table 6, column ?p(e|f)) clearly
show that our joint models are competitive with the
channel model features by outperforming the orig-
inal baseline with all channel model features (24.7
BLEU) by 0.2 BLEU (ww-rnn-dim50.n5, ww-rnn-
dim50.c5). As a second experiment, we removed all
channel model features (column ?p(e|f), p(f |e)),
diminishing baseline accuracy to 22.5 BLEU. In this
setting, the best joint model is able to make up 1.5
of the 2.2 BLEU lost due to removal of the channel
1050
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
sent-lsa-dim80 25.2 25.2 26.3 25.1 25.6 147
sent-lsa-dim240 25.1 25.0 26.2 24.9 25.4 126
ww-rnn-dim50.n5 24.9 25.0 26.3 24.8 25.4 61
ww-rnn-dim50.p5n5 25.0 24.8 26.2 24.7 25.3 59
ww-rnn-dim50.p3 25.1 25.1 26.5 24.9 25.6 143
ww-rnn-dim50.c5 24.8 24.9 26.0 24.8 25.3 16
ww-lsa-dim50.n5 25.0 25.0 26.2 24.8 25.4 76
ww-lsa-dim50.p3 25.1 25.1 26.5 24.9 25.6 151
Table 5: Translation accuracy of the joint model with various encodings of the foreign sentence measured on the
French-English task. Perplexity (PPL) is based on news2011.
model features, while modeling only a single trans-
lation direction. This setup also shows the negligible
effect of the target-only language model in the ab-
sence of translation scores, whereas the joint models
are much more effective since they do model transla-
tion. Overall, the best joint models prove very com-
petitive to the traditional channel features.
5.3 Oracle Experiment
The previous section examined the effect of a set
of basic foreign sentence representations. Although
we find some benefit from these representations, the
differences are not large. One might naturally ask
whether there is greater potential upside from this
channel model. Therefore we turn to measuring the
upper bound on accuracy for the joint approach as a
whole.
Specifically, we would like to find a bound on ac-
curacy given an ideal representation of the source
sentence. To answer this question, we conducted an
experiment where the joint model has access to an
LSA representation of the reference translation.
Table 7 shows that the joint approach has an ora-
cle accuracy of up to 4.3 BLEU above the baseline.
This clearly confirms that the joint approach can ex-
ploit the additional information to improve BLEU,
given a good enough representation of the foreign
sentence. In terms of perplexity, we see an improve-
ment of up to 65% over the target-only model. It
should be noted that since LSA representations are
computed on reference words, perplexity no longer
has its standard meaning.
BLEU PPL
Baseline 25.2 341
target-only 26.4 218
oracle (sent-lsa-dim40) 27.7 124
oracle (sent-lsa-dim80) 28.5 103
oracle (sent-lsa-dim160) 29.0 86
oracle (sent-lsa-dim240) 29.5 76
Table 7: Oracle accuracy of the joint model when us-
ing an LSA encoding of the references, measured on the
news2011 French-English task.
5.4 Target Language Projections
Our experiments so far showed that joint models
based on direct representations of the source words
are very competitive to the traditional channel mod-
els (?5.2). However, these experiments have not
shown any improvements over the normal recurrent
neural network language model. The previous sec-
tion demonstrated that good representations can lead
to substantial gains (?5.3). In order to bridge the gap,
we propose to learn a separate transform from the
foreign words to an encoding of the reference target
words, thus making the source-side representations
look more like the target-side encodings used in the
oracle experiment.
Specifically, we learn a linear transform
d? : x? r mapping directly from a vector en-
coding of the foreign sentence x to an l-dimensional
LSA representation r of the reference sentence. At
test and training time we apply d? to the foreign
words and use the transformation instead of a direct
1051
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
proj-lsa-dim40 25.1 25.3 26.5 25.2 25.8 145
proj-lsa-dim80 25.1 25.3 26.6 25.2 25.8 134
Table 8: Translation accuracy of the joint model with a source-target transform, measured on the French-English task.
Perplexity (PPL) is based on news2011; differences to target-only are significant at the p < 0.001 level.
source-side representation.
The transform models all foreign words in the par-
allel corpus except singletons, which are collapsed
into a unique class, similar to the recurrent neural
network language model. We train the transform to
minimize the squared error with respect to the ref-
erence LSA vector using an SGD online learner:
?? = argmin
?
n?
i=1
(
ri ? d?(xi)
)2
(1)
We found a simple constant learning rate, tuned
on the validation data, to be as effective as sched-
ules based on constant decay, or reducing the learn-
ing rate when the validation error increased. Our
feature-set includes unigram and bigram word fea-
tures. The value of unigram features is simply the
unigram count in that sentence; bigram features re-
ceive a weight of the bigram count divided by two
to help prevent overfitting. Then the vector for each
sentence was divided by its L2 norm. Both weight-
ing and normalization led to substantial improve-
ments in test set error. More complex features such
as skip-bigrams, trigrams and character n-grams did
not yield any significant improvements. Even this
representation of sentences is composed of a large
number of instances, and so we resorted to feature
hashing by computing feature ids as the least signif-
icant 20 bits of each feature name. Our best trans-
form achieved a cosine similarity of 0.816 on the
training data, 0.757 on the validation data, and 0.749
on news2011.
The results (Table 8) show that the transform im-
proves over the recurrent neural network language
model on all test sets and by 0.2 BLEU on average.
We verified significance over the target-only model
using paired bootstrap resampling (Koehn, 2004)
over all test sets (7526 sentences) at the p < 0.001
level. Overall, we improve accuracy by up to 1.5
BLEU and by 1.1 BLEU on average across all test
sets over the decoder 1-best with our joint language
and translation model.
6 Related Work
Our approach of combining language and translation
modeling is very much in line with recent work on
n-gram-based translation models (Crego and Yvon,
2010), and more recently continuous space-based
translation models (Le et al, 2012a; Gao et al,
2013). The joint model presented in this paper dif-
fers in a number of key aspects: we use a recur-
rent architecture representing an unbounded history
of both source and target words, rather than a feed-
forward style network. Feed-forward networks and
n-gram models have a finite history which makes
predictions independent of anything but a small his-
tory of words. Furthermore, we only model the
target-side which is different to previous work mod-
eling both sides.
We introduced a new algorithm to tackle lattice
rescoring with an unbounded model. The auto-
matic speech recognition community has previously
addressed this issue by either approximating long-
span language models via simpler but more tractable
models (Deoras et al, 2011b), or by identifying con-
fusable subsets of the lattice from which n-best lists
are constructed and rescored (Deoras et al, 2011a).
We extend their work by directly mapping a recur-
rent neural network model onto the structure of the
lattice, rescoring all states instead of focusing only
on subsets.
7 Conclusion and Future Work
Joint language and translation modeling with recur-
rent neural networks leads to substantial gains over
the 1-best decoder output, raising accuracy by up
to 1.5 BLEU and by 1.1 BLEU on average across
1052
several test sets. The joint approach also improves
over the gains of the recurrent neural network lan-
guage model, adding 0.2 BLEU on average across
several test sets. Our models are competitive to the
traditional channel models, outperforming them in a
head-to-head comparison.
Furthermore, we tackled the issue of lattice
rescoring with an unbounded recurrent model by
means of a novel algorithm that keeps a beam of re-
current histories. Finally, we have shown that the
recurrent neural network language model can sig-
nificantly improve over n-gram baselines across a
range of language-pairs, even when the baselines
were trained on 575 times more data.
In future work we plan to directly learn represen-
tations of the source-side during training of the joint
model. Thus, the model itself can decide which en-
coding is best for the task. We also plan to change
the cross entropy objective to a BLEU-inspired ob-
jective in a discriminative training regime, which we
hope to be more effective. We would also like to ap-
ply recent advances in tackling the vanishing gradi-
ent problem (Pascanu et al, 2013) using a regular-
ization term to maintain the magnitude of the gradi-
ents during back propagation through time. Finally,
we would like to integrate the recurrent model di-
rectly into first-pass decoding, a straightforward ex-
tension of lattice rescoring using the algorithm we
developed.
Acknowledgments
We would like to thank Anthony Aue, Hany Has-
san Awadalla, Jon Clark, Li Deng, Sauleh Eetemadi,
Jianfeng Gao, Qin Gao, Xiaodong He, Will Lewis,
Arul Menezes, and Kristina Toutanova for helpful
discussions related to this work as well as for com-
ments on previous drafts. We would also like to
thank the anonymous reviewers for their comments.
References
Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-Son
Le, Aure?lien Max, Guillaume Wisniewski, Franc?ois
Yvon, Gilles Adda, Josep Maria Crego, Adrien
Lardilleux, Thomas Lavergne, and Artem Sokolov.
2011. LIMSI @ WMT11. In Proc. of WMT, pages
309?315, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhu-
vana Ramabhadran. 2012. Deep Neural Network
Language Models. In NAACL-HLT Workshop on the
Future of Language Modeling for HLT, pages 20?28,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, Dec.
Josep Crego and Franois Yvon. 2010. Factored bilingual
n-gram language models for statistical machine trans-
lation. Machine Translation, 24(2):159?175.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Anoop Deoras, Toma?s? Mikolov, and Kenneth Church.
2011a. A Fast Re-scoring Strategy to Capture Long-
Distance Dependencies. In Proc. of EMNLP, pages
1116?1127, Stroudsburg, PA, USA, July. Association
for Computational Linguistics.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink,
M. Karafiat, and Sanjeev Khudanpur. 2011b. Varia-
tional Approximation of Long-Span Language Models
for LVCSR. In Proc. of ICASSP, pages 5532?5535.
Ahmad Emami and Frederick Jelinek. 2005. A Neural
Syntactic Language Model. Machine Learning, 60(1-
3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.
2013. Learning Semantic Representations for the
Phrase Translation Model. Technical Report MSR-
TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum En-
tropy Training. In Proc. of ICASSP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of HLT-NAACL, pages 127?133, Edmonton, Canada,
May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic, Jun.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP,
pages 388?395, Barcelona, Spain, Jul.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous Space Translation Models with
1053
Neural Networks. In Proc. of HLT-NAACL, pages 39?
48, Montre?al, Canada. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012b. LIMSI @ WMT12. In Proc. of WMT, pages
330?337, Montre?al, Canada, June. Association for
Computational Linguistics.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based Minimum
Error Rate Training for Statistical Machine Transla-
tion. In Proc. of EMNLP, pages 725?734, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Toma?s? Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Language
Model. In Proc. of Spoken Language Technologies
(SLT), pages 234?239, Dec.
Toma?s? Mikolov, Karafia?t Martin, Luka?s? Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
Neural Network based Language Model. In Proc. of
INTERSPEECH, pages 1045?1048.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proc. of ASRU, pages 196?201.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of Recurrent Neural Network Language Model.
In Proc. of ICASSP, pages 5528?5531.
Toma?s? Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Toma?s? Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training Recurrent Neural
Networks. Proc. of ICML, abs/1211.5063.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space Lan-
guage Models on a GPU for Statistical Machine Trans-
lation. In NAACL-HLT Workshop on the Future of
Language Modeling for HLT, pages 11?19. Associa-
tion for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schlu?ter, and Hermann Ney. 2013.
Comparison of Feedforward and Recurrent Neural
Network Language Models. In IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing, pages 8430?8434, Vancouver, Canada, May.
Geoff Zweig and Konstantin Makarychev. 2013. Speed
Regularization and Optimality in Word Classing. In
Proc. of ICASSP.
1054
Proceedings of NAACL-HLT 2013, pages 746?751,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Linguistic Regularities in Continuous Space Word Representations
Tomas Mikolov? , Wen-tau Yih, Geoffrey Zweig
Microsoft Research
Redmond, WA 98052
Abstract
Continuous space language models have re-
cently demonstrated outstanding results across
a variety of tasks. In this paper, we ex-
amine the vector-space word representations
that are implicitly learned by the input-layer
weights. We find that these representations
are surprisingly good at capturing syntactic
and semantic regularities in language, and
that each relationship is characterized by a
relation-specific vector offset. This allows
vector-oriented reasoning based on the offsets
between words. For example, the male/female
relationship is automatically learned, and with
the induced vector representations, ?King -
Man + Woman? results in a vector very close
to ?Queen.? We demonstrate that the word
vectors capture syntactic regularities by means
of syntactic analogy questions (provided with
this paper), and are able to correctly answer
almost 40% of the questions. We demonstrate
that the word vectors capture semantic regu-
larities by using the vector offset method to
answer SemEval-2012 Task 2 questions. Re-
markably, this method outperforms the best
previous systems.
1 Introduction
A defining feature of neural network language mod-
els is their representation of words as high dimen-
sional real valued vectors. In these models (Ben-
gio et al, 2003; Schwenk, 2007; Mikolov et al,
2010), words are converted via a learned lookup-
table into real valued vectors which are used as the
?Currently at Google, Inc.
inputs to a neural network. As pointed out by the
original proposers, one of the main advantages of
these models is that the distributed representation
achieves a level of generalization that is not possi-
ble with classical n-gram language models; whereas
a n-gram model works in terms of discrete units that
have no inherent relationship to one another, a con-
tinuous space model works in terms of word vectors
where similar words are likely to have similar vec-
tors. Thus, when the model parameters are adjusted
in response to a particular word or word-sequence,
the improvements will carry over to occurrences of
similar words and sequences.
By training a neural network language model, one
obtains not just the model itself, but also the learned
word representations, which may be used for other,
potentially unrelated, tasks. This has been used to
good effect, for example in (Collobert and Weston,
2008; Turian et al, 2010) where induced word rep-
resentations are used with sophisticated classifiers to
improve performance in many NLP tasks.
In this work, we find that the learned word repre-
sentations in fact capture meaningful syntactic and
semantic regularities in a very simple way. Specif-
ically, the regularities are observed as constant vec-
tor offsets between pairs of words sharing a par-
ticular relationship. For example, if we denote the
vector for word i as xi, and focus on the singu-
lar/plural relation, we observe that xapple?xapples ?
xcar?xcars, xfamily?xfamilies ? xcar?xcars, and
so on. Perhaps more surprisingly, we find that this
is also the case for a variety of semantic relations, as
measured by the SemEval 2012 task of measuring
relation similarity.
746
The remainder of this paper is organized as fol-
lows. In Section 2, we discuss related work; Section
3 describes the recurrent neural network language
model we used to obtain word vectors; Section 4 dis-
cusses the test sets; Section 5 describes our proposed
vector offset method; Section 6 summarizes our ex-
periments, and we conclude in Section 7.
2 Related Work
Distributed word representations have a long his-
tory, with early proposals including (Hinton, 1986;
Pollack, 1990; Elman, 1991; Deerwester et al,
1990). More recently, neural network language
models have been proposed for the classical lan-
guage modeling task of predicting a probability dis-
tribution over the ?next? word, given some preced-
ing words. These models were first studied in the
context of feed-forward networks (Bengio et al,
2003; Bengio et al, 2006), and later in the con-
text of recurrent neural network models (Mikolov et
al., 2010; Mikolov et al, 2011b). This early work
demonstrated outstanding performance in terms of
word-prediction, but also the need for more compu-
tationally efficient models. This has been addressed
by subsequent work using hierarchical prediction
(Morin and Bengio, 2005; Mnih and Hinton, 2009;
Le et al, 2011; Mikolov et al, 2011b; Mikolov et
al., 2011a). Also of note, the use of distributed
topic representations has been studied in (Hinton
and Salakhutdinov, 2006; Hinton and Salakhutdi-
nov, 2010), and (Bordes et al, 2012) presents a se-
mantically driven method for obtaining word repre-
sentations.
3 Recurrent Neural Network Model
The word representations we study are learned by a
recurrent neural network language model (Mikolov
et al, 2010), as illustrated in Figure 1. This architec-
ture consists of an input layer, a hidden layer with re-
current connections, plus the corresponding weight
matrices. The input vector w(t) represents input
word at time t encoded using 1-of-N coding, and the
output layer y(t) produces a probability distribution
over words. The hidden layer s(t) maintains a rep-
resentation of the sentence history. The input vector
w(t) and the output vector y(t) have dimensional-
ity of the vocabulary. The values in the hidden and
Figure 1: Recurrent Neural Network Language Model.
output layers are computed as follows:
s(t) = f (Uw(t) +Ws(t?1)) (1)
y(t) = g (Vs(t)) , (2)
where
f(z) =
1
1 + e?z
, g(zm) =
ezm
?
k e
zk
. (3)
In this framework, the word representations are
found in the columns of U, with each column rep-
resenting a word. The RNN is trained with back-
propagation to maximize the data log-likelihood un-
der the model. The model itself has no knowledge
of syntax or morphology or semantics. Remark-
ably, training such a purely lexical model to max-
imize likelihood will induce word representations
with striking syntactic and semantic properties.
4 Measuring Linguistic Regularity
4.1 A Syntactic Test Set
To understand better the syntactic regularities which
are inherent in the learned representation, we created
a test set of analogy questions of the form ?a is to b
as c is to ? testing base/comparative/superlative
forms of adjectives; singular/plural forms of com-
mon nouns; possessive/non-possessive forms of
common nouns; and base, past and 3rd person
present tense forms of verbs. More precisely, we
tagged 267M words of newspaper text with Penn
747
Category Relation Patterns Tested # Questions Example
Adjectives Base/Comparative JJ/JJR, JJR/JJ 1000 good:better rough:
Adjectives Base/Superlative JJ/JJS, JJS/JJ 1000 good:best rough:
Adjectives Comparative/
Superlative
JJS/JJR, JJR/JJS 1000 better:best rougher:
Nouns Singular/Plural NN/NNS,
NNS/NN
1000 year:years law:
Nouns Non-possessive/
Possessive
NN/NN POS,
NN POS/NN
1000 city:city?s bank:
Verbs Base/Past VB/VBD,
VBD/VB
1000 see:saw return:
Verbs Base/3rd Person
Singular Present
VB/VBZ, VBZ/VB 1000 see:sees return:
Verbs Past/3rd Person
Singular Present
VBD/VBZ,
VBZ/VBD
1000 saw:sees returned:
Table 1: Test set patterns. For a given pattern and word-pair, both orderings occur in the test set. For example, if
?see:saw return: ? occurs, so will ?saw:see returned: ?.
Treebank POS tags (Marcus et al, 1993). We then
selected 100 of the most frequent comparative adjec-
tives (words labeled JJR); 100 of the most frequent
plural nouns (NNS); 100 of the most frequent pos-
sessive nouns (NN POS); and 100 of the most fre-
quent base form verbs (VB). We then systematically
generated analogy questions by randomly matching
each of the 100 words with 5 other words from the
same category, and creating variants as indicated in
Table 1. The total test set size is 8000. The test set
is available online. 1
4.2 A Semantic Test Set
In addition to syntactic analogy questions, we used
the SemEval-2012 Task 2, Measuring Relation Sim-
ilarity (Jurgens et al, 2012), to estimate the extent
to which RNNLM word vectors contain semantic
information. The dataset contains 79 fine-grained
word relations, where 10 are used for training and
69 testing. Each relation is exemplified by 3 or
4 gold word pairs. Given a group of word pairs
that supposedly have the same relation, the task is
to order the target pairs according to the degree to
which this relation holds. This can be viewed as an-
other analogy problem. For example, take the Class-
Inclusion:Singular Collective relation with the pro-
1http://research.microsoft.com/en-
us/projects/rnn/default.aspx
totypical word pair clothing:shirt. To measure the
degree that a target word pair dish:bowl has the same
relation, we form the analogy ?clothing is to shirt as
dish is to bowl,? and ask how valid it is.
5 The Vector Offset Method
As we have seen, both the syntactic and semantic
tasks have been formulated as analogy questions.
We have found that a simple vector offset method
based on cosine distance is remarkably effective in
solving these questions. In this method, we assume
relationships are present as vector offsets, so that in
the embedding space, all pairs of words sharing a
particular relation are related by the same constant
offset. This is illustrated in Figure 2.
In this model, to answer the analogy question a:b
c:d where d is unknown, we find the embedding
vectors xa, xb, xc (all normalized to unit norm), and
compute y = xb ? xa + xc. y is the continuous
space representation of the word we expect to be the
best answer. Of course, no word might exist at that
exact position, so we then search for the word whose
embedding vector has the greatest cosine similarity
to y and output it:
w? = argmaxw
xwy
?xw??y?
When d is given, as in our semantic test set, we
simply use cos(xb ? xa + xc, xd) for the words
748
Figure 2: Left panel shows vector offsets for three word
pairs illustrating the gender relation. Right panel shows
a different projection, and the singular/plural relation for
two words. In high-dimensional space, multiple relations
can be embedded for a single word.
provided. We have explored several related meth-
ods and found that the proposed method performs
well for both syntactic and semantic relations. We
note that this measure is qualitatively similar to rela-
tional similarity model of (Turney, 2012), which pre-
dicts similarity between members of the word pairs
(xb, xd), (xc, xd) and dis-similarity for (xa, xd).
6 Experimental Results
To evaluate the vector offset method, we used
vectors generated by the RNN toolkit of Mikolov
(2012). Vectors of dimensionality 80, 320, and 640
were generated, along with a composite of several
systems, with total dimensionality 1600. The sys-
tems were trained with 320M words of Broadcast
News data as described in (Mikolov et al, 2011a),
and had an 82k vocabulary. Table 2 shows results
for both RNNLM and LSA vectors on the syntactic
task. LSA was trained on the same data as the RNN.
We see that the RNN vectors capture significantly
more syntactic regularity than the LSA vectors, and
do remarkably well in an absolute sense, answering
more than one in three questions correctly. 2
In Table 3 we compare the RNN vectors with
those based on the methods of Collobert and We-
ston (2008) and Mnih and Hinton (2009), as imple-
mented by (Turian et al, 2010) and available online
3 Since different words are present in these datasets,
we computed the intersection of the vocabularies of
the RNN vectors and the new vectors, and restricted
the test set and word vectors to those. This resulted
in a 36k word vocabulary, and a test set with 6632
2Guessing gets a small fraction of a percent.
3http://metaoptimize.com/projects/wordreprs/
Method Adjectives Nouns Verbs All
LSA-80 9.2 11.1 17.4 12.8
LSA-320 11.3 18.1 20.7 16.5
LSA-640 9.6 10.1 13.8 11.3
RNN-80 9.3 5.2 30.4 16.2
RNN-320 18.2 19.0 45.0 28.5
RNN-640 21.0 25.2 54.8 34.7
RNN-1600 23.9 29.2 62.2 39.6
Table 2: Results for identifying syntactic regularities for
different word representations. Percent correct.
Method Adjectives Nouns Verbs All
RNN-80 10.1 8.1 30.4 19.0
CW-50 1.1 2.4 8.1 4.5
CW-100 1.3 4.1 8.6 5.0
HLBL-50 4.4 5.4 23.1 13.0
HLBL-100 7.6 13.2 30.2 18.7
Table 3: Comparison of RNN vectors with Turian?s Col-
lobert and Weston based vectors and the Hierarchical
Log-Bilinear model of Mnih and Hinton. Percent correct.
questions. Turian?s Collobert andWeston based vec-
tors do poorly on this task, whereas the Hierarchical
Log-Bilinear Model vectors of (Mnih and Hinton,
2009) do essentially as well as the RNN vectors.
These representations were trained on 37M words
of data and this may indicate a greater robustness of
the HLBL method.
We conducted similar experiments with the se-
mantic test set. For each target word pair in a rela-
tion category, the model measures its relational sim-
ilarity to each of the prototypical word pairs, and
then uses the average as the final score. The results
are evaluated using the two standard metrics defined
in the task, Spearman?s rank correlation coefficient
? and MaxDiff accuracy. In both cases, larger val-
ues are better. To compare to previous systems, we
report the average over all 69 relations in the test set.
From Table 4, we see that as with the syntac-
tic regularity study, the RNN-based representations
perform best. In this case, however, Turian?s CW
vectors are comparable in performance to the HLBL
vectors. With the RNN vectors, the performance im-
proves as the number of dimensions increases. Sur-
prisingly, we found that even though the RNN vec-
749
Method Spearman?s ? MaxDiff Acc.
LSA-640 0.149 0.364
RNN-80 0.211 0.389
RNN-320 0.259 0.408
RNN-640 0.270 0.416
RNN-1600 0.275 0.418
CW-50 0.159 0.363
CW-100 0.154 0.363
HLBL-50 0.149 0.363
HLBL-100 0.146 0.362
UTD-NB 0.230 0.395
Table 4: Results in measuring relation similarity
tors are not trained or tuned specifically for this task,
the model achieves better results (RNN-320, RNN-
640 & RNN-1600) than the previously best perform-
ing system, UTD-NB (Rink and Harabagiu, 2012).
7 Conclusion
We have presented a generally applicable vector off-
set method for identifying linguistic regularities in
continuous space word representations. We have
shown that the word representations learned by a
RNNLM do an especially good job in capturing
these regularities. We present a new dataset for mea-
suring syntactic performance, and achieve almost
40% correct. We also evaluate semantic general-
ization on the SemEval 2012 task, and outperform
the previous state-of-the-art. Surprisingly, both re-
sults are the byproducts of an unsupervised maxi-
mum likelihood training criterion that simply oper-
ates on a large amount of text data.
References
Y. Bengio, R. Ducharme, Vincent, P., and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Reseach, 3(6).
Y. Bengio, H. Schwenk, J.S. Sene?cal, F. Morin, and J.L.
Gauvain. 2006. Neural probabilistic language models.
Innovations in Machine Learning, pages 137?186.
A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012.
Joint learning of words and meaning representations
for open-text semantic parsing. In Proceedings of 15th
International Conference on Artificial Intelligence and
Statistics.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th
international conference on Machine learning, pages
160?167. ACM.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
J.L. Elman. 1991. Distributed representations, simple re-
current networks, and grammatical structure. Machine
learning, 7(2):195?225.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504?507.
G. Hinton and R. Salakhutdinov. 2010. Discovering bi-
nary codes for documents by learning deep generative
models. Topics in Cognitive Science, 3(1):74?91.
G.E. Hinton. 1986. Learning distributed representations
of concepts. In Proceedings of the eighth annual con-
ference of the cognitive science society, pages 1?12.
Amherst, MA.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. Semeval-2012 task 2: Measuring de-
grees of relational similarity. In *SEM 2012: The First
Joint Conference on Lexical and Computational Se-
mantics (SemEval 2012), pages 356?364. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and
F. Yvon. 2011. Structured output layer neural network
language model. In Proceedings of ICASSP 2011.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proceedings of ASRU 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Tomas Mikolov. 2012. RNN toolkit.
A. Mnih and G.E. Hinton. 2009. A scalable hierarchical
distributed language model. Advances in neural infor-
mation processing systems, 21:1081?1088.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
750
international workshop on artificial intelligence and
statistics, pages 246?252.
J.B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46(1):77?105.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
*SEM 2012: The First Joint Conference on Lexical
and Computational Semantics (SemEval 2012), pages
413?418. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492
? 518.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
P.D. Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533?585.
751
Proceedings of NAACL-HLT 2013, pages 1000?1009,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Combining Heterogeneous Models for Measuring Relational Similarity
Alisa Zhila?
Instituto Politecnico Nacional
Mexico City, Mexico
alisa.zhila@gmail.com
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Geoffrey Zweig
Microsoft Research
Redmond, WA 98052, USA
gzweig@microsoft.com
Tomas Mikolov?
BRNO University of Technology
BRNO, Czech Republic
tmikolov@gmail.com
Abstract
In this work, we study the problem of mea-
suring relational similarity between two word
pairs (e.g., silverware:fork and clothing:shirt).
Due to the large number of possible relations,
we argue that it is important to combine mul-
tiple models based on heterogeneous informa-
tion sources. Our overall system consists of
two novel general-purpose relational similar-
ity models and three specific word relation
models. When evaluated in the setting of a
recently proposed SemEval-2012 task, our ap-
proach outperforms the previous best system
substantially, achieving a 54.1% relative in-
crease in Spearman?s rank correlation.
1 Introduction
The problem of measuring relational similarity is
to determine the degree of correspondence between
two word pairs. For instance, the analogous word
pairs silverware:fork and clothing:shirt both exem-
plify well a Class-Inclusion:Singular Collective re-
lation and thus have high relational similarity. Un-
like the problem of attributional similarity, which
measures whether two words share similar attributes
and is addressed in extensive research work (Bu-
danitsky and Hirst, 2006; Reisinger and Mooney,
2010; Radinsky et al, 2011; Agirre et al, 2009; Yih
and Qazvinian, 2012), measuring relational similar-
ity is a relatively new research direction pioneered
by Turney (2006), but with many potential appli-
cations. For instance, problems of identifying spe-
cific relations between words, such as synonyms,
?Work conducted while interning at Microsoft Research.
antonyms or associations, can be reduced to mea-
suring relational similarity compared to prototypical
word pairs with the desired relation (Turney, 2008).
In scenarios like information extraction or question
answering, where identifying the existence of cer-
tain relations is often the core problem, measuring
relational similarity provides a more flexible solu-
tion rather than creating relational classifiers for pre-
defined or task-specific categories of relations (Tur-
ney, 2006; Jurgens et al, 2012).
In order to promote this research direction, Ju-
rgens et al (2012) proposed a new shared task of
measuring relational similarity in SemEval-2012 re-
cently. In this task, each submitted system is re-
quired to judge the degree of a target word pair
having a particular relation, measured by its re-
lational similarity compared to a few prototypical
example word pairs. The system performance is
evaluated by its correlation with the human judg-
ments using two evaluation metrics, Spearman?s
rank correlation and MaxDiff accuracy (more de-
tails of the task and evaluation metrics will be given
in Sec. 3). Although participating systems incorpo-
rated substantial amounts of information from lex-
ical resources (e.g., WordNet) and contextual pat-
terns from large corpora, only one system (Rink and
Harabagiu, 2012) is able to outperform a simple
baseline that uses PMI (pointwise mutual informa-
tion) scoring, which demonstrates the difficulty of
this task.
In this paper, we explore the problem of mea-
suring relational similarity in the same task setting.
We argue that due to the large number of possible
relations, building an ensemble of relational simi-
1000
larity models based on heterogeneous information
sources is the key to advance the state-of-the-art on
this problem. By combining two general-purpose re-
lational similarity models with three specific word-
relation models covering relations like IsA and syn-
onymy/antonymy, we improve the previous state-
of-the-art substantially ? having a relative gain of
54.1% in Spearman?s rank correlation and 14.7% in
the MaxDiff accuracy!
Our main contributions are threefold. First, we
propose a novel directional similarity method based
on the vector representation of words learned from
a recurrent neural network language model. The re-
lation of two words is captured by their vector off-
set in the latent semantic space. Similarity of rela-
tions can then be naturally measured by a distance
function in the vector space. This method alone
already performs better than all existing systems.
Second, unlike the previous finding, where SVMs
learn a much poorer model than naive Bayes (Rink
and Harabagiu, 2012), we show that using a highly-
regularized log-linear model on simple contextual
pattern features collected from a document collec-
tion of 20GB, a discriminative approach can learn a
strong model as well. Third, we demonstrate that by
augmenting existing word-relation models, which
cover only a small number of relations, the overall
system can be further improved.
The rest of this paper is organized as follows. We
first survey the related work in Sec. 2 and formally
define the problem in Sec. 3. We describe the indi-
vidual models in detail in Sec. 4. The combination
approach is depicted in Sec. 5, along with experi-
mental comparisons to individual models and exist-
ing systems. Finally, Sec. 6 concludes the paper.
2 Related Work
Building a classifier to determine whether a relation-
ship holds between a pair of words is a natural ap-
proach to the task of measuring relational similarity.
While early work was mostly based on hand-crafted
rules (Finin, 1980; Vanderwende, 1994), Rosario
and Hearst (2001) introduced a machine learning ap-
proach to classify word pairs. They targeted clas-
sifying noun modifier pairs from the medical do-
main into 13 classes of semantic relations. Fea-
tures for each noun modifier pair were constructed
using large medical lexical resources and a multi-
class classifier was trained using a feed-forward neu-
ral network with one hidden layer. This work was
later extended by Nastase and Szpakowicz (2003)
to classify general domain noun-modifier pairs into
30 semantic relations. In addition to extracting fea-
tures using WordNet and Roget?s Thesaurus, they
also experimented with several different learners in-
cluding decision trees, memory-based learning and
inductive logic programming methods like RIPPER
and FOIL. Using the same dataset as in (Nastase
and Szpakowicz, 2003), Turney and Littman (2005)
created a 128-dimentional feature vector for each
word pair based on statistics of their co-occurrence
patterns in Web documents and applied the k-NN
method (k = 1 in their work).
Measuring relational similarity, which determines
whether two word pairs share the same relation, can
be viewed as an extension of classifying relations
between two words. Treating a relational similar-
ity measure as a distance metric, a testing pair of
words can be judged by whether they have a rela-
tion that is similar to some prototypical word pairs
having a particular relation. A multi-relation clas-
sifier can thus be built easily in this framework as
demonstrated in (Turney, 2008), where the prob-
lems of identifying synonyms, antonyms and asso-
ciated words are all reduced to finding good anal-
ogous word pairs. Measuring relational similarity
has been advocated and pioneered by Turney (2006),
who proposed a latent vector space model for an-
swering SAT analogy questions (e.g., mason:stone
vs. carpenter:wood). In contrast, we take a slightly
different view when building a relational similarity
measure. Existing classifiers for specific word re-
lations (e.g., synonyms or Is-A) are combined with
general relational similarity measures. Empirically,
mixing heterogeneous models tends to make the fi-
nal relational similarity measure more robust.
Although datasets for semantic relation classifica-
tion or SAT analogous questions can be used to eval-
uate a relational similarity model, their labels are ei-
ther binary or categorical, which makes the datasets
suboptimal for determining the quality of a model
when evaluated on instances of the same relation
class. As a result, Jurgens et al (2012) proposed a
new task of ?Measuring Degrees of Relational Simi-
larity? at SemEval-2012, which includes 79 relation
1001
categories exemplified by three or four prototypical
word pairs and a schematic description. For exam-
ple, for the Class-Inclusion:Taxonomic relation, the
schematic description is ?Y is a kind/type/instance
of X?. Using Amazon Mechanical Turk1, they col-
lected word pairs for each relation, as well as their
degrees of being a good representative of a partic-
ular relation when compared with defining exam-
ples. Participants of this shared task proposed var-
ious kinds of approaches that leverage both lexical
resources and general corpora. For instance, the
Duluth systems (Pedersen, 2012) created word vec-
tors based on WordNet and estimated the degree of
a relation using cosine similarity. The BUAP sys-
tem (Tovar et al, 2012) represented each word pair
as a whole by a vector of 4 different types of fea-
tures: context, WordNet, POS tags and the aver-
age number of words separating the two words in
text. The degree of relation was then determined
by the cosine distance of the target pair from the
prototypical examples of each relation. Although
their models incorporated a significant amount of
information of words or word pairs, unfortunately,
the performance were not much better than a ran-
dom baseline, which indicates the difficulty of this
task. In comparison, a supervised learning approach
seems more promising. The UTD system (Rink and
Harabagiu, 2012), which mined lexical patterns be-
tween co-occurring words in the corpus and then
used them as features to train a Naive Bayes classi-
fier, achieved the best results. However, potentially
due to the large feature space, this strategy did not
work as well when switching the learning algorithm
to SVMs.
3 Problem Definition & Task Description
Following the setting of SemEval-2012 Task 2 (Ju-
rgens et al, 2012), the problem of measur-
ing the degree of relational similarity is to rate
word pairs by the degree to which they are
prototypical members of a given relation class.
For instance, comparing to the prototypical word
pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of
the Class-Inclusion:Singular Collective relation, we
would like to know among the input word pairs
{dish:bowl, book:novel, furniture:desk}, which one
1http://www.mturk.com
best demonstrates the relation.
Because our approaches are evaluated using the
data provided in this SemEval-2012 task, we de-
scribe briefly below how the data was collected, as
well as the metrics used to evaluate system perfor-
mance. The dataset consists of 79 relation classes
that are chosen according to (Bejar et al, 1991)
and broadly fall into 10 main categories, includ-
ing Class-Inclusion, Part-Whole, Similar and more.
With the help of Amazon Mechanical Turk, Jurgens
et al (2012) used a two-phase approach to collect
word pairs and their degrees. In the first phase,
a lexical schema, such as ?a Y is one item in a
collection/group of X? for the aforementioned rela-
tion Class-Inclusion:Singular Collective, and a few
prototypical pairs for each class were given to the
workers, who were asked to provide approximately
a list of 40 word pairs representing the same rela-
tion class. Naturally, some of these pairs were bet-
ter examples than the others. Therefore, in the sec-
ond phase, the goal was to measure the degree of
their similarity to the corresponding relation. This
was done using the MaxDiff technique (Louviere
and Woodworth, 1991). For each relation, about one
hundred questions were first created. Each question
consists of four different word pairs randomly sam-
pled from the list. The worker was then asked to
choose the most and least representative word pairs
for the specific relation in each question.
The set of 79 word relations were randomly split
into training and testing sets. The former contains
10 relations and the latter has 69. Word pairs in all
79 relations were given to the task participants in ad-
vance, but only the human judgments of the training
set were available for system development. In this
work, we treat the training set as the validation set
? all the model exploration and refinement is done
using this set of data, as well as the hyper-parameter
tuning when learning the final model combination.
The quality of a relational similarity measure is
estimated by its correlation to human judgments.
This is evaluated using two metrics in the task: the
MaxDiff accuracy and Spearman?s rank correlation
coefficient (?). A system is first asked to pick the
most and least representative word pairs of each
question in the MaxDiff setting. The average accu-
racy of the predictions compared to the human an-
swers is then reported. In contrast, Spearman?s ?
1002
measures the correlation between the total orderings
of all word pairs of a relation, where the total order-
ing is derived from the MaxDiff answers (see (Jur-
gens et al, 2012) for the exact procedure).
4 Models for Relational Similarity
We investigate three types of models for relational
similarity. Operating in a word vector space, the di-
rectional similarity model compares the vector dif-
ferences of target and prototypical word pairs to es-
timate their relational similarity. The lexical pat-
tern method collects contextual information of pairs
of words when they co-occur in large corpora, and
learns a highly regularized log-linear model. Finally,
the word relation models incorporate existing, spe-
cific word relation measures for general relational
similarity.
4.1 Directional Similarity Model
Our first model for relational similarity extends pre-
vious work on semantic word vector representa-
tions to a directional similarity model for pairs of
words. There are many different methods for cre-
ating real-valued semantic word vectors, such as
the distributed representation derived from a word
co-occurrence matrix and a low-rank approxima-
tion (Landauer et al, 1998), word clustering (Brown
et al, 1992) and neural-network language model-
ing (Bengio et al, 2003; Mikolov et al, 2010). Each
element in the vectors conceptually represents some
latent topicality information of the word. The goal
of these methods is that words with similar mean-
ings will tend to be close to each other in the vector
space.
Although the vector representation of single
words has been successfully applied to problems
like semantic word similarity and text classifica-
tion (Turian et al, 2010), the issue of how to repre-
sent and compare pairs of words in a vector space
remains unclear (Turney, 2012). In a companion
paper (Mikolov et al, 2013), we present a vector
offset method which performs consistently well in
identifying both syntactic and semantic regularities.
This method measures the degree of the analogy
?a is to b as c is to d? using the cosine score of
(~vb?~va +~vc, ~vd), where a, b, c, d are the four given
words and ~va, ~vb, ~vc, ~vd are the corresponding vec-
q 
shirt
clothing
furniture
desk
v1
v2'
v2'
Figure 1: Directional vectors ?1 and ?2 capture the rela-
tions of clothing:shirt and furniture:desk respectively in
this semantic vector space. The relational similarity of
these two word pairs is estimated by the cosine of ?.
tors. In this paper, we propose a variant called the
directional similarity model, which performs bet-
ter for semantic relations. Let ?i = (wi1 , wi2) and
?j = (wj1 , wj2) be the two word pairs being com-
pared. Suppose (~vi1 , ~vi2) and (~vj1 , ~vj2) are the cor-
responding vectors of these words. The directional
vectors of ?i and ?j are defined as ~?i ? ~vi2 ? ~vi1
and ~?j ? ~vj2 ? ~vj1 , respectively. Relational simi-
larity of these two word pairs can be measured by
some distance function of ?i and ?j , such as the co-
sine function:
~?i ? ~?j
?~?i??~?j?
The rationale behind this variant is as follows. Be-
cause the difference of two word vectors reveals the
change from one word to the other in terms of mul-
tiple topicality dimensions in the vector space, two
word pairs having similar offsets (i.e., being rela-
tively parallel) can be interpreted as they have simi-
lar relations. Fig. 1 further illustrates this method.
Compared to the original method, this variant
places less emphasis on the similarity between
words wj1 and wj2 . That similarity is necessary
for syntactic relations where the words are often re-
lated by morphology, but not for semantic relations.
On semantic relations studied in this paper, the di-
rectional similarity model performs about 18% rela-
tively better in Spearman?s ? than the original one.
The quality of the directional similarity method
depends heavily on the underlying word vector
space model. We compared two choices with dif-
1003
Word Embedding Spearman?s ? MaxDiff Acc. (%)
LSA-80 0.055 34.6
LSA-320 0.066 34.4
LSA-640 0.102 35.7
RNNLM-80 0.168 37.5
RNNLM-320 0.214 39.1
RNNLM-640 0.221 39.2
RNNLM-1600 0.234 41.2
Table 1: Results of measuring relational similarity using
the directional similarity method, evaluated on the train-
ing set. The 1600-dimensional RNNLM vector space
achieves the highest Spearman?s ? and MaxDiff accuracy.
ferent dimensionality settings: the word embedding
learned from the recurrent neural network language
model (RNNLM)2 and the LSA vectors, both were
trained using the same Broadcast News corpus of
320M words as described in (Mikolov et al, 2011).
All the word vectors were first normalized to unit
vectors before applying the directional similarity
method. Given a target word pair, we computed
its relational similarity compared with the prototyp-
ical word pairs of the same relation. The average
of these measurements was taken as the final model
score. Table 1 summarizes the results when evalu-
ated on the training set. As shown in the table, the
RNNLM vectors consistently outperform their LSA
counterparts with the same dimensionality. In addi-
tion, more dimensions seem to preserve more infor-
mation and lead to better performance. Therefore,
we take the 1600-dimensional RNNLM vectors to
construct our final directional similarity model.
4.2 Lexical Pattern Model
Our second model for measuring relational similar-
ity is built based on lexical patterns. It is well-known
that contexts in which two words co-occur often pro-
vide useful cues for identifying the word relation.
For example, having observed frequent text frag-
ments like ?X such as Y?, it is likely that there is a
Class-Inclusion:Taxonomic relation between X and
Y; namely, Y is a type of X. Indeed, by mining lexical
patterns from a large corpus, the UTD system (Rink
and Harabagiu, 2012) managed to outperform other
participants in the SemEval-2012 task of measuring
relational similarity.
2http://www.fit.vutbr.cz/?imikolov/rnnlm
In order to find more co-occurrences of each pair
of words, we used a large document set that con-
sists of the Gigaword corpus (Parker et al, 2009),
Wikipedia and LA Times articles3, summing up to
more than 20 Gigabytes of texts. For each word
pair (w1, w2) that co-occur in a sentence, we col-
lected the words in between as its context (or so-
called ?raw pattern?). For instance, ?such as? would
be the context extracted from ?X such as Y? for
the word pair (X, Y). To reduce noise, contexts with
more than 9 words were dropped and 914,295 pat-
terns were collected in total.
Treating each raw pattern as a feature where the
value is the logarithm of the occurrence count, we
then built a probabilistic classifier to determine the
association of the context and relation. For each re-
lation, we treated all its word pairs as positive ex-
amples and all the word pairs in other relations as
negative examples4. 79 classifiers were trained in
total, where each one was trained using 3,218 ex-
amples. The degree of relational similarity of each
word pair can then be judged by the output of the
corresponding classifier5. Although this seems like a
standard supervised learning setting, the large num-
ber of features poses a challenge here. Using almost
1M features and 3,218 examples, the model could
easily overfit if not regularized properly, which may
explain why learning SVMs on pattern features per-
formed poorly (Rink and Harabagiu, 2012). In-
stead of employing explicit feature selection meth-
ods, we used an efficient L1 regularized log-linear
model learner (Andrew and Gao, 2007) and chose
the hyper-parameters based on model performance
on the training data. The final models we chose
were trained with L1 = 3, where 28,065 features
in average were selected automatically by the algo-
3We used a Nov-2010 dump of English Wikipedia, which
contains approximately 917M words after pre-processing. The
LA Times corpus consists of articles from 1985 to 2002 and has
about 1.1B words.
4Given that not all word pairs belonging to the same relation
category are equally good, removing those with low judgment
scores may help improve the quality of the labeled data. We
leave this study to future work.
5Training a separate classifier for each MaxDiff question us-
ing all words pairs except the four target pairs appears to be a
better setting, as it would avoid including the target pairs in the
training process. We did not use this setting because it is more
complicated and performed roughly the same empirically.
1004
rithm. The performance on the training data is 0.322
in Spearman?s ? and 41.8% in MaxDiff accuracy.
4.3 Word Relation Models
The directional similarity and lexical pattern mod-
els can be viewed as general purpose methods for
relational similarity as they do not differentiate the
specific relation categories. In contrast, for specific
word relations, there exist several high-quality meth-
ods. Although they are designed for detecting spe-
cific relations between words, incorporating them
could still improve the overall results. Next, we ex-
plore the use of some of these word relation mod-
els, including information encoded in the knowledge
base and a lexical semantic model for synonymy and
antonymy.
4.3.1 Knowledge Bases
Predetermined types of relations can often be
found in existing lexical and knowledge databases,
such as WordNet?s Is-A taxonomy and the exten-
sive relations stored in the NELL (Carlson et al,
2010) knowledge base. Although in theory, these
resources can be directly used to solve the problem
of relational similarity, such direct approaches often
suffer from two practical issues. First, the word cov-
erage of these databases is usually very limited and
it is common that the relation of a given word pair
is absent. Second, the degree of relation is often not
included, which makes the task of measuring the de-
gree of relational similarity difficult.
One counter example, however, is Probase (Wu
et al, 2012), which is a knowledge base that es-
tablishes connections between more than 2.5 mil-
lion concepts discovered automatically from the
Web. For the Is-A and Attribute relations it en-
codes, Probase also returns the probability that two
input words share the relation, based on the co-
occurrence frequency. We used some relations in
the training set to evaluate the quality of Probase.
For instance, its Is-A model performs exception-
ally well on the relation Class-Inclusion:Taxonomic,
reaching a high Spearman?s ? = 0.642 and MaxD-
iff accuracy 55.8%. Similarly, its Attribute model
performs better than our lexical pattern model
on Attribute:Agent Attribute-State with Spearman?s
? = 0.290 and MaxDiff accuracy 32.7%.
4.3.2 Lexical Semantics Measures
Most lexical semantics measures focus on the se-
mantic similarity or relatedness of two words. Since
our task focuses on distinguishing the difference be-
tween word pairs in the same relation category. The
crude relatedness model does not seem to help in our
preliminary experimental study. Instead, we lever-
age the recently proposed polarity-inducing latent
semantic analysis (PILSA) model (Yih et al, 2012),
which specifically estimates the degree of synonymy
and antonymy. This method first forms a signed co-
occurrence matrix using synonyms and antonyms in
a thesaurus and then generalizes it using a low-rank
approximation derived by SVD. Given two words,
the cosine score of their PILSA vectors tend to be
negative if they are antonymous and positive if syn-
onymous. When tested on the Similar:Synonymity
relation, it has a Spearman?s ? = 0.242 and MaxD-
iff accuracy 42.1%, both are better than those of our
directional similarity and lexical pattern models.
5 Model Combination
In order to fully leverage the diverse models pro-
posed in Sec. 4, we experiment with a model combi-
nation approach and conduct a model ablation study.
Performance of the combined and individual models
is evaluated using the test set and compared with ex-
isting systems.
We seek an optimal linear combination of all the
individual models by treating their output as fea-
tures and use a logistic regression learner to learn
the weights6. The training setting is essentially the
same as the one used to learn the lexical pattern
model (Sec. 4.2). For each relation, we treat all the
word pairs in this relation group as positive exam-
ples and all other word pairs as negative ones. Con-
sequently, 79 sets of weights for model combination
are learned in total. The average Spearman?s ? of the
10 training relations is used for selecting the values
of the L1 and L2 regularizers7. Evaluated on the re-
maining 69 relations (i.e., the test set), the average
results of each main relation group and the overall
6Nonlinear methods, such as MART (Friedman, 2001), do
not perform better in our experiments (not reported here).
7We tested 15 combinations, where L1 ? {0, 0.01, 0.1} and
L2 ? {0, 0.001, 0.01, 1, 10}. The parameter setting that gave
the highest Spearman rank correlation coefficient score on the
training set was selected.
1005
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519
Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329
Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303
Contrast -0.049 0.000 0.142 0.206 0.063 0.298 -0.012 -0.032 -0.079 0.268
Attribute 0.037 -0.095 0.044 0.158 0.431 0.198 -0.008 0.016 -0.052 0.406
Non-Attribute -0.070 0.009 0.079 0.098 0.195 0.117 0.036 0.078 -0.093 0.296
Case Relations 0.090 -0.037 -0.011 0.241 0.503 0.288 0.076 -0.075 0.059 0.473
Cause-Purpose -0.011 0.114 0.021 0.183 0.362 0.234 0.044 -0.059 0.038 0.296
Space-Time 0.013 0.035 0.055 0.375 0.439 0.248 0.064 -0.002 -0.018 0.443
Reference 0.142 -0.001 0.028 0.346 0.301 0.119 0.033 -0.123 0.021 0.208
Average 0.018 0.014 0.050 0.229 0.324? 0.235 0.058? -0.010? -0.009? 0.353?
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 30.1 29.0 26.7 39.1 46.7 43.4 59.6 24.7 32.3 51.2
Part-Whole 31.9 35.1 29.4 40.9 43.9 38.1 31.3 29.5 31.0 42.9
Similar 31.5 29.1 37.1 39.8 38.5 38.4 30.8 36.3 34.2 43.3
Contrast 30.4 32.4 38.3 40.9 33.6 42.2 32.3 31.8 30.1 42.8
Attribute 30.2 29.2 31.9 36.5 47.9 38.3 30.7 31.0 28.8 48.3
Non-Attribute 28.9 30.4 36.0 36.8 38.7 36.7 32.3 32.8 27.7 42.6
Case Relations 32.8 29.5 28.2 40.6 54.3 42.2 32.8 25.7 31.0 50.6
Cause-Purpose 30.8 35.4 29.5 36.3 45.3 38.0 30.3 28.1 32.0 41.7
Space-Time 30.6 32.5 31.9 43.2 50.0 39.2 33.2 29.3 30.6 47.7
Reference 35.1 30.0 31.9 41.2 45.7 36.9 30.4 27.2 30.2 42.5
Average 31.2 31.7 32.4 39.4 44.5? 39.2 33.3? 29.8? 30.7? 45.2?
Table 2: Average Spearman?s ? (Top) and MaxDiff accuracy (%) (Bottom) of each major relation group and all 69
testing relations. The best result in each row is highlighted in boldface font. Statistical significance tests are conducted
by comparing each of our systems with the previous best performing system, UTDNB . ? and ? indicate the difference
in the average results is statistically significant with 95% or 99% confidence level, respectively.
results are presented in Table 2. For comparison, we
also show the performance of a random baseline and
the best performing system of each participant in the
SemEval-2012 task.
We draw two conclusions from this table. First,
both of our general relational similarity models, the
directional similarity (DS) and lexical pattern (Pat)
models are fairly strong. The former outperforms
the previous best system UTDNB in both Spear-
man?s ? and MaxDiff accuracy, where the differ-
ences are statistically significant8; the latter has
comparable performance, where the differences are
not statistically significant. In contrast, while the
IsA relation from Probase is exceptionally good
in identifying Class-Inclusion relations, with high
Spearman?s ? = 0.619 and MaxDiff accuracy
8We conducted a paired-t test on the results of each of the
69 relation. The difference is considered statistically significant
if the p-value is less than 0.05.
59.6%, it does not have high correlations with hu-
man judgments in other relations. Like in the case of
Probase Attribute and PILSA, specific word-relation
models individually are not good measures for gen-
eral relational similarity. Second, as expected, com-
bining multiple diverse models (Com) is a robust
strategy, which provides the best overall perfor-
mance. It achieves superior results in both evalua-
tion metrics compared to UTDNB and only a lower
Spearman?s ? value in one of the ten relation groups
(namely, Reference). The differences are statisti-
cally significant with p-value less than 10?3.
In order to understand the interaction among dif-
ferent component models, we conducted an ablation
study by iteratively removing one model from the fi-
nal combination. The weights are re-trained using
the same procedure that finds the best regularization
parameters with the help of training data. Table 3
summarizes the results and compares them with the
1006
Spearman?s ? MaxDiff Accuracy (%)
Relation Group Com. -Attr -IsA -PILSA -DS -Pat Com. -Attr -IsA -PILSA -DS -Pat
Class-Inclusion 0.519 0.557 0.467 0.593 0.490 0.570 51.2 53.7 49.2 54.6 49.3 56.2
Part-Whole 0.329 0.326 0.335 0.331 0.277 0.285 42.9 42.1 42.6 41.8 38.5 42.9
Similar 0.303 0.269 0.302 0.281 0.256 0.144 43.3 41.2 42.7 40.5 40.2 38.9
Contrast 0.268 0.234 0.267 0.289 0.260 0.156 42.8 42.0 42.4 41.5 42.7 38.1
Attribute 0.406 0.409 0.405 0.433 0.164 0.447 48.3 47.8 48.2 49.1 36.9 49.0
Non-Attribute 0.296 0.287 0.296 0.276 0.123 0.283 42.6 42.9 42.6 41.8 36.0 43.0
Case Relations 0.473 0.497 0.470 0.484 0.309 0.498 50.6 52.5 50.2 50.9 42.9 53.2
Cause-Purpose 0.296 0.282 0.299 0.301 0.205 0.296 41.7 41.6 41.6 41.2 36.6 44.1
Space-Time 0.443 0.425 0.443 0.420 0.269 0.431 47.7 47.2 47.7 46.9 40.5 49.5
Reference 0.208 0.238 0.205 0.168 0.102 0.210 42.5 42.3 42.6 41.8 36.1 41.4
Average 0.353 0.348 0.350 0.354 0.238? 0.329 45.2 45.0 44.9? 44.7 39.6? 45.4
Table 3: Average Spearman?s ? and MaxDiff accuracy results of different model combinations. Com indicates combin-
ing all models, where other columns show the results when the specified model is removed. The best result in each row
is highlighted in boldface font. Statistical significance tests are conducted by comparing each ablation configuration
with Com. ? indicates the difference in the average results is statistically significant with 99% confidence level.
original combination model.
Overall, it is clear that the directional similarity
method based on RNNLM vectors is the most crit-
ical component model. Removing it from the fi-
nal combination decreases both the Spearman?s ?
and MaxDiff accuracy by a large margin; both dif-
ferences (Com vs. -DS) are statistically significant.
The Probase IsA model also has an important im-
pact on the performance on the Class-Inclusion re-
lation group. Eliminating the IsA model makes
the overall MaxDiff accuracy statistically signifi-
cantly lower (Com vs. -IsA). Again, the benefits
of incorporating Probase Attribute and PILSA mod-
els are not clear. Removing them from the final
combination lowers the MaxDiff accuracy, but nei-
ther the difference in Spearman?s ? nor MaxDiff
accuracy is statistically significant. Compared to
the RNNLM directional similarity model, the lex-
ical pattern model seems less critical. Removing
it lowers the Similar and Contrast relation groups,
but improves some other relation groups like Class-
Inclusion and Case Relations. The final MaxDiff ac-
curacy becomes slightly higher but the Spearman?s
? drops a little (Com vs. -Pat); neither is statistically
significant.
Notice that the main purpose of the ablation study
is to verify the importance of an individual compo-
nent model when a significant performance drop is
observed after removing it. However, occasionally
the overall performance may go up slightly. Typi-
cally this is due to the fact that some models do not
provide useful signals to a particular relation, but in-
stead introduce more noise. Such effects can often
be alleviated when there are enough quality training
data, which is unfortunately not the case here.
6 Conclusions
In this paper, we presented a system that combines
heterogeneous models based on different informa-
tion sources for measuring relational similarity. Our
two individual general-purpose relational similarity
models, directional similarity and lexical pattern
methods, perform strongly when compared to ex-
isting systems. After incorporating specific word-
relation models, the final system sets a new state-of-
the-art on the SemEval-2012 task 2 test set, achiev-
ing Spearman?s ? = 0.353 and MaxDiff accuracy
45.4% ? resulting in 54.1% and 14.7% relative im-
provement in these two metrics, respectively.
Despite its simplicity, our directional similarity
approach provides a robust model for relational sim-
ilarity and is a critical component in the final sys-
tem. When the lexical pattern model is included, our
overall model combination method can be viewed
as a two-stage learning system. As demonstrated in
our work, with an appropriate regularization strat-
egy, high-quality models can be learned in both
stages. Finally, as we observe from the positive ef-
fect of adding the Probase IsA model, specific word-
relation models can further help improve the system
1007
although they tend to cover only a small number of
relations. Incorporating more such models could be
a steady path to enhance the final system.
In the future, we plan to pursue several research
directions. First, as shown in our experimental re-
sults, the model combination approach does not al-
ways outperform individual models. Investigating
how to select models to combine for each specific re-
lation or relation group individually will be our next
step for improving this work. Second, because the
labeling process of relational similarity comparisons
is inherently noisy, it is unrealistic to request a sys-
tem to correlate human judgments perfectly. Con-
ducting some user study to estimate the performance
ceiling in each relation category may help us focus
on the weaknesses of the final system to enhance
it. Third, it is intriguing to see that the directional
similarity model based on the RNNLM vectors per-
forms strongly, even though the RNNLM training
process is not related to the task of relational sim-
ilarity. Investigating the effects of different vector
space models and proposing some theoretical jus-
tifications are certainly interesting research topics.
Finally, we would like to evaluate the utility our ap-
proach in other applications, such as the SAT anal-
ogy problems proposed by Turney (2006) and ques-
tion answering.
Acknowledgments
We thank Richard Socher for valuable discussions,
Misha Bilenko for his technical advice and anony-
mous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML ?07.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cog-
nitive and psychometric analysis of analogical prob-
lem solving. Recent research in psychology. Springer-
Verlag.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13?47, March.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
J.H. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist, 29(5):1189?
1232.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
degrees of relational similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 356?364, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH, pages 1045?1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for train-
ing large scale neural network language models. In
ASRU.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013.
Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth edi-
tion. Technical report, Linguistic Data Consortium,
Philadelphia.
Ted Pedersen. 2012. Duluth: Measuring degrees of re-
lational similarity with the gloss vector measure of se-
mantic relatedness. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
1008
2012), pages 497?501, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ?10.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP-01, pages 82?90.
Mireya Tovar, J. Alejandro Reyes, Azucena Montes,
Darnes Vilarin?o, David Pinto, and Saul Leo?n. 2012.
BUAP: A first approximation to relational similarity
measuring. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 502?505, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research
(JAIR), 44:533?585.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782?788.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.
Zhu. 2012. Probase: a probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of
Data, pages 481?492, May.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
1009
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601?610,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Computational Approaches to Sentence Completion
Geoffrey Zweig, John C. Platt
Christopher Meek
Christopher J.C. Burges
Microsoft Research
Redmond, WA 98052
Ainur Yessenalina
Cornell University
Computer Science Dept.
Ithaca, NY 14853
Qiang Liu
Univ. of California, Irvine
Info. & Comp. Sci.
Irvine, California 92697
Abstract
This paper studies the problem of sentence-
level semantic coherence by answering SAT-
style sentence completion questions. These
questions test the ability of algorithms to dis-
tinguish sense from nonsense based on a vari-
ety of sentence-level phenomena. We tackle
the problem with two approaches: methods
that use local lexical information, such as the
n-grams of a classical language model; and
methods that evaluate global coherence, such
as latent semantic analysis. We evaluate these
methods on a suite of practice SAT questions,
and on a recently released sentence comple-
tion task based on data taken from five Conan
Doyle novels. We find that by fusing local
and global information, we can exceed 50%
on this task (chance baseline is 20%), and we
suggest some avenues for further research.
1 Introduction
In recent years, standardized examinations have
proved a fertile source of evaluation data for lan-
guage processing tasks. They are valuable for many
reasons: they represent facets of language under-
standing recognized as important by educational ex-
perts; they are organized in various formats designed
to evaluate specific capabilities; they are yardsticks
by which society measures educational progress;
and they affect a large number of people.
Previous researchers have taken advantage of this
material to test both narrow and general language
processing capabilities. Among the narrower tasks,
the identification of synonyms and antonyms has
been studied by (Landauer and Dumais, 1997; Mo-
hammed et al, 2008; Mohammed et al, 2011; Tur-
ney et al, 2003; Turney, 2008), who used ques-
tions from the Test of English as a Foreign Lan-
guage (TOEFL), Graduate Record Exams (GRE)
and English as a Second Language (ESL) exams.
Tasks requiring broader competencies include logic
puzzles and reading comprehension. Logic puzzles
drawn from the Law School Administration Test
(LSAT) and the GRE were studied in (Lev et al,
2004), which combined an extensive array of tech-
niques to solve the problems. The DeepRead sys-
tem (Hirschman et al, 1999) initiated a long line of
research into reading comprehension based on test
prep material (Charniak et al, 2000; Riloff and The-
len, 2000; Wang et al, 2000; Ng et al, 2000).
In this paper, we study a new class of problems
intermediate in difficulty between the extremes of
synonym detection and general question answer-
ing - the sentence completion questions found on
the Scholastic Aptitude Test (SAT). These questions
present a sentence with one or two blanks that need
to be filled in. Five possible words (or short phrases)
are given as options for each blank. All possible an-
swers except one result in a nonsense sentence. Two
examples are shown in Figure 1.
The questions are highly constrained in the sense
that all the information necessary is present in the
sentence itself without any other context. Neverthe-
less, they vary widely in difficulty. The first of these
examples is relatively simple: the second half of the
sentence is a clear description of the type of behavior
characterized by the desired adjective. The second
example is more sophisticated; one must infer from
601
1. One of the characters in Milton Murayama?s
novel is considered because he deliber-
ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)
industrious (E) tyrannical
2. Whether substances are medicines or poisons
often depends on dosage, for substances that are
in small doses can be in large.
(A) useless .. effective
(B) mild .. benign
(C) curative .. toxic
(D) harmful .. fatal
(E) beneficial .. miraculous
Figure 1: Sample sentence completion questions
(Educational-Testing-Service, 2011).
the contrast between medicine and poison that the
correct answer involves a contrast, either useless vs.
effective or curative vs. toxic. Moreover, the first, in-
correct, possibility is perfectly acceptable in the con-
text of the second clause alone; only irrelevance to
the contrast between medicine and poison eliminates
it. In general, the questions require a combination of
semantic and world knowledge as well as occasional
logical reasoning. We study the sentence comple-
tion task because we believe it is complex enough to
pose a significant challenge, yet structured enough
that progress may be possible.
As a first step, we have approached the prob-
lem from two points-of-view: first by exploiting lo-
cal sentence structure, and secondly by measuring
a novel form of global sentence coherence based
on latent semantic analysis. To investigate the use-
fulness of local information, we evaluated n-gram
language model scores, from both a conventional
model with Good-Turing smoothing, and with a re-
cently proposed maximum-entropy class-based n-
gram model (Chen, 2009a; Chen, 2009b). Also
in the language modeling vein, but with potentially
global context, we evaluate the use of a recurrent
neural network language model. In all the language
modeling approaches, a model is used to compute a
sentence probability with each of the potential com-
pletions. To measure global coherence, we propose
a novel method based on latent semantic analysis
(LSA). We find that the LSA based method performs
best, and that both local and global information can
be combined to exceed 50% accuracy. We report re-
sults on a set of questions taken from a collection
of SAT practice exams (Princeton-Review, 2010),
and further validate the methods with the recently
proposed MSR Sentence Completion Challenge set
(Zweig and Burges, 2011).
Our paper thus makes the following contributions:
First, we present the first published results on the
SAT sentence completion task. Secondly, we eval-
uate the effectiveness of both local n-gram informa-
tion, and global coherence in the form of a novel
LSA-based metric. Finally, we illustrate that the lo-
cal and global information can be effectively fused.
The remainder of this paper is organized as fol-
lows. In Section 2 we discuss related work. Section
3 describes the language modeling methods we have
evaluated. Section 4 outlines the LSA-based meth-
ods. Section 5 presents our experimental results. We
conclude with a discussion in Section 6.
2 Related Work
The past work which is most similar to ours is de-
rived from the lexical substitution track of SemEval-
2007 (McCarthy and Navigli, 2007). In this task,
the challenge is to find a replacement for a word or
phrase removed from a sentence. In contrast to our
SAT-inspired task, the original answer is indicated.
For example, one might be asked to find alternates
for match in ?After the match, replace any remain-
ing fluid deficit to prevent problems of chronic de-
hydration throughout the tournament.? Two consis-
tently high-performing systems for this task are the
KU (Yuret, 2007) and UNT (Hassan et al, 2007)
systems. These operate in two phases: first they find
a set of potential replacement words, and then they
rank them. The KU system uses just an N-gram lan-
guage model to do this ranking. The UNT system
uses a large variety of information sources, and a
language model score receives the highest weight.
N-gram statistics were also very effective in (Giu-
liano et al, 2007). That paper also explores the use
of Latent Semantic Analysis to measure the degree
of similarity between a potential replacement and its
context, but the results are poorer than others. Since
the original word provides a strong hint as to the pos-
602
sible meanings of the replacements, we hypothesize
that N-gram statistics are largely able to resolve the
remaining ambiguities. The SAT sentence comple-
tion sentences do not have this property and thus are
more challenging.
Related to, but predating the Semeval lexical sub-
stitution task are the ESL synonym questions pro-
posed by Turney (2001), and subsequently consid-
ered by numerous research groups including Terra
and Clarke (2003) and Pado and Lapata (2007).
These questions are similar to the SemEval task, but
in addition to the original word and the sentence
context, the list of options is provided. Jarmasz and
Szpakowicz (2003) used a sophisticated thesaurus-
based method and achieved state-of-the art perfor-
mance, which is 82%.
Other work on standardized tests includes the syn-
onym and antonym tasks mentioned in Section 1,
and more recent work on a SAT analogy task in-
troduced by (Turney et al, 2003) and extensively
used by other researchers (Veale, 2004; Turney and
Littman, 2005; D. et al, 2009).
3 Sentence Completion via Language
Modeling
Perhaps the most straightforward approach to solv-
ing the sentence completion task is to form the com-
plete sentence with each option in turn, and to eval-
uate its likelihood under a language model. As
discussed in Section 2, this was found be be very
effective in the ranking phase of several SemEval
systems. In this section, we describe the suite of
state-of-the-art language modeling techniques for
which we will present results. We begin with n-
gram models; first a classical n-gram backoff model
(Chen and Goodman, 1999), and then a recently pro-
posed class-based maximum-entropy n-gram model
(Chen, 2009a; Chen, 2009b). N-gram models have
the obvious disadvantage of using a very limited
context in predicting word probabilities. There-
fore we evaluate the recurrent neural net model of
(Mikolov et al, 2010; Mikolov et al, 2011b). This
model has produced record-breaking perplexity re-
sults in several tasks (Mikolov et al, 2011a), and has
the potential to encode sentence-span information in
the network hidden-layer activations. We have also
evaluated the use of parse scores, using an off-the-
shelf stochastic context free grammar parser. How-
ever, the grammatical structure of the alternatives is
often identical. With scores differing only in the fi-
nal non-terminal/terminal rewrites, this did little bet-
ter than chance. The use of other syntactically de-
rived features, for example based on a dependency
parse, are likely to be more effective, but we leave
this for future work.
3.1 Backoff N-gram Language Model
Our baseline model is a Good-Turing smoothed
model trained with the CMU language modeling
toolkit (Clarkson and Rosenfeld, 1997). For the SAT
task, we used a trigram language model trained on
1.1B words of newspaper data, described in Section
5.1. All bigrams occurring at least twice were re-
tained in the model, along with all trigrams occur-
ring at least three times. The vocabulary consisted
of all words occurring at least 100 times in the data,
along with every word in the development or test
sets. This resulted in a 124k word vocabulary and
59M n-grams. For the Conan Doyle data, which we
henceforth refer to as the Holmes data (see Section
5.1), the smaller amount of training data allowed us
to use 4-grams and a vocabulary cutoff of 3. This re-
sulted in 26M n-grams and a 126k word vocabulary.
3.2 Maximum Entropy Class-Based N-gram
Language Model
Word-class information provides a level of abstrac-
tion which is not available in a word-level lan-
guage model; therefore we evaluated a state-of-the-
art class based language model. Model M (Chen,
2009a; Chen, 2009b) is a recently proposed class
based exponential n-gram language model which
has shown improvements across a variety of tasks
(Chen, 2009b; Chen et al, 2009; Emami et al,
2010). The key ideas are the modeling of word n-
gram probabilities with a maximum entropy model,
and the use of word-class information in the defini-
tion of the features. In particular, each word w is
assigned deterministically to a class c, allowing the
n-gram probabilities to be estimated as the product
of class and word parts
P (wi|wi?n+1 . . . wi?2wi?1) =
P (ci|ci?n+1 . . . ci?2ci?1, wi?n+1 . . . wi?2wi?1)
P (wi|wi?n+1 . . . wi?2wi?1, ci).
603
Both components are themselves maximum entropy
n-gram models in which the probability of a word
or class label l given history h is determined by
1
Z exp(
?
k fk(h, l)). The features fk(h, l) used are
the presence of various patterns in the concatena-
tion of hl, for example whether a particular suffix
is present in hl.
3.3 Recurrent Neural Net Language Model
Many of the questions involve long-range depen-
dencies between words. While n-gram models have
no ability to explicitly maintain long-span context,
the recently proposed recurrent neural-net model of
(Mikolov et al, 2010) does. Related approaches
have been proposed by (Sutskever et al, 2011;
Socher et al, 2011). In this model, a set of neu-
ral net activations s(t) is maintained and updated at
each sentence position t. These activations encapsu-
late the sentence history up to the tth word in a real-
valued vector which typically has several hundred
dimensions. The word at position t is represented as
a binary vector w(t) whose length is the vocabulary
size, and with a ?1? in a position uniquely associated
with the word, and ?0? elsewhere. w(t) and s(t) are
concatenated to predict an output distribution over
words, y(t). Updating is done with two weight ma-
trices u and v and nonlinear functions f() and g()
(Mikolov et al, 2011b):
x(t) = [w(t)T s(t ? 1)T ]T
sj(t) = f(
?
i
xi(t)uji)
yk(t) = g(
?
j
sj(t)vkj)
with f() being a sigmoid and g() a softmax:
f(x) =
1
1 + exp(?z)
, g(zm) =
exp(zm)
?
k exp(zk)
The output y(t) is a probability distribution over
words, and the parameters u and v are trained with
back-propagation to minimize the Kullback-Leibler
(KL) divergence between the predicted and observed
distributions. Because of the recurrent connections,
this model is similar to a nonlinear infinite impulse
response (IIR) filter, and has the potential to model
long span dependencies. Theoretical considerations
(Bengio et al, 1994) indicate that for many prob-
lems, this may not be possible, but in practice it is
an empirical question.
4 Sentence Completion via Latent
Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used method for representing
words and documents in a low dimensional vector
space. The method is based on applying singular
value decomposition (SVD) to a matrix W repre-
senting the occurrence of words in documents. SVD
results in an approximation of W by the product
of three matrices, one in which each word is rep-
resented as a low-dimensional vector, one in which
each document is represented as a low dimensional
vector, and a diagonal scaling matrix. The simi-
larity between two words can then be quantified as
the cosine-similarity between their respective scaled
vectors, and document similarity can be measured
likewise. It has been used in numerous tasks, rang-
ing from information retrieval (Deerwester et al,
1990) to speech recognition (Bellegarda, 2000; Coc-
caro and Jurafsky, 1998).
To perform LSA, one proceeds as follows. The
input is a collection of n documents which are ex-
pressed in terms of words from a vocabulary of size
m. These documents may be actual documents such
as newspaper articles, or simply as in our case no-
tional documents such as sentences. Next, a m x n
matrix W is formed. At its simplest, the ijth entry
contains the number of times word i has occurred in
document j - its term frequency or TF value. More
conventionally, the entry is weighted by some no-
tion of the importance of word i, for example the
negative logarithm of the fraction of documents that
contain it, resulting in a TF-IDF weighting (Salton
et al, 1975). Finally, to obtain a subspace represen-
tation of dimension d, W is decomposed as
W ? USV T
where U is m x d, V T is d x n, and S is a d x d diag-
onal matrix. In applications, d << n and d << m;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the rows
of US - which represents the words - behave sim-
ilarly to the original rows of W , in the sense that
the cosine similarity between two rows in US ap-
proximates the cosine similarity between the corre-
604
sponding rows in W . Cosine similarity is defined as
sim(x,y) = x?y?x??y? .
4.1 Total Word Similarity
Perhaps the simplest way of doing sentence comple-
tion with LSA is to compute the total similarity of a
potential answer a with the rest of the words in the
sentence S, and to choose the most related option.
We define the total similarity as:
totsim(a,S) =
?
w?S
sim(a,w)
When the completion requires two words, total sim-
ilarity is the sum of the contributions for both words.
This is our baseline method for using LSA, and one
of the best methods we have found.
4.2 Sentence Reconstruction
Recall that LSA approximates a weighted word-
document matrix W as the product of low rank
matrices U and V along with a scaling matrix S:
W ? USV T . Using singular value decomposition,
this is done so as to minimize the mean square re-
construction error
?
ij Q
2
ij whereQ = W?USV
T .
From the basic definition of LSA, each column ofW
(representing a document) is represented as
Wj = USV Tj , (1)
that is, as a linear combination of the set of basis
functions formed by the columns of US, with the
combination weights specified in V Tj . When a new
document is presented, it is also possible to repre-
sent it in terms of the same basis vectors. Moreover,
we may take the reconstruction error induced by this
representation to be a measure of how consistent the
new document is with the original set of documents
used to determine U S and V (Bellegarda, 2000).
It remains to represent a new document in terms
of the LSA bases. This is done as follows (Deer-
wester et al, 1990; Bellegarda, 2000), again with
the objective of minimizing the reconstruction error.
First, note that since U is column-orthonormal, (1)
implies that
Vj = W Tj US
?1 (2)
Thus, if we notionally index a new document by p,
we proceed by forming a new column (document)
vector Wp using the standard term-weighting, and
then find its LSA-space representation Vp using (2).
We can evaluate the reconstruction quality by insert-
ing the result in (1). The reconstruction error is then
||(UUT ? I)Wp||2
Note that if all the dimensions are retained, the re-
construction error is zero; in the case that only the
highest singular vectors are used, however, it is not.
Due to the fact that the sentences vary in length we
choose the number of retained singular vectors as a
fraction f of the sentence length. If the answer has
n words we use the top nf components. In practice,
a f of 1.2 was selected on the basis of development
set results.
4.3 A LSA N-gram Language Model
In the context of speech recognition, LSA has been
combined with classical n-gram language models
in (Coccaro and Jurafsky, 1998; Bellegarda, 2000).
The crux of this idea is to interpolate an n-gram lan-
guage model probability with one based on LSA,
with the intuition that the standard n-gram model
will do a good job predicting function words, and
the LSA model will do a good job on words pre-
dicted by their long-span context. This logic makes
sense for the sentence completion task as well, mo-
tivating us to evaluate it.
To do this, we adopt the procedure of (Coccaro
and Jurafsky, 1998), using linear interpolation be-
tween the n-gram and LSA probabilities:
p(w|history) =
?png(w|history) + (1 ? ?)plsa(w|history)
The probability of a word given its history is com-
puted by the LSA model in the following way. Let h
be the sum of all the LSA word vectors in the his-
tory. Let m be the smallest cosine similarity be-
tween h and any word in the vocabulary V : m =
minw?V sim(h,w). The probability of a word w in
the context of history h is given by
Plsa(w|h) =
sim(h,w) ? m
?
q?V (sim(h, q) ? m)
Since similarity can be negative, subtracting the
minimum (m) ensures that all the estimated prob-
abilities are between 0 and 1.
605
4.4 Improving Efficiency and Expressiveness
Given the basic framework described above, a num-
ber of enhancements are possible. In terms of ef-
ficiency, recall that it is necessary to perform SVD
on a term-document matrix. The data we used was
grouped into paragraph ?documents,? of which there
were over 27 million, with 2.6 million unique words.
While the resulting matrix is highly sparse, it is nev-
ertheless impractical to perform SVD. We overcome
this difficulty in two ways. First, we restrict the set
of documents used to those which are ?relevant? to
a given test set. This is done by requiring that a doc-
ument contain at least one of the potential answer-
words. Secondly, we restrict the vocabulary to the
set of words present in the test set. For the sentence-
reconstruction method of Section 4.2, we have found
it convenient to do data selection per-sentence.
To enhance the expressive power of LSA, the term
vocabulary can be expanded from unigrams to bi-
grams or trigrams of words, thus adding information
about word ordering. This was also used in the re-
construction technique.
5 Experimental Results
5.1 Data Resources
We present results with two datasets. The first is
taken from 11 Practice Tests for the SAT & PSAT
2011 Edition (Princeton-Review, 2010). This book
contains eleven practice tests, and we used all the
sentence completion questions in the first five tests
as a development set, and all the questions in the last
six tests as the test set. This resulted in sets with 95
and 108 questions respectively. Additionally, we re-
port results on the recently released MSR Sentence
Completion Challenge (Zweig and Burges, 2011).
This consists of a set of 1, 040 sentence completion
questions based on sentences occurring in five Co-
nan Doyle Sherlock Holmes novels, and is identical
in format to the SAT questions. Due to the source of
this data, we refer to it as the Holmes data.
To train models, we have experimented with a
variety of data sources. Since there is no publi-
cally available collection of SAT questions suitable
to training, our methods have all relied on unsu-
pervised data. Early on, we ran a set of experi-
ments to determine the relevance of different types
of data. Thinking that data from an encyclopedia
Data Dev % Correct Test % Correct
Encarta 26 33
Wikipedia 32 31
LA Times 39 42
Table 1: Effectiveness of different types of training data.
might be useful, we evaluated an electronic version
of the 2003 Encarta encyclopedia, which has ap-
proximately 29M words. Along similar lines, we
used a collection of Wikipedia articles consisting of
709M words. This data is the entire Wikipedia as of
January 2011, broken down into sentences, with fil-
tering to remove sentences consisting of URLs and
Wiki author comments. Finally, we used a com-
mercial newspaper dataset consisting of all the Los
Angeles Times data from 1985 to 2002, containing
about 1.1B words. These data sources were evalu-
ated using the baseline n-gram LM approach of Sec-
tion 3.1. Initial experiments indicated that that the
Los Angeles Times data is best suited to this task
(see Table 1), and our SAT experiments use this
source. For the MSR Sentence Completion data,
we obtained the training data specified in (Zweig
and Burges, 2011), consisting of approximately 500
19th-century novels available from Project Guten-
berg, and comprising 48M words.
5.2 Human Performance
To provide human benchmark performance, we
asked six native speaking high school students and
five graduate students to answer the questions on the
development set. The high-schoolers attained 87%
accuracy and the graduate students 95%. Zweig and
Burges (2011) cite a human performance of 91%
on the Holmes data. Statistics from a large cross-
section of the population are not available. As a fur-
ther point of comparison, we note that chance per-
formance is 20%.
5.3 Language Modeling Results
Table 2 summarizes our language modeling results
on the SAT data. With the exception of the base-
line backoff n-gram model, these techniques were
too computationally expensive to utilize the full Los
Angeles Times corpus. Instead, as with LSA, a ?rel-
evant? corpus was selected of the sentences which
contain at least one answer option from either the
606
Method Data (Dev / Test) Dev Test
3-gram GT 1.1B / 1.1B 39% 42%
Model M 193M / 236M 35 41
RNN 36M / 44M 37 42
LSA-LM 293M / 358 M 48 44
Table 2: Performance of language modeling methods on
SAT questions.
Method Dev ppl Dev Test ppl Test
3-gram GT 195 36% 190 44%
Model M 178 36 175 42
RNN 147 37 144 42
Table 3: Performance of language modeling methods us-
ing identical training data and vocabularies.
development or test set. Separate subsets were made
for development and test data. This data was further
sub-sampled to obtain the training set sizes indicated
in the second column. For the LSA-LM, an interpo-
lation weight of 0.1 was used for the LSA score, de-
termined through optimization on the development
set. We see from this table that the language models
perform similarly and achieve just above 40% on the
test set.
To make a more controlled comparison that nor-
malizes for the amount of training data, we have
trained Model M, and the Good-Turing model on
the same data subset as the RNN, and with the same
vocabulary. In Table 3, we present perplexity re-
sults on a held-out set of dev/test-relevant Los Ange-
les Times data, and performance on the actual SAT
questions. Two things are notable. First, the re-
current neural net has dramatically lower perplexity
than the other methods. This is consistent with re-
sults in (Mikolov et al, 2011a). Secondly, despite
the differences in perplexity, the methods show little
difference on SAT performance. Because Model M
was not better, only uses n-gram context, and was
used in the construction of the Holmes data (Zweig
and Burges, 2011), we do not consider it further.
5.4 LSA Results
Table 4 presents results for the methods of Sections
4.1 and 4.2. Of all the methods in isolation, the sim-
ple approach of Section 4.1 - to use the total cosine
similarity between a potential answer and the other
words in the sentence - has performed best. The ap-
Method Dev Test
Total Word Similarity 46% 46%
Reconstruction Error 53 41
Table 4: SAT performance of LSA based methods.
Method Test
3-input LSA 46%
LSA + Good-Turing LM 53
LSA + Good-Turing LM + RNN 52
Table 5: SAT test set accuracy with combined methods.
proach of using reconstruction error performed very
well on the development set, but unremarkably on
the test set.
5.5 Combination Results
A well-known trick for obtaining best results from
a machine learning system is to combine a set of
diverse methods into a single ensemble (Dietterich,
2000). We use ensembles to get the highest accuracy
on both of our data sets.
We use a simple linear combination of the out-
puts of the other models discussed in this paper. For
the LSA model, the linear combination has three in-
puts: the total word similarity, the cosine similarity
between the sum of the answer word vectors and the
sum of the rest of sentence?s word vectors, and the
number of out-of-vocabulary terms in the answer.
Each additional language model beyond LSA con-
tributes an additional input: the probability of the
sentence under that language model.
We train the parameters of the linear combination
on the SAT development set. The training minimizes
a loss function of pairs of answers: one correct and
one incorrect fill-in from the same question. We use
the RankNet loss function (Burges et al, 2005):
min
~w
f(~w ? (~x ? ~y)) + ?||~w||2
where ~x are the input features for the incorrect an-
swer, ~y are the features for the correct answer, ~w
are the weights for the combination, and f(z) =
log(1 + exp(z)). We tune the regularizer via 5-
fold cross validation, and minimize the loss using
L-BFGS (Nocedal and Wright, 2006). The results
on the SAT test set for combining various models
are shown in Table 5.
607
5.6 Holmes Data Results
To measure the robustness of our approaches, we
have applied them to the MSR Sentence Completion
set (Zweig and Burges, 2011), termed the Holmes
data. In Table 6, we present the results on this set,
along with the comparable SAT results. Note that
the latter are derived from models trained with the
Los Angeles Times data, while the Holmes results
are derived from models trained with 19th-century
novels. We see from this table that the results are
similar across the two tasks. The best performing
single model is LSA total word similarity.
For the Holmes data, combining the models out-
performs any single model. We train the linear com-
bination function via 5-fold cross-validation: the
model is trained five times, each time on 3/5 of the
data, the regularization tuned on 1/5 of the data, and
tested on 1/5. The test results are pooled across all
5 folds and are shown in Table 6. In this case, the
best combination is to blend LSA, the Good-Turing
language model, and the recurrent neural network.
6 Discussion
To verify that the differences in accuracy between
the different algorithms are not statistical flukes, we
perform a statistical significance test on the out-
puts of each algorithm. We use McNemar?s test,
which is a matched test between two classifiers (Di-
etterich, 1998). We use the False Discovery Rate
method (Benjamini and Hochberg, 1995) to control
the false positive rate caused by multiple tests. If
we allow 2% of our tests to yield incorrectly false
results, then for the SAT data, the combination of
the Good-Turing smoothed language model with an
LSA-based global similarity model (52% accuracy)
is better that the baseline alone (42% accuracy).
Secondly, for the Holmes data, we can state that
LSA total similarity beats the recurrent neural net-
work, which in turn is better than the baseline n-
gram model. The combination of all three is sig-
nificantly better than any of the individual models.
To better understand the system performance and
gain insight into ways of improving it, we have ex-
amined the system?s errors. Encouragingly, one-
third of the errors involve single-word questions
which test the dictionary definition of a word. This
is done either by stating the definition, or provid-
Method SAT Holmes
Chance 20% 20%
GT N-gram LM 42 39
RNN 42 45
LSA Total Similarity 46 49
Reconstruction Error 41 41
LSA-LM 44 42
Combination 53 52
Human 87 to 95 91
Table 6: Performance of methods on the MSR Sentence
Completion Challenge, contrasted with SAT test set.
ing a stereotypical use of the word. An example of
the first case is: ?Great artists are often prophetic
(visual): they perceive what we cannot and antici-
pate the future long before we do.? (The system?s
incorrect answer is in parentheses.) An example
of the second is: ?One cannot help but be moved
by Theresa?s heartrending (therapeutic) struggle to
overcome a devastating and debilitating accident.?
At the other end of the difficulty spectrum are
questions involving world knowledge and/or logical
implications. An example requiring both is, ?Many
fear that the ratification (withdrawal) of more le-
nient tobacco advertising could be detrimental to
public health.? About 40% of the errors require this
sort of general knowledge to resolve. Based on our
analysis, we believe that future research could prof-
itably exploit the structured information present in
a dictionary. However, the ability to identify and
manipulate logical relationships and embed world
knowledge in a manner amenable to logical manip-
ulation may be necessary for a full solution. It is
an interesting research question if this could be done
implicitly with a machine learning technique, for ex-
ample recurrent or recursive neural networks.
7 Conclusion
In this paper we have investigated methods for
answering sentence-completion questions. These
questions are intriguing because they probe the abil-
ity to distinguish semantically coherent sentences
from incoherent ones, and yet involve no more con-
text than the single sentence. We find that both local
n-gram information and an LSA-based global coher-
ence model do significantly better than chance, and
that they can be effectively combined.
608
References
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks, 5(2):157 ?166.
Y. Benjamini and Y. Hochberg. 1995. Controlling the
fase discovery rate: a practical and powerful approach
to multiple testing. J. Royal Statistical Society B,
53(1):289?300.
C. Burges, T. Shaked., E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proc. ICML, pages 89?
96.
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo
Braz, Benjamin Garrett, Margaret Kosmala, Tomer
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun,
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa
Zorn. 2000. Reading comprehension programs in
a statistical-language-processing class. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 1?5. Asso-
ciation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech and Language, 13(4):359?393.
S. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and
A. Sethy. 2009. Scaling shrinkage-based language
models. In ASRU.
S. Chen. 2009a. Performance prediction for exponential
language models. In NAACL-HLT.
S. Chen. 2009b. Shrinking exponential language models.
In NAACL-HLT.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings ESCA Eurospeech,
http://www.speech.cs.cmu.edu/SLM/toolkit.html.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, ICSLP.
Bollegala D., Matsuo Y., and Ishizuka M. 2009. Measur-
ing the similarity between implicit semantic relations
from the web. InWorldWideWeb Conference (WWW).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
T.G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
T.G. Dietterich. 2000. Ensemble methods in machine
learning. In International Workshop on Multiple Clas-
sifier Systems, pages 1?15. Springer-Verlag.
Educational-Testing-Service. 2011.
https://satonlinecourse.collegeboard.com/sr/digital assets/
assessment/pdf/0833a611-0a43-10c2-0148-
cc8c0087fb06-f.pdf.
A. Emami, S. Chen, A. Ittycheriah, H. Soltau, and
B. Zhao. 2010. Decoding with shrinkage-based lan-
guage models. In Interspeech.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 145?148, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, SemEval ?07,
pages 410?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lynette Hirschman, Mark Light, Eric Breck, and John D.
Burger. 1999. Deep read: A reading comprehension
system. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and
Roger Levy. 2004. Solving logic puzzles: from ro-
bust processing to precise semantics. In Proceedings
of the 2nd Workshop on Text Meaning and Interpreta-
tion, pages 9?16. Association for Computational Lin-
guistics.
Jarmasz M. and Szpakowicz S. 2003. Roget?s thesaurus
and semantic similarity. In Recent Advances in Natu-
ral Language Processing (RANLP).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
609
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas
Burget, and Jan Cernocky. 2011a. Empirical evalua-
tion and combination of advanced language modeling
techniques. In Proceedings of Interspeech 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng
Kwan. 2000. A machine learning approach to answer-
ing questions for reading comprehension tests. In Pro-
ceedings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the 38th
Annual Meeting of the Association for Computational
Linguistics - Volume 13, EMNLP ?00, pages 124?132.
J. Nocedal and S. Wright. 2006. Numerical Optimiza-
tion. Springer-Verlag.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33 (2), pages 161?199.
Princeton-Review. 2010. 11 Practice Tests for the SAT
& PSAT, 2011 Edition. The Princeton Review.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehension
tests. In Proceedings of the 2000 ANLP/NAACL Work-
shop on Reading comprehension tests as evaluation for
computer-based language understanding sytems - Vol-
ume 6, ANLP/NAACL-ReadingComp ?00, pages 13?
19.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In Proceedings of the 2011 International Con-
ference on Machine Learning (ICML-2011).
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural networks.
In Proceedings of the 2011 International Conference
on Machine Learning (ICML-2011).
E. Terra and C. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning (ECML).
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
T. Veale. 2004. Wordnet sits the sat: A knowledge-based
approach to lexical analogy. In European Conference
on Artificial Intelligence (ECAI).
W. Wang, J. Auer, R. Parasuraman, I. Zubarev,
D. Brandyberry, and M. P. Harper. 2000. A ques-
tion answering system developed as a project in a
natural language processing course. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 28?35.
Deniz Yuret. 2007. Ku: word sense disambiguation
by substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 207?213, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Geoffrey Zweig and Christopher J.C. Burges. 2011. The
Microsoft Research sentence completion challenge.
Technical Report MSR-TR-2011-129, Microsoft.
610
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 29?36,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Challenge Set for Advancing Language Modeling
Geoffrey Zweig and Chris J.C. Burges
Microsoft Research
Redmond, WA 98052
Abstract
In this paper, we describe a new, publicly
available corpus intended to stimulate re-
search into language modeling techniques
which are sensitive to overall sentence coher-
ence. The task uses the Scholastic Aptitude
Test?s sentence completion format. The test
set consists of 1040 sentences, each of which
is missing a content word. The goal is to select
the correct replacement from amongst five al-
ternates. In general, all of the options are syn-
tactically valid, and reasonable with respect to
local N-gram statistics. The set was gener-
ated by using an N-gram language model to
generate a long list of likely words, given the
immediate context. These options were then
hand-groomed, to identify four decoys which
are globally incoherent, yet syntactically cor-
rect. To ensure the right to public distribution,
all the data is derived from out-of-copyright
materials from Project Gutenberg. The test
sentences were derived from five of Conan
Doyle?s Sherlock Holmes novels, and we pro-
vide a large set of Nineteenth and early Twen-
tieth Century texts as training material.
1 Introduction
Perhaps beginning with Claude Shannon?s use of
N-gram statistics to compute the perplexity of let-
ter sequences (Shannon and Weaver, 1949), N-gram
models have grown to be the most commonly used
type of language model in human language tech-
nologies. At the word level, N-gram modeling tech-
niques have been extensively refined, with state-
of-the-art techniques based on smoothed N-gram
counts (Kneser and Ney, 1995; Chen and Good-
man, 1999), multi-layer perceptrons (Schwenk and
Gauvain, 2002; Schwenk, 2007) and maximum-
entropy models (Rosenfeld, 1997; Chen, 2009a;
Chen, 2009b). Trained on large amounts of data,
these methods have proven very effective in both
speech recognition and machine translation applica-
tions.
Concurrent with the refinement of N-grammodel-
ing techniques, there has been an important stream
of research focused on the incorporation of syntac-
tic and semantic information (Chelba and Jelinek,
1998; Chelba and Jelinek, 2000; Rosenfeld et al,
2001; Yamada and Knight, 2001; Khudanpur and
Wu, 2000; Wu and Khudanpur, 1999). Since in-
tuitively, language is about expressing meaning in
a highly structured syntactic form, it has come as
something of a surprise that the improvements from
these methods have been modest, and the methods
have yet to be widely adopted in non-research sys-
tems.
One explanation for this is that the tasks to which
language modeling has been most extensively ap-
plied are largely soluble with local information. In
the speech recognition application, there is a fun-
damental confluence of acoustic and linguistic in-
formation, and the language model can be thought
of as resolving ambiguity only between acoustically
confusable words (Printz and Olsen, 2002). Since
words which are acoustically similar, e.g. ?bill? and
?spill? usually appear in very different textual con-
texts, the local information of an N-gram language
model may be adequate to distinguish them. To a
lesser degree, in a machine translation application,
29
1. One of the characters in Milton Murayama?s
novel is considered because he deliber-
ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)
industrious (E) tyrannical
2. Whether substances are medicines or poisons
often depends on dosage, for substances that are
in small doses can be in large.
(A) useless .. effective
(B) mild .. benign
(C) curative .. toxic
(D) harmful .. fatal
(E) beneficial .. miraculous
Figure 1: Sample sentence completion questions
(Educational-Testing-Service, 2011).
the potential phrase translations may be similar in
meaning and local information may again suffice to
make a good selection.
In this paper, we present a language processing
corpus which has been explicitly designed to be non-
solvable using purely N-gram based methods, and
which instead requires some level of semantic pro-
cessing. To do this, we draw inspiration from the
standardized testing paradigm, and propose a sen-
tence completion task along the lines of that found
in the widely used Scholastic Aptitude Test. In this
type of question, one is given a sentence with one or
two words removed, and asked to select from among
a set of five possible insertions. Two examples of
SAT test questions are shown in Figure 1.
As can be seen, the options available all make
sense from the local N-gram point of view, and are
all syntactically valid; only semantic considerations
allow the correct answer to be distinguished. We
believe this sort of question is useful for two key
reasons: first, its full solution will require language
modeling techniques which are qualitatively differ-
ent than N-grams; and secondly, the basic task for-
mulation has been externally determined and is a
widely used method for assessing human abilities.
Unfortunately, to date no publicly available corpus
of such questions has been released.
The contribution of this work is to release a public
corpus of sentence completion questions designed to
stimulate research in language modeling technology
which moves beyond N-grams to explicitly address
global sentence coherence. The corpus is based
purely on out-of-copyright data from Project Guten-
berg, thus allowing us to distribute it. The test ques-
tions consist of sentences taken from five Sherlock
Holmes novels. In each, a word has been removed,
and the task is to choose from among five alterna-
tives. One of the options is the original word, and the
other four ?decoys? have been generated from an N-
gram language model using local context. Sampling
from an N-gram model is done to generate alternates
which make sense locally, but for which there is no
other reason to expect them to make sense globally.
To ensure that synonyms of the correct answer are
not present, and that the options are syntactically
reasonable, the decoys have been hand selected from
among a large number of possibilities suggested by
the N-gram model. The training data consists of
approximately 500 out-of-copyright Nineteenth and
early Twentieth century novels, also from Project
Gutenberg.
We expect that the successful development of
models of global coherence will be useful in a va-
riety of tasks, including:
? the interactive generation of sentence comple-
tion questions for vocabulary tutoring applica-
tions;
? proof-reading;
? automated grading of essays and other student
work; and
? sentence generation in free-form dialog appli-
cations.
The remainder of this paper is organized as fol-
lows. In Section 2, we describe the process by which
we made the corpus. Section 3 provides guidance
as to the proper use of the data. In Section 4, we
present baseline results using several simple auto-
mated methods for answering the questions. Finally,
in Section 5, we discuss related work.
2 The Question Generation Process
Question generation was done in two steps. First,
a candidate sentence containing an infrequent word
30
was selected, and alternates for that word were auto-
matically determined by sampling with an N-gram
language model. The N-gram model used the im-
mediate history as context, thus resulting in words
that may ?look good? locally, but for which there
is no a-priori reason to expect them to make sense
globally. In the second step, we eliminated choices
which are obviously incorrect because they consti-
tute grammatical errors. Choices requiring semantic
knowledge and logical inference were preferred, as
described in the guidelines, which we give in Sec-
tion 3. Note that an important desideratum guid-
ing the data generation process was requiring that
a researcher who knows exactly how the data was
created, including knowing which data was used to
train the language model, should nevertheless not be
able to use that information to solve the problem.
We now describe the data that was used, and then
describe the two steps in more detail.
2.1 Data Used
Seed sentences were selected from five of Co-
nan Doyle?s Sherlock Holmes novels: The Sign of
Four (1890), The Hound of the Baskervilles (1892),
The Adventures of Sherlock Holmes (1892), The
Memoirs of Sherlock Holmes (1894), and The Val-
ley of Fear (1915). Once a focus word within
the sentence was selected, alternates to that word
were generated using an N-gram language model.
This model was trained on approximately 540 texts
from the Project Gutenberg collection, consisting
mainly of 19th century novels. Of these 522 had
adequate headers attesting to lack of copyright,
and they are now available at the Sentence Com-
pletion Challenge website http://research.
microsoft.com/en-us/projects/scc/.
2.2 Automatically Generating Alternates
Alternates were generated for every sentence con-
taining an infrequent word. A state-of-the-art class-
based maximum entropy N-gram model (Chen,
2009b) was used to generate the alternates. Ide-
ally, these alternates would be generated according
to P (alternate|remainder of sentence). This can
be done by computing the probability of the com-
pleted sentence once for every possible vocabulary
word, and then normalizing and sampling. However,
the normalization over all words is computationally
expensive, and we have used a procedure based on
sampling based on the preceding two word history
only, and then re-ordering based on a larger context.
The following procedure was used:
1. Select a focus word with overall frequency less
than 10?4. For example, we might select ?ex-
traordinary? in ?It is really the most extraordi-
nary and inexplicable business.?
2. Use the two-word history immediately preced-
ing the selected focus word to predict alter-
nates. We sampled 150 unique alternates at this
stage, requiring that they all have frequency
less than 10?4. For example, ?the most? pre-
dicts ?handsome? and ?luminous.?
3. If the original (correct) sentence has a better
score than any of these alternates, reject the
sentence.
4. Else, score each option according to how well it
and its immediate predecessor predict the next
word. For example, the probability of ?and?
following ?most handsome? might be 0.012.
5. Sort the predicted words according to this
score, and retain the top 30 options.
In step 3, omitting questions for which the correct
sentence is the best makes the set of options more
difficult to solve with a language model alone. How-
ever, by allowing the correct sentence to potentially
fall below the set of alternates retained, an opposite
bias is created: the language model will tend to as-
sign a lower score to the correct option than to the
alternates (which were chosen by virtue of scoring
well). We measured the bias by performing a test on
the 1,040 test sentences using the language model,
and choosing the lowest scoring candidate as the an-
swer. This gave an accuracy of 26% (as opposed to
31%, found by taking the highest scoring candidate:
recall that a random choice would give 20% in ex-
pectation). Thus although there is some remaining
bias for the answer to be low scoring, it is small.
When a language model other than the precise one
used to generate the data is used, the score reversal
test yielded 17% correct. The correct polarity gave
39%. If, however, just the single score used to do
the sort in the last step is used (i.e. the probability
31
of the immediate successor alone), then the lowest
scoring alternate is correct about 38% of the time -
almost as much as the language model itself. The
use of the word score occurring two positions af-
ter the focus also achieves 38%, though a positive
polarity is beneficial here. Combined, these scores
achieve about 43%. Neither is anywhere close to
human performance. We are currently evaluating
a second round of test questions, in which we still
sample options based on the preceding history, but
re-order them according the the total sentence prob-
ability P (w1 . . . wN ).
The overall procedure has the effect of providing
options which are both well-predicted by the imme-
diate history, and predictive of the immediate future.
Since in total the procedure uses just four consec-
utive words, it cannot be expected to provide glob-
ally coherent alternates. However, sometimes it does
produce synonyms to the correct word, as well as
syntactically invalid options, which must be weeded
out. For this, we examine the alternates by hand.
2.3 Human Grooming
The human judges picked the best four choices of
impostor sentences from the automatically gener-
ated list of thirty, and were given the following in-
structions:
1. All chosen sentences should be grammatically
correct. For example: He dances while he ate
his pipe would be illegal.
2. Each correct answer should be unambiguous.
In other words, the correct answer should al-
ways be a significantly better fit for that sen-
tence than each of the four impostors; it should
be possible to write down an explanation as to
why the correct answer is the correct answer,
that would persuade most reasonable people.
3. Sentences that might cause offense or contro-
versy should be avoided.
4. Ideally the alternatives will require some
thought in order to determine the correct an-
swer. For example:
? Was she his [ client | musings | discomfi-
ture | choice | opportunity ] , his friend ,
or his mistress?
would constitute a good test sentence. In order
to arrive at the correct answer, the student must
notice that, while ?musings? and ?discomfi-
ture? are both clearly wrong, the terms friend
and mistress both describe people, which there-
fore makes client a more likely choice than
choice or opportunity.
5. Alternatives that require understanding proper-
ties of entities that are mentioned in the sen-
tence are desirable. For example:
? All red-headed men who are above the age
of [ 800 | seven | twenty-one | 1,200 |
60,000 ] years , are eligible.
requires that the student realize that a man can-
not be seven years old, or 800 or more. How-
ever, such examples are rare: most often, arriv-
ing at the answer will require thought, but not
detailed entity knowledge, such as:
? That is his [ generous | mother?s | suc-
cessful | favorite | main ] fault , but on
the whole he?s a good worker.
6. Dictionary use is encouraged, if necessary.
7. A given sentence from the set of five novels
should only be used once. If more than one
focus word has been identified for a sentence
(i.e. different focuses have been identified, in
different positions), choose the set of sentences
that generates the best challenge, according to
the above guidelines.
Note that the impostors sometimes constitute a
perfectly fine completion, but that in those cases, the
correct completion is still clearly identifiable as the
most likely completion.
2.4 Sample Questions
Figure 2 shows ten examples of the Holmes
derived questions. The full set is available
at http://research.microsoft.com/
en-us/projects/scc/.
3 Guidelines for Use
It is important for users of this data to realize the fol-
lowing: since the test data was taken from five 19th
century novels, the test data itself is likely to occur in
32
1) I have seen it on him , and could to it.
a) write b) migrate c) climb d) swear e) contribute
2) They seize him and use violence towards him in order to make him sign some papers to make
over the girl?s of which he may be trustee to them.
a) appreciation b) activity c) suspicions d) administration e) fortune
3) My morning?s work has not been , since it has proved that he has the very strongest
motives for standing in the way of anything of the sort.
a) invisible b) neglected c) overlooked d) wasted e) deliberate
4) It was furred outside by a thick layer of dust , and damp and worms had eaten through the wood
, so that a crop of livid fungi was on the inside of it.
a) sleeping b) running c) resounding d) beheaded e) growing
5) Presently he emerged , looking even more than before.
a) instructive b) reassuring c) unprofitable d) flurried e) numerous
6) We took no to hide it.
a) fault b) instructions c) permission d) pains e) fidelity
7) I stared at it , not knowing what was about to issue from it.
a) afterwards b) rapidly c) forever d) horror-stricken e) lightly
8) The probability was , therefore , that she was the truth , or , at least , a part of the truth.
a) addressing b) telling c) selling d) surveying e) undergoing
9) The furniture was scattered about in every direction , with dismantled shelves and open drawers
, as if the lady had hurriedly them before her flight.
a) warned b) rebuked c) assigned d) ransacked e) taught
10) The sun had set and was settling over the moor.
a) dusk b) mischief c) success d) disappointment e) laughter
Figure 2: The first ten questions from the Holmes Corpus.
the index of most Web search engines, and in other
large scale data-sets that were constructed from web
data (for example, the Google N-gram project). For
example, entering the string That is his fault , but on
the whole he?s a good worker (one of the sentence
examples given above, but with the focus word re-
moved) into the Bing search engine results in the
correct (full) sentence at the top position. It is im-
portant to realize that researchers may inadvertently
get better results than truly warranted because they
have used data that is thus tainted by the test set.
To help prevent any such criticism from being lev-
eled at a particular publication, we recommend than
in any set of published results, the exact data used
for training and validation be specified. The train-
ing data provided on our website may also be con-
sidered ?safe? and useful for making comparisons
across sites.
4 Baseline Results
4.1 A Simple 4-gram model
As a sanity check we constructed a very simple N-
gram model as follows: given a test sentence (with
the position of the focus word known), the score for
that sentence was initialized to zero, and then incre-
33
mented by one for each bigram match, by two for
each trigram match, and by three for each 4-gram
match, where a match means that the N-gram in
the test sentence containing the focus word occurs
at least once in the background data. This simple
method achieved 34% correct (compared to 20% by
random choice) on the test set.
4.2 Smoothed N-gram model
As a somewhat more sophisticated baseline, we use
the CMU language modeling toolkit 1 to build a 4-
gram language model using Good-Turing smooth-
ing. We kept all bigrams and trigrams occurring
in the data, as well as four-grams occurring at least
twice. We used a vocabulary of the 126k words that
occurred five or more times, resulting in a total of
26M N-grams. Sentences were ordered according to
their probability according to the language model:
P (w1 . . . wN ). This improved by 5% absolute on
the simple baseline to achieve 39% correct.
4.3 Latent Semantic Analysis Similarity
As a final benchmark, we present scores for a novel
method based on latent semantic analysis. In this
approach, we treated each sentence in the training
data as a ?document? and performed latent semantic
analysis (Deerwester et al, 1990) to obtain a 300
dimensional vector representation of each word in
the vocabulary. Denoting two words by their vectors
x,y, their similarity is defined as the cosine of the
angle between them:
sim(x,y) =
x ? y
? x ?? y ?
.
To decide which option to select, we computed the
average similarity to every other word in the sen-
tence, and then output the word with the greatest
overall similarity. This results in our best baseline
performance, at 49% correct.
4.4 Benchmark Summary
Table 1 summarizes our benchmark study. First, for
reference, we had an unaffiliated human answer a
random subset of 100 questions. Ninety-one per-
cent were answered correctly, showing that scores
in the range of 90% are reasonable to expect. Sec-
ondly, we tested the performance of the same model
1http://www.speech.cs.cmu.edu/SLM/toolkit.html
Method % Correct (N=1040)
Human 91
Generating Model 31
Smoothed 3-gram 36
Smoothed 4-gram 39
Positional combination 43
Simple 4-gram 34
Average LSA Similarity 49
Table 1: Summary of Benchmarks
(Model M) that was used to generate the data. Be-
cause this model output alternates that it assigns
high-probability, there is a bias against it, and it
scored 31%. Smoothed 3 and 4-gram models built
with the CMU toolkit achieved 36 to 39 percent. Re-
call that the sampling process introduced some bias
into the word scores at specific positions relative to
the focus word. Exploiting the negative bias induced
on the immediately following word, and combin-
ing it with the score of the word two positions in
the future, we were able to obtain 43%. The sim-
ple 4-gram model described earlier did somewhat
worse than the other N-gram language models, and
the LSA similarity model did best with 49%. As
a further check on this data, we have run the same
tests on 108 sentence completion questions from a
practice SAT exam (Princeton Review, 11 Practice
Tests for the SAT & PSAT, 2011 Edition). To train
language models for the SAT question task, we used
1.2 billion words of Los Angeles Times data taken
from the years 1985 through 2002. Results for the
SAT data are similar, with N-gram language models
scoring 42-44% depending on vocabulary size and
smoothing, and LSA similarity attaining 46%.
These results indicate that the ?Holmes? sentence
completion set is indeed a challenging problem, and
has a level of difficulty roughly comparable to that
of SAT questions. Simple models based on N-gram
statistics do quite poorly, and even a relatively so-
phisticated semantic-coherence model struggles to
beat the 50% mark.
5 Related Work
The past work which is most similar to ours is de-
rived from the lexical substitution track of SemEval-
2007 (McCarthy and Navigli, 2007). In this task,
the challenge is to find a replacement for a word or
34
phrase removed from a sentence. In contrast to our
SAT-inspired task, the original answer is indicated.
For example, one might be asked to find replace-
ments for match in ?After the match, replace any re-
maining fluid deficit to prevent problems of chronic
dehydration throughout the tournament.? Scoring
is done by comparing a system?s results with those
produced by a group of human annotators (not un-
like the use of multiple translations in machine trans-
lation). Several forms of scoring are defined us-
ing formulae which make the results impossible to
compare with correct/incorrect multiple choice scor-
ing. Under the provided scoring metrics, two con-
sistently high-performing systems in the SemEval
2007 evaluations are the KU (Yuret, 2007) and UNT
(Hassan et al, 2007) systems. These operate in two
phases: first they find a set of potential replacement
words, and then they rank them. The KU system
uses just an N-gram language model to do this rank-
ing. The UNT system uses a large variety of infor-
mation sources, each with a different weight. A lan-
guage model is used, and this receives the highest
weight. N-gram statistics were also very effective -
according to one of the scoring paradigms - in (Giu-
liano et al, 2007); as a separate entry, this paper fur-
ther explored the use of Latent Semantic Analysis
to measure the degree of similarity between a poten-
tial replacement and its context, but the results were
poorer than others. Since the original word provides
a strong hint as to the possible meanings of the re-
placements, we hypothesize that N-gram statistics
are largely able to resolve the remaining ambigui-
ties, thus accounting for the good performance of
these methods on this task. The Holmes data does
not have this property and thus may be more chal-
lenging.
ESL synonym questions were studied by Turney
(2001), and subsequently considered by numerous
research groups including Terra and Clarke (2003)
and Pado and Lapata (2007). These questions are
easier than the SemEval task because in addition to
the original word and the sentence context, the list
of options is provided. For example, one might be
asked to identify a replacement for ?rusty? in ?A
[rusty] nail is not as strong as a clean, new one.
(corroded; black; dirty; painted).? Jarmasz and
Szpakowicz (2003) used a sophisticated thesaurus-
based method and achieved state-of-the art perfor-
mance on the ESL synonyms task, which is 82%.
Again the Holmes data does not have the property
that the intended meaning is signaled by providing
the original word, thus adding extra challenge.
Although it was not developed for this task, we
believe the recurrent language modeling work of
Mikolov (2010; 2011b; 2011a) is also quite rel-
evant. In this work, a recurrent neural net lan-
guage model is used to achieve state-of-the-art per-
formance in perplexity and speech recognition er-
ror rates. Critically, the recurrent neural net does
not maintain a fixed N-gram context, and its hid-
den layer has the potential to model overall sen-
tence meaning and long-span coherence. While the-
oretical results (Bengio et al, 1994) indicate that
extremely long-range phenomena are hard to learn
with a recurrent neural network, in practice the span
of usual sentences may be manageable. Recursive
neural networks (Socher et al, 2011) offer similar
advantages, without the theoretical limitations. Both
offer promising avenues of research.
6 Conclusion
In this paper we have described a new, publicly
available, corpus of sentence-completion questions.
Whereas for many traditional language modeling
tasks, N-gram models provide state-of-the-art per-
formance, and may even be fully adequate, this task
is designed to be insoluble with local models. Be-
cause the task now allows us to measure progress
in an area where N-gram models do poorly, we ex-
pect it to stimulate research in fundamentally new
and more powerful language modeling methods.
References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks, 5(2):157 ?166.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics - Volume 1,
ACL ?98, pages 225?231, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
35
language modeling. Computer Speech and Language,
14(4):283 ? 332.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech and Language, 13(4):359 ?
393.
S. Chen. 2009a. Performance prediction for exponential
language models. In NAACL-HLT.
S. Chen. 2009b. Shrinking exponential language models.
In NAACL-HLT.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
Educational-Testing-Service. 2011.
https://satonlinecourse.collegeboard.com/sr/ digi-
tal assets/assessment/pdf/0833a611-0a43-10c2-0148-
cc8c0087fb06-f.pdf.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 145?148, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, SemEval ?07,
pages 410?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sanjeev Khudanpur and Jun Wu. 2000. Maximum
entropy techniques for exploiting syntactic, semantic
and collocational dependencies in language modeling.
Computer Speech and Language, 14(4):355 ? 372.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
ICASSP.
Jarmasz M. and Szpakowicz S. 2003. Roget?s thesaurus
and semantic similarity. In Recent Advances in Natu-
ral Language Processing (RANLP).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas
Burget, and Jan Cernocky. 2011a. Empirical evalua-
tion and combination of advanced language modeling
techniques. In Proceedings of Interspeech 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33 (2), pages 161?199.
Harry Printz and Peder A. Olsen. 2002. Theory and prac-
tice of acoustic confusability. Computer Speech and
Language, 16(1):131 ? 164.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language models:
a vehicle for linguistic-statistical integration. Com-
puter Speech and Language, 15(1):55 ? 73.
R. Rosenfeld. 1997. A whole sentence maximum en-
tropy language model. In Proceedings ASRU.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proceedings of ICASSP.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492
? 518.
Claude E. Shannon and Warren Weaver. 1949. The
Mathematical Theory of Communication. University
of Illinois Press.
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In Proceedings of the 2011 International Con-
ference on Machine Learning (ICML-2011).
E. Terra and C. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning (ECML).
Jun Wu and Sanjeev Khudanpur. 1999. Combining non-
local, syntactic and n-gram dependencies in language
modeling. In Proceedings of Eurospeech.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings of
the 39th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?01, pages 523?530, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Deniz Yuret. 2007. Ku: word sense disambiguation
by substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 207?213, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
36
