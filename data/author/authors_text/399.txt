Why Are They Excited?
Identifying and Explaining Spikes in Blog Mood Levels
Krisztian Balog Gilad Mishne Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam
kbalog,gilad,mdr@science.uva.nl
Abstract
We describe a method for discovering ir-
regularities in temporal mood patterns ap-
pearing in a large corpus of blog posts,
and labeling them with a natural language
explanation. Simple techniques based
on comparing corpus frequencies, coupled
with large quantities of data, are shown to
be effective for identifying the events un-
derlying changes in global moods.
1 Introduction
Blogs, diary-like web pages containing highly
opinionated personal commentary, are becoming
increasingly popular. This new type of media of-
fers a unique look into people?s reactions and feel-
ings towards current events, for a number of rea-
sons. First, blogs are frequently updated, and like
other forms of diaries are typically closely linked
to ongoing events in the blogger?s life. Second, the
blog contents tend to be unmoderated and subjec-
tive, more so than mainstream media?expressing
opinions, thoughts, and feeling. Finally, the large
amount of blogs enables aggregation of thousands
of opinions expressed every minute; this aggrega-
tion allows abstractions of the data, cleaning out
noise and focusing on the main issues.
Many blog authoring environments allow blog-
gers to tag their entries with highly individual (and
personal) features. Users of LiveJournal, one of
the largest weblog communities, have the option
of reporting their mood at the time of the post;
users can either select a mood from a predefined
list of common moods such as ?amused? or ?an-
gry,? or enter free-text. A large percentage of Live-
Journal users tag their postings with a mood. This
results in a stream of hundreds of weblog posts
tagged with mood information per minute, from
hundreds of thousands of users across the globe.
The collection of such mood reports from many
bloggers gives an aggregate mood of the blogo-
sphere for each point in time: the popularity of
different moods among bloggers at that time.
In previous work, we introduced a tool for
tracking the aggregate mood of the blogosphere,
and showed how it reflects global events (Mishne
and de Rijke, 2006a). The tool?s output includes
graphs showing the popularity of moods in blog
posts during a given interval; e.g., Figure 1 plots
the mood level for ?scared? during a 10 day pe-
riod. While such graphs reflect some expected
patterns (e.g., an increase in ?scared? around Hal-
loween in Figure 1), we have also witnessed spikes
and drops for which no associated event was
Figure 1: Blog posts labeled ?scared? during the October 26?
November 5, 2005 interval. The dotted (black) curve indi-
cates the absolute number of posts labeled ?scared,? while
the solid (red) curve shows the rate of change.
known to us. In this paper, we address this is-
sue: we seek algorithms for identifying unusual
changes in mood levels and explaining the under-
lying reasons for these changes. By ?explanation?
we mean a short snippet of text that describes the
event that caused the unusual mood change.
To produce such explanations, we proceed as
follows. If unusual spikes occur in the level of
mood m, we examine the language used in blog
posts labeled with m around and during the pe-
riod in which the spike occurs. We interpret words
207
that are not expected given a long-term language
model for m as signals for the spike in m?s level.
To operationalize the idea of ?unexpected words?
for a given mood, we use standard methods for
corpus comparison; once identified, we use the
?unexpected words? to consult a news corpus from
which we retrieve a small text snippet that we then
return as the desired explanation.
In Section 2 we briefly discuss related work.
Then, we detail how we detect spikes in mood lev-
els (in Section 3) and how we generate natural lan-
guage explanations for such spikes (in Section 4).
Experimental results are presented in Section 5,
and in Section 6 we present our conclusions.
2 Related work
As to burstiness phenomena in web data, Klein-
berg (2002) targets email and research papers, try-
ing to identify sharp rises in word frequencies in
document streams. Bursts can be found by search-
ing periods when a given word tends to appear at
unusually short intervals. Kumar et al (2003) ex-
tend Kleinberg?s algorithm to discover dense pe-
riods of ?bursty? intra-community link creation in
the blogspace, while Nanno et al (2004) extend it
to work on blogs. We use a simple comparison be-
tween long-term and short-term language models
associated with a given mood to identify unusual
word usage patterns.
Recent years have witnessed an increase in re-
search on extracting subjective and other non-
factual aspects of textual content; see (Shanahan et
al., 2005) for an overview. Much work in this area
focuses on recognizing and/or annotating evalu-
ative textual expressions. In contrast, work that
explores mood annotations is relatively scarce.
Mishne (2005) reports on text mining experiments
aimed at automatically tagging blog posts with
moods. Mishne and de Rijke (2006a) lift this work
to the aggregate level, and use natural language
processing and machine learning to estimate ag-
gregate mood levels from the text of blog entries.
3 Detecting spikes
Our first task is to identify spikes in moods re-
ported in blog posts. Many of the moods reported
by LiveJournal users display a cyclic behavior.
There are some obvious moods with a daily cycle.
For instance, people feel awake in the mornings
and tired in the evening (Figure 2). Other moods
show a weekly cycle. For instance, people drink
more at the weekends (Figure 3).
Figure 2: Daily cycles for ?awake? and ?tired.?
Figure 3: Weekend cycles for ?drunk.?
Our idea of detecting spikes tries to deal with
these cyclic events and aims at finding global
changes. Let POSTS (mood, date, hour) be the
number of posts labelled with a given mood and
created within a one-hour interval at the speci-
fied date. Similarly, ALLPOSTS (date, hour) is
the number of all posts created within the interval
specified by the date and hour. The ratio of posts
labeled with a given mood to all posts could be
expressed for all days of a week (Sunday, . . . , Sat-
urday) and for all one-hour intervals (0, . . . , 23)
using the formula:
R(mood, day, hour) =
?
DW (date)=day POSTS (mood, date, hour)
?
DW (date)=day ALLPOSTS (date, hour)
,
where day = 0, . . . , 6 and DW (date) is a day-of-
the-week function that returns 0, . . . , 6 depending
on the date argument.
The level of a given mood is changed within
a one-hour interval of a day, if the ratio of posts
labelled with that mood to all posts, created within
the interval, is significantly different from the ratio
that has been observed on the same hour of the
similar day of the week. Formally:
D(mood, date, hour) =
POSTS(mood,date,hour)
ALLPOSTS(date,hour)
R(mood,DW (date), hour) .
If |D| (the absolute value of D) exceeds a thresh-
old we conclude that a spike has occurred, while
208
the sign of D makes it possible to distinguish be-
tween positive and negative spikes. The absolute
value of D expresses the degree of the peak.
This method of identifying spikes allows us to
look at a period of a few hours instead of only
one, which is an effective smoothing method, es-
pecially if a sufficient number of posts cannot be
observed for a given mood.
4 Explaining peaks
Our next task is to explain the peaks identified by
the methods listed previously. We proceed in two
steps. First, we discover features in the peaking
interval which display a significantly different lan-
guage usage from that found in the general lan-
guage associated with the mood. Then we form
queries using these ?overused? words as well as
the date(s) of the peaking interval and run these as
queries against a news corpus.
4.1 Overused words To discover the reasons
underlying mood changes we use corpus-based
techniques to identify changes in language usage.
We compare two corpora: (1) the full set of blog
posts, referred to as the standard corpus, and (2) a
corpus associated with the peaking interval, re-
ferred to as the sample corpus.
To compare word frequencies across the two
corpora we apply the log-likelihood statistical
test (Dunning, 1993). Let Oi be the observed
frequency of a term, Ni its total frequency, and
Ei = (Ni ?
?
i Oi)/
?
i Ni its expected frequency
in corpus i (where i takes values 1 and 2 for the
standard and sample corpus, respectively). Then,
the log-likelihood value is calculated according to
this formula: ?2 ln? = 2?i Oi ln
(
Oi
Ei
)
.
4.2 Finding explanations Given the start and
end dates of a peaking interval and a list of
overused words from this period, a query is
formed. This query is then submitted to (head-
lines of) a news corpus. A headline is retrieved if
it contains at least one of the overused words and
is dated within the peaking interval or the day be-
fore the beginning of the peak. The hits are ranked
based on the number of overused terms contained
in the headline.
5 Experiments
In this section we illustrate our methods with some
examples and provide a preliminary analysis of
their effectiveness.
5.1 The blog corpus Our corpus consists of
all public blogs published in LiveJournal during
a 90 day period from July 5 to October 2, 2005,
adding up to a total of 19 million blog posts. For
each entry, the text of the post along with the date
and time are indexed. Posts without an explicit
mood indication (10M) are discarded. We applied
standard preprocessing steps (stopword removal,
stemming) to the text of blog posts.
5.2 The news corpus The collection con-
tains around 1000 news headlines that have
been published in Wikinews (http://www.
wikinews.org) during the period of July-
September, 2005.
5.3 Case studies We present three particular
cases where an irregular behavior in a certain
mood could be observed. We examine how accu-
rately the overused terms describe the events that
caused the spikes.
5.3.1 Harry Potter In July, 2005, a peak in
?excited? was discovered; see Figure 4, where the
shaded (green) area indicates the ?peak area.?
Figure 4: Peak in ?excited? around July 16, 2005.
Step 1 of our peak explanation method (Sec-
tion 4) reveals the following overused terms dur-
ing the peak period: ?potter,? ?book,? ?excit,?
?hbp,? ?read,? ?princ,? ?midnight.? Step 2 of
our peak explanation method (Section 4) exploits
these words to retrieve the following headline
from the news collection: ?July 16. Harry Potter
and the Half-Blood Prince released.?
5.3.2 Hurricane Katrina Our next exam-
ple illustrates the need for careful thresholding
when defining peaks (see Section 3). We show
peaks in ?worried? discovered around late Au-
gust, with a 40% and 80% threshold. Clearly, far
more peaks are identified with the lower threshold,
while the peaks identified in the bottom plot (with
the higher threshold), all appear to be clear peaks.
The overused terms during the peak period include
?orlean,? ?worri,? ?hurrican,? ?gas,? ?katrina? In
209
Figure 5: Peaks in ?worried? around August 29, 2005. (Top:
threshold 40% change; bottom: threshold 80% change)
Step 2 of our explanation method we retrieve the
following news headlines (top 5 shown only):
(Sept 1) Hurricane Katrina: Resources regarding
missing/located people
(Sept 2) Crime in New Orleans sharply increases
after Hurricane Katrina
(Sept 1) Fats Dominomissing in the wake of Hur-
ricane Katrina
(Aug 30) At least 55 killed by Hurricane Katrina;
serious flooding across affected region
(Aug 26) Hurricane Katrina strikes Florida, kills
seven
5.3.3 London terror attacks On July 7 a
sharp spike could be observed in the ?sad? mood;
see Figure 6; the tone of the shaded area shows the
degree of the peak. Overused terms identified for
this period include ?london,? ?attack,? ?terrorist,?
?bomb,? ?peopl?, ?explos.? Consulting our news
Figure 6: Peak in ?sad? around July 7, 2005.
corpus produced the following top ranked results:
(July 7) Coordinated terrorist attack hits London
(July 7) British PrimeMinister Tony Blair speaks
about London bombings
(July 7) Bomb scare closes main Edinburgh thor-
oughfare
(July 7) France raises security level to red in re-
sponse to London bombings
(July 6) Tanzania accused of supporting terror-
ism to destabilise Burundi
5.4 Failure analysis Evaluation of the meth-
ods described here is non-trivial. We found that
our peak detection method is effective despite its
simplicity. Anecdotal evidence suggests that our
approach to finding explanations underlying un-
usual spikes and drops in mood levels is effective.
We expect that it will break down, however, in case
the underlying cause is not news related but, for in-
stance, related to celebrations or public holidays;
news sources are unlikely to cover these.
6 Conclusions
We described a method for discovering irregulari-
ties in temporal mood patterns appearing in a large
corpus of blog posts, and labeling them with a
natural language explanation. Our method shows
that simple techniques based on comparing corpus
frequencies, coupled with large quantities of data,
are effective for identifying the events underlying
changes in global moods.
Acknowledgments This research was supported
by the Netherlands Organization for Scientific
Research (NWO) under project numbers 016.-
054.616, 017.001.190, 220-80-001, 264-70-050,
365-20-005, 612.000.106, 612.000.207, 612.013.-
001, 612.066.302, 612.069.006, 640.001.501, and
640.002.501.
References
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Comput. Ling., 19(1):61?74.
J. Kleinberg. 2002. Bursty and hierarchical structure in
streams. In Proc. 8th ACM SIGKDD Intern. Conf. on
Knowledge Discovery and Data Mining, pages 1?25.
R. Kumar, J. Novak, P. Raghavan, and A. Tomkins. 2003. On
the bursty evolution of blogspace. In Proc. 12th Intern.
World Wide Web Conf., pages 568?576.
G. Mishne and M. de Rijke. 2006a. Capturing global mood
levels using blog posts. In AAAI 2006 Spring Symp. on
Computational Approaches to Analysing Weblogs (AAAI-
CAAW 2006). To appear.
G. Mishne and M. de Rijke. 2006b. MoodViews: Tools
for blog mood analysis. In AAAI 2006 Spring Symp. on
Computational Approaches to Analysing Weblogs (AAAI-
CAAW 2006).
G. Mishne. 2005. Experiments with mood classification in
blog posts. In Style2005 ? 1st Workshop on Stylistic Anal-
ysis of Text for Information Access, at SIGIR 2005.
T. Nanno, T. Fujiki, Y. Suzuki, and M. Okumura. 2004. Au-
tomatically collecting, monitoring, and mining Japanese
weblogs. In Proc. 13th International World Wide Web
Conf., pages 320?321.
J.G. Shanahan, Y. Qu, and J. Wiebe, editors. 2005. Comput-
ing Attitude and Affect in Text: Theory and Applications.
Springer.
210
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1057?1065,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Generative Blog Post Retrieval Model that Uses
Query Expansion based on External Collections
Wouter Weerkamp
w.weerkamp@uva.nl
Krisztian Balog
k.balog@uva.nl
ISLA, University of Amsterdam
Maarten de Rijke
mdr@science.uva.nl
Abstract
User generated content is characterized
by short, noisy documents, with many
spelling errors and unexpected language
usage. To bridge the vocabulary gap be-
tween the user?s information need and
documents in a specific user generated
content environment, the blogosphere, we
apply a form of query expansion, i.e.,
adding and reweighing query terms. Since
the blogosphere is noisy, query expansion
on the collection itself is rarely effective
but external, edited collections are more
suitable. We propose a generative model
for expanding queries using external col-
lections in which dependencies between
queries, documents, and expansion doc-
uments are explicitly modeled. Differ-
ent instantiations of our model are dis-
cussed and make different (in)dependence
assumptions. Results using two exter-
nal collections (news andWikipedia) show
that external expansion for retrieval of user
generated content is effective; besides,
conditioning the external collection on the
query is very beneficial, and making can-
didate expansion terms dependent on just
the document seems sufficient.
1 Introduction
One of the grand challenges in information re-
trieval is to bridge the vocabulary gap between a
user and her information need on the one hand and
the relevant documents on the other (Baeza-Yates
and Ribeiro-Neto, 1999). In the setting of blogs
or other types of user generated content, bridging
this gap becomes even more challenging. This has
several causes: (i) the spelling errors, unusual, cre-
ative or unfocused language usage resulting from
the lack of top-down rules and editors in the con-
tent creation process, and (ii) the (often) limited
length of user generated documents.
Query expansion, i.e., modifying the query by
adding and reweighing terms, is an often used
technique to bridge the vocabulary gap. In gen-
eral, query expansion helps more queries than
it hurts (Balog et al, 2008b; Manning et al,
2008). However, when working with user gener-
ated content, expanding a query with terms taken
from the very corpus in which one is searching
tends to be less effective (Arguello et al, 2008a;
Weerkamp and de Rijke, 2008b)?topic drift is
a frequent phenomenon here. To be able to ar-
rive at a richer representation of the user?s infor-
mation need, while avoiding topic drift resulting
from query expansion against user generated con-
tent, various authors have proposed to expand the
query against an external corpus, i.e., a corpus dif-
ferent from the target (user generated) corpus from
which documents need to be retrieved.
Our aim in this paper is to define and evaluate
generative models for expanding queries using ex-
ternal collections. We propose a retrieval frame-
work in which dependencies between queries,
documents, and expansion documents are explic-
itly modeled. We instantiate the framework in
multiple ways by making different (in)dependence
assumptions. As one of the instantiations we ob-
tain the mixture of relevance models originally
proposed by Diaz and Metzler (2006).
We address the following research questions:
(i) Can we effectively apply external expansion in
the retrieval of user generated content? (ii) Does
conditioning the external collection on the query
help improve retrieval performance? (iii) Can we
obtain a good estimate of this query-dependent
collection probability? (iv) Which of the collec-
tion, the query, or the document should the selec-
tion of an expansion term be dependent on? In
other words, what are the strongest simplifications
in terms of conditional independencies between
variables that can be assumed, without hurting per-
formance? (v) Do our models show similar behav-
ior across topics or do we observe strong per-topic
1057
differences between models?
The remainder of this paper is organized as fol-
lows. We discuss previous work related to query
expansion and external sources in ?2. Next, we
introduce our retrieval framework (?3) and con-
tinue with our main contribution, external expan-
sion models, in ?4. ?5 details how the components
of the model can be estimated. We put our models
to the test, using the experimental setup discussed
in ?6, and report on results in ?7. We discuss our
results (?8) and conclude in ?9.
2 Related Work
Related work comes in two main flavors: (i) query
modeling in general, and (ii) query expansion us-
ing external sources (external expansion). We
start by shortly introducing the general ideas be-
hind query modeling, and continue with a quick
overview of work related to external expansion.
2.1 Query Modeling
Query modeling, i.e., transformations of simple
keyword queries into more detailed representa-
tions of the user?s information need (e.g., by as-
signing (different) weights to terms, expanding the
query, or using phrases), is often used to bridge the
vocabulary gap between the query and the doc-
ument collection. Many query expansion tech-
niques have been proposed, and they mostly fall
into two categories, i.e., global analysis and local
analysis. The idea of global analysis is to expand
the query using global collection statistics based,
for instance, on a co-occurrence analysis of the en-
tire collection. Thesaurus- and dictionary-based
expansion as, e.g., in Qiu and Frei (1993), also
provide examples of the global approach.
Our focus in this paper is on local approaches
to query expansion, that use the top retrieved doc-
uments as examples from which to select terms
to improve the retrieval performance (Rocchio,
1971). In the setting of language modeling ap-
proaches to query expansion, the local analysis
idea has been instantiated by estimating addi-
tional query language models (Lafferty and Zhai,
2003; Tao and Zhai, 2006) or relevance mod-
els (Lavrenko and Croft, 2001) from a set of feed-
back documents. Yan and Hauptmann (2007) ex-
plore query expansion in a multimedia setting.
Balog et al (2008b) compare methods for sam-
pling expansion terms to support query-dependent
and query-independent query expansion; the lat-
ter is motivated by the wish to increase ?aspect
recall? and attempts to uncover aspects of the in-
formation need not captured by the query. Kur-
land et al (2005) also try to uncover multiple as-
pects of a query, and to that they provide an iter-
ative ?pseudo-query? generation technique, using
cluster-based language models. The notion of ?as-
pect recall? is mentioned in (Buckley, 2004; Har-
man and Buckley, 2004) and identified as one of
the main reasons of failure of the current informa-
tion retrieval systems. Even though we acknowl-
edge the possibilities of our approach in improving
aspect recall, by introducing aspects mainly cov-
ered by the external collection being used, we are
currently unable to test this assumption.
2.2 External Expansion
The use of external collections for query expan-
sion has a long history, see, e.g., (Kwok et al,
2001; Sakai, 2002). Diaz and Metzler (2006) were
the first to give a systematic account of query ex-
pansion using an external corpus in a language
modeling setting, to improve the estimation of rel-
evance models. As will become clear in ?4, Diaz
and Metzler?s approach is an instantiation of our
general model for external expansion.
Typical query expansion techniques, such as
pseudo-relevance feedback, using a blog or blog
post corpus do not provide significant perfor-
mance improvements and often dramatically hurt
performance. For this reason, query expansion
using external corpora has been a popular tech-
nique at the TREC Blog track (Ounis et al, 2007).
For blog post retrieval, several TREC participants
have experimented with expansion against exter-
nal corpora, usually a news corpus, Wikipedia, the
web, or a mixture of these (Zhang and Yu, 2007;
Java et al, 2007; Ernsting et al, 2008). For the
blog finding task introduced in 2007, TREC par-
ticipants again used expansion against an exter-
nal corpus, usually Wikipedia (Elsas et al, 2008a;
Ernsting et al, 2008; Balog et al, 2008a; Fautsch
and Savoy, 2008; Arguello et al, 2008b). The mo-
tivation underlying most of these approaches is to
improve the estimation of the query representa-
tion, often trying to make up for the unedited na-
ture of the corpus from which posts or blogs need
to be retrieved. Elsas et al (2008b) go a step fur-
ther and develop a query expansion technique us-
ing the links in Wikipedia.
Finally, Weerkamp and de Rijke (2008b) study
1058
external expansion in the setting of blog retrieval
to uncover additional perspectives of a given topic.
We are driven by the same motivation, but where
they considered rank-based result combinations
and simple mixtures of query models, we take
a more principled and structured approach, and
develop four versions of a generative model for
query expansion using external collections.
3 Retrieval Framework
We work in the setting of generative language
models. Here, one usually assumes that a doc-
ument?s relevance is correlated with query likeli-
hood (Ponte and Croft, 1998; Miller et al, 1999;
Hiemstra, 2001). Within the language model-
ing approach, one builds a language model from
each document, and ranks documents based on the
probability of the document model generating the
query. The particulars of the language modeling
approach have been discussed extensively in the
literature (see, e.g., Balog et al (2008b)) and will
not be repeated here. Our final formula for ranking
documents given a query is based on Eq. 1:
logP (D|Q) ?
logP (D) +
?
t?Q
P (t|?Q) logP (t|?D) (1)
Here, we see the prior probability of a document
being relevant, P (D) (which is independent of the
query Q), the probability of a term t for a given
query model, ?Q, and the probability of observ-
ing the term t given the document model, ?D.
Our main interest lies in in obtaining a better es-
timate of P (t|?Q). To this end, we take the query
model to be a linear combination of the maximum-
likelihood query estimate P (t|Q) and an expanded
query model P (t|Q?):
P (t|?Q) = ?Q ?P (t|Q)+ (1??Q) ?P (t|Q?) (2)
In the next section we introduce our models for es-
timating p(t|Q?), i.e., query expansion using (mul-
tiple) external collections.
4 Query Modeling Approach
Our goal is to build an expanded query model that
combines evidence from multiple external collec-
tions. We estimate the probability of a term t in the
expanded query Q? using a mixture of collection-
specific query expansion models.
P (t|Q?) =
?
c?C P (t|Q, c) ? P (c|Q), (3)
where C is the set of document collections.
To estimate the probability of a term given the
query and the collection, P (t|Q, c), we compute
the expectation over the documents in the collec-
tion c:
P (t|Q, c) =
?
D?c
P (t|Q, c,D) ? P (D|Q, c). (4)
Substituting Eq. 4 back into Eq. 3 we get
P (t|Q?) = (5)
?
c?C
P (c|Q) ?
?
D?c
P (t|Q, c,D) ? P (D|Q, c).
This, then, is our query model for combining evi-
dence from multiple sources.
The following subsections introduce four in-
stances of the general external expansion model
(EEM) we proposed in this section; each of the in-
stances differ in independence assumptions:
? EEM1 (?4.1) assumes collection c to be inde-
pendent of query Q and document D jointly,
and document D individually, but keeps the
dependence on Q and of t and Q on D.
? EEM2 (?4.2) assumes that term t and collec-
tion c are conditionally independent, given
document D and query Q; moreover, D and
Q are independent given c but the depen-
dence of t and Q on D is kept.
? EEM3 (?4.3) assumes that expansion term t
and original query Q are independent given
document D.
? On top of EEM3, EEM4 (?4.4) makes one
more assumption, viz. the dependence of col-
lection c on query Q.
4.1 External Expansion Model 1 (EEM1)
Under this model we assume collection c to be
independent of query Q and document D jointly,
and document D individually, but keep the depen-
dence on Q. We rewrite P (t|Q, c) as follows:
P (t|Q, c)
=
?
D?c
P (t|Q,D) ? P (t|c) ? P (D|Q)
=
?
D?c
P (t, Q|D)
P (Q|D)
? P (t|c) ?
P (Q|D)P (D)
P (Q)
?
?
D?c
P (t, Q|D) ? P (t|c) ? P (D) (6)
Note that we drop P (Q) from the equation as it
does not influence the ranking of terms for a given
1059
query Q. Further, P (D) is the prior probability
of a document, regardless of the collection it ap-
pears in (as we assumed D to be independent of
c). We assume P (D) to be uniform, leading to the
following equation for ranking expansion terms:
P (t|Q?) ?
?
c?C
P (t|c) ? P (c|Q) ?
?
D?c
P (t, Q|D). (7)
In this model we capture the probability of the ex-
pansion term given the collection (P (t|c)). This
allows us to assign less weight to terms that are
less meaningful in the external collection.
4.2 External Expansion Model 2 (EEM2)
Here, we assume that term t and collection c are
conditionally independent, given document D and
query Q: P (t|Q, c,D) = P (t|Q,D). This leaves
us with the following:
P (t|Q,D) =
P (t, Q,D)
P (Q,D)
=
P (t, Q|D) ? P (D)
P (Q|D) ? P (D)
=
P (t, Q|D)
P (Q|D)
(8)
Next, we assume document D and query Q to
be independent given collection c: P (D|Q, c) =
P (D|c). Substituting our choices into Eq. 4 gives
us our second way of estimating P (t|Q, c):
P (t|Q, c) =
?
D?c
P (t, Q|D)
P (Q|D)
? P (D|c) (9)
Finally, we put our choices so far together, and
implement Eq. 9 in Eq. 3, yielding our final term
ranking equation:
P (t|Q?) ? (10)
?
c?C
P (c|Q) ?
?
D?c
P (t, Q|D)
P (Q|D)
? P (D|c).
4.3 External Expansion Model 3 (EEM3)
Here we assume that expansion term t and both
collection c and original query Q are independent
given document D. Hence, we set P (t|Q, c,D) =
P (t|D). Then
P (t|Q, c)
=
?
D?c
P (t|D) ? P (D|Q, c)
=
?
D?c
P (t|D) ?
P (Q|D, c) ? P (D|c)
P (Q|c)
?
?
D?c
P (t|D) ? P (Q|D, c) ? P (D|c)
We dropped P (Q|c) as it does not influence the
ranking of terms for a given query Q. Assuming
independence of Q and c given D, we obtain
P (t|Q, c) ?
?
D?c
P (D|c) ? P (t|D) ? P (Q|D)
so
P (t|Q?) ?
?
c?C
P (c|Q) ?
?
D?c
P (D|c) ? P (t|D) ? P (Q|D).
We follow Lavrenko and Croft (2001) and assume
that P (D|c) = 1|Rc| , the size of the set of top
ranked documents in c (denoted by Rc), finally ar-
riving at
P (t|Q?) ?
?
c?C
P (c|Q)
|Rc|
?
?
D?Rc
P (t|D) ? P (Q|D). (11)
4.4 External Expansion Model 4 (EEM4)
In this fourth model we start from EEM3 and drop
the assumption that c depends on the query Q, i.e.,
P (c|Q) = P (c), obtaining
P (t|Q?) ?
?
c?C
P (c)
|Rc|
?
?
D?Rc
P (t|D) ? P (Q|D). (12)
Eq. 12 is in fact the ?mixture of relevance models?
external expansion model proposed by Diaz and
Metzler (2006). The fundamental difference be-
tween EEM1, EEM2, EEM3 on the one hand and
EEM4 on the other is that EEM4 assumes inde-
pendence between c and Q (thus P (c|Q) is set to
P (c)). That is, the importance of the external col-
lection is independent of the query. How reason-
able is this choice? Mishne and de Rijke (2006)
examined queries submitted to a blog search en-
gine and found many to be either news-related
context queries (that aim to track mentions of a
named entity) or concept queries (that seek posts
about a general topic). For context queries such as
cheney hunting (TREC topic 867) a news collec-
tion is likely to offer different (relevant) aspects
of the topic, whereas for a concept query such as
jihad (TREC topic 878) a knowledge source such
as Wikipedia seems an appropriate source of terms
that capture aspects of the topic. These observa-
tions suggest the collection should depend on the
query.
1060
EEM3 and EEM4 assume that expansion term t
and original query Q are independent given doc-
ument D. This may or may not be too strong an
assumption. Models EEM1 and EEM2 also make
independence assumptions, but weaker ones.
5 Estimating Components
The models introduced above offer us several
choices in estimating the main components. Be-
low we detail how we estimate (i) P (c|Q), the
importance of a collection for a given query,
(ii) P (t|c), the unimportance of a term for an ex-
ternal collection, (iii) P (Q|D), the relevance of
a document in the external collection for a given
query, and (iv) P (t, Q|D), the likelihood of a term
co-occurring with the query, given a document.
5.1 Importance of a Collection
Represented as P (c|Q) in our models, the im-
portance of an external collection depends on the
query; how we can estimate this term? We con-
sider three alternatives, in terms of (i) query clar-
ity, (ii) coherence and (iii) query-likelihood, using
documents in that collection.
First, query clarity measures the structure of a
set of documents based on the assumption that a
small number of topical terms will have unusu-
ally large probabilities (Cronen-Townsend et al,
2002). We compute the query clarity of the top
ranked documents in a given collection c:
clarity(Q, c) =
?
t
P (t|Q) ? log
P (t|Q)
P (t|Rc)
Finally, we normalize clarity(Q, c) over all col-
lections, and set P (c|Q) ? clarity(Q,c)P
c??C clarity(Q,c
?) .
Second, a measure called ?coherence score? is
defined by He et al (2008). It is the fraction of
?coherent? pairs of documents in a given set of
documents, where a coherent document pair is one
whose similarity exceeds a threshold. The coher-
ence of the top ranked documents Rc is:
Co(Rc) =
?
i6=j?{1,...,|Rc|} ?(di, dj)
|Rc|(|Rc| ? 1)
,
where ?(di, dj) is 1 in case of a similar pair (com-
puted using cosine similarity), and 0 otherwise.
Finally, we set P (c|Q) ? Co(Rc)P
c??C Co(Rc? )
.
Third, we compute the conditional probability
of the collection using Bayes? theorem. We ob-
serve that P (c|Q) ? P (Q|c) (omitting P (Q) as it
will not influence the ranking and P (c) which we
take to be uniform). Further, for the sake of sim-
plicity, we assume that all documents within c are
equally important. Then, P (Q|c) is estimated as
P (Q|c) =
1
|c|
?
?
D?c
P (Q|D) (13)
where P (Q|D) is estimated as described in ?5.3,
and |c| is the number of documents in c.
5.2 Unimportance of a Term
Rather than simply estimating the importance of
a term for a given query, we also estimate the
unimportance of a term for a collection; i.e., we
assign lower probability to terms that are com-
mon in that collection. Here, we take a straight-
forward approach in estimating this, and define
P (t|c) = 1 ? n(t,c)P
t? n(t
?,c) .
5.3 Likelihood of a Query
We need an estimate of the probability of a query
given a document, P (Q|D). We do so by using
Hauff et al (2008)?s refinement of term dependen-
cies in the query as proposed by Metzler and Croft
(2005).
5.4 Likelihood of a Term
Estimating the likelihood of observing both the
query and a term for a given document P (t, Q|D)
is done in a similar way to estimating P (Q|D), but
now for t, Q in stead of Q.
6 Experimental Setup
In his section we detail our experimental setup:
the (external) collections we use, the topic sets
and relevance judgements available, and the sig-
nificance testing we perform.
6.1 Collections and Topics
We make use of three collections: (i) a collec-
tion of user generated documents (blog posts),
(ii) a news collection, and (iii) an online knowl-
edge source. The blog post collection is the TREC
Blog06 collection (Ounis et al, 2007), which con-
tains 3.2 million blog posts from 100,000 blogs
monitored for a period of 11 weeks, from Decem-
ber 2005 to March 2006; all posts from this period
have been stored as HTML files. Our news col-
lection is the AQUAINT-2 collection (AQUAINT-
2, 2007), from which we selected news articles
that appeared in the period covered by the blog
1061
collection, leaving us with about 150,000 news
articles. Finally, we use a dump of the English
Wikipedia from August 2007 as our online knowl-
edge source; this dump contains just over 3.8 mil-
lion encyclopedia articles.
During 2006?2008, the TRECBlog06 collec-
tion has been used for the topical blog post re-
trieval task (Weerkamp and de Rijke, 2008a) at the
TREC Blog track (Ounis et al, 2007): to retrieve
posts about a given topic. For every year, 50 topics
were developed, consisting of a title field, descrip-
tion, and narrative; we use only the title field, and
ignore the other available information. For all 150
topics relevance judgements are available.
6.2 Metrics and Significance
We report on the standard IR metrics Mean Aver-
age Precision (MAP), precision at 5 and 10 doc-
uments (P5, P10), and the Mean Reciprocal Rank
(MRR). To determine whether or not differences
between runs are significant, we use a two-tailed
paired t-test, and report on significant differences
for ? = .05 (M and O) and ? = .01 (N and H).
7 Results
We first discuss the parameter tuning for our four
EEM models in Section 7.1. We then report on the
results of applying these settings to obtain our re-
trieval results on the blog post retrieval task. Sec-
tion 7.2 reports on these results. We follow with a
closer look in Section 8.
7.1 Parameters
Our model has one explicit parameter, and one
more or less implicit parameter. The obvious pa-
rameter is ?Q, used in Eq. 2, but also the num-
ber of terms to include in the final query model
makes a difference. For training of the param-
eters we use two TREC topic sets to train and
test on the held-out topic set. From the training
we conclude that the following parameter settings
work best across all topics: (EEM1) ?Q = 0.6,
30 terms; (EEM2) ?Q = 0.6, 40 terms; (EEM3
and EEM4) ?Q = 0.5, 30 terms. In the remainder
of this section, results for our models are reported
using these parameter settings.
7.2 Retrieval Results
As a baseline we use an approach without exter-
nal query expansion, viz. Eq. 1. In Table 1 we
list the results on the topical blog post finding task
model P (c|Q) MAP P5 P10 MRR
Baseline 0.3815 0.6813 0.6760 0.7643
EEM1
uniform 0.3976N 0.7213N 0.7080N 0.7998
0.8N/0.2W 0.3992 0.7227 0.7107 0.7988
coherence 0.3976 0.7187 0.7060 0.7976
query clarity 0.3970 0.7187 0.7093 0.7929
P (Q|c) 0.3983 0.7267 0.7093 0.7951
oracle 0.4126N 0.7387M 0.7320N 0.8252M
EEM2
uniform 0.3885N 0.7053M 0.6967M 0.7706
0.9N/0.1W 0.3895 0.7133 0.6953 0.7736
coherence 0.3890 0.7093 0.7020 0.7740
query clarity 0.3872 0.7067 0.6953 0.7745
P (Q|c) 0.3883 0.7107 0.6967 0.7717
oracle 0.3995N 0.7253N 0.7167N 0.7856
EEM3
uniform 0.4048N 0.7187M 0.7207N 0.8261N
coherence 0.4058 0.7253 0.7187 0.8306
query clarity 0.4033 0.7253 0.7173 0.8228
P (Q|c) 0.3998 0.7253 0.7100 0.8133
oracle 0.4194N 0.7493N 0.7353N 0.8413
EEM4 0.5N/0.5W 0.4048N 0.7187M 0.7207N 0.8261N
Table 1: Results for all model instances on all top-
ics (i.e., 2006, 2007, and 2008); aN/bW stands
for the weights assigned to the news (a) and
Wikipedia corpora (b). Significance is tested be-
tween (i) each uniform run and the baseline, and
(ii) each other setting and its uniform counterpart.
of (i) our baseline, and (ii) our model (instanti-
ated by EEM1, EEM2, EEM3, and EEM4). For
all models that contain the query-dependent col-
lection probability (P (c|Q)) we report on multi-
ple ways of estimating this: (i) uniform, (ii) best
global mixture (independent of the query, obtained
by a sweep over collection probabilities), (iii) co-
herence, (iv) query clarity, (v) P (Q|c), and (vi) us-
ing an oracle for which optimal settings were ob-
tained by the same sweep as (ii). Note that meth-
ods (i) and (ii) are not query dependent; for EEM3
we do not mention (ii) since it equals (i). Finally,
for EEM4 we only have a query-independent com-
ponent, P (c): the best performance here is ob-
tained using equal weights for both collections.
A few observations. First, our baseline per-
forms well above the median for all three years
(2006?2008). Second, in each of its four instances
our model for query expansion against external
corpora improves over the baseline. Third, we
see that it is safe to assume that a term is depen-
dent only on the document from which it is sam-
pled (EEM1 vs. EEM2 vs. EEM3). EEM3 makes
the strongest assumptions about terms in this re-
spect, yet it performs best. Fourth, capturing the
dependence of the collection on the query helps,
as we can see from the significant improvements
of the ?oracle? runs over their ?uniform? counter-
parts. However, we do not have a good method
yet for automatically estimating this dependence,
1062
as is clear from the insignificant differences be-
tween the runs labeled ?coherence,? ?query clar-
ity,? ?P (Q|c)? and the run labeled ?uniform.?
8 Discussion
Rather than providing a pairwise comparison of all
runs listed in the previous section, we consider two
pairwise comparisons?between (an instantion of)
our model and the baseline, and between two in-
stantiations of our model?and highlight phenom-
ena that we also observed in other pairwise com-
parisons. Based on this discussion, we also con-
sider a combination of approaches.
8.1 EEM1 vs. the Baseline
We zoom in on EEM1 and make a per-topic com-
parison against the baseline. First of all, we
observe behavior typical for all query expansion
methods: some topics are helped, some are not af-
fected, and some are hurt by the use of EEM1; see
Figure 1, top row. Specifically, 27 topics show a
slight drop in AP (maximum drop is 0.043 AP), 3
topics do not change (as no expansion terms are
identified) and the remainder of the topics (120)
improve in AP. The maximum increase in AP is
0.5231 (+304%) for topic 949 (ford bell); Top-
ics 887 (world trade organization, +87%), 1032
(I walk the line, +63%), 865 (basque, +53%), and
1014 (tax break for hybrid automobiles, +50%)
also show large improvements. The largest drop (-
20% AP) is for topic 1043 (a million little pieces,
a controversial memoir that was in the news dur-
ing the time coverd by the blog crawl); because we
do not do phrase or entity recognition in the query,
but apply stopword removal, it is reduced to mil-
lion pieces which introduced a lot of topic drift.
Let us examine the ?collection preference? of
topics: 35 had a clear preference for Wikipedia, 32
topics for news, and the remainder (83 topics) re-
quired a mixture of both collections. First, we look
at topics that require equal weights for both collec-
tions; topic 880 (natalie portman, +21% AP) con-
cerns a celebrity with a largeWikipedia biography,
as well as news coverage due to new movie re-
leases during the period covered by the blog crawl.
Topic 923 (challenger, +7% AP) asks for infor-
mation on the space shuttle that exploded dur-
ing its launch; the 20th anniversary of this event
was commemorated during the period covered by
the crawl and therefore it is newsworthy as well
as present in Wikipedia (due to its historic im-
pact). Finally, topic 869 (muhammad cartoon,
+20% AP) deals with the controversy surrounding
the publication of cartoons featuring Muhammad:
besides its obvious news impact, this event is ex-
tensively discussed in multiple Wikipedia articles.
As to topics that have a preference for
Wikipedia, we see some very general ones (as is to
be expected): Topic 942 (lawful access, +30%AP)
on the government accessing personal files; Topic
1011 (chipotle restaurant, +13% AP) on infor-
mation concerning the Chipotle restaurants; Topic
938 (plug awards, +21% AP) talks about an award
show. Although this last topic could be expected to
have a clear preference for expansion terms from
the news corpus, the awards were not handed out
during the period covered by the news collection
and, hence, full weight is given to Wikipedia.
At the other end of the scale, topics that show a
preference for the news collection are topic 1042
(david irving, +28% AP), who was on trial dur-
ing the period of the crawl for denying the Holo-
caust and received a lot of media attention. Further
examples include Topic 906 (davos, +20% AP),
which asks for information on the annual world
economic forum meeting in Davos in January,
something typically related to news, and topic 949
(ford bell, +304% AP), which seeks information
on Ford Bell, Senate candidate at the start of 2006.
8.2 EEM1 vs. EEM3
Next we turn to a comparison between EEM1
and EEM3. Theoretically, the main difference
between these two instantiations of our general
model is that EEM3 makes much stronger sim-
plifying indepence assumptions than EEM1. In
Figure 1 we compare the two, not only against
the baseline, but, more interestingly, also in terms
of the difference in performance brought about by
switching from uniform estimation of P (c|Q) to
oracle estimation. Most topics gain in AP when
going from the uniform distribution to the oracle
setting. This happens for both models, EEM1 and
EEM3, leading to less topics decreasing in AP
over the baseline (the right part of the plots) and
more topics increasing (the left part). A second
observation is that both gains and losses are higher
for EEM3 than for EEM1.
Zooming in on the differences between EEM1
and EEM3, we compare the two in the same way,
now using EEM3 as ?baseline? (Figure 2). We ob-
serve that EEM3 performs better than EEM1 in 87
1063
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
Figure 1: Per-topic AP differences between the
baseline and (Top): EEM1 and (Bottom): EEM3,
for (Left): uniform P (c|Q) and (Right): oracle.
-0.4
-0.2
 0
 0.2
 0.4
AP
 dif
fere
nce
topics
Figure 2: Per-topic AP differences between EEM3
and EEM1 in the oracle setting.
cases, while EEM1 performs better for 60 topics.
Topics 1041 (federal shield law, 47% AP), 1028
(oregon death with dignity act, 32%AP), and 1032
(I walk the line, 32% AP) have the highest differ-
ence in favor of EEM3; Topics 877 (sonic food in-
dustry, 139% AP), 1013 (iceland european union,
25% AP), and 1002 (wikipedia primary source,
23% AP) are helped most by EEM1. Overall,
EEM3 performs significantly better than EEM1 in
terms of MAP (for ? = .05), but not in terms of
the early precision metrics (P5, P10, and MRR).
8.3 Combining Our Approaches
One observation to come out of ?8.1 and 8.2 is that
different topics prefer not only different external
expansion corpora but also different external ex-
pansion methods. To examine this phenomemon,
we created an articificial run by taking, for ev-
ery topic, the best performing model (with settings
optimized for the topic). Twelve topics preferred
the baseline, 37 EEM1, 20 EEM2, and 81 EEM3.
The articifical run produced the following results:
MAP 0.4280, P5 0.7600, P10 0.7480, and MRR
0.8452; the differences in MAP and P10 between
this run and EEM3 are significant for ? = .01.
We leave it as future work to (learn to) predict for
a given topic, which approach to use, thus refining
ongoing work on query difficulty prediction.
9 Conclusions
We explored the use of external corpora for query
expansion in a user generated content setting. We
introduced a general external expansion model,
which offers various modeling choices, and in-
stantiated it based on different (in)dependence as-
sumptions, leaving us with four instances.
Query expansion using external collection is
effective for retrieval in a user generated con-
tent setting. Furthermore, conditioning the collec-
tion on the query is beneficial for retrieval perfor-
mance, but estimating this component remains dif-
ficult. Dropping the dependencies between terms
and collection and terms and query leads to bet-
ter performance. Finally, the best model is topic-
dependent: constructing an artificial run based on
the best model per topic achieves significant better
results than any of the individual models.
Future work focuses on two themes: (i) topic-
dependent model selection and (ii) improved es-
timates of components. As to (i), we first want
to determine whether a query should be expanded,
and next select the appropriate expansion model.
For (ii), we need better estimates of P (Q|c);
one aspect that could be included is taking P (c)
into account in the query-likelihood estimate of
P (Q|c). One can make this dependent on the task
at hand (blog post retrieval vs. blog feed search).
Another possibility is to look at solutions used in
distributed IR. Finally, we can also include the es-
timation of P (D|c), the importance of a document
in the collection.
Acknowledgements
We thank our reviewers for their valuable feed-
back. This research is supported by the DuOMAn
project carried out within the STEVIN programme
which is funded by the Dutch and Flemish Gov-
ernments (http://www.stevin-tst.org) under project
number STE-09-12, and by the Netherlands Or-
ganisation for Scientific Research (NWO) under
project numbers 017.001.190, 640.001.501, 640.-
002.501, 612.066.512, 612.061.814, 612.061.815,
640.004.802.
1064
References
AQUAINT-2 (2007). URL: http://trec.nist.gov/
data/qa/2007 qadata/qa.07.guidelines.
html#documents.
Arguello, J., Elsas, J., Callan, J., and Carbonell, J. (2008a).
Document representation and query expansion models for
blog recommendation. In Proceedings of ICWSM 2008.
Arguello, J., Elsas, J. L., Callan, J., and Carbonell, J. G.
(2008b). Document representation and query expansion
models for blog recommendation. In Proc. of the 2nd Intl.
Conf. on Weblogs and Social Media (ICWSM).
Baeza-Yates, R. and Ribeiro-Neto, B. (1999). Modern Infor-
mation Retrieval. ACM.
Balog, K., Meij, E., Weerkamp, W., He, J., and de Rijke, M.
(2008a). The University of Amsterdam at TREC 2008:
Blog, Enterprise, and Relevance Feedback. In TREC 2008
Working Notes.
Balog, K., Weerkamp, W., and de Rijke, M. (2008b). A few
examples go a long way: constructing query models from
elaborate query formulations. In SIGIR ?08: Proceedings
of the 31st annual international ACM SIGIR conference on
Research and development in information retrieval, pages
371?378, New York, NY, USA. ACM.
Buckley, C. (2004). Why current IR engines fail. In SIGIR
?04, pages 584?585.
Cronen-Townsend, S., Zhou, Y., and Croft, W. B. (2002). Pre-
dicting query performance. In SIGIR02, pages 299?306.
Diaz, F. and Metzler, D. (2006). Improving the estimation of
relevance models using large external corpora. In SIGIR
?06: Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in infor-
mation retrieval, pages 154?161, New York, NY, USA.
ACM.
Elsas, J., Arguello, J., Callan, J., and Carbonell, J. (2008a).
Retrieval and feedback models for blog distillation. In The
Sixteenth Text REtrieval Conference (TREC 2007) Pro-
ceedings.
Elsas, J. L., Arguello, J., Callan, J., and Carbonell, J. G.
(2008b). Retrieval and feedback models for blog feed
search. In SIGIR ?08: Proceedings of the 31st annual in-
ternational ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 347?354, New
York, NY, USA. ACM.
Ernsting, B., Weerkamp, W., and de Rijke, M. (2008). Lan-
guage modeling approaches to blog post and feed finding.
In The Sixteenth Text REtrieval Conference (TREC 2007)
Proceedings.
Fautsch, C. and Savoy, J. (2008). UniNE at TREC 2008: Fact
and Opinion Retrieval in the Blogsphere. In TREC 2008
Working Notes.
Harman, D. and Buckley, C. (2004). The NRRC reliable in-
formation access (RIA) workshop. In SIGIR ?04, pages
528?529.
Hauff, C., Murdock, V., and Baeza-Yates, R. (2008). Im-
proved query difficulty prediction for the web. In CIKM
?08: Proceedings of the seventeenth ACM conference on
Conference on information and knowledge management,
pages 439?448.
He, J., Larson, M., and de Rijke, M. (2008). Using
coherence-based measures to predict query difficulty.
In 30th European Conference on Information Retrieval
(ECIR 2008), page 689694. Springer, Springer.
Hiemstra, D. (2001). Using Language Models for Informa-
tion Retrieval. PhD thesis, University of Twente.
Java, A., Kolari, P., Finin, T., Joshi, A., and Martineau, J.
(2007). The blogvox opinion retrieval system. In The Fif-
teenth Text REtrieval Conference (TREC 2006) Proceed-
ings.
Kurland, O., Lee, L., and Domshlak, C. (2005). Better than
the real thing?: Iterative pseudo-query processing using
cluster-based language models. In SIGIR ?05, pages 19?
26.
Kwok, K. L., Grunfeld, L., Dinstl, N., and Chan, M. (2001).
TREC-9 cross language, web and question-answering
track experiments using PIRCS. In TREC-9 Proceedings.
Lafferty, J. and Zhai, C. (2003). Probabilistic relevance mod-
els based on document and query generation. In Language
Modeling for Information Retrieval, Kluwer International
Series on Information Retrieval. Springer.
Lavrenko, V. and Croft, W. B. (2001). Relevance based lan-
guage models. In SIGIR ?01, pages 120?127.
Manning, C. D., Raghavan, P., and Schu?tze, H. (2008). Intro-
duction to Information Retrieval. Cambridge University
Press.
Metzler, D. and Croft, W. B. (2005). A markov random field
model for term dependencies. In SIGIR ?05, pages 472?
479, New York, NY, USA. ACM.
Miller, D., Leek, T., and Schwartz, R. (1999). A hidden
Markov model information retrieval system. In SIGIR ?99,
pages 214?221.
Mishne, G. and de Rijke, M. (2006). A study of blog search.
In Lalmas, M., MacFarlane, A., Ru?ger, S., Tombros, A.,
Tsikrika, T., and Yavlinsky, A., editors, Advances in In-
formation Retrieval: Proceedings 28th European Confer-
ence on IR Research (ECIR 2006), volume 3936 of LNCS,
pages 289?301. Springer.
Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and
Soboroff, I. (2007). Overview of the TREC 2006 Blog
Track. In The Fifteenth Text Retrieval Conference (TREC
2006). NIST.
Ponte, J. M. and Croft, W. B. (1998). A language modeling
approach to information retrieval. In SIGIR ?98, pages
275?281.
Qiu, Y. and Frei, H.-P. (1993). Concept based query expan-
sion. In SIGIR ?93, pages 160?169.
Rocchio, J. (1971). Relevance feedback in information re-
trieval. In The SMART Retrieval System: Experiments in
Automatic Document Processing. Prentice Hall.
Sakai, T. (2002). The use of external text data in cross-
language information retrieval based on machine transla-
tion. In Proceedings IEEE SMC 2002.
Tao, T. and Zhai, C. (2006). Regularized estimation of mix-
ture models for robust pseudo-relevance feedback. In SI-
GIR ?06: Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in
information retrieval, pages 162?169, New York, NY,
USA. ACM.
Weerkamp, W. and de Rijke, M. (2008a). Credibility im-
proves topical blog post retrieval. In ACL-08: HLT, pages
923?931.
Weerkamp, W. and de Rijke, M. (2008b). Looking at things
differently: Exploring perspective recall for informal text
retrieval. In 8th Dutch-Belgian Information Retrieval
Workshop (DIR 2008), pages 93?100.
Yan, R. and Hauptmann, A. (2007). Query expansion us-
ing probabilistic local feedback with application to mul-
timedia retrieval. In CIKM ?07: Proceedings of the six-
teenth ACM conference on Conference on information and
knowledge management, pages 361?370, New York, NY,
USA. ACM.
Zhang, W. and Yu, C. (2007). UIC at TREC 2006 Blog Track.
In The Fifteenth Text REtrieval Conference (TREC 2006)
Proceedings.
1065
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 468?471,
Prague, June 2007. c?2007 Association for Computational Linguistics
UVA: Language Modeling Techniques for Web People Search
Krisztian Balog
ISLA, University of Amsterdam
kbalog@science.uva.nl
Leif Azzopardi
University of Glasgow
leif@dcs.gla.ac.uk
Maarten de Rijke
ISLA, University of Amsterdam
mdr@science.uva.nl
Abstract
In this paper we describe our participation in
the SemEval 2007 Web People Search task.
Our main aim in participating was to adapt
language modeling tools for the task, and to
experiment with various document represen-
tations. Our main finding is that single pass
clustering, using title, snippet and body to
represent documents, is the most effective
setting.
1 Introduction
The goal of the Web People Search task at SemEval
2007 was to disambiguate person names in a web
searching scenario (Artiles et al, 2007). Participants
were presented with the following setting: given a
list of documents retrieved from a web search engine
using a person?s name as a query, group documents
that refer to the same individual.
Our aim with the participation was to adapt lan-
guage modeling techniques to this task. To this
end, we employed two methods: single pass cluster-
ing (SPC) and probabilistic latent semantic analysis
(PLSA). Our main finding is that the former leads to
high purity, while the latter leads to high inverse pu-
rity scores. Furthermore, we experimented with var-
ious document representations, based on the snip-
pets and body text. Highest overall performance was
achieved with the combination of both.
The remainder of the paper is organized as fol-
lows. In Section 2 we present the two approaches we
employed for clustering documents. Next, in Sec-
tion 3 we discuss document representation and pre-
processing. Section 4 reports on our experiments.
We conclude in Section 5.
2 Modeling
2.1 Single Pass Clustering
We employed single pass clustering (Hill., 1968) to
automatically assign pages to clusters, where we as-
sume that each cluster is a set of pages related to one
particular sense of the person.
The process for assignment was performed as fol-
lows: The first document was taken and assigned
to the first cluster. Then each subsequent document
was compared against each cluster with a similarity
measure based on the log odds ratio (initially, there
was only the initial one created). A document was
assigned to the most likely cluster, as long as the
similarity score was higher than a threshold ?; oth-
erwise, the document was assigned to a new cluster,
unless the maximum number of desired clusters ?
had been reached; in that case the document was as-
signed to the last cluster (i.e., the left overs).
The similarity measure we employed was the log
odds ratio to decide whether the document was more
likely to be generated from that cluster or not. This
approach follows Kalt (1996)?s work on document
classification using the document likelihood by rep-
resenting the cluster as a multinomial term distribu-
tion (i.e., a cluster language model) and predicting
the probability of a document D, given the cluster
language model, i.e., p(D|?C). It is assumed that
the terms t in a document are sampled independently
and identically, so the log odds ratio is calculated as
468
follows:
logO(D,C) = log
p(D|?C)
p(D|?C?)
(1)
= log
?
t?D p(t|?C)
n(t,D)
?
t?D p(t|?C?)n(t,D)
,
where n(t,D) is the number of times a term ap-
pears in a document, and the ?C represents the lan-
guage model that represents not being in the cluster.
Note this is similar to a well-known relevance mod-
eling approach, where the clusters are relevance and
non-relevance, except, here, it is applied in the con-
text of classification as done by Kalt (1996).
The cluster language model was estimated by per-
forming a linear interpolation between the empirical
probability of a term occurring in the cluster p(t|C)
and the background model p(t), the probability of
a term occurring at random in the collection, i.e.,
p(t|?C) = ? ? p(t|C) + (1 ? ?) ? p(t), where ? was
set to 0.5.1 The ?not in the cluster? language model
was approximated by using the background model
p(t). The similarity threshold above (used for de-
ciding whether to assign a document to an existing
cluster) was set to ? = 1, and ? was set to 100.
2.2 Probabilistic Latent Semantic Analysis
The second method for disambiguation we em-
ployed was probabilistic latent semantic analysis
(PLSA) (Hofmann, 1999). PLSA clusters docu-
ments based on the term-document co-occurrence
which results in semantic decomposition of the term
document matrix into a lower dimensional latent
space. Formally, PLSA can be defined as:
p(t, d) = p(d)
?
z
p(t|z)p(z|d), (2)
where p(t, d) is the probability of term t and doc-
ument d co-occurring, p(t|z) is the probability of a
term given a latent topic z and p(z|d) is the probabil-
ity of a latent topic in a document. The prior prob-
ability of the document, p(d), was assumed to be
uniform. This decomposition can be obtained auto-
matically using the EM algorithm (Hofmann, 1999).
Once estimated, we assumed that each latent topic
represents one of the different senses of the person,
1This value was not tuned but selected based on best per-
forming range suggested by Lavrenko and Croft (2001).
so the document is assigned to one of the person-
topics. Here, we made the assignment based on the
maximum p(z|d), so if p(z|d) = max p(z|d), then d
was assigned to z.
In order to automatically select the number of
person-topics, we performed the following process
to decide when the appropriate number of person-
topics (defined by k) have been identified: (1) we
set k = 2 and computed the log-likelihood of the de-
composition on a held out sample of data; (2) we in-
cremented k and computed the log-likelihood; if the
log-likelihood had increased over a given threshold
(0.001) then we repeated step 2, else (3) we stopped
as we have maximized the log-likelihood of the de-
compositions, with respect to the number person-
topics. This point was assumed to be the optimal
with respect to the number of person senses. Since,
we are focusing on identifying the true number of
classes, this should result in higher inverse purity,
whereas with the single pass clustering the number
of clusters is not restricted, and so we would expect
single pass clustering to produce more clusters but
with a higher purity.
We used Lemur2 and the PennAspect implemen-
tation of PLSA (Schein et al, 2002) for our exper-
iments, where the parameters for PLSA where set
as follows. For each k we performed 10 initializa-
tions where the best initialization in terms of log-
likelihood was selected. The EM algorithm was
run using tempering with up to 100 EM Steps. For
tempering the setting suggested in (Hofmann, 1999)
were used. The models were estimated on 90% of
the data and 10% of the data was held out in order to
compute the log-likelihood of the decompositions.
3 Document Representation
This section describes the various document repre-
sentations we considered, and preprocessing steps
we applied.
For each document, we considered the title, snip-
pet, and body text. Title and snippet were pro-
vided by the output of the search engine results
(person name.xml files), while the body text
was extracted from the crawled index.html files.
2http://www.lemurproject.org
469
Method Title+Snippet Body Title+Snippet+Body
Pur InvP F0.5 F0.2 Pur InvP F0.5 F0.2 Pur InvP F0.5 F0.2
Train data
SPC 0.903 0.298 0.422 0.336 0.776 0.416 0.482 0.434 0.768 0.438 0.506 0.456
PLSA 0.589 0.833 0.636 0.716 0.591 0.656 0.563 0.592 0.579 0.724 0.588 0.641
Test data
SPC 0.867 0.541 0.640 0.575 0.818 0.570 0.647 0.596 0.810 0.607 0.669 0.628
PLSA 0.292 0.892 0.383 0.533 0.311 0.869 0.413 0.563 0.305 0.923 0.405 0.566
Table 1: Results of the clustering methods using various document representations.
3.1 Acquiring Plain-Text Content from HTML
Our aim is to extract the plain-text content from
HTML pages and to leave out blocks or segments
that contain little or no useful textual information
(headers, footers, navigation menus, adverts, etc.).
To this end, we exploit the fact that most web-
pages consist of blocks of text content with rel-
atively little markup, interspersed with navigation
links, images with captions, etc. These segments of
a page are usually separated by block-level HTML
tags. Our extractor first generates a syntax tree from
the HTML document. We then traverse this tree
while bookkeeping the stretch of uninterrupted non-
HTML text we have seen. Each time we encounter a
block-level HTML tag we examine the buffer of text
we have collected, and if it is longer than a threshold,
we output it. The threshold for the minimal length of
buffer text was empirically set to 10. In other words,
we only consider segments of the page, separated
by block-level HTML tags, that contain 10 or more
words.
3.2 Indexing
We used a standard (English) stopword list but we
did not apply stemming. A separate index was built
for each person, using the Lemur toolkit. We created
three index variations: title+snippet, body,
and title+snippet+body.
In our official run we used the
title+snippet+body index; however, in
the next section we report on all three variations.
4 Results
Table 1 reports on the results of our experiments us-
ing the Single Pass Clustering (SPC) and Probabilis-
tic Latent Semantic Analysis (PLSA) methods with
various document representations. The measures
(purity, inverse purity, and F-score with ? = 0.5
and ? = 0.2) are presented for both the train and
test data sets.
The results clearly demonstrate the difference in
the behaviors of the two clustering methods. SPC
assigns people to the same cluster with high preci-
sion, as is reflected by the high purity scores. How-
ever, it is overly restrictive, and documents that be-
long to the same person are distributed into a number
of clusters, which should be further merged. This
explains the low inverse purity scores. Further ex-
periments should be performed to evaluate to which
extent this restrictive behavior could be controlled
by the ? parameter of the method.
In contrast with SPC, the PLSA method produces
far fewer clusters per person. These clusters may
cover multiple referents of a name, as is witnessed
by the low purity scores. On the other hand, inverse
purity scores are very high, which means referents
are usually not dispersed among clusters.
As to the various document representations, we
found that highest overall performance was achieved
with the combination of title, snippet, and body text.
Since the data was not homogenous, it would be
interesting to see how performance varies on the dif-
ferent names. We leave this analysis to further work.
Our official run employed the SPC method, using
the title+snippet+body index. The results
of our official submission are presented in Table 2.
Our purity score was the highest of all submissions,
and our system was ranked overall 4th, based on the
F?=0.5 measure.
470
Pur InvP F0.5 F0.2
Lowest 0.30 0.60 0.40 0.55
Highest 0.81 0.95 0.78 0.83
Average 0.54 0.82 0.60 0.69
UVA 0.81 0.60 0.67 0.62
Table 2: Official submission results and statistics.
5 Conclusions
We have described our participation in the SemEval
2007 Web People Search task. Our main aim in par-
ticipating was to adapt language modeling tools for
the task, and to experiment with various document
representations. Our main finding is that single pass
clustering, using title, snippet and body to represent
documents, is the most effective setting.
We explored the two very different clustering
schemes with contrasting characteristics. Looking
forward, possible improvements might be pursued
by combining the two approaches into a more robust
system.
6 Acknowledgments
Krisztian Balog was supported by the Netherlands
Organization for Scientific Research (NWO) un-
der project number 220-80-001. Maarten de Rijke
was supported by NWO under project numbers
017.001.190, 220-80-001, 264-70-050, 354-20-005,
600.065.120, 612-13-001, 612.000.106, 612.066.-
302, 612.069.006, 640.001.501, 640.002.501, and
by the E.U. IST programme of the 6th FP for RTD
under project MultiMATCH contract IST-033104.
References
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
SemEval-2007 WePS Evaluation: Establishing a
benchmark for the Web People Search Task. In Pro-
ceedings of Semeval 2007, Association for Computa-
tional Linguistics.
D. R. Hill. 1968. A vector clustering technique. In
Samuelson, editor, Mechanised Information Storage,
Retrieval and Dissemination, North-Holland, Amster-
dam.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI?99, Stockholm.
T. Kalt. 1996. A new probabilistic model of text classifi-
cation and retrieval. Technical Report CIIR TR98-18,
University of Massachusetts, January 25, 1996.
V. Lavrenko and W. B. Croft. 2001. Relevance-based
language models. In Proceedings of the 24th annual
international ACM SIGIR conference, pages 120?127,
New Orleans, LA. ACM Press.
Andrew I. Schein, Alexandrin Popescul, Lyle H. Un-
gar, and David M. Pennock. 2002. Meth-
ods and metrics for cold-start recommendations.
In SIGIR ?02: Proceedings of the 25th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 253?260, New York, NY, USA. ACM
Press. See http://www.cis.upenn.edu/
datamining/software dist/PennAspect/.
471
