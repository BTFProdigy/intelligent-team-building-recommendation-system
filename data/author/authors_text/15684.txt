Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 379?390, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Natural Language Questions for the Web of Data
Mohamed Yahya1, Klaus Berberich1, Shady Elbassuoni2
Maya Ramanath3, Volker Tresp4, Gerhard Weikum1
1 Max Planck Institute for Informatics, Germany
2 Qatar Computing Research Institute
3 Dept. of CSE, IIT-Delhi, India 4 Siemens AG, Corporate Technology, Munich, Germany
{myahya,kberberi,weikum}@mpi-inf.mpg.de
selbassuoni@qf.org.qa
ramanath@cse.iitd.ac.in volker.tresp@siemens.com
Abstract
The Linked Data initiative comprises struc-
tured databases in the Semantic-Web data
model RDF. Exploring this heterogeneous
data by structured query languages is tedious
and error-prone even for skilled users. To ease
the task, this paper presents a methodology
for translating natural language questions into
structured SPARQL queries over linked-data
sources.
Our method is based on an integer linear pro-
gram to solve several disambiguation tasks
jointly: the segmentation of questions into
phrases; the mapping of phrases to semantic
entities, classes, and relations; and the con-
struction of SPARQL triple patterns. Our so-
lution harnesses the rich type system provided
by knowledge bases in the web of linked data,
to constrain our semantic-coherence objective
function. We present experiments on both the
question translation and the resulting query
answering.
1 Introduction
1.1 Motivation
Recently, very large, structured, and semantically
rich knowledge bases have become available. Ex-
amples are Yago (Suchanek et al2007), DBpe-
dia (Auer et al2007), and Freebase (Bollacker et
al., 2008). DBpedia forms the nucleus of the Web of
Linked Data (Heath and Bizer, 2011), which inter-
connects hundreds of RDF data sources with a total
of 30 billion subject-property-object (SPO) triples.
The diversity of linked-data sources and their high
heterogeneity make it difficult for humans to search
and discover relevant information. As linked data
is in RDF format, the standard approach would be
to run structured queries in triple-pattern-based lan-
guages like SPARQL, but only expert programmers
are able to precisely specify their information needs
and cope with the high heterogeneity of the data
(and absence or very high complexity of schema in-
formation). For less initiated users the only option
to query this rich data is by keyword search (e.g.,
via services like sig.ma (Tummarello et al2010)).
None of these approaches is satisfactory. Instead, the
by far most convenient approach would be to search
in knowledge bases and the Web of linked data by
means of natural-language questions.
As an example, consider a quiz question like
?Which female actor played in Casablanca and is
married to a writer who was born in Rome??.
The answer could be found by querying sev-
eral linked data sources together, like the IMDB-
style LinkedMDB movie database and the DB-
pedia knowledge base, exploiting that there are
entity-level sameAs links between these collections.
One can think of different formulations of the
example question, such as ?Which actress from
Casablanca is married to a writer from Rome??. A
possible SPARQL formulation, assuming a user fa-
miliar with the schema of the underlying knowl-
edge base(s), could consist of the following six
triple patterns (joined by shared-variable bind-
ings): ?x hasGender female, ?x isa actor, ?x
actedIn Casablanca (film), ?x marriedTo ?w,
?w isa writer, ?w bornIn Rome. This complex
query, which involves multiple joins, would yield
good results, but it is difficult for the user to come
379
up with the precise choices for relations, classes, and
entities. This would require familiarity with the con-
tents of the knowledge base, which no average user
is expected to have. Our goal is to automatically cre-
ate such structured queries by mapping the user?s
question into this representation. Keyword search is
usually not a viable alternative when the information
need involves joining multiple triples to construct
the final result, notwithstanding good attempts like
that of Pound et al2010). In the example, the obvi-
ous keyword query ?female actress Casablanca mar-
ried writer born Rome? lacks a clear specification of
the relations among the different entities.
1.2 Problem
Given a natural language question qNL and a knowl-
edge base KB, our goal is to translate qNL into a
formal query qFL that captures the information need
expressed by qNL.
We focus on input questions that put the em-
phasis on entities, classes, and relations between
them. We do not consider aggregations (counting,
max/min, etc.) and negations. As a result, we gener-
ate structured queries of the form known as conjunc-
tive queries or select-project-join queries in database
terminology. Our target language is SPARQL 1.0,
where the above focus leads to queries that consist of
multiple triple patterns, that is, conjunctions of SPO
search conditions. We do not use any pre-existing
query templates, but generate queries from scratch
as they involve a variable number of joins with a-
priori unknown join structure.
A major challenge is in the ambiguity of
the phrases occurring in a natural-language ques-
tion. Phrases can denote entities (e.g., the city
of Casablanca or the movie Casablanca), classes
(e.g., actresses, movies, married people), or rela-
tions/properties (e.g., marriedTo between people,
played between people and movies). A priori, we do
not know if a phrase should be mapped to an entity,
a class, or a relation. In fact, some phrases may de-
note any of these three kinds of targets. For example,
a phrase like ?wrote score for? in a question about
film music composers, could map to the composer-
film relation wroteSoundtrackForFilm, to the class
of movieSoundtracks (a subclass of music pieces),
or to an entity like the movie ?The Score?. Depend-
ing on the choice, we may arrive at a structurally
good query (with triple patterns that can actually
be joined) or at a meaningless and non-executable
query (with disconnected triple patterns). This gen-
eralized disambiguation problem is much more chal-
lenging than the more focused task of named entity
disambiguation (NED). It is also different from gen-
eral word sense disambiguation (WSD), which fo-
cuses on the meaning of individual words (e.g., map-
ping them to WordNet synsets).
1.3 Contribution
In our approach, we introduce new elements towards
making translation of questions into SPARQL triple
patterns more expressive and robust. Most impor-
tantly, we solve the disambiguation and mapping
tasks jointly, by encoding them into a comprehen-
sive integer linear program (ILP): the segmentation
of questions into meaningful phrases, the mapping
of phrases to semantic entities, classes, and rela-
tions, and the construction of SPARQL triple pat-
terns. The ILP harnesses the richness of large knowl-
edge bases like Yago2 (Hoffart et al2011b), which
has information not only about entities and relations,
but also about surface names and textual patterns
by which web sources refer to them. For example,
Yago2 knows that ?Casablanca? can refer to the city
or the film, and ?played in? is a pattern that can de-
note the actedIn relation. In addition, we can lever-
age the rich type system of semantic classes. For ex-
ample, knowing that Casablanca is a film, for trans-
lating ?played in? we can focus on relations with a
type signature whose range includes films, as op-
posed to sports teams, for example. Such informa-
tion is encoded in judiciously designed constraints
for the ILP. Although we intensively harness Yago2,
our approach does not depend on a specific choice of
knowledge base or language resource for type infor-
mation and phrase/name dictionaries. Other knowl-
edge bases such as DBpedia can be easily plugged
in.
Based on these ideas, we have developed a frame-
work and system, called DEANNA (DEep Answers
for maNy Naturally Asked questions), that com-
prises a full suite of components for question de-
composition, mapping constituents into the seman-
tic concept space, generating alternative candidate
mappings, and computing a coherent mapping of all
constituents into a set of SPARQL triple patterns that
380
can be directly executed on one or more linked data
sources.
2 Background
We use the Yago2 knowledge base, with its rich
type system, as a semantic backbone. Yago2 is com-
posed of instances of binary relations derived from
Wikipedia and WordNet. The instances, called facts,
provide both ontological information and instance
data. Figure 1 shows sample facts from Yago2. Each
fact is composed of semantic items that can be di-
vided into relations, entities, and classes. Entities
and classes together are referred to as concepts.
Subject Predicate Object
film subclassOfproduction
Casablanca (film)type film
?Casablanca? means Casablanca (film)
?Casablanca? means Casablanca, Morocco
Ingrid Bergman actedIn Casablanca (film)
Figure 1: Sample knowledge base
Examples of relations are type, subclassOf, and
actedIn. Each relation has a type signature: classes
for the relation?s domain and range. Classes, such as
person and film group entities. Entities are repre-
sented in canonical form such as Ingrid Bergman
and Casablanca (film). A special type of entities
are literals, such as strings, numbers, and dates.
3 Framework
Given a natural language question, Figure 2 shows
the tasks DEANNA performs to translate a ques-
tion into a structured query. The first three steps
prepare the input for constructing a disambiguation
graph for mapping the phrases in a question onto
entities, classes, and relations, in a coherent man-
ner. The fourth step formulates this generalized dis-
ambiguation problem as an ILP with complex con-
straints and computes the best solution using an
ILP solver. Finally, the fifth and sixth step together
use the disambiguated mapping to construct an exe-
cutable SPARQL query.
A question sentence is a sequence of tokens,
qNL = (t0, t1, ..., tn). A phrase is a contiguous sub-
sequence of tokens (ti, ti+1, ..., ti+l) ? qNL, 0 ?
i, 0 ? l ? n. The input question is fed into the fol-
lowing pipeline of six steps:
1. Phrase detection. Phrases are detected that
potentially correspond to semantic items such as
?Who?, ?played in?, ?movie? and ?Casablanca?.
2. Phrase mapping to semantic items. This in-
cludes finding that the phrase ?played in? can ei-
ther refer to the semantic relation actedIn or to
playedForTeam and that the phrase ?Casablanca?
can potentially refer to Casablanca (film) or
Casablanca, Morocco. This step merely constructs
a candidate space for the mapping. The actual dis-
ambiguation is addressed by step 4, discussed below.
3. Q-unit generation. Intuitively, a q-unit is a triple
composed of phrases. Their generation and role will
be discussed in detail in the next section.
4. Joint disambiguation, where the ambiguities in
the phrase-to-semantic-item mapping are resolved.
This entails resolving the ambiguity in phrase bor-
ders, and above all, choosing the best fitting can-
didates from the semantic space of entities, classes,
and relations. Here, we determine for our running
example that ?played in? refers to the semantic re-
lation actedIn and not to playedForTeam and the
phrase ?Casablanca? refers to Casablanca (film)
and not Casablanca, Morocco.
5. Semantic items grouping to form semantic
triples. For example, we determine that the relation
marriedTo connects person referred to by ?Who?
and writer to form the semantic triple person
marriedTo writer. This is done via q-units.
6. Query generation. For SPARQL queries, seman-
tic triples such as person marriedTo writer have
to be mapped to suitable triple patterns with appro-
priate join conditions expressed through common
variables: ?x type person, ?x marriedTo ?w, and
?w type writer for the example.
3.1 Phrase Detection
A detected phrase p is a pair < Toks, l > where
Toks is a phrase and l is a label, l ?
{concept, relation}, indicating whether a phrase is
a relation phrase or a concept phrase. Pr is the set of
all detected relation phrases and Pc is the set of all
detected concept phrases.
One special type of detected relation phrase is
the null phrase, where no relation is explicitly men-
tioned, but can be induced. The most prominent ex-
ample of this is the case of adjectives, such as ?Aus-
tralian movie?, where we know there is a relation
being expressed between ?Australia? and ?movie?.
We use multiple detectors for detecting phrases of
381
 qu
est
ion
ph
ras
es
str
uct
ure
d
qu
ery
 
Concept
&Relation
Phrase
Detection  
Joint
Disambi-
guation
  
ma
pp
ing
s
can
did
ate
gra
ph
 
entities & namesclasses & subclassesrelations & pattternsincl. dictionaries & statistics
  
Knowledge Base
 
Concept
&Relation
Phrase
Mapping
1 2 3 4
     
5
  SemanticItems
Grouping
Query
Gene-
ration
6
sel
ect
ed
s-n
od
es
trip
le
pa
tte
rns
 
Q-unit  
Genera- 
tion
Figure 2: Architecture of DEANNA.
different types. For concept detection, we use a de-
tector that works against a phrase-concept dictionary
which looks as follows:
{?Rome?,?eternal city?} ? Rome
{?Casablanca?} ? Casablanca (film)
We experimented with using third-party named en-
tity recognizers but the results were not satisfactory.
This dictionary was mostly constructed as part of
the knowledge base, independently of the question-
to-query translation task in the form of instances of
the means relation in Yago2, an example of which is
shown in Figure 1
For relation detection, we experimented with var-
ious approaches. We mainly rely on a relation detec-
tor based on ReVerb (Fader et al2011) with addi-
tional POS tag patterns, in addition to our own which
looks for patterns in dependency parses.
3.2 Phrase Mapping
After phrases are detected, each phrase is mapped
to a set of semantic items. The mapping of concept
phrases also relies on the phrase-concept dictionary.
To map relation phrases, we rely on a corpus of
textual patterns to relation mappings of the form:
{?play?,?star in?,?act?,?leading role?} ? actedIn
{?married?, ?spouse?,?wife?} ? marriedTo
Distinct phrase occurrences will map to different
semantic item instances. We discuss why this is im-
portant when we discuss the construction of the dis-
ambiguation graph and variable assignment in the
structured query.
3.3 Dependency Parsing & Q-Unit Generation
Dependency parsing identifies triples of to-
kens, or triploids, ?trel, targ1, targ2?, where
trel, targ1, targ2 ? qNL are seeds for phrases, with
the triploid acting as a seed for a potential SPARQL
triple pattern. Here, trel is the seed for the relation
phrase, while targ1 and targ2 are seeds for the two
arguments. At this point, there is no attempt to
assign subject/object roles to the arguments.
Triploids are collected by looking for specific de-
pendency patterns in dependency graphs (de Marn-
effe et al2006). The most prominent pattern we
look for is a verb and its arguments. Other patterns
include adjectives and their arguments, preposition-
ally modified tokens and objects of prepositions.
By combining triploids with detected phrases, we
obtain q-units. A q-unit is a triple of sets of phrases,
?{prel ? Pr}, {parg1 ? Pc}, {parg2 ? Pc}?, where
trel ? prel and similarly for arg1 and arg2. Concep-
tually, one can view a q-unit as a placeholder node
with three sets of edges, each connecting the same
q-node to a phrase that corresponds to a relation or
concept phrase in the same q-unit. This notion of
nodes and edges will be made more concrete when
we present our disambiguation graph construction.
3.4 Disambiguation of Phrase Mappings
The core contribution of this paper is a framework
for disambiguating phrases into semantic items ?
covering relations, classes, and entities in a unified
manner. This can be seen as a joint task combining
382
named entity disambiguation for entities, word sense
disambiguation for classes (common nouns), and re-
lation extraction. The next section presents the dis-
ambiguation framework in detail.
3.5 Query Generation
Once phrases are mapped to unique semantic items,
we proceed to generate queries in two steps. First,
semantic items are grouped into triples. This is done
using the triploids generated earlier. The power of
using a knowledge base is that we have a rich type
system that allows us to tell if two semantic items
are compatible or not. Each relation has a type sig-
nature and we check whether the candidate items are
compatible with the signature.
We did not assign subject/object roles in triploids
and q-units because a natural language relation
phrase might express the inverse of a semantic rela-
tion, e.g., the natural language expression ?directed
by? and the relation isDirectorOf with respect to
the movies domain are inverses of each other. There-
fore, we check which assignment of arg1 and arg2
is compatible with the semantic relation. If both ar-
rangements are compatible, then we give preference
to the assignment given by the dependency parsers.
Once semantic items are grouped into triples, it
is an easy task to expand them to SPARQL triple
patterns. This is done by replacing each seman-
tic class with a distinct type-constrained variable.
Note that this is the reason why each distinct phrase
maps to a distinct instance of a semantic class, to
ensure correct variable assignment. This becomes
clear when we consider the question ?Which singer
is married to a singer??, which requires two distinct
variables each constrained to bind to an entity of
type singer.
4 Joint Disambiguation
The goal of the disambiguation step is to compute
a partial mapping of phrases onto semantic items,
such that each phrase is assigned to at most one
semantic item. This step also resolves the phrase-
boundary ambiguity, by enforcing that only non-
overlapping phrases are mapped. As the result of
disambiguating one phrase can influence the map-
ping of other phrases, we consider all phrases jointly
in one big disambiguation task.
In the following, we construct a disambiguation
graph that encodes all possible mappings. We im-
pose a variety of complex constraints (mutual ex-
clusion among overlapping phrases, type constraints
among the selected semantic items, etc.), and define
an objective function that aims to maximize the joint
quality of the mapping. The graph construction it-
self may resemble similar models used in NED (e.g.,
(Milne and Witten, 2008; Kulkarni et al2009; Hof-
fart et al2011a)). Recall, however, that our task is
more complex because we jointly consider entities,
classes, and relations in the candidate space of pos-
sible mappings. Because of this complication and
to capture our complex constraints, we do not em-
ploy graph algorithms, but model the general disam-
biguation problem as an ILP.
4.1 Disambiguation Graph
Joint disambiguation takes place over a disambigua-
tion graph DG = (V,E), where V = Vs ? Vp ? Vq
and E = Esim ? Ecoh ? Eq, where:
? Vs is the set of semantic items, vs ? Vs is an
s-node.
? Vp is the set of phrases, vp ? Vp is called a p-
node. We denote the set of p-nodes correspond-
ing to relation phrases by Vrp and the set of p-
nodes corresponding to concept phrases by Vrc .
? Vq is a set of placeholder nodes for q?units,
called q-nodes. They represent phrase triples.
? Esim ? Vp ? Vs is a set of weighted similarity
edges that capture the strength of the mapping
of a phrase to a semantic item.
? Ecoh ? Vs ? Vs is a set of weighted coherence
edges that capture the semantic coherence be-
tween two semantic items. Semantic coherence
is discussed in more detail later in this section.
? Eq ? Vq?Vp?d, where d ? {rel, arg1, arg2}
is a q-edge. Each such edge connects a place-
holder q-node to a p-node with a specific role
as a relation, or one of the two arguments. A
q-unit, as presented earlier, can be seen as a q-
node along with its outgoing q-edges.
Figure 3 shows the disambiguation graph for our
running example (excluding coherence edges be-
tween s-nodes).
4.2 Edge Weights
We next describe how the weights on similarity
edges and semantic coherence edges are defined.
383
q1
q
2
q
3
a writer
Casablanca
played
played in
Who
married
married to
is married to
was born
born
Rome
c:writer
r:bornIn
r:bornOnDate
e:Max_Born
e:Born_(film)
e:Sydne_Rome
r:Rome
e:White_House
e:Casablanca
e:Casablanca_(film)
e:Played_(film)
r:actedIn
r:hasMusicalRole
c:person
e:Married_(series)
c: married_person
r:marriedTo
q-nodes p-nodes
s-nodes
arg1 arg2
rel
Figure 3: Disambiguation graph for the running example.
4.2.1 Semantic Coherence
Semantic coherence, Cohsem, captures to what
extent two semantic items occur in the same context.
This is different from semantic similarity (Simsem),
which is usually evaluated using the distance be-
tween nodes in a taxonomy (Resnik, 1995). While
we expect Simsem(George Bush, Woody Allen) to
be higher than Simsem(Woody Allen, Terminator)
we would like Cohsem(Woody Allen, Terminator),
both of which are from the entertainment domain, to
be higher than Cohsem(George Bush, Woody Allen).
For Yago2, we characterize an entity e by its in-
links InLinks(e): the set of Yago2 entities whose
corresponding Wikipedia pages link to the entity.
To be able to compare semantic items of different
semantic types (entities, relations, and classes), we
need to extend this to classes and relations. For class
c with entities e, its inlinks are defined as follows:
InLinks(c) =
?
e?c Inlinks(e)
For relations, we only consider those that map en-
tities to entities (e.g. actedIn, produced), for which
we define the set of inlinks as follows:
InLinks(r) =
?
(e1,e2)?r
(InLinks(e1) ? InLinks(e2))
The intuition behind this is that when the two argu-
ments of an instance of the relation co-occur, then
the relation is being expressed.
We define the semantic coherence (Cohsem) be-
tween two semantic items s1 and s2 as the Jaccard
coefficient of their sets of inlinks.
4.2.2 Similarity Weights
Similarity weights are computed differently for
entities, classes, and relations. For entities, we use a
normalized prior score based on how often a phrase
refers to a certain entity in Wikipedia. For classes,
we use a normalized prior that reflects the number
of members in a class. Finally, for relations, similar-
ity reflects the maximum n-gram similarity between
the phrase and any of the relation?s surface forms.
We use Lucene for indexing and searching the rela-
tion surface forms.
4.3 Disambiguation Graph Processing
The result of disambiguation is a subgraph of the
disambiguation graph, yielding the most coherent
mappings. We employ an ILP to this end. Before
describing our ILP, we state some necessary defini-
tions:
? Triple dimensions: d ? {rel, arg1, arg2}
? Tokens: T = {t0, t1, ..., tn}.
? Phrases: P = {p0, p1, ..., pk}.
? Semantic items: S = {s0, s1, ..., sl}.
? Token occurrences: P(t) = {p ? P | t ? p}.
? Xi ? {0, 1} indicates if p-node i is selected.
? Yij ? {0, 1} indicates if p-node i maps to s-
node j.
? Zkl ? {0, 1} indicates if s-nodes k, l are both
selected so that their coherence edge matters.
? Qmnd ? {0, 1} indicates if the q-edge between
q-node m and p-node n for d is selected.
? Cj , Ej and Rj are {0, 1} constants indicating
if s-node j is a class, entity, or relation, resp.
? wij is the weight for a p?s similarity edge.
? vkl is the weight for an s?s semantic coherence
edge.
? trc ? {0, 1} indicates if the relation s-node r is
type-compatible with the concept s-node c.
Given the above definitions, our objective func-
tion is
maximize ?
?
i,j wijYij + ?
?
k,l vklZkl+
?
?
m,n,dQmnd
384
subject to the following constraints:
1. A p-node can be assigned to one s-node at most:
?
j Yij ? 1, ?i
2. If a p-s similarity edge is chosen, then the respec-
tive p-node must be chosen:
Yij ? Xi, ?j
3. If s-nodes k and l are chosen (Zkl = 1), then there
are p-nodes mapping to each of the s-nodes k and l
( Yik = 1 for some i and Yjl = 1 for some j):
Zkl ?
?
i Yik and Zkl ?
?
j Yjl
4. No token can appear as part of two phrases:
?
i?P(t)Xi ? 1, ?t ? T
5. At most one q-edge is selected for a dimension:
?
nQmnd ? 1, ?m, d
6. If the q-edge mnd is chosen (Qmnd = 1) then
p-node n must be selected:
Qmnd ? Xn, ?m, d
7. Each semantic triple should include a relation:
Er ? Qmn?d +Xn? + Yn?r ? 2 ?m,n?, r, d = rel
8. Each triple should have at least one class:
Cc1 + Cc2 ? Qmn??d1 +Xn?? + Yn???c1+
Qmn???d2 +Xn??? + Yn???c2 ? 5,
?m,n??, n???, r, c1, c2, d1 = arg1, d2 = arg2
This is not invoked for existential questions that
return Boolean answers and are translated to ASK
queries in SPARQL. An example is the question
?Did Tom Cruise act in Top Gun??, which can be
translated to ASK {Tom Cruise actedIn Top Gun}.
9. Type constraints are respected (through q-edges):
trc1 + trc2 ? Qmn?d1 +Xn? + Yn?r+
Qmn??d2 +Xn?? + Yn???c1+
Qmn???d3 +Xn??? + Yn???c2 ? 7
?m,n?, n??, n???, r, c1, c2,
d1 = rel, d2 = arg1, d3 = arg2
The above is a sophisticated ILP, and most likely
NP-hard. However, even with ten thousands of vari-
ables it is within the regime of modern ILP solvers.
In our experiments, we used Gurobi (Gur, 2011), and
achieved run-times ? typically of a few seconds.
q
1
q
2
q
3
a writer
Casablanca
played in
Who
is married to
was born
Rome
c:writer
r:bornIn
r:Rome
e:Casablanca
r:actedIn
c:person
r:marriedTo
q-nodes p-nodes
s-nodes
Figure 4: Computed subgraph for the running example.
Figure 4 shows the resulting subgraph for the dis-
ambiguation graph of Figure 3. Note how common
p-nodes between q-units capture joins.
5 Evaluation
5.1 Datasets
Our experiments are based on two collections of
questions: the QALD-1 task for question answer-
ing over linked data (QAL, 2011) and a collection
of questions used in (Elbassuoni et al2011; El-
bassuoni et al2009) in the context of the NAGA
project, for informative ranking of SPARQL query
answers (Elbassuoni et al2009) evaluated the
SPARQL queries, but the underlying questions are
formulated in natural language.) The NAGA collec-
tion is based on linking data from IMDB with the
Yago2 knowledge base. This is an interesting linked-
data case: IMDB provides data about movies, actors,
directors, and movie plots (in the form of descrip-
tive keywords and phrases); Yago2 adds semantic
types and relational facts for the participating enti-
ties. Yago2 provides nearly 3 million concepts and
100 relations, of which 41 lie within the scope of
our framework.
Typical example questions for these two col-
lections are: ?Which software has been published
by Mean Hamster Software?? for QALD-1, and
?Which director has won the Academy Award for
Best director and is married to an actress that has
won the Academy Award for Best Actress?? for
NAGA. For both collections, some questions are
out-of-scope for our setting, because they mention
entities or relations that are not available in the un-
derlying datasets, contain date or time comparisons,
or involve aggregation such as counting. After re-
385
moving these questions, our test set consists of 27
QALD-1 training questions out of a total of 50 and
44 NAGA questions, out of a total of 87. We used
the 19 questions from the QALD-1 test set that are
within the scope of our method for tuning the hyper-
parameters (?, ?, ?) in the ILP objective function.
5.2 Evaluation Metrics
We evaluated the output of DEANNA at three
stages in the processing pipeline: a) after the dis-
ambiguation of phrases, b) after the generation of
the SPARQL query, and c) after obtaining answers
from the underlying linked-data sources. This way,
we could obtain insights into our building blocks,
in addition to assessing the end-to-end performance.
In particular, we could assess the goodness of the
question-to-query translation independently of the
actual answer quality which may depend on partic-
ularities of the underlying datasets (e.g., slight mis-
matches between query terminology and the names
in the data.)
At each of the three stages, the output was shown
to two human assessors who judged whether an out-
put item was good or not. If the two were in dis-
agreement, then a third person resolved the judg-
ment.
For the disambiguation stage, the judges looked
at each q-node/s-node pair, in the context of the
question and the underlying data schemas, and de-
termined whether the mapping was correct or not
and whether any expected mappings were missing.
For the query-generation stage, the judges looked
at each triple pattern and determined whether the
pattern was meaningful for the question or not and
whether any expected triple pattern was missing.
Note that, because our approach does not use any
query templates, the same question may generate se-
mantically equivalent queries that differ widely in
terms of their structure. Hence, we rely on our eval-
uation metrics that are based on triple patterns, as
there is no gold-standard query for a given ques-
tion. For the query-answering stage, the judges were
asked to identify if the result sets for the generated
queries are satisfactory.
With these assessments, we computed over-
all quality measures by both micro-averaging and
macro-averaging. Micro-averaging aggregates over
all assessed items (e.g., q-node/s-node pairs or triple
patterns) regardless of the questions to which they
belong. Macro-averaging first aggregates the items
for the same question, and then averages the quality
measure over all questions.
For a question q and item set s in one of the stages
of evaluation, let correct(q, s) be the number of cor-
rect items in s, ideal(q) be the size of the ideal item
set and retrieved(q, s) be the number of retrieved
items, we define coverage and precision as follows:
cov(q, s) = correct(q, s)/ideal(q)
prec(q, s) = correct(q, s)/retrieved(q, s).
5.3 Results & Discussion
5.3.1 Disambiguation
Table 1 shows the results for disambiguation in
terms of macro and micro coverage and precision.
For both datasets, coverage is high as few mappings
are missing. We obtain perfect precision for QALD-
1 as no mapping that we generate is incorrect, while
for NAGA we generate few incorrect mappings.
5.3.2 Query Generation
Table 2 shows the same metrics for the generated
triple patterns. The results are similar to those for
disambiguation. Missing or incorrect triple patterns
can be attributed to (i) incorrect mappings in the dis-
ambiguation stage or (ii) incorrect detection of de-
pendencies between phrases despite having the cor-
rect mappings.
5.3.3 Question Answering
Table 3 shows the results for query answering.
Here, we attempt to generate answers to questions
by executing the generated queries over the datasets.
The table shows the number of questions for which
the system successfully generated SPARQL queries
(#queries), and among those, how many resulted
in satisfactory answers as judged by our evalua-
tors (#satisfactory). Answers were considered un-
satisfactory when: 1) the generated SPARQL query
was wrong, 2) the result set was empty due to the
incompleteness of the underlying knowledge base,
or 3) a small fraction of the result set was relevant
to the question. For both sets of questions, most of
the queries that were perceived unsatisfactory were
ones that returned no answers. Table 4 shows a
set of example QALD questions, the corresponding
SPARQL queries and sample answers.
386
Benchmark QALD-1 NAGA
covmacro 0.973 0.934
precmacro 1.000 0.934
covmicro 0.963 0.945
precmicro 1.000 0.941
Table 1: Disambiguation
Benchmark QALD-1 NAGA
covmacro 0.975 0.894
precmacro 1.000 0.941
covmicro 0.956 0.847
precmicro 1.000 0.906
Table 2: Query generation
Benchmark QALD-1 NAGA
#questions 27 44
#queries 20 41
#satisfactory 10 15
#relaxed +3 +3
Table 3: Query answering
Question Generated Query Sample Answers
1. Who was the wife of President
Lincoln?
?x marriedTo Abraham Lincoln .
?x type person
Mary Todd Lincoln
2. In which films did Julia Roberts
as well as Richard Gere play?
?x type movie . Richard Gere actedIn ?x .
Julia Roberts actedIn ?x
Runaway Bride
Pretty Woman
3. Which actors were born in
Germany?
?x type actor . ?x bornIn Germany NONE
Table 4: Example questions, the generated SPARQL queries and their answers
Queries that produced no answers, such as the
third query in Table 4 were further relaxed using an
incarnation of the techniques described in (Elbas-
suoni et al2009), by retaining the triple patterns
expressing type constraints and relaxing all other
triple patterns. Relaxing a triple pattern was done
by replacing all entities with variables and casting
entity mentions into keywords that are attached to
the relaxed triple pattern. For example, the QALD
question ?Which actors were born in Germany??
was translated into the following SPARQL query:
?x type actor . ?x bornIn Germany which pro-
duced no answers when run over the Yago2 knowl-
edge base since the relation bornIn relates peo-
ple to cities and not countries in Yago2. The
query was then relaxed into: ?x type actor . ?x
bornIn ?z[Germany]. This relaxed (and keyword-
augmented) triple-pattern query was then processed
the same way as triple-pattern queries without any
keywords. The results of such query were then
ranked based on how well they match the keyword
conditions specified in the relaxed query using the
ranking model in (Elbassuoni et al2009). Using
this technique, the top ranked results for the relaxed
query were all actors born in German cities as shown
in Table 5.
After relaxation, the judges again assessed the re-
sults of the relaxed queries and determined whether
they were satisfactory or not. The number of addi-
tional queries that obtained satisfactory answers af-
ter relaxation are shown under #relaxed in Table 3.
The evaluation data, in addition to a demonstra-
tion of our system (Yahya et al2012), can be found
at http://mpi-inf.mpg.de/yago-naga/deanna/.
6 Related Work
Question answering has a long history in NLP and
IR research. The Web and Wikipedia have proved to
be a valuable resource for answering fact-oriented
questions. State-of-the-art methods (Hirschman and
Gaizauskas, 2001; Kwok et al2001; Zheng, 2002;
Katz et al2007; Dang et al2007; Voorhees, 2003)
cast the user?s question into a keyword query to a
Web search engine (perhaps with phrases for loca-
tion and person names or other proper nouns). Key
to finding good results is to retrieve and rank sen-
tences or short passages that contain all or most key-
words and are likely to yield good answers. Together
with trained classifiers for the question type (and
thus the desired answer type), this methodology per-
forms fairly well for both factoid and list questions.
IBM?s Watson project (Ferrucci et al2010)
demonstrated a new kind of deep QA. A key ele-
ment in Watson?s approach is to decompose com-
plex questions into several cues and sub-cues,
with the aim of generating answers from matches
for the various cues (tapping into the Web and
Wikipedia). Knowledge bases like DBpedia (Auer
et al2007), Freebase (Bollacker et al2008), and
Yago (Suchanek et al2007)) are used for both an-
swering parts of questions that can be translated to
structured form (Chu-Carroll et al2012) and type-
checking possible answer candidates and thus filter-
ing out spurious results (Kalyanpur et al2011).
The recent QALD-1 initiative (QAL, 2011) pro-
posed a benchmark task to translate questions into
SPARQL queries over linked-data sources like DB-
pedia and MusicBrainz. FREyA (Damljanovic et
al., 2011), the best performing system, relies on
387
Q: ?x type actor . ?x wasBornIn ?z[Germany]
Martin Lawrence type actor . Martin Lawrence wasBornIn Frankfurt am Main
Robert Schwentke type actor . Robert Schwentke wasBornIn Stuttgart
Willy Millowitsch type actor . Willy Millowitsch wasBornIn Cologne
Jerry Zaks type actor . Jerry Zaks wasBornIn Stuttgart
Table 5: Top-4 results for the QALD question ?Which actors were born in Germany?? after relaxation
interaction with the user to interpret the question.
Earlier work on mapping questions into structured
queries includes the work by Frank et al2007) and
Unger and Cimiano (2011). Frank et al2007) used
lexical-conceptual templates for query generation.
However, this work did not address the crucial issue
of disambiguating the constituents of the question.
In Pythia, Unger and Cimiano (2011) relied on an
ontology-driven grammar for the question language
so that questions could be directly mapped onto the
vocabulary of the underlying ontology. Such gram-
mars are obviously hard to craft for very large, com-
plex, and evolving knowledge bases. Nalix is an at-
tempt to bring question answering to XML data (Li,
Yang, and Jagadish, 2007) by mapping questions to
XQuery expressions, relying on human interaction
to resolve possible ambiguity.
Very recently, Unger et al2012) developed a
template-based approach based on Pythia, where
questions are automatically mapped to structured
queries in a two step process. First, a set of query
templates are generated for a question, independent
of the knowledge base, determining the structure of
the query. After that, each template is instantiated
with semantic items from the knowledge base. This
performs reasonably well for the QALD-1 bench-
mark: out of 50 test questions, 34 could be mapped,
and 19 were correctly answered.
Efforts on user-friendly exploration of struc-
tured data include keyword search over relational
databases (Bhalotia et al2002) and structured key-
word search (Pound et al2010). The latter is a com-
promise between full natural language and struc-
tured queries, where the user provides the structure
and the system takes care of the disambiguation of
keyword phrases.
Our joint disambiguation method was inspired
by recent work on NED (Milne and Witten, 2008;
Kulkarni et al2009; Hoffart et al2011a) and
WSD (Navigli, 2009). In contrast to this prior work
on related problems, our graph construction and
constraints are more complex, as we address the
joint mapping of arbitrary phrases onto entities,
classes, or relations. Moreover, instead of graph al-
gorithms or factor-graph learning, we use an ILP for
solving the ambiguity problem. This way, we can ac-
commodate expressive constraints, while being able
to disambiguate all phrases in a few seconds.
DEANNA uses dictionaries of names and phrases
for entities, classes, and relations. Spitkovsky and
Chang (2012) recently released a huge dictionary of
pairs of phrases and Wikipedia links, derived from
Google?s Web index. For relations, Nakashole et al
(2012) released PATTY, a large taxonomy of pat-
terns with semantic types.
7 Conclusions and Future Work
We presented a method for translating natural-
language questions into structured queries. The nov-
elty of this method lies in modeling several map-
ping stages as a joint ILP problem. We harness type
signatures and other information from large-scale
knowledge bases. Although our model, in princi-
ple, leads to high combinatorial complexity, we ob-
served that the Gurobi solver could handle our ju-
diciously designed ILP very efficiently. Our experi-
mental studies showed very high precision and good
coverage of the query translation, and good results
in the actual question answers.
Future work includes relaxing some of the limita-
tions that our current approach still has. First, ques-
tions with aggregations cannot be handled at this
point. Second, queries sometimes return empty an-
swers although they perfectly capture the original
question, but the underlying data sources are incom-
plete or represent the relevant information in an un-
expected manner. We plan to extend our approach of
combining structured data with textual descriptions,
and generate queries that combine structured search
predicates with keyword or phrase matching.
388
References
Auer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyga-
niak, R.; and Ives, Z. G. 2007. DBpedia: A Nucleus
for a Web of Open Data. In ISWC/ASWC.
Bhalotia, G.; Hulgeri, A.; Nakhe, C.; Chakrabarti, S.; and
Sudarshan, S. 2002. Keyword Searching and Brows-
ing in Databases using BANKS. In ICDE.
Bollacker, K. D.; Evans, C.; Paritosh, P.; Sturge, T.; and
Taylor, J. 2008. Freebase: a Collaboratively Created
Graph Database for Structuring Human Knowledge.
In SIGMOD.
Chu-Carroll, J.; Fan, J.; Boguraev, B. K.; Carmel, D.; and
Sheinwald, D.; Welty, C. 2012. Finding needles in the
haystack: Search and candidate generation. In IBM J.
Res. & Dev., vol 56, no.3/4.
Damljanovic, D.; Agatonovic, M.; and Cunningham, H.
2011. FREyA: an Interactive Way of Querying Linked
Data using Natural Language.
Dang, H. T.; Kelly, D.; and Lin, J. J. 2007. Overview of
the trec 2007 question answering track. In TREC.
de Marneffe, M. C.; Maccartney, B.; and Manning, C. D.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
Elbassuoni, S.; Ramanath, M.; Schenkel, R.; Sydow, M.;
and Weikum, G. 2009. Language-model-based rank-
ing for queries on rdf-graphs. In CIKM.
Elbassuoni, S.; Ramanath, M.; and Weikum, G. 2011.
Query relaxation for entity-relationship search. In
ESWC.
Fader, A.; Soderland, S.; and Etzioni, O. 2011. Iden-
tifying relations for open information extraction. In
EMNLP.
Ferrucci, D. A.; Brown, E. W.; Chu-Carroll, J.; Fan, J.;
Gondek, D.; Kalyanpur, A.; Lally, A.; Murdock, J. W.;
Nyberg, E.; Prager, J. M.; Schlaefer, N.; and Welty,
C. A. 2010. Building Watson: An Overview of the
DeepQA Project. AI Magazine 31(3).
Frank, A.; Krieger, H.-U.; Xu, F.; Uszkoreit, H.; Crys-
mann, B.; Jo?rg, B.; and Scha?fer, U. 2007. Question
Answering from Structured Knowledge Sources. J.
Applied Logic 5(1).
Gurobi Optimization, Inc. 2012. Gurobi Optimizer Ref-
erence Manual. http://www.gurobi.com/.
Heath, T., and Bizer, C. 2011. Linked Data: Evolving
the Web into a Global Data Space. San Rafael, CA:
Morgan & Claypool, 1 edition.
Hirschman, L., and Gaizauskas, R. 2001. Natural Lan-
guage Question Answering: The View from Here. Nat.
Lang. Eng. 7.
Hoffart, J.; Mohamed, A. Y.; Bordino, I.; Fu?rstenau, H.;
Pinkal, M.; Spaniol, M.; Taneva, B.; Thaterm S.; and
Weikum, G. 2011. Robust Disambiguation of Named
Entities in Text. In EMNLP.
Hoffart, J.; Suchanek, F. M.; Berberich, K.; Lewis-
Kelham, E.; de Melo, G.; and Weikum, G. 2011.
Yago2: exploring and querying world knowledge in
time, space, context, and many languages. In WWW
(Companion Volume).
Kalyanpur, A.; Murdock, J. W.; Fan, J.; and Welty, C. A.
2011. Leveraging community-built knowledge for
type coercion in question answering. In International
Semantic Web Conference.
Katz, B.; Felshin, S.; Marton, G.; Mora, F.; Shen, Y. K.;
Zaccak, G.; Ammar, A.; Eisner, E.; Turgut, A.; and
Westrick, L. B. 2007. CSAIL at TREC 2007 Ques-
tion Answering. In TREC.
Kulkarni, S.; Singh, A.; Ramakrishnan, G.; and
Chakrabarti, S. 2009. Collective annotation of
wikipedia entities in web text. In KDD.
Kwok, C. C. T.; Etzioni, O.; and Weld, D. S. 2001. Scal-
ing Question Answering to the Web. In WWW.
Li, Y.; Yang, H.; and Jagadish, H. V. 2007. NaLIX:
A Generic Natural Language Search Environment for
XML Data. ACM Trans. Database Syst. 32(4).
Milne, D. N., and Witten, I. H. 2008. Learning to link
with wikipedia. In CIKM.
Ndapandula Nakashole, Gerhard Weikum and Fabian
Suchanek 2012. PATTY: A Taxonomy of Relational
Patterns with Semantic Types. In EMNLP.
Navigli, R. 2009. Word sense disambiguation: A survey.
ACM Comput. Surv. 41(2).
Pound, J.; Ilyas, I. F.; and Weddell, G. E. 2010. Ex-
pressive and Flexible Access to Web-extracted Data: A
Keyword-based Structured Query Language. In SIG-
MOD.
2011. 1st Workshop on Question Answering over
Linked Data (QALD-1). http://www.sc.cit-ec.uni-
bielefeld.de/qald-1.
Resnik, P. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. In IJCAI.
Spitkovsky, V. I. Spitkovsky; Chang, A. X. ; 2012. A
Cross-Lingual Dictionary for English Wikipedia Con-
cepts. In LREC.
Suchanek, F. M.; Kasneci, G.; and Weikum, G. 2007.
Yago: a core of semantic knowledge. In WWW.
Tummarello, G.; Cyganiak, R.; Catasta, M.; Danielczyk,
S.; Delbru, R.; and Decker, S. 2010. Sig.ma: Live
views on the web of data. J. Web Sem. 8(4).
Unger, C.; and Cimiano, P. 2011. Pythia: Compositional
Meaning Construction for Ontology-Based Question
Answering on the Semantic Web. In NLDB.
Unger, C.; Bu?hmann, L.; Lehmann, J.; Ngonga Ngomo,
A.-C.; Gerber, D.; and Cimiano, P. 2012. Template-
based question answering over RDF data. In WWW.
Voorhees, E. M. 2003. Overview of the trec 2003 ques-
tion answering track. In TREC.
389
Yahya, M.; Berberich, K.; Elbassuoni, S.; Ramanath, M.;
Tresp, V.; and Weikum, G. 2012. Deep answers
for naturally asked questions on the web of data. In
WWW.
Zheng, Z. 2002. AnswerBus Question Answering Sys-
tem. In HLT.
390
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325?335,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
ReNoun: Fact Extraction for Nominal Attributes
Mohamed Yahya
?
Max Planck Institute for Informatics
myahya@mpi-inf.mpg.de
Steven Euijong Whang, Rahul Gupta, Alon Halevy
Google Research
{swhang,grahul,halevy}@google.com
Abstract
Search engines are increasingly relying on
large knowledge bases of facts to provide
direct answers to users? queries. How-
ever, the construction of these knowledge
bases is largely manual and does not scale
to the long and heavy tail of facts. Open
information extraction tries to address this
challenge, but typically assumes that facts
are expressed with verb phrases, and there-
fore has had difficulty extracting facts for
noun-based relations.
We describe ReNoun, an open information
extraction system that complements pre-
vious efforts by focusing on nominal at-
tributes and on the long tail. ReNoun?s ap-
proach is based on leveraging a large on-
tology of noun attributes mined from a text
corpus and from user queries. ReNoun
creates a seed set of training data by us-
ing specialized patterns and requiring that
the facts mention an attribute in the ontol-
ogy. ReNoun then generalizes from this
seed set to produce a much larger set of ex-
tractions that are then scored. We describe
experiments that show that we extract facts
with high precision and for attributes that
cannot be extracted with verb-based tech-
niques.
1 Introduction
One of the major themes driving the current evo-
lution of search engines is to make the search
experience more efficient and mobile friendly
for users by providing them concrete answers to
queries. These answers, that apply to queries
about entities that the search engine knows about
(e.g., famous individuals, organizations or loca-
tions) complement the links that the search en-
?
Work done during an internship at Google Research.
gine typically returns (Sawant and Chakrabati,
2013; Singhal, 2012; Yahya et al., 2012). To
support such answers, the search engine main-
tains a knowledge base that describes various at-
tributes of an entity (e.g., (Nicolas Sarkozy,
wife, Carla Bruni)). Upon receiving a query,
the search engine tries to recognize whether the
answer is in its knowledge base.
For the most part, the aforementioned knowl-
edge bases are constructed using manual tech-
niques and carefully supervised information ex-
traction algorithms. As a result, they obtain high
coverage on head attributes, but low coverage on
tail ones, such as those shown in Table 1. For ex-
ample, they may have the answer for the query
?Sarkozy?s wife?, but not for ?Hollande?s ex-
girlfriend? or ?Google?s philanthropic arm?. In
addition to broadening the scope of query answer-
ing, extending the coverage of the knowledge base
to long tail attributes can also facilitate providing
Web answers to the user. Specifically, the search
engine can use lower-confidence facts to corrob-
orate an answer that appears in text in one of the
top Web results and highlight them to the user.
This paper describes ReNoun, an open-
information extraction system that focuses on ex-
tracting facts for long tail attributes. The obser-
vation underlying our approach is that attributes
from the long tail are typically expressed as nouns,
whereas most previous work on open-information
extraction (e.g., Mausam et al. (2012)) extend
techniques for extracting attributes expressed in
verb form. Hence, the main contribution of our
work is to develop an extraction system that com-
plements previous efforts, focuses on nominal at-
tributes and is effective for the long tail. To that
end, ReNoun begins with a large but imperfect on-
tology of nominal attributes that is extracted from
text and the query stream (Gupta et al., 2014).
ReNoun proceeds by using a small set of high-
precision extractors that exploit the nominal na-
325
Attribute Fact Phrase Verb form seen
legal affairs (NPR, legal affairs NPR welcomed Nina Totenberg as 7
correspondent correspondent, Nina Totenberg) its new legal affairs correspondent.
economist (Princeton, economist, Princeton economist Paul Krugman 7
Paul Krugman) was awarded the Nobel prize in 2008.
ex-boyfriend (Trierweiler, ex-boyfriend, Trierweiler did not have any children 3
Hollande with her ex-boyfriend Hollande.
staff writer (The New Yorker, staff writer, Adam Gopnik is one of The New 3
Adam Gopnik) Yorker?s best staff writers.
Table 1: Examples of noun phrases as attributes, none which are part of a verb phrase. Additionally, the first two attributes do
not occur within a verb phrase in a large corpus (see ? 2 for details) in a setting where they can be associated with a triple.
ture of the attributes to obtain a training set, and
then generalizes from the training set via distant
supervision to find a much larger set of extraction
patterns. Finally, ReNoun scores extracted facts
by considering how frequently their patterns ex-
tract triples and the coherence of these patterns,
i.e., whether they extract triples for semantically
similar attributes. Our experiments demonstrate
that ReNoun extracts a large body of high preci-
sion facts, and that these facts are not extracted
with techniques based on verb phrases.
2 Preliminaries
The goal of ReNoun is to extract triples of the form
(S,A,O), where S is subject, A is the attribute, and
O is the object. In our setting, the attribute is al-
ways a noun phrase. We refer to the subject and
object as the arguments of the attribute.
ReNoun takes as input a set of attributes, which
can be collected using the methods described in
Gupta el al. (2014), Lee et al. (2012), and Pasca
and van Durme (2007). In this work, we use Biper-
pedia (Gupta et al., 2014), which is an ontology
of nominal attributes automatically extracted from
Web text and user queries. For every attribute,
Biperpedia supplies the Freebase (Bollacker et al.,
2008) domain type (e.g., whether the attribute ap-
plies to people, organizations or hotels). Since the
attributes themselves are the result of an extraction
algorithm, they may include false positives (i.e.,
attributes that do not make sense).
The focus of ReNoun is on attributes whose val-
ues are concrete objects (e.g., wife, protege,
chief-economist). Other classes of attributes
that we do not consider in this work are (1) nu-
meric (e.g., population, GDP) that are better ex-
tracted from Web tables (Cafarella et al., 2008),
and (2) vague (e.g., culture, economy) whose
value is a narrative that would not fit the current
mode of query answering on search engines.
We make the distinction between the fat head
and long tail of attributes. To define these two sets,
we ordered the attributes in decreasing order of the
number of occurrences in the corpus
1
. We defined
the fat head to be the attributes until the point N
in the ordering such that the sum of the total num-
ber of occurrences of attributes before N equaled
the number of total occurrences of the attributes
after N . In our news corpus, the fat head included
218 attributes (i.e., N = 218) and the long tail
included 60K attributes. Table 2 shows examples
from both.
Fat head
daughter, headquarters
president, spokesperson,
Long tail
chief economist, defender,
philanthropic arm, protege
Table 2: Examples of fat head and long tail attributes.
The output of ReNoun is a set of facts, where
each fact could be generated by multiple extrac-
tions. We store the provenance of each extraction
and the number of times each fact was extracted.
Noun versus verb attributes
ReNoun?s goal is to extract facts for attributes ex-
pressed as noun phrases. A natural question is
whether we can exploit prior work on open in-
formation extraction, which focused on extracting
relations expressed as verbs. For example, if we
can extract facts for the attribute advised or is
advisor of, we can populate the noun attribute
advisor with the same facts. In Section 7.2 we
demonstrate that this approach is limited for sev-
eral reasons.
First, attributes in knowledge bases are typically
expressed as noun phrases. Table 3 shows that
1
The occurrences were weighted by the number of se-
mantic classes they occur with in the ontology because many
classes overlap.
326
Knowledge Base %Nouns %Verbs
Freebase 97 3
DBpedia 96 4
Table 3: Percentage of attributes expressed as nouns phrases
among the 100 attributes with the most facts.
the vast majority of the attributes in both Freebase
and DBpedia (Auer et al., 2007) are expressed as
nouns even for the fat head (and even more so for
the long tail). Hence, if we extract the verb form
of attributes we would need to translate them into
noun form, which would require us to solve the
paraphrasing problem and introduce more sources
of error (Madnani and Dorr, 2010). Second, as we
dig deeper into the long tail, attributes tend to be
expressed in text more in noun form rather than
verb form. One of the reasons is that the attribute
names tend to get longer and therefore unnatural
to express as verbs (e.g. chief privacy officer, au-
tomotive division). Finally, there is often a sub-
tle difference in meaning between verb forms and
noun forms of attributes. For example, it is com-
mon to see the phrase ?Obama advised Merkel on
saving the Euro,? but that would not necessarily
mean we want to say that Obama is an advisor of
Angela Merkel, in the common sense of advisor.
Processed document corpus
ReNoun extracts facts from a large corpus of
400M news articles. We exploit rich synactic
and linguistic cues, by processing these docu-
ments with a natural language processing pipeline
comprising of ? dependency parsing, noun phrase
chunking, named entity recognition, coreference
resolution, and entity resolution to Freebase. The
chunker identifies nominal mentions in the text
that include our attributes of interest. As discussed
later in the paper, we exploit the dependency
parse, coreference and entity resolution heavily
during various stages of our pipeline.
3 Overview of ReNoun
Since ReNoun aims at extracting triples for at-
tributes not present in head-heavy knowledge
bases, one key challenge is that we do not have any
labeled data (i.e. known facts) for such attributes,
especially in the long tail. Therefore ReNoun has
an initial seed fact extraction step that automati-
cally generates a small corpus of relatively precise
seed facts for all attributes, so that distant supervi-
sion can be employed. The second big challenge
is to filter the noise from the resulting extractions.
ReNoun?s extraction pipeline, shown in Fig-
ure 1, is composed of four stages.
Seed fact extraction: We begin by extracting a
small number of high-precision facts for our at-
tributes. For this step, we rely on manually spec-
ified lexical patterns that are specifically tailored
for noun phrases, but are general enough to be in-
dependent of any specific attributes. When apply-
ing such patterns, we exploit coreference to make
the generated seed facts more precise by requiring
the attribute and object noun phrases of a seed fact
to refer to the same real-world entity. This is elab-
orated further in Section 4.
Extraction pattern generation: Utilizing the
seed facts, we use distant supervision (Mintz et al.,
2009) to learn a set of dependency parse patterns
that are used to extract a lot more facts from the
text corpus.
Candidate generation: We apply the learned de-
pendency parse patterns from the previous stage
to generate a much larger set of extractions. We
aggregate all the extractions that give rise to the
same fact and store with them the provenance of
the extraction. The extractions generated here are
called candidates because they are assigned scores
that determine how they are used. The applica-
tion consuming an extraction can decide whether
to discard an extraction or use it, and in this case
the manner in which it is used, based on the scores
we attach to it and the application?s precision re-
quirements.
Scoring: In the final stage, we score the facts, re-
flecting our confidence in their correctness. In-
tuitively, we give a pattern a high score if it ex-
tracts many facts that have semantically similar at-
tributes, and then propagate this score to the facts
extracted by the pattern (Section 6).
4 Seed fact extraction
Since we do not have facts, but only attributes, the
first phase of ReNoun?s pipeline is to extract a set
of high-precision seed facts that are used to train
more general extraction patterns. ReNoun extracts
seed facts using a manually crafted set of extrac-
tion rules (see Table 4). However, the extraction
rules and the application of these rules are tailored
to our task of extracting noun-based attributes.
Specifically, when we apply an extraction rule
to generate a triple (S,A,O), we require that (1) A
is an attribute in our ontology, and (2) the value of
A and the object O corefer to the same real-world
327
	
 	


 	
 

	




Figure 1: Extraction Pipeline: we begin with a set of high-precision extractors and use distant supervision to train other
extractors. We then apply the new extractors and score the resulting triples based on the frequency and coherence of the
patterns that produce them.
1. the A of S, O ? the CEO of Google, Larry Page
2. the A of S is O ? the CEO of Google is Larry Page
3. O, S A ? Larry Page, Google CEO
4. O, S?s A ? Larry Page, Google?s CEO
5. O, [the] A of S ? Larry Page, [the] CEO of Google
6. SAO ? Google CEO Larry Page
7. S A, O ? Google CEO, Larry Page
8. S?s A, O ? Google?s CEO, Larry Page
Table 4: High precision patterns used for seed fact extraction
along with an example of each. Here, the object (O) and the
attribute (A) corefer and the subject (S) is in close proxim-
ity. In all examples, the resulting fact is (Google, CEO,
Larry Page). Patterns are not attribute specific.
entity. For example, in Figure 2, CEO is in our on-
tology and we can use a coreference resolver to in-
fer that CEO and Larry Page refer to the same en-
tity. The use of coreference follows from the sim-
ple observation that objects will often be referred
to by nominals, many of which are our attributes of
interest. Since the sentence matches our sixth ex-
traction rule, ReNoun extracts the triple (Google,
CEO, Larry Page).
Document:
?[Google]
1
[CEO]
2
[Larry Page]
2
started his term in 2011,
when [he]
2
succeeded [Eric Schmidt]
3
. [Schmidt]
3
has
since assumed the role of executive chairman of [the
company]
1
.?
(a)
Coreference clusters:
# Phrases Freebase ID
1 Google, the company /m/045c7b
2 Larry Page, CEO, he /m/0gjpq
3 Eric Schmidt, Schmidt /m/01gqf4
(b)
Figure 2: Coreference clusters: (a) a document annotated
with coreference clusters; (b) a table showing each cluster
with the representative phrases in bold and the Freebase ID
to which each cluster maps.
We rely on a coreference resolver in the spirit of
Haghighi and Klein (2009). The resolver clusters
the mentions of entities in a document so the ref-
erences in each cluster are assumed to refer to the
same real-world entity. The resolver also chooses
for each cluster a representative phrase, which is a
proper noun or proper adjective (e.g., Canadian).
Other phrases in the same cluster can be other
proper nouns or adjectives, common nouns like
CEO or pronouns like he in the example. Each
cluster is possibly linked by an entity resolver to
a Freebase entity using a unique Freebase ID. Fig-
ure 2(b) shows the coreference clusters from the
sample document, with representative phrases in
bold, along with the Freebase ID of each clus-
ter. Note that in our example the phrase execu-
tive chairman, which is also in our ontology of
attributes, is not part of any coreference cluster.
Therefore, the fact centered around this attribute
in the example will not be part of the seed extrac-
tions, but could be extracted in the next phase. The
resulting facts use Freebase IDs for the subject and
object (for readability, we will use entity names
in the rest of this work). In summary, our seed
extraction proceeds in two steps. First, we find
sentences with candidate attribute-object pairs that
corefer and in which the attribute is in our ontol-
ogy. Second, we match these sentences against our
hand-crafted rules to generate the extractions. In
Section 7 we show that the precision of our seed
facts is 65% for fat head attributes and 80% for
long tail ones.
5 Pattern and candidate fact generation
In this section we describe how ReNoun uses the
seed facts to learn a much broader set of extrac-
tion patterns. ReNoun uses the learned patterns
to extract many more candidate facts that are then
assigned scores reflecting their quality.
5.1 Dependency patterns
We use the seed facts to learn patterns over de-
pendency parses of text sentences. A dependency
parse of a sentence is a directed graph whose ver-
tices correspond to tokens labeled with the word
and the POS tag, and the edges are syntactic rela-
tions between the corresponding tokens (de Marn-
effe et al., 2006). A dependency pattern is a sub-
graph of a dependency parse where some words
have been replaced by variables, but the POS tags
328
have been retained (called delexicalization). A de-
pendency pattern enables us to extract sentences
with the same dependency parse as the sentence
that generated the pattern, modulo the delexical-
ized words. We note that one big benefit of using
dependency patterns is that they generalize well,
as they ignore extra tokens in the sentence that do
not belong to the dependency subgraph of interest.
5.2 Generating dependency patterns
The procedure for dependency pattern generation
is shown in Algorithm 1, and Figure 3 shows an
example of its application. The input to the algo-
rithm is the ontology of attributes, the seed facts
(Section 4), and our processed text corpus (Sec-
tion 2).
Algorithm 1: Dependency pattern generation
input : Set of attributes A, Seed facts I , Corpus D.
P := An empty set of dependency pattern-attribute pairs.
foreach sentence s ? D do
foreach triple t = (S,A,O) found in s do
if t ? I then
G(s) = dependency parse of s
P
?
= minimal subgraph of G(s)
containing the head tokens of S, A and O
P = Delexicalize(P
?
, S, A, O)
P = P ? {?P,A?}
return P
Attributes: A ={executive chairman}
Seed fact: I = {(Google, executive chairman, Eric Schmidt)}
Sentence: s =?An executive chairman, like Eric Schmidt of Google, wields influence
over company operations.?An/DET
executive/NNchairman/NN detnn
like/INprep Schmidt/NNPpobj Eric/NNPnn of/INprep Google/NNPpobj
(a)
chairman/NN like/INprep Schmidt/NNPpobj of/INprep Google/NNPpobj
(b)
{A/N} like/INprep {O/N}pobj of/INprep {S/N}pobj
(c)
Figure 3: Dependency pattern generation using seed facts,
corresponding to Algorithm 1: (a) shows the input to the pro-
cedure (dependency parse partially shown); (b) P
?
; (c) P .
The procedure iterates over the sentences in the
corpus, looking for matches between a sentence
s and a seed fact f . A sentence s matches f if
s contains (i) the attribute in f , and (ii) phrases in
coreference clusters that map to the same Freebase
IDs as the subject and object of f . When a match
is found, we generate a pattern as follows.
We denote by P
?
the minimal subgraph of the
dependency parse of s containing the head tokens
of the subject, attribute and object (Figure 3 (b)).
We delexicalize the three vertices corresponding
to the head tokens of the subject, attribute and ob-
ject by variables indicating their roles. The POS
tag associated with the attribute token is always a
noun. The subject and object are additionally al-
lowed to have pronouns and adjectives associated
with their tokens. All POS tags corresponding to
nouns are lifted to N, in order to match the vari-
ous types of nouns. We denote the resulting de-
pendency pattern by P and add it to our output,
associated with the matched attribute. We note
that in principle, the vertex corresponding to the
head of the attribute does not need to be delexi-
calized. However, we do this to improve the ef-
ficiency of pattern-matching, since we will often
have patterns for different attributes differing only
at the attribute vertex.
It is important to note that because of the man-
ner in which the roles of subject and object were
assigned during seed fact extraction, the patterns
ReNoun generates clearly show which argument
will take the role of the subject, and which will
take the role of the object. This is in contrast
to previous work such as Ollie (Mausam at al.,
2012), where the assignment depends on the order
in which the arguments are expressed in the sen-
tence from which the fact is being extracted. For
example, from the sentence ?Opel was described
as GM?s most successful subsidiary.? and the seed
fact (GM, subsidiary, Opel), the pattern that
ReNoun generates will consistently extract facts
like (BMW, subsidiary, Rolls-Royce), and not
the incorrect inverse, regardless of the relative or-
dering of the two entities in the sentence.
At this point we have dependency patterns ca-
pable of generating more extractions for their seed
fact attributes. For efficient matching, we use the
output of Algorithm 1 to generate a map from de-
pendency patterns to their attributes with entries
like that shown in Figure 4(a). This way, a pat-
tern match can be propagated to all its mapped at-
tributes in one shot, as we explain in Section 5.3.
Finally, we discard patterns that do not pass a sup-
port threshold, where support is the number of dis-
tinct seed facts from which a pattern could be gen-
erated.
329
{A/N} like/INprep {O/N}pobj of/INprep {S/N}pobj
attributes: {executive chairman, creative director, ...}
(a)
?An executive chairman, like Steve Chase of AOL, is
responsible for representing the company.?
?
(AOL, executive chairman, Steve Chase)
(b)
?A creative director, like will.i.am of 3D Systems, may also
be referred to as chief creative officer.?
?
(3D Systems, creative director, will.i.am)
(c)
Figure 4: A dependency pattern and its use in extraction: (a)
the pattern in our running example and the set of attributes to
which it applies; (b) and (c) sentences matching the pattern
and the resulting extractions.
5.3 Applying the dependency patterns
Given the learned patterns, we can now generate
new extractions. Each match of a pattern against
the corpus will indicate the heads of the poten-
tial subject, attribute and object. The noun phrase
headed by the token matching the {A/N} vertex is
checked against the set of attributes to which the
pattern is mapped. If the noun phrase is found
among these attributes, then a triple (S, A, O) is
constructed from the attribute and the Freebase en-
tities to which the tokens corresponding to the S
and O nodes in the pattern are resolved. This triple
is then emitted as an extraction along with the pat-
tern that generated it. Figure 4(b) and (c) show two
sentences that match the dependency pattern in our
running example and the resulting extractions.
Finally, we aggregate our extractions by their
generated facts. For each fact f , we save the dis-
tinct dependency patterns that yielded f and the
total number of times it was found in the corpus.
6 Scoring extracted facts
In this section we describe how we score the can-
didate facts extracted by applying the dependency
patterns in Section 5. Recall that a fact may be
obtained from multiple extractions, and assigning
scores to each fact (rather than each extraction) en-
ables us to consider all extractions of a fact in ag-
gregate.
We score facts based on the patterns which ex-
tract them. Our scheme balances two character-
istics of a pattern: its frequency and coherence.
Pattern frequency is defined as the number of ex-
has/VBZ {S/N}nsubj children/NNSdobj with/INprep {A/N}pobj {O/N}appos
attributes: {ex-wife, boyfriend, ex-partner}
frequency(P ) = 574, coherence(P ) = 0.429
Example: ?Putin has two children with his ex-wife,
Lyudmila.?
(a)
{A/N} {S/N}nn{O/N} nn
attributes: {ex-wife, general manager, subsidiary,... }
frequency(P ) = 52349038, coherence(P ) = 0.093
Example: ?Chelsea F.C. general manager Jos?e Mourinho...?
(b)
Figure 5: (a) a coherent pattern extracting facts for semanti-
cally similar attributes and (b) an incoherent pattern.
tractions produced by the pattern. Our first ob-
servation is that patterns with a large number of
extractions are always able to produce correct ex-
tractions (in addition to incorrect ones). We also
observe that generic patterns produce more er-
roneous facts compared to more targeted ones.
To capture this, we introduce pattern coherence,
which reflects how targeted a pattern is based on
the attributes to which it applies. For example,
we observed that if an extraction pattern yields
facts for the coherent set of attributes ex-wife,
boyfriend, and ex-partner, then its output is
consistently good. On the other hand, a pattern
that yields facts for a less coherent set of attributes
ex-wife, general manager, and subsidiary is
more likely to produce noisy extractions. Generic,
more incoherent patterns are more sensitive to
noise in the linguistic annotation of a document.
Figure 5 shows an example pattern for each case,
along with its frequency and coherence.
We capture coherence of attributes using word-
vector representations of attributes that are cre-
ated over large text corpora (Mikolov et al., 2013).
The word-vector representation v(w) for a word
w (multi-word attributes can be preprocessed into
single words) is computed in two steps. First, the
algorithm counts the number of occurrences of a
word w
1
that occurs within the text window cen-
tered at w (typically a window of size 10), pro-
ducing an intermediate vector that potentially has
a non-zero value for every word in the corpus.
The intermediate vector is then mapped to a much
smaller dimension (typically less than 1000) to
produce v(w). As shown in (Mikolov et al., 2013),
two words w
1
and w
2
for which the cosine dis-
330
tance between v(w
1
) and v(w
2
) is small tend to
be semantically similar. Therefore, a pattern is co-
herent if it applies to attributes deemed similar as
per their word vectors.
Given an extraction pattern P that extracts facts
for a set of attributes A, we define the coherence
of P to be the average pairwise coherence of all at-
tributes inA, where the pairwise coherence of two
attributes a
1
and a
2
is the cosine distance between
v(a
1
) and v(a
2
).
Finally, we compute the score of a fact f by
summing the product of frequency and coherence
for each pattern of f as shown in Equation 1.
S(f) =
?
P?Pat(f)
frequency(P )? coherence(P ) (1)
7 Experimental Evaluation
We describe a set of experiments that validate the
contributions of ReNoun. In Sections 7.2 and 7.3
we validate our noun-centric approach: we show
that extractions based on verb phrases cannot yield
the results of ReNoun and that NomBank, the re-
source used by state of the art in semantic role-
labeling for nouns, will not suffice either. In Sec-
tions 7.4-7.6 we evaluate the different components
of ReNoun and its overall quality, and in Sec-
tion 7.7 we discuss the cases in which ReNoun was
unable to extract any facts.
7.1 Setting
We used the fat head (FH) and long tail (LT) at-
tributes and annotated news corpus described in
Section 2. When evaluating facts, we used major-
ity voting among three human judges, unless oth-
erwise noted. The judges were instructed to con-
sider facts with inverted subjects and objects as in-
correct. For example, while (GM, subsidiary,
Opel) is correct, its inverse is incorrect.
7.2 Verb phrases are not enough
State-of-art open information extraction systems
like Ollie (Mausam at al., 2012) assume that a re-
lation worth extracting is expressed somewhere in
verb form. We show this is not the case and jus-
tify our noun-centric approach. In this experiment
we compare ReNoun to a custom implementation
of Ollie that uses the same corpus as ReNoun and
supports multi-word attributes. While Ollie does
try to find relations expressed as nouns, its seed
facts are relations expressed as verbs.
We randomly sampled each of FH and LT for
100 attributes for which ReNoun extracts facts and
ReNoun Ollie
flagship company -
railway minister -
legal affairs correspondent -
spokesperson be spokesperson of
president-elect be president elect of
co-founder be co-founder of
Table 5: ReNoun attributes with and without a corresponding
Ollie relation.
asked a judge to find potentially equivalent Ol-
lie relations. Note that we did not require the
judge to find exactly the same triple (thereby bias-
ing the experiment towards finding more attribute
matches). Furthermore, the judge was instructed
that a verb phrase like advised by should be con-
sidered a match to the ReNoun attribute advisor.
However, looking at the data, most facts involving
the relation advised are not synonymous with the
advisor relation as we think of it (e.g., ?Obama
advised Merkel on saving the Euro?). This obser-
vation suggests that there is an even more subtle
difference between the meaning of verb expres-
sions and noun-based expressions in text. This ex-
periment, therefore, gives an upper bound on the
number of ReNoun attributes that Ollie can cover.
For FH, not surprisingly, we could find matches
for 99 of the 100 attributes. However, for LT, only
31 of the 100 attributes could be found, even under
our permissive setting. Most attributes that could
not be matched were multi-word noun phrases.
While in principle, one could use the Ollie patterns
that apply to the head of a multi-word attribute, we
found that we generate more interesting patterns
for specific multi-word attributes. Table 5 shows
examples of attributes with and without verb map-
pings in Ollie.
We also compare in the other direction and esti-
mate the portion of Ollie relations centered around
nouns for which ReNoun fails to extract facts. For
this experiment, we randomly sampled 100 Ollie
relations that contained common nouns whose ob-
jects are concrete values, and looked for equivalent
attributes in ReNoun extractions. ReNoun extracts
facts for 48 of the Ollie relations. Among the 52
relations with no facts, 25 are not in Biperpedia
(which means that ReNoun cannot extract facts for
them no matter what). For the other 27 relations,
ReNoun did not extract facts for the following
reasons. First, some relations expressed actions,
which cannot be expressed using nouns only, and
are not considered attributes describing the subject
entity (e.g., citation of in ?Obama?s citation
331
of the Bible?). Second, some relations have the
object (a common noun) embedded within them
(e.g., have microphone in) and do not have cor-
responding attributes that can be expressed us-
ing nouns only. The remaining relations either
have meaningless extractions or use common noun
phrases as arguments. ReNoun only uses proper
nouns (i.e., entities) for arguments because facts
with common noun arguments are rarely interest-
ing without more context. We note that the major-
ity of the 25 Ollie relations without corresponding
Biperpedia attributes also fall into one of the three
categories above.
7.3 Comparison against NomBank
In principle, the task of extracting noun-mediated
relations can be compared to that of semantic role
labeling (SRL) for nouns. The task in SRL is to
identify a relation, expressed either through a verb
or a noun, map it to a semantic frame, and map
the arguments of the relation to the various roles
within the frame. State of the art SRL systems,
such as that of Johansson and Nugues (2008), are
trained on NomBank (Meyers et al., 2004) for
handling nominal relations, which also means that
they are limited by the knowledge it has. We asked
a judge to manually search NomBank for 100 at-
tributes randomly drawn from each of FH and LT
for which ReNoun extracts facts. For multi-word
attributes, we declare a match if its head word was
found. We were able to find 80 matches for the
FH attributes and 42 for LT ones. For example,
we could not find entries for the noun attributes
coach or linebacker (of a football team). This
result is easy to explain by the fact that NomBank
only has 4700 attributes.
In addition, for some nouns, the associated
frames do not allow for the extraction of triples.
For example, all frames for the noun member spec-
ify one argument only, so in the sentence ?John
became a member of ACM?, the output relation is
(ACM, member) instead of the desired triple (ACM,
member, John).
As we did with Ollie, we also looked at nouns
from NomBank for which ReNoun does not ex-
tract facts. Out of a random sample of 100 Nom-
Bank nouns, ReNoun did not extract facts for
29 nouns (four of which are not in Biperpedia).
The majority of the missed nouns cannot be used
by ReNoun because they either take single ar-
guments (instead of two) or take either preposi-
tional phrases or common nouns (instead of proper
nouns correponding to entities) as one their argu-
ments.
7.4 Quality of seed facts
In Section 4, we described our method for ex-
tracting seed facts for our attributes. Applying
the method to our corpus resulted in 139M extrac-
tions, which boiled down to about 680K unique
facts covering 11319 attributes. We sampled 100
random facts from each of FH and LT, and ob-
tained 65% precision for FH seed facts and 80%
precision for LT ones. This leads us to two obser-
vations.
First, the precision of seed facts for LT attributes
is high, which makes them suitable for use as
a building block in a distant supervision scheme
to learn dependency parse patterns. We are pri-
marily interested in LT attributes, which earlier
approaches cannot deal with satisfactorily as we
demonstrated above.
Second, LT attributes have higher precision than
FH attributes. One reason is that multi-word at-
tributes (which tend to be in LT) are sometimes
incorrectly chunked, and only their head words are
recognized as attributes (which are more likely to
be in FH). For example, in the phrase ?America?s
German coach, Klinsmann?, the correct attribute
is German coach (LT), but bad chunking may pro-
duce the attribute coach (FH) with Germany as the
subject. Another reason is that FH attributes are
likely to occur in speculative contexts where the
presence of the attribute is not always an asser-
tion of a fact. (While both FH and LT attributes
can be subject to speculative contexts, we observe
this more for FH than LT in our data.) For ex-
ample, before a person is a railway minister
of a country, there is little mention of her along
with the attribute. However, before a person is
elected president, there is more media about her
candidacy. Speculative contexts, combined with
incorrect linguistic analysis of sentences, can re-
sult in incorrect seed facts (e.g., from ?Republi-
can favorite for US president, Mitt Romney, vis-
ited Ohio?, we extract the incorrect seed fact (US,
president, Mitt Romney)).
7.5 Candidate generation
Using the seed facts, we ran our candidate gen-
eration algorithm (Section 5). In the first step of
the algorithm we produced a total of about 2 mil-
lion unique dependency patterns. A third of these
332
patterns could extract values for exactly one at-
tribute. Manual inspection of these long tail pat-
terns showed that they were either noise, or do not
generalize. We kept patterns supported by at least
10 seed facts, yielding more than 30K patterns.
We then applied the patterns to the corpus. The
result was over 460M extractions, aggregated into
about 40M unique facts. Of these, about 22M facts
were for LT attributes, and 18M for FH. We now
evaluate the quality of these facts.
7.6 Scoring extracted facts
In Section 6, we presented a scheme for scoring
facts using pattern frequency and coherence. To
show its effectiveness we (i) compare it against
other scoring schemes, and (ii) show the quality
of the top-k facts produced using this scheme, for
various k. To compute coherence, we generated
attribute word vectors using the word2vec
2
tool
trained on a dump of Wikipedia.
First, we compare the quality of our scoring
scheme (FREQ COH) with three other schemes as
shown in Table 6. The scheme FREQ is identical
to FREQ COH except that all coherences are set
to 1. PATTERN counts the number of distinct pat-
terns that extract the fact while PATTERN COH
sums the pattern coherences. We generated a ran-
dom sample of 252 FH and LT nouns with no en-
tity disambiguation errors by the underlying nat-
ural language processing pipeline. The justifi-
cation is that none of the schemes we consider
here capture such errors. Accounting for such
errors requires elaborate signals from the entity
linking system, which we leave for future work.
For each scoring scheme, we computed the Spear-
man?s rank correlation coefficient ? between the
scores and manual judgments (by three judges). A
larger ? indicates more correlation, and comput-
ing ? was statistically significant (p-value<0.01)
for all schemes.
Scheme Spearman?s ?
FREQ 0.486
FREQ COH 0.495
PATTERN 0.265
PATTERN COH 0.257
Table 6: Scoring schemes
FREQ and FREQ COH dominate, which shows
that considering the frequency with which patterns
perform extraction helps. The two schemes, how-
ever, are very close to each other. We observed
2
https://code.google.com/p/word2vec/
FH LT
k Precision #Attr Precision #Attr
10
2
1.00 8 1.00 50
10
3
0.98 36 1.00 294
10
4
0.96 78 0.98 1548
10
5
0.82 106 0.96 5093
10
6
0.74 124 0.70 7821
All 0.18 141 0.26 11178
Table 7: Precision of random samples of the top-k scoring
facts, along with the attribute yield.
that adding coherence helps when two facts have
similar frequencies, but this effect is tempered
when considering a large number of facts.
Second, we evaluate the scoring of facts gener-
ated by ReNoun by the precision of top-k results
for several values of k. In this evaluation, facts
with disambiguation errors are counted as wrong.
The particular context in which ReNoun is applied
will determine where in the ordering to set the
threshold of facts to consider. We compute pre-
cision based on a sample of 50 randomly chosen
facts for each k. Table 7 shows the precision re-
sults, along with the number of distinct attributes
(#Attr) for which values are extracted at each k.
As we can see, ReNoun is capable of generat-
ing a large number of high quality facts (?70%
precise at 1M), which our scoring method man-
ages to successfully surface to the top. The ma-
jor sources of error were (i) incorrect dependency
parsing mainly due to errors in boilerplate text re-
moval from news documents, (ii) incorrect coref-
erence resolution of pronouns, (iii) incorrect entity
resolution against Freebase, and (iv) cases where
a triple is not sufficient (e.g., ambassador where
both arguments are countries.)
7.7 Missed extractions
We analyze why ReNoun does not extract facts for
certain attributes. For FH, we investigate all the 77
attributes for which ReNoun is missing facts. For
LT, there are about 50K attributes without corre-
sponding facts, and we use a random sample of
100 of those attributes.
Cause FH LT Example
Vague 23 37 culture
Numeric 4 26 rainfall
Object not KB entity 11 6 email
Plural 30 15 member firms
Bad attribute / misspell 3 4 newsies
Value expected 6 12 nationality
Total 77 100
Table 8: Analysis of attributes with no extractions.
333
Table 8 shows the categorization of the missed
attributes. The first three categories are cases that
are currently outside the scope of ReNoun: vague
attributes whose values are long narratives, nu-
meric attributes, and typed attributes (e.g., email)
whose values are not modeled as Freebase enti-
ties. The next two categories are due to limitations
of the ontology, e.g., plural forms of attributes are
not always synonymized with singular forms and
some attributes are bad. Finally, the ?Value ex-
pected? category contains the attributes for which
ReNoun should have extracted values. One reason
for missing values is that the corpus itself does not
contain values of all attributes. Another reason is
that some attributes are not verbalized in text. For
example, attributes like nationality are usually
not explicitly stated when expressed in text.
8 Related Work
Open information extraction (OIE) was introduced
by Banko et al. (2007). For a pair of noun phrases,
their system, TEXTRUNNER, looks for the at-
tribute (or more generally the relation) in the text
between them and uses a classifier to judge the
trustworthiness of an extraction. WOE
parse
(Wu
and Weld, 2010) extends this by using dependency
parsing to connect the subject and object. Both
systems assume that the attribute is between its
two arguments, an assumption that ReNoun drops
since it is not suitable for nominal attributes.
Closest to our work are ReVerb (Fader et al.,
2011) and Ollie (Mausam at al., 2012). ReVerb
uses POS tag patterns to locate verb relations and
then looks at noun phrases to the left and right for
arguments. Ollie uses the ReVerb extractions as
its seeds to train patterns that can further extract
triples. While Ollie?s patterns themselves are not
limited to verb relations (they also support noun
relations), the ReVerb seeds are limited to verbs,
which makes Ollie?s coverage on noun relations
also limited. In comparison, ReNoun take a noun-
centric approach and extracts many facts that do
not exist in Ollie.clo
ClausIE (Del Corro and Gemulla, 2013) is an
OIE framework that exploits knowledge about the
grammar of the English language to find clauses
in a sentence using its dependency parse. The
clauses are subsequently used to generate extrac-
tions at multiple granularities, possibly with more
than triples. While ClausIE comes with a prede-
fined set of rules on how to extract facts from a
dependency parse, ReNoun learns such rules from
its seed facts.
Finally, Nakashole et al. (2014) and Mintz et al.
(2009) find additional facts for attributes that al-
ready have facts in a knowledge base. In contrast,
ReNoun is an OIE framework whose goal is to find
facts for attributes without existing facts.
9 Conclusions
We described ReNoun, an open information ex-
traction system for nominal attributes that focuses
on the long tail. The key to our approach is to start
from a large ontology of nominal attributes and ap-
ply noun-specific manual patterns on a large pre-
processed corpus (via standard NLP components)
to extract precise seed facts. We then learn a set of
dependency patterns, which are used to generate a
much larger set of candidate facts. We proposed a
scoring function for filtering candidate facts based
on pattern frequency and coherence. We demon-
strated that the majority of long tail attributes in
ReNoun do not have corresponding verbs in Ol-
lie. Finally, our experiments show that our scor-
ing function is effective in filtering candidate facts
(top-1M facts are ?70% precise).
In the future, we plan to extend ReNoun to ex-
tract triples whose components are not limited to
Freebase IDs. As an example, extending ReNoun
to handle numerical or typed attributes would in-
volve extending our extraction pattern learning
to accommodate units (e.g., kilograms) and other
special data formats (e.g., addresses).
Acknowledgments
We would like to thank Luna Dong, Anjuli Kan-
nan, Tara McIntosh, and Fei Wu for many discus-
sions about the paper.
334
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary G. Ives
2007. DBpedia: A Nucleus for a Web of Open Data.
In Proceedings of the International Semantic Web
Conference.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni 2007.
Open Information Extraction from the Web. In Pro-
ceedings of the International Joint Conference on
Artificial Intelligence .
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a Col-
laboratively Created Graph Database for Structuring
Human Knowledge. In Proceedings of the Interna-
tional Conference on Management of Data.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang,
Eugene Wu, and Yang Zhang 2008. WebTables:
Exploring the Power of Tables on the Web. In Pro-
ceedings of the VLDB Endowment.
Luciano Del Corro and Rainer Gemulla. 2013.
ClausIE: Clause-based Open Information Extrac-
tion. In Proceedings of the International World Wide
Web Conference.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning 2006. Generating Typed
Dependency Parses from Phrase Structure Parses.
In Proceedings of Language Resources and Evalu-
ation.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of Empirical Methods in
Natural Language Processing.
Rahul Gupta, Alon Halevy, Xuezhi Wang, Steven
Whang, and Fei Wu. 2014. Biperpedia: An On-
tology for Search Applications. In Proceedings of
the VLDB Endowment.
Aria Haghighi and Dan Klein 2009. Simple Corefer-
ence Resolution with Rich Syntactic and Semantic
Features. In Proceedings of Empirical Methods in
Natural Language Processing.
Richard Johansson and Pierre Nugues 2008. The Ef-
fect of Syntactic Representation on Semantic Role
Labeling. In Proceedings of the International Con-
ference on Computational Linguistics.
Taesung Lee, Zhongyuan Wang, Haixun Wang, and
Seung-won Hwang 2013. Attribute Extraction and
Scoring: A Probabilistic Approach. In Proceedings
of the International Conference on Data Engineer-
ing .
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open Language
Learning for Information Extraction. In Proceed-
ings of Empirical Methods in Natural Language
Processing.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. arXiv.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant Supervision for Relation Ex-
traction Without Labeled Data. In Proceedings of
the Association for Computational Linguistics.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating Noun Ar-
gument Structure for NomBank. In Proceedings of
Language Resources and Evaluation.
Nitin Madnani and Bonnie J. Dorr. 2010. Generat-
ing Phrasal and Sentential Paraphrases: A Survey of
Data-Driven Methods. In Computational Linguis-
tics 36(3).
Ndapandula Nakashole, Martin Theobald, and Gerhard
Weikum. 2011. Scalable Knowledge Harvesting
with High Precision and High Recall. In Proceed-
ings of Web Search and Data Mining.
Marius Pasca. 2014. Acquisition of Open-domain
Classes via Intersective Semantics. In Proceedings
of the International World Wide Web Conference.
Marius Pasca and Benjamin Van Durme. 2007. What
You Seek Is What You Get: Extraction of Class At-
tributes from Query Logs. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence .
Uma Sawant and Soumen Chakrabarti. 2013. Learn-
ing Joint Query Interpretation and Response Rank-
ing. In Proceedings of the International World Wide
Web Conference.
Amit Singhal. 2012 Introducing the
Knowledge Graph: things, not strings
http://googleblog.blogspot.com/2012/
05/introducing-knowledge-graph-
things-not.html
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction Using Wikipedia. In Proceedings of the
the Association for Computational Linguistics.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for the
Web of Data. In Proceedings of Empirical Methods
in Natural Language Processing.
335
