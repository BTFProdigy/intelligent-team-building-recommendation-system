Product Named Entity Recognition Based on Hierarchical Hidden
Markov Model?
Feifan Liu, Jun Zhao, Bibo Lv, Bo Xu
National Laboratory of Pattern Recognition
Institute of Automation Chinese Academy of Sciences
Beijing P.O. Box 2728, 100080
{ffliu,jzhao,bblv,xubo}@nlpr.ia.ac.cn
Hao Yu
FUJITSU R&D
Xiao Yun Road No.26
Chao Yang District, Beijing, 100016
yu@frdc.fujitsu.com
Abstract
A hierarchical hidden Markov model
(HHMM) based approach of product
named entity recognition (NER) from
Chinese free text is presented in this pa-
per. Characteristics and challenges in
product NER is also investigated and
analyzed deliberately compared with
general NER. Within a unified statis-
tical framework, the approach we pro-
posed is able to make probabilistically
reasonable decisions to a global opti-
mization by leveraging diverse range
of linguistic features and knowledge
sources. Experimental results show that
our approach performs quite well in two
different domains.
1 Introduction
Named entity recognition(NER) plays a sig-
nificantly important role in information extrac-
tion(IE) and many other applications. Previous
study on NER is mainly focused either on the
proper name identification of person(PER), lo-
cation(LOC), organization(ORG), time(TIM) and
numeral(NUM) expressions almost in news do-
main, which can be viewed as general NER, or
other named entity (NE) recognition in specific
domain such as biology.
As far as we know, however, there is little prior
research work conducted by far on product named
0This work was supported by the Natural Sciences Foun-
dation of China(60372016,60272041) and the Natural Sci-
ence Foundation of Beijing(4052027).
entity recognition which can be crucial and valu-
able in many business IE applications, especially
with the increasing research interest in Market
Intelligence Management(MIM), Enterprise Con-
tent Management (ECM) [Pierre 2002] and etc.
This paper describes a prototype system for
product named entity recognition, ProNER, in
which a HHMM-based approach is employed.
Within a unified statistical framework, the ap-
proach based on a mixture model is able to make
probabilistically reasonable decisions to a global
optimization by exploiting diverse range of lin-
guistic features and knowledge sources. Experi-
mental results show that ProNER performs quite
well in two different domains.
2 Related Work
Up to now not much work has been done on
product named entity recognition, nor systematic
analysis of characteristics for this task. [Pierre
2002] developed an English NER system capable
of identifying product names in product views. It
employed a simple Boolean classifier for identi-
fying product name, which was constructed from
the list of product names. The method is sim-
ilar to token matching and has a limitation for
product NER applications. [Bick et al 2004] rec-
ognized named entities including product names
based on constraint grammar based parser for
Danish. This rule-based approach is highly de-
pendent on the performance of Danish parser and
suffers from its weakness in system portability.
[C. Niu et al 2003] presented a bootstrapping ap-
proach for English named entity recognition us-
ing successive learners of parsing-based decision
40
System Statistical Model Linguistic Feature Combinative Points
[Zhang et al 2003] HMM semantic role, tokens pattern rules
[Sun et al 2002] class-based LM word form, NE category cue words list
[Tsai et al 2004] ME model tokens knowledge representation
Table 1: Comparison between several Chinese NER systems1
list and HMM, and promising experiment results
(F-measure: 69.8%) on product NE (correspond-
ing to our PRO) were obtained. Its main advan-
tage lies in that manual annotation of a sizable
training corpus can be avoided, but it suffers from
two problems, one is that it is difficult to find suf-
ficient concept-based seeds needed in bootstrap-
ping for the coverage of the variations of PRO
subcategories, another it is highly dependent on
parser performance as well.
Research on product NER is still at its early
stage, especially in Chinese free text collec-
tions. However, considerable amount of work
has been done in the last decade on the gen-
eral NER task and biological NER task. The
typical machine learning approaches for English
NE are transformation-based learning[Aberdeen
et al 1995], hidden Markov model[Bikel et
al. 1997], maximum entropy model[Borthwick,
1999], support vector machine learning[Eunji Yi
et al 2004], unsupervised model[Collins et al
1999]and etc.
For Chinese NER, the prevailing methodology
applied recently also lie in machine learning com-
bining other knowledge base or heuristic rules,
which can be compared on the whole in three as-
pects showed in Table 1.
In short, the trend in NER is to adopt a statis-
tical framework which try to exploit some knowl-
edge base as well as different level of text features
within and outside NEs. Further those ideas, we
present a hybrid approach based on HHMM [S.
Fine et al 1998] which will be described in de-
tail.
3 Problem Statements and Analysis
3.1 Task Definition
3.1.1 Definition of Product Named Entity
In our study, only three kinds of prod-
uct named entities are considered, namely
1Note: LM(language model); ME(maximum entropy).
Brand Name(BRA), Product Type(TYP), Product
Name(PRO), and BRA and TYP are often embed-
ded in PRO. In the following two examples, there
are two BRA NEs, one TYP NE and one PRO
NE all of which belong to the family of product
named entities.
Exam 1: ??(Benq)/BRA ??(brand)?
? ? ? ?(market shares)? ?(steadily)?
?(ascend)b
Exam 2: ? ?(corporation)? ?(will)?
?(deliver) [Canon/BRA 334?(ten thou-
sand)? ?(pixels)? ?(digital)? ?(camera)
Pro90IS/TYP]/PROb
Brand Name refer to proper name of product
trademark such as ???(Benq)? in Exam 1.
Product Type is a kind of product named en-
tities indicating version or series information of
product, which can consist of numbers, English
characters, or other symbols such as ?+? and ?-
? etc.In our study, two principles should be fol-
lowed.
(1) Chinese characters are not considered to
be TYP, nor subpart of TYP although some of
them can contain version or series information.
For instance, in ?2005????(happy new
year)?(version)??(cell phone)?, here ???
??(happy new year)?(version)?should not be
considered as a TYP.
(2) Numbers are essential elements in prod-
uct type entity. For instance, in ?PowerShot
??(series)??(digital)??(camera)?, ?Pow-
erShot? is not considered as a TYP, however,
in ?PowerShot S10 ??(digital)??(camera)?,
?PowerShot S10? can make up of a TYP.
Product Name, as showed above in Exam 2, is
a kind of product named entities expressing self-
contained proper name for some specified product
in real world compared to BRA and TYP which
only express one attribute of product. i.e. a PRO
NE must be assigned with distinctly discrimina-
tive information which can not shared with other
general product-related expressions.
41
(1) Product-related expressions which are em-
bedded with either BRA or TYP can be qual-
ified to be a PRO entity. e.g. ?BenQ?
?(flash)?(disk)? is a PRO entity, but the gen-
eral product-related expression ???(flash)?
?(market)??(investigation)? cannot make up
of a PRO entity.
(2) Product-related expressions indicating
some specific version or series information which
is unique for a BRA can also be considered as a
PRO entity. e.g. ?DIGITAL IXUS??(series)?
?(digital)? ?(camera)? is a PRO because
?DIGITAL IXUS? series is unique for Canon
product, but ?? ?(intelligent)?(version)?
?(cell phone)? is not a PRO because the at-
tribute of ?intelligent version? can be assigned to
any cell phone product.
3.1.2 Product Named Entity Recognition
Product named entity recognition involves the
identification of product-related proper names
in free text and their classification into differ-
ent kinds of product named entities, referring to
PRO, TYP and BRA in this paper.In comparison
with general NER, nested product NEs should be
tagged separately rather than being tagged just as
a single item, shown as Figure 1.
3.2 Challenges for Product Named Entity
Recognition
?For general named entities, there are some
cues which are very useful for entity recogni-
tion, such as ???(city), ????(Inc.), and etc. In
comparison, product named entities have no such
named conventions and cues, resulting in higher
boundary ambiguities and more complex NE can-
didate triggering difficulties.
?In comparison with general NER, more chal-
lenges in product NER result from miscellaneous
classification ambiguities. Many entities with
identical form can be a kind of general named en-
tity, a kind of product named entity, or just com-
mon words.
?In comparison with general named entities,
product named entities show more flexible vari-
ant forms. The same entity can be expressed in
several different forms due to spelling variation,
word permutation and etc. This also compounds
the difficulties in product named entity recogni-
tion.
?In comparison with general named entities,
it is more frequent that product named entities are
nested as Figure 1 illustrates. More efforts have
to be made to identify such named entities sepa-
rately.
3.3 Our Solutions
We adopt the following strategies in triggering
and disambiguating process respectively.
(1) As to product NER, it?s pivotal to control
the triggering candidates efficiently for the bal-
ance between precision and recall. Here we use
the knowledge base such as brand word list, and
other heuristic information which can be easily
acquired.
(2)After triggering candidates, we try to em-
ploy a statistical model to make the most of
multi-level context information mentioned above
in disambiguation. We choose hierarchical hid-
den Markov model (HHMM) [S. Fine et al 1998]
for its more powerful ability to model the multi-
plicity of length scales and recursive nature of se-
quences.
42
4 Hybrid Approach for Product NE
Recognition
4.1 Overall Workflow of ProNER
?Preprocessing: Segment, POS tagging and
general NER is primarily conducted using our off-
shelf SegNer2.0 toolkit on input text.
?Generating Product NE Candidates: First,
BRA or ORG and TYP are triggered by brand
word list and some word features respectively.
Here we categorize the triggering word features
into six classes: alphabet string, alphanumeric
string, digits, alphabet string with fullwidth, dig-
its with fullwidth and other symbols except Chi-
nese characters. Then PRO are triggered by BRA
and TYP candidates as well as some clue words
indicating type information to some extent such
as ???(version), ????(series). In this step the
model structure(topology) of HHMM[S. Fine et
al. 1998] is dynamically constructed, and some
conjunction words or punctuations and specified
maximum length of product NE are used to con-
trol it.
?Disambiguating Candidates: In this mod-
ule, boundary and classification ambiguities be-
tween candidates are resolved simultaneously.
And Viterbi algorithm is applied for most-likely
state sequences based on the HHMM topology.
4.2 Integration with Heuristic Information
To get more efficient control in triggering process
above, we try to integrate some heuristic informa-
tion. The heuristic rules we used are as domain-
independent as possible in order that they can
be integrated with statistical model systematically
rather than just some tricks on it.
(1) Stop Word List:
Common English words, English brand word,
and some punctuations are extracted automati-
cally from training set to make up of stop word
list for TYP; by co-occurrence statistics between
ORG and its contexts, some words are extracted
from the contexts to make up of stop word list
for PRO in order to overcome the case that brand
word is prone to bind its surroundings to be a
PRO.
(2) Constrain Rules:
Rule 1: For the highly frequent pattern ??
?+?????(number + English quantifier
ES PS5IS2IS1
IS0
ES PS1 PS2 PS4PS3ES
0.2 0.5
0.3
0.7  0.3 0.5 0.30.7
0.2
0.3
Figure 2 Structure of Hierarchical Hidden
Markov Model (HHMM)
word), all the corresponding TYP candidates trig-
gered by categorized word features(CWF) should
be removed.
Rule 2: Product NE candidates in which some
binate symbols don?t match each other should be
removed.
Rule 3: Unreasonable symbols such as ?-? or
?:? should not occur in the beginning or end of
product NE candidates.
4.3 HHMM for product NER application
By HHMM [S. Fine et al 1998] the product
NER can be formulated as a tagging problem us-
ing Viterbi algorithm. Unlike traditional HMM
in POS tagging, here the topology of HHMM is
not fixed and internal states can be also a similar
stochastic model on themselves, called internal
states compared to production states which will
emit only observations.
Our HHMM structure actually consists of three
level approximately illustrated as figure 2 in
which IS denotes internal state, PS denotes pro-
duction state and ES denote end state at ev-
ery level. For our application, an input se-
quence from our SegNer2.0 toolkit can be formal-
ized as w1/t1w2/t2 . . . wi/ti . . . wn/tn, among
which wi and ti is the ith word and its part-of-
speech, n is the number of words. The POS
tag set here is the combination of tag set from
Peking University(PKU-POS) and our general
NE categories(GNEC) including PER(person),
LOC(location), ORG(organization), TIM(time ex-
pression), NUM(numeric expression). Therefore
we can construct our HHMM model by the state
set {S} consisting of {GNEC}, {BRA, PRO,
TYP}, and {V} as well as the observation set {O}
consisting of {V} which is the word set from
training data. That is to say, the word forms
43
in {V} which are not included in NEs are also
viewed as production states.
In our model, only PRO are internal state which
may activate other production states such as BRA
and TYP resulting in recursive HMM. In consis-
tence with S. Fine?s work, qdi (1? d ? D) is used
to indicate the ith state in the dth level of hierar-
chy. So, the product NER problem is to find the
most-likely state activation sequence Q*, a multi-
scale list of states, based on the dynamic topol-
ogy of HHMM given a observation sequence W
= w1w2 . . . wi . . . wn, formulated as follows based
on Bayes rule (P (W )=1).
Q?= argmax
Q
P (Q|W )= argmax
Q
P (Q)P (W |Q)
(1)
From the root node of HHMM, activity flows
to all other nodes at different levels according to
their transition probability. For description conve-
nience, we take the kth level as example(activated
by the mth state at the k-1th level).
P (Q) ?= p(qk1 |qk?1m )
? ?? ?
vertical transition
horizontal transition
? ?? ?
p(qk2 |qk1 )
|qk|
?
j=3
p(qkj |qkj?1, qkj?2)
(2)
P (W |Q)=
?
???????
???????
?=
|qkPS |?
j=1
p([wqkj ?begin...wqkj ?end]|q
k
j )
if qkj /? {IS}
activate other states recursively
if qkj ? {IS}
(3)
Where |qk| is the number of all states and |qkPS |
is the number of production states in the kth level;
wqkj ?begin...wqkj ?end indicates the word sequence
corresponding to the state qkj .
(1) In equation (3), if qkj ? {{GNEC},{V}},
p([wqkj ?begin...wqkj ?end]|q
k
j )=1, because we as-
sume that the general NER results from the pre-
ceding toolkit are correct;
(2) If qkj = PRO, production states in the
(k+1)th level will be activated by this internal
state through equation (2),(3) and go back when
arriving at an end state, thus hierarchical compu-
tation is implemented;
(3) If qkj =BRA, we assign equation (3) a con-
stant value in that BRA candidates consist of only
a single brand word in our method. In addition
brand word can also generate ORG candidates,
thus we can assign equation (3) as follows.
p([wqkj ?begin...wqkj ?end]|q
k
j = BRA) = 0.5 (4)
(4) If qkj = TY P , categorized word fea-
tures(CWFs) defined in section 4.1 are applied,
i.e. the words associated with the current state are
replaced with their CWFs (WC) acting as obser-
vations. Then we can compute the emission prob-
ability of this TYP production state as the follow-
ing equation, among which |qkj | is the length of
observation sequence associated with the current
state.
p([wqkj ?begin...wqkj ?end]|q
k
j = TY P )
?=p(wc1|begin)p(end|wc|qkj |)
|qkj |?
m=2
p(wcm|wcm?1)
All the parameters in every level of HHMM can
be acquired using maximum likelihood method
with smoothing from training data.
4.4 Mixture of Two Hierarchical Hidden
Markov Models
Now we have implemented a simple HHMM
for product NER. Note that in the above
model(HHMM-1), we exploit both internal and
external features of product NEs only at lev-
els of simply semantic classification and just
word form. To achieve our motivation in sec-
tion 3.3, we construct another HHMM(HHMM-
2) for exploiting multi-level contexts by mixing
with HHMM-1.
In HHMM-2, the difference from HHMM-1
lies in the state set SII and observation set OII .
Because the input text will be processed by seg-
ment, POS tagging and general NER, as a alterna-
tive, we can also take T=t1t2 . . . ti . . . tn as obser-
vation sequence, i.e. OII={PKU-POS}. Accord-
ingly, SII= {{PKU-POS}, {GNEC}, BRA, TYP,
44
Data Sets PRO BRA TYP PER LOC ORG
DataSetPRO1.2 12,432 5,047 10,606 424 1,733 4,798
OpenTestSet 1800 803 1364 39 207 614
CloseTestSet 1553 513 1296 55 248 619
Table 2: Overview of Data Sets
PRO}, among which PRO is internal state. Sim-
ilarly, the problem is formulated as follows with
HHMM-2.
Q?II = argmax
QII
P (QII |T )
= argmax
QII
P (QII)P (T |QII) (5)
The description and computation of HHMM-2
is similar to HHMM-1 and is omitted here.
We can see that besides making use of semantic
classification of NEs in common, HHMM-1 and
HHMM-2 exploit word form and part-of-speech
(POS) features respectively. Word form features
make the model more discriminative, while POS
features result in robustness. Intuitively, the mix-
ture of these two models is desirable for higher
performance in product NER by balancing the ro-
bustness and discrimination which can be formu-
lated in logarithmic form as follows.
(Q?, Q?II)
= argmax
Q,QII
{log(P (Q)) + log(P (W |Q))
+ ?[log(P (QII)) + log(P (T |QII))]} (6)
Where ? is a tuning parameter for adjusting the
weight of two models.
5 Experiments and analysis
5.1 Data Set Preparation
A large number of web pages in mobile phone
and digital domain are compiled into text collec-
tions, DataSetPRO, on which multi-level process-
ing were performed. Our final version, DataSet-
PRO1.2, consists of 1500 web pages, roughly
1,000,000 Chinese characters. Randomly se-
lected 140 texts (digital 70, mobile phone 70) are
separated from DataSetPRO1.2 as our OpenTest-
Set, the rest as TrainingSet, from which 160 texts
are extracted as CloseTestSet. Table 2 illustrates
the overview of them.
5.2 Experiments
Due to various and flexible forms of product NEs,
though some boundaries of recognized NEs are
inconsistent with manual annotation, they are also
reasonable. So soft evaluation is also applied
in our experiments to make the evaluation more
reasonable. The main idea is that a discount
score will be given to recognized NEs with wrong
boundary but correct detection and classification.
However, strict evaluation only score completely
correct ones.
All the results is conducted on OpenTestSet un-
less it is particularly specified. Also, the evalu-
ation scores used below are obtained mainly by
45
Digital Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.864 0.799 0.830 0.762 0.744 0.753
TYP 0.903 0.906 0.905 0.828 0.944 0.882
BRA 0.824 0.702 0.758 0.723 0.705 0.714
Mobile Phone Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.917 0.935 0.926 0.799 0.856 0.827
TYP 0.959 0.976 0.967 0.842 0.886 0.864
BRA 0.911 0.741 0.818 0.893 0.701 0.785
Table 3: Experimental Results in Digital and Mobile Phone Domain
soft metrics, and strict scores are also given for
comparison in experiment 3.
1. Evaluation on the Influence of ? in the Mix-
ture Model.
In the mixture model denoted as equation (6),
the ? value reflects the different contribution of
two individual models to the overall system per-
formance. The larger ?, the more contribution
made by HHMM-2. Figure 3, 4, 5 illustrate the
varying curves of recognition performance with
the ? value on PRO, TYP, BRA respectively.
Note that, if ? equal to 1 then two models
are mixed with equivalent weight. We can see
that, as ? goes up, the F-measures of PRO and
TYP increase obviously firstly, and begin to go
down slightly after a period of growing flat. It
can be explained that HHMM-2 mainly exploits
part-of-speech and general NER features which
can relieve the sparseness problem to some ex-
tent, which is more serious in HHMM-1 due to
using lower level of contextual information such
as word form. However, as ? becomes larger,
the problem of imprecise modeling in HHMM-
2 will be more salient and begin to illustrate a
side-effect in the mixture model. Whereas, the
influence of ? on BRA is negligible because its
candidates are triggered by the relatively reliable
knowledge base and its sub-model in HHMM is
assigned a constant as shown in equation(4).
Summings-up:
(1) Mixture with HHMM-2 can make up the
weakness of HHMM-1.
(2) HHMM-2 can make more contributions
to the mixture model under the conditions that
limited annotated data is available at present. In
our system, ? is assigned to 8 based on above ex-
perimental results.
2. Evaluation on the portability of ProNER in
two domains.
First, we can see from Table 3 that ProNER
have achieved fairly high performance in both
digital and mobile phone domain. This can val-
idate to some extent the portability of our sys-
tem?which is consistent with our initial motiva-
tion.
Second, the results also show that our system
performs slightly better in mobile phone domain
for both close test and open test. This can be ex-
plained that there are more challenging ambigui-
ties in digital domain due to more complex prod-
uct taxonomy and more flexible variants of prod-
uct NEs.
Summings-up: The results provide promising
evidence on the portability of our system to dif-
ferent domains though there are some differences
between them.
3. Evaluation on the efficiency of the mixture
model and the improvement of the triggering
control with heuristics.
In table 4, ?1? denotes HHMM-1; ?2? denotes
HHMM-2; ?+? means the mixture model; ?*?
means integrating with heuristics mentioned in
section 4.2.
The results reveal that the mixture model out-
performs each individual model with both soft
and strict metrics. Also, the results show that
heuristic information can increase the F-measure
of PRO and TYP by 10 points or so for both indi-
46
HHMM
BRA TYP PRO
strict
score
soft
score
strict
score
soft
score
strict
score
soft
score
1 0.68 0.72 0.57 0.66 0.52 0.61
1* 0.70 0.74 0.70 0.80 0.63 0.72
2 0.67 0.73 0.66 0.74 0.61 0.68
2* 0.70 0.74 0.76 0.85 0.70 0.76
1+2 0.70 0.75 0.67 0.77 0.67 0.72
1+2* 0.72 0.76 0.76 0.87 0.75 0.80
Table 4: Improvement results (F-measure) with
heuristics and model mixture
vidual model and the mixture model. Addition-
ally we can see that HHMM-2 performs better
on the whole than HHMM-1, which is consistent
with experiment 1 that heavier weights should be
assigned to HHMM-2 in the mixture model.
Summings-up:
(1) Either HHMM-1 or HHMM-2 can not
perform quite well independently, but systemat-
ical integration of them can achieve obvious per-
formance improvement due to the leverage of di-
verse levels of linguistic features by their efficient
interaction.
(2) Heuristic information can highly enhance
the performance for both individual model and the
mixture model.
6 Conclusions and Future Work
This paper presented a hierarchical HMM (hidden
Markov model) based approach of product named
entity recognition from Chinese free text. By uni-
fying some heuristic rules into a statistical frame-
work based on a mixture model of HHMM, the
approach we proposed can leverage diverse range
of linguistic features and knowledge sources to
make probabilistically reasonable decisions for a
global optimization. The prototype system we
built achieved the overall F-measure of 79.7%,
86.9%, 75.8% corresponding to PRO, TYP, BRA
respectively, which also provide experimental ev-
idence to some extent on its portability to differ-
ent domains.
Our future work will focus on the following:
(1) Using long dependency information;
(2) Integrating segment, POS tagging, general
NER and product NER to avoid error spread.
References
John M. Pierre. (2002) Mining Knowledge from Text
Collections Using Automatically Generated Meta-
data. In: Procs of Fourth International Conference
on Practical Aspects of Knowledge Management.
Michael Collins and Yoram Singer. (1999) Unsuper-
vised Models for Named Entity Classification. In:
Proc. of EMNLP/VLC-99.
Eunji Yi, Gary Geunbae Lee, and Soo-Jun Park.
(2004) SVM-based Biological Named Entity
Recognition using Minimum Edit-Distance Feature
Boosted by Virtual Examples. In: Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP-04).
Bick, Eckhard (2004) A Named Entity Recognizer for
Danish. In: Proc. of 4th International Conf. on Lan-
guage Resources and Evaluation,pp:305-308.
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou,
Changning Huang. (2002) Chinese Named Entity
Identification Using Class-based Language Model.
In: COLING 2002. Taipei, Taiwan.
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,
Shuo Bai. Chinese Named Entity Recognition Us-
ing Role Model. Special Iissue ?Word Formation
and Chinese Language processing? of the Inter-
national Journal of Computational Linguistics and
Chinese Language Processing, 8(2),2003, pp:29-60
Aberdeen, John et al (1995)MITRE: Description of
the ALEMBIC System Used for MUC-6. Proc. of
MUC-6, pp. 141-155
D.M. Bikel, S. Miller, R. Schwartz, R. Weischedel.
(1997) Nymble: a High-Performance Learning
Name-finder. In: Fifth Conference on Applied Nat-
ural Language Processing, pp 194-201.
Borthwick. A. (1999) A Maximum Entropy Approach
to Named Entity Recognition. PhD Dissertation.
Tzong-Han Tsai, S.H. Wu, C.W. Lee, Cheng-Wei
Shih, and Wen-Lian Hsu. (2004) Mencius: A Chi-
nese Named Entity Recognizer Using the Maxi-
mum Entropy-based Hybrid Model. International
Journal of Computational Linguistics and Chinese
Language Processing, Vol. 9, No 1.
Cheng Niu, W. Li, J.h. Ding and R.K. Srihari. (2003) A
Bootstrapping Approach to Named Entity Classifi-
cation Using Successive Learners. In: Proceedings
of the 41st ACL, Sapporo, Japan, pp:335-342.
S. Fine, Y. Singer, N. Tishby. (1998) The Hierarchical
Hidden Markov Model: Analysis and Applications.
Machine Learning. 32(1), pp:41-62
47
Proceedings of ACL-08: HLT, pages 541?549,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Chinese-English Backward Transliteration Assisted with Mining Mono-lingual Web Pages    Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu  National Laboratory of Pattern Recognition   Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China {fyang,jzhao,bzou,kliu,ffliu}@nlpr.ia.ac.cn     
Abstract 
In this paper, we present a novel backward transliteration approach which can further as-sist the existing statistical model by mining monolingual web resources. Firstly, we em-ploy the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to re-rank the revised candidates based on the in-formation extracted from monolingual web pages. To get a better precision during the re-ranking process, a variety of web-based in-formation is exploited to adjust the ranking score, so that some candidates which are less possible to be transliteration names will be as-signed with lower ranks. The experimental re-sults show that the proposed framework can significantly outperform the baseline translit-eration system in both precision and recall. 1 Introduction* The task of Name Entity (NE) translation is to translate a name entity from source language to target language, which plays an important role in machine translation and cross-language informa-tion retrieval (CLIR). Transliteration is a subtask in NE translation, which translates NEs based on the phonetic similarity. In NE translation, most person names are transliterated, and some parts of location names or organization names also need to be trans-literated. Transliteration has two directions: for-ward transliteration which transforms an original name into target language, and backward translit-eration which recovers a name back to its original expression. For instance, the original English per-                                                           *Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn.  
son name ?Clinton? can be forward transliterated to its Chinese expression ??/ke ?/lin?/dun? and the backward transliteration is the inverse process-ing. In this paper, we focus on backward translit-eration from Chinese to English. Many previous researches have tried to build a transliteration model using statistical approach [Knight and Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, 2003; Gao, 2004]. There are two main challenges in statistical backward trans-literation: First, statistical transliteration approach selects the most probable translations based on the knowledge learned from the training data. This approach, however, does not work well when there are multiple standards [Gao, 2004]. Second, back-ward transliteration is more challenging than for-ward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most important causes in introducing noises is that: some silent syllables in original names have been missing when they are transliterated to target lan-guage. For example, when ?Campbell? is translit-erated into ??/kan?/bei?/er?, the ?p? is missing.  In order to make up the disadvantages of statisti-cal approach, some researchers have been seeking for the assistance of web resource. [Wang et al, 2004; Cheng et al, 2004; Nagata et al, 2001; Zhang et al 2005] used bilingual web pages to ex-tract translation pairs. Other efforts have been made to combine a statistical transliteration model with web mining [Al-Onaizan and Knight, 2002; Long Jiang et al 2007]. Most of these methods need bilingual resources. However, those kinds of resources are not readily available in many cases. Moreover, to search for bilingual pages, we have to depend on the performance of search engines. We can?t get Chinese-English bilingual pages when the input is a Chinese query. Therefore, the existing 
541
 
assistance approaches using web-mining to assist transliteration are not suitable for Chinese to Eng-lish backward transliteration. Thus in this paper, we mainly focus on the fol-lowing two problems to be solved in transliteration. Problem I: Some silent syllables are missing in English-Chinese forward transliteration. How to recover them effectively and efficiently in back-ward transliteration is still an open problem. Problem II: Statistical transliteration always chooses the translations based on probabilities. However, in some cases, the correct translation may have lower probability. Therefore, more stud-ies are needed on combination with other tech-niques as supplements. Aiming at these two problems, we propose a method which mines monolingual web resources to assist backward transliteration. The main ideas are as follows. We assume that for every Chinese en-tity name which needs to be backward transliter-ated to an English original name, the correct transliteration exists somewhere in the web. What we need to do is to find out the answers based on the clues given by statistical transliteration results. Different from the traditional methods which ex-tract transliteration pairs from bilingual pages, we only use monolingual web resources. Our method has two advantages. Firstly, there are much more monolingual web resources available to be used. Secondly, our method can revise the transliteration candidates to the existing words before the subse-quent re-ranking process, so that we can better mine the correct transliteration from the Web. Concretely, there are two phases involved in our approach. In the first phase, we split the result of transliteration into syllables, and then a syllable-based searching processing can be employed to revise the result in a word list generated from web pages, with an expectation of higher recall of trans-literation. In the second phase, we use a revised word as a search query to get its contexts and hit information, which are integrated into the AdaBoost classifier to determine whether the word is a transliteration name or not with a confidence score. This phase can readjust the candidate?s score to a more reasonable point so that precision of transliteration can be improved. Table 1 illustrates how to transliterate the Chinese name ??/a?/jia?/xi? back to ?Agassi?.  Chinese name Transliteration results Revised Candidate Re-rank Results 
??? a  jia xi Agassi 
aggasi agahi agacy agasie ? 
agasi agathi agathe agassi ? 
agassi agasi agache agga ? Table 1. An example of transliteration flow The experimental results show that our approach improves the recall from 41.73% to 59.28% in open test when returning the top-100 results, and the top-5 precision is improved from 19.69% to 52.19%. The remainder of the paper is structured as fol-lows. Section 2 presents the framework of our sys-tem. We discuss the details of our statistical transliteration model in Section 3. In Section 4, we introduce the approach of revising and re-ranking the results of transliteration. The experiments are reported in Section 5. The last section gives the conclusion and the prediction of future work. 2 System Framework  Our system has three main modules. 
  Figure 1. System framework 1) Statistical transliteration: This module re-ceives a Chinese Pinyin sequence as its input, and output the N-best results as the transliteration can-didates.  2) Candidate transliteration revision through syllable-based searching: In the module, a transliteration candidate is transformed into a syllable query. We use a syllable-based searching strategy to select the revised candidate from a huge word list. Each word in the list is indexed by sylla-bles, and the similarity between the word and the query is calculated. The most similar words are returned as the revision results. This module guar-
Monolingual web pages 
Words list 
Chinese name 
Statistical model 
Transliteration candidates Syllable-based search 
Revised candidates Re-ranking phase 
Final results Search engine 
542
 
antees the transliteration candidates are all existing words. 3) Revised candidate re-ranking in web pages: In the module, we search the revised candi-dates to get their contexts and hit information which we can use to score the probability of being a transliteration name. This phase doesn?t generate new candidates, but re-rank the revised candidate set to improve the performance in top-5. Under this framework, we can solve the two problems of statistical model mentioned above.  (1) The silent syllables will be given lower weights in syllable-based search, so the missing syllables will be recovered through selecting the most similar existing words which can contain some silent syllables.  (2) The query expansion technology can recall more potential transliteration candidates by ex-panding syllables to their ?synonymies?. So the mistakes introduced when selecting syllables in statistical transliteration will be corrected through giving suitable weights to synonymies.  Through the revision phase, the results of statis-tical model which may have illegal spelling will be mapped to its most similar existing words. That can improve the recall. In re-ranking phase, the revised candidate set will be re-ranked to put the right answer on the top using hybrid information got from web resources. So the precision of trans-literation will be improved. 3 Statistical Transliteration Model We use syllables as translation units to build a sta-tistical Chinese-English backward transliteration model in our system. 3.1 Traditional Statistical Translation Model [P. Brown et al, 1993] proposed an IBM source-channel model for statistical machine translation (SMT). When the channel output f= f1,f2 ?. fn ob-served, we use formula (1) to seek for the original sentence e=e1,e2 ?. en with the most likely poste-riori. 
' argmax ( | ) argmax ( | ) ( )
e e
e P e f P f e P e= =
      (1) The translation model ( | )P f e  is estimated from a paired corpus of foreign-language sentences and their English translations. The language model ( )P e  is trained from English texts. 
3.2 Our Transliteration Model The alignment method is the base of statistical transliteration model. There are mainly two kinds of alignment methods: phoneme-based alignment [Knight and Graehl, 1998; Virga and Khudanpur, 2003] and grapheme-based alignment [Long Jiang, 2007]. In our system, we adopt the syllable-based alignment from Chinese pinyin to English syllables, where the syllabication rules mentioned in [Long Jiang et al, 2007] are used. For example, Chinese name ??/xi ?/er ?/dun? and its backward transliteration ?Hilton? can be aligned as follows. ?Hilton? is split into syllable sequence as ?hi/l/ton?, and the alignment pairs are ?xi-hi?, ?er-l?, ?dun-ton?.  Based on the above alignment method, we can get our statistical Chinese-English backward trans-literation model as, 
argmax ( | ) ( )
E
E p PY ES p ES=
            (2) Where, PY is a Chinese Pinyin sequence, ES is a English syllables sequence, ( | )p PY ES  is the probability of translating ES into PY, ( )p E S  is the generative probability of a English syllable lan-guage model. 3.3 The Difference between Backward Trans-literation and Traditional Translation Chinese-English backward transliteration has some differences from traditional translation. 1) We don?t need to adjust the order of sylla-bles when transliteration.  2) The language model in backward translitera-tion describes the relationship of syllables in words. It can?t work as well as the language model de-scribing the word relationship in sentences. We think that the crucial problem in backward transliteration is selecting the right syllables at every step. It?s very hard to obtain the exact an-swer only based on the statistical transliteration model. We will try to improve the statistical model performance with the assistance of mining web resources. 4 Mining Monolingual Web Pages to As-sist Backward Transliteration  In order to get assistance from monolingual Web resource to improve statistical transliteration, our 
543
 
method contains two main phases: ?revision? and ?re-ranking?. In the revision phase, transliteration candidates are revised using syllable-based search in the word list, which are generated by collecting the existing words in web pages. Because the proc-ess of named entity recognition may lose some NEs, we will reserve all the words in web corpus without any filtering. The revision process can im-prove the recall through correcting some mistakes in the transliteration results of statistical model. In the re-ranking phase, we search every revised candidate on English pages, score them according to their contexts and hit information so that the right answer will be given a higher rank.  4.1 Using Syllable-based Retrieval to Revise Transliteration Candidates In this section, we will propose two methods re-spectively for the two problems of statistical model mentioned in section 1.  4.1.1  Syllable-based retrieval model When we search a transliteration candidate tci in the word list, we firstly split it into syllables {es1,es2,?..esn}. Then this syllable sequence is used as a query for syllable-based searching.  We define some notions here. ? Term set T={t1,t2?.tk} is an orderly set of all syllables which can be viewed as terms.  ? Pinyin set P={py1,py2?.pyk} is an orderly set of all Pinyin.  ? An input word can be represented by a vec-tor of syllables {es1,es2,?..esn}.  We calculate the similarity between a translitera-tion result and each word in the list to select the most similar words as the revised candidates. The {es1,es2,?..,esn} will be transformed into a vector Vquery={t1,t2?.tk} where ti represents the ith term in T. The value of ti is equal to 0 if the ith term doesn?t appear in query. In the same way, the word in list can also be transformed into vector represen-tation. So the similarity can be calculated as the inner product between these two vectors.  We don?t use tf and idf conceptions as traditional information retrieval (IR) to calculate the terms? weight. We use the weight of ti to express the ex-pectation probability of ith term having pronuncia-tion. If the term has a lower probability of having pronunciation, its weight is low. So when we searching, the missing silent syllables in the results 
of statistical transliteration model can be recovered because such syllables have little impact on simi-larity measurement. The formula we used is as fol-lows. 
( , )
/
query word
word py
V V
Sim query word
L L
?
=
            (3) 
The numerator is the inner product of two vec-tors. The denominator is the length of word Lword divided by the length of Chinese pinyin sequence Lpy. In this formula, the more syllables in one word, the higher score of inner production it may get, but the word will get a loss for its longer length. The word which has the shortest length and the highest syllable hitting ratio will be the best. Another difference from traditional IR is how to deal with the order of the words in a query. Ac-cording to transliteration, the similarity must be calculated under the limitation of keeping order, which can?t be satisfied by current methods. We use the algorithm like calculating the edit distance between two words. The syllables are viewed as the units which construct a word. The edit distance calculation finds the best matching with the least operation cost to change one word to another word by using deletion/addition/insertion operations on syllables. But the complexity will be too high to afford if we calculate the edit distance between a query and each word in the list. So, we just calcu-late the edit distance for the words which get high score without the order limitation. This trade off method can save much time but still keep perform-ance. 4.1.2  Mining the Equivalent through Syllable Expansion In most collections, the same concept may be re-ferred to using different words. This issue, known as synonymy, has an impact on the recall of most information retrieval systems. In this section, we try to use the expansion technology to solve prob-lem II. There are three kinds of expansions to be explained below.  Syllable expansion based on phonetic similar-ity: The syllables which correspond to the same Chinese pinyin can be viewed as synonymies. For example, the English syllables ?din? and ?tin? can be aligned to the same Chinese pinyin ?ding?. Given a Chinese pinyin sequence {py1,py2,?..pyn} as the input of transliteration model, for every pyi, there are a set of syllables 
544
 
{es1, es2 ?.. esk} which can be selected as its translation. The statistical model will select the most probable one, while others containing the right answer are discarded. To solve this problem, we expand the query to take the synonymies of terms into consideration. We create an expansion set for each Chinese pinyin. A syllable esi will be selected into the expansion set of pyj based on the alignment probability P(esi|pyj) which can be ex-tracted from the training corpus. The phonetic similarity expansion is based on the input Chinese Pinyin sequence, so it?s same for all candidates. Syllable expansion based on syllable similar-ity: If two syllables have similar alignment prob-ability with every pinyin, we can view these two syllables as synonymy. Therefore, if a syllable is in the query, its synonymies should be contained too. For example, ?fea? and ?fe? can replace each other. To calculate the similarity, we first obtain the alignment probability P(pyj|esk) of every syllable. Then the distance between any two syllables will be calculated using formula (4). 
1
1
( , ) ( | ) ( | )
N
j k i j i k
i
Sim es es P py es P py es
N
=
=
?
(4) This formula is used to evaluate the similarity of two syllables in alignment. The expansion set of the ith syllable can be generated by selecting the most similar N syllables. This kind of expansion is conducted upon the output of statistical translitera-tion model. Syllable expansion based on syllable edit dis-tance: The disadvantage of last two expansions is that they are entirely dependent on the training set. In other word, if some syllables haven?t appeared in the training corpus, they will not be expanded. To solve the problem, we use the method of expan-sion based on edit distance. We use edit distance to measure the similarity between two syllables, one is in training set and the other is absent. Because the edit distance expansion is not very relevant to pronunciation, we will give this expansion method a low weight in combination. It works when new syllables arise.  Combine the above three strategies: We will combine the three kinds of expansion method to-gether. We use the linear interpolation to integrate them. The formulas are follows.   
(1 )
pre sy ed
S S S S? ? ?= ? + +                (5) 
(1 )
pre py ed
S S S S? ? ?= ? + +                (6) 
where Spre is the score of exact matching, Ssy is the score of expansion based on syllables similarity and Spy based on phonetic similarity. We will ad-just these parameters to get the best performance. The experimental results and analysis will be re-ported in section 5.3. 4.2 Re-Ranking the Revised Candidates Set using the Monolingual Web Resource In the first phase, we have generated the revised candidate set {rc1,rc2,?,rcn} from the word list us-ing the transliteration results as clues. The objec-tive is to improve the overall recall. In the second phase, we try to improve the precision, i.e. we wish to re-rank the candidate set so that the correct an-swer will be put in a higher rank. [Al-Onaizan et al, 2002] has proposed some methods to re-score the transliteration candidates. The limitation of their approach is that some can-didates are propbale not existing words, with which we will not get any information from web. So it can only re-rank the transliteration results to improve the precision of top-5. In our work, we can improve the recall of transliteration through the revising process before re-ranking. In this section, we employ the AdaBoost frame-work which integrates several kinds of features to re-rank the revised candidate set. The function of the AdaBoost classifier is to calculate the probabil-ity of the candidate being a NE. Then we can re-rank the revised candidate set based on the score. The features used in our system are as follows. NE or not: Using rci as query to search for monolingual English Web Pages, we can get the context set {Ti1, Ti2??Tin} of rci. Then for every Tik, we use the named entity recognition (NER) software to determine whether rci is a NE or not. If rci is recognized as a NE in some Tik, rci will get a score. If rci can?t be recognized as NE in any con-texts, it will be pruned. The hit of the revised candidate: We can get the hit information of rci from search engine. It is used to evaluate the importance of rci. Unlike [Al-Onaizan et al, 2002], in which the hit can be used to eliminate the translation results which contain illegal spelling, we just use hit number as a feature. The limitation of compound NEs: When trans-literating a compound NE, we always split them into several parts, and then combine their translit-eration results together. But in this circumstance, 
545
 
every part can add a limitation in the selection of the whole NE. For example: ??/xi?/la?/li ?  ?/ke?/lin?/dun? is a compound name. ??/xi?/la?/li? can be transliterate to ?Hilary? or ?Hilaly? and ??/ke?/lin?/dun? can be transliterate to ?Clinton? or ?Klinton?. But the combination of ?Hilary?Clinton? will be selected for it is the most common combination. So the hit of combination query will be extracted as a feature in classifier. Hint words around the NE: We can take some hint words around the NE into the query, in order to add some limitations to filter out noisy words. For example: ??? (president)? can be used as hint word for ???? (Clinton)?. To find the hint words, we first search the Chinese name in Chi-nese web pages. The frequent words can be ex-tracted as hint words and they will be translated to English using a bilingual dictionary. These hint words are combined with the revised candidates to search English web pages. So, the hit of the query will be extracted as feature. The formula of AdaBoost is as follow. 
1
( ) ( ( ))
T
t t
t
H x sign h x?
=
=
?
                 (7) 
Where 
t
?  is the weight for the ith weak classifier 
( )
t
h x . 
t
?  can be calculated based on the precision of its corresponding classifier. 5 Experiments We carry out experiments to investigate how much the revision process and the re-ranking process can improve the performance compared with the base-line of statistical transliteration model. We will also evaluate to which extents we can solve the two problems mentioned in section 1 with the as-sistance of Web resources. 5.1 Experimental data The training corpus for statistical transliteration model comes from the corpus of Chinese <-> Eng-lish Name Entity Lists v 1.0 (LDC2005T34). It contains 565,935 transliteration pairs. Ruling out those pairs which are not suitable for the research on Chinese-English backward transliteration, such as Chinese-Japanese, we select a training set which contains 14,443 pairs of Chinese-European & American person names. In the training set, 1,344 
pairs are selected randomly as the close test data. 1,294 pairs out of training set are selected as the open test data. To set up the word list, a 2GB-sized collection of web pages is used. Since 7.42% of the names in the test data don?t appear in the list, we use Google to get the web page containing the ab-sent names and add these pages into the collection. The word list contains 672,533 words. 5.2 Revision phase vs. statistical approach Using the results generated from statistical model as baseline, we evaluate the revision module in recall first. The statistical transliteration model works in the following 4 steps: 1) Chinese name are transformed into pinyin representation and the English names are split into syllables. 2) The GIZA++1 tool is invoked to align pinyin to sylla-bles, and the alignment probabilities ( | )P py es are obtained. 3) Those frequent sequences of syllables are combined as phrases. For example, ?be/r/g???berg?, ?s/ky???sky?. 4) Camel 2  de-coder is executed to generate 100-best candidates for every name. We compare the statistical transliteration results with the revised results in Table 2. From Table 2 we can find that the recall of top-100 after revision is improved by 13.26% in close test set and 17.55% in open test set. It proves that the revision module is effective for correcting the mistakes made in statistical transliteration model. Transliteration results Revised results  close open close open Top1 33.64% 9.41% 27.15% 11.04% Top5 40.37% 13.38% 42.83% 19.69% Top10 47.79% 17.56% 56.98% 26.52% Top20 61.88% 25.44% 71.05% 37.81% Top50 66.49% 36.19% 82.16% 46.22% Top100 72.52% 41.73% 85.78% 59.28% Table 2. Statistical model vs. Revision module To show the effects of the revision on the two above-mentioned problems in which the statistical model does not solve well: the losing of silent syl-lables and the selection bias problem, we make a statistics of the improvements with a measurement of ?correction time?. For a Chinese word whose correct transliteration appears in top-100 candidates only if it has been 
                                                           1 http://www.fjoch.com/GIZA++.html 2 http://www.nlp.org.cn 
546
 
revised, we count the ?correction time?. For exam-ple, when ?Argahi? is revised to ?Agassi? the cor-rection time is ?1? for Problem II and ?1? for Problem I, because in ?hi?? ?si? the syllable is expanded, and in ?si? ??ssi? an ?s? is added.   Close test Open test Problem I 0.6931 0.7853 Problem II 0.9264 1.1672 Table 3. Average time of correction This measurement reflects the efficiency of the revision of search strategy, in contrast to those spelling correction techniques in which several operations of ?add? and ?expand? are inevitable. It has proved that the more an average correction time is, the more efficient our strategy is.  
?
???
???
???
???
???
???
???
???
???
?
? ? ? ? ? ? ? ? ?
??????? ?????????  Figure 2. Length influence in recall comparison The recall of the statistical model relies on the length of English name in some degree. It is more difficult to obtain an absolutely correct answer for longer names, because they may contain more si-lent and confused syllables. However, through the revision phase, this tendency can be effectively alleviated. In Figure 2, we make a comparison be-tween the results of the statistical model and the revision module with the changing of syllable?s length in open test. The curves demonstrate that the revision indeed prevents the decrease of recall for longer names. 5.3 Parameter setting in the revision phase We will show the experimental results when set-ting different parameters for query expansion. In the expansion based on phonetic similarity, for every Chinese pinyin, we select at most 20 sylla-bles to create an expansion set. We set 0.1? =  in formula (5). The results are shown in the columns labeled ?exp1? in Table 4. From the results we can conclude that, we get the best performance when 
0.4? = . That means the performance is best when the weight of exact 
matching is a little larger than the weight of fuzzy matching. We can also see that, higher weight of exact matching will lead to low recall, while higher weight of fuzzy matching will bring noise in. The expansion method based on syllable similar-ity is also evaluated. For every syllable, we select at most 15 syllables to create the expansion set. We set 0.1? = . The results are shown in the columns labeled ?exp2? in Table 4. From the results we can conclude that, we get the best performance when 0.5? = . It means that we can?t put emphasis on any matching methods. Comparison with the expansion based on phonetic similarity, the performance is poorer. It means that the expansion based on phonetic similarity is more suitable for revising transliteration candidates. 5.4 Revision phase vs. re-ranking phase After the phase of revising transliteration candi-dates, we re-rank the revised candidate set with the assistance of monolingual web resources. In this section, we will show the improvement in preci-sion after re-ranking. We have selected four kinds of features to inte-grate in the AdaBoost framework. To determine whether the candidate is NE or not in its context, we use the software tool Lingpipe3. The queries are sent to google, so that we can get the hit of queries and the top-10 snippets will be extracted as context. The comparison of revision results and re-ranking results is shown as follows. Revised results Re-ranked results  close open close open Top1 27.15% 11.04% 58.08% 38.63% Top5 42.83% 19.69% 76.35% 52.19% Top10 56.98% 26.52% 83..92% 54.33% Top20 71.05% 37.81% 83.92% 57.61% Top50 82.16% 46.22% 83.92% 57.61% Top100 85.78% 59.28% 85.78% 59.28% Table 5. Revision results vs. Re-ranking results From these results we can conclude that, after re-ranking phase, the noisy words will get a lower 
                                                           3 http://www.alias-i.com/lingpipe/ 
547
 
0.2? =  0.3? =   0.4? =  0.5? =  0.6? =  0.7? =  0.8? =   exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 Top1 13.46 13.32 13.79 13.61 11.04 12.70 11.65 10.93 10.83 11.25 9.62 10.63 8.73 10.18 Top5 21.58 19.59 23.27 20.17 19.69 18.28 21.07 17.25 22.05 16.84 17.90 16.26 17.38 15.34 Top10 27.39 22.71 28.41 24.73 26.52 22.93 26.83 21.81 27.26 20.39 24.38 21.20 25.42 18.20 Top20 35.23 34.88 35.94 29.49 37.81 31.57 38.59 33.04 36.52 31.72 35.25 29.75 34.65 27.62 Top50 43.91 40.63 43.75 40.85 46.22 41.46 48.72 42.79 45.48 40.49 41.57 39.94 42.81 38.07 Top100 53.76 48.47 54.38 52.04 59.28 53.15 57.36 53.46 55.19 51.83 55.63 49.52 53.41 47.15 Table 4.  Parameters Experiment rank. Through the revision module, we get both higher recall and higher precision than statistical transliteration model when at most 5 results are returned. We also use the average rank and average recip-rocal rank (ARR) [Voorhees and Tice, 2000] to evaluate the improvement. ARR is calculated as       
1
1 1
( )
M
i
ARR
M R i
=
=
?
                             (8) where ( )R i  is the rank of the answer of ith test word. M is the size of test set. The higher of ARR, the better the performance is. The results are shown as Table 6. Statistical  model Revision  module Re-rank  Module  close open close open close open Average rank 37.63 70.94 24.52 58.09 16.71 43.87 ARR 0.3815 0.1206 0.3783 0.1648 0.6519 0.4492 Table 6. ARR and AR evaluation The ARR after revision phase is lower than the statistical model. Because the goal of revision module is to improve the recall as possible as we can, some noisy words will be introduced in. The noisy words will be pruned in re-ranking module. That is why we get the highest ARR value at last. So we can conclude that the revision module im-proves recall and re-ranking module improves pre-cision, which help us get a better performance than pure statistical transliteration model 6 Conclusion In this paper, we present a new approach which can revise the results generated from statistical transliteration model with the assistance of mono-lingual web resource. Through the revision process, the recall of transliteration results has been im-proved from 72.52% to 85.78% in the close test set and from 41.73% to 59.28% in open test set, re-spectively. We improve the precision in re-ranking phase, the top-5 precision can be improved to 76.35% in close test and 52.19% in open test. The 
promising results show that our approach works pretty well in the task of backward transliteration. In the future, we will try to improve the similar-ity measurement in the revision phase. And we also wish to develop a new approach using the transliteration candidates to search for their right answer more directly and effectively. Acknowledgments The work is supported by the National High Tech-nology Development 863 Program of China under Grants no. 2006AA01Z144, the National Natural Science Foundation of China under Grants No. 60673042, the Natural Science Foundation of Bei-jing under Grants no. 4073043. References  Yaser Al-Onaizan and Kevin Knight. 2002. Translating named entities using monolingual and bilingual re-sources. In Proc.of ACL-02.  Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics 24(4). Wei-Hao Lin and Hsin-His Chen. 2002 Backward Ma-chine Transliteration by Learning Phonetic Similarity. In Proc. Of the 6th CoNLL Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A New Approach for English-Chinese Named Entity Alignment. In Proc. of EMNLP-2004. Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng Niu, 2007. Named Entity Translation with Web Min-ing and Transliteration. In Proc. of IJCAI-2007. Wei Gao. 2004. Phoneme-based Statistical Translitera-tion of Foreign Name for OOV Problem. A thesis of Master. The Chinese University of Hong Kong. Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining translations of OOV terms from the web through cross-lingual query expansion. SIGIR 2005. Pu-Jen Cheng, Wen-Hsiang Lu, Jer-Wen Teng, and Lee-Feng Chien. 2004 Creating Multilingual Transla-tion Lexicons with Regional Variations Using Web Corpora. In Proc. of ACL-04 Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the Web as a Bilingual Dictionary. In Proc. of ACL 2001 Workshop on Data-driven Methods in Machine Translation. 
548
 
Paola Virga and Sanjeev Khudanpur. 2003. Translitera-tion of proper names in cross-lingual information re-trieval. In Proc. of the ACL workshop on Multi-lingual Named Entity Recognition. Jenq-Haur Wang, Jei-Wen Teng, Pu-Jen Cheng, Wen-Hsiang Lu, Lee-Feng Chien. 2004. Translating un-known cross-lingual queries in digital libraries using a web-based approach. In Proc. of JCDL 2004. E.M.Voorhees and D.M.Tice. 2000. The trec-8 question answering track report. In Eighth Text Retrieval Con-ference (TREC-8) 
549
Proceedings of NAACL HLT 2007, Companion Volume, pages 101?104,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Look Who is Talking: Soundbite Speaker Name Recognition in  
Broadcast News Speech 
Feifan Liu, Yang Liu 
Department of Computer Science 
The University of Texas at Dallas, Richardson, TX 
{ffliu,yangl}@hlt.utdallas.edu 
 
 
Abstract 
Speaker name recognition plays an important 
role in many spoken language applications, 
such as rich transcription, information extrac-
tion, question answering, and opinion mining. 
In this paper, we developed an SVM-based 
classification framework to determine the 
speaker names for those included speech seg-
ments in broadcast news speech, called sound-
bites. We evaluated a variety of features with 
different feature selection strategies. Experi-
ments on Mandarin broadcast news speech 
show that using our proposed approach, the 
soundbite speaker name recognition (SSNR) 
accuracy is 68.9% on our blind test set, an ab-
solute 10% improvement compared to a base-
line system, which chooses the person name 
closest to the soundbite. 
1 Introduction 
Broadcast news (BN) speech often contains speech or 
interview quotations from specific speakers other than 
reporters and anchors in a show. Identifying speaker 
names for these speech segmentations, called soundbites 
(Maskey and Hirschberg, 2006), is useful for many 
speech processing applications, e.g., question answering, 
opinion mining for a specific person. This has recently 
received increasing attention in programs such as the 
DARPA GALE program, where one query template is 
about a person?s opinion or statement. 
Previous work in this line includes speaker role de-
tection (e.g., Liu, 2006; Maskey and Hirschberg, 2006) 
and speaker diarization (e.g., Canseco et al, 2005). In 
this paper, we formulate the problem of SSNR as a tra-
ditional classification task, and proposed an SVM-based 
identification framework to explore rich linguistic fea-
tures. Experiments on Mandarin BN speech have shown 
that our proposed approach significantly outperforms 
the baseline system, which chooses the closest name as 
the speaker for a soundbite.  
2 Related Work 
To our knowledge, no research has yet been conducted 
on soundbite speaker name identification in Mandarin 
BN domain. However, this work is related to some ex-
tent to speaker role identification, speaker diarization, 
and named entity recognition. 
Speaker role identification attempts to classify speech 
segments based on the speakers? role (anchor, reporter, 
or others). Barzilay et al (2000) used BoosTexter and 
the maximum entropy model for this task in English BN 
corpus, obtaining a classification accuracy of about 80% 
compared to the chance of 35%. Liu (2006) combined a 
generative HMM approach with the conditional maxi-
mum entropy method to detect speaker roles in Manda-
rin BN, reporting a classification accuracy of 81.97% 
against the baseline of around 50%. In Maskey and 
Hirschberg (2006), the task is to recognize soundbites 
(which make up of a large portion of the ?other? role 
category in Liu (2006)). They achieved a recognition 
accuracy of 67.4% in the English BN domain. Different 
from their work, our goal is to identify the person who 
spoke those soundbites, i.e., associate each soundbite 
with a speaker name if any. 
Speaker diarization in BN aims to find speaker 
changes, group the same speakers together, and recog-
nize speaker names. It is an important component for 
rich transcription (e.g., in the DARPA EARS program). 
So far most work in this area has only focused on 
speaker segmentation and clustering, and not included 
name recognition. However, Canseco et al (2005) were 
able to successfully use linguistic information (e.g., 
related to person names) to improve performance of BN 
speaker segmentation and clustering.  
This work is also related to named entity recognition 
(NER), especially person names. There has been a large 
amount of research efforts on NER; however, instead of 
101
recognizing all the names in a document, our task is to 
find the speaker for a particular speech segment.  
3 Framework for Soundbite Speaker 
Name Recognition (SSNR) 
Figure 1 shows our system diagram. SSNR is conducted 
using the speech transcripts, assuming the soundbite 
segments are provided. After running NER in the tran-
scripts, we obtain candidate person names. For a sound-
bite, we use the name hypotheses from the region both 
before and after the soundbite. A ?region? is defined 
based on the turn and topic segmentation information. 
To determine which name among the candidates is the 
corresponding speaker for the soundbite, we recast this 
problem as a binary classification problem for every 
candidate name and the soundbite, which we call an 
instance. A positive tag for an instance means that the 
name is the soundbite speaker. Each instance has an 
associated feature vector, described further in the fol-
lowing section. Note that if a name occurs more than 
once, only one instance is created for it. 
 
Train Set Dev/Test Set
Preprocessing (word 
segmentation, NER)
Instance Creation 
for SSNR
Feature Vector 
Representation
Trained 
Model
Model Training 
and Optimizing
Conflict 
Resolution
Training Testing 
Output
 
Figure 1. System diagram for SSNR. 
 
Any classification approach can be used in this gen-
eral framework for SSNR. We choose to use an SVM 
classifier in our experiments because of its superior per-
formance in many classification tasks. 
3.1 Features  
The features that we have explored can be grouped into 
three categories.  
Positional Features (PF) 
? PF-1: the position of the candidate name relative to 
the soundbite. We hypothesize that names closer to 
a soundbite are more likely to be the soundbite 
speaker. This feature value can be ?last?, ?first?, 
?mid?, or ?unique?. For example, ?last? for a candi-
date before a soundbite means that it is the closest 
name among the hypotheses before the soundbite. 
?Unique? indicates that the candidate is the only 
person name in the region before or after the sound-
bite. Note that if a candidate name occurs more than 
once, the PF-1 feature corresponds to the closest 
name to the soundbite.  
? PF-2: the position of a name in its sentence. Typi-
cally a name appearing earlier in a sentence (e.g., a 
subject) is more likely to be quoted later.  
? PF-3: an indicator feature to show where the name 
has occurred, before, inside, or after the soundbite. 
We added this because it is rare that a name inside a 
soundbite is the speaker of that soundbite.  
? PF-4: an indicator to denote if a candidate is in the 
last sentence just before the soundbite turn, or is in 
the first sentence just after the soundbite turn. 
Frequency Features (Freq) 
We hypothesize that a name with more occurrences 
might be an important subject and thus more likely to be 
the speaker of the soundbite, therefore we include the 
frequency of a candidate name in the feature set.  
Lexical Features (LF) 
In order to capture the cue words around the soundbite 
speaker names in the transcripts, we included unigram 
features. For example, ?pre_word+1=?/said? denotes 
that the candidate name is followed by the word ??
/said?, and that ?pre? means this happens in the region 
before the soundbite.  
3.2 Conflict Resolution 
Another component in the system diagram that is worth 
pointing out is ?conflict resolution?. Since our approach 
treats each candidate name as a separate classification 
task, we need to post-process the cases where there are 
multiple or no positive hypotheses for a soundbite dur-
ing testing. To resolve this situation, we choose the in-
stance with the best confidence value from the classifier.  
4 Experiments 
4.1 Experimental Setup 
We use the TDT4 Mandarin broadcast news data in our 
experiment. The data set consists of about 170 hours 
(336 shows) of news speech from different sources. 
Speaker turns and soundbite segment information were 
annotated manually in the transcripts. Our current study 
102
only uses the soundbites that have a human-labeled 
speaker name in the surrounding transcripts. There are 
1292 such soundbites in our corpus. We put aside 1/10 
of the data as the development set, another 1/10 as the 
test set, and used the rest as our training set. All the 
transcripts were automatically tagged with named enti-
ties using the NYU tagger (Ji and Grishman, 2005). For 
the classifier, we used the libSVM toolkit (Chang and 
Lin, 2001) and the RBF kernel in our experiments.  
A reasonable baseline for SSNR is to choose the 
closest person name before a soundbite as its speaker. 
We will compare our system performance to this base-
line approach.  
We used two performance metrics in our experi-
ments. First is the instance classification accuracy (CA) 
for the candidate names in the framework of the binary 
classification task. Second, we compute name recogni-
tion accuracy (RA) for the soundbites as follows: 
FilesinSoundbitesof
NamesCorrectwithSoundbitesof
RA
#
#=  
4.2 Effects of Different Manually Selected 
Feature Subsets 
We used 10-fold cross validation on the training set to 
evaluate the effect of different features and also for pa-
rameter optimization. Table 1 shows the instance classi-
fication results. ?PF, Freq, LF? are the features 
described in Section 3.1. ?LF-before? means the uni-
gram features before the soundbites. ?All-before? de-
notes using all the features before the soundbites. 
 
Optimized Para. Feature 
Subsets C G 
CA 
(%) 
PF-1 0.125 2 83.48
+PF-2 2048 1.22e-4 85.62
+PF-3 2048 4.88e-4 85.79
+PF-4 2 0.5 86.18
+Freq 2 0.5 86.18
+LF-before 32 7.81e-3 88.44
+LF-after 
i.e., All features 8 0.0313 88.44
All-before 8 0.0313 88.03
Table 1. Instance classification accuracy (CA) using 
different feature sets. C and G are the optimized pa-
rameters in the SVM model. 
 
We notice that the system performance generally 
improves with incrementally expended feature sets, 
yielding an accuracy of 88.44% using all the features.  
Some features seem not helpful to system performance, 
such as ?Freq? and ?LF-after?. Using all the features 
before the soundbites achieves comparable performance 
to using all the features, indicating that the region before 
a soundbite contributes more than that after it. This is 
expected since the reporters typically have already men-
tioned the person?s name before a soundbite. In addition, 
we evaluated some compound features using our current 
feature definition, but adding those did not improve the 
system performance.  
4.3 Automatic Feature Selection 
We also performed automatic feature selection for the 
SVM model based on the F-score criterion (Chen and 
Lin, 2006). There are 6048 features in total in our sys-
tem. Figure 2 shows the classification performance in 
the training set using different number of features via 
automatic feature selection. 
 
88.61
90.79
87.188.12
88.44
88.61
88.7390.14
88.44
86
87
88
89
90
91
92
60
48
30
24
18
39
15
12 75
6
37
8
18
9 94 47
# of features
CA
(%
)
 
Figure 2. Instance classification accuracy (CA) using F-
score based feature selection. 
 
We can see that automatic feature selection further im-
proves the classification performance (2.36% higher 
accuracy than that in Table 1). Table 2 lists some of the 
top features based on their F-scores. Consistent with our 
expectation, we observe that position related features, as 
well as cue words, are good indicators for SSNR.  
 
Feature F-score 
Justbeforeturn (PF-4) 0.3543 
pre_contextpos=last (PF-1) 0.2857 
pre_senpos=unique (PF-2) 0.0631 
pre_word+1=???/morning? (LF) 0.0475 
pre_word+1= ??/said? (LF) 0.0399 
bool_pre=1 (PF-3) 0.0353 
Justafterturn (PF-4) 0.0349 
pre_contextpos=mid (PF-1) 0.0329 
post_contextpos=first (PF-1) 0.0323 
pre_word+1= ???/today? (LF) 0.0288 
pre_word-1=???/reporter? (LF) 0.0251 
pre_word+1=???/express? (LF) 0.0246 
Table 2. Top features ordered by F-score values. 
   
103
4.4 Performance on Development Set 
Up to now our focus has been on feature selection based 
on instance classification accuracy. Since our ultimate 
goal is to identify soundbite speaker names, we chose 
several promising configurations based on the results 
above to apply to the development set and evaluate the 
soundbite name recognition accuracy. Results using the 
two metrics are presented in Table 3. 
Feature Set CA (%) RA (%) 
Baseline 84.0 59.3 
PF 86.7 54.2 
PF+Freq 86.7 60.4 
PF+Freq+LF-before 87.8 63.5 
PF+Freq+LF-before 
+LF-after (ALL) 88.3 67.7 
Top 1512 by f-score 85.6 62.5 
Top 1839 by f-score 85.4 60.4 
Table 3. Results on the dev set using two metrics: in-
stance classification accuracy (CA), and soundbite name 
recognition accuracy (RA). The oracle RA is 79.1%.  
 
Table 3 shows that using all the features (ALL) 
performs the best, yielding an improvement of 4.3% and 
8.4% compared to the baseline in term of the CA and RA 
respectively. However, using the automatically selected 
feature sets (the last two rows in Table 3) only slightly 
outperforms the baseline. This suggests that the F-score 
based feature selection strategy on the training set may 
not generalize well. Interestingly, ?Freq? and ?LF-after? 
features show some useful contribution (the 4th and 6th 
row in Table 3) respectively on the development set, 
different from the results on the training set using 10-
fold cross validation. The results using the two metrics 
also show that they are not always correlated. 
Because of the possible NER errors, we also meas-
ure the oracle RA, defined as the percent of the sound-
bites for which the correct speaker name (based on NER) 
appears in the region surrounding the soundbite. The 
oracle RA on this data set is 79.1%. We also notice that 
8.3% of the soundbites do not have the correct name 
hypothesis due to an NER boundary error, and that 
12.5% is because of missing errors. 
We used the method as described in Section 3.2 to 
resolve conflicts for the results shown in Table 3. In 
addition, we evaluated another approach?we resort to 
the baseline (i.e., chose the name that is closest to the 
soundbite) for those soundbites that have multiple or no 
positive hypothesis. Our experiments on the develop-
ment set showed this approach degrades system per-
formance (e.g., RA of around 61% using all the features).  
4.5 Results on Blind Test Set 
Finally, we applied the all-feature configuration to our 
blind test data and obtained the results as shown in Ta-
ble 4. Using all the features significantly outperforms 
the baseline. The gain is slightly better than that on the 
development set, although the oracle accuracy is also 
higher on the test set. 
 CA (%) RA (oracle: 85.8%) 
Baseline 81.3 58.4 
All feature 85.1 68.9 
Table 4. Results on the test set.    
5 Conclusion 
We proposed an SVM-based approach for soundbite 
speaker name recognition and examined various linguis-
tic features. Experiments in Mandarin BN corpus show 
that our approach yields an identification accuracy of 
68.9%, significantly better than 58.4% from the baseline. 
Our future work will focus on exploring more useful 
features, such as part-of-speech and semantic features. 
In addition, we plan to test this framework using auto-
matic speech recognition output, speaker segmentation, 
and soundbite segment detection.  
6 Acknowledgement 
We thank Sameer Maskey, Julia Hirschberg, and Mari 
Ostendorf for useful discussions, and Heng Ji for shar-
ing the Mandarin named entity tagger. This work is 
supported by DARPA under Contract No. HR0011-06-
C-0023. Any opinions expressed in this material are 
those of the authors and do not necessarily reflect the 
views of DARPA. 
References 
S. Maskey and J. Hirschberg. 2006. Soundbite Detec-
tion in Broadcast News Domain. In Proc. of INTER-
SPEECH2006.  pp: 1543-1546. 
Y. Liu. 2006. Initial Study on Automatic Identification 
of Speaker Role in Broadcast News Speech. In Proc.  
of HLT-NAACL. pp: 81-84. 
R. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker. 
2000. The Rules Behind Roles: Identifying Speaker 
Role in Radio Broadcasts. In Proc. of AAAI. 
L. Canseco, L. Lamel, and J.-L. Gauvain. 2005. A Com-
parative Study Using Manual and Automatic Tran-
scriptions for Diarization. In Proc. of ASRU. 
H. Ji and R. Grishman. 2005. Improving Name Tagging 
by Reference Resolution and Relation Detection. In 
Proc. of ACL. pp: 411-418. 
Y.-W. Chen and C.-J. Lin. 2006. Combining SVMs with 
Various Feature Selection Strategies. Feature Extrac-
tion, Foundations and Applications, Springer.  
C. Chang and C. Lin. 2001. LIBSVM: A Library for 
Support Vector Machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
104
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 620?628,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Approaches for Automatic Keyword Extraction Using
Meeting Transcripts
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX 75080, USA
{ffliu,deana,feiliu,yangl}@hlt.utdallas.edu
Abstract
This paper explores several unsupervised ap-
proaches to automatic keyword extraction
using meeting transcripts. In the TFIDF
(term frequency, inverse document frequency)
weighting framework, we incorporated part-
of-speech (POS) information, word clustering,
and sentence salience score. We also evalu-
ated a graph-based approach that measures the
importance of a word based on its connection
with other sentences or words. The system
performance is evaluated in different ways, in-
cluding comparison to human annotated key-
words using F-measure and a weighted score
relative to the oracle system performance, as
well as a novel alternative human evaluation.
Our results have shown that the simple un-
supervised TFIDF approach performs reason-
ably well, and the additional information from
POS and sentence score helps keyword ex-
traction. However, the graph method is less
effective for this domain. Experiments were
also performed using speech recognition out-
put and we observed degradation and different
patterns compared to human transcripts.
1 Introduction
Keywords in a document provide important infor-
mation about the content of the document. They
can help users search through information more effi-
ciently or decide whether to read a document. They
can also be used for a variety of language process-
ing tasks such as text categorization and informa-
tion retrieval. However, most documents do not
provide keywords. This is especially true for spo-
ken documents. Current speech recognition system
performance has improved significantly, but there
is no rich structural information such as topics and
keywords in the transcriptions. Therefore, there is
a need to automatically generate keywords for the
large amount of written or spoken documents avail-
able now.
There have been many efforts toward keyword ex-
traction for text domain. In contrast, there is less
work on speech transcripts. In this paper we fo-
cus on one speech genre ? the multiparty meeting
domain. Meeting speech is significantly different
from written text and most other speech data. For
example, there are typically multiple participants
in a meeting, the discussion is not well organized,
and the speech is spontaneous and contains disflu-
encies and ill-formed sentences. It is thus ques-
tionable whether we can adopt approaches that have
been shown before to perform well in written text
for automatic keyword extraction in meeting tran-
scripts. In this paper, we evaluate several differ-
ent keyword extraction algorithms using the tran-
scripts of the ICSI meeting corpus. Starting from
the simple TFIDF baseline, we introduce knowl-
edge sources based on POS filtering, word cluster-
ing, and sentence salience score. In addition, we
also investigate a graph-based algorithm in order to
leverage more global information and reinforcement
from summary sentences. We used different per-
formance measurements: comparing to human an-
notated keywords using individual F-measures and
a weighted score relative to the oracle system per-
formance, and conducting novel human evaluation.
Experiments were conducted using both the human
transcripts and the speech recognition (ASR) out-
620
put. Overall the TFIDF based framework seems to
work well for this domain, and the additional knowl-
edge sources help improve system performance. The
graph-based approach yielded worse results, espe-
cially for the ASR condition, suggesting further in-
vestigation for this task.
2 Related Work
TFIDF weighting has been widely used for keyword
or key phrase extraction. The idea is to identify
words that appear frequently in a document, but do
not occur frequently in the entire document collec-
tion. Much work has shown that TFIDF is very ef-
fective in extracting keywords for scientific journals,
e.g., (Frank et al, 1999; Hulth, 2003; Kerner et al,
2005). However, we may not have a big background
collection that matches the test domain for a reli-
able IDF estimate. (Matsuo and Ishizuka, 2004) pro-
posed a co-occurrence distribution based method us-
ing a clustering strategy for extracting keywords for
a single document without relying on a large corpus,
and reported promising results.
Web information has also been used as an ad-
ditional knowledge source for keyword extraction.
(Turney, 2002) selected a set of keywords first and
then determined whether to add another keyword hy-
pothesis based on its PMI (point-wise mutual infor-
mation) score to the current selected keywords. The
preselected keywords can be generated using basic
extraction algorithms such as TFIDF. It is impor-
tant to ensure the quality of the first selection for the
subsequent addition of keywords. Other researchers
also used PMI scores between each pair of candidate
keywords to select the top k% of words that have
the highest average PMI scores as the final keywords
(Inkpen and Desilets, 2004).
Keyword extraction has also been treated as a
classification task and solved using supervised ma-
chine learning approaches (Frank et al, 1999; Tur-
ney, 2000; Kerner et al, 2005; Turney, 2002; Tur-
ney, 2003). In these approaches, the learning al-
gorithm needs to learn to classify candidate words
in the documents into positive or negative examples
using a set of features. Useful features for this ap-
proach include TFIDF and its variations, position of
a phrase, POS information, and relative length of a
phrase (Turney, 2000). Some of these features may
not work well for meeting transcripts. For exam-
ple, the position of a phrase (measured by the num-
ber of words before its first appearance divided by
the document length) is very useful for news article
text, since keywords often appear early in the doc-
ument (e.g., in the first paragraph). However, for
the less well structured meeting domain (lack of ti-
tle and paragraph), these kinds of features may not
be indicative. A supervised approach to keyword ex-
traction was used in (Liu et al, 2008). Even though
the data set in that study is not very big, it seems that
a supervised learning approach can achieve reason-
able performance for this task.
Another line of research for keyword extrac-
tion has adopted graph-based methods similar to
Google?s PageRank algorithm (Brin and Page,
1998). In particular, (Wan et al, 2007) attempted
to use a reinforcement approach to do keyword ex-
traction and summarization simultaneously, on the
assumption that important sentences usually contain
keywords and keywords are usually seen in impor-
tant sentences. We also find that this assumption also
holds using statistics obtained from the meeting cor-
pus used in this study. Graph-based methods have
not been used in a genre like the meeting domain;
therefore, it remains to be seen whether these ap-
proaches can be applied to meetings.
Not many studies have been performed on speech
transcripts for keyword extraction. The most rel-
evant work to our study is (Plas et al, 2004),
where the task is keyword extraction in the mul-
tiparty meeting corpus. They showed that lever-
aging semantic resources can yield significant per-
formance improvement compared to the approach
based on the relative frequency ratio (similar to
IDF). There is also some work using keywords for
other speech processing tasks, e.g., (Munteanu et
al., 2007; Bulyko et al, 2007; Wu et al, 2007; De-
silets et al, 2002; Rogina, 2002). (Wu et al, 2007)
showed that keyword extraction combined with se-
mantic verification can be used to improve speech
retrieval performance on broadcast news data. In
(Rogina, 2002), keywords were extracted from lec-
ture slides, and then used as queries to retrieve rel-
evant web documents, resulting in an improved lan-
guage model and better speech recognition perfor-
mance of lectures. There are many differences be-
tween written text and speech ? meetings in par-
ticular. Thus our goal in this paper is to investi-
621
gate whether we can successfully apply some exist-
ing techniques, as well as propose new approaches
to extract keywords for the meeting domain. The
aim of this study is to set up some starting points for
research in this area.
3 Data
We used the meetings from the ICSI meeting data
(Janin et al, 2003), which are recordings of naturally
occurring meetings. All the meetings have been
transcribed and annotated with dialog acts (DA)
(Shriberg et al, 2004), topics, and extractive sum-
maries (Murray et al, 2005). The ASR output for
this corpus is obtained from a state-of-the-art SRI
conversational telephone speech system (Zhu et al,
2005), with a word error rate of about 38.2% on
the entire corpus. We align the human transcripts
and ASR output, then map the human annotated DA
boundaries and topic boundaries to the ASR words,
such that we have human annotation of these infor-
mation for the ASR output.
We recruited three Computer Science undergradu-
ate students to annotate keywords for each topic seg-
ment, using 27 selected ICSI meetings.1 Up to five
indicative key words or phrases were annotated for
each topic. In total, we have 208 topics annotated
with keywords. The average length of the topics
(measured using the number of dialog acts) among
all the meetings is 172.5, with a high standard devi-
ation of 236.8. We used six meetings as our devel-
opment set (the same six meetings as the test set in
(Murray et al, 2005)) to optimize our keyword ex-
traction methods, and the remaining 21 meetings for
final testing in Section 5.
One example of the annotated keywords for a
topic segment is:
? Annotator I: analysis, constraints, template
matcher;
? Annotator II: syntactic analysis, parser, pattern
matcher, finite-state transducers;
? Annotator III: lexicon, set processing, chunk
parser.
Note that these meetings are research discussions,
and that the annotators may not be very familiar with
1We selected these 27 meetings because they have been used
in previous work for topic segmentation and summarization
(Galley et al, 2003; Murray et al, 2005).
the topics discussed and often had trouble deciding
the important sentences or keywords. In addition,
limiting the number of keywords that an annotator
can select for a topic also created some difficulty.
Sometimes there are more possible keywords and
the annotators felt it is hard to decide which five are
the most topic indicative. Among the three annota-
tors, we notice that in general the quality of anno-
tator I is the poorest. This is based on the authors?
judgment, and is also confirmed later by an indepen-
dent human evaluation (in Section 6).
For a better understanding of the gold standard
used in this study and the task itself, we thoroughly
analyzed the human annotation consistency. We re-
moved the topics labeled with ?chitchat? by at least
one annotator, and also the digit recording part in
the ICSI data, and used the remaining 140 topic seg-
ments. We calculated the percentage of keywords
agreed upon by different annotators for each topic,
as well as the average for all the meetings. All of the
consistency analysis is performed based on words.
Figure 1 illustrates the annotation consistency over
different meetings and topics. The average consis-
tency rate across topics is 22.76% and 5.97% among
any two and all three annotators respectively. This
suggests that people do not have a high agreement
on keywords for a given document. We also notice
that the two person agreement is up to 40% for sev-
eral meetings and 80% for several individual top-
ics, and the agreement among all three annotators
reaches 20% and 40% for some meetings or topics.
This implies that the consistency depends on topics
(e.g., the difficulty or ambiguity of a topic itself, the
annotators? knowledge of that topic). Further studies
are needed for the possible factors affecting human
agreement. We are currently creating more annota-
tions for this data set for better agreement measure
and also high quality annotation.
4 Methods
Our task is to extract keywords for each of the topic
segments in each meeting transcript. Therefore, by
?document?, we mean a topic segment in the re-
mainder of this paper. Note that our task is different
from keyword spotting, where a keyword is provided
and the task is to spot it in the audio (along with its
transcript).
The core part of keyword extraction is for the sys-
622
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120
3 agree
2 agree
0
0.1
0.2
0.3
0.4
0.5
1 3 5 7 9 11 13 15 17 19 21 23 25 27
3 agree
2 agree
Figure 1: Human annotation consistency across differ-
ent topics (upper graph) and meetings (lower graph). Y-
axis is the percent of the keywords agreed upon by two or
three annotators.
tem to assign an importance score to a word, and
then pick the top ranked words as keywords. We
compare different methods for weight calculation in
this study, broadly divided into the following two
categories: the TFIDF framework and the graph-
based model. Both are unsupervised learning meth-
ods.2 In all of the following approaches, when se-
lecting the final keywords, we filter out any words
appearing on the stopword list. These stopwords are
generated based on the IDF values of the words us-
ing all the meeting data by treating each topic seg-
ment as a document. The top 250 words from this
list (with the lowest IDF values) were used as stop-
words. We generated two different stopword lists for
human transcripts and ASR output respectively. In
addition, in this paper we focus on performing key-
word extraction at the single word level, therefore
no key phrases are generated.
2Note that by unsupervised methods, we mean that no data
annotated with keywords is needed. These methods do require
the use of some data to generate information such as IDF, or
possibly a development set to optimize some parameters or
heuristic rules.
4.1 TFIDF Framework
(A) Basic TFIDF weighting
The term frequency (TF) for a word wi in a doc-
ument is the number of times the word occurs in the
document. The IDF value is:
IDFi = log(N/Ni)
whereNi denotes the number of the documents con-
taining word wi, and N is the total number of the
documents in the collection. We also performed L2
normalization for the IDF values when combining
them with other scores.
(B) Part of Speech (POS) filtering
In addition to using a stopword list to remove
words from consideration, we also leverage POS in-
formation to filter unlikely keywords. Our hypothe-
sis is that verb, noun and adjective words are more
likely to be keywords, so we restrict our selection to
words with these POS tags only. We used the TnT
POS tagger (Brants, 2000) trained from the Switch-
board data to tag the meeting transcripts.
(C) Integrating word clustering
One weakness of the baseline TFIDF is that it
counts the frequency for a particular word, without
considering any words that are similar to it in terms
of semantic meaning. In addition, when the docu-
ment is short, the TF may not be a reliable indicator
of the importance of the word. Our idea is therefore
to account for the frequency of other similar words
when calculating the TF of a word in the document.
For this, we group all the words into clusters in an
unsupervised fashion. If the total term frequency
of all the words in one cluster is high, it is likely
that this cluster contributes more to the current topic
from a thematic point of view. Thus we want to as-
sign higher weights to the words in this cluster.
We used the SRILM toolkit (Stolcke, 2002) for
automatic word clustering over the entire docu-
ment collection. It minimizes the perplexity of the
induced class-based n-gram language model com-
pared to the original word-based model. Using the
clusters, we then adjust the TF weighting by inte-
grating with the cluster term frequency (CTF):
TF CTF (wi) = TF (wi)??(
P
wl?Ci,wl 6=wi freq(wl))
where the last summation component means the to-
tal term frequency of all the other words in this docu-
ment that belong to the same clusterCi as the current
623
word wi. We set parameter ? to be slightly larger
than 1. We did not include stopwords when adding
the term frequencies for the words in a cluster.
(D) Combining with sentence salience score
Intuitively, the words in an important sentence
should be assigned a high weight for keyword ex-
traction. In order to leverage the sentence infor-
mation, we adjust a word?s weight by the salience
scores of the sentences containing that word. The
sentence score is calculated based on its cosine sim-
ilarity to the entire meeting. This score is often used
in extractive summarization to select summary sen-
tences (Radev et al, 2001). The cosine similarity
between two vectors, D1 and D2, is defined as:
sim(D1, D2) =
?
i t1it2i??
i t21i ?
??
i t22i
where ti is the term weight for a word wi, for which
we use the TFIDF value.
4.2 Graph-based Methods
For the graph-based approach, we adopt the itera-
tive reinforcement approach from (Wan et al, 2007)
in the hope of leveraging sentence information for
keyword extraction. This algorithm is based on the
assumption that important sentences/words are con-
nected to other important sentences/words.
Four graphs are created: one graph in which sen-
tences are connected to other sentences (S-S graph),
one in which words are connected to other words
(W-W graph), and two graphs connecting words to
sentences with uni-directional edges (W-S and S-W
graphs). Stopwords are removed before the creation
of the graphs so they will be ineligible to be key-
words.
The final weight for a word node depends on its
connection to other words (W-W graph) and other
sentences (W-S graph); similarly, the weight for
a sentence node is dependent on its connection to
other sentences (S-S graph) and other words (S-W
graph). That is,
u = ?UTu+ ?W? T v
v = ?V T v + ?W Tu
where u and v are the weight vectors for sentence
and word nodes respectively, U, V,W, W? represent
the S-S, W-W, S-W, and W-S connections. ? and ?
specify the contributions from the homogeneous and
the heterogeneous nodes. The initial weight is a uni-
form one for the word and sentence vector. Then
the iterative reinforcement algorithm is used until
the node weight values converge (the difference be-
tween scores at two iterations is below 0.0001 for all
nodes) or 5,000 iterations are reached.
We have explored various ways to assign weights
to the edges in the graphs. Based on the results on
the development set, we use the following setup in
this paper:
? W-W Graph: We used a diagonal matrix for
the graph connection, i.e., there is no connec-
tion among words. The self-loop values are
the TFIDF values of the words. This is also
equivalent to using an identity matrix for the
word-word connection and TFIDF as the initial
weight for each vertex in the graph. We investi-
gated other strategies to assign a weight for the
edge between two word nodes; however, so far
the best result we obtained is using this diago-
nal matrix.
? S-W and W-S Graphs: The weight for an
edge between a sentence and a word is the TF
of the word in the sentence multiplied by the
word?s IDF value. These weights are initially
added only to the S-W graph, as in (Wan et al,
2007); then that graph is normalized and trans-
posed to create the W-S graph.
? S-S Graph: The sentence node uses a vector
space model and is composed of the weights of
those words connected to this sentence in the
S-W graph. We then use cosine similarity be-
tween two sentence vectors.
Similar to the above TFIDF framework, we also
use POS filtering for the graph-based approach. Af-
ter the weights for all the words are determined, we
select the top ranked words with the POS restriction.
5 Experimental Results: Automatic
Evaluation
Using the approaches described above, we com-
puted weights for the words and then picked the top
five words as the keywords for a topic. We chose five
keywords since this is the number of keywords that
624
human annotators used as a guideline, and it also
yielded good performance in the development set.
To evaluate system performance, in this section we
use human annotated keywords as references, and
compare the system output to them. The first metric
we use is F-measure, which has been widely used
for this task and other detection tasks. We compare
the system output with respect to each human anno-
tation, and calculate the maximum and the average
F-scores. Note that our keyword evaluation is word-
based. When human annotators choose key phrases
(containing more than one word), we split them into
words and measure the matching words. Therefore,
when the system only generates five keywords, the
upper bound of the recall rate may not be 100%. In
(Liu et al, 2008), a lenient metric is used which ac-
counts for some inflection of words. Since that is
highly correlated with the results using exact word
match, we report results based on strict matching in
the following experiments.
The second metric we use is similar to Pyramid
(Nenkova and Passonneau, 2004), which has been
used for summarization evaluation. Instead of com-
paring the system output with each individual hu-
man annotation, the method creates a ?pyramid?
using all the human annotated keywords, and then
compares system output to this pyramid. The pyra-
mid consists of all the annotated keywords at dif-
ferent levels. Each keyword has a score based on
how many annotators have selected this one. The
higher the score, the higher up the keyword will be in
the pyramid. Then we calculate an oracle score that
a system can obtain when generating k keywords.
This is done by selecting keywords in the decreas-
ing order in terms of the pyramid levels until we
obtain k keywords. Finally for the system hypoth-
esized k keywords, we compute its score by adding
the scores of the keywords that match those in the
pyramid. The system?s performance is measured us-
ing the relative performance of the system?s pyramid
scores divided by the oracle score.
Table 1 shows the results using human transcripts
for different methods on the 21 test meetings (139
topic segments in total). For comparison, we also
show results using the supervised approach as in
(Liu et al, 2008), which is the average of the 21-
fold cross validation. We only show the maximum
F-measure with respect to individual annotations,
since the average scores show similar trend. In ad-
dition, the weighted relative scores already accounts
for the different annotation and human agreement.
Methods F-measure weighted relative score
TFIDF 0.267 0.368
+ POS 0.275 0.370
+ Clustering 0.277 0.367
+ Sent weight 0.290 0.404
Graph 0.258 0.364
Graph+POS 0.277 0.380
Supervised 0.312 0.401
Table 1: Keyword extraction results using human tran-
scripts compared to human annotations.
We notice that for the TFIDF framework, adding
POS information slightly helps the basic TFIDF
method. In all the meetings, our statistics show that
adding POS filtering removed 2.3% of human anno-
tated keywords from the word candidates; therefore,
this does not have a significant negative impact on
the upper bound recall rate, but helps eliminate un-
likely keyword candidates. Using word clustering
does not yield a performance gain, most likely be-
cause of the clustering technique we used ? it does
clustering simply based on word co-occurrence and
does not capture semantic similarity properly.
Combining the term weight with the sentence
salience score improves performance, supporting the
hypothesis that summary sentences and keywords
can reinforce each other. In fact we performed an
analysis of keywords and summaries using the fol-
lowing two statistics:
(1) k = Psummary(wi)Ptopic(wi)
where Psummary(wi) and Ptopic(wi) represent the
the normalized frequency of a keyword wi in the
summary and the entire topic respectively; and
(2) s = PSsummaryPStopic
where PSsummary represents the percentage of the
sentences containing at least one keyword among all
the sentences in the summary, and similarly PStopic
is measured using the entire topic segment. We
found that the average k and s are around 3.42 and
6.33 respectively. This means that keywords are
625
more likely to occur in the summary compared to the
rest of the topic, and the chance for a summary sen-
tence to contain at least one keyword is much higher
than for the other sentences in the topic.
For the graph-based methods, we notice that
adding POS filtering also improves performance,
similar to the TFIDF framework. However, the
graph method does not perform as well as the TFIDF
approach. Comparing with using TFIDF alone, the
graph method (without using POS) yielded worse re-
sults. In addition to using the TFIDF for the word
nodes, information from the sentences is used in the
graph method since a word is linked to sentences
containing this word. The global information in the
S-S graph (connecting a sentence to other sentences
in the document) is propagated to the word nodes.
Unlike the study in (Wan et al, 2007), this infor-
mation does not yield any gain. We did find that the
graph approach performed better in the development
set, but it seems that it does not generalize to this test
set.
Compared to the supervised results, the TFIDF
approach is worse in terms of the individual maxi-
mum F-measure, but achieves similar performance
when using the weighted relative score. However,
the unsupervised TFIDF approach is much simpler
and does not require any annotated data for train-
ing. Therefore it may be easily applied to a new
domain. Again note that these results used word-
based selection. (Liu et al, 2008) investigated
adding bigram key phrases, which we expect to
be independent of these unigram-based approaches
and adding bigram phrases will yield further per-
formance gain for the unsupervised approach. Fi-
nally, we analyzed if the system?s keyword ex-
traction performance is correlated with human an-
notation disagreement using the unsupervised ap-
proach (TFIDF+POS+Sent weight). The correla-
tion (Spearman?s ? value) between the system?s
F-measure and the three-annotator consistency on
the 27 meetings is 0.5049 (p=0.0072). This indi-
cates that for the meetings with a high disagreement
among human annotators, it is also challenging for
the automatic systems.
Table 2 shows the results using ASR output for
various approaches. The performance measure is
the same as used in Table 1. We find that in gen-
eral, there is a performance degradation compared
to using human transcripts, which is as expected.
We found that only 59.74% of the human annotated
keywords appear in ASR output, that is, the upper
bound of recall is very low. The TFIDF approach
still outperforms the graph method. Unlike on hu-
man transcripts, the addition of information sources
in the TFIDF approach did not yield significant per-
formance gain. A big difference from the human
transcript condition is the use of sentence weight-
ing ? adding it degrades performance in ASR, in
contrast to the improvement in human transcripts.
This is possibly because the weighting of the sen-
tences is poor when there are many recognition er-
rors from content words. In addition, compared to
the supervised results, the TFIDF method has sim-
ilar maximum F-measure, but is slightly worse us-
ing the weighted score. Further research is needed
for the ASR condition to investigate better modeling
approaches.
Methods F-measure weighted relative score
TFIDF 0.191 0.257
+ POS 0.196 0.259
+ Clustering 0.196 0.259
+ Sent weigh 0.178 0.241
Graph 0.173 0.223
Graph+POS 0.183 0.233
Supervised 0.197 0.269
Table 2: Keyword extraction results using ASR output.
6 Experimental Results: Human
Evaluation
Given the disagreement among human annotators,
one question we need to answer is whether F-
measure or even the weighted relative scores com-
pared with human annotations are appropriate met-
rics to evaluate system-generated keywords. For
example, precision measures among the system-
generated keywords how many are correct. How-
ever, this does not measure if the unmatched system-
generated keywords are bad or acceptable. We
therefore performed a small scale human evaluation.
We selected four topic segments from four differ-
ent meetings, and gave output from different sys-
tems to five human subjects. The subjects ranged
in age from 22 to 63, and all but one had only basic
knowledge of computers. We first asked the eval-
626
uators to read the entire topic transcript, and then
presented them with the system-generated keywords
(randomly ordered by different systems). For com-
parison, the keywords annotated by our three hu-
man annotators were also included without reveal-
ing which sets of keywords were generated by a
human and which by a computer. Because there
was such disagreement between annotators regard-
ing what made good keywords, we instead asked our
evaluators to mark any words that were definitely
not keywords. Systems that produced more of these
rejected words (such as ?basically? or ?mmm-hm?)
are assumed to be worse than those containing fewer
rejected words. We then measured the percentage of
rejected keywords for each system/annotator. The
results are shown in Table 3. Not surprisingly, the
human annotations rank at the top. Overall, we find
human evaluation results to be consistent with the
automatic evaluation metrics in terms of the ranking
of different systems.
Systems Rejection rate
Annotator 2 8%
Annotator 3 19%
Annotator 1 25%
TFIDF + POS 28%
TFIDF 30%
Table 3: Human evaluation results: percentage of the re-
jected keywords by human evaluators for different sys-
tems/annotators.
Note this rejection rate is highly related to the re-
call/precision measure in the sense that it measures
how many keywords are acceptable (or rejected)
among the system generated ones. However, instead
of comparing to a fixed set of human annotated key-
words (e.g., five) and using that as a gold standard
to compute recall/precision, in this evaluation, the
human evaluator may have a larger set of accept-
able keywords in their mind. We also measured the
human evaluator agreement regarding the accepted
or bad keywords. We found that the agreement on
a bad keyword among five, four, and three human
evaluator is 10.1%, 14.8%, and 10.1% respectively.
This suggests that humans are more likely to agree
on a bad keyword selection compared to agreement
on the selected keywords, as discussed in Section 3
(even though the data sets in these two analysis are
not the same). Another observation from the human
evaluation is that sometimes a person rejects a key-
word from one system output, but accepts that on
the list from another system. We are not sure yet
whether this is the inconsistency from human evalu-
ators or whether the judgment is based on a word?s
occurrence with other provided keywords and thus
some kind of semantic coherence. Further investi-
gation on human evaluation is still needed.
7 Conclusions and Future Work
In this paper, we evaluated unsupervised keyword
extraction performance for the meeting domain, a
genre that is significantly different from most pre-
vious work. We compared several different ap-
proaches using the transcripts of the ICSI meeting
corpus. Our results on the human transcripts show
that the simple TFIDF based method is very compet-
itive. Adding additional knowledge such as POS and
sentence salience score helps improve performance.
The graph-based approach performs less well in this
task, possibly because of the lack of structure in
this domain. We use different performance measure-
ments, including F-measure with respect to individ-
ual human annotations and a weighted metric rela-
tive to the oracle system performance. We also per-
formed a new human evaluation for this task and our
results show consistency with the automatic mea-
surement. In addition, experiments on the ASR out-
put show performance degradation, but more impor-
tantly, different patterns in terms of the contributions
of information sources compared to using human
transcripts. Overall the unsupervised approaches are
simple but effective; however, system performance
compared to the human performance is still low,
suggesting more work is needed for this domain.
For the future work, we plan to investigate dif-
ferent weighting algorithms for the graph-based ap-
proach. We also need a better way to decide the
number of keywords to generate instead of using a
fixed number. Furthermore, since there are multiple
speakers in the meeting domain, we plan to incor-
porate speaker information in various approaches.
More importantly, we will perform a more rigorous
human evaluation, and also use extrinsic evaluation
to see whether automatically generated keywords fa-
cilitate tasks such as information retrieval or meeting
browsing.
627
Acknowledgments
This work is supported by NSF award IIS-0714132.
Any opinions expressed in this work are those of the
authors and do not necessarily reflect the views of
NSF.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of the 6th Applied NLP Conference.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 30.
I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and
O. Cetin. 2007. Web resources for language modeling
in conversational speech recognition. ACM Transac-
tions on Speech and Language Processing, 5:1?25.
A. Desilets, B.D. Bruijn, and J. Martin. 2002. Extracting
keyphrases from spoken audio documents. In Infor-
mation Retrieval Techniques for Speech Applications,
pages 339?342.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 688?673.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216?223.
D. Inkpen and A. Desilets. 2004. Extracting
semantically-coherent keyphrases from speech. Cana-
dian Acoustics Association, 32:130?131.
A. Janin, D. Baron, J. Edwards, D. Ellis, G . Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI meeting corpus. In
Proceedings of ICASSP.
Y.H. Kerner, Z. Gross, and A. Masa. 2005. Automatic
extraction and learning of keyphrases from scientific
articles. In Computational Linguistics and Intelligent
Text Processing, pages 657?669.
F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword
extraction for the meeting corpus using supervised ap-
proach and bigram expansion. In Proceedings of IEEE
SLT.
Y. Matsuo and M. Ishizuka. 2004. Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artifi-
cial Intelligence, 13(1):157?169.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modeling for automatic lecture tran-
scription. In Proceedings of Interspeech.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005.
Evaluating automatic summaries of meeting record-
ings. In Proceedings of ACL 2005 MTSE Workshop,
pages 33?40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method.
In Proceedings of HLT/NAACL.
L. Plas, V. Pallotta, M. Rajman, and H. Ghorbel. 2004.
Automatic keyword extraction from spoken text. a
comparison of two lexical resources: the EDR and
WordNet. In Proceedings of the LREC.
D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Ex-
periments in single and multi-document summariza-
tion using MEAD. In Proceedings of The First Docu-
ment Understanding Conference.
I. Rogina. 2002. Lecture and presentation tracking in an
intelligent meeting room. In Proceedings of ICMI.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. In Proceedings of SIGDial Workshop, pages
97?100.
A. Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303?336.
P.D. Turney. 2002. Mining the web for lexical knowl-
edge to improve keyphrase extraction: Learning from
labeled and unlabeled data. In National Research
Council, Institute for Information Technology, Techni-
cal Report ERB-1096.
P.D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of IJCAI, pages 434?439.
X. Wan, J. Yang, and J. Xiao. 2007. Towards an iter-
ative reinforcement approach for simultaneous docu-
ment summarization and keyword extraction. In Pro-
ceedings of ACL, pages 552?559.
C.H. Wu, C.L. Huang, C.S. Hsu, and K.M. Lee. 2007.
Speech retrieval using spoken keyword extraction and
semantic verification. In Proceedings of IEEE Region
10 Conference, pages 1?4.
Q. Zhu, A. Stolcke, B. Chen, and N. Morgan. 2005.
Using MLP features in SRI?s conversational speech
recognition system. In Proceedings of Interspeech.
628
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 672?679,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Unsupervised Language Model Adaptation Incorporating  
Named Entity Information 
Feifan Liu and Yang Liu 
Department of Computer Science 
The University of Texas at Dallas, Richardson, TX, USA 
{ffliu,yangl}@hlt.utdallas.edu 
  
Abstract 
Language model (LM) adaptation is im-
portant for both speech and language 
processing. It is often achieved by com-
bining a generic LM with a topic-specific 
model that is more relevant to the target 
document.  Unlike previous work on un-
supervised LM adaptation, this paper in-
vestigates how effectively using named 
entity (NE) information, instead of con-
sidering all the words, helps LM adapta-
tion. We evaluate two latent topic analysis 
approaches in this paper, namely, cluster-
ing and Latent Dirichlet Allocation 
(LDA). In addition, a new dynamically 
adapted weighting scheme for topic mix-
ture models is proposed based on LDA 
topic analysis. Our experimental results 
show that the NE-driven LM adaptation 
framework outperforms the baseline ge-
neric LM. The best result is obtained us-
ing the LDA-based approach by 
expanding the named entities with syntac-
tically filtered words, together with using 
a large number of topics, which yields a 
perplexity reduction of 14.23% compared 
to the baseline generic LM. 
1 Introduction 
Language model (LM) adaptation plays an impor-
tant role in speech recognition and many natural 
language processing tasks, such as machine trans-
lation and information retrieval. Statistical N-gram 
LMs have been widely used; however, they capture 
only local contextual information. In addition, even 
with the increasing amount of LM training data, 
there is often a mismatch problem because of dif-
ferences in domain, topics, or styles. Adaptation of 
LM, therefore, is very important in order to better 
deal with a variety of topics and styles. 
Many studies have been conducted for LM ad-
aptation. One method is supervised LM adaptation, 
where topic information is typically available and a 
topic specific LM is interpolated with the generic 
LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 
2005). In contrast, various unsupervised ap-
proaches perform latent topic analysis for LM ad-
aptation. To identify implicit topics from the 
unlabeled corpus, one simple technique is to group 
the documents into topic clusters by assigning only 
one topic label to a document (Iyer and Ostendorf, 
1996). Recently several other methods in the line 
of latent semantic analysis have been proposed and 
used in LM adaptation, such as latent semantic 
analysis (LSA) (Bellegarda, 2000), probabilistic 
latent semantic analysis (PLSA) (Gildea and Hof-
mann, 1999), and LDA (Blei et al, 2003). Most of 
these existing approaches are based on the ?bag of 
words? model to represent documents, where all 
the words are treated equally and no relation or 
association between words is considered.  
Unlike prior work in LM adaptation, this paper 
investigates how to effectively leverage named 
entity information for latent topic analysis. Named 
entities are very common in domains such as 
newswire or broadcast news, and carry valuable 
information, which we hypothesize is topic indica-
tive and useful for latent topic analysis. We com-
pare different latent topic generation approaches as 
well as model adaptation methods, and propose an 
LDA based dynamic weighting method for the 
topic mixture model. Furthermore, we expand 
672
named entities by incorporating other content 
words, in order to capture more topic information. 
Our experimental results show that the proposed 
method of incorporating named information in LM 
adaptation is effective. In addition, we find that for 
the LDA based adaptation scheme, adding more 
content words and increasing the number of topics 
can further improve the performance significantly. 
The paper is organized as follows. In Section 2 
we review some related work. Section 3 describes 
in detail our unsupervised LM adaptation approach 
using named entities. Experimental results are pre-
sented and discussed in Section 4. Conclusion and 
future work appear in Section 5. 
2 Related Work 
There has been a lot of previous related work on 
LM adaptation. Suzuki and Gao (2005) compared 
different supervised LM adaptation approaches, 
and showed that three discriminative methods sig-
nificantly outperform the maximum a posteriori 
(MAP) method. For unsupervised LM adaptation, 
an earlier attempt is a cache-based model (Kuhn 
and Mori, 1990), developed based on the assump-
tion that words appearing earlier in a document are 
likely to appear again. The cache concept has also 
been used to increase the probability of unseen but 
topically related words, for example, the trigger-
based LM adaptation using the maximum entropy 
approach (Rosenfeld, 1996). 
Latent topic analysis has recently been investi-
gated extensively for language modeling. Iyer and 
Ostendorf (1996) used hard clustering to obtain 
topic clusters for LM adaptation, where a single 
topic is assigned to each document. Bellegarda 
(2000) employed Latent Semantic Analysis (LSA) 
to map documents into implicit topic sub-spaces 
and demonstrated significant reduction in perplex-
ity and word error rate (WER). Its probabilistic 
extension, PLSA, is powerful for characterizing 
topics and documents in a probabilistic space and 
has been used in LM adaptation. For example, 
Gildea and Hofmann (1999) reported noticeable 
perplexity reduction via a dynamic combination of 
many unigram topic models with a generic trigram 
model. Proposed by Blei et al (2003), Latent 
Dirichlet Allocation (LDA) loosens the constraint 
of the document-specific fixed weights by using a 
prior distribution and has quickly become one of 
the most popular probabilistic text modeling tech-
niques. LDA can overcome the drawbacks in the 
PLSA model, and has been shown to outperform 
PLSA in corpus perplexity and text classification 
experiments (Blei et al, 2003). Tam and Schultz 
(2005) successfully applied the LDA model to un-
supervised LM adaptation by interpolating the 
background LM with the dynamic unigram LM 
estimated by the LDA model. Hsu and Glass (2006) 
investigated using hidden Markov model with 
LDA to allow for both topic and style adaptation. 
Mrva and Woodland (2006) achieved WER reduc-
tion on broadcast conversation recognition using 
an LDA based adaptation approach that effectively 
combined the LMs trained from corpora with dif-
ferent styles: broadcast news and broadcast con-
versation data. 
In this paper, we investigate unsupervised LM 
adaptation using clustering and LDA based topic 
analysis. Unlike the clustering based interpolation 
method as in (Iyer and Ostendorf, 1996), we ex-
plore different distance measure methods for topic 
analysis. Different from the LDA based framework 
as in (Tam and Schultz, 2005), we propose a novel 
dynamic weighting scheme for the topic adapted 
LM. More importantly, the focus of our work is to 
investigate the role of named entity information in 
LM adaptation, which to our knowledge has not 
been explored.  
3 Unsupervised LM Adaptation Integrat-
ing Named Entities (NEs) 
3.1 Overview of the NE-driven LM Adapta-
tion Framework 
Figure 1 shows our unsupervised LM adaptation 
framework using NEs. For training, we use the text 
collection to train the generic word-based N-gram 
LM. Then we apply named entity recognition 
(NER) and topic analysis to train multiple topic 
specific N-gram LMs. During testing, NER is per-
formed on each test document, and then a dynami-
cally adaptive LM based on the topic analysis 
result is combined with the general LM. Note that 
in this figure, we evaluate the performance of LM 
adaptation using the perplexity measure. We will 
evaluate this framework for N-best or lattice res-
coring in speech recognition in the future. 
In our experiments, different topic analysis 
methods combined with different topic matching 
and adaptive schemes result in several LM adapta-
673
tion paradigms, which are described below in de-
tails. 
 
Training Text Test Text
NER NER
Latent Topic 
Analysis
Compute 
Perplexity
Generic N-gram 
Training
Topic Model 
Training
Topic Matching
Topic Model 
Adaptation
Model 
Interpolation
 
Figure 1. Framework of NE-driven LM adaptation. 
 
3.2 NE-based Clustering for LM Adaptation 
Clustering is a simple unsupervised topic analysis 
method. We use NEs to construct feature vectors 
for the documents, rather than considering all the 
words as in most previous work. We use the 
CLUTO1 toolkit to perform clustering. It finds a 
predefined number of clusters based on a specific 
criterion, for which we chose the following func-
tion: 
? ?
= ?
=
K
i Suv
k
i
uvsimSSS
1 ,
*
21 ),(maxarg)( L  
where K is the desired number of clusters, Si is the 
set of documents belonging to the ith cluster, v and 
u represent two documents, and sim(v, u) is the 
similarity between them. We use the cosine dis-
tance to measure the similarity between two docu-
ments: 
||||||||
),(
uv
uv
uvsim rr
rr
?
?=                         (1) 
where v
r
 and u
r
 are the feature vectors represent-
ing the two documents respectively, in our experi-
ments composed of NEs. For clustering, the 
elements in every feature vector are scaled based 
on their term frequency and inverse document fre-
                                                          
1 Available at http://glaros.dtc.umn.edu/gkhome/views/cluto 
quency, a concept widely used in information re-
trieval.   
After clustering, we train an N-gram LM, called 
a topic LM, for each cluster using the documents in 
it. 
During testing, we identify the ?topic? for the 
test document, and interpolate the topic specific 
LM with the background LM, that is, if the test 
document belongs to the cluster S*, we can predict 
a word wk in the document given the word?s his-
tory hk using the following equation: 
)|()1(
)|()|(
* kkSTopic
kkGeneralkk
hwp
hwphwp
??+
=
?
?
      (2) 
where ?  is the interpolation weight. 
We investigate two approaches to find the topic 
assignment S* for a given test document. 
(A) cross-entropy measure 
For a test document d=w1,w2,?,wn with a word 
distribution pd(w) and a cluster S with a topic LM 
ps(w), the cross entropy CE(d, S) can be computed 
as: 
?
=
?==
n
i
isidsd wpwpppHSdCE
1
2 ))((log)(),(),(
    From the information theoretic perspective, the 
cluster with the lower cross entropy value is ex-
pected to be more topically correlated to the test 
document. For each test document, we compute the 
cross entropy values according to different clusters, 
and select the cluster S* that satisfies: 
),(minarg
1
*
i
Ki
SdCES
??
=  
(B) cosine similarity  
For each cluster, its centroid can be obtained by: 
?
=
= i
n
k
ik
i
i un
cv
1
1
 
where uik is the vector for the kth document in the ith 
cluster, and ni is the number of documents in the ith 
cluster. The distance between the test document 
and a cluster can then be easily measured by the 
cosine similarity function as in Equation (1). Our 
goal here is to find the cluster S* which the test 
document is closest to, that is, 
||||||||
maxarg
1
*
i
i
Ki cvd
cvd
S ?
?=
??
r
r
 
674
where d
r
is the feature vector for the test document.   
3.3 NE-based LDA for LM Adaptation 
LDA model (Blei et al, 2003) has been introduced 
as a new, semantically consistent generative model, 
which overcomes overfitting and the problem of 
generating new documents in PLSA. It is a three-
level hierarchical Bayesian model. Based on the 
LDA model, a document d is generated as follows. 
? Sample a vector of K topic mixture weights 
?  from a prior Dirichlet distribution with 
parameter ? : 
?
=
?=
K
k
k
kf
1
1);( ????  
? For each word w in d, pick a topic k from the 
multinomial distribution ? . 
? Pick a word w from the multinomial distri-
bution kw,?  given the kth topic. 
For a document d=w1,w2,?wn, the LDA model 
assigns it the following probability: 
? ?? ???
????
? ?=
= =?
????? dfdp n
i
K
k
kkwi
);()(
1 1
 
We use the MATLAB topic Toolbox 1.3 (Grif-
fiths et al, 2004) in the training set to obtain the 
document-topic matrix, DP, and the word-topic 
matrix, WP. Note that here ?words? correspond to 
the elements in the feature vector used to represent 
the document (e.g., NEs). In the DP matrix, an en-
try cik represents the counts of words in a document 
di that are from a topic zk (k=1,2,?,K). In the WP 
matrix, an entry fjk represents the frequency of a 
word wj generated from a topic zk (k=1,2,?,K) 
over the training set.  
For training, we assign a topic zi* to a document 
di such that ik
Kk
i cz ??
=
1
* maxarg . Based on the docu-
ments belonging to the different topics, K topic N-
gram LMs are trained. This ?hard clustering? strat-
egy allows us to train an LM that accounts for all 
the words rather than simply those NEs used in 
LDA analysis, as well as use higher order N-gram 
LMs, unlike the ?unigram? based LDA in previous 
work. 
For a test document d = w1,w2,?,wn that is gen-
erated by multiple topics under the LDA assump-
tion, we formulate a dynamically adapted topic 
model using the mixture of LMs from different 
topics: 
?
=
? ?=
K
i
kkzikkadaptLDA hwphwp i
1
)|()|( ?  
where )|( kkz hwp i  stands for the i
th topic LM, and 
?i is the mixture weight. Different from the idea of 
dynamic topic adaptation in (Tam and Schultz, 
2005), we propose a new weighting scheme to cal-
culate ?i that directly uses the two resulting matri-
ces from LDA analysis during training: 
?
=
=
n
j
jjkk dwpwzp
1
)|()|(?  
??
==
== n
q
q
j
jK
p
jp
jk
jk
wfreq
wfreq
dwp
f
f
wzp
11
)(
)(
)|(,)|(  
where freq(wj) is the frequency of a word wj in the 
document d. Other notations are consistent with the 
previous definitions.  
Then we interpolate this adapted topic model 
with the generic LM, similar to Equation (2): 
)|()1(
)|()|(
kkadaptLDA
kkGeneralkk
hwp
hwphwp
??+
=
?
?
      (3) 
4 Experiments 
4.1 Experimental Setup 
 # of files # of words # of NEs
Training Data 23,985 7,345,644 590,656
Test Data 2,661 831,283 65,867 
Table 1. Statistics of our experimental data. 
 
The data set we used is the LDC Mandarin TDT4 
corpus, consisting of 337 broadcast news shows 
with transcriptions. These files were split into 
small pieces, which we call documents here, ac-
cording to the topic segmentation information 
marked in the LDC?s transcription. In total, there 
are 26,646 such documents in our data set. We 
randomly chose 2661 files as the test data (which 
is balanced for different news sources). The rest 
was used for topic analysis and also generic LM 
training. Punctuation marks were used to deter-
mine sentences in the transcriptions. We used the 
NYU NE tagger (Ji and Grishman, 2005) to recog-
nize four kinds of NEs: Person, Location, Organi-
675
zation, and Geo-political. Table 1 shows the statis-
tics of the data set in our experiments.  
We trained trigram LMs using the SRILM tool-
kit (Stolcke, 2002). A fixed weight (i.e., ?  in 
Equation (2) and (3)) was used for the entire test 
set when interpolating the generic LM with the 
adapted topic LM. Perplexity was used to measure 
the performance of different adapted LMs in our 
experiments.  
4.2 Latent Topic Analysis Results 
 
 Topic # of  Files 
Top 10 Descriptive Items  
(Translated from Chinese) 
1 3526 
U.S., Israel, Washington, Palestine, 
Bush, Clinton, Gore, Voice of Amer-
ica, Mid-East, Republican Party 
2 3067 
Taiwan, Taipei, Mainland, Taipei 
City, Chinese People?s Broadcasting 
Station, Shuibian Chen,  the Execu-
tive Yuan, the Legislative Yuan, De-
mocratic Progressive Party, 
Nationalist Party 
3 4857 
Singapore, Japan, Hong Kong, Indo-
nesia, Asia, Tokyo, Malaysia, Thai-
land, World, China 
4 4495 
World, German, Landon, Russia, 
France, England, Xinhua News 
Agency, Europe, U.S., Italy 
Cluster-
ing 
Based 
5 7586 
China, Beijing, Nation, China Central 
Television Station, Xinhua News 
Agency, Shanghai, World, State 
Council, Zemin Jiang, Beijing City
1 5859 
China, Japan, Hong Kong, Beijing, 
Shanghai, World, Zemin Jiang, Ma-
cao,  China Central Television Sta-
tion, Africa 
2 3794 
U.S., Bush, World,  Gore,  South 
Korea, North Korea, Clinton, George 
Walker Bush, Asia, Thailand 
3 4640 
Singapore, Indonesia, Team, Israel, 
Europe, Germany, England, France, 
Palestine, Wahid 
4 4623 
Taiwan, Russia, Mainland, India, 
Taipei, Shuibian Chen, Philippine, 
Estrada, Communist Party of China, 
RUS. 
LDA 
Based 
5 4729 
Xinhua News Agency, Nation, Bei-
jing, World, Canada, Sydney, Brazil, 
Beijing City, Education Ministry, 
Cuba 
Table 2.  Topic analysis results using clustering 
and LDA (the number of documents and the top 10 
words (NEs) in each cluster). 
 
For latent topic analysis, we investigated two ap-
proaches using named entities, i.e., clustering and 
LDA. 5 latent topics were used in both approaches. 
Table 2 illustrates the resulting topics using the top 
10 words in each topic. We can see that the words 
in the same cluster share some similarity and that 
the words in different clusters seem to be ?topi-
cally? different. Note that errors from automatic 
NE recognition may impact the clustering results. 
For example, ??/team? in the table (in topic 3 in 
LDA results) is an error and is less discriminative 
for topic analysis. 
Table 3 shows the perplexity of the test set us-
ing the background LM (baseline) and each of the 
topic LMs, from clustering and LDA respectively. 
We can see that for the entire test set, a topic LM 
generally performs much worse than the generic 
LM. This is expected, since the size of a topic clus-
ter is much smaller than that of the entire training 
set, and the test set may contain documents from 
different topics. However, we found that when us-
ing an optimal topic model (i.e., the topic LM that 
yields the lowest perplexity among the 5 topic 
LMs), 23.45% of the documents in the test set have 
a lower perplexity value than that obtained from 
the generic LM. This suggests that a topic model 
could benefit LM adaptation and motivates a dy-
namic topic adaptation approach for different test 
documents. 
 
 Perplexity 
Baseline 502.02 
CL-1 1054.36 
CL-2 1399.16 
CL-3 919.237 
CL-4 962.996 
CL-5 981.072 
LDA-1 1224.54 
LDA-2 1375.97 
LDA-3 1330.44 
LDA-4 1328.81 
LDA-5 1287.05 
Table 3. Perplexity results using the baseline LM 
vs. the single topic LMs. 
 
4.3 Clustering vs. LDA Based LM Adaptation 
In this section, we compare three LM adaptation 
paradigms. As we discussed in Section 3, two of 
them are clustering based topic analysis, but using 
different strategies to choose the optimal cluster; 
and the third one is based on LDA analysis that 
676
uses a dynamic weighting scheme for adapted 
topic mixture model.  
Figure 2 shows the perplexity results using dif-
ferent interpolation parameters with the general 
LM.  5 topics were used in both clustering and 
LDA based approaches (as in Section 4.2). ?CL-
CE? means clustering based topic analysis via 
cross entropy criterion, ?CL-Cos? represents clus-
tering based topic analysis via cosine distance cri-
terion, and ?LDA-MIX? denotes LDA based topic 
mixture model, which uses 5 mixture topic LMs. 
 
440
450
460
470
480
490
500
510
520
530
540
0.4 0.5 0.6 0.7 0.8
?
P
er
pl
ex
ity
Baseline CL-CE CL-Cos LDA-MIX
 
Figure 2. Perplexity using different LM adaptation 
approaches and different interpolation weights?  
with the general LM. 
 
We observe that all three adaptation approaches 
outperform the baseline when using a proper inter-
polation weight. ?CL-CE? yields the best perplex-
ity of 469.75 when ?  is 0.5, a reduction of 6.46% 
against the baseline perplexity of 502.02. For clus-
tering based adaptation, between the two strategies 
used to determine the topic for a test document, 
?CL-CE? outperforms ?CL-Cos?. This indicates 
that the cosine distance measure using only names 
is less effective than cross entropy for LM adapta-
tion. In addition, cosine similarity does not match 
perplexity as well as the CE-based distance meas-
ure. Similarly, for the LDA based approach, using 
only NEs may not be sufficient to find appropriate 
weights for the topic model. This also explains the 
bigger interpolation weight for the general LM in 
CL-Cos and LDA-MIX than that in ?CL-CE?.   
For a fair comparison between the clustering 
and LDA based LM adaptation approaches, we 
also evaluated using the topic mixture model for 
the clustering based approach and using only one 
topic in the LDA based method. For clustering 
based adaptation, we constructed topic mixture 
models using the weights obtained from a linear 
normalization of the two distance measures pre-
sented in Section 3.2. In order to use only one topic 
model in LDA based adaptation, we chose the 
topic cluster that has the largest weight in the 
adapted topic mixture model (as in Sec 3.3). Table 
4 shows the perplexity for the three approaches 
(CL-Cos, CL-CE, and LDA) using the mixture 
topic models versus a single topic LM. We observe 
similar trends as in Figure 2 when changing the 
interpolation weight ? with the generic LM; there-
fore, in Table 4 we only present results for one op-
timal interpolation weight. 
 
 Single-Topic Mixture-Topic
CL-Cos (? =0.7) 498.01 497.86 
CL-CE (? =0.5) 469.75 483.09 
LDA (? =0.7) 488.96 489.14 
Table 4. Perplexity results using the adapted topic 
model (single vs. mixture) for clustering and LDA 
based approaches. 
 
We can see from Table 4 that using the mixture 
model in clustering based adaptation does not im-
prove performance. This may be attributed to how 
the interpolation weights are calculated. For ex-
ample, only names are used in cosine distance, 
and the normalized distance may not be appropri-
ate weights. We also notice negligible difference 
when only using one topic in the LDA based 
framework. This might be because of the small 
number of topics currently used. Intuitively, using 
a mixture model should yield better performance, 
since LDA itself is based on the assumption of 
generating words from multiple topics. We will 
investigate the impact of the number of topics on 
LM adaptation in Section 4.5. 
4.4 Effect of Different Feature Configura-
tions on LM Adaptation 
We suspect that using only named entities may not 
provide enough information about the ?topics? of 
the documents, therefore we investigate expanding 
the feature vectors with other words. Since gener-
ally content words are more indicative of the topic 
of a document than function words, we used a POS 
tagger (Hillard et al, 2006) to select words for la-
tent topic analysis. We kept words with three POS 
classes: noun (NN, NR, NT), verb (VV), and modi-
677
fier (JJ), selected from the LDC POS set2. This is 
similar to the removal of stop words widely used in 
information retrieval.  
Figure 3 shows the perplexity results for three 
different feature configurations, namely, all-words 
(w), names (n), and names plus syntactically fil-
tered items (n+), for the CL-CE and LDA based 
approaches. The LDA based LM adaptation para-
digm supports our hypothesis. Using named infor-
mation instead of all the words seems to efficiently 
eliminate redundant information and achieve better 
performance. In addition, expanding named enti-
ties with syntactically filtered items yields further 
improvement. For CL-CE, using named informa-
tion achieves the best result among the three con-
figurations. This might be because that the 
clustering method is less powerful in analyzing the 
principal components as well as dealing with re-
dundant information than the LDA model. 
 
460
465
470
475
480
485
490
495
500
505
0.4 0.5 0.6 0.7 0.8
?
P
er
pl
ex
ity
CL-CE(w) CL-CE(n) CL-CE(n+)
LDA-MIX(w) LDA-MIX(n) LDA-MIX(n+)
 
Figure 3. Comparison of perplexity using different 
feature configurations. 
4.5 Impact of Predefined Topic Number on 
LM Adaptation 
LDA based topic analysis typically uses a large 
number of topics to capture the fine grained topic 
space. In this section, we evaluate the effect of the 
number of topics on LM adaptation. For compari-
son, we evaluate this for both LDA and CL-CE, 
similar to Section 4.3. We use the ?n+? feature 
configuration as in Section 4.4, that is, names plus 
POS filtered items. When using a single-topic 
adapted model in the LDA or CL-CE based ap-
proach, finer-grained topic analysis (i.e., increasing 
the number of topics) leads to worse performance 
mainly because of the smaller clusters for each 
topic; therefore, we only show results here using 
                                                          
2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf 
the mixture topic adapted models. Figure 4 shows 
the perplexity results using different numbers of 
topics. The interpolation weight? with the general 
LM is 0.5 in all the experiments. For the topic mix-
ture LMs, we used a maximum of 9 mixtures (a 
limitation in the current SRILM toolkit) when the 
number of topics is greater than 9.  
We observe that as the number of topics in-
creases, the perplexity reduces significantly for 
LDA. When the number of topics is 50, the 
adapted LM using LDA achieves a perplexity re-
duction of 11.35% compared to using 5 topics, and 
14.23% against the baseline generic LM. Therefore, 
using finer-grained multiple topics in dynamic ad-
aptation improves system performance. When the 
number of topics increases further, e.g., to 100, the 
performance degrades slightly. This might be due 
to the limitation of the number of the topic mix-
tures used. A similar trend is observable for the 
CL-CE approach, but the effect of the topic num-
ber is much greater in LDA than CL-CE.  
  
435.2
477.2
430.6
445.8
485.7
477.3471.8 467.2
483.1 485.1
400
420
440
460
480
500
n=5 n=10 n=20 n=50 n=100
# of Topics
P
er
pl
ex
it
y
LDA CL-CE
 
Figure 4. Perplexity results using different prede-
fined numbers of topics for LDA and CL-CE.  
4.6 Discussion 
As we know, although there is an increasing 
amount of training data available for LM training, 
it is still only for limited domains and styles. Creat-
ing new training data for different domains is time 
consuming and labor intensive, therefore it is very 
important to develop algorithms for LM adaptation. 
We investigate leveraging named entities in the 
LM adaptation task. Though some errors of NER 
may be introduced, our experimental results have 
shown that exploring named information for topic 
analysis is promising for LM adaptation.  
Furthermore, this framework may have other 
advantages. For speech recognition, using NEs for 
topic analysis can be less vulnerable to recognition 
678
errors. For instance, we may add a simple module 
to compute the similarity between two NEs based 
on the word tokens or phonetics, and thus compen-
sate the recognition errors inside NEs. Whereas, 
word-based models, such as the traditional cache 
LMs, may be more sensitive to recognition errors 
that are likely to have a negative impact on the 
prediction of the current word. From this point of 
view, our framework can potentially be more ro-
bust in the speech processing task. In addition, the 
number of NEs in a document is much smaller than 
that of the words, as shown in Table 1; hence, us-
ing NEs can also reduce the computational com-
plexity, in particular in topic analysis for training. 
5 Conclusion and Future Work 
We compared several unsupervised LM adaptation 
methods leveraging named entities, and proposed a 
new dynamic weighting scheme for topic mixture 
model based on LDA topic analysis. Experimental 
results have shown that the NE-driven LM adapta-
tion approach outperforms using all the words, and 
yields perplexity reduction compared to the base-
line generic LM. In addition, we find that for the 
LDA based method, adding other content words, 
combined with an increased number of topics, can 
further improve the performance, achieving up to 
14.23% perplexity reduction compared to the base-
line LM. 
The experiments in this paper combine models 
primarily through simple linear interpolation. Thus 
one direction of our future work is to develop algo-
rithms to automatically learn appropriate interpola-
tion weights. In addition, our work in this paper 
has only showed promising results in perplexity 
reduction. We will investigate using this frame-
work of LM adaptation for N-best or lattice rescor-
ing in speech recognition. 
Acknowledgements 
We thank Mari Ostendorf, Mei-Yuh Hwang, and 
Wen Wang for useful discussions, and Heng Ji for 
sharing the Mandarin named entity tagger. This 
work is supported by DARPA under Contract No. 
HR0011-06-C-0023. Any opinions expressed in 
this material are those of the authors and do not 
necessarily reflect the views of DARPA. 
 
 
References 
 
J. Bellegarda. 2000. Exploiting Latent Semantic Infor-
mation in Statistical Language Modeling. In IEEE 
Transactions on Speech and Audio Processing. 
88(80):1279-1296.   
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet 
Allocation. Journal of Machine Learning Research. 
3:993-1022. 
D. Gildea and T. Hofmann. 1999. Topic-Based Lan-
guage Models using EM. In Proc. of Eurospeech. 
T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. 
2004. Integrating Topics and Syntax. Adv. in Neural 
Information Processing Systems. 17:537-544. 
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tur, M. Harper, M. Ostendorf, and W. Wang. 2006. 
Impact of Automatic Comma Prediction on 
POS/Name Tagging of Speech. In Proc. of the First 
Workshop on Spoken Language Technology (SLT).  
P. Hsu and J. Glass. 2006. Style & Topic Language 
Model Adaptation using HMM-LDA. In Proc. of 
EMNLP, pp:373-381. 
R. Iyer and M. Ostendorf. 1996. Modeling Long Dis-
tance Dependence in Language: Topic Mixtures vs. 
Dynamic Cache Models. In Proc. of ICSLP. 
H. Ji and R. Grishman. 2005. Improving NameTagging 
by Reference Resolution and Relation Detection. In 
Proc. of ACL. pp: 411-418. 
R. Kneser and V. Steinbiss. 1993. On the Dynamic Ad-
aptation of Stochastic language models. In Proc. of 
ICASSP, Vol 2, pp: 586-589. 
R. Kuhn and R.D. Mori. 1990. A Cache-Based Natural 
Language Model for Speech Recognition. In IEEE 
Transactions on Pattern Analysis and Machine Intel-
ligence, 12: 570-583.  
D. Mrva and P.C. Woodland. 2006. Unsupervised Lan-
guage Model Adaptation for Mandarin Broadcast 
Conversation Transcription. In Proc. of 
INTERSPEECH, pp:2206-2209. 
R. Rosenfeld. 1996. A Maximum Entropy Approach to 
Adaptive Statistical Language Modeling. Computer, 
Speech and Language, 10:187-228. 
A. Stolcke. 2002. SRILM ? An Extensible Language 
Modeling Toolkit. In Proc. of ICSLP. 
H. Suzuki and J. Gao. 2005. A Comparative Study on 
Language Model Adaptation Techniques Using New 
Evaluation Metrics, In Proc. of HLT/EMNLP. 
Y.C. Tam and T. Schultz. 2005. Dynamic Language 
Model Adaptation Using Variational Bayes Inference. 
In Proc. of INTERSPEECH, pp:5-8. 
679
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 201?204,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Correlation between ROUGE and Human Evaluation of Extractive Meeting
Summaries
Feifan Liu, Yang Liu
The University of Texas at Dallas
Richardson, TX 75080, USA
ffliu,yangl@hlt.utdallas.edu
Abstract
Automatic summarization evaluation is critical to
the development of summarization systems. While
ROUGE has been shown to correlate well with hu-
man evaluation for content match in text summa-
rization, there are many characteristics in multiparty
meeting domain, which may pose potential prob-
lems to ROUGE. In this paper, we carefully exam-
ine how well the ROUGE scores correlate with hu-
man evaluation for extractive meeting summariza-
tion. Our experiments show that generally the cor-
relation is rather low, but a significantly better cor-
relation can be obtained by accounting for several
unique meeting characteristics, such as disfluencies
and speaker information, especially when evaluating
system-generated summaries.
1 Introduction
Meeting summarization has drawn an increasing atten-
tion recently; therefore a study on the automatic evalu-
ation metrics for this task is timely. Automatic evalua-
tion helps to advance system development and avoids the
labor-intensive and potentially inconsistent human eval-
uation. ROUGE (Lin, 2004) has been widely used for
summarization evaluation. In the news article domain,
ROUGE scores have been shown to be generally highly
correlated with human evaluation in content match (Lin,
2004). However, there are many differences between
written texts (e.g., news wire) and spoken documents, es-
pecially in the meeting domain, for example, the pres-
ence of disfluencies and multiple speakers, and the lack
of structure in spontaneous utterances. The question of
whether ROUGE is a good metric for meeting summa-
rization is unclear. (Murray et al, 2005) have reported
that ROUGE-1 (unigram match) scores have low correla-
tion with human evaluation in meetings.
In this paper we investigate the correlation between
ROUGE and human evaluation of extractive meeting
summaries and focus on two issues specific to the meet-
ing domain: disfluencies and multiple speakers. Both
human and system generated summaries are used. Our
analysis shows that by integrating meeting characteristics
into ROUGE settings, better correlation can be achieved
between the ROUGE scores and human evaluation based
on Spearman?s rho in the meeting domain.
2 Related work
Automatic summarization evaluation can be broadly clas-
sified into two categories (Jones and Galliers, 1996): in-
trinsic and extrinsic evaluation. Intrinsic evaluation, such
as relative utility based metric proposed in (Radev et al,
2004), assesses a summarization system in itself (for ex-
ample, informativeness, redundancy, and coherence). Ex-
trinsic evaluation (Mani et al, 1998) tests the effective-
ness of a summarization system on other tasks. In this
study, we concentrate on the automatic intrinsic summa-
rization evaluation. It has been extensively studied in
text summarization. Different approaches have been pro-
posed to measure matches using words or more mean-
ingful semantic units, for example, ROUGE (Lin, 2004),
factoid analysis (Teufel and Halteren, 2004), pyramid
method (Nenkova and Passonneau, 2004), and Basic El-
ement (BE) (Hovy et al, 2006).
With the increasing recent research of summarization
moving into speech, especially meeting recordings, is-
sues related to spoken language are yet to be explored
for their impact on the evaluation metrics. Inspired by
automatic speech recognition (ASR) evaluation, (Hori et
al., 2003) proposed the summarization accuracy metric
(SumACCY) based on a word network created by merg-
ing manual summaries. However (Zhu and Penn, 2005)
found a statistically significant difference between the
ASR-inspired metrics and those taken from text summa-
rization (e.g., RU, ROUGE) on a subset of the Switch-
board data. ROUGE has been used in meeting summa-
rization evaluation (Murray et al, 2005; Galley, 2006),
yet the question remained whether ROUGE is a good
metric for the meeting domain. (Murray et al, 2005)
showed low correlation of ROUGE and human evalua-
tion in meeting summarization evaluation; however, they
201
simply used ROUGE as is and did not take into account
the meeting characteristics during evaluation.
In this paper, we ask the question of whether ROUGE
correlates with human evaluation of extractive meeting
summaries and whether we can modify ROUGE to ac-
count for the meeting style for a better correlation with
human evaluation.
3 Experimental Setup
3.1 Data
We used the ICSI meeting data (Janin et al, 2003) that
contains naturally-occurring research meetings. All the
meetings have been transcribed and annotated with dialog
acts (DA) (Shriberg et al, 2004), topics, and extractive
summaries (Murray et al, 2005).
For this study, we used the same 6 test meetings as in
(Murray et al, 2005; Galley, 2006). Each meeting al-
ready has 3 human summaries from 3 common annota-
tors. We recruited another 3 human subjects to generate
3 more human summaries, in order to create more data
points for a reliable analysis. The Kappa statistics for
those 6 different annotators varies from 0.11 to 0.35 for
different meetings. The human summaries have different
length, containing around 6.5% of the selected DAs and
13.5% of the words respectively. We used four different
system summaries for each of the 6 meetings: one based
on the MMR method in MEAD (Carbonell and Gold-
stein, 1998; et al, 2003), the other three are the system
output from (Galley, 2006; Murray et al, 2005; Xie and
Liu, 2008). All the system generated summaries contain
around 5% of the DAs and 16% of the words of the entire
meeting. Thus, in total we have 36 human summaries and
24 system summaries on the 6 test meetings, on which
the correlation between ROUGE and human evaluation
is calculated and investigated.
All the experiments in this paper are based on human
transcriptions, with a central interest on whether some
characteristics of the meeting recordings affect the corre-
lation between ROUGE and human evaluations, without
the effect from speech recognition or automatic sentence
segmentation errors.
3.2 Automatic ROUGE Evaluation
ROUGE (Lin, 2004) measures the n-grammatch between
system generated summaries and human summaries. In
most of this study, we used the same options in ROUGE
as in the DUC summarization evaluation (NIST, 2007),
and modify the input to ROUGE to account for the fol-
lowing two phenomena.
? Disfluencies
Meetings contain spontaneous speech with many
disfluencies, such as filled pauses (uh, um), dis-
course markers (e.g., I mean, you know), repetitions,
corrections, and incomplete sentences. There have
been efforts on the study of the impact of disfluen-
cies on summarization techniques (Liu et al, 2007;
Zhu and Penn, 2006) and human readability (Jones
et al, 2003). However, it is not clear whether dis-
fluencies impact automatic evaluation of extractive
meeting summarization.
Since we use extractive summarization, summary
sentences may contain difluencies. We hand anno-
tated the transcripts for the 6 meetings and marked
the disfluencies such that we can remove them to
obtain cleaned up sentences for those selected sum-
mary sentences. To study the impact of disfluencies,
we run ROUGE using two different inputs: sum-
maries based on the original transcription, and the
summaries with disfluencies removed.
? Speaker information
The existence of multiple speakers in meetings
raises questions about the evaluation method. (Gal-
ley, 2006) considered some location constrains in
meeting summarization evaluation, which utilizes
speaker information to some extent. In this study
we use the data in separate channels for each speaker
and thus have the speaker information available for
each sentence. We associate the speaker ID with
each word, treat them together as a new ?word? in
the input to ROUGE.
3.3 Human Evaluation
Five human subjects (all undergraduate students in Com-
puter Science) participated in human evaluation. In to-
tal, there are 20 different summaries for each of the 6
test meetings: 6 human-generated, 4 system-generated,
and their corresponding ones with disfluencies removed.
We assigned 4 summaries with different configurations to
each human subject: human vs. system generated sum-
maries, with or without disfluencies. Each human evalu-
ated 24 summaries in total, for the 6 test meetings.
For each summary, the human subjects were asked to
rate the following statements using a scale of 1-5 accord-
ing to the extent of their agreement with them.
? S1: The summary reflects the discussion flow in the meet-
ing very well.
? S2: Almost all the important topic points of the meeting
are represented.
? S3: Most of the sentences in the summary are relevant to
the original meeting.
? S4: The information in the summary is not redundant.
? S5: The relationship between the importance of each topic
in the meeting and the amount of summary space given to
that topic seems appropriate.
? S6: The relationship between the role of each speaker and
the amount of summary speech selected for that speaker
seems appropriate.
? S7: Some sentences in the summary convey the same
meaning.
? S8: Some sentences are not necessary (e.g., in terms of
importance) to be included in the summary.
? S9: The summary is helpful to someone who wants to
know what are discussed in the meeting.
202
These statements are an extension of those used in
(Murray et al, 2005) for human evaluation of meeting
summaries. The additional ones we added were designed
to account for the discussion flow in the meetings. Some
of the statements above are used to measure similar as-
pects, but from different perspectives, such as S5 and S6,
S4 and S7. This may reduce some accidental noise in hu-
man evaluation. We grouped these statements into 4 cat-
egories: Informative Structure (IS): S1, S5 and S6; Infor-
mative Coverage (IC): S2 and S9; Informative Relevance
(IRV): S3 and S8; and Informative Redundancy (IRD):
S4 and S7.
4 Results
4.1 Correlation between Human Evaluation and
Original ROUGE Score
Similar to (Murray et al, 2005), we also use Spearman?s
rank coefficient (rho) to investigate the correlation be-
tween ROUGE and human evaluation. We have 36 hu-
man summaries and 24 system summaries for the 6 meet-
ings in our study. For each of the human summaries,
the ROUGE scores are generated using the other 5 hu-
man summaries as references. For system generated sum-
maries, we calculate the ROUGE score using 5 human
references, and then obtain the average from 6 such se-
tups. The correlation results are presented in Table 1.
In addition to the overall average for human evaluation
(H AVG), we calculated the average score for each evalu-
ation category (see Section 3.3). For ROUGE evaluation,
we chose the F-measure for R-1 (unigram) and R-SU4
(skip-bigram with maximum gap length of 4), which is
based on our observation that other scores in ROUGE are
always highly correlated (rho>0.9) to either of them for
this task. We compute the correlation separately for the
human and system summaries in order to avoid the im-
pact due to the inherent difference between the two dif-
ferent summaries.
Correlation on Human Summaries
H AVG H IS H IC H IRV H IRD
R-1 0.09 0.22 0.21 0.03 -0.20
R-SU4 0.18 0.33 0.38 0.04 -0.30
Correlation on System Summaries
R-1 -0.07 -0.02 -0.17 -0.27 -0.02
R-SU4 0.08 0.05 0.01 -0.15 0.14
Table 1: Spearman?s rho between human evaluation (H) and
ROUGE (R) with basic setting.
We can see that R-SU4 obtains a higher correlation
with human evaluation than R-1 on the whole, but still
very low, which is consistent with the previous conclu-
sion from (Murray et al, 2005). Among the four cat-
egories, better correlation is achieved for information
structure (IS) and information coverage (IC) compared
to the other two categories. This is consistent with what
ROUGE is designed for, ?recall oriented understudy gist-
ing evaluation? ? we expect it to model IS and IC well
by ngram and skip-bigram matching but not relevancy
(IRV) and redundancy (IRD) effectively. In addition, we
found low correlation on system generated summaries,
suggesting it is more challenging to evaluate those sum-
maries both by humans and the automatic metrics.
4.2 Impacts of Disfluencies on Correlation
Table 2 shows the correlation results between ROUGE
(R-SU4) and human evaluation on the original and
cleaned up summaries respectively. For human sum-
maries, after removing disfluencies, the correlation be-
tween ROUGE and human evaluation improves on the
whole, but degrades on information structure (IS) and in-
formation coverage (IC) categories. However, for sys-
tem summaries, there is a significant gain of correlation
on those two evaluation categories, even though no im-
provement on the overall average score. Our hypothesis
for this is that removing disfluencies helps remove the
noise in the system generated summaries and make them
more easily to be evaluated by human and machines. In
contrast, the human created summaries have better qual-
ity in terms of the information content and may not suffer
as much from the disfluencies contained in the summary.
Correlation on Human Summaries
H AVG H IS H IC H IRV H IRD
Original 0.18 0.33 0.38 0.04 -0.30
Disfluencies 0.21 0.21 0.31 0.19 -0.16
removed
Correlation on System Summaries
Original 0.08 0.05 0.01 -0.15 0.14)
Disfluencies 0.08 0.22 0.19 -0.02 -0.07
removed
Table 2: Effect of disfluencies on the correlation between R-
SU4 and human evaluation.
4.3 Incorporating Speaker Information
We further incorporated speaker information in ROUGE
setting using the summaries with disfluencies removed.
Table 3 presents the resulting correlation values between
ROUGE SU4 score and human evaluation. For human
summaries, adding speaker information slightly degraded
the correlation, but it is still better compared to using
the original transcripts (results in Table 1). For the sys-
tem summaries, the overall correlation is significantly im-
proved, with some significant improvement in the infor-
mation redundancy (IRD) category. This suggests that
by leveraging speaker information, ROUGE can assign
better credits or penalties to system generated summaries
(same words from different speakers will not be counted
as a match), and thus yield better correlation with human
evaluation; whereas for human summaries, this may not
happen often. For similar sentences from different speak-
ers, human annotators are more likely to agree with each
203
other in their selection compared to automatic summa-
rization.
Correlation on Human Summaries
Speaker Info. H AVG H IS H IC H IRV H IRD
NO 0.21 0.21 0.31 0.19 -0.16
YES 0.20 0.20 0.27 0.12 -0.09
Correlation on System Summaries
NO 0.08 0.22 0.19 -0.02 -0.07
YES 0.14 0.20 0.16 0.02 0.21
Table 3: Effect of speaker information on the correlation be-
tween R-SU4 and human evaluation.
5 Conclusion and Future Work
In this paper, we have made a first attempt to system-
atically investigate the correlation of automatic ROUGE
scores with human evaluation for meeting summariza-
tion. Adaptations on ROUGE setting based on meeting
characteristics are proposed and evaluated using Spear-
man?s rank coefficient. Our experimental results show
that in general the correlation between ROUGE scores
and human evaluation is low, with ROUGE SU4 score
showing better correlation than ROUGE-1 score. There
is significant improvement in correlation when disfluen-
cies are removed and speaker information is leveraged,
especially for evaluating system-generated summaries. In
addition, we observe that the correlation is affected differ-
ently by those factors for human summaries and system-
generated summaries.
In our future work we will examine the correlation be-
tween each statement and ROUGE scores to better rep-
resent human evaluation results instead of using simply
the average over all the statements. Further studies are
also needed using a larger data set. Finally, we plan to in-
vestigate meeting summarization evaluation using speech
recognition output.
Acknowledgments
The authors thank University of Edinburgh for providing the an-
notated ICSI meeting corpus and Michel Galley for sharing his
tool to process the annotated data. We also thank Gabriel Mur-
ray and Michel Galley for letting us use their automatic summa-
rization system output for this study. This work is supported by
NSF grant IIS-0714132. Any opinions expressed in this work
are those of the authors and do not necessarily reflect the views
of NSF.
References
J. Carbonell and J. Goldstein. 1998. The use of mmr, diversity-
based reranking for reordering documents and producing
summaries. In SIGIR, pages 335?336.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In EMNLP,
pages 364?372.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for
automatic speech summarization. In EUROSPEECH, pages
2825?2828.
E. Hovy, C. Lin, L. Zhou, and J. Fukumoto. 2006. Automated
summarization evaluation with basic elements. In LREC.
A. Janin, D. Baron, J. Edwards, D. Ellis, G. Gelbart, N. Norgan,
B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. The icsi meeting corpus. In ICASSP.
K. S. Jones and J. Galliers. 1996. Evaluating natural language
processing systems: An analysis and review. Lecture Notes
in Artificial Intelligence.
D. Jones, F. Wlof, E. Gilbson, E. Williams, E. Fedorenko,
D. Reynolds, and M. Zissman. 2003. Measuring the
readability of automatic speech-to-text transcripts. In EU-
ROSPEECH, pages 1585?1588.
C. Lin. 2004. Rouge: A package for automatic evaluation of
summaries. In Workshop on Text Summarization Branches
Out at ACL, pages 74?81.
Y. Liu, F. Liu, B. Li, and S. Xie. 2007. Do disfluencies af-
fect meeting summarization? a pilot study on the impact of
disfluencies. In MLMI Workshop, Poster Session.
I. Mani, T. Firmin, D. House, M. Chrzanowski, G. Klein,
L. Hirschman, B. Sundheim, and L. Obrst. 1998. The tipster
summac text summarization evaluation: Final report. Tech-
nical report, The MITRE Corporation.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Eval-
uating automatic summaries of meeting recordings. In ACL
2005 MTSE Workshop, pages 33?40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. In
HLT/NAACL.
NIST. 2007. Document understanding conference (DUC).
http://duc.nist.gov/.
D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. C?elebi,
E. Drabek, W. Lam, D. Liu, H. Qi, H. Saggion, S. Teufel,
M. Topper, and A. Winkel. 2003. The MEAD Multidocu-
ment Summarizer. http://www.summarization.com/mead/.
D. R. Radev, H. Jing, M. Stys, and T. Daniel. 2004. Centroid-
based summarization of multiple documents. Information
Processing and Management, 40:919?938.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The icsi meeting recorder dialog act (mrda) corpus. In SIG-
DAL Workshop, pages 97?100.
S. Teufel and H. Halteren. 2004. Evaluating information con-
tent by factoid analysis: Human annotation and stability. In
EMNLP.
S. Xie and Y. Liu. 2008. Using corpus and knowledge-based
similarity measure in maximummarginal relevance for meet-
ing summarization. In ICASSP.
X. Zhu and G. Penn. 2005. Evaluation of sentence selection for
speech summarization. In ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summariza-
tion.
X. Zhu and G. Penn. 2006. Comparing the roles of tex-
tual, acoustic and spoken-language features on spontaneous-
conversation summarization. In HLT/NAACL.
204
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 309?312,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Blog Polarity Classification via Topic Analysis and Adaptive
Methods
Feifan Liu
University of Wisconsin, Milwaukee
liuf@uwm.edu
Dong Wang, Bin Li, Yang Liu
The University of Texas at Dallas
dongwang,yangl@hlt.utdallas.edu
Abstract
In this paper we examine different linguistic features
for sentimental polarity classification, and perform a
comparative study on this task between blog and re-
view data. We found that results on blog are much
worse than reviews and investigated two methods
to improve the performance on blogs. First we ex-
plored information retrieval based topic analysis to
extract relevant sentences to the given topics for po-
larity classification. Second, we adopted an adaptive
method where we train classifiers from review data
and incorporate their hypothesis as features. Both
methods yielded performance gain for polarity clas-
sification on blog data.
1 Introduction
Sentimental analysis is a task of text categorization that
focuses on recognizing and classifying opinionated text
towards a given subject. Different levels of sentimental
analysis has been performed in prior work, from binary
classes to more fine grained categories. Pang et al (2002)
defined this task as a binary classification task and ap-
plied it to movie reviews. More sentiment classes, such
as document objectivity and subjectivity as well as dif-
ferent rating scales on the subjectivity, have also been
taken into consideration (Pang and Lee, 2005; Boiy et
al., 2007). In terms of granularity, this task has been
investigated from building word level sentiment lexicon
(Turney, 2002; Moilanen and Pulman, 2008) to detecting
phrase-level (Wilson et al, 2005; Agarwal et al, 2009)
and sentence-level (Riloff and Wiebe, 2003; Hu and Liu,
2004) sentiment orientation. However, most previous
work has mainly focused on reviews (Pang et al, 2002;
Hu and Liu, 2004), news resources (Wilson et al, 2005),
and multi-domain adaptation (Blitzer et al, 2007; Man-
sour et al, 2008). Sentiment analysis on blogs (Chesley
et al, 2005; Kim et al, 2009) is still at its early stage.
In this paper we investigate binary polarity classifica-
tion (positive vs. negative). We evaluate the genre effect
between blogs and review data and show the difference of
feature effectiveness. We demonstrate improved polarity
classification performance in blogs using two methods:
(a) integrating topic relevance analysis to perform topic
specific polarity classification; (b) adopting an adaptive
method by incorporating multiple classifiers? hypotheses
from different review domains as features. Our manual
analysis also points out some challenges and directions
for further study in blog domain.
2 Features for Polarity Classification
For the binary polarity classification task, we use a super-
vised learning framework to determine whether a docu-
ment is positive or negative. We used a subjective lex-
icon, containing 2304 positive words and 4145 negative
words respectively, based on (Wilson et al, 2005). The
features we explored are listed below.
(i) Lexical features (LF)
We use the bag of words for the lexical features as they
have been shown very useful in previous work.
(ii) Polarized lexical features (PL)
We tagged each sentiment word in our data set with its
polarity tag based on the sentiment lexicon (?POS? for
positive, and ?NEG? for negative), along with its part-
of-speech tag. For example, in the sentence ?It is good,
and I like it?, ?good? is tagged as ?POS/ADJ?, ?like? is
tagged as ?POS/VRB?. Then we encode the number of
the polarized tags in a document as features.
(iii) Polarized bigram features (PB)
Contextual information around the polarized words
can be useful for sentimental analysis. A word may
flip the polarity of its neighboring sentiment words even
though this word itself is not necessarily a negative word.
For example, in ?Given her sudden celebrity with those
on the left...? (a sentence in a political blog), ?sudden?
preceding ?celebrity? implies the author?s negative atti-
tude towards ?her?. We combine the sentiment word?s
polarized tag and its following and preceding word or
its part-of-speech to comprise different bigram features
to represent this kind of contextual information. For ex-
309
ample, in ?I recommend this.?, ?recommend? is a posi-
tive verb, denoted as ?POS/VRB?, and the bigram fea-
tures including this tag and its previous word ?I? are
?I POS/VRB? and ?pron POS/VRB?.
(iv) Transition word features (T)
Transition words, such as ?although?, ?even though?,
serve as function words that may change the literal opin-
ion polarity in the current sentence. This information has
not been widely explored for sentiment analysis. In this
study, we compiled a transition word list containing 31
words. We use the co-occurring feature between a transi-
tion word and its nearby content words (noun, verb, ad-
jective and adverb) or polarized tags of sentiment words
within the same sentence, but not spanning over other
transition words. For example, in ?Although it is good?,
we use features like ?although is?,?although good? and
?although POS/ADJ?, where ?POS/ADJ? is the PL fea-
ture for word ?good?.
3 Feature Effectiveness on Blogs and
Reviews
The blog data we used is from the TREC Blog Track eval-
uation in 2006 and 2007. The annotation was conducted
for the 100 topics used in the evaluation (blogs are rele-
vant to a given topic and also opinionated). We use 6,896
positive and 5,300 negative blogs. For the review data,
we combined multiple review data sets from (Pang et al,
2002; Blitzer et al, 2007) together. It contains reviews
from movies and four product domains (kitchen, elec-
tronics, books, and dvd), each of which has 1000 neg-
ative and 1000 positive samples. For the data without
sentence information (e.g., blog data, some review data),
we generated sentences using the maximum entropy sen-
tence boundary detection tool1. We used TnT tagger to
obtain the part-of-speech tags for these data sets.
For classification, we use the maximum entropy clas-
sifier2 with a Gaussian prior of 1 and 100 iterations in
model training. For all the experiments below, we use
a 10-fold cross validation setup and report the average
classification accuracy. Table 1 shows classification re-
sults using various feature sets on blogs and review data.
We keep the lexical feature (LF) as a base feature, and
investigate the effectiveness of adding more different fea-
tures. We used Wilcox signed test for statistical signifi-
cance test. Symbols ??? and ??? in the table indicate the
significant level of 0.05 and 0.1 respectively, compared to
the baseline performance using LF feature setup.
For the review domain, most of the feature sets can sig-
nificantly improve the classification performance over the
baseline of using ?LF? features. ?PB? features yielded
more significant improvement than ?PL? or ?T? feature
categories. Combining ?PL? and ?T? features resulted in
some slight further improvement, achieving the best ac-
1http://stp.ling.uu.se/?gustav/java/classes/MXTERMINATOR.html
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
Feature Set Blogs Reviews
LF 72.07 81.67
LF+PL 70.93 81.93
LF+PB 72.44 83.62?
LF+T 72.17 81.76
LF+PL+PB 70.81 83.61?
LF+PL+T 72.74 82.13?
LF+PB+T 72.29 83.73?
LF+PL+PB+T 71.85 83.94?
Table 1: Polarity classification results (accuracy in %) using
different features for blogs and reviews.
curacy of 83.94%. We notice that incorporating our pro-
posed transition feature (T) always achieves some gain on
different feature settings, suggesting that those transition
features are useful for sentimental analysis on reviews.
From Table 1, we can see that overall the performance
on blogs is worse than on the review data. We hypoth-
esize this may be due to the large variation in terms of
contents and styles in blogs. Regarding the feature ef-
fectiveness, we also observe some differences between
blogs and reviews. Adding the polarized bigram feature
and transition feature (PB and T) individually can yield
some improvement; however, adding both of them did
not result in any further improvement ? performance de-
grades compared to LF+PB. Interestingly, although ?PL?
feature alone does not seem to help, by adding ?PL? and
?T? together, the performance achieved the best accuracy
of 72.74%. We also found that adding all the features
together hurts the performance, suggesting that different
features interact with each other and some do not com-
bine well (e.g., PB and T features). In addition, all the
improvements here are not statistically significant.
Note that for the blog data, we randomly split them for
the cross validation experiments regardless of the queries.
In order to better understand whether the poor results on
blog data is due to the effect of different queries, we per-
formed another experiment where for each query, we ran-
domly divided the corresponding blogs into training and
test splits. Only 66 queries were kept for this experi-
ments ? we did not include those queries that have fewer
than 10 relevant blogs. The results for the query balanced
split on blogs are shown in Figure 1. We also include re-
sults for the five individual review data sets in order to see
the topic effect. We present results using four represen-
tative feature sets chosen according to Table 1. For the
review data, we notice some difference across different
data sets, suggesting their inherent difference in terms of
task difficulty. We observe slight performance increase
for some feature sets using the query balanced setup for
blog data, but overall it is still much worse than the review
data. This shows that the query unbalanced training/test
split does not explain the performance gap between blogs
and reviews. This is consistent with (Zhang et al, 2007)
that found that a query-independent classifier performs
even better than query-dependent one. We expect that the
310
query unbalanced setup is more realistic, therefore, in the
following experiments, we continue with this setup.
7173
7577
7981
8385
87
Accur
acy(%
)
blog_data
blog_data_query_balancedreview_books
review_dvd
review_kitchen
review_movie
review_elec
Figure 1: Polarity classification results on query balanced blog
data and five individual review data sets.
4 Improving Blog Polarity Classification
To improve the performance of polarity classification on
blogs, we propose two methods: (a) extract only topic-
relevant segments from blogs for sentiment analysis; (b)
apply adaptive methods to leverage review data.
4.1 Using topic-relevant blog context
Generally a review is written towards one product or one
kind of service, but a blog may cover several topics with
possibly different opinions towards each topic. The blog
data we used is annotated based on some specific topics
in the TREC Blog Track evaluation. Take topic 870 in
the data as an example, ?Find opinions on alleged use
of steroids by baseball player Barry?. There is one blog
that talks about 5 different baseball players in issues of
using steroids. Since the reference opinion tag of a blog
is determined by polarity towards the given query topic, it
might be confusing for the classifier if we use the whole
blog to derive features. Recently topic analysis has been
used for polarity classification (Zhang et al, 2007; Titov
and McDonald, 2008; Wiegand and Klakow, 2009). We
take a different approach in this study.
In order to obtain a topic-relevant context, we retrieved
the top 10 relevant sentences corresponding to the given
topic using the Lemur toolkit3. Then we used these sen-
tences and their immediate previous and following sen-
tences for feature extraction in the same way as what
we did on the whole blog. In addition to using all the
words in the relevant context, we also investigated using
only content words since those are more topic indicative
than function words. We extracted content words (nouns,
verbs, adjectives and adverbs) from each blog in their
original order and apply the same feature extraction pro-
cess as for using all the words.
3http://www.lemurproject.org/lemur/
Table 2 shows the blog polarity classification results
using the whole blog vs. relevant context composed of
all the words or only content words. For the significance
test, the comparison was done for using relevant context
with all the words vs. using the whole blog; and us-
ing content words only vs. using all the words in rele-
vant context. Each comparison was with respect to the
same feature setup. We observe improved polarity classi-
fication performance when using sentence retrieval based
topic analysis to extract relevant context. Using all the
words in the topic relevant context, all the improvements
compared to using the original entire blog are statistically
significant at the level of 0.01. We also notice that un-
like on the entire blog document, the ?PL? features con-
tribute positively when combined with ?LF?. All the fea-
ture settings with ?PL? perform very well. The best ac-
curacy of 75.32% is achieved using feature combination
of ?LF+PL? or ?LF+PL+T?. This suggests that polarized
lexical features suffered from the off-topic content when
using the entire blog and are more useful within contexts
of certain topics.
When using content words only, we observe consistent
gain across all the feature sets. Three feature settings,
?LF+PB?,?LF+T? and ?LF+PL+PB+T?, achieve statisti-
cally significant further improvement (compared to using
all the words of relevant contexts). The best accuracy
(75.6%) is achieved by using the ?LF+PB? features.
Feature Set Whole Relevant Context
Blog All Words Content Words
LF 72.07 74.92? 75.14
LF+PL 70.93 75.32? 75.34
LF+PB 72.44 75.03? 75.6?
LF+T 72.17 75.01? 75.35?
LF+PL+PB 70.81 75.27? 75.35
LF+PL+T 72.74 75.32? 75.41
LF+PB+T 72.29 75.17? 75.42
LF+PL+PB+T 71.85 75.21? 75.45?
Table 2: Blog polarity classification results (accuracy in %) us-
ing topic relevant context composed of all the words or only
content words.
4.2 Adaptive methods using review data
Domain adaptation has been studied in some previous
work (e.g., (Blitzer et al, 2007; Mansour et al, 2008)).
In this paper, we evaluate two adaptive approaches in or-
der to leverage review data to improve blog polarity clas-
sification. In the first approach, in each of the 10-fold
cross-validation training, we pool the blog training data
(90% of the entire blog data) together with all the review
data from 5 different domains. In the second method, we
augment features with hypotheses obtained from classi-
fiers trained using other domain data. Specifically, we
first trained 5 classifiers from 5 review domain data sets
respectively, and encoded the hypotheses from different
classifiers as features for blog training (together with the
original features of the blog data). Results of these two
approaches are shown in Table 3. We use the topic rele-
311
vant context with content words only in this experiment,
and present results for different feature combinations (ex-
cept the baseline ?LF? setting). The significance test is
conducted in comparison to the results using only blog
data for training, for the same feature setting.
We find that the first approach does not yield any gain,
even though the added data is about the same size as
the blog data. It indicates that due to the large differ-
ence between the two genres, simply combining blogs
and reviews in training is not effective. However, we
can see that using augmented features in training signifi-
cantly improved the performance across different feature
sets. The best result is achieved using ?LF+T? features,
76.84% compared with the best accuracy of 75.6% when
using the blog data only (?LF+PB? features).
Feature Set Only Blog Pool Data Augment Features
LF+PL 75.34 75.05 76.12?
LF+PB 75.6 74.35 76.28?
LF+T 75.35 74.47 76.84?
LF+PL+PB 75.35 74.94 76.7?
LF+PL+T 75.41 74.85 76.32?
LF+PB+T 75.42 74.46 76.3?
LF+PL+PB+T 75.45 74.96 76.53?
Table 3: Results (accuracy in %) of blog polarity classification
using two methods leveraging review data.
4.3 Error analysis
Notice that after achieving some improvements the per-
formance on blogs is still much worse than on review
data. Thus we performed a manual error analysis for a
better understanding of the difficulties of sentiment anal-
ysis on blog data, and identified the following challenges.
(a) Idiomatic expressions. Compared to reviews, blog-
gers seem to use more idioms. For example, ?Of course
he has me over the barrel...? expresses negative opinion,
however, there are no superficially indicative features.
(b) Ironic writing style. Some bloggers prefer ironic
style especially when speaking against something or
somebody, whereas opinions are often expressed using
plain writing style in reviews. Simply using the surface
word level features is not able to model these properly.
(c) Background knowledge. In some political blogs,
the polarized expressions are implicit. Correctly recog-
nizing them requires background knowledge and deeper
language analysis techniques.
5 Conclusions and Future Work
In this paper, we have evaluated various features and the
domain effect on sentimental polarity classification. Our
experiments on blog and review data demonstrated dif-
ferent feature effectiveness and the overall poorer perfor-
mance on blogs than reviews. We found that the polarized
features and the transition word features we introduced
are useful for polarity classification. We also show that
by extracting topic-relevant context and considering only
content words, the system can achieve significantly better
performance on blogs. Furthermore, an adaptive method
using augmented features can effectively leverage data
from other domains, and yield improvement compared
to using in-domain training or training on combined data
from different domains. For our future work, we plan
to investigate other adaption methods, and try to address
some of the problems identified in our error analysis.
6 Acknowledgment
The authors thank the three anonymous reviewers for
their suggestions.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen McKeown. 2009.
Contextual phrase-level polarity analysis using lexical affect
scoring and syntactic n-grams. In Proc. of EACL.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proc. of ACL.
Erik Boiy, Pieter Hens, Koen Deschacht, and Marie-Francine
Moens. 2007. Automatic sentiment analysis in on-line text.
In Proc. of ELPUB.
Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Srihari.
2005. Using verbs and adjectives to automatically classify
blog sentiment. In Proc. of AAAI.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proc. of ACM SIGKDD.
Jungi Kim, Jin-Ji Li, and Jong-Hyeok Lee. 2009. Discovering
the discriminative views: Measuring term weights for senti-
ment analysis. In Proc. of ACL-IJCNLP.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
2008. Domain adaptation with multiple sources. In Proc.
of NIPS.
Karo Moilanen and Stephen Pulman. 2008. The good, the bad,
and the unknown: Morphosyllabic sentiment tagging of un-
seen words. In Proc. of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to rat-
ing scales. In Proc. of ACL.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proc. of EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proc. of WWW.
Peter D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Proc. of ACL.
Michael Wiegand and Dietrich Klakow. 2009. Topic-Related
polarity classification of blog sentences. In Proc. of the 14th
Portuguese Conference on Artificial Intelligence: Progress
in Artificial Intelligence, pages 658?669.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proc. of HLT-EMNLP.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opinion re-
trieval from blogs. In Proc. of CIKM, pages 831?840.
312
