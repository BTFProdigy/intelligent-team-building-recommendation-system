Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860?865,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
___________________  
*Corresponding author 
Cross-lingual Opinion Analysis via Negative Transfer Detection  
Lin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang1 
1Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School, 
Harbin Institute of Technology, Shenzhen 518055 
2Department Of Computing, the Hong Kong Polytechnic University 
guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn, 
csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn 
 
Abstract 
Transfer learning has been used in opin-
ion analysis to make use of available lan-
guage resources for other resource scarce 
languages. However, the cumulative 
class noise in transfer learning adversely 
affects performance when more training 
data is used. In this paper, we propose a 
novel method in transductive transfer 
learning to identify noises through the 
detection of negative transfers. Evalua-
tion on NLP&CC 2013 cross-lingual 
opinion analysis dataset shows that our 
approach outperforms the state-of-the-art 
systems. More significantly, our system 
shows a monotonic increase trend in per-
formance improvement when more train-
ing data are used.  
1 Introduction 
Mining opinions from text by identifying their 
positive and negative polarities is an important 
task and supervised learning methods have been 
quite successful. However, supervised methods 
require labeled samples for modeling and the 
lack of sufficient training data is the performance 
bottle-neck in opinion analysis especially for re-
source scarce languages. To solve this problem, 
the transfer leaning method (Arnold et al, 2007) 
have been used to make use of samples from a 
resource rich source language to a resource 
scarce target language, also known as cross lan-
guage opinion analysis (CLOA). 
In transductive transfer learning (TTL) where 
the source language has labeled data and the tar-
get language has only unlabeled data, an algo-
rithm needs to select samples from the unlabeled 
target language as the training data and assign 
them with class labels using some estimated con-
fidence. These labeled samples in the target lan-
guage, referred to as the transferred samples, also 
have a probability of being misclassified. During 
training iterations, the misclassification introduc-
es class noise which accumulates, resulting in a 
so called negative transfer that affects the classi-
fication performance.  
In this paper, we propose a novel method 
aimed at reducing class noise for TTL in CLOA. 
The basic idea is to utilize transferred samples 
with high quality to identify those negative trans-
fers and remove them as class noise to reduce 
noise accumulation in future training iterations. 
Evaluations on NLP&CC 2013 CLOA evalua-
tion data set show that our algorithm achieves the 
best result, outperforming the current state-of-
the-art systems. More significantly, our system 
shows a monotonic increasing trend in perfor-
mance when more training data are used beating 
the performance degradation curse of most trans-
fer learning methods when training data reaches 
certain size. 
The rest of the paper is organized as follows. 
Section 2 introduces related works in transfer 
learning, cross lingual opinion analysis, and class 
noise detection technology. Section 3 presents 
our algorithm. Section 4 gives performance eval-
uation. Section 5 concludes this paper. 
2 Related works 
TTL has been widely used before the formal 
concept and definition of TTL was given in (Ar-
nold, 2007). Wan introduced the co-training 
method into cross-lingual opinion analysis (Wan, 
2009; Zhou et al, 2011), and Aue et al intro-
duced transfer learning into cross domain analy-
sis (Aue, 2005) which solves similar problems. 
In this paper, we will use the terms source lan-
guage and target language to refer to all cross 
lingual/domain analysis. 
Traditionally, transfer learning methods focus 
on how to estimate the confidence score of trans-
ferred samples in the target language or domain 
(Blitzer et al 2006, Huang et al, 2007; Sugiya-
ma et al, 2008, Chen et al 2011, Lu et al, 2011). 
In some tasks, researchers utilize NLP tools such 
as alignment to reduce the bias towards that of 
860
 the source language in transfer learning (Meng et 
al., 2012). However, detecting misclassification 
in transferred samples (referred to as class noise) 
and reducing negative transfers are still an unre-
solved problem. 
There are two basic methods for class noise 
detection in machine learning. The first is the 
classification based method (Brodley and Friedl, 
1999; Zhu et al 2003; Zhu 2004; Sluban et al, 
2010) and the second is the graph based method 
(Zighed et al 2002; Muhlenbach et al 2004; 
Jiang and Zhou, 2004). Class noise detection can 
also be applied to semi-supervised learning be-
cause noise can accumulate in iterations too. Li 
employed Zighed?s cut edge weight statistic 
method in self-training (Li and Zhou, 2005) and 
co-training (Li and Zhou, 2011). Chao used Li?s 
method in tri-training (Chao et al 2008). (Fuku-
moto et al 2013) used the support vectors to de-
tect class noise in semi-supervised learning.  
In TTL, however, training and testing samples 
cannot be assumed to have the same distributions. 
Thus, noise detection methods used in semi-
supervised learning are not directly suited in 
TTL. Y. Cheng has tried to use semi-supervised 
method (Jiang and Zhou, 2004) in transfer learn-
ing (Cheng and Li, 2009). His experiment 
showed that their approach would work when the 
source domain and the target domain share simi-
lar distributions. How to reduce negative trans-
fers is still a problem in transfer learning. 
3 Our Approach 
In order to reduce negative transfers, we pro-
pose to incorporate class noise detection into 
TTL. The basic idea is to first select high quality 
labeled samples after certain iterations as indica-
tor to detect class noise in transferred samples. 
We then remove noisy samples that cause nega-
tive transfers from the current accumulated train-
ing set to retain an improved set of training data 
for the remainder of the training phase. This neg-
ative sample reduction process can be repeated 
several times during transfer learning. Two ques-
tions must be answered in this approach: (1) how 
to measure the quality of transferred samples, 
and (2) how to utilize high quality labeled sam-
ples to detect class noise in training data. 
3.1 Estimating Testing Error 
To determine the quality of the transferred 
samples that are added iteratively in the learning 
process, we cannot use training error to estimate 
true error because the training data and the test-
ing data have different distributions. In this work, 
we employ the Probably Approximately Correct 
(PAC) learning theory to estimate the error 
boundary. According to the PAC learning theory, 
the least error boundary ? is determined by the 
size of the training set m and the class noise rate 
?, bound by the following relation: 
  ?   (   )                      ( ) 
In TTL, m increases linearly, yet ? is multi-
plied in each iteration. This means the signifi-
cance of m to performance is higher at the begin-
ning of transfer learning and gradually slows 
down in later iterations. On the contrary, the in-
fluence of class noise increases. That is why per-
formance improves initially and gradually falls to 
negative transfer when noise accumulation out-
performs the learned information as shown in 
Fig.1. In TTL, transferred samples in both the 
training data and test data have the same distribu-
tion. This implies that we can apply the PAC 
theory to analyze the error boundary of the ma-
chine learning model using transferred data. 
 
Figure 1 Negative transfer in the learning process 
According to PAC theorem with an assumed 
fixed probability ? (Angluin and Laird, 1988), 
the least error boundary ? is given by:   
  ?   (   ? )  ( (   ) )       ( ) 
where N is a constant decided by the hypothesis 
space.  In any iteration during TTL, the hypothe-
sis space is the same and the probability ? is 
fixed. Thus the least error boundary is deter-
mined by the size of the transferred sample m 
and the class noise of transferred samples ?. Ac-
cording to (2), we apply a manifold assumption 
based method to estimate ?. Let T be the number 
of iterations to serve as one period. We then es-
timate the least error boundary before and after 
each T to measure the quality of transferred sam-
ples during each T. If the least error boundary is 
reduced, it means that transferred samples used 
in this period are of high quality and can improve 
the performance. Otherwise, the transfer learning 
algorithm should stop.  
861
 3.2 Estimating Class Noise 
For formula (2) to work, we need to know the 
class noise rate ? to calculate the error boundary. 
Obviously, we cannot use conditional probabili-
ties from the training data in the source language 
to estimate the noise rate ? of the transferred 
samples because the distribution of source lan-
guage is different from that of target language. 
Consider a KNN graph on the transferred 
samples using any similarity metric, for example, 
cosine similarity, for any two connected vertex 
(     )and (     ) in the graph from samples to 
classes, the edge weight is given by: 
       (     )                         ( ) 
Furthermore, a sign function for the two vertices 
(     )and (     ), is defined as: 
    {
          
          
                   ( ) 
According to the manifold assumption, the 
conditional probability  (  |  ) can be approxi-
mated by the frequency of  (     ) which is 
equal to  (     ). In opinion annotations, the 
agreement of two annotators is often no larger 
than 0.8. This means that for the best cases 
 (     )=0.2. Hence     follows a Bernoulli 
distribution with p=0.2 for the best cases in 
manual annotations.  
Let      (     )  be the vertices that are 
connected to the     vertex, the statistical magni-
tude of the     vertex can be defined as: 
   ?                                 ( )  
where j refers to the     vertex that is connected 
to the     vertex.  
From the theory of cut edge statics, we know 
that the expectation of    is: 
    (     )  ?                  ( )  
And the variance of    is: 
  
   (     ) (     )  ?    
 
 ( )  
By the Center Limit Theorem (CLT),    fol-
lows the normal distribution: 
(     )
  
  (   )                    ( )  
To detect the noise rate of a sample (     ) , 
we can use (8) as the null hypothesis to test the 
significant level. Let    denotes probability of 
the correct classification for a transferred sample. 
   should follow a normal distribution,  
   
 
?    
?  
 
(    )
 
   
   
  
           ( )  
Note that experiments (Li and Zhou, 2011; 
Cheng and Li, 2009; Brodley and Friedl, 1999) 
have shown that     is related to the error rate of 
the example (     ), but it does not reflect the 
ground-truth probability in statistics. Hence we 
assume the class noise rate of example (     ) is: 
                              (  ) 
 We take the general significant level of 0.05 
to reject the null hypothesis. It means that if    of 
(     ) is larger than 0.95, the sample will be 
considered as a class noisy sample. Furthermore, 
   can be used to estimate the average class noise 
rate of a transferred samples in (2). 
In our proposed approach, we establish the 
quality estimate period T to conduct class noise 
detection to estimate the class noise rate of trans-
ferred samples. Based on the average class noise 
we can get the least error boundary so as to tell if 
an added sample is of high quality. If the newly 
added samples are of high quality, they can be 
used to detect class noise in transferred training 
data. Otherwise, transfer learning should stop. 
The flow chart for negative transfer is in Fig.2. 
SLS(labeled)
TLS
(unlabeled)
Classifier
Top k
TS
 period 1
TS
period 2
TS
 period n
KNN 
graph
Estimate ?i and ?n 
?n ? ?n-1?
Output SLS and TS 
(period 1 to n-1)
No
Yes
Del te TS
 ?i? 0.95 
period 1 to n-1
Input 
Input 
T iterations per period
Transfer
process
Negative
transfer
detection
Figure 2 Flow charts of negative transfer detection 
In the above flow chart, SLS and TLS refer to 
the source and target language samples, respec-
tively. TS refers to the transferred samples. Let T 
denote quality estimate period T in terms of itera-
tion numbers. The transfer process select k sam-
ples in each iteration. When one period of trans-
fer process finishes, the negative transfer detec-
tion will estimate the quality by comparing and 
either select the new transferred samples or re-
move class noise accumulated up to this iteration. 
4 Experiment 
4.1 Experiment Setting 
The proposed approach is evaluated on the 
NLP&CC 2013 cross-lingual opinion analysis (in 
862
 short, NLP&CC) dataset 1 . In the training set, 
there are 12,000 labeled English Amazon.com 
products reviews, denoted by Train_ENG, and 
120 labeled Chinese product reviews, denoted as 
Train_CHN, from three categories, DVD, BOOK, 
MUSIC. 94,651 unlabeled Chinese products re-
views from corresponding categories are used as 
the development set, denoted as Dev_CHN. In 
the testing set, there are 12,000 Chinese product 
reviews (shown in Table.1). This dataset is de-
signed to evaluate the CLOA algorithm which 
uses Train_CHN, Train_ENG and Dev_CHN to 
train a classifier for Test_CHN. The performance 
is evaluated by the correct classification accuracy 
for each category in Test_CHN2:  
          
                                  
    
 
where c is either DVD, BOOK or MUSIC. 
Team DVD Book Music 
Train_CHN 40 40 40 
Train_ENG 4000 4000 4000 
Dev_CHN 17814 47071 29677 
Test_CHN 4000 4000 4000 
Table.1 The NLP&CC 2013 CLOA dataset 
In the experiment, the basic transfer learning 
algorithm is co-training. The Chinese word seg-
mentation tool is ICTCLAS (Zhang et al 2003) 
and Google Translator3 is the MT for the source 
language. The monolingual opinion classifier is 
SVMlight4, word unigram/bigram features are em-
ployed. 
4.2 CLOA Experiment Results 
Firstly, we evaluate the baseline systems 
which use the same monolingual opinion classi-
fier with three training dataset including 
Train_CHN, translated Train_ENG and their un-
ion, respectively.  
 DVD Book Music Accuracy 
Train_CHN 0.552 0.513 0.500 0.522 
Train_ENG 0.729 0.733 0.722 0.728 
Train_CHN 
+Train_ENG 
0.737 0.722 0.742 0.734 
Table.2 Baseline performances  
It can be seen that using the same method, the 
classifier trained by Train_CHN are on avergage 
20% worse than the English counter parts.The 
combined use of Train_CHN and translated 
Train_ENG, however, obtained similar 
                                                 
1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 
2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 
3https://translate.google.com 
4http://svmlight.joachims.org/ 
performance to the English counter parts. This 
means the predominant training comes from the 
English training data. 
In the second set of experiment, we compare  
our proposed approach to the official results in 
NLP&CC 2013 CLOA evaluation and the result 
is given in Table 3. Note that in Table 3, the top 
performer of NLP&CC 2013 CLOA evaluation 
is the HLT-HITSZ system(underscored in the 
table), which used the co-training method in 
transfer learning (Gui et al 2013), proving that 
co-training is quite effective for cross-lingual 
analysis. With the additional negative transfer 
detection, our proposed approach achieves the 
best performance on this dataset outperformed 
the top system (by HLT-HITSZ) by a 2.97% 
which translate to 13.1% error reduction im-
provement to this state-of-the-art system as 
shown in the last row of Table 3.     
Team DVD Book Music Accuracy 
BUAA 0.481 0.498 0.503 0.494 
BISTU 0.647 0.598 0.661 0.635 
HLT-HITSZ 0.777 0.785 0.751 0.771 
THUIR 0.739 0.742 0.733 0.738 
SJTU 0.772 0.724 0.745 0.747 
WHU 0.783 0.770 0.760 0.771 
Our approach 0.816 0.801 0.786 0.801 
Error 
Reduction 
0.152 0.072 0.110 0.131 
Table.3 Performance compares with NLP&CC 
2013 CLOA evaluation results 
To further investigate the effectiveness of our 
method, the third set of experiments evaluate the 
negative transfer detection (NTD) compared to 
co-training (CO) without negative transfer 
detection as shown in Table.4 and Fig.3 Here, we 
use the union of Train_CHN and Train_ENG as 
labeled data and Dev_CHN as unlabeled data to 
be transferred in the learning algorithms. 
 DVD Book Music Mean 
NTD 
Best case 0.816 0.801 0.786 0.801 
Best period 0.809 0.798 0.782 0.796 
Mean 0.805 0.795 0.781 0.794 
CO 
Best case 0.804 0.796 0.783 0.794 
Best period 0.803 0.794 0.781 0.792 
Mean 0.797 0.790 0.775 0.787 
Table.4 CLOA performances 
Taking all categories of data, our proposed 
method improves the overall average precision 
(the best cases) from 79.4% to 80.1% when 
compared to the state of the art system which 
translates to error reduction of 3.40% (p-
value?0.01 in Wilcoxon signed rank test). Alt-
hough the improvement does not seem large, our 
863
  
   
Figure 3 Performance of negative transfer detection vs. co-training 
algorithm shows a different behavior in that it 
can continue to make use of available training 
data to improve the system performance. In other 
words, we do not need to identify the tipping 
point where the performance degradation can 
occur when more training samples are used. Our 
approach has also shown the advantage of stable 
improvement.  
In the most practical tasks, co-training based 
approach has the difficulty to determine when to 
stop the training process because of the negative 
transfer. And thus, there is no sure way to obtain 
the above best average precision. On the contrary, 
the performance of our proposed approach keeps 
stable improvement with more iterations, i.e. our 
approach has a much better chance to ensure the 
best performance. Another experiment is con-
ducted to compare the performance of our pro-
posed transfer learning based approach with su-
pervised learning. Here, the achieved perfor-
mance of 3-folder cross validation are given in 
Table 5. 
 DVD Book Music Average 
Supervised 0.833 0.800 0.801 0.811 
Our approach 0.816 0.801 0.786 0.801 
Table.5 Comparison with supervised learning  
The accuracy of our approach is only 1.0% 
lower than the supervised learning using 2/3 of 
Test_CHN. In the BOOK subset, our approach 
achieves match result. Note that the performance 
gap in different subsets shows positive correla-
tion to the size of Dev_CHN. The more samples 
are given in Dev_CHN, a higher precision is 
achieved even though these samples are unla-
beled. According to the theorem of PAC, we 
know that the accuracy of a classifier training 
from a large enough training set with confined 
class noise rate will approximate the accuracy of 
classifier training from a non-class noise training 
set. This experiment shows that our proposed 
negative transfer detection controls the class 
noise rate in a very limited boundary. Theoreti-
cally speaking, it can catch up with the perfor-
mance of supervised learning if enough unla-
beled samples are available. In fact, such an ad-
vantage is the essence of our proposed approach.  
5 Conclusion 
In this paper, we propose a negative transfer 
detection approach for transfer learning method 
in order to handle cumulative class noise and 
reduce negative transfer in the process of transfer 
learning. The basic idea is to utilize high quality 
samples after transfer learning to detect class 
noise in transferred samples. We take cross lin-
gual opinion analysis as the data set to evaluate 
our method. Experiments show that our proposed 
approach obtains a more stable performance im-
provement by reducing negative transfers. Our 
approach reduced 13.1% errors than the top sys-
tem on the NLP&CC 2013 CLOA evaluation 
dataset. In BOOK category it even achieves bet-
ter result than the supervised learning. Experi-
mental results also show that our approach can 
obtain better performance when the transferred 
samples are added incrementally, which in pre-
vious works would decrease the system perfor-
mance. In future work, we plan to extend this 
method into other language/domain resources to 
identify more transferred samples.  
Acknowledgement 
This research is supported by NSFC 61203378, 
61300112, 61370165, Natural Science Founda-
tion of GuangDong S2013010014475, MOE 
Specialized Research Fund for the Doctoral Pro-
gram of Higher Education 20122302120070,  
Open Projects Program of National Laboratory 
of Pattern Recognition?Shenzhen Foundational 
Research Funding JCYJ20120613152557576, 
JC201005260118A, Shenzhen International Co-
operation Research Funding 
GJHZ20120613110641217 and Hong Kong Pol-
ytechnic University Project code Z0EP. 
DVD Book Music 
864
 Reference 
Angluin, D., Laird, P. 1988. Learning from Noisy 
Examples. Machine Learning, 2(4): 343-370. 
Arnold, A., Nallapati, R., Cohen, W. W. 2007. A 
Comparative Study of Methods for Transductive 
Transfer Learning. In Proc. 7th IEEE ICDM Work-
shops, pages 77-82. 
Aue, A., Gamon, M. 2005. Customizing Sentiment 
Classifiers to New Domains: a Case Study, In Proc. 
of t RANLP. 
Blitzer, J., McDonald, R., Pereira, F. 2006. Domain 
Adaptation with Structural Correspondence Learn-
ing. In Proc. EMNLP, 120-128. 
Brodley, C. E., Friedl, M. A. 1999. Identifying and 
Eliminating Mislabeled Training Instances. Journal 
of Artificial Intelligence Research, 11:131-167. 
Chao, D., Guo, M. Z., Liu, Y.,  Li, H. F. 2008.  Partic-
ipatory Learning based Semi-supervised Classifica-
tion. In Proc. of 4th ICNC, pages 207-216. 
Cheng, Y., Li, Q. Y. 2009. Transfer Learning with 
Data Edit. LNAI, pages 427?434. 
Chen, M., Weinberger, K. Q.,  Blitzer, J. C. 2011.  
Co-Training for Domain Adaptation. In Proc. of 
23th NIPS. 
Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text 
Classification from Positive and Unlabeled Data 
using Misclassified Data Correction. In Proc. of 
51st ACL, pages 474-478. 
Gui, L., Xu, R.,  Xu, J., et al 2013. A Mixed Model 
for Cross Lingual Opinion Analysis. In CCIS, 400, 
pages 93-104. 
Huang, J., Smola, A., Gretton, A., Borgwardt, K.M., 
Scholkopf, B. 2007. Correcting Sample Selection 
Bias by Unlabeled Data. In Proc. of 19th NIPS,  
pages 601-608. 
Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for 
kNN Classifiers with Neural Network Ensemble. In 
LNCS, 3173,  pages 356-361. 
Li, M., Zhou, Z. H. 2005. SETRED: Self-Training 
with Editing. In Proc. of PAKDD, pages 611-621. 
Li, M., Zhou, Z. H. 2011.  COTRADE: Confident Co-
Training With Data Editing. IEEE Transactions on 
Systems, Man, and Cybernetics?Part B: Cyber-
netics, 41(6):1612-1627. 
Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011. 
Joint Bilingual Sentiment Classification with Un-
labeled Parallel Corpora. In Proc. of 49th ACL, 
pages 320-330. 
Meng, X. F., Wei, F. R., Liu, X. H., et al 2012. 
Cross-Lingual Mixture Model for Sentiment Clas-
sification. In Proc. of 50th ACL, pages 572-581. 
Muhlenbach, F., Lallich, S., Zighed, D. A. 2004. 
Identifying and Handling Mislabeled Instances.  
Journal of Intelligent Information System, 22(1): 
89-109. 
Pan, S. J., Yang, Q. 2010. A Survey on Transfer 
Learning, IEEE Transactions on Knowledge and 
Data Engineering, 22(10):1345-1360. 
Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for 
Multi-view Learning and Manifold Co-
Regularization. In Proc. of 25th  ICML, pages 976?
983. 
Sluban, B., Gamberger, D., Lavra, N. 2010.  Advanc-
es in Class Noise Detection. In Proc.19th ECAI,  
pages 1105-1106. 
Sugiyama, M.,  Nakajima, S., Kashima, H., Buenau, 
P.V., Kawanabe, M. 2008. Direct Importance Es-
timation with Model Selection and its Application 
to Covariate Shift Adaptation. In Proc. 20th NIPS. 
Wan, X. 2009. Co-Training for Cross-Lingual Senti-
ment Classification, In Proc. of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP,  235?243. 
Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q. 
2003. HHMM-based Chinese Lexical Analyzer 
ICTCLAS. In 2nd SIGHAN workshop affiliated 
with 41th ACL, pages 184-187. 
 Zhou, X., Wan X., Xiao, J. 2011. Cross-Language 
Opinion Target Extraction in Review Texts. In 
Proc. of IEEE 12th ICDM, pages 1200-1205. 
Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003.  Eliminating 
Class Noise in Large Datasets. In Proc. of 12th 
ICML, pages 920-927. 
Zhu, X. Q. 2004. Cost-guided Class Noise Handling 
for Effective Cost-sensitive Learning In Proc. of 4th  
IEEE ICDM,  pages 297-304. 
Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.  
Separability Index in Supervised Learning. In Proc. 
of PKDD, pages 475-487. 
865
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 448?451,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
HITSZ_CITYU: Combine Collocation, Context Words and Neighbor-
ing Sentence Sentiment in Sentiment Adjectives Disambiguation 
 
 
Ruifeng Xu1,2, Jun Xu1 
1Harbin Institute of Technology, 
Shenzhen Campus, China 
xuruifeng@hitsz.edu.cn 
hit.xujun@gmail.com 
Chunyu Kit2 
2City University of Hong Kong, 
Hong Kong 
ctckit@cityu.edu.hk  
  
 
Abstract 
This paper presents the HIT_CITYU systems 
in Semeval-2 Task 18, namely, disambiguat-
ing sentiment ambiguous adjectives. The base-
line system (HITSZ_CITYU_3) incorporates 
bi-gram and n-gram collocations of sentiment 
adjectives, and other context words as features 
in a one-class Support Vector Machine (SVM) 
classifier. To enhance the baseline system, col-
location set expansion and characteristics 
learning based on word similarity and semi-
supervised learning are investigated, respec-
tively. The final system (HITSZ_CITYU_1/2) 
combines collocations, context words and 
neighboring sentence sentiment in a two-class 
SVM classifier to determine the polarity of 
sentiment adjectives. The final systems 
achieved 0.957 and 0.953 (ranked 1st and 2nd) 
macro accuracy, and 0.936 and 0.933 (ranked 
2nd and 3rd) micro accuracy, respectively.  
 
1 Introduction 
Sentiment analysis is always puzzled by the con-
text-dependent sentiment words that one word 
brings positive, neutral or negative meanings in 
different contexts. Hatzivassiloglou and 
Mckeown (1997) predicated the polarity of ad-
jectives by using the pairs of adjectives linked by 
consecutive or negation conjunctions. Turney 
and Littman (2003) determined the polarity of 
sentiment words by estimating the point-wise 
mutual information between sentiment words 
and a set of seed words with strong polarity. An-
dreevskaia and Bergler (2006) used a Sentiment 
Tag Extraction Program to extract sentiment-
bearing adjectives from WordNet. Esuli and Se-
basian (2006) studied the context-dependent sen-
timent words in WordNet but ignored the in-
stances in real context. Wu et al (2008) applied 
collocation plus a SVM classifier in Chinese sen-
timent adjectives disambiguation. Xu et al (2008) 
proposed a semi-supervised learning algorithm to 
learn new sentiment word and their context-
dependent characteristics.  
Semeval-2 Task 18 is designed to provide a 
common framework and dataset for evaluating 
the disambiguation techniques for Chinese sen-
timent adjectives. The HITSZ_CITYU group 
submitted three runs corresponding to one base-
line system and one improved systems (two runs). 
The baseline system (HITSZ_CITYU_3) is 
based on collocations between sentiment words 
and their targets as well as their context words. 
For the ambiguous adjectives, 412 positive and 
191 negative collocations are built from a 100-
million-word corpus as the seed collocation set. 
Using the context words of seed collocations as 
features, a one-class SVM classifier is trained in 
the baseline system. Using HowNet-based word 
similarity as clue, the seed collocations are ex-
panded to improve the coverage of collocation-
based technique. Furthermore, a semi-supervised 
learning algorithm is developed to learn new col-
locations between sentiment words and their tar-
gets from raw corpus. Finally, the inner sentence 
features, such as collocations and context words, 
and the inter sentence features, i.e. neighboring 
sentence sentiments, are incorporated to deter-
mine the polarity of ambiguous adjectives. The 
improved systems (HITSZ_CITYU_1/2) 
achieved 0.957 and 0.953 macro accuracy 
(ranked 1st and 2nd) and 0.936 and 0.933 micro 
accuracy (ranked 2nd and 3rd), respectively. This 
result shows that collocation, context-words and 
neighboring sentence sentiment are effective in 
sentiment adjectives disambiguation. 
The rest of this paper is organized as follows. 
Section 2 presents the collocation extraction sub-
system based on lexical statistics. Section 3 
448
presents the baseline system and Section 4 
presents the improved systems. The experiment 
results are given in Section 5 and finally, Section 
6 concludes. 
2 Collocation Extraction 
A lexical statistics-based collocation extraction 
subsystem is developed to identify both the bi-
gram and n-gram collocations of sentiment ad-
jectives. This subsystem is based on our previous 
research on Chinese collocation extraction. It 
recognizes the co-occurring words of a headword 
as collocations which have co-occurrence fre-
quency significance among all co-occurring 
words and co-occurrence position significance 
among all co-occurring positions.  
For a sentiment adjective, noted as whead, any 
word within the [-5,+5] context window is a co-
word, denoted as wco-i for 1? i ? k, where k is the 
total number of different co-words of whead.  
BI-Strength(whead,wco-i) between a head word 
whead and a co-word w co-i (i=1, to k) is designed 
to measure the co-occurrence frequency signifi-
cance as follows:  
)()(
)()(
5.0
)()(
)()(
5.0
),(
minmaxminmax icoico
icoicohead
headhead
headicohead
icohead
wfwf
wfwwf
wfwf
wfwwf
wwStrengthBI
??
???
?
?
??+?
??
=?
(1) 
where, fmax(whead) , fmin(whead) and )( headwf are the 
highest, lowest and average co-occurrence fre-
quencies among all the co-words of whead,, re-
spectively; fmax(wco-i), fmin(wcoi) and )( icowf ?  are 
respectively the highest, lowest and average co-
occurrence frequencies of the co-words for wco-i. 
The value of BI-Strength(whead wco-j) ranges from 
-1 to 1, and a larger value means a stronger asso-
ciation. Suppose f(whead,wco-i, m) is the frequency 
that wco-i co-occurs with whead at position m(?
5<=m<=5). The BI-Spread(whead,wco-i) is de-
signed to characterizes the significance that wco-i 
around whead at neighbouring places as follows: 
 
),,(
|),(),,(|
),(
5
5
5
5
?
?
?=
?
?=
??
?
?
=?
m
icohead
m
icoheadicohead
icohead
mwwf
wwfmwwf
wwSpreadBI
(2) 
where, ),( icohead wwf ? , fmax(whead,,wco-i), and fmin 
(whea,,dwco-i) are the average, highest, and lowest 
co-occurrence frequencies among all 10 posi-
tions, respectively. The value of BI-Spread(whead, 
wco-i) ranges from 0 to 1. A larger value means 
that whead and wco-i tend to co-occur in one or two 
positions.  
The word pairs satisfying, (1) BI-
Strength(whead wco-j)>K0 and (2) BI-Spread(whead, 
wco-i)>U0, are extracted as bi-gram collocations, 
where K0 and U0 are empirical threshold.  
Based on the extracted bi-gram collocations, 
the appearance of each co-word in each position 
around whead is analyzed. For each of the possible 
relative distances from whead, only words occupy-
ing the position with a probability greater than a 
given threshold T are kept. Finally, the adjacent 
words satisfying the threshold requirement are 
combined as n-gram collocations. 
3 The Baseline System 
The baseline system incorporates collocation and 
context words as features in a one-class SVM 
classifier. It consists of two steps: 
 STEP 1: To match a test instance containing 
seed collocation set. If the instance cannot be 
matched by any collocations, go to STEP 2. 
STEP 2: Use a trained classifier to indentify 
the sentiment of the word.  
The collocations of 14 testing sentiment adjec-
tives are extracted from a 100-million-word cor-
pus. Collocations with obvious and consistent 
sentiment are manually identified. 412 positive 
and 191 negative collocations are established as 
the seed collocation set.  
We think that the polarity of a word can be de-
termined by exploiting the association of its co-
occurring words in sentence. We assume that, the 
two instances of an ambiguous sentiment adjec-
tives that have similar neighboring nouns may 
have the same polarity. Gamon and Aue (2005) 
made an assumption to label sentiment terms. 
We extract 13,859 sentences containing collo-
cations between negative adjective and targets 
in seed collocation set or collocations between 
ambiguous adjective and negative modifier 
(such as ?? too) as the training data. These 
sentences are assume negative. A single-class 
classifier is then trained to recognize negative 
sentences. Three types of features are used:  
(1) Context features include bag of words 
within context in window of [-5, +5] 
(2) Collocation features contain bi-grams in 
window [-5,+5] 
(3) Collocation features contain n-grams in 
window [-5,+5] 
In our research, SVM with linear kernel is 
employed and the open source SVM package ? 
LIBSVM is selected for the implementation.  
4 The Improved System 
The preliminary experiment shows that the base-
line system is not satisfactory, especially the 
449
coverage is low. It is observed that the seed col-
location set covers 17.54% of sentences contain-
ing the ambiguous adjectives while the colloca-
tions between adjective and negative modifier 
covers only 11.28%. Therefore, we expand the 
sentiment adjective-target collocation set based 
on word similarity and a semi-supervised learn-
ing algorithm orderly. We then incorporate both 
inner-sentence features (collocations, context 
words, etc.) and inter-sentence features in the 
improved systems for sentiment adjectives dis-
ambiguation.  
4.1 Collocation Set Expansion based on 
Word Similarity 
First, we expand the seed collocation set on the 
target side. The words strongly similar to known 
targets are identified by using a word similarity 
calculation package, provided by HowNet (a 
Chinese thesaurus). Once these words co-occur 
with adjective within a context window more 
often than a threshold, they are appended to seed 
collocation set. For example, ??-??(low ca-
pacity)?is expanded from a seed collocation ??
-?? (low capacity)?. 
Second, we manually identify the words hav-
ing the same ?trend? as the testing adjectives. 
For example, ??? increase? is selected as a 
same-trend word of ?? high?. The collocations 
of ???? are extracted from corpus. Its collo-
cated targets with confident and consistent sen-
timent are appended to the sentiment collocation 
set of ??? if they co-occurred with ??? more 
than a threshold. In this way, some low-
frequency sentiment collocation can be obtained. 
4.2 Semi-supervised Learning of Sentiment 
Collocations 
A semi-supervised learning algorithm is devel-
oped to further expand the collocation seed set, 
which is described as follows. (It is revised based 
on our previous research (Xu et al 2008). The 
basic assumption here is that, the sentiment of a 
sentence having ambiguous adjectives can be 
estimated based on the sentiment of its neighbor-
ing sentences.  
 
Input: Raw training corpus, labeled as Su,  
Step 1. The sentences holding strong polarities 
are recognized from Su which satisfies any two of 
following requirements, (1) contains known con-
text-free sentiment word (CFSW); (2) contains 
more than three known context-dependent senti-
ment words (CDSW); (3) contains collocations 
between degree adverbs and known CDSWs; (4) 
contains collocations between degree adverbs 
and opinion operators (the verbs indicate a opi-
nion operation, such as?? praise); (5) contains 
known opinion indicator and known CDSWs. 
Step 2. Identify the strong non-opinionated sen-
tences in Su. The sentences satisfying all of fol-
lowing four conditions are recognized as non-
opinionated ones, (1) have no known sentiment 
words; (2) have no known opinion operators; (3) 
have no known degree adverbs and (4) have no 
known opinion indicators.  
Step 3. Identify the opinion indicators in the rest 
sentences. Determine their polarities if possible 
and mark the conjunction (e.g.? and) or nega-
tion relationship (e.g.? but) in the sentences. 
Step 4. Match the CFSWs and known CDSWs in 
Su. The polarities of CFSWs are assigned based 
on sentiment lexicon.  
Step 5. If a CDSW occurs in a sentence with cer-
tain orientations which is determined by the opi-
nion indicators, its polarity is assigned as the 
value suggested. If a CDSW co-occur with a 
seed collocated target, it polarity is assigned ac-
cording to the seed sentiment collocation set. 
Otherwise, if a CDSW co-occur with a CFSW in 
the same sentence, or the neighboring continual 
or compound sentence, the polarity of CDSW is 
assigned as the same as CFSW, or the reversed 
polarity if a negation indicator is detected. 
Step 6. Update the polarity scores of CDSWs in 
the target set by using the cases where the polari-
ty is determined in Step 5. 
Step 7. Determine the polarities of CDSWs in 
the undetermined sentences. Suppose Si is a sen-
tence and the polarity scores of all its CFSWs 
and CDSWs are known, its polarity, labeled as 
Plo(Si), is estimated by using the polarity scores 
of all of the opinion words in this sentence, viz.: 
  
? ?+ ?= )(_)(_ )(_)(pos_)( CDSWnegPCDSWposP CFSWnegPCFSWPSiPlo (3) 
A large value (>0) of Plo(si) implies that si tends 
to be positive, and vice versa.  
Step 8. If the sentence polarity cannot be deter-
mined by its components, we use the polarity of 
its neighboring sentences sj-1 and sj+1, labeled as 
Plo(sj-1) and Plo(sj+1), respectively, to help de-
termine Plo(sj), viz.:  
)(5.0)(*)(5.0)( 11 +? ?++?= jjjj sPlosPlosPlosPlo (4) 
where, Plo*(sj) is the polarity score of Sj (Fol-
lowing Equation 3) but ignore the contribution of 
testing adjectives while 0.5 are empirical weights.  
450
Step 9. After all of the polarities of known 
CDSWs in the training data are determined, up-
date the collocation set by identifying co-
occurred pairs with consistent sentiment. 
Step 10. Repeat Step 5 to Step 9 to re-estimate 
the sentiment of CDSWs and expand the colloca-
tion set, until the collocation set converge. 
 
In this way, the seed collocation set is further 
expanded and their sentiment characteristics are 
obtained.  
4.3 Sentiment Adjectives Classifier 
We incorporate the following 8 groups of fea-
tures in a linear-kernel two-class SVM classifier 
to classify the sentences with sentiment adjec-
tives into positive or negative: 
(1) The presence of known positive/negative 
opinion indicator and opinion operator 
(2) The presence of known positive/negative 
CFSW 
(3) The presence of known positive/negative 
CDSW(exclude the testing adjectives) 
(4) The presence of known positive/negative 
adjective-target bi-gram collocations 
(5) The presence of known positive/negative 
adjective-target n-gram collocations 
(6) The coverage of context words surround-
ing the adjectives in the context words in 
training positive/negative sentences 
(7) The sentiment of -1 sentence 
(8) The sentiment of +1 sentence 
The classifier is trained by using the sentences 
with determined sentiment which is obtained in 
the semi-supervised learning stage. 
5 Evaluations and Conclusion 
The ACL-SEMEVAL task 18 testing dataset 
contains 14 ambiguous adjectives and 2,917 in-
stances. HITSZ_CITYU group submitted three 
runs. Run-1 and Run-2 are two runs correspond-
ing to the improved system and Run-3 is the 
baseline system. The achieved performances are 
listed in Table 1.  
 
Run ID Marco Accuracy Micro Accuracy
1 0.953 0.936 
2 0.957 0.933 
3(baseline) 0.629 0.665 
Table 1: Performance of HITSZ_CITYU Runs 
 
It is observed that the improved systems 
achieve promising results which is obviously 
higher than the baseline. They are ranked 1st and 
2nd in Macro Accuracy evaluation and 2nd and 3rd 
in Micro Accuracy evaluation among 16 submit-
ted runs, respectively. 
6 Conclusion 
In this paper, we proposed similarity-based and 
semi-supervised based methods to expand the 
adjective-target seed collocation set. Meanwhile, 
we incorporate both inner-sentence (collocations 
and context words) and inter-sentence features in 
a two-class SVM classifier for the disambigua-
tion of sentiment adjectives. The achieved prom-
ising results show the effectiveness of colloca-
tion features, context words features and senti-
ment of neighboring sentences. Furthermore, we 
found that the neighboring sentence sentiments 
are important features for the disambiguation of 
sentiment ambiguous adjectives, which is ob-
viously different from the traditional word sense 
disambiguation that emphasize the inner-
sentence features. 
References  
Andreevskaia, A. and Bergler, S. 2006. Mining 
WordNet for fuzzy sentiment: Sentiment tag ex-
traction from WordNet glosses. In Proceedings of 
EACL 2006, pp. 209-216 
Esuli, A. and Sebastian, F. 2006. SENTIWORDNET: 
A publicly available lexical resource for opinion 
mining. In Proceeding of LREC 2006, pp. 417-422. 
Hatzivassiloglou, V. and McKeown, K. R. 1997. Pre-
dicting the semantic orientation of adjectives. In 
Proceeding of ACL 1997, pp.174-181 
Michael Gamon and Anthony Aue. 2005. Automatic 
identification of sentiment vocabulary: Exploiting 
low association with known sentiment terms. In 
Proceedings of the ACL05 Workshop on Feature 
Engineering for Machine Learning in Natural 
Language Processing, pp.57-64 
Ruifeng Xu, Kam-Fai Wong et al 2008. Learning 
Knowledge from Relevant Webpage for Opinion 
Analysis, in Proceedings of 2008 IEEE / WIC / 
ACM Int. Conf. Web Intelligence, pp. 307-313  
Turney, P. D. and Littman, M. L. 2003. Measuring 
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on In-
formation Systems, vol. 21, no. 4, pp.315-346 
Yunfang Wu, Miao Wang and Peng Jin. 2008. Dis-
ambiguating sentiment ambiguous adjectives, In 
Proceedings of Int. Conf. on Natural Language 
Processing and Knowledge Engineering 2008, pp. 
1-8 
451
 Combine Person Name and Person Identity Recognition and Docu-
ment Clustering for Chinese Person Name Disambiguation 
Ruifeng Xu1,2,Jun Xu1,Xiangying Dai1
Harbin Institute of Technology,  
Shenzhen Postgraduate School, China 
{xuruifeng.hitsz;hit.xujun; 
mi-chealdai}@gmail.com 
     Chunyu Kit2 
         2City University of Hong Kong,    
        Hong Kong, China 
    ctckit@cityu.edu.hk 
 
Abstract 
This paper presents the HITSZ_CITYU 
system in the CIPS-SIGHAN bakeoff 
2010 Task 3, Chinese person name dis-
ambiguation. This system incorporates 
person name string recognition, person 
identity string recognition and an agglo-
merative hierarchical clustering for 
grouping the documents to each identical 
person. Firstly, for the given name index 
string, three segmentors are applied to 
segment the sentences having the index 
string into Chinese words, respectively. 
Their outputs are compared and analyzed. 
An unsupervised clustering is applied 
here to help the personal name recogni-
tion. The document set is then divided 
into subsets according to each recog-
nized person name string. Next, the sys-
tem identifies/extracts the person identity 
string from the sentences based on lex-
icon and heuristic rules. By incorporat-
ing the recognized person identity string, 
person name, organization name and 
contextual content words as features, an 
agglomerative hierarchical clustering is 
applied to group the similar documents 
in the document subsets to obtain the fi-
nal person name disambiguation results. 
Evaluations show that the proposed sys-
tem, which incorporates extraction and 
clustering technique, achieves encourag-
ing recall and good overall performance. 
1 Introduction 
Many people may have the same name which 
leads to lots of ambiguities in text, especially for 
some common person names. This problem puz-
zles many information retrieval and natural lan-
guage processing tasks. The person name ambi-
guity problem becomes more serious in Chinese 
text. Firstly, Chinese names normally consist of 
two to four characters. It means that for a two-
character person name, it has only one character 
as surname to distinguish from other person 
names with the same family name. It leads to 
thousands of people have the same common 
name, such ??  and ?? . Secondly, some 
three-character or four-character person name 
may have one two-character person name as its 
substring such as ?? and ???, which leads 
to more ambiguities. Thirdly, some Chinese per-
son name string has the sense beyond the person 
name. For example, a common Chinese name, 
?? has a sense of ?Peak?. Thus, the role of a 
string as person name or normal word must be 
determined. Finally, Chinese text is written in 
continuous character strings without word gap. It 
leads to the problem that some person names 
may be segmented into wrong forms.  
In the recent years, there have been many re-
searches on person name disambiguation 
(Fleischman and Hovy 2004; Li et al 2004; Niu 
et al 2004; Bekkerman and McCallum 2005; 
Chen and Martin 2007; Song et al 2009). To 
promote the research in this area, Web People 
Search (WePS and WePS2) provides a standard 
evaluation, which focuses on information extrac-
tion of personal named-entities in Web data (Ar-
tiles et al, 2007; Artiles et al, 2009; Sekine and 
Artiles, 2009). Generally speaking, both cluster-
based techniques which cluster documents cor-
responding to one person with similar contexts, 
global features and document features (Han et al 
2004; Pedersen et al 2005; Elmacioglu et al 
2007; Pedersen and Anagha 2007; Rao et al 
2007) and information extraction based tech-
niques which recognizes/extracts the description 
features of one person name (Heyl and Neumann 
2007; Chen et al 2009) are adopted. Consider-
ing that these evaluations are only applied to 
English text, CIPS-SIGHAN 2010 bakeoff pro-
posed the first evaluation campaign on Chinese 
person name disambiguation. In this evaluation, 
corresponding to given index person name string, 
the systems are required to recognize each iden-
tical person having the index string as substring 
and classify the document corresponding to each 
identical person into a group. 
This paper presents the design and implemen-
tation of HITSZ_CITYU system in this bakeoff. 
This system incorporates both recogni-
tion/extract technique and clustering technique 
for person name disambiguation. It consists of 
two major components. Firstly, by incorporating 
word segmentation, named entity recognition, 
and unsupervised clustering, the system recog-
nize the person name string in the document and 
then classify the documents into subsets corres-
ponding to the person name. Secondly, for the 
documents having the same person name string, 
the system identifies the person identify string, 
other person name, organization name and con-
textual context words as features. An agglomera-
tive hierarchical clustering algorithm is applied 
to cluster the documents to each identical person. 
In this way, the documents corresponding to 
each identical person are grouped, i.e. the person 
name ambiguities are removed. The evaluation 
results show that the HITSZ_CITYU system 
achieved 0.8399(B-Cubed)/0.8853(P-IP) preci-
sions and 0.9329(B-Cubed)/0.9578(P-IP) recall, 
respectively. The overall F1 performance 
0.8742(B-Cubed)/0.915(P-IP) is ranked 2nd in 
ten participate teams. These results indicate that 
the proposed system incorporating both extrac-
tion and clustering techniques achieves satisfac-
tory recall and overall performance. 
The rest of this report is organized as follows. 
Section 2 describes and analyzes the task. Sec-
tion 3 presents the word segmentation and per-
son name recognition and Section 4 presents the 
person description extraction and document 
clustering. Section 5 gives and discusses the 
evaluation results. Finally, Section 6 concludes.  
2 Task Description 
CIPS-SIGHAN bakeoff on person name disam-
biguation is a clustering task. Corresponding to 
26 person name query string, the systems are 
required to cluster the documents having the in-
dex string into multiple groups, which each 
group representing a separate entity.   
HITSZ_CITYU system divided the whole 
task into two subtasks: 
1. Person name recognition. It includes:  
1.1  Distinguish person name/ non person 
name in the document. For a given index 
string ??, in Example 1, ?? is a person 
name while in Example 2, ?? is a noun 
meaning ?peak? rather than a person name. 
Example 1. ????????????
???????(Gaofeng, the Negotiator 
and professor of Beijing People's Police 
College, said). 
Example 2. ??????? 11.83%??
?? (This value raise to the peak value of 
11.83%). 
1.2  Recognize the exact person name, espe-
cially for three-character to four-character 
names. For a given index string, ??, a 
person name ?? should be identified in 
Example 3 while??? should be identi-
fied from Example 4. 
Example 3. ????????????
??????? (Li Yan from Chinese 
team one is the highest one in the female 
athletes participating this game). 
Example 4. ?????????  (The 
soldier Li YanQing is an orphan) 
2. Cluster the documents for each identical 
person. That is for each person recognized 
person name, cluster documents into groups 
while each group representing an individual 
person.  For the non person names instances 
(such as Example 2), they are clustered into 
a discarded group. Meanwhile, the different 
person with the same name should be sepa-
rated. For example, ?? in the Example 3 
and Example 5 is a athlete and a painter, re-
spectively. These two sentences should be 
cluster into different groups. 
Example 5. ????????????
????(The famous painter Li Yan , who 
involved in hosting this exhibition, said that) 
3 Person Name Recognition 
As discussed in Section 2, HITSZ_CITYU sys-
tem firstly recognizes the person names from the 
text including distinguish the person name/ non-
person name word and recognize the different 
person name having the name index string. In 
our study, we adopted three developed word 
segmentation and named entity recognition tools 
to generate the person name candidates. The 
three tools are: 
1. Language Processing Toolkit from Intel-
ligent Technology & Natural Language 
Processing Lab (ITNLP) of Harbin Insti-
tute of Technology (HIT).  
http://www.insun.hit.edu.cn/ 
2. ICTCLAS from Chinese Academy of 
Sciences. http://ictclas.org/ 
3. The Language Technology Platform from 
Information Retrieval Lab of Harbin Insti-
tute of Technology. http://ir.hit.edu.cn 
We apply the three tools to segment and tag 
the documents into Chinese words. The recog-
nized person name having the name index string 
will be labeled as /nr while the index string is 
labeled as discard if it is no recognized as a per-
son name even not a word.  For the sentences 
having no name index string, we simply vote the 
word segmentation results by as the output. As 
for the sentences having name index string, we 
conduct further analysis on the word segmenta-
tion results.  
1. For the cases that the matched string is 
recognized as person name and non-
person name by different systems, respec-
tively, we selected the recognized person 
name as the output. For example, in  
Example 6. ???????????
????????????? (Secre-
tary for Health, Welfare and Food, Yang 
Yongqiang commended the excellent work 
of Tse Wanwen). 
the segmentation results by three segmen-
tors are ???/nr |discarded|???/nr, 
respectively. We select ???/nr as the 
output. 
2. For the cases that three systems generate 
different person names, we further incor-
porating unsupervised clustering results 
for determination. Here, an agglomerative 
hierarchical clustering with high threshold 
is applied (the details of clustering will be 
presented in Section 4).  
Example 7. ?????? (Zhufang 
overcome three barriers) 
In this example, the word segmentation 
results are ??/nr, ???/nr, ???
/nr, respectively. It is shown that there is 
a segmentation ambiguity here because 
both ?? and ??? are legal Chinese 
person names. Such kinds of ambiguity 
cannot be solved by segmentors indivi-
dually. We further consider the clustering 
results. Since the Example 7 is clustered 
with the documents having the segmenta-
tion results of ??, two votes (emphasize 
the clustering confidence) for ?? are as-
signed. Thus, ?? and ??? obtained 3 
votes and 2 votes in this case, respectively, 
and thus ?? is selected as the output. 
3. For cases that the different person name 
forms having the same votes, the longer 
person name is selected. In the following 
example, 
Example 8. ???????????
??????????? (Prof. Zhang 
Mingxuan, the deputy director of Shang-
hai Municipal Education Commission, 
said at the forum) 
The segmentation form of ?? and ??
?  received the same votes, thus, the 
longer one??? is selected as the out-
put. 
In this component, we applied three segmen-
tors (normally using the local features only) with 
the help of clustering to (using both the local and 
global features) recognize person name in the 
text with high accuracy. It is important to ensure 
the recall performance of the final output. Noted, 
in order to ensure the high precision of cluster-
ing, we set a high similarity threshold here.  
4 Person Name Disambiguation 
4.1 Person Identity Recognition/Extraction 
A person is distinguished by its associated 
attributes in which its identity description is es-
sential. For example, a person name has the 
identity of ?? president and ?? farmer, re-
spectively, tends to be two different persons. 
Therefore, in HITSZ_CITYU system, the person 
identity is extracted based on lexicon and heuris-
tic rules before person name disambiguation. 
We have an entity lexicon consisting of 85 
suffixes and 248 prefix descriptor for persons as 
the initial lexicon. We further expand this lex-
icon through extracting frequently used entity 
words from Gigaword. Here, we segmented 
documents in Gigaword into word sequences. 
For each identified person name, we collect its 
neighboring nouns. The associations between the 
nouns and person name can be estimated by their 
?2 test value. For a candidate entity wa and per-
son name wb, (here, wb is corresponding to per-
son name class with the label /nr), the following 
2-by-2 table shown the dependence of their oc-
currence.  
Table 1 The co-occurrence of two words 
 
awx = awx ?  
bwy =  C11 C12 
bwy ?  C21 C22 
For wa and wb, ?2 test (chi-square test) esti-
mates the differences between observed and ex-
pected values as follows: 
)()()()(
)(
2221221221112211
2
211222112
CCCCCCCC
CCCCN
+++++++
??=?       (1) 
where, N is the total number of words in the 
corpus. The nouns having the ?2 value greater 
than a threshold are extracted as entity descrip-
tors. 
In person entity extraction subtask, for each 
sentence has the recognized person name, the 
system matches its neighboring nouns (-2 to +2 
words surrounding the person name) with the 
entries in entity descriptor lexicon. The matched 
entity descriptors are extracted.  
In this part, several heuristic rules are applied 
to handle some non-neighboring cases. Two ex-
ample rules with cases are given below. 
Example Rule 1. The prefix entity descriptor 
will be assigned to parallel person names with 
the split mark of ?/? , ???and ???,???(and). 
??????? /??  (Chinese players 
Gong Yuechun/Wang Hui)?>  
?? player-??? Gong Yuechun 
?? player-?? Wang Hui 
Example Rule 2. The entity descriptor will be 
assigned to each person in the structure of paral-
lel person name following ??(etc.)? and then a 
entity word. 
???????????????????
??? (The painter, Liu Bingsen, Chen Daz-
hang, Li Yan, Jin Hongjun, etc., paint a.. ) -> 
??? Liu Bingsen - ??? painter 
??? Chen Dazhang - ??? painter 
?? Li Yan - ??? painter 
??? Jin Hongjun - ??? painter 
Furthermore, the HITSZ_CITYU system ap-
plies several rules to identify a special kind of 
person entity, i.e. the reporter or author using 
structure information. For example, in the be-
ginning or the end of a document, there is a per-
son name in a bracket means this person and this 
name appear in the document for only once; 
such person name is regarded as the reporter or 
author. (????????) ?>??? Jin Lin-
peng - ?? reporter 
(??? ??) ?>??? Jin Linpeng - ?? 
reporter 
4.2 Clustering-based Person Name Disam-
biguation 
For the document set corresponding to each giv-
en index person name, we firstly split the docu-
ment set into: (1) Discarded subset, (2) Subset 
with different recognized person name. The sub-
sets are further split into (2-1) the person is the 
author/reporter and (2-2) the person is not the 
author/reporter. The clustering techniques are 
then applied to group documents in each (2-2) 
subset into several clusters which each cluster is 
corresponding to each identical person.  
In the Chinese Person Name Disambiguation 
task, the number of clusters contained in a subset 
is not pre-available. Thus, the clustering method 
which fixes the number of clusters, such as k-
nearest neighbor (k-NN) is not applicable. Con-
sidering that Agglomerative Hierarchical Clus-
tering (AHC) algorithm doesn?t require the fixed 
number of cluster and it performs well in docu-
ment categorization (Jain and Dubes 1988), it is 
adopted in HITSZ_CITYU system. 
Preprocessing and Document Representation 
Before representing documents, a series of pro-
cedures are adopted to preprocess these docu-
ments including stop word removal. Next, we 
select feature words for document clustering. 
Generally, paragraphs containing the target per-
son name usually contain more person-related 
information, such as descriptor, occupation, af-
filiation, and partners. Therefore, larger weights 
should be assigned to these words. Furthermore, 
we further consider the appearance position of 
the features. Intuitively, local feature words with 
small distance are more important than the glob-
al features words with longer distance. 
We implemented some experiments on the 
training data to verify our point. Table 2 and Ta-
ble 3 show the clustering performance achieved 
using different combination of global features 
and local features as well as different similarity 
thresholds.  
Table 2. Performance achieved on training set 
with different weights (similarity threshold 0.1) 
Feature words Precision Recall F-1 
Paragraph 0.820 0.889 0.849 
All 0.791 0.880 0.826 
All+ Paragraph?1 0.791 0.904 0.839 
All+ Paragraph?2 0.802 0.908 0.848 
All+ Paragraph?3 0.824 0.909 0.860 
All+ Paragraph?4 0.831 0.911 0.865 
All+ Paragraph?5 0.839 0.910 0.869 
All+ Paragraph?6 0.833 0.905 0.864 
All+ Paragraph?7 0.838 0.904 0.867 
 
Table 3. Performance achieved on training set 
with different weights (similarity threshold 0.15) 
Feature words Precision Recall F-1 
Paragraph 0. 901       0.873        0.883 
All 0.859        0.867 0.859 
All+ Paragraph?1 0.875 0.887 0.877 
All+ Paragraph?2 0.885 0.890 0.884 
All+ Paragraph?3 0.889 0.887 0.885 
All+ Paragraph?4 0.896 0.887 0.880 
All+ Paragraph?5 0.906 0.882 0.891 
All+ Paragraph?6 0.905 0.884 0.891 
All+ Paragraph?7 0.910 0.882 0.893 
In this two tables, ?Paragraph? means that we 
only select words containing in paragraph which 
contains the person index name as feature words 
(which are the local features), and ?All? means 
that we select all words but stop words in a doc-
ument as feature words. ?All+ Paragraph?k? 
means feature words consist of two parts, one 
part is obtained from ?All?, the other is gained 
from ?Paragraph?, at the same time, we assign 
the feature weights to the two parts, respectively. 
The feature weight coefficient of ?All? is 
)1(1 +k , while the feature weight coefficient of 
?All+ Paragraph?k? is )1( +kk . 
It is shown that, the system perform best using 
appropriate feature weight coefficient distribu-
tion. Therefore, we select all words in the docu-
ment (besides stop words) as global feature 
words and the words in paragraph having the 
index person name as local feature words. We 
then assign the corresponding empirical feature 
weight coefficient to the global/local features, 
respectively. A document is now represented as 
a vector of feature words as follows: 
)))(,());(,());(,(()( 2211 dwtdwtdwtdV nnL?   (2) 
where, d is a document, it  is a feature word, 
)(dwi  is the feature weight of it  in the document 
d . In this paper, we adopt a widely used weight-
ing scheme, named Term Frequency with In-
verse Document Frequency (TF-IDF). In addi-
tion, for each document, we need to normalize 
weights of features because documents have dif-
ferent lengths. The weight of word it in docu-
ment d  is shown as: 
 
? +
+?
=
=
n
i i
i
i
i
i
df
N
dtf
df
N
dtf
dw
1
2))05.0log(*)((
)05.0log()(
)(
        (3)
 
where )(dtf i means how many times word it oc-
curs in the document d , idf  means how many 
documents contains word it , and N  is the num-
ber of documents in the corpus. 
Similarity Estimation 
We use the cosine distance as similarity calcula-
tion function. After the normalization of weights 
of each document, the similarity between docu-
ment 1d  and document 2d  is computed as: 
? ?=
?? 21
2121 )()(),(
ddit
ii dwdwddsim   (4) 
where it  is the term which appears in document 
1d  and document 2d  simultaneously, )( 1dwi  and 
)( 1dwi  are the weights of it  in document 1d  and  
document 2d  respectively. If it  does not appear 
in a document, the corresponding weight in the 
document is zero. 
Agglomerative Hierarchical Clustering (AHC) 
AHC is a bottom-up hierarchical clustering 
method. The framework of AHC is described as 
follows: 
Assign each document to a single cluster. 
Calculate all pair-wise similarities between 
clusters. 
Construct a distance matrix using the similari-
ty values.  
Look for the pair of clusters with the largest 
similarity.  
Remove the pair from the matrix and merge 
them. 
Evaluate all similarities from this new cluster 
to all other clusters, and update the matrix. 
Repeat until the largest similarity in the matrix 
is smaller than some similarity criteria. 
There are three methods to estimate the simi-
larity between two different clusters during the 
cluster mergence: single link method, average 
link method and complete link method (Nallapati 
et al 2004). The three methods define the similar-
ity between two clusters 1c  and 2c  as follows: 
Single link method: The similarity is the 
largest of all similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
),(max),(
2,1
21 ji
cjdcid
ddsimccsim
??
=      (5) 
Average link method: The similarity is the 
average of the similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
21
1 2
21
),(
),(
cc
ddsim
ccsim
cid cjd
ji
?
? ?
= ? ?         (6) 
Complete link method: The similarity is the 
smallest of all similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
),(min),(
2,1
21 ji
cjdcid
ddsimccsim
??
=       (7) 
where, id   and jd   are the documents belongs 
to clusters 1c  and 2c , respectively.  
We evaluated the AHC algorithm with the 
above three link methods. The achieved perfor-
mance are given in Table 4. It is shown that the 
system performs best with the complete link me-
thod. Therefore, the complete link method is 
selected for the bakeoff testing. 
Table 4. Performance achieved on training set 
with different link method 
Similarity 
threshold 
Link method Precision Recall F1 
0.1 Single link 0.048 1.000 0.089 
0.1 Average link 0.839 0.910 0.869 
0.1 Complete link 0.867 0.888 0.874 
0.15 Single link 0.048 1.000 0.089 
0.15 Average link 0.906 0.882 0.891 
0.15 Complete link 0.923 0.868 0.891 
5 Evaluations 
The task organizer provides two set of evalua-
tion criteria. They are purity-based score (usual-
ly used in IR), B-cubed score (used in WePS-2), 
respectively. The details of the evaluation crite-
ria are given in the task overview.  
The performance achieved by the top-3 sys-
tems are shown in Table 5. 
Table 5. Performance of Top-3 Systems 
 B-Cubed P-IP 
System Precision Recall F1 Precision Recall F1 
NEU 0.957 0.883 0.914 0.969 0.925 0.945
HITSZ 0.839 0.932 0.874 0.885 0.958 0.915
DLUT 0.826 0.913 0.863 0.879 0.942 0.907
 
The evaluation results show that the 
HITSZ_CITYU system achieved overall F1 per-
formance of 0.8742(B-Cubed)/ 0.915(P-IP), re-
spectively.   
It is also shown that HITSZ_CITYU achieves 
the highest the recall performance. It shows that 
the proposed system is good at split the docu-
ment to different identical persons. Meanwhile, 
this system should improve the capacity on 
merge small clusters to enhance the precision 
and overall performance. 
6 Conclusions 
The presented HITSZ_CITYU system applies 
multi-segmentor and unsupervised clustering to 
achieve good accuracy on person name string 
recognition. The system then incorporates entity 
descriptor extraction, feature word extraction 
and agglomerative hierarchical clustering me-
thod for person name disambiguation. The 
achieved encouraging performance shown the 
high performance word segmentation/name rec-
ognition and extraction-based technique are 
helpful to improve the cluster-based person 
name disambiguation. 
References 
Andrea Heyl and G?nter Neumann. DFKI2: An In-
formation Extraction based Approach to People 
Disambiguation. Proceedings of ACL SEMEVAL 
2007, 137-140, 2007. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine, The 
SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task, Pro-
ceedings of Semeval 2007, Association for Com-
putational Linguistics, 2007. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 
?WePS 2 Evaluation Campaign: Overview of the 
Web People Search Clustering Task, In 2nd Web 
People Search Evaluation Workshop (WePS 2009), 
18th WWW Conference, 2009 
Bekkerman, Ron and McCallum, Andrew, Disambi-
guating Web Appearances of People in a Social 
Network, Proceedings of WWW2005, pp.463-470, 
2005 
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen 
Kan, and Dongwon Lee. PSNUS: Web People 
Name Disambiguation by Simple Clustering with 
Rich Features. Proceedings of ACL SEMEVAL 
2007, 268-271, 2007. 
Fei Song, Robin Cohen, Song Lin, Web People 
Search Based on Locality and Relative Similarity 
Measures, Proceedings of WWW 2009 
Fleischman M. B. and Hovy E., Multi-document Per-
son Name Resolution, Proceedings of ACL-42, 
Reference Resolution Workshop, 2004 
Hui Han , Lee Giles , Hongyuan Zha , Cheng Li , 
Kostas Tsioutsiouliklis, Two Supervised Learning 
Approaches for Name Disambiguation in Author 
Citations, Proceedings of the 4th ACM/IEEE-CS 
joint conference on Digital libraries, 2004 
Jain, A. K. and Dubes, R.C. Algorithms for Cluster-
ing Data, Prentice Hall, Upper Saddle River, N.J., 
1988  
Nallapati, R., Feng, A., Peng, F., Allan, J., Event 
Threading within News Topics, Proceedings of-
CIKM 2004, pp. 446?453, 2004 
Niu, Cheng, Wei Li, and Rohini K. Srihari,Weakly 
Supervised Learning for Cross-document Person 
Name Disambiguation Supported by Information 
Extraction, Proceedings of ACL 2004 
Pedersen, Ted, Amruta Purandare, and Anagha Kul-
karni, Name Discrimination by Clustering Similar 
Contexts, Proceedings of the Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico, 
2005 
Pedersen, Ted and Anagha Kulkarni, Unsupervised 
Discrimination of Person Names in Web Contexts, 
Proceedings of the Eighth International Confe-
rence on Intelligent Text Processing and Computa-
tional Linguistics, Mexico City, 2007. 
Rao, Delip, Nikesh Garera and David Yarowsky, 
JHU1: An Unsupervised Approach to Person 
Name Disambiguation using Web Snippets, In 
Proceedings of ACL Semeval 2007 
Sekine, Satoshi and Javier Artiles. WePS 2 Evalua-
tion Campaign: overview of the Web People 
Search Attribute Extraction Task, Proceedings of 
2nd Web People Search Evaluation Workshop 
(WePS 2009), 18th WWW Conference, 2009  
Xin Li, Paul Morie, and Dan Roth, Robust Reading: 
Identification and Tracing of Ambiguous Names, 
Proceedings of NAACL,pp. 17-24, 2004. 
Ying Chen, Sophia Yat Mei Lee, Chu-Ren Huang, 
PolyUHK: A Robust Information Extraction Sys-
tem for Web Personal Names, Proceedings of 
WWW 2009 
Ying Chen and Martin J.H. CU-COMSEM: Explor-
ing Rich Features for Unsupervised Web Personal 
Name Disambiguation, Proceedings of ACL Se-
meval 2007 
 
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 182?188,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Instance Level Transfer Learning for Cross Lingual Opinion Analysis
Ruifeng Xu, Jun Xu and Xiaolong Wang
Key Laboratory of Network Oriented Intelligent Computation
Department of Computer Science and Technology
Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China
{xuruifeng,xujun}@hitsz.edu.cn, wangxl@insun.hit.edu.cn
Abstract
This paper presents two instance-level transfer
learning based algorithms for cross lingual
opinion analysis by transferring useful
translated opinion examples from other
languages as the supplementary training
data for improving the opinion classifier in
target language. Starting from the union of
small training data in target language and
large translated examples in other languages,
the Transfer AdaBoost algorithm is applied
to iteratively reduce the influence of low
quality translated examples. Alternatively,
starting only from the training data in target
language, the Transfer Self-training algorithm
is designed to iteratively select high quality
translated examples to enrich the training
data set. These two algorithms are applied to
sentence- and document-level cross lingual
opinion analysis tasks, respectively. The
evaluations show that these algorithms
effectively improve the opinion analysis by
exploiting small target language training data
and large cross lingual training data.
1 Introduction
In recent years, with the popularity of Web 2.0,
massive amount of personal opinions including
comments, reviews and recommendations in dif-
ferent languages have been shared on the Internet.
Accordingly, automated opinion analysis has
attracted growing attentions. Opinion analysis, also
known as sentiment analysis, sentiment classifica-
tion, and opinion mining, aims to identify opinions
in text and classify their sentiment polarity (Pang
and Lee, 2008).
Many sentiment resources such as sentiment
lexicons (e.g., SentiWordNet (Esuli and Sebastiani,
2006))and opinion corpora (e.g., MPQA (Blitzer
et al, 2007)) have been developed on different
languages in which most of them are for English.
The lack of reliably sentiment resources is one
of the core issues in opinion analysis for other
languages. Meanwhile, the manually annotation
is costly, thus the amount of available annotated
opinion corpora are still insufficient for supporting
supervised learning, even for English. These facts
motivate to ?borrow? the opinion resources in one
language (source language, SL) to another language
(target language, TL) for improving the opinion
analysis on the target language.
Cross lingual opinion analysis (CLOA) tech-
niques are investigated to improve opinion analysis
in TL through leveraging the opinion-related
resources, such as dictionaries and annotated
corpus in SL. Some CLOA works used bilingual
dictionaries (Mihalcea et al, 2007), or aligned
corpus (Kim and Hovy, 2006) to align the expres-
sions between source and target languages. These
works are puzzled by the limited aligned opinion
resources. Alternatively, some works used machine
translation system to do the opinion expression
alignment. Banea et al (2008) proposed several
approaches for cross lingual subjectivity analysis by
directly applying the translations of opinion corpus
in source language to train the opinion classifier
on target language. Wan (2009) combined the
annotated English reviews, unannotated Chinese
reviews and their translations to co-train two
separate classifiers for each language, respectively.
182
These works directly used all of the translation of
annotated corpus in source language as the training
data for target language without considering the
following two problems: (1) the machine translation
errors propagate to following CLOA procedure; (2)
The annotated corpora from different languages are
collected from different domains and different writ-
ing styles which lead the training and testing data
having different feature spaces and distributions.
Therefore, the performances of these supervised
learning algorithms are affected.
To address these problems, we propose two
instance level transfer learning based algorithms
to estimate the confidence of translated SL ex-
amples and to transfer the promising ones as
the supplementary TL training data. We firstly
apply Transfer AdaBoost (TrAdaBoost) (Dai et
al., 2007) to improve the overall performance with
the union of target and translated source language
training corpus. A boosting-like strategy is used
to down-weight the wrongly classified translated
examples during iterative training procedure. This
method aims to reduce the negative affection of low
quality translated examples. Secondly, we propose
a new Transfer Self-training algorithm (TrStr). This
algorithm trains the classifier by using only the
target language training data at the beginning. By
automatically labeling and selecting the translated
examples which is correct classified with higher
confidence, the classifier is iteratively trained by
appending new selected training examples. The
training procedure is terminated until no new
promising examples can be selected. Differen-
t from TrAdaBoost, TrStr aims to select high
quality translated examples for classifier training.
These algorithms are evaluated on sentence- and
document-level CLOA tasks, respectively. The
evaluations on simplified Chinese (SC) opinion
analysis by using small SC training data and large
traditional Chinese (TC) and English (EN) training
data, respectively, show that the proposed transfer
learning based algorithms effectively improve the
CLOA. Noted that, these algorithms are applicable
to different language pairs.
The rest of this paper is organized as follows.
Section 2 describes the transfer learning based
approaches for opinion analysis. Evaluations and
discussions are presented in Section 3. Finally,
Section 4 gives the conclusions and future work.
2 CLOA via Transfer Learning
Given a large translated SL opinion training data,
the objective of this study is to transfer more high
quality training examples for improving the TL
opinion analysis rather than use the whole translated
training data. Here, we propose to investigate the
instance level transfer learning based approaches.
In the case of transfer learning, the set of trans-
lated training SL examples is denoted by Ts =
{(xi, yi)}ni=1, and the TL training data is denoted
by Tt={(xi, yi)}n+mi=n+1, while the size of Tt is much
smaller than that of Ts, i.e., |m| ? |n|. The idea
of transfer learning is to use Tt as the indicator to
estimate the quality of translated examples. By
appending selected high quality translated examples
as supplement training data, the performance of
opinion analysis on TL is expected to be enhanced.
2.1 The TrAdaBoost Approach
TrAdaBoost is an extension of the AdaBoost
algorithm (Freund and Schapir, 1996). It uses
boosting technique to adjust the sample weight
automatically (Dai et al, 2007). TrAdaBoost joins
both the source and target language training data
during learning phase with different re-weighting
strategy. The base classifier is trained on the
union of the weighted source and target examples,
while the training error rate is measured on the
TL training data only. In each iteration, for a SL
training example, if it is wrongly classified by prior
base classifier, it tends to be a useless examples
or conflict with the TL training data. Thus, the
corresponding weight will be reduced to decrease
its negative impact. On the contrary, if a TL training
example is wrongly classified, the corresponding
weight will be increased to boost it. The ensemble
classifier is obtained after several iterations.
In this study, we apply TrAdaBoost algorithm
with small revision to fit the CLOA task, as de-
scribed in Algorithm 1. Noted that, our revised
algorithm can handle multi-category problem which
is different with original TrAdaBoost algorithm for
binary classification problem only. More details and
theoretical analysis of TrAdaBoost are given in Dai
et al?s work (Dai et al, 2007).
183
Algorithm 1 CLOA with TrAdaBoost.
Input: Ts, translated opinion training data in SL,
n= |Ts|; Tt, training data in TL , m= |Tt|; L,
base classifier; K, iteration number.
1: Initialize the distribution of training samples:
D1(i) = 1/(n+m).
2: for each k ? [1,K] do
3: Get a hypothesis hk by training L with the
combined training set Ts ? Tt using distribu-
tion Dk: hk = L(Ts ? Tt, Dk).
4: Calculate the training error of hk on Tt:
?t =
?n+m
i=n+1
Dk(i)?I[hk(xi) ?=yi]
?n+m
i=n+1 Dk(i)
.
5: if ?t = 0 or ?k ? 1/2 then
6: K = k ? 1, break.
7: end if
8: Set ?k = ?k/(1? ?k), ? = 1/(1 +
?
2 lnn
K ).
9: if hk(xi) ?= yi then
10: Update the distribution:
Dk+1(i) =
{ Dk(i)?
Zk
1 ? i ? n
Dk(i)/?k
Zk
n + 1 ? i ? n + m
, where
Zk is a normalization constant and
?n+m
i=1 Dk+1(i) = 1.
11: end if
12: end for
Output: argmaxy
?K
?K/2?I[hk(x) = y]log(1/?k)
/* I[?] is an indicator function, which equals 1 if the
inner expression is true and 0 otherwise.*/
2.2 The Transfer Self-training Approach
Different from TrAdaBoost which focuses on the
filtering of low quality translated examples, we
propose a new Transfer Self-training algorithm
(TrStr) to iteratively train the classifier through
transferring high quality translated SL training data
to enrich the TL training data. The flow of this
algorithm is given in Algorithm 2.
The algorithm starts from training a classifier
on Tt. This classifier is then applied to Ts, the
translated SL training data. For each category in
Ts (subjective/objective or positive/negative in our
different experiments), top p correctly classified
translated examples are selected. These translated
examples are regarded as high quality ones and thus
they are appended in the TL training data. Next, the
classifier is re-trained on the enriched training data.
The updated classifier is applied to SL examples
again to select more high quality examples. Such
Algorithm 2 CLOA with Transfer Self-training.
Input: Ts, translated opinion training data in SL,
n= |Ts|; Tt, training data in TL , m= |Tt|; L,
base classifier; K, iteration number.
1: T0 = Tt, k = 1.
2: Get a hypothesis hk by training a base classifier
L with the training set Tk?1.
3: for each instance (xi, yi) ? Ts do
4: Use hk to label (xi, yi) .
5: if ht(xi) = yi then
6: Add (xi, yi)to T ?
7: end if
8: end for
9: Choose p instances per class with top confi-
dence from T ? and denote the set as Tp.
10: Tk = Tk?1
?
Tp, Ts = Ts ? Tp.
11: k = k + 1.
12: Iterate K times over steps 2 to 11 or repeat until
Tp = ?.
Output: Final classifier by using the enriched train-
ing set Tk.
procedure terminates until the increments are less
than a specified threshold or the maximum number
of iterations is exceeded. The final classifier is
obtained by training on the union of target data and
selected high quality translated SL training data.
3 Evaluation and Discussion
The proposed approaches are evaluated on sentence-
and document-level opinion analysis tasks in the
bi-lingual case, respectively. In our experiments,
the TL is simplified Chinese (SC) and the SL for
the two experiments are English (EN)/traditional
Chinese (TC) and EN, respectively.
3.1 Experimental Setup
3.1.1 Datasets
In the sentence-level opinionated sentence recog-
nition experiment , the dataset is from the NTCIR-7
Multilingual Opinion Analysis Tasks (MOAT) (Se-
ki et al, 2008) corpora. The information of
this dataset is given in Table 1. Two experi-
ments are performed. The first one is denoted by
SenOR : TC ? SC, which uses TCs as source
language training dataset, while the second one
184
is SenOR : EN ? SC, which uses ENs1. SCs
is shrunk to different scale as the target language
training corpus by random. The opinion analysis
results are evaluated with simplified Chinese testing
dataset SCt under lenient and strict evaluation
standard 2, respectively, as described in (Seki et al,
2008).
Note Lang. Data Total subjective/objectiveLenient Strict
SCs SC Training 424 130/294 \SCt Test 4877 1869/3008 898/2022
TCs TC Training 1365 740/625 \
ENs EN Training 1694 648/1046 \
Table 1: The NTCIR-7 MOAT Corpora(unit:sentence).
In the document-level review polarity classifi-
cation experiment,, we used the dataset adopted
in (Wan, 2009). Its English subset is collected by
Blitzer et al (2007), which contains a collection of
8,000 product reviews about four types of products:
books, DVDs, electronics and kitchen appliances.
For each type of products, there are 1,000 positive
reviews and 1,000 negative ones, respectively. The
Chinese subset has 451 positive reviews and 435
negative reviews of electronics products such as
mp3 players, mobile phones etc. In our experiments,
the Chinese subset is further split into two parts
randomly: TL training dataset and test set. The
cross lingual review polarity classification task is
then denoted by DocSC: EN?SC.
In this study, Google Translate3 is choose for pro-
viding machine translation results.
3.1.2 Base Classifier and Baseline Methods
This study focus on the approaches improving the
opinion analysis by using cross lingual examples,
while the classifier improving on target language is
not our major target. Therefore, in the experiments,
a Support Vector Machines (SVM) with linear
kernel is used as the base classifier. We use the
1There are only 248 sentences in NTCIR-7 MOAT English
training data set. It is too small to use for CLOA. We s-
plit some samples from the test set to build a new English
dataset for training, which contains all sentences from topics:
N01,N02,T01,N02,N03,N04,N05,N06 and N07.
2All sentences are annotated by 3 assessors, strict standard
means all 3 assessors have the same annotation and lenient
means any 2 of them have the same annotation.
3http://translate.google.com/
open source SVM package ?LIBSVM(Chang and
Lin, 2001) with all default parameters. In the
opinionated sentence recognition experiment, we
use the presences of following linguistic features
to represent each sentence example including
opinion word, opinion operator, opinion indicator,
the unigram and bigram of Chinese words. It is
developed with the reference of (Xu et al, 2008).
In the review polarity classification experiment, we
use unigram, bigram of Chinese words as features
which is suggested by (Wan, 2009). Here, document
frequency is used for feature selection. Meanwhile,
term frequency weighting is chosen for document
representation.
In order to investigate the effectiveness of the
two proposed transfer learning approaches, they
are compared with following baseline methods: (1)
NoTr(T), which applies SVM with only TL training
data; (2) NoTr(S),which applies SVM classifier with
only the translated SL training data; (3) NoTr(S&T),
which applies SVM with the union of TL and SL
training data.
3.1.3 Evaluation Criteria
Accuracy (Acc), precision (P), recall (R) and F-
measure (F1) are used as evaluation metrics. All the
performances are the average of 10 experiments.
3.2 Experimental Results and Discussion
Here, the number of iterations in TrAdaBoost is set
to 10 in order to avoid over-discarding SL examples.
3.2.1 Sentence Level CLOA Results
The achieved performance of the opinionated
sentence recognition task under lenient and strict
evaluation are given in Table 2 respectively, in
which only 1/16 target train data is used as Tt.
It is shown that NoTr(T) achieves a acceptable
accuracy, but the recall and F1 for ?subjective?
category are obviously low. For the two sub-tasks,
i.e. SenOR : TC ?SC and SenOR :EN ?SC
tasks, the accuracies achieved by NoTr(S&T) are
always between that of NoTr(T) and NoTr(S).
The reason is that some translated examples from
source language may likely conflict with the target
language training data. It is shown that the direct
use of all of the translated training data is infeasible.
It is also shown that our approaches achieve better
185
Method Sub-task
Lenient Evaluation Strict Evaluation
Acc subjective objective Acc subjective objectiveP R F1 P R F1 P R F1 P R F1
NoTr(T) .6254 .534 .3468 .355 .6824 .7985 .7115 .6922 .5259 .3900 .3791 .7725 .8264 .7776
NoTr(S)
TC
?
SC
.6059 .4911 .7828 .6035 .7861 .4960 .6082 .6448 .4576 .8352 .5912 .8845 .5603 .6860
NoTr(S&T) .6101 .4943 .7495 .5957 .7711 .5236 .6235 .6531 .4632 .8051 .588 .8714 .5856 .7004
TrAdaBoost .6533 .5335 .7751 .6314 .8063 .5777 .6720 .7184 .5273 .8473 .6494 .9077 .6611 .7643
TrStr .6625 .5448 .7309 .6238 .7884 .6199 .6934 .7304 .5414 .8182 .6511 .896 .6914 .7801
NoTr(S)
EN
?
SC
.6590 .5707 .4446 .4998 .6966 .7922 .7413 .7390 .5872 .5100 .5459 .7944 .8408 .8169
NoTr(S&T) .6411 .5292 .5759 .5515 .7212 .6817 .7009 .7105 .5254 .608 .5637 .8129 .7560 .7834
TrAdaBoost .6723 .5988 .4371 .5018 .7019 .8184 .7549 .7630 .6485 .5019 .5614 .8002 .8789 .8371
TrStr .6686 .5691 .5746 .5678 .7360 .7271 .7292 .7484 .589 .6276 .6026 .8315 .8021 .8147
Table 2: The Performance of Opinionated Sentence Recognition Task.
performance on both tasks while few TL training
data is used. In which, TrStr performances the
best on SenOR:TC?SC task while TrAdaBoost
outperforms other methods on SenOR :EN?SC
task. The proposed transfer learning approaches
enhanced the accuracies achieved by NoTr(S&T)
for 4.2-8.6% under lenient evaluation and 5.3-11.8%
under strict evaluation, respectively.
3.2.2 Document Level CLOA Results
Method Acc positive negativeP R F1 P R F1
NoTr(T) .7542 .7447 .8272 .7747 .8001 .6799 .7235
NoTr(S) .7122 .6788 .8248 .7447 .7663 .5954 .6701
NoTr(S&T) .7531 .714 .8613 .7801 .8187 .6415 .7179
TrAdaBoost .7704 .8423 .6594 .7376 .7285 .8781 .7954
TrStr .7998 .8411 .7338 .7818 .7727 .8638 .8144
Table 3: The Results of Chinese Review Polarity Classi-
fication Task (Features:Unigrams; m=20).
Method Acc positive negativeP R F1 P R F1
NoTr(T) .7518 .7399 .8294 .7741 .7983 .6726 .7185
NoTr(S) .7415 .7143 .8204 .7637 .7799 .6598 .7148
NoTr(S&T) .7840 .7507 .8674 .8035 .8385 .6982 .7592
TrAdaBoost .7984 .8416 .7297 .7792 .7707 .8652 .8138
TrStr .8022 .8423 .7393 .7843 .7778 .8634 .8164
Table 4: The Results of Chinese Review Polarity Classi-
fication Task (Features:Unigrams+Bigrams; m=20).
Table 3 and Table 4 give the achieved results of
different methods on the task DocSC : EN?SC
by using 20 Chinese annotated reviews as Tt. It is
shown that transfer learning approaches outperform
other methods, in which TrStr performs better than
TrAdaBoost when unigram+bigram features are
used. Compared to NoTr(T&S), the accuracies
are increased about 1.8-6.2% relatively. Overall,
the transfer learning approaches are shown are
beneficial to TL polarity classification.
3.2.3 Influences of Target Training Corpus Size
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(a) SenOR : TC ? SC
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(b) SenOR : EN ? SC
Figure 1: Performances with Different Size of SCs on
Opinionated Sentence Recognition Task under Lenient E-
valuation
In order to estimate the influence of different size
of TL training data, we conduct a set of experiments
on both tasks. Fig 1 and Fig 2 show the influence
186
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  20  30  40  50  60  70  80  90  100
Ac
cu
ra
cy
Number of Target Training Instances
Transfer Self-training
TrAdaBoost
NoTr(S&T)
NoTr(T)
(a) Unigrams
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  20  30  40  50  60  70  80  90  100
Ac
cu
ra
cy
Number of Target Training Instances
Transfer Self-training
TrAdaBoost
NoTr(S&T)
NoTr(T)
(b) Unigrams+Bigrams
Figure 3: Performances with Different Number of TL Training Instances on Task of DocSC: EN?SC
 0.6
 0.65
 0.7
 0.75
 0.8
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(a) SenOR : TC ? SC
 0.6
 0.65
 0.7
 0.75
 0.8
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(b) SenOR : EN ? SC
Figure 2: Performances with Different Size of SCs on
Opinionated Sentence Recognition Task under Strict E-
valuation
on the opinionated sentence recognition task under
lenient and strict evaluation respectively. Fig 3
shows the influence on task DocSC : EN ?SC.
Fig 3(a) shows the results use unigram features
and Fig 3(b) uses both unigrams and bigrams. It is
observed that TrAdaBoost and TrStr achieve better
performances than the baseline NoTr(S&T) in most
cases. More specifically, TrStr performs the best
when few TL training data is used. When more TL
training data is used, the performance improvements
by transfer learning approaches become small. The
reason is that less target training data is helpful to
transfer useful knowledge in translated examples.
If too much TL training data is used, the weights
of SL instances may decrease exponentially after
several iterations, and thus more source training
data is not obviously helpful.
4 Conclusions and Future Work
To address the problems in CLOA caused by inac-
curate translations and different domain/category
distributions between training data in different
languages, two transfer learning based algorithms
are investigated to transfer promising translated SL
training data for improving the TL opinion analysis.
In this study, Transfer AdaBoost and Transfer
Self-Training algorithms are investigated to reduce
the influences of low quality translated examples
and to select high quality translated examples,
respectively. The evaluations on sentence- and
document-level opinion analysis tasks show that the
proposed algorithms improve opinion analysis by
using the union of few TL training data and selected
cross lingual training data.
One of our future directions is to develop other
transfer leaning algorithms for CLOA task. Another
future direction is to employ other moderate weight-
ing scheme on source training dataset to reduce the
over-discarding of training examples from source
language.
187
References
Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends
in Information Retrieval, 2(1?2):1?135.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. Proceedings of the 5th Inter-
national Conference on Language Resources and
Evaluation, 417?422.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and e-
motions in language. Language Resources and E-
valuation, 39(2?3):165?210.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. Proceedings of the
45th Annual Meeting of the Association of Com-
putational Linguistics, Prague, Czech Republic.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. Proceedings of
HLT/NAACL-2006, 200?207.
Carmen Banea, Rada Mihalcea, Janyce Wiebe and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, Honolulu, Hawaii,
127?135.
Xiaojun Wan. 2009. Co-training for cross-lingual
sentiment classification. Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP
of the AFNLP, Suntec, Singapore, 235?243.
Wenyuan Dai ,Qiang Yang, GuiRong Xue and Yong
Yu. 2007. Boosting for transfer learning. Pro-
ceedings of the 24th International Conference on
Machine Learning, 193?200.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: domain adaptation for sentiment classi-
fication. Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics,
440?447.
Xiaojun Wan. 2008. Using bilingual knowledge
and ensemble techniques for unsupervised chi-
nese sentiment analysis. Proceedings of EMNLP
2008,553?561.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. Proceedings
of the 13th International Conference on Machine
Learning, 148?156.
Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kand. 2008. Overview
of multilingual opinion analysis task at NTCIR-7.
Proceeding of NTCIR-7, NII, Tokyo, 185?203.
Ruifeng Xu, Kam-Fai Wong, Qin Lu, and Yunqing
Xia 2008. Learning Multilinguistic Knowledge
for Opinion Analysis. D. S. Huang et al, edi-
tors:Proceedings of ICIC 2008, volume 5226 of L-
NCS, 993?1000, Springer-Verlag.
Chih-Chung Chang and Chih-Jen Lin.
2001. LIBSVM: a library for support
vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
188
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 107?112,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Incorporating Rule-based and Statistic-based Techniques for 
Coreference Resolution 
 
 
Ruifeng Xu, Jun Xu, Jie Liu, Chengxiang Liu, Chengtian Zou, Lin Gui, Yanzhen 
Zheng, Peng Qu
Human Language Technology Group, Key Laboratory of Network Oriented Intelligent 
Computation, Shenzhen Graduate School, Harbin Institute of Technology, China 
{xuruifeng.hitsz;hit.xujun;lyjxcz;matitalk;chsky.zou;monta3pt;
zhyz.zheng;viphitqp@gmail.com} 
 
 
Abstract 
This paper describes a coreference resolution 
system for CONLL 2012 shared task 
developed by HLT_HITSZ group, which 
incorporates rule-based and statistic-based 
techniques. The system performs coreference 
resolution through the mention pair 
classification and linking. For each detected 
mention pairs in the text, a Decision Tree (DT) 
based binary classifier is applied to determine 
whether they form a coreference. This 
classifier incorporates 51 and 61 selected 
features for English and Chinese, respectively. 
Meanwhile, a rule-based classifier is applied to 
recognize some specific types of coreference, 
especially the ones with long distances. The 
outputs of these two classifiers are merged. 
Next, the recognized coreferences are linked to 
generate the final coreference chain. This 
system is evaluated on English and Chinese 
sides (Closed Track), respectively. It achieves 
0.5861 and 0.6003 F1 score on the 
development data of English and Chinese, 
respectively. As for the test dataset, the 
achieved F1 scores are 0.5749 and 0.6508, 
respectively. This encouraging performance 
shows the effectiveness of our proposed 
coreference resolution system. 
1 Introduction 
Coreference resolution aims to find out the 
different mentions in a document which refer to the 
same entity in reality (Sundheim and Beth, 1995; 
Lang et al 1997; Chinchor and Nancy, 1998;). It is 
a core component in natural language processing 
and information extraction.  Both rule-based 
approach (Lee et al 2011) and statistic-based 
approach (Soon et al, 2001; Ng and Cardie, 2002; 
Bengtson and Roth, 2008; Stoyanov et al, 2009; 
Chen et al 2011) are proposed in coreference 
resolution study. Besides the frequently used 
syntactic and semantic features, the more linguistic 
features are exploited in recent works (Versley, 
2007; Kong et al 2010). 
CoNLL-2012 proposes a shared task, ?Modeling 
multilingual unrestricted coreference in the 
OntoNotes? (Pradhan et al 2012). This is an 
extension of the CoNLL-2011 shared task. The 
task involves automatic anaphoric mention 
detection and coreference resolution across three 
languages including English, Chinese and Arabic. 
HLT_HITSZ group participated in the Closed 
Track evaluation on English and Chinese side. This 
paper presents the framework and techniques of 
HLT_HITSZ system which incorporates both rule-
based and statistic-based techniques. In this system, 
the mentions are firstly identified based on the 
provided syntactic information. The mention pairs 
in the document are fed to a Decision Tree based 
classifier to determine whether they form a 
coreference or not. The rule-based classifiers are 
then applied to recognize some specific types of 
coreference, in particular, the long distance ones. 
Finally, the recognized coreference are linked to 
obtain the final coreference resolution results. This 
system incorporates lexical, syntactical and 
semantic features. Especially for English, WordNet 
is used to provide semantic information of the 
mentions, such as semantic distance and the 
107
category of the mentions and so on. Other than the 
officially provided number and gender data, we 
generated some lexicons from the training dataset 
to obtain the values of some features. This system 
achieves 0.5861 and 0.6003 F1 scores on English 
and Chinese development data, respectively, and 
0.5749 and 0.6508 F1 scores on English and 
Chinese testing data, respectively. The achieved 
encouraging performances show that the proposed 
incorporation of rule-based and statistic-based 
techniques is effective. 
The rest of this report is organized as below. 
Section 2 presents the mention detection. Section 3 
presents the coreference determination and Section 
4 presents the coreference linking. The 
experimental results are given in Section 5 in detail. 
Finally, Section 6 concludes this report. 
2 Mention Detection 
In this stage, the system detects the mentions from 
the text. The pairs of these mentions in one 
document are regarded as the coreference 
candidates. Thus, the high recall is a more 
important target than higher precision for this stage. 
Corresponding to English and Chinese, we adopted 
different detection methods, respectively. 
2.1 Mention Detection - English 
HLT_HITSZ system chooses the marked noun 
phrase (NP), pronouns (PRP) and PRP$ in English 
data as the mentions. The system selects most 
named entities (NE) as the mentions but filter out 
some specific types. Firstly, the NEs which cannot 
be labeled either as NP or NML are filter out 
because there are too cases that the pairs of these 
NEs does not corefer even they are in the same 
form as shown in the training dataset. Second, the 
NEs of ORDINAL, PERCENT and MONEY types 
are filtered because they have very low coreference 
ratio (less than 2%). Furthermore, for the cases that 
NPs overlapping a shorter NP, normally, only the 
longer one are choose. An exception is that if the 
shorter NPs are in parallel structures with the same 
level to construct a longer NP. For example, for a 
NP ?A and B?, ?A?, ?B? and ?A and B? as 
regarded ed as three different mentions.   
2.2 Mention Detection ? Chinese 
HLT_HITSZ system extracts all NPs and PNs as 
the mention candidates. For the NPs have the 
overlaps, we handle them in three ways: 1. For the 
cases that two NPs share the same tail, the longer 
NP is kept and the rest discarded; 2. For cases that 
the longer NP has a NR as its tail, the NPs which 
share the same tail are discarded; 3. In MZ and 
NW folders, they are many mentions nested 
marked as the nested co-referent mentions. The 
system selects the longest NP as mention in this 
stage while the other mention candidates in the 
longest NP will be recalled in the post processing 
stage. 
3 Coreference Determination 
Any pair of two detected mentions in one 
document becomes one coreference candidate. In 
this stage, the classifiers are developed to 
determine whether this pair be a coreference or not. 
During the generation of mention pairs, it is 
observed that linking any two mentions in one 
document as candidates leads to much noises.  The 
statistical observation on the Chinese training 
dataset show that 90% corefered mention pairs are 
in the distance of 10 sentences. Similar results are 
found in the English training dataset while the 
context window is set to 5 sentences. Therefore, in 
this stage, the context windows for generating 
mention pairs as coreference candidates for 
English and Chinese are limited to 5 and 10 
sentences, respectively. 
3.1 The Statistic-based Coreference 
Determination 
The same framework is adopted in the statistical-
based coreference determination for English and 
Chinese, respectively, which is based on a machine 
learning-based statistical classifier and selected 
language-dependent features. Through transfer the 
examples in the training test into feature-valued 
space, the classifier is trained. This binary 
classifier will be applied to determine whether the 
input mention pair be a coreference or not. Here, 
we evaluated three machine learning based 
classifiers including Decision Tree, Support Vector 
Machines and Maximum Entropy on the training 
data while Decision Tree perform the best. Thus, 
DT classifier is selected. Since the annotations on 
the training data from different directory show 
some inconsistence, multiple classifiers 
corresponding to each directory are trained 
individually.  
108
3.1.1 Features - English 
51 features are selected for English coreference 
determination. The features are camped to six 
categories. Some typical features are listed below: 
1. Basic features: 
(1) Syntactic type of the two mentions, 
includes NP, NE, PRP, PRP$. Here, only 
the NPs which do not contain any named 
entities or its head word isn?t a named 
entity are considered as an NP while the 
others are discarded. 
(2) If one mention is a PRP or PRP$, use an 
ID to specify which one it is.  
(3) The sentence distance between two 
mentions. 
(4) Whether one mention is contained by 
another one. 
2. Parsing features: 
(1) Whether two mentions belong to one NP. 
(2) The phrase distance between the two 
mentions.  
(3) The predicted arguments which the two 
mentions belong to. 
3. Named entity related features: 
(1) If both of the two mentions may be 
considered as named entities, whether 
they have the same type. 
(2) If one mention is a common NP or PRP 
and another one can be considered as 
named entity, whether the words of the 
common NP or PRP can be used to refer 
this type of named entity. This knowledge 
is extracted from the training dataset. 
(3) Whether the core words of the two named 
entity type NP match each other.    
4. Features for PRP: 
(1) If both mentions are PRP or PRP$, use an 
ID to show what they are. The PRP$ with 
the same type will be assigned the same 
ID, for example, he, him and his. 
(2) Whether the two mentions has the same 
PRP ID. 
5. Semantic Features: 
(1) Whether the two mentions have the same 
headword. 
(2) Whether the two mentions belong to the 
same type. Here, we use WordNet to get 
three most common sense of each NP and 
compare the type they belong to.  
(3) The semantic distance between two 
mentions. WordNet is used here.  
(4) The natures of the two mentions, including 
number, gender, is human or not, and 
match each other or not. We use WordNet 
and a lexicon extracted from the gender 
and number file here. 
6. Document features: 
(1) How many speakers in this document. 
(2) Whether the mention is the first or the last 
sentence of the document. 
(3) Whether the two mentions are from the 
same speaker. 
3.1.2 Features - Chinese 
There are 61 features adopted in Chinese side. 
Because of the restriction of closed crack, most of 
features use the position and POS information. It is 
mentionable that the ways for calculating the 
features values. For instance, the sentence distance 
is not the real sentence distance in the document. 
For instead, the value is the number of sentences in 
which there are at least one mention between the 
mention pair. This ignores the sentences of only 
modal particles.  
The 61 features are camped into five groups. 
Some example features are listed below. 
1. Basic information: 
(1) The matching degree of two mentions 
(2) The word distance of two mentions 
(3) The sentence distance of two mentions 
2. Parsing information: 
(1) Predicted arguments which the two 
mentions belong to and corresponding 
layers. 
3. POS features 
(1) Whether the mention is NR 
(2) Whether the two mentions are both NR 
and are matched 
4. Semantic features: 
(1) Whether the two mention is related 
(2) Whether the two mentions corefer in the 
history. Since the restriction of closed 
track, we did not use any additional 
semantic resources. Here, we extract the 
co-reference history from the training set 
to obtain some semantic information, such 
as ?NN ??? and ?NN ??? corefered in 
the training data, and they are regarded as 
coreference in the testing data.  
5. Document Features: 
109
(1) Whether the two mentions have the same 
speaker. 
(2) Whether the mention is a human. 
(3) Whether the mention is the first mention in 
the sentence. 
(4) Whether the sentence to which the mention 
belongs to is the first sentence. 
(5) Whether the sentence to which the mention 
belongs to is the second sentence 
(6) Whether the sentence to which the mention 
belongs to is the last sentence 
(7) The number of the speakers in the 
document. 
3.2 The Rule-based Coreference 
Determination 
The rule-based classifier is developed to recognize some 
specific types of coreference and especially, the long 
distance ones.  
3.2.1 Rule-based Classifier - English 
To achieve a high precision, only the mention pairs 
of NE-NE (include NPs those can be considered as 
NE) or NP-NP types with the same string are 
classified here.  
For the NE-NE pair, the classifier identifies their 
NE part from the whole NP, if their strings are the 
same, they are considered as coreference. 
For the NP-NP pair, the pairs satisfy the 
following rules are regarded as coreference. 
(1) The POS of the first word isn?t ?JJR? or ?JJ?. 
(2) If NP has only one word, its POS isn?t ?NNS? 
or ?NNPS?. 
(3) The NP have no word like ?every?, ?every-?, 
?none?, ?no?,  ?any?,  ?some?,  ?each?. 
(4) If the two NP has article, they can?t be both 
?a? or ?an?. 
Additionally, for the PRP mention pairs, only 
?I?, ?me?, ?my? with the same speaker can be 
regarded as coreference. 
3.2.2 Rule-based Classifier - Chinese 
A rule-based classifier is developed to determine 
whether the mention pairs between PNs and 
mentions not PN corefer or not. For instance, the 
mention pairs between the PN ??? which is after a 
comma and the mention which is marked as ARG0 
in the same sentence. In the sentence ?????? 
??  ?  ?  ??  ??  ??  ??  ?  ???, 
because the mention pair between ??????? 
and the first ??? match the mentioned above rule, 
it  is classified as a positive one. The result on the 
development set shows that the rule-based 
classifier brings good improvement. 
4 Coreference Chain Construction  
4.1 Coreference Chain Construction-English 
The evaluation on development data shows that the 
achieved precision of our system is better than 
recall.  Thus, in this stage, we simply link every 
pair of mentions together if there is any links can 
link them together to generate the initial 
coreference chain. After that, the mentions have 
the distance longer than 5 sentences are observed. 
The NE-NE or NP-NP mention pairs between one 
known coreference and an observing mention with 
long distance are classified to determine they are 
corefered or not by using a set of rules. The new 
detected conference will be linked to the initial 
coreference chain.  
4.2 Coreference Chain Construction-Chinese 
The coreference chain construction for Chinese is 
similar to English. Furthermore, as mentioned 
above, in MZ and NW folders, there are many 
mentions nested marked as the nested co-
referenced mentions. In this stage, HLT_HITSZ 
system generates the nested co-reference mentions 
for improving the analysis for these two folders. 
Additionally, the system uses some rules to 
improve the coreference chain construction. We 
find that the trained classifier performs poor in co-
reference resolution related to Pronoun. So, most  
rules adopted here are related to these Pronouns: 
????, ???, ???, ???, ???, ????, ????, 
???. We use these rules to bridge the chain of 
pronouns and the chain of other type. 
Although high precision for NT co-reference 
cases are achieved through string matching, the 
recall is not satisfactory. It partially attributes to 
the fact that the flexible use of Chinese. For 
example, to express the year of 1980, we found 
???????, ??????, ? ?????, ???
? ?, ?1980 ? ?. Similar situation happens for 
month (?, ??) and day (?,?), we conclude 
most situations to several templates to improve the 
rule-based conference resolution. 
110
5 Evaluation Results 
5.1 Dataset 
The status of training dataset, development dataset 
and testing dataset in CoNLL 2012 for English and 
Chinese are given in Table 1 and Table 2, 
respectively. 
 Files Sentence Cluster Coreference
Train 1,940 74,852 35,101 155,292 
Development 222 9,603 4,546 19,156 
Test 222 9,479 n/a n/a 
Table 1. Status of CoNLL 2012 dataset - English 
 
 Files Sentence Cluster Coreference
Train 1,391 36,487 28,257 102,854 
Develop 172 6,083 3,875 14,383 
Test 166 4,472 n/a n/a 
Table 2. Status of CoNLL 2012 dataset - Chinese 
5.2 Evaluation on Mention Detection 
Firstly, the mention detection performance is evaluated. 
The performance achieved on the development dataset 
(Gold/Auto) and test data on English and Chinese are 
given in Table 3 and Table 4, respectively. In which, 
Gold means the development dataset with gold 
manually annotation and Auto means the automatically 
generated annotations.  
 Precision Recall F1 
Develop-Gold 0.8499 0.6716 0.7503 
Develop-Auto 0.8456 0.6256 0.7192 
Test 0.8455 0.6264 0.7196 
Table 3. Performance on Mention Detection - English 
 
 Precision Recall F1 
Develop-Gold 0.7402 0.7360 0.7381 
Develop-Auto 0.6987 0.6429 0.6697 
Test 0.7307 0.7502 0.7403 
Table 4. Performance on Mention Detection - Chinese 
 
Generally speaking, our system achieves acceptable 
mention detection performance, but further 
improvements are desired.  
5.3 Evaluation on Coreference Resolution 
The performance on coreference resolution is next 
evaluated. The achieved performances on the 
development data (Gold/Auto) and test dataset on 
English and Chinese are given in Table 5 and Table 6, 
respectively. It is shown that the OF performance drops 
0.0309(Gold) and 0.0112(Auto) from development 
dataset to test dataset on English, respectively. On the 
contrary, the OF performance increases 0.0096(Gold) 
and 0.0505(Auto) from development dataset to test 
dataset on Chinese, respectively. Compared with the 
performance reported in CoNLL2012 shared task, our 
system achieves a good result, ranked 3rd, on Chinese. 
The results show the effectiveness of our proposed 
system. 
 Precision Recall F1 
MUC 0.7632 0.6455 0.6994 
BCUB 0.7272 0.6797 0.7027 
CEAFE 0.3637 0.4840 0.4154 
OF-Develop-Gold   0.6058 
MUC 0.7571 0.5993 0.6691 
BCUB 0.7483 0.6441 0.6923 
CEAFE 0.3350 0.4865 0.3968 
OF-Develop-Auto   0.5861 
MUC 0.7518 0.5911 0.6618 
BCUB 0.7329 0.6228 0.6734 
CEAFE 0.3264 0.4829 0.3895 
OF-Test   0.5749 
Table 5. Performance on Coreference Resolution ? 
English  
 Precision Recall F1 
MUC 0.6892 0.6655 0.6771
BCUB 0.7547 0.7410 0.7478
CEAFE 0.4876 0.5105 0.4988
OF-Develop-Gold   0.6412
MUC 0.6535 0.5643 0.6056
BCUB 0.7812 0.6809 0.7276
CEAFE 0.4322 0.5101 0.4679
OF-Develop-Auto   0.6003
MUC 0.6928 0.6595 0.6758
BCUB 0.7765 0.7328 0.7540
CEAFE 0.5072 0.5390 0.6253
OF-Test(Gold parses)   0.6508
MUC 
BCUB 
CEAFE 
OF-Test-Predicted-mentions 
(Auto parses) 
0.5502 
0.6839 
0.5040 
0.6147
0.7638
0.4481
0.5807
0.7216
0.4744
0.5922
MUC 
BCUB 
CEAFE 
OF-Test-Gold-mention-
boundaries(Auto parses) 
0.6354 
0.7136 
0.5390 
0.6873
0.7870
0.4907
0.6603
0.7485
0.5137
0.6408
MUC 
BCUB 
CEAFE 
OF-Test-Gold-mentions 
(Auto parses) 
0.6563 
0.6505 
0.7813 
0.9407
0.9123
0.4377
0.7732
0.7595
0.5611
0.6979
Table 6. Performance on Coreference Resolution ? 
Chinese 
111
6 Conclusions 
This paper presents the HLT_HITSZ system for 
CoNLL2012 shared task. Generally speaking, this 
system uses a statistic-based classifier to handle 
short distance coreference resolution and uses a 
rule-based classifier to handle long distance cases. 
The incorporation of rule-based and statistic-based 
techniques is shown effective to improve the 
performance of coreference resolution. In our 
future work, more semantic and knowledge bases 
will be incorporated to improve coreference 
resolution in open track. 
 
Acknowledgement 
This research is supported by HIT.NSFIR.201012 
from Harbin Institute of Technology, China and 
China Postdoctoral Science Foundation No. 
2011M500670. 
References  
B. Baldwin. 1997. CogNIAC: High Precision 
Coreference with Limited Knowledge and Linguistic 
Resources. Proceedings of Workshop on Operational 
Factors in Practical, Robust Anaphora Resolution for 
Unrestricted Texts. 
E. Bengtson, D. Roth. 2008. Understanding the Value of 
Features for Coreference Resolution. Proceedings of 
EMNLP 2008, 294-303. 
M. S. Beth M. 1995. Overview of Results of the MUC-6 
Evaluation. Proceedings of the Sixth Message 
Understanding Conference (MUC-6) 
W. P. Chen, M. Y. Zhang, B. Qin,  2011. Coreference 
Resolution System using Maximum Entropy 
Classifier. Proceedings of CoNLL-2011. 
N. A. Chinchor. 1998. Overview of MUC-7/MET-2. 
Proceedings of the Seventh Message Understanding 
Conference (MUC-7). 
F. Kong, G. D. Zhou, L. H. Qian, Q. M. Zhu. 2010. 
Dependency-driven Anaphoricity Determination for 
Coreference Resolution. Proceedings of COLING 
2010, 599-607  
J. Lang, B. Qin, T. Liu. 2007. Intra-document 
Coreference Resolution: The State of the Art. Journal 
of Chinese Language and Computing , 2007, 17( 4) : 
227-253. 
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. 
Surdeanu, D. Jurafsky. 2011. Stanford?s Multi-Pass 
Sieve Coreference Resolution System at the CoNLL-
2011 Shared Task. Proceedings of CoNLL-2011. 
V. Ng and C. Cardie. 2002. Improving Machine 
Learning Approaches to Coreference Resolution. 
Proceedings of ACL 2002. 
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A 
Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational 
Linguistics, 27(4):521-544 
S. Pradhan and A. Moschitti et al 2012. CoNLL-2012 
Shared Task: Modeling Multilingual Unrestricted 
Coreference in OntoNotes. Proceedings of CoNLL 
2012 
V. Stoyanov, N. Gilbert, C. Cardie, E. Riloff. 2009. 
Conundrums in Noun Phrase Coreference Resolution: 
Making Sense of the State-of-the-Art. Proceeding 
ACL 2009  
Y. Versley. 2007. Antecedent Selection Techniques for 
High-recall Coreference Resolution. Proceedings of 
EMNLP/CoNLL 2007. 
Y. Yang, N. W. Xue, P. Anick. 2011. A Machine 
Learning-Based Coreference Detection System For 
OntoNotes.  Proceedings of CoNLL-2011. 
 
 
112
