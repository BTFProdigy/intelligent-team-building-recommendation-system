Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 817?822,
Dublin, Ireland, August 23-24, 2014.
UWB: Machine Learning Approach to Aspect-Based Sentiment Analysis
Tom
?
a
?
s Brychc??n
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
brychcin@kiv.zcu.cz
Michal Konkol
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
konkol@kiv.zcu.cz
Josef Steinberger
Department of Computer
Science and Engineering,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
jstein@kiv.zcu.cz
Abstract
This paper describes our system partici-
pating in the aspect-based sentiment anal-
ysis task of Semeval 2014. The goal
was to identify the aspects of given tar-
get entities and the sentiment expressed to-
wards each aspect. We firstly introduce
a system based on supervised machine
learning, which is strictly constrained and
uses the training data as the only source
of information. This system is then ex-
tended by unsupervised methods for latent
semantics discovery (LDA and semantic
spaces) as well as the approach based on
sentiment vocabularies. The evaluation
was done on two domains, restaurants and
laptops. We show that our approach leads
to very promising results.
1 Introduction
The majority of current sentiment analysis ap-
proaches tries to detect the overall polarity of a
sentence (or a document) regardless of the target
entities (e.g. restaurants) and their aspects (e.g.
food, price). By contrast, the ABSA (aspect based
sentiment analysis) task is concerned with identi-
fying the aspects of given target entities and esti-
mating the sentiment polarity for each mentioned
aspect.
The aspect scenario can be decomposed into
two tasks: aspect extraction and aspect sentiment
classification (Liu, 2012).
The task of aspect extraction is to recognize
aspects of the entity and more generally can be
seen as an information extraction task. The ba-
sic approach is finding frequent nouns and noun
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails: http://creativecommons.org/licenses/
by/4.0/
phrases (Liu et al., 2005; Blair-Goldensohn et
al., 2008; Moghaddam and Ester, 2010; Long et
al., 2010). Aspect extraction can be also seen as
a special case of the general information extrac-
tion problem. The most dominant methods are
based on sequential learning (e.g. HMM ? Hidden
Markov Models (Rabiner, 2010) or CRF ? Condi-
tional Random Fields (Lafferty et al., 2001)). An-
other group of methods use topic models (Mei et
al., 2007; Titov and McDonald, 2008; Blei et al.,
2003).
Aspect sentiment classification determines
whether the opinions on different aspects are
positive, negative, or neutral. While lexicon-based
approaches use a list of aspect-related sentiment
phrases as the core resource (Ding et al., 2008; Hu
and Liu, 2004), the key issue for learning methods
is to determine the scope of each sentiment
expression, i.e., whether it covers the aspect in
the sentence (Jiang et al., 2011; Boiy and Moens,
2009).
The most of the research in aspect-level senti-
ment analysis has been done in English, however,
there were some attempts to tackle the aspect-level
task in other languages (e.g. in Czech (Steinberger
et al., 2014)).
The rest of the article is organized as follows.
In Section 2, we summarize the ABSA shared task
(Pontiki et al., 2014). Then, we give a description
of our participating system (Section 3). In Section
4, we discuss our results in the task. We partic-
ipated with both the constrained and the uncon-
strained variants of the system.
2 The ABSA task
Datasets consisting of customer reviews with
human-authored annotations identifying the men-
tioned aspects of the target entities and the senti-
ment polarity of each aspect were provided. The
experiments were run in two domains: restaurant
and laptop reviews.
817
Each team could submit two versions of sys-
tems ? constrained and unconstrained. The con-
strained system uses only the training data and
other resources (such as lexicons) for training. The
unconstrained system can use additional data.
We use another definition of these types, which
is not against the rules. Our constrained systems
are based purely on ABSA training data, with-
out any external knowledge such as dictionaries or
rules. Our unconstrained systems use additional
dictionaries, rule-based extensions and unlabeled
data. From our point of view, hand-crafted dictio-
naries and rules are external knowledge and thus it
is the same as adding external data.
The task consists of the four subtasks.
2.1 Subtask 1: Aspect term extraction
Given a set of sentences with pre-identified enti-
ties (restaurants or laptops), the task is to identify
the aspect terms present in the sentence and return
a list containing all the distinct aspect terms.
I liked the service and the staff, but not the food.
? {service, staff, food}
2.2 Subtask 2: Aspect term polarity
For a given set of aspect terms within a sentence,
the task is to determine the polarity of each aspect
term: positive, negative, neutral or conflict (i.e.,
both positive and negative).
I hated their fajitas, but their salads were great.
? {fajitas: negative, salads: positive}
2.3 Subtask 3: Aspect category detection
Given a predefined set of aspect categories, the
task is to identify the aspect categories discussed
in a given sentence. Aspect categories are typi-
cally coarser than the aspect terms of Subtask 1,
and they do not necessarily occur as terms in the
given sentence.
For example, the following categories were de-
fined for the restaurants? domain: food, service,
price, ambience and anecdotes/miscellaneous.
The restaurant was expensive, but the menu was
great. ? {price, food}
2.4 Subtask 4: Aspect category polarity
Given a set of pre-identified aspect categories, the
task is to determine the polarity (positive, nega-
tive, neutral or conflict) of each aspect category.
The restaurant was expensive, but the menu was
great. ? {price: negative, food: positive}
3 System description
We use machine learning approach to all subtasks.
For aspect term extraction we use CRF. For the
other three tasks we use the Maximum Entropy
classifier. We use the Brainy (Konkol, 2014) im-
plementation of these algorithms.
During the data preprocessing, we use simple
word tokenizer based on regular expressions. All
tokens are lowercased for tasks 2 and 4.
We will firstly describe all the features used in
this paper because the tasks share some of them.
These features are then referenced in the descrip-
tions of individual subtasks.
Words (W) ? Word occurrence on a given posi-
tion in the context window.
Bag of Words (BoW) ? Occurrence of a word in
a sentence (or context window).
Bigrams (B) ? Bigram occurrence on a given po-
sition in the context window.
Bag of Bigrams (BoB) ? Occurrence of a bigram
in a sentence (or context window).
Tf-idf ? Term frequency?inverse document fre-
quency for all tokens in the sentence.
Learned Dictionary (LD) ? Dictionary of terms
based on training data.
Suffixes (S) ? Suffix of a word (2-4 characters).
Sentiment Dictionary (SD) ? Dictionary created
using semi-automatic triangulation method
(Steinberger et al., 2012). The score is nor-
malized.
Senti Wordnet (SW) ? See (Baccianella et al.,
2010).
LDA ? See Section 3.1.
Word Clusters (WC) ? See Section 3.2. Cluster
occurrence on a given position in the context
window.
Bag of Clusters (BoC) ? Same as word clusters,
but without information about position.
818
We use two features that are not in common
use in similar tasks ? Latent Dirichlet Allocation
and word clusters based on semantic spaces. Both
these features use large amount of unlabeled data
to discover latent semantics. We downloaded the
restaurant reviews from http://opentable.
com. This corpus consists of 409,665 reviews
(documents) with about 27 million words. The
opentable corpus is used as the training data for
these features. Unfortunately, we did not find any
large corpus for laptop domain, thus presented un-
supervised features are used in restaurant domain
only.
We devote the following two subsections to de-
scribe these features. Then we introduce our ap-
proach to the individual tasks.
3.1 Latent Dirichlet Allocation
The Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a topic model that is assumed to provide
useful information for particular subtasks. We use
LDA implementation from the MALLET (McCal-
lum, 2002) software package. For each experi-
ment we always train the 400 topics LDA (no sig-
nificant difference was observed between different
numbers of topics) with 1,000 iterations of Gibbs
sampling. The hyperparameters of Dirichlet dis-
tributions were initially set to ? = 50/K, where
K is the number of topics and ? = 0.1. This set-
ting is recommended by (Griffiths and Steyvers,
2004). The topic probabilities are directly used as
new features to the classifier.
3.2 Word clusters
We use same approach as presented in (Brychc??n
and Konop??k, 2014), where word clusters derived
from semantic spaces improved language model-
ing. As recommended by these authors, we use
COALS (Correlated Occurrence Analogue to Lex-
ical Semantics) (Rohde et al., 2004) and HAL
(Hyperspace Analogue to Language) (Lund and
Burgess, 1996) for representing the word mean-
ing and the Repeated Bisection algorithm for clus-
tering. Similar approach has been already used
for sentiment analysis in (Habernal and Brychc??n,
2013) and (Brychc??n and Habernal, 2013).
The parameters of semantic spaces are set as
follows. For both semantic spaces we use a four-
word context window (in both directions). HAL
uses a matrix consisting of 50,000 columns, which
keeps the largest amount of information. COALS
uses a matrix with only 14,000 columns (as rec-
ommended by the authors of the algorithm). The
SVD reduction was not used in our experiments.
Implementation of the HAL, COALS algo-
rithms is available in an open source package S-
Space (Jurgens and Stevens, 2010). For cluster-
ing, we use the implementation from the CLUTO
software package (Karypis, 2003). As a measure
of the similarity between two words, we use the
cosine similarity of word vectors.
For both semantic spaces the word vectors are
clustered into four different depths: 100, 500,
1,000, and 5,000 clusters (i.e. eight different clus-
ter sets). The occurrences of particular clusters
represent additional features to the classifiers.
3.3 Aspect term extraction
Our approach for aspect term extraction is based
on Conditional Random Fields (CRF). The choice
was based on similarity with the named entity
recognition task, where CRF are regarded as the
current state of the art (Konkol and Konop??k,
2013). We use the BIO model for representing as-
pect terms (Ramshaw and Marcus, 1999).
The constrained feature set consists of: W, BoW,
B, LD, S. It is extended by WC for the uncon-
strained case.
3.4 Aspect term polarity
During the detection of the aspect term polarities,
the words affecting the sentiment of the aspect
term are assumed to be close in most of cases.
Thus we use a context window of 10 words in both
directions around the target aspect term. We as-
sume the further the word or bigram is from the
target aspect term, the lower impact it has on the
polarity label. To model this assumption we use
a weight for each word and bigram feature taken
from the Gaussian distribution according to the
distance from the aspect term. The mean is set
to 0 and the variance is optimized on training data.
As a feature set for the constrained approach we
use only BoW, BoB and for the unconstrained ap-
proach we use BoC, SD, SW above that.
3.5 Aspect category detection
Aspect category detection is based on a set of bi-
nary Maximum Entropy classifiers, one for each
class. The final decision is simply assembled from
decisions of individual classifiers.
For this task we use BoW, Tf-Idf for the con-
strained approach and add LDA, BoC for uncon-
strained approach.
819
Team Const. Rank P [%] R[%] F
1
[%] Rank ACC[%]
A
s
p
e
c
t
t
e
r
m
s
R
e
s
t
a
u
r
a
n
t
s
Best ? 1. 85.35 82.71 84.01 1. 80.95
UWB U 7. 82.70 76.28 79.36 4. 77.69
UWB C 12. 83.28 70.28 76.23 12. 72.13
Average ? 14-15. 76.74 67.26 70.78 18. 69.15
Semeval Baseline ? ? ? ? 47.15 ? 64.28
L
a
p
t
o
p
s
Best ? 1. 84.80 66.51 74.55 1. 70.49
UWB U ? ? ? ? 4. 66.67
UWB C 14. 77.33 49.54 60.39 10. 62.54
Average ? 14. 68.97 50.45 56.20 16. 59.01
Semeval Baseline ? ? ? ? 35.64 ? 51.07
A
s
p
e
c
t
c
a
t
e
g
o
r
i
e
s
Best ? 1. 91.04 86.24 87.58 1. 82.92
UWB U 4. 84.36 78.93 81.55 8. 72.78
UWB C 5. 85.09 77.37 81.04 9. 72.78
Average ? 11. 76.00 72.26 73.79 12-13. 69.51
Semeval Baseline ? ? ? ? 63.89 ? 65.66
Table 1: Comparison of our constrained (C) and unconstrained (U) system with Semeval baseline, best
and average results. P , R, and F
1
denote the precision, recall and F-measure, respectively, used for
measuring aspect term and category detection. ACC denotes the accuracy, used for measuring aspect
term and category sentiment polarity detection.
3.6 Aspect category polarity
For this task we always take the whole sentence
into account. We cannot take a limited window
as we do not know where exactly the category is
mentioned in the sentence. Moreover, it can be at
several positions. To distinguish between differ-
ent categories we again use standalone Maximum
Entropy classifier for each category.
The constrained feature set consists of: BoW,
BoB, Tf-Idf. It is extended by BoC, LDA, SD, SW
for the unconstrained case.
4 Results
The ABSA task was a competition between re-
search teams from around the world. There were
21 to 32 submitted systems for individual tasks.
We have submitted both constrained (no ex-
ternal knowledge, dictionaries or rules) and un-
constrained systems for all tasks, except uncon-
strained system for aspect term extraction in the
laptops domain.
Table 1 shows results of our systems (UWB)
and compares them with the best and average sys-
tems as well as with the Semeval baseline. The
average system is not any particular system. It is
represented by average rank and metrics (metrics
are averaged separately).
Our systems performed quite well. In all
tasks, we outperform the Semeval baseline sys-
tem. Moreover, we are always above average (F-
measure and accuracy) in all tasks. We were three
times in the fourth place and our unconstrained
systems were always in top ten.
Table 2 presents the 10-fold cross-validation re-
sults on restaurant training data. We can clearly
see, that any of our extension (LDA, clusters, sen-
timent vocabularies) brings at least some improve-
ment.
5 Conclusion
This paper covers our participation in the ABSA
task of Semeval 2014. The ABSA task consists
of 4 subtasks. For each subtask we propose both
constrained (no external knowledge) and uncon-
strained approach. The constrained versions of
our system are based purely on machine learning
techniques. The unconstrained versions extend the
constrained feature set by LDA, semantic spaces
and sentiment dictionaries.
The proposed approaches achieved very good
results. The constrained versions were always
above average, often by a large margin. The un-
constrained versions were ranked among the best
systems.
820
P [%] R[%] F
1
[%]
Constrained 68.72 82.14 74.83
Constrained + WC 76.77 82.51 79.53
(a) Aspect term extraction
ACC[%]
Constrained 65.91
Constrained+BoC 70.05
Constrained+SD+SW 68.13
All 71.02
(b) Aspect term polarity
P [%] R[%] F
1
[%]
Constrained 74.56 80.69 77.51
Constrained + LDA 75.96 81.94 78.84
Constrained + BoC 77.01 81.42 79.16
All 77.28 81.62 79.39
(c) Aspect category extraction
ACC[%]
Constrained 66.69
Constrained+LDA 67.85
Constrained+BoC 68.61
Constrained+SD+SW 69.28
All 70.20
(d) Aspect category polarity
Table 2: 10 fold cross-validation results on the restaurants training data for individual features. P , R,
and F
1
denote the precision, recall and F-measure, respectively, used for measuring aspect term and
category detection. ACC denotes the accuracy, used for measuring aspect term and category sentiment
polarity detection.
Acknowledgements
This work was supported by grant no. SGS-
2013-029 Advanced computing and information
systems, by the European Regional Development
Fund (ERDF) and by project ?NTIS - New Tech-
nologies for Information Society?, European Cen-
tre of Excellence, CZ.1.05/1.1.00/02.0090, and by
project MediaGist, EU?s FP7 People Programme
(Marie Curie Actions), no. 630786.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for lo-
cal service reviews. In Proceedings of WWW-2008
workshop on NLP in the Information Explosion Era.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multilingual web texts. Information retrieval,
12(5):526?558.
Tom?a?s Brychc??n and Ivan Habernal. 2013. Un-
supervised improving of sentiment analysis using
global target context. In Proceedings of the In-
ternational Conference Recent Advances in Natu-
ral Language Processing RANLP 2013, pages 122?
128, Hissar, Bulgaria, September. INCOMA Ltd.
Shoumen, BULGARIA.
Tom?a?s Brychc??n and Miloslav Konop??k. 2014. Seman-
tic spaces for improving language modeling. Com-
puter Speech & Language, 28(1):192 ? 209.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the Conference on Web Search and
Web Data Mining.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235, April.
Ivan Habernal and Tom?a?s Brychc??n. 2013. Semantic
spaces for sentiment analysis. In Text, Speech and
Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 482?489, Berlin Heidelberg.
Springer.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
821
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics.
David Jurgens and Keith Stevens. 2010. The s-space
package: An open source package for word space
models. In In Proceedings of the ACL 2010 System
Demonstrations.
George Karypis. 2003. Cluto - a clustering toolkit.
Michal Konkol and Miloslav Konop??k. 2013. Crf-
based czech named entity recognizer and consolida-
tion of czech ner research. In Ivan Habernal and
V?aclav Matou?sek, editors, Text, Speech and Dia-
logue, volume 8082 of Lecture Notes in Computer
Science, pages 153?160. Springer Berlin Heidel-
berg.
Michal Konkol. 2014. Brainy: A machine learn-
ing library. In Leszek Rutkowski, Marcin Ko-
rytkowski, Rafa Scherer, Ryszard Tadeusiewicz,
Lotfi A. Zadeh, and Jacek M. Zurada, editors, Artifi-
cial Intelligence and Soft Computing, volume 8468
of Lecture Notes in Computer Science. Springer
Berlin Heidelberg.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the web. In Proceedings of International
Conference on World Wide Web.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of Coling 2010: Poster
Volume.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods Instru-
ments and Computers, 28(2):203?208.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of International Conference on World
Wide Web.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: an unsupervised opinion miner from
unstructured product reviews. In Proceeding of
the ACM conference on Information and knowledge
management.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval 2014), Dublin, Ireland.
Lawrence Rabiner. 2010. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of the IEEE, pages 257?286.
Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157?176. Springer.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2004. An improved method for
deriving word meaning from lexical co-occurrence.
Cognitive Psychology, 7:573?605.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia Vzquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems,
53(4):689 ? 694.
Josef Steinberger, Tom?a?s Brychc??n, and Michal
Konkol. 2014. Aspect-level sentiment analysis
in czech. In Proceedings of the 5th Workshop on
Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis, Baltimore, USA,
June. Association for Computational Linguistics.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of International Conference on World Wide
Web.
822
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 24?30,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Aspect-Level Sentiment Analysis in Czech
Josef Steinberger
Department of Computer
Science and Engineering,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
jstein@kiv.zcu.cz
Tom
?
a
?
s Brychc??n
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
brychcin@kiv.zcu.cz
Michal Konkol
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
konkol@kiv.zcu.cz
Abstract
This paper presents a pioneering re-
search on aspect-level sentiment analysis
in Czech. The main contribution of the
paper is the newly created Czech aspect-
level sentiment corpus, based on data from
restaurant reviews. We annotated the cor-
pus with two variants of aspect-level senti-
ment ? aspect terms and aspect categories.
The corpus consists of 1,244 sentences and
1,824 annotated aspects and is freely avail-
able to the research community. Further-
more, we propose a baseline system based
on supervised machine learning. Our
system detects the aspect terms with F-
measure 68.65% and their polarities with
accuracy 66.27%. The categories are rec-
ognized with F-measure 74.02% and their
polarities with accuracy 66.61%.
1 Introduction
The interest in sentiment analysis (SA) is increas-
ing with the amount of easily accessible content on
the web, especially from the social media. Sen-
timent polarity is one of the critical information
needed for many analysis of the data. Its use
ranges from analysing product reviews (Stepanov
and Riccardi, 2011) to predicting sales and stock
markets using social media monitoring (Yu et al.,
2013).
The majority of current approaches tries to de-
tect the overall polarity of a sentence (or a docu-
ment) regardless of the target entities (e.g., restau-
rants, laptops) and their aspects (e.g., food, price,
battery, screen). By contrast, the aspect-driven
sentiment analysis identifies the aspects of a given
target entity and estimates the sentiment polarity
for each mentioned aspect. This opens up com-
pletely new possibilities how to analyse the data.
The most of the research in automatic sentiment
analysis has been devoted to English. There were
several attempts in Czech (Steinberger et al., 2011;
Veselovsk?a, 2012; Habernal et al., 2013; Brychc??n
and Habernal, 2013), but all were focused on the
global (sentence- or document-level) sentiment.
Although Czech is not a widely-spoken language
on the global scale, it is in many ways similar
to other Slavic languages and their speakers al-
together represent an important group. The rich
morphology and the free word order also makes it
interesting from the linguistic perspective.
Our main goal is the creation of a aspect-level
corpus as there is no such resource for Czech.
We would like to support the beginning of aspect-
level sentiment analysis for Czech and a human-
annotated corpus is the first step in this direc-
tion. In addition, we want to provide results of
a baseline system (based on machine leaning tech-
niques). This creates an easily reproducible start-
ing point and allows anyone to quickly join the re-
search of this task.
The rest of the paper is organised as follows.
Section 2 is devoted to related work. It covers the
aspect-level SA and sentiment analysis in Czech.
Then we introduce the aspect-level architecture
(Section 3) used for both the annotation of the cor-
pus (Section 4) and for the automatic supervised
approach (Section 5). In Section 6 we sumarize
our contribution and reveal our future plans.
2 Related work
The impact of SA can be seen in many practical
applications, The users? opinions are mostly ex-
tracted either on a certain polarity scale, or binary
(positive, negative). From the point of view of
the granularity, the polarity has been assigned to
a document or to a sentence. However, classify-
ing opinions at the document level or the sentence
level is often insufficient for applications because
they do not identify opinion targets or assign sen-
timents to such targets (Liu, 2012). Even if we
recognize the target entity (as the entity-centered
24
approaches do (e.g. Steinberger et al. (2011)), a
positive opinion about the entity does not mean
that the author has positive opinions about all as-
pects of the entity. Aspect-based sentiment analy-
sis, which has been also called ?feature-based? (Hu
and Liu, 2004), goes even deeper as it attempts to
identify (and assign the polarity to) aspects of the
target entity within a sentence (Hajmohammadi et
al., 2012). Whenever we talk about an aspect,
we must know which entity it belongs to. In the
further discussion, we often omit the entity as we
analysed restaurant reviews and thus our target en-
tities are the reviewed restaurants.
2.1 Aspect-based sentiment analysis
The aspect scenario can be decomposed into two
tasks: aspect extraction and aspect sentiment clas-
sification (Liu, 2012).
2.1.1 Aspect extraction
The task of aspect extraction, which can also be
seen as an information extraction task, is to detect
aspects that have been evaluated. For example, in
the sentence, The voice quality of this phone is
amazing, the aspect is voice quality of the entity
represented by this phone.
The basic approach is finding frequent nouns
and noun phrases. In (Liu et al., 2005), a specific
method based on a sequential learning method was
proposed to extract aspects from pros and cons,
Blair-Goldensohn et al. (2008) refined the frequent
noun and noun phrase approach by considering
mainly those noun phrases that are in sentiment-
bearing sentences or in some syntactic patterns
which indicate sentiments. Moghaddam and Ester
(2010) augmented the frequency-based approach
with an additional pattern-based filter to remove
some non-aspect terms. Long et al. (2010) ex-
tracted aspects (nouns) based on frequency and in-
formation distance.
Using supervised learning is another option.
Aspect extraction can be seen as a special case
of the general information extraction problem.
The most dominant methods are based on sequen-
tial learning. Since these are supervised tech-
niques, they need manually labeled data for train-
ing. One needs to manually annotate aspects
and non-aspects in a corpus. The current state-
of-the-art sequential learning methods are Hid-
den Markov Models (HMM) (Rabiner, 2010) and
Conditional Random Fields (CRF) (Lafferty et al.,
2001).
The last group of methods use topic models
(Mei et al., 2007; Titov and McDonald, 2008;
Blei et al., 2003). There are two main basic mod-
els, pLSA (Probabilistic Latent Semantic Analy-
sis) (Hofmann, 1999) and LDA (Latent Dirichlet
allocation) (Blei et al., 2003). In the SA context,
one can design a joint model to model both senti-
ment words and topics at the same time, due to the
observation that every opinion has a target.
2.1.2 Aspect sentiment classification
This task is to determine whether the opinions on
different aspects are positive, negative, or neutral.
The classification approaches can be divided to
supervised learning approaches and lexicon-based
approaches. Supervised learning performs bet-
ter in a particular application domain but it has
difficulty to scale up to a large number of do-
mains. Lexicon-based techniques often lose the
fight against the learning but they are suitable for
open-domain applications (Liu, 2012).
The key issue for learning methods is to de-
termine the scope of each sentiment expression,
i.e., whether it covers the aspect of interest in the
sentence. In (Jiang et al., 2011), a dependency
parser was used to generate a set of aspect de-
pendent features for classification. A related ap-
proach was also used in (Boiy and Moens, 2009),
which weights each feature based on the position
of the feature relative to the target aspect in the
parse tree.
Lexicon-based approaches use a list of senti-
ment phrases as the core resource. The method
in (Ding et al., 2008) has four steps to assign
a polarity to an aspect: mark sentiment words
and phrases, apply sentiment shifters, handle but-
clauses and aggregate opinions using an aggrega-
tion function (e.g. Hu and Liu (2004)).
2.2 Sentiment analysis for Czech
Pilot study of Czech sentiment analysis was shown
in (Steinberger et al., 2012) where sentiment dic-
tionaries for many languages (including Czech)
were created using semi-automatic ?triangulation?
method.
Veselovsk?a (2012) created a small corpus con-
taining polarity categories for 410 news sentences
and used the Naive Bayes and lexicon-based clas-
sifiers.
Three large labeled corpora (10k Facebook
posts, 90k movie reviews, and 130k product
reviews) were introduced in (Habernal et al.,
25
2013).Authors also evaluate three different classi-
fiers, namely Naive Bayes, SVM (Support Vector
Machines) and Maximum Entropy on these data.
Recently, Habernal and Brychc??n (2013) experi-
mented with building word clusters, obtained from
semantic spaces created on unlabeled data, as an
additional source of information to tackle the high
flection issue in Czech.
These results were later outperformed by
another unsupervised extension (Brychc??n and
Habernal, 2013), where the global target context
was shown to be very useful source of informa-
tion.
3 The task definition
The aspect-level sentiment analysis firstly identi-
fies the aspects of the target entity and then assigns
a polarity to each aspect. There are several ways
to define aspects and polarities. We use the defini-
tion based on the Semeval2014?s Aspect-based SA
task, which distinguishes two types of aspect-level
sentiment ? aspect terms and aspect categories.
The task is decomposed into the following 4
subtasks. We briefly describe each subtask and
give some examples of source sentences and the
expected results of the subtask.
3.1 Subtask 1: Aspect term extraction
Given a set of sentences with pre-identified enti-
ties (e.g., restaurants), the task is to identify the
aspect terms present in the sentence and return a
list containing all the distinct aspect terms. An as-
pect term names a particular aspect of the target
entity.
Examples:
D?eti dostaly naprosto krvav?e maso.
(They brought a totally bloody meat to the kids.)
? {maso (meat)}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
? {Tla?cenka (porkpie), pol?evka (soup)}
3.2 Subtask 2: Aspect term polarity
For a given set of aspect terms within a sentence,
the task is to determine the polarity of each aspect
term: positive, negative, neutral or bipolar (i.e.,
both positive and negative).
Examples:
D?eti dostaly naprosto krvav?e maso.
(They brought a totally bloody meat to the kids.)
? {maso (meat): negative}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
?{Tla?cenka (porkpie): negative, pol?evka (soup):
positive}
3.3 Subtask 3: Aspect category detection
Given a predefined set of aspect categories (e.g.,
price, food), the task is to identify the aspect cat-
egories discussed in a given sentence. Aspect cat-
egories are typically coarser than the aspect terms
of Subtask 1, and they do not necessarily occur as
terms in the given sentence.
For example, given the set of aspect categories
food, service, price, ambience:
P?riv??tala n?as velmi p?r??jemn?a serv??rka, ale tak?e
m??stnost s o?sunt?el?ym n?abytkem.
(We found a very nice waitress but also a room
with time-worn furniture.)
? {service, ambience}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
? {food}
3.4 Subtask 4: Aspect category polarity
Given a set of pre-identified aspect categories
(e.g., {food, price}), the task is to determine the
polarity (positive, negative, neutral or bipolar) of
each aspect category.
Examples:
P?riv??tala n?as velmi p?r??jemn?a serv??rka, ale tak?e
m??stnost s o?sunt?el?ym n?abytkem.
(We found a very nice waitress but also a room
with time-worn furniture.)
? {service: positive, ambience: negative}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
? {food: bipolar}
4 Building the aspect-level corpus
Aspect-level annotations are strictly connected to
the analysed domain. As our final goal is going
multilingual, we work on the domains selected for
the Semeval2014?s Aspect-based SA task (restau-
rants, laptop) which will allow us to compare ap-
proaches for both English and Czech on the same
domains.
We started with the restaurants and in the future,
we would also like to cover the laptops.
26
We downloaded restaurant reviews from www.
nejezto.cz. Ten restaurants with the largest
number of reviews were selected. The reviews
were splitted into sentences. Average number of
sentences per restaurant was 223.
4.1 Guidelines
The purpose of this annotation was to detect as-
pects and their sentiment polarity within sen-
tences. The target entities were particular restau-
rants. For a given restaurant, the annotator had
following tasks:
1. Identify irrelevant sentences: Sentences
that do not contain any information rele-
vant to the topic of restaurants. They were
later filtered out of the corpus. Example:
Ur?a?zet n?ekoho pro jeho n?azor je ned?ustojn?e
dosp?el?eho ?clov?eka. (Offencing somebody for
his opinion is discreditable for an adult.)
2. Identify aspect terms: Single or multiword
terms naming particular aspects of the target
entity. These are either full nominal phrases
(?sp??z a restovan?e brambory ? skewer with
fried potatoes) or verbs (stoj?? ? priced). Ref-
erences, names or pronouns should not be an-
notated.
3. Aspect term polarity: Each aspect term has
to be assigned one of the following polarities
based on the sentiment that is expressed in the
sentence about it: positive, negative, bipo-
lar (both positive and negative sentiment) and
neutral (neither positive nor negative senti-
ment).
4. Aspect category: The task of the annotator is
to identify the aspect categories discussed in
a sentence given the following five aspect cat-
egories: food, service, price, ambience, gen-
eral (sentences mentioning the restaurant as
a whole). Example: Celkov?e doporu?cuji a
vr?at??m se tam ? Overall I would recommend
it and go back again. ? general.
5. Aspect category polarity: Each aspect cat-
egory discussed by a particular sentence has
to be assigned one of the following polarities
based on the sentiment that is expressed in the
sentence about it: positive, negative, bipolar,
neutral.
4.2 Annotation statistics
Three native Czech speakers annotated in total
1,532 sentences. 18.8% of the sentences were
marked as irrelevant, leaving 1,244 sentences for
further analysis. Their average agreement for the
task of aspect terms? identification was 82.6%
(measured by F-measure). Only strict matches
were considered correct. In the case of identi-
fying the categories, their average agreement (F-
measure) was 91.8%. The annotators agreed on
85.5% (accuracy) in the task of assigning polarity
to terms and on 82.4% (accuracy) in the case of
the category polarity assignment. It corresponds
to Cohen?s  of 0.762, resp. 0.711, which rep-
resents a substantial agreement level (Pustejovsky
and Stubbs, 2013), therefore the task can be con-
sidered as well-defined.
There were several reasons of disagreement.
The annotators did not always itentify the same
terms, mainly in the cases with general meaning.
In the case of polarity, the annotators did not agree
on the most difficult cases to which bipolar class
could be assigned:
Trochu p?resolen?a om?a?cka, ale jinak luxus.
(Too salted sauce, but luxury otherwise.)
? {food: bipolar vs. positive}
The cases, on which the two annotators did not
agree, were judged by the third super-annotator
and golden standard data were created. The final
dataset
1
contains 1244 sentences. The sentences
contain 1824 annotated aspect terms (679 positive,
725 negative, 403 neutral, 17 bipolar) and 1365
categories (521 positive, 569 negative, 246 neu-
tral, 28 bipolar).
5 Results of the supervised approach
5.1 Overview
We use machine learning approach in all subtasks.
For aspect term extraction we use Conditional
Random Fields (CRF). For the other three tasks
we use the Maximum Entropy classifier. We use
the Brainy
2
implementation of these algorithms.
During the data preprocessing, we use simple
word tokenizer based on regular expressions. All
tokens are lowercased for tasks 3 and 4. Due to the
complex morphology of Czech we also use the un-
1
We will provide the dataset at http://liks.fav.
zcu.cz/sentiment.
2
Available at http://home.zcu.cz/
?
konkol/
brainy.php
27
supervised stemmer called HPS
3
, that has already
proved to be useful in sentiment analysis (Haber-
nal et al., 2013; Habernal and Brychc??n, 2013;
Brychc??n and Habernal, 2013).
All particular subtasks share following features:
? Bag of words: The occurrence of a word.
? Bag of bigrams: The occurrence of a bigram.
? Bag of stems: The occurrence of a stem.
? Bag of stem bigrams: The occurrence of a
stem bigram.
5.2 Aspect term extraction
The system for aspect term extraction is based on
CRF. The choice of CRF is based on a current state
of the art in named entity recognition (see for ex-
ample (Konkol and Konop??k, 2013)) as it is a very
similar task. We use the BIO (Ramshaw and Mar-
cus, 1999) model to represent aspect terms. In ad-
dition to the previously mentioned features we use
affixes and learned dictionaries. Affixes are sim-
ply prefixes and suffixes of length 2 to 4. Learned
dictionaries are phrases that are aspect terms in the
training data.
Our system achieved 58.14 precision, 83.80 re-
call and 68.65 F-measure.
5.3 Aspect term polarity
During the detection of the aspect term polarities,
the words affecting the sentiment of the aspect
term are assumed to be close in most of cases.
Thus we use a small window (10 words in both
directions) around the target aspect term. We as-
sume the further the word or bigram is from the
target aspect term, the lower impact it has on sen-
timent label. To model this assumption we use
a weight for each word and bigram feature taken
from the Gaussian distribution according to dis-
tance from aspect term. The mean is set to 0 and
variance is optimized on training data. The classi-
fier uses only the features presented in section 5.1.
The results are presented in table 1.
5.4 Aspect category detection
Aspect category detection is based on the Maxi-
mum Entropy classifiers. We use one binary clas-
sifier for each category. Each classifier then de-
cides whether the sentence has the given category
3
Available at http://liks.fav.zcu.cz/HPS.
Table 1: Aspect term polarity results. P , R and
F
m
denote the precision, recall and F-measure.
The results are expressed by percentages.
label P [%] R[%] F
m
[%]
negative 76.41 63.31 69.25
neutral 33.75 50.18 40.36
positive 74.78 76.82 75.78
Accuracy: 66.27%
or not. For this task we use only the bag of stems
and Tf-Idf features.
Our system achieved 68.71 precision, 80.21 re-
call and 74.02 F-measure.
5.5 Aspect category polarity
For the category polarity detection we use the
same features as for aspect term polarity detec-
tion. However in this case, we always take the
whole sentence into account. We cannot take a
limited window as we do not know where exactly
the category is mentioned in the sentence. More-
over, it can be at several positions. To distinguish
between different categories we use multiple Max-
imum Entropy classifiers, one for each category.
The results are shown in table 2.
Table 2: Aspect category polarity results. P ,
R and F
m
denote the precision, recall and F-
measure. The results are expressed by percent-
ages.
label P [%] R[%] F
m
[%]
negative 74.07 66.04 69.83
neutral 37.80 46.73 41.80
positive 72.12 75.30 73.67
Accuracy: 66.61%
5.6 Discussion
In section 5 we described our system for aspect-
level sentiment analysis and showed the results.
We do not use any language-dependent features,
everything is learned from the training data. It is
thus possible to say that our system is both lan-
guage and domain independent, i.e. the system is
able to work for any domain or language, if the
training data are provided.
From another perspective, the already trained
model is language and domain dependent (i.e. the
model trained on restaurant domain probably will
not perform well on laptop domain). The depen-
28
dence on the domain has multiple reasons. First,
the categories are defined strictly for one domain
(e.g. food, price, etc.). Second, many words can
have different sentiment polarity in different do-
mains.
In general, the sentiment analysis deals with
many problems. These problems are much more
evident for Czech as a representative of language
with rich morphology and also with almost free
word order. Here are two examples, where our
system wrongly estimate the sentiment label.
Na nic si nejde st?e?zovat.
(There is nothing to complain about.)
? {general: positive}
The sentence contains words that frequently oc-
cur in negative reviews: nic - nothing, st?e?zovat -
complain; but the sentence is positive.
O t?ech labu?znick?ych a delikatesn??ch z?a?zitc??ch si
?clov?ek pouze p?re?cte, ale realita je jin?a.
(One can only read about these gourmand and de-
licious experiences, but the reality is completely
different.)
? {food: negative}
Sentence contains words like labu?znick?ych -
gourmand and delikatesn??ch - delicious that are
strictly positive, but in this context it is mentioned
negatively.
As we already said, this is the pilot study of
aspect-level sentiment analysis in Czech. Several
studies about sentence-level sentiment analysis of
Czech have been already published, and thus it
is worth comparing how these two tasks differ in
terms of difficulty. Note that the aspect-level sen-
timent analysis has to deal with multiple aspects
and categories in a given sentence, and thus it is
apparently a much more difficult task.
We believe the results of (Brychc??n and Haber-
nal, 2013) on Czech movie reviews dataset can be
a comparable example of sentence-level sentiment
analysis as they also distinguish 3 sentiment labels
(positive, negative and neutral) and the data are
taken from a closed domain (movies). Their best
result (given by the model with all extensions) is
81.53%. Our best results are 66.27% and 66.61%
for aspect and category polarity detection, respec-
tively.
6 Conclusion
The aspect level sentiment analysis has not been
studied for Czech yet. The main reason for this is
the lack of annotated data. In this paper, we create
a high quality gold data for this task, we describe
our approach to their annotation and discuss their
properties. Corpus is available for free at http:
//liks.fav.zcu.cz/sentiment.
We also propose a baseline model based on
state-of-the-art supervised machine learning tech-
niques. Our system is language and domain inde-
pendent, i.e. it can be easily trained on data from
another domain or language. It achieved 68.65%
F-measure in the aspect term detection, 74.02% F-
measure in the aspect category assigning, 66.27%
accuracy in the aspect term polarity classification,
and 66.61% accuracy in the aspect category polar-
ity classification.
In the future, we would like to continue the
aspect-level research direction in three ways. We
would like to extend the currently created restau-
rant reviews? corpus, to add the second (laptop?s)
domain to the corpus, and finally, to experiment
with extensions to the baseline system. As the
corpus for the Semeval2014 aspect-based SA task
contains review sentences from the same domains,
we will be able to compare the results of the sys-
tem cross-lingually.
Acknowledgments
This work was supported by grant no. SGS-
2013-029 Advanced computing and information
systems, by the European Regional Development
Fund (ERDF), by project ?NTIS - New Tech-
nologies for Information Society?, European Cen-
tre of Excellence, CZ.1.05/1.1.00/02.0090, and by
project MediaGist, EU?s FP7 People Programme
(Marie Curie Actions), no 630786.
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for lo-
cal service reviews. In Proceedings of WWW-2008
workshop on NLP in the Information Explosion Era.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multilingual web texts. Information retrieval,
12(5):526?558.
Tom?a?s Brychc??n and Ivan Habernal. 2013. Unsuper-
vised improving of sentiment analysis using global
29
target context. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing RANLP 2013, pages 122?128, Hissar,
Bulgaria, September. Incoma Ltd. Shoumen, Bul-
garia.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the Conference on Web Search and
Web Data Mining.
Ivan Habernal and Tom?a?s Brychc??n. 2013. Semantic
spaces for sentiment analysis. In Text, Speech and
Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 482?489, Berlin Heidelberg.
Springer.
Ivan Habernal, Tom?a?s Pt?a?cek, and Josef Steinberger.
2013. Sentiment analysis in czech social media us-
ing supervised machine learning. In Proceedings of
the 4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 65?74, Atlanta, Georgia, June. Association
for Computational Linguistics.
Mohammad Sadegh Hajmohammadi, Roliana Ibrahim,
and Zulaiha Ali Othman. 2012. Opinion mining and
sentiment analysis: A survey. International Journal
of Computers & Technology, 2(3).
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of Conference on Uncer-
tainty in Artificial Intelligence.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics.
Michal Konkol and Miloslav Konop??k. 2013. Crf-
based czech named entity recognizer and consolida-
tion of czech ner research. In Ivan Habernal and
V?aclav Matou?sek, editors, Text, Speech and Dia-
logue, volume 8082 of Lecture Notes in Computer
Science, pages 153?160. Springer Berlin Heidel-
berg.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the web. In Proceedings of International
Conference on World Wide Web.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of Coling 2010: Poster
Volume.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of International Conference on World
Wide Web.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: an unsupervised opinion miner from
unstructured product reviews. In Proceeding of
the ACM conference on Information and knowledge
management.
James Pustejovsky and Amber Stubbs. 2013. Natural
Language Annotation for Machine Learning. OR-
eilly Media, Sebastopol, CA 95472.
Lawrence Rabiner. 2010. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of the IEEE, pages 257?286.
Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157?176. Springer.
J. Steinberger, P. Lenkova, M. Kabadjov, R. Stein-
berger, and E. van der Goot. 2011. Multilingual
entity-centered sentiment analysis evaluated by par-
allel corpora. In Proceedings of the 8th Interna-
tional Conference Recent Advances in Natural Lan-
guage Processing, RANLP?11, pages 770?775.
J. Steinberger, M. Ebrahim, Ehrmann M., A. Hur-
riyetoglu, M. Kabadjov, P. Lenkova, R. Steinberger,
H. Tanev, S. Vzquez, and V. Zavarella. 2012. Cre-
ating sentiment dictionaries via triangulation. Deci-
sion Support Systems, 53:689?694.
E.A. Stepanov and G. Riccardi. 2011. Detecting gen-
eral opinions from customer surveys. In Data Min-
ing Workshops (ICDMW), 2011 IEEE 11th Interna-
tional Conference on, pages 115?122.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of International Conference on World Wide
Web.
Kate?rina Veselovsk?a. 2012. Sentence-level sentiment
analysis in czech. In Proceedings of the 2nd Interna-
tional Conference on Web Intelligence, Mining and
Semantics. ACM.
Liang-Chih Yu, Jheng-Long Wu, Pei-Chann Chang,
and Hsuan-Shou Chu. 2013. Using a contextual en-
tropy model to expand emotion words and their in-
tensity for the sentiment classification of stock mar-
ket news. Knowledge Based Syst, 41:89?97, March.
30
