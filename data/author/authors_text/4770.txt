Converting Text into Agent Animations: Assigning Gestures to Text 
Yukiko I. Nakano?   Masashi Okamoto?    Daisuke Kawahara?   Qing Li?   Toyoaki Nishida?
?Japan Science and Technology Agency 
2-5-1 Atago, Minato-ku, Tokyo, 105-6218 Japan
?The University of Tokyo  
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656 Japan
{nakano, okamoto, kawahara, liqing, nishida}@kc.t.u-tokyo.ac.jp 
Abstract 
This paper proposes a method for assigning 
gestures to text based on lexical and syntactic 
information. First, our empirical study identi-
fied lexical and syntactic information strongly 
correlated with gesture occurrence and sug-
gested that syntactic structure is more useful 
for judging gesture occurrence than local syn-
tactic cues. Based on the empirical results, we 
have implemented a system that converts text 
into an animated agent that gestures and 
speaks synchronously. 
1 Introduction  
The significant advances in computer graphics over the 
last decade have improved the expressiveness of ani-
mated characters and have promoted research on inter-
face agents, which serve as mediators of human-
computer interactions. As an interface agent has an em-
bodied figure, it can use its face and body to display 
nonverbal behaviors while speaking.  
Previous studies in human communication suggest 
that gestures in particular contribute to better under-
standing of speech. About 90% of all gestures by 
speakers occur when the speaker is actually uttering 
something (McNeill, 1992). Experimental studies have 
shown that spoken sentences are heard twice as accu-
rately when they are presented along with a gesture 
(Berger & Popelka, 1971). Comprehension of a descrip-
tion accompanied by gestures is better than that accom-
panied by only the speaker?s face and lip movements 
(Rogers, 1978). These previous studies suggest that 
generating appropriate gestures synchronized with 
speech is a promising approach to improving the per-
formance of interface agents. In previous studies of 
multimodal generation, gestures were determined ac-
cording to the instruction content (Andre, Rist, & Mul-
ler, 1999; Rickel & Johnson, 1999), the task situation in 
a learning environment (Lester, Stone, & Stelling, 
1999), or the agent?s communicative goal in conversa-
tion (Cassell et al, 1994; Cassell, Stone, & Yan, 2000). 
These approaches, however, require the contents devel-
oper (e.g., a school teacher designing teaching materi-
als) to be skilled at describing semantic and pragmatic 
relations in logical form. A different approach, (Cassell, 
Vilhjalmsson, & Bickmore, 2001) proposes a toolkit 
that takes plain text as input and automatically suggests 
a sequence of agent behaviors synchronized with the 
synthesized speech. However, there has been little work 
in computational linguistics on how to identify and ex-
tract linguistic information in text in order to generate 
gestures.  
Our study has addressed these issues by considering 
two questions. (1) Is the lexical and syntactic informa-
tion in text useful for generating meaningful gestures? 
(2) If so, how can the information be extracted from the 
text and exploited in a gesture decision mechanism in 
an interface agent? Our goal is to develop a media con-
version technique that generates agent animations syn-
chronized with speech from plain text.  
This paper is organized as follows. The next section 
reviews theoretical issues about the relationships be-
tween gestures and syntactic information. The empirical 
study we conducted based on these issues is described 
in Sec. 3. In Sec. 4 we describe the implementation of 
our presentation agent system, and in the last section we 
discuss future directions.  
2 Linguistic Theories and Gesture Studies 
In this section we review linguistic theories and discuss 
the relationship between gesture occurrence and syntac-
tic information.  
Linguistic quantity for reference: McNeill (McNeill, 
1992) used communicative dynamism (CD), which 
represents the extent to which the message at a given 
point is ?pushing the communication forward? (Firbas, 
1971), as a variable that correlates with gesture occur-
rence. The greater the CD, the more probable the occur-
rence of a gesture. As a measure of CD, McNeill chose 
the amount of linguistic material used to make the refer-
ence (Givon, 1985). Pronouns have less CD than full 
nominal phrases (NPs), which have less CD than modi-
fied full NPs. This implies that the CD can be estimated 
by looking at the syntactic structure of a sentence.  
Theme/Rheme: McNeill also asserted that the theme 
(Halliday, 1967) of a sentence usually has the least CD 
and is not normally accompanied by a gesture. Gestures 
usually accompany the rhemes, which are the elements 
of a sentence that plausibly contribute information 
about the theme, and thus have greater CD. In Japanese 
grammar there is a device for marking the theme explic-
itly. Topic marking postpositions (or ?topic markers?), 
typically ?wa,? mark a nominal phrase as the theme. 
This facilitates the use of syntactic analysis to identify 
the theme of a sentence. Another interesting aspect of 
information structure is that in English grammar, a wh-
interrogative (what, how, etc.) at the beginning of a 
sentence marks the theme and indicates that the content 
of the theme is the focus (Halliday, 1967). However, we 
do not know whether such a special type of theme is 
more likely to co-occur with a gesture or not.  
Given/New: Given and new information demonstrate 
an aspect of theme and rheme. Given information usu-
ally has a low degree of rhematicity, while new infor-
mation has a high degree. This implies that rhematicity 
can be estimated by determining whether the NP is the 
first mention (i.e., new information) or has already been 
mentioned (i.e., old or given information).  
Contrastive relationship: Prevost (1996) reported that 
intonational accent is often used to mark an explicit 
contrast among the salient discourse entities. On the 
basis of this finding and Kendon?s theory about the rela-
tionship between intonation phrases and gesture place-
ments (Kendon, 1972), Cassell & Prevost (1996) 
developed a method for generating contrastive gestures 
from a semantic representation. In syntactic analysis, a 
contrastive relation is usually expressed as a coordina-
tion, which is a syntactic structure including at least two 
conjuncts linked by a conjunction.  
Figure 1 shows an example of the correlation between 
gesture occurrence and the dependency structure of a 
Japanese sentence. Bunsetsu units (8)-(9) and (10)-(13) 
in the figure are conjuncts. A ?bunsetsu unit? in Japa-
nese corresponds to a phrase in English, such as a noun 
phrase or a prepositional phrase. Each conjunct is ac-
companied by a gesture. Bunsetsu (14) is a complement 
containing a verbal phrase; it depends on bunsetsu (15), 
which is an NP. Thus, bunsetsu (15) is a modified full 
NP and thus has large linguistic quantity.  
3 Empirical Study 
To identify linguistic features that might 
be useful for judging gesture occurrence, 
we videotaped seven presentation talks 
and transcribed three minutes for each of 
them. The collected data included 2124 
bunsetsu units and 343 gestures. 
Gesture Annotation: Three coders dis-
cussed how to code the half the data and reached a con-
sensus on gesture occurrence. After this consensus on 
the coding scheme was established1, one of the coders 
annotated the rest of the data. A gesture consists of 
preparation, stroke, and retraction (McNeill, 1992), and 
a stroke co-occurs with the most prominent syllable 
(Kendon, 1972). Thus, we annotated the stroke time as 
well as the start and end time of each gesture.  
Linguistic Analysis: Each bunsetsu unit was automati-
cally annotated with linguistic information using a Japa-
nese syntactic analyzer (Kurohashi & Nagao, 1994)2. 
The information was determined by asked the following 
questions for each bunsetsu unit. 
(a) If it is an NP, is it modified by a clause or a com-
plement? 
(b) If it is an NP, what type of postpositional particle 
marks its end (e.g., ?wa?, ?ga?, ?wo?)? 
(c) Is it a wh-interrogative? 
(d) Are all the content words in the bunsetsu unit have 
mentioned in a preceding sentence? 
(e) Is it a constituent of a coordination? 
Moreover, as we noticed that some lexical entities fre-
quently co-occurred with a gesture in our data, we used 
the syntactic analyzer to annotate additional lexical in-
formation based on the following questions.  
(f) Is the bunsetsu unit an emphatic adverbial phrase 
(e.g., very, extremely), or is it modified by a pre-
ceding emphatic adverb (e.g., very important is-
sue)? 
(g) Does it include a cue word (e.g., now, therefore)? 
(h) Does it include a numeral (e.g., thousands of people, 
99 times)? 
We then investigated the correlation between these 
lexical and syntactic features and the occurrence of ges-
ture strokes.  
Result: The results are summarized in Table 1. The 
baseline gesture occurrence frequency was 10.1% per 
bunsetsu unit (a gesture occurred once about every ten 
                                                          
1 Inter-coder reliability among the three coders in catego-
rizing the gestures (beat, iconic, etc.) was sufficiently high 
(Kappa = 0.81). Although we did not measure agreement on 
gesture occurrence itself, this result suggests that the coders 
had very similar schemes for recognizing gestures.  
2 To prevent the effects of parsing errors, errors in syntac-
tic dependency analysis were corrected manually for about 
13% of the data.  
shindo-[ga] atae-rareru-to-ka sore-[ni] kawaru kasokudo-[ga] atae-rareru-to iu-youna jyoukyou-de
<parallel>
<nominal>
<complement>
<verbal>
(8) (9) (10) (11) (12) (13) (14) (15)
?a situation where seismic intensity is given, or degree of acceleration is given?
Figure 1: Example analysis of syntactic dependency  
Underlined phrases are accompanied by gestures, and strokes occur at dou-
ble-underlined parts. Case markers are enclosed by square brackets [ ]. 
Table 1. Summary of results 
Case Syntactic/lexical information of a bunsetsu unit 
Gesture 
occurrence
C1 (a) NP modified by clause 0.382 
C2 
Quantity of 
modification Pronouns, other 
types of NPs 
(b) Case marker = ?wo? 
& (d) New information 
0.281 
C3 (c) WH-interrogative 0.414 
C4 (e) Coordination 0.477 
C5 (f) Emphatic adverb itself 0.244 
C6 
Emphatic 
adverbial phrase (f?) Following emphatic adverb 0.350 
C7 (g) Cue word 0.415 
C8 (h) Numeral 0.393 
C9 Other (baseline) 0.101 
 
bunsetsu units). A gesture stroke most frequently co-
occurred with a bunsetsu unit forming a coordination 
(47.7%). When an NP was modified by a full clause, it 
was accompanied by a gesture 38.2% of the time. For 
the other types of noun phrases, including pronouns, 
when an accusative case marked with case marker ?wo? 
was new information (i.e., it was not mentioned in a 
previous sentence), a gesture co-occurred with the 
phrase 28.1% of the time. Moreover, gesture strokes 
frequently co-occurred with wh-interrogatives (41.4%), 
cue words (41.5%), and numeral words (39.3%). Ges-
ture strokes frequently occurred right after emphatic 
adverbs (35%) rather than with the adverb (24.4%).  
These cases listed in Table 1 had a 3 to 5 times higher 
probability of gesture occurrence than the baseline and 
accounted for 75% of all the gestures observed in the 
data. Our results suggest that these types of lexical and 
syntactic information can be used to distinguish be-
tween where a gesture should be assigned and where 
one should not be assigned. They also indicate that the 
syntactic structure of a sentence more strongly affects 
gesture occurrence than theme or rheme and than given 
or new information specified by local grammatical cues, 
such as topic markers and case markers.  
4 System Implementation 
4.1 Overview 
We used our results to build a presentation agent system, 
SPOC (Stream-oriented Public Opinion Channel).? This 
system enables a user to embody a story (written text) 
as a multimodal presentation featuring video, graphics, 
speech, and character animation. A snapshot of the 
SPOC viewer is shown in Figure 2.  
In order to implement a storyteller in SPOC, we de-
veloped an agent behavior generation system we call 
?CAST (Conversational Agent System for neTwork 
applications).? Taking text input, CAST automatically 
selects agent gestures and other nonverbal behaviors, 
calculates an animation schedule, and produces synthe-
sized voice output for the agent. As shown in Figure 2, 
CAST consists of four main components: (1) the Agent 
Behavior Selection Module (ABS), (2) the Language 
Tagging Module (LTM), (3) the agent animation system, 
and (4) a text-to-speech engine (TTS). The received text 
input is first sent to the ABS. The ABS selects appro-
priate gestures and facial expressions based on the lin-
guistic information calculated by the LTM. It then 
obtains the timing information from the TTS and calcu-
lates a time schedule for the set of agent actions. The 
output from the ABS is a set of animation instructions 
that can be interpreted and executed by the agent anima-
tion system. 
4.2 Determining Agent Behaviors 
Tagging linguistic information: First, the LTM parses 
the input text and calculates the linguistic information 
described in Sec. 3. For example, bunsetsu (9) in Figure 
1 has the following feature set. 
{Text-ID: 1, Sentence-ID: 1, Bunsetsu-ID: 9, Govern: 8, De-
pend-on: 13, Phrase-type: VP, Linguistic-quantity: NA, Case-
marker: NA, WH-interrogative: false, Given/New: new, Coor-
dinate-with: 13, Emphatic-Adv: false, Cue-Word: false, Nu-
meral: false} 
The text ID of this bunsetsu unit is 1, the sentence ID 
is 1, the bunsetsu ID is 9. This bunsetsu governs bun-
setsu 8 and depends on bunsetsu 13. It conveys new 
information and, together with bunsetsu 13, forms a 
parallel phrase.  
Assigning gestures: Then, for each bunsetsu unit, the 
ABS decides whether to assign a gesture or not based 
on the empirical results shown in Table 1. For example, 
bunsetsu unit (9) shown above matches case C4 in Ta-
ble 1, where a bunsetsu unit is a constituent of coordina-
tion. In this case, the system assigns a gesture to the 
bunsetsu with 47.7 % probability. In the current imple-
mentation, if a specific gesture for an emphasized con-
cept is defined in the gesture animation library (e.g., a 
gesture animation expressing ?big?), it is preferred to a 
?beat gesture? (a simple flick of the hand or fingers up 
and down (McNeill, 1992)). If a specific gesture is not 
defined, a beat gesture is used as the default. 
Animation ID
Start/end time
Agent Behavior 
Selection Module 
(ABS) Language Tagging 
Module (LTM)
Text-to-Speech 
(TTS)
S-POC Viewer
Input 
text
CAST
Video
Graphics
Graphics + Camera work
Agent Animation 
System
This is 
our ?
This is 
our 
Figure 2: Overview of CAST and SPOC 
The output of the ABS is stored in XML format. The 
type of action and the start and end times of the action 
are indicated by XML tags. In the example shown in 
Figure 3, the agent first gazes towards the user. It then 
performs contrast gestures at the second and sixth bun-
setsu units and a beat gesture at the eighth bunsetsu unit.  
Finally, the ABS transforms the XML into a time 
schedule by accessing the TTS engine and estimating 
the phoneme and bunsetsu boundary timings. The 
scheduling technique is similar to that described by 
(Cassell et al, 2001). The ABS also assigns visemes for 
the lip-sync and the facial expressions, such as head 
movement, eye gaze, blink, and eyebrow movement.  
5 Discussion and Conclusion 
We have addressed the issues related to assigning ges-
tures to text and converting the text into agent anima-
tions synchronized with speech. First, our empirical 
study identified useful lexical and syntactic information 
for assigning gestures to plain text. Specifically, when a 
bunsetsu unit is a constituent of coordination, gestures 
occur almost half the time. Gestures also frequently co-
occur with nominal phrases modified by a clause. These 
findings suggest that syntactic structure is a stronger 
determinant of gesture occurrence than theme or rheme 
and given or new information specified by local gram-
matical cues.  
We plan to enhance our model by incorporating more 
general discourse level information, though the current 
system exploits cue words as a very partial kind of dis-
course information. For instance, gestures frequently 
occur at episode boundaries. Pushing and popping of a 
discourse segment (Grosz & Sidner, 1986) may also 
affect gesture occurrence. Therefore, by integrating a 
discourse analyzer into the LTM, more general struc-
tural discourse information can be used in the model. 
Another important direction is to evaluate the effective-
ness of agent gestures in actual human-agent interaction. 
We expect that if our model can generate gestures with 
appropriate timing for emphasizing important words 
and phrases, users can perceive agent presentations as 
being more alive and comprehensible. We plan to con-
duct a user study to examine this hypothesis.  
References 
Andre, E., Rist, T., & Muller, J. (1999). Employing AI meth-
ods to control the behavior of animated interface agents. Ap-
plied Artificial Intelligence, 13, 415-448. 
Berger, K. W., & Popelka, G. R. (1971). Extra-facial Gestures 
in Relation to Speech-reading. Journal of Communication 
Disorders, 3, 302-308. 
Cassell, J. et al (1994). Animated Conversation: Rule-Based 
Generation of Facial Expression, Gesture and Spoken Intona-
tion for Multiple Conversational Agents. Paper presented at 
the SIGGRAPH '94. 
Cassell, J., & Prevost, S. (1996). Distribution of Semantic 
Features Across Speech and Gesture by Humans and Com-
puters. Paper presented at the Workshop on the Integration of 
Gesture in Language and Speech. 
Cassell, J., Stone, M., & Yan, H. (2000). Coordination and 
Context-Dependence in the Generation of Embodied Conver-
sation. Paper presented at the INLG 2000. 
Cassell, J., Vilhjalmsson, H., & Bickmore, T. (2001). BEAT: 
The Behavior Expression Animation Toolkit. Paper presented 
at the SIGGRAPH 01. 
Firbas, J. (1971). On the Concept of Communicative Dyna-
mism in the Theory of Functional Sentence Perspective. Phi-
lologica Pragensia, 8, 135-144. 
Givon, T. (1985). Iconicity, Isomorphism and Non-arbitrary 
Coding in Syntax. In J. Haiman (Ed.), Iconicity in Syntax (pp. 
187-219): John Benjamins. 
Grosz, B., & Sidner, C. (1986). Attention, Intentions, and the 
Structure of Discourse. Computational Linguistics, 12(3), 175-
204. 
Halliday, M. A. K. (1967). Intonation and Grammar in British 
English. The Hague: Mouton. 
Kendon, A. (1972). Some Relationships between Body Mo-
tion and Speech. In A. W. Siegman & B. Pope (Eds.), Studies 
in Dyadic Communication (pp. 177-210). Elmsford, NY: Per-
gamon Press. 
Kurohashi, S., & Nagao, M. (1994). A Syntactic Analysis 
Method of Long Japanese Sentences Based on the Detection 
of Conjunctive Structures. Computational Linguistics, 20(4), 
507-534. 
Lester, J. C., Stone, B., & Stelling, G. (1999). Lifelike Peda-
gogical agents for Mixed-Initiative Problem Solving in Con-
structivist Learning Environments. User Modeling and User-
Adapted Interaction, 9(1-2), 1-44. 
McNeill, D. (1992). Hand and Mind: What Gestures Reveal 
about Thought. Chicago, IL/London, UK: The University of 
Chicago Press. 
Prevost, S. A. (1996). An Informational Structural Approach 
to Spoken Language Generation. Paper presented at the 34th 
Annual Meeting of the Association for Computational Lin-
guistics, Santa Cruz, CA. 
Rickel, J., & Johnson, W. L. (1999). Animated Agents for 
Procedural Training in Virtual Reality: Perception, Cognition 
and Motor Control. Applied Artificial Intelligence, 13(4-5), 
343-382. 
Rogers, W. (1978). The Contribution of Kinesic Illustrators 
towards the Comprehension of Verbal Behavior within Utter-
ances. Human Communication Research, 5, 54-62. 
 
<Gaze type="towards">
shindo-ga
<Gesture_right type="contrast" handshape_right="stroke1@2">
atae-rareru-to-ka
</Gesture_right> 
sore-ni
kawaru
kasokudo-ga
<Gesture_right type="contrast" handshape_right="stroke2@2">
atae-rareru-to
</Gesture_right> 
iu-youna
<Gesture_right type="best" handshape_right="stroke1">
jyoukyou-de
</Gesture_right>
?  
Figure 3: Example of CAST output
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 641?648,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Concept Unification of Terms in Different Languages for IR 
 
Qing Li,  Sung-Hyon Myaeng 
Information & Communications 
University, Korea 
{liqing,myaeng}@icu.ac.kr
Yun Jin  
Chungnam National 
University, Korea 
wkim@cnu.ac.kr 
Bo-yeong Kang 
Seoul National University, 
Korea 
comeng99@snu.ac.kr 
 
  
 
Abstract 
Due to the historical and cultural reasons, 
English phases, especially the proper 
nouns and new words, frequently appear 
in Web pages written primarily in Asian 
languages such as Chinese and Korean. 
Although these English terms and their 
equivalences in the Asian languages refer 
to the same concept, they are erroneously 
treated as independent index units in tra-
ditional Information Retrieval (IR). This 
paper describes the degree to which the 
problem arises in IR and suggests a novel 
technique to solve it. Our method firstly 
extracts an English phrase from Asian 
language Web pages, and then unifies the 
extracted phrase and its equivalence(s) in 
the language as one index unit. Experi-
mental results show that the high preci-
sion of our conceptual unification ap-
proach greatly improves the IR perform-
ance. 
1 Introduction 
The mixed use of English and local languages 
presents a classical problem of vocabulary mis-
match in monolingual information retrieval 
(MIR). The problem is significant especially in 
Asian language because words in the local lan-
guages are often mixed with English words. Al-
though English terms and their equivalences in a 
local language refer to the same concept, they are 
erroneously treated as independent index units in 
traditional MIR. Such separation of semantically 
identical words in different languages may limit 
retrieval performance. For instance, as shown in 
Figure 1, there are three kinds of Chinese Web 
pages containing information related with 
?Viterbi Algorithm (?????)?.  The first 
case contains ?Viterbi Algorithm? but not its 
Chinese equivalence ???????. The second 
 
 
Figure 1.  Three Kinds of Web Pages 
contains ??????? but not ?Viterbi Algo-
rithm?. The third has both of them. A user would 
expect that a query with either ?Viterbi Algo-
rithm? or ??????? would retrieve all of 
these three groups of Chinese Web pages. Oth-
erwise some potentially useful information will 
be ignored.  
Furthermore, one English term may have sev-
eral corresponding terms in a different language. 
For instance, Korean words ?????, ?????, 
and ????? are found in local Web pages, 
which all correspond to the English word ?digi-
tal? but are in different forms because of differ-
ent phonetic interpretations. Establishing an 
equivalence class among the three Korean words 
and the English counterpart is indispensable. By 
doing so, although the query is ?????, the 
Web pages containing ?????, ????? or 
?digital? can be all retrieved. The same goes to 
Chinese terms. For example, two same semantic 
Chinese terms ????? and ????? corre-
spond to one English term ?Viterbi?. There 
should be a semantic equivalence relation be-
tween them. 
Although tracing the original English term 
from a term in a native language by back trans-
literation (Jeong et al, 1999) is a good way to 
build such mapping, it is only applicable to the 
words that are amenable for transliteration based 
on the phoneme. It is difficult to expand the 
method to abbreviations and compound words. 
641
Since English abbreviations frequently appear in 
Korean and Chinese texts, such as 
??????? (WTO)? in Korean, ?????
?? (WTO)? in Chinese, it is essential in IR to 
have a mapping between these English abbrevia-
tions and the corresponding words. The same 
applies to the compound words like ???? 
(Seoul National University)? in Korean, ????
(mad cow disease)? in Chinese. Realizing the 
limitation of the transliteration, we present a way 
to extract the key English phrases in local Web 
pages and conceptually unify them with their 
semantically identical terms in the local language.  
2 Concept Unification 
The essence of the concept unification of terms 
in different languages is similar to that of the 
query translation for cross-language information 
retrieval (CLIR) which has been widely explored 
(Cheng et al, 2004; Cao and Li, 2002; Fung et 
al., 1998; Lee, 2004; Nagata et al, 2001; Rapp, 
1999; Zhang et al, 2005; Zhang and Vine, 2004).  
For concept unification in index, firstly key Eng-
lish phrases should be extracted from local Web 
pages. After translating them into the local lan-
guage, the English phrase and their translation(s) 
are treated as the same index units for IR. Differ-
ent from previous work on query term translation 
that aims at finding relevant terms in another 
language for the target term in source language, 
conceptual unification requires a high translation 
precision. Although the fuzzy Chinese transla-
tions (e.g. ??? (virus), ???  (designer?s 
name), ???? (computer virus)) of  English 
term ?CIH? can enhance the CLIR performance 
by the ?query expansion? gain (Cheng et al, 
2004), it does not work in the conceptual unifica-
tion of terms in different languages for IR.   
While there are lots of additional sources to be 
utilized for phrase translation (e.g., anchor text, 
parallel or comparable corpus), we resort to the 
mixed language Web pages which are the local 
Web pages with some English words, because 
they are easily obtainable and frequently self-
refresh.  
Observing the fact that English words some-
times appear together with their equivalence in a 
local language in Web texts as shown in Figure 1, 
it is possible to mine the mixed language search-
result pages obtained from Web search engines 
and extract proper translations for these English 
words that are treated as queries. Due to the lan-
guage nature of Chinese and Korean, we inte-
grate the phoneme and semanteme instead of 
statistical information alone to pick out the right 
translation from the search-result pages. 
3 Key Phrase Extraction 
Since our intention is to unify the semantically 
identical words in different languages and index 
them together, the primary task is to decide what 
kinds of key English phrases in local Web pages 
are necessary to be conceptually unified.  
In (Jeong et al, 1999), it extracts the Korean 
foreign words for concept unification based on 
statistical information. Some of the English 
equivalences of these Korean foreign words, 
however, may not exist in the Korean Web pages. 
Therefore, it is meaningless to do the cross-
language concept unification for these words. 
The English equivalence would not benefit any 
retrieval performance since no local Web pages 
contain it, even if the search system builds a se-
mantic class among both local language and 
English for these words. In addition, the method 
for detecting Korean foreign words may bring 
some noise. The Korean terms detected as for-
eign words sometimes are not meaningful. 
Therefore, we do it the other way around by 
choosing the English phrases from the local Web 
pages based on a certain selection criteria.  
Instead of extracting all the English phrases in 
the local Web pages, we only select the English 
phrases that occurred within the special marks 
including quotation marks and parenthesis. Be-
cause English phrases within these markers re-
veal their significance in information searching 
to some extent. In addition, if the phrase starts 
with some stemming words (e.g., for, as) or in-
cludes some special sign, it is excluded as the 
phrases to be translated.  
4 Translation of English Phrases 
In order to translate the English phrases extracted, 
we query the search engine with English phrases 
to retrieve the local Web pages containing them. 
For each document returned, only the title and 
the query-biased summary are kept for further 
analysis. We dig out the translation(s) for the 
English phrases from these collected documents.  
4.1 Extraction of Candidates for Selection 
After querying the search engine with the Eng-
lish phrase, we can get the snippets (title and 
summary) of Web texts in the returned search-
result pages as shown in Figure 1. The next step 
then is to extract translation candidates within a 
window of a limited size, which includes the 
642
English phrase, in the snippets of Web texts in 
the returned search-result pages. Because of the 
agglutinative nature of the Chinese and Korean 
languages, we should group the words in the lo-
cal language into proper units as translation can-
didates, instead of treating each individual word 
as candidates. There are two typical ways: one is 
to group the words based on their co-occurrence 
information in the corpus (Cheng et al, 2004), 
and the other is to employ all sequential combi-
nations of the words as the candidates (Zhang 
and Vine, 2004). Although the first reduces the 
number of candidates, it risks losing the right 
combination of words as candidates. We adopt 
the second in our approach, so that,  return to the 
aforementioned example in Figure 1, if there are 
three Chinese characters (???) within the pre-
defined window, the translation candidates for 
English phrases ?Viterbi? are  ???,???, ???, 
??? ?, ????, and ?????. The number of 
candidates in the second method, however, is 
greatly increased by enlarging the window size 
k . Realizing that the number of words, n , avail-
able in the window size, k , is generally larger 
than the predefined maximum length of candi-
date, m ,  it is unreasonable to use all adjacent 
sequential combinations of available words 
within the window size k . Therefore, we tune 
the method as follows: 
1. If n m? , all adjacent sequential combina-
tions of words within the window are treated as 
candidates 
2. If n m> , only adjacent sequential combina-
tions of which the word number is less than m  
are regarded as candidates. For example, if we 
set n  to 4 and m  to 2, the window ? 1 2 3 4w w w w ? 
consists of four words. Therefore, only ? 1 2w w ?, 
? 2 3w w ?, ? 3 4w w ?, ? 1w ?, ? 2w ?? ? 3w ?, ? 4w ? are 
employed as the candidates for final translation 
selection.  
Based on our experiments, this tuning method 
achieves the same performance while reducing 
the candidate size greatly. 
4.2 Selection of candidates  
The final step is to select the proper candidate(s) 
as the translation(s) of the key English phrase. 
We present a method that considers the statistical, 
phonetic and semantic features of the English 
candidates for selection.  
Statistical information such as co-occurrence, 
Chi-square, mutual information between the 
English term and candidates helps distinguish the 
right translation(s). Using Cheng?s Chi-square 
method (Cheng et al, 2004), the probability to 
find the right translation for English specific 
term is around 30% in the top-1 case and 70% in 
the top-5 case. Since our goal is to find the corre-
sponding counterpart(s) of the English phrase to 
treat them as one index unit in IR, the accuracy 
level is not satisfactory. Since it seems difficult 
to improve the precision solely through variant 
statistical methods, we also consider semantic 
and phonetic information of candidates besides 
the statistical information. For example, given 
the English Key phrase ?Attack of the clones?, 
the right Korean translation ??????? is far 
away from the top-10 selected by Chi-square 
method (Cheng et al, 2004). However, based on 
the semantic match of ???? and ?Attack?, and 
the phonetic match of ???? and ?clones?, we 
can safely infer they are the right translation. The 
same rule applies to the Chinese translation  ??
??????, where ????? is phonetically 
match for ?clones? and ???? semantically cor-
responds to ?attack?.  
 In selection step, we first remove most of the 
noise candidates based on the statistical method 
and re-rank the candidates based on the semantic 
and phonetic similarity. 
4.3 Statistical model 
There are several statistical models to rank the 
candidates.  Nagata (2001) and Huang (2005) use 
the frequency of co-occurrence and the textual 
distance, the number of words between the Key 
phrase and candidates in texts to rank the candi-
dates, respectively. Although the details of the 
methods are quite different, both of them share 
the same assumption that the higher co-
occurrence between candidates and the Key 
phrase, the more possible they are the right trans-
lations for each other. In addition, they observed 
that most of the right translations for the Key 
phrase are close to it in the text, especially, right 
after or before the key phrase (e.g. ? ?  
?????(FBI)???). Zhang (2004) sug-
gested a statistical model based on the frequency 
of co-occurrence and the length of the candidates. 
In the model, since the distance between the key 
phrase and a candidate is not considered, the 
right translation located far away from the key 
phrase also has a chance to be selected. We ob-
serve, however, that such case is very rare in our 
study, and most of right translations are located 
within 5~8 words. The distance information is a 
valuable factor to be considered.  
643
In our statistical model, we consider the fre-
quency, length and location of candidates to-
gether. The intuition is that if the candidate is the 
right translation, it tends to co-occur with the key 
phrase frequently; its location tends to be close to 
the key phrase; and the longer the candidates? 
length, the higher the chance to be the right 
translation. The formula to calculate the ranking 
score for a candidate is as follows: 
1
( ) ( , )( , ) (1 )
max max
ki k i
FL i
len Freq len
len c d q cw q c ? ?
?
= ? + ? ?
?
 
where ( , )k id q c  is the word distance between the 
English phrase  q  and the candidate ic  in the k-
th occurrence of candidate in the search-result 
pages. If  q  is adjacent to ic  , the word distance 
is one. If there is one word between them, it is 
counted as two and so forth.  ?  is the coefficient 
constant, and maxFreq len?  is the max reciprocal of 
( , )k id q c  among all the candidates. ( )ilen c  is the 
number of characters in the candidate ic .  
4.4 Phonetic and semantic model 
Phonetic and semantic match: There has been 
some related work on extracting term translation 
based on the transliteration model (Kang and 
Choi, 2002; Kang and Kim, 2000). Different 
from transliteration that attempts to generate 
English transliteration given a foreign word in 
local language, our approach is a kind a match 
problem since we already have the candidates 
and aim at selecting the right candidates as the 
final translation(s) for the English key phrase. 
While the transliteration method is partially 
successful, it suffers form the problem that trans-
literation rules are not applied consistently. The 
English key phrase for which we are looking for 
the translation sometimes contains several words 
that may appear in a dictionary as an independent 
unit. Therefore, it can only be partially matched 
based on the phonetic similarity, and the rest part 
may be matched by the semantic similarity in 
such situation. Returning to the above example, 
?clone? is matched with ???? by phonetic 
similarity. ?of? and ?attack? are matched with 
??? and ???? respectively by semantic simi-
larity. The objective is to find a set of mappings 
between the English word(s) in the key phrase 
and the local language word(s) in candidates, 
which maximize the sum of the semantic and 
phonetic mapping weights. We call the sum as 
SSP (Score of semanteme and phoneme). The 
higher SSP value is, the higher the probability of 
the candidate to be the right translation.  
The solution for a maximization problem can 
be found using an exhaustive search method. 
However, the complexity is very high in practice 
for a large number of pairs to be processed.  As 
shown in Figure 2, the problem can be repre-
sented as a bipartite weighted graph matching 
problem. Let the English key phrase, E, be repre-
sented as a sequence of tokens 1,..., mew ew< > , and 
the candidate in local language, C, be repre-
sented as a sequence of tokens 1,..., ncw cw< > . 
Each English and candidate token is represented 
as a graph vertex. An edge ( , )i jew cw  is formed 
with the weight ( , )i jew cw?  calculated as the av-
erage of normalized semantic and phonetic val-
ues, whose calculation details are explained be-
low. In order to balance the number of vertices 
on both sides, we add the virtual vertex (vertices) 
with zero weight on the side with less number of 
vertices. The SSP is calculated:  
n
( )
i=1
SSP=argmax ( , )i ikw ew???
 
where ?  is a permutation of {1, 2, 3, ?, n}. It 
can be solved by the Kuhn-Munkres algorithm 
(also known as Hungarian algorithm) with poly-
nomial time complexity (Munkres, 1957).  
 
 
Figure 2. Matching based on the semanteme and 
phoneme 
Phonetic & Semantic Weights: If two lan-
guages have a close linguistic relationship such 
as English and French, cognate matching (Davis, 
1997) is typically employed to translate the un-
translatable terms. Interestingly, Buckley et al, 
(2000) points out that ?English query words are 
treated as potentially misspelled French words? 
and attempts to treat English words as variations 
of French words according to lexicographical 
rules.  However, when two languages are very 
distinct, e.g., English?Korean, English?Chinese, 
transliteration from English words is utilized for 
cognate matching. 
Phonetic weight is the transliteration probabil-
ity between English and candidates in local lan-
guage. We adopt the method in (Jeong et al, 
1999) with some adjustments. In essence, we 
compute the probabilities of particular English 
?? ?? ?
The of Clones Attack
644
key phrase EW given a candidate in the local 
language CW.  
1 1
1 1 1
( , ) ( ,..., , ,..., )
1( ,..., , ,..., ) log ( | ) ( | )
phoneme phoneme m k
phoneme n k j j j j
j
EW CW e e c c
g g c c P g g P c g
n
? ?
? ?
=
= = ?
 
where the English phrase consists of a string of 
English alphabets 1,..., me e , and the candidate in 
the local language is comprised of  a string of 
phonetic elements. 1,..., kc c . For Korean language, 
the phonetic element is the Korean alphabets 
such as ???, ???, ??? , ??? and etc. For Chi-
nese language, the phonetic elements mean the 
elements of ?pinying?.  ig  is a pronunciation unit 
comprised of one or more English alphabets 
( e.g., ?ss? for ???, a Korean alphabet ).  
The first term in the product corresponds to 
the transition probability between two states in 
HMM and the second term to the output prob-
ability for each possible output that could corre-
spond to the state, where the states are all possi-
ble distinct English pronunciation units for the 
given Korean or Chinese word. Because the dif-
ference between Korean/Chinese and English 
phonetic systems makes the above uni-gram 
model almost impractical in terms of output 
quality, bi-grams are applied to substitute the 
single alphabet in the above equation. Therefore, 
the phonetic weight should be calculated as:  
1 1 1 1
1( , ) log ( | ) ( | )phoneme j j j j j j j j
j
E C P g g g g P c c g g
n
? + ? + += ?
where 1 1( | )j j j jP c c g g+ +  is computed from the 
training corpus as the ratio between the fre-
quency of 1j jc c +  in the candidates, which were 
originated from 1j jg g + in English words, to the 
frequency of 1j jg g + . If 1j =  or j n= , 1jg ?  or 
1jg + , 1jc +  is substituted with a space marker.  
The semantic weight is calculated from the bi-
lingual dictionary. The current bilingual diction-
ary we employed for the local languages are Ko-
rean-English WorldNet and LDC Chinese-
English dictionary with additional entries in-
serted manually. The weight relies on the degree 
of overlaps between an English translation and 
the candidate  
semanteme
No. of  overlapping unitsw (E,C)= argmax
total No. of   units  
 
For example, given the English phrase ?Inha 
University? and its candidate ???? (Inha 
University), ?University? is translated into 
?????, therefore, the semantic weight be-
tween ?University? and ??? is about 0.33 be-
cause only one third of the full translation is 
available in the candidate. 
Due to the range difference between phonetic 
and semantic weights, we normalized them by 
dividing the maximum phonetic and semantic 
weights in each pair of the English phrase and a 
candidate if the maximum is larger than zero.  
The strategy for us to pick up the final transla-
tion(s) is distinct on two different aspects from 
the others. If the SSP values of all candidates are 
less than the threshold, the top one obtained by 
statistical model is selected as the final transla-
tion. Otherwise, we re-rank the candidates ac-
cording to the SSP value. Then we look down 
through the new rank list and draw a ?virtual? 
line if there is a big jump of SSP value. If there is 
no big jump of SSP values, the ?virtual? line is 
drawn at the bottom of the new rank list. Instead 
of the top-1 candidate, the candidates above the 
?virtual? line are all selected as the final transla-
tions. It is because that an English phrase may 
have more than one correct translation in the lo-
cal language. Return to the previous example, the 
English term ?Viterbi? corresponds to two Chi-
nese translations ????? and ?????. The 
candidate list based on the statistical information 
is ???, ??, ??, ???,?,????. We 
then calculate the SSP value of these candidates 
and re-rank the candidates whose SSP values are 
larger than the threshold which we set to 0.3. 
Since the SSP value of ????(0.91)? and ??
??(0.91)? are both larger than the threshold 
and there is no big jump, both of them are se-
lected as the final translation.  
5 Experimental Evaluation  
Although the technique we developed has values 
in their own right and can be applied for other 
language engineering fields such as query trans-
lation for CLIR, we intend to understand to what 
extent monolingual information retrieval effec-
tiveness can be increased when relevant terms in 
different language are treated as one unit while 
indexing. We first examine the translation preci-
sion and then study the impact of our approach 
for monolingual IR. 
We crawls the web pages of a specific domain 
(university & research) by WIRE crawler pro-
vided by center of Web Research, university of 
Chile (http://www.cwr.cl/projects/WIRE/). Cur-
rently, we have downloaded 32 sites with 5,847 
645
Korean Web pages and 74 sites with 13,765 Chi-
nese Web pages. 232 and 746 English terms 
were extracted from Korean Web pages and Chi-
nese Web pages, respectively.  The accuracy of 
unifying semantically identical words in different 
languages is dependant on the translation per-
formance. The translation results are shown in 
table 1.  As it can be observed, 77% of English 
terms from Korean web pages and 83% of Eng-
lish terms from Chinese Web pages can be 
strictly translated into accurate Korean and Chi-
nese, respectively. However, additional 15% and 
14% translations contained at least one Korean 
and Chinese translations, respectively.  The er-
rors were brought in by containing additional 
related information or incomplete translation. For 
instance, the English term ?blue chip? is trans-
lated into ???(blue chip)?, ???? (a kind of 
stock)?. However, another acceptable translation 
???? (a kind of stock)? is ignored.  An ex-
ample for incomplete translation is English 
phrase ? SIGIR 2005? which only can be trans-
late into ?????????? (international 
conference of computer information retrieval? 
ignoring the year.  
 
Korean Chinese  
No. % No. % 
Exactly correct 179 77% 618 83% 
At least one is 
correct but not all 35 15% 103 14% 
Wrong translation 18 8% 25 3% 
Total 232 100% 746 100% 
Table 1. Translation performance 
We also compare our approach with two well-
known translation systems. We selected 200 
English words and translate them into Chinese 
and Korean by these systems.  Table2 and Table 
3 show the results in terms of the top 1, 3, 5 in-
clusion rates for Korean and Chinese translation, 
respectively. ?Exactly and incomplete? transla-
tions are all regarded as the right translations.  
?LiveTrans? and ?Google? represent the systems 
against which we compared the translation abil-
ity. Google provides a machine translation func-
tion to translate text such as Web pages. Al-
though it works pretty well to translate sentences, 
it is ineligible for short terms where only a little 
contextual information is available for translation. 
LiveTrans (Cheng et al, 2004) provided by the 
WKD lab in Academia Sinica is the first un-
known word translation system based on web-
mining. There are two ways in this system to 
translate words: the fast one with lower precision 
is based on the ?chi-square? method ( 2? ) and the 
smart one with higher precision is based on ?con-
text-vector? method (CV) and ?chi-square? 
method ( 2? ) together. ?ST? and ?ST+PS? repre-
sent our approaches based on statistic model and 
statistic model plus phonetic and semantic model, 
respectively.   
 
 Top -1 Top-3 Top -5
Google 56% NA NA 
?Fast? 
2? 37% 43% 53.5%Live 
Trans ?Smart? 
2? +CV 
42% 49% 60% 
ST(dk=1) 28.5 % 41% 47% 
ST 39 % 46.5% 55.5%
Our 
Methods
ST+PS 93% 93% 93% 
Table 2. Comparison (Chinese case) 
 
 Top -1 Top-3 Top -5
Google 44% NA NA 
?Fast? 
2? 28% 37.5% 45% Live 
Trans ?Smart? 
2? +CV 
24.5% 44% 50% 
ST(dk=1) 26.5 % 35.5% 41.5%
ST 32 % 40% 46.5%
Our 
Methods
ST+PS 89% 89.5% 89.5%
Table 3. Comparison  (Korean case) 
 
Even though the overall performance of Li-
veTrans? combined method ( 2? +CV) is better 
than the simple method ( 2? ) in both Table 2 and 
3, the same doesn?t hold for each individual. For 
instance, ?Jordan?  is the English translation of 
Korean term   ?????, which  ranks 2nd and 
5th in ( 2? )  and ( 2? +CV), respectively. The con-
text-vector sometimes misguides the selection.  
In our two-step selection approach, the final 
selection would not be diverted by the false sta-
tistic information. In addition, in order to exam-
ine the contribution of distance information in 
the statistical method, we ran our experiments 
based on statistical method (ST) with two differ-
ent conditions. In the first case, we set  ( , )k id q c  to 
1, that is, the location information of all candi-
dates is ignored. In the second case, ( , )k id q c  is 
calculated based on the real textual distance of 
the candidates. As in both Table 2 and Table 3, 
the later case shows better performance. 
As shown in both Table 2 and Table 3, it can 
be observed that ?ST+PS? shows the best per-
formance, then followed by ?LiveTrans (smart)?, 
?ST?, ?LiveTrans(fast)?, and ?Google?. The sta-
646
tistical methods seem to be able to give a rough 
estimate for potential translations without giving 
high precision. Considering the contextual words 
surrounding the candidates and the English 
phrase can further improve the precision but still 
less than the improvement made by the phonetic 
and semantic information in our approach. High 
precision is very important to the practical appli-
cation of the translation results. The wrong trans-
lation sometimes leads to more damage to its 
later application than without any translation 
available.  For instance, the Chinese translation 
of ?viterbi? is ???(algorithm)? by LiveTrans 
(fast). Obviously, treating ?Viterbi? and  ??? 
(algorithm)?as one index unit is not acceptable.    
We ran monolingual retrieval experiment to 
examine the impact of our concept unification on 
IR. The retrieval system is based on the vector 
space model with our own indexing scheme to 
which the concept unification part was added. 
We employed the standard tf idf?  scheme for 
index term weighting and idf  for query term 
weighting. Our experiment is based on KT-SET 
test collection (Kim et al, 1994). It contains 934 
documents and 30 queries together with rele-
vance judgments for them. 
In our index scheme, we extracted the key 
English phrases in the Korean texts, and trans-
lated them. Each English phrases and its equiva-
lence(s) in Korean is treated as one index unit. 
The baseline against which we compared our 
approach applied a relatively simple indexing 
technique. It uses a dictionary that is Korean-
English WordNet, to identify index terms. The 
effectiveness of the baseline scheme is compara-
ble with other indexing methods (Lee and Ahn, 
1999). While there is a possibility that an index-
ing method with a full morphological analysis 
may perform better than our rather simple 
method, it would also suffer from the same prob-
lem, which can be alleviated by concept unifica-
tion approach. As shown in Figure 3, we ob-
tained 14.9 % improvement based on mean aver-
age 11-pt precision. It should be also noted that 
this result was obtained even with the errors 
made by the unification of semantically identical 
terms in different languages. 
6 Conclusion 
In this paper, we showed the importance of the 
unification of semantically identical terms in dif-
ferent languages for Asian monolingual informa-
tion retrieval, especially Chinese and Korean. 
Taking the utilization of the high translation ac-
curacy of our previous work, we successfully 
unified the most semantically identical terms in 
the corpus.  This is along the line of work where 
researchers attempt to index documents with 
concepts rather than words. We would extend 
our work along this road in the future. 
 
Recall
0.0 .2 .4 .6 .8 1.0
P
re
ci
si
on
0.0
.2
.4
.6
.8
1.0
Baseline
Conceptual Unification
 
Figure 3. Korean Monolingual IR 
Reference 
Buckley, C., Mitra, M., Janet, A. and Walz, C.C.. 
2000.  Using Clustering and Super Concepts within 
SMART: TREC 6. Information Processing & 
Management. 36(1): 109-131. 
Cao, Y. and Li., H.. 2002.  Base Noun Phrase Transla-
tion Using Web Data and the EM Algorithm. In 
Proc. of. the 19th COLING. 
Cheng, P.,  Teng, J., Chen, R., Wang, J., Liu,W., 
Chen, L.. 2004.  Translating Specific Queries with 
Web Corpora for Cross-language Information Re-
trieval. In Proc. of ACM SIGIR. 
Davis, M.. 1997. New Experiments in Cross-language 
Text Retrieval at NMSU's Computing Research 
Lab. In Proc. Of TREC-5. 
Fung, P. and Yee., L.Y.. 1998. An IR Approach for 
Translating New Words from Nonparallel, Compa-
rable Texts. In Proc. of  COLING/ACL-98. 
Huang, F., Zhang, Y. and Vogel, S.. 2005. Mining 
Key Phrase Translations from Web Corpora, In 
Proc. of the Human Language Technologies Con-
ference (HLT-EMNLP).   
Jeong, K. S., Myaeng, S. H., Lee, J. S., Choi, K. S.. 
1999. Automatic identification and back-
transliteration of foreign words for information re-
trieval. Information Processing & Management. 
35(4): 523-540. 
Kang, B. J., and Choi, K. S. 2002. Effective Foreign 
Word Extraction for Korean Information Retrieval. 
Information Processing & Management, 38(1): 91-
109. 
647
Kang, I. H. and Kim, G. C.. 2000. English-to-Korean 
Transliteration using Multiple Unbounded Over-
lapping Phoneme Chunks. In Proc. of COLING . 
Kim, S.-H. et al. 1994. Development of the Test Set 
for Testing Automatic Indexing. In Proc. of the 
22nd KISS Spring Conference. (in Korean).  
Lee, J, H. and Ahn, J. S.. 1996. Using N-grams for 
Korean Test Retrieval. In Proc. of SIGIR. 
Lee, J. S.. 2004.  Automatic Extraction of Translation 
Phrase Enclosed within Parentheses using Bilin-
gual Alignment Method. In Proc. of the 5th China-
Korea Joint Symposium on Oriental Language 
Processing and Pattern Recognition. 
Munkres, J.. 1957. Algorithms for the Assignment 
and Transportation Problems. J. Soc. Indust. Appl. 
Math., 5 (1957).  
Nagata, M., Saito, T., and Suzuki, K.. 2001. Using the 
Web as a Bilingual Dictionary. In Proc. of ACL 
'2001 DD-MT Workshop. 
Rapp, R.. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
corpora. In Proc. of ACL. 
Zhang, Y., Huang, F. and Vogel, S.. 2005. Mining 
Translations of OOV Terms from the Web through 
Cross-lingual Query Expansion, In Proc. of ACM 
SIGIR-05. 
Zhang, Y. and Vines, P.. 2004. Using the Web for 
Automated Translation Extraction in Cross-
Language Information Retrieval. In Proc. of ACM 
SIGIR-04. 
648
An Approach for Combining Content-based and Collaborative Filters 
Qing Li 
Dept. of Computer Sciences 
Kumoh National Institute of Technology
Kumi, kyungpook, 730-701,South Korea
liqing@se.Kumoh.ac.kr 
 Byeong Man Kim 
Dept. of Computer Sciences  
Kumoh National Institute of Technology
Kumi, kyungpook, 730-701,South Korea
bmkim@se.Kumoh.ac.kr 
 
 
Abstract 
In this work, we apply a clustering tech-
nique to integrate the contents of items 
into the item-based collaborative filtering 
framework. The group rating information 
that is obtained from the clustering result 
provides a way to introduce content in-
formation into collaborative recommenda-
tion and solves the cold start problem. 
Extensive experiments have been con-
ducted on MovieLens data to analyze the 
characteristics of our technique. The re-
sults show that our approach contributes 
to the improvement of prediction quality 
of the item-based collaborative filtering, 
especially for the cold start problem. 
1 Introduction 
There are two dominant research paradigms of in-
formation filtering: content-based and collabora-
tive filtering. Content-based filtering selects the 
right information for users by comparing represen-
tations of searching information to representations 
of contents of user profiles which express interests 
of users. Content-based information filtering has 
proven to be effective in locating textual items 
relevant to a topic using techniques, such as Boo-
lean queries (Anick et al, 1990; Lee et al, 1993; 
Verhoeff et al, 1961), vector-space queries (Salton 
and Buckley, 1998), probabilistic model (Robert-
son and Sparck, 1976), neural network (Kim and 
Raghavan, 2000) and fuzzy set model (Ogawa et 
al., 1991). However, content-based filtering has 
some limitations: 
 
? It is hard for content-based filtering to pro-
vide serendipitous recommendations, be-
cause all the information is selected and 
recommended based on the content. 
? It is hard for novices to use content-based 
systems effectively. 
Collaborative filtering is the technique of using 
peer opinions to predict the interests of others. A 
target user is matched against the database to dis-
cover neighbors, who have historically had similar 
interests to target user. Items that neighbors like 
are then recommended to the target user. The Tap-
estry text filtering system, developed by Nichols 
and others at the Xerox Palo Alto Research Center 
(PARC), applied collaborative filtering (Douglas, 
1993; Harman, 1994). The GroupLens project at 
the University of Minnesota is a popular collabora-
tive system. Collaborative systems have been 
widely used in so many areas, such as Ringo sys-
tem recommends music albums (Upendar and Patti, 
1995), MovieLens system recommends movies, 
Jeter system recommends jokes (Gupta et al, 1999) 
and Flycasting recommends online radio (Hauver, 
2001). 
Collaborative filtering system overcomes some 
limitations of content-based filtering. The system 
can suggest items (the things to be recommended, 
such as books, music etc.) to users and recommen-
dations are based on the ratings of items, instead of 
the contents of the items, which can improve the 
quality of recommendations. Although collabora-
tive filtering has been successfully used in both 
research and practice, there still remain some chal-
lenges for it as an efficient information filtering. 
This work was supported by Korea Research Foundation 
Grant (KRF-2002-041-D00459). 
? Cold start problem, where recommendations 
are required for items that no user has yet 
rated.  
? Although collaborative filtering can improve 
the quality of recommendations based on the 
user ratings, it completely denies any infor-
mation that can be extracted from contents. 
It is obvious that the content-based filtering 
does not suffer the above problems. So it is a natu-
ral way to combine them in order to achieve a bet-
ter performance of filtering, and take the 
advantages of each. 
The rest of the paper is organized as follows. 
The next section provides a brief describing of re-
lated work. In section 3, we present the detail algo-
rithmic components of our approach, and look into 
the methods of grouping items, calculating the 
similarities between items and solving the cold 
start problem. Section 4 describes our experimental 
work. It provides details of our data sets, evalua-
tion metrics, results of our experiment and discus-
sion of the results. The final section provides some 
concluding remarks. 
2 Related work 
Proposed approaches to hybrid system, which 
combines content-based and collaborative filters 
together, can be categorized into two groups.  
One group is the linear combination of results 
of collaborative and content-based filtering, such 
as systems that are described by Claypool (1999) 
and Wasfi (1999). ProfBuilder (Wasfi, 1999) rec-
ommends web pages using both content-based and 
collaborative filters, and each creates a recommen-
dation list without combining them to make a 
combined prediction. Claypool (1999) describes a 
hybrid approach for an online newspaper domain, 
combining the two predictions using an adaptive 
weighted average: as the number of users access-
ing an item increases, the weight of the collabora-
tive component tends to increase. But how to 
decide the weights of collaborative and content-
based components is unclearly given by the author. 
The other group is the sequential combination 
of content-based filtering and collaborative filter-
ing. In this system, firstly, content-based filtering 
algorithm is applied to find users, who share simi-
lar interests. Secondly, collaborative algorithm is 
applied to make predictions, such as RAAP 
(Delgado et al, 1998) and Fab filtering systems 
(Balabanovic and Shoham, 1990). RAAP is a con-
tent-based collaborative information filtering for 
helping the user to classify domain specific infor-
mation found in the WWW, and also recommends 
these URLs to other users with similar interests. To 
decide the similar interests of users is using scal-
able Pearson correlation algorithm based on web 
page category. Fab system, which uses content-
based techniques instead of user ratings to create 
profiles of users. So the quality of predictions is 
fully depended on the content-based techniques, 
inaccurate profiles result in inaccurate correlations 
with other users and thus make poor predictions.  
As for collaborative recommendation, there are 
two ways to calculate the similarity for clique rec-
ommendation ? item-based and user-based. Sarwar 
(Sarwar et al 2001) has proved that item-based 
collaborative filtering is better than user-based col-
laborative filtering at precision and computation 
complexity. 
Figure1. Overview of the our approach 
3 Overview of our approach 
In this paper, we suggest a technique that intro-
duces the contents of items into the item-based 
collaborative filtering to improve its prediction 
quality and solve the cold start problem. Shortly, 
we call the technique ICHM (Item-based Cluster-
ing Hybrid Method). 
In ICHM, we integrate the item information and 
user ratings to calculate the item-item similarity. 
Figure 1 shows this procedure. The detail proce-
dure of our approach is described as follows: 
? Apply clustering algorithm to group the 
items, then use the result, which is repre-
sented by the fuzzy set, to create a group-
rating matrix. 
? Compute the similarity: firstly, calculate the 
similarity of group-rating matrix using ad-
justed-cosine algorithm, then calculate the 
similarity of item-rating matrix using Pear-
son correlation-based algorithm. At last, the 
Rating Data 
             
 + 
Item
rating
Collaborative 
filter Group
rating
Group
rater
Clustering Itemcontent
Item 
group 
vector
total similarity is the linear combination of 
the above two. 
? Make a prediction for an item by perform-
ing a weighted average of deviations from 
the neighbour?s mean. 
3.1  Group rating 
The goal of grouping ratings is to group the items 
into several cliques and provides content-based 
information for collaborative similarity calculation. 
    Each item has it?s own attribute features, such as 
movie item, which may have actor, actress, direc-
tor, genre, and synopsis as its attribute features. 
Thus, we can group the items based on them. 
The algorithm that is applied for grouping rat-
ings is derived from K-means Clustering Algo-
rithm (Han and Kamber, 2000). The difference is 
that we apply the fuzzy set theory to represent the 
affiliation between object and cluster. As shown in 
Figure 2, firstly, items are grouped into a given 
number of clusters. After completion of grouping, 
the probability of one object j (here one object 
means one item) to be assigned to a certain cluster 
is calculated as follows. 
( , )Pr ( , ) 1-                                (1)
( , )
CS j ko j k
MaxCS i k
=  
where Pr ( , )o j k means the probability of object j  to 
be assigned to cluster k ; The ( , )CS j k  means the 
function to calculate the counter-similarity be-
tween object j  and cluster k ;  ( , )Max CS i k means 
the maximum counter-similarity between an object 
and cluster k . 
 
Input : the number of clusters k  and item attributes  
Output: a set of k clusters that minimizes the squared-
error criterion, and the probability of each item to be 
assigned to each cluster center, which are represented as 
a fuzzy set. 
(1) Arbitrarily choose k  objects as the initial cluster 
centers 
(2) Repeat (a) and (b) until no change 
(a) (Re) assign each object to the cluster to which the 
object is the most similar, based on the mean value of 
the objects in the cluster 
(b) Update the cluster means, i.e., calculate the mean 
value of the objects of each cluster; 
(3) Compute the probability between objects and each 
cluster center. 
Figure 2.  Algorithm for grouping ratings 
The counter-similarity ( , )CS j k  can be calcu-
lated by Euclidean distance or Cosine method.  
3.2 Similarity computation 
As we can see, after grouping the items, we get a 
new rating matrix. We can use the item-based col-
laborative algorithm to calculate the similarity and 
make the predictions for users. 
There are many ways to compute the similarity. 
In our approach, we use two of them, and make a 
linear combination of their results. 
3.2.1 Pearson correlation-based similarity 
The most common measure for calculating the 
similarity is the Pearson correlation algorithm. 
Pearson correlation measures the degree to which a 
linear relationship exists between two variables. 
The Pearson correlation coefficient is derived from 
a linear regression model, which relies on a set of 
assumptions regarding the data, namely that the 
relationship must be linear, and the errors must be 
independent and have a probability distribution 
with mean 0 and constant variance for every set-
ting of the independent variable (McClave and 
Dietrich, 1998). 
, ,1
2 2
, ,1 1
( )( )cov( , )( , )      (2)
( ) ( )
m
u k k u l lu
m m
k l u k k u l lu i
R R R Rk lsim k l
R R R R? ?
=
= =
? ?= =
? ?
?
? ?  
where ( , )sim k l  means the similarity between item 
k  and l ; m  means the total number of users, who 
rated on both item k  and l ; kR , lR  are the average 
ratings of item k  and l , respectively;  
,u kR , ,u lR mean the rating of user u on item k  and l  
respectively. 
3.2.2 Adjust cosine similarity 
Cosine similarity once has been used to calculate 
the similarity of users but it has one shortcoming. 
The difference in rating scale between different 
users will result in a quite different similarity. For 
instance, if Bob only rates score 4 on the best 
movie, he never rates 5 on any movie; and he rates 
1 on the bad movie, instead of the standard level 
score 2. But Oliver always rates according to the 
standard level. He rates score 5 on the best movie, 
and 2 on the bad movie. If we use traditional co-
sine similarity, both of them are quite different. 
The adjusted cosine similarity (Sarwar et al, 2001) 
was provided to offset this drawback. 
, ,1
2 2
, ,1 1
( )( )
 ( , )               (3)
( ) ( )
m
u k u u l uu
m m
u k u u l uu u
R R R R
sim k l
R R R R
=
= =
? ?=
? ?
?
? ?  
where ( , )sim k l  means the similarity between item 
k  and l ; m  means the total number of users, who 
rates on both item k  and l ; uR  are the average rat-
ings of user u ; ,u kR , ,u lR mean the rating of user u on 
item k  and l  respectively. 
3.2.3 Linear combination of similarity 
Due to difference in value range between item-
rating matrix and group-rating matrix, we use dif-
ferent methods to calculate the similarity. As for 
item-ratings matrix, the rating value is integer; As 
for group-rating matrix, it is the real value ranging 
from 0 to 1. The natural way is to enlarge the con-
tinuous data range from [0 1] to [1 5] or reduce the 
discrete data range from [1 5] to [0 1] and then ap-
ply Pearson correlation-based algorithm or ad-
justed cosine algorithm to calculate similarity. We 
call this enlarged ICHM. We also propose another 
method: firstly, use Pearson correlation-based al-
gorithm to calculate the similarity from item-rating 
matrix, and then calculate the similarity from 
group-rating matrix by adjusted cosine algorithm, 
at last, the total user similarity is linear combina-
tion of the above two, we call this combination 
ICHM. 
 ( , ) ( , ) (1- ) ( , )            (4)item groupsim k l sim k l c sim k l c= ? + ?  
where ( , )sim k l  means the similarity between item 
k and l ; c  means the combination coefficient; 
( , )itemsim k l means that the similarity between item 
k and l , which is calculated from item-rating ma-
trix; ( , )groupsim k l means that the similarity between 
item k and l , which is calculated from group-
rating matrix. 
3.3 Collaborative prediction 
Prediction for an item is then computed by per-
forming a weighted average of deviations from the 
neighbour?s mean. Here we use top N  rule to se-
lect the nearest N  neighbours based on the simi-
larities of items. The general formula for a 
prediction on item k of user u (Resnick et al, 1994) 
is: 
,1
,
1
( ) ( , )
                          (5)
( , )
n
u i ii
u k k n
i
R R sim k i
P R
sim k i
=
=
? ?= + ? ?  
where ,u kP  represents the predication for the user 
u on item k ; n  means the total neighbours of item 
k ; ,u iR means the user u  rating on item i ; kR  is the 
average ratings on item k ; ( , )sim k i  means the simi-
larity between item k  and its? neighbour i ; iR  
means the average ratings on item i .  
3.4 Cold start problem 
In traditional collaborative filtering approach, it is 
hard for pure collaborative filtering to recommend 
a new item to user since no user made any rating 
on this new item. However, in our approach, based 
on the information from group-rating matrix, we 
can make predictions for the new item. In our ex-
periment, it shows a good recommendation per-
formance for the new items. In Equation 5, kR  is 
the average rating of all ratings on item k . As for 
the new item, no user makes any rating on it, kR  
should be the zero. Since kR  is the standard base-
line of user ratings and it is zero, it is unreasonable 
for us to apply Equation 5 to new item. Therefore, 
for a new item, we use the neighborsR , the average rat-
ing of all ratings on the new item?s nearest 
neighbour instead of kR , which is inferred by the 
group-rating matrix. 
3.5 A scenario of our approach 
z Users:  
         Number of users: three 
         User name: Tom, Jack, and Oliver 
z Items: 
         Item category: movie 
Number of items: five 
Title of items: Gone with the Wind, Pearl 
Harbour, Swordfish, Hero, The Sound of Music 
z Ratings: 1~5 integer score 
Too  bad:1  Bad:2  Common:3  Good:4  too good:5 
Table 1: Item-rating 
 Tom  Jack  Olive
Gone with the Wind 5 3  
Swordfish 5 2 4 
Pearl Harbour 2 5 
Hero 4 2  
The Sound of Music    
Table 2. Group-rating 
 Cluster1  Cluster2 
Gone with the Wind 98% 0.13% 
Swordfish 100% 0.02% 
Pearl Harbour 1.0% 95% 
Hero 95% 1.2% 
The Sound of Music 0.12% 98% 
The following is a procedure of our approach. 
? Based on the item contents, such as movie 
genre, director, actor, actress, even synopsis, 
we apply clustering algorithm to group the 
items. Here, we use fuzzy set to represent 
the clustering result. Assume the result is as 
follows: Cluster 1: {Gone with the Wind 
(98%), Swordfish (100%), Pearl Harbour 
(1.0%), Hero (95%), The Sound of Music 
(0.12%)}, Cluster 2: {Gone with the Wind 
(0.13%), Swordfish (0.02%), Pearl Harbour 
(95%), Hero (1.2%), The Sound of Music 
(98%)}, the number in the parenthesis fol-
lowing the movie name means the probabil-
ity of the movie belonging to the cluster. 
? We use group-rating engine to make a 
group-rating matrix. As Table 2 shows. 
Then combine the group-rating matrix and 
item-rating matrix to form a new rating ma-
trix.  
? Now, we can calculate the similarity be-
tween items based on this new unified rating 
data matrix. The similarity between items 
consists of two parts. The first part calcu-
lates the similarity based on user ratings, us-
ing the Pearson correlation-based algorithm. 
The second part calculates the similarity 
based on the clustering result by using ad-
justed cosine algorithm. The total similarity 
between items is the linear combination of 
them. For example, when we calculate the 
similarity between Gone with the Wind and 
Swordfish, firstly, itemsim(G,S) and groupsim(G,S)  
are calculated based on Equation 2 and 3 
separately. 
item 2 2 2 2
(5-4) (5-3.5)+(3-4) (2-3.5)sim(G,S) = 1
(5-4) +(3-4) (5-3.5) +(3.5-2)
? ? =?  
group
2 2 2 2
  sim(G,S) =
(0.98-0.59) (1-0.59)+(0.013-0.39) (0.002-0.39)
(0.98-0.59) +(0.013-0.39) (1-0.59) +(0.002-0.39)
0.9999
? ?
=
 
Secondly, sim(G,S) is calculated based on 
Formula 4, here the combination coefficient 
is 0.4. 
sim(G,S)=1 (1-0.4)+0.9999 0.4=0.9999 ? ?  
? Then, predictions for items are calculated by 
performing a weighted average of deviations 
from the neighbour?s mean.  
In the example, we can observe, the item - The 
Sound of Music, which no one make any rating on, 
can be treated as a new item. In traditional item-
based collaborative method, which makes predic-
tion only based on item-based matrix (Table 1), it 
is impossible to make predictions on this item. 
However, in our approach, we can make prediction 
for users, based on group rating (Table 2). 
From the description of our approach, we can 
observe that this approach can fully realize the 
strengths of content-based filtering, mitigating the 
effects of the new user problem. In addition, when 
calculating the similarity, our approach considers 
the information not only from personal tastes but 
also from the contents, which provides a latent 
ability for better prediction and makes serendipi-
tous recommendation. 
3.6 UCHM 
UCHM 
 Movie 1 Movie 2 Movie 3 Cluster 1 Cluster 2
User 1 ? ? ? ? ? 
User 2 ? ? ? ? ? 
User 3 ? ? ? ? ? 
ICHM 
ICHM User 1 User 2 User 3 Cluster 1 Cluster 2
Movie 1 ? ? ? ? ? 
Movie 2 ? ? ? ? ? 
Movie 3 ? ? ? ? ? 
Figure 3. UCHM & ICHM  
 
Clustering technique not only can be applied to 
item-based collaborative recommenders but also 
can be applied to user-based collaborative recom-
menders. Shortly we call the late one UCHM 
(User-based Clustering Hybrid Method)  
In UCHM, clustering is based on the attributes 
of user profiles and clustering result is treated as 
items. However, in ICHM, clustering is based on 
the attributes of items and clustering result is 
treated as users, as Figure 3 shows. 
In Combination UCHM, we apply Equation 2 to 
calculate the similarity in user-rating matrix, and 
User-rating   Matrix Group-rating Matrix   
Group-rating   Matrix   Item-rating Matrix 
Equation 3 to calculate the similarity in group-
rating matrix.  Then make a linear combination of 
them. When we apply Equation 2 and 3 to UCHM, 
k  and l  mean the user and u means the item, in-
stead the original meaning. 
As for UCHM, clustering is based on the user 
profiles. User profiles indicate the information 
needs or preferences on items that users are inter-
ested in. A user profile can consist of several pro-
file vectors and each profile vector represents an 
aspect of his preferences, such as movie genre, 
director, actor, actress and synopsis. The profile 
vectors are automatically constructed from rating 
data by the following simple equation. 
( )   /                                           8A m n=  
where, n is the number of items whose ranking 
value is lager than a given threshold, m is the num-
ber of items containing attribute A among n items 
and its ranking is larger than threshold. In our ex-
periment, we set the value of the threshold as 3.  
For example, in Section 3.5, Tom makes ratings on 
four movies, and three of them lager than the 
threshold 3. From the genre information, we know 
Gone with the Wind belongs to love genre, sword-
fish and Hero belong to action genre. So Tom?s 
profile is as follows. Tom {love (1/3), action (2/3)}. 
4 Experimental evaluations 
4.1   Data set 
Currently, we perform experiment on a subset of 
movie rating data collected from the MovieLens 
web-based recommender. The data set contained 
100,000 ratings from 943 users and 1,682 movies, 
with each user rating at least 20 items. We divide 
data set into a training set and a test data set.  
4.2 Evaluation metrics 
MAE (Mean Absolute Error) has widely been used 
in evaluating the accuracy of a recommender sys-
tem by comparing the numerical recommendation 
scores against the actual user ratings in the test 
data. The MAE is calculated by summing these 
absolute errors of the corresponding rating-
prediction pairs and then computing the average.  
, ,1                               (7)
n
u i u iu
P R
MAE
n
= ?= ?  
where ,u iP  means the user u prediction on item i ; 
,u iR  means the user u  rating on item i  in the test 
data; n is the number of rating-prediction pairs be-
tween the test data and the prediction result. The 
lower the MAE, the more accurate. 
4.3 Behaviours of our method  
0.735
0.745
0.755
0.765
0.775
0 5 10 20 30 40 50 60 70
No. of Clusters
MA
E
ICHM UCHM
 
Figure 4. Sensitivity of the cluster size 
We implement group-rating method described in 
section 3.1 and test them on MovieLens data with 
the different number of clusters. Figure 4 shows 
the experimental results. It can be observed that the 
number of clusters does affect the quality of pre-
diction, no matter in UCHM or ICHM. 
Figure 5. Coefficient 
In order to find the optimal combination coeffi-
cient c in the Equation 4, we conducted a series of 
experiments by changing combination coefficient 
from 0 to 1 with a constant step 0.1. Figure 5 
shows that when the coefficient arrives at 0.4, an 
optimal recommendation performance is achieved. 
0.73
0.735
0.74
0.745
0.75
0.755
0.76
10 20 30 40 50 60NO.of Neigbors
MA
E
Cosine Angle Euclidean Distance
Figure 6. Grouping items 
As described in Section 3.2, our grouping rat-
ings method needs to calculate similarity between 
0.73
0.74
0.75
0.76
0.77
0.78
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Coefficient
MA
E
ICHM UCHM
objects and clusters. So, we try two methods ? one 
is Euclidean distance and the other cosine angle. It 
can be observed in Figure 6 that the approach of 
cosine angle method has a trend to show better per-
formance than the Euclidean Distance method, but 
the difference is negligible. 
Figure 7. Comparison 
From the Figure 7, it can be observed that the 
performance of combination ICHM is the best, and 
the second is the enlarged ICHM, which is fol-
lowed by the item-based collaborative method, the 
last is UCHM (User-based Clustering Hybrid 
Method) which applies the clustering technique 
described in Section 3 to user-based collaborative 
filtering, where user profiles are clustered instead 
of item contents.  
We also can observe that the size of neighbour-
hood does affect the quality of prediction (Her-
locker et al, 1999). The performance improves as 
we increase the neighbourhood size from 10 to 30, 
then tends to be flat. 
1
2
3
4
5
1 2 3 4 5 6 7 8 9 10 11
Movie items
Ra
ti
ng
s
Real Value Predict Value
 
Figure 8.  Cold start problem 
Table 3. MAE of new item 
 10 20 30 40 50 100 
MAE 0.743 0.755 0.812 0.732 0.762 0.757
As for cold start problem, we choose the items 
from the training data set and delete all the ratings 
of those items, thus we can treat them as new items. 
First, we randomly selected item No.946. In the 
test data, user No.946 has 11 ratings, which is de-
scribed by bar real value in Figure 8. We can ob-
serve that the prediction for a new item can 
partially reflect the user preference. To generalize 
the observation, we randomly select the number of 
items from 10 to 50 with the step of 10 and 100 
from the test data, and delete all the ratings of 
those items and treat them as new items. Table 3 
shows that ICHM can solve the cold start problem. 
0.72
0.73
0.74
0.75
MA
E
Synopsis Genre
 
Figure 9. Item attribute 
When we apply clustering method to movie 
items, we use the item attribute ? movie genre. 
However, our approach can consider more dimen-
sion of item attribute, such as actor, actress, and 
director, even the synopsis. In order to observe the 
effect of the high dimension item attributes, we 
collect the 100 movie synopsis from Internet 
Movie Database (http://www.imdb.com) to provide 
attribute information for clustering movies. In our 
experiment, it shows that the correct attributes of 
movies can further improve the performance of 
recommender system, as Figure 9 shows. 
4.4    Our method versus the classic one 
Although some hybrid recommender systems have 
already exited, it is hard to make an evaluation 
among them. Some systems (Delgado et al, 1998) 
use Boolean value (relevant or irrelevant) to repre-
sent user preferences, while others use numeric 
value. The same evaluation metrics cannot make a 
fair comparison. Further more, the quality of some 
systems depends on the time, in which system pa-
rameters are changed with user feedback (Claypool 
et al, 1999), and Claypool does not clearly de-
scribe how to change the weight with time passed. 
However, we can make a simple concept compari-
son. In Fab system, the similarity for prediction is 
only based on the user profiles. As for UCHM, 
which groups the content information of user pro-
files and uses user-based collaborative algorithm 
instead of ICHM, the impact of combination coef-
ficient can be observed in Figure 5. In UCHM, 
when the value of coefficient equals to 1, it de-
scribes condition that Fab applied, which means 
the similarity between users is only calculated 
from the group-rating matrix. In that condition, the 
MAE shows the worst quality of recommendation.  
0.735
0.745
0.755
0.765
0.775
10 20 30 40 50 60 70 80 90 100
No. of neighbors
MA
E
Combination ICHM
Item-based Collaborative
Enlarged ICHM
Combination UCHM
5 Conclusions 
We apply clustering technique to the item content 
information to complement the user rating infor-
mation, which improves the correctness of collabo-
rative similarity, and solves the cold start problem. 
Our work indicates that the correct application of 
the item information can improve the 
recommendation performance. 
References 
Anick, P. G., Brennan, J. D., Flynn, R. A., Hanssen, D. 
R., Alvey, B. and Robbins, J.M.. 1990. A Direct Ma-
nipulation Interface for Boolean Information Re-
trieval via Natural Language Query, In Proc. ACM-
SIGIR Conf., pp.135-150.  
Balabanovic, M. and Shoham, Y.. 1997. Fab: Content-
Based, Collaborative Recommendation, Communica-
tions of the ACM, 40(3), pp.66-72. 
Claypool, M., Gokhale, A., Miranda, T., Murnikov, P., 
Netes, D. and Sartin, M.. 1999. Combining content-
based and collaborative filters in an online newspa-
per , In Proc. ACM-SIGIR Workshop on Recom-
mender Systems: Algorithms and Evaluation. 
Delgado, J., Ishii, N. and Ura, T.. 1998. Content-based 
Collaborative Information Filtering: Actively Learn-
ing to Classify and Recommend Documents, In Proc. 
Second Int. Workshop, CIA'98, pp.206-215. 
Douglas B. Terry. 1993. A tour through tapestry, In 
Proc. ACM Conf. on Organizational Computing Sys-
tems (COOCS). pp.21?30. 
Gupta, D., Digiovanni, M., Narita, H. and Goldberg, K.. 
1999.  Jester 2.0: A New Linear-Time Collaborative 
Filtering Algorithm Applied to Jokes, In Proc. ACM-
SIGIR Workshop on Recommender Systems: Algo-
rithms and Evaluation. 
Han, J., and Kamber, M.. 2000. Data mining: Concepts 
and Techniques. New York: Morgan-Kaufman. 
Harman D.. 1994. Overview of TREC-3, In Proc.TREC-
3, pp.1-19. 
Hauver, D. B.. 2001. Flycasting: Using Collaborative 
Filtering to Generate a Play list for Online Radio, In 
Int. Conf. on Web Delivery of Music. 
Herlocker, J., Konstan, J., Borchers A., and Riedl, J.. 
1999. An algorithmic framework for performing col-
laborative Filtering, In Proc. ACM-SIGIR Conf., 
1999, pp. 230-237. 
Kim, M. and Raghavan, V.V.. 2000. Adaptive concept-
based retrieval using a neural network, In Proc. Of 
ACM-SIGIR Workshop on Mathematical/Formal 
Methods in IR. 
McClave, J. T. and Dietrich, F. H.. 1998. Statistics. San 
Francisco: Ellen Publishing Company. 
Lee, J.H., Kim, M.H. and Lee, Y.H.. 1993.  Ranking 
documents in thesaurus-based Boolean retrieval sys-
tems, Information Processing and Management, 30(1), 
pp.79-91. 
Oard, D.W. and Marchionini, G.. 1996. A conceptual 
framework for text filtering, Technical Report EE-
TR-96-25, CAR-TR-830, CS-TR3643. 
Ogawa, Y., Morita, T. and Kobayashi, K.. 1991. A fuzzy 
document retrieval system using the keyword connec-
tion matrix and a learning method, Fuzzy sets and 
Systems, 1991, pp.39, pp.163-179. 
O'Conner, M. and Herlocker, J.. 1999. Clustering items 
for collaborative filtering, In Proc. ACM-SIGIR 
Workshop on Recommender Systems. 
Resnick, P., Iacovou, N., Suchak, M., Bergstorm, P. and 
Riedl, J.. 1994. GroupLens: An open architecture for 
collaborative filtering of Netnews, In Proc. ACM 
Conf. on Computer-Supported Cooperative Work. 
pp.175-186. 
Ricardo Baeza-Yates, Berthier Riberio-Neto. 1999. 
Modern Information Retrieval. New York:Addison-
Wesley Publishers. 
Robertson S. E. and Sparck Jones K.. 1976. Relevance 
weighting of search terms, J. of the American Society 
for Information Science, 1976, pp.27, pp.129-146. 
Salton, G. and Buckley, C.. 1988. Term-weight ap-
proaches in automatic retrieval, Information Proc-
essing and Management, 24(5), 1988, pp.513-523. 
Sarwar, B. M., Karypis, G., Konstan, J. A. and Riedl, J.. 
2001. Item-based Collaborative Filtering Recom-
mendation Algorithms, In Proc. Tenth Int. WWW 
Conf. 2001, pp. 285-295. 
Upendra, S. and Patti, M.. 1995. Social Information 
Filtering: Algorithms for Automating "Word of 
Mouth", In Proc. ACM CHI'95 Conf. on Human Fac-
tors in Computing Systems. pp.210?217. 
Verhoeff, J., Goffman, W. and Belzer, J.. 1961. Ineffi-
ciency of the use of the boolean functions for infor-
mation retrieval systems, Communications of the 
ACM, 4, pp.557--558, pp.594. 
Wasfi, A. M. A.. 1999. Collecting User Access Patterns 
for Building user Profiles and Collaborative Filter-
ing, In Int. Conf. on Intelligent User Interfaces. 
pp.57- 64. 
 
Extraction of User Preferences from a Few Positive Documents 
Byeong Man Kim, Qing Li 
Dept. of Computer Sciences 
Kumoh National Institute of Technology 
Kumi, kyungpook, 730-701,South Korea 
(Bmkim, liqing)@se.Kumoh.ac.kr
Jong-Wan Kim 
School of Computer & Information  
Taegu University 
Kyungsan-City, Kyungpook, South Korea
jwkim@biho.taegu.ac.kr 
 
 
Abstract 
In this work, we propose a new method 
for extracting user preferences from a few 
documents that might interest users. For 
this end, we first extract candidate terms 
and choose a number of terms called ini-
tial representative keywords (IRKs) from 
them through fuzzy inference. Then, by 
expanding IRKs and reweighting them us-
ing term co-occurrence similarity, the fi-
nal representative keywords are extracted. 
Performance of our approach is heavily 
influenced by effectiveness of selection 
method for IRKs so we choose fuzzy in-
ference because it is more effective in 
handling the uncertainty inherent in se-
lecting representative keywords of docu-
ments. The problem addressed in this 
paper can be viewed as the one of finding 
a representative vector of documents in 
the linear text classification literature. So, 
to show the usefulness of our approach, 
we compare it with two famous methods - 
Rocchio and Widrow-Hoff - on the 
Reuters-21578 collection. The results 
show that our approach outperforms the 
other approaches. 
1 Introduction 
Agent technology is able to provide increasingly 
more services for individuals, groups, and organi-
zations. Agents, which have been developed for 
Internet, have addressed many tasks such as infor-
mation finding, filtering and presentation, contract 
negotiation, and electronic commerce (Soltysiak 
and Crabtree, 2000). Most of them rely on the 
knowledge of the user. The inclusion of user in-
formation becomes a key area.  
A user model that represents some aspects of a 
user?s information needs or preferences can be use-
ful in any information system design, and in the 
case of information filtering (Kim et al, 2000). 
User models can be constructed by hand, or 
learned automatically based on feedback provided 
by the users. Some systems require users to explic-
itly specify their profiles, often as a set of key-
words or categories. But it is difficult for a user to 
exactly and correctly specify their information 
needs. The machine learning techniques offer the 
potential to automatic construction and continuous 
refinement of user model.  
The research systems adopting the machine 
learning techniques have been applied feedback 
techniques that explicitly provide relevance judg-
ments on documents. Studies have shown that such 
explicit feedback from the user is clearly useful 
(Goldberg, 1992; Yan and Garcia-Molina, 1995), 
but, in practice, many users are unwilling to pro-
vide relevance judgments on documents (Pazzani, 
M., Billsus, 1997; Baeza-Yates and  Ribeiro-Neto, 
1999) . Users may have problems to decide about 
some documents.  An alternative is to use implicit 
feedback where document relevance is inferred 
from user?s behavior, which has received increased 
attention in recent years (Nichols, 1997; Konstan et 
al., 1997; Kim, 2000)   
This paper focuses upon the extraction of user 
preferences from a few documents that might in-
terest a user. It does not consider how to provide 
relevance judgment on documents, i.e. it assumes 
This work was supported by grant No. 2000-1-51200-008-2
 from the Korea Science & Engineering Foundation 
that relevant documents are given explicitly or im-
plicitly. Our approach is based on the vector space 
model (Baeza-Yates and Ribeiro-Neto, 1999), 
where text-based documents are represented as 
vectors of term weights. So, the problem addressed 
in this paper is how to extract representative key-
words from documents provided by a user and 
what weights should be assigned to these keywords. 
We present a new technique to solve this problem. 
The proposed method is composed of two parts, 
one is to select initial representative keywords 
(IRKs) and the other is to automatically expand 
and reweight IRKs. For the first part, we can con-
sider feature selection methods (Yang and Peder-
sen, 1997) that focus on performance improvement 
and dimensionality reduction of document classifi-
ers for a huge amount of documents covering vari-
ous categories. However, since this kind of 
methods select features using information of other 
categories and negative document sets as well as 
positive ones, it is impossible to apply these to the 
target problem in this paper that extract feature 
keywords from only few positive documents in the 
same category. As alternatives, we can consider 
the Rocchio algorithm and Widrow-Hoff algorithm 
used as a training algorithm for linear text classi-
fier since these algorithms can extract keywords 
and assign weights to them effectively with only 
positive document sets. However, here, a new 
technique that adopts fuzzy inference to extract or 
generate IRKs from a few example documents (the 
set of documents judged relevant by the users) is 
suggested since the existing algorithms did not 
show good results as we expected.  
For the second part, we can choose one of 
query term expansion and term weight modifica-
tion methods based on vector model (Xu and Croft , 
1996; Mitra et al,1998; Baeza-Yates and  Ribeiro-
Neto, 1999). Instead, we take a new approach 
where the term co-occurrence similarity is intro-
duced as a measure of similarity between the dis-
tributions within the feedbacked documents of a 
given term and the initial query. With this similar-
ity and the document frequency in feedbacked 
documents, the weight of the term in the new query 
was calculated.  
In the next section, Rocchio and Widrow-Hoff 
algorithms are reviewed. Section 3 presents a 
method for user?s preference extraction. The ex-
periments to test the proposed method will be out-
lined in Section 4. Finally, conclusion is followed. 
2 Background 
To extract a user?s preference from example docu-
ments is the same problem as finding their 
representative vector in linear text classifiers. A 
variety of algorithms for training linear classifiers 
have been suggested. Among them, here, we only 
review two widely used algorithms, Rocchio algo-
rithm and Widrow-Hoff algorithm, for comparing 
with our method.  
The Rocchio algorithm (David et al, 1996) is a 
batch algorithm. So, it produces a new weight vec-
tor w  from an existing weight vector oldw  by ana-
lyzing the entire set of training data at once. The 
j th?  component of w   is : 
c
Ci
ji
c
Ci
ji
joldj nn
x
n
x
ww ?
?
?
?
+= ??
,,
, ???               (1) 
where, ,i jx  means j th?  component of i th?  docu-
ment vector ix  and  n  is the number of training 
documents. C is the set of positive training docu-
ments, and cn is the number of positive training 
documents. The parameter ?? , and ?  control the 
relative impact of the original weight vector, the 
positive examples, and the negative examples, re-
spectively. However, in our experiments, ? = 0, 
? =1, and ?  = 0 because only positive examples 
are given in our application. Neither original 
weight vector nor negative examples is given.  
The Widrow-Hoff algorithm (David et al, 1996) 
is an online algorithm where one training example 
is presented at a time. It updates its current weight 
vector based on the example and then discards the 
example, retaining only the new weight vector. A 
new weight vector wi+1 is computed from an old 
weight vector iw  and a training document ix  with 
class label iy . The class label iy  is 1 if a training 
document ix  is in the set of positive or relevant 
training documents, otherwise 0. In our application, 
iy  is always 1 because we deal with only positive 
examples. The initial weight vector w1 is typically 
set to zero vector, w1 = (0, ... 0). 
1, , ,2 ( )i j i j i i i i jw w w x y x?+ = ? ? ?               (2) 
where,  ?   is the learning rate which controls how 
quickly the weight vector w is allowed to change 
and ii xw ? is the cosine value of the two vectors. 
3 Extraction of user preferences 
User preferences are extracted from a few example 
documents through two steps: a) the first step gen-
erates a set of keywords called IRKs (Initial Repre-
sentative Keywords) which corresponds to the 
initial user query in the relevance feedback tech-
niques of IR and b) these IRKs are expanded and 
reweighted by a relevance feedback technique. 
It is very important to select IRKs reflecting 
user?s preferences well from example or training 
documents (set of documents judged relevant by 
the user) because we have to calculate term co-
occurrences similarity between these IRKs and 
candidate terms within each example document. 
Three factors of a term (term frequency, document 
frequency within positive examples, and IDF) are 
used to calculate the importance of a specific term.  
Since these factors essentially have inexact and 
uncertain characteristics, we combine them by 
fuzzy inference instead of a simple equation.  
The IRKs are selected based on the selection 
criteria that each example document has at least 
one or more IRKs. After selecting the IRKs, we 
perform term modification process based on the 
term co-occurrence similarity between these IRKs 
and candidate terms. The Rocchio and Widrow-
Hoff algorithms do not consider the term co-
occurrence relationship within training documents. 
But, we regard the term co-occurrence relationship 
as the key factor to calculate the importance of 
terms under the assumption that the IRKs reflect 
user?s preferences well.  
3.1 Calculation of the Representativeness of 
Terms through Fuzzy Inference 
The given positive examples are transformed into 
the set of candidate terms through eliminating 
stopwords and stemming by Porter?s algorithm. 
The TF, DF, and IDF of each term are calculated 
based on this set and used as inputs of fuzzy infer-
ence. From now on, we will explain these three 
input variables. The TF (Term Frequency) is the 
term frequency of a specific term not in a docu-
ment but in a set of documents, which is calculated 
by dividing total occurrences of the term in a set of 
documents by the number of documents in the set 
containing the term. It needs to be normalized for 
being used in fuzzy inference. The following 
shows the normalized term frequency (NTF). 
max
i
i
i
j
j
j
TF
DF
NTF
TF
DF
= ? ?? ?? ?? ?
              (3) 
where, iTF  is the frequency of term ti in the exam-
ple documents, iDF  is the number of documents 
having term ti in the example document, 
[ ]j jMax x means the maximum value of vari-
able jx . 
The DF (Document Frequency) represents the 
frequency of documents having a specific term 
within the example documents. The normalized 
document frequency, NDF, is defined in equation 
(4), where iDF is the number of documents having 
term ti in the example documents. 
     
max
i
i
j j
DF
NDF
DF
=                        (4) 
The IDF (Inverse Document Frequency) repre-
sents the inverse document frequency of a specific 
term over an entire document collection not exam-
ple documents. The normalized inverse document 
frequency, NIDF, is defined as follows: 
,    log
max
i
i i
j j i
IDF NNIDF IDF
IDF n
= =              (5) 
where, N is the total number of documents and in  
is the number of documents containing term ti . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Fuzzy input/output variables 
0 0.2 0.4 0.6 0.8 1 
Z S M XX L
1
TW 
11 0.2 0.7
L S L
10.80.61 0.1 0.3
M 
1
NTF=S NDF, NIDF 
1
(a) Input variable    
(b) Output variable   
Z: zero       
S: small       
M: middle    
L : large       
X: x large     
XX:xx 
larger    
 
S
Figure 1 shows the membership functions of the 
input/output variables - 3 inputs (NTF, NDF, NIDF) 
and 1 output (TW) - used in our method. As you 
can see in Figure 1(a), NTF variable has 
{ S(Small), L(Large) }, and NDF and NIDF vari-
ables have { S(Small), M(Middle), L(Large) } as 
linguistic labels (or terms). The fuzzy output vari-
able, TW (Term Weight) which represents the im-
portance of a term, has six linguistic labels as 
shown in Figure 1(b). 
The 18 fuzzy rules are involved to infer the 
term weight (TW). The rules are constructed based 
on the intuition that the important or representative 
terms may occur across many positive example 
documents but not in general documents, i.e., their 
NDF and NIDF are very high. As shown in Table 1, 
the TW of a term is Z in most cases regardless of 
its NDF and NTF if its NIDF is S, because such 
term may occur frequently in any document and 
thus its NDF and NTF can be high. When NDF of 
a term is high and its NIDF is also high, the term is 
considered as a representative keyword and then 
the output value is between X and XX. The other 
rules were set similarly. 
Table 1. Fuzzy inference rules 
NIDF 
NDF 
S M L NIDF 
NDF      
S M L 
S Z Z S S Z S M
M Z M L M Z L X
L S L X L S X XX
NTF = S NTF = L
We can get the term weight TW through the 
following procedure. But, the output is in the form 
of fuzzy set and thus has to be converted to the 
crisp value. In this paper, the center of gravity 
method is used to defuzzify the output  (Lee, 1990). 
? Apply the NTF, NDF, and NIDF fuzzy val-
ues to the antecedent portions of 18 fuzzy 
rules. 
? Find the minimum value among the mem-
bership degrees of three input fuzzy values. 
? Classify every 18 membership degree into 6 
groups according to the fuzzy output vari-
able TW. 
? Calculate the maximum output value for 
each group and then generate 6 output val-
ues. 
3.2 Selection of Initial Representative Key-
words 
After calculation of the term weights of candidate 
terms through fuzzy inference, some candidate 
terms are selected as IRKs based on their weights 
with the constraint that each example document 
should contain at least one or more IRKs. The al-
gorithm for selection of IRKs is given in Figure 2. 
Let us consider the following example to under-
stand our selection procedure. 
i) An example document set, DS, is composed of 
documents d1, d2, d3, d4, d5, and d6. Each 
document contains the following terms:  
d1 = {a, b, f}, d2 = {a, c, d}, d3 = {d, e, f}, 
d4 = {d, f},  d5 = {b, c, e},  d6 = {e, f} 
ii) A candidate term set, TS, is composed of {(a, 
0.9), (b, 0.8), (c, 0.7), (d, 0.6), (e, 0.5), (f, 0.4)}, 
where (ti, TWi) represents that TWi is the term 
weight of term ti. 
If we apply the algorithm in Figure 2 to this ex-
ample, then temporary variables in line 2, 3 and 4 
are initialized. The statement block from line 5 to 
line 14 is executed repeatedly until at least one or 
more IRKs are extracted from every example 
document in DS. Let us assume that the documents 
in the example document set are processed in se-
quence. After the first loop of the statement block 
from line 5 to line 14 is executed, the output value 
of ITS contains only term ?a?. There is no change 
in ITS after the second loop of the block because 
term ?a? has already been included in ITS. After 
d3, the third loop of the block, is processed, a term 
?d? is newly added to ITS. So, there is {a, d} in 
ITS. After d4, d5, and d6 are sequentially proc-
essed, none, term ?b?, and term ?e? are added to 
ITS, respectively. Therefore the algorithm return 
ITS having a set of terms {a, b, d, e}. We can find 
the algorithm in Figure 2 works well according to 
our constraint. 
 
Input: DS (Example Documents Set) 
           TS (Candidate Terms Set) 
 1] Procedure get_ITS(DS, TS) 
 2] ITS: Initial Representative Terms Set, initialized to empty. 
 3] TS': Temporary Terms Set, initialized to TS. 
 4] d, t: Document and Term element respectively. 
 5] Repeat 
 6]   Select a document element as d from DS.   
 7]   Repeat 
 8]      Select the highest element as t in TS' 
              according to the weight.  
 9]      If t appears in d and not member in ITS 
             Then Add t to ITS.   
10]     Remove t from TS. 
11]   Until t appears in d.  
12]   Remove d from DS. 
13]   Assign TS to TS'. 
14] Until DS is empty.    
15] Return ITS. 
Figure 2. The algorithm for selection of initial rep-
resentative terms 
3.3 Automatic Expansion and Reweighting of  
IRKs 
After the IRKs are selected, additional terms are 
selected to be expanded in the order of their 
weights calculated by the method in Section 3.1. 
Let us assume that 5 terms are used to represent a 
user's preference and the number of IRKs is 3. 
Then, 2 terms with highest weights except IRKs 
are selected additionally. The IRKs and these terms 
constitute the final representative keywords 
(FRKs) and are reweighted by considering the co-
occurrence similarity with IRKs.  For this end, the 
relevance degrees of the FRKs in every document 
are calculated with the equation (6). Each positive 
example document represents user?s preferable 
content. In other words, each document tends to 
contain general or specific or partial contents. We 
regard the IRKs as the essential terms of the given 
positive examples. So, the possibility that the re-
lated terms, e.g., synonym, collocated terms and so 
on, occurred together with these IRKs in the same 
document set increases.  
1
)(
log1 1
2
+
? ?
?= =
n
tfkf
RD
n
j
ikjk
pik              (6) 
where, RDik is the relevance degree between IRKs 
and candidate term ti in document dk, kfjk is the fre-
quency of initial representative keyword j in 
document dk, tfik is the frequency of candidate term 
ti in document dk, n is the number of IRKs, p is a 
control parameter. In our experiments, p is set to 
10. The RDik is treated as 0 if it has negative value. 
For example, let K be a set of IRKs consisting 
of k1, k2 and k3 terms and their frequencies in 
document d1 be 4, 3, and 1, respectively. Also, let 
the frequency of term t1 be 2. Then, its relevance 
degree is calculated as follows: 
1
3
)1(12log1
222
1011 +?++?=RD =  0.762 
As shown in the above equation, RDik is in-
versely proportional to the sum of term frequency 
difference between initial representative term and 
candidate term. So, the higher is the value of Rd, 
the more similar the co-occurrence is, that is, the 
equation reflects the co-occurrence similarity be-
tween initial representative terms and a candidate 
term appropriately. After calculating the relevance 
degree of a candidate term, the weight of the term 
in the set of example documents is determined by 
the following equation: 
iikik
n
k
ikikri
IDFTFw
RDww
?=
? ?=
=1
)(
                (7) 
where, wri is the weight of term ti in the document 
set, wik is the weight of term ti in document dk, TFik 
is the frequency of term ti in document dk, IDFi is 
the inverse document frequency of term ti, and n is 
the number of example documents. 
The equation (7) is a modification of the Roc-
chio's in Section 2. Different from that equation, 
we additionally use the term relevance degree be-
tween initial representative terms and a candidate 
term. Let us assume that the IDF value of the can-
didate term t1 is 1.0 and it occurs 3, 2, and 1 within 
document d1, d2 and d3, respectively. If the rele-
vance degrees for three documents are also as-
sumed to 0.3, 0.5, and 0.7, respectively, then the 
weight of candidate term ti is calculated as below. 
82.1))7.00.1()5.00.2()4.00.3((1 =?+?+?=rw  
Finally, the weights of the FRKs are calculated 
by the following equation: 
rikii www +=                              (8) 
where, wki, is  the initial weight of term ti. Instead 
of using the weight obtained by fuzzy inference, 
the initial weight wki of term ti is recalculated by 
the equation (9), if the term is in IRKs and other-
wise 0. The equation is the one introduced to as-
sign a weight to an initial query term in IR systems 
based on the vector space model (Baeza-Yates and  
Ribeiro-Neto, 1999). 
???
?
???
?????
?
???
? ?+=
ijj
i
ki n
N
freq
freq
w log
max
5.0
5.0        (9) 
where, freqi is the frequency of initial representa-
tive keyword ti, ni is the frequency of documents in 
which ti appear, and N is the total number of docu-
ments. 
Let K = {t1, t3, t4} be the set of IRKs, WK = 
{3.0, 2.0, 1.0} be the set of their weights calculated 
by the equation (9), T = {t1, t2, t3, t4, t5} be the 
set of FRKs, and WT = {5.0, 4.0, 3.0, 2.0, 1.0} be 
their weights through the equation (7). Then, we 
can get the final weights of FRKs, {8.0,4.0, 5.0, 
3.0, 1.0}. 
4 Experiments 
We used Reuters-21578 data as an experimental 
document set. This collection has five different sets 
of contents related categories. They are 
EXCHANGES, ORGS, PEOPLE, PLACES and 
TOPICS. Some of the categories set have up to 265 
categories, but some of them have just 39 catego-
ries. We chose the TOPICS categories set which 
has 135 categories. We divided the documents ac-
cording to the ?ModeApte? split. There are 9603 
training documents and 3299 test documents. 
Among the 135 categories, we first chose only 90 
ones that have at least one training example and 
one testing example. Then, we finally selected 21 
categories that have from 10 to 30 training docu-
ments. The 3019 documents of those categories are 
used as testing documents. The document fre-
quency information from 7770 training documents 
in 90 categories is used to calculate IDF values of 
terms. We did not consider negative documents 
under the assumption that only positive documents 
coincident with users? preferences were given im-
plicitly or explicitly . 
Documents are ranked by the cosine similarity 
and the following F-measure (Baeza-Yates and 
Ribeiro-Neto, 1999), which is a weighted combina-
tion of recall and precision and popularly used for 
performance evaluation. Since the maximum value 
for F can be interpreted as the best possible com-
promise between recall and precision, we use this 
maximum value.  
)/(2
11
2
jjjj
jj
j RPRP
RP
F +=
+
=          (10) 
where, Rj and Pj are the recall and precision for the 
j?th document in the ranking and Fj is their har-
monic mean.  
First, our method was compared to the Rocchio 
and Widrow-Hoff algorithms. To see the effect of 
the number of FRKs, we made experiments by 
varying it from 5 to 30 in increment 5 and for the 
case that all terms are used.  Table 2 shows the 
overall or summary result of the proposed method 
compared to the two existing algorithms for 
21categories. The result shows that our method is 
better than the others in all cases, especially when 
10 terms are used to represent user preferences. 
Table 3 shows the detail result in that case, i.e. the 
F-values and the performance improvement ratios 
when 10 terms are used. The proposed method has 
achieved about 20% over Rocchio algorithm and 
10% over Widrow-Hoff algorithm on the average.  
When 5 terms are used to represent user prefer-
ences, 19 categories among 21 categories are used 
because ?strategic-metal? and ?pet-chem? catego-
ries do not satisfy the constraint in Section 3.2, i.e., 
5 terms are too few to cover all training documents. 
Table 2. Performance of 21 categories in the 
REUTERS corpus and comparison with two exist-
ing algorithms. 
 Our Rocchio W.H. 
5 0.582 0.511 0.566 
10 0.594 0.496 0.540 
15 0.571 0.490 0.529 
20 0.552 0.489 0.522 
25 0.545 0.491 0.493 
30 0.541 0.495 0.500 
All 0.490 0.467 0.483 
It is not clear which component of our method 
mainly contributes to such improvement since our 
method consists of two main components - one is 
for extracting IRKs, the other for expanding and 
reweighting of IRKs. To analyze our method, we 
made several variants of the proposed method and 
did experiments with them. The variants are named 
by the sequence of the following symbols.  
IF, IR, IW: mean that IRKs are selected based on 
the weight obtained by the method in Section 3.1, 
the Rocchio algorithm, and the Widrow-Hoff algo-
rithm, respectively. 
RC, RR, RW: mean that terms are reweighted by 
the method in Section 3.3, the Rocchio algorithm, 
and the Widrow-Hoff algorithm, respectively. 
EC, EF, ER, EW: mean that expanded terms are 
selected based on the weight obtained by applying 
the method in Section 3.3, the method in Section 
3.1, the Rocchio algorithm, and the Widrow-Hoff 
algorithm, respectively. 
For example, the proposed method in Section 3 is 
named as IF_EF_RC, which means IRKs, and ex-
panded terms are selected based on the weight cal-
culated by the method in Section 3.1 and then 
reweighted by the method in Section 3.3. For an-
other example, the method called by IF_RC_EC 
means that IRKs are selected based on the weight 
obtained by the method in Section 3.1 and then all 
terms are reweighed by the method in Section 3.3 
before expanded terms are selected. 
In the proposed method, fuzzy inference tech-
nique is used to extract IRKs. So, we tried two 
variants, IR_ER_RC and IW_EW_RC, where the 
Rocchio and Widrow-Hoff algorithms are used 
respectively to calculate the representativeness (or 
weights) of terms instead of the method in Section 
3.1, and then IRKs and expanded terms are se-
lected based on these weights. The variants all use 
the reweighting scheme in Section 3.3. Table 4 
shows that other keyword extraction algorithms do 
not show any benefit over the fuzzy inference ap-
proach. We can also observe that when one of the 
existing algorithms is combined with the second 
component of our method, the performance im-
provement over the case that the algorithm solely 
is used is negligible. 
The method to extract IRKs reflecting user?s 
preference directly affects the result of the term 
reweighting process because the process is based 
on the term co-occurrence similarity with the IRKs. 
If the terms that are far from user?s preference are 
extracted as IRKs, then some terms that actually 
are improper in representing user?s information 
needs may be assigned with high weights during 
the reweighting process and then the final vector 
generated from the results may be disqualified 
from representing user?s preferences. So, we can 
know that our fuzzy inference technique is effec-
tive to extract IRKs from the results in Table 4. 
To demonstrate the usefulness of the second 
part of our method, i.e., the expansion and re-
weighting technique, we also tried the 5 variants of 
our method (IF_RC_EC, IF_RR_ER, IF_RW_EW, 
IF_EF_RR, IF_EF_RW). Table 5 shows the all 
variants are not better than the original though they 
outperform Rocchio and Widrow-Hoff algorithms. 
5 Conclusions 
In this study, we apply fuzzy inference technique 
and term reweighting scheme based on the term 
co-occurrence similarity to the problem that extract 
important keywords representing contents of 
documents presented by users. We have conducted 
extensive experiments on the Reuters-21578 col-
lection. The results show that our method outper-
forms two well-known training algorithms for 
linear text classifiers. Moreover, some variants of 
our method have been explored to analyze the 
characteristics of our method. Though this paper 
only describes how to extract user preferences 
from example documents, the technique will be 
applicable to several areas such as query modifica-
tion in IR, user profile modification in information 
filtering, text summarization and so forth directly 
or with some modifications.  
Since only positive examples are considered in 
our method, the method is not applicable to a 
document set containing negative examples. For 
covering negative examples, it needs to modify the 
fuzzy inference rules with considering additional 
input variables.  The proposed method was also 
designed for a small set of documents. So, we 
could not achieve performance improvement as 
described in this paper when our method is applied 
to a large set of documents. However, such a prob-
lem will be alleviated if clustering techniques are 
used together as in (Alberto et al, 2001; Lam and 
Ho, 1998; Ugur et al, 2000). 
Table 3. The detail result when 10 terms are used 
for user preferences 
 Our Rocchio W.H. 
lumber 0.7273 0.4444 0.6667 
dmk 0.4 0.4444 0.4 
sunseed 0.5714 0.3333 0.3333 
lei 1 0.8 1 
soy-meal 0.6667 0.5143 0.5185 
fuel 0.4615 0.4615 0.4615 
heat 0.75 0.75 0.75 
soy-oil 0.3704 0.2692 0.32 
lead 0.5625 0.5 0.5 
strategic- 0.13333 0.1053 0.1408 
hog 0.8 0.6 0.8 
orange 0.9091 0.9091 0.8571 
housing 0.5714 0.6667 0.5714 
tin 0.96 0.7857 0.9231 
rapeseed 0.6154 0.5714 0.6154 
wpi 0.5714 0.5882 0.5882 
pet-chem 0.3704 0.2727 0.2759 
silver 0.381 0.4 0.5 
zinc 0.8966 0.6667 0.6842 
retail 0.1667 0.0548 0.0548 
sorghum 0.5882 0.2727 0.3871 
Average 0.5940 0.4957 0.5404 
Table 4. The performance of our method and its 
two variants that use Rocchio and Widrow-Hoff 
algorithms instead of fuzzy inference, respectively. 
Table 5. The performance of our method and its 
five variants that use different reweighting and ex-
panding approaches. 
References 
Alberto Diaz Esteban, Manuel J. Mana Lopez, Manuel 
de Buenaga Rodriguez, Jose Ma Gomez Hidalgo and 
Pablo Gervas Gomez-Navarro. 2001. Using linear 
classifiers in the integration of user modeling and 
text content analysis in the personalization of a web-
based Spanish news servic, In Proceedings. of the 
Workshop on Machine Learning, Information Re-
trieval and User Modeling, 8th International Confer-
ence on User Modeling. 
Baeza-Yates, R. and Ribeiro-Neto B.. 1999.  Modern 
Information Retrieval, ACM Press, USA. 
David D. Lewis, Robert E. Schapire , James P. Callan 
and Ron Papka. 1996. Training algorithms for linear 
text classifiler, In Proc. of SIGIR-96, 19th ACM In-
ternational Conference on Research and Develop-
ment in Information Retrieval. 
Goldberg D., Nichols D., Oki B. M., and Terry D.. 1992. 
Using collaborative filtering to weave an information 
tapestr, Communication of the ACM, 35(12), p61-70. 
Kim, J., Oard, D.W., and Romanik, K.. 2000. User 
modeling for information filtering based on implicit 
feedback, In Proceedings. of ISKO-France. 
Konstan J. A. , Miller B. N., Maltz D., Herlocker J. L., 
Gordon L.R. and Riedl J.. 1997. GroupLens: Apply-
ing collaborative filtering to Usenet News, Commu-
nication of the ACM, 40(3), p 77-87. 
Lam K. and Ho C.. 1998. Using a generalized instance 
set for automatic text categorization, In 21th Ann. Int. 
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, p81-89. 
Lee C.C.. 1990. Fuzzy logic in control systems: fuzzy 
logic controller-part I, IEEE Trans. On Systems, 
Man, and Cybernetics, 20 (2) , p408-418. 
Mitra, M., Singhal, A., and Buckley, C.,. 1998. Improv-
ing Automatic Query Expansion, In Proceedings of 
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information 
Retrieval, p206-214, 1998. 
Nichols D. M.. 1997. Implicit ratings and filteri?, In 
Proceedings of the 5th DELOS Workshop on Filter-
ing and Collaborative Filtering, p10-12. 
Pazzani, M. and  Billsus, D.. 1997. Learning and revis-
ing user profiles: the identification of interesting Web 
site, Machine Learning, 1997. 
Seo, Y. and Zhang, B.. 2001. Personalized Web Docu-
ment Filtering Using Reinforcement Learning, Ap-
plied Artificial Intelligence. 
Soltysiak, S. J. and Crabtree, I. B.. 2000.  Automatic 
Learning of User Profiles?Towards the Personaliza-
tion of Agent Services, BT Technology Journal, 16 
(3), p110?117. 
Ugur ?etintemel, Franklin Michael J. and Lee Giles C.. 
2000 . Self-Adaptive User Profiles for Large-Scale 
Data Delivery, ICDE, p622-633. 
Xu Jinix and Croft W. B.. 1996. Query Expansion Us-
ing Local and Global Document Analysis, In Pro-
ceeding of ACM SIGIR International Conference on 
Research and Development in Information Retrieval, 
p4-11. 
Yan T. W. and Garcia-Molin H.. 1995. SIFT- A tool for 
wide-area information dissemination, In Proceedings 
of the 1995 USENIX Technical Conference, p177-
186. 
Yang, Y. and  Pedersen, J..  1997. A comparative study 
on feature selection in text categorization, In Pro-
ceedings of the 14th International Conference on 
Machine Learning, p412-420. 
 
 IF_EF_RC IR_ER_RC IW_EW_RC
5 0.582 0.509 0.571 
10 0.594 0.505 0.528 
15 0.571 0.502 0.537 
20 0.552 0.491 0.526 
25 0.545 0.487 0.518 
30 0.541 0.497 0.510 
All 0.490 0.478 0.490 
 
IF_EF
_RC 
IF_RC
_EC 
IF_RR
_ER 
IF_RW
_EW 
IF_EF
_RR 
IF_EF
_RW
5 0.582 0.571 0.546 0.580 0.545 0.570
10 0.594 0.520 0.498 0.549 0.551 0.561
15 0.571 0.514 0.491 0.508 0.518 0.517
20 0.552 0.513 0.495 0.533 0.497 0.538
25 0.545 0.509 0.498 0.503 0.491 0.521
30 0.541 0.515 0.506 0.512 0.498 0.511
All 0.490 0.488 0.478 0.494 0.465 0.483
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1139?1145,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Exploiting Social Relations and Sentiment for Stock Prediction 
 
Jianfeng Si* Arjun Mukherjee? Bing Liu? Sinno Jialin Pan* Qing Li? Huayi Li? 
* Institute for Infocomm Research, Singapore 
{ thankjeff@gmail.com, jspan@i2r.a-star.edu.sg} 
?Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA 
{ arjun4787@gmail.com, liub@cs.uic.edu, lhymvp@gmail.com} 
? Department of Computer Science, City University of Hong Kong, Hong Kong, China 
qing.li@cityu.edu.hk 
 
 
Abstract 
In this paper we first exploit cash-tags (?$? fol-
lowed by stocks? ticker symbols) in Twitter to 
build a stock network, where nodes are stocks 
connected by edges when two stocks co-occur 
frequently in tweets. We then employ a labeled 
topic model to jointly model both the tweets and 
the network structure to assign each node and 
each edge a topic respectively. This Semantic 
Stock Network (SSN) summarizes discussion 
topics about stocks and stock relations. We fur-
ther show that social sentiment about stock 
(node) topics and stock relationship (edge) topics 
are predictive of each stock?s market. For predic-
tion, we propose to regress the topic-sentiment 
time-series and the stock?s price time series. Ex-
perimental results demonstrate that topic senti-
ments from close neighbors are able to help im-
prove the prediction of a stock markedly. 
1 Introduction 
Existing research has shown the usefulness of 
public sentiment in social media across a wide 
range of applications. Several works showed so-
cial media as a promising tool for stock market 
prediction (Bollen et al., 2011; Ruiz et al., 2012; 
Si et al., 2013). However, the semantic relation-
ships between stocks have not yet been explored. 
In this paper, we show that the latent semantic 
relations among stocks and the associated social 
sentiment can yield a better prediction model.  
On Twitter, cash-tags (e.g., $aapl for Apple 
Inc.) are used in a tweet to indicate that the tweet 
talks about the stocks or some other related in-
formation about the companies. For example, 
one tweet containing cash-tags: $aapl and $goog 
(Google Inc.), is ?$AAPL is loosing customers. 
everybody is buying android phones! $GOOG?. 
Such joint mentions directly reflect some kind of 
latent relationship between the involved stocks, 
which motivates us to exploit such information 
for the stock prediction.  
We propose a notion of Semantic Stock Net-
work (SSN) and use it to summarize the latent 
semantics of stocks from social discussions. To 
our knowledge, this is the first work that uses 
cash-tags in Twitter for mining stock semantic 
relations. Our stock network is constructed based 
on the co-occurrences of cash-tags in tweets. 
With the SSN, we employ a labeled topic model 
to jointly model both the tweets and the network 
structure to assign each node and each edge a 
topic respectively. Then, a lexicon-based senti-
ment analysis method is used to compute a sen-
timent score for each node and each edge topic. 
To predict each stock?s performance (i.e., the 
up/down movement of the stock?s closing price), 
we use the sentiment time-series over the SSN 
and the price time series in a vector autoregres-
sion (VAR) framework.  
We will show that the neighbor relationships in 
SSN give very useful insights into the dynamics 
of the stock market. Our experimental results 
demonstrate that topic sentiments from close 
neighbors of a stock can help improve the predic-
tion of the stock market markedly. 
2 Related work 
2.1 Social Media & Economic Indices 
Many algorithms have been proposed to produce 
meaningful insights from massive social media 
data. Related works include detecting and sum-
marizing events (Weng and Lee, 2011; Weng et 
al., 2011; Baldwin et al., 2012; Gao et al., 2012) 
and analyzing sentiments about them (Pang and 
Lee, 2008; Liu, 2012), etc. Some recent literature 
also used Twitter as a sentiment source for stock 
market prediction (Bollen et al., 2011; Si et al., 
2013). This paper extends beyond the correlation 
between social media and stock market, but fur-
1139
ther exploits the social relations between stocks 
from the social media context. 
  Topic modeling has been widely used in social 
media. Various extensions of the traditional LDA 
model (Blei et al., 2003) has been proposed for 
modeling social media data (Wang et al., 2011, 
Jo and Oh, 2011; Liu et al., 2007; Mei et al., 
2007; Diao et al., 2012). Ramage et al. (2009; 
2011) presented a partially supervised learning 
model called Labeled LDA to utilize supervision 
signal in topic modeling. Ma et al. (2013) pre-
dicted the topic popularity based on hash-tags on 
Twitter in a classification framework. 
2.2 Financial Networks for Stock 
Financial network models study the correlations 
of stocks in a graph-based view (Tse et al., 2010; 
Mantegna, 1999; Vandewalle et al., 2001; On-
nela et al., 2003; Bonanno et al., 2001). The usu-
al approach is to measure the pairwise correla-
tion of stocks? historical price series and then 
connect the stocks based on correlation strengths 
to build a correlation stock network (CSN). 
However, our approach leverages social media 
posts on stock tickers. The rationale behind is 
that micro-blogging activities have been shown 
to be highly correlated with the stock market 
(Ruiz et al., 2012; Mao et al., 2012). It is more 
informative, granular to incorporate latest devel-
opments of the market as reflected in social me-
dia instead of relying on stocks? historical price.  
3 Semantic Stock Network (SSN) 
3.1 Construction of SSN 
We collected five months (Nov. 2 2012 - Apr. 3 
2013) of English tweets for a set of stocks in the 
Standard & Poor's 100 list via Twitter?s REST 
API, using cash-tags as query keywords. For 
preprocessing, we removed tweets mentioning 
more than five continuous stock tickers as such 
tweets usually do not convey much meaning for 
our task. Finally, we obtained 629,977 tweets in 
total. Table 1 shows the top five most frequent 
stocks jointly mentioned with Apple Inc. in our 
dataset. Formally, we define the stock network as 
an undirected graph ? = {? , ?}. The node set 
? comprises of stocks, ??,? ? ?  stands for the 
edge between stock nodes ? and ? and the edge 
weight is the number of co-occurrences. On ex-
ploring the co-occurrence statistics in pilot stud-
ies, we set a minimum weight threshold of 400 to 
filter most non-informative edges. Figure 1 
demonstrates a segment of the stock network 
constructed from our dataset. 
3.2 Semantic Topics over the Network 
Figure 2 illustrates our annotation for each tweet. 
For a tweet, ? with three cash-tags: {?1, ?2, ?3}, we annotate ?  with the label set, ?? =
 {?1, ?2, ?3, ?1,2, ?1,3, ?2,3}. (?1,2 is ?aapl_goog? 
if ?1is ?aapl? and ?2 is ?goog?). Then, the topic assignments of words in ? are constrained to top-
ics indexed by its label set, ??. Given the annota-tions as labels, we use the Labeled LDA model 
(Ramage et al., 2009) to jointly learn the topics 
over nodes and edges. Labeled-LDA assumes 
that the set of topics are the distinct labels in a 
labeled set of documents, and each label corre-
sponds to a unique topic. Similar to LDA (Blei et 
al., 2003), Labeled-LDA models each document 
as an admixture of latent topics and generates 
each word from a chosen topic. Moreover, La-
beled-LDA incorporates supervision by simply 
constraining the model to use only those topics 
that correspond to a document?s observed label 
set (Ramage et al., 2009). For model inference, 
we use collapsed Gibbs sampling (Bishop, 2006) 
and the symmetric Dirichlet Priors are set to: 
? = 0.01, ? = 0.01 as suggested in (Ramage et 
al., 2010). The Gibbs Sampler is given as: 
?(?? = ?|???)~
 ?(??,?)?1+ ?
?(??,?)?1+ |???|??
? ?(?,??)?1+??(?,?)?1+ |? |?? (1) 
where ?(??, ?) is the number of words in ?? as-signed to topic ?, while ?(??,?) is the marginal-ized sum. |??? | is the size of label subset of ??. 
 Figure 2. Tweet label design. 
$goog $amzn $ebay $msft $intc
43263 23266 14437 11891 2486
Table 1. co-occurrence statistics with $aapl. 
 
Figure 1. An example stock network. 
1140
?(?, ?) is the term frequency of word ? in topic 
?. |? | is the vocabulary size. The subscript -1 is 
used to exclude the count assignment of the cur-
rent word ?? . The posterior on the document?s topic distribution {??,?} and topic?s word distri-
bution {??,?} can be estimated as follows: 
??,? =  
?(??,?)+ ?
?(??,?)+ |???|??
                (2) 
??,? =  
?(?,??)+?
?(?,?)+ |? |??                   (3) 
Later, parameters {??,?} will be used to compute 
the sentiment score for topics. 
3.3 Leveraging Sentiment over SSN for 
Stock Prediction 
We define a lexicon based sentiment score in the 
form of opinion polarity for each node-indexed 
and edge-indexed topic as follows: 
?(?) = ? ??,?
|? |
?=1
?(?), ?(?) ? [?1,1]  (4) 
where ?(?) denotes the opinion polarity of word 
?. ??,?  is the word probability of ? in topic ? 
(Eq.3). Based on an opinion lexicon ?, ?(?) = 
+1 if ? ? ????, ?(?) = -1 if ? ? ???? and ?(?) 
= 0 otherwise. We use the opinion English lexi-
con contributed by Hu and Liu (2004).  
Considering the inherent dynamics of both the 
stock markets and social sentiment, we organize 
the tweets into daily based sub-sets according to 
their timestamps to construct one ????  ( ? ?
[1, ? ]) for each day. Then, we apply a Labeled 
LDA for each ???? and compute the sentiment scores for each ???? ?s nodes and edges. This yields a sentiment time series for the node, ? , 
{?(?)1, ?(?)2, ? , ?(?)? } and for the edge, ??,?, 
{?(??,?)1, ?(??,?)2, ? , ?(??,?)? } . We intro-
duce a vector autoregression model (VAR) 
(Shumway and Stoffer, 2011) by regressing sen-
timent time series together with the stock price 
time series to predict the up/down movement of 
the stock?s daily closing price. 
As usual in time series analysis, the regression 
parameters are learned during a training phase 
and then are used for forecasting under sliding 
windows, i.e., to train in period [?, ? + ?] and to 
predict on time ? + ? + 1. Here the window size 
? refers to the number of days in series used in 
model training. A VAR model for two variables 
{??} and {??} can be written as: 
?? =  ? (??????? + ???????)????=1 + ??  (5) where {?} are white noises, {?} are model pa-
rameters, and ??? notes the time steps of histori-
cal information to use. In our experiment, {??} is the target stock?s price time series, {??} is the covariate sentiment/price time series, and we will 
try ??? ? ?2,3?. We use the ?dse? library in R 
language to fit our VAR model based on least 
square regression. 
4 Experiments 
4.1 Tweets in Relation to the Stock Market 
Micro-blogging activities are well correlated 
with the stock market. Figure 3 shows us how the 
Twitter activities response to a report announce-
ment of $aapl (Jan. 23 2013). The report was 
made public soon after the market closed at 
4:00pm, while the tweets volume rose about two 
hours earlier and reached the peak at the time of 
announcement, then it arrived the second peak at 
the time near the market?s next opening (9:30am). 
By further accumulating all days? tweet volume 
in our dataset as hourly based statistics, we plot 
the volume distribution in Figure 4. Again, we 
note that trading activities are well reflected by 
tweet activities. The volume starts to rise drasti-
cally two or three hours before the market opens, 
and then reaches a peak at 9:00pm. It drops dur-
ing the lunch time and reaches the second peak 
around 2:00pm (after lunch). Above observations 
clearly show that market dynamics are discussed 
in tweets and the content in tweets? discussion 
very well reflects the fine-grained aspects of 
stock market trading, opening and closing. 
 
Figure 3. Tweet activity around $aapl?s earnings 
report date on Jan. 23 2013. 
 
Figure 4. Tweet volume distribution in our data 
over hours averaged across each day. 
0
500
1000
1500
2000
2500
Time (date-hour)
0
0.02
0.04
0.06
0.08
0.1
0 2 4 6 8 10 12 14 16 18 20 22
Time (hourly)
1141
4.2 Stock Prediction 
This section demonstrates the effectiveness of 
our SSN based approach for stock prediction. 
We leverage the sentiment time-series on two 
kinds of topics from SSN: 1). Node topic from 
the target stock itself, 2). Neighbor node/edge 
topics. We note that the price correlation stock 
network (CSN) (e.g., Bonanno et al., 2001; Man-
tegna, 1999) also defines neighbor relationships 
based on the Pearson's correlation coefficient 
(Tse et al., 2010) between pair of past price se-
ries (We get the stock dataset from Yahoo! Fi-
nance, between Nov. 2 2012 and Apr. 3 2013).  
 We build a two variables VAR model to pre-
dict the movement of a stock?s daily closing 
price. One variable is the price time series of the 
target stock ({??} in Eq.5); another is the covari-ate sentiment/price time series ({??}  in Eq.5). We setup two baselines according to the sources 
of the covariate time series as follows: 
1. Covariate price time series from CSN, we try 
the price time series from the target stock?s 
closest neighbor which takes the maximum 
historical price correlation in CSN. 
2. With no covariate time series, we try the tar-
get stock?s price only based on the univariate 
autoregression (AR) model. 
 To summarize, we try different covariate sen-
timent (?(. )) or price (?(. )) time series from 
SSN or CSN together with the target stock?s 
price time series (? ?) to predict the movement of 
one day ahead price (???). The accuracy is com-
puted based on the correctness of the predicted 
directions as follows, i.e., if the prediction ??? 
takes the same direction as the actual price value, 
we increment #(???????) by 1, #(?????????) is 
the total number of test.  
???????? = #(???????)#(?????????)       (6) 
 Figure 5 details the prediction of $aapl on dif-
ferent training window sizes of [15, 60] and lags. 
{?(????), ?(????), ?(????), ?(????_????)} are 
from SSN, ?(????)  is from CSN ($dell (Dell 
Inc.) takes the maximum price correlation score 
of 0.92 with $aapl), and ? ? =  ?(????)  is the 
univariate AR model, using the target stock?s 
price time series only. Table 2 further summariz-
es the performance comparison of different ap-
proaches reporting the average (and best) predic-
tion accuracies over all time windows and dif-
ferent lag settings. Comparing to the univariate 
AR model (?? only), we see that the sentiment 
based time-series improve performances signifi-
cantly. Among SSN sentiment based approach-
es, the ?(????) helps improve the performance 
mostly and gets the best accuracy of 0.78 on ??? 
2 and training window size of 53. On average, 
?(????) achieves a net gain over ?(????) in the 
range of 29% with lag 2 (0.62 = 1.29 x 0.48) and 
14% with lag 3 (0.57 = 1.14 x 0.50). Also, 
?(????_????)  performs better than ?(????) . 
The result indicates that $aapl?s stock perfor-
mance is highly influenced by its competitor. 
?(????) also performs well, but we will see rela-
tionships from CSN may not be so reliable. 
We further summarize some other prediction 
cases in Table 3 to show how different covariate 
sentiment sources ( ?(. ) ) and price sources 
(?(. )) from their closest neighbor nodes help 
predict their stocks, which gives consistent con-
clusions. We compute the ?-test for SSN based 
prediction accuracies against that of CSN or 
price only based approaches among all testing 
 Source Lag = 2 Lag = 3 
?? only self 0.49(0.57)	 0.47(0.52)
CSN: 
P(.)+??	 dell	 0.55(0.64)	 0.57(0.67)	
 
SSN: 
S(.)+?? 
aapl 0.48(0.56)	 0.50(0.61)
goog 0.62(0.78) 0.57(0.69) 
aapl_goog 0.55(0.65) 0.52(0.56) 
msft 0.52(0.65) 0.54(0.61) 
Table 2. Performance comparison of the average and 
best (in parentheses) prediction accuracies over all 
training window sizes for prediction on $aapl. 
 
 
Figure 5. Prediction on $aapl. (x-axis is the training 
window size, y-axis is the prediction accuracy) 
with different covariate sources. 
0.2
0.3
0.4
0.5
0.6
0.7
0.8
15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60
(a) Prediction of $aapl on lag 2
P* P(dell)+P*
S(aapl)+P* S(goog)+P*
S(aapl_goog)+P* S(msft)+P*
0.2
0.3
0.4
0.5
0.6
0.7
15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60
(b) Prediction of $aapl on lag 3
P* P(dell)+P*
S(aapl)+P* S(goog)+P*
S(aapl_goog)+P* S(msft)+P*
1142
window sizes ([15, 60]), and find that SSN based 
approaches are significantly (? -value < 0.001) 
better than others.  
We note that tweet volumes of most S&P100 
stocks are too small for effective model building, 
as tweets discuss only popular stocks, other 
stocks are not included due to their deficient 
tweet volume.  
We make the following observations: 
  1. CSN may find some correlated stock pairs 
like $ebay and $amzn, $wmt and $tgt, but some-
times, it also produces pairs without real-world 
relationships like $tgt and $vz, $qcom and $pfe, 
etc. In contrast, SSN is built on large statistics of 
human recognition in social media, which is like-
ly to be more reliable as shown. 
  2. Sentiment based approaches {?(?)} consist-
ently perform better than all price based ones 
{??, ? (?)}. For ?(?)  based predictions, senti-
ment discovered from the target stock?s closest 
neighbors in SSN performs best in general. This 
empirical finding dovetails with qualitative re-
sults in the financial analysis community (Mizik 
& Jacobson, 2003; Porter, 2008), where compa-
nies? market performances are more likely to be 
influenced by their competitors. But for Google, 
its stock market is not so much influenced by 
other companies (it gets the best prediction accu-
racy on ?(????), i.e., the internal factor). It can 
be explained by Google Inc.?s relatively stable 
revenue structure, which is well supported by its 
leading position in the search engine market. 
  3. The business of offline companies like Target 
Corp. ($tgt) and Wal-Mart Stores Inc. ($wmt) are 
highly affected by online companies like $amzn. 
Although competition exists between $tag and 
$wmt, their performances seem to be affected 
more by a third-party like $amzn (In Table 3, 
??????? predicts the best for both). Not surpris-
ingly, these offline companies have already been 
trying to establish their own online stores and 
markets. 
5 Conclusion 
This paper proposed to build a stock network 
from co-occurrences of ticker symbols in tweets. 
The properties of SSN reveal some close rela-
tionships between involved stocks, which pro-
vide good information for predicting stocks 
based on social sentiment. Our experiments show 
that SSN is more robust than CSN in capturing 
the neighbor relationships, and topic sentiments 
from close neighbors of a stock significantly im-
prove the prediction of the stock market.   
Acknowledgments 
This work was supported in part by a grant from 
the National Science Foundation (NSF) under 
grant no. IIS-1111092). 
Target ? ???? only CSN:  P(.)+?? SSN:  S(.)+?? 
 
goog 
  dis(0.96) goog aapl amzn 
2 0.48(0.59) 0.53(0.60) 0.59(0.65) 0.44(0.53) 0.42(0.49) 
3 0.46(0.54) 0.53(0.62) 0.56(0.67) 0.50(0.59) 0.43(0.49) 
 
amzn 
  csco(0.90) amzn goog msft 
2 0.48(0.54) 0.48(0.55) 0.47(0.54) 0.57(0.66) 0.60(0.68) 
3 0.46(0.53) 0.49(0.53) 0.43(0.50) 0.55(0.63) 0.57(0.66) 
 
ebay 
  amzn(0.81) ebay amzn goog 
2 0.49(0.55) 0.51(0.57) 0.44(0.53) 0.57(0.64) 0.56(0.62) 
3 0.48(0.58) 0.49(0.54) 0.45(0.58) 0.54(0.64) 0.54(0.61) 
 
tgt 
  vz(0.88) tgt wmt amzn 
2 0.43(0.53) 0.43(0.54) 0.46(0.55) 0.49(0.56) 0.49(0.59) 
3 0.44(0.50) 0.40(0.53) 0.44(0.48) 0.41(0.48) 0.48(0.54) 
 
wmt 
  tgt(0.86) wmt tgt amzn 
2 0.53(0.59) 0.53(0.63) 0.52(0.61) 0.52(0.60) 0.60(0.65) 
3 0.53(0.64) 0.48(0.57) 0.55(0.66) 0.48(0.58) 0.58(0.66) 
 
qcom 
  pfe(0.88) qcom aapl intc 
2 0.53(0.6) 0.55(0.63) 0.57(0.61) 0.46(0.54) 0.63(0.70) 
3 0.54(0.61) 0.48(0.55) 0.56(0.65) 0.51(0.61) 0.61(0.67) 
Table 3. Average and best (in parentheses) prediction accuracies (over window sizes of [15, 
60]) of some other cases with different covariates, cell of dis(0.96) means ?$dis? takes the 
maximum price correlation strength of 0.96 with ?$goog? (similar for others in column 
CSN). The best performances are highlighted in bold.  
1143
References 
Baldwin T., Cook P., Han B., Harwood A., Karuna-
sekera S., and Moshtaghi M. 2012. A support plat-
form for event detection using social intelligence. 
In Proceedings of the Demonstrations at the 13th 
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL '12). 
Association for Computational Linguistics, 
Stroudsburg, PA, USA, 69-72. 
Bishop C.M. 2006. Pattern Recognition and Machine 
Learning. Springer. 
Blei D., NG A., and Jordan M. 2003. Latent Dirichlet 
allocation. Journal of Machine Learning Research 
3:993-1022. 
Bollen J., Mao H., and Zeng X.J. 2011. Twitter mood 
predicts the stock market. Journal of Computer 
Science 2(1):1-8.  
Bonanno G., Lillo F., and Mantegna R.N. 2001. High- 
frequency cross-correlation in a set of stocks, 
Quantitative Finance, Taylor and Francis Journals, 
vol. 1(1), 96-104. 
Cohen J., Cohen P., West S.G., and Aiken L.S. 2003. 
Applied Multiple Regression/Correlation Analysis 
for the Behavioral Sciences, (3rd ed.) Hillsdale, NJ: 
Lawrence Erlbaum Associates. 
Diao Q., Jiang J., Zhu F., and Lim E.P. 2012. Finding 
bursty topics from microblogs. In Proceedings of 
the 50th Annual Meeting of the Association for 
Computational Linguistics: Long Papers - Volume 
1 (ACL '12), Vol. 1. Association for Computational 
Linguistics, Stroudsburg, PA, USA, 536-544. 
Gao W., Li P., and Darwish K. 2012. Joint topic mod-
eling for event summarization across news and so-
cial media streams. CIKM 2012: 1173-1182 
Hu M. and Liu B. 2004. Mining and summarizing 
customer reviews.  In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining, 22-25. Seattle, Wash-
ington (KDD-2004). 
Jo Y. and Oh A. 2011. Aspect and sentiment unifica-
tion model for online review analysis. In ACM 
Conference in Web Search and Data Mining 
(WSDM-2011). 
Liu B. 2012. Sentiment analysis and opinion mining. 
Morgan & Claypool Publishers. 
Liu Y., Huang X., An A., and Yu X. 2007. ARSA: a 
sentiment-aware model for predicting sales per-
formance using blogs. In Proceedings of the 30th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retriev-
al, 607-614. ACM, New York, NY. 
Ma Z., Sun A., and Cong G. 2013. On predicting the 
popularity of newly emerging hashtags in Twitter. 
In Journal of the American Society for Information 
Science and Technology, 64(7): 1399-1410 (2013) 
Mantegna R. 1999. Hierarchical structure in financial 
markets, The European Physical Journal B - Con-
densed Matter and Complex Systems, Springer, vol. 
11(1), pages 193-197, September. 
Mao Y., Wei W., Wang B., and Liu B. 2012. Corre-
lating S&P 500 stocks with Twitter data. In Pro-
ceedings of the First ACM International Workshop 
on Hot Topics on Interdisciplinary Social Net-
works Research (HotSocial '12). ACM, New York, 
NY, USA, 69-72 
Mei Q., Ling X., Wondra M., Su H., and Zhai C. 2007. 
Topic sentiment mixture: modeling facets and 
opinions in weblogs. In Proceedings of Interna-
tional Conference on World Wide Web (WWW-
2007). 
Mizik N. and Jacobson R. 2003. Trading off between 
value creation and value appropriation: The finan-
cial implications of shifts in strategic emphasis. 
Journal of Marketing, 63-76. 
Onnela J.P., Chakraborti A., and Kaski K. 2003. Dy-
namics of market correlations: taxonomy and port-
folio analysis, Phys. Rev. E 68, 056110. 
Pang B. and Lee L. 2008. Opinion Mining and Senti-
ment Analysis. Now Publishers Inc. 
Porter M.E. 2008. The Five Competitive Forces That 
Shape Strategy.HBR, Harvard Business Review. 
Ramage D., Dumais S.T., and Liebling D. 2010. 
Characterizing microblogging using latent topic 
models. In Proceedings of ICWSM 2010. 
Ramage D., Hall D., Nallapati R., and Manning C.D. 
2009. Labeled LDA: A supervised topic model for 
credit attribution in multi-labeled corpora. In Pro-
ceedings of the 2009 Conference on Empirical 
Methods in Natural Language Processing (EMNLP 
2009). 
Ramage D., Manning C.D., and Dumais S.T. 2011. 
Partially labeled topic models for interpretable text 
mining. In Proceedings of KDD 2011 
Ruiz E.J., Hristidis V., Castillo C., Gionis A., and 
Jaimes A. 2012. Correlating financial time series 
with micro-blogging activity. In Proceedings of the 
fifth ACM international conference on Web search 
and data mining, pp. 513-522. ACM Press, NY 
(WSDM-2012). 
Shumway R.H. and Stoffer D.S. 2011. Time Series 
Analysis and Its Applications: With R Examples, 
3rd ed. 
Si J., Mukherjee A., Liu B., Li Q., Li H., and Deng X. 
2013. Exploiting Topic based Twitter Sentiment 
for Stock Prediction. In Proceedings of the 51st 
1144
Annual Meeting of the Association for Computa-
tional Linguistics. ACL?13, Sofia, Bulgaria, 24-29.   
Tse C.K., Liu J., and Lau F.C.M. 2010. A network 
perspective of the stock market, Journal of Empiri-
cal Finance. 17(4): 659-667. 
Vandewalle N., Brisbois F., and Tordoir X. 2001. 
Self-organized critical topology of stock markets, 
Quantit. Finan., 1, 372?375. 
Wang X., Wei F., Liu X., Zhou M., and Zhang M. 
2011. Topic sentiment analysis in twitter: a graph-
based hashtag sentiment classification approach. 
CIKM 2011: 1031-1040 
Weng J. and Lee B.S. 2011. Event Detection in Twit-
ter. In Proceedings of the International AAAI Con-
ference on Weblogs and Social Media 2011. 
Weng J.Y., Yang C.L., Chen B.N., Wang Y.K., and 
Lin S.D. 2011. IMASS: An Intelligent Microblog 
Analysis and Summarization System. ACL (Sys-
tem Demonstrations) 2011: 133-138. 
 
1145
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 257?265,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Recommendation in Internet Forums and Blogs
Jia Wang
Southwestern Univ.
of Finance &
Economics China
wj96@sina.cn
Qing Li
Southwestern Univ.
of Finance &
Economics China
liq t@swufe.edu.cn
Yuanzhu Peter Chen
Memorial Univ. of
Newfoundland
Canada
yzchen@mun.ca
Zhangxi Lin
Texas Tech Univ.
USA
zhangxi.lin
@ttu.edu
Abstract
The variety of engaging interactions
among users in social medial distinguishes
it from traditional Web media. Such a fea-
ture should be utilized while attempting to
provide intelligent services to social me-
dia participants. In this article, we present
a framework to recommend relevant infor-
mation in Internet forums and blogs using
user comments, one of the most represen-
tative of user behaviors in online discus-
sion. When incorporating user comments,
we consider structural, semantic, and au-
thority information carried by them. One
of the most important observation from
this work is that semantic contents of user
comments can play a fairly different role
in a different form of social media. When
designing a recommendation system for
this purpose, such a difference must be
considered with caution.
1 Introduction
In the past twenty years, the Web has evolved
from a framework of information dissemination to
a social interaction facilitator for its users. From
the initial dominance of static pages or sites, with
addition of dynamic content generation and pro-
vision of client-side computation and event han-
dling, Web applications have become a preva-
lent framework for distributed GUI applications.
Such technological advancement has fertilized vi-
brant creation, sharing, and collaboration among
the users (Ahn et al, 2007). As a result, the role
of Computer Science is not as much of designing
or implementing certain data communication tech-
niques, but more of enabling a variety of creative
uses of the Web.
In a more general context, Web is one of the
most important carriers for ?social media?, e.g. In-
ternet forums, blogs, wikis, podcasts, instant mes-
saging, and social networking. Various engaging
interactions among users in social media differ-
entiate it from traditional Web sites. Such char-
acteristics should be utilized in attempt to pro-
vide intelligent services to social media users.
One form of such interactions of particular inter-
est here is user comments. In self-publication, or
customer-generated media, a user can publish an
article or post news to share with others. Other
users can read and comment on the posting and
these comments can, in turn, be read and com-
mented on. Digg (www.digg.com), Yahoo!Buzz
(buzz.yahoo.com) and various kinds of blogs are
commercial examples of self-publication. There-
fore, reader responses to earlier discussion provide
a valuable source of information for effective rec-
ommendation.
Currently, self-publishing media are becoming
increasingly popular. For instance, at this point of
writing, Technorati is indexing over 133 million
blogs, and about 900,000 new blogs are created
worldwide daily1. With such a large scale, infor-
mation in the blogosphere follows a Long Tail Dis-
tribution (Agarwal et al, 2010). That is, in aggre-
gate, the not-so-well-known blogs can have more
valuable information than the popular ones. This
gives us an incentive to develop a recommender
to provide a set of relevant articles, which are ex-
pected to be of interest to the current reader. The
user experience with the system can be immensely
enhanced with the recommended articles. In this
work, we focus on recommendation in Internet fo-
rums and blogs with discussion threads.
Here, a fundamental challenge is to account for
topic divergence, i.e. the change of gist during
the process of discussion. In a discussion thread,
the original posting is typically followed by other
readers? opinions, in the form of comments. Inten-
1http://technorati.com/
257
tion and concerns of active users may change as
the discussion goes on. Therefore, recommenda-
tion, if it were only based on the original posting,
can not benefit the potentially evolving interests of
the users. Apparently, there is a need to consider
topic evolution in adaptive content-based recom-
mendation and this requires novel techniques in
order to capture topic evolution precisely and to
prevent drastic topic shifting which returns com-
pletely irrelevant articles to users.
In this work, we present a framework to recom-
mend relevant information in Internet forums and
blogs using user comments, one of the most rep-
resentative recordings of user behaviors in these
forms of social media.
It has the following contributions.
? The relevant information is recommended
based on a balanced perspective of both the
authors and readers.
? We model the relationship among comments
and that relative to the original posting us-
ing graphs in order to evaluate their combined
impact. In addition, the weight of a comment
is further enhanced with its content and with
the authority of its poster.
2 Related Work
In a broader context, a related problem is content-
based information recommendation (or filtering).
Most information recommender systems select ar-
ticles based on the contents of the original post-
ings. For instance, Chiang and Chen (Chiang and
Chen, 2004) study a few classifiers for agent-based
news recommendations. The relevant news selec-
tions of these work are determined by the textual
similarity between the recommended news and the
original news posting. A number of later proposals
incorporate additional metadata, such as user be-
haviors and timestamps. For example, Claypool et
al. (Claypool et al, 1999) combine the news con-
tent with numerical user ratings. Del Corso, Gull??,
and Romani (Del Corso et al, 2005) use times-
tamps to favor more recent news. Cantador, Bel-
login, and Castells (Cantador et al, 2008) utilize
domain ontology. Lee and Park (Lee and Park,
2007) consider matching between news article at-
tributes and user preferences. Anh et al (Ahn
et al, 2007) and Lai, Liang, and Ku (Lai et al,
2003) construct explicit user profiles, respectively.
Lavrenko et al (Lavrenko et al, 2000) propose
the e-Analyst system which combines news stories
with trends in financial time series. Some go even
further by ignoring the news contents and only us-
ing browsing behaviors of the readers with similar
interests (Das et al, 2007).
Another related problem is topic detection and
tracking (TDT), i.e. automated categorization of
news stories by their themes. TDT consists
of breaking the stream of news into individual
news stories, monitoring the stories for events
that have not been seen before, and categorizing
them (Lavrenko and Croft, 2001). A topic is mod-
eled with a language profile deduced by the news.
Most existing TDT schemes calculate the similar-
ity between a piece of news and a topic profile to
determine its topic relevance (Lavrenko and Croft,
2001) (Yang et al, 1999). Qiu (Qiu et al, 2009)
apply TDT techniques to group news for collabo-
rative news recommendation. Some work on TDT
takes one step further in that they update the topic
profiles as part of the learning process during its
operation (Allan et al, 2002) (Leek et al, 2002).
Most recent researches on information recom-
mendation in social media focus on the blogo-
sphere. Various types of user interactions in the
blogosphere have been observed. A prominent
feature of the blogosphere is the collective wis-
dom (Agarwal et al, 2010). That is, the knowledge
in the blogosphere is enriched by such engaging
interactions among bloggers and readers as post-
ing, commenting and tagging. Prior to this work,
the linking structure and user tagging mechanisms
in the blogosphere are the most widely adopted
ones to model such collective wisdom. For ex-
ample, Esmaili et al (Esmaili et al, 2006) fo-
cus on the linking structure among blogs. Hayes,
Avesani, and Bojars (Hayes et al, 2007) explore
measures based on blog authorship and reader tag-
ging to improve recommendation. Li and Chen
further integrate trust, social relation and semantic
analysis (Li and Chen, 2009). These approaches
attempt to capture accurate similarities between
postings without using reader comments. Due
to the interactions between bloggers and readers,
blog recommendation should not limit its input to
only blog postings themselves but also incorporate
feedbacks from the readers.
The rest of this article is organized as follows.
We first describe the design of our recommenda-
tion framework in Section 3. We then evaluate
the performance of such a recommender using two
258
  
 
		

 








 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 24?29,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploiting Topic based Twitter Sentiment for Stock Prediction 
Jianfeng Si* Arjun Mukherjee? Bing Liu? Qing Li* Huayi Li? Xiaotie Deng? 
*Department of Computer Science, City University of Hong Kong, Hong Kong, China 
*{ thankjeff@gmail.com, qing.li@cityu.edu.hk} 
?Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA 
?{ arjun4787@gmail.com, liub@cs.uic.edu, lhymvp@gmail.com} 
?AIMS Lab, Department of Computer Science, Shanghai Jiaotong University, Shanghai, China 
?deng-xt@cs.sjtu.edu.cn  
 
Abstract 
This paper proposes a technique to leverage 
topic based sentiments from Twitter to help 
predict the stock market. We first utilize a con-
tinuous Dirichlet Process Mixture model to 
learn the daily topic set. Then, for each topic 
we derive its sentiment according to its opin-
ion words distribution to build a sentiment 
time series. We then regress the stock index 
and the Twitter sentiment time series to predict 
the market. Experiments on real-life S&P100 
Index show that our approach is effective and 
performs better than existing state-of-the-art 
non-topic based methods. 
1 Introduction 
Social media websites such as Twitter, Facebook, 
etc., have become ubiquitous platforms for social 
networking and content sharing. Every day, they 
generate a huge number of messages, which give 
researchers an unprecedented opportunity to uti-
lize the messages and the public opinions con-
tained in them for a wide range of applications 
(Liu, 2012). In this paper, we use them for the 
application of stock index time series analysis. 
Here are some example tweets upon querying 
the keyword ?$aapl? (which is the stock symbol 
for Apple Inc.) in Twitter: 
1. ?Shanghai Oriental Morning Post confirm-
ing w Sources that $AAPL TV will debut 
in May, Prices range from $1600-$3200, 
but $32,000 for a 50"wow.? 
2. ?$AAPL permanently lost its bid for a ban 
on U.S. sales of the Samsung Galaxy Nex-
us http://dthin.gs/XqcY74.? 
3. ?$AAPL is loosing customers. everybody is 
buying android phones! $GOOG.? 
As shown, the retrieved tweets may talk about 
Apple?s products, Apple?s competition relation-
ship with other companies, etc. These messages 
are often related to people?s sentiments about 
Apple Inc., which can affect or reflect its stock 
trading since positive sentiments can impact 
sales and financial gains. Naturally, this hints 
that topic based sentiment is a useful factor to 
consider for stock prediction as they reflect peo-
ple?s sentiment on different topics in a certain 
time frame. 
This paper focuses on daily one-day-ahead 
prediction of stock index based on the temporal 
characteristics of topics in Twitter in the recent 
past. Specifically, we propose a non-parametric 
topic-based sentiment time series approach to 
analyzing the streaming Twitter data. The key 
motivation here is that Twitter?s streaming mes-
sages reflect fresh sentiments of people which 
are likely to be correlated with stocks in a short 
time frame. We also analyze the effect of training 
window size which best fits the temporal dynam-
ics of stocks. Here window size refers to the 
number of days of tweets used in model building. 
Our final prediction model is built using vec-
tor autoregression (VAR). To our knowledge, 
this is the first attempt to use non-parametric 
continuous topic based Twitter sentiments for 
stock prediction in an autoregressive framework. 
2 Related Work 
2.1 Market Prediction and Social Media 
Stock market prediction has attracted a great deal 
of attention in the past. Some recent researches 
suggest that news and social media such as blogs, 
micro-blogs, etc., can be analyzed to extract pub-
lic sentiments to help predict the market (La-
vrenko et al, 2000; Schumaker and Chen, 2009). 
Bollen et al (2011) used tweet based public 
mood to predict the movement of Dow Jones 
*   The work was done when the first author was visiting 
University of Illinois at Chicago. 
 
 
 
 
 
 
24
Industrial Average index. Ruiz et al (2012) stud-
ied the relationship between Twitter activities 
and stock market under a graph based view. 
Feldman et al (2011) introduced a hybrid ap-
proach for stock sentiment analysis based on 
companies? news articles.  
2.2 Aspect and Sentiment Models 
Topic modeling as a task of corpus exploration 
has attracted significant attention in recent years. 
One of the basic and most widely used models is 
Latent Dirichlet Allocation (LDA) (Blei et al, 
2003). LDA can learn a predefined number of 
topics and has been widely applied in its extend-
ed forms in sentiment analysis and many other 
tasks (Mei et al, 2007; Branavan et al, 2008; Lin 
and He, 2009; Zhao et al, 2010; Wang et al, 
2010; Brody and Elhadad, 2010; Jo and Oh, 2011; 
Moghaddam and Ester, 2011; Sauper et al, 2011; 
Mukherjee and Liu, 2012; He et al, 2012).  
The Dirichlet Processes Mixture (DPM) model 
is a non-parametric extension of LDA (Teh et al, 
2006), which can estimate the number of topics 
inherent in the data itself. In this work, we em-
ploy topic based sentiment analysis using DPM 
on Twitter posts (or tweets). First, we employ a 
DPM to estimate the number of topics in the 
streaming snapshot of tweets in each day.  
Next, we build a sentiment time series based 
on the estimated topics of daily tweets. Lastly, 
we regress the stock index and the sentiment 
time series in an autoregressive framework. 
3 Model 
We now present our stock prediction framework. 
3.1 Continuous DPM Model 
Comparing to edited articles, it is much harder to 
preset the number of topics to best fit continuous 
streaming Twitter data due to the large topic di-
versity in tweets. Thus, we resort to a non-
parametric approach: the Dirichlet Process Mix-
ture (DPM) model, and let the model estimate the 
number of topics inherent in the data itself. 
Mixture model is widely used in clustering and 
can be formalized as follows: 
   ?      (       )
 
              (1) 
where    is a data point,    is its cluster label, K 
is the number of topics,  (       ) is the sta-
tistical (topic) models: *  +   
  and     is the 
component weight satisfying      and  
?      . 
In our setting of DPM, the number of mixture 
components (topics) K is unfixed apriori but es-
timated from tweets in each day. DPM is defined 
as in (Neal, 2010): 
               (  )  
              
         (   )                 (2) 
where    is the parameter of the model that      
belongs to, and   is defined as a Dirichlet Pro-
cess with the base measure H and the concentra-
tion parameter   (Neal, 2010). 
We note that neighboring days may share the 
same or closely related topics because some top-
ics may last for a long period of time covering 
multiple days, while other topics may just last for 
a short period of time. Given a set of time-
stamped tweets, the overall generative process 
should be dynamic as the topics evolve over time. 
There are several ways to model this dynamic 
nature (Sun et al, 2010; Kim and Oh, 2011; 
Chua and Asur, 2012; Blei and Lafferty, 2006; 
Wang et al, 2008). In this paper, we follow the 
approach of Sun et al (2010) due to its generality 
and extensibility. 
Figure 1 shows the graphical model of our con-
tinuous version of DPM (which we call cDPM). 
As shown, the tweets set is divided into daily 
based collections: *         +  *    +   
     are the 
observed tweets and *    +   
     are the model pa-
rameters (latent topics) that generate these tweets. 
For each subset of tweets,    (tweets of day  ), 
we build a DPM on it. For the first day (   ), 
the model functions the same as a standard DPM, 
i.e., all the topics use the same base measure, 
      ( ). However, for later days (   ), 
besides the base measure,      ( ), we make 
use of topics learned from previous days as pri-
ors. This ensures smooth topic chains or links 
(details in ?3.2). For efficiency, we only consider 
topics of one previous day as priors. 
We use collapsed Gibbs sampling (Bishop, 
2006) for model inference. Hyper-parameters are 
set to:              ;       as in 
(Sun et al, 2010; Teh et al, 2006) which have 
been shown to work well. Because a tweet has at 
most 140 characters, we assume that each tweet 
contains only one topic. Hence, we only need to 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM. 
? 
 
   
 
   
        
 
   
 
     
 
     
 
   
 
    
    
    
   
 
 
     
 
     
 
 
     
 
           
25
sample the topic assignment    for each tweet   . 
According to different situations with respect 
to a topic?s prior, for each tweet    in   , the 
conditional distribution for    given all other 
tweets? topic assignments, denoted by    , can be 
summarized as follows: 
1.    is a new topic: Its candidate priors contain 
the symmetric base prior    ( )  and topics 
*      +   
     learned from            .  
? If    takes a symmetric base prior: 
 (     
           )  
 
     
 (    )
 (       )
?  (      )
   
   
?  ( )
   
   
           (3) 
where the first part denotes the prior proba-
bility according to the Dirichlet Process and 
the second part is the data likelihood (this 
interpretation can similarly be applied to the 
following three equations).  
? If    takes one topic k from *      +   
     as 
its prior: 
 (      
            )   
 
       
     
 (    )
 (       )
?  (         ( )     )
   
   
?  (         ( ))
   
   
 (4) 
2. k is an existing topic: We already know its 
prior. 
? If k takes a symmetric base prior: 
  (               )  
 
  
  
     
 (        ( )
  )
 (           ( )
  )
?  (           
  )
   
   
?  (      
  )
   
   
 (5) 
? If k takes topic        as its prior:  
  (               )  
  
  
     
 .        ( )
  /
 .           ( )
  /
?  (         ( )          
  )
   
   
?  (         ( )     
  )
   
   
 (6) 
Notations in the above equations are listed as 
follows: 
?      is the number of topics learned in day t-1. 
? |V| is the vocabulary size. 
?    is the document length of   . 
?      is the term frequency of word   in   . 
?       ( ) is the probability of word   in pre-
vious day?s topic k.  
?   
   is the number of tweets assigned to topic k 
excluding the current one   .  
?     
   is the term frequency of word   in topic k, 
with statistic from    excluded. While    ( )
   
denotes the marginalized sum of all words in 
topic k with statistic from    excluded. 
Similarly, the posteriors on *    ( )+  (topic 
word distributions) are given according to their 
prior situations as follows: 
? If topic k takes the base prior: 
           ( )   (      ) (         ( )) ?     (7) 
where      is the frequency of word   in topic 
k and    ( )  is the marginalized sum over all 
words. 
? otherwise, it is defined recursively as: 
    ( )  (          ( )      ) (         ( ))?  (8) 
where       serves as the topic prior for     . 
Finally, for each day we estimate the topic 
weights,    as follows:  
        ?      ?                              (9) 
where    is the number of tweets in topic k. 
3.2 Topic-based Sentiment Time Series 
Based on an opinion lexicon   (a list of positive 
and negative opinion words, e.g., good and bad), 
each opinion word,     is assigned with a po-
larity label  ( ) as ?+1? if it is positive and ?-1? 
if negative. We spilt each tweet?s text into opin-
ion part and non-opinion part. Only non-opinion 
words in tweets are used for Gibbs sampling. 
Based on DPM, we learn a set of topics from 
the non-opinion words space  . The correspond-
ing tweets? opinion words share the same topic 
assignments as its tweet. Then, we compute the 
posterior on opinion word probability,     
 ( ) 
for topic   analogously to equations (7) and (8). 
Finally, we define the topic based sentiment 
score  (   ) of topic   in day t as a weighted 
linear combination of the opinion polarity labels: 
 (   )   ?     
 ( )
   
    ( );  (   )   ,    -    (10) 
According to the generative process of cDPM, 
topics between neighboring days are linked if a 
topic k takes another topic as its prior. We regard 
this as evolution of topic k. Although there may 
be slight semantic variation, the assumption is 
reasonable. Then, the sentiment scores for each 
topic series form the sentiment time series {?, 
S(t-1, k), S(t, k), S(t+1, k), ...}. 
Figure 2 demonstrates the linking process 
where a triangle denotes a new topic (with base 
symmetric prior), a circle denotes a middle topic 
(taking a topic from the previous day as its prior, 
 
 
 
 
           0      ?       t-1          t         t+1    ?    N 
Figure 2: Linking the continuous topics via 
neighboring priors. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM  
 
 
? 
... 
 
 
? 
 
 
.
.
. 
 
 
? 
 
 
? ? 
26
while also supplying prior for the next day) and 
an ellipse denotes an end topic (no further topics 
use it as a prior). In this example, two continuous 
topic chains or links (via linked priors) exist for 
the time interval [t-1, t+1]: one in light grey color, 
and the other in black. As shown, there may be 
more than one topic chain/link (5-20 in our ex-
periments) for a certain time interval1.Thus, we 
sort multiple sentiment series according to their 
accumulative weights of topics over each link: 
?     
  
    
. In our experiments, we try the top 
five series and use the one that gives the best re-
sult, which is mostly the first (top ranked) series 
with a few exceptions of the second series. The 
topics mostly focus on hot keywords like: news, 
stocknews, earning, report, which stimulate ac-
tive discussions on the social media platform. 
3.3 Time Series Analysis with VAR 
For model building, we use vector autoregression 
(VAR). The first order (time steps of historical 
information to use: lag = 1) VAR model for two 
time series *  + and *  + is given by:  
                                                   
                                               (11) 
where * + are the white noises and * + are model 
parameters. We use the ?dse? library2 in the R 
language to fit our VAR model based on least 
square regression. 
 Instead of training in one period and predicting 
over another disjointed period, we use a moving 
training and prediction process under sliding 
windows3 (i.e., train in [t, t + w] and predict in-
dex on t + w + 1) with two main considerations: 
? Due to the dynamic and random nature of both 
the stock market and public sentiments, we are 
more interested in their short term relationship. 
? Based on the sliding windows, we have more 
training and testing points.  
Figure 3 details the algorithm for stock index 
prediction. The accuracy is computed based on 
the index up and down dynamics, the function 
     (    ) returns True only if   (our predic-
tion) and   (actual value) share the same index 
up or down direction. 
 
 
                                                 
1 The actual topic priors for topic links are governed by the 
four cases of the Gibbs Sampler. 
2 http://cran.r-project.org/web/packages/dse 
3  This is similar to the autoregressive moving average 
(ARMA) models. 
4 Dataset 
We collected the tweets via Twitter?s REST API 
for streaming data, using symbols of the Stand-
ard & Poor's 100 stocks (S&P100) as keywords. 
In this study, we focus only on predicting the 
S&P100 index. The time period of our dataset is 
between Nov. 2, 2012 and Feb. 7, 2013, which 
gave us 624782 tweets. We obtained the S&P100 
index?s daily close values from Yahoo Finance. 
5 Experiment 
5.1 Selecting a Sentiment Metric 
Bollen et al (2011) used the mood dimension, 
Calm together with the index value itself to pre-
dict the Dow Jones Industrial Average. However, 
their Calm lexicon is not publicly available. We 
thus are unable to perform a direct comparison 
with their system. We identified and labeled a 
Calm lexicon (words like ?anxious?, ?shocked?, 
?settled? and ?dormant?) using the opinion lexi-
con4 of Hu and Liu (2004) and computed the sen-
timent score using the method of Bollen et al 
(2011) (sentiment ratio). Our pilot experiments 
showed that using the full opinion lexicon of Hu 
and Liu (2004) actually performs consistently 
better than the Calm lexicon. Hence, we use the 
entire opinion lexicon in Hu and Liu (2004). 
5.2 S&P100INDEX Movement Prediction 
We evaluate the performance of our method by 
comparing with two baselines. The first (Index) 
uses only the index itself, which reduces the 
VAR model to the univariate autoregressive 
model (AR), resulting in only one index time 
series {  } in the algorithm of Figure 3.  
                                                 
4 http://cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar 
Parameter:  
w: training window size; lag: the order of VAR;  
Input:   : the date of time series; {  }: sentiment time 
series; {  }: index time series; 
Output: prediction accuracy. 
1. for t = 0, 1, 2, ?, N-w-1 
2. { 
3.        = VAR( ,     -  ,     -, lag); 
4.             
 =      .Predict(x[t+w+1-lag, t+w],  
  y[t+w+1-lag, t+w]); 
5.       if (     (      
        ) )   
 rightNum++;  
6.     } 
7.    Accuracy = rightNum / (N-w); 
8.    Return Accuracy; 
Figure 3: Prediction algorithm and accuracy 
 
 
 
 
 
 
 
 
 
Figure 1: Continuous DPM  
27
 When considering Twitter sentiments, existing 
works (Bollen et al, 2011, Ruiz et al, 2012) 
simply compute the sentiment score as ratio of 
pos/neg opinion words per day. This generates a 
lexicon-based sentiment time series, which is 
then combined with the index value series to give 
us the second baseline Raw.  
 In summary, Index uses index only with the 
AR model while Raw uses index and opinion 
lexicon based time series. Our cDPM uses index 
and the proposed topic based sentiment time se-
ries. Both Raw and cDPM employ the two di-
mensional VAR model. We experiment with dif-
ferent lag settings from 1-3 days. 
 We also experiment with different training 
window sizes, ranging from 15 - 30 days, and 
compute the prediction accuracy for each win-
dow size. Table 1 shows the respective average 
and best accuracies over all window sizes for 
each lag and Table 2 summarizes the pairwise 
performance improvements of averaged scores 
over all training window sizes. Figure 4 show the 
detailed accuracy comparison for lag 1 and lag 3.  
    From Table 1, 2, and Figure 4, we note: 
i. Topic-based public sentiments from tweets 
can improve stock prediction over simple sen-
timent ratio which may suffer from backchan-
nel noise and lack of focus on prevailing top-
ics. For example, on lag 2, Raw performs 
worse by 8.6% than Index itself. 
ii. cDPM outperforms all others in terms of both 
the best accuracy (lag 3) and the average ac-
curacies for different window sizes. The max-
imum average improvement reaches 25.0% 
compared to Index at lag 1 and 15.1% com-
pared to Raw at lag 3. This is due to the fact 
that cDPM learns the topic based sentiments 
instead of just using the opinion words? ratio 
like Raw, and in a short time period, some 
topics are more correlated with the stock mar-
ket than others. Our proposed sentiment time 
series using cDPM can capture this phenome-
non and also help reduce backchannel noise 
of raw sentiments.  
iii. On average, cDPM gets the best performance 
for training window sizes within [21, 22], and 
the best prediction accuracy is 68.0% on win-
dow size 22 at lag 3. 
6 Conclusions 
Predicting the stock market is an important but 
difficult problem. This paper showed that Twit-
ter?s topic based sentiment can improve the pre-
diction accuracy beyond existing non-topic based 
approaches. Specifically, a non-parametric topic-
based sentiment time series approach was pro-
posed for the Twitter stream. For prediction, vec-
tor autoregression was used to regress S&P100 
index with the learned sentiment time series. Be-
sides the short term dynamics based prediction, 
we believe that the proposed method can be ex-
tended for long range dependency analysis of 
Twitter sentiments and stocks, which can render 
deep insights into the complex phenomenon of 
stock market. This will be part of our future work. 
Acknowledgments 
This work was supported in part by a grant from 
the National Science Foundation (NSF) under 
grant no. IIS-1111092 and a strategic research 
grant from City University of Hong Kong (pro-
ject number: 7002770). 
Lag Index Raw cDPM 
1 0.48(0.54) 0.57(0.59) 0.60(0.64) 
2 0.58(0.65) 0.53(0.62) 0.60(0.63) 
3 0.52(0.56) 0.53(0.60) 0.61(0.68) 
Table 1: Average (best) accuracies over all 
training window sizes and different lags 1, 2, 3. 
Lag Raw vs. Index cDPM vs. Index cDPM vs. Raw 
1 18.8% 25.0% 5.3% 
2 -8.6% 3.4% 13.2% 
3 1.9% 17.3% 15.1% 
Table 2: Pairwise improvements among Index, 
Raw and cDPM averaged over all training win-
dow sizes. 
 
Figure 4: Comparison of prediction accuracy of 
up/down stock index on S&P 100 index for dif-
ferent training window sizes. 
0.25
0.45
0.65
15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
A
cc
u
ra
cy
 
Training window size 
Comparison on Lag 1 
Index Raw cDPM
0.25
0.35
0.45
0.55
0.65
0.75
18 19 20 21 22 23 24 25 26 27 28 29 30
A
cc
u
ra
cy
 
Training Window size 
Comparison on Lag 3 
Index Raw cDPM
28
References 
Bishop, C. M. 2006. Pattern Recognition and Machine 
Learning. Springer. 
Blei, D., Ng, A. and Jordan, M. 2003. Latent Dirichlet 
allocation. Journal of Machine Learning Research 
3:993?1022. 
Blei, D. and Lafferty, J. 2006. Dynamic topic models. 
In Proceedings of the 23rd International Confer-
ence on Machine Learning (ICML-2006). 
Bollen, J., Mao, H. N., and Zeng, X. J. 2011. Twitter 
mood predicts the stock market. Journal of Com-
puter Science 2(1):1-8.  
Branavan, S., Chen, H., Eisenstein J. and Barzilay, R. 
2008. Learning document-level semantic properties 
from free-text annotations. In Proceedings of the 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2008). 
Brody, S. and Elhadad, S. 2010. An unsupervised 
aspect-sentiment model for online reviews. In Pro-
ceedings of the 2010 Annual Conference of the 
North American Chapter of the ACL (NAACL-
2010). 
Chua, F. C. T. and Asur, S. 2012. Automatic Summa-
rization of Events from Social Media, Technical 
Report, HP Labs. 
Feldman, R., Benjamin, R., Roy, B. H. and Moshe, F. 
2011. The Stock Sonar - Sentiment analysis of 
stocks based on a hybrid approach. In Proceedings 
of 23rd IAAI Conference on Artificial Intelligence 
(IAAI-2011). 
He, Y., Lin, C., Gao, W., and Wong, K. F. 2012. 
Tracking sentiment and topic dynamics from social 
media. In Proceedings of the 6th International 
AAAI Conference on Weblogs and Social Media 
(ICWSM-2012). 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004). 
Jo, Y. and Oh, A. 2011. Aspect and sentiment unifica-
tion model for online review analysis. In Proceed-
ings of ACM Conference in Web Search and Data 
Mining (WSDM-2011). 
Kim, D. and Oh, A. 2011. Topic chains for under-
standing a news corpus. CICLing (2): 163-176. 
Lavrenko, V., Schmill, M., Lawrie, D., Ogilvie, P., 
Jensen, D. and Allan, J. 2000. Mining of concur-
rent text and time series. In Proceedings of the 6th 
KDD Workshop on Text Mining, 37?44. 
Lin, C. and He, Y. 2009. Joint sentiment/topic model 
for sentiment analysis. In Proceedings of ACM In-
ternational Conference on Information and 
Knowledge Management (CIKM-2009). 
Liu, B. 2012. Sentiment analysis and opinion mining. 
Morgan & Claypool Publishers.  
Mei, Q., Ling, X., Wondra, M., Su, H. and Zhai, C. 
2007. Topic sentiment mixture: modeling facets 
and opinions in weblogs. In Proceedings of Interna-
tional Conference on World Wide Web (WWW-
2007). 
Moghaddam, S. and Ester, M. 2011. ILDA: Interde-
pendent LDA model for learning latent aspects and 
their ratings from online product reviews.  In Pro-
ceedings of the Annual ACM SIGIR International 
conference on Research and Development in In-
formation Retrieval (SIGIR-2011). 
Mukherjee A. and Liu, B. 2012. Aspect extraction 
through semi-supervised modeling. In Proceedings 
of the 50th Annual Meeting of the Association for 
Computational Linguistics (ACL-2012).  
Neal, R.M. 2000. Markov chain sampling methods for 
dirichlet process mixture models. Journal of Com-
putational and Graphical Statistics, 9(2):249-265. 
Ruiz, E. J., Hristidis, V., Castillo, C., Gionis, A. and 
Jaimes, A. 2012. Correlating financial time series 
with micro-blogging activity. In Proceedings of the 
fifth ACM international conference on Web search 
and data mining (WSDM-2012), 513-522.  
Sauper, C., Haghighi, A. and Barzilay, R. 2011. Con-
tent models with attitude. Proceedings of the 49th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). 
Schumaker, R. P. and Chen, H. 2009. Textual analysis 
of stock market prediction using breaking financial 
news. ACM Transactions on Information Systems 
27(February (2)):1?19. 
Sun, Y. Z., Tang, J. Han, J., Gupta M. and Zhao, B. 
2010. Community Evolution Detection in Dynamic 
Heterogeneous Information Networks. In Proceed-
ings of KDD Workshop on Mining and Learning 
with Graphs (MLG'2010), Washington, D.C. 
Teh, Y., Jordan M., Beal, M. and Blei, D. 2006. Hier-
archical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101[476]:1566-1581. 
Wang, C. Blei, D. and Heckerman, D. 2008. Continu-
ous Time Dynamic Topic Models. Uncertainty in 
Artificial Intelligence (UAI 2008), 579-586 
Wang, H., Lu, Y.  and Zhai, C. 2010. Latent aspect 
rating analysis on review text data: a rating regres-
sion approach. Proceedings of ACM SIGKDD In-
ternational Conference on Knowledge Discovery 
and Data Mining (KDD-2010). 
Zhao, W. Jiang, J. Yan, Y. and Li, X. 2010. Jointly 
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP-2010). 
29
