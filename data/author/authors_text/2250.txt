Legal Texts Summarization by Exploration of the
Thematic Structures and Argumentative Roles
Atefeh Farzindar and Guy Lapalme
RALI, De?partement d?Informatique et Recherche Ope?rationnelle
Universite? de Montre?al, Que?bec, Canada, H3C 3J7
{farzinda,lapalme}@iro.umontreal.ca
Abstract
In this paper we describe our method for the sum-
marization of legal documents helping a legal ex-
pert determine the key ideas of a judgment. Our
approach is based on the exploration of the docu-
ment?s architecture and its thematic structures in or-
der to build a table style summary for improving co-
herency and readability of the text. We present the
components of a system, called LetSum, built with
this approach, its implementation and some prelim-
inary evaluation results.
1 Introduction
The goal of a summary is to give the reader an accu-
rate and complete idea of the contents of the source
(Mani, 2001). In this research, we focused on a
problem referred to as legal text summarization.
As ever larger amounts of legal documents become
available electronically, interest in automatic sum-
marization has continued to grow in recent years. In
this paper, we present our method for producing a
very short text from a long legal document (a record
of the proceedings of federal courts in Canada) and
present it as a table style summary. The goal of
this project is to develop a system to create a sum-
mary for the needs of lawyers, judges and experts
in the legal domain. Our approach investigates the
extraction of the most important units based on the
identification of thematic structures of the document
and the determination of semantic roles of the tex-
tual units in the judgment (Farzindar, 2004). The
remainder of the paper is organized as follows. Sec-
tion 2 introduces the motivation of the research and
the context of the work. Section 3 reports on the
results of our analysis of a corpus of legal abstracts
written by professional abstractors. Section 4 de-
scribes our method for the exploration of document
architecture and the components of the system that
we have developed to produce a summary. Section 5
presents some related work in this domain. Section
6 concludes the paper and presents some prelimi-
nary evaluation results for the components of our
system.
2 Context of the Work
In Canada, the Canadian Legal Information Insti-
tute project (CANLII) aims at gathering legisla-
tive and judicial texts, as well as legal commen-
taries, from federal, provincial and territorial ju-
risdictions in order to make primary sources of
Canadian law accessible for free on the Internet
(http://www.canlii.org). The large vol-
ume of legal information in electronic form creates
a need for the creation and production of powerful
computational tools in order to extract relevant in-
formation in a condensed form.
But why are we interested in the processing of
previous legal decisions and in their summaries?
First, because a court order generally gives a so-
lution to a legal problem between two or several
parties. The decision also contains the reasons
which justify the solution and constitute a law ju-
risprudence precedent from which it is possible to
extract a legal rule that can be applied to simi-
lar cases. To find a solution to a legal problem
not directly indicated in the law, lawyers look for
precedents of similar cases. For a single query in
a data base of law reports, we often receive hun-
dreds of documents that are very long to study for
which legal experts and law students request sum-
maries. In Quebec REJB (R e?pertoire e?lectronique
de jurisprudence du Barreau) and SOQUIJ (Soci e?t e?
qu e?b e?coise d?information juridique) are two orga-
nizations which provide manual summaries for le-
gal resources, but the human time and expertise re-
quired makes their services very expensive. For ex-
ample the price of only one summary with its full
text, provided by SOQUIJ is 7.50 $ can. Some legal
information systems have been developed by private
companies like QuickLaw in Canada and WEST-
LAW and LEXIS in the United States, however no
existing system completely satisfies the specific re-
quirements of this field.
One reason for the difficulty of this work is the
complexity of the domain: specific vocabularies of
Between:
JASPER NATIONAL PARK Applicants and THE ATTORNEY GENERAL OF CANADA Respondent,
Docket: T-1557-98
Judgment Professional abstract Role
[1] This application for judicial review arises
out of a decision (the Decision) announced on
or about the 30th of June 1998 by the Minister
of Canadian Heritage (the Minister) to close
the Maligne River (the River) in Jasper Na-
tional Park to all boating activity, beginning
in 1999.
Judicial review of Minister of Canadian
Heritage?s decision to close Maligne River
in Jasper National Park to all boating activ-
ity beginning in 1999 to protect habitat of
harlequin ducks.
INTRO-
DUCTION
[7] The applicants offer commercial rafting
trips to Park visitors in this area each year
from mid-June to sometime in September.
Applicants offer commercial rafting trips on
River.
CONTEXT
[10] Consequently, a further environmental
assessment regarding commercial rafting on
the Maligne River was prepared in 1991. The
assessment indicated that rafting activity had
expanded since 1986, with an adverse impact
on Harlequin ducks along the Maligne River.
1991 environmental assessment indicating
rafting having adverse impact on harlequin
ducks along river.
CONTEXT
Table 1: Alignment of the units of the original judgment with the professional abstract
the legal domain and legal interpretations of expres-
sions produce many ambiguities. For example, the
word sentence can have two very different mean-
ings: one is a sequence of words and the other is a
more particular meaning in law, the decision as to
what punishment is to be imposed. Similarly dis-
position which means nature, effort, mental attitude
or property but in legal terms it means the final part
of a judgement indicating the nature of a decision:
acceptation of a inquiry or dismission.
Most previous systems of automatic summariza-
tion are limited to newspaper articles and scien-
tific articles (Saggion and Lapalme, 2002). There
are important differences between news style and
the legal language: statistics of words, probability
of selection of textual units, position of paragraphs
and sentences, words of title and lexical chains rela-
tions between words of the title and the key ideas of
the text, relations between sentences and paragraphs
and structures of the text.
For judgments, we show that we can identify dis-
cursive structures for the different parts of the deci-
sion and assign some argumentative roles to them.
Newspapers articles often repeat the most important
message but, in law, important information may ap-
pear only once. The processing of a legal document
requires detailed attention and it is not straight for-
ward to adapt the techniques developed for other
types of document to the legal domain.
3 Observations from a Corpus
3.1 Composition
Our corpus contains 3500 judgments of the Federal
Court of Canada, which are available in HTML on
http://www.canlii.org/ca/cas/fct/.
We analyzed manually 50 judgments in English and
15 judgments in French as well as their summaries
written by professional legal abstractors. The
average size of the documents that are input to
our system are judgments between 500 and 4000
words long (2 to 8 pages), which form 80% of all
3500 judgments; 10% of the documents having
less than 500 words (about one page) and so
they do not need a summary. Only 10% of the
decisions have more than 4000 words. Contrary
to some existing systems (Moens et al, 1999) that
focus only on limited types of judgments, such
as criminal cases, our research deals with many
categories of texts such as: Access to information,
Administrative law, Air law, Broadcasting, Com-
petition, Constitutional law, Copyright, Customs
and Excise - Customs Act, Environment, Evidence,
Human rights, Maritime law, Official languages,
Penitentiaries, Unemployment insurance and etc.
3.2 Structure of Legal Judgments
During our corpus analysis, we compared model
summaries written by humans with the texts of
the original judgments. We have identified the
organisational architecture of a typical judgment.
Thematic structures Content Judgment Summary
DECISION DATA Name of the jurisdiction,
place of the hearing,
date of the decision,
identity of the author,
names of parties,
title of proceeding and
Authority and doctrine
INTRODUCTION Who? did what? to whom? 5 % 12 %
CONTEXT Facts in chronological order or by description 24 % 20 %
JURIDICAL ANALYSIS Comments by the judge, finding of facts and
application of the law
67 % 60 %
CONCLUSION Final decision of the court 4 % 8 %
Table 2: Table of summary shows the thematic structures in a jugement and percentage of the contribution
of each thematic structure in source judgment and its human made summary
!"#$%&?(
)#*$#+&%&?,+
)#-#(&?,+
./,01(&?,+
20#+&?3(%&?,+4,5
610*$#+&
,/*%+?7%&?,+4
8?+04&"#
/#-#9%+&41+?&744
:;&/%(&4&"#
"?*"#7&47(,/#0
1+?&744
817?,+4&"#
#;&/%(&#04<%/&7
:-?$?+%&?,+4,5
1+?$<,/&%+&
#-#$#+&7
!%=-#47&>-#
71$$%/>
8?-&#/?+*
?,?7#
@#01(&?,+4
A#*%-
B,(1$#+&
:-?$?+%&?,+4,5
C1,&%&?,+74
B#&#/$?+%&?,+4,5
)#$%+&?(4@,-#7
Figure 1: The procedural steps for generating of table style summary
The paragraphs that address the same subject are
grouped as members of a block. We annotated the
blocks with a label describing their semantic roles.
We also manually annotated citations which are tex-
tual units (sentence or paragraph) quoted by the
judge as reference, for example an article of law
or other jurisprudence. The citations account for a
large part of the text of the judgment, but they are
not considered relevant for the summary, therefore
these segments will be eliminated during the infor-
mation filtering stage.
The textual units considered as important by the
professional abstractors were aligned manually with
one or more elements of the source text. Table 1
shows an example of an alignment between a human
summary and the original judgment. We look for a
match between the information considered impor-
tant in the professional abstract and the information
in the source documents. Our observation shows
that, for producing a summary, a professional ab-
stractor mainly relies on the manual extraction of
important units while conforming to general guide-
lines. The collection of these selected units forms a
summary.
During this analysis, we observed that texts
of jurisprudence are organized according to a
macrostructure and contain various levels of infor-
mation, independently of the category of judgment.
Proposed guidelines by Judge Mailhot of the Court
of Appeal of Quebec (Mailhot, 1998) and (Branting
et al, 1997) on legal judgments support this idea
that it is possible to define organisational structures
for decisions. Jurisprudence is organized by the dis-
course itself, which makes it possible to segment the
texts thematically.
Textual units dealing with the same subject form
a thematic segment set. In this context, we distin-
guish the layered thematic segments, which divide
the legal decisions into different discursive struc-
tures. The identification of these structures sepa-
rates the key ideas from the details of a judgment
and improves readability and coherency in the sum-
mary. We will present the argumentative roles of
each level of discourse, and their importance in
the judgment from the point of view of the key
and principal ideas. Table 2 shows the structure
of a jurisprudence and its different discourse lev-
els. Therefore, in the presentation of a final sum-
mary, we propose to preserve this organization of
the structures of the text in order to build a table
style summary with five themes:
DECISION DATA contains the name of the jurisdic-
tion, the place of the hearing, the date of the de-
cision, the identity of the author, names of par-
ties, title of proceeding, authority and doctrine.
It groups all the basic preliminary information
which is needed for planning the decision.
INTRODUCTION describes the situation before the
court and answers these questions: who are the
parties? what did they do to whom?
CONTEXT explains the facts in chronological or-
der, or by description. It recomposes the story
from the facts and events between the par-
ties and findings of credibility on the disputed
facts.
JURIDICAL ANALYSIS describes the comments
of the judge and finding of facts, and the ap-
plication of the law to the facts as found. For
the legal expert this section of judgment is the
most important part because it gives a solution
to the problem of the parties and leads the judg-
ment to a conclusion.
CONCLUSION expresses the disposition which is
the final part of a decision containing the infor-
mation about what is decided by the court. For
example, it specifies if the person is discharged
or not or the cost for a party.
During our corpus analysis, we computed the
distribution of the information (number of words
shown in Table 2) in each level of thematic structure
of the judgment. The average length of a judgment
is 3500 words and 350 words for its summary i.e. a
compression rate of about 10%.
4 Method for Producing Table Style
Summary
Our approach for producing the summary first iden-
tifies thematic structures and argumentative roles in
the document. We extract the relevant sentences and
present them as a table style summary. Showing the
information considered important which could help
the user read and navigate easily between the sum-
mary and the source judgment. For each sentence
of the summary, the user can determine the theme
by looking at its rhetorical role. If a sentence seems
more important for a user and more information is
needed about this topic, the complete thematic seg-
ment containing the selected sentence could be pre-
sented. The summary is built in four phases (Fig-
ure 1): thematic segmentation, filtering of less im-
portant units such as citations of law articles, selec-
tion of relevant textual units and production of the
summary within the size limit of the abstract.
The implementation of our approach is a system
called LetSum (Legal text Summarizer), which has
been developed in Java and Perl. Input to the sys-
tem is a legal judgment in English. To determine the
Part-of-Speech tags, the tagger described by (Hep-
ple, 2000) is used. The semantic grammars and
rules are developed in JAPE language (Java Anno-
tations Pattern Engine) and executed by a GATE
transducer (Cunningham et al, 2002).
4.1 Components of LetSum
Thematic segmentation for which we performed
some experiments with two statistic segmenters:
one described by Hearst for the TexTiling system
(Hearst, 1994) and the C99 segmenter described by
Choi (Choi, 2000), both of which apply a clustering
function on a document to find classes divided by
theme. But because the results of these numerical
segmenters were not satisfactory enough to find the
thematic structures of the legal judgments, we de-
cided to develop a segmentation process based on
the specific knowledge of the legal field.
Category of section title Linguistic markers Examples of section title
Begin of the judgment decision, judgment, reason,
order
Reasons for order, Reasons for
judgment and order
INTRODUCTION introduction, summary Introduction, Summary
CONTEXT facts, background The factual background, Agreed
statement of facts
JURIDICAL ANALYSIS analysis, decision, discussion Analysis and Decision of the court
CONCLUSION conclusion, disposiotion, cost Conclusion and Costs
Table 3: The linguistic markers in section titles
Each thematical segment can be associated with
an argumentative role in the judgment based on the
following information: the presence of significant
section titles (Table 3 shows categories and features
of the section titles), the absolute and relative posi-
tions of a segment, the identification of direct or nar-
rative style (as the border of CONTEXT and JURIDI-
CAL ANALYSIS segments), certain linguistic mark-
ers.
The linguistic markers used for each thematic
segment are organized as follows:
CONTEXT introduces the parties with the verb to
be (eg. the application is company X), describes the
application request like: advise, indicate, request
and explains the situation in the past tense and nar-
ration form.
In JURIDICAL ANALYSIS, the judge gives his ex-
planation on the subject thus the style of expression
is direct such as: I, we, this court, the cue phrases
(Paice, 1981) like: In reviewing the sections No. of
the Act, Pursuant to section No., As I have stated, In
the present case, The case at bar is.
In CONCLUSION the classes of verbs are: note,
accept, summarise, scrutinize, think, say, satisfy,
discuss, conclude, find, believe, reach, persuade,
agree, indicate, review, the concepts such as: opin-
ion, conclusion, summary, because, cost, action, the
cue phrases: in the case at bar, for all the above
reasons, in my view, my review of, in view of the evi-
dence, finally, thus, consequently, in the result. This
segment contains the final result of court decision
using phrases such as: The motion is dismissed, the
application must be granted. The important verbs
are: allow, deny, dismiss, grant, refuse.
Filtering identifies parts of the text which can be
eliminated, without losing relevant information for
the summary. In a judgment, the citation units (sen-
tence or paragraph) occupy a large volume in the
text, up to 30%, of the judgment, whereas their con-
tents are less important for the summary. This is
why we remove citations inside blocks of thematic
segments. We thus filter two categories of segments:
submissions and arguments that report the points of
view of the parties in the litigation and citations re-
lated for previous issues or references to applicable
legislation. In the case of eliminating a citation of
a legislation (eg. law?s article), we save the refer-
ence of the citation in DECISION DATA in the field
of authority and doctrine.
The identification of citations is based on two
types of markers: direct and indirect. A direct
marker is one of the linguistic indicators that we
classified into three classes: verbs, concepts (noun,
adverb, adjective) and complementary indications.
Examples of verbs of citation are: conclude, define,
indicate, provide, read, reference, refer, say, state,
summarize. Examples of the concepts are: follow-
ing, section, subsection, page, paragraph, pursuant.
Complementary indications include numbers, cer-
tain preposition, relative clauses and typographic
marks (colon, quotation marks).
The indirect citations are the neighboring units of
a quoted phrase. For example, in Table 4 a citation
is shown. For detecting CITATION segment units
such as paragraph 78(1), which reads as follows:
are identified using direct markers (shown here in
bold) but surrounding textual units with numbers
are also quotations. We thus developed a linear inte-
gration identification mechanism for sentences fol-
lowing a quoted sentence for determining a group
of citations.
Selection builds a list of the best candidate units
for each structural level of the summary. LetSum
computes a score for each sentence in the judgment
based on heuristic functions related to the following
information: position of the paragraphs in the doc-
ument, position of the paragraphs in the thematic
segment, position of the sentences in the paragraph,
distribution of the words in document and corpus
(tf ? idf ). Depending on the given information in
each layered segment, we have identified some cue
words and linguistic markers. The thematic segment
can change the value of linguistic indicators. For ex-
ample, the phrase application is dismissed that can
be considered as a important feature in the CON-
CLUSION might not have the same value in CON-
TEXT segment. At the end of this stage, the pas-
sages with the highest resulting scores are sorted to
determine the most relevant ones.
Production of the final summary in which the
selected sentences are normalized and displayed in
tabular format. The final summary is about 10%
of source document. The elimination of the unim-
portant sentences takes into account length statistics
presented in Table 2. In the INTRODUCTION seg-
ment, units with the highest score are kept within
10% of the size of summary. In the CONTEXT seg-
ment, the selected units occupy 24% of the sum-
mary length. The contribution of the JURIDICAL
ANALYSIS segment is 60% and the units with the
role CONCLUSION occupy 6% of the summary.
4.2 Current State of LetSum
Table 4 shows an example of the output after the ex-
ecution of the Selection module of LetSum (mod-
ules of Figure 1 up to the horizontal line) applied
on a judgment of Federal Court of Canada (2468
words). Thematic segmentation module has di-
vided the text into structural blocks according to
the rhetorical roles (given to the left of braces in
Table 4). The Filtering module removes citation
blocks and its enumerated quoted paragraphs (e.g.
paragraph (15) in tablet). Selection module chooses
total relevant textual units (shown in bold in Table 4)
in each thematic segment. The units are selected
according to their argumentative role in the judge-
ment. Here the length of all extracted units is 313
words.
Preliminary evaluations of components of Let-
Sum are very promising; we obtained 0.90 F-
measure for thematic segmentation and 0.97 F-
measure for filtering stage (detection of 57 quoted
segment correctly on 60).
From this information, the Production module
(currently being implemented) could concatenate
textual units with some grammatical modification to
produce a short summary.
5 Related Research
LetSum is the one of the few systems developed
specifically for the summarization of legal docu-
ments. All of these approaches attest the impor-
tance of the exploration of thematic structures in le-
gal documents.
The FLEXICON project (Smith and Deedman,
1987) generates a summary of legal cases by us-
ing information retrieval based on location heuris-
tics, occurrence frequency of index terms and the
use of indicator phrases. A term extraction module
that recognizes concepts, case citations, statute ci-
tations and fact phrases leads to a document profile.
This project was developed for the decision reports
of Canadian courts, which are similar to our corpus.
SALOMON (Moens et al, 1999) automatically
extracts informative paragraphs of text from Belgian
legal cases. In this project a double methodology
was used. First, the case category, the case struc-
ture and irrelevant text units are identified based
on a knowledge base represented as a text gram-
mar. Consequently, general data and legal foun-
dations concerning the essence of the case are ex-
tracted. Secondly, the system extracts informative
text units of the alleged offences and of the opinion
of the court based on the selection of representative
objects.
More recently, SUM (Grover et al, 2003) exam-
ined the use of rhetorical and discourse structure in
level of the sentence of legal cases for finding the
main verbes. The methodology is based on (Teufel
and Moens, 2002) where sentences are classified ac-
cording to their argumentative role.
These studies have shown the interest of summa-
rization in a specialized domain such as legal texts
but none of these systems was implemented in an
environment such as CANLII which has to deal with
thousands of texts and produce summaries for each.
6 Conclusion
In this paper, we have presented our approach for
dealing with automatic summarization techniques.
This work refers to the problem of processing of a
huge volume of electronic documents in the legal
field which becomes more and more difficult to ac-
cess. Our method is based on the extraction of rele-
vant units in the source judgment by identifying the
discourse structures and determining the semantic
roles of thematic segments in the document. The
presentation of the summary is in a tabular form di-
vided by the following thematic structures: DECI-
SION DATA, INTRODUCTION, CONTEXT, JURIDI-
CAL ANALYSIS and CONCLUSION. The generation
of summary is done in four steps: thematic segmen-
tation to detect the document structures, filtering to
eliminate unimportant quotations and noises, selec-
tion of the candidate units and production of table
style summary. The system is currently being fi-
nalized and preliminary evaluation results are very
promising.
7 Acknowledgements
We would like to thanks LexUM group of le-
gal information-processing laboratory of the Public
DECISION DATA
?
?
?
?
?
?
?
?
?
?
?
Name of the jurisdiction: Federal Court of Canada, Place of the hearing: Ottawa
Date of the decision: 31/12/97, Identity of the author: J.E. Dub e?
Names of parties: Commissioner of official languages of canad, Applicant
- and - Air Canada, Respondent
Title of proceeding: Official languages, Docket number: T-1989-96
Authority and doctrine : Official Languages Act, R.S.C., 1985 (4th Supp.), c. 31
INTRODUCTION
?
?
?
?
?
?
?
(1) An order was made by this Court on February 4, 1997 authorizing the respondent (Air Canada) to
raise preliminary objections to the notice of an originating motion filed by the applicant (the Commis-
sioner). As a result, this motion filed by Air Canada on March 18, 1997 raises six alternative prelim-
inary objections asking the Court to strike out in part the motion made by the Commissioner on
September 6, 1996 under section 78 of the Official Languages Act.
CONTEXT
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1. Facts
(2) The Commissioner?s originating motion, which was filed with the consent of the com-
plainant Paul Comeau, concerns Air Canada?s failure to provide ground services in the
French language at the Halifax airport. The Commissioner asks this Court to declare that
there is a significant demand for services in French in Air Canada?s office at the Halifax airport
and that Air Canada is failing to discharge its duties under Part IV of the Act. Part IV estab-
lishes language-related duties for communications with and services to the public, including the
travelling public, where there is significant demand.
(3) The Commissioner?s motion is filed by the complainant Paul Comeau.
...
CITATION
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(15) The point of departure is paragraph 78(1), which reads as follows:
78. (1) The Commissioner may
(a) within the time limits prescribed by paragraph 77(2)( a) or ( b), apply to the Court for
a remedy under this Part in relation to a complaint investigated by the Commissioner if the
Commissioner has the consent of the complainant.
(b) appear before the Court on behalf of any person who has applied under section 77 for a
remedy under this Part; or
(c) with leave of the Court, appear as a party to any proceedings under this Part.
ANALYSIS
?
?
?
?
?
?
?
?
?
(16) Air Canada?s position is therefore that the Commissioner may only apply for a rem-
edy limited to facts relating to a specific complaint, the investigation of that complaint
and the resulting reports and recommendations. In my view, this interpretation is too
narrow and is inconsistent with the general objectives of the Act and its remedial and
quasi-constitutional nature.
...
CONCLUSION
?
?
?
?
?
?
?
?
?
?
?
7. Conclusion
(29) Thus, to ensure that the judge presiding at the hearing on the merits can correctly assess the
situation in light of all the material evidence, no reference or evidence filed by the Commissioner
in the three affidavits mentioned above should be struck out.
(30) This motion to strike by Air Canada with respect to the preliminary objections must accord-
ingly be dismissed.
Table 4: Output produced by the LetSum?s modules: Thematic segmentation, Filtering and Selection.
Source judgment is divided into thematic blocks associated with rhetorical roles, citation block will be
removed in the filtering phase and textual units (shown in bold) have been selected as relevant.
Law Research Center at the University of Montreal
for their valuable suggestions. This project sup-
ported by Public Law Research Center and Natu-
ral Sciences and Engineering Research Council of
Canada.
References
L. Karl Branting, Charles B. Callaway, Bradford W.
Mott, and James C. Lester. 1997. A frame-
work for self-explaining legal documents. In
Proceedings of the Sixth International Confer-
ence on Artificial Intelligence and Law (ICAIL-
97), pages 72?81, University of Melbourne, Mel-
bourne, Australia, June 30-July 3.
Freddy Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proceding of
the 1 st North American Chapter of the Associa-
tion for Computational Linguistics, pages 26?33,
Seattle, Washington.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: A framework and graphi-
cal development environment for robust nlp tools
and applications. In Proceedings of the 40th An-
niversary Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, July.
Atefeh Farzindar. 2004. D e?veloppement d?un
syste`me de r e?sum e? automatique de textes ju-
ridiques. In TALN-RECITAL?2004, pages 39?44,
Fe`s, Maroc, 19-22 April.
Claire Grover, Ben Hachey, and Chris Korycinski.
2003. Summarising legal texts: Sentential tense
and argumentative roles. In Dragomir Radev and
Simone Teufel, editors, HLT-NAACL 2003 Work-
shop: Text Summarization (DUC03), pages 33?
40, Edmonton, Alberta, Canada, May 31 - June
1.
Marti A. Hearst. 1994. Multi-paragraph segmenta-
tion of expository text. In the 32nd Meeting of the
Association for Computational Linguistics, Los
Cruces, NM, June.
Mark Hepple. 2000. Independence and commit-
ment: Assumptions for rapid training and execu-
tion of rule-based part-of-speech taggers. In the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2000), pages 278?
285, October.
Louise Mailhot. 1998. Decisions, Decisions: a
handbook for judicial writing. Editions Yvon
Blais, Qu e?bec, Canada.
Inderjeet Mani. 2001. Automatic Text Summariza-
tion. John Benjamins Publishing Company.
Marie-Francine Moens, C. Uyttendaele, and J. Du-
mortier. 1999. Abstracting of legal cases: the
potential of clustering based on the selection of
representative objects. Journal of the American
Society for Information Science, 50(2):151?161.
Chris D. Paice. 1981. The automatic generation of
literary abstracts: An approach based on identi-
fication of self-indicating phrases. In O. R. Nor-
man, S. E. Robertson, C. J. van Rijsbergen, and
P. W. Williams, editors, Information Retrieval
Research, London: Butterworth.
Horacio Saggion and Guy Lapalme. 2002. Gener-
ating indicative-informative summaries with su-
mum. Computational Linguistics, 28(4).
J. C. Smith and Cal Deedman. 1987. The applica-
tion of expert systems technology to case-based
law. ICAIL, pages 84?93.
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles - experiments with rele-
vance and rhetorical status. Computational Lin-
guistics, 28(4):409?445.
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 80?89,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Translating Government Agencies? Tweet Feeds:
Specificities, Problems and (a few) Solutions
Fabrizio Gotti, Philippe Langlais
{gottif,felipe}@iro.umontreal.ca
RALI-DIRO
Universite? de Montre?al
C.P. 6128, Succ Centre-Ville
Montre?al (Que?bec) Canada
H3C 3J7
Atefeh Farzindar
farzindar@nlptechnologies.ca
NLP Technologies Inc.
52 Le Royer
Montre?al
(Que?bec) Canada
H2Y 1W7
Abstract
While the automatic translation of tweets has
already been investigated in different scenar-
ios, we are not aware of any attempt to trans-
late tweets created by government agencies.
In this study, we report the experimental re-
sults we obtained when translating 12 Twitter
feeds published by agencies and organizations
of the government of Canada, using a state-of-
the art Statistical Machine Translation (SMT)
engine as a black box translation device. We
mine parallel web pages linked from the URLs
contained in English-French pairs of tweets in
order to create tuning and training material.
For a Twitter feed that would have been other-
wise difficult to translate, we report significant
gains in translation quality using this strategy.
Furthermore, we give a detailed account of the
problems we still face, such as hashtag trans-
lation as well as the generation of tweets of
legal length.
1 Introduction
Twitter is currently one of the most popular online
social networking service after Facebook, and is the
fastest-growing, with the half-a-billion user mark
reached in June 2012.1 According to Twitter?s blog,
no less than 65 millions of tweets are published each
day, mostly in a single language (40% in English).
This hinders the spread of information, a situation
witnessed for instance during the Arab Spring.
1http://semiocast.com/publications/
2012_07_30_Twitter_reaches_half_a_billion_
accounts_140m_in_the_US
Solutions for disseminating tweets in different
languages have been designed. One solution con-
sists in manually translating tweets, which of course
is only viable for a very specific subset of the ma-
terial appearing on Twitter. For instance, the non-
profit organization Meedan2 has been founded in or-
der to organize volunteers willing to translate tweets
written in Arabic on Middle East issues. Another
solution consists in using machine translation. Sev-
eral portals are facilitating this,3 mainly by using
Google?s machine translation API.
Curiously enough, few studies have focused on
the automatic translation of text produced within so-
cial networks, even though a growing number of
these studies concentrate on the automated process-
ing of messages exchanged on social networks. See
(Gimpel et al, 2011) for a recent review of some of
them.
Some effort has been invested in translating short
text messages (SMSs). Notably, Munro (2010) de-
scribes the service deployed by a consortium of vol-
unteer organizations named ?Mission 4636? during
the earthquake that struck Haiti in January 2010.
This service routed SMSs alerts reporting trapped
people and other emergencies to a set of volunteers
who translated Haitian Creole SMSs into English,
so that primary emergency responders could under-
stand them. In Lewis (2010), the authors describe
how the Microsoft translation team developed a sta-
tistical translation engine (Haitian Creole into En-
glish) in as little as 5 days, during the same tragedy.
2http://news.meedan.net/
3http://www.aboutonlinetips.com/
twitter-translation-tools/
80
Jehl (2010) addresses the task of translating En-
glish tweets into German. She concludes that the
proper treatment of unknown words is of the utmost
importance and highlights the problem of producing
translations of up to 140 characters, the upper limit
on tweet lengths. In (Jehl et al, 2012), the authors
describe their efforts to collect bilingual tweets from
a stream of tweets acquired programmatically, and
show the impact of such a collection on developing
an Arabic-to-English translation system.
The present study participates in the effort for the
dissemination of messages exchanged over Twitter
in different languages, but with a very narrow focus,
which we believe has not been addressed specifically
yet: Translating tweets written by government in-
stitutions. What sets these messages apart is that,
generally speaking, they are written in a proper lan-
guage (without which their credibility would pre-
sumably be hurt), while still having to be extremely
brief to abide by the ever-present limit of 140 char-
acters. This contrasts with typical social media texts
in which a large variability in quality is observed
(Agichtein et al, 2008).
Tweets from government institutions can also dif-
fer somewhat from some other, more informal so-
cial media texts in their intended audience and ob-
jectives. Specifically, such tweet feeds often attempt
to serve as a credible source of timely information
presented in a way that engages members of the
lay public. As such, translations should present a
similar degree of credibility, ease of understanding,
and ability to engage the audience as in the source
tweet?all while conforming to the 140 character
limits.
This study attempts to take these matters into ac-
count for the task of translating Twitter feeds emitted
by Canadian governmental institutions. This could
prove very useful, since more than 150 Canadian
agencies have official feeds. Moreover, while only
counting 34 million inhabitants, Canada ranks fifth
in the number of Twitter users (3% of all users) after
the US, the UK, Australia, and Brazil.4 This cer-
tainly explains why Canadian governments, politi-
cians and institutions are making an increasing use
of this social network service. Given the need of
4http://www.techvibes.com/blog/how-
canada-stacks-up-against-the-world-on-
twitter-2012-10-17
Canadian governmental institutions to disseminate
information in both official languages (French and
English), we see a great potential value in targeted
computer-aided translation tools, which could offer
a significant reduction over the current time and ef-
fort required to manually translate tweets.
We show that a state-of-the-art SMT toolkit, used
off-the-shelf, and trained on out-domain data is un-
surprisingly not up to the task. We report in Sec-
tion 2 our efforts in mining bilingual material from
the Internet, which proves eventually useful in sig-
nificantly improving the performance of the engine.
We test the impact of simple adaptation scenarios
in Section 3 and show the significant improvements
in BLEU scores obtained thanks to the corpora we
mined. In Section 4, we provide a detailed account
of the problems that remain to be solved, including
the translation of hashtags (#-words) omnipresent
in tweets and the generation of translations of legal
lengths. We conclude this work-in-progress and dis-
cuss further research avenues in Section 5.
2 Corpora
2.1 Bilingual Twitter Feeds
An exhaustive list of Twitter feeds published by
Canadian government agencies and organizations
can be found on the GOV.PoliTWiTTER.ca web
site.5 As of this writing, 152 tweet feeds are listed,
most of which are available in both French and En-
glish, in keeping with the Official Languages Act
of Canada. We manually selected 20 of these feed
pairs, using various exploratory criteria, such as
their respective government agency, the topics being
addressed and, importantly, the perceived degree of
parallelism between the corresponding French and
English feeds.
All the tweets of these 20 feed pairs were gathered
using Twitter?s Streaming API on 26 March 2013.
We filtered out the tweets that were marked by the
API as retweets and replies, because they rarely have
an official translation. Each pair of filtered feeds
was then aligned at the tweet level in order to cre-
ate bilingual tweet pairs. This step was facilitated
by the fact that timestamps are assigned to each
tweet. Since a tweet and its translation are typi-
5http://gov.politwitter.ca/directory/
network/twitter
81
Tweets URLs mis. probs sents
. HealthCanada
1489 995 1 252 78,847
. DFAIT MAECI ? Foreign Affairs and Int?l Trade
1433 65 0 1081 10,428
. canadabusiness
1265 623 1 363 138,887
. pmharper ? Prime Minister Harper
752 114 2 364 12,883
. TCS SDC ? Canadian Trade Commissioner Service
694 358 1 127 36,785
. Canada Trade
601 238 1 92 22,594
. PHAC GC ? Public Health Canada
555 140 0 216 14,617
. cida ca ? Canadian Int?l Development Agency
546 209 2 121 18,343
. LibraryArchives
490 92 1 171 6,946
. CanBorder ? Canadian Border matters
333 88 0 40 9,329
. Get Prepared ? Emergency preparedness
314 62 0 11 10,092
. Safety Canada
286 60 1 17 3,182
Table 1: Main characteristics of the Twitter and URL cor-
pora for the 12 feed pairs we considered. The (English)
feed name is underlined, and stands for the pair of feeds
that are a translation of one another. When not obvious,
a short description is provided. Each feed name can be
found as is on Twitter. See Sections 2.1 and 2.3 for more.
cally issued at about the same time, we were able to
align the tweets using a dynamic programming al-
gorithm miminizing the total time drift between the
English and the French feeds. Finally, we tokenized
the tweets using an adapted version of Twokenize
(O?Connor et al, 2010), accounting for the hashtags,
usernames and urls contained in tweets.
We eventually had to narrow down further the
number of feed pairs of interest to the 12 most pro-
lific ones. For instance, the feed pair PassportCan6
that we initially considered contained only 54 pairs
of English-French tweets after filtering and align-
ment, and was discarded because too scarce.
6https://twitter.com/PassportCan
Did you know it?s best to test for #radon in
the fall/winter? http://t.co/CDubjbpS
#health #safety
L?automne/l?hiver est le meilleur moment pour
tester le taux de radon.
http://t.co/4NJWJmuN #sante? #se?curite
Figure 1: Example of a pair of tweets extracted from the
feed pair HealthCanada .
The main characteristics of the 12 feed pairs we
ultimately retained are reported in Table 1, for a to-
tal of 8758 tweet pairs. The largest feed, in terms
of the number of tweet pairs used, is that of Health
Canada7 with over 1 489 pairs of retained tweets
pairs at the time of acquisition. For reference, that
is 62% of the 2 395 ?raw? tweets available on the
English feed, before filtering and alignment. An ex-
ample of a retained pair of tweets is shown in Fig-
ure 1. In this example, both tweets contain a short-
ened url alias that (when expanded) leads to web-
pages that are parallel. Both tweets also contain so-
called hashtags (#-words): 2 of those are correctly
translated when going from English to French, but
the hashtag #radon is not translated into a hashtag in
French, instead appearing as the plain word radon,
for unknown reasons.
2.2 Out-of-domain Corpora: Parliament
Debates
We made use of two different large corpora in or-
der to train our baseline SMT engines. We used the
2M sentence pairs of the Europarl version 7 corpus.8
This is a priori an out-of-domain corpus, and we did
not expect much of the SMT system trained on this
dataset. Still, it is one of the most popular parallel
corpus available to the community and serves as a
reference.
We also made use of 2M pairs of sentences we
extracted from an in-house version of the Canadian
Hansard corpus. This material is not completely out-
of-domain, since the matters addressed within the
Canadian Parliament debates likely coincide to some
degree with those tweeted by Canadian institutions.
The main characteristics of these two corpora are re-
ported in Table 2. It is noteworthy that while both
7https://twitter.com/HealthCanada
8http://www.statmt.org/europarl/
82
Corpus sents tokens types s length
hansard en 2M 27.1M 62.2K 13.6
hansard fr 2M 30.7M 82.2K 15.4
europarl en 2M 55.9M 94.5K 28.0
europarl fr 2M 61.6M 129.6K 30.8
Table 2: Number of sentence pairs, token and token types
in the out-of-domain training corpora we used. s length
stands for the average sentence length, counted in tokens.
corpora contain an equal number of sentence pairs,
the average sentence length in the Europarl corpus is
much higher, leading to a much larger set of tokens.
2.3 In-domain Corpus: URL Corpus
As illustrated in Figure 1, many tweets act as
?teasers?, and link to web pages containing (much)
more information on the topic the tweet feed typi-
cally addresses. Therefore, a natural way of adapt-
ing a corpus-driven translation engine consists in
mining the parallel text available at those urls.
In our case, we set aside the last 200 tweet pairs of
each feed as a test corpus. The rest serves as the url-
mining corpus. This is necessary to avoid testing our
system on test tweets whose URLs have contributed
to the training corpus.
Although simple in principle, this data collection
operation consists in numerous steps, outlined be-
low:
1. Split each feed pair in two: The last 200 tweet
pairs are set aside for testing purposes, the rest
serves as the url-mining corpus used in the fol-
lowing steps.
2. Isolate urls in a given tweet pair using our to-
kenizer, adapted to handle Twitter text (includ-
ing urls).
3. Expand shortened urls. For instance, the url
in the English example of Figure 1 would
be expanded into http://www.hc-sc.
gc.ca/ewh-semt/radiation/radon/
testing-analyse-eng.php, using the
expansion service located at the domain t.co.
There are 330 such services on the Web.
4. Download the linked documents.
5. Extract all text from the web pages, without tar-
geting any content in particular (the site menus,
breadcrumb, and other elements are therefore
retained).
6. Segment the text into sentences, and tokenize
them into words.
7. Align sentences with our in-house aligner.
We implemented a number of restrictions during
this process. We did not try to match urls in cases
where the number of urls in each tweet differed (see
column mis.?mismatches?in Table 1). The col-
umn probs. (problems) in Table 1 shows the count of
url pairs whose content could not be extracted. This
happened when we encountered urls that we could
not expand, as well as those returning a 404 HTTP
error code. We also rejected urls that were identi-
cal in both tweets, because they obviously could not
be translations. We also filtered out documents that
were not in html format, and we removed document
pairs where at least one document was difficult to
convert into text (e.g. because of empty content, or
problematic character encoding). After inspection,
we also decided to discard sentences that counted
less than 10 words, because shorter sentences are
too often irrelevant website elements (menu items,
breadcrumbs, copyright notices, etc.).
This 4-hour long operation (including download)
yielded a number of useful web documents and ex-
tracted sentence pairs reported in Table 1 (columns
URLs and sents respectively). We observed that the
density of url pairs present in pairs of tweets varies
among feeds. Still, for all feeds, we were able to
gather a set of (presumably) parallel sentence pairs.
The validity of our extraction process rests on the
hypothesis that the documents mentioned in each
pair of urls are parallel. In order to verify this, we
manually evaluated (a posteriori) the parallelness of
a random sample of 50 sentence pairs extracted for
each feed. Quite fortunately, the extracted material
was of excellent quality, with most samples contain-
ing all perfectly aligned sentences. Only canadabusi-
ness, LibraryArchives and CanBorder counted a sin-
gle mistranslated pair. Clearly, the websites of the
Canadian institutions we mined are translated with
great care and the tweets referring to them are metic-
ulously translated in terms of content links.
83
3 Experiments
3.1 Methodology
All our translation experiments were conducted with
Moses? EMS toolkit (Koehn et al, 2007), which in
turn uses gizapp (Och and Ney, 2003) and SRILM
(Stolcke, 2002).
As a test bed, we used the 200 bilingual tweets
we acquired that were not used to follow urls, as de-
scribed in Sections 2.1 and 2.3. We kept each feed
separate in order to measure the performance of our
system on each of them. Therefore we have 12 test
sets.
We tested two configurations: one in which an
out-of-domain translation system is applied (with-
out adaptation) to the translation of the tweets of
our test material, another one where we allowed the
system to look at in-domain data, either at training
or at tuning time. The in-domain material we used
for adapting our systems is the URL corpus we de-
scribed in section 2.3. More precisely, we prepared
12 tuning corpora, one for each feed, each contain-
ing 800 heldout sentence pairs. The same number of
sentence pairs was considered for out-domain tuning
sets, in order not to bias the results in favor of larger
sets. For adaptation experiments conducted at train-
ing time, all the URL material extracted from a spe-
cific feed (except for the sentences of the tuning sets)
was used. The language model used in our experi-
ments was a 5-gram language model with Kneser-
Ney smoothing.
It must be emphasized that there is no tweet mate-
rial in our training or tuning sets. One reason for this
is that we did not have enough tweets to populate our
training corpus. Also, this corresponds to a realistic
scenario where we want to translate a Twitter feed
without first collecting tweets from this feed.
We use the BLEU metric (Papineni et al, 2002)
as well as word-error rate (WER) to measure trans-
lation quality. A good translation system maximizes
BLEU and minimizes WER. Due to initially poor
results, we had to refine the tokenizer mentioned
in Section 2.1 in order to replace urls with serial-
ized placeholders, since those numerous entities typ-
ically require rule-based translations. The BLEU
and WER scores we report henceforth were com-
puted on such lowercased, tokenized and serialized
texts, and did not incur penalties that would have
train tune canadabusiness DFAIT MAECI
fr?en wer bleu wer bleu
hans hans 59.58 21.16 61.79 19.55
hans in 58.70 21.35 60.73 20.14
euro euro 64.24 15.88 62.90 17.80
euro in 63.23 17.48 60.58 21.23
en?fr wer bleu wer bleu
hans hans 62.42 21.71 64.61 21.43
hans in 61.97 22.92 62.69 22.00
euro euro 64.66 19.52 63.91 21.65
euro in 64.61 18.84 63.56 22.31
Table 3: Performance of generic systems versus systems
adapted at tuning time for two particular feeds. The tune
corpus ?in? stands for the URL corpus specific to the feed
being translated. The tune corpora ?hans? and ?euro? are
considered out-of-domain for the purpose of this experi-
ment.
otherwise been caused by the non-translation of urls
(unknown tokens), for instance.
3.2 Translation Results
Table 3 reports the results observed for the two main
configurations we tested, in both translation direc-
tions. We show results only for two feeds here:
canadabusiness, for which we collected the largest
number of sentence pairs in the URL corpus, and
DFAIT MAECI for which we collected very little
material. For canadabusiness, the performance of the
system trained on Hansard data is higher than that
of the system trained on Europarl (? ranging from
2.19 to 5.28 points of BLEU depending on the con-
figuration considered). For DFAIT MAECI , supris-
ingly, Europarl gives a better result, but by a more
narrow margin (? ranging from 0.19 to 1.75 points
of BLEU). Both tweet feeds are translated with
comparable performance by SMT, both in terms
of BLEU and WER. When comparing BLEU per-
formances based solely on the tuning corpus used,
the in-domain tuning corpus created by mining urls
yields better results than the out-domain tuning cor-
pus seven times out of eight for the results shown in
Table 3.
The complete results are shown in Figure 2, show-
ing BLEU scores obtained for the 12 feeds we con-
sidered, when translating from English to French.
Here, the impact of using in-domain data to tune
84
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
in out in out in out in out in out in out in out in out in out in out in out in out
Canada_Trade canadabusiness CanBorder cida_ca DFAIT_MAECI Get_Prepared HealthCanada LibraryArchives PHAC_GC pmharper Safety_Canada TCS_SDC
B
LE
U
 s
co
re
 
euro hans
train
corpus tune
Moyenne de bleu
direction
Figure 2: BLEU scores measured on the 12 feed pairs we considered for the English-to-French translation direction.
For each tweet test corpus, there are 4 results: a dark histogram bar refers to the Hansard training corpus, while a
lighter grey bar refers to an experiment where the training corpus was Europarl. The ?in? category on the x-axis
designates an experiment where the tuning corpus was in-domain (URL corpus), while the ?out? category refers to an
out-of-domain tuning set. The out-of-domain tuning corpus is Europarl or Hansard, and always matches the nature of
training corpora.
the system is hardly discernible, which in a sense
is good news, since tuning a system for each feed
is not practical. The Hansard corpus almost always
gives better results, in keeping with its status as a
corpus that is not so out-of-domain as Europarl, as
mentioned above. The results for the reverse trans-
lation direction show the same trends.
In order to try a different strategy than using only
tuning corpora to adapt the system, we also investi-
gated the impact of training the system on a mix of
out-of-domain and in-domain data. We ran one of
the simplest adaptation scenarios where we concate-
nated the in-domain material (train part of the URL
corpus) to the out-domain one (Hansard corpus) for
the two feeds we considered in Table 3. The results
are reported in Table 4.
We measured significant gains both in WER and
BLEU scores in conducting training time versus tun-
ing time adaptation, for the canadabusiness feed (the
largest URL corpus). For this corpus, we observe
an interesting gain of more than 6 absolute points in
BLEU scores. However, for the DFAIT MAECI (the
smallest URL corpus) we note a very modest loss in
translation quality when translating from French and
a significant gain in the other translation direction.
These figures could show that mining parallel sen-
tences present in URLs is a fruitful strategy for adapt-
ing the translation engine for feeds like canadabusi-
ness that display poor performance otherwise, with-
out harming the translation quality for feeds that per-
Train corpus WER BLEU
fr?en
hans+canbusiness 53.46 (-5.24) 27.60 (+6.25)
hans+DFAIT 60.81 (+0.23) 20.83 (-0.40)
en?fr
hans+canbusiness 57.07 (-4.90) 26.26 (+3.34)
hans+DFAIT 61.80 (-0.89) 24.93 (+2.62)
Table 4: Performance of systems trained on a concatena-
tion of out-of-domain and in-domain data. All systems
were tuned on in-domain data. Absolute gains are shown
in parentheses, over the best performance achieved so far
(see Table 3).
form reasonably well without additional resources.
Unfortunately, it suggests that retraining a system is
required for better performance, which might hinder
the deployment of a standalone translation engine.
Further research needs to be carried out to determine
how many tweet pairs must be used in a parallel URL
corpus in order to get a sufficiently good in-domain
corpus.
4 Analysis
4.1 Translation output
Examples of translations produced by the best sys-
tem we trained are reported in Figure 3. The first
translation shows a case of an unknown French word
(soumissionnez). The second example illustrates
85
a typical example where the hashtags should have
been translated but were left unchanged. The third
example shows a correct translation, except that the
length of the translation (once the text is detok-
enized) is over the size limit allowed for a tweet.
Those problems are further analyzed in the remain-
ing subsections.
4.2 Unknown words
Unknown words negatively impact the quality of
MT output in several ways. First, they typically ap-
pear untranslated in the system?s output (we deemed
most appropriate this last resort strategy). Sec-
ondly, they perturb the language model, which often
causes other problems (such as dubious word order-
ing). Table 5 reports the main characteristics of the
words from all the tweets we collected that were not
present in the Hansard train corpus.
The out-of-vocabulary rate with respect to token
types hovers around 33% for both languages. No
less than 42% (resp. 37%) of the unknown English
(resp. French) token types are actually hashtags. We
defer their analysis to the next section. Also, 15%
(resp. 10%) of unknown English token types are
user names (@user), which do not require transla-
tion.
English French
tweet tokens 153 234 173 921
tweet types 13 921 15 714
OOV types 4 875 (35.0%) 5 116 (32.6%)
. hashtag types 2 049 (42.0%) 1 909 (37.3%)
. @user types 756 (15.5%) 521 (10.2%)
Table 5: Statistics on out-of-vocabulary token types.
We manually analyzed 100 unknown token types
that were not hashtags or usernames and that did not
contain any digit. We classified them into a num-
ber of broad classes whose distributions are reported
in Table 6 for the French unknown types. A simi-
lar distribution was observed for English unknown
types. While we could not decide of the nature of
21 types without their context of use (line ?type),
we frequently observed English types, as well as
acronyms and proper names. A few unknown types
result from typos, while many are indeed true French
types unseen at training time (row labeled french ),
some of which being very specific (term). Amus-
ingly, the French verbal neologism twitter (to tweet)
is unknown to the Hansard corpus we used.
french 26 sautez, perforateurs , twitter
english 22 successful , beauty
?types 21 bumbo , tra
name 11 absorbica , konzonguizi
acronym 7 hna , rnc
typo 6 gazouilli , pendan
term 3 apostasie , sibutramine
foreign 2 aanischaaukamikw, aliskiren
others 2 francophonesURL
Table 6: Distribution of 100 unknown French token types
(excluding hashtags and usernames).
4.3 Dealing with Hashtags
We have already seen that translating the text in
hashtags is often suitable, but not always. Typically,
hashtags in the middle of a sentence are to be trans-
lated, while those at the end typically should not be.
A model should be designed for learning when to
translate an hashtag or not. Also, some hashtags are
part of the sentence, while others are just (semantic)
tags. While a simple strategy for translating hash-
tags consists in removing the # sign at translation
time, then restoring it afterwards, this strategy would
fail in a number of cases that require segmenting the
text of the hashtag first. Table 7 reports the per-
centage of hashtags that should be segmented before
being translated, according to a manual analysis we
conducted over 1000 hashtags in both languages we
considered. While many hashtags are single words,
roughly 20% of them are not and require segmenta-
tion.
4.4 Translating under size constraints
The 140 character limit Twitter imposes on tweets is
well known and demands a certain degree of conci-
sion even human users find sometimes bothersome.
For machine output, this limit becomes a challeng-
ing problem. While there exists plain?but inelegant?
workarounds9, there may be a way to produce tweet
translations that are themselves Twitter-ready. (Jehl,
9The service eztweets.com splits long tweets into smaller
ones; twitlonger.com tweets the beginning of a long message,
86
SRC: vous soumissionnez pour obtenir de gros contrats ? voici 5 pratiques exemplaires a` suivre . URL
TRA: you soumissionnez big contracts for best practices ? here is 5 URL to follow .
REF: bidding on big contracts ? here are 5 best practices to follow . URL
SRC: avis de #sante?publique : maladies associe?es aux #salmonelles et a` la nourriture pour animaux de com-
pagnie URL #rappel
TRA: notice of #sante?publique : disease associated with the #salmonelles and pet food #rappel URL
REF: #publichealth notice : illnesses related to #salmonella and #petfood URL #recall
SRC: des ha??tiens de tous les a?ges , milieux et me?tiers te?moignent de l? aide qu? ils ont rec?ue depuis le se?isme
. URL #ha??ti
TRA: the haitian people of all ages and backgrounds and trades testify to the assistance that they have received
from the earthquake #ha??ti URL .
REF: #canada in #haiti : haitians of all ages , backgrounds , and occupations tell of the help they received .
URL
Figure 3: Examples of translations produced by an engine trained on a mix of in- and out-of-domain data.
w. en fr example
1 76.5 79.9 intelligence
2 18.3 11.9 gender equality
3 4.0 6.0 africa trade mission
4 1.0 1.4 closer than you think
5 0.2 0.6 i am making a difference
6 ? 0.2 fonds aide victime se?cheresse
afrique est
Table 7: Percentage of hashtags that require segmentation
prior to translation. w. stands for the number of words
into which the hashtag text should be segmented.
2010) pointed out this problem and reported that
3.4% of tweets produced were overlong, when trans-
lating from German to English. The reverse direc-
tions produced 17.2% of overlong German tweets.
To remedy this, she tried modifying the way BLEU
is computed to penalize long translation during the
tuning process, with BLEU scores worse than sim-
ply truncating the illegal tweets. The second strategy
the author tried consisted in generating n-best lists
and mining them to find legal tweets, with encour-
aging results (for n = 30 000), since the number
of overlong tweets was significantly reduced while
leaving BLEU scores unharmed.
In order to assess the importance of the problem
for our system, we measured the lengths of tweets
that a system trained like hans+canbusiness in Ta-
ble 4 (a mix of in- and out-of-domain data) could
produce. This time however, we used a larger test set
and provides a link to read the remainder. One could also simply
truncate an illegal tweet and hope for the best...
counting 498 tweets. To measure the lengths of their
translations, we first had to detokenize the transla-
tions produced, since the limitation applies to ?nat-
ural? text only. For each URL serialized token, we
counted 18 characters, the average length of a (short-
ened) url in a tweet. When translating from French
to English, the 498 translations had lengths ranging
from 45 to 138 characters; hence, they were all legal
tweets. From English to French, however, the trans-
lations are longer, and range from 32 characters to
223 characters, with 22.5% of them overlong.
One must recall that in our experiments, no tweets
were seen at training or tuning time, which explains
why the rate of translations that do not meet the
limit is high. This problem deserves a specific treat-
ment for a system to be deployed. One interest-
ing solution already described by (Jehl, 2010) is to
mine the n-best list produced by the decoder in or-
der to find the first candidate that constitutes a legal
tweet. This candidate is then picked as the trans-
lation. We performed this analysis on the canad-
abusiness output described earlier, from English to
French. We used n =1, 5, 10, 20, 50, 100, 200, 500,
1000, 5000, 10000, 30000 and computed the result-
ing BLEU scores and remaining percentage of over-
long tweets. The results are shown in Figure 4. The
results clearly show that the n-best list does contain
alternate candidates when the best one is too long.
Indeed, not only do we observe that the percentage
of remaining illegal tweets can fall steadily (from
22.4% to 6.6% for n = 30 000) as we dig deeper into
the list, but also the BLEU score stays unharmed,
showing even a slight improvement, from an ini-
87
tial 26.16 to 26.31 for n = 30 000. This counter-
intuitive result in terms of BLEU is also reported
in (Jehl, 2010) and is probably due to a less harsh
brevity penalty by BLEU on shorter candidates.
2013-03-01
nbest list
n wer ser BLEU
1 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.159.35 97.79 0.26 6
5 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.559.3 97.79 0.2622
10 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1059.31 97.79 0.2623
20 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.2059.31 97.79 0.26 2
50 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.5059.34 97.79 0.2622
100 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.10059.27 97.79 0.2628
200 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.20059.23 97.79 0.2633
500 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.50059.24 97.79 0 263
1000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.100059.21 97.79 0.2634
5000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.500059.26 97.79 0.2635
10000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1000059.26 97.79 0.2638
30000 /u/gottif/p j/nlptech2012/data/tweets/correction/nbest/trans.3000059.32 97.79 0.26 1
0%
5%
10%
15%
20%
25%
0.261
0.262
0.262
0.263
0.263
0.264
0.264
1 10 100 1000 10000 100000
%
 o
f o
ve
rlo
ng
 tw
ee
ts
 
BL
EU
 sc
or
e 
n-best list length (n) 
BLEU % overlong
Figure 4: BLEU scores and percentage of overlong
tweets when mining the n-best list for legal tweets, when
the first candidate is overlong. The BLEU scores (dia-
mond series) should be read off the left-hand vertical axis,
while the remaining percentage of illegal tweets (circle
series) should be read off the right-hand axis.
5 Discussion
We presented a number of experiments where we
translated tweets produced by Canadian govern-
ments institutions and organizations. Those tweets
have the distinguishing characteristic (in the Twitter-
sphere) of being written in proper English or French.
We show that mining the urls mentioned in those
tweets for parallel sentences can be a fruitful strat-
egy for adapting an out-of-domain translation engine
to this task, although further research could show
other ways of using this resource, whose quality
seems to be high according to our manual evalua-
tion. We also analyzed the main problems that re-
main to be addressed before deploying a useful sys-
tem.
While we focused here on acquiring useful cor-
pora for adapting a translation engine, we admit that
the adaptation scenario we considered is very sim-
plistic, although efficient. We are currently inves-
tigating the merit of different methods to adaptation
(Zhao et al, 2004; Foster et al, 2010; Daume III and
Jagarlamudi, 2011; Razmara et al, 2012; Sankaran
et al, 2012).
Unknown words are of concern, and should be
dealt with appropriately. The serialization of urls
was natural, but it could be extended to usernames.
The latter do not need to be translated, but reduc-
ing the vocabulary is always desirable when work-
ing with a statistical machine translation engine.
One interesting subcategories of out-of-vocabulary
tokens are hashtags. According to our analysis,
they require segmentation into words before being
translated in 20% of the cases. Even if they are
transformed into regular words (#radon?radon or
#genderequality?gender equality), however, it is
not clear at this point how to detect if they are used
like normally-occurring words in a sentence, as in
(#radon is harmful) or if they are simply tags added
to the tweet to categorize it.
We also showed that translating under size con-
straints can be handled easily by mining the n-best
list produced by the decoder, but only up to a point.
A remaining 6% of the tweets we analyzed in detail
could not find a shorter version. Numerous ideas
are possible to alleviate the problem. One could for
instance modify the logic of the decoder to penal-
ize hypotheses that promise to yield overlong trans-
lations. Another idea would be to manually in-
spect the strategies used by governmental agencies
on Twitter when attempting to shorten their mes-
sages, and to select those that seem acceptable and
implementable, like the suppression of articles or the
use of authorized abbreviations.
Adapting a translation pipeline to the very specific
world of governmental tweets therefore poses mul-
tiple challenges, each of which can be addressed in
numerous ways. We have reported here the results of
a modest but fertile subset of these adaptation strate-
gies.
Acknowledgments
This work was funded by a grant from the Natu-
ral Sciences and Engineering Research Council of
Canada. We also wish to thank Houssem Eddine
Dridi for his help with the Twitter API.
88
References
Eugene Agichtein, Carlos Castillo, Debora Donato, Aris-
tides Gionis, and Gilad Mishne. 2008. Finding high-
quality content in social media. In Proceedings of
the 2008 International Conference on Web Search and
Data Mining, WSDM ?08, pages 183?194.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In 49th ACL, pages 407?412, Portland,
Oregon, USA, June.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP,
pages 451?459, Cambridge, MA, October.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL
(Short Papers), pages 42?47.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012. Twit-
ter translation using translation-based cross-lingual re-
trieval. In 7th Workshop on Statistical Machine Trans-
lation, pages 410?421, Montre?al, June.
Laura Jehl. 2010. Machine translation for twitter. Mas-
ter?s thesis, School of Philosophie, Psychology and
Language Studies, University of Edinburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondr?ej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. pages 177?
180.
William D. Lewis. 2010. Haitian creole: How to build
and ship an mt engine from scratch in 4 days, 17 hours,
& 30 minutes. In EAMT, Saint-Raphael.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation, Denver.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory Search and Topic
Summarization for Twitter. In William W. Cohen,
Samuel Gosling, William W. Cohen, and Samuel
Gosling, editors, ICWSM. The AAAI Press.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei J.
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th ACL, Jeju, Republic of Korea, jul.
Baskaran Sankaran, Majid Razmara, Atefeh Farzindar,
Wael Khreich, Fred Popowich, and Anoop Sarkar.
2012. Domain adaptation techniques for machine
translation and their evaluation in a real-world setting.
In Proceedings of 25th Canadian Conference on Arti-
ficial Intelligence, Toronto, Canada, may.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In 20th
COLING.
89
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 102?110,
Coling 2014, Dublin, Ireland, August 24 2014.
Collaboratively Constructed Linguistic Resources for Language Vari-ants and their Exploitation in NLP Applications ? the case of Tunisian Arabic and the Social Media 
  Fatiha Sadat University of Quebec in Mon-treal 201 President Kennedy, Mon-treal, QC, Canada sadat.fatiha@uqam.ca 
Fatma Mallek  University of Quebec in Mon-treal 201 President Kennedy, Mon-treal, QC, Canada mallek.fatma@uqam.ca 
Rahma Sellami Sfax University, Sfax, Tunisia rah-ma.sellami@fsegs.rnu.tn  Mohamed Mahdi Boudabous Sfax University, Sfax, Tunisia mehdiboudabous@gmail.com 
Atefeh Farzindar NLP Technologies Inc. 52 LeRoyer Street W, Montreal, Canada farzindar@nlptechnologies.ca  Abstract Modern Standard Arabic (MSA) is the formal language in most Arabic countries. Ara-bic Dialects (AD) or daily language differs from MSA especially in social media communication. However, most Arabic social media texts have mixed forms and many variations especially between MSA and AD. This paper aims to bridge the gap be-tween MSA and AD by providing a framework for the translation of texts of social media. More precisely, this paper focuses on the Tunisian Dialect of Arabic (TAD) with an application on automatic machine translation for a social media text into MSA and any other target language. Linguistic tools such as a bilingual TAD-MSA lexicon and a set of grammatical mapping rules are collaboratively constructed and exploited in addition to a language model to produce MSA sentences of Tunisian dialectal sen-tences. This work is a first-step towards collaboratively constructed semantic and lexi-cal resources for Arabic Social Media within the ASMAT (Arabic Social Media Anal-ysis Tools) project.  1 Introduction The explosive growth of social media has led to a wide range of new challenges for machine transla-tion and language processing. The language used in social media occupies a new space between struc-tured and unstructured media, formal and informal language, and dialect and standard usage. Yet these new platforms have given a digital voice to millions of user on the Internet, giving them the opportuni-ty to communicate on the first truly global stage ? the Internet (Colbath, 2012). Social media poses three major computational challenges, dubbed by Gartner the 3Vs of big data: volume, velocity, and variety1. Natural Language Processing (NLP) methods, in particular, face further difficulties arising from the short, noisy, and strongly contextualised nature of social media. In order to address the 3Vs of social media, new language technologies have emerged, such as the identification and definition of users' language varieties and the translation to a different language, than the source.                                                 1 http://en.wikipedia.org/wiki/Big_data  This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
102
Furthermore, language in social media is very rich with linguistic innovations, morphology and lexical changes. People are not only socially connected across the world but also emotionally and linguistical-ly (Sadat, 2013).  The importance of social media stems from the fact that the use of social networks has made every-body a potential author, which means that the language is now closer to the user than to any prescribed norms. Thus, considerable interest has recently been focused on the analysis of social media in order to create or enrich NLP tools and applications. There are, however, still many challenges to be faced depending on the used language and its variants.  This paper deal with Arabic language and its variants for the analysis of social media and the col-laborative construction of linguistic tools, such as lexical dictionaries and grammars and their exploita-tion in NLP applications, such as translation technologies.  Basically, Arabic is considered as morphologically rich and complex language, which presents sig-nificant challenges for NLP and its applications. It is the official language in 22 countries spoken by more than 350 million people around the world2. Moreover, Arabic language exists in a state of di-glossia where the standard form of the language, Modern Standard Arabic (MSA) and the regional dialects (AD) live side-by-side and are closely related (Elfardy and Diab, 2013). Arabic has more than 22 variants, refereed a as dialects; some countries share the same dialects, while many dialects may exist alongside MSA within the same Arab country. Arabic Dialects (AD) or daily language differs from MSA especially in social media communication. However, most Arabic social media texts have mixed forms and many variations especially between MSA and AD. This paper describes our efforts to create linguistic resources and translation tool for TDA to MSA. First, a bilingual TDA-MSA lexicon and a set of TDA mapping rules for the social media context are collaboratively constructed. Second, these tools are exploited in addition to a language model extract-ed from MSA corpus, to produce MSA sentences of the Tunisian dialectal sentences of social media. The rule-based translation system can be coupled with a statistical machine translation system from MSA into any language, example French or English to provide a translation from TDA to French or English of original Tunisian dialectal sentences of social media. This paper is organized as follows. In Section 2, we present some related works to this research. Section 3 discusses the Tunisian Dialect of Arabic  (TDA) and its challenges in social media context. In Section 4, we present the collaboratively construct linguistic tools for the social media. Section 5 presents some evaluations of the combined TDA-MSA rule-based translation and disambiguation sys-tem. Section 6 concludes this paper and gives some future extensions. 2 Related Work There have been several works on Arabic NLP. However, most traditional techniques have focused on MSA, since it is understood across a wide spectrum of audience in the Arab world and is widely used in the spoken and written media. Few works relate the processing of dialectal Arabic that is different from processing MSA. First, dialects leverage different subsets of MSA vocabulary, introduce different new vocabulary that are more based on the geographical location and culture, exhibit distinct grammatical rules, and adds new morphologies to the words. The gap between MSA and Arabic dia-lects has affected morphology, word order, and vocabulary (Kirchhoff  and Vergyri, 2004). Almeman and Lee (2013) have shown in their work that only 10% of words (uni-gram) share between MSA and dialects. Second, one of the challenges for Arabic NLP applications is the mixture usage of both AD and MSA within the same text in social media context.  Recently, research groups have started focusing on dialects. For instance, Columbia University provides a morphological analyzer (MAGEAD) for Levantine verbs and assumes the input is non-noisy and purely Levantine (Habash and Rambow, 2006b). Given that DA and MSA do not have much in terms of parallel corpora, rule-based methods to translate DA-to-MSA or other methods to collect word-pair lists have been explored. Abo Bakr et al. (2008) introduced a hybrid approach to translate a sentence from Egyptian Arabic into MSA. This hy-brid system consists of a statistical system for tokenizing and tagging, and a rule-based system for the construction of diacritized MSA sentences. Al-Sabbagh and Girju (2010) described an approach of                                                 2 http://en.wikipedia.org/wiki/Geographic_distribution_of_Arabic#Population 
103
mining the web to build a DA-to-MSA lexicon. Salloum and Habash (2012) developed Elissa, a dia-lectal to standard Arabic tool that employs a rule-based translation approach and relies on morphologi-cal analysis, morphological transfer rules and dictionaries in addition to language models to produce MSA paraphrases of dialectal sentences. Using closely related languages has been shown to improve MT quality when resources are limited. In the context of Arabic dialect translation, Sawaf (2010) built a hybrid MT system that uses both sta-tistical and rule-based approaches for DA-to-English MT. In his approach, DA (but not TDA) is nor-malized into MSA by performing a combination of character- and morpheme-level mappings. They then translated the normalized source to English using a hybrid MT or alternatively a Statistical MT system.  Very few researches were reported on Tunisian variant of Arabic or any other Maghrebi variant. Hamdi et al. (2013) presented a translation system between MSA TDA verbal forms. Their approach relies on modeling the translation process over the deep morphological representations of roots and patterns, commonly used to model Semitic morphology. The reported results are aat 80% recall in the TDA into MSA and 84% recall in the opposite direction. However, the translation process was highly ambiguous, and a contextual disambiguation process was therefore necessary for such a process to be of practical use. Boudjelbene et al. (2013a, 2013b) described a method for building a bilingual diction-ary using explicit knowledge about the relation between TDA and MSA and presented an automatic process for creating Tunisian Dialect corpora. However, their work focused on verbs mainly in order to adapt MAGEAD morphological analyser and generator of arabic dialect to TDA (Hamdi et al., 2013). Also, they developed a tool that generates TDA corpora and enrich semi-automatically the dic-tionaries they built. Experiments in progress showed that the integration of translated data improves lexical coverage and the perplexity of language models significantly. Their research was very pertinent for TDA but did not consider the mixture form of social media corpora.  Shaalan (2010) presented a rule-based approach for Arabic NLP and developed a transfer-based machine translation system of English noun phrase to Arabic. Their research showed that a rapid de-velopment of rule-based systems is feasible, especially in the absence of linguistic resources and the difficulties faced in adapting tools from other languages due to peculiarities an the nature of Arabic language.  In real-life practise, a company named Qordoba3 launched social media translation service for Ara-bic in general. However, no demonstration or freely available version was found online. Furthermore, a new Twitter service automatically translates tweets from some Arabic language variants to English. However, this translation tool is not 100% accurate4. 3 The Tunisian Dialect of Arabic and its Challenges in Social Media Tunisian, or Tunisian Arabic5 (TDA) is a Maghrebi dialect of the Arabic language, spoken by some 11 million people in coastal Tunisia. It is usually known by its own speakers as Derja, which means dia-lect, to distinguish it from Standard Arabic, or as Tunsi, which means Tunisian. In the interior of the country it merges, as part of a dialect continuum, into Algerian Arabic and Libyan Arabic. The morphology, syntax, pronunciation and vocabulary of Tunisian Arabic are quite different from Standard or Classical Arabic. TDA, like other Maghrebi dialects, has a vocabulary mostly Ara-bic, with significant Berber substrates, and many words and loanwords borrowed from Berber, French, Turkish, Italian and Spanish. Derja is mutually spoken and understood in the Maghreb countries, especially Morocco, Algeria and Tunisia, but hard to understand for middle east-ern Arabic speakers. It continues to evolve by integrating new French or English words, notably in technical fields, or by replacing old French and Spanish ones with Standard Arabic words within some circles. Moreover, Tunisian is also closely related to Maltese, which is not considered to be a dialect of Arabic for sociolinguistic reasons.  An exemple is the following sentences in Tunisian Dialect of Arabic (TDA) in social media, as pre-sented in Figure 1. The underlined words (also in red) cannot be analyzable by MSA morphological analyzers, and thus need their own TDA analysis. Moreover, there are some words (in blue) expressed                                                 3 http://www.wamda.com/2013/06/qordoba-launches-new-social-media-translation-service 4 http://www.neurope.eu/article/twitter-launches-arabic-translation-service 5 http://en.wikipedia.org/wiki/Tunisian_Arabic 
104
in languages other than Arabic, French in this case. At least three morphological tools are needed for this short text that is very common in social media. It is often assumed that for the country in question, users of social media will use mostly if not al-ways, the native language. However, this isn?t always the case. Languages will be mixed up with up to three languages in a single ?tweet? or blog. A Tunisian user of social media can involve the following languages or their variants: 1) native tongue (Arabic dialect), 2) MSA (example for greetings), 3) Eng-lish and 4) the Colonial country?s language, which is French in our case. In the case of Tunisian, French words/numbers may be used that sound like an Arabic word. An accurate machine translation for social media should manage this level of complexity, especially when ones add numerical charac-ters and an ever-changing Lexicon of words.  
  Figure 1. Example of Social media text including a mixture of MSA, TDA and French language 4 Collaboratively Constructed Linguistic Tools for TDA  This section describes our effort in collaboratively constructing some linguitic tools that help translate Tunisian text of social media into MSA and other target languages considering MSA as a pivot language. Among these tools, a bilingual TDA-MSA lexicon and a set of mapping rules that will be integrated in the rule-based translation system (TDA-into-MSA). Furthermore, a language Modeling of MSA will help disambiguate the many translation hypothesis and thus select the best phrasal translation in MSA.  
 Figure 2. An example from the TDA-MSA lexicon database 
105
4.1 The TDA-MSA Bilingual Lexicon  We have manually and collaboratively developed a bilingual TDA-MSA lexicon that contains around 1,600 source words in TDA and its corresponding translations in MSA, defined by a human expert. Furthermore, our research on some downloadded extracts from Tunisian blogs (around 6,000 words), showed a difference between verb morphology in TDA and that in MSA. We find that in TDA, the gender distinction is not marked. Similarly, we noticed the absence of the masculine and feminine dual in TDA.   In this phase, our aim was to build a bilingual lexicon of Tunisian nouns and verbs and their translations into MSA. Note that a term can be a noun, a verb, an adverb, etc. Furthermore, the most used imported words from other language than Arabic (Berbere, French, English, Turkish, Spanish, Maltese) and used in social media context were considered in this lexicon.  These TDA-MSA couples are stored in an XML database. Figure 2 shows a bilingual TDA-MSA extract from the lexicon database, encoded in XML. 4.2 Grammatical Mapping Rules for TDA  Our second collaboratively constructed linguistic tool, consists on a set of mapping rules that were checked by human experts. This set consists of some rules applied on verbs transformation in TDA and their corresponding translation into MSA. In final, we have defined a set of 226 mapping rules from TDA into MSA on verbs transformation. Figure 3 shows an extract of the defined rules, encoded in XML. Figure 4 shows an example of a verb in TDA and its translation ito MSA using rule number 171 of the collaboratively built set of mapping rules. 4.3 Automatic Rule-based TDA-MSA Machine Translation  We have developed a rule-based translation system that is able to translate any social media text in TDA into MSA. This rule-based translation system can be coupled with any statistical machine trans-lation system from MSA to another language to provide a translation of original Tunisian dialectal sentences of social media from TDA to that other language.  Figure 5 shows the different steps used in the translation of any social media text from TDA into MSA. First, for each word in the TDA social media text, we proceed by searching in the TDA-MSA lexicon database for the corresponding translation of the TDA word. Mostly, TDA nouns and imported words from other languages than Arabic were included in the lexicon.  Second, we proceed by search-ing in the database of mapping rules for the source verb in TDA and its corresponding MSA transla-tion, as shown in Figure 4. Last, both word-by-word translation candidates are extracted from the lexi-cons and using the set of mapping rules; thus considered as translation hypothesis.    
 Figure 3. An example of some mapping rules from TDA to MSA 
106
  Figure 4. An example of the application of rule 171 for a verb in TDA and its translation into MSA 
4.4 Language Modeling  The rule-based translation system is based on a word-by-word translation using the bilingual lexicon and the set of mapping rules. Thus, most of the time, one TDA sentence will have more than one pos-sible translation. The language modeling (LM) of the target language (MSA) combined to the previous rule-based translation system will help disambiguate and select the best translation hypothesis in MSA.  
 Figure 5. The rule-based translation approach for an automatic mapping from TDA to MSA 5 Evaluations  We have carried out some experiments and evaluations on the accuracy of the translation of TDA so-cial media texts into MSA. First, we collected manually a TDA corpus consisting of 6,000 words from some Tunisian forums and blogs. This corpus is very heterogeneous and multilingual, as many words are not in TDA but in MSA, French, English and sometimes using a certain style and form of social media, example using tweeter or SMS slangs). An extract of this corpus is presented in Figure 1. 
107
For evaluation purposes, we considered a reference set of 50 phrases in TDA, translated manually into MSA. We also considered these 50 TDA phrases as the test set. Thus, we applied the proposed rule-based approach on this test set. In order to combine adequately the rule-based translation approach to the language modeling (in MSA), we considered using the United Nation Arabic corpus to train a trigram language model. This training corpus contains around 50M words after cleaning the Latin content.  A preprocessing step is very crucial to any Arabic language processing. We considered tokenizing the MSA words using the D3 (Habash and Sadat, 2006a) scheme to overcome all problems of aggluti-nation. The D3 scheme splits off clitics as follows: the class of conjunction clitics (w+ and f+), the class of particles (l+, k+, b+ and s+), the definite article (Al+) and all pronominal enclitics. These pre-processing are applied for both the hypothesis translation sentences and the training corpus, both in MSA. In addition to this preprocessing step, manual cleaning the MSA corpus of Latin contents was required. Thus, a trigram language model was implemented using the SRILM toolkit (Stolcke, 2002) on this training MSA corpus.  Next, we extracted all possible trigrams from the preprocessed MSA hypotheses translations and we computed the probability that these trigrams extracted appear in the MSA corpus based on the lan-guage model. A probability for each hypothesis translation is computed based on a trigram language model (LM). The hypothesis translation that has the highest probability is considered as the best trans-lation. Evaluations of the best translation sentence from TDA to MSA against the reference sentence in MSA were completed using the BLEU metric for automatic machine translation (Papineni et al., 2002). Our experiment produced a score of 14.32 BLEU. This low score could be related to our rule-based translation approach that is word-based and to the high number of unknown words in our source test file in other language variants than TDA. Adopting a phrasal translation and solving the problem of unknown words should be more effective. Unfortunately, we could not found an available TDA-MSA test and reference files to conduct better evaluations in machine translation and social media context.    6 Conclusion and Future Work Social media has become a key communication tool for people around the world. Building any NLP tool for texts extracted from social media is very challenging and daunting task and always be limited by the rapid changes in the social media. Considering an Arabic social media text is much more chal-lenging because of the dominant use of English, French and other languages which intend to bring more problems to solve. This paper presents our effort to create linguistic resources such as a bilingual lexicon, a set of grammatical mapping rule and a ruel-based translation and disambiguation system for the translation of any social media text from TDA into MSA. A language modeling of MSA is used in the disambiguation phase and the selection of  the best translation phrase. As for future work, we intend to enlarge the set of words in the TDA-MSA lexicon as well as the set of mapping rules. We intend to develop more grammatical rules for not only verbs but also adjec-tives and nouns. Furthermore, it would be interesting to build a parallel or comparable TDA-MSA corpus by selecting the most pertinent sources of social media and mining the web. A phrase-based statistical machine translation can be built using this parallel/comparable corpus and coupled to the rule-based translation system.  What we presented in this draft is a research on exploiting social media corpora for Arabic in order to analyze them and exploit them for NLP applications, such as machine translation within the scope of the ASMAT project. Reference Hitham Abo-Bakr, Khaled Shaalan, and Ibrahim Ziedan. 2008. A Hybrid Approach for Converting Written Egyptian Colloquial Dialect into Discretized Arabic. Proceedings of the 6th International Conference on In-formatics and Systems 2008. Cairo University. 
108
Rania Al-Sabbagh and Roxana Girju. 2010. Mining the Web for the Induction of a Dialectical Arabic Lexicon. Proceedings of the 7th International Conference on Language Resources and Evaluation LREC 2010. Vallet-ta, Malta, May 19-21, 2010. Khalid Almeman and Mark Lee. 2013. Automatic Building of Arabic Multi Dialect Text Corpora by Bootstrap-ping Dialect Words. In Communications, Signal Processing, and their Applications ICCSPA 2013. Sharjah, UAE, Feb.12-14, 2013. Rahma Boujelbane, Mariem Ellouze Khemekhem, Siwar BenAyed, and Lamia Hadrich Belguith. 2013. Building Bilingual Lexicon to Create Dialect Tunisian Corpora and Adapt Language Model. Proceedings of the 2nd Workshop on Hybrid Approaches to Translation, ACL 2013. Sofia, Bulgaria. Rahma Boujelbane, Mariem Ellouze Khemekhem, and Lamia Hadrich Belguith. 2013. Mapping Rules for Build-ing a Tunisian Dialect Lexicon and Generating Corpora. Proceedings of the International Joint Conference on Natural Language Processing. Nagoya, Japan.   David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic Dialects. Proceedings of the European Chapter of ACL EACL 2006. Sean Colbath. 2012. Language and Translation Challenges in Social Media. Proceedings of AMTA 2012, Gov-ernment presentations. Submitted by Raytheon BBN Technologies. Oct. 28th to Nov. 1st, 2012. San Diego, USA.  Mona Diab and Nizar Habash. 2007. Arabic Dialect Processing Tutorial. Proceedings of HLT-NAACL, Tutorial Abstracts 2007: 5-6. Heba Elfardy and Mona Diab. 2013. Sentence-Level Dialect Identification in Arabic. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, Sofia, Bulgaria. 2013. Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-processing Schemes for Statistical Machine Translation. Pro-ceedings of the Human Language Technology Conference of the NAACL. Companion volume: 49?52. New York City, USA. Nizar Habash and Owen Rambow. 2006. MAGEAD: A Morphological Analyzer and Generator for the Arabic Dialects. Proceedings of the 21st International Conference on Computational Linguistics and 44st Annual Meeting of the Association for Computational Linguistics: 681?688, Sydney, Australia. Ahmed Hamdi, Rahma Boujelbane, Nizar Habash, and Alexis Nasr. 2013. The Effects of Factorizing Root and Pattern Mapping in Bidirectional Tunisian - Standard Arabic Machine Translation. Proceedings of MT Sum-mit 2013, Nice, France. Ahmed Hamdi, Rahma Boujelbane, Nizar Habash, and Alexis Nasr. 2013. Un Syst?me de Traduction de Verbes entre Arabe Standard et Arabe Dialectal par Analyse Morphologique Profonde. Proceedings of TALN 2013, Nantes, France. Hanaa Kilany, Hassan. Gadalla, Howaida Arram, Ashraf Yacoub, Alaa El-Habashi, and Cynthia McLemore. 2002. Egyptian Colloquial Arabic Lexicon. LDC catalog number LDC99L22. Katrin Kirchhoff, Jeff Bilmes, Sourin Das, Nicolae Duta, Melissa Egan, Gang Ji, Feng He, John Henderson, Daben Liu, Mohamed Noamany, Pat Schone, Richard Schwartz, and Dimitra Vergyri. 2003. Novel Ap-proaches to Arabic Speech Recognition: Report from the 2002 Johns Hopkins Summer Workshop. Proceed-ings of IEEE International Conference on Acoustics, Speech, and Signal Processing. Hong Kong, China. Katrin Kirchhoff and Dimitra Vergyri. 2004. Cross-dialectal Acoustic Data Sharing for Arabic Speech Recogni-tion. Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, 2004.  Kishore Papineni, Salim Roukos, Todd Ward, and Wei J. Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting of the Association for Computational Lin-guistics: 311?318. Philadelphia, USA. Fatiha Sadat. 2013. Arabic social media analysis for the construction and the enrichment of NLP tools. In Cor-pus Linguistics 2013. Lancaster University, UK. Jul. 22-26, 2013. Hassan Sajjad, Kareem Darwish and Yonatan Belinkov. 2013. Translating Dialectal Arabic to English. Proceed-ings of the 51st Annual Meeting of the Association for Computational Linguistics: 1?6. Sofia, Bulgaria, Aug. 4-9 2013. 
109
Wael Salloum and Nizar Habash. 2011. Dialectal to Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation. Proceedings of the First Workshop on Algorithms and Resources for Model-ling of Dialects and Language Varieties. Edinburgh, Scotland. Wael Salloum and Nizar Habash. 2012. Elissa: A Dialectal to Standard Machine Translation System. Proceed-ings of Coling 2012: Demonstration Papers: 385-392, Mumbai, India. Hassan Sawaf. 2010. Arabic Dialect Handling in Hybrid Machine Translation. Proceedings of the Conference of the Association for Machine Translation in the Americas AMTA 2010. Denver, Colorado. Khaled Shaalan. 2010. Rule-based Approach in Arabic Natural Language Processing. International Journal on Information and Communication Technologies, 3(3). Andreas Stolcke. 2002.  SRILM?An Extensible Language Modeling Toolkit. Proceedings of ICSLP, 2002. Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Ma-khoul, Omar F. Zaidan, and Chris Callison-Burch. 2012. Machine translation of Arabic Dialects. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Montreal, Canada. 
110
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 22?27,
Dublin, Ireland, August 24 2014.
Automatic Identification of Arabic Language Varieties and Dialects in Social Media 
 Fatiha Sadat University of Quebec in Montreal, 201 President Ken-nedy, Montreal, QC, Canada sadat.fatiha@uqam.ca 
Farnazeh Kazemi NLP Technologies Inc. 52 Le Royer Street W., Montreal, QC, Canada kazemi@nlptechnologies.ca 
Atefeh Farzindar NLP Technologies Inc. 52 Le Royer Street W., Montreal, QC, Canada farzindar@nlptechnologies.ca   Abstract  Modern Standard Arabic (MSA) is the formal language in most Arabic countries. Arabic Dia-lects (AD) or daily language differs from MSA especially in social media communication. However, most Arabic social media texts have mixed forms and many variations especially be-tween MSA and AD. This paper aims to bridge the gap between MSA and AD by providing a framework for AD classification using probabilistic models across social media datasets. We present a set of experiments using the character n-gram Markov language model and Naive Bayes classifiers with detailed examination of what models perform best under different condi-tions in social media context. Experimental results show that Naive Bayes classifier based on character bi-gram model can identify the 18 different Arabic dialects with a considerable over-all accuracy of 98%.   1 Introduction Arabic is a morphologically rich and complex language, which presents significant challenges for nat-ural language processing and its applications. It is the official language in 22 countries spoken by more than 350 million people around the world1. Moreover, the Arabic language exists in a state of diglossia where the standard form of the language, Modern Standard Arabic (MSA) and the regional dialects (AD) live side-by-side and are closely related (Elfardy and Diab, 2013). Arabic has more than 22 dialects; some countries share the same dialect, while many dialects may exist alongside MSA within the same Arab country.  Modern Standard Arabic (MSA) is the written form of Arabic used mostly in education and scripted speech; it is also the formal communication language. Spoken Arabic is often referred to as colloquial Arabic, dialects, or vernaculars. Thus. Arabic dialects (AD) or colloquial languages are spoken varie-ties of Arabic and the daily language of several people. Arabic dialects and MSA share a considerable number of semantic, syntactic, morphological and lexical features; however, these features have many differences (Al-Sabbagh and Girju, 2013). Recently, considerable interest was given to Arabic dialects and the written varieties of Arabic found on social networking sites such as chats, micro-blog, blog and forums, which is the target re-search of sentiment analysis, opinion mining, machine translation, etc. Social media poses three major computational challenges, dubbed by Gartner the 3Vs of big data: vol-ume, velocity, and variety2. NLP methods, in particular, face further difficulties arising from the short, noisy, and strongly contextualised nature of social media. In order to address the 3Vs of social media, new language technologies have emerged, such as the identification and definition of users' language varieties and the translation to a different language, than the source. 
                                                1 http://en.wikipedia.org/wiki/Geographic_distribution_of_Arabic#Population 2 http://en.wikipedia.org/wiki/Big_data  This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
22
Dialect identification is essential and considered the first prepossessing component for any natural language application dealing with Arabic and its variation such as machine translation, information retrieval for social media, sentiments analysis, opinion extraction, etc. Herein, we present our effort on a part of the ASMAT project (Arabic Social Media Analysis Tools), which aims at creating tools for analyzing social media in Arabic. This project paves the way for end user targets (like machine translation and sentiment analysis) through pre-processing and normalization. There are, however, still many challenges to be faced.  This paper presents a first-step towards the ultimate goal of identifying and defining languages and dialects within the social media text. This paper is organized as follows: Section 2 presents related work. Sections 3 and 4 describe the probabilistic approach based on the character n-gram Markov lan-guage model and Naive Bayes classifier. Section 5 presents the data set, the several conducted exper-iments and their results. Conclusions and future work are presented in Section 6. 2 Related Work There have been several works on Arabic Natural Language Processing (NLP). However, most tradi-tional techniques have focused on MSA, since it is understood across a wide spectrum of audience in the Arab world and is widely used in the spoken and written media. Few works relate the processing of dialectal Arabic that is different from processing MSA. First, dialects leverage different subsets of MSA vocabulary, introduce different new vocabulary that are more based on the geographical location and culture, exhibit distinct grammatical rules, and adds new morphologies to the words. The gap be-tween MSA and Arabic dialects has affected morphology, word order, and vocabulary (Kirchhoff and Vergyri, 2004). Almeman and Lee (2013) have shown in their work that only 10% of words (uni-gram) share between MSA and dialects.  Second, one of the challenges for Arabic NLP applications is the mixture usage of both AD and MSA within the same text in social media context.  Recently, research groups have started focusing on dialects. For instance, Columbia University provides a morphological analyzer (MAGAED) for Levantine verbs and assumes the input is non-noisy and purely Levantine (Habash and Rambow, 2006).  Dialect Identification task has the same nature of language identification (LI) task. LI systems achieved high accuracy even with short texts (Baldwin and Lui, 2010), (Cavnar and Trenkle, 1994), (Joachims, 1998), (Kikui,1996); however, the challenge still exists when the document contains a mix-ture of different languages, which is actually the case for the task of dialect identification, where text is a mixture of MSA and dialects, and the dialects share a considerable amount of vocabularies. Biadsy and al. (2009) present a system that identifies dialectal words in speech and their dialect of origin through the acoustic signals. Salloum and Habash (2011) tackle the problem of AD to English Ma-chine Translation (MT) by pivoting through MSA. The authors present a system that applies transfer rules from AD to MSA then uses state of the art MSA to English MT system. Habash and al. (2012) present CODA, a Conventional Orthography for Dialectal Arabic that aims to standardize the orthog-raphy of all the variants of AD while Dasigi and Diab (2011) present an unsupervised clustering ap-proach to identify orthographic variants in AD.  Recently, Elfardy and Diab (Elfardy and al., 2013) introduced a supervised approach for perform-ing sentence level dialect identification between Modern Standard Arabic and Egyptian Dialectal Ara-bic. The system achieved an accuracy of 85.5% on an Arabic online-commentary dataset outperform-ing a previously proposed approach achieving 80.9% and reflecting a significant gain over a majority baseline of 51.9% and two strong baseline systems of 78.5% and 80.4%, respectively (Elfardy and Diab, 2012). Our proposed approach for dialect identification focuses on character-based n-gram Markov lan-guage models and Naive Bayes classifiers. Character n-gram model is well suited for language identification and dialect identification tasks that have many languages and/or dialects, little training data and short test samples. One of the main reasons to use a character-based model is that most of the variation between dia-lects, is based on affixation, which can be extracted easily by the language model, though also there are word-based features which can be detected by lexicons.  
23
3 N-Gram Markov Language Model There are two popular techniques for language identification. The first approach is based on popular words or stop-words for each language, which score the text based on these words (Gotti and al., 2013). The second approach is more statistical oriented. This approach is based on n-gram model (Cavnar and Trenkle, 1994), Hidden Markov model (Dunning, 1994) and support vector machine (Joachims, 1998). A language model is one of the main components in many NLP tools and applications. Thus, lot of efforts have been spent for developing and improving features of the language models.  Our proposed approach uses the Markov model to calculate the probability that an input text is derived from a given language model built from training data (Dunning, 1994). This model enables the com-putation of the probability P(S) or likelihood, of a sentence S, by using the following chain formula in equation 1: P(w1,w2,..., wn ) = P(w1)  P(wi |w1... wi?1)i=2n?       (1) Where, the sequence (w1, ?, wn) represents the sequence of characters in a sentence S. P(wi | w1, ?, wi-1) represents the probability of the character wi given the sequence w1, ?, wi-1. Generally, the related approach that determines the probability of a word sequence is not very help-ful because of its computational cost that is considered as very expensive. Markov models assume that we can predict the probability of some future unit without looking too far into the past. So we could apply the Markov assumption to the above chain probability in Formula 1, by looking to zero character (uni-gram), one character (bi-gram), two characters (tri-gram).  The intuition behind using n-gram models in a dialect identification task is related to the variation in the affixations that are attached to words, which can be detected by bi-gram or tri-gram models. 4 Na?ve Bayed Classifier Naive Bayes classifier is a simple and effective probabilistic learning algorithm. A naive Bayes classi-fier is a simple probabilistic classifier based on applying Bayes' theorem with strong (naive) independ-ence assumptions. A more descriptive term for the underlying probability model would be "independent feature model"3. In text classification, this classifier assigns the most likely category or class to a given document d from a set of pre-define N classes as c1, c2, ..., cN. the classification function f maps a document to a category (f: D ? C) by maximizing the probability of the following equation (Peng and Schuurmans, 2003): P(c|d) = P(c)?P(d | c)P(d)                              (7) Where, d and c denote each the document and the category, respectively. In text classification a document d can be represented by a vector of T attribute d=(t1,t2, ..., tT). Assuming that all attribute ti are independent given the category c, then we can calculate p(d|c) with the following equation: c?CargmaxP(c|d)= c?CargmaxP(c)?   P(t i |c)i=1T?        (8) The attribute term ti can be a vocabulary term, local n-gram, word average length, or global syntac-tic and semantic properties (Peng and Schuurmans, 2003). 5 Experiments and Results We have carried out two sets of experiments. The first set of experiments uses the character n-gram language model, while the second one uses the Naive Bayes classifier. The developed system identi-fies Arabic dialects using character n-gram models where the probability of each (Uni-gram, Bi-gram and Tri-gram) is calculated based on the training data within social media context.                                                  3http://en.wikipedia.org/wiki/Naive_Bayes_classifier 
24
5.1 Data The System has been trained and tested using a data set collected from blogs and forums of dif-ferent countries with Arabic as an official language. We have considered each regional language or dialect as belonging to one Arab country, although in reality a country most of the time may have several dialects. Moreover, there is a possible division of regional language within the six regional groups, as follows: Egyptian, Levantine, Gulf, Iraqi, Maghrebi and others (Zaidan and Callison-Burch, 2012). The different group divisions with their involved countries are defined as follows: - Egyptian: Egypt;   - Iraqi: Iraq;  - Gulf: Bahrein, Emirates, Kuwait, Qatar, Oman and Saudi Arabia; - Maghrebi: Algeria, Tunisia, Morocco, Libya, Mauritania; - Levantine: Jordan, Lebanon, Palestine, Syria; - Others: Sudan. Moreover, there might be many other possible sub-divisions in one division, especially in the large region such as the Maghrebi. We used a data set that consists on the crowd source of social media texts such as forums and blogs. This set of data was manually collected and constructed using sev-eral (around eighteen) forums sites in Arabic. The collected texts were manually segmented to co-herent sentences or paragraphs. For each dialect, sentences were saved in XML format with addi-tional information such as sequence number, country, date, and the link. Table 1 shows some sta-tistics about the collected text such as the total number of sentences or paragraph and number of words for each dialect. For each dialect 100 sentences were selected randomly for test purposes and were excluded from the training data. Moreover, statistics on the data set for each group of countries can be constructed, following the data set of Table 1.  Country #sentences #words Egypt 7 203 72 784 Bahrain 3 536 36 006 Emirates 4 405 43 868 Kuwait 3 318 44 811 Oman 4 814 77 018 Qatar 2 524 22 112 Saudi Arabia 9 882 82 206 Jordon 1 944 18 046 Lebanon 3 569 26 455 Palestine 316 3 961 Syria 3 459 43 226 Algeria 731 10 378 Libya 370 5 300 Mauritania 2 793 62 694 Morocco 2 335 30 107 Tunisia 3 843 18 199 Iraq 1 042 13 675 Sudan 5 775 28 368 Table 1: Statistics about the dataset for each country 5.2 Results and Discussion We have carried out three different experiments using uni-gram, bi-gram and tri-gram character for each experiment base on either the Markov language model or the Naive Bayes classifier. These dif-ferent experiments show how character distribution (uni-gram) or the affixes of size 2 or 3 (bi-gram or tri-gram) help distinguish between Arabic dialects. The set of experiments were conducted on 18 dia-lects representing 18 countries. Furthermore, we conducted the experiments on six groups of Arabic dialects, which represent six areas as described in the earlier section.  
25
For evaluation purposes, we considered the accuracy as a proportion of true identified test data and the F-Measure as a balanced mean between precision and recall. Our conducted experiments showed that the character-based uni-gram distribution helps the identification of two dialects, the Mauritanian and the Moroccan with an overall F-measure of 60% and an overall accuracy of 96%. Furthermore, the bi-gram distribution of two characters affix helps recognize four dialects, the Mauritanian, Moroccan, Tunisian and Qatari, with an overall F-measure of 70% and overall accuracy of 97%. Last, the tri-gram distribution of three characters affix helps recognize four dialects, the Mauritani-an, Tunisian, Qatari and Kuwaiti, with an overall F-measure of 73% and an overall accuracy of 98%. Our comparative results show that the character-based tri-gram and bi-gram distributions have per-formed better than the uni-gram distribution for most dialects. Overall, for eighteen dialects, the bi-gram model performed better than other models (uni-gram and tri-gram). Since many dialects are related to a region, and these Arabic dialects are approximately similar, we also consider the accuracy of dialects group. Again, the bi-gram and tri-gram character Markov lan-guage model performed almost same, although the F-Measure of bi-gram model for all dialect groups is higher than tri-gram model except for the Egyptian dialect. Therefore, in average for all dialects, the character-based bi-gram language model performs better than the character-based uni-gram and tri-gram models. Our results show that the Naive Bayes classifiers based on character uni-gram, bi-gram and tri-gram have better results than the previous character-based uni-gram, bi-gram and tri-gram Markov language models, respectively. An overall F-measure of 72% and an accuracy of 97% were noticed for the eighteen Arabic dialects. Furthermore, the Naive Bayes classifier that is based on a bi-gram model has an overall F-measure of 80% and an accuracy of 98%, except for the Palestinian dialect because of the small size of data. The Naive Bayes classifier based on the tri-gram model showed an overall F-measure of 78% and an accuracy of 98% except for the Palestinian and Bahrain dialects. This classifi-er could not distinguish between Bahrain and Emirati dialects because of the similarities on their three affixes. In addition, the naive Bayes classifier based on a character bi-gram performed better than the classifier based on a character tri-gram. Also, the accuracy of dialect groups for the Naive Bayes clas-sifier based on character bi-gram model yielded better results than the two other models (uni-gram and tri-gram). 6 Conclusion In this study, we presented a comparative study on dialect identification of Arabic language using so-cial media texts; which is considered as a very hard and challenging task. We studied the impact of the character n-gram Markov models and the Naive Bayes classifiers using three n-gram models, uni-gram, bi-gram and tri-gram. Our results showed that the Naive Bayes classifier performs better than the character n-gram Markov model for most Arabic dialects. Furthermore, the Naive Bayes classifier based on character bi-gram model was more accurate than other classifiers that are based on character uni-gram and tri-gram. Last, our study showed that the six Arabic dialect groups could be distin-guished using the Naive Bayes classifier based on character n-gram model with a very good perfor-mance.  As for future work, it would be interesting to explore the impact of the number of dialects or lan-guages on a classifier. Also, it would be interesting to explore the influence of size of training and test set for both character n-gram Markov model and Naive Bayes classifier based on character n-gram model. We are planning to use more social media data from Twitter or Facebook in order to estimate the accuracy of these two models in the identification of the dialect and the language. Another exten-sion to this work is to study a hybrid model for dialect identification involving character-based and word-based models. Finally, what we presented in this draft is a preliminary research on exploiting social media corpora for Arabic in order to analyze them and exploit them for NLP applications. Fur-ther extensions to this research include the translation of social media data to other languages and dia-lects, within the scope of the ASMAT project. Reference 
26
Al-Sabbagh R. and Girju R. 2013. Yadac : Yet another dialectal arabic corpus. In N. C. C. Chair, K. Choukri, T. Declerck, M. U. Doan, B. Maegaard, J. Mariani, J. Odijk, and S. Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12), Istanbul, Turkey, May 2012. Almeman K. and Lee M. 2013. Automatic building of arabic multi dialect text corpora by bootstrapping dialect words. In Communications, Signal Processing, and their Applications (ICCSPA),  2013. Baldwin T. and Lui M. 2010. Language identification: The long and the short of the matter. In Human Language Technologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Computa-tional Linguistics, HLT'10, pages 229{237, Stroudsburg, PA, USA, 2010. Association for Computational Linguistics. Biadsy F., Hirschberg J., and Habash N. 2009. Spoken arabic dialect identification using phonotactic modeling. In Proceedings of the Workshop on Computational Approaches to Semitic Languages at the meeting of the European Association for Computational Linguistics (EACL), Athens, Greece. Cavnar W. B., Trenkle J. M. 1994. N-gram-based text categorization. 1994. In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval. Ann Arbor MI, 48113(2):161{175, 1994. Dasigi P. and Diab M. 2011. Codact: Towards identifying orthographic variants in dialectal arabic. In Proceedings of the 5th International Joint Conference on Natural Language Processing (ICJNLP), Chiangmai, Thailand, 2011. Dunning T. 1994. Statistical identification of languages. Citeseer, 1994. Elfardy H. and Diab M. 2012. Simplified guidelines for the creation of large scale dialectal arabic annotations. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey. Elfardy H. and Diab M. 2013. Sentence-Level Dialect Identification in Arabic, In Proceedings of the 51st An-nual Meeting of the Association for Computational Linguistics, ACL 2013, Sofia, Bulgaria. 2013. Elfardy H., Al-Badrashiny M., Elfardy M. and Diab M. 2013. Sentence Level Dialect Identification in Arabic. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 456?461, Sofia, Bulgaria, August 4-9 2013. Gotti F., Langlais P., and Farzindar A. 2013. Translating government agencies' tweet feeds: Specificities, pro-blems and (a few) solutions. In Proceedings of the Workshop on Language Analysis in Social Media, Atlanta, Georgia, June 2013. Association for Computational Linguistics, Association for Computational Linguistics. Habash N. and Rambow O. 2006. MAGEAD: A morphological analyzer and generator for the Arabic dialects. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pages 681?688, Sydney, Australia, July. Association for Computational Linguistics. Habash N., Diab M., and Rabmow O. 2012. Conventional orthography for dialectal arabic. In Proceedings of the Language Resources and Evaluation Conference (LREC), Istanbul, Turkey, 2012. Joachims T. 1998. Text categorization with support vector machines: Learning with many relevant features. Springer, 1998. Kikui G.-i. 1996. Identifying, the coding system and language, of on-line documents on the internet. In Proceedings of the 16th Conference on Computational Linguistics - Volume 2, COLING '96, pages 652{657, Stroudsburg, PA, USA, 1996. Association for Computational Linguistics. Kirchhoff K. and Vergyri D. 2004. Cross-dialectal acoustic data sharing for arabic speech recognition. In Acous-tics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP'04). IEEE International Conference on, vo-lume 1, pages I-765. IEEE, 2004. Peng F. and Schuurmans D. 2003. Combining naive bayes and n-gram language models for text classi_cation. In Advances in Information Retrieval, pages 335{350. Springer, 2003. Salloum W. and Habash N. 2011. Dialectal to standard arabic paraphrasing to improve arabic-english statistical machine translation. In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties , pages 10?21. Association for Computational Linguistics. Suliman A. F. 2008. Automatic Identification of Arabic Dialects USING Hidden Markov Models. Doctoral Dis-sertation, University of Pittsburgh. 2008. Zaidan O. F. and Callison-Burch C. 2011. The arabic online commentary dataset: an annotated dataset of infor-mal arabic with high dialectal content. In Proceedings of ACL, pages 37?41, 2011. Zaidan O. F. and Callison-Burch C. 2012. Arabic dialect identi_cation. volume 1, Microsoft Research, 2012.  
27
