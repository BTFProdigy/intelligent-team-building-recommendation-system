Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 57?62,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Constructing An Anaphorically Annotated Corpus With Non-Experts:
Assessing The Quality Of Collaborative Annotations.
Jon Chamberlain
University of Essex
School of Computer Science
and Electronic Engineering
jchamb@essex.ac.uk
Udo Kruschwitz
University of Essex
School of Computer Science
and Electronic Engineering
udo@essex.ac.uk
Massimo Poesio
University of Essex
School of Computer Science
and Electronic Engineering
poesio@essex.ac.uk
Abstract
This paper reports on the ongoing work
of Phrase Detectives, an attempt to cre-
ate a very large anaphorically annotated
text corpus. Annotated corpora of the size
needed for modern computational linguis-
tics research cannot be created by small
groups of hand-annotators however the
ESP game and similar games with a pur-
pose have demonstrated how it might be
possible to do this through Web collabora-
tion. We show that this approach could be
used to create large, high-quality natural
language resources.
1 Introduction
The statistical revolution in natural language pro-
cessing (NLP) has resulted in the first NLP
systems and components really usable on a
large scale, from part-of-speech (POS) taggers
to parsers (Jurafsky and Martin, 2008). But it
has also raised the problem of creating the large
amounts of annotated linguistic data needed for
training and evaluating such systems.
This requires trained annotators, which is pro-
hibitively expensive both financially and in terms
of person-hours (given the number of trained an-
notators available) on the scale required.
Recently, however, Web collaboration has
started to emerge as a viable alternative.
Wikipedia and similar initiatives have shown
that a surprising number of individuals are willing
to help with resource creation and scientific
experiments. The goal of the ANAWIKI project
1
is to experiment with Web collaboration as a
solution to the problem of creating large-scale
linguistically annotated corpora. We do this by
developing tools through which members of our
scientific community can participate in corpus
1
http://www.anawiki.org
creation and by engaging non-expert volunteers
with a game-like interface. In this paper we
present ongoing work on Phrase Detectives
2
,
a game designed to collect judgments about
anaphoric annotations, and we report a first
analysis of annotation quality in the game.
2 Related Work
Large-scale annotation of low-level linguistic in-
formation (part-of-speech tags) began with the
Brown Corpus, in which very low-tech and time
consuming methods were used. For the cre-
ation of the British National Corpus (BNC), the
first 100M-word linguistically annotated corpus, a
faster methodology was developed using prelimi-
nary annotation with automatic methods followed
by partial hand-correction (Burnard, 2000).
Medium and large-scale semantic annotation
projects (for wordsense or coreference) are a re-
cent innovation in Computational Linguistics. The
semi-automatic annotation methodology cannot
yet be used for this type of annotation, as the qual-
ity of, for instance, coreference resolvers is not
yet high enough on general text. Nevertheless the
semantic annotation methodology has made great
progress with the development, on the one end,
of effective quality control methods (Hovy et al,
2006) and on the other, of sophisticated annotation
tools such as Serengeti (St?uhrenberg et al, 2007).
These developments have made it possible to
move from the small-scale semantic annotation
projects, the aim of which was to create resources
of around 100K words in size (Poesio, 2004b),
to the efforts made as part of US initiatives such
as Automatic Context Extraction (ACE), Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES), and GALE to create 1 mil-
lion word corpora. Such techniques could not be
expected to annotate data on the scale of the BNC.
2
http://www.phrasedetectives.org
57
2.1 Collaborative Resource Creation
Collaborative resource creation on the Web offers
a different solution to this problem. The motiva-
tion for this is the observation that a group of in-
dividuals can contribute to a collective solution,
which has a better performance and is more ro-
bust than an individual?s solution as demonstrated
in simulations of collective behaviours in self-
organizing systems (Johnson et al, 1998).
Wikipedia is perhaps the best example of col-
laborative resource creation, but it is not an iso-
lated case. The gaming approach to data collec-
tion, termed games with a purpose, has received
increased attention since the success of the ESP
game (von Ahn, 2006).
2.2 Human Computation
Human computation, as a more general concept
than games with a purpose, has become popular
in numerous research areas. The underlying as-
sumption of learning from a vast user population
has been largely the same in each approach. Users
are engaged in different ways to achieve objectives
such as:
? Assigning labels to items
? Learning to rank
? Acquiring structured knowledge
An example of the first category is the ESP
game which was a project to label images with
tags through a competitive game. 13,500 users
played the game, creating 1.3M labels in 3 months
(von Ahn, 2006). Other examples of assigning
lables to items include Phetch and Peekaboom
(von Ahn et al, 2006).
Learning to rank is a very different objective.
For example user judgements are collected in the
Picture This game (Bennett et al, 2009). This is
a two player game where the user has to select
the best matching image for a given query from
a small set of potential candidates. The aim is
to learn a preference ranking from the user votes
to predict the preference of future users. Several
methods for modeling the collected preferences
confirmed the assumption that a consensus rank-
ing from one set of users can be used to model
another.
Phrase Detectives is in the third category, i.e. it
aims to acquire structured knowledge, ultimately
Figure 1: A screenshot of the Annotation Mode.
leading to a linguistically annotated corpus. An-
other example of aiming to acquire large amounts
of structured knowledge is the Open Mind Com-
monsense project, a project to mine commonsense
knowledge to which 14,500 participants con-
tributed nearly 700,000 sentences (Singh, 2002).
Current efforts in attempting to acquire large-
scale world knowledge from Web users include
Freebase
3
and True Knowledge
4
. A slightly dif-
ferent approach to the creation of commonsense
knowledge has been pursued in the Semantic Me-
diaWiki project (Kr?otzsch et al, 2007), an effort to
develop a ?Wikipedia way to the Semantic Web?:
i.e., to make Wikipedia more useful and to support
improved search of web pages via semantic anno-
tation.
3 The Phrase Detectives game
Phrase Detectives offers a simple graphical user
interface for non-expert users to learn how to
annotate text and to make annotation decisions
(Chamberlain et al, 2008).
In order to use Web collaboration to create an-
notated data, a number of issues have to be ad-
dressed. First among these is motivation. For any-
body other than a few truly dedicated people, an-
notation is a very boring task. This is where the
promise of the game approach lies. Provided that
a suitably entertaining format can be found, it may
be possible to get people to tag quite a lot of data
without them even realizing it.
3
http://www.freebase.com/
4
http://www.trueknowledge.com/
58
The second issue is being able to recruit suf-
ficient numbers of useful players to make the re-
sults robust. Both of these issues have been ad-
dressed in the incentive structures of Phrase De-
tectives (Chamberlain et al, 2009).
Other problems still remain, most important of
which is to ensure the quality of the annotated
data. We have identified four aspects that need to
be addressed to control annotation quality:
? Ensuring users understand the task
? Attention slips
? Malicious behaviour
? Genuine ambiguity of data
These issues have been addressed at the design
stage of the project (Kruschwitz et al, 2009).
The goal of the game is to identify relationships
between words and phrases in a short text. An ex-
ample of a task would be to highlight an anaphor-
antecedent relation between the markables (sec-
tions of text) ?This parrot? and ?He? in ?This parrot
is no more! He has ceased to be!? Markables are
identified in the text by automatic pre-processing.
There are two ways to annotate within the game:
by selecting a markable that corefers to another
one (Annotation Mode); or by validating a deci-
sion previously submitted by another player (Vali-
dation Mode).
Annotation Mode (see Figure 1) is the simplest
way of collecting judgments. The player has to lo-
cate the closest antecedent markable of an anaphor
markable, i.e. an earlier mention of the object. By
moving the cursor over the text, markables are re-
vealed in a bordered box. To select it the player
clicks on the bordered box and the markable be-
comes highlighted. They can repeat this process if
there is more than one antecedent markable (e.g.
for plural anaphors such as ?they?). They submit
the annotation by clicking the Done! button.
The player can also indicate that the highlighted
markable has not been mentioned before (i.e. it is
not anaphoric), that it is non-referring (for exam-
ple, ?it? in ?Yeah, well it?s not easy to pad these
Python files out to 150 lines, you know.?) or that
it is the property of another markable (for exam-
ple, ?a lumberjack? being a property of ?I? in ?I
wanted to be a lumberjack!?).
In Validation Mode (see Figure 2) the player
is presented with an annotation from a previous
Figure 2: A screenshot of the Validation Mode.
player. The anaphor markable is shown with the
antecedent markable(s) that the previous player
chose. The player has to decide if he agrees with
this annotation. If not he is shown the Annotation
Mode to enter a new annotation.
In the game groups of players work on the same
task over a period of time as this is likely to lead
to a collectively intelligent decision (Surowiecki,
2005). An initial group of players are asked to an-
notate a markable. If all the players agree with
each other then the markable is considered com-
plete.
However it is likely that the first group of play-
ers will not agree with each other (62% of mark-
ables are given more than one relationship). In this
case each unique relationship for the markable is
validated by another group of players. This type of
validation has also been proposed elsewhere, e.g.
(Krause and Aras, 2009).
When the users register they begin with the
training phase of the game. Their answers are
compared with Gold Standard texts to give them
feedback on their decisions and to get a user rat-
ing, which is used to determine whether they need
more training. Contextual instructions are also
available during the game.
The corpus used in the game is created from
short texts including, for example, Wikipedia arti-
cles selected from the ?Featured Articles? and the
page of ?Unusual Articles?; stories from Project
Gutenberg including Aesop?s Fables, Sherlock
Holmes and Grimm?s Fairy Tales; and dialogue
texts from Textfile.com.
59
Expert 1 vs. Expert 2 Expert 1 vs. Game Expert 2 vs. Game
Overall agreement 94.1% 84.5% 83.9%
DN agreement 93.9% 96.0% 93.1%
DO agreement 93.3% 72.7% 70.0%
NR agreement 100.0% 100.0% 100.0%
PR agreement 100.0% 0.0% 0.0%
Table 1: Agreement figures for overall, discourse-new (DN), discourse-old (DO), non-referring (NR)
and property (PR) attributes.
4 Results
The first public version of Phrase Detectives
went live in December 2008. 1.1 million words
have been converted and made ready for annota-
tion. Over 920 players have submitted more than
380,000 annotations and validations of anaphoric
relations. 46 documents have been fully anno-
tated, meaning that at least 8 players have ex-
pressed their judgment on each markable, and
each distinct anaphoric relation that these players
assigned has been checked by four more players.
To put this in perspective, the GNOME corpus,
produced by traditional methods, included around
3,000 annotations of anaphoric relations (Poesio,
2004a) whereas OntoNotes
5
3.0, with 1 million
words, contains around 140,000 annotations.
4.1 Agreement on annotations
A set of tools were developed to examine the de-
cisions of the players, and address the following
questions:
? How do the collective annotations produced
by the game compare to annotations assigned
by an expert annotator?
? What is the agreement between two experts
annotating the same texts?
The answer to the first question will tell us
whether the game is indeed successful at obtain-
ing anaphoric annotations collaboratively within
the game context. Anaphoric annotations are how-
ever considered much harder than other tasks such
as part-of-speech tagging. Therefore we ask the
second question which will give us an upper bound
of what can be expected from the game in the best
possible case.
We analysed five completed documents from
the Wikipedia corpus containing 154 markables.
5
http://www.ldc.upenn.edu
We first looked at overall agreement and then
broke it down into individual types of anaphoric
relations. The following types of relation can be
assigned by players:
? DN (discourse-new): this markable has no
anaphoric link to any previous markable.
? DO (discourse-old): this markable has an
anaphoric link and the player needs to link
it to the most recent antecedent.
? NR (non-referring): this markable does not
refer to anything e.g. pleonistic ?it?.
? PR (property attribute): this markable repre-
sents a property of a previously mentioned
markable.
DN is the most common relation with 70% of all
markables falling in this category. 20% of mark-
ables are DO and form a coreference chain with
markables previously mentioned. Less than 1% of
markables are non-referring. The remaining mark-
ables have been identified as property attributes.
Each document was also manually annotated in-
dividually by two experts. Overall, we observe
84.5% agreement between Expert 1 and the game
and 83.9% agreement between Expert 2 and the
game. In other words, in about 84% of all cases the
relation obtained from the majority vote of non-
experts was identical to the one assigned by an ex-
pert. Table 1 gives a detailed breakdown of pair-
wise agreement values.
The agreement between the two experts is
higher than between an expert and the game. This
on its own is not surprising. However, an indi-
cation of the difficulty of the annotation task is the
fact that the experts only agree in 94% of all cases.
This can be seen as an upper boundary of what we
might get out of the game.
Furthermore, we see that the figures for DN are
very similar for all three comparisons. This seems
to be the easiest type of relation to be detected.
60
DO relations appear to be more difficult to de-
tect. However if we relax the DO agreement con-
dition and do not check what the antecedent is, we
get agreement figures above 90% in all cases: al-
most 97% between the two experts and between
91% and 93% when comparing an expert with the
game. A number of these cases which are assigned
as DO but with different antecedents are actually
coreference chains which link to the same object.
Extracting coreference chains from the game is
part of the future work.
Although non-referring markables are rare, they
are correctly identified in every case. We additon-
ally checked every completed markable identified
as NR in the corpus and found that there was 100%
precision in 54 cases.
Property (PR) relations are very hard to identify
and not a single one resulted from the game.
4.2 Disagreement on annotations
Disagreements between experts and the game
were examined to understand whether the game
was producing a poor quality annotation or
whether the markable was in fact ambiguous.
These are cases where the gold standard as cre-
ated by an expert is not the interpretation derived
from the game.
? In 60% of all cases where the game proposed
a relation different from the expert annota-
tion, the expert marked this relation to be
a possible interpretation as well. In other
words, the majority of disagreements are not
false annotations but alternatives such as am-
biguous interpretations or references to other
markables in the same coreference chain. If
we counted these cases as correct, we get an
agreement ratio of above 93%, close to pair-
wise expert agreement.
? In cases of disagreement the relation identi-
fied by the expert was typically the second or
third highest ranked relation in the game.
? The cumulative score of the expert relation
(as calculated by the game) in cases of dis-
agreement was 4.5, indicating strong player
support for the expert relation even though it
wasn?t the top answer. A relation with a score
of zero would be interpreted as one that has
as many players supporting it as it has players
disagreeing.
4.3 Discussion
There are very promising results in the agreement
between an expert and the top answer produced
from the game. By ignoring property relations and
the identification of coreference chains, the results
are close to what is expected from an expert. The
particular difficulty uncovered by this analysis is
the correct identification of properties attributes.
The analysis of markables with disagreement
show that some heuristics and filtering should be
applied to extract the highest quality decisions
from the game. In many of the cases the game
recorded plausible interpretations of different re-
lations, which is valuable information when ex-
ploring more difficult and ambiguous markables.
These would also be the markables that automatic
anaphora resolution systems would have difficulty
solving.
The data that was used to generate the results
was not filtered in any way. It would be possible
to ignore annotations from users who have a low
rating (judged when players annotate a gold stan-
dard text). Annotation time could also be a factor
in filtering the results. On average an annotation
takes 9 seconds in Annotation Mode and 11 sec-
onds in Validation Mode. Extreme variation from
this may indicate that a poor quality decision has
been made.
A different approach could be to identify those
users who have shown to provide high quality in-
put. A knowledge source could be created based
on input from these users and ignore everything
else. Related work in this area applies ideas from
citation analysis to identify users of high expertise
and reputation in social networks by, e.g., adopting
Kleinberg?s HITS algorithm (Yeun et al, 2009) or
Google?s PageRank (Luo and Shinaver, 2009).
The influence of document type may have a sig-
nificant impact on both the distribution of mark-
able types as well as agreement between ex-
perts and the game. We have only analysed the
Wikipedia documents, however discourse texts
from Gutenberg may provide different results.
5 Conclusions
This first detailed analysis of the annotations col-
lected from a collaborative game aiming at a large
anaphorically annotated corpus has demonstrated
that high-quality natural language resources can
be collected from non-expert users. A game ap-
proach can therefore be considered as a possible
61
alternative to expert annotations.
We expect that the finally released corpus will
apply certain heuristics to address the cases of dis-
agreement between experts and consensus derived
from the game.
6 Future Work
This paper has focused on percentage agreement
between experts and the game output but this is
a very simplistic approach. Various alternative
agreement coefficients have been proposed that
correct for chance agreement. One such measure
is Cohen?s ? (Cohen, 1960) which we are using to
perform a more indepth analysis of the data.
The main part of our future work remains the
creation of a very large annotated corpus. To
achieve this we are converting source texts to in-
clude them in the game (our aim is a 100M word
corpus). We have already started converting texts
in different languages to be included in the next
version of the game.
Acknowledgments
ANAWIKI is funded by a grant from the Engineer-
ing and Physical Sciences Research Council (EP-
SRC), grant number EP/F00575X/1. Thanks to
Daniela Goecke, Nils Diewald, Maik St?uhrenberg
and Daniel Jettka (University of Bielefeld), Mark
Schellhase (University of Essex) and all the play-
ers who have contributed to the project
References
P. N. Bennett, D. M. Chickering, and A. Mitya-
gin. 2009. Learning consensus opinion: min-
ing data from a labeling game. In Proceedings of
the 18th International World Wide Web Conference
(WWW2009), pages 121?130, Madrid.
L. Burnard. 2000. The British National Corpus Ref-
erence guide. Technical report, Oxford University
Computing Services, Oxford.
J. Chamberlain, M. Poesio, and U. Kruschwitz. 2008.
Phrase Detectives - A Web-based Collaborative An-
notation Game. In Proceedings of I-Semantics,
Graz.
J. Chamberlain, M. Poesio, and U. Kruschwitz. 2009.
A new life for a dead parrot: Incentive structures in
the Phrase Detectives game. In Proceedings of the
Webcentives Workshop at WWW?09, Madrid.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20(1):37?46.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proceedings of HLT-NAACL06.
N. L. Johnson, S. Rasmussen, C. Joslyn, L. Rocha,
S. Smith, and M. Kantor. 1998. Symbiotic Intel-
ligence: Self-Organizing Knowledge on Distributed
Networks Driven by Human Interaction. In Pro-
ceedings of the Sixth International Conference on
Artificial Life. MIT Press.
D. Jurafsky and J. H. Martin. 2008. Speech and Lan-
guage Processing- 2
nd
edition. Prentice-Hall.
M. Krause and H. Aras. 2009. Playful tagging folkson-
omy generation using online games. In Proceedings
of the 18th International World Wide Web Confer-
ence (WWW2009), pages 1207?1208, Madrid.
M. Kr?otzsch, D. Vrande`ci?c, M. V?olkel, H. Haller, and
R. Studer. 2007. Semantic Wikipedia. Journal of
Web Semantics, 5:251?261.
U. Kruschwitz, J. Chamberlain, and M. Poesio. 2009.
(Linguistic) Science Through Web Collaboration in
the ANAWIKI Project. In Proceedings of Web-
Sci?09, Athens.
X. Luo and J. Shinaver. 2009. MultiRank: Reputation
Ranking for Generic Semantic Social Networks. In
Proceedings of the WWW 2009 Workshop on Web
Incentives (WEBCENTIVES?09), Madrid.
M. Poesio. 2004a. Discourse annotation and semantic
annotation in the gnome corpus. In Proceedings of
the ACL Workshop on Discourse Annotation.
M. Poesio. 2004b. The MATE/GNOME scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.
P. Singh. 2002. The public acquisition of com-
monsense knowledge. In Proceedings of the AAAI
Spring Symposium on Acquiring (and Using) Lin-
guistic (and World) Knowledge for Information Ac-
cess, Palo Alto, CA.
M. St?uhrenberg, D. Goecke, N. Diewald, A. Mehler,
and I. Cramer. 2007. Web-based annotation of
anaphoric relations and lexical chains. In Proceed-
ings of the ACL Linguistic Annotation Workshop,
pages 140?147.
J. Surowiecki. 2005. The Wisdom of Crowds. Anchor.
L. von Ahn, R. Liu, and M. Blum. 2006. Peekaboom:
a game for locating objects in images. In Proceed-
ings of CHI ?06, pages 55?64.
L. von Ahn. 2006. Games with a purpose. Computer,
39(6):92?94.
C. A. Yeun, M. G. Noll, N. Gibbins, C. Meinel, and
N. Shadbolt. 2009. On Measuring Expertise in Col-
laborative Tagging Systems. In Proceedings of Web-
Sci?09, Athens.
62
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 106?115,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Automatic Creation of Arabic Named Entity Annotated Corpus Using
Wikipedia
Maha Althobaiti, Udo Kruschwitz, and Massimo Poesio
School of Computer Science and Electronic Engineering
University of Essex
Colchester, UK
{mjaltha, udo, poesio}@essex.ac.uk
Abstract
In this paper we propose a new methodology to ex-
ploit Wikipedia features and structure to automati-
cally develop an Arabic NE annotated corpus. Each
Wikipedia link is transformed into an NE type of
the target article in order to produce the NE an-
notation. Other Wikipedia features - namely redi-
rects, anchor texts, and inter-language links - are
used to tag additional NEs, which appear without
links in Wikipedia texts. Furthermore, we have de-
veloped a filtering algorithm to eliminate ambiguity
when tagging candidate NEs. Herein we also in-
troduce a mechanism based on the high coverage of
Wikipedia in order to address two challenges partic-
ular to tagging NEs in Arabic text: rich morphology
and the absence of capitalisation. The corpus cre-
ated with our new method (WDC) has been used to
train an NE tagger which has been tested on differ-
ent domains. Judging by the results, an NE tagger
trained on WDC can compete with those trained on
manually annotated corpora.
1 Introduction
Supervised learning techniques are well known
for their effectiveness to develop Named Entity
Recognition (NER) taggers (Bikel et al., 1997;
Sekine and others, 1998; McCallum and Li, 2003;
Benajiba et al., 2008). The main disadvantage of
supervised learning is that it requires a large an-
notated corpus. Although a substantial amount
of annotated data is available for some languages,
for other languages, including Arabic, more work
is needed to enrich their linguistic resources. In
fact, changing the domain or just expanding the
set of classes always requires domain-specific ex-
perts and new annotated data, both of which cost
time and effort. Therefore, current research fo-
cuses on approaches that require minimal human
intervention to facilitate the process of moving the
NE classifiers to new domains and to expand NE
classes.
Semi-supervised and unsupervised learning ap-
proaches, along with the automatic creation of
tagged corpora, are alternatives that avoid manu-
ally annotated data (Richman and Schone, 2008;
Althobaiti et al., 2013). The high coverage and
rich informational structure of online encyclope-
dias can be exploited for the automatic creation of
datasets. For example, many researchers have in-
vestigated the use of Wikipedia?s structure to clas-
sify Wikipedia articles and to transform links into
NE annotations according to the link target type
(Nothman et al., 2008; Ringland et al., 2009).
In this paper we present our approach to au-
tomatically derive a large NE annotated corpora
from Arabic Wikipedia. The key to our method
lies in the exploitation of Wikipedia?s concepts,
specifically anchor texts
1
and redirects, to handle
the rich morphology in Arabic, and thereby elim-
inate the need to perform any deep morphologi-
cal analysis. In addition, a capitalisation probabil-
ity measure has been introduced and incorporated
into the approach in order to replace the capitalisa-
tion feature that does not exist in the Arabic script.
This capitalisation measure has been utilised in or-
der to filter ambiguous Arabic NE phrases during
annotation process.
The remainder of this paper is structured as fol-
lows: Section 2 illustrates structural information
about Wikipedia. Section 3 includes background
information on NER, including recent work. Sec-
tion 4 summarises the proposed methodology.
Sections 5, 6, and 7 describe the proposed algo-
rithm in detail. The experimental setup and the
evaluation results are reported and discussed in
Section 8. Finally, the conclusion features com-
ments regarding our future work.
1
The terms ?anchor texts? and ?link labels? are used inter-
changeably in this paper.
106
2 The Structure of Wikipedia
Wikipedia is a free online encyclopedia project
written collaboratively by thousands of volunteers,
using MediaWiki
2
. Each article in Wikipedia is
uniquely identified by its title. The title is usually
the most common name for the entity explained
in the article.
2.1 Types of Wikipedia Pages
2.1.1 Content Pages
Content pages (aka Wikipedia articles) contain the
majority of Wikipedia?s informative content. Each
content page describes a single topic and has a
unique title. In addition to the text describing the
topic of the article, content pages may contain ta-
bles, images, links and templates.
2.1.2 Redirect Pages
A redirect page is used if there are two or more
alternative names that can refer to one entity
in Wikipedia. Thus, each alternative name is
changed into a title whose article contains a redi-
rect link to the actual article for that entity. For ex-
ample, ?UK? is an alternative name for the ?United
Kingdom?, and consequently, the article with the
title ?UK? is just a pointer to the article with the
title ?United Kingdom?.
2.1.3 List of Pages
Wikipedia offers several ways to group articles.
One method is to group articles by lists. The items
on these lists include links to articles in a particu-
lar subject area, and may include additional infor-
mation about the listed items. For example, ?list
of scientists? contains links to articles of scientists
and also links to more specific lists of scientists.
2.2 The Structure of Wikipedia Articles
2.2.1 Categories
Every article in the Wikipedia collection should
have at least one category. Categories should be
on vital topics that are useful to the reader. For
example, the Wikipedia article about the United
Kingdom in Wikipedia is associated with a set of
categories that includes ?Countries bordering the
Atlantic Ocean?, and ?Countries in Europe?.
2
An open source wiki package written in PHP
2.2.2 Infobox
An infobox is a fixed-format table added to the
top right-hand or left-hand corner of articles to
provide a summary of some unifying parameters
shared by the articles. For instance, every scientist
has a name, date of birth, birthplace, nationality,
and field of study.
2.3 Links
A link is a method used by Wikipedia to link pages
within wiki environments. Links are enclosed in
doubled square brackets. A vertical bar, the ?pipe?
symbol, is used to create a link while labelling it
with a different name on the current page. Look at
the following two examples,
1 - [[a]] is labelled ?a? on the current page and
links to taget page ?a?.
2 - [[a|b]] is labelled ?b? on the current page, but
links to target page ?a?.
In the second example, the anchor text (aka link
label) is ?a?, while ?b?, a link target, refers to the
title of the target article. In the first example, the
anchor text shown on the page and the title of the
target article are the same.
3 Related Work
Current NE research seeks out adequate alter-
natives to traditional techniques such that they
require minimal human intervention and solve
deficiencies of traditional methods. Specific
deficiencies include the limited number of NE
classes resulting from the high cost of setting up
corpora, and the difficulty of adapting the system
to new domains.
One of these trends is distant learning, which
depends on the recruitment of external knowledge
to increase the performance of the classifier, or
to automatically create new resources used in the
learning stage.
Kazama and Torisawa (2007) exploited
Wikipedia-based features to improve their NE
machine learning recogniser?s F-score by three
percent. Their method retrieved the corresponding
Wikipedia entry for each candidate word sequence
in the CoNLL 2003 dataset and extracted a cate-
gory label from the first sentence of the entry.
The automatic creation of training data has
also been investigated using external knowledge.
An et al. (2003) extracted sentences containing
listed entities from the web, and produced a
1.8 million Korean word dataset. Their corpus
107
performed as well as manually annotated training
data. Nothman et al. (2008) exploited Wikipedia
to create a massive corpus of named entity
annotated text. They transformed Wikipedia?s
links into named entity annotations by classifying
the target articles into standard entity types
3
.
Compared to MUC, CoNLL, and BBN corpora,
their Wikipedia-derived corpora tend to perform
better than other cross-corpus train/test pairs.
Nothman et al. (2013) automatically created
massive, multilingual training annotations for
named entity recognition by exploiting the text
and internal structure of Wikipedia. They first
categorised each Wikipedia article into named
entity types, training and evaluating on 7,200
manually-labelled Wikipedia articles across nine
languages: English, German, French, Italian,
Polish, Spanish, Dutch, Portuguese, and Russian.
Their cross-lingual approach achieved up to 95%
accuracy. They transformed Wikipedia?s links
into named entity annotations by classifying the
target articles into standard entity types. This
technique produced reasonable annotations, but
was not immediately able to compete with exist-
ing gold-standard data. They better aligned their
automatic annotations to the gold standard corpus
by deducing additional links and heuristically
tweaking the Wikipedia corpora. Following this
approach, millions of words in nine languages
were annotated. Wikipedia-trained models were
evaluated against CONLL shared task data and
other gold-standard corpora. Their method out-
performed Richman and Schone (2008) and Mika
et al. (2008), and achieved scores 10% higher
than models trained on newswire when tested on
manually annotated Wikipedia text.
Alotaibi and Lee (2013) automatically de-
veloped two NE-annotated sets from Arabic
Wikipedia. The corpora were built using the
mechanism that transforms links into NE an-
notations, by classifying the target articles into
named entity types. They used POS-tagging,
morphological analysis, and linked NE phrases to
detect other mentions of NEs that appear without
links in text. By contrast, our method does not
require POS-tagging or morphological analysis
and just identifies unlinked NEs by matching
phrases from an automatically constructed and
filtered alternative names with identical terms in
3
The terms ?type?, ?class? and ?category? are used inter-
changeably in this paper.
the articles texts, see Section 6. The first dataset
created by Alotaibi and Lee (2013) is called
WikiFANE(whole) and contains all sentences
retrieved from the articles. The second set, which
is called WikiFANE(selective), is constructed by
selecting only the sentences that have at least one
named entity phrase.
4 Summary of the Approach
All of our experiments were conducted on the
26 March 2013 Arabic version of the Wikipedia
dump
4
. A parser was created to handle the medi-
awiki markup and to extract structural information
from the Wikipedia dump such as a list of redirect
pages along with their target articles, a list of pairs
containing link labels and their target articles in
the form ?anchor text, target article?, and essential
information for each article (e.g., title, body text,
categories, and templates).
Many of Wikipedia?s concepts such as links, an-
chor texts, redirects, and inter-language links have
been exploited to transform Wikipedia into a NE
annotated corpus. More details can be found in
the next sections. Generally, the following steps
are necessary to develop the dataset:
1. Classify Wikipedia articles into a specific set
of NE types.
2. Identify matching text in the title and the first
sentence of each article and label the match-
ing phrases according to the article type.
3. Label linked phrases in the text according to
the NE type of the target article.
4. Compile a list of alternative titles for articles
and filter out ambiguous ones.
5. Identify matching phrases in the list and the
Wikipedia text.
6. Filter sentences to prevent noisy sentences
being included in the corpus.
We explain each step in turn in the following sec-
tions.
5 Classifying Wikipedia Articles into NE
Categories
Categorising Wikipedia articles is the initial step
in producing NE training data. Therefore, all
Wikipedia articles need to be classified into a
specific set of named entity types.
4
http://dumps.wikimedia.org/arwiki/
108
5.1 The Dataset and Annotation
In order to develop a Wikipedia document clas-
sifier, we used a set of 4,000 manually classi-
fied Wikipedia articles that are available free on-
line
5
. The set was manually classified using the
ACE (2008) taxonomy and a new class (Product).
Therefore, there were eight coarse-grained cate-
gories in total: Facility, Geo-Political, Location,
Organisation, Person, Vehicle, Weapon, and Prod-
uct. As our work adheres to the CoNLL definition,
we mapped these classified Wikipedia articles into
CoNLL NE types ? namely person, location, or-
ganisation, miscellaneous, or other ? based on the
CoNLL 2003 annotation guidelines (Chinchor et
al., 1999).
5.2 The Classification of Wikipedia Articles
Many researchers have already addressed the task
of classifying Wikipedia articles into named entity
types (Dakka and Cucerzan, 2008; Tardif et al.,
2009). Alotaibi and Lee (2012) is the only study
that has experimented with classifying the Arabic
version of Wikipedia into NE classes. They have
explored the use of Naive Bayes, Multinomial
Naive Bayes, and SVM for classifying Wikipedia
articles, and achieved a F-score ranging from 78%
and 90% using different language-dependent and
independent features.
We conducted three experiments that used a
simple bag-of-words features extracted from dif-
ferent portions of the Wikipedia document and
metadata. We summarise the portions of the doc-
ument included in each experiment below:
Exp1: Experiment 1 involved tokens from the
article title and the entire article body.
Exp2: Rich metadata in Wikipedia proved ef-
fective for the classification of articles (Tardif et
al., 2009; Alotaibi and Lee, 2012). Therefore, in
Experiment 2 we included tokens from categories,
templates ? specifically ?Infobox? ? as well as to-
kens from the article title and first sentence of the
document.
Exp3: Experiment 3 involved the same set of
tokens as experiment 2 except that categories and
infobox features were marked with suffixes to dif-
ferentiate them from tokens extracted from the ar-
ticle body text. This step of distinguishing tokens
based on their location in the document improved
the accuracy of document?s classification (Tardif
et al., 2009; Alotaibi and Lee, 2012).
5
www.cs.bham.ac.uk/?fsa081/
In order to optimise features, we implemented a
filtered version of the bag-of-words article repre-
sentation (e.g., removing punctuation marks and
symbols) to classify the Arabic Wikipedia doc-
uments instead of using a raw dataset (Alotaibi
and Lee, 2012). In addition, the same study
shows the high impact of applying tokenisation
6
as opposed to the neutral effect of using stem-
ming. We used the filtered features proposed in
the study of Alotaibi and Lee (2012), which in-
cluded removing punctuation marks, symbols, fil-
tering stop words, and normalising digits. We ex-
tended the features, however, by utilising the to-
kenisation scheme that involves separating con-
junctions, prepositions, and pronouns from each
word.
The feature set has been represented using Term
Frequency-Inverse Document Frequency (TF ?
IDF ). This representation method is a numeri-
cal statistic that reflects how important a token is
to a document.
5.3 The Results of Classifying the Wikipedia
Articles
As for the learning process, our Wikipedia doc-
uments classifier was trained using Liblinear
7
.
80% of the 4,000 hand-classified Wikipedia
articles were dedicated to the training stage, while
20% were specified to test the classifier. Table
1 is a comparison of the precision, recall, and
F-measure of the classifiers that resulted from the
three experiments. The Exp3 classifier performed
better than the other classifiers. Therefore, it was
selected to classify all of the Wikipedia articles.
At the end of this stage, we obtained a list of
pairs containing each Wikipedia article and its
NE Type. We stored this list in a database in
preparation for the next stage: developing the
NE-tagged training corpus.
Table 1: The results of the three Wikipedia docu-
ment classifiers.
6
It is also called decliticization or segmentation.
7
www.csie.ntu.edu.tw/?cjlin/liblinear/
109
6 The Annotation Process
6.1 Utilising the Titles of Articles and Link
Targets
Identifying corresponding words in the article ti-
tle and the entire body of text and then tagging the
matching phrases with the NE-type can be a risky
process, especially for terms with more than one
meaning. For example, the title of the article de-
scribing the city (
	
?A?, ?Cannes?)
8
can also, in Ara-
bic, refer to the past verb (
	
?A?, ?was?). The portion
of the Wikipedia article unlikely to produce errors
during the matching process is the first sentence,
which usually contains the definition of the term
the Wikipedia article is written about (Zesch et al.,
2007).
When identifying matching terms in the arti-
cle title and the first sentence, we found that ar-
ticle titles often contain abbreviations, while the
first sentence spells out entire words. This pat-
tern makes it difficult to identify matching terms
in the title and first sentence, and frequently ap-
pears in biographical Wikipedia articles. For ex-
ample, one article is entitled (?


	
P@Q? @ Q?K
.
?K
.
@, ?Abu
Bakr Al-Razi?), but the first sentence states the full
name of the person: (?


	
P@Q? @ AK


Q?
	
P
	
?K
.
??


m
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 1?6,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Finding the Right Supervisor: Expert-Finding in a University Domain
Fawaz Alarfaj, Udo Kruschwitz, David Hunter and Chris Fox
School of Computer Science and Electronic Engineering
University of Essex
Colchester, CO4 3SQ, UK
{falarf, udo, dkhunter, foxcj}@essex.ac.uk
Abstract
Effective knowledge management is a key fac-
tor in the development and success of any or-
ganisation. Many different methods have been
devised to address this need. Applying these
methods to identify the experts within an or-
ganisation has attracted a lot of attention. We
look at one such problem that arises within
universities on a daily basis but has attracted
little attention in the literature, namely the
problem of a searcher who is trying to iden-
tify a potential PhD supervisor, or, from the
perspective of the university?s research office,
to allocate a PhD application to a suitable su-
pervisor. We reduce this problem to identify-
ing a ranked list of experts for a given query
(representing a research area).
We report on experiments to find experts in a
university domain using two different meth-
ods to extract a ranked list of candidates:
a database-driven method and a data-driven
method. The first one is based on a fixed list
of experts (e.g. all members of academic staff)
while the second method is based on auto-
matic Named-Entity Recognition (NER). We
use a graded weighting based on proximity be-
tween query and candidate name to rank the
list of candidates. As a baseline, we use a
system that ranks candidates simply based on
frequency of occurrence within the top docu-
ments.
1 Introduction
The knowledge and expertise of individuals are sig-
nificant resources for organisation. Managing this
intangible resource effectively and efficiently con-
stitutes an essential and very important task (Non-
aka and Takeuchi, 1995; Law and Ngai, 2008). Ap-
proaching experts is the primary and most direct way
of utilising their knowledge (Yang and Huh, 2008;
Li et al, 2011). Therefore, it is important to have a
means of locating the right experts within organisa-
tions. The expert-finding task can be categorised as
an information retrieval task similar to a web search,
but where the results are people rather than docu-
ments. An expert-finding system allows users to in-
put a query, and it returns a ranked list of experts.
Here we look at a university context. We start
with a real-world problem which is to identify a list
of experts within an academic environment, e.g. a
university intranet. The research reported here is
based on an empirical study of a simple but effective
method in which a system that applies the concept of
expert-finding has been designed and implemented.
The proposed system will contribute to provide an
expert-search service to all of the university?s stake-
holders.
Expert-finding systems require two main re-
sources in order to function: a list of candidates and
a collection of data from which the evidence of ex-
pertise can be extracted. We present two approaches
to address this problem, a database-driven and a
data-driven method using NER. The main differ-
ence between the two methods is the way in which
the candidates? list is constructed. In the database
method, the candidates are simply selected from a
known list of experts, e.g. the university?s academic
staff. In the NER method, the candidates are ex-
tracted automatically from the pages returned by an
1
underlying search engine. This method promises to
be more useful for finding experts from a wider (and
possibly more up-to-date) range of candidates. Both
methods apply the same ranking function(s), as will
be discussed below.
This paper will survey related work in Section 2
and introduce the expert-finding task in a university
domain in Section 3. The process of ranking experts
will be discussed in Section 4. The evaluation will
be described in Section 4, followed by a discussion
of the experiment?s results in Section 5.
2 Related Work
The expert-finding task addresses the problem of re-
trieving a ranked list of people who are knowledge-
able about a given topic. This task has found its
place in the commercial environment as early as the
1980?s, as discussed in Maybury (2006); however,
there was very limited academic research on finding
and ranking experts until the introduction of the en-
terprise track at the 2005 Text REtrieval Conference
(TREC) (Craswell et al, 2005).
When expert-finding we must know the experts?
profiles, These profiles may be generated manually
or automatically. Manually created profiles may be
problematic. If, for example, experts enter their own
information, they may exaggerate or downplay their
expertise. In addition, any changes of expertise for
any expert requires a manual update to the expert?s
profile. Thus incurring high maintenance costs. An
example of manually generated profiles is the work
of Dumais and Nielsen (1992). Although their sys-
tem automatically assigns submitted manuscripts to
reviewers, the profiles of the reviewers or experts are
created manually.
The alternative is to generate the profiles automat-
ically, for example by extracting relevant informa-
tion from a document collection. The assumption is
that individuals will tend to be expert in the topics
of documents with which they are associated. Ex-
perts can be associated with the documents in which
they are mentioned (Craswell et al, 2001) or with
e-mails they have sent or received (Balog and de Ri-
jke, 2006a; Campbell et al, 2003; Dom et al, 2003).
They can also be associated with their home pages
or CVs (Maybury et al, 2001), and with documents
they have written (Maybury et al, 2001; Becerra-
Fernandez, 2000). Finally, some researchers use
search logs to associate experts with the web pages
they have visited (Wang et al, 2002; Macdonald and
White, 2009).
After associating candidate experts with one or
more of the kinds of textual evidence mentioned
above, the next step is to find and rank candidates
based on a user query. Many methods have been
proposed to perform this task. Craswell et al (2001)
create virtual documents for each candidate (or em-
ployee). These virtual documents are simply con-
catenated texts of all documents from the corpus as-
sociated with a particular candidate. Afterwards,
the system indexes and processes queries for the
employee?s documents. The results would show a
list of experts based on the ten best matching em-
ployee documents. Liu et al (2005) have applied
expert-search in the context of a community-based
question-answering service. Based on a virtual doc-
ument approach, their work applied three language
models: the query likelihood model, the relevance
model and the cluster-based language model. They
concluded that combining language models can en-
hance the retrieval performance.
Two principal approaches recognised for expert-
finding can be found in the literature. Both were
first proposed by Balog et al (2006b). The mod-
els are called the candidate model and the doc-
ument model, or Model 1 and Model 2, respec-
tively. Different names have been used for the
two methods. Fang and Zhai (2007) refer to them
as ?Candidate Generation Models and Topic Gen-
eration Models?. Petkova and Croft (2006) call
them the ?Query-Dependent Model? and the ?Query-
Independent Model?. The main difference between
the models is that the candidate-based approaches
(Model 1) build a textual representation of candi-
date experts, and then rank the candidates based on
the given query, whereas the document-based ap-
proaches (Model 2) first find documents that are rel-
evant to the query, and then locate the associated ex-
perts in these documents.
Balog et al (2006b) have compared the two mod-
els and concluded that Model 2 outperforms Model
1 on all measures (for this reason, we will adopt
Model 2).
As Model 2 proved to be more efficient, it formed
the basis of many other expert-search systems (Fang
2
and Zhai, 2007; Petkova and Croft, 2007; Yao
et al, 2008). Fang and Zhai developed a mixture
model using proximity-based document representa-
tion. This model makes it possible to put different
weights on different representations of a candidate
expert (Fang and Zhai, 2007). Another mixture of
personal and global language models was proposed
by Serdyukov and Hiemstra (2008). They combined
two criteria for personal expertise in the final rank-
ing: the probability of generation of the query by the
personal language model and a prior probability of
candidate experts that expresses their level of activ-
ity in the important discussions on the query topic.
Zhu et al (2010) claimed that earlier language
models did not consider document features. They
proposed an approach that incorporates: internal
document structure; document URLs; page rank;
anchor texts; and multiple levels of association be-
tween experts and topics.
All of the proposed frameworks assume that the
more documents associated with a candidate that
score highly with respect to a query, the more likely
the candidate is to have relevant expertise for that
query. Macdonald and Ounis (2008) developed a
different approach, called the Voting Model. In their
model, candidate experts are ranked first by consid-
ering a ranking of documents with respect to the
users? query. Then, using the candidate profiles,
votes from the ranked documents are converted into
votes for candidates.
There have been attempts to tackle the expert-
finding problem using social networks. This has
mainly been investigated from two directions. The
first direction uses graph-based measures on social
networks to produce a ranking of experts (Campbell
et al, 2003; Dom et al, 2003). The second direc-
tion assumes similarities among the neighbours in a
social network and defines a smoothing procedure
to rank experts (Karimzadehgan et al, 2009; Zhang
et al, 2007).
Some have argued that it is not enough to find ex-
perts by looking only at the queries? without tak-
ing the users into consideration. They claim that
there are several factors that may play a role in
decisions concerning which experts to recommend.
Some of these factors are the users? expertise level,
social proximity and physical proximity (Borgatti
and Cross, 2003; McDonald and Ackerman, 1998;
Shami et al, 2008). McDonald and Ackerman
(1998) emphasised the importance of the accessi-
bility of the expert. They argued that people usu-
ally prefer to contact the experts who are physically
or organisationally close to them. Moreover, Shami
et al (2008) found that people prefer to contact ex-
perts they know, even when they could potentially
receive more information from other experts who are
located outside their social network.
Woudstra and van den Hooff (2008) identified a
number of factors in selecting experts that are re-
lated to quality and accessibility. They argued that
the process of choosing which candidate expert to
contact might differ depending on the specific situa-
tion.
Hofmann et al (2010) showed that many of these
factors can be modelled. They claimed that integrat-
ing them with retrieval models can improve retrieval
performance. Smirnova and Balog (2011) provided
a user-oriented model for expert-finding where they
placed an emphasis on the social distance between
the user and the expert. They considered a number
of social graphs based on organisational hierarchy,
geographical location and collaboration.
3 Expert-Finding in a University
In any higher educational institution, finding an ap-
propriate supervisor is a critical task for research
students, a task that can be very time consuming,
especially if academics describe their work using
terms that a student is not familiar with. A searcher
may build up a picture of who is likely to have
the relevant expertise by looking for university aca-
demic staff who have written numerous documents
about the general topic, who have authored docu-
ments exactly related to the topic, or who list the
topic as one of their research interest areas. Au-
tomating this process will not only help research stu-
dents find the most suitable supervisors, but it also
allow the university to allocate applications to super-
visors, and help researchers find other people inter-
ested in the particular topics.
3.1 Method
The two approaches we apply, database-driven, and
data-driven using NER1 are illustrated in Figure 1.
1We use OpenNLP to identify named entities.
3
Search API URL Filter
Page ReaderHTML Parser
Normalizer
Candidate 
Extractor
Candidate 
Evaluator
Named-Entity 
Recognition 
Candidate 
Evaluator
DB Connection
Update 
Candidate Rank
Display Experts
Query URLs Filtered URLs
Web Page
String
Parsed Html
Page
Normalized 
Web Page
String
Candidate List 
Normalized 
Web Page
String
Candidate
List
Candidate Rank 
For Page
Candidate Rank 
For Page
Named-Entity Recognition 
Method Database Method
Figure 1: System Architecture.
The main difference between the two methods is the
way in which the candidates? list is constructed. We
argue that each method has its advantages. In the
database method, the candidates are simply the uni-
versity?s academic staff. This avoids giving results
unrelated to the university. It would be appropriate if
the aim is to find the experts from among the univer-
sity academics. In the data-driven method, the can-
didates are extracted from the pages returned by the
underlying search engine. The experts found by this
method are not necessarily university staff. They
could be former academics, PhD students, visiting
professors, or newly appointed staff.
Both methods apply the same ranking functions,
one baseline function which is purely based on fre-
quency and one which takes into account proximity
of query terms with matches of potential candidates
in the retrieved set of documents.
3.2 The Baseline Approach
The baseline we chose for ranking candidates is the
frequency of appearance of names in the top twenty
retrieved documents. The system counts how many
times the candidate?s name appears in the document
d(cc). Then it calculates the candidate metric cm by
dividing the candidate count d(cc) by the number of
tokens in the document d(nt).
Equation 1 defines the metric, where cm is the
final candidate?s metric for all documents and n is
the number of documents.
cm =
n?
d=1
d(cc)
d(nt)
(1)
3.3 Our Approach
Our approach takes into account the proximity be-
tween query terms and candidate names in the
matching documents in the form of a distance
weight. This measure will adds a distance weight
value to the main candidate?s metric that was gener-
ated earlier. Similar approaches have been proposed
in the literature for different expert search applica-
tions Lu et al (2006); Cao et al (2005). The dis-
tance weight will be higher whenever the name ap-
pears closer to the query term, within a +/- 10 word
window.
We experiment with two different formulae. The
first formula is as follows:
cm1 =
n?
i=1
m?
j=1
(cm+
1
? ? ?ij
), ?ij =
{
dij if dij ? 10
0 Otherwise
(2)
where n is the number of times the candidate?s name
has been found in the matching documents, m is the
number of times the (full) query has been identified,
and dij is the distance between the name position
and query position (? has been set empirically to 3).
The second formula is:
cm2 =
n?
i=1
m?
j=1
(cm+
1
cm?ij
), ?ij =
{
dij if dij ? 10
0 Otherwise
(3)
This equation is designed to return a smaller value
as the distance x increases, and to give the candidate
with lower frequency a higher weight.
In both cases, candidates are ranked according to
the final score and displayed in order so that the can-
didates who are most likely to be experts are dis-
played at the top of the list.
4 Evaluation
As with any IR system, evaluation can be difficult.
In the given context one might argue that precision
4
is more important than recall. In any case, recall can
be difficult to measure precisely. To address these is-
sues we approximate a gold standard as follows. We
selected one school within the university for which
a page of research topics with corresponding aca-
demics exists In this experiment we take this map-
ping as a complete set of correct matches. In this
page, there are 371 topics (i.e. potential queries) di-
vided among 28 more general research topics. Each
topic/query is associated with one or more of the
school?s academic staff. It is presumed that those
names belong to experts on the corresponding top-
ics.
Table 1 illustrates some general topics with the
number of (sub)topic they contain. Table 2 list some
of the topics.
Topic N
Analogue and Digital Systems Architectures 2
Artificial Intelligence 26
Audio 12
Brain Computer Interface 18
Computational Finance Economics and Management 1
Computational Intelligence 10
. . . . . .
Table 1: Distribution of topics - N denotes the number of
topics for the corresponding general topic area.
High-Speed Lasers And Photodetectors
Human Behaviour And The Psychology
Human Motion Tracking
Human-Centred Robotics
Hybrid Heuristics
Hybrid Intelligent Systems Which Include Neuro-Fuzzy Systems
Hypercomplex Algebras And Fourier Transforms
Hypercomplex Fourier Transforms And Filters
Table 2: Some topics/queries
The measure used to test the system is recall at
the following values {3, 5, 7, 10, 15, 20}. We
also measure Mean Average Precision at rank 20
(MAP@20).
5 Results and Discussion
Table 3 shows the system results where BL is the
baseline result. There are two main findings. First
of all, the database-driven approach outperforms
the data-driven approach. Secondly, our approach
which applies a grading of results based on prox-
imity between queries and potential expert names
significantly outperforms the baseline approach that
only considers frequency, that is true for both formu-
lae we apply when ranking the results (using paired
t-tests applied to MAP with p<0.0001). However,
the differences between cm1 and cm2 tend not to be
significantly different.
BL cm1 cm2
NER DB NER DB NER DB
R@3 0.47 0.48 0.49 0.76 0.58 0.79
R@5 0.56 0.60 0.58 0.83 0.68 0.86
R@7 0.61 0.64 0.62 0.87 0.72 0.88
R@10 0.65 0.69 0.68 0.89 0.78 0.90
R@15 0.69 0.72 0.74 0.91 0.80 0.91
R@20 0.71 0.75 0.76 0.92 0.82 0.93
MAP 0.20 0.28 0.50 0.61 0.52 0.66
Table 3: Performance Measures
It is perhaps important to mention that our data is
fairly clean. More noise would make the creation of
relational database more difficult. In that case the
data-driven approach may become more appropri-
ate.
6 Conclusion
The main objective of this work was to explore
expert-finding in a university domain, an area that
has to the best of our knowledge so far attracted
little attention in the literature. The main finding
is that a database-driven approach (utilising a fixed
set of known experts) outperforms a data-driven ap-
proach which is based on automatic named-entity
recognition. Furthermore, exploiting proximity be-
tween query and candidate outperforms a straight
frequency measure.
There are a number of directions for future work.
For example, modelling the user background and
interests could increase the system?s effectiveness.
Some more realistic end-user studies could be used
to evaluate the systems. Consideration could be
given to term dependence and positional models as
in Metzler and Croft (2005), which might improve
our proximity-based scoring function. Finaly, our
gold standard collection penalises a data-driven ap-
proach, which might offer a broader range of ex-
perts. We will continue this line of work using
both technical evaluation measures as well as user-
focused evaluations.
5
References
K. Balog and M. de Rijke. Finding experts and their details in e-mail
corpora. In Proceedings of the 15th international conference on
World Wide Web, pages 1035?1036. ACM, 2006a.
K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert
finding in enterprise corpora. In Proceedings of the 29th annual
international ACM SIGIR conference on Research and development
in information retrieval, pages 43?50. ACM, 2006b.
I. Becerra-Fernandez. Facilitating the online search of experts at NASA
using expert seeker people-finder. In Proceedings of the 3rd Interna-
tional Conference on Practical Aspects of Knowledge Management
(PAKM), Basel, Switzerland, 2000.
S.P. Borgatti and R. Cross. A relational view of information seeking and
learning in social networks. Management science, 49(4):432?445,
2003.
C.S. Campbell, P.P. Maglio, A. Cozzi, and B. Dom. Expertise identifica-
tion using email communications. In Proceedings of the twelfth in-
ternational conference on Information and knowledge management,
pages 528?531. ACM, 2003.
Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at enterprise
track of trec 2005. In 14th Text Retrieval Conference (TREC 2005),
2005.
N. Craswell, D. Hawking, A.M. Vercoustre, and P. Wilkins. P@ noptic
expert: Searching for experts not just for documents. In Ausweb
Poster Proceedings, Queensland, Australia, 2001.
N. Craswell, A.P. de Vries, and I. Soboroff. Overview of the TREC-
2005 enterprise track. In TREC 2005 Conference Notebook, pages
199?205, 2005.
B. Dom, I. Eiron, A. Cozzi, and Y. Zhang. Graph-based ranking algo-
rithms for e-mail expertise analysis. In Proceedings of the 8th ACM
SIGMOD workshop on Research issues in data mining and knowl-
edge discovery, pages 42?48. ACM, 2003.
S.T. Dumais and J. Nielsen. Automating the assignment of submitted
manuscripts to reviewers. In Proceedings of the 15th annual inter-
national ACM SIGIR conference on Research and development in
information retrieval, pages 233?244. ACM, 1992.
H. Fang and C.X. Zhai. Probabilistic models for expert finding. Ad-
vances in Information Retrieval, pages 418?430, 2007.
K. Hofmann, K. Balog, T. Bogers, and M. de Rijke. Contextual fac-
tors for finding similar experts. Journal of the American Society for
Information Science and Technology, 61(5):994?1014, 2010.
M. Karimzadehgan, R. White, and M. Richardson. Enhancing expert
finding using organizational hierarchies. Advances in Information
Retrieval, pages 177?188, 2009.
C. Law and E. Ngai. An empirical study of the effects of knowledge
sharing and learning behaviors on firm performance. Expert Systems
with Applications, 34(4):2342?2349, 2008.
M. Li, L. Liu, and C. Li. An approach to expert recommendation based
on fuzzy linguistic method and fuzzy text classification in knowl-
edge management systems. Expert Systems with Applications, 38
(7):8586?8596, 2011.
X. Liu, W.B. Croft, and M. Koll. Finding experts in community-based
question-answering services. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge management,
pages 315?316. ACM, 2005.
W. Lu, S. Robertson, A. MacFarlane, and H. Zhao. Window-based
enterprise expert search. In Proceeddings of the 15th Text REtrieval
Conference (TREC 2006). NIST, 2006.
C. Macdonald and I. Ounis. Voting techniques for expert search. Knowl-
edge and information systems, 16(3):259?280, 2008.
C. Macdonald and R.W. White. Usefulness of click-through data in
expert search. In SIGIR, volume 9, pages 816?817, 2009.
M. Maybury, R. D?Amore, and D. House. Expert finding for collabo-
rative virtual environments. Communications of the ACM, 44(12):
55?56, 2001.
M.T. Maybury. Expert finding systems. MITRE Center for Integrated
Intelligence Systems Bedford, Massachusetts, USA, 2006.
D.W. McDonald and M.S. Ackerman. Just talk to me: a field study
of expertise location. In Proceedings of the 1998 ACM conference
on Computer supported cooperative work, pages 315?324. ACM,
1998.
D. Metzler and W.B. Croft. A markov random field model for term de-
pendencies. In Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in information re-
trieval, pages 472?479. ACM, 2005.
I. Nonaka and H. Takeuchi. The knowledge creating company: How
Japanese The knowledge creating company: How Japanese compa-
nies create the dynamics of innovation. Oxford University Press,
New York, 1995.
D. Petkova and W.B. Croft. Hierarchical language models for ex-
pert finding in enterprise corpora. In Tools with Artificial Intel-
ligence, 2006. ICTAI?06. 18th IEEE International Conference on,
pages 599?608. IEEE, 2006.
D. Petkova and W.B. Croft. Proximity-based document representation
for named entity retrieval. In Proceedings of the sixteenth ACM con-
ference on Conference on information and knowledge management,
pages 731?740. ACM, 2007.
P. Serdyukov and D. Hiemstra. Modeling documents as mixtures of
persons for expert finding. Advances in Information Retrieval, pages
309?320, 2008.
N.S. Shami, K. Ehrlich, and D.R. Millen. Pick me: link selection in
expertise search results. In Proceeding of the twenty-sixth annual
SIGCHI conference on Human factors in computing systems, pages
1089?1092. ACM, 2008.
E. Smirnova and K. Balog. A user-oriented model for expert finding.
Advances in Information Retrieval, pages 580?592, 2011.
J. Wang, Z. Chen, L. Tao, W.Y. Ma, and L. Wenyin. Ranking user?s
relevance to a topic through link analysis on web logs. In Proceed-
ings of the 4th international workshop on Web information and data
management, pages 49?54. ACM, 2002.
L. Woudstra and B. van den Hooff. Inside the source selection pro-
cess: Selection criteria for human information sources. Information
Processing & Management, 44(3):1267?1278, 2008.
K. Yang and S. Huh. Automatic expert identification using a text au-
tomatic expert identification using a text categorization technique in
knowledge management systems. Expert Systems with Expert Sys-
tems with Applications, 34(2):1445?1455, 2008.
J. Yao, J. Xu, and J. Niu. Using role determination and expert min-
ing in the enterprise environment. In Proceedings of the 2008 Text
REtrieval Conference (TREC 2008), 2008.
J. Zhang, J. Tang, and J. Li. Expert finding in a social network. Ad-
vances in Databases: Concepts, Systems and Applications, pages
1066?1069, 2007.
J. Zhu, X. Huang, D. Song, and S. Ru?ger. Integrating multiple doc-
ument features in language models for expert finding. Knowledge
and Information Systems, 23(1):29?54, 2010.
6
Addressing the Resource
Bottleneck to Create
Large-Scale Annotated Texts
Jon Chamberlain
University of Essex (UK)
email: jchamb@essex.ac.uk
Massimo Poesio
University of Essex (UK) & Universit? di Trento (Italy)
email: poesio@essex.ac.uk
Udo Kruschwitz
University of Essex (UK)
email: udo@essex.ac.uk
Abstract
Large-scale linguistically annotated resources have become available in
recent years. This is partly due to sophisticated automatic and semi-
automatic approaches that work well on specific tasks such as part-of-
speech tagging. For more complex linguistic phenomena like anaphora
resolution there are no tools that result in high-quality annotations with-
out massive user intervention. Annotated corpora of the size needed for
modern computational linguistics research cannot however be created by
small groups of hand annotators. The ANAWIKI project strikes a balance
between collecting high-quality annotations from experts and applying a
game-like approach to collecting linguistic annotation from the general
Web population. More generally, ANAWIKI is a project that explores to
what extend expert annotations can be substituted by a critical mass of
non-expert judgements.
375
376 Chamberlain, Poesio, and Kruschwitz
1 Introduction
Syntactically annotated language resources have long been around, but the greatest
obstacle to progress towards systems able to extract semantic information from text
is the lack of semantically annotated corpora large enough to be used to train and
evaluate semantic interpretation methods. Recent efforts to create resources to sup-
port large evaluation initiatives in the USA such as Automatic Context Extraction
(ACE), Translingual Information Detection, Extraction and Summarization (TIDES),
and GALE are beginning to change this, but just at a point when the community is
beginning to realize that even the 1M word annotated corpora created in substantial
efforts such as Prop-Bank (Palmer et al, 2005) and the OntoNotes initiative (Hovy
et al, 2006) are likely to be too small.
Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to
be prohibitively expensive. Such a large hand-annotation effort would be even less
sensible in the case of semantic annotation tasks such as coreference or wordsense
disambiguation, given on the one side the greater difficulty of agreeing on a ?neutral?
theoretical framework, on the other the difficulty of achieving more than moderate
agreement on semantic judgments (Poesio and Artstein, 2005).
The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphor-
ically annotated resources (Poesio et al, 2008) by taking advantage of the collabora-
tion of the Web community, both through co-operative annotation efforts using tra-
ditional annotation tools and through the use of game-like interfaces. This makes
ANAWIKI a very ambitious project. It is not clear to what extend expert annotations
can in fact be substituted by those judgements submitted by the general public as part
of a game. If successful, ANAWIKI will actually be more than just an anaphora anno-
tation tool. We see it as a framework aimed at creating large-scale annotated corpora
in general.
2 Creating Resources through Web Collaboration
Large-scale annotation of low-level linguistic information (part-of-speech tags) be-
gan with the Brown Corpus, in which very low-tech and time consuming methods
were used; but already for the creation of the British National Corpus (BNC), the first
100M-word linguistically annotated corpus, a faster methodology was developed con-
sisting of preliminary annotation with automatic methods followed by partial hand-
correction (Burnard, 2000). Medium and large-scale semantic annotation projects
(coreference, wordsense) are a fairly recent innovation in Computational Linguistics
(CL). The semi-automatic annotation methodology cannot yet be used for this type of
annotation, as the quality of, for instance, coreference resolvers is not yet high enough
on general text.
Collective resource creation on the Web offers a different way to the solution of
this problem. Wikipedia is perhaps the best example of collective resource creation,
but it is not an isolated case. The willingness of Web users to volunteer on the Web
extends to projects to create resources for Artificial Intelligence. One example is the
OpenMind Commonsense project, a project to mine commonsense knowledge (Singh,
2002) to which 14,500 participants contributed nearly 700,000 sentences. A more
1http://www.anawiki.org
Addressing the Resource Bottleneck to Create Annotated Texts 377
recent, and perhaps more intriguing, development is the use of interactive game-style
interfaces to collect knowledge such as von Ahn et al (2006). Perhaps the best known
example of this approach is the ESP game, a project to label images with tags through
a competitive game (von Ahn, 2006); 13,500 users played the game, creating 1.3M
labels in 3 months. If we managed to attract 15,000 volunteers, and each of them were
to annotate 10 texts of 700 words, we would get a corpus of the size of the BNC.
ANAWIKI builds on the proposals for marking anaphoric information allowing for
ambiguity developed in ARRAU (Poesio and Artstein, 2005) and previous projects.
The ARRAU project found that (i) using numerous annotators (up to 20 in some ex-
periments) leads to a much more robust identification of the major interpretation al-
ternatives (although outliers are also frequent); and (ii) the identification of alternative
interpretations is much more frequently a case of implicit ambiguity (each annotator
identifies only one interpretation, but these are different) than of explicit ambiguity
(annotators identifying multiple interpretations). The ARRAU project also developed
methods to analyze collections of such alternative interpretations and to identify out-
liers via clustering that will be exploited in this project.
Figure 1: A screenshot of the Serengeti expert annotation tool.
3 Annotation Tools
Attempts to create hand annotated corpora face the dilemma of either going for the
traditional CL approach of high-quality annotation (of limited size) by experts or to
involve a large population of non-experts which could result in large-scale corpora
of inferior quality. The ANAWIKI project bridges this gap by combining both ap-
proaches to annotate the data: an expert annotation tool and a game interface. Both
378 Chamberlain, Poesio, and Kruschwitz
Figure 2: A screenshot of the Game Interface (Annotation Mode).
tools are essential parts of ANAWIKI.We briefly describe both, with a particular focus
on the game interface.
3.1 Expert Annotation Tool
An expert annotation tool is used to obtain Gold Standard annotations from computa-
tional linguists. In the case of anaphora annotationwe use the Serengeti tool developed
at the University of Bielefeld (St?hrenberg et al, 2007). The anaphoric annotation of
markables within this environment will be very detailed and will serve as a training
corpus as well as quality check for the second tool (see below). Figure 1 is a screen-
shot of this interface.
3.2 Game Interface
A game interface is used to collect annotations from the general Web population. The
game interface integrates with the database of the expert annotation tool but aims to
collect large-scale (rather than detailed) anaphoric relations. Users are simply asked
to assign an anaphoric link but are not asked to specify what type (or what features)
are present.
Phrase Detectives2 is a game offering a simple user interface for non-expert users
to learn how to annotate text and to make annotation decisions. The goal of the game
is to identify relationships between words and phrases in a short text. Markables are
2http://www.phrasedetectives.org
Addressing the Resource Bottleneck to Create Annotated Texts 379
identified in the text by automatic pre-processing. There are 2 ways to annotate within
the game: by selecting the markable that is the antecedent of the anaphor (Annotation
Mode ? see Figure 2); or by validating a decision previously submitted by another
user (Validation Mode). One motivation for Validation Mode is that we anticipate it
to be twice as fast as Annotation Mode (Chklovski and Gil, 2005).
Users begin the game at the training level and are given a set of annotation tasks
created from the Gold Standard. They are given feedback and guidance when they
select an incorrect answer and points when they select the correct answer. When
the user gives enough correct answers they graduate to annotating texts that will be
included in the corpus. Occasionally, a graduated user will be covertly given a Gold
Standard text to annotate. This is the foundation of the user rating system used to
judge the quality of the user?s annotations.
The game is designed to motivate users to annotate the text correctly by using com-
parative scoring (awarding points for agreeing with the Gold Standard), and retroac-
tive scoring (awarding points to the previous user if they are agreed with by the current
user). Using leader boards and assigning levels for points has been proven to be an
effective motivator, with users often using these as targets (von Ahn, 2006). The game
interface is described in more detail elsewhere (Chamberlain et al, 2008).
4 Challenges
We are aiming at a balanced corpus, similar to the BNC, that includes texts from
Project Gutenberg, the Open American National Corpus, the Enron corpus and other
freely available sources. The chosen texts are stripped of all presentation formatting,
HTML and links to create the raw text. This is automatically parsed to extract mark-
ables consisting of noun phrases. The resulting XML format is stored in a relational
database that can be used in both the expert annotation tool and the game.
There are a number of challenges remaining in the project. First of all, the fully
automated processing of a substantial (i.e. multi-million) word corpus comprising
more than just news articles turned out to be non-trivial both in terms of robustness of
the processing tools as well as in terms of linguistic quality.
A second challenge is to recruit enough volunteers to annotate a 100 million word
corpus within the timescale of the project. It is our intention to use social networking
sites (including Facebook, Bebo, and MySpace) to attract volunteers to the game and
motivate participation by providing widgets (code segments that display the user?s
score and links to the game) to add to their profile pages.
Finally, the project?s aim is to generate a sufficiently large collection of annotations
from which semantically annotated corpora can be constructed. The usefulness of the
created resources can only be proven, for example, by training anaphora resolution
algorithms on the resulting annotations. This will be future work.
5 Next Steps
We are currently in the process of building up a critical mass of source texts. Our aim
is to have a corpus size of 1M words by September 2008. By this time we also intend
having a multilingual user interface (initially English, Italian and German) with the
capacity to annotate texts in different languages although this is not the main focus.
380 Chamberlain, Poesio, and Kruschwitz
In the future we will be considering extending the interface to include different anno-
tation tasks, for example marking coreference chains or Semantic Web mark-up. We
would like to present the game interface to gain feedback from the linguistic commu-
nity.
Acknowledgements
ANAWIKI is funded by EPSRC (EP/F00575X/1). Thanks to Daniela Goecke, Maik
St?hrenberg, Nils Diewald and Dieter Metzing. We also want to thank all volunteers
who have already contributed to the project and the reviewers for valuable feedback.
References
Burnard, L. (2000). The British National Corpus Reference guide. Technical report,
Oxford University Computing Services, Oxford.
Chamberlain, J., M. Poesio, and U. Kruschwitz (2008). Phrase Detectives: A Web-
based Collaborative Annotation Game. In Proceedings of the International Con-
ference on Semantic Systems (I-Semantics?08), Graz. Forthcoming.
Chklovski, T. and Y. Gil (2005). Improving the design of intelligent acquisition in-
terfaces for collecting world knowledge from web contributors. In Proceedings of
K-CAP ?05, pp. 35?42.
Hovy, E., M.Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). OntoNotes:
The 90% Solution. In Proceedings of HLT-NAACL06.
Palmer, M., D. Gildea, and P. Kingsbury (2005). The proposition bank: An annotated
corpus of semantic roles. Computational Linguistics 31(1), 71?106.
Poesio, M. and R. Artstein (2005). The reliability of anaphoric annotation, recon-
sidered: Taking ambiguity into account. In Proceedings of the ACL Workshop on
Frontiers in Corpus Annotation, pp. 76?83.
Poesio, M., U. Kruschwitz, and J. Chamberlain (2008). ANAWIKI: Creating anaphor-
ically annotated resources through Web cooperation. In Proceedings of LREC?08,
Marrakech.
Singh, P. (2002). The public acquisition of commonsense knowledge. In Proceedings
of the AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, Palo Alto, CA.
St?hrenberg, M., D. Goecke, N. Diewald, A. Mehler, and I. Cramer (2007). Web-
based annotation of anaphoric relations and lexical chains. In Proceedings of the
ACL Linguistic Annotation Workshop, pp. 140?147.
von Ahn, L. (2006). Games with a purpose. Computer 39(6), 92?94.
von Ahn, L., R. Liu, and M. Blum (2006). Peekaboom: a game for locating objects in
images. In Proceedings of CHI ?06, pp. 55?64.
