Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 793?800
Manchester, August 2008
Metric Learning for Synonym Acquisition
Nobuyuki Shimizu
Information Technology Center
University of Tokyo
shimizu@r.dl.itc.u-tokyo.ac.jp
Masato Hagiwara
Graduate School of Information Science
Nagoya University
hagiwara@kl.i.is.nagoya-u.ac.jp
Yasuhiro Ogawa and Katsuhiko Toyama
Graduate School of Information Science
Nagoya University
{yasuhiro,toyama}@kl.i.is.nagoya-u.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
n3@dl.itc.u-tokyo.ac.jp
Abstract
The distance or similarity metric plays an
important role in many natural language
processing (NLP) tasks. Previous stud-
ies have demonstrated the effectiveness of
a number of metrics such as the Jaccard
coefficient, especially in synonym acqui-
sition. While the existing metrics per-
form quite well, to further improve perfor-
mance, we propose the use of a supervised
machine learning algorithm that fine-tunes
them. Given the known instances of sim-
ilar or dissimilar words, we estimated the
parameters of the Mahalanobis distance.
We compared a number of metrics in our
experiments, and the results show that the
proposed metric has a higher mean average
precision than other metrics.
1 Introduction
Accurately estimating the semantic distance be-
tween words in context has applications for
machine translation, information retrieval (IR),
speech recognition, and text categorization (Bu-
danitsky and Hirst, 2006), and it is becoming
clear that a combination of corpus statistics can be
used with a dictionary, thesaurus, or other knowl-
edge source such as WordNet or Wikipedia, to in-
crease the accuracy of semantic distance estima-
tion (Mohammad and Hirst, 2006). Although com-
piling such resources is labor intensive and achiev-
ing wide coverage is difficult, these resources to
some extent explicitly capture semantic structures
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of concepts and words. In contrast, corpus statis-
tics achieve wide coverage, but the semantic struc-
ture of a concept is only implicitly represented in
the context. Assuming that two words are semanti-
cally closer if they occur in similar contexts, statis-
tics on the contexts of words can be gathered and
compared for similarity, by using a metric such as
the Jaccard coefficient.
Our proposal is to extend and fine-tune the latter
approach with the training data obtained from the
former. We apply metric learning to this task. Al-
though still in their infancy, distance metric learn-
ing methods have undergone rapid development in
the field of machine learning. In a setting simi-
lar to semi-supervised clustering, where known in-
stances of similar or dissimilar objects are given,
a metric such as the Mahalanobis distance can be
learned from a few data points and tailored to fit a
particular purpose. Although classification meth-
ods such as logistic regression now play impor-
tant roles in natural language processing, the use
of metric learning has yet to be explored.
Since popular current methods for synonym ac-
quisition require no statistical learning, it seems
that supervised machine learning should easily
outperform them. Unfortunately, there are obsta-
cles to overcome. Since metric learning algorithms
usually learn the parameters of a Mahalanobis dis-
tance, the number of parameters is quadratic to the
number of features. They learn how two features
should interact to produce the final metric. While
traditional metrics forgo examining of the interac-
tions entirely, in applying metrics such as Jaccard
coefficient, it is not uncommon nowadays to use
more than 10,000 features, a number that a typical
metric learner is incapable of processing. Thus we
have two options: one is to find the most impor-
tant features and model the interactions between
793
them, and the other is simply to use a large number
of features. We experimentally examined the two
options and found that metric learning is useful in
synonym acquisition, despite it utilizing fewer fea-
tures than traditional methods.
The remainder of this paper is organized as fol-
lows: in section 2, we review prior work on syn-
onym acquisition and metric learning. In section
3, we introduce the Mahalanobis distance metric
and a learning algorithm based on this metric. In
section 4 and 5, we explain the experimental set-
tings and propose the use of normalization to make
the Mahalanobis distances work in practice, and
then in section 6, we discuss issues we encountered
when applying this metric to synonym acquisition.
We conclude in section 7.
2 Prior Work
As this paper is based on two different lines of re-
search, we first review the work in synonym acqui-
sition, and then review the work in generic metric
learning. To the best of the authors? knowledge,
none of the metric learning algorithms have been
applied to automatic synonym acquisition.
Synonym relation is important lexical knowl-
edge for many natural language processing
tasks including automatic thesaurus construction
(Croach and Yang, 1992; Grefenstette, 1994) and
IR (Jing and Croft, 1994). Various methods (Hin-
dle, 1990; Lin, 1998) of automatically acquiring
synonyms have been proposed. They are usu-
ally based on the distributional hypothesis (Har-
ris, 1985), which states that semantically simi-
lar words share similar contexts, and they can be
roughly viewed as the combinations of two steps:
context extraction and similarity calculation. The
former extracts useful features from the contexts of
words, such as surrounding words or dependency
structure. The latter calculates how semantically
similar two given words are based on similarity or
distance metrics.
Many studies (Lee, 1999; Curran and Moens,
2002; Weeds et al, 2004) have investigated
similarity calculation, and a variety of dis-
tance/similarity measures have already been com-
pared and discussed. Weeds et al?s work is espe-
cially useful because it investigated the character-
istics of metrics based on a few criteria such as
the relative frequency of acquired synonyms and
clarified the correlation between word frequency,
distributional generality, and semantic generality.
However, all of the existing research conducted
only a posteriori comparison, and as Weeds et al
pointed out, there is no one best measure for all ap-
plications. Therefore, the metrics must be tailored
to applications, even to corpora and other settings.
We next review the prior work in generic metric
learning. Most previous metric learning methods
learn the parameters of the Mahalanobis distance.
Although the algorithms proposed in earlier work
(Xing et al, 2002; Weinberger et al, 2005; Glober-
son and Roweis, 2005) were shown to yield excel-
lent classification performance, these algorithms
all have worse than cubic computational complex-
ity in the dimensionality of the data. Because of
the high dimensionality of our objects, we opted
for information-theoretic metric learning proposed
by (Davis et al, 2007). This algorithm only uses
an operation quadratic in the dimensionality of the
data.
Other work on learning Mahalanobis metrics in-
cludes online metric learning (Shalev-Shwartz et
al., 2004), locally-adaptive discriminative methods
(Hastie and Tibshirani, 1996), and learning from
relative comparisons (Schutz and Joahims, 2003).
Non-Mahalanobis-based metric learning methods
have also been proposed, though they seem to suf-
fer from suboptimal performance, non-convexity,
or computational complexity. Examples include
neighborhood component analysis (Goldberger et
al., 2004).
3 Metric Learning
3.1 Problem Formulation
To set the context for metric learning, we first de-
scribe the objects whose distances from one an-
other we would like to know. As noted above re-
garding the distributional hypothesis, our object is
the context of a target word. To represent the con-
text, we use a sparse vector in Rd. Each dimension
of an input vector represents a feature of the con-
text, and its value corresponds to the strength of
the association. The vectors of two target words
represent their contexts as points in multidimen-
sional feature-space. A suitable metric (for exam-
ple, Euclidean) defines the distance between the
two points, thereby estimating the semantic dis-
tance between the target words.
Given points x
i
, x
j
? R
d
, the (squared) Ma-
halanobis distance between them is parameter-
ized by a positive definite matrix A as follows
d
A
(x
i
, x
j
) = (x
i
? x
j
)
?
A(x
i
? x
j
). The Ma-
794
halanobis distance is a straightforward extension
of the standard Euclidean distance. If we let A
be the identity matrix, the Mahalanobis distance
reduces to the Euclidean distance. Our objective
is to obtain the positive definite matrix A that pa-
rameterizes the Mahalanobis distance, so that the
distance between the vectors of two synonymous
words is small, and the distance between the vec-
tors of two dissimilar words is large. Stated more
formally, the Mahalanobis distance between two
similar points must be smaller than a given upper
bound, i.e., d
A
(x
i
, x
j
) ? u for a relatively small
value of u. Similarly, two points are dissimilar if
d
A
(x
i
, x
j
) ? l for sufficiently large l.
As we discuss below, we were able to use
the Euclidean distance to acquire synonyms quite
well. Therefore, we would like the positive definite
matrix A of the Mahalanobis distance to be close to
the identity matrix I . This keeps the Mahalanobis
distance similar to the Euclidean distance, which
would help to prevent overfitting the data. To op-
timize the matrix, we follow the information theo-
retic metric learning approach described in (Davis
et al, 2007). We summarize the problem formula-
tion advocated by this approach in this section and
the learning algorithm in the next section.
To define the closeness between A and I , we
use a simple bijection (up to a scaling function)
from the set of Mahalanobis distances to the set
of equal mean multivariate Gaussian distributions.
Without loss of generalization, let the equal mean
be ?. Then given a Mahalanobis distance pa-
rameterized by A, the corresponding Gaussian is
p(x;A) =
1
Z
exp(?
1
2
d
A
(x, ?)) where Z is the
normalizing factor. This enables us to measure
the distance between two Mahalanobis distances
with the Kullback-Leibler (KL) divergence of two
Gaussians:
KL(p(x; I)||p(x;A)) =
?
p(x, I) log
(
p(x; I)
p(x;A)
)
dx.
Given pairs of similar points S and pairs of dis-
similar points D, the optimization problem is:
min
A
KL(p(x; I)||p(x;A))
subject to d
A
(x
i
, x
j
) ? u (i, j) ? S
d
A
(x
i
, x
j
) ? l (i, j) ? D
3.2 Learning Algorithm
(Davis and Dhillon, 2006) has shown that the
KL divergence between two multivariate Gaus-
sians can be expressed as the convex combination
of a Mahalanobis distance between mean vectors
and the LogDet divergence between the covariance
matrices. The LogDet divergence equals
D
ld
(A,A
0
) = tr(AA
?1
0
)? log det(AA
?1
0
)? n
for n by n matrices A,A
0
. If we assume the means
of the Gaussians to be the same, we have
KL(p(x;A
0
||p(x,A)) =
1
2
D
ld
(A,A
0
)
The optimization problem can be restated as
min
A0
D
ld
(A, I)
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? u (i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? l (i, j) ? D
We then incorporate slack variables into the for-
mulation to guarantee the existence of a feasible
solution for A. The optimization problem be-
comes:
min
A0
D
ld
(A, I) + ?D
ld
(diag(?), diag(?
0
))
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? D
where c(i, j) is the index of the (i, j)-th constraint
and ? is a vector of slack variables whose compo-
nents are initialized to u for similarity constraints
and l for dissimilarity constraints. The tradeoff
between satisfying the constraints and minimiz-
ing D
ld
(A, I) is controlled by the parameter ?.
To solve this optimization problem, the algorithm
shown in Algorithm 3.1 repeatedly projects the
current solution onto a single constraint.
This completes the summary of (Davis et al,
2007).
4 Experimental Settings
In this section, we describe the experimental set-
tings including the preprocessing of data and fea-
tures, creation of the query word sets, and settings
of the cross validation.
4.1 Features
We used a dependency structure as the context for
words because it is the most widely used and one
of the best performing contextual information in
the past studies (Ruge, 1997; Lin, 1998). As the
extraction of an accurate and comprehensive de-
pendency structure is in itself a complicated task,
the sophisticated parser RASP Toolkit 2 (Briscoe
et al, 2006) was utilized to extract this kind of
word relation.
Let N(w, c) be the raw cooccurrence count of
word w and context c, the grammatical relation
795
Algorithm
3.1: INFORMATION THEORETIC METRIC LEARNING
Input :
X(d by n matrix), I(identity matrix)
S(set of similar pairs),D(set of dissimilar pairs)
?(slack parameter), c(constraint index function)
u, l(distance thresholds)
Output :
A(Mahalanobis matrix)
A := I
?
ij
:= 0
?
c(i,j)
:= u for (i, j) ? S; otherwise, ?
c(i,j)
:= l
repeat
Pick a constraint (i, j) ? S or (i, j) ? D
p := (x
i
? x
j
)
?
A(x
i
? x
j
)
? := 1 if (i, j) ? S,?1 otherwise.
? := min(?
ij
,
?
2
(
1
p
?
?
?
c(i,j)
))
? := ??/(1? ???
c(i,j)
)
?
c(i,j)
:= ??
c(i,j)
/(? + ???
c(i,j)
)
?
ij
:= ?
ij
? ?
A := A + ?A(x
i
? x
j
)(x
i
? x
j
)
?
A
until convergence
return (A)
in which w occurs. These raw counts were ob-
tained from New York Times articles (July 1994)
extracted from English Gigaword 1. The section
consists of 7,593 documents and approx. 5 million
words. As discussed below, we limited the vocab-
ulary to the nouns in the Longman Defining Vo-
cabulary (LDV) 2. The features were constructed
by weighting them using pointwise mutual infor-
mation: wgt(w, c) = PMI(w, c) = log P (w,c)
P (w)P (c)
.
Co-occurrence data constructed this way can
yield more than 10,000 context types, rendering
metric learning impractical. As the applications
of feature selection reduce the performance of the
baseline metrics, we tested them in two different
settings: with and without feature selection. To
mitigate this problem, we applied a feature selec-
tion technique to reduce the feature dimensional-
ity. We selected features using two approaches.
The first approach is a simple frequency cutoff, ap-
plied as a pre-processing to filter out words and
contexts with low frequency and to reduce com-
putational cost. Specifically, all words w such
that
?
c
N(w, c) < ?
f
and contexts c such that
?
w
N(w, c) < ?
f
, with ?
f
= 5, are removed from
the co-occurrence data.
The second approach is feature selection by con-
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
2http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
text importance (Hagiwara et al, 2008). First, the
context importance score for each context type is
calculated, and then the least important context
types are eliminated, until a desired numbers of
them remains. To measure the context importance
score, we used the number of unique words the
context co-occurs with: df(c) = |{w|N(w, c) >
0}|. We adopted this context selection criterion
on the assumption that the contexts shared by
many words should be informative, and the syn-
onym acquisition performance based on normal
distributional similarity calculation retains its orig-
inal level of performance until up to almost 90%
of context types are eliminated (Hagiwara et al,
2008). In our experiment, we selected features
rather aggressively, finally using only 10% of the
original contexts. These feature reduction oper-
ations reduced the dimensionality to a figure as
small as 1,281, while keeping the performance loss
at a minimum.
4.2 Similarity and Distance Functions
We compared seven similarity/distance functions
in our experiments: cosine similarity, Euclidean
distance, Manhattan distance, Jaccard coeffi-
cient, vector-based Jaccard coefficient (Jaccardv),
Jensen-Shannon Divergence (JS) and skew diver-
gence (SD99). We first define some notations. Let
C(w) be the set of context types that co-occur with
word w, i.e., C(w) = {c|N(w, c) > 0}, and w
i
be
the feature vector corresponding to word w, i.e.,
w
i
= [wgt(w
i
, c
1
) ... wgt(w
i
, c
M
)]
?
. The first
three, the cosine, Euclidean and Manhattan dis-
tance, are vector-based metrics.
cosine similarity
w
1
?w
2
||w
1
|| ? ||w
2
||
Euclidean distance
?
?
c?C(w
1
)?C(w
2
)
(wgt(w
1
, c)? wgt(w
2
, c))
2
Manhattan distance
?
c?C(w
1
)?C(w
2
)
|wgt(w
1
, c)? wgt(w
2
, c)|
Jaccard coefficient
?
c?C(w
1
)?C(w
2
)
min(wgt(w
1
, c),wgt(w
2
, c))
?
c?C(w
1
)?C(w
2
)
max(wgt(w
1
, c),wgt(w
2
, c))
,
796
vector-based Jaccard coefficient (Jaccardv)
w
i
?w
j
||w
i
|| + ||w
j
|| ?w
i
?w
j
.
Jensen-Shannon divergence (JS)
1
2
{KL(p
1
||m) + KL(p
2
||m)}, m = p
1
+ p
2
.
JS and SD99 are based on the KL divergence, so
the vectors must be normalized to form a probabil-
ity distribution. For notational convenience, we let
p
i
be the probability distribution representation of
feature vector w
i
, i.e., p
i
(c) = N(w
i
, c)/N(w
i
).
While the KL divergence suffers from the so-called
zero-frequency problem, a symmetric version of
the KL divergence called the Jensen-Shannon di-
vergence naturally avoids it.
skew divergence (SD99)
KL(p
1
||?p
2
+ (1? ?)p
1
).
As proposed by (Lee, 2001), the skew diver-
gence also avoids the zero-frequency problem by
mixing the original distribution with the target dis-
tribution. Parameter ? is set to 0.99.
4.3 Query Word Set and Cross Validation
To formalize the experiments, we must prepare a
set of query words for which synonyms are known
in advance. We chose the Longman Defining
Vocabulary (LDV) as the candidate set of query
words. For each word in the LDV, we consulted
three existing thesauri: Roget?s Thesaurus (Ro-
get, 1995), Collins COBUILD Thesaurus (Collins,
2002), and WordNet (Fellbaum, 1998). Each LDV
word was looked up as a noun to obtain the union
of synonyms. After removing words marked ?id-
iom?, ?informal? or ?slang? and phrases com-
prised of two or more words, this union was used
as the reference set of query words. LDV words for
which no noun synonyms were found in any of the
reference thesauri were omitted. From the remain-
ing 771 LDV words, there were 231 words that had
five or more synonyms in the combined thesaurus.
We selected these 231 words to be the query words
and distributed them into five partitions so as to
conduct five-fold cross validation. Four partitions
were used in training, and the remaining partition
was used in testing. For each fold, we created
the training set from four partitions as follows; for
each query word in the partitions, we randomly se-
lected five synonymous words and added the pairs
of query words and synonymous words to S, the
set of similar pairs. Similarly, five pairs of query
words and dissimilar words were randomly added
to D, the set of dissimilar pairs. The training set
for each fold consisted of S and D. Since a learner
trained on an imbalanced dataset may not learn
to discriminate enough between classes, we sam-
pled dissimilar pairs to create an evenly distributed
training dataset.
To make the evaluation realistic, we used a dif-
ferent method to create the test set: we paired each
query word with each of the 771 remaining words
to form the test set. Thus, in each fold, the training
set had an equal number of positive and negative
pairs, while in the test set, negative pairs outnum-
bered the positive pairs. While this is not a typical
setting for cross validation, it renders the evalua-
tion more realistic since an automatic synonym ac-
quisition system in operation must be able to pick
a few synonyms from a large number of dissimilar
words.
The meta-parameters of the metric learning
model were simply set u = 1, l = 2 and ? = 1.
Each training set consisted of 1,850 pairs, and the
test set consisted of 34,684 pairs. Since we con-
ducted five-fold cross validation, the reported per-
formance in this paper is actually a summary over
different folds.
4.4 Evaluation Measures
We used an evaluation program for KDD Cup
2004 (Caruana et al, 2004) called Perf to measure
the effectiveness of the metrics in acquiring syn-
onyms. To use the program, we used the following
formula to convert each distance metric to a simi-
larity metric. s(x
i
, x
j
) = 1/(1 + exp(d(x
i
, x
j
))).
Below, we summarize the three measures we
used: Mean Average Precision, TOP1, and Aver-
age Rank of Last Synonym.
Mean Average Precision (APR)
Perf implements a definition of average preci-
sion sometimes called ?expected precision?. Perf
calculates the precision at every recall where it is
defined. For each of these recall values, Perf finds
the threshold that produces the maximum preci-
sion, and takes the average over all of the recall
values greater than 0. Average precision is mea-
sured on each query, and then the mean of each
query?s average precision is used as the final met-
ric. A mean average precision of 1.0 indicates per-
fect prediction. The lowest possible mean average
797
precision is 0.0.
Average Rank of Last Synonym (RKL)
As in other evaluation measures, synonym can-
didates are sorted by predicted similarity, and this
metric measures how far down the sorted cases we
must go to find the last true synonym. A rank of
1 indicates that the last synonym is placed in the
top position. Given a query word, the highest ob-
tainable rank is N if there are N synonyms in the
corpus. The lower this measure is the better. Aver-
age ranks near 771 indicate poor performance.
TOP1
In each query, synonym candidates are sorted by
predicted similarity. If the word that ranks at the
top (highest similarity to the query word) is a true
synonym of the query word, Perf scores a 1 for
that query, and 0 otherwise. If there are ties, Perf
scores 0 unless all of the tied cases are synonyms.
TOP1 score ranges from 1.0 to 0.0. To achieve 1.0,
perfect TOP1 prediction, a similarity metric must
place a true synonym at the top of the sorted list
in every query. In the next section, we report the
mean of each query?s TOP1.
5 Results
The evaluations of the metrics are listed in Table
1. The figure on the left side of ? represents the
performance with 1,281 features, and that on the
right side with 12,812 features. Of all the met-
rics in Table 1, only the Mahalanobis L2 is trained
with the previously presented metric learning al-
gorithm. Thus, the values for the Mahalanobis
L2 are produced by the five-fold cross validation,
while the rest are given by the straight application
of the metrics discussed in Section 4.2 to the same
dataset. Strictly speaking, this is not a fair com-
parison, since we ought to compare a supervised
learning with a supervised learning. However, our
baseline is not the simple Euclidean distance; it
is the Jaccard coefficient and cosine similarity, a
handcrafted, best performing metric for synonym
acquisition, with 10 times as many features.
The computational resources required to obtain
the Mahalanobis L2 results were as follows: in the
training phase, each fold of cross validation took
about 80 iterations (less than one week) to con-
verge on a Xeon 5160 3.0GHz. The time required
to use the learned distance was a few hours at most.
At first, we were unable to perform competi-
tively with the Euclidean distance. As seen in Ta-
ble 1, the TOP1 measure of the Euclidean distance
is only 1.732%. This indicates that the likelihood
of finding the first item on the ranked list to be a
true synonym is 1.732%. The vector-based Jac-
card coefficient performs much better than the Eu-
clidean distance, placing a true synonym at the top
of the list 30.736% of the time.
Table 2 shows the Top 10 Words for Query
?branch?. The results for the Euclidean distance
rank ?hut? and other dissimilar words highly. This
is because the norm of such vectors is small, and in
a high dimensional space, the sparse vectors near
the origin are relatively close to many other sparse
vectors. To overcome this problem, we normal-
ized the input vectors by the L2 norm x? = x/||x||
This normalization enables the Euclidean distance
to perform very much like the cosine similarity,
since the Euclidean distance between points on a
sphere acts like the angle between the vectors. Sur-
prisingly, normalization by L2 did not affect other
metrics all that much; while the performances of
some metrics improved slightly, the L2 normaliza-
tion lowered that of the Jaccardv metric.
Once we learned the normalization trick, the
learned Mahalanobis distance consistently outper-
formed all other metrics, including the ones with
10 times more features, in all three evaluation
measures, achieving an APR of 18.66%, RKL of
545.09 and TOP1 of 45.455%.
6 Discussion
Examining the learned Mahalanobis matrix re-
vealed interesting features. The matrix essentially
shows the covariance between features. While it
was not as heavily weighted as the diagonal ele-
ments, we found that its positive non-diagonal el-
ements were quite interesting. They indicate that
some of the useful features for finding synonyms
are correlated and somewhat interchangeable. The
example includes a pair of features, (dobj begin
*) and (dobj end *). It was a pleasant surprise to
see that one implies the other. Among the diag-
onal elements of the matrix, one of the heaviest
features was being the direct object of ?by?. This
indicates that being the object of the preposition
?by? is a good indicator that two words are simi-
lar. A closer inspection of the NYT corpus showed
that this preposition overwhelmingly takes a per-
son or organization as its object, indicating that
words with this feature belong to the same class
of a person or organization. Similarly, the class
798
Metric APR RKL TOP1
Cosine 0.1184 ? 0.1324 580.27 ? 579.00 0.2987 ? 0.3160
Euclidean 0.0229 ? 0.0173 662.74 ? 695.71 0.0173 ? 0.0000
Euclidean L2 0.1182 ? 0.1324 580.30 ? 578.99 0.2943 ? 0.3160
Jaccard 0.1120 ? 0.1264 580.76 ? 579.51 0.2684 ? 0.2943
Jaccard L2 0.1113 ? 0.1324 580.29 ? 570.88 0.2640 ? 0.2987
Jaccardv 0.1189 ? 0.1318 580.50 ? 580.19 0.3073 ? 0.3030
Jaccardv L2 0.1184 ? 0.1254 580.27 ? 570.00 0.2987 ? 0.3160
JS 0.0199 ? 0.0170 681.97 ? 700.53 0.0129 ? 0.0000
JS L2 0.0229 ? 0.0173 679.21 ? 699.00 0.0303 ? 0.0086
Manhattan 0.0181 ? 0.0168 687.73 ? 701.47 0.0043 ? 0.0000
Manhattan L2 0.0185 ? 0.0170 686.56 ? 701.11 0.0043 ? 0.0086
SD99 0.0324 ? 0.1039 640.71 ? 588.16 0.0173 ? 0.2640
SD99 L2 0.0334 ? 0.1117 633.32 ? 586.78 0.0216 ? 0.2900
Mahalanobis L2 0.1866 545.09 0.4545
Table 1: Evaluation of Various Metrics, as Number of Features Increase from 1,281 to 12,812
Cosine Euclidean Euclidean L2 Jaccard Jaccardv Mahalanobis L2
1 (*) office hut (*) office (*) office (*) office (*) division
2 area wild area border area group
3 (*) division polish (*) division area (*) division (*) office
4 border thirst border plant border line
5 group hollow group (*) division group period
6 organization shout organization mouth organization organization
7 store fold store store store (*) department
8 mouth dear mouth circle mouth charge
9 plant hate plant stop plant world
10 home wake home track home body
(*) = a true synonym
Table 2: Top 10 Words for Query ?branch?
of words that ?to? and ?within?, take as an objects
were clear from the corpus: ?to? takes a person
or place, ?within? takes duration of time 3. Other
heavy features includes being the object of ?write?
or ?about?. While not obvious, we postulate that
having these words as a part of the context indi-
cates that a word is an event of some type.
7 Conclusion
We applied metric learning to automatic synonym
acquisition for the first time, and our experiments
showed that the learned metric significantly out-
performs existing similarity metrics. This outcome
indicates that while we must resort to feature se-
lection to apply metric learning, the performance
gain from the supervised learning is enough to off-
set the disadvantage and justify its usage in some
applications. This leads us to think that a com-
bination of the learned metric with unsupervised
metrics with even more features may produces the
best results. We also discussed interesting features
found in the learned Mahalanobis matrix. Since
3Interestingly, we note that not all prepositions were as
heavy: ?beyond? and ?without? were relatively light among
the diagonal elements. In the NYT corpus, the class of words
they take was not as clear as, for example, ?by?.
metric learning is known to boost clustering per-
formance in a semi-supervised clustering setting,
we believe these automatically identified features
would be helpful in assigning a target word to a
word class.
References
T. Briscoe, J. Carroll and R. Watson. 2006. The Sec-
ond Release of the RASP System. Proc. of the COL-
ING/ACL 2006 Interactive Presentation Sessions,
77?80.
T. Briscoe, J. Carroll, J. Graham and A. Copestake,
2002. Relational evaluation schemes. Proc. of the
Beyond PARSEVAL Workshop at the Third Interna-
tional Conference on Language Resources and Eval-
uation, 4?8.
A. Budanitsky and G. Hirst. 2006. Evaluat-
ing WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
R. Caruana, T. Jachims and L. Backstrom. 2004. KDD-
Cup 2004: results and analysis ACM SIGKDD Ex-
plorations Newslatter, 6(2):95?108.
C. J. Croach and B. Yang. 1992. Experiments in au-
tomatic statistical thesaurus construction. the 15th
799
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
77?88.
J. R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Workshop on Un-
supervised Lexical Acquisition. Proc. of the ACL
SIGLEX, 231?238.
J. V. Davis and I. S. Dhillon. 2006. Differential En-
tropic Clustering of Multivariate Gaussians. Ad-
vances in Neural Information Processing Systems
(NIPS).
J. V. Davis, B. Kulis, P. Jain, S. Sra and I. S. Dhillon.
2007. Information Theoretic Metric Learning. Proc.
of the International Conference on Machine Learn-
ing (ICML).
A. Globerson and S. Roweis. 2005. Metric Learning by
Collapsing Classes. Advances in Neural Information
Processing Systems (NIPS).
J. Goldberger, S. Roweis, G. Hinton and R. Salakhut-
dinov. 2004. Neighbourhood Component Analysis.
Advances in Neural Information Processing Systems
(NIPS).
G. Grefenstette. 1994. Explorations in Automatic The-
suarus Discovery. Kluwer Academic Publisher.
M. Hagiwara, Y. Ogawa, and K. Toyama. 2008. Con-
text Feature Selection for Distributional Similarity.
Proc. of IJCNLP-08, 553?560.
Z. Harris. 1985. Distributional Structure. Jerrold
J. Katz (ed.) The Philosophy of Linguistics. Oxford
University Press. 26?47.
T. Hastie and R. Tibshirani. 1996. Discriminant adap-
tive nearest neighbor classification. Pattern Analysis
and Machine Intelligence, 18, 607?616.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. of the ACL, 268?275.
J. J. Jiang and D. W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. Proceedings of International Conference on
Research on Computational Linguistics (ROCLING
X), Taiwan.
Y. Jing and B. Croft. 1994. An Association The-
saurus for Information Retrieval. Proc. of Recherche
d?Informations Assiste?e par Ordinateur (RIAO),
146?160.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?
774.
L. Lee. 1999. Measures of distributional similarity.
Proc. of the ACL, 23?32
L. Lee. 2001. On the Effectiveness of the Skew Diver-
gence for Statistical Language Analysis. Artificial
Intelligence and Statistics 2001, 65?72.
S. Mohammad and G. Hirst. 2006. Distributional mea-
sures of concept-distance: A task-oriented evalua-
tion. Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Sydney, Australia.
P. Resnik. 1995. Using information content to evaluate
semantic similarity. Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-95), 448?453, Montreal, Canada.
G. Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Foun-
dations of Computer Science: Potential - Theory -
Cognition, LNCS, Volume 1337, 499?506, Springer
Verlag, Berlin, Germany.
S. Shalev-Shwartz, Y. Singer and A. Y. Ng. 2004. On-
line and Batch Learning of Pseudo-Metrics. Proc. of
the International Conference on Machine Learning
(ICML).
M. Schutz and T. Joachims. 2003. Learning a Dis-
tance Metric from Relative Comparisons. Advances
in Neural Information Processing Systems (NIPS)..
J. Weeds, D. Weir and D. McCarthy. 2004. Character-
ising Measures of Lexical Distributional Similarity.
Proc. of COLING 2004, 1015?1021.
K. Q. Weinberger, J. Blitzer and L. K. Saul. 2005.
Distance Metric Learning for Large Margin Nearest
Neighbor Classification. Advances in Neural Infor-
mation Processing Systems (NIPS).
E. P. Xing, A. Y. Ng, M. Jordan and S. Russell 2002.
Distance metric learning with application to cluster-
ing with sideinformation. Advances in Neural Infor-
mation Processing Systems (NIPS).
Y. Yang and J. O. Pedersen. 1997. A Comparative
Study on Feature Selection in Text Categorization.
Proc. of the International Conference on Machine
Learning (ICML), 412?420.
800
Context Feature Selection for Distributional Similarity
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama
Graduate School of Information Science,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603
{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp
Abstract
Distributional similarity is a widely used
concept to capture the semantic relatedness
of words in various NLP tasks. However, ac-
curate similarity calculation requires a large
number of contexts, which leads to imprac-
tically high computational complexity. To
alleviate the problem, we have investigated
the effectiveness of automatic context selec-
tion by applying feature selection methods
explored mainly for text categorization. Our
experiments on synonym acquisition have
shown that while keeping or sometimes in-
creasing the performance, we can drastically
reduce the unique contexts up to 10% of the
original size. We have also extended the
measures so that they cover context cate-
gories. The result shows a considerable cor-
relation between the measures and the per-
formance, enabling the automatic selection
of effective context categories for distribu-
tional similarity.
1 Introduction
Semantic similarity of words is one of the most im-
portant lexical knowledge for NLP tasks including
word sense disambiguation and synonym acquisi-
tion. To measure the semantic relatedness of words,
a concept called distributional similarity has been
widely used. Distributional similarity represents the
relatedness of two words by the commonality of
contexts the words share, based on the distributional
hypothesis (Harris, 1985), which states that seman-
tically similar words share similar contexts.
A wide range of contextual information, such
as surrounding words (Lowe and McDonald, 2000;
Curran and Moens, 2002a), dependency or case
structure (Hindle, 1990; Ruge, 1997; Lin, 1998),
and dependency path (Lin and Pantel, 2001; Pado
and Lapata, 2007), has been utilized for similar-
ity calculation, and achieved considerable success.
However, a major problem which arises when adopt-
ing distributional similarity is that it easily yields a
huge amount of unique contexts. This can lead to
high dimensionality of context space, often up to the
order of tens or hundreds of thousands, which makes
the calculation computationally impractical. Be-
cause not all of the contexts are useful, it is strongly
required for the efficiency to eliminate the unwanted
contexts to ease the expensive cost.
To tackle this issue, Curran and Moens (2002b)
suggest assigning an index vector of canonical at-
tributes, i.e., a small number of representative el-
ements extracted from the original vector, to each
word. When the comparison is performed, canonical
attributes of two target words are firstly consulted,
and the original vectors are referred to only if the
attributes have a match between them. However, it
is not clear whether the condition for canonical at-
tributes they adopted, i.e., that the attributes must be
the most weighted subject, direct object, or indirect
object, is optimal in terms of the performance.
There are also some existing studies which paid
attention to the comparison of context categories
for synonym acquisition (Curran and Moens, 2002a;
Hagiwara et al, 2006). However, they have con-
ducted only a posteriori comparison based on perfor-
mance evaluation, and we are afraid that these find-
553
ings are somewhat limited to their own experimental
settings which may not be applicable to completely
new settings, e.g., one with a new set of contexts
extracted from different sources. Therefore, general
quantitative measures which can be used for reduc-
tion and selection of any kind of contexts and con-
text categories are strongly required.
Shifting our attention from word similarity to
other areas, a great deal of studies on feature selec-
tion has been conducted in the literature, especially
for text categorization (Yang and Pedersen, 1997)
and gene expression classification (Ding and Peng,
2003). Whereas these methods have been successful
in reducing feature size while keeping classification
performance, the problem of distributional similar-
ity is radically different from that of classification,
and whether the same methods are applicable and
effective for automatic context selection in the simi-
larity problem is yet to be investigated.
In this paper, we firstly introduce existing quan-
titative methods for feature selection, namely, DF,
TS, MI, IG, CHI2, and show how to apply them to
the distributional similarity problem to measure the
context importance. We then extracted dependency
relations as context from the corpus, and conducted
automatic synonym acquisition experiments to eval-
uate the context selection performance, reducing the
unimportant contexts based on the feature selection
methods. Finally we extend the context importance
to cover context categories (RASP2 grammatical re-
lations), and show that the above methods are also
effective in selecting categories.
This paper is organized as follows: in Section
2, five existing context selection methods are in-
troduced, and how to apply classification-based se-
lection methods to distributional similarity is de-
scribed. In Section 3 and 4, the synonym acquisition
method and evaluation measures, AP and CC, em-
ployed in the evaluation experiments are detailed.
Section 5 includes two main experiments and their
results: context reduction and context category se-
lection, along with experimental settings and discus-
sions. Section 6 concludes this paper.
2 Context Selection Methods
In this section, context selection methods proposed
for text categorization or information retrieval are
introduced. In the following, n and m represent
the number of unique words and unique contexts,
respectively, and N(w, c) denotes the number of co-
occurrence of word w and context c.
2.1 Document Frequency (DF)
Document frequency (DF), commonly used for
weighting in information retrieval, is the number of
documents a term co-occur with. However, in the
distributional similarity settings, DF corresponds to
word frequency, i.e., the number of unique words the
context co-occurs with:
df(c) = |{w|N(w, c) > 0}|.
The motivation of adopting DF as a context selection
criterion is the assumption that the contexts shared
by many words should be informative. It is to note,
however, that the contexts with too high DF are not
always useful, since there are some exceptions in-
cluding so-called stopwords.
2.2 Term Strength (TS)
Term strength (TS), proposed by Wilbur and
Sirotkin (1992) and applied to text categorization
by Yang and Wilbur (1996), measures how likely a
term is to appear in ?similar documents,? and it is
shown to achieve a successful outcome in reducing
the amount of vocabulary for text retrieval. For dis-
tributional similarity, TS is defined as:
s(c) = P (c ? C(w2)|c ? C(w1)),
where (w1, w2) is a related word pair and C(w) is
a set of contexts co-occurring with the word w, i.e.,
C(w) = {c|N(w, c) > 0}. s(c) is calculated, let-
ting PH be a set of related word pairs, as
s(c) = |{(w1, w2) ? PH |c ? C(w1) ? C(w2)}|
|{(w1, w2) ? PH |c ? C(w1)}|
.
What makes TS different from DF is that it re-
quires a training set PH consisting of related word
pairs. We used the test set for class s = 1 as PH
described in the next section.
2.3 Formalization of Distributional Similarity
The following methods, MI, IG, and CHI2, are rad-
ically different from the above ones, in that they are
554
designed essentially for ?class classification? prob-
lems. Thus we formalize distributional similarity as
a classification problem as described below.
First of all, we deal with word pairs, instead of
words, as the targets of classification, and define fea-
tures f1, ..., fm corresponding to contexts c1, ..., cm,
for each pair. The feature fj = 1 if the two words of
the pair has the context cj in common, and fj = 0
otherwise. Then, we define target class s, so that
s = 1 when the pair is semantically related, and
s = 0 if not. These defined, distributional similar-
ity is formalized as a binary classification problem
which assigns the word pairs to the class s ? {0, 1}
based on the features c1, ..., cm. Finally, to calcu-
late the specific values of the following feature im-
portance measures, we prepare two test sets of re-
lated word pairs for class s = 1 and unrelated ones
for class s = 0. This enables us to apply existing
feature selection methods designed for classification
problems to the automatic context selection.
The two test sets, related and unrelated one, are
prepared using the reference sets described in Sec-
tion 4. More specifically, we created 5,000 related
word pairs by extracting from synonym pairs in the
reference set, and 5,000 unrelated ones by firstly cre-
ating random pairs of LDV, whose detail is described
later, and then manually making sure that no related
pairs are included in these random pairs.
2.4 Mutual Information (MI)
Mutual information (MI), commonly used for word
association and co-occurrence weighing in statisti-
cal NLP, is the measure of the degree of dependence
between two events. The pointwise MI value of fea-
ture f and class s is calculated as:
I(f, s) = log P (f, s)
P (f)P (s)
.
To obtain the final context importance, we combine
the MI value over both of the classes as Imax(cj) =
maxs?{0,1} I(fj , s). Note that, here we employed
the maximum value of pointwise MI values since
it is claimed to be the best in (Yang and Peder-
sen, 1997), although there can be other combination
ways such as weighted average.
2.5 Information Gain (IG)
Information gain (IG), often employed in the ma-
chine learning field as a criterion for feature impor-
tance, is the amount of gained information of an
event by knowing the outcome of the other event,
and is calculated as the weighted sum of the point-
wise MI values over all the event combinations:
G(cj) =
?
fj?{0,1}
?
s?{0,1}
P (fj , s) log
P (fj , s)
P (fj)P (s)
.
2.6 ?2 Statistic (CHI2)
?2 statistic (CHI2) estimates the lack of indepen-
dence between classes and features, which is equal
to the summed difference of observed and expected
frequency over the contingency table cells. More
specifically, letting F jnm(n,m ? {0, 1}) be the num-
ber of word pairs with fj = n and s = m, and the
number of all pairs be N , ?2 statistic is defined as:
?2(cj)
= N(F11F00 ? F01F10)
(F11 + F01)(F10 + F00)(F11 + F10)(F01 + F00)
.
3 Synonym Acquisition Method
This section describes the synonym acquisition
method, a major and important application of distri-
butional similarity, which we employed for the eval-
uation of automatic context selection. Here we men-
tion how to extract the original contexts from cor-
pora in detail, as well as the calculation of weight
and similarity between words.
3.1 Context Extraction
We adopted dependency structure as the context of
words since it is the most widely used and well-
performing contextual information in the past stud-
ies (Ruge, 1997; Lin, 1998). As the extraction of ac-
curate and comprehensive dependency structure is in
itself a difficult task, the sophisticated parser RASP
Toolkit 2 (Briscoe et al, 2006) was utilized to ex-
tract this kind of word relations. Take the following
sentence for example:
Shipments have been relatively level since January,
the Commerce Department noted.
555
RASP outputs the extracted dependency structure
as n-ary relations as follows, which are called gram-
matical relations. Annotations regarding suffix, part
of speech tags, offsets for individual words are omit-
ted for simplicity.
(ncsubj be Shipment _)
(aux be have)
(xcomp _ be level)
(ncmod _ be relatively)
(ccomp _ level note)
(ncmod _ note since)
(ncsubj note Department _)
(det Department the)
(ncmod _ Department Commerce)
(dobj since January)
While the RASP outputs are n-ary relations in
general, what we need here is co-occurrences of
words and contexts, so we extract the set of co-
occurrences of stemmed words and contexts by tak-
ing out the target word from the relation and replac-
ing the slot by an asterisk ?*?:
(words) - (contexts)
Shipment - ncsubj:be:*_
have - aux:be:*
be - ncsubj:*:Shipment:_
be - aux:*:have
be - xcomp:_:*:level
be - ncmod:_:*:relatively
relatively - ncmod:_:be:*
level - xcomp:_:be:*
level - ccomp:_:*:note
...
Summing all these up produces the raw co-
occurrence count N(w, c) of word w and context c.
3.2 Similarity Calculation
Although it is possible to use the raw count acquired
above for the similarity calculation, directly using
the raw count may cause performance degradation,
thus we need an appropriate weighting measure. In
response to the preliminary experiment results, we
employed pointwise mutual information as weight:
wgt(w, c) = log P (w, c)
P (w)P (c)
Here we made a small modification to bind the
weight to non-negative such that wgt(w, c) ? 0,
because negative weight values sometimes worsen
the performance (Curran and Moens, 2002b). The
weighting by PMI is applied after the pre-processing
including frequency cutoff and context selection.
As for the similarity measure, we used Jaccard co-
efficient, which is widely adopted to capture overlap
proportion of two sets:
?
c?C(w1)?C(w2) min(wgt(w1, c),wgt(w2, c))
?
c?C(w1)?C(w2) max(wgt(w1, c),wgt(w2, c))
.
4 Evaluation Measures
This section describes the two evaluation methods
we employed ? average precision (AP) and corre-
lation coefficient (CC).
4.1 Average Precision (AP)
The first evaluation measure, average precision
(AP), is a common evaluation scheme for informa-
tion retrieval, which evaluates how accurately the
methods are able to extract synonyms. We first pre-
pare a set of query words, for which synonyms are
obtained to evaluate the precision. We adopted the
Longman Defining Vocabulary (LDV) 1 as the can-
didate set of query words. For each word in LDV,
three existing thesauri are consulted: Roget?s The-
saurus (Roget, 1995), Collins COBUILD Thesaurus
(Collins, 2002), and WordNet (Fellbaum, 1998).
The union of synonyms obtained when the LDV
word is looked up as a noun is used as the refer-
ence set, except for words marked as ?idiom,? ?in-
formal,? ?slang? and phrases comprised of two or
more words. The LDV words for which no noun
synonyms are found in any of the reference thesauri
are omitted. From the remaining 771 LDV words,
100 query words are randomly extracted, and for
each of them the eleven precision values at 0%, 10%,
..., and 100% recall levels are averaged to calculate
the final AP value.
4.2 Correlation Coefficient (CC)
The second evaluation measure is correlation coef-
ficient (CC) between the target similarity and the
reference similarity, i.e., the answer value of sim-
ilarity for word pairs. The reference similarity is
calculated based on the closeness of two words in
the tree structure of WordNet. More specifically, the
similarity between word w with senses w1, ..., wm1
and word v with senses v1, ..., vm2 is obtained as fol-
lows. Let the depth of node wi and vj be di and dj ,
1http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
556
and the depth of the deepest common ancestors of
both nodes be ddca. The similarity is then
sim(w, v) = max
i,j
sim(wi, vj) = maxi,j
2 ? ddca
di + dj
,
which takes the value between 0.0 and 1.0. Then,
the value of CC is calculated as the correlation co-
efficient of reference similarities r = (r1, r2, ..., rn)
and target similarities s = (s1, s2, ..., sn) over the
word pairs in sample set Ps, which is created by
choosing the most similar 2,000 word pairs from
4,000 randomly created pairs from LDV. To avoid
test-set dependency, all the CC values presented in
this paper are the average values of three trials using
different test sets.
5 Experiments
Now we describe the experimental settings and the
evaluation results of context selection methods.
5.1 Experimental Settings
As for the corpus, New York Times section of En-
glish Gigaword 2, consisting of around 914 million
words and 1.3 million documents was analyzed to
obtain word-context co-occurrences. Frequency cut-
off was applied as a pre-processing in order to filter
out any words and contexts with low frequency and
to reduce computational cost. More specifically, any
words w such that
?
c tf(w, c) < ?f and any con-
texts c such that
?
w tf(w, c) < ?f , with ?f = 40,
were removed from the co-occurrence data.
Since we set our purpose here to the automatic
acquisition of synonymous nouns, only the nouns
except for proper nouns were selected. To distin-
guish nouns, using POS tags annotated by RASP2,
any words with POS tags APP, ND, NN, NP, PN, PP
were labeled as nouns. This left a total of 40,461
unique words and 139,618 unique context, which
corresponds to the number of vectors and the dimen-
sionality of semantic space, respectively.
5.2 Context Reduction
In the first experiment, we show the effectiveness of
the five contextual selection methods introduced in
Section 2 for context reduction problem. The five
2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
measures were calculated for each context, and con-
texts were sorted by their importance. The change of
performance, AP and CC, was calculated on elimi-
nating the low-ranked contexts and varying the pro-
portion of remaining ones, until only 0.2% (279 in
number) of the unique contexts are left.
The result is displayed in Figure 1. The overall
observation is that the performance not only kept the
original level but also slightly improved even during
the ?aggressive? reduction when more than 80% of
the original contexts were eliminated and less than
20,000 contexts were left. It was not until 90% (ap-
prox. 10,000 remaining) elimination that the AP
values began to fall. The tendency of performance
change was almost the same for AP and CC, but
we observe a slight difference regarding which of
the five measures were effective. More specifically,
TS, IG and CHI2 worked well for AP, and DF, TS,
while CHI2 did for CC. On the whole, TS and CHI2
were performing the best, whereas the performance
of MI quickly worsened. Although the task is dif-
ferent, this experiment showed a very consistent re-
sult compared with the one of Yang and Pedersen?s
(1997). This means that feature selection methods
are also effective for context selection in distribu-
tional similarity, and our formalization of the prob-
lem described in Section 2 turned out to be appro-
priate for the purpose.
5.3 Context Category Selection
We are then naturally interested in what kinds of
contexts are included in these top-ranked effective
ones and how much they affect the overall perfor-
mance. To investigate this, we firstly built a set of
elite contexts, by gathering each top 10% (13,961
in number) contexts chosen by DF, TS, IG, and
CHI2, and obtaining the intersection of these four
top-ranked contexts. It was found that these four had
a great deal of overlap among them, the number of
which turned out to be 6,440.
Secondly, to measure the degree of effect a con-
text category has, we defined category importance
as the sum of all IG values of the contexts which
belong to the category. The reason is that, (a) IG
was one of the best-performing criteria as the previ-
ous experiment showed, and (b) IG value for a set of
contexts can be calculated as the sum of IG values of
individual elements, assuming that all the contexts
557
0.10
0.15
0.20
0.25
020000400006000080000100000120000
Number of Unique Context
Cor
rela
tion
 Co
effi
cien
t (C
C)
DF
TS
MI
IG
CHI2
`
(c)
6.0%
8.0%
10.0%
12.0%
14.0%
020000400006000080000100000120000
Number of Unique Context
Ave
rage
 Pre
cisi
on (
AP)
DF
TS
MI
IG
CHI2 (a)
0.10
0.15
0.20
0.25
05000100001500020000
Number of Unique Context
Cor
rela
tion
 Co
effi
cien
t (C
C)
DF
TS
IG
CHI2
`
(d)
6.0%
8.0%
10.0%
12.0%
14.0%
05000100001500020000
Number of Unique Context
Ave
rage
 Pre
cisi
on (
AP)
DF
TS
IG
CHI2 (b)
Figure 1: Performance of synonym acquisition on automatic context reduction
(a) The overall view and (b) the close-up of 0 to 20,000 unique contexts for AP,
and (c) the overall view and (b) the close-up for CC
are mutually independent, which is a naive but prac-
tical assumption because of the high independence
of acquired contexts from corpora.
For the categories: ncsubj, dobj, obj, obj2,
ncmod, xmod, cmod, ccomp, det, ta, based on the
RASP2 grammatical relations which occur fre-
quently (more than 1.0%) in the corpus, their cat-
egory importance within the elite context set was
computed and showed in Figure 2. The graph also
shows the performance of individual context cat-
egories, calculated when each category was sepa-
rately extracted from the entire corpus. The re-
sult indicates that there is a considerable correlation
(r = 0.760) between category importance and per-
formance, which means it is possible to predict the
final performance of any context categories by cal-
culating their category importance values in the lim-
ited size of selected context set.
As for the qualitative difference of category types,
the result also shows the effectiveness of modifica-
tion (ncmod) category, which is consistent with the
result (Hagiwara et al, 2006) that mod is more con-
tributing than subj and obj, which have been ex-
tensively used in the past. However, it can be seen
that the reason why the ncmod performs well may be
only because it is the largest category in size (2,515
558
0% 2% 4% 6% 8% 10% 12% 14%
ncsubj
dobj 
obj 
obj2 
ncmod 
xmod
cmod
ccomp
det
ta 
Average Precision (AP)
0 2 4 6 8 10
Category Importance (CI)
AP
CI
Figure 2: Performance of synonym acquisition vs
context category importance
in the elite contexts). The investigation of the rela-
tions between context size and performance should
be conducted in the future.
6 Conclusion
In this study, we firstly introduced feature selec-
tion methods, previously proposed for text catego-
rization, and showed how to apply them for auto-
matic context selection for distributional similarity
by formalizing the similarity problem as classifica-
tion. We then extracted dependency-based context
from the corpus, and conducted evaluation experi-
ments on automatic synonym acquisition.
The experimental results showed that while keep-
ing or even improving the original performance, it
is possible to eliminate a large proportion of con-
texts (almost up to 90%). We also extended the con-
text importance to cover context categories based on
RASP2 grammatical relations, and showed a consid-
erable correlation between the importance and the
actual performance, suggesting the possibility of au-
tomatic context category selection.
As the future works, we should further discuss
other kinds of formalization of distributional simi-
larity and their impact, because we introduced and
only briefly described a quite simple formalization
model in Section 2.3. More detailed investigations
on the contributions of sub-categories of contexts,
and other contexts than dependency structure, such
as surrounding words and dependency path, is also
the future work.
References
Ted Briscoe, John Carroll and Rebecca Watson. 2006.
The Second Release of the RASP System. Proc. of the
COLING/ACL 2006 Interactive Presentation Sessions,
77?80.
Collins. 2002. Collins Cobuild Major New Edition CD-
ROM. HarperCollins Publishers.
James R. Curran and Marc Moens. 2002. Scaling Con-
text Space. Proc. of ACL 2002, 231?238.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Workshop
on Unsupervised Lexical Acquisition. Proc. of ACL
SIGLEX, 231?238.
Chris Ding and Hanchuan Peng. 2003. Minimum Re-
dundancy Feature Selection from Microarray Gene
Expression Data. Proc. of the IEEE Computer Soci-
ety Conference on Bioinformatics, 523?528.
Editors of the American Heritage Dictionary. 1995. Ro-
get?s II: The New Thesaurus, 3rd ed. Houghton Mif-
flin.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database, MIT Press.
Masato Hagiwara, Yasuhiro Ogawa, Katsuhiko Toyama.
2006. Selection of Effective Contextual Information
for Automatic Synonym Acquisition. Proc. of COL-
ING/ACL 2006, 353?360.
Zellig Harris. 1985. Distributional Structure. Jerrold J.
Katz (ed.) The Philosophy of Linguistics. Oxford Uni-
versity Press. 26?47
Donald Hindle. 1990. Noun classification from
predicate-argument structures. Proc. of the 28th An-
nual Meeting of the ACL, 268?275.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. Proc. of the 22nd
Annual Conference of the Cognitive Science Society,
675?680.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?774.
559
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, Volume 7, Issue 4, 343?360.
Seastian Pado and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models Com-
putational Linguistics, Volume 33, Issue 2, 161?199.
Gerda Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Founda-
tions of Computer Science: Potential - Theory - Cogni-
tion, LNCS, Volume 1337, 499?506, Springer Verlag,
Berlin, Germany.
Yiming Yang and John Wilbur. 1996. Using corpus
statistics to remove redundant words in text categoriza-
tion. Journal of the American Society for Information
Science, Volume 47, Issue 5, 357?369.
Yiming Yang and Jan O. Pedersen. 1997. A Compara-
tive Study on Feature Selection in Text Categorization.
Proc. of ICML 97, 412?420.
John Wilbur and Karl Sirotkin. 1992. The automatic
identification of stop words. Journal of Information
Science, 45?55.
560
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 353?360,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Selection of Effective Contextual Information
for Automatic Synonym Acquisition
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama
Graduate School of Information Science,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603
{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp
Abstract
Various methods have been proposed for
automatic synonym acquisition, as syn-
onyms are one of the most fundamen-
tal lexical knowledge. Whereas many
methods are based on contextual clues
of words, little attention has been paid
to what kind of categories of contex-
tual information are useful for the pur-
pose. This study has experimentally inves-
tigated the impact of contextual informa-
tion selection, by extracting three kinds of
word relationships from corpora: depen-
dency, sentence co-occurrence, and prox-
imity. The evaluation result shows that
while dependency and proximity perform
relatively well by themselves, combina-
tion of two or more kinds of contextual in-
formation gives more stable performance.
We?ve further investigated useful selection
of dependency relations and modification
categories, and it is found that modifi-
cation has the greatest contribution, even
greater than the widely adopted subject-
object combination.
1 Introduction
Lexical knowledge is one of the most important re-
sources in natural language applications, making it
almost indispensable for higher levels of syntacti-
cal and semantic processing. Among many kinds
of lexical relations, synonyms are especially use-
ful ones, having broad range of applications such
as query expansion technique in information re-
trieval and automatic thesaurus construction.
Various methods (Hindle, 1990; Lin, 1998;
Hagiwara et al, 2005) have been proposed for syn-
onym acquisition. Most of the acquisition meth-
ods are based on distributional hypothesis (Har-
ris, 1985), which states that semantically similar
words share similar contexts, and it has been ex-
perimentally shown considerably plausible.
However, whereas many methods which adopt
the hypothesis are based on contextual clues con-
cerning words, and there has been much consid-
eration on the language models such as Latent
Semantic Indexing (Deerwester et al, 1990) and
Probabilistic LSI (Hofmann, 1999) and synonym
acquisition method, almost no attention has been
paid to what kind of categories of contextual infor-
mation, or their combinations, are useful for word
featuring in terms of synonym acquisition.
For example, Hindle (1990) used co-
occurrences between verbs and their subjects
and objects, and proposed a similarity metric
based on mutual information, but no exploration
concerning the effectiveness of other kinds of
word relationship is provided, although it is
extendable to any kinds of contextual information.
Lin (1998) also proposed an information theory-
based similarity metric, using a broad-coverage
parser and extracting wider range of grammatical
relationship including modifications, but he didn?t
further investigate what kind of relationships
actually had important contributions to acquisi-
tion, either. The selection of useful contextual
information is considered to have a critical impact
on the performance of synonym acquisition. This
is an independent problem from the choice of
language model or acquisition method, and should
therefore be examined by itself.
The purpose of this study is to experimen-
tally investigate the impact of contextual infor-
mation selection for automatic synonym acqui-
sition. Because nouns are the main target of
353
synonym acquisition, here we limit the target of
acquisition to nouns, and firstly extract the co-
occurrences between nouns and three categories of
contextual information ? dependency, sentence
co-occurrence, and proximity ? from each of
three different corpora, and the performance of
individual categories and their combinations are
evaluated. Since dependency and modification re-
lations are considered to have greater contribu-
tions in contextual information and in the depen-
dency category, respectively, these categories are
then broken down into smaller categories to ex-
amine the individual significance.
Because the consideration on the language
model and acquisition methods is not the scope of
the current study, widely used vector space model
(VSM), tf?idf weighting scheme, and cosine mea-
sure are adopted for similarity calculation. The re-
sult is evaluated using two automatic evaluation
methods we proposed and implemented: discrimi-
nation rate and correlation coefficient based on the
existing thesaurus WordNet.
This paper is organized as follows: in Section
2, three kinds of contextual information we use
are described, and the following Section 3 explains
the synonym acquisition method. In Section 4 the
evaluation method we employed is detailed, which
consists of the calculation methods of reference
similarity, discrimination rate, and correlation co-
efficient. Section 5 provides the experimental con-
ditions and results of contextual information se-
lection, followed by dependency and modification
selection. Section 6 concludes this paper.
2 Contextual Information
In this study, we focused on three kinds of con-
textual information: dependency between words,
sentence co-occurrence, and proximity, that is, co-
occurrence with other words in a window, details
of which are provided the following sections.
2.1 Dependency
The first category of the contextual information we
employed is the dependency between words in a
sentence, which we suppose is most commonly
used for synonym acquisition as the context of
words. The dependency here includes predicate-
argument structure such as subjects and objects
of verbs, and modifications of nouns. As the ex-
traction of accurate and comprehensive grammat-
ical relations is in itself a difficult task, the so-
dependent
mod
ncmod xmod cmod detmod
arg_mod arg aux conj
subj_or_dobj
subj
ncsubj xsubj csubj
comp
obj clausal
obj2dobj iobjxcomp ccomp
mod
subj
obj
Figure 1: Hierarchy of grammatical relations and
groups
phisticated parser RASP Toolkit (Briscoe and Car-
roll, 2002) was utilized to extract this kind of
word relations. RASP analyzes input sentences
and provides wide variety of grammatical infor-
mation such as POS tags, dependency structure,
and parsed trees as output, among which we paid
attention to dependency structure called grammat-
ical relations (GRs) (Briscoe et al, 2002).
GRs represent relationship among two or more
words and are specified by the labels, which con-
struct the hierarchy shown in Figure 1. In this hier-
archy, the upper levels correspond to more general
relations whereas the lower levels to more specific
ones. Although the most general relationship in
GRs is ?dependent?, more specific labels are as-
signed whenever possible. The representation of
the contextual information using GRs is as fol-
lows. Take the following sentence for example:
Shipments have been relatively level
since January, the Commerce Depart-
ment noted.
RASP outputs the extracted GRs as n-ary rela-
tions as follows:
(ncsubj note Department obj)
(ncsubj be Shipment _)
(xcomp _ be level)
(mod _ level relatively)
(aux _ be have)
(ncmod since be January)
(mod _ Department note)
(ncmod _ Department Commerce)
354
(detmod _ Department the)
(ncmod _ be Department)
While most of GRs extracted by RASP are bi-
nary relations of head and dependent, there are
some relations that contain additional slot or ex-
tra information regarding the relations, as shown
?ncsubj? and ?ncmod? in the above example. To
obtain the final representation that we require for
synonym acquisition, that is, the co-occurrence
between words and their contexts, these relation-
ships must be converted to binary relations, i.e.,
co-occurrence. We consider the concatenation of
all the rest of the target word as context:
Department ncsubj:note:*:obj
shipment ncsubj:be:*:_
January ncmod:since:be:*
Department mod:_:*:note
Department ncmod:_:*:Commerce
Commerce ncmod:_:Department:*
Department detmod:_:*:the
Department ncmod:_:be:*
The slot for the target word is replaced by ?*? in
the context. Note that only the contexts for nouns
are extracted because our purpose here is the auto-
matic extraction of synonymous nouns.
2.2 Sentence Co-occurrence
As the second category of contextual information,
we used the sentence co-occurrence, i.e., which
sentence words appear in. Using this context is,
in other words, essentially the same as featuring
words with the sentences in which they occur.
Treating single sentences as documents, this fea-
turing corresponds to exploiting transposed term-
document matrix in the information retrieval con-
text, and the underlying assumption is that words
that commonly appear in the similar documents or
sentences are considered semantically similar.
2.3 Proximity
The third category of contextual information,
proximity, utilizes tokens that appear in the vicin-
ity of the target word in a sentence. The basic as-
sumption here is that the more similar the distri-
bution of proceeding and succeeding words of the
target words are, the more similar meaning these
two words possess, and its effectiveness has been
previously shown (Macro Baroni and Sabrina Bisi,
2004). To capture the word proximity, we consider
a window with a certain radius, and treat the la-
bel of the word and its position within the window
as context. The contexts for the previous example
sentence, when the window radius is 3, are then:
shipment R1:have
shipment R2:be
shipment R3:relatively
January L1:since
January L2:level
January L3:relatively
January R1:,
January R2:the
January R3:Commerce
Commerce L1:the
Commerce L2:,
Commerce L3:January
Commerce R1:Department
...
Note that the proximity includes tokens such as
punctuation marks as context, because we suppose
they offer useful contextual information as well.
3 Synonym Acquisition Method
The purpose of the current study is to investigate
the impact of the contextual information selection,
not the language model itself, we employed one
of the most commonly used method: vector space
model (VSM) and tf?idf weighting scheme. In this
framework, each word is represented as a vector
in a vector space, whose dimensions correspond
to contexts. The elements of the vectors given by
tf?idf are the co-occurrence frequencies of words
and contexts, weighted by normalized idf. That
is, denoting the number of distinct words and con-
texts as N and M , respectively,
wi = t[tf(wi, c1) ? idf(c1) ... tf(wi, cM ) ? idf(cM )],
(1)
where tf(wi, cj) is the co-occurrence frequency of
word wi and context cj . idf(cj) is given by
idf(cj) = log(N/df(cj))maxk log(N/df(vk)) , (2)
where df(cj) is the number of distinct words that
co-occur with context cj .
Although VSM and tf?idf are naive and simple
compared to other language models like LSI and
PLSI, they have been shown effective enough for
the purpose (Hagiwara et al, 2005). The similar-
ity between two words are then calculated as the
cosine value of two corresponding vectors.
4 Evaluation
This section describes the evaluation methods we
employed for automatic synonym acquisition. The
evaluation is to measure how similar the obtained
similarities are to the ?true? similarities. We firstly
prepared the reference similarities from the exist-
ing thesaurus WordNet as described in Section 4.1,
355
and by comparing the reference and obtained sim-
ilarities, two evaluation measures, discrimination
rate and correlation coefficient, are calculated au-
tomatically as described in Sections 4.2 and 4.3.
4.1 Reference similarity calculation using
WordNet
As the basis for automatic evaluation methods, the
reference similarity, which is the answer value that
similarity of a certain pair of words ?should take,?
is required. We obtained the reference similarity
using the calculation based on thesaurus tree struc-
ture (Nagao, 1996). This calculation method re-
quires no other resources such as corpus, thus it is
simple to implement and widely used.
The similarity between word sense wi and word
sense vj is obtained using tree structure as follows.
Let the depth1 of node wi be di, the depth of node
vj be dj , and the maximum depth of the common
ancestors of both nodes be ddca. The similarity
between wi and vj is then calculated as
sim(wi, vj) = 2 ? ddcadi + dj , (3)
which takes the value between 0.0 and 1.0.
Figure 2 shows the example of calculating the
similarity between the word senses ?hill? and
?coast.? The number on the side of each word
sense represents the word?s depth. From this tree
structure, the similarity is obtained:
sim(?hill?, ?coast?) = 2 ? 35 + 5 = 0.6. (4)
The similarity between word w with senses
w1, ..., wn and word v with senses v1, ..., vm is de-
fined as the maximum similarity between all the
pairs of word senses:
sim(w, v) = max
i,j
sim(wi, vj), (5)
whose idea came from Lin?s method (Lin, 1998).
4.2 Discrimination Rate
The following two sections describe two evalua-
tion measures based on the reference similarity.
The first one is discrimination rate (DR). DR, orig-
inally proposed by Kojima et al (2004), is the rate
1To be precise, the structure of WordNet, where some
word senses have more than one parent, isn?t a tree but a
DAG. The depth of a node is, therefore, defined here as the
?maximum distance? from the root node.
entity     0
inanimate-object     1
natural-object     2
geological-formation     3
4 natural-elevation
5 hill
shore     4
coast     5
Figure 2: Example of automatic similarity calcu-
lation based on tree structure
(answer, reply)(phone, telephone)(sign, signal)(concern, worry)
(animal, coffee)(him, technology)(track, vote)(path, youth)
? ?
highly related unrelated
Figure 3: Test-sets for discrimination rate calcula-
tion.
(percentage) of pairs (w1, w2) whose degree of as-
sociation between two words w1, w2 is success-
fully discriminated by the similarity derived by
the method under evaluation. Kojima et al dealt
with three-level discrimination of a pair of words,
that is, highly related (synonyms or nearly syn-
onymous), moderately related (a certain degree of
association), and unrelated (irrelevant). However,
we omitted the moderately related level and lim-
ited the discrimination to two-level: high or none,
because of the difficulty of preparing a test set that
consists of moderately related pairs.
The calculation of DR follows these steps: first,
two test sets, one of which consists of highly re-
lated word pairs and the other of unrelated ones,
are prepared, as shown in Figure 3. The similar-
ity between w1 and w2 is then calculated for each
pair (w1, w2) in both test sets via the method un-
der evaluation, and the pair is labeled highly re-
lated when similarity exceeds a given threshold t
and unrelated when the similarity is lower than t.
The number of pairs labeled highly related in the
highly related test set and unrelated in the unre-
lated test set are denoted na and nb, respectively.
356
DR is then given by:
1
2
( na
Na +
nb
Nb
)
, (6)
where Na and Nb are the numbers of pairs in
highly related and unrelated test sets, respectively.
Since DR changes depending on threshold t, max-
imum value is adopted by varying t.
We used the reference similarity to create these
two test sets. Firstly, Np = 100, 000 pairs of
words are randomly created using the target vo-
cabulary set for synonym acquisition. Proper
nouns are omitted from the choice here because
of their high ambiguity. The two testsets are then
created extracting n = 2, 000 most related (with
high reference similarity) and unrelated (with low
reference similarity) pairs.
4.3 Correlation coefficient
The second evaluation measure is correlation co-
efficient (CC) between the obtained similarity and
the reference similarity. The higher CC value is,
the more similar the obtained similarities are to
WordNet, thus more accurate the synonym acqui-
sition result is.
The value of CC is calculated as follows. Let
the set of the sample pairs be Ps, the sequence of
the reference similarities calculated for the pairs
in Ps be r = (r1, r2, ..., rn), the corresponding
sequence of the target similarity to be evaluated
be r = (s1, s2, ..., sn), respectively. Correlation
coefficient ? is then defined by:
? =
1
n
?n
i=1(ri ? r?)(si ? s?)
?r?s , (7)
where r?, s?, ?r, and ?s represent the average of r
and s and the standard deviation of r and s, re-
spectively. The set of the sample pairs Ps is cre-
ated in a similar way to the preparation of highly
related test set used in DR calculation, except that
we employed Np = 4, 000, n = 2, 000 to avoid
extreme nonuniformity.
5 Experiments
Now we desribe the experimental conditions and
results of contextual information selection.
5.1 Condition
We used the following three corpora for the ex-
periment: (1) Wall Street Journal (WSJ) corpus
(approx. 68,000 sentences, 1.4 million tokens),
(2) Brown Corpus (BROWN) (approx. 60,000
sentences, 1.3 million tokens), both of which are
contained in Treebank 3 (Marcus, 1994), and (3)
written sentences in WordBank (WB) (approx.
190,000 sentences, 3.5 million words) (Hyper-
Collins, 2002). No additional annotation such as
POS tags provided for Treebank was used, which
means that we gave the plain texts stripped off any
additional information to RASP as input.
To distinguish nouns, using POS tags annotated
by RASP, any words with POS tags APP, ND, NN,
NP, PN, PP were labeled as nouns. The window
radius for proximity is set to 3. We also set a
threshold tf on occurrence frequency in order to
filter out any words or contexts with low frequency
and to reduce computational cost. More specifi-
cally, any words w such that ?c tf(w, c) < tf and
any contexts c such that ?w tf(w, c) < tf were
removed from the co-occurrence data. tf was set
to tf = 5 for WSJ and BROWN, and tf = 10 for
WB in Sections 5.2 and 5.3, and tf = 2 for WSJ
and BROWN and tf = 5 for WB in Section 5.4.
5.2 Contextual Information Selection
In this section, we experimented to discover what
kind of contextual information extracted in Sec-
tion 2 is useful for synonym extraction. The per-
formances, i.e. DR and CC are evaluated for each
of the three categories and their combinations.
The evaluation result for three corpora is shown
in Figure 4. Notice that the range and scale of the
vertical axes of the graphs vary according to cor-
pus. The result shows that dependency and prox-
imity perform relatively well alone, while sen-
tence co-occurrence has almost no contributions
to performance. However, when combined with
other kinds of context information, every category,
even sentence co-occurrence, serves to ?stabilize?
the overall performance, although in some cases
combination itself decreases individual measures
slightly. It is no surprise that the combination of all
categories achieves the best performance. There-
fore, in choosing combination of different kinds of
context information, one should take into consid-
eration the economical efficiency and trade-off be-
tween computational complexity and overall per-
formance stability.
5.3 Dependency Selection
We then focused on the contribution of individual
categories of dependency relation, i.e. groups of
grammatical relations. The following four groups
357
65.0%
65.5%
66.0%
66.5%
67.0%
67.5%
68.0%
68.5%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.09
0.10
0.11
0.12
0.13
correlation
 coefficient
 (CC))
DR
CC
dep sent prox dep
sent
dep
prox
sent
prox
all
(1) WSJ
DR
 = 52.8%
CC
 = -0.0029
sent:
65.0%
65.5%
66.0%
66.5%
67.0%
67.5%
68.0%
68.5%
69.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.13
0.14
0.15
correlation
 coefficient
 (CC))
DR
CC
dep sent prox dep
sent
dep
prox
sent
prox
all
(2) BROWN
DR
 = 53.8%
CC
 = 0.060
sent:
66.0%
66.5%
67.0%
67.5%
68.0%
68.5%
69.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.16
0.17
0.18
0.19
correlation
 coefficient
 (CC))
DR
CC
dep sent prox dep
sent
dep
prox
sent
prox
all
(3) WB
DR
 = 52.2%
CC
 = 0.0066
sent:
Figure 4: Contextual information selection perfor-
mances
Discrimination rate (DR) and correlation coefficient (CC)
for (1) Wall Street Journal corpus, (2) Brown Corpus, and
(3) WordBank.
of GRs are considered for comparison conve-
nience: (1) subj group (?subj?, ?ncsubj?, ?xsubj?,
and ?csubj?), (2) obj group (?obj?, ?dobj?, ?obj2?,
and ?iobj?), (3) mod group (?mod?, ?ncmod?,
?xmod?, ?cmod?, and ?detmod?), and (4) etc
group (others), as shown in the circles in Figure
1. This is because distinction between relations
in a group is sometimes unclear, and is consid-
ered to strongly depend on the parser implemen-
tation. The final target is seven kinds of combina-
tions of the above four groups: subj, obj, mod, etc,
subj+obj, subj+obj+mod, and all.
The two evaluation measures are similarly cal-
culated for each group and combination, and
shown in Figure 5. Although subjects, objects,
and their combination are widely used contextual
information, the performances for subj and obj
categories, as well as their combination subj+obj,
were relatively poor. On the contrary, the re-
sult clearly shows the importance of modification,
which alone is even better than widely adopted
subj+obj. The ?stabilization effect? of combina-
tions observed in the previous experiment is also
confirmed here as well.
Because the size of the co-occurrence data
varies from one category to another, we conducted
another experiment to verify that the superiority
of the modification category is simply due to the
difference in the quality (content) of the group,
not the quantity (size). We randomly extracted
100,000 pairs from each of mod and subj+obj cat-
egories to cancel out the quantity difference and
compared the performance by calculating aver-
aged DR and CC of ten trials. The result showed
that, while the overall performances substantially
decreased due to the size reduction, the relation
between groups was preserved before and after the
extraction throughout all of the three corpora, al-
though the detailed result is not shown due to the
space limitation. This means that what essentially
contributes to the performance is not the size of
the modification category but its content.
5.4 Modification Selection
As the previous experiment shows that modifica-
tions have the biggest significance of all the depen-
dency relationship, we further investigated what
kind of modifications is useful for the purpose. To
do this, we broke down the mod group into these
five categories according to modifying word?s cat-
egory: (1) detmod, when the GR label is ?det-
358
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
68.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
correlation
 coefficient
 (CC))
DR
CC
subj obj mod etc subj
obj
subj
obj
mod
all
(1) WSJ
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
68.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
correlation
 coefficient
 (CC))
DR
CC
subj obj mod etc subj
obj
subj
obj
mod
all
(2) BROWN
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
68.0%
70.0%
dis
cr
im
in
at
ion
 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
correlation
 coefficient
 (CC))
DR
CC
subj obj mod etc subj
obj
subj
obj
mod
all
(3) WB
Figure 5: Dependency selection performances
Discrimination rate (DR) and correlation coefficient (CC)
for (1) Wall Street Journal corpus, (2) Brown Corpus, and
(3) WordBank.
50.0%
52.0%
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
correlation
 coefficient
 (CC))
DR
CC
detmod
ncmod-n
ncmod-j
ncmod-p
etc all
(1) WSJ
50.0%
52.0%
54.0%
56.0%
58.0%
60.0%
62.0%
64.0%
66.0%
dis
cr
im
ina
tio
n 
ra
te
 
(DR
)a
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
correlation
 coefficient
 (CC))
DR
CC
detmod
ncmod-n
ncmod-j
ncmod-p
etc all
(2) BROWN
CC
 = -0.018
57.0%
59.0%
61.0%
63.0%
65.0%
67.0%
dis
cr
im
in
at
ion
 
ra
te
 
(DR
)a
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
correlation
 coefficient
 (CC))
DR
CC
detmod
ncmod-n
ncmod-j
ncmod-p
etc all
(3) WB
Figure 6: Modification selection performances
Discrimination rate (DR) and correlation coefficient (CC)
for (1) Wall Street Journal corpus, (2) Brown Corpus, and
(3) WordBank.
359
mod?, i.e., the modifying word is a determiner, (2)
ncmod-n, when the GR label is ?ncmod? and the
modifying word is a noun, (3) ncmod-j, when the
GR label is ?ncmod? and the modifying word is an
adjective or number, (4) ncmod-p, when the GR
label is ?ncmod? and the modification is through a
preposition (e.g. ?state? and ?affairs? in ?state of
affairs?), and (5) etc (others).
The performances for each modification cate-
gory are evaluated and shown in Figure 6. Al-
though some individual modification categories
such as detmod and ncmod-j outperform other cat-
egories in some cases, the overall observation is
that all the modification categories contribute to
synonym acquisition to some extent, and the ef-
fect of individual categories are accumulative. We
therefore conclude that the main contributing fac-
tor on utilizing modification relationship in syn-
onym acquisition isn?t the type of modification,
but the diversity of the relations.
6 Conclusion
In this study, we experimentally investigated the
impact of contextual information selection, by ex-
tracting three kinds of contextual information ?
dependency, sentence co-occurrence, and proxim-
ity ? from three different corpora. The acqui-
sition result was evaluated using two evaluation
measures, DR and CC using the existing thesaurus
WordNet. We showed that while dependency and
proximity perform relatively well by themselves,
combination of two or more kinds of contextual
information, even with the poorly performing sen-
tence co-occurrence, gives more stable result. The
selection should be chosen considering the trade-
off between computational complexity and overall
performance stability. We also showed that modi-
fication has the greatest contribution to the acqui-
sition of all the dependency relations, even greater
than the widely adopted subject-object combina-
tion. It is also shown that all the modification cate-
gories contribute to the acquisition to some extent.
Because we limited the target to nouns, the re-
sult might be specific to nouns, but the same exper-
imental framework is applicable to any other cate-
gories of words. Although the result also shows
the possibility that the bigger the corpus is, the
better the performance will be, the contents and
size of the corpora we used are diverse, so their
relationship, including the effect of the window ra-
dius, should be examined as the future work.
References
Marco Baroni and Sabrina Bisi 2004. Using cooccur-
rence statistics and the web to discover synonyms
in a technical language. Proc. of the Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004).
Ted Briscoe and John Carroll. 2002. Robust Accu-
rate Statistical Annotation of General Text. Proc. of
the Third International Conference on Language Re-
sources and Evaluation (LREC 2002), 1499?1504.
Ted Briscoe, John Carroll, Jonathan Graham and Ann
Copestake 2002. Relational evaluation schemes.
Proc. of the Beyond PARSEVAL Workshop at the
Third International Conference on Language Re-
sources and Evaluation, 4?8.
Scott Deerwester, et al 1990. Indexing by Latent Se-
mantic Analysis. Journal of the American Society
for Information Science, 41(6):391?407.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press.
Masato Hagiwara, Yasuhiro Ogawa, Katsuhiko
Toyama. 2005. PLSI Utilization for Automatic
Thesaurus Construction. Proc. of The Second In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-05), 334?345.
Zellig Harris. 1985. Distributional Structure. Jerrold
J. Katz (ed.) The Philosophy of Linguistics. Oxford
University Press. 26?47.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. Proc. of the 28th An-
nual Meeting of the ACL, 268?275.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. Proc. of the 22nd International Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR ?99), 50?57.
Kazuhide Kojima, Hirokazu Watabe, and Tsukasa
Kawaoka. 2004. Existence and Application of
Common Threshold of the Degree of Association.
Proc. of the Forum on Information Technology
(FIT2004) F-003.
Collins. 2002. Collins Cobuild Mld Major New Edi-
tion CD-ROM. HarperCollins Publishers.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. Proc. of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational linguistics (COLING-ACL ?98), 786?774.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Makoto Nagao (ed.). 1996. Shizengengoshori.
The Iwanami Software Science Series 15, Iwanami
Shoten Publishers.
360
PLSI Utilization for Automatic Thesaurus Construction
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama
Graduate School of Information Science, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603
{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp
Abstract. When acquiring synonyms from large corpora, it is important to deal
not only with such surface information as the context of the words but also their
latent semantics. This paper describes how to utilize a latent semantic model PLSI
to acquire synonyms automatically from large corpora. PLSI has been shown
to achieve a better performance than conventional methods such as tf?idf and
LSI, making it applicable to automatic thesaurus construction. Also, various PLSI
techniques have been shown to be effective including: (1) use of Skew Divergence
as a distance/similarity measure; (2) removal of words with low frequencies, and
(3) multiple executions of PLSI and integration of the results.
1 Introduction
Thesauri, dictionaries in which words are arranged according to meaning, are one of the
most useful linguistic sources, having a broad range of applications, such as information
retrieval and natural language understanding. Various thesauri have been constructed
so far, including WordNet [6] and Bunruigoihyo [14]. Conventional thesauri, however,
have largely been compiled by groups of language experts, making the construction
and maintenance cost very high. It is also difficult to build a domain-specific thesaurus
flexibly. Thus it is necessary to construct thesauri automatically using computers.
Many studies have been done for automatic thesaurus construction. In doing so,
synonym acquisition is one of the most important techniques, although a thesaurus gen-
erally includes other relationships than synonyms (e.g., hypernyms and hyponyms). To
acquire synonyms automatically, contextual features of words, such as co-occurrence
and modification are extracted from large corpora and often used. Hindle [7], for ex-
ample, extracted verb-noun relationships of subjects/objects and their predicates from a
corpus and proposed a method to calculate similarity of two words based on their mu-
tual information. Although methods based on such raw co-occurrences are simple yet
effective, in a naive implementation some problems arise: namely, noises and sparse-
ness. Being a collection of raw linguistic data, a corpus generally contains meaningless
information, i.e., noises. Also, co-occurrence data extracted from corpora are often very
sparse, making them inappropriate for similarity calculation, which is also known as the
?zero frequency problem.? Therefore, not only surface information but also latent se-
mantics should be considered when acquiring synonyms from large corpora.
Several latent semantic models have been proposed so far, mainly for information
retrieval and document indexing. The most commonly used and prominent ones are La-
tent Semantic Indexing (LSI) [5] and Probabilistic LSI (PLSI) [8]. LSI is a geometric
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 334?345, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
PLSI Utilization for Automatic Thesaurus Construction 335
model based on the vector space model. It utilizes singular value decomposition of the
co-occurrence matrix, an operation similar to principal component analysis, to auto-
matically extract major components that contribute to the indexing of documents. It can
alleviate the noise and sparseness problems by a dimensionality reduction operation,
that is, by removing components with low contributions to the indexing. However, the
model lacks firm, theoretical basis [9] and the optimality of inverse document frequency
(idf) metric, which is commonly used to weight elements, has yet to be shown [13].
On the contrary, PLSI, proposed by Hofmann [8], is a probabilistic version of LSI,
where it is formalized that documents and terms co-occur through a latent variable.
PLSI puts no assumptions on distributions of documents or terms, while LSI performs
optimal model fitting, assuming that documents and terms are under Gaussian distribu-
tion [9]. Moreover, ad hoc weighting such as idf is not necessary for PLSI, although it
is for LSI, and it is shown experimentally to outperform the former model [8].
This study applies the PLSI model to the automatic acquisition of synonyms by es-
timating each word?s latent meanings. First, a number of verb-noun pairs were collected
from a large corpus using heuristic rules. This operation is based on the assumption that
semantically similar words share similar contexts, which was also employed in Hindle?s
work [7] and has been shown to be considerably plausible. Secondly, the co-occurrences
obtained in this way were fit into the PLSI model, and the probability distribution of
latent classes was calculated for each noun. Finally, similarity for each pair of nouns
can be calculated by measuring the distances or the similarity between two probability
distributions using an appropriate distance/similarity measure. We then evaluated and
discussed the results using two evaluation criteria, discrimination rates and scores.
This paper also discusses basic techniques when applying PLSI to the automatic
acquisition of synonyms. In particular, the following are discussed from methodological
and experimental views: (1) choice of distance/similarity measures between probability
distributions; (2) filtering words according to their frequencies of occurrence; and (3)
multiple executions of PLSI and integration of the results.
This paper is organized as follows: in Sect. 2 a brief explanation of the PLSI model
and calculation is provided, and Sect. 3 outlines our approach. Sect. 4 shows the results
of comparative experiments and basic techniques. Sect. 5 concludes this paper.
2 The PLSI Model
This section provides a brief explanation of the PLSI model in information retrieval
settings. The PLSI model, which is based on the aspect model, assumes that document
d and term w co-occur through latent class z, as shown in Fig. 1 (a).
The co-occurrence probability of documents and terms is given by:
P (d, w) = P (d)
?
z
P (z|d)P (w|z). (1)
Note that this model can be equivalently rewritten as
P (d, w) =
?
z
P (z)P (d|z)P (w|z), (2)
336 M. Hagiwara, Y. Ogawa, and K. Toyama
d z w
P(d) P(z|d) P(w|z)
d z w
P(z)
P(d|z) P(w|z)
(a) (b)
Fig. 1. PLSI model asymmetric (a) and symmetric (b) parameterization
corpus
co-occurrence
(v, c, n)
(eat, obj, lunch)
(eat, obj, hamburger)
(have, obj, breakfast)
???
PLSI model
z n
P(v) P(z|v) P(n|z)
latent class
noun
(v,c)
verb+case
0.0
0.1
0.2
0.3
0.4
0.5
0.6
1 2 3 4 5 6 7 8 9 10
lunch
0 .0
0 .1
0 .2
0 .3
0 .4
0 .5
0 .6
1 2 3 4 5 6 7 8 9 10
breakfast
),( 21 wwsim
latent class distribution
similarity calculation
P(z|n)
P(z|n)
Fig. 2. Outline of our approach
whose graphical model representation is shown in Fig. 1 (b). This is a symmetric pa-
rameterization with respect to documents and terms. The latter parameterization is used
in the experiment section because of its simple implementation.
Theoretically, probabilities P (d), P (z|d), P (w|z) are determined by maximum
likelihood estimation, that is, by maximizing the likelihood of document term
co-occurrence:
L =
?
d,w
N(d, w) log P (d, w), (3)
where N(d, w) is the frequency document d and term w co-occur.
While the co-occurrence of document d and term w in the corpora can be observed
directly, the contribution of latent class z cannot be directly seen in this model. For
the maximum likelihood estimation of this model, the EM algorithm [1], which is used
for the estimation of systems with unobserved (latent) data, is used. The EM algorithm
performs the estimation iteratively, similar to the steepest descent method.
3 Approach
The original PLSI model, as described above, deals with co-occurrences of documents
and terms, but it can also be applied to verbs and nouns in the corpora. In this way, latent
PLSI Utilization for Automatic Thesaurus Construction 337
John gave presents to his colleagues.
John    gave    presents    to    his    colleagues.   
NNP
NP
VBD
VP
NNS TO NNS
NP
PRP$
PPNP
S
[John]    gave    [presents]    to    [his    colleagues]
NP S VP
NPVPVBD
PPVPVBD NPTO PP
(a) Original sentence
(b) Parsing result
(c) Dependency structure
(d) Co-occurrence extraction from dependencies
John    gave
NP S VP
(?give?, subj, ?John?)
gave   presents
VBDVP NP
(?give?, obj, ?present?)
gave   to   his   colleagues
TO PP NP
(?give?, ?to?, ?colleague?)
PPVPVBD
n
NP S VP
v (v, subj, n)
(v, obj, n)
(v, prep, n)
but (v, obj, n) when the verb is ?be? + past participle.
n
NP VP baseVP
v
n
NP PP
prep
PP* VP baseVP
v
Rule 1?
Rule 2?
Rule 3?
(e) Rules for co-occurrence identification
Fig. 3. Co-occurrence extraction
class distribution, which can be interpreted as latent ?meaning? corresponding to each
noun, is obtained. Semantically similar words are then obtained accordingly, because
words with similar meaning have similar distributions. Fig. 2 outlines our approach,
and the following subsections provide the details.
3.1 Extraction of Co-occurrence
We adopt triples (v, c, n) extracted from the corpora as co-occurrences fit into the PLSI
model, where v, c, and n represent a verb, case/preposition, and a noun, respectively.
The relationships between nouns and verbs, expressed by c, include case relation (sub-
ject and object) as well as what we call here ?prepositional relation,? that is, a co-
occurrence through a preposition. Take the following sentence for example:
John gave presents to his colleagues.
First, the phrase structure (Fig. 3(b)) is obtained by parsing the original sentence
(Fig. 3(a)). The resulting tree is then used to derive the dependency structure (Fig. 3(c)),
using Collins? method [4]. Note that dependencies in baseNPs (i.e., noun phrases that
do not contain NPs as their child constituents, shown as the groups of words enclosed
by square brackets in Fig. 3(c)), are ignored. Also, we introduced baseVPs, that is,
sequences of verbs 1, modals (MD), or adverbs (RB), of which the last word must be
a verb. BaseVPs simplify the handling of sequences of verbs such as ?might not be?
1 Ones expressed as VB, VBD, VBG, VBN, VBP, and VBZ by the Penn Treebank POS tag set
[15].
338 M. Hagiwara, Y. Ogawa, and K. Toyama
and ?is always complaining.? The last word of a baseVP represents the entire baseVP
to which it belongs. That is, all the dependencies directed to words in a baseVP are
redirected to the last verb of the baseVP.
Finally, co-occurrences are extracted and identified by matching the dependency
patterns and the heuristic rules for extraction, which are all listed in Fig. 3 (e). For
example, since the label of the dependency ?John? ??gave? is ?NP S VP?, the noun
?John? is identified as the subject of the verb ?gave? (Fig. 3(d)). Likewise, the de-
pendencies ?presents???gave? and ?his colleagues???to???gave? are identified as a
verb-object relation and prepositional relation through ?to?.
A simple experiment was conducted to test the effectiveness of this extraction
method, using the corpus and the parser mentioned in the experiment section. Co-
occurrence extraction was performed for the 50 sentences randomly extracted from
the corpus, and precision and recall turned out to be 88.6% and 78.1%, respectively. In
this context, precision is more important than recall because of the substantial size of
the corpus, and some of the extraction errors result from parsing error caused by the
parser, whose precision is claimed to be around 90% [2]. Therefore, we conclude that
this method and its performance are sufficient for our purpose.
3.2 Applying PLSI to Extracted Co-occurence Data
While the PLSI model deals with dyadic data (d, w) of document d and term w, the co-
occurrences obtained by our method are triples (v, c, n) of a verb v, a case/preposition
c, and a noun n. To convert these triples into dyadic data (pairs), verb v and case/
preposition c are paired as (v, c) and considered a new ?virtual? verb v. This enables it
to handle the triples as the co-occurrence (v, n) of verb v and noun n to which the PLSI
model becomes applicable. Pairing verb v and case/preposition c also has a benefit that
such phrasal verbs as ?look for? or ?get to? can be naturally treated as a single verb.
After the application of PLSI, we obtain probabilities P (z), P (v|z), and P (n|z).
Using Bayes theorem, we then obtain P (z|n), which corresponds to the latent class
distribution for each noun. In other words, distribution P (z|n) represents the features
of meaning possessed by noun n. Therefore, we can calculate the similarity between
nouns n1 and n2 by measuring the distance or similarity between the two correspond-
ing distribution, P (z|n1) and P (z|n2), using an appropriate measure. The choice of
measure affects the synonym acquisition results and experiments on comparison of dis-
tance/similarity measures are detailed in Sect. 4.3.
4 Experiments
This section includes the results of comparison experiments and those on the basic PLSI
techniques.
4.1 Conditions
The automatic acquisition of synonyms was conducted according to the method de-
scribed in Sect. 3, using WordBank (190,000 sentences, 5 million words) [3] as a cor-
PLSI Utilization for Automatic Thesaurus Construction 339
pus. Charniak?s parser [2] was used for parsing and TreeTagger [16] for stemming. A
total of 702,879 co-occurrences was extracted by the method described in Sect. 3.1.
When using EM algorithm to implement PLSI, overfitting, which aggravates the
performance of the resultant language model, occasionally occurs. We employed the
tempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem. TEM
algorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17],
and helps avoid local extrema by introducing inverse temperature ?. The parameter was
set to ? = 0.86, considering the results of the preliminary experiments.
As the similarity/distance measure and frequency threshold tf , Skew Divergence
(? = 0.99) and tf = 15 were employed in the following experiments in response to the
results from the experiments described in Sects. 4.3 and 4.5. Also, because estimation
by EM algorithm is started from the random parameters and consequently the PLSI
results change every time it is executed, the average performance of the three executions
was recorded, except in Sect. 4.6.
4.2 Measures for Performance
The following two measures, discrimination rate and scores, were employed for the
evaluation of automated synonym acquisition.
Discrimination rate Discrimination rate, originally proposed by Kojima et al [10], is
the rate (percentage) of pairs (w1, w2) whose degree of association between two words
w1, w2 is successfully discriminated by the similarity derived by a method. Kojima
et al dealt with three-level discrimination of a pair of words, that is, highly related
(synonyms or nearly synonymous), moderately related (a certain degree of association),
and unrelated (irrelevant). However, we omitted the moderately related level and limited
the discrimination to two-level: high or none, because of the high cost of preparing a
test set that consists of moderately related pairs.
The calculation of discrimination rate follows these steps: first, two test sets, one
of which consists of highly related word pairs and the other of unrelated ones, were
prepared, as shown in Fig. 4. The similarity between w1 and w2 is then calculated for
each pair (w1, w2) in both test sets via the method under evaluation, and the pair is
labeled highly related when similarity exceeds a given threshold t and unrelated when
the similarity is lower than t. The number of pairs labeled highly related in the highly
related test set and unrelated in the unrelated test set are denoted na and nb, respectively.
The discrimination rate is then given by:
1
2
(
na
Na
+
nb
Nb
)
, (4)
where Na and Nb are the numbers of pairs in highly related and unrelated test sets,
respectively. Since the discrimination rate changes depending on threshold t, maximum
value is adopted by varying t.
We created a highly related test set using the synonyms in WordNet [6]. Pairs in a
unrelated test set were prepared by first choosing two words randomly and then con-
firmed by hand whether the consisting two words are truly irrelevant. The numbers of
pairs in the highly and unrelated test sets are 383 and 1,124, respectively.
340 M. Hagiwara, Y. Ogawa, and K. Toyama
(answer, reply)
(phone, telephone)
(sign, signal)
(concern, worry)
(animal, coffee)
(him, technology)
(track, vote)
(path, youth)
?
?
highly related unrelated
Fig. 4. Test-sets for discrimination rate calcula-
tion
base word: computer
rank synonym sim sim? rel.(p) p ? sim?
1 equipment 0.6 0.3 B(0.5) 0.15
2 machine 0.4 0.2 A(1.0) 0.20
3 Internet 0.4 0.2 B(0.5) 0.10
4 spray 0.4 0.2 C(0.0) 0.00
5 PC 0.2 0.1 A(1.0) 0.10
total 2.0 1.0 0.55
Table 5. Procedure for score calculation
Scores We propose a score which is similar to precision used for information retrieval
evaluation, but different in that it considers the similarity of words. This extension is
based on the notion that the more accurately the degrees of similarity are assigned to
the results of synonym acquisition, the higher the score values should be.
Described in the following, along with Table 5, is the procedure for score calcula-
tion. Table 5 shows the obtained synonyms and their similarity with respect to the base
word ?computer.? Results are obtained by calculating the similarity between the base
word and each noun, and ranking all the nouns in descending order of similarity sim.
The highest five are used for calculations in this example.
The range of similarity varies based on such factors as the employed distance/
similarity measure, which unfavorably affects the score value. To avoid this, the val-
ues of similarity are normalized such that their sum equals one, as shown in the column
sim? in Fig. 5. Next, the relevance of each synonym to the base word is checked and
evaluated manually, giving them three-level grades: highly related (A), moderately re-
lated (B), and unrelated (C), and relevance scores p = 1.0, 0.5, 0.0 are assigned for
each grade, respectively (?rel.(p)? column in Fig. 5). Finally, each relevance score p is
multiplied by corresponding similarity sim? , and the products (the p ? sim? column
in Fig. 5) are totaled and then multiplied by 100 to obtain a score, which is 55 in this
case. In actual experiments, thirty words chosen randomly were adopted as base words,
and the average of the scores of all base words was employed. Although this example
considers only the top five words for simplicity, the top twenty words were used for
evaluation in the following experiments.
4.3 Distance/Similarity Measures of Probability Distribution
The choice of distance measure between two latent class distributions P (z|ni), P (z|nj)
affects the performance of synonym acquisition. Here we focus on the following seven
distance/similarity measures and compare their performance.
? Kullback-Leibler (KL) divergence [12]: KL(p || q) = ?x p(x) log(p(x)/q(x))
? Jensen-Shannon (JS) divergence [12]: JS(p, q) = {KL(p || m)+KL(q || m)}/2,
m = (p + q)/2
? Skew Divergence [11]: s?(p || q) = KL(p || ?q + (1 ? ?)p)
? Euclidean distance: euc(p, q) = ||p ? q||
? L1 distance: L1(p, q) =
?
x |p(x) ? q(x)|
PLSI Utilization for Automatic Thesaurus Construction 341
? Inner product: p ? q =
?
x p(x)q(x)
? Cosine: cos(p, q) = (p ? q)/||p|| ? ||q||
KL divergence is widely used for measuring the distance between two probabil-
ity distributions. However, it has such disadvantages as asymmetricity and zero fre-
quency problem, that is, if there exists x such that p(x) = 0, q(x) = 0, the distance is
not defined. JS divergence, in contrast, is considered the symmetrized KL divergence
and has some favorable properties: it is bounded [12] and does not cause the zero fre-
quency problem. Skew Divergence, which has recently been receiving attention, has
also solved the zero frequency problem by introducing parameter ? and mixing the
two distributions. It has shown that Skew Divergence achieves better performance than
the other measures [11]. The other measures commonly used for calculation of the
similarity/distance of two vectors, namely Euclidean distance, L1 distance (also called
Manhattan Distance), inner product, and cosine, are also included for comparison.
Notice that the first five measures are of distance (the more similar p and q, the lower
value), whereas the others, inner product and cosine, are of similarity (the more similar
p and q, the higher value). We converted distance measure D to a similarity measure
sim by the following expression:
sim(p, q) = exp{??D(p, q)}, (5)
inspired by Mochihashi and Matsumoto [13]. Parameter ? was determined in such a
way that the average of sim doesn?t change with respect to D. Because KL divergence
and Skew Divergence are asymmetric, the average of both directions (e.g. for KL diver-
gence, 12 (KL(p||q) + KL(q||p))) is employed for the evaluation.
Figure 6 shows the performance (discrimination rate and score) for each measure. It
can be seen that Skew Divergence with parameter ? = 0.99 shows the highest perfor-
mance of the seven, with a slight difference to JS divergence. These results, along with
several studies, also show the superiority of Skew Divergence. In contrast, measures for
vectors such as Euclidean distance achieved relatively poor performance compared to
those for probability distributions.
4.4 Word Filtering by Frequencies
It may be difficult to estimate the latent class distributions for words with low frequen-
cies because of a lack of sufficient data. These words can be noises that may degregate
the results of synonym acquisition. Therefore, we consider removing such words with
low frequencies before the execution of PLSI improves the performance. More specif-
ically, we introduced threshold tf on the frequency, and removed nouns ni such that
?
j tf
i
j < tf and verbs vj such that
?
i tf
i
j < tf from the extracted co-occurrences.
The discrimination rate change on varying threshold tf was measured and shown
in Fig. 7 for d = 100, 200, and 300. In every case, the rate increases with a moderate
increase of tf , which shows the effectiveness of the removal of low frequency words.
We consequently fixed tf = 15 in other experiments, although this value may depend
on the corpus size in use.
342 M. Hagiwara, Y. Ogawa, and K. Toyama
50.0%
55.0%
60.0%
65.0%
70.0%
75.0%
80.0%
KL
JS s(0
.99)
s(0
.95)
s(0
.90)
E
uc
.
 dist
.
L1
 dist
.
cosine
inner
 prod
.distance/similarity measure
di
s c
rim
in
a t
io
n
 
ra
te
 
(%
)
5.0
7.0
9.0
11.0
13.0
15.0
17.0
19.0
21.0
23.0
25.0
sco
re
disc. rate
score
Fig. 6. Performances of distance/similarity
measures
70.0%
71.0%
72.0%
73.0%
74.0%
75.0%
76.0%
77.0%
78.0%
0 5 10 15 20 25 30
threshold
di
sc
rim
in
at
io
n
 r
at
e 
(%
)
d=100
d=200
d=300
Fig. 7. Discrimination rate measured by varying
threshold tf
4.5 Comparison Experiments with Conventional Methods
Here the performances of PLSI and the following conventional methods are compared.
In the following, N and M denote the numbers of nouns and verbs, respectively.
? tf: The number of co-occurrence tf ij of noun ni and verb vj is used directly for
similarity calculation. The corresponding vector ni to noun ni is given by:
ni = t[tf i1 tf
i
2 ... tf
i
M ]. (6)
? tf?idf: The vectors given by tf method are weighted by idf. That is,
n?i =
t[tf i1 ? idf1 tf i2 ? idf2 ... tf iM ? idfM ], (7)
where idfj is given by
idfj =
log(N/dfj)
maxk log(N/dfk)
, (8)
using dfj , the number of distinct nouns that co-occur with verb vj .
? tf+LSI: A co-occurrence matrix X is created using vectors ni defined by tf:
X = [n1 n2 ... nN ], (9)
to which LSI is applied.
? tf?idf+LSI : A co-occurrence matrix X? is created using vectors n?i defined by
tf?idf:
X? = [n?1 n
?
2 ... n
?
N ], (10)
to which LSI is applied.
? Hindle?s method: The method described in [7] is used. Whereas he deals only
with subjects and objects as verb-noun co-occurrence, we used all the kinds of
co-occurrence mentioned in Sect. 3.1, including prepositional relations.
PLSI Utilization for Automatic Thesaurus Construction 343
60.0%
62.0%
64.0%
66.0%
68.0%
70.0%
72.0%
74.0%
76.0%
78.0%
number of latent classes
di
s c
rim
in
a t
io
n
 r
a t
e  
(%
)
PLSI
tf
?
idf+LSI
tf+LSI
tf
?
idf
tf
Hindletf
tf
?
idf
Hindle
0 500 1000
5.0
7.0
9.0
11.0
13.0
15.0
17.0
19.0
21.0
23.0
number of latent classes
sc
o
re
Hindle
tf
?
idf
tf
0 500 1000
Fig. 8. Performances of PLSI and conventional methods
The values of discrimination rate and scores are calculated for PLSI as well as the
methods described above, and the results are shown in Fig. 8. Because the number of
latent classes d must be given beforehand for PLSI and LSI, the performances of the
latent semantic models are measured varying d from 50 to 1,000 with a step of 50. The
cosine measure is used for the similarity calculation of tf, tf?idf, tf+LSI, and tf?idf+LSI.
The results reveal that the highest discrimination rate is achieved by PLSI, with the
latent class number of approximately 100, although LSI overtakes with an increase of
d. As for the scores, the performance of PLSI stays on top for almost all the values of
d, strongly suggesting the superiority of PLSI over the conventional method, especially
when d is small, which is often.
The performances of tf and tf+LSI, which are not weighted by idf, are consistently
low regardless of the value of d. PLSI and LSI distinctly behave with respect to d,
especially in the discrimination rate, whose cause require examination and discussion.
4.6 Integration of PLSI Results
In maximum likelihood estimation by EM algorithm, the initial parameters are set to
values chosen randomly, and likelihood is increased by an iterative process. Therefore,
the results are generally local extrema, not global, and they vary every execution, which
is unfavorable. To solve this problem, we propose to execute PLSI several times and
integrate the results to obtain a single one.
To achieve this, PLSI is executed several times for the same co-occurrence data
obtained via the method described in Sect. 3.1. This yields N values of similarity
sim1(ni, nj), ..., simN (ni, nj) for each noun pair (ni, nj). These values are integrated
using one of the following four schemes to obtain a single value of similarity sim(ni, nj).
? arithmetic mean: sim(ni, nj) = 1N
?N
k=1 simk(ni, nj)
? geometric mean:sim(ni, nj) = N
?
?N
k=1 simk(ni, nj)
? maximum: sim(ni, nj) = maxk simk(ni, nj)
? minimum: sim(ni, nj) = mink simk(ni, nj)
344 M. Hagiwara, Y. Ogawa, and K. Toyama
70.0%
71.0%
72.0%
73.0%
74.0%
75.0%
76.0%
77.0%
78.0%
di
s c
rim
in
a t
io
n
 
ra
te
 
(%
)
15.0
17.0
19.0
21.0
23.0
25.0
sco
re
disc . rate
score
1 2 3 arith.
mean
geo.
mean
max min
before integration after integration
Fig. 9. Integration result for N = 3
71.0%
72.0%
73.0%
74.0%
75.0%
76.0%
77.0%
1 2 3 4 5 6 7 8 9 10
N
di
sc
rim
in
at
io
n
 r
at
e 
(%
)
15.0
17.0
19.0
21.0
23.0
25.0
27.0
29.0
31.0
sc
o
re
integrated (disc. score)
maximum (disc. score)
average (disc. rate)
integrated (score)
maximum (score)
average (score)
Fig. 10. Integration results varying N
Integration results are shown in Fig. 9, where the three sets of performance on the
left are the results of single PLSI executions, i.e., before integration. On the right are
the results after integration by the four schemes. It can be observed that integration
improves the performance. More specifically, the results after integration are as good or
better than any of the previous ones, except when using the minimum as a scheme.
An additional experiment was conducted that varied N from 1 to 10 to confirm that
such performance improvement is always achieved by integration. Results are shown in
Fig. 10, which includes the average and maximum of the N PLSI results (unintegrated)
as well as the performance after integration using arithmetic average as the scheme.
The results show that the integration consistently improves the performance for all 2 ?
N ? 10. An increase of the integration performance was observed for N ? 5, whereas
increases in the average and maximum of the unintegrated results were relatively low.
It is also seen that using N > 5 has less effect for integration.
5 Conclusion
In this study, automatic synonym acquisition was performed using a latent semantic
model PLSI by estimating the latent class distribution for each noun. For this purpose,
co-occurrences of verbs and nouns extracted from a large corpus were utilized. Discrim-
ination rates and scores were used to evaluate the current method, and it was found that
PLSI outperformed such conventional methods as tf?idf and LSI. These results make
PLSI applicable for automatic thesaurus construction. Moreover, the following tech-
niques were found effective: (1) employing Skew Divergence as the distance/similarity
measure between probability distributions; (2) removal of words with low frequencies,
and (3) multiple executions of PLSI and integration of the results.
As future work, the automatic extraction of the hierarchical relationship of words
also plays an important role in constructing thesauri, although only synonym relation-
ships were extracted this time. Many studies have been conducted for this purpose, but
extracted hyponymy/hypernymy relations must be integrated in the synonym relations
to construct a single thesaurus based on tree structure. The characteristics of the latent
class distributions obtained by the current method may also be used for this purpose.
PLSI Utilization for Automatic Thesaurus Construction 345
In this study, similarity was calculated only for nouns, but one for verbs can be
obtained using an identical method. This can be achieved by pairing noun n and case /
preposition c of co-occurrence (v, c, n), not v and c as previously done, and executing
PLSI for the dyadic data (v, (c, n)). By doing this, the latent class distributions for each
verb v, and consequently the similarity between them, are obtained.
Moreover, although this study only deals with verb-noun co-occurrences, other in-
formation such as adjective-noun modifications or descriptions in dictionaries may be
used and integrated. This will be an effective way to improve the performance of auto-
matically constructed thesauri.
References
1. Bilmes, J. 1997. A gentle tutorial on the EM algorithm and its application to parameter
estimation for gaussian mixture and hidden markov models. Technical Report ICSI-TR-97-
021, International Computer Science Institute (ICSI), Berkeley, CA.
2. Charniak, E. 2000. A maximum-entropy-inspired parser. NAACL 1, 132?139.
3. Collins. 2002. Collins Cobuild Major New Edition CD-ROM. HarperCollins Publishers.
4. Collins, M. 1996. A new statistical parser based on bigram lexical dependencies. Proc. of
34th ACL, 184?191.
5. Deerwester, S., et al 1990. Indexing by Latent Semantic Analysis. Journal of the American
Society for Information Science, 41(6):391?407.
6. Fellbaum, C. 1998. WordNet: an electronic lexical database. MIT Press.
7. Hindle, D. 1990. Noun classification from predicate-argument structures. Proc. of the 28th
Annual Meeting of the ACL, 268?275.
8. Hofmann, T. 1999. Probabilistic Latent Semantic Indexing. Proc. of the 22nd International
Conference on Research and Development in Information Retrieval (SIGIR ?99), 50?57.
9. Hofmann, T. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Ma-
chine Learning, 42:177?196.
10. Kojima, K., et. al. 2004. Existence and Application of Common Threshold of the Degree of
Association. Proc. of the Forum on Information Technology (FIT2004) F-003.
11. Lee, L. 2001. On the Effectiveness of the Skew Divergence for Statistical Language Analysis.
Artificial Intelligence and Statistics 2001, 65?72.
12. Lin, J. 1991. Divergence measures based on the shannon entropy. IEEE Transactions on
Information Theory, 37(1):140?151.
13. Mochihashi, D., Matsumoto, Y. 2002. Probabilistic Representation of Meanings. IPSJ SIG-
Notes Natural Language, 2002-NL-147:77?84.
14. The National Institute of Japanese Language. 2004. Bunruigoihyo. Dainippontosho.
15. Santorini, B. 1990. Part-of-Speech Tagging Guidelines for the Penn Treebank Project.
ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz
16. Schmid, H. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. Proc. of the
First International Conference on New Methods in Natural Language Processing (NemLap-
94), 44?49.
17. Ueda, N., Nakano, R. 1998. Deterministic annealing EM algorithm. Neural Networks,
11:271?282.
