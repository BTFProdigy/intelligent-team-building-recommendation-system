Named Entity Chunking Techniques 
in Supervised Learning for Japanese Named Entity Recognition 
Manabu Sassano  
Fujitsu Laboratories, Ltd. 
4-4-1, Kamikodanaka, Nakahara-ku, 
Kawasaki 211-8588, Japan 
sassano(@ilab.fujitsu.(:o.j I) 
Takeh i to  Utsuro  
l )el)artment of Intbrlnation 
and Computer  Sciences, 
Toyohashi University of Technology 
lcnl)aku-cho, ~l~)yohashi 441-8580, Jat)an 
utsm'og))ics, rut. ac.j p 
Abst rac t  
This 1)aper focuses on the issue of named entity 
chunking in Japanese named entity recognition. 
We apply the SUl)ervised decision list lean> 
ing method to Japanese named entity recogni- 
tion. We also investigate and in(:ori)orate sev- 
eral named-entity noun phrase chunking tech.- 
niques and experimentally evaluate and con> 
t)are their l)erfornlanee, ill addition, we t)rot)ose 
a method for incorporating richer (:ontextua\] 
ilflbrmation as well as I)atterns of constituent 
morphenms within a named entity, which h~ve 
not 1)een considered ill previous research, and 
show that the t)roi)osed method outt)erfi)rms 
these t)revious ai)proa('hes. 
1 I n t roduct ion  
It is widely a.greed that named entity recog- 
nition is an imt)ort;ant ste t) ti)r various al)pli- 
(:ations of natural language 1)ro(:('.ssing such as 
intbnnation retrieval, maclfine translation, in- 
tbrmation extraction and natural language un- 
derstanding. In tile English language, the 
task of named entity recognition is one of the 
tasks of the Message Understanding Confer- 
once (MUC) (e..g., MUC-7 (19!)8)) and has 
be.on studied intensively. In the .}al)anese lan- 
guage~ several recent conferences, uch as MET 
(Multilingual Entity Task, MET-I (Maiorano, 
1996) and MET-2 (MUC, 1998)) and IREX (In- 
formation l{etriew~l and Extraction Exercise) 
Workshop (IREX Committee, 1999), focused on 
named entity recognition ms one of their con- 
test tasks, thus promoting research on Jat)anese 
named entity recognition. 
In Japanese named entity recognition, it is 
quite common to apply morphological analy- 
sis as a t)reprocessing step and to segment 
the sentence string into a sequence of mor- 
i)henles. Then, hand-crafted t)attern m~tching 
rules and/or statistical named entity recognizer 
are apt)lied to recognize named entities. It is 
ofl;en the case that named entities to be rec- 
ognized have different segmentation boundaries 
from those of morpheums obtained by the mor- 
phological analysis. For example, in our anal- 
ysis of the \]Ill,F,X workshop's training corpus of 
llallled entities, about half of the mtmed enti- 
ties have segmentation boundaries that al'e dif- 
ferellt \]'rein the result of morphological nalysis 
t)y a .\]al)anese lnorphological nalyzer BI~EAK- 
FAST (Sassano et al, 1997) (section 2). Thus, in 
.Japanese named entity recognition: among the 
most difficult problems is how to recognize such 
named entities that have segmentation bound- 
ary mismatch against he morphemes ot)tained 
l)y morphological nalysis. Furthermore, in al- 
most 90% of (:ases of those segmentation t)oulld- 
ary mismatches, named entities to l)e recognized 
can t)e (teconq)osed into several mort)heroes as 
their constituents. This means that the 1)roblem 
of recognizing named entities in those cases can 
be solved by incorporating techniques of base 
noun phrase chunking (Ramshaw and Marcus, 
1995). 
In this paper, we tbcus on the issue of named 
entity chunking in Japanese name.d entity recog- 
nition. First, we take a supervised learning ap- 
proach rather than a hand-crafted rule based 
approach, because the tbnner is nlore promis- 
ing than the latter with respect o the amomlt 
of  human labor if requires, as well as its adaI)t- 
abi l i ty to a new domain  or a new def init ion of  
named entities. In general, creating training 
data tbr supervised learning is somewhat easier 
than creating pattern matching rules by hand. 
Next, we apply Yarowsky's method tbr super- 
vised decision list learning I (Yarowsky, 1994) to 
1VVe choose tile decision list learning method as the 
705 
Table 1: Statistics of NE Types of IREX 
NE Type 
ORGANIZATION 
PERSON 
LOCATION 
ARTIFACT 
DATE 
TIME 
MONEY 
PERCENT 
Total 
frequency (%) 
Training 
3676 (19.7) 
3840 (20.6) 
5463 (29.2) 
747 (4.0) 
3567 (19.1) 
502 (2.7) 
390 (2.1) 
492 (2.6) 
18677 
Test 
361 (23.9) 
338 (22.4) 
413 (27.4) 
48 (3.2) 
260 (17.2) 
54 (3.5) 
15 (1.0) 
21 (1.4) 
1510 
Japanese named entity recognition, into which 
we incorporate several noun phrase chunking 
techniques (sections 3 and 4) and experimen- 
tally evaluate their performance on the IREX 
, workshop's training and test data (section 5). 
As one of those noun phrase chunking tech- 
niques, we propose a method for incorporating 
richer contextual information as well as patterns 
of constituent morphemes within a named en- 
tity, compared with those considered in tire pre- 
vious research (Sekine et al, 1998; Borthwick, 
1999), and show that the proposed method out- 
perlbrms these approaches. 
2 Japanese Named Ent i ty  
Recogn i t ion  
2.1 Task of the IREX Workshop 
The task of named entity recognition of the 
IREX workshop is to recognize ight named en- 
tity types in Table 1 (IREX Conmfittee, 1999). 
The organizer of the IREX workshop provided 
1,174 newspaper articles which include 18,677 
named entities as tire training data. In the for- 
mal run (general domain) of the workshop, the 
participating systems were requested to recog- 
nize 1,510 nanmd entities included in the held- 
out 71 newspaper articles. 
2.2 Segmentation Boundaries of 
Morphemes and Named Entities 
In the work presented here, we compare the seg- 
mentation boundaries of named entities in tire 
IREX workshop's training corpus with those of 
supervised learning technique mainly because it is easy 
to implement and quite straightibrward toextend a su- 
pervised lem'ning version to a milfimally supervised ver- 
sion (Collins and Singer, 1999; Cucerzan and Yarowsky, 
1999). We also reported in (Utsuro and Sassano, 2000) 
the experimental results of a minimally supervised ver- 
sion of Japanese named entity recognition. 
Table 2: Statistics of Boundary Match vs. Mis- 
lnatch of Morphemes (M) attd Named Entities 
(NE) 
Match/Misnmtch II freq. of NE Tags (%) 
1 M to 1 NE 10480 (56.1) 
n(> 2) Ms 
to 
1 NE 
n=2 
n=3 
n > 4 
4557 (24.4) 
1658 (8.9) 7175 
96o (5.1) (38.4) 
other boundary mismatch 1022 (5.5) 
Total J\[ 18677 
morphemes which were obtained through mor- 
phological analysis by a Japanese morphologi- 
cal attalyzer BREAKFAST (Sassano et al, 1997). 2 
Detailed statistics of the comparison are pro- 
vided in 'Fable 2. Nearly half of the named 
entities have bmmdary mismatches against he 
morI)hemes and also almost 90% of the named 
entities with boundary mismatches can be tie- 
composed into more than one morpheme. Fig-- 
ure 1 shows some examples of such cases, a
3 Chunk ing  and Tagging Named 
Ent i t ies  
In this section, we formalize the problem of 
named entity chunking in Japanese named en- 
tity recognition. We describe ~t novel tech- 
nique as well as those proposed in the previous 
works on nan ted entity recognition. The novel 
technique incorporates richer contextual infor- 
mation as well as p~tterns of constituent mor- 
phemes within ~ named entity, compared with 
the techniques proposed in previous research on 
named entity recognition and base noun phrase 
chunking. 
3.1 Task Definition 
First, we will provide out" definition of the task 
of Japanese named entity chunking. Suppose 
'~The set of part-of-speech tags of lllU.~AKFAST consists 
of about 300 tags. mmAKFaST achieves 99.6% part-of- 
speech accuracy against newspaper a ticles. 
aIn most cases of the "other boundary mismatch" in 
Table 2, one or more named entities have to be rec- 
ognized as a part of a correctly analyzed morpheme 
and those cases are not caused by errors of morpholog- 
ical analysis. One frequent example of this type is a 
Japanese verbal noun "hou-bei (visiting United States)" 
which consists of two characters "hou (visitin.q)" and "bet 
(United States)", where "bet (United States)" has to be 
recognized as <LOCATION>. \Ve believe that 1)ouudary 
mismatches ofthis type can be easily solved by employ- 
ink a supervised learning technique such as the decision 
list learning method. 
706 
'Dfl)le 3: Exmoding Schemes of Named Entity Chunldng States 
Named Entity Tag 
Mort)heine Sequence 
Illside/()utside Encoding 
Stmt/End Encoding 
<0RG> <LOC> <L0C> 
- . .  M I M M \] M 
0 0RG_I 0 L0C_I LOC_I LOC_I LOC_B 0 
0 0RG_U 0 LOC_S L0C_C LOC_E LOC_U 0 
V Mo i - l )hemes  to 1 Named Ent i ty \ ]  
<ORGANIZATION> 
.... Roshia gun -..  
( S<,s,~i,. 0 ( a,',,.j) 
<PERSON> 
.... Murayama IbIni ichi shushOUprimc \]"" 
(last nmne) (first name) ( minister" 
\[3 Morphemes to1 Named Entity\] 
<TIME> 
gozen ku .ii " ? 
(AM) (niuc) (o ?1ock) 
<ARTIFACT> 
hokubei .jiyuu-1)oueki kyoutei - - ? 
Norfl~ 
( America ) (flee trade.) (treaty) 
Figure 1: Ex~mq)les of B(mndary Mismatch of 
Morl)hemes mid Named Entities 
that a sequen('e of morl)hemes i given as 1)e- 
low: 
Left; l{,ight 
( Context ) (Named Entity) ( Context; )
? . .~ I ' _ '~ . . .~ IL  ~, i~ '~. . .M/ ' . . .~ , l / ,  ''~ ~1~". . .~, f / " . . .  
t 
(Current Position) 
Then, given tht~t the current t)osition is at 
the morpheme M .N1': the task of tanned elltity 
eh l l l l k i l lg  is to  ass ign  a, C\] luuki l lg  s ta te  (to })e de-  
scribed in Section 3.2) as well ~rs a nmned entity 
type to the morl)helne Mi NE at tim current po- 
sition, considering the patterns of surrounding 
morl)hemes. Note that in the SUl)ervised learn- 
ing phase we can use the (:lmnking iuibnnation 
on which morphemes constitute a ngune(l entity, 
and whi(-h morphemes are in the lefl;/right con- 
texts of tit(; named entity. 
3.2 Encoding Schemes of Named 
Ent i ty  Chunking States 
In this t)at)er, we evalu~te the following two 
s('hemes of encoding ctmnking states of nalned 
entities. EXalnples of these encoding s(:hemes 
are shown in Table 3. 
3.2.1 Ins ide/Outs ide  Encoding 
The Inside/Outside scheme of encoding chunk- 
ing states of base noun phrases was studied in 
Ibmlshaw and Marcus (1995). This scheme dis- 
tinguishes the tbllowing three states: 0 the 
word at the current position is outside any base 
holm phrase. I the word at the current po- 
sition is inside some base holm phrase. B the 
word at the current position marks the begin- 
ning of ~ base noml t)hrase that immediately fop 
lows another base noun phrase. We extend this 
scheme to named entity chunking by further dis- 
tinguishing each of the states I and B into eight 
named entity types. 4 Thus, this scheme distin- 
guishes 2 x 8 + 1 = 17 states. 
3.2.2 S tar t /End  Encoding 
The Start /End scheme of encoding clmnking 
states of nmned entities was employed in Sekine 
e,t al. (1998) and Borthwick (1999). This 
scheme distinguishes the, following four states 
for each named entity type: S the lllOlTt)\]lellle 
at the (:urreld; position nmrks the l)eginldng of a 
lUl.in(xt (;lltity consisting of more than one mor- 
1)\]mme. C l;he lnOrl)heme ~I; the cm'r(mt )osi -
tion marks the middle of a mmmd entity (:onsist- 
ing of more tlmn one lilOrt)hellle. E -- the illOf 
t)heme, at the current position ram:ks the ending 
of a n~mmd entity consisting of more than one 
morl)heme. U - the morpheme at the current 
t)osition is a named entity consisting of only one, 
mort)heine. The scheme ;dso considers one ad(li- 
tional state for the position outside any named 
entity: 0 t;he mort)heine at the current posi- 
tion is outside any named entity. Thus, in our 
setting, this scheme distinguishes 4 x 8 + 1 = 33 
states. 
a.3 Preced ing /Subsequent  Morphemes 
as Contextua l  Clues 
In this l)aper, we ewfluate the following two 
l l l ode ls  of  considering preceding/subsequent 
4\Ve allow the, state :c_B for a named entity tyt)e x 
only when the, morl)hcme at t, he current 1)osition marks 
the 1)egimdng ofa named entity of the type a" that im- 
mediately follows a nmned entity of the same type x. 
707 
morphemes as contextual clues to named entity 
clmnking/tagging. Here we provide a basic out- 
line of these models, and the details of how to 
incorporate them into the decision list learning 
framework will be described in Section 4.2.2. 
3.a.1 3-gram Model 
In this paper, we refer to the model used in 
Sekine et al (1998) and Borthwick (1999) as a 
3-gram model. Suppose that the current posi- 
tion is at the morpheme M0, as illustrated be- 
low. Then, when assigning a chunking state as 
well as a named entity type to the morpheme 
M0, the 3-gram model considers the preceding 
single morpheme M-1 as well as the subsequent 
single morpheme M1 as the contextual clue. 
Left Current Right 
( Context ) ( Position ) ( Context ) 
? . .  M0 M, . . .  (1) 
The major disadvantage of the 3-gram model 
is that in the training phase it does not 
take into account whether or not the l)re- 
ceding/subsequent morphemes constitute one 
named entity together with the mort)heine at 
the current position. 
a.a.2 Variable Length Model 
In order to overcome this disadvantage of the 3- 
gram model, we propose a novel model, namely 
the "Variable Length Model", which incorpo- 
rates richer contextual intbrmation as well as 
patterns of constituent morl)hemes within a 
named entity. In principle, as part of the train- 
ing phase this model considers which of the pre- 
ceding/subsequent morphenms constitute one 
named entity together with the morpheme at 
the current position. It also considers sev- 
eral morphemes in the lefl;/right contexts of the 
named entity. Here we restrict this model to ex- 
plicitly considering the cases of named entities 
of the length up to three morphenms and only 
implicitly considering those longer than three 
morphemes. We also restrict it to considering 
two morphemes in both left and right contexts 
of the named entity. 
Left 
( Context ) 
... ML2MI_'I 
ll,ight 
(Named Entity) ( Context ) 
M# . . .  ... Mm(<3 ) 
1" (2) 
(Current Position) 
4 Superv ised Learning for Japanese 
Named Ent i ty  Recogn i t ion  
This section describes how to apply tile deci- 
sion list learning method to chunking/tagging 
named entities. 
4.1 Decision List Learning 
A decision list (Rivest, 1987; Yarowsky, 1994) 
is a sorted list of decision rules, each of which 
decides the wflue of a decision D given some ev- 
idence E. Each decision rule in a decision list is 
sorted in descending order with respect o some 
preference value, and rules with higher prefer- 
ence values are applied first when applying the 
decision list to some new test; data. 
First, the random variable D representing a 
decision w, ries over several possible values, and 
the random w~riable E representing some evi- 
dence varies over '1' and '0' (where '1' denotes 
the presence of the corresponding piece of evi- 
dence, '0' its absence). Then, given some train- 
ing data in which the correct value of the deci- 
sion D is annotated to each instance, the con- 
ditional probabilities P(D = x I E = 1) of ob- 
serving the decision D = x under the condition 
of the presence of the evidence E (E = 1) are 
calculated and the decision list is constructed 
by the tbllowing procedure. 
1. For each piece of evidence, we calculate the 
Iw of likelihood ratio of the largest; condi- 
tional probability of the decision D = :rl 
(given the presence of that piece of ev- 
idence) to the second largest conditional 
probability of the decision D =x2: 
I E=I) 
l?g2 P(D=x2 I E=I )  
Then~ a decision list is constructed with 
pieces of evidence sorted in descending or- 
der with respect to their log of likelihood 
ratios, where the decision of the rule at each 
line is D = xl with the largest conditional 
probabil i ty) 
'~Yarowsky (1994) discusses everal techniques for 
avoiding the problems which arise when an observed 
count is 0. lq-om among those techniques, we employ 
tlm simplest ram, i.e., adding a small constant c~ (0.1 < 
< 0.25) to the numerator and denominator. With 
this inodification, more frcquent evidence is preferred 
when several evidence candidates exist with the same 
708 
2. The final line of a decision list; ix defined as 
% default', where the log of likelihood ratio 
is calculated D<)m the ratio of the largest; 
marginal )robability of the decision D = x t 
to the second largest marginal l)rol)at)ility 
of the decision D =x2: 
P(D =:/11) 
log~ p (D = x'2) 
The 'default' decision of this final line is 
D = Xl with the largest lnarginal probabil- 
ity. 
4.2 Decision List Learning for 
Chunking/Tagging Named Entities 
4.2.1 Decision 
For each of the two schemes of enco(li1~g chunk- 
ing states of nalned entities descrit)ed in Sec- 
tion 3.2, as the l)ossible values of the <teei- 
sion D, we consider exactly the same categories 
of chunking states as those described in Sec- 
tion 3.2. 
4.2.2 Evidence 
The evidence E used in the decision list learn- 
ing is a combination of the tbatures of preced- 
ing/subsequent inorphemes as well as the mor- 
pheme at; the current position. The following 
describes how to form the evidence E fi)r 1)oth 
the a-gram nlodel and varial)le length model. 
3-,gram Model 
The evidence E ret)resents a tut)le (F - l ,  F0, F1 ), 
where F-1 and F1 denote the features of imme- 
diately t)receding/subsequent morphemes M_~ 
and M1, respectively, F0 the featm:e of the mor- 
pheme 54o at the current position (see Fonnuta 
(1) in Section 3.3.1). The definition of the pos- 
sible values of those tbatures F_l ,  F0, and 1'~ 
are given below, where Mi denotes the roo f  
1)\]mnm itself (i.e., including its lexicM tbrm as 
well as part-of-sl)eech), C,i the character type 
(i.e., JaI)anese (hiragana or katakana), Chinese 
(kanji), numbers, English alphabets, symbols, 
and all possible combinations of these) of Mi, 
Ti the part-of-st)eech of Mi: 
F_  1 ::m_ \]~//--1 I (C -1 ,  T - l )  I T - t  Inu l l  
mlsmoothed conditional probability P(D = x \[ E = 1). 
Yarowsky's training Mgoritl,m also ditfcrs omewhat in 
his use of the ratio *'(~D=,d*~-j)' which is equivalent in 
the case of binary classifications, and also by the interpo- 
lation between the global probalfilities (used here) and 
tl,e residual prol)abilities further conditional on higher- 
ranked patterns failing to match in the list. 
17'1 ::--~ \]~/-/1 I (C , ,V ; ) I  T* Inu l \ ]  
F0 ::- M0 I(C0,T0) lT0 
As the evidence E, we consider each possible 
coml)ination of the values of those three f'ea- 
lures. 
Variable Length Model 
The evidence E rel>resents a tuple 
(FL,FNu, FIt), where FL and Fl~ denote 
the features of the morphemes ML_2ML1 and 
Mff'M~ ~ in the left/right contexts of the current 
named entity, respectively, FNE the features 
of the morphemes MN~""  " MNE " "" MNEm(_<3) 
constituting the current named entity (see 
Formula (2) in Section 3.3.2). The definition of 
the possible values of those features 1 L, FNI,:, 
and FI~ arc given below, where F NI~ denotes 
the feature of the j - th constituent morpheme 
M .NJ~ within the current nalne(1 entity, and a 
k/l NI~ is the morl)heme at the cm'ren~ i)osition: 
FL ::= M*_'2M~ ~ \ [M~ Inull 
Fu ::= M\ ]~M~IM~Inu l l  
FNE FNEFNE IFNE r.NI'2 7z~NE FNE : := i i+1 i+2 \[ * i-1 * i * i+1 
\] I~NI'A~NI'21pNE 17NE~NI,2 
? , FNE , (3) 
~NIC MN~c (Cm,: T~VJ~ , ,NI,: 
As the evidence E, we consider each possit)le 
(:oml)ination of the wfiues of those three fba- 
tures, except that the tbllowing three restric- 
tions are applied. 
1. In the cases where the current named en- 
tity consists of up to three mort)heroes , as 
the possible values of the feature FNIi in 
the definition (3), we consider only those 
which are consistent with the requirement 
that each nlort)heme M NE is a constituent 
of the cun'ent named entity. For exainple, 
suppose that the cun'ent named entity con- 
sists of three morphemes, where the cur- 
rent position is at the middle of those con- 
stituent morphemes as below: 
Left Right 
( Context ) (Named Entity) ( Context ) 
I. L M1N~'M N+~M~u I~ t~ ? " M1 Mi -'- _/l//_ 2/~//_ 1 
1" (4) 
(Current Position) 
Then, as the possible values of the feature 
FN\],;, we consider only the tbllowing ibm': 
rN .  ::= \[ F.g U.g 
709 
2. II1 the cases where the eurrellt ilalned entity 
consists of more than three morphemes, 
only the three constituent morphemes are 
regarded as within the current named en- 
tity and the rest are treated as if they 
were outside the named entity. For exam- 
pie, suppose that the current named en- 
tity consists of four morphemes as below: 
Left Right 
( Context ) (Named Entity) ( Context ) 
L L 
$ 
(Current Position) 
Iit this case, the fourth constitnent mor- 
pheme M N1c is treated as if it were in the 
right context of the current named entity 
as below: 
Left Right 
( Context ) (Named Entity) ( Context ) 
'.,~ 1. ~r.,vJ,:~,Nu ~,.,,'.r,_,C M~ZMff 
t 
(Curren~ Position) 
3. As the evidence E, among the possible 
combination of the values of three t'ea- 
tures /~,, ENId, and F/t, we only accept 
those in which the positions of the mor- 
phemes are continuous, and reject those 
discontimmus combinations. For example, 
in the case of Formula (4:) above, as the 
evidence E, we accel)t the combination 
(Mq,  M 'My , ull), while we r( iect 
(ML1, M~EM~ 1':, 1,ull). 
4.3 Procedures  for Training and 
Testing 
Next we will briefly describe the entire pro- 
cesses of learning the decision list tbr etmnk- 
ing/tagging named entities as well as applying 
it to chunking/tagging unseen named entities. 
4.3.1 Training 
In the training phase, at the positions where 
the corresponding morpheme is a constitnent of 
a named entity, as described in Section 4.2, each 
al lowable combination of features is considered 
as the evidence E. On the other hand, at the 
positions where the corresponding morpheme is
outside any named entity, the way the combi- 
nation of t~at;ures i  considered is diflbrent in 
the variable length model, in that the exception 
\]. in the previous section is no longer applied. 
Theretbre, all the possible wflues of the feature 
FNB in Definition (3) are accepted. Finally, the 
frequency of each decision D and evidence E is 
counted and the decision list is learned as de- 
scribed in Section 4.1. 
4.3.2 Testing 
When applying the decision list to chunk- 
ing/tagging nnseen amed entities, first, at each 
morpheme position, the combination of features 
is considered as in the case of the non-entity po- 
sition in the training phase. Then, the decision 
list is consulted and all the decisions of the rules 
with a log of likelihood ratio above a certain 
threshold are recorded. Finally, as in the case 
of previous research (Sekine et al, 1998; Berth- 
wick, 1999), the most appropriate sequence of 
the decisions that are consistent throughout the 
whole sequence is searched for. By consistency 
of the decisions, we mean requirements such as 
that the decision representing the beginning of 
some named entity type has to be followed by 
that representing the middle of the same entity 
type (in the case of Start /End encoding). Also, 
in our case, the appropriateness of the sequence 
of the decisions is measured by the stun of the 
log of likelihood ratios of 1;t1(; corresponding de- 
cision rules. 
5 Exper imenta l  Eva luat ion  
We experimentally evaluate the performance 
of the supervised learning tbr Japanese nalned 
entity recognition on the IREX workshop's 
training and test data. We compare the re- 
suits of the confl)inations of the two encod- 
ing schemes of named entity chunking states 
(the Inside/Outside and the Start /End encod- 
ing schemes) and the two at)preaches to contex- 
tual feature design (the 3-gram and the Variable 
Length models). For each of those combina- 
tions, we search tbr an optintal threshold of the 
log of likelihood ratio in the decision list. The 
performance of each combination measured by 
F-measure (fl = 1) is given in Table 4. 
In this ewduation, we exclude the named 
entities with "other boundary mismatch" in 
Tat)le 2. We also classify the system 
output according to the number of con- 
stitnent lnorphemes of each named entity 
and evaluate the peribnnance tbr each sub- 
set of the system output. For each sub- 
710 
3-gram 
Variable 
Length 
' l 'al)le 4: Ewduation \]{esults Measured by F-measm'e (fl = 1) 
~, Mori)lmlnes to 1 Named Entity 
- . ,  > J_ll ,,, = I,,.=21,..=311.,>_21.,,.>_31,,>4 
inside/Outside 72.9 75.9 79.7 51.4 69.4 42.5 29.2 
Start/End 72.7 76.6 79.6 43.7 68.1 37.8 29.6 
inside/Outside lJ 74.3 77.6 80.0 55.5 70.9 49.9 41.0 
Start/End t \ [72.1  77.0 75.6 51.5 67.2 48.6 43.6 
set, we compare,' the performmme of the fore' 
combinations of {3-grmn, Vm'iable Length} ? 
{Inside/Outside, S|;~n't/EIld} mM show the 
highest mrtbrmance with bold-faced font. 
Several remarkable points of these re, suits of 
1)erfbrmance omparison can be stated as below: 
? Among the four coml)inations, the Variable 
Length Model with hlside/()utside Ent:od- 
ing 1)erfi)rms best in tot~fl (n > 1) as well 
as in the recognition of named entities con- 
sisting of more thml one morl)heme (',, -- 
2, 3, n > 2, 3). 
? in the re,(:ognil;ion of ilsAll(;d elll;ities con- 
sisting of more than two mOl"l)henles (~, = 
3: ?t ~ 3, 4)~ the Vm'ial)le Lellgth Model 
l)erforlllS signific;mtly t)etter thml the 3- 
rill "t(.~ {~l'alll mo(le\]. .tn\],' result (:letu'ly SUpl)orts 
the (;l~iin that our modeli\]xg of the Vm'i- 
nl)le Length Model has an adva,ntnge in the 
recognition ()f long named entities. 
" Ill general, the Inside/Outside n(:oding 
scheme l)erfol'lns slightly t)etl;er th;m the 
Sta\]'t/l'3nd encoding s(:henm, (Well though 
the tbrmer distinguislms (:onsidera|)ly ti~wer 
sl;ates th;m the latter. 
6 Conc lus ion  
In this 1)~per, we al)plied the supervised eci- 
si(m list learning method to ,\]at)anese mmmd en- 
tity recognition, into wlfich we, incorporated sev- 
eral n(mn phrase chunking teelmiques ~md ex- 
perimentally evaluated their pertbrmance. We, 
showed that a novel technique that we proposed 
out, performed those using previously considered 
(;otd;extual fe~tu\]:es. 
7 Acknowledgments  
This research was c~rried out while the au- 
thors were visiting scholars at l)epartment of 
Computer Science, Johns Hopkins University. 
The ~mthors would like to thank Prof  David 
Yarowsky of Johns Hopkins University for in- 
valual)le sut)porl;s to this research. 
References  
A. Borthwick. 1999. A JaI)mmse named entity rec- 
ognizer constructed by a non-speaker ofJapanese. 
In Proc. of the II~EX Workshop, pages 187 193. 
54. Collins a.nd Y. Singer. 1999. Unsupervised mod- 
els of named entity classification. In P.roc. of 
the 1999 Joint SIGDAT Cm@rcncc on Empiri- 
cal Mcth, ods in Natural Languagc P~vccssing and 
Very Large Corpora, pages 100 110. 
S. Cucerzmt and D. Yarowsky. 1999. l~anguage inde- 
1)endent named entity recognition combining mor- 
1)hological mid contextual evideime, in Proc. of 
th, c 1999 Joi'nt SIGDAT Cm@rence on Empiri- 
cal Methods in Natural Language PTvccssin9 mid 
Very Large Corpora, pages 90 99. 
IREX Committee, editor. 1!)99. P~vcecdings of the 
\]REX Workshop. (in Japanese). 
S. Maiorano. 1996. The multilingual entity task 
(MET): Jalmne, se, results. In ISvc. of TIPSTEI~, 
PIH)U1/,AM P HA,5'1'; 11, pa.ges 449 45\]. 
MUC. 1998. l)'rocccdings oJ"l,h,e, 7th Message Unde'r- 
standing ConJ?rence, (MUC-7). 
L. l{alnshaw and M. Ma.rcus. 1995. Text chunking 
using trmlsforma.tion-based \] mning. In P,roc. of 
th, c 3rd Work,vh, op on l/cry Larg(: Corpora~ Im.ges 
83 -94. 
ILL. Rive, st. 1987, Le, arning decision lists. Machine 
Learning, 2:229 246. 
M. Sassano, Y. Saito, and K. Matsui. 1997. 
,J~l)alle,se morphological mmlyzer for NLP apl)li- 
(:atioils. Ill Proc. of thc, 3rd gn'ttual Meeting of 
th, c Association for Natural Language Processing, 
1)a.ges 441- 444. (in Jal)anese). 
S. Sekine, lL Grishman, and H. Shinnou. 1998. 
A decision tree method tbr tinding and ('lassit~- 
ing names in Jat)almse texts. In Proc. of the 6th 
Workshop on Very La~yc Ctnpora, pages 148-152. 
T. Utsuro mid M. Sassano. 2000. Minimally su- 
pervised ,\]almnese named e, ntity recognition: I{e- 
source, s and evahmtion. In Proc. of thc 2nd Inter- 
national Confcrcncc on Lanquaqc Resources and 
Evahtation, pages 1229 -1236. 
1). Yarowsky. 1994. Decision lists for lexical mnbi- 
guity resolution: Al)t)lication to accent restora.- 
tion in Spanish and French. In Proc. of the 32rid 
Annual Mecl, ing of ACL, 1)ages 88 -95. 
711 
Linear-Time Dependency Analysis for Japanese
Manabu Sassano
Fujitsu Laboratories, Ltd.
4-1-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
sassano@jp.fujitsu.com
Abstract
We present a novel algorithm for Japanese dependency
analysis. The algorithm allows us to analyze dependency
structures of a sentence in linear-time while keeping a
state-of-the-art accuracy. In this paper, we show a formal
description of the algorithm and discuss it theoretically
with respect to time complexity. In addition, we eval-
uate its efficiency and performance empirically against
the Kyoto University Corpus. The proposed algorithm
with improved models for dependency yields the best ac-
curacy in the previously published results on the Kyoto
University Corpus.
1 Introduction
Efficiency in parsing as well as accuracy is one of
very important issues in natural languages process-
ing. Although we often focus much on parsing ac-
curacy, studies of its efficiency are also important,
especially for practical NLP applications. Improv-
ing efficiency without loss of accuracy is a really big
challenge.
The main purpose of this study is to propose an
efficient algorithm to analyze dependency structures
of head final languages such as Japanese and to
prove its efficiency both theoretically and empiri-
cally. In this paper, we present a novel efficient al-
gorithm for Japanese dependency analysis. The al-
gorithm allows us to analyze dependency structures
of a sentence in linear-time while keeping a state-
of-the-art accuracy. We show a formal description
of the algorithm and discuss it theoretically with
respect to time complexity. In addition to this,
we evaluate its efficiency and performance empir-
ically against the Kyoto University Corpus (Kuro-
hashi and Nagao, 1998), which is a parsed corpus
of news paper articles in Japanese.
The remainder of the paper is organized as fol-
lows. Section 2 describes the syntactic characteris-
tics of Japanese and the typical sentence processing
of Japanese. In Section 3 previous work of depen-
dency analysis of Japanese as well as of English is
briefly reviewed. After these introductory sections,
our proposed algorithm is described in Section 4.
Next, improved models for estimating dependency
of two syntactic chunks called bunsetsus are pro-
posed in Section 5. Section 6 describes experimen-
tal results and discussion. Finally, in Section 7 we
conclude this paper by summarizing our contribu-
tions and pointing out some future directions.
2 Parsing Japanese
2.1 Syntactic Properties of Japanese
The Japanese language is basically an SOV lan-
guage. Word order is relatively free. In English the
syntactic function of each word is represented with
word order, while in Japanese postpositions repre-
sent the syntactic function of each word. For ex-
ample, one or more postpositions following a noun
play a similar role to declension of nouns in Ger-
man, which indicates a grammatical case.
Based on such properties, a bunsetsu1 was de-
vised and has been used to analyze syntactically a
sentence in Japanese. A bunsetsu consists of one
or more content words followed by zero or more
function words. By defining a bunsetsu like that,
we can analyze a sentence in a similar way that is
used when analyzing a grammatical role of words
in inflecting languages like German.
Thus, strictly speaking, bunsetsu order rather
than word order is free except the bunsetsu that con-
tains a main verb of a sentence. Such bunsetsu must
be placed at the end of the sentence. For example,
the following two sentences have an identical mean-
ing: (1) Ken-ga kanojo-ni hon-wo age-ta. (2) Ken-
ga hon-wo kanojo-ni age-ta. (-ga: subject marker,
-ni: dative case particle, -wo: accusative case par-
ticle. English translation: Ken gave a book to her.)
Note that the rightmost bunsetsu ?age-ta,? which is
composed of a verb stem and a past tense marker,
has to be placed at the end of the sentence.
1
?Bunsetsu? is composed of two Chinese characters, i.e.,
?bun? and ?setsu.? ?Bun? means a sentence and ?setsu? means
a segment. A ?bunsetsu? is considered to be a small syntactic
segment in a sentence. A eojeol in Korean (Yoon et al, 1999)
is almost the same concept as a bunsetsu. Chunks defined in
(Abney, 1991) for English are also very similar to bunsetsus.
We here list the constraints of Japanese depen-
dency including ones mentioned above.
C1. Each bunsetsu has only one head except the
rightmost one.
C2. Each head bunsetsu is always placed at the
right hand side of its modifier.
C3. Dependencies do not cross one another.
These properties are basically shared also with Ko-
rean and Mongolian.
2.2 Typical Steps of Parsing Japanese
Since Japanese has the properties above, the follow-
ing steps are very common in parsing Japanese:
1. Break a sentence into morphemes (i.e. mor-
phological analysis).
2. Chunk them into bunsetsus.
3. Analyze dependencies between these bunset-
sus.
4. Label each dependency with a semantic role
such as agent, object, location, etc.
We focus on dependency analysis in Step 3.
3 Previous Work
We review here previous work, mainly focusing on
time complexity. In English as well as in Japanese,
dependency analysis has been studied (e.g., (Laf-
ferty et al, 1992; Collins, 1996; Eisner, 1996)). The
parsing algorithms in their papers require 
time where  is the number of words.2
In dependency analysis of Japanese it is very
common to use probabilities of dependencies be-
tween each two bunsetsus in a sentence. Haruno
et al (1998) used decision trees to estimate the
dependency probabilities. Fujio and Matsumoto
(1998) applied a modified version of Collins? model
(Collins, 1996) to Japanese dependency analysis.
Both Haruno et al, and Fujio and Matsumoto used
the CYK algorithm, which requires  time,
where  is a sentence length, i.e., the number of
bunsetsus. Sekine et al (2000) used Maximum
Entropy (ME) Modeling for dependency probabili-
ties and proposed a backward beam search to find
the best parse. This beam search algorithm re-
quires  time. Kudo and Matsumoto (2000)
also used the same backward beam search together
with SVMs rather than ME.
There are few statistical methods that do not use
dependency probabilities of each two bunsetsus.
2Nivre (2003) proposes a deterministic algorithm for pro-
jective dependency parsing, the running time of which is linear.
The algorithm has been evaluated on Swedish text.
Ken-ga kanojo-ni ano hon-wo age-ta.
Ken-subj to her that book-acc gave.
ID 0 1 2 3 4
Head 4 4 3 4 -
Figure 3: Sample Sentence
Sekine (2000) observed that 98.7% of the head lo-
cations are covered by five candidates in a sentence.
Maruyama and Ogino (Maruyama and Ogino, 1992)
also observed similar phenomena. Based on this ob-
servation, Sekine (2000) proposed an efficient anal-
ysis algorithm using deterministic finite state trans-
ducers. This algorithm, in which the limited num-
ber of bunsetsus are considered in order to avoid
exhaustive search, takes  time. However, his
parser achieved an accuracy of 77.97% on the Ky-
oto University Corpus, which is considerably lower
than the state-of-the-art accuracy around 89%.
Another interesting method that does not use de-
pendency probabilities between each two bunsetsus
is the cascaded chunking model by Kudo and Mat-
sumoto (2002) based on the idea in (Abney, 1991;
Ratnaparkhi, 1997). They used the model with
SVMs and achieved an accuracy of 89.29%, which
is the best result on the Kyoto University Corpus.
Although the number of dependencies that are es-
timated in parsing are significantly fewer than that
either in CYK or the backward beam search, the up-
per bound of time complexity is still .
Thus, it is still an open question as to how we an-
alyze dependencies for Japanese in linear time with
a state-of-the-art accuracy. The algorithm described
below will be an answer to this question.
4 Algorithm
4.1 Algorithm to Parse a Sentence
The pseudo code for our algorithm of dependency
analysis is shown in Figure 1. This algorithm is used
with any estimator that decides whether a bunsetsu
modifies another bunsetsu. A trainable classifier,
such as an SVM, a decision tree, etc., is a typical
choice for the estimator. We assume that we have
some classifier to estimate the dependency between
two bunsetsus in a sentence and the time complex-
ity of the classifier is not affected by the sentence
length.
Apart from the estimator, variables used for pars-
ing are only two data structures. One is for input
and the other is for output. The former is a stack for
keeping IDs of modifier bunsetsus to be checked.
The latter is an array of integers that stores head IDs
that have already been analyzed.
Following the presented algorithm, let us parse a
// Input: N: the number of bunsetsus in a sentence.
// w[]: an array that keeps a sequence of bunsetsus in the sentence.
// Output: outdep[]: an integer array that stores an analysis result, i.e., dependencies between
// the bunsetsus. For example, the head of w[j] is outdep[j].
//
// stack: a stack that holds IDs of modifier bunsetsus in the sentence. If it is empty, the pop
// method returns EMPTY ().
// function estimate dependency(j, i, w[]):
// a function that returns non-zero when the j-th bunsetsu should
// modify the i-th bunsetsu. Otherwise returns zero.
function analyze(w[], N, outdep[])
stack.push(0); // Push 0 on the stack.
for (int i = 1; i  N; i++)  // Variable i for a head and j for a modifier.
int j = stack.pop(); // Pop a value off the stack.
while (j != EMPTY && (i == N  1  estimate dependency(j, i, w))) 
outdep[j] = i; // The j-th bunsetsu modifies the i-th bunsetsu.
j = stack.pop(); // Pop a value off the stack to update j.

if (j != EMPTY)
stack.push(j);
stack.push(i);

Figure 1: Pseudo Code for Analyzing Dependencies. Note that ?i == N - 1? means the i-th bunsetsu is the
rightmost one in the sentence.
// indep[]: an integer array that holds correct dependencies given in a training corpus.
//
// function estimate dependency(j, i, w[], indep[]):
// a function that returns non-zero if indep[j] == i, otherwise returns zero.
// It also prints a feature vector (i.e., an encoded example) with a label which is decided to be
// 1 (modify) or -1 (not modify) depending on whether the j-th bunsetsu modifies the i-th.
function generate examples(w[], N, indep[])
stack.push(0);
for (int i = 1; i  N; i++) 
int j = stack.pop();
while (j != EMPTY && (i == N  1  estimate dependency(j, i, w, indep))) 
j = stack.pop();

if (j != EMPTY)
stack.push(j);
stack.push(i);

Figure 2: Pseudo Code for Generating Training Examples. Variables w[], N, and stack are the same as in
Figure 1.
sample sentence in Figure 3. For explanation, we
here assume that we have a perfect classifier as esti-
mate dependency() in Figure 1, which can return a
correct decision for the sample sentence.
First, we push 0 (Ken-ga) on the stack for the bun-
setsu ID at the top of the sentence. After this initial-
ization, let us see how analysis proceeds at each iter-
ation of the for loop. At the first iteration we check
the dependency between the zero-th bunsetsu and
the 1st (kanojo-ni). We push 0 and 1 because the
zero-th bunsetsu does not modify the 1st. Note that
the bottom of the stack is 0 rather than 1. Smaller
IDs are always stored at lower levels of the stack.
Due to this, we do not break the non-crossing con-
straint (C3. in Section 2.1).
At the second iteration we pop 1 off the stack and
check the dependency between the 1st bunsetsu and
the 2nd (ano). Since the 1st does not modify the
2nd, we again push 1 and 2.
At the third iteration we pop 2 off the stack and
check the dependency for the 2nd and the 3rd (hon-
wo). Since the 2nd modifies the 3rd, the dependency
is stored in outdep[]. The value of outdep[j] repre-
sents the head of the -th bunsetsu. For example,
outdep[2] = 3 means the head of the 2nd bunsetsu
is the 3rd. Then we pop 1 off the stack and check the
dependency between the 1st and the 3rd. We push
again 1 since the 1st does not modify the 3rd. After
that, we push 3 on the stack. The stack now has 3, 1
and 0 in top-to-bottom order.
At the fourth iteration we pop 3 off the stack. We
do not have to check the dependency between the
3rd and the 4th (age-ta) because the 4th bunsetsu is
the last bunsetsu in the sentence. Now we set out-
dep[3] = 4. Next, we pop 1 off the stack. Also in
this case, we do not have to check the dependency
between the 1st and the 4th. Similarly the zero-th
bunsetsu modifies the 4th. As a result we set out-
dep[1] = 4 and outdep[0] = 4. Now the stack is
empty and we finish the analysis function. Finally,
we have obtained a dependency structure through
the array outdep[].
4.2 Time Complexity
At first glance, the upper bound of the time com-
plexity of this algorithm seems to be  because
it involves a double loop; however, it is not. We will
show that the upper bound is  by considering
how many times the condition part of the while loop
in Figure 1 is executed. The condition part of the
while loop fails    times because the outer for
loop will be executed from 1 to  . On the other
hand, the same condition part successes  times
because outdep[j] = i is executed    times. For
each bunsetsu ID , outdep[j] = i is surely executed
once because by executing j = stack.pop() the value
of  is lost and it is never pushed on the stack again.
That is the body of the while loop will be executed
at most    times which is equal to the number
of the bunsetsus except the last one. Therefore the
total number of execution of the condition part of
the while loop is  , which is obtained by sum-
ming up  and . This means that the upper
bound of time complexity is .
4.3 Algorithm to Generate Training Examples
When we prepare training examples for the train-
able classifier used with this algorithm, we use the
algorithm shown in Figure 2. It is almost the same
as the algorithm for analyzing in Figure 1. The dif-
ferences are that we give correct dependencies to es-
timate dependency() through indep[] and we obvi-
ously do not have to store the head IDs to outdep[].
4.4 Summary and Theoretical Comparison
with Related Work
The algorithm presented here has the following fea-
tures:
F1. It is independent on specific machine learning
methodologies. Any trainable classifiers can
be used.
F2. It scans a sentence just once in a left-to-right
manner.
F3. The upper bound of time complexity is .
The number of the classifier call, which is most
time consuming, is at most   .
F4. The flow and the used data structures are very
simple. Therefore, it is easy to implement.
One of the most related models is the cascaded
chunking model by (Kudo and Matsumoto, 2002).
Their model and our algorithm share many fea-
tures including F1.3 The big difference between
theirs and ours is how many times the input sen-
tence has to be scanned (F2). With their model we
have to scan it several times, which leads to some
computational inefficiency, i.e., at the worst case


 computation is required. Our strict left-to-
right parsing is more suitable also for practical ap-
plications such as real time speech recognition. In
addition, the flow and the data strucutres are much
simpler (F4) than those of the cascaded chunking
model where an array for chunk tags is used and it
must be updated while scanning the sentence sev-
eral times.
Our parsing method can be considered to be one
of the simplest forms of shift-reduce parsing. The
difference from typical use of shift-reduce parsing
is that we do not need several types of actions and
only the top of the stack is inspected. The reason for
these simplicities is that Japanese has the C2 con-
straint (Sec. 2.1) and the target task is dependency
analysis rather than CFG parsing.
5 Models for Estimating Dependency
In order to evaluate the proposed algorithm empir-
ically, we use SVMs (Vapnik, 1995) for estimating
dependencies between two bunsetsus because they
have excellent properties. One of them is that com-
binations of features in an example are automati-
cally considered with polynomial kernels. Excellent
performances have been reported for many classifi-
cation tasks. Please see (Vapnik, 1995) for formal
descriptions of SVMs.
3Kudo and Matsumoto (2002) give more comprehensive
comparison with the probabilistic models as used in (Uchimoto
et al, 1999).
At estimate dependency() in Figure 1, we encode
an example with features described below. Then we
give it to the SVM and receive the estimated deci-
sion as to whether a bunsetsu modifies the other.
5.1 Standard Features
By the ?standard features? here we mean the fea-
ture set commonly used in (Uchimoto et al, 1999;
Sekine et al, 2000; Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2002). We employ the fea-
tures below for each bunsetsu:
1. Rightmost Content Word - major POS, minor
POS, conjugation type, conjugation form, sur-
face form (lexicalized form)
2. Rightmost Function Word - major POS, minor
POS, conjugation type, conjugation form, sur-
face form (lexicalized form)
3. Punctuation (periods, and commas)
4. Open parentheses and close parentheses
5. Location - at the beginning of the sentence or
at the end of the sentence.
In addition, features as to the gap between two bun-
setsus are also used. They include: distance, parti-
cles, parentheses, and punctuation.
5.2 Local Contexts of the Current Bunsetsus
Local contexts of a modifier and its possible head
would be useful because they may represent fixed
expressions, case frames, or other collocational re-
lations. Assume that the -th bunsetsu is a modifier
and the -th one is a possible head. We consider
three bunsetsus in the local contexts of the -th and
the -th: the (  )-th bunsetsu if it modifies the
-th, the ( )-th one, and the ( )-th one. Note
that in our algorithm the (  )-th always modifies
the -th when checking the dependency between the
-th bunsetsu and the -th where    . In order
to keep the data structure simple in the proposed al-
gorithm, we did not consider more distant bunsetsus
from both the -th and the -th. It is easy to check
whether the (  )-th bunsetsus modifies the -th
one through outdep[]. Note that this use of local
contexts is similar to the dynamic features in (Kudo
and Matsumoto, 2002)4.
5.3 Richer Features Inside a Bunsetsu
With the standard features we will miss some case
particles if the bunsetsu has two or more function
words. Suppose that a bunsetsu has a topic marker
4Their model extracts three types of dynamic features from
modifiers of the -th bunsetsu (Type B), modifiers of the -th
bunsetsu (Type A), and heads of the -th bunsetsu (Type C).
Since in our proposed algorithm analysis proceeds in a left-to-
right manner, we have to use stacking (Wolpert, 1992) or other
techniques to employ the type C features.
as well as a case particle. In this case the case par-
ticle is followed by the topic marker. Thus we miss
the case particle since in the standard features only
the rightmost function word is employed. In order
to capture this information, we use as features also
all the particles in each bunsetsu.
Another important features missed in the stan-
dard are ones of the leftmost word of a possible
head bunsetsu, which often has a strong association,
e.g., an idiomatic fixed expression, with the right-
most word of its modifier. Furthermore, we use as a
feature the surface form of the leftmost word of the
bunsetsu that follows a possible head. This feature
is used with ones in Section 5.2.
5.4 Features for Conjunctive Structures
Detecting conjunctive structures is one of hard tasks
in parsing long sentences correctly. Kurohashi and
Nagao (1994) proposed a method to detect conjunc-
tive structures by calculating similarity scores be-
tween two sequences of bunsetsus.
So far few attempts have been made to explore
features for detecting conjunctive structures. As a
first step we tried two preliminary features for con-
junctive structures. If the current modifier bunsetsu
is a distinctive key bunsetsu (Kurohashi and Nagao,
1994, page 510), these features are triggered. One
is a feature which is activated when a modifier bun-
setsu is a distinctive key bunsetsu. The other is a
feature which is activated when a modifier is a dis-
tinctive key bunsetsu and the content words of both
the modifier and its possible head are equal to each
other. For simplicity, we limit the POS of these con-
tent words to nouns.
6 Experimental Results and Discussion
We implemented a parser and SVM tools in C++
and used them for experiments.
6.1 Corpus
We used the Kyoto University Corpus Version 2
(Kurohashi and Nagao, 1998) to evaluate the pro-
posed algorithm. Our parser was trained on the ar-
ticles on January 1st through 8th (7,958 sentences)
and tested on the article on January 9th (1,246 sen-
tences). The article on January 10th were used for
development. The usage of these articles is the same
as in (Uchimoto et al, 1999; Sekine et al, 2000;
Kudo and Matsumoto, 2002).
6.2 SVM setting
Polynomial kernels with the degree of 3 are used
and the misclassification cost is set to 1 unless stated
otherwise.
6.3 Results
Accuracy. Performances of our parser on the test
set is shown in Table 1. For comparison to pre-
Dependency Sentence
Acc.(%) Acc.(%)
Standard 88.72 45.88
Full 89.56 48.35
w/o Context 88.91 46.33
w/o Rich 89.19 47.05
w/o Conj 89.41 47.86
Table 1: Performance on Test Set. Context, Rich,
and Conj mean the features in Sec. 5.2, 5.3, and
5.4, respectively.
0
2
4
6
8
10
12
14
0 5 10 15 20 25 30 35 40 45
Se
co
nd
s
Sentence Length (Number of Bunsetsus)
Figure 4: Observed Running Time
vious work we use the standard measures for the
Kyoto University Corpus: dependency accuracy and
sentence accuracy. The dependency accuracy is the
percentage of correct dependencies and the sentence
accuracy is the percentage of sentences, all the de-
pendencies in which are correctly analyzed.
The accuracy with the standard feature set is rel-
atively good. Actually, this accuracy is almost the
same as that of the cascaded chunking model with-
out dynamic features (Kudo and Matsumoto, 2002).
Our parser with the full feature set yields an accu-
racy of 89.56%, which is the best in the previously
published results.
Asymptotic Time Complexity. Figure 4 shows
the running time of our parser on the test set using
a workstation (Ultra SPARC II 450 MHz with 1GB
memory). It clearly shows that the running time is
proportional to the sentence length and this obser-
vation is consistent with our theoretical analysis in
Section 4.2.
One might think that although the upper bound
of time complexity is lower than those of previous
work, actual processing of our parser is not so fast.
Slowness of our parser is mainly due to a huge com-
putation of kernel evaluations in SVMs. The SVM
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
0 5 10 15 20 25 30 35 40 45
Se
co
nd
s
Sentence Length (Number of Bunsetsus)
Figure 5: Observed Running Time with Linear Ker-
nel. The misclassification cost is set to 0.0056.
classifiers in our experiments have about forty thou-
sand support vectors. Therefore, for every deci-
sion of dependency also a huge computation of dot
products is required. Fortunately, solutions to this
problem have already been given by Kudo and Mat-
sumoto (2003). They proposed methods to convert
a polynomial kernel with higher degrees to a simple
linear kernel and reported a new classifier with the
converted kernel was about 30 to 300 times faster
than the original one while keeping the accuracy. By
applying their methods to our parser, its processing
time would be enough practical.
In order to roughly estimate the improved speed
of our parser, we built a parser with a linear kernel
and ran it on the same test set. Figure 5 shows the
observed time of the parser with a linear kernel us-
ing the same machine. The parser runs fast enough.
It can parse a very long sentence within 0.02 sec-
onds. Furthermore, accuracy as well as speed of
this parser was much better than we expected. It
achieves a dependency accuracy of 87.36% and a
sentence accuracy of 40.60%. These accuracies are
slightly better than those in (Uchimoto et al, 1999),
where combinations of features are manually se-
lected.
6.4 Comparison to Related Work
We compare our parser to those in related work. A
summary of the comparison is shown in Table 2.
It clearly shows that our proposed algorithm with
SVMs has a good property with regard to time
complexity and in addition our parser successfully
achieves a state-of-the-art accuracy.
Theoretical comparison with (Kudo and Mat-
sumoto, 2002) is described in Section 4.4. Uchi-
moto et al (1999) used the backward beam search
with ME. According to (Sekine et al, 2000), the an-
alyzing time followed a quadratic curve. In contrast,
Algorithm/Model Time Acc.(%)
Complexity
This paper Stack Dependency Analysis (SVMs)  89.56
Stack Dependency Analysis (linear SVMs)  87.36
KM02 Cascaded Chunking (SVMs)  89.29
KM00 Backward Beam Search (SVMs)  89.09
USI99 Backward Beam Search (ME)  87.14
Seki00 Deterministic Finite State Transducer  77.97
Table 2: Comparison to Related Work. KM02 = Kudo and Matsumoto 2002, KM00 = Kudo and Matsumoto
2000, USI99 = Uchimoto et al 1999, and Seki00 = Sekine 2000.
our parser analyzes a sentence in linear time keep-
ing a better accuracy. Sekine (2000) also proposed
a very fast parser that runs in linear time; however,
accuracy is greatly sacrificed.
7 Conclusion and Future Directions
We have presented a novel algorithm for Japanese
dependency analysis. The algorithm allows us
to analyze dependency structures of a sentence in
linear-time while keeping a state-of-the-art accu-
racy. We have shown a formal description of the
algorithm and discussed it theoretically in terms of
time complexity. In addition, we have evaluated its
efficiency and performance empirically against the
Kyoto University Corpus. Our parser gives the best
accuracy, 89.56%, in the previously published re-
sults.
In the future, it would be interesting to apply
this algorithm to speech recognition in which it is
more desirable to analyze a sentence in a left-to-
right manner. Another interesting direction would
be to explore features for conjunctive structures. Al-
though we found some useful features, they were
not enough to improve the performance much. We
expect stacking would be useful.
References
S. P. Abney. 1991. Parsing by chunks. In R. C. Berwick,
S. P. Abney, and C. Tenny, editors, Principle-Based
Parsing: Computation and Psycholinguistics, pages
257?278. Kluwer Academic Publishers.
M. Collins. 1996. A new statistical parser based on bi-
gram lexical dependencies. In Proc. of ACL-96, pages
184?191.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING-96, pages 340?345.
M. Fujio and Y. Matsumoto. 1998. Japanese depen-
dency structure analysis based on lexicalized statis-
tics. In Proc. of EMNLP-1998, pages 88?96.
M. Haruno, S. Shirai, and Y. Ooyama. 1998. Using de-
cision trees to construct a practical parser. In Proc. of
COLING/ACL-98, pages 505?511.
T. Kudo and Y. Matsumoto. 2000. Japanese dependency
structure analysis based on support vector machines.
In Proc. of EMNLP/VLC 2000, pages 18?25.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proc. of
CoNLL-2002, pages 63?69.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proc. of ACL-03, pages
24?31.
S. Kurohashi and M. Nagao. 1994. A syntactic analysis
method of long Japanese sentences based on the de-
tection of conjunctive structures. Computational Lin-
guistics, 20(4):507?534.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proc. of the 1st LREC, pages 719?724.
J. Lafferty, D. Sleator, and D. Temperley. 1992. Gram-
matical trigrams: A probabilistic model of link gram-
mar. In Proc. of the AAAI Fall Symp. on Probabilistic
Approaches to Natural Language, pages 89?97.
H. Maruyama and S. Ogino. 1992. A statistical property
of Japanese phrase-to-phrase modifications. Mathe-
matical Linguistics, 18(7):348?352.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. of IWPT-03, pages 149?
160.
A. Ratnaparkhi. 1997. A linear observed time statistical
parser based on maximum entropy models. In Proc.
of EMNLP-1997, pages 1?10.
S. Sekine, K. Uchimoto, and H. Isahara. 2000. Back-
ward beam search algorithm for dependency analysis
of Japanese. In Proc. of COLING-00, pages 754?760.
S. Sekine. 2000. Japanese dependency analysis using
a deterministic finite state transducer. In Proc. of
COLING-00, pages 761?767.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese
dependency structure analysis based on maximum en-
tropy models. In Proc. of EACL-99, pages 196?203.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag.
D. H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
J. Yoon, K. Choi, and M. Song. 1999. Three types of
chunking in Korean and dependency analysis based on
lexical association. In Proc. of the 18th Int. Conf. on
Computer Processing of Oriental Languages, pages
59?65.
Using a Partially Annotated Corpus
to Build a Dependency Parser for Japanese
Manabu Sassano
Fujitsu Laboratories, Ltd., 4-1-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
sassano@jp.fujitsu.com
Abstract. We explore the use of a partially annotated corpus to build a depen-
dency parser for Japanese. We examine two types of partially annotated corpora.
It is found that a parser trained with a corpus that does not have any grammatical
tags for words can demonstrate an accuracy of 87.38%, which is comparable to
the current state-of-the-art accuracy on the Kyoto University Corpus. In contrast,
a parser trained with a corpus that has only dependency annotations for each
two adjacent bunsetsus (chunks) shows moderate performance. Nonetheless, it
is notable that features based on character n-grams are found very useful for a
dependency parser for Japanese.
1 Introduction
Corpus-based supervised learning is now a standard approach to build a system which
shows high performance for a given task in NLP. However, the weakness of such ap-
proach is to need an annotated corpus. Corpus annotation is labor intensive and very
expensive. To reduce or avoid the cost of annotation, various approaches are proposed,
which include unsupervised learning, minimally supervised learning (e.g., [1]), and ac-
tive learning (e.g., [2,3]).
To discuss clearly the cost of corpus annotation, we here consider a simple model
of the cost:
annotation cost ?
?
t
c(t)n(t)
where t is a type of annotation such as POS tagging, chunk tagging, etc., c(t) is a cost
per type t annotation, and n(t) is the number of type t annotation.
Previous work to tackle the problem of annotation cost has mainly focused on reduc-
ing n(t). For example, in active learning, useful examples to be annotated are selected
based on some criteria, and then the number of examples to be annotated is considerably
reduced. In contrast, we here focus on reducing c(t) instead of n(t). Obviously, if some
portion of annotations are not given, the performance of a NLP system will deteriorate.
The question here is how much the performance deteriorates. Is there a good trade-off
between saving the cost and losing the performance?
Minimizing portions of annotations is also very important from the point of view of
engineering. Suppose that we want to build an annotated corpus to make a parser for
some real-world application. The design and strategy of corpus annotation is crucial in
order to get a good parser while saving the cost. Furthermore, we have to keep in mind
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 82?92, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
Using a Partially Annotated Corpus 83
the maintenance cost of both the corpus and the parser. For example, we may find some
errors in the annotations and the design of linguistic categories. In this situation fewer
annotations lead to saving the cost because the corpus is more stable and less prone
to errors.
The main purpose of this study is to explore the use of a partially annotated corpus
to build a dependency parser for Japanese. In this paper, we describe experiments to
investigate the feasibility of a partially annotated corpus. In addition, we propose fea-
tures for parsing which are based on character n-grams. Even if grammatical tags are
not given, a parser with these features demonstrates better performance than does the
maximum entropy parser [4] with full grammatical features. Similarly, we have con-
ducted experiments on bunsetsu (described in Sect. 2.1) chunking trained with a corpus
which does not have grammatical tags. After that, we have tested a parser trained with
a corpus which is partially annotated for dependency structures.
2 Parsing Japanese
2.1 Syntactic Properties of Japanese
The Japanese language is basically an SOV language. Word order is relatively free. In
English the syntactic function of each word is represented with word order, while in
Japanese postpositions represent the syntactic function of each word. For example, one
or more postpositions following a noun play a similar role to declension of nouns in
German, which indicates a grammatical case.
Based on such properties, the concept of bunsetsus1 was devised and has been used
to describe the structure of a sentence in Japanese. A bunsetsu consists of one or more
content words followed by zero or more function words. By defining a bunsetsu like
that, we can analyze a sentence in a similar way that is used when analyzing the gram-
matical role of words in inflecting languages like German.
Thus, strictly speaking, bunsetsu order rather than word order is free except the
bunsetsu that contains the main verb of a sentence. Such bunsetsu must be placed at
the end of the sentence. For example, the following two sentences have an identical
meaning: (1) Ken-ga kanojo-ni hon-wo age-ta. (2) Ken-ga hon-wo kanojo-ni age-ta.
(-ga: subject marker, -ni: dative case particle, -wo: accusative case particle. English
translation: Ken gave a book to her.) Note that the rightmost bunsetsu ?age-ta,? which
is composed of a verb stem and a past tense marker, has to be placed at the end of the
sentence.
We here list the constraints of Japanese dependency including ones mentioned above.
C1. Each bunsetsu has only one head except the rightmost one.
C2. Each head bunsetsu is always placed at the right hand side of its modifier.
C3. Dependencies do not cross one another.
These properties are basically shared also with Korean and Mongolian.
1 The word ?bunsetsu? in Japanese is composed of two Chinese characters, i.e., ?bun? and ?setsu.?
?Bun? means a sentence and ?setsu? means a segment. A ?bunsetsu? is considered to be a
small syntactic segment in a sentence. A eojeol in Korean [5] is almost the same concept as a
bunsetsu. Chunks defined in [6] for English are also very similar to bunsetsus.
84 M. Sassano
2.2 Typical Steps of Parsing Japanese
Because Japanese has the properties above, the following steps are very common in
parsing Japanese:
S1. Break a sentence into morphemes (i.e. morphological analysis).
S2. Chunk them into bunsetsus.
S3. Analyze dependencies between these bunsetsus.
S4. Label each dependency with a semantic role such as agent, object, location, etc.
Note that since Japanese does not have explicit word delimiters like white spaces, we
first have to tokenize a sentence into morphemes and at the same time give a POS tag
to each morpheme (S1). Therefore, when building an annotated corpus of Japanese, we
have to decide boundaries of each word (morpheme) and POS tags of all the words.
3 Experimental Setup
3.1 Parsing Algorithm
We employ the Stack Dependency Analysis (SDA) algorithm [7] to analyze the depen-
dency structure of a sentence in Japanese. This algorithm, which takes advantage of C1,
C2, and C3 in Sect. 2.1, is very simple and easy to implement. Sassano [7] has proved
its efficiency in terms of time complexity and reported the best accuracy on the Kyoto
University Corpus [8]. The SDA algorithm as well as Cascaded Chunking Model [9] is
a shift-reduce type algorithm.
The pseudo code of SDA is shown in Fig. 1. This algorithm is used with any esti-
mator that decides whether a bunsetsu modifies another bunsetsu. A trainable classifier,
such as an SVM, a decision tree, etc., is a typical choice for the estimator.
3.2 Corpus
To facilitate comparison with previous results, we used the Kyoto University Corpus
Version 2 [8]. Parsers used in experiments were trained on the articles on January 1st
through 8th (7,958 sentences) and tested on the articles on January 9th (1,246 sen-
tences). The articles on January 10th were used for development. The usage of these
articles is the same as in [4,10,9,7].
3.3 Choice for Classifiers
We use SVMs [11] for estimating dependencies between two bunsetsus because they
have excellent properties. One of them is that combinations of features in an example
are automatically considered with polynomial kernels. Excellent performance has been
reported for many NLP tasks including Japanese dependency parsing, e.g., [9]. Please
see [11] for formal descriptions of SVMs.
3.4 SVM Setting
Polynomial kernels with the degree of 3 are used and the misclassification cost is set to 1.
Using a Partially Annotated Corpus 85
//
// Input: N: the number of bunsetsus in a sentence.
// w[]: an array that keeps a sequence of bunsetsus in the sentence.
//
// Output: outdep[]: an integer array that stores an analysis result,
// i.e., dependencies between the bunsetsus. For example, the
// head of w[j] is outdep[j].
//
// stack: a stack that holds IDs of modifier bunsetsus
// in the sentence. If it is empty, the pop method
// returns EMPTY (?1).
//
// function estimate dependency(j, i, w[]):
// a function that returns non-zero when the j-th
// bunsetsu should modify the i-th bunsetsu.
// Otherwise returns zero.
//
procedure analyze(w[], N, outdep[])
// Push 0 on the stack.
stack.push(0);
// Variable i for a head and j for a modifier.
for (int i = 1; i < N; i++) {
// Pop a value off the stack.
int j = stack.pop();
while (j != EMPTY && (i == N ? 1 || estimate dependency(j, i, w))) {
// The j-th bunsetsu modifies the i-th bunsetsu.
outdep[j] = i;
// Pop a value off the stack to update j.
j = stack.pop();
}
if (j != EMPTY)
stack.push(j);
stack.push(i);
}
Fig. 1. Pseudo code of the Stack Dependency Analysis algorithm. Note that ?i == N ? 1?
means the i-th bunsetsu is the rightmost one in the sentence. Any classifiers can be used in esti-
mate dependency().
4 Dropping POS Tags
First we conducted experiments on dropping POS tags. In corpus building for a parser,
disambiguating POS tags is one of time consuming tasks. In addition, it takes much
time to prepare guidelines for POS tagging. Furthermore, in the case of a Japanese
corpus, we will need more time because we have to deal with word boundaries as well
as POS tags. Therefore, it would be desirable to avoid or reduce POS annotations while
minimizing the loss of performance of the parser.
86 M. Sassano
4.1 Features
To examine the effect of dropping POS tags, we built the following four sets of features
and measured parsing performance with these feature sets.
Standard Features. By the ?standard features? here we mean the feature set commonly
used in [4,10,12,9,7]. We employ the features below for each bunsetsu:
1. Rightmost Content Word - major POS, minor POS, conjugation type, conjugation
form, surface form (lexicalized form)
2. Rightmost Function Word - major POS, minor POS, conjugation type, conjugation
form, surface form (lexicalized form)
3. Punctuation (periods, and commas)
4. Open parentheses and close parentheses
5. Location - at the beginning of the sentence or at the end of the sentence.
In addition, features as to the gap between two bunsetsus are also used. They include:
distance, particles, parentheses, and punctuation.
Words-Only Features. If POS tags are not available, we have to use only tokens
(words) as features. In addition, we cannot identify easily content words and function
words in a bunsetsu. Therefore, we here chose the simplest form of feature sets. We con-
structed a bag of words in each bunsetsu and then used them as features. For example,
we assume that there are three words in a bunsetsu: keisan (computational), gengogaku
(linguistics), no (of). In this case we get {keisan, gengogaku, no} as features.
Character N-Gram Features. Next we constructed a feature set without word bound-
aries or POS tags. In this feature set, we can use only the character string of a bunsetsu.
At first glance, such a feature set is silly and it seems that a corpus without POS tags
cannot yield a good parser. It is because no explicit syntactic information is given.
Can we extract good features from a string? We found useful ideas in Sato and
Kawase?s papers [13,14]. They define a similarity score between two sentences in
Japanese and use it for ranking translation examples. Their similarity score is based on
character subsequence matching. Just raw character strings are used and neither mor-
phological analysis, POS tagging, nor parsing is applied. Although no advanced analy-
sis was applied, they had good results enough for translation-aid. In [13], DP matching
based scores are investigated, and in [14], the number of common 2-grams and 3-grams
of characters between two sentences is incorporated into a similarity score.
In our experiments we use blended n-grams which are both 1-grams and 2-grams.
All the 1-grams and 2-grams from the character string of a bunsetsu are extracted as
features. For example, suppose we have a bunsetsu the string of which is a sequence of
three characters: kano-jo-no where ?-? represents a boundary between Japanese charac-
ters and this string is actually written with three characters in Japanese. The following
features are extracted from the string: kano, jo, no, $-kano, kano-jo, jo-no, no-$, where
?$? represents a bunsetsu boundary.
Combination of ?Standard Features? and Character N-grams. The fourth feature
set that we have investigated is a combination of ?standard features? and character n-
grams, which are described in the previous subsection.
Using a Partially Annotated Corpus 87
4.2 Results and Discussion
Performance of parsers trained with these feature sets on the development set and
the test set is shown in Table 1. For comparison to previous work we use the stan-
dard measures for the Kyoto University Corpus: dependency accuracy and sentence
accuracy. The dependency accuracy is the percentage of correct dependencies and the
sentence accuracy is the percentage of sentences, all the dependencies in which are
correctly analyzed.
Table 1. Performance on Development Set and Test Set
Dev. Set Test Set
Feature Set Dep. Acc. Sent. Acc. Dep. Acc. Sent. Acc.
?Standard? 88.97 46.18 88.72 45.28
Bag of Words (Words Only) 85.22 35.02 84.43 34.95
Character N-Grams 87.79 42.66 87.38 40.84
?Standard? + Character N-Grams 89.72 47.04 89.07 46.89
To our surprise, the parser with the feature set based on character n-grams achieved
an accuracy of 87.38%, which is very good. Although this is worse than that of ?stan-
dard feature set,? the performance is still surprising. We considered POS tags were
essential for parsing. Why so successful?
The reason would be explained by the writing system of Japanese and its usage. In
modern Japanese text mainly five different scripts are used: kanji, hiragana, katakana,
Arabic numerals, and Latin letters. Usage of these scripts indicates implicitly the gram-
matical role of a word. For example, kanji is mainly used to represent nouns or stems of
verbs and adjectives. It is never used for particles, which are always written in hiragana.
Essential morphological and syntactic categories are also often indicated in hiragana.
Conjugation forms of verbs and adjectives are represented with one or two hiragana
characters. Syntactic roles of a bunsetsu are often indicated by the rightmost morpheme
in it. Most of such morphemes are endings of verbs or adjectives, or particles. In other
words, the rightmost characters in a bunsetsu are expected to indicate the syntactic role
of a bunsetsu.
Bunsetsu Chunking. After we observed the results of the experiments on parsing, a
new question arose to us. Can we chunk tokens to bunsetsus without POS tags, too? We
carried out additional experiments on bunsetsu chunking. Following [15], we encode
bunsetsu chunking as a tagging problem. In bunsetsu chunking, we use the chunk tag
set {B, I} where B marks the first word of some bunsetsu and words marked I are
inside a bunsetsu. In these experiments on bunsetsu chunking, we estimated the chunk
tag of each word using a SVM from five words and their derived attributes. These five
words are a word to be estimated and its two preceding/following words. Features are
extracted from the followings for each word: word (token) itself, major POS, minor
POS, conjugation type, conjugation form, the leftmost character, the character type of
the leftmost character, the rightmost character, and the character type of the rightmost
character. A character type has a value which indicates a script. It can be either kanji,
hiragana, katakana, Arabic numerals, or Latin letters.
88 M. Sassano
We conducted experiments with four sets of features. Performance on the develop-
ment set and the test set is shown in Table 2. We used the same performance measures
as in [16]. Precision (p) is defined as the percentage of words correctly marked B among
all the words that the system marked B. Recall (r) is defined as the percentage of words
correctly marked B among all the words that are marked B in the training set. F-measure
is defined as: F-measure = 2pq/(p + q).
Table 2. Bunsetsu Chunking Performance on Development Set and Test Set. Grammatical tags
include POS tags and conjugation types/forms.
Feature Set Dev. Set (F) Test Set (F)
Surface Form + Grammatical Tags 99.58 99.57
Surface Form Only 97.65 97.02
Surface Form + Char. Features (No Grammatical Tags) 99.09 99.07
Mixed 99.64 99.64
The bunsetsu chunker with surface forms only yielded worse performance than did
that with the grammatical tags including major/minor POS and conjugation type/form.
However, the chunker with character features achieved good performance even if gram-
matical tags are not available. In addition, the feature set in which all the available
features are used gives the best among the feature sets we tested. Again we found that
features based on characters compensate performance deterioration caused by no gram-
matical tags.
We have found that both a practical parser and a practical bunsetsu chunker can be
constructed from a corpus which does not have POS information. This means we can
make a parser for Japanese which is less dependent on a morphological analyzer. It
would be useful for improving the modularity of an analysis system for Japanese.
5 Dropping Longer Dependency Annotations
As previous work [4,17] reports, approximately 65% of bunsetsus modify the one on
their immediate right hand side. From this observation, we simplify dependency an-
notations. For each bunsetsu we give either the D tag or O where bunsetsus marked
D modify the one on their immediate right hand side and bunsetsus marked O do not.
Ken-ga kanojo-ni ano hon-wo age-ta.
Ken-subj to her that book-acc gave.
ID 0 1 2 3 4
Head 4 4 3 4 -
{D, O} O O D D -
Fig. 2. Sample sentence with dependency annotations. Bunsetsus marked D modify the one on
their immediate right hand side and bunsetsus marked O do not. An English translation is ?Ken
gave that book to her.?
Using a Partially Annotated Corpus 89
Figure 2 shows a sample sentence with dependency annotations. This encoding scheme
represents some portion of the dependency structure of a sentence. Annotating under
this scheme is easier than selecting the head of each bunsetsu. We examined usefulness
of this type of partially annotated corpus following the encoding scheme above.
5.1 Using Partial Dependency Annotations
The SDA algorithm, which we employ for experiments, can work with a partially an-
notated corpus to parse a sentence in Japanese2. In training, first we construct a training
set only from dependency annotations between two adjacent bunsetsus. We ignore re-
lations between two bunsetsus which have a longer dependency. After that, we train a
classifier for parsing from the training set. In testing, we use the classifier for both two
adjacent bunsetsus and other pairs of bunsetsus.
5.2 Results and Discussion
Performance on the development set and the test set are shown in Table 3.
The parser trained with the partially annotated corpus yielded good performance.
However, its accuracy is considerably worse than that of the parser with the fully an-
notated corpus. This tendency is clearer in terms of sentence accuracy. To examine
differences in terms of quantity, we plot the learning curves with the two corpora. The
curves are shown in Fig. 3.
Table 3. Performance of parsers trained with the fully annotated corpus and the partially anno-
tated corpus
# of Training Dev. Set Test Set
Training Set Examples Dep. Acc. Sent. Acc. Dep. Acc. Sent. Acc.
Full 98,689 88.97 46.18 88.72 45.28
Adjacent Annotations Only 61,899 85.65 38.00 85.50 38.58
How many sentences which are partially annotated do we need in order to achieve
a given accuracy with some number of fully annotated sentences? It is found that we
need 8 ? 17 times the number of sentences when using the partially annotated corpus
instead of the fully annotated one. If hiring linguistic experts for annotation is much
more expensive than hiring non experts, or it is difficult to find a large enough number
of experts, this type of partially annotated corpus could be useful.
The naive approach we examined was not so effective in the light of the number of
sentences to be required. However, we should note that a partially annotated corpus is
easier to maintain the consistency of annotations.
6 Related Work
In this section we briefly review related work from three points of view, i.e., parsing
performance, the use of partially annotated corpora, and the use of character n-grams.
2 Cascaded Chunking Model [9] also can be applicable to use a partially annotated corpus.
90 M. Sassano
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0 1000 2000 3000 4000 5000 6000 7000 8000
Ac
c u
ra
c y
Number of sentences
Full Annotations
Partial Annotations
Fig. 3. Learning curves of parsers trained with the partially annotated corpus and the fully anno-
tated corpus
Parsing Performance. Although improvement of the performance of a parser is not
a primary concern in this paper, comparison with other results will indicate to us how
practical the parser is. Table 4 summarizes comparison to related work on parsing accu-
racy. Our parsers demonstrated good performance although they did not outperform the
best. It is notable that the parser which does not use any explicit grammatical tags out-
performs one by [4], which employs a maximum entropy model with full grammatical
features given by a morphological analyzer.
Table 4. Comparison to related work on parsing accuracy. KM02 = Kudo and Matsumoto 2002
[9], KM00 = Kudo and Matsumoto 2000 [12], USI99 = Uchimoto et al 1999 [4], Seki00 = Sekine
2000 [18], and Sass04 = Sassano 2004 [7].
Algorithm/Model/Features Acc.(%)
This paper Stack Dependency Analysis (cubic SVM) w/ char. n-grams 89.07
Stack Dependency Analysis (cubic SVM) w/ char. n-grams, no POS 87.38
Sass04 Stack Dependency Analysis (cubic SVM) w/ various enriched features 89.56
KM02 Cascaded Chunking (cubic SVM) w/ dynamic features 89.29
KM00 Backward Beam Search (cubic SVM) 89.09
USI99 Backward Beam Search (MaxEnt) 87.14
Seki00 Deterministic Finite State Transducer 77.97
Use of Partially Annotated Corpora. Several papers address the use of partially anno-
tated corpora. Pereira and Schabes [19] proposed an algorithm of inferring a stochastic
context-free grammar from a partially bracketed corpus. Riezler et al [20] presented
a method of discriminative estimation of an exponential model on LFG parses from
partially labeled data.
Our study differs in that we focus more on avoiding expensive types of annotations
while minimizing the loss of performance of a parser.
Using a Partially Annotated Corpus 91
Use of Character N-grams. Character n-grams are often used for POS tagging of un-
known words, unsupervised POS tagging, and measures of string similarity. The num-
ber of common n-grams between two sentences is used for a similarity measure in [14].
This usage is essentially the same as in the spectrum kernel [21], which is one of string
kernels [22].
7 Conclusion
We have explored the use of a partially annotated corpus for building a dependency
parser for Japanese. We have examined two types of partially annotated corpora. It is
found that a parser trained with a corpus that does not have any grammatical tags for
words can demonstrate an accuracy of 87.38%, which is comparable to the current state-
of-the-art accuracy. In contrast, a parser trained with a corpus that has only dependency
annotations for each two adjacent bunsetsus shows moderate performance. Nonethe-
less, it is notable that features based on character n-grams are found very useful for a
dependency parser for Japanese.
References
1. Yarowsky, D.: Unsupervised word sense disambiguation rivaling supervised methods. In:
Proc. of ACL-1995. (1995) 189?196
2. Thompson, C.A., Califf, M.L., Mooney, R.J.: Active learning for natural language parsing
and information extraction. In: Proc. of the Sixteenth International Conference on Machine
Learning. (1999) 406?414
3. Tang, M., Luo, X., Roukos, S.: Active learning for statistical natural language parsing. In:
Proc. of ACL-2002. (2002) 120?127
4. Uchimoto, K., Sekine, S., Isahara, H.: Japanese dependency structure analysis based on
maximum entropy models. In: Proc. of EACL-99. (1999) 196?203
5. Yoon, J., Choi, K., Song, M.: Three types of chunking in Korean and dependency analysis
based on lexical association. In: Proc. of the 18th Int. Conf. on Computer Processing of
Oriental Languages. (1999) 59?65
6. Abney, S.P.: Parsing by chunks. In Berwick, R.C., Abney, S.P., Tenny, C., eds.: Principle-
Based Parsing: Computation and Psycholinguistics. Kluwer Academic Publishers (1991)
257?278
7. Sassano, M.: Linear-time dependency analysis for Japanese. In: Proc. of COLING 2004.
(2004) 8?14
8. Kurohashi, S., Nagao, M.: Building a Japanese parsed corpus while improving the parsing
system. In: Proc. of the 1st LREC. (1998) 719?724
9. Kudo, T., Matsumoto, Y.: Japanese dependency analysis using cascaded chunking. In: Proc.
of CoNLL-2002. (2002) 63?69
10. Sekine, S., Uchimoto, K., Isahara, H.: Backward beam search algorithm for dependency
analysis of Japanese. In: Proc. of COLING-00. (2000) 754?760
11. Vapnik, V.N.: The Nature of Statistical Learning Theory. Springer-Verlag (1995)
12. Kudo, T., Matsumoto, Y.: Japanese dependency structure analysis based on support vector
machines. In: Proc. of EMNLP/VLC 2000. (2000) 18?25
13. Sato, S.: CTM: An example-based translation aid system. In: Proc. of COLING-92. (1992)
1259?1263
92 M. Sassano
14. Sato, S., Kawase, T.: A high-speed best match retrieval method for Japanese text. Technical
Report IS-RR-94-9I, Japan Advanced Institute of Science and Technology, Hokuriku (1994)
15. Ramshaw, L.A., Marcus, M.P.: Text chunking using transformation-based learning. In: Proc.
of VLC 1995. (1995) 82?94
16. Murata, M., Uchimoto, K., Ma, Q., Isahara, H.: Bunsetsu identification using category-
exclusive rules. In: Proc. of COLING-00. (2000) 565?571
17. Maruyama, H., Ogino, S.: A statistical property of Japanese phrase-to-phrase modifications.
Mathematical Linguistics 18 (1992) 348?352
18. Sekine, S.: Japanese dependency analysis using a deterministic finite state transducer. In:
Proc. of COLING-00. (2000) 761?767
19. Pereira, F., Schabes, Y.: Inside-outside reestimation from partially bracketed corpora. In:
Proc. of ACL-92. (1992) 128?135
20. Riezler, S., King, T.H., Kaplan, R.M., Crouch, R., III, J.T.M., Johnson, M.: Parsing the Wall
Street Journal using a lexical-functional grammar and discriminative estimation techniques.
In: Proc. of ACL-2002. (2002) 271?278
21. Leslie, C., Eskin, E., Noble, W.S.: The spectrum kernel: A string kernel for SVM protein
classification. In: Proc. of the 7th Pacific Symposium on Biocomputing. (2002) 564?575
22. Lodhi, H., Saunders, C., Shawe-Tayor, J., Cristianini, N., Watkins, C.: Text classification
using string kernels. Journal of Machine Learning Research 2 (2002) 419?444
An Empirical Study of Active Learning with Support Vector Machines for
Japanese Word Segmentation
Manabu Sassano
Fujitsu Laboratories Ltd.
4-1-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
sassano@jp.fujitsu.com
Abstract
We explore how active learning with Sup-
port Vector Machines works well for a
non-trivial task in natural language pro-
cessing. We use Japanese word segmenta-
tion as a test case. In particular, we discuss
how the size of a pool affects the learning
curve. It is found that in the early stage
of training with a larger pool, more la-
beled examples are required to achieve a
given level of accuracy than those with a
smaller pool. In addition, we propose a
novel technique to use a large number of
unlabeled examples effectively by adding
them gradually to a pool. The experimen-
tal results show that our technique requires
less labeled examples than those with the
technique in previous research. To achieve
97.0 % accuracy, the proposed technique
needs 59.3 % of labeled examples that
are required when using the previous tech-
nique and only 17.4 % of labeled exam-
ples with random sampling.
1 Introduction
Corpus-based supervised learning is now a stan-
dard approach to achieve high-performance in nat-
ural language processing. However, the weakness
of supervised learning approach is to need an anno-
tated corpus, the size of which is reasonably large.
Even if we have a good supervised-learning method,
we cannot get high-performance without an anno-
tated corpus. The problem is that corpus annotation
is labour intensive and very expensive. In order to
overcome this, some unsupervised learning methods
and minimally-supervised methods, e.g., (Yarowsky,
1995; Yarowsky and Wicentowski, 2000), have
been proposed. However, such methods usually de-
pend on tasks or domains and their performance of-
ten does not match one with a supervised learning
method.
Another promising approach is active learning, in
which a classifier selects examples to be labeled, and
then requests a teacher to label them. It is very dif-
ferent from passive learning, in which a classifier
gets labeled examples randomly. Active learning is
a general framework and does not depend on tasks
or domains. It is expected that active learning will
reduce considerably manual annotation cost while
keeping performance. However, few papers in the
field of computational linguistics have focused on
this approach (Dagan and Engelson, 1995; Thomp-
son et al, 1999; Ngai and Yarowsky, 2000; Hwa,
2000; Banko and Brill, 2001). Although there are
many active learning methods with various classi-
fiers such as a probabilistic classifier (McCallum and
Nigam, 1998), we focus on active learning with Sup-
port Vector Machines (SVMs) because of their per-
formance.
The Support Vector Machine, which is introduced
by Vapnik (1995), is a powerful new statistical learn-
ing method. Excellent performance is reported in
hand-written character recognition, face detection,
image classification, and so forth. SVMs have
been recently applied to several natural language
tasks, including text classification (Joachims, 1998;
Dumais et al, 1998), chunking (Kudo and Mat-
sumoto, 2000b; Kudo and Matsumoto, 2001), and
dependency analysis (Kudo and Matsumoto, 2000a).
SVMs have been greatly successful in such tasks.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 505-512.
                         Proceedings of the 40th Annual Meeting of the Association for
Additionally, SVMs as well as boosting have good
theoretical background.
The objective of our research is to develop an ef-
fective way to build a corpus and to create high-
performance NL systems with minimal cost. As
a first step, we focus on investigating how active
learning with SVMs, which have demonstrated ex-
cellent performance, works for complex tasks in nat-
ural language processing. For text classification, it
is found that this approach is effective (Tong and
Koller, 2000; Schohn and Cohn, 2000). They used
less than 10,000 binary features and less than 10,000
examples. However, it is not clear that the approach
is readily applicable to tasks which have more than
100,000 features and more than 100,000 examples.
We use Japanese word segmentation as a test case.
The task is suitable for our purpose because we have
to handle combinations of more than 1,000 charac-
ters and a very large corpus (EDR, 1995) exists.
2 Support Vector Machines
In this section we give some theoretical definitions
of SVMs. Assume that we are given the training data
(x
i
; y
i
); : : : ; (x
l
; y
l
);x
i
2 R
n
; y
i
2 f+1; 1g
The decision function g in SVM framework is de-
fined as:
g(x) = sgn(f(x)) (1)
f(x) =
l
X
i=1
y
i

i
K(x
i
;x) + b (2)
where K is a kernel function, b 2 R is a thresh-
old, and 
i
are weights. Besides the 
i
satisfy the
following constraints:
0  
i
 C;8i and
l
X
i=1

i
y
i
= 0;
where C is a missclassification cost. The x
i
with
non-zero 
i
are called Support Vectors. For linear
SVMs, the kernel function K is defined as:
K(x
i
;x) = x
i
 x:
In this case, Equation 2 can be written as:
f(x) = w  x+ b (3)
1. Build an initial classifier
2. While a teacher can label examples
(a) Apply the current classifier to each unla-
beled example
(b) Find the m examples which are most in-
formative for the classifier
(c) Have the teacher label the subsample of m
examples
(d) Train a new classifier on all labeled exam-
ples
Figure 1: Algorithm of pool-based active learning
wherew =
P
l
i=1
y
i

i
x
i
. To train an SVM is to find
the 
i
and the b by solving the following optimiza-
tion problem:
maximize
l
X
i=1

i
 
1
2
l
X
i;j=1

i

j
y
i
y
j
K(x
i
;x
j
)
subject to 0  
i
 C;8i and
l
X
i=1

i
y
i
= 0:
3 Active Learning for Support Vector
Machines
3.1 General Framework of Active Learning
We use pool-based active learning (Lewis and Gale,
1994). SVMs are used here instead of probabilistic
classifiers used by Lewis and Gale. Figure 1 shows
an algorithm of pool-based active learning1. There
can be various forms of the algorithm depending on
what kind of example is found informative.
3.2 Previous Algorithm
Two groups have proposed an algorithm for SVMs
active learning (Tong and Koller, 2000; Schohn and
Cohn, 2000)2. Figure 2 shows the selection algo-
rithm proposed by them. This corresponds to (a) and
(b) in Figure 1.
1The figure described here is based on the algorithm by
Lewis and Gale (1994) for their sequential sampling algorithm.
2Tong and Koller (2000) propose three selection algorithms.
The method described here is simplest and computationally ef-
ficient.
1. Compute f(x
i
) (Equation 2) over all x
i
in a
pool.
2. Sort x
i
with jf(x
i
)j in decreasing order.
3. Select top m examples.
Figure 2: Selection Algorithm
1. Build an initial classifier.
2. While a teacher can label examples
(a) Select m examples using the algorithm in
Figure 2.
(b) Have the teacher label the subsample of m
examples.
(c) Train a new classifier on all labeled exam-
ples.
(d) Add new unlabeled examples to the pri-
mary pool if a specified condition is true.
Figure 3: Outline of Tow Pool Algorithm
3.3 Two Pool Algorithm
We observed in our experiments that when using the
algorithm in the previous section, in the early stage
of training, a classifier with a larger pool requires
more examples than that with a smaller pool does (to
be described in Section 5). In order to overcome the
weakness, we propose two new algorithms. We call
them ?Two Pool Algorithm? generically. It has two
pools, i.e., a primary pool and a secondary one, and
moves gradually unlabeled examples to the primary
pool from the secondary instead of using a large
pool from the start of training. The primary pool
is used directly for selection of examples which are
requested a teacher to label, whereas the secondary
is not. The basic idea is simple. Since we cannot
get good performance when using a large pool at the
beginning of training, we enlarge gradually a pool of
unlabeled examples.
The outline of Two Pool Algorithm is shown in
Figure 3. We describe below two variations, which
are different in the condition at (d) in Figure 3.
Our first variation, which is called Two Pool Al-
gorithm A, adds new unlabeled examples to the pri-
mary pool when the increasing ratio of support vec-
tors in the current classifier decreases, because the
gain of accuracy is very little once the ratio is down.
This phenomenon is observed in our experiments
(Section 5). This observation has also been reported
in previous studies (Schohn and Cohn, 2000).
In Two Pool Algorithm we add new unlabeled ex-
amples so that the total number of examples includ-
ing both labeled examples in the training set and un-
labeled examples in the primary pool is doubled. For
example, suppose that the size of a initial primary
pool is 1,000 examples. Before starting training,
there are no labeled examples and 1,000 unlabeled
examples. We add 1,000 new unlabeled examples to
the primary pool when the increasing ratio of sup-
port vectors is down after t examples has been la-
beled. Then, there are the t labeled examples and
the (2; 000   t) unlabeled examples in the primary
pool. At the next time when we add new unlabeled
examples, the number of newly added examples is
2,000 and then the total number of both labeled in
the training set and unlabeled examples in the pri-
mary pool is 4,000.
Our second variation, which is called Two Pool
Algorithm B, adds new unlabeled examples to the
primary pool when the number of support vectors of
the current classifier exceeds a threshold d. The d is
defined as:
d = N
?
100
; 0 < ?  100 (4)
where ? is a parameter for deciding when unlabeled
examples are added to the primary pool and N is
the number of examples including both labeled ex-
amples in the training set and unlabeled ones in the
primary pool. The ? must be less than the percentage
of support vectors of a training set3. When deciding
how many unlabeled examples should be added to
the primary pool, we use the strategy as described in
the paragraph above.
4 Japanese Word Segmentation
4.1 Word Segmentation as a Classification Task
Many tasks in natural language processing can be
formulated as a classification task (van den Bosch
3Since typically the percentage of support vectors is small
(e.g., less than 30 %), we choose around 10 % for ?. We need
further studies to find the best value of ? before or during train-
ing.
et al, 1996). Japanese word segmentation can be
viewed in the same way, too (Shinnou, 2000). Let a
Japanese character sequence be s = c
1
c
2
   c
m
and
a boundary b
i
exist between c
i
and c
i+1
. The b
i
is
either +1 (word boundary) or  1 (non-boundary).
The word segmentation task can be defined as de-
termining the class of the b
i
. We use an SVM to
determine it.
4.2 Features
We assume that each character c
i
has two attributes.
The first attribute is a character type (t
i
). It can
be hiragana4, katakana, kanji (Chinese characters),
numbers, English alphabets, kanji-numbers (num-
bers written in Chinese), or symbols. A character
type gives some hints to segment a Japanese sen-
tence to words. For example, kanji is mainly used
to represent nouns or stems of verbs and adjectives.
It is never used for particles, which are always writ-
ten in hiragana. Therefore, it is more probable that a
boundary exists between a kanji character and a hi-
ragana character. Of course, there are quite a few
exceptions to this heuristics. For example, some
proper nouns are written in mixed hiragana, kanji
and katakana.
The second attribute is a character code (k
i
). The
range of a character code is from 1 to 6,879. JIS X
0208, which is one of Japanese character set stan-
dards, enumerates 6,879 characters.
We use here four characters to decide a word
boundary. A set of the attributes of c
i 1
; c
i
; c
i+1
,
and c
i+2
is used to predict the label of the b
i
. The
set consists of twenty attributes: ten for the char-
acter type (t
i 1
t
i
t
i+1
t
i+2
, t
i 1
t
i
t
i+1
, t
i 1
t
i
, t
i 1
,
t
i
t
i+1
t
i+2
, t
i
t
i+1
, t
i
, t
i+1
t
i+2
, t
i+1
, t
i+2
), and an-
other ten for the character code (k
i 1
k
i
k
i+1
k
i+2
,
k
i 1
k
i
k
i+1
, k
i 1
k
i
, k
i 1
, k
i
k
i+1
k
i+2
, k
i
k
i+1
, k
i
,
k
i+1
k
i+2
, k
i+1
, and k
i+2
).
5 Experimental Results and Discussion
We used the EDR Japanese Corpus (EDR, 1995) for
experiments. The corpus is assembled from var-
ious sources such as newspapers, magazines, and
textbooks. It contains 208,000 sentences. We se-
lected randomly 20,000 sentences for training and
4Hiragana and katakana are phonetic characters which rep-
resent Japanese syllables. Katakana is primarily used to write
foreign words.
10,000 sentences for testing. Then, we created ex-
amples using the feature encoding method in Sec-
tion 4. Through these experiments we used the orig-
inal SVM tools, the algorithm of which is based on
SMO (Sequential Minimal Optimization) by Platt
(1999). We used linear SVMs and set a missclas-
sification cost C to 0:2.
First, we changed the number of labeled examples
which were randomly selected. This is an experi-
ment on passive learning. Table 2 shows the accu-
racy at different sizes of labeled examples.
Second, we changed the number of examples in
a pool and ran the active learning algorithm in Sec-
tion 3.2. We use the same examples for a pool as
those used in the passive learning experiments. We
selected 1,000 examples at each iteration of the ac-
tive learning. Figure 4 shows the learning curve of
this experiment and Figure 5 is a close-up of Fig-
ure 4. We see from Figure 4 that active learning
works quite well and it significantly reduces labeled
examples to be required. Let us see how many la-
beled examples are required to achieve 96.0 % ac-
curacy. In active learning with the pool, the size of
which is 2,500 sentences (97,349 examples), only
28,813 labeled examples are needed, whereas in pas-
sive learning, about 97,000 examples are required.
That means over 70 % reduction is realized by ac-
tive learning. In the case of 97 % accuracy, approx-
imately the same percentage of reduction is realized
when using the pool, the size of which is 20,000 sen-
tences (776,586 examples).
Now let us see how the accuracy curve varies de-
pending on the size of a pool. Surprisingly, the per-
formance of a larger pool is worse than that of a
smaller pool in the early stage of training5. One rea-
son for this could be that support vectors in selected
examples at each iteration from a larger pool make
larger clusters than those selected from a smaller
pool do. In other words, in the case of a larger pool,
more examples selected at each iteration would be
similar to each other. We computed variances6of
each 1,000 selected examples at the learning itera-
tion from 2 to 11 (Table 1). The variances of se-
5Tong and Koller (2000) have got the similar results in a
text classification task with two small pools: 500 and 1000.
However, they have concluded that a larger pool is better than
a smaller one because the final accuracy of the former is higher
than that of the latter.
6The variance 2 of a set of selected examples x
i
is defined
Table 1: Variances of Selected Examples
Iteration 2 3 4 5 6 7 8 9 10 11
1,250 Sent. Size Pool 16.87 17.25 17.85 17.63 17.24 17.37 17.34 17.73 17.94 17.57
20,000 Sent. Size Pool 16.66 17.03 16.92 16.75 16.80 16.72 16.91 16.93 16.87 16.97
lected examples using the 20,000 sentence size pool
is always lower than those using the 1,250 sentence
size pool. The result is not inconsistent with our hy-
pothesis.
Before we discuss the results of Two Pool Algo-
rithm, we show in Figure 6 how support vectors of
a classifier increase and the accuracy changes when
using the 2,500 sentence size pool. It is clear that
after the accuracy improvement almost stops, the in-
crement of the number of support vectors is down.
We also observed the same phenomenon with differ-
ent sizes of pools. We utilize this phenomenon in
Algorithm A.
Next, we ran Two Pool Algorithm A7. The result
is shown in Figure 7. The accuracy curve of Algo-
rithm A is better than that of the previously proposed
method at the number of labeled examples roughly
up to 20,000. After that, however, the performance
of Algorithm A does not clearly exceed that of the
previous method.
The result of Algorithm B is shown in Figure 8.
We have tried three values for ? : 5 %, 10 %, and 20
%. The performance with ? of 10 %, which is best,
is plotted in Figure 8. As noted above, the improve-
ment by Algorithm A is limited, whereas it is re-
markable that the accuracy curve of Algorithm B is
always the same or better than those of the previous
algorithm with different sizes of pools (the detailed
information about the performance is shown in Ta-
ble 3). To achieve 97.0 % accuracy Algorithm B re-
quires only 59,813 labeled examples, while passive
as:

2
=
1
n
n
X
i=1
jjx
i
 mjj
2
where m = 1
n
P
n
i=1
x
i
and n is the number of selected exam-
ples.
7In order to stabilize the algorithm, we use the following
strategy at (d) in Figure 3: add new unlabeled examples to the
primary pool when the current increment of support vectors is
less than half of the average increment.
Table 2: Accuracy at Different Labeled Data Sizes
with Random Sampling
# of
Sen-
tences
# of Ex-
amples
# of
Binary
Features
Accuracy
(%)
21 813 5896 89.07
41 1525 10224 90.30
81 3189 18672 91.65
162 6167 32258 92.93
313 12218 56202 93.89
625 24488 98561 94.73
1250 48701 168478 95.46
2500 97349 288697 96.10
5000 194785 493942 96.66
10000 387345 827023 97.10
20000 776586 1376244 97.40
learning requires about 343,0008 labeled examples
and the previous method with the 200,000 sentence
size pool requires 100,813. That means 82.6 % and
40.7 % reduction compared to passive learning and
the previous method with the 200,000 sentence size
pool, respectively.
6 Conclusion
To our knowledge, this is the first paper that reports
the empirical results of active learning with SVMs
for a more complex task in natural language process-
ing than a text classification task. The experimental
results show that SVM active learning works well
for Japanese word segmentation, which is one of
such complex tasks, and the naive use of a large pool
with the previous method of SVM active learning is
less effective. In addition, we have proposed a novel
technique to improve the learning curve when using
a large number of unlabeled examples and have eval-
8We computed this by simple interpolation.
Table 3: Accuracy of Different Active Learning Al-
gorithms
Pool Size
# of Algo. Algo. 1250 5,000 20,000
Ex. A B Sent. Sent. Sent.
813 89.07 89.07 89.07 89.07 89.07
1813 91.70 91.70 91.48 90.89 90.61
3813 93.82 93.82 93.60 93.11 92.42
6813 94.62 94.93 94.90 94.23 93.53
12813 95.24 95.87 95.29 95.42 94.82
24813 95.98 96.43 95.46 96.20 95.80
48813 96.51 96.88 96.51 96.62
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Ac
cu
ra
cy
Number of labeled examples
Passive (Random Sampling)
Active (1250 Sent. Size Pool)
Active (2500 Sent. Size Pool)
Active (5000 Sent. Size Pool)
Active (20,000 Sent. Size Pool)
Figure 4: Accuracy Curve with Different Pool Sizes
0.91
0.92
0.93
0.94
0.95
0.96
0 5000 10000 15000 20000 25000
Ac
cu
ra
cy
Number of labeled examples
Passive (Random Sampling)
Active (1250 Sent. Size Pool)
Active (2500 Sent. Size Pool)
Active (5000 Sent. Size Pool)
Active (20,000 Sent. Size Pool)
Figure 5: Accuracy Curve with Different Pool Sizes
(close-up)
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Ac
cu
ra
cy
Number of labeled examples
0
5000
10000
15000
20000
25000
30000
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
N
um
be
r o
f S
up
po
rt 
Ve
ct
or
s
Number of labeled examples
Figure 6: Change of Accuracy and Number of Sup-
port Vectors of Active Learning with 2500 Sentence
Size Pool
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Ac
cu
ra
cy
Number of labeled examples
Passive (Random Sampling)
Active (Algorithm A)
Active (20,000 Sent. Size Pool)
Figure 7: Accuracy Curve of Algorithm A
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Ac
cu
ra
cy
Number of labeled examples
Passive (Random Sampling)
Active (Algorithm B)
Active (20,000 Sent. Size Pool)
Figure 8: Accuracy Curve of Algorithm B
uated it by Japanese word segmentation. Our tech-
nique outperforms the method in previous research
and can significantly reduce required labeled exam-
ples to achieve a given level of accuracy.
References
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings of ACL-2001, pages 26?33.
Ido Dagan and Sean P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers.
In Proceedings of the Tweleveth International Confer-
ence on Machine Learning, pages 150?157.
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algorithms
and representations for text categorization. In Pro-
ceedings of the ACM CIKM International Conference
on Information and Knowledge Management, pages
148?155.
EDR (Japan Electoric Dictionary Research Institute),
1995. EDR Electoric Dictionary Technical Guide.
Rebecca Hwa. 2000. Sample selection for statitical
grammar induction. In Proceedings of EMNLP/VLC
2000, pages 45?52.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning.
Taku Kudo and Yuji Matsumoto. 2000a. Japanese depen-
dency structure analysis based on support vector ma-
chines. In Proceedings of the 2000 Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 18?25.
Taku Kudo and Yuji Matsumoto. 2000b. Use of support
vector learning for chunk identification. In Proceed-
ings of the 4th Conference on CoNLL-2000 and LLL-
2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001, pages 192?199.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the Seventeenth Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Rettrieval, pages 3?12.
Andrew Kachites McCallum and Kamal Nigam. 1998.
Employing EM and pool-based active learning for text
classification. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 359?
367.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL-2000,
pages 117?216.
John C. Platt. 1999. Fast training of support vec-
tor machines using sequential minimal optimization.
In Bernhard Scho?lkopf, Christopher J.C. Burges, and
Alexander J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 185?208. MIT
Press.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning.
Hiroyuki Shinnou. 2000. Deterministic Japanese word
segmentation by decision list method. In Proceedings
of the Sixth Pacific Rim International Conference on
Artificial Intelligence, page 822.
Cynthia A. Thompson, Mary Leaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of the Sixteenth International Conference on
Machine Learning, pages 406?414.
Simon Tong and Daphne Koller. 2000. Support vector
machine active learning with applications to text clas-
sification. In Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning.
Antal van den Bosch, Walter Daelemans, and Ton Wei-
jters. 1996. Morphological analysis as classification:
an inductive-learning approach. In Proceedings of the
Second International Conference on New Methods in
Natural Language Processing, pages 79?89.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In Proceedings of ACL-2000, pages
207?216.
David Yarowsky. 1995. Unsupervised word sence dis-
ambiguation rivaling supvervised methods. In Pro-
ceedings of ACL-1995, pages 189?196.
Combining Outputs of Multiple Japanese Named Entity Chunkers
by Stacking
Takehito Utsuro
Department of Information
and Computer Sciences,
Toyohashi University of Technology
Tenpaku-cho, Toyohashi 441-8580, Japan
utsuro@ics.tut.ac.jp
Manabu Sassano
Fujitsu Laboratories, Ltd.
4-4-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
sassano@jp.fujitsu.com
Kiyotaka Uchimoto
Keihanna Human Info-Communications Research Center,
Communications Research Laboratory
Hikaridai Seika-cho, Kyoto 619-0289, Japan
uchimoto@crl.go.jp
Abstract
In this paper, we propose a method for
learning a classifier which combines out-
puts of more than one Japanese named
entity extractors. The proposed combi-
nation method belongs to the family of
stacked generalizers, which is in principle
a technique of combining outputs of sev-
eral classifiers at the first stage by learn-
ing a second stage classifier to combine
those outputs at the first stage. Individ-
ual models to be combined are based on
maximum entropy models, one of which
always considers surrounding contexts of
a fixed length, while the other consid-
ers those of variable lengths according to
the number of constituent morphemes of
named entities. As an algorithm for learn-
ing the second stage classifier, we employ
a decision list learning method. Experi-
mental evaluation shows that the proposed
method achieves improvement over the
best known results with Japanese named
entity extractors based on maximum en-
tropy models.
1 Introduction
In the recent corpus-based NLP research, sys-
tem combination techniques have been successfully
applied to several tasks such as parts-of-speech
tagging (van Halteren et al, 1998), base noun
phrase chunking (Tjong Kim Sang, 2000), and pars-
ing (Henderson and Brill, 1999; Henderson and
Brill, 2000). The aim of system combination is to
combine portions of the individual systems? outputs
which are partial but can be regarded as highly ac-
curate. The process of system combination can be
decomposed into the following two sub-processes:
1. Collect systems which behave as differently as
possible: it would help a lot if at least the col-
lected systems tend to make errors of differ-
ent types, because simple voting technique can
identify correct outputs.
Previously studied techniques for collecting
such systems include: i) using several exist-
ing real systems (van Halteren et al, 1998;
Brill and Wu, 1998; Henderson and Brill, 1999;
Tjong Kim Sang, 2000), ii) bagging/boosting
techniques (Henderson and Brill, 1999; Hen-
derson and Brill, 2000), and iii) switching the
data expression and obtaining several mod-
els (Tjong Kim Sang, 2000).
2. Combine the outputs of the several systems:
previously studied techniques include: i) vot-
ing techniques (van Halteren et al, 1998;
Tjong Kim Sang, 2000; Henderson and Brill,
1999; Henderson and Brill, 2000), ii) switch-
ing among several systems according to con-
fidence values they provide (Henderson and
Brill, 1999), iii) stacking techniques (Wolpert,
1992) which train a second stage classifier for
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 281-288.
                         Proceedings of the Conference on Empirical Methods in Natural
combining outputs of classifiers at the first
stage (van Halteren et al, 1998; Brill and Wu,
1998; Tjong Kim Sang, 2000).
In this paper, we propose a method for combining
outputs of (Japanese) named entity chunkers, which
belongs to the family of stacking techniques. In
the sub-process 1, we focus on models which dif-
fer in the lengths of preceding/subsequent contexts
to be incorporated in the models. As the base model
for supervised learning of Japanese named entity
chunking, we employ a model based on the maxi-
mum entropy model (Uchimoto et al, 2000), which
performed the best in IREX (Information Retrieval
and Extraction Exercise) Workshop (IREX Commit-
tee, 1999) among those based on machine learning
techniques. Uchimoto et al (2000) reported that the
optimal number of preceding/subsequent contexts to
be incorporated in the model is two morphemes to
both left and right from the current position. In this
paper, we train several maximum entropy models
which differ in the lengths of preceding/subsequent
contexts, and then combine their outputs.
As the sub-process 2, we propose to apply a stack-
ing technique which learns a classifier for com-
bining outputs of several named entity chunkers.
This second stage classifier learns rules for accept-
ing/rejecting outputs of several individual named en-
tity chunkers. The proposed method can be applied
to the cases where the number of constituent systems
is quite small (e.g., two). Actually, in the experimen-
tal evaluation, we show that the results of combining
the best performing model of Uchimoto et al (2000)
with the one which performs poorly but extracts
named entities quite different from those of the
best performing model can help improve the perfor-
mance of the best model.
2 Named Entity Chunking based on
Maximum Entropy Models
2.1 Task of the IREX Workshop
The task of named entity recognition of the IREX
workshop is to recognize eight named entity types
in Table 1 (IREX Committee, 1999). The organizer
of the IREX workshop provided 1,174 newspaper
articles which include 18,677 named entities as the
training data. In the formal run (general domain)
Table 1: Statistics of NE Types of IREX
frequency (%)
NE Type Training Test
ORGANIZATION 3676 (19.7) 361 (23.9)
PERSON 3840 (20.6) 338 (22.4)
LOCATION 5463 (29.2) 413 (27.4)
ARTIFACT 747 (4.0) 48 (3.2)
DATE 3567 (19.1) 260 (17.2)
TIME 502 (2.7) 54 (3.5)
MONEY 390 (2.1) 15 (1.0)
PERCENT 492 (2.6) 21 (1.4)
Total 18677 1510
of the workshop, the participating systems were re-
quested to recognize 1,510 named entities included
in the held-out 71 newspaper articles.
2.2 Named Entity Chunking
We first provide our definition of the task of
Japanese named entity chunking (Sekine et al,
1998; Borthwick et al, 1998; Uchimoto et al,
2000). Suppose that a sequence of morphemes is
given as below:
(
Left
Context ) (Named Entity) (
Right
Context )
? ? ?M
L
?k
? ? ?M
L
?1
M
NE
1
? ? ?M
NE
i
? ? ?M
NE
m
M
R
1
? ? ?M
R
l
? ? ?
?
(Current Position)
Given that the current position is at the morpheme
MNE
i
, the task of named entity chunking is to assign
a chunking state (to be described in section 2.3.1) to
the morpheme MNE
i
at the current position, consid-
ering the patterns of surrounding morphemes. Note
that in the supervised learning phase, we can use the
chunking information on which morphemes consti-
tute a named entity, and which morphemes are in the
left/right contexts of the named entity.
2.3 The Maximum Entropy Model
In the maximum entropy model (Della Pietra et al,
1997), the conditional probability of the output y
given the context x can be estimated as the follow-
ing p
?
(y | x) of the form of the exponential family,
where binary-valued indicator functions called fea-
ture functions f
i
(x, y) are introduced for expressing
a set of ?features?, or ?attributes? of the context x
and the output y. A parameter ?
i
is introduced for
each feature f
i
, and is estimated from a training data.
p
?
(y | x) =
exp
(
?
i
?
i
f
i
(x, y)
)
?
y
exp
(
?
i
?
i
f
i
(x, y)
)
Uchimoto et al (2000) defines the context x as the
patterns of surrounding morphemes as well as that at
the current position, and the output y as the named
entity chunking state to be assigned to the mor-
pheme at the current position.
2.3.1 Named Entity Chunking States
Uchimoto et al (2000) classifies classes of
named entity chunking states into the following 40
tags:
? Each of eight named entity types plus an ?OP-
TIONAL? type are divided into four chunking
states, namely, the beginning/middle/end of an
named entity, or an named entity consisting of
a single morpheme. This amounts to 9?4 = 36
classes.
? Three more classes are distinguished for mor-
phemes immediately preceding/following a
named entity, as well as the one between two
named entities.
? Other morphemes are assigned the class
?OTHER?.
2.3.2 Features
Following Uchimoto et al (2000), feature func-
tions for morphemes at the current position as well
as the surrounding contexts are defined. More
specifically, the following three types of feature
functions are used: 1
1. 2052 lexical items that are observed five times
or more within two morphemes from named
entities in the training corpus.
2. parts-of-speech tags of morphemes2.
3. character types of morphemes (i.e., Japanese
(hiragana or katakana), Chinese (kanji), num-
bers, English alphabets, symbols, and their
combinations).
As for the number of preceding/subsequent mor-
phemes as contextual clues, we consider the follow-
ing models:
1Minor modifications from those of Uchimoto et al (2000)
are: i) we used character types of morphemes because they are
known to be useful in the Japanese named entity chunking, and
ii) the sets of parts-of-speech tags are different.
2As a Japanese morphological analyzer, we used BREAK-
FAST (Sassano et al, 1997) with the set of about 300 part-of-
speech tags. BREAKFAST achieves 99.6% part-of-speech accu-
racy against newspaper articles.
5-gram model
This model considers the preceding two mor-
phemes M
?2
, M
?1
as well as the subsequent two
morphemes M
1
, M
2
as the contextual clue. Both in
(Uchimoto et al, 2000) and in this paper, this is the
model which performs the best among all the indi-
vidual models without system combination.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?2
M
?1
M
0
M
1
M
2
? ? ?
7-gram model
This model considers the preceding three mor-
phemes M
?3
, M
?2
, M
?1
as well as the subsequent
three morphemes M
1
, M
2
, M
3
as the contextual
clue.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?3
M
?2
M
?1
M
0
M
1
M
2
M
3
? ? ?
9-gram model
This model considers the preceding four mor-
phemes M
?4
, M
?3
, M
?2
, M
?1
as well as the subse-
quent four morphemes M
1
, M
2
, M
3
, M
4
as the con-
textual clue.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?4
? ? ?M
?1
M
0
M
1
? ? ?M
4
? ? ?
For both 7-gram and 9-gram models, we consider
the following three modifications to those models:
? with all features
? with lexical items and parts-of-speech
tags (without the character types) of
M
{(?4),?3,3,(4)}
? with only the lexical items of M
{(?4),?3,3,(4)}
In our experiments, the number of features is
13,200 for 5-gram model and 15,071 for 9-gram
model. The number of feature functions is 31,344
for 5-gram model and 35,311 for 9-gram model.
Training a variable length (5?9-gram) model,
testing with 9-gram model
The major disadvantage of the 5/7/9-gram models
is that in the training phase it does not take into ac-
count whether or not the preceding/subsequent mor-
phemes constitute one named entity together with
the morpheme at the current position. Consider-
ing this disadvantage, we examine another model,
namely, variable length model, which incorporates
variable length contextual information. In the train-
ing phase, this model considers which of the preced-
ing/subsequent morphemes constitute one named
entity together with the morpheme at the current po-
sition (Sassano and Utsuro, 2000). It also considers
several morphemes in the left/right contexts of the
named entity. Here we restrict this model to explic-
itly considering the cases of named entities of the
length up to three morphemes and only implicitly
considering those longer than three morphemes. We
also restrict it to considering two morphemes in both
left and right contexts of the named entity.
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
? ? ?MNE
i
? ? ?MNE
m(?3)
MR
1
MR
2
? ? ?
?
(Current Position)
1. In the cases where the current named entity
consists of up to three morphemes, all the con-
stituent morphemes are regarded as within the
current named entity. The following is an ex-
ample of this case, where the current named
entity consists of three morphemes, and the
current position is at the middle of those con-
stituent morphemes as below:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MR
1
MR
2
? ? ?
? (1)
(Current Position)
2. In the cases where the current named entity
consists of more than three morphemes, only
the three constituent morphemes are regarded
as within the current named entity and the rest
are treated as if they were outside the named
entity. For example, suppose that the cur-
rent named entity consists of four morphemes:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MNE
4
MR
1
MR
2
? ? ?
?
(Current Position)
In this case, the fourth constituent morpheme
MNE
4
is treated as if it were in the right context
of the current named entity as below:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MNE
4
MR
1
? ? ?
?
(Current Position)
In the testing phase, we apply this model consid-
ering the preceding four morphemes as well as the
subsequent four morphemes at every position, as in
the case of 9-gram model3.
We consider the following three modifications to
this model, where we suppose that the morpheme at
the current position be M
0
:
? with all features
? with lexical items and parts-of-speech tags
(without the character types) of M
{?4,?3,3,4}
? with only the lexical items of M
{?4,?3,3,4}
3 Learning to Combine Outputs of Named
Entity Chunkers
3.1 Data Sets
The following gives the training and test data sets
for our framework of learning to combine outputs of
named entity chunkers.
1. TrI : training data set for learning individual
named entity chunkers.
2. TrC: training data set for learning a classifier
for combining outputs of individual named en-
tity chunkers.
3. Ts: test data set for evaluating the classifier for
combining outputs of individual named entity
chunkers.
3.2 Procedure
The following gives the procedure for learning the
classifier to combine outputs of named entity chun-
kers using TrI and TrC.
1. Train the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) using TrI .
2. Apply the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) to TrC, respectively,
and obtain the list of chunked named entities
NEList
i
(TrC) for each named entity chun-
ker NEchk
i
.
3Note that, as opposed to the training phase, the length of
preceding/subsequent contexts is fixed in the testing phase of
this model. Although this discrepancy between training and
testing damages the performance of this single model (sec-
tion 4.1), it is more important to note that this model tends to
have distribution of correct/over-generated named entities dif-
ferent from that of the 5-gram model. In section 4, we exper-
imentally show that this difference is the key to improving the
named entity chunking performance by system combination.
Table 2: Examples of Event Expressions for Combining Outputs of Multiple Systems
Segment Morpheme(POS) NE Outputs ofIndividual Systems Event Expressions
System 0 System 1
.
.
.
SegEv
i
rainen
(?next year?,
temporal noun)
10gatsu
(?October?,
temporal noun)
rainen
(DATE)
10gatsu
(DATE)
rainen
-10gatsu
(DATE)
{
systems=?0?,mlength=1,
NEtag=DATE,
POS=?temporal noun?, class
NE
=?
}
{
systems=?0?,mlength=1,
NEtag=DATE,
POS=?temporal noun?, class
NE
=?
}
{
systems=?1?,mlength=2,
NEtag=DATE,
POS=?temporal noun, temporal noun?,
class
NE
=+
}
.
.
.
SegEv
i+1
seishoku
(?reproductive?, noun)
iryou
(?medical?, noun)
gijutsu
(?technology?, noun)
seishoku
-iryou
-gijutsu
(ARTIFACT)
{
systems=?0?, class
sys
=?no outputs?
}
{
systems=?1?,mlength=3,
NEtag=ARTIFACT,
POS=?noun,noun,noun?, class
NE
=?
}
nitsuite
(?about?, particle)
.
.
.
3. Align the lists NEList
i
(TrC) (i = 1, . . . , n)
of chunked named entities according to the po-
sitions of the chunked named entities in the text
TrC, and obtain the event expression TrCev
of TrC.
4. Train the classifier NEchk
cmb
for combining
outputs of individual named entity chunkers us-
ing the event expression TrCev.
The following gives the procedure for applying the
learned classifier to Ts.
1. Apply the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) to Ts, respectively,
and obtain the list of chunked named entities
NEList
i
(Ts) for each named entity chunker
NEchk
i
.
2. Align the lists NEList
i
(Ts) (i=1, . . . , n) of
chunked named entities according to the posi-
tions of the chunked named entities in the text
Ts, and obtain the event expression Tsev of
Ts.
3. Apply NEchk
comb
to Tsev and evaluate its
performance.
3.3 Data Expressions
3.3.1 Events
The event expression TrCev of TrC is obtained
by aligning the lists NEList
i
(TrC) (i =1, . . . , n)
of chunked named entities, and is represented as a
sequence of segments, where each segment is a set
of aligned named entities. Chunked named enti-
ties are aligned under the constraint that those which
share at least one constituent morpheme have to be
aligned into the same segment. Examples of seg-
ments, into which named entities chunked by two
systems are aligned, are shown in Table 2. In the
first segment SegEv
i
, given the sequence of the two
morphemes, the system No.0 decided to extract two
named entities, while the system No.1 chunked the
two morphemes into one named entity. In those
event expressions, systems indicates the list of the
indices of the systems which output the named en-
tity, mlength gives the number of the constituent
morphemes, NEtag gives one of the nine named
entity types, POS gives the list of parts-of-speech
of the constituent morphemes, and class
NE
indi-
cates whether the named entity is a correct one com-
pared against the gold standard (?+?), or the one
over-generated by the systems (???).
In the second segment SegEv
i+1
, only the sys-
tem No.1 decided to extract a named entity from
the sequence of the three morphemes. In this case,
the event expression for the system No.0 is the one
which indicates that no named entity is extracted by
the system No.0.
In the training phase, each segment SegEv
j
of
event expression constitutes a minimal unit of an
event, from which features for learning the classi-
fier are extracted. In the testing phase, the classes
of each system?s outputs are predicted against each
segment SegEv
j
.
3.3.2 Features and Classes
In principle, features for learning the classifier for
combining outputs of named entity chunkers are rep-
resented as a set of pairs of the system indices list
?p, . . . , q? and a feature expression F of the named
entity:
f =
{
?systems=?p, . . . , q?, F ?
? ? ?
?systems=?p?, . . . , q??, F ??
}
(2)
In the training phase, any possible feature of this
form is extracted from each segment SegEv
j
of
event expression. The system indices list ?p, . . . , q?
indicates the list of the systems which output the
named entity. A feature expression F of the named
entity can be any possible subset of the full feature
expression {mlength= ? ? ? , NEtag= ? ? ? , POS =
? ? ?}, or the set indicating that the system outputs no
named entity within the segment.
F =
?
?
?
?
?
?
?
any subset of
{
mlength= ? ? ? ,
NEtag= ? ? ? , POS= ? ? ?
}
{
class
sys
=?no outputs?
}
In the training and testing phases, within each
segment SegEv
j
of event expression, a class is as-
signed to each system, where each class classi
sys
for
the i-th system is represented as a list of the classes
of the named entities output by the system:
classi
sys
=
{
+/?, . . . , +/?
?no output? (i = 1, . . . , n)
3.4 Learning Algorithm
We apply a simple decision list learning method
to the task of learning a classifier for combining
outputs of named entity chunkers4. A decision
list (Yarowsky, 1994) is a sorted list of decision
rules, each of which decides the value of class given
some features f of an event. Each decision rule in
a decision list is sorted in descending order with
respect to some preference value, and rules with
higher preference values are applied first when ap-
plying the decision list to some new test data. In
this paper, we simply sort the decision list according
to the conditional probability P (class
i
| f) of the
class
i
of the i-th system?s output given a feature f .
4 Experimental Evaluation
We experimentally evaluate the performance of the
proposed system combination method using the
IREX workshop?s training and test data.
4.1 Comparison of Outputs of Individual
Systems
First, Table 3 shows the performance of the indi-
vidual models described in the section 2.3.2, where
trained with the IREX workshop?s training data, and
tested against the IREX workshop?s test data as Ts.
The 5-gram model performs the best among those
individual models.
Next, assuming that each of the models other
than the 5-gram model is combined with the 5-gram
model, Table 4 compares the named entities of their
outputs. Recall rate of the correct named entities in
the union of their outputs, as well as the overlap rate5
of the over-generated entities against those included
in the output of the 5-gram model are shown.
From the Tables 3 and 4, it is clear that the 7-gram
and 9-gram models are quite similar to the 5-gram
model both in the performance and in the distribu-
tion of correct/over-generated named entities. On
the other hand, variable length models have distri-
bution of correct/over-generated named entities a lit-
4It is quite straightforward to apply any other supervised
learning algorithms to this task.
5For a model X , the overlap rate of the over-generated enti-
ties against those included in the output of the 5-gram model is
defined as: (# of the intersection of the over-generated entities
output by the 5-gram model and those output by the model X)/
(# of the over-generated entities output by the 5-gram model).
Table 3: Performance of Individual Models against
Ts (F-measure (? = 1) (%))
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 80.78 80.81 80.71
9-gram 80.13 80.53 80.53
variable length 45.12 77.02 75.16
5-gram 81.16
Table 4: Difference between 5-gram model and
Other Individual Models (Recall of the Union /
Overlap Rate of Over-generated Entities) (%)
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 79.8/85.2 79.8/85.2 79.7/91.2
9-gram 79.7/84.7 79.7/86.1 79.5/90.7
variable
length 82.6/27.3 81.4/63.4 80.4/72.7
tle different from that of the 5-gram model. Vari-
able length models have lower performance mainly
because of the difference between the training and
testing phases with respect to the modeling of con-
text lengths. Especially, the variable length model
with ?all? features of M
{?4,?3,3,4}
has much lower
performance as well as significantly different dis-
tribution of correct/over-generated named entities.
This is because character types features are so gen-
eral that many (erroneous) named entities are over-
generated, while sometimes they contribute to find-
ing named entities that are never detected by any of
the other models.
4.2 Results of Combining System Outputs
This section reports the results of combining the out-
put of the 5-gram model with that of 7-gram models,
9-gram models, and the variable length models. As
the training data sets TrI and TrC, we evaluate the
following two assignments (a) and (b), where D
CRL
denotes the IREX workshop?s training data:
(a) TrI: D
CRL
? D200
CRL
(200 articles from D
CRL
)
TrC: D200
CRL
(b) TrI = TrC = D
CRL
We use the IREX workshop?s test data for Ts.
In the assignment (a), TrI and TrC are disjoint,
while in the assignment (b), individual named entity
chunkers are applied to their own training data, i.e.,
closed data. The assignment (b) is for the sake of
avoiding data sparseness in learning the classifier for
combining outputs of two named entity chunkers.
Table 5 shows the peformance in F-measure (? =
1) for both assignments (a) and (b). For both (a) and
Table 5: Performance of Combining 5-gram model
and Other Individual Models (against Ts, F-measure
(? = 1) (%))
(a) TrI = D
CRL
? D200
CRL
, TrC = D200
CRL
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 81.54 81.53 80.60
9-gram 81.31 81.26 80.60
variable length 83.43 81.55 81.85
(b) TrI = TrC = D
CRL
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 81.97 81.83 81.58
9-gram 81.53 81.66 81.52
variable length 84.07 83.07 82.50
(b), ?5-gram + variable length (All)? significantly
outperforms the 5-gram model, which is the best
model among all the individual models without sys-
tem combination. It is remarkable that models which
perform poorly but extract named entities quite dif-
ferent from those of the best performing model can
actually help improve the best model by the pro-
posed method. The performance for the assignment
(b) is better than that for the assignment (a). This re-
sult claims that the training data size should be larger
when learning the classifier for combining outputs of
two named entity chunkers.
In the Table 6, for the best performing result (i.e.,
5-gram + variable length (All)) as well as the con-
stituent individual models (5-gram model and vari-
able length model (All)), we classify the system
output according to the number of constituent mor-
phemes of each named entity. In the Table 7, we
classify the system output according to the named
entity types. The following summarizes several re-
markable points of these results: i) the benefit of the
system combination is more in the improvement of
precision rather than in that of recall. This means
that the proposed system combination technique is
useful for detecting over-generation of named en-
tity chunkers, ii) the combined outputs of the 5-gram
model and the variable length model improve the re-
sults of chunking longer named entities quite well
compared with shorter named entities. This is the
effect of the variable length features of the variable
length model.
Table 6: Evaluation Results of Combining System Outputs, per # of constituent morphemes
(TrI = TrC = D
CRL
, F-measure (? = 1) / Recall / Precision (%))
n Morphemes to 1 Named Entity
n ? 1 n = 1 n = 2 n = 3 n ? 4
5-gram 81.16 83.60 86.94 68.42 50.59
78.87/83.60 84.97/82.28 85.90/88.00 63.64/73.98 35.83/86.00
variable length (All) 45.12 53.77 56.63 33.74 16.78
51.50/40.15 38.69/88.14 71.37/47.93 57.34/23.91 40.00/10.62
5-gram + variable length (All) 84.07 85.06 88.96 75.19 65.96
81.45/86.86 85.12/84.99 87.42/90.56 69.93/81.30 51.67/91.18
Table 7: Evaluation Results of Combining System Outputs, per NE type
(TrI = TrC = D
CRL
, F-measure (? = 1) (Recall, Precision) (%))
ORGANI- PER- LOCA- ARTI- DATE TIME MONEY PER-
ZATION SON TION FACT CENT
67.74 81.82 77.04 30.43 91.49 93.20 92.86 87.18
5-gram (58.45) (79.88) (71.91) (29.17) (88.85) (88.89) (86.67) (80.95)
(80.53) (83.85) (82.96) (31.82) (94.29) (97.96) (100.00) (94.44)
35.48 48.45 38.47 5.80 78.60 56.90 60.61 87.18
variable length (All) (37.40) (48.52) (32.93) (22.92) (81.92) (61.11) (66.67) (80.95)
(33.75) (48.38) (46.26) (3.32) (75.53) (53.23) (55.56) (94.44)
5-gram + 72.18 84.15 79.58 38.71 92.86 93.20 92.86 87.18
variable length (All) (62.88) (81.66) (73.61) (37.50) (90.00) (88.89) (86.67) (80.95)
(84.70) (86.79) (86.61) (40.00) (95.90) (97.96) (100.00) (94.44)
5 Conclusion
This paper proposed a method for learning a classi-
fier to combine outputs of more than one Japanese
named entity chunkers. Experimental evaluation
showed that the proposed method achieved improve-
ment in F-measure over the best known results with
an ME model (Uchimoto et al, 2000), when a com-
plementary model extracted named entities quite dif-
ferently from the best performing model.
References
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Proc.
6th Workshop on VLC, pages 152?160.
E. Brill and J. Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. 17th COLING
and 36th ACL, pages 191?195.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
J. C. Henderson and E. Brill. 1999. Exploiting diversity
in natural language processing: Combining parsers. In
Proc. 1999 EMNLP and VLC, pages 187?194.
J. C. Henderson and E. Brill. 2000. Bagging and boost-
ing a treebank parser. In Proc. 1st NAACL, pages 34?
41.
IREX Committee, editor. 1999. Proceedings of the IREX
Workshop. (in Japanese).
M. Sassano and T. Utsuro. 2000. Named entity chunking
techniques in supervised learning for Japanese named
entity recognition. In Proceedings of the 18th COL-
ING, pages 705?711.
M. Sassano, Y. Saito, and K. Matsui. 1997. Japanese
morphological analyzer for NLP applications. In Proc.
3rd Annual Meeting of the Association for Natural
Language Processing, pages 441?444. (in Japanese).
S. Sekine, R. Grishman, and H. Shinnou. 1998. A deci-
sion tree method for finding and classifying names in
Japanese texts. In Proc. 6th Workshop on VLC, pages
148?152.
E. Tjong Kim Sang. 2000. Noun phrase recognition by
system combination. In Proc. 1st NAACL, pages 50?
55.
K. Uchimoto, Q. Ma, M. Murata, H. Ozaku, and H. Isa-
hara. 2000. Named entity extraction based on a maxi-
mum entropy model and transformation rules. In Proc.
38th ACL, pages 326?335.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Im-
proving data driven wordclass tagging by system com-
bination. In Proc. 17th COLING and 36th ACL, pages
491?497.
D. H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in Span-
ish and French. In Proc. 32nd ACL, pages 88?95.
Learning with Multiple Stacking for Named Entity Recognition
Koji Tsukamoto and Yutaka Mitsuishi and Manabu Sassano
Fujitsu Laboratories Ltd.
 
tukamoto,mitsuishi-y,sassano  @jp.fujitsu.com
1 Introduction
In this paper, we present a learning method us-
ing multiple stacking for named entity recognition.
In order to take into account the tags of the sur-
rounding words, we propose a method which em-
ploys stacked learners using the tags predicted by
the lower level learners. We have applied this ap-
proach to the CoNLL-2002 shared task to improve
a base system.
2 System Description
Before describing our system, let us see one aspect
of the named entity recognition, the outline of our
method, and the relation to the previous works.
The task of named entity recognition can be re-
garded as a process of assigning a named entity tag
to each given word, taking into account the patterns
of surrounding words. Suppose that a sequence of
words is given as below:
... W  , W  , W  , W  , W  ...
Then, given that the current position is at word W  ,
the task is to assign tag T  to W  .
In the named entity recognition task, an entity is
often made up of a sequence of words, rather than
a single word. For example, an entity ?the United
States of America? consists of five words. In order
to allocate a tag to each word, the tags of the sur-
rounding words (we call these tags the surrounding
tags) can be a clue to predict the tag of the word
(we call this tag the current tag). For the test set,
however, these tags are unknown.
In order to take into account the surrounding tags
for the prediction of the current tag, we propose a
method which employs multiple stacked learners,
an extension of stacking method (Wolpert, 1992).
Stacking based method for named entity recognition
usually employs two or more level learners. The
higher level learner uses the current tags predicted
by its lower level learners. In our method, by con-
trast, the higher level learner uses not only the cur-
rent tag but also the surrounding tags predicted by
the lower level learner. Our aim is to leverage the
performance of the base system using the surround-
ing tags as the features.
At least two groups have previously proposed
systems which use the predicted surrounding tags.
One system, proposed by van Halteren et al (1998),
also uses stacking method. This system uses four
completely different types of taggers as the first
level learners, because it has been assumed that first
level learners should be as different as possible. The
tags predicted by the first level learners are used as
the features of the second level learner.
The other system, proposed by (Kudo and Mat-
sumoto, 2000; Yamada et al, 2001), uses the ?dy-
namic features?. In the test phase, the predicted tags
of the preceding (or subsequent) words are used as
the features, which are called ?dynamic features?.
In the training phase, the system uses the answer
tags of the preceding (or subsequent) words as the
features.
More detailed descriptions of our system are
shown below:
2.1 Learning Algorithm
As the learning algorithm for all the levels , we use
an extension of AdaBoost, the real AdaBoost.MH
which is extended to handle multiclass problems
(Schapire and Singer, 1999). For weak learners, we
use decision stumps (Schapire and Singer, 1999),
which select only one feature to classify an exam-
ple.
2.2 Features
We use the following types of the features for the
prediction of the tag of the word.
 surface form of W 	 , W  , W  , W  and W  .
Word Feature Example Text
Digit 25
Digit+Alphabet CDG1
Symbol .
Uppercase EFE
Capitalized Australia
Lowercase(word length 
 characters) necesidad
Lowercase(word length  characters) del
Other hoy,
Table 1: Word features and examples
 One of the eight word features in Table 1.
These features are similar to those used in
(Bikel et al, 1997).
 First and last two/three letters of W 
 Estimated tag of W  based on the word uni-
gram model in the training set.
Additionally, we use the surrounding tag feature.
This feature is discussed in Section 2.3.
2.3 Multiple Stacking
In order to take into account the tags of the sur-
rounding words, our system employs stacked learn-
ers. Figure 1 gives the outline of the learning and
applying algorithm of our system. In the learning
phase, the base system is trained at first. After that,
the higher level learners are trained using word fea-
tures (described in Section 2.2), current tag T  and
surrounding tags T 	  predicted by the lower
level learner. While these tag may not be correctly
predicted , if the accuracy of the prediction of the
lower level learner is improved, the features used in
each prediction become accurate. In the applying
phase, all of the learners are cascaded in the order.
Compared to the previous systems (van Halteren
et al, 1998; Kudo and Matsumoto, 2000; Yamada
et al, 2001), our system is: (i) employing more
than two levels stacking, (ii) using only one algo-
rithm and training only one learner at each level,
(iii) using the surrounding tag given by the lower
level learner. (iv) using both the preceding and sub-
sequent tags as the features. (v) using the predicted
tags instead of the answer tags in the training phase.
3 Experiments and Results
In this section, the experimental conditions and the
results of the proposed method are shown.
In order to improve the performance of the base
system, the tag sequence to be predicted is format-
ted according to IOB1, even though the sequence
Let L  denote the  th level learner and let T   denote  th
level output tags for W  .
Learning:
1. Train the base learner L  using the features described in
Table 1.
2. for  = 1,...,N
 Get T

ff  	
 	  
 
 
  
 

    Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49?52,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Unified Single Scan Algorithm
for Japanese Base Phrase Chunking and Dependency Parsing
Manabu Sassano
Yahoo Japan Corporation
Midtown Tower,
9-7-1 Akasaka, Minato-ku,
Tokyo 107-6211, Japan
msassano@yahoo-corp.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We describe an algorithm for Japanese
analysis that does both base phrase chunk-
ing and dependency parsing simultane-
ously in linear-time with a single scan of a
sentence. In this paper, we show a pseudo
code of the algorithm and evaluate its per-
formance empirically on the Kyoto Uni-
versity Corpus. Experimental results show
that the proposed algorithm with the voted
perceptron yields reasonably good accu-
racy.
1 Introduction
Single scan algorithms of parsing are important for
interactive applications of NLP. For instance, such
algorithms would be more suitable for robots ac-
cepting speech inputs or chatbots handling natural
language inputs which should respond quickly in
some situations even when human inputs are not
clearly ended.
Japanese sentence analysis typically consists of
three major steps, namely morphological analysis,
bunsetsu (base phrase) chunking, and dependency
parsing. In this paper, we describe a novel algo-
rithm that combines the last two steps into a sin-
gle scan process. The algorithm, which is an ex-
tension of Sassano?s (2004), allows us to chunk
morphemes into base phrases and decide depen-
dency relations of the phrases in a strict left-to-
right manner. We show a pseudo code of the al-
gorithm and evaluate its performance empirically
with the voted perceptron on the Kyoto University
Corpus (Kurohashi and Nagao, 1998).
2 Japanese Sentence Structure
In Japanese NLP, it is often assumed that the struc-
ture of a sentence is given by dependency relations
Meg-ga kare-ni ano pen-wo age-ta.
Meg-subj to him that pen-acc give-past.
ID 0 1 2 3 4
Head 4 4 3 4 -
Figure 1: Sample sentence (bunsetsu-based)
among bunsetsus. A bunsetsu is a base phrasal
unit and consists of one or more content words fol-
lowed by zero or more function words.
In addition, most of algorithms of Japanese de-
pendency parsing, e.g., (Sekine et al, 2000; Sas-
sano, 2004), assume the three constraints below.
(1) Each bunsetsu has only one head except the
rightmost one. (2) Dependency links between bun-
setsus go from left to right. (3) Dependency links
do not cross one another. In other words, depen-
dencies are projective.
A sample sentence in Japanese is shown in Fig-
ure 1. We can see all the constraints are satisfied.
3 Previous Work
As far as we know, there is no dependency parser
that does simultaneously both bunsetsu chunking
and dependency parsing and, in addition, does
them with a single scan. Most of the modern
dependency parsers for Japanese require bunsetsu
chunking (base phrase chunking) before depen-
dency parsing (Sekine et al, 2000; Kudo and Mat-
sumoto, 2002; Sassano, 2004). Although word-
based parsers are proposed in (Mori et al, 2000;
Mori, 2002), they do not build bunsetsus and are
not compatible with other Japanese dependency
parsers. Multilingual parsers of participants in the
CoNLL 2006 shared task (Buchholz and Marsi,
2006) can handle Japanese sentences. But they are
basically word-based.
49
Meg ga kare ni ano pen wo age-ta.
Meg subj him to that pen acc give-past.
ID 0 1 2 3 4 5 6 7
Head 1 7 3 7 6 6 7 -
Type B D B D D B D -
Figure 2: Sample sentence (morpheme-based).
?Type? represents the type of dependency relation.
4 Algorithm
4.1 Dependency Representation
In our proposed algorithm, we use a morpheme-
based dependency structure instead of a bunsetsu-
based one. The morpheme-based representation
is carefully designed to convey the same informa-
tion on dependency structure of a sentence without
the loss from the bunsetsu-based one. The right-
most morpheme of the bunsetsu t should modify
the rightmost morpheme of the bunsetsu u when
the bunsetsu t modifies the bunsetsu u. Every
morpheme except the rightmost one in a bunsetsu
should modify its following one. The sample sen-
tence in Figure 1 is converted to the sentence with
our proposed morpheme-based representation in
Figure 2.
Take for instance, the head of the 0-th bunsetsu
?Meg-ga? is the 4-th bunsetsu ?age-ta.? in Fig-
ure 1. This dependency relation is represented by
that the head of the morpheme ?ga? is ?age-ta.? in
Figure 2.
The morpheme-based representation above can-
not explicitly state the boundaries of bunsetsus.
Thus we add the type to every dependency rela-
tion. A bunsetsu boundary is represented by the
type associated with every dependency relation.
The type ?D? represents that this relation is a de-
pendency of two bunsetsus, while the type ?B?
represents a sequence of morphemes inside of a
given bunsetsu. In addition, the type ?O?, which
represents that two morphemes do not have a de-
pendency relation, is used in implementations of
our algorithm with a trainable classifier. Following
this encoding scheme of the type of dependency
relations bunsetsu boundaries exist just after the
morphemes that have the type ?D?. Inserting ?|?
after every morpheme with ?D? of the sentence in
Figure 2 results in Meg-ga | kare-ni | ano | pen-wo
| age-ta. This is identical to the sentence with the
bunsetsu-based representation in Figure 1.
Input: w
i
: morphemes in a given sentence.
N : the number of morphemes.
Output: h
j
: the head IDs of morphemes w
j
.
t
j
: the type of dependency relation. A possible
value is either ?B?, ?D?, or ?O?.
Functions: Push(i, s): pushes i on the stack s.
Pop(s): pops a value off the stack s.
Dep(j, i, w, t): returns true when w
j
should
modify w
i
. Otherwise returns false. Sets
always t
j
.
procedure Analyze(w, N , h, t)
var s: a stack for IDs of modifier morphemes
begin
Push(?1, s); { ?1 for end-of-sentence }
Push(0, s);
for i ? 1 to N ? 1 do begin
j ? Pop(s);
while (j 6= ?1
and (Dep(j, i, w, t) or (i = N ? 1)) ) do
begin
h
j
? i; j ? Pop(s)
end
Push(j, s); Push(i, s)
end
end
Figure 3: Pseudo code for base phrase chunking
and dependency parsing.
4.2 Pseudo Code for the Proposed Algorithm
The algorithm that we propose is based on (Sas-
sano, 2004), which is considered to be a simple
form of shift-reduce parsing. The pseudo code of
our algorithm is presented in Figure 3. Important
variables here are h
j
and t
j
where j is an index
of morphemes. The variable h
j
holds the head ID
and the variable t
j
has the type of dependency re-
lation. For example, the head and the dependency
relation type of ?Meg? in Figure 2 are represented
as h
0
= 1 and t
0
= ?B? respectively. The flow
of the algorithm, which has the same structure as
Sassano?s (2004), is controlled with a stack that
holds IDs for modifier morphemes. Decision of
the relation between two morphemes is made in
Dep(), which uses a machine learning-based clas-
sifier that supports multiclass prediction.
The presented algorithm runs in a left-to-right
manner and its upper bound of the time complex-
ity is O(n). Due to space limitation, we do not
discuss its complexity here. See (Sassano, 2004)
50
for further details.
5 Experiments and Discussion
5.1 Experimental Set-up
Corpus For evaluation, we used the Kyoto Uni-
versity Corpus Version 2 (Kurohashi and Nagao,
1998). The split for training/test/development is
the same as in other papers, e.g., (Uchimoto et al,
1999).
Selection of a Classifier and its Setting We im-
plemented a parser with the voted perceptron (VP)
(Freund and Schapire, 1999). We used a poly-
nomial kernel and set its degree to 3 because cu-
bic kernels proved to be effective empirically for
Japanese parsing (Kudo and Matsumoto, 2002).
The number of epoch T of VP was selected using
the development test set. For multiclass predic-
tion, we used the pairwise method (Kre?el, 1999).
Features We have designed rather simple fea-
tures based on the common feature set (Uchimoto
et al, 1999; Kudo and Matsumoto, 2002; Sassano,
2004) for bunsetsu-based parsers. We use the fol-
lowing features for each morpheme:
1. major POS, minor POS, conjugation type,
conjugation form, surface form (lexicalized
form)
2. Content word or function word
3. Punctuation (periods and commas)
4. Open parentheses and close parentheses
5. Location (at the beginning or end of the sen-
tence)
Gap features between two morphemes are also
used since they have proven to be very useful and
contribute to the accuracy (Uchimoto et al, 1999;
Kudo and Matsumoto, 2002). They are repre-
sented as a binary feature and include distance (1,
2, 3, 4 ? 10, or 11 ?), particles, parentheses, and
punctuation.
In our proposed algorithm basically two mor-
phemes are examined to estimate their dependency
relation. Context information about the current
morphemes to be estimated would be very use-
ful and we can incorporate such information into
our model. We assume that we have the j-th mor-
pheme and the i-th one in Figure 3. We also use
the j?n, ..., j?1, j+1, ..., j+n morphemes and
the i ? n, ..., i ? 1, i + 1, ..., i + n ones, where n
Measure Accuracy (%)
Dependency Acc. 93.96
Dep. Type Acc. 99.49
Both 93.92
Table 1: Performance on the test set. This result is
achieved by the following parameters: The size of
context window is 2 and epoch T is 4.
Bunsetsu-based Morpheme-based
Previous 88.48 95.09
Ours NA 93.96
Table 2: Dependency accuracy. The system with
the previous method employs the algorithm (Sas-
sano, 2004) with the voted perceptron.
is the size of the context window. We examined 0,
1, 2 and 3 for n.
5.2 Results and Discussion
Accuracy Performances of our parser on the test
set is shown in Table 1. The dependency accuracy
is the percentage of the morphemes that have a
correct head. The dependency type accuracy is the
percentage of the morphemes that have a correct
dependency type, i.e., ?B? or ?D?. The bottom line
of Table 1 shows the percentage of the morphemes
that have both a correct head and a correct depen-
dency type. In all these measures we excluded the
last morpheme in a sentence, which does not have
a head and its associated dependency type.
The accuracy of dependency type in Table 1
is interpreted to be accuracy of base phrase
(bunsetsu) chunking. Very accurate chunking is
achieved.
Next we examine the dependency accuracy. In
order to recognize how accurate it is, we com-
pared the performance of our parser with that of
the parser that uses one of previous methods. We
implemented a parser that employs the algorithm
of (Sassano, 2004) with the commonly used fea-
tures and runs with VP instead of SVM, which
Sassano (2004) originally used. His parser, which
cannot do bunsetsu chunking, accepts only a chun-
ked sentence and then produces a bunsetsu-based
dependency structure. Thus we cannot directly
compare results with ours. To enable us to com-
pare them we gave bunsetsu chunked sentences by
our parser to the parser of (Sassano, 2004) instead
of giving directly the correct chunked sentences
51
Window Size Dep. Acc. Dep. Type Acc.
0 (T = 1) 82.71 99.29
1 (T = 2) 93.57 99.49
2 (T = 4) 93.96 99.49
3 (T = 3) 93.79 99.42
Table 3: Performance change depending on the
context window size
 0
 0.5
 1
 1.5
 2
 2.5
 3
 0  10  20  30  40  50  60  70  80  90  100
Se
co
nd
s
Sentence Length (Number of Morphemes)
Figure 4: Running time on the test set. We used
a PC (Intel Xeon 2.33 GHz with 8GB memory on
FreeBSD 6.3).
in the Kyoto University Corpus. And then we re-
ceived results from the parser of (Sassano, 2004),
which are bunsetsu-based dependency structures,
and converted them to morpheme-based structures
that follow the scheme we propose in this paper.
Finally we have got results that have the compat-
ible format and show a comparison with them in
Table 2.
Although the bunsetsu-based parser outper-
formed slightly our morpheme-based parser in this
experiment, it is still notable that our method
yields comparable performance with even a sin-
gle scan of a sentence for dependency parsing in
addition to bunsetsu chunking. According to the
results in Table 2, we suppose that performance of
our parser roughly corresponds to about 86?87%
in terms of bunsetsu-based accuracy.
Context Window Size Performance change de-
pending on the size of context window is shown
in Table 3. Among them the best size is 2. In
this case, we use ten morphemes to determine
whether or not given two morphemes have a de-
pendency relation. That is, to decide the relation
of morphemes j and i (j < i), we use morphemes
j?2, j?1, j, j+1, j+2 and i?2, i?1, i, i+1, i+2.
Running Time and Asymptotic Time Complex-
ity We have observed that the running time is
proportional to the sentence length (Figure 4). The
theoretical time complexity of the proposed algo-
rithm is confirmed with this observation.
6 Conclusion and Future Work
We have described a novel algorithm that com-
bines Japanese base phrase chunking and depen-
dency parsing into a single scan process. The pro-
posed algorithm runs in linear-time with a single
scan of a sentence.
In future work we plan to combine morpholog-
ical analysis or word segmentation into our pro-
posed algorithm. We also expect that structure
analysis of compound nouns can be incorporated
by extending the dependency relation types. Fur-
thermore, we believe it would be interesting to
discuss linguistically and psycholinguistically the
differences between Japanese and other European
languages such as English. We would like to know
what differences lead to easiness of analyzing a
Japanese sentence.
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of CoNLL
2006, pages 149?164.
Y. Freund and R. E. Schapire. 1999. Large margin classifi-
cation using the perceptron algorithm. Machine Learning,
37(3):277?296.
U. Kre?el. 1999. Pairwise classification and support vec-
tor machines. In B. Scho?lkopf, C. J. Burges, and A. J.
Smola, editors, Advances in Kernel Methods: Support
Vector Learning, pages 255?268. MIT Press.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of CoNLL-
2002, pages 63?69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proc. of LREC-1998, pages 719?724.
S. Mori, M. Nishimura, N. Itoh, S. Ogino, and H. Watanabe.
2000. A stochastic parser based on a structural word pre-
diction model. In Proc. of COLING 2000, pages 558?564.
S. Mori. 2002. A stochastic parser based on an SLM with
arboreal context trees. In Proc. of COLING 2002.
M. Sassano. 2004. Linear-time dependency analysis for
Japanese. In Proc. of COLING 2004, pages 8?14.
S. Sekine, K. Uchimoto, and H. Isahara. 2000. Back-
ward beam search algorithm for dependency analysis of
Japanese. In Proc. of COLING-00, pages 754?760.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese
dependency structure analysis based on maximum entropy
models. In Proc. of EACL-99, pages 196?203.
52
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 189?192,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Learning Semantic Categories from Clickthrough Logs
Mamoru Komachi
Nara Institute of Science and Technology (NAIST)
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
mamoru-k@is.naist.jp
Shimpei Makimoto and Kei Uchiumi and Manabu Sassano
Yahoo Japan Corporation
Midtown Tower, 9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
{smakimot,kuchiumi,msassano}@yahoo-corp.jp
Abstract
As the web grows larger, knowledge ac-
quisition from the web has gained in-
creasing attention. In this paper, we pro-
pose using web search clickthrough logs
to learn semantic categories. Experimen-
tal results show that the proposed method
greatly outperforms previous work using
only web search query logs.
1 Introduction
Compared to other text resources, search queries
more directly reflect search users? interests (Sil-
verstein et al, 1998). Web search logs are get-
ting a lot more attention lately as a source of in-
formation for applications such as targeted adver-
tisement and query suggestion.
However, it may not be appropriate to use
queries themselves because query strings are often
too heterogeneous or inspecific to characterize the
interests of the user population. Although it is not
clear that query logs are the best source of learning
semantic categories, all the previous studies using
web search logs rely on web search query logs.
Therefore, we propose to use web search
clickthrough logs to learn semantic categories.
Joachims (2002) developed a method that utilizes
clickthrough logs for training ranking of search
engines. A search clickthrough is a link which
search users click when they see the result of
their search. The intentions of two distinct search
queries are likely to be similar, if not identical,
when they have the same clickthrough. Search
clickthrough logs are thus potentially useful for
learnin semantic categories. Clickthrough logs
have the additional advantage that they are avail-
able in abundance and can be stored at very low
cost.1 Our proposed method employs search click-
1As for data availability, MSN Search query logs
(RFP 2006 dataset) were provided to WSCD09: Work-
through logs to improve semantic category acqui-
sition in both precision and recall.
We cast semantic category acquisition from
search logs as the task of learning labeled in-
stances from few labeled seeds. To our knowledge
this is the first study that exploits search click-
through logs for semantic category learning.2
2 Related Work
There are many techniques that have been devel-
oped to help elicit knowledge from query logs.
These algorithms use contextual patterns to extract
a category or a relation in order to learn a target in-
stance which belongs to the category (e.g. cat in
animal class) or a pair of words in specific relation
(e.g. headquarter to a company). In this work,
we focus on extracting named entities of the same
class to learn semantic categories.
Pas?ca and Durme (2007) were the first to dis-
cover the importance of search query logs in nat-
ural language processing applications. They fo-
cused on learning attributes of named entities, and
thus their objective is different from ours. An-
other line of new research is to combine various re-
sources such as web documents with search query
logs (Pas?ca and Durme, 2008; Talukdar et al,
2008). We differ from this work in that we use
search clickthrough logs rather than search query
logs.
Komachi and Suzuki (2008) proposed a boot-
strapping algorithm called Tchai, dedicated to the
task of semantic category acquisition from search
query logs. It achieves state-of-the-art perfor-
mance for this task, but it only uses web search
query logs.
shop on Web Search Click Data 2009 participants. http://
research.microsoft.com/en-US/um/people/nickcr/WSCD09/
2After the submission of this paper, we found that (Xu et
al., 2009) also applies search clickthrough logs to this task.
This work independently confirms the effectiveness of click-
through logs to this task using different sources.
189
Figure 1: Labels of seeds are propagated to unla-
beled nodes.
3 Quetchup3 Algorithm
In this section, we describe an algorithm for
learning semantic categories from search logs us-
ing label propagation. We name the algorithm
Quetchup.
3.1 Semi-supervised Learning by Laplacian
Label Propagation
Graph-based semi-supervised methods such as la-
bel propagation are known to achieve high perfor-
mance with only a few seeds and have the advan-
tage of scalability.
Figure 1 illustrates the process of label propa-
gation using a seed term ?singapore? to learn the
Travel domain.
This is a bipartite graph whose left-hand side
nodes are terms and right-hand side nodes are
patterns. The strength of lines indicates related-
ness between each node. The darker a node, the
more likely it belongs to the Travel domain. Start-
ing from ?singapore,? the pattern ?? airlines? 4 is
strongly related to ?singapore,? and thus the label
of ?singapore? will be propagated to the pattern.
On the other hand, the pattern ?? map? is a neu-
tral pattern which co-occurs with terms other than
the Travel domain such as ?google? and ?yahoo.?
Since the term ?china? shares two patterns, ?? air-
lines? and ?? map,? with ?singapore,? the label of
the seed term ?singapore? propagates to ?china.?
?China? will then be classified in the Travel do-
main. In this way, label propagation gradually
propagates the label of seed instances to neigh-
bouring nodes, and optimal labels are given as the
3Query Term Chunk Processor
4
? is the place into which a query fits.
Input:
Seed instance vector F (0)
Instance similarity matrix A
Output:
Instance score vector F (t)
1: Construct the normalized Laplacian matrix L = I ?
D
?1/2
AD
?1/2
2: Iterate F (t + 1) = ?(?L)F (t) + (1 ? ?)F (0) until
convergence
Figure 2: Laplacian label propagation algorithm
labels at which the label propagation process has
converged.
Figure 2 describes label propagation based on
the regularized Laplacian. Let a sample x
i
be x
i
?
X , F (0) be a score vector of x comprised of a
label set y
i
? Y , and F (t) be a score vector of
x after step t. Instance-instance similarity matrix
A is defined as A = W TW where W is a row-
normalized instance-pattern matrix. The (i, j)-th
element of W
ij
contains the normalized frequency
of co-occurrence of instance x
i
and pattern p
j
. D
is a diagonal degree matrix of N where the (i, i)th
element of D is given as D
ii
=
?
j
N
ij
.
This algorithm in Figure 2 is similar to (Zhou
et al, 2004) except for the method of construct-
ing A and the use of graph Laplacian. Zhou et al
proposed a heuristic to set A
ii
= 0 to avoid self-
reinforcement5 because Gaussian kernel was used
to create A. The Laplacian label propagation does
not need such a heuristic because the graph Lapla-
cian automatically reduces self-reinforcement by
assigning negative weights to self-loops.
In the task of learning one category, scores of la-
beled (seed) instances are set to 1 whereas scores
of unlabeled instances are set to 0. The output is
a score vector which holds relatedness to seed in-
stances in descending order. In the task of learning
two categories, scores of seed instances are set to
either 1 or ?1, respectively, and the final label of
instance x
i
will be determined by the sign of out-
put score vector y
i
.
Label propagation has a parameter ? ? (0, 1]
that controls how much the labels of seeds are em-
phasized. As ? approaches 0 it puts more weight
on labeled instances, while as ? increases it em-
ploys both labeled and unlabeled data.
There exists a closed-form solution for Lapla-
cian label propagation:
5Avoiding self-reinforcement is important because it
causes semantic drift, a phenomenon where frequent in-
stances and patterns unrelated to seed instances infect seman-
tic category acquisition as iteration proceeds.
190
Category Seed
Travel jal (Japan Airlines), ana (All Nippon
Airways), jr (Japan Railways),
(jalan: online travel guide site), his
(H.I.S.Co.,Ltd.: travel agency)
Finance (Mizuho Bank),
(Sumitomo Mitsui Banking Corporation),
jcb, (Shinsei Bank),
(Nomura Securities)
Table 1: Seed terms for each category
F
?
=
?
?
t=0
(?(?L))
t
F (0) = (I + ?L)
?1
F (0)
However, the matrix inversion leads to O(n3)
complexity, which is far from realistic in a real-
world configuration. Nonetheless, it can be ap-
proximated by fixing the number of steps for label
propagation.
4 Experiments with Web Search Logs
We will describe experimental result comparing
a previous method Tchai to the proposed method
Quetchup with clickthrough logs (Quetchup
click
)
and with query logs (Quetchup
query
).
4.1 Experimental Settings
Search logs We used Japanese search logs col-
lected in August 2008 from Yahoo! JAPAN Web
Search. We thresholded both search query and
clickthrough logs and retained the top 1 million
distinct queries. Search logs are accompanied by
their frequencies within the logs.
Construction of an instance-pattern matrix
We used clicked links as clickthrough patterns.
Links clicked less than 200 times were removed.
After that, links which had only one co-occurring
query were pruned. 6 On the other hand, we used
two term queries as contextual patterns. For in-
stance, if one has the term ?singapore? and the
query ?singapore airlines,? the contextual pattern
?? airlines? will be created. Query patterns appear-
ing less than 100 times were discarded.
The (i, j)-th element of a row-normalized
instance-pattern matrix W is given by
W
ij
=
|x
i
,p
j
|
?
k
|x
i
,p
k
|
.
Target categories We used two categories,
Travel and Finance, to compare proposed methods
with (Komachi and Suzuki, 2008).
6Pruning facilitates the computation time and reduces the
size of instance-pattern matrix drastically.
When a query was a variant of a term or con-
tains spelling mistakes, we estimated original form
and manually assigned a semantic category. We
allowed a query to have more than two categories.
When a query had more than two terms, we as-
signed a semantic category to the whole query tak-
ing each term into account.7
System We used the same seeds presented in Ta-
ble 1 for both Tchai and Quetchup. We used the
same parameter for Tchai described in (Komachi
and Suzuki, 2008), and collected 100 instances by
iterating 10 times and extracting 10 instances per
iteration. The number of iteration of Quetchup is
set to 10. The parameter ? is set to 0.0001.
Evaluation It is difficult in general to define re-
call for the task of semantic category acquisition
since the true set of instances is not known. Thus,
we evaluated all systems using precision at k and
relative recall (Pantel and Ravichandran, 2004).8
Relative recall is the coverage of a system given
another system as baseline.
4.2 Experimental Result
4.2.1 Effectiveness of Clickthrough Logs
Figures 3 to 6 plot precision and relative recall
for three systems to show effectiveness of search
clickthrough logs in improvement of precision and
relative recall. Relative recall of Quetchup
click
and
Tchai were calculated against Quetchup
query
.
Quetchup
click
gave the best precision among
three systems, and did not degenerate going down
through the list. In addition, it was demonstrated
that Quetchup
click
gives high recall. This result
shows that search clickthrough logs effectively im-
prove both precision and recall for the task of se-
mantic category acquisition.
On the other hand, Quetchup
query
degraded in
precision as its rank increased. Manual check of
the extracted queries revealed that the most promi-
nent queries were Pornographic queries, followed
by Food, Job and Housing, which frequently ap-
pear in web search logs. Other co-occurrence met-
rics such as pointwise mutual information would
be explored in the future to suppress the effect of
frequent queries.
In addition, Quetchup
click
constantly out-
performed Tchai in both the Travel and Fi-
7Since web search query logs contain many spelling mis-
takes, we experimented in a realistic configuration.
8Typically, precision at k is the most important measure
since the top k highest scored terms are evaluated by hand.
191
 0
 0.2
 0.4
 0.6
 0.8
 1
 10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
Quetchup (click)
Quetchup (query)
Tchai
Figure 3: Precision of Travel domain
 0
 0.2
 0.4
 0.6
 0.8
 1
 10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
Quetchup (click)
Quetchup (query)
Tchai
Figure 4: Precision of Finance domain
 0
 2
 4
 6
 8
 10
 10  20  30  40  50  60  70  80  90  100
R
el
at
iv
e 
re
ca
ll
Rank
Quetchup (click)
Tchai
Figure 5: Relative recall of Travel domain
 0
 2
 4
 6
 8
 10
 10  20  30  40  50  60  70  80  90  100
R
el
at
iv
e 
re
ca
ll
Rank
Quetchup (click)
Tchai
Figure 6: Relative recall of Finance domain
nance domains in precision and outperfomed
Quetchup
query
in relative recall. The differences
between the two domains of query-based systems
seem to lie in the size of correct instances. The Fi-
nance domain is a closed set which has only a few
effective query patterns, whereas Travel domain is
an open set which has many query patterns that
match correct instances. Quetchup
click
has an ad-
ditional advantage that it is stable across over the
ranked list, because the variance of the number of
clicked links is small thanks to the nature of the
ranking algorithm of search engines.
5 Conclusion
We have proposed a method called Quetchup
to learn semantic categories from search click-
through logs using Laplacian label propagation.
The proposed method greatly outperforms previ-
ous method, taking the advantage of search click-
through logs.
Acknowledgements
The first author is partly supported by the grant-in-
aid JSPS Fellowship for Young Researchers. We
thank the anonymous reviewers for helpful com-
ments and suggestions.
References
T. Joachims. 2002. Optimizing Search Engines Using Click-
through Data. KDD, pages 133?142.
M. Komachi and H. Suzuki. 2008. Minimally Supervised
Learning of Semantic Knowledge from Query Logs. IJC-
NLP, pages 358?365.
M. Pas?ca and B. V. Durme. 2007. What You Seek is What
You Get: Extraction of Class Attributes from Query Logs.
IJCAI-07, pages 2832?2837.
M. Pas?ca and B. V. Durme. 2008. Weakly-Supervised Ac-
quisition of Open-Domain Classes and Class Attributes
from Web Documents and Query Logs. ACL-2008, pages
19?27.
P. Pantel and D. Ravichandran. 2004. Automatically Label-
ing Semantic Classes. HLT/NAACL-04, pages 321?328.
C. Silverstein, M. Henzinger, H. Marais, and M. Moricz.
1998. Analysis of a Very Large AltaVista Query Log. Dig-
ital SRC Technical Note 1998-014.
P. P. Talukdar, J. Reisinger, M. Pas?ca, D. Ravichandran,
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised Ac-
quisition of Labeled Class Instances using Graph Random
Walks. EMNLP-2008, pages 581?589.
G. Xu, S. Yang, and H. Li. 2009. Named Entity Mining
from Click-Through Log Using Weakly Supervised Latent
Dirichlet Allocation. KDD. to appear.
D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scho?kopf.
2004. Learning with Local and Global Consistency.
NIPS, 16:321?328.
192
Virtual Examples for Text Classification with Support Vector Machines
Manabu Sassano
Fujitsu Laboratories Ltd.
4-1-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
sassano@jp.fujitsu.com
Abstract
We explore how virtual examples (artifi-
cially created examples) improve perfor-
mance of text classification with Support
Vector Machines (SVMs). We propose
techniques to create virtual examples for
text classification based on the assump-
tion that the category of a document is un-
changed even if a small number of words
are added or deleted. We evaluate the pro-
posed methods by Reuters-21758 test set
collection. Experimental results show vir-
tual examples improve the performance of
text classification with SVMs, especially
for small training sets.
1 Introduction
Corpus-based supervised learning is now a stan-
dard approach to achieve high-performance in nat-
ural language processing. However, the weakness
of supervised learning approach is to need an anno-
tated corpus, the size of which is reasonably large.
Even if we have a good supervised-learning method,
we cannot get high-performance without an anno-
tated corpus. The problem is that corpus annota-
tion is labor intensive and very expensive. In or-
der to overcome this, several methods are proposed,
including minimally-supervised learning methods
(e.g., (Yarowsky, 1995; Blum and Mitchell, 1998)),
and active learning methods (e.g., (Thompson et
al., 1999; Sassano, 2002)). The spirit behind these
methods is to utilize precious labeled examples max-
imally.
Another method following the same spirit is one
using virtual examples (artificially created exam-
ples) generated from labeled examples. This method
has been rarely discussed in natural language pro-
cessing. In terms of active learning, Lewis and Gale
(1994) mentioned the use of virtual examples in text
classification. They did not, however, take forward
this approach because it did not seem to be possi-
ble that a classifier created virtual examples of doc-
uments in natural language and then requested a hu-
man teacher to label them.
In the field of pattern recognition, some kind of
virtual examples has been studied. The first re-
port of methods using virtual examples with Sup-
port Vector Machines (SVMs) is that of Scho?lkopf
et al (1996), who demonstrated significant improve-
ment of the accuracy in hand-written digit recogni-
tion (Section 3). They created virtual examples from
labeled examples based on prior knowledge of the
task: slightly translated (e.g., 1 pixel shifted to the
right) images have the same label (class) of the orig-
inal image. Niyogi et al (1998) also discussed the
use of prior knowledge by creating virtual examples
and thereby expanding the effective training set size.
The purpose of this study is to explore the effec-
tiveness of virtual examples in NLP, motivated by
the results of Scho?lkopf et al (1996). To our knowl-
edge, use of virtual examples in corpus-based NLP
has never been studied so far. It is, however, im-
portant to investigate this approach by which it is
expected that we can alleviate the cost of corpus an-
notation. In particular, we focus on virtual examples
with Support Vector Machines, introduced by Vap-
nik (1995). The reason for this is that SVM is one of
most successful machine learning methods in NLP.
For example, NL tasks to which SVMs have been
applied are text classification (Joachims, 1998; Du-
mais et al, 1998), chunking (Kudo and Matsumoto,
2001), dependency analysis (Kudo and Matsumoto,
2002) and so forth.
In this study, we choose text classification as a
first case of the study of virtual examples in NLP be-
cause text classification in real world requires mini-
mizing annotation cost, and it is not too complicated
to perform some non-trivial experiments. Moreover,
there are simple methods, which we propose, to gen-
erate virtual examples from labeled examples (Sec-
tion 4). We show how virtual examples can improve
the performance of a classifier with SVM in text
classification, especially for small training sets.
2 Support Vector Machines
In this section we give some theoretical definitions
of SVMs. Assume that we are given the training data
(x
i
; y
i
); : : : ; (x
l
; y
l
);x
i
2 R
n
; y
i
2 f+1; 1g:
The decision function g in SVM framework is de-
fined as:
g(x) = sgn(f(x)) (1)
f(x) =
l
X
i=1
y
i

i
K(x
i
;x) + b (2)
where K is a kernel function, b 2 R is a threshold,
and 
i
are weights. Besides, the weights 
i
satisfy
the following constraints:
8i : 0  
i
 C and
l
X
i=1

i
y
i
= 0;
where C is a misclassification cost. The vectors x
i
with non-zero 
i
are called Support Vectors. For
linear SVMs, the kernel function K is defined as:
K(x
i
;x) = x
i
 x:
In this case, Equation 2 can be written as:
f(x) = w  x+ b (3)
where w =
P
l
i=1
y
i

i
x
i
. To train an SVM is to
find 
i
and b by solving the following optimization
negative example
positive example
support vector
Figure 1: Hyperplane (solid) and Support Vectors
problem:
maximize
l
X
i=1

i
 
1
2
l
X
i;j=1

i

j
y
i
y
j
K(x
i
;x
j
)
subject to 8i : 0  
i
 C and
l
X
i=1

i
y
i
= 0:
The solution gives an optimal hyperplane, which is a
decision boundary between the two classes. Figure 1
illustrates an optimal hyperplane and its support vec-
tors.
3 Virtual Examples and Virtual Support
Vectors
Virtual examples are generated from labeled exam-
ples.1 Based on prior knowledge of a target task, the
label of a generated example is set to the same value
as that of the original example.
For example, in hand-written digit recognition,
virtual examples can be created on the assumption
that the label of an example is unchanged even if the
example is shifted by one pixel in the four princi-
pal directions (Scho?lkopf et al, 1996; DeCoste and
Scho?lkopf, 2002).
Virtual examples that are generated from support
vectors are called virtual support vectors (Scho?lkopf
1We discuss here only virtual examples which are generated
from labeled examples. We do not consider examples, the labels
of which are not known.
Virtual
Examples
Figure 2: Hyperplane and Virtual Examples
et al, 1996). Reasonable virtual support vectors are
expected to give a better optimal hyperplane. As-
suming that virtual support vectors represent natu-
ral variations of examples of a target task, the de-
cision boundary should be more accurate. Figure 2
illustrates the idea of virtual support vectors. Note
that after virtual support vectors are given, the hy-
perplane is different from that in Figure 1.
4 Virtual Examples for Text Classification
We assume on text classification the following:
Assumption 1 The category of a document is un-
changed even if a small number of words are added
or deleted.
This assumption is reasonable. In typical cases of
text classification most of the documents usually
contain two or more keywords which may indicate
the categories of the documents.
Following Assumption 1, we propose two meth-
ods to create virtual examples for text classification.
One method is to delete some portion of a document.
The label of a virtual example is given from the orig-
inal document. The other method is to add a small
number of words to a document. The words to be
added are taken from documents, the label of which
is the same as that of the document. Although one
can invent various methods to create virtual exam-
ples based on Assumption 1, we propose here very
simple ones.
Document Id Feature Vector (x) Label (y)
1 (f
1
; f
2
; f
3
; f
4
; f
5
) +1
2 (f
2
; f
4
; f
5
; f
6
) +1
3 (f
2
; f
3
; f
5
; f
6
; f
7
) +1
4 (f
1
; f
3
; f
8
; f
9
; f
10
)  1
5 (f
1
; f
8
; f
10
; f
11
)  1
Table 1: Example of Document Set
Before describing our methods, we describe text
representation which we used in this study. We to-
kenize a document to words, downcase them and
then remove stopwords, where the stopword list of
freeWAIS-sf2 is used. Stemming is not performed.
We adopt binary feature vectors where word fre-
quency is not used.
Now we describe the two proposed methods:
GenerateByDeletion and GenerateByAddition. As-
sume that we are given a feature vector (a document)
x and x0 is a generated vector from x. GenerateBy-
Deletion algorithm is:
1. Copy x to x0.
2. For each binary feature f of x0, if rand() 
t then remove the feature f , where rand() is
a function which generates a random number
from 0 to 1, and t is a parameter to decide how
many features are deleted.
For example, suppose that we have a set of docu-
ments as in Table 1. Some possible virtual examples
generated from Document 1 by GenerateByDeletion
algorithm are (f
2
; f
3
; f
4
; f
5
;+1), (f
1
; f
3
; f
4
;+1),
or (f
1
; f
2
; f
4
; f
5
;+1).
On the other hand, GenerateByAddition algo-
rithm is:
1. Collect from a training set documents, the label
of which is the same as that of x.
2. Concatenate all the feature vectors (documents)
to create an array a of features. Each element
of a is a feature which represents a word.
3. Copy x to x0.
2Available at http://ls6-www.informatik.uni-dortmund.de/ir/
projects/freeWAIS-sf/
Category Name Training Test
earn 2877 1087
acq 1650 719
money-fx 538 179
grain 433 149
crude 389 189
trade 369 117
interest 347 131
ship 197 89
wheat 212 71
corn 181 56
Table 2: Number of Training and Test Examples
4. For each binary feature f of x0, if rand()  t
then select a feature randomly from a and put
it to x0.
For example, when we want to generate a virtual
example from Document 2 in Table 1 by Generate-
ByAddition algorithm, first we create an array a =
(f
1
; f
2
; f
3
; f
4
; f
5
; f
2
; f
4
; f
5
; f
6
; f
2
; f
3
; f
5
; f
6
; f
7
).
In this case, some possible virtual examples by
GenerateByAddition are (f
1
; f
2
; f
4
; f
5
; f
6
;+1),
(f
2
; f
3
; f
4
; f
5
; f
6
;+1), or (f
2
; f
4
; f
5
; f
6
; f
7
;+1).
An example such as (f
2
; f
4
; f
5
; f
6
; f
10
;+1) is never
generated from Document 2 because there are no
positive documents which have f
10
.
5 Experimental Results and Discussion
5.1 Test Set Collection
We used the Reuters-21578 dataset3 to evaluate the
proposed methods. The dataset has several splits of a
training set and a test set. We used here ?ModApte?
split, which is most widely used in the literature on
text classification. This split has 9,603 training ex-
amples and 3,299 test examples. More than 100 cat-
egories are in the dataset. We use, however, only the
most frequent 10 categories. Table 2 shows the 10
categories and the number of training and test exam-
ples in each of the categories.
5.2 Performance Measures
We use F-measure (van Rijsbergen, 1979; Lewis
and Gale, 1994) as a primal performance measure
3Available from David D. Lewis?s page: http://
www.daviddlewis.com/resources/testcollections/reuters21578/
to evaluate the result. F-measure is defined as:
F-measure =
(1 + 
2
)pq

2
p + q
(4)
where p is precision and q is recall and  is a param-
eter which decides the relative weight of precision
and recall. The p and the q are defined as:
p =
number of positive and correct outputs
number of positive outputs
q =
number of positive and correct outputs
number of positive examples
In Equation 4, usually  = 1 is used, which means
it gives equal weight to precision and recall.
When we evaluate the performance of a classifier
to a multiple category dataset, there are two ways
to compute F-measure: macro-averaging and micro-
averaging (Yang, 1999). The former way is to first
compute F-measure for each category and then aver-
age them, while the latter way is to first compute pre-
cision and recall for all the categories and use them
to calculate the F-measure.
5.3 SVM setting
Through our experiments we used our original SVM
tools, the algorithm of which is based on SMO (Se-
quential Minimal Optimization) by Platt (1999). We
used linear SVMs and set a misclassification cost C
to 0:016541 which is 1=(the average of x x) where
x is a feature vector in the 9,603 size training set.
For simplicity, we fixed C through all the experi-
ments. We built a binary classifier for each of the 10
categories shown in Table 2.
5.4 Results
First, we carried out experiments using GenerateBy-
Deletion and GenerateByAddition separately to cre-
ate virtual examples, where a virtual example was
created per Support Vector. We did not generate
virtual examples from non support vectors. We set
the parameter t to 0:054 for GenerateByDeletion and
GenerateByAddition for all the experiments.
To build an SVM with virtual examples we use
the following steps:
4We first tried t = 0:01; 0:05; and 0:10 with GenerateBy-
Deletion using the 9603 size training set. The value t = 0:05
yielded best micro-average F-measure for the test set. We used
the same value also for GenerateByAddition.
1. Train an SVM.
2. Extract Support Vectors.
3. Generate virtual examples from the Support
Vectors.
4. Train another SVM using both the original la-
beled examples and the virtual examples.
We evaluated the performance of the two methods
depending on the size of a training set. We created
subsamples by selecting randomly from the 9603
size training set. We prepared seven sizes: 9603,
4802, 2401, 1200, 600, 300, and 150.5 Micro-
average F-measures of the two methods are shown
in Table 3. We see from Table 3 that both the meth-
ods give better performance than that of the origi-
nal SVM. The smaller the number of examples in
the training set is, the larger the gain is. For the
9603 size training set, the gain of GenerateByDele-
tion is 0.75 (= 90:17   89:42), while for the 150
size set, the gain is 6.88 (= 60:16   53:28). These
results suggest that in the smaller training sets there
are not enough various examples to give a accurate
decision boundary and therefore the effect of virtual
examples is larger at the smaller training sets. It
is reasonable to conclude that GenerateByDeletion
and GenerateByAddition generated good virtual ex-
amples for the task and this led to the performance
gain.
After we found that the simple two methods to
generate virtual support vectors were effective, we
examined a combined method which is to use both
GenerateByDeletion and GenerateByAddition. Two
virtual examples are generated per Support Vector.
The performance of the combined method is also
shown in Table 3. The performance gain of the com-
bined method is larger than that with either Gener-
ateByDeletion or GenerateByAddition.
Furthermore, we carried out another experiment
with a combined method to create two virtual exam-
ples with GenerateByDeletion and GenerateByAd-
dition respectively. That is, four virtual examples
were generated from a Support Vector. The perfor-
mance of that setting is shown in Table 3. The best
5Since we selected samples randomly, some smaller training
sets of low frequent categories may have had few or even zero
positive examples.
50
55
60
65
70
75
80
85
90
95
100 1000 10000
M
ic
ro
-a
ve
ra
ge
 F
-m
ea
su
re
 (b
eta
 = 
1)
Number of Examples in Training Set
SVM + 4 Virtual SVs Per SV
SVM
Figure 3: Micro-Average F-Measure versus Number
of Examples in the Training Set
40
45
50
55
60
65
70
75
80
85
100 1000 10000
M
ac
ro
-a
ve
ra
ge
 F
-m
ea
su
re
 (b
eta
 = 
1)
Number of Examples in Training Set
SVM + 4 Virtual SVs per SV
SVM
Figure 4: Macro-Average F-Measure versus Num-
ber of Examples in the Training Set. For the smaller
training sets F-measures cannot be computed be-
cause the precisions are undefined.
result is achieved by the combined method to create
four virtual examples per Support Vector.
For the rest of this section, we limit our discussion
to the comparison of the results of the original SVM
and SVM with four virtual examples per SV (SVM
with 4 VSVs). The learning curves of the original
SVM and SVM with 4 VSVs are shown in Figures 3
and 4. It is clear that SVM with 4 VSVs outper-
forms the original SVM considerably in terms of
both micro-average F-measure and macro-average
F-measure. SVM with 4 VSVs achieves a given
level of performance with roughly half of the labeled
examples which the original SVM requires. One
might suppose that the improvement of F-measure
Number of Examples in Training Set
Method 9603 4802 2401 1200 600 300 150
Original SVM 89.42 86.58 81.69 77.24 71.08 64.44 53.28
SVM + 1 VSV per SV (GenerateByDeletion) 90.17 88.62 84.45 81.11 75.32 70.11 60.16
SVM + 1 VSV per SV (GenerateByAddition) 90.00 88.51 84.48 81.14 75.33 69.59 60.04
SVM + 2 VSVs per SV (Combined) 90.27 89.33 86.27 83.59 77.44 72.81 64.22
SVM + 4 VSVs per SV (Combined) 90.45 89.69 87.12 84.97 79.16 73.25 65.05
Table 3: Comparison of Micro-Average F-measure of Different Methods. ?VSV? means virtual SV.
1
2
3
4
5
6
100 1000 10000
Er
ro
r R
at
e 
(%
)
Number of Examples in Training Set
SVM + 4 Virtual SVs per SV
SVM
Figure 5: Error Rate versus Number of Examples in
the Training Set
is realized simply because the recall gets highly
improved while the error rate increases. We plot
changes of the error rate for 32990 tests (3299 tests
for each of the 10 categories) in Figure 5. SVM with
4 VSVs still outperforms the original SVM signifi-
cantly.6
The performance changes for each of the 10 cat-
egories are shown in Tables 4 and 5. SVM with 4
VSVs is better than the original SVM for almost
all the categories and all the sizes except for ?inter-
est? and ?wheat? at the 9603 size training set. For
low frequent categories such as ?ship?, ?wheat? and
?corn?, the classifiers of the original SVM perform
poorly. There are many cases where they never out-
put ?positive?, i.e. the recall is zero. It suggests that
the original SVM fails to find a good hyperplane due
to the imbalanced training sets which have very few
6We have done the significance test which is called ?p-test?
in (Yang and Liu, 1999), requiring significance at the 0.05 level.
Although at the 9603 size training set the improvement of the
error rate is not statistically significant, in all the other cases the
improvement is significant.
positive examples. In contrast, SVM with 4 VSVs
yields better results for such harder cases.
6 Conclusion and Future Directions
We have explored how virtual examples improve the
performance of text classification with SVMs. For
text classification, we have proposed methods to cre-
ate virtual examples on the assumption that the label
of a document is unchanged even if a small num-
ber of words are added or deleted. The experimen-
tal results have shown that our proposed methods
improve the performance of text classification with
SVMs, especially for small training sets. Although
the proposed methods are not readily applicable to
NLP tasks other than text classification, it is notable
that the use of virtual examples, which has been very
little studied in NLP, is empirically evaluated.
In the future, it would be interesting to employ
virtual examples with methods to use both labeled
and unlabeled examples (e.g., (Blum and Mitchell,
1998; Nigam et al, 1998; Joachims, 1999)). The
combined approach may yield better results with a
small number of labeled examples. Another interest-
ing direction would be to develop methods to create
virtual examples for the other tasks (e.g., named en-
tity recognition, POS tagging, and parsing) in NLP.
We believe we can use prior knowledge on these
tasks to create effective virtual examples.
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th COLT, pages 92?100.
Dennis DeCoste and Bernhard Scho?lkopf. 2002. Train-
ing invariant support vector machines. Machine
Learning, 46:161?190.
Number of Examples in the Training Set
Category 9603 4802 2401 1200 600 300 150
earn 98.06 97.49 97.40 96.39 95.94 94.85 93.73
acq 91.94 89.87 84.43 84.01 78.17 63.10 12.03
money-fx 64.90 61.69 56.03 51.69 17.91 01.11 05.38
grain 86.96 81.68 75.20 59.63 41.27 06.49 -
crude 84.59 81.52 67.11 33.33 01.05 - -
trade 74.89 64.58 54.86 40.26 12.80 01.69 -
interest 63.89 60.29 50.27 35.15 08.57 05.88 -
ship 66.19 44.07 32.73 02.22 - - -
wheat 89.61 80.60 38.30 08.11 - - -
corn 84.62 62.79 10.17 - - - -
Macro-average 80.56 72.46 56.65 - - - -
Micro-average 89.42 86.58 81.69 77.24 71.08 64.44 53.28
Table 4: F-Measures for the Reuters Categories with the Original SVM. The hyphen ?-? denotes the case
where F-measure cannot be computed because the classifier always says ?negative? and therefore its preci-
sion is undefined. The scores in bold means that the score of the original SVM is better than that of SVM
with 4 Virtual SVs per SV (shown in Table 5).
Number of Examples in the Training Set
Category 9603 4802 2401 1200 600 300 150
earn 98.07 98.02 97.56 97.37 97.14 96.00 95.46
acq 94.20 93.06 91.71 88.81 88.92 78.70 59.92
money-fx 70.83 73.10 62.86 65.68 47.91 32.43 33.76
grain 89.20 84.72 85.11 80.44 60.79 44.10 01.00
crude 84.93 86.33 76.92 74.36 15.53 02.00 -
trade 75.83 73.21 62.31 43.53 37.58 18.32 01.65
interest 62.73 63.16 65.77 63.35 59.11 37.50 11.92
ship 73.68 67.14 50.79 30.48 06.45 02.22 -
wheat 87.42 82.61 87.94 68.91 10.67 - -
corn 87.50 84.11 46.75 68.09 03.45 - -
Macro-average 82.44 80.55 72.77 68.10 42.76 - -
Micro-average 90.45 89.69 87.12 84.97 79.16 73.25 65.05
Table 5: F-Measures for the Reuters Categories with SVM with 4 Virtual SVs per SV. The scores in bold
means that the score of SVM with 4 Virtual SVs per SV is better than that of the original SVM (shown in
Table 4).
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algorithms
and representations for text categorization. In Pro-
ceedings of the ACM CIKM International Conference
on Information and Knowledge Management, pages
148?155.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning, pages 137?142.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the 16th International Conference on
Machine Learning, pages 200?209.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL-2002, pages 63?69.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the Seventeenth Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 3?12.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 1998. Learning to classify text from
labeled and unlabeled documents. In Proceedings of
the Fifteenth National Conference on Artificial Intelli-
gence (AAAI-98), pages 792?799.
Partha Niyogi, Federico Girosi, and Tomaso Poggio.
1998. Incorporating prior information in machine
learning by creating virtual examples. In Proceedings
of IEEE, volume 86, pages 2196?2207.
John C. Platt. 1999. Fast training of support vec-
tor machines using sequential minimal optimization.
In Bernhard Scho?lkopf, Christopher J.C. Burges, and
Alexander J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 185?208. MIT
Press.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proceedings of ACL-2002,
pages 505?512.
Bernhard Scho?lkopf, Chris Burges, and Vladimir Vap-
nik. 1996. Incorporating invariances in support vector
learning machines. In C. von der Malsburg, W. von
Seelen, J.C. Vorbru?ggen, and B. Sendhoff, editors, Ar-
tificial Neural Networks ? ICANN?96, Springer Lec-
ture Notes in Computer Science, Vol. 1112, pages 47?
52.
Cynthia A. Thompson, Mary Leaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of the Sixteenth International Conference on
Machine Learning, pages 406?414.
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, 2nd edition.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings of SIGIR-
99, 2nd ACM International Conference on Research
and Development in Information Retrieval, pages 42?
49.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. Journal of Informa-
tion Retrieval, 1(1/2):67?88.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL-1995, pages 189?196.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 269?278, Dublin, Ireland, August 23-29 2014.
Rapid Development of a Corpus with Discourse Annotations
using Two-stage Crowdsourcing
Daisuke Kawahara
??
Yuichiro Machida
?
Tomohide Shibata
??
Sadao Kurohashi
??
Hayato Kobayashi
?
Manabu Sassano
?
?
Graduate School of Informatics, Kyoto University
?
CREST, Japan Science and Technology Agency
?
Yahoo Japan Corporation
{dk, shibata, kuro}@i.kyoto-u.ac.jp, machida@nlp.ist.i.kyoto-u.ac.jp,
{hakobaya, msassano}@yahoo-corp.jp
Abstract
We present a novel approach for rapidly developing a corpus with discourse annotations using
crowdsourcing. Although discourse annotations typically require much time and cost owing to
their complex nature, we realize discourse annotations in an extremely short time while retaining
good quality of the annotations by crowdsourcing two annotation subtasks. In fact, our experi-
ment to create a corpus comprising 30,000 Japanese sentences took less than eight hours to run.
Based on this corpus, we also develop a supervised discourse parser and evaluate its performance
to verify the usefulness of the acquired corpus.
1 Introduction
Humans understand text not by individually interpreting clauses or sentences, but by linking such a text
fragment with another in a particular context. To allow computers to understand text, it is essential to
capture the precise relations between these text fragments. This kind of analysis is called discourse
parsing or discourse structure analysis, and is an important and fundamental task in natural language
processing (NLP). Systems for discourse parsing are, however, available only for major languages, such
as English, owing to the lack of corpora with discourse annotations.
For English, several corpora with discourse annotations have been developed manually, consuming a
great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al.,
2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson,
2005). Discourse parsers trained on these corpora have also been developed and practically used. To
create the same resource-rich environment for another language, a quicker method than the conventional
time-consuming framework should be sought. One possible approach is to use crowdsourcing, which
has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008;
Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource
the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of
spans with a certain relation and identifying the relation between the pair.
In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the proce-
dure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for
crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a
discourse unit for the span is a costly process, and thus we adopt a clause as the discourse unit, since this
is reliable enough to be automatically detected. We also limit the length of each target document to three
sentences and at most five clauses to facilitate the annotation task. Secondly, we detect and annotate
clause pairs in a document that hold logical discourse relations. However, since this is too complicated
to assign as one task using crowdsourcing, we divide the task into two steps: determining the existence
of logical discourse relations and annotating the type of relation. Our two-stage approach is a robust
method in that it confirms the existence of the discourse relations twice. We also designed the tagset
of discourse relations for crowdsourcing, which consists of two layers, where the upper layer contains
the following three classes: ?CONTINGENCY,? ?COMPARISON? and ?OTHER.? Although the task
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
269
settings are simplified for crowdsourcing, the obtained corpus and knowledge of discourse parsing could
be still useful in general discourse parsing.
In our experiments, we crowdsourced discourse annotations for Japanese, for which there are no pub-
licly available corpora with discourse annotations. The resulting corpus consists of 10,000 documents,
each of which comprises three sentences extracted from the web. Carrying out this two-stage crowd-
sourcing task took less than eight hours. The time elapsed was significantly shorter than the conventional
corpus building method.
We also developed a discourse parser by exploiting the acquired corpus with discourse annotations.
We learned a machine learning-based model for discourse parsing based on this corpus and evaluated its
performance. An F1 value of 37.9% was achieved for contingency relations, which would be roughly
comparable with state-of-the-art discourse parsers on English. This result indicates the usefulness of the
acquired corpus. The resulting discourse parser would be effectively exploited in NLP applications, such
as sentiment analysis (Zirn et al., 2011) and contradiction detection (Murakami et al., 2009; Ennals et
al., 2010).
The novel contributions of this study are summarized below:
? We propose a framework for developing a corpus with discourse annotations using two-stage crowd-
sourcing, which is both cheap and quick to execute, but still retains good quality of the annotations.
? We construct a Japanese discourse corpus in an extremely short time.
? We develop a discourse parser based on the acquired corpus.
The remainder of this paper is organized as follows. Section 2 introduces related work, while Section
3 describes our proposed framework and reports the experimental results for the creation of a corpus with
discourse annotations. Section 4 presents a method for discourse parsing based on the corpus as well as
some experimental results. Section 5 concludes the paper.
2 Related Work
Snow et al. (2008) applied crowdsourcing to five NLP annotation tasks, but the settings of these tasks
are very simple. There have also been several attempts to construct language resources with complex
annotations using crowdsourcing. Negri et al. (2011) proposed a method for developing a cross-lingual
textual entailment (CLTE) corpus using crowdsourcing. They tackled this complex data creation task by
dividing it into several simple subtasks: sentence modification, type annotation and sentence translation.
The creative CLTE task and subtasks are quite different from our non-creative task and subtasks of
discourse annotations. Fossati et al. (2013) proposed FrameNet annotations using crowdsourcing. Their
method is a single-step approach to only detect frame elements. They verified the usefulness of their
approach through an experiment on a small set of verbs with only two frame ambiguities per verb.
Although they seem to be running a larger-scale experiment, its result has not been revealed yet. Hong
and Baker (2011) presented a crowdsourcing method for selecting FrameNet frames, which is a part of
the FrameNet annotation process. Since their task is equivalent to word sense disambiguation, it is not
very complex compared to the whole FrameNet annotation process. These FrameNet annotations are
still different from discourse annotations, which are our target. To the best of our knowledge, there have
been no attempts to crowdsource discourse annotations.
There are several manually-crafted corpora with discourse annotation for English, such as the Penn
Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse
Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles.
Several attempts have been made to manually create corpora with discourse annotations for languages
other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (news-
paper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents;
1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres;
267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences com-
pared with the English corpora containing several tens of thousands sentences.
270
In recent years, there have been many studies on discourse parsing on the basis of the above hand-
annotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009;
Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty
et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing
can be attributed to the existence of corpora with discourse annotations. However, the target language is
mostly English since English is the only language that has large-scale discourse corpora. To develop and
improve discourse parsers for languages other than English, it is necessary to build large-scale annotated
corpora, especially in a short period if possible.
3 Development of Corpus with Discourse Annotations using Crowdsourcing
3.1 Corpus Specifications
We develop a tagged corpus in which pairs of discourse units are annotated with discourse relations.
To achieve this, it is necessary to determine target documents, discourse units, and a discourse relation
tagset. The following subsections explain the details of these three aspects.
3.1.1 Target Text and Discourse Unit
In previous studies on constructing discourse corpora, the target documents were mainly newspaper
texts, such as the Wall Street Journal for English. However, discourse parsers trained on such newspaper
corpora usually have a problem of domain adaptation. That is to say, while discourse parsers trained on
newspaper corpora are good at analyzing newspaper texts, they generally cannot perform well on texts
of other domains.
To address this problem, we set out to create an annotated corpus covering a variety of domains.
Since the web contains many documents across a variety of domains, we use the Diverse Document
Leads Corpus (Hangyo et al., 2012), which was extracted from the web. Each document in this corpus
consists of the first three sentences of a Japanese web page, making these short documents suitable for
our discourse annotation method based on crowdsourcing.
We adopt the clause as a discourse unit, since spans are too fine-grained to annotate using crowdsourc-
ing and sentences are too coarse-grained to capture discourse relations. Clauses, which are automatically
identified, do not need to be manually modified since they are thought to be reliable enough. Clause
identification is performed using the rules of Shibata and Kurohashi (2005). For example, the following
rules are used to identify clauses as our discourse units:
? clauses that function as a relatively strong boundary in a sentence are adopted,
? relative clauses are excluded.
Since workers involved in our crowdsourcing task need to judge whether clause pairs have discourse
relations, the load of these workers increases combinatorially as the number of clauses in a sentence
increases. To alleviate this problem, we limit the number of clauses in a document to five. This limitation
excludes only about 5% of the documents in the original corpus.
Our corpus consists of 10,000 documents corresponding to 30,000 sentences. The total number of
clauses in this corpus is 39,032, and thus the average number of clauses in a document is 3.9. The total
number of clause pairs is 59,426.
3.1.2 Discourse Relation Tagset
One of our supposed applications of discourse parsing is to automatically generate a bird?s eye view of a
controversial topic as in Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010),
which identify various relations between statements, including contradictory relations. We assume that
expansion relations, such as elaboration and restatement, and temporal relations are not important for this
purpose. This setting is similar to the work of Bethard et al. (2008), which annotated temporal relations
independently of causal relations. We also suppose that temporal relations can be annotated separately
for NLP applications that require temporal information. We determined the tagset of discourse relations
271
Upper type Lower type Example
CONTINGENCY
Cause/Reason ???????????????????
[since (I) pushed the button] [hot water was turned on]
Purpose ?????????????????????
[to pass the exam] [(I) studied a lot]
Condition ?????????????????
[if (you) push the button] [hot water will be turned on]
Ground ??????????????????????????
[here is his/her bag] [he/she would be still in the company]
COMPARISON
Contrast ?????????????????????????????
[at that restaurant, sushi is good] [ramen is so-so]
Concession ??????????????????????????
[that restaurant is surely good] [the price is high]
OTHER (Other) ???????????????????
[After being back home] [it began to rain]
Table 1: Discourse relation tagset with examples.
by referring to the Penn Discourse Treebank. This tagset consists of two layers, where the upper layer
contains three classes and the lower layer seven classes as follows:
? CONTINGENCY
? Cause/Reason (causal relations and not conditional relations)
? Purpose (purpose-action relations where the purpose is not necessarily accomplished)
? Condition (conditional relations)
? Ground (other contingency relations including pragmatic cause/condition)
? COMPARISON (same as the Penn Discourse Treebank)
? Contrast
? Concession
? OTHER (other weak relation or no relation)
Note that we do not consider the direction of relations to simplify the annotation task for crowdsourcing.
Table 1 shows examples of our tagset.
Therefore, our task is to annotate clause pairs in a document with one of the discourse relations given
above. Sample annotations of a document are shown below. Here, clause boundaries are shown by ?::?
and clause pairs that are not explicitly marked are allocated the ?OTHER? relation.
Cause/Reason ?????::??????????::????????????::???????
???????::??????????????
... [the surgery of my father ended safely] [(I) am relieved a little bit]
Contrast ???????????????????????::????????????
????????????????????????::???????????
??????::????????
... [There is tailwind to live,] [there is also headwind.]
3.2 Two-stage Crowdsourcing for Discourse Annotations
We create a corpus with discourse annotations using two-stage crowdsourcing. We divide the annotation
task into the following two subtasks: determining whether a clause pair has a discourse relation excluding
?OTHER,? and then, ascertaining the type of discourse relation for a clause pair that passes the first stage.
272
Probability Number
= 1.0 64
> 0.99 554
> 0.9 1,065
> 0.8 1,379
> 0.5 2,655
> 0.2 4,827
> 0.1 5,895
> 0.01 9,068
> 0.001 12,277
> 0.0001 15,554
Table 2: Number of clause pairs resulting from the judgments of discourse relation existence.
3.2.1 Stage 1: Judgment of Discourse Relation Existence
This subtask determines whether each clause pair in a document has one of the following discourse
relations: Cause/Reason, Purpose, Condition, Ground, Contrast, and Concession (that is, all the relations
except ?OTHER?). Workers are shown examples of these relations and asked to determine only the
existence thereof.
In this subtask, an item presented to a worker at a particular time consists of all the judgments of
clause pairs in a document. By adopting this approach, each worker considers the entire document when
making his/her judgments.
3.2.2 Stage 2: Judgment of Discourse Relation Type
This subtask involves ascertaining the discourse relation type for a clause pair that passes the first stage.
The result of this subtask is one of the seven lower types in our discourse relation tagset. Workers
are shown examples of these types and then asked to select one of the relations. If a worker chooses
?OTHER,? this corresponds to canceling the positive determination of the existence of the discourse
relation in stage one.
In this subtask, an item is the judgment of a clause pair. That is, if a document contains more than
one clause pair that must be judged, the judgments for this document are divided into multiple items,
although this is rare.
3.3 Experiment and Discussion
We conducted an experiment of the two-stage crowdsourcing approach using Yahoo! Crowdsourcing.
1
To increase the reliability of the produced corpus, we set the number of workers for each item for each
task to 10. The reason why we chose this value is as follows. While Snow et al. (2008) claimed that an
average of 4 non-expert labels per item in order to emulate expert-level label quality, the quality of some
tasks increased by increasing the number of workers to 10. We also tested hidden gold-standard items
once every 10 items to examine worker?s quality. If a worker failed these items in serial, he/she would
have to take a test to continue the task.
We obtained judgments for the 59,426 clause pairs in the 10,000 documents of our corpus in the
first stage of crowdsourcing, i.e., the subtask of determining the existence of discourse relations. We
calculated the probability of each label using GLAD
2
(Whitehill et al., 2009), which was proved to
be more reliable than the majority voting. This probability corresponds to the probability of discourse
relation existence of each clause pair. Table 2 lists the results. We set a probability threshold to select
those clause pairs whose types were to be judged in the second stage of crowdsourcing. With this
threshold set to 0.01, 9,068 clause pairs (15.3% of all the clause pairs) were selected. The threshold was
set fairly low to allow low-probability judgments to be re-examined in the second stage.
1
http://crowdsourcing.yahoo.co.jp/
2
http://mplab.ucsd.edu/?jake/OptimalLabelingRelease1.0.3.tar.gz
273
Lower type All prob > 0.8
Cause/Reason 2,104 1,839 (87.4%)
Purpose 755 584 (77.4%)
Condition 1,109 925 (83.4%)
Ground 442 273 (61.8%)
Contrast 437 354 (81.0%)
Concession 80 49 (61.3%)
Sum of the above discourse relations 4,927 4,024 (81.7%)
Other 4,141 3,753 (90.6%)
Total 9,068 7,777 (85.8%)
Table 3: Results of the judgments of lower discourse relation types.
Upper type All prob > 0.8
CONTINGENCY 4,439 3,993 (90.0%)
COMPARISON 516 417 (80.8%)
Sum of the above discourse relations 4,955 4,410 (89.0%)
OTHER 4,113 3,753 (91.2%)
Total 9,068 8,163 (90.0%)
Table 4: Results of the judgments of upper discourse relation types.
The discourse relation types of the 9,068 clause pairs were determined in the second stage of crowd-
sourcing. We extended GLAD (Whitehill et al., 2009) for application to multi-class tasks, and calculated
the probability of the labels of each clause pair. We assigned the label (discourse relation type) with the
highest probability to each clause pair. Table 3 gives some statistics of the results. The second column in
this table denotes the numbers of each discourse relation type, while the third column gives the numbers
of each type of clause pair with a probability higher than 0.80. Table 4 gives statistics of the results when
the lower discourse relation types are merged into the upper types. Table 5 shows some examples of the
resulting annotations.
Carrying out the two separate subtasks using crowdsourcing took approximately three hours and five
hours with 1,458 and 1,100 workers, respectively. If we conduct this task at a single stage, it would take
approximately 33 (5 hours / 0.153) hours. It would be four times longer than our two-stage approach.
Such single-stage approach is also not robust since it does not have a double check mechanism, with
which the two-stage approach is equipped. We spent 111 thousand yen and 113 thousand yen (approx-
imately 1,100 USD, respectively) for these subtasks, which would be extremely less expensive than the
projects of conventional discourse annotations.
For the examples in Table 5, we confirmed that the discourse relation types of the top four examples
were surely correct. However, we judged the type (Contrast) of the bottom example as incorrect. Since
the second clause is an instantiation of the first clause, the correct type should be ?Other.? We found such
errors especially in the clause pairs with a probability lower than 0.80.
4 Development of Discourse Parser based on Acquired Discourse Corpus
To verify the usefulness of the acquired corpus with discourse annotations, we developed a supervised
discourse parser based on the corpus, and evaluated its performance. We built two discourse parsers using
the annotations of the lower and upper discourse relation types, respectively. From the annotations in the
first stage of crowdsourcing (i.e., judging the existence of discourse relations), we assigned annotations
with a probability less than 0.01 as ?OTHER.? Of the annotations acquired in the second stage (i.e.,
judging discourse relation types), we adopted those with a probability greater than 0.80 and discarded
the rest. After this preprocessing, we obtained 58,135 (50,358 + 7,777) instances of clause pairs for
the lower-type discourse parser and 58,521 (50,358 + 8,163) instances of clause pairs for the upper-type
274
Prob # W Type Document
1.00 6/10 Cause/Reason ???????????????????????????????
??????????????????????????????
????
... [Since the flower blooms in the fifth lunar month] [it is called ?Sat-
suki.?] ...
0.99 4/10 Condition ??????????????????????????????
???????????????????????????????
?????????????????????????????
??????
[If you click the balloon on the map] [you can see the recommended
route] ...
0.81 3/10 Purpose ?????????????????????????????
?????????????????????????????
??????????????????????????????
?????????????????????????????
... [And seeking ?Great harvest?] [each country is engaged in a war]
0.61 2/10 Cause/Reason ??????????????????????????????
?????????????????????????????
??????????????????????????????
??????????????????????????????
?????
... [by transmitting power to the front and rear axle with the combina-
tion of gears and shafts] [(it) drives the four wheels.]
0.54 3/10 Contrast ?????????????????????????????
??????????????????????????????
???????????????????????????
... [a scramble for customers by department stores would be severe.]
[What comes out is the possibility of the closure of Fukuoka Mit-
sukoshi.]
Table 5: Examples of Annotations. The first column denotes the estimated label probability and the
second column denotes the number of workers that assigned the designated type. In the fourth column,
the clause pair annotated with the type is marked with?? ([ ] in English translations).
discourse parser. Of these, 4,024 (6.9%) and 4,410 (7.5%) instances, respectively, had one of the types
besides ?OTHER.? We conducted experiments using five-fold cross validation on these instances.
To extract features of machine learning, we applied the Japanese morphological analyzer, JUMAN,
3
and the Japanese dependency parser, KNP,
4
to the corpus. We used the features listed in Table 6, which
are usually used for discourse parsing.
We adopted Opal (Yoshinaga and Kitsuregawa, 2010)
5
for the machine learning implementation. This
tool enables online learning using a polynomial kernel. As parameters for Opal, we used the passive-
aggressive algorithm (PA-I) with a polynomial kernel of degree two as a learner and the extension to
multi-class classification (Matsushima et al., 2010). The numbers of classes were seven and three for the
lower- and upper-type discourse parsers, respectively. We set the aggressiveness parameter C to 0.001,
which generally achieves good performance for many classification tasks. Other parameters were set to
the default values of Opal.
To measure the performance of the discourse parsers, we adopted precision, recall and their harmonic
mean (F1). These metrics were calculated as the proportion of the number of correct clause pairs to the
3
http://nlp.ist.i.kyoto-u.ac.jp/EN/?JUMAN
4
http://nlp.ist.i.kyoto-u.ac.jp/EN/?KNP
5
http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
275
Name Description
clause distance clause distance between two clauses
sentence distance sentence distance between two clauses
bag of words bag of words (lemmas) for each clause
predicate a content word (lemma) of the predicate of each clause
conjugation form of predicate a conjugation form of the predicate of each clause
conjunction a conjunction if it is located at the beginning of a clause
word overlapping ratio an overlapping ratio of words between the two clauses
clause type a lexical type output by KNP for each clause (about 100 types)
topic marker existence existence of a topic marker in each clause
topic marker cooccurrence existence of a topic marker in both clauses
Table 6: Features for our discourse parsers.
Type Precision Recall F1
Cause/Reason 0.623 (441/708) 0.240 (441/1,839) 0.346
Purpose 0.489 (44/90) 0.075 (44/584) 0.131
Condition 0.581 (256/441) 0.277 (256/925) 0.375
Ground 0.000 (0/12) 0.000 (0/273) 0.000
Contrast 0.857 (6/7) 0.017 (6/354) 0.033
Concession 0.000 (0/0) 0.000 (0/49) 0.000
Other 0.944 (53,702/56,877) 0.992 (53,702/54,111) 0.968
Table 7: Performance of our lower-type discourse parser.
Type Precision Recall F1
CONTINGENCY 0.625 (1,084/1,735) 0.272 (1,084/3,993) 0.379
COMPARISON 0.412 (7/17) 0.017 (7/417) 0.032
OTHER 0.942 (53,454/56,769) 0.988 (53,454/54,111) 0.964
Table 8: Performance of our upper-type discourse parser.
number of all recognized or gold-standard ones for each discourse relation type. Tables 7 and 8 give the
accuracies for the lower- and upper-type discourse parsers, respectively.
From Table 8, we can see that our upper-type discourse parser achieved an F1 of 37.9% for contingency
relations. It is difficult to compare our results with those in previous work due to the use of different data
set and different languages. We, however, anticipate that our results would be comparable with those
of state-of-the-art English discourse parsers. For example, the end-to-end discourse parser of Lin et al.
(2012) achieved an F1 of 20.6% ? 46.8% on the Penn Discourse Treebank.
We also obtained a low F1 for comparison relations. This tendency is similar to the previous results
on the Penn Discourse Treebank. The biggest cause of this low F1 is the lack of unambiguous explicit
discourse connectives for these relations. Although there are explicit discourse connectives in Japanese,
many of them have multiple meanings and cannot be used as a direct clue for discourse relation detection
(e.g., as described in Kaneko and Bekki (2014)). As reported in Pitler et al. (2009) and other studies,
the identification of implicit discourse relations are notoriously difficult. To improve its performance, we
need to incorporate external knowledge sources other than the training data into the discourse parsers.
A promising way is to use large-scale knowledge resources that are automatically acquired from raw
corpora.
276
5 Conclusion
We presented a rapid approach for building a corpus with discourse annotations and a discourse parser
using two-stage crowdsourcing. The acquired corpus is made publicly available and can be used for
research purposes.
6
This corpus can be used not only to build a discourse parser but also to evaluate
its performance. The availability of the corpus with discourse annotations will accelerate the develop-
ment and improvement of discourse parsing. In the future, we intend integrating automatically acquired
knowledge from corpora into the discourse parsers to further enhance their performance. We also aim to
apply our framework to other languages without available corpora with discourse annotations.
References
Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a corpus of temporal-
causal structure. In Proceedings of the 6th International Conference on Language Resources and Evaluation,
pages 908?915.
Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disam-
biguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers), pages 69?73.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and
Dialogue.
Iria da Cunha, Juan-Manuel Torres-Moreno, and Gerardo Sierra. 2011. On the development of the RST Spanish
treebank. In Proceedings of the 5th Linguistic Annotation Workshop (LAW V), pages 1?10.
Rob Ennals, Beth Trushkowsky, and John Mark Agosta. 2010. Highlighting disputed claims on the web. In
Proceedings of the 19th international conference on World Wide Web, pages 341?350.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 60?68. Association for Computational Linguistics.
Marco Fossati, Claudio Giuliano, and Sara Tonelli. 2013. Outsourcing FrameNet to the crowd. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and Richard Johansson. 2011. End-to-end discourse parser
evaluation. In Fifth IEEE International Conference on Semantic Computing (ICSC), pages 169?172.
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. 2012. Building a diverse document leads corpus
annotated with semantic relations. In Proceedings of 26th Pacific Asia Conference on Language Information
and Computing, pages 535?544.
Hugo Hernault, Helmut Prendinger, David duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser
using support vector machine classification. Dialogue & Discourse, 1(3):1?33.
Jisup Hong and Collin F. Baker. 2011. How good is the crowd at ?real? WSD? In Proceedings of the 5th Linguistic
Annotation Workshop, pages 30?37.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 904?915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining intra- and multi-sentential
rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, pages 486?496.
Kimi Kaneko and Daisuke Bekki. 2014. Building a Japanese corpus of temporal-causal-discourse structures
based on SDRT for extracting causal relations. In Proceedings of the EACL 2014 Workshop on Computational
Approaches to Causality in Language (CAtoCL), pages 33?39.
6
http://nlp.ist.i.kyoto-u.ac.jp/EN/?DDLC
277
Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging synthetic discourse data via multi-task learning for implicit
discourse relation recognition. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 476?485.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, pages 1?34.
Shin Matsushima, Nobuyuki Shimizu, Kazuhiro Yoshida, Takashi Ninomiya, and Hiroshi Nakagawa. 2010. Exact
passive-aggressive algorithm for multiclass classification using support class. In Proceedings of 2010 SIAM
International Conference on Data Mining (SDM2010), pages 303?314.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information credibility analysis by visualizing arguments. In Pro-
ceedings of the 3rd Workshop on Information Credibility on the Web, pages 43?50.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide
and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora. In Proceedings of the
2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679.
Thiago Alexandre Salgueiro Pardo, Maria das Grac?as Volpe Nunes, and Lucia Helena Machado Rino. 2004.
Dizer: An automatic discourse analyzer for Brazilian Portuguese. In Advances in Artificial Intelligence?SBIA
2004, pages 224?234. Springer.
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13?16.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in
text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn discourse treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation, pages 2961?2968.
Tomohide Shibata and Sadao Kurohashi. 2005. Automatic slide generation based on discourse structure analysis.
In Proceedings of Second International Joint Conference on Natural Language Processing, pages 754?766.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and AndrewNg. 2008. Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 254?263.
Manfred Stede. 2004. The Potsdam commentary corpus. In Proceedings of the 2004 ACL Workshop on Discourse
Annotation, pages 96?102.
Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 566?574.
Jacob Whitehill, Paul Ruvolo, Ting fan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should
count more: Optimal integration of labels from labelers of unknown expertise. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22,
pages 2035?2043.
FlorianWolf and Edward Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational
Linguistics, 31(2):249?287.
Naoki Yoshinaga and Masaru Kitsuregawa. 2010. Kernel slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING2010), pages
1245?1253.
C?acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt, and Michael Strube. 2011. Fine-grained sentiment analysis
with structural features. In Proceedings of 5th International Joint Conference on Natural Language Processing,
pages 336?344.
278
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 79?83,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Deterministic Word Segmentation
Using Maximum Matching with Fully Lexicalized Rules
Manabu Sassano
Yahoo Japan Corporation
Midtown Tower, 9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
msassano@yahoo-corp.jp
Abstract
We present a fast algorithm of word seg-
mentation that scans an input sentence
in a deterministic manner just one time.
The algorithm is based on simple max-
imum matching which includes execu-
tion of fully lexicalized transformational
rules. Since the process of rule match-
ing is incorporated into dictionary lookup,
fast segmentation is achieved. We eval-
uated the proposed method on word seg-
mentation of Japanese. Experimental re-
sults show that our segmenter runs consid-
erably faster than the state-of-the-art sys-
tems and yields a practical accuracy when
a more accurate segmenter or an annotated
corpus is available.
1 Introduction
The aim of this study is to improve the speed
of word segmentation. Applications for many
Asian languages including Chinese and Japanese
require word segmentation. Such languages do not
have explicit word delimiters such as white spaces.
Word segmentation is often needed before every
task of fundamental text processing such as count-
ing words, searching for words, indexing docu-
ments, and extracting words. Therefore, the per-
formance of word segmentation is crucial for these
languages. Take for instance, information retrieval
(IR) systems for documents in Japanese. It typi-
cally uses a morphological analyzer1 to tokenize
the content of the documents. One of the most
time consuming tasks in IR systems is indexing,
which uses morphological analysis intensively.
Major approaches to Japanese morphological
analysis (MA) are based on methods of finding
1 Japanese has a conjugation system in morphology and
does not put white spaces between words. Therefore, we
have to do morphological analysis in order to segment a given
sentence into words and give an associated part-of-speech
(POS) tag to each word. In the main stream of the research
of Japanese language processing, morphological analysis has
meant to be a joint task of segmentation and POS tagging.
the best sequence of words along with their part-
of-speech tags using a dictionary where they use
the Viterbi search (e.g., (Nagata, 1994), (Kudo et
al., 2004)). However, computation cost of mod-
ern MA systems is mainly attributed to the Viterbi
search as Kaji et al. (2010) point out.
One of methods of improving the speed of MA
or word segmentation will be to avoid or reduce
the Viterbi search. We can avoid this by using
maximum matching in the case of word segmenta-
tion. Since there are many applications such as IR
and text classification, where part-of-speech tags
are not mandatory, in this paper we focus on word
segmentation and adopt maximum matching for it.
However, maximum matching for Japanese word
segmentation is rarely used these days because the
segmentation accuracy is not good enough and the
accuracy of MA is much higher. In this paper we
investigate to improve the accuracy of maximum-
matching based word segmentation while keeping
speedy processing.
2 Segmentation Algorithm
Our algorithm is basically based on maximum
matching, or longest matching (Nagata, 1997). Al-
though maximum matching is very simple and
easy to implement, a segmenter with this algo-
rithm is not sufficiently accurate. For the pur-
pose of improving the segmentation accuracy, sev-
eral methods that can be combined with maximum
matching have been examined. In previous studies
(Palmer, 1997; Hockenmaier and Brew, 1998), the
combination of maximum matching and character-
based transformational rules has been investigated
for Chinese. They have reported promising results
in terms of accuracy and have not mentioned the
running time of their methods, which might sup-
posedly be very slow because we have to scan an
input sentence many times to apply learned trans-
formational rules.
In order to avoid such heavy post processing,
we simplify the type of rules and incorporate the
process of applying rules into a single process of
maximum matching for dictionary lookup. We
79
Input: c
i
: sentence which is represented as a char-
acter sequence. N : the number of characters in a
given sentence. t: dictionary, the data structure of
which should be a trie. o
j
: map an ID j to a single
word or a sequence of words. h
j
: total length of
o
j
.
Function: Lookup(t, c, i, N ): search the dic-
tionary t for the substring c starting at the position
i up to N by using maximum matching. This re-
turns the ID of the entry in the dictionary t when it
matches and otherwise returns ?1.
procedure Segment(c, N , t)
var i: index of the character sequence c
var j: ID of the entry in the trie dictionary t
begin
i ? 1
while (i ? N ) do begin
j = Lookup(t, c, i, N )
if (j = ?1) then
{ unknown word as a single character }
print c
i
; i = i+ 1
else
print o
j
; i = i+ h
j
{ if o
j
is a sequence of words, print each
word in the sequence with a delimiter.
Otherwise print o
j
as a single token. }
endif
print delimiter
{ delimiter will be a space or something. }
end
end
Figure 1: Algorithm of word segmentation with
maximum matching that incorporates execution of
transformational rules.
show in Figure 1 the pseudo code of the algorithm
of word segmentation using maximum matching,
where the combination of maximum matching and
execution of simplified transformational rules is
realized. If each of the data o
j
in Figure 1 is a sin-
gle token, the algorithm which is presented here is
identical with segmentation by maximum match-
ing.
We use the following types of transformational
rules: c
0
c
1
...c
l?1
c
l
? w
0
...w
m
where c
i
is a char-
acter and w
j
is a word (or morpheme). Below are
sample rules for Japanese word segmentation:
? ???? (ha-na-i-ka) ? ? (ha; topic-
marker)?? (na-i; ?does not exist?)? (ka;
?or?) 2
? ???? (dai-ko-gaku-bu) ? ? (dai; ?uni-
versity?)??? (ko-gaku-bu; ?the faculty of
engineering?)
Note that the form of the left hand side of the rule
is the sequence of characters, not the sequence of
words. Due to this simplification, we can combine
dictionary lookup by maximum matching with ex-
ecution of transformational rules and make them
into a single process. In other words, if we find
a sequence of characters of the left hand side of a
certain rule, then we write out the right hand side
of the rule immediately. This construction enables
us to naturally incorporate execution (or appli-
cation) of transformational rules into dictionary-
lookup, i.e., maximum matching.
Although the algorithm in Figure 1 does not
specify the algorithm or the implementation of
function Lookup(), a trie is suitable for the struc-
ture of the dictionary. It is known that an effi-
cient implementation of a trie is realized by using
a double-array structure (Aoe, 1989), which en-
ables us to look up a given key at the O(n) cost,
where n is the length of the key. In this case the
computation cost of the algorithm of Figure 1 is
O(n).
We can see in Figure 1 that the Viterbi search
is not executed and the average number of dictio-
nary lookups is fewer than the number of char-
acters of an input sentence because the average
length of words is longer than one. This contrasts
with Viterbi-based algorithms of word segmenta-
tion or morphological analysis that always require
dictionary lookup at each character position in a
sentence.
3 Learning Transformational Rules
3.1 Framework of Learning
The algorithm in Figure 1 can be combined with
rules learned from a reference corpus as well as
hand-crafted rules. We used here a modified ver-
sion of Brill?s error-driven transformation-based
learning (TBL) (Brill, 1995) for rule learning.
In our system, an initial system is a word seg-
menter that uses maximum matching with a given
2 If we use simple maximum matching, i.e., with no trans-
formational rules, to segment the samples here, we will get
wrong segmentations as follows: ???? ? ?? (ha-na;
?flower?) ?? (i-ka; ?squid?), ???? ? ?? (dai-ku;
?carpenter?)?? (gaku-bu; ?faculty?).
80
w?
0
w
?
1
? ? ?w
?
n
? w
0
w
1
? ? ?w
m
Lw
?
0
w
?
1
? ? ?w
?
n
? Lw
0
w
1
? ? ?w
m
w
?
0
w
?
1
? ? ?w
?
n
R ? w
0
w
1
? ? ?w
m
R
Lw
?
0
w
?
1
? ? ?w
?
n
R ? Lw
0
w
1
? ? ?w
m
R
Table 1: Rule templates for error-driven learning
word list and words which occur in a given ref-
erence corpus (a training corpus). Our segmenter
treats an unknown word, which is not in the dictio-
nary, as a one-character word as shown in Figure 1.
3.2 Generating Candidate Rules
In order to generate candidate rules, first we
compare the output of the current system with
the reference corpus and extract the differences
(Tashiro et al., 1994) as rules that have the follow-
ing form: Lw?
0
w
?
1
? ? ?w
?
n
R ? Lw
0
w
1
? ? ?w
m
R
where w?
0
w
?
1
? ? ?w
?
n
is a word sequence in the sys-
tem output and w
0
w
1
? ? ?w
m
is a word sequence
in the reference corpus and L is a word in the left
context and R is a word in the right context. After
this extraction process, we generate four lexical-
ized rules from each extracted rule by using the
templates defined in Table 1.
3.3 Learning Rules
In order to reduce huge computation when learn-
ing a rule at each iteration of TBL, we use some
heuristic strategy. The heuristic score h is defined
as: h = f ? (n + m) where f is a frequency of
the rule in question and n is the number of words
in w?
0
w
?
1
? ? ?w
?
n
and m is the number of words in
w
0
w
1
? ? ?w
m
. After sorting the generated rules as-
sociated with the score h, we apply each candidate
rule in decreasing order of h and compute the error
reduction. If we get positive reduction, we obtain
this rule and incorporate it into the current dictio-
nary and then proceed to the next iteration. If we
do not find any rules that reduce errors, we termi-
nate the learning process.
4 Experiments and Discussion
4.1 Corpora and an Initial Word List
In our experiments for Japanese we used the Kyoto
University Text Corpus Version 4 (we call it KC4)
(Kurohashi and Nagao, 2003), which includes
newspaper articles, and 470M Japanese sentences
(Kawahara and Kurohashi, 2006), which is com-
piled from the Web. For training, we used two
sets of the corpus. The first set is the articles on
January 1st through 8th (7,635 sentences) of KC4.
The second one is 320,000 sentences that are se-
lected from the 470M Web corpus. Note that the
Web corpus is not annotated and we use it after
word segmentation is given by JUMAN 6.0 (Kuro-
hashi and Kawahara, 2007). The test data is a set
of sentences in the articles on January 9th (1,220
sentences). The articles on January 10th were used
for development.
We used all the words in the dictionary of JU-
MAN 6.0 as an initial word list. The number of
the words in the dictionary is 542,061. They are
generated by removing the grammatical informa-
tion such as part-of-speech tags from the entries in
the original dictionary of JUMAN 6.0.
4.2 Results and Discussion
Segmentation Performance We used word
based F-measure and character-wise accuracy to
evaluate the segmentation performance.
Table 2 shows comparison of various systems
including ours. It is natural that since our sys-
tem uses only fully lexicalized rules and does not
use any generalized rules, it achieves a moderate
performance. However, by using the Web cor-
pus that contains 320,000 sentences, it yields an
F-measure of near 0.96, which is at the same level
as the F-measure of HMMs (baseline) in (Kudo et
al., 2004, Table 3). We will discuss how we can
improve it in a later section.
Segmentation Speed Table 3 shows comparison
of the segmentation speed of various systems for
320,000 sentences of the Web corpus. Since, in
general, such comparison is heavily dependent on
the implementation of the systems, we have to be
careful for drawing any conclusion. However, we
can see that our system, which does not use the
Viterbi search, achieved considerably higher pro-
cessing speed than other systems.
Further Improvement The method that we
have presented so far is based on lexicalized rules.
That is, we do not have any generalized rules. The
system does not recognize an unknown English
word as a single token because most of such words
are not in the dictionary and then are split into sin-
gle letters. Similarly, a number that does not ap-
pear in the training corpus is split into digits.
It is possible to improve the presented method
by incorporating relatively simple post-processing
that concatenates Arabic numerals, numerals in
81
System # of Sent. F-measure Char. Acc. # of Rules
JUMAN 6.0 NA 0.9821 0.9920 NA
MeCab 0.98 w/ jumandic 7,958 0.9861 0.9939 NA
Ours w/o training corpus 0 0.8474 0.9123 0
Ours w/ KC4 7,635 0.9470 0.9693 2228
w/ Web320K 320,000 0.9555 0.9769 24267
Table 2: Performance summary of various systems and configurations. Jumandic for MeCab (Kudo et
al., 2004) is stemmed from the dictionary of JUMAN.
System (Charset Encoding) Model/Algorithm Time (sec.)
JUMAN 6.0 (EUC-JP) Markov model w/ hand-tuned costs 161.09
MeCab 0.98 (UTF-8) w/ jumandic CRFs 13.71
KyTea 0.3.3 (UTF-8) w/ jumandic Pointwise prediction w/ SVM 188.01
Ours (UTF-8) Maximum matching w/ rules 3.22
Table 3: Running time on the Web320K corpus. We used a PC (Intel Xeon 2.33 GHz with 8GB memory
on FreeBSD 6.3). The model for segmentation of KyTea (Neubig et al., 2011) in our experiments is
trained with the word list of JUMAN on KC4 (see in Section 4.1).
System F-measure
JUMAN 6.0 0.9821
MeCab 0.98 w/ jumandic 0.9861
KyTea 0.3.3 w/ jumandic 0.9789
MEMMs (Uchimoto et al., 2001) 0.9644
HMMs (Kudo et al., 2004, Table 3) 0.9622
Ours w/ KC4 0.9470
Ours w/ KC4 + post-proc. 0.9680
Ours w/ Web320K 0.9555
Ours w/ Web320K + post-proc. 0.9719
Table 4: Performance comparison to other sys-
tems.
kanji3, Latin characters, and katakana4 ones. This
type of post processing is commonly used in
Japanese morphological analysis. JUMAN and
MeCab have a similar mechanism and use it.
As an additional experiment, we incorporated
this post processing into our segmenter and mea-
sured the performance. The result is shown in Ta-
ble 4. The segmenter with the post processing
yields an F-measure of 0.9719 when it is trained
on the 320k Web corpus. We observed that the
performance gap between state-of-the-art systems
such as JUMAN and MeCab and ours becomes
smaller. Additional computation time was +10%
3 Kanji in Japanese, or hanzi in Chinese, is a ideographic
script. Kanji means Chinese characters.
4 Katakana is one of the phonetic scripts used in Japanese.
It is mainly used to denote loan words and onomatopoeias.
Such type of words are very productive and are often un-
known words in Japanese language processing.
for the post processing and this means the seg-
menter with the post processing is still much faster
than other sophisticated MA systems. Many ap-
plications which have to process a huge amount of
documents would gain the benefits from our pro-
posed methods.
5 Related Work
The use of transformational rules for improving
word segmentation as well as morphological anal-
ysis is not new. It is found in previous work (Papa-
georgiou, 1994; Palmer, 1997; Hockenmaier and
Brew, 1998; Gao et al., 2004). However, their ap-
proaches require the Viterbi search and/or a heavy
post process such as cascaded transformation in
order to rewrite the output of the base segmenter.
This leads to slow execution and systems that in-
corporate such approaches have much higher cost
of computation than ours.
6 Conclusion
We have proposed a new combination of maxi-
mum matching and fully lexicalized transforma-
tional rules. The proposed method allows us to
carry out considerably faster word segmentation
with a practically reasonable accuracy. We have
evaluated the effectiveness of our method on cor-
pora in Japanese. The experimental results show
that we can combine our methods with either
an existing morphological analyzer or a human-
edited training corpus.
82
References
Jun-Ichi Aoe. 1989. An efficient digital search al-
gorithm by using a double-array structure. IEEE
Transactions on Software Engineering, 15(9):1066?
1077.
Eric Brill. 1995. Transformation-based error driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Jianfeng Gao, Andi Wu, Cheng-Ning Huang,
Hong qiao Li, Xinsong Xia, and Hauwei Qin.
2004. Adaptive Chinese word segmentation. In
Proc. of ACL-2004, pages 462?469.
Julia Hockenmaier and Chris Brew. 1998. Error-driven
learning of Chinese word segmentation. In Proc. of
PACLIC 12, pages 218?229.
Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga,
and Masaru Kitsuregawa. 2010. Efficient staggered
decoding for sequence labeling. In Proc. of ACL
2010, pages 485?494.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC 2006,
pages 1344?1347.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
Japanese morphological analysis. In Proc. of
EMNLP 2004, pages 230?237.
Sadao Kurohashi and Daisuke Kawahara. 2007.
JUMAN (a User-Extensible Morphological An-
alyzer for Japanese). http://nlp.ist.
i.kyoto-u.ac.jp/index.php?JUMAN,
http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?JUMAN.
Sadao Kurohashi and Makoto Nagao. 2003. Build-
ing a Japanese parsed corpus. In Anne Abeille, edi-
tor, Treebanks: Building and Using Parsed Corpora,
pages 249?260. Kluwer Academic Publishers.
Masaaki Nagata. 1994. A stochastic Japanese morpho-
logical analyzer using a forward-DP backward-A*
n-best search algorithm. In Proc. of COLING-94,
pages 201?207.
Masaaki Nagata. 1997. A self-organizing Japanese
word segmenter using heuristic word identification
and re-estimation. In Proc. of WVLC-5, pages 203?
215.
Graham Neubig, Yosuke Nagata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. of ACL-
2011.
David D. Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proc. of ACL-1997,
pages 321?328.
Constantine P. Papageorgiou. 1994. Japanese word
segmentation by hidden Markov model. In Proc. of
HLT-1994, pages 283?288.
Toshihisa Tashiro, Noriyoshi Uratani, and Tsuyoshi
Morimoto. 1994. Restructuring tagged corpora with
morpheme adjustment rules. In Proc. of COLING-
1994, pages 569?573.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a morpho-
logical analysis of Japanese using maximum entropy
aided by a dictionary. In Proc. of EMNLP 2001,
pages 91?99.
83
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 356?365,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Smaller Constituents Rather Than Sentences
in Active Learning for Japanese Dependency Parsing
Manabu Sassano
Yahoo Japan Corporation
Midtown Tower,
9-7-1 Akasaka, Minato-ku,
Tokyo 107-6211, Japan
msassano@yahoo-corp.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
We investigate active learning methods for
Japanese dependency parsing. We propose
active learning methods of using partial
dependency relations in a given sentence
for parsing and evaluate their effective-
ness empirically. Furthermore, we utilize
syntactic constraints of Japanese to ob-
tain more labeled examples from precious
labeled ones that annotators give. Ex-
perimental results show that our proposed
methods improve considerably the learn-
ing curve of Japanese dependency parsing.
In order to achieve an accuracy of over
88.3%, one of our methods requires only
34.4% of labeled examples as compared to
passive learning.
1 Introduction
Reducing annotation cost is very important be-
cause supervised learning approaches, which have
been successful in natural language processing, re-
quire typically a large number of labeled exam-
ples. Preparing many labeled examples is time
consuming and labor intensive.
One of most promising approaches to this is-
sue is active learning. Recently much attention has
been paid to it in the field of natural language pro-
cessing. Various tasks have been targeted in the
research on active learning. They include word
sense disambiguation, e.g., (Zhu and Hovy, 2007),
POS tagging (Ringger et al, 2007), named entity
recognition (Laws and Schu?tze, 2008), word seg-
mentation, e.g., (Sassano, 2002), and parsing, e.g.,
(Tang et al, 2002; Hwa, 2004).
It is the main purpose of this study to propose
methods of improving active learning for parsing
by using a smaller constituent than a sentence as
a unit that is selected at each iteration of active
learning. Typically in active learning for parsing a
sentence has been considered to be a basic unit for
selection. Small constituents such as chunks have
not been used in sample selection for parsing. We
use Japanese dependency parsing as a target task
in this study since a simple and efficient algorithm
of parsing is proposed and, to our knowledge, ac-
tive learning for Japanese dependency parsing has
never been studied.
The remainder of the paper is organized as fol-
lows. Section 2 describes the basic framework of
active learning which is employed in this research.
Section 3 describes the syntactic characteristics of
Japanese and the parsing algorithm that we use.
Section 4 briefly reviews previous work on active
learning for parsing and discusses several research
challenges. In Section 5 we describe our proposed
methods and others of active learning for Japanese
dependency parsing. Section 6 describes experi-
mental evaluation and discussion. Finally, in Sec-
tion 7 we conclude this paper and point out some
future directions.
2 Active Learning
2.1 Pool-based Active Learning
Our base framework of active learning is based on
the algorithm of (Lewis and Gale, 1994), which is
called pool-based active learning. Following their
sequential sampling algorithm, we show in Fig-
ure 1 the basic flow of pool-based active learning.
Various methods for selecting informative exam-
ples can be combined with this framework.
2.2 Selection Algorithm for Large Margin
Classifiers
One of the most accurate approaches to classifica-
tion tasks is an approach with large margin classi-
fiers. Suppose that we are given data points {xi}
such that the associated label yi will be either ?1
or 1, and we have a hyperplane of some large mar-
gin classifier defined by {x : f(x) = 0} where the
356
1. Build an initial classifier from an initial la-
beled training set.
2. While resources for labeling examples are
available
(a) Apply the current classifier to each un-
labeled example
(b) Find the m examples which are most in-
formative for the classifier
(c) Have annotators label the m examples
(d) Train a new classifier on all labeled ex-
amples
Figure 1: Flow of the pool-based active learning
Lisa-ga kare-ni ano pen-wo age-ta.
Lisa-subj to him that pen-acc give-past.
ID 0 1 2 3 4
Head 4 4 3 4 -
Figure 2: Sample sentence. An English translation
is ?Lisa gave that pen to him.?
classification function is G(x) = sign{f(x)}. In
pool-based active learning with large margin clas-
sifiers, selection of examples can be done as fol-
lows:
1. Compute f(xi) over all unlabeled examples
xi in the pool.
2. Sort xi with |f(xi)| in ascending order.
3. Select top m examples.
This type of selection methods with SVMs is dis-
cussed in (Tong and Koller, 2000; Schohn and
Cohn, 2000). They obtain excellent results on text
classification. These selection methods are simple
but very effective.
3 Japanese Parsing
3.1 Syntactic Units
A basic syntactic unit used in Japanese parsing is
a bunsetsu, the concept of which was initially in-
troduced by Hashimoto (1934). We assume that
in Japanese we have a sequence of bunsetsus be-
fore parsing a sentence. A bunsetsu contains one
or more content words and zero or more function
words.
A sample sentence in Japanese is shown in Fig-
ure 2. This sentence consists of five bunsetsus:
Lisa-ga, kare-ni, ano, pen-wo, and age-ta where
ga, ni, and wo are postpositions and ta is a verb
ending for past tense.
3.2 Constraints of Japanese Dependency
Analysis
Japanese is a head final language and in written
Japanese we usually hypothesize the following:
? Each bunsetsu has only one head except the
rightmost one.
? Dependency links between bunsetsus go
from left to right.
? Dependencies do not cross one another.
We can see that these constraints are satisfied in
the sample sentence in Figure 2. In this paper we
also assume that the above constraints hold true
when we discuss algorithms of Japanese parsing
and active learning for it.
3.3 Algorithm of Japanese Dependency
Parsing
We use Sassano?s algorithm (Sassano, 2004) for
Japanese dependency parsing. The reason for this
is that it is very accurate and efficient1. Further-
more, it is easy to implement. His algorithm is
one of the simplest form of shift-reduce parsers
and runs in linear-time.2 Since Japanese is a head
final language and its dependencies are projective
as described in Section 3.2, that simplification can
be made.
The basic flow of Sassano?s algorithm is shown
in Figure 3, which is slightly simplified from the
original by Sassano (2004). When we use this al-
gorithm with a machine learning-based classifier,
function Dep() in Figure 3 uses the classifier to
decide whether two bunsetsus have a dependency
relation. In order to prepare training examples for
the trainable classifier used with his algorithm, we
first have to convert a treebank to suitable labeled
instances by using the algorithm in Figure 4. Note
1Iwatate et al (2008) compare their proposed algorithm
with various ones that include Sassano?s, cascaded chunk-
ing (Kudo and Matsumoto, 2002), and one in (McDonald et
al., 2005). Kudo and Matsumoto (2002) compare cascaded
chunking with the CYK method (Kudo and Matsumoto,
2000). After considering these results, we have concluded
so far that Sassano?s is a reasonable choice for our purpose.
2Roughly speaking, Sassano?s is considered to be a sim-
plified version, which is modified for head final languages, of
Nivre?s (Nivre, 2003). Classifiers with Nivre?s are required
to handle multiclass prediction, while binary classifiers can
work with Sassano?s for Japanese.
357
Input: wi: bunsetsus in a given sentence.
N : the number of bunsetsus.
Output: hj : the head IDs of bunsetsus wj .
Functions: Push(i, s): pushes i on the stack s.
Pop(s): pops a value off the stack s.
Dep(j, i, w): returns true when wj should
modify wi. Otherwise returns false.
procedure Analyze(w, N , h)
var s: a stack for IDs of modifier bunsetsus
begin
{?1 indicates no modifier candidate}
Push(?1, s);
Push(0, s);
for i ? 1 to N ? 1 do begin
j ? Pop(s);
while (j 6= ?1
and ((i = N ? 1) or Dep(j, i, w)) ) do
begin
hj ? i;
j ? Pop(s)
end
Push(j, s);
Push(i, s)
end
end
Figure 3: Algorithm of Japanese dependency pars-
ing
that the algorithm in Figure 4 does not generate
every pair of bunsetsus.3
4 Active Learning for Parsing
Most of the methods of active learning for parsing
in previous work use selection of sentences that
seem to contribute to the improvement of accuracy
(Tang et al, 2002; Hwa, 2004; Baldridge and Os-
borne, 2004). Although Hwa suggests that sample
selection for parsing would be improved by select-
ing finer grained constituents rather than sentences
(Hwa, 2004), such methods have not been investi-
gated so far.
Typical methods of selecting sentences are
3We show a sample set of generated examples for training
the classifier of the parser in Figure 3. By using the algorithm
in Figure 4, we can obtain labeled examples from the sample
sentences in Figure 2: {0, 1, ?O?}, {1, 2, ?O?}, {2, 3, ?D?},
and {1, 3, ?O?}. Please see Section 5.2 for the notation
used here. For example, an actual labeled instance generated
from {2, 3, ?D?} will be like ?label=D, features={modifier-
content-word=ano, ..., head-content-word=pen, ...}.?
Input: hi: the head IDs of bunsetsus wi.
Function: Dep(j, i, w, h): returns true if hj = i.
Otherwise returns false. Also prints a
feature vector with a label according to hj .
procedure Generate(w, N , h)
begin
Push(?1, s);
Push(0, s);
for i ? 1 to N ? 1 do begin
j ? Pop(s);
while (j 6= ?1
and ((i = N ? 1) or Dep(j, i, w, h)) ) do
begin
j ? Pop(s)
end
Push(j, s);
Push(i, s)
end
end
Figure 4: Algorithm of generating training exam-
ples
based on some entropy-based measure of a given
sentence (e.g., (Tang et al, 2002)). We cannot
use this kind of measures when we want to select
other smaller constituents than sentences. Other
bigger problem is an algorithm of parsing itself.
If we sample smaller units rather than sentences,
we have partially annotated sentences and have to
use a parsing algorithm that can be trained from
incompletely annotated sentences. Therefore, it is
difficult to use some of probabilistic models for
parsing. 4
5 Active Learning for Japanese
Dependency Parsing
In this section we describe sample selection meth-
ods which we investigated.
5.1 Sentence-wise Sample Selection
Passive Selection (Passive) This method is to
select sequentially sentences that appear in the
training corpus. Since it gets harder for the read-
ers to reproduce the same experimental setting, we
4We did not employ query-by-committee (QBC) (Seung
et al, 1992), which is another important general framework
of active learning, since the selection strategy with large mar-
gin classifiers (Section 2.2) is much simpler and seems more
practical for active learning in Japanese dependency parsing
with smaller constituents.
358
avoid to use random sampling in this paper.
Minimum Margin Selection (Min) This
method is to select sentences that contain bun-
setsu pairs which have smaller margin values
of outputs of the classifier used in parsing. The
procedure of selection of MIN are summarized as
follows. Assume that we have sentences si in the
pool of unlabeled sentences.
1. Parse si in the pool with the current model.
2. Sort si with min |f(xk)| where xk are bun-
setsu pairs in the sentence si. Note that xk
are not all possible bunsetsu pairs in si and
they are limited to bunsetsu pairs checked in
the process of parsing si.
3. Select top m sentences.
Averaged Margin Selection (Avg) This method
is to select sentences that have smaller values of
averaged margin values of outputs of the classi-
fier in a give sentences over the number of deci-
sions which are carried out in parsing. The differ-
ence between AVG and MIN is that for AVG we
use
?
|f(xk)|/l where l is the number of calling
Dep() in Figure 3 for the sentence si instead of
min |f(xk)| for MIN.
5.2 Chunk-wise Sample Selection
In chunk-wise sample selection, we select bun-
setsu pairs rather than sentences. Bunsetsu pairs
are selected from different sentences in a pool.
This means that structures of sentences in the pool
are partially annotated.
Note that we do not use every bunsetsu pair in
a sentence. When we use Sassano?s algorithm, we
have to generate training examples for the classi-
fier by using the algorithm in Figure 4. In other
words, we should not sample bunsetsu pairs inde-
pendently from a given sentence.
Therefore, we select bunsetsu pairs that have
smaller margin values of outputs given by the clas-
sifier during the parsing process. All the sentences
in the pool are processed by the current parser. We
cannot simply split the sentences in the pool into
labeled and unlabeled ones because we do not se-
lect every bunsetsu pair in a given sentence.
Naive Selection (Naive) This method is to select
bunsetsu pairs that have smaller margin values of
outputs of the classifier. Then it is assumed that
annotators would label either ?D? for the two bun-
setsu having a dependency relation or ?O?, which
represents the two does not.
Modified Simple Selection (ModSimple) Al-
though NAIVE seems to work well, it did not (dis-
cussed later). MODSIMPLE is to select bunsetsu
pairs that have smaller margin values of outputs
of the classifier, which is the same as in NAIVE.
The difference between MODSIMPLE and NAIVE
is the way annotators label examples. Assume that
we have an annotator and the learner selects some
bunsetsu pair of the j-th bunsetsu and the i-th bun-
setsu such that j < i. The annotator is then asked
what the head of the j-th bunsetsu is. We define
here the head bunsetsu is the k-th one.
We differently generate labeled examples from
the information annotators give according to the
relation among bunsetsus j, i, and k.
Below we use the notation {s, t, ?D?} to de-
note that the s-th bunsetsu modifies the t-th one.
The use of ?O? instead of ?D? indicates that the
s-th does not modify the t-th. That is generating
{s, t, ?D?} means outputting an example with the
label ?D?.
Case 1 if j < i < k, then generate {j, i, ?O?} and
{j, k, ?D?}.
Case 2 if j < i = k, then generate {j, k, ?D?}.
Case 3 if j < k < i, then generate {j, k, ?D?}.
Note that we do not generate {j, i, ?O?} in
this case because in Sassano?s algorithm we
do not need such labeled examples if j de-
pends on k such that k < i.
Syntactically Extended Selection (Syn) This
selection method is one based on MODSIMPLE
and extended to generate more labeled examples
for the classifier. You may notice that more labeled
examples for the classifier can be generated from
a single label which the annotator gives. Syntac-
tic constraints of the Japanese language allow us
to extend labeled examples.
For example, suppose that we have four bunset-
sus A, B, C, and D in this order. If A depends
on C, i.e., the head of A is C, then it is automati-
cally derived that B also should depend on C be-
cause the Japanese language has the no-crossing
constraint for dependencies (Section 3.2). By uti-
lizing this property we can obtain more labeled ex-
amples from a single labeled one annotators give.
In the example above, we obtain {A,B, ?O?} and
{B,C, ?D?} from {A,C, ?D?}.
359
Although we can employ various extensions to
MODSIMPLE, we use a rather simple extension in
this research.
Case 1 if (j < i < k), then generate
? {j, i, ?O?},
? {k ? 1, k, ?D?} if k ? 1 > j,
? and {j, k, ?D?}.
Case 2 if (j < i = k), then generate
? {k ? 1, k, ?D?} if k ? 1 > j,
? and {j, k, ?D?}.
Case 3 if (j < k < i), then generate
? {k ? 1, k, ?D?} if k ? 1 > j,
? and {j, k, ?D?}.
In SYN as well as MODSIMPLE, we generate
examples with ?O? only for bunsetsu pairs that oc-
cur to the left of the correct head (i.e., case 1).
6 Experimental Evaluation and
Discussion
6.1 Corpus
In our experiments we used the Kyoto University
Corpus Version 2 (Kurohashi and Nagao, 1998).
Initial seed sentences and a pool of unlabeled sen-
tences for training are taken from the articles on
January 1st through 8th (7,958 sentences) and the
test data is a set of sentences in the articles on Jan-
uary 9th (1,246 sentences). The articles on Jan-
uary 10th were used for development. The split of
these articles for training/test/development is the
same as in (Uchimoto et al, 1999).
6.2 Averaged Perceptron
We used the averaged perceptron (AP) (Freund
and Schapire, 1999) with polynomial kernels. We
set the degree of the kernels to 3 since cubic ker-
nels with SVM have proved effective for Japanese
dependency parsing (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2002). We found the best
value of the epoch T of the averaged perceptron
by using the development set. We fixed T = 12
through all experiments for simplicity.
6.3 Features
There are features that have been commonly used
for Japanese dependency parsing among related
papers, e.g., (Kudo and Matsumoto, 2002; Sas-
sano, 2004; Iwatate et al, 2008). We also used
the same features here. They are divided into three
groups: modifier bunsetsu features, head bunsetsu
features, and gap features. A summary of the fea-
tures is described in Table 1.
6.4 Implementation
We implemented a parser and a tool for the av-
eraged perceptron in C++ and used them for ex-
periments. We wrote the main program of active
learning and some additional scripts in Perl and sh.
6.5 Settings of Active Learning
For initial seed sentences, first 500 sentences are
taken from the articles on January 1st. In ex-
periments about sentence wise selection, 500 sen-
tences are selected at each iteration of active learn-
ing and labeled5 and added into the training data.
In experiments about chunk wise selection 4000
pairs of bunsetsus, which are roughly equal to the
averaged number of bunsetsus in 500 sentences,
are selected at each iteration of active learning.
6.6 Dependency Accuracy
We use dependency accuracy as a performance
measure of a parser. The dependency accuracy is
the percentage of correct dependencies. This mea-
sure is commonly used for the Kyoto University
Corpus.
6.7 Results and Discussion
Learning Curves First we compare methods for
sentence wise selection. Figure 5 shows that MIN
is the best among them, while AVG is not good
and similar to PASSIVE. It is observed that active
learning with large margin classifiers also works
well for Sassano?s algorithm of Japanese depen-
dency parsing.
Next we compare chunk-wise selection with
sentence-wise one. The comparison is shown in
Figure 6. Note that we must carefully consider
how to count labeled examples. In sentence wise
selection we obviously count the number of sen-
tences. However, it is impossible to count such
number when we label bunsetsus pairs.
Therefore, we use the number of bunsetsus that
have an annotated head. Although we know this
may not be a completely fair comparison, we be-
lieve our choice in this experiment is reasonable
5In our experiments human annotators do not give labels.
Instead, labels are given virtually from correct ones that the
Kyoto University Corpus has.
360
Bunsetsu features for modifiers rightmost content word, rightmost function word, punctuation,
and heads parentheses, location (BOS or EOS)
Gap features distance (1, 2?5, or 6 ?), particles, parentheses, punctuation
Table 1: Features for deciding a dependency relation between two bunsetsus. Morphological features
for each word (morpheme) are major part-of-speech (POS), minor POS, conjugation type, conjugation
form, and surface form.
for assessing the effect of reduction by chunk-wise
selection.
In Figure 6 NAIVE has a better learning curve
compared to MIN at the early stage of learning.
However, the curve of NAIVE declines at the later
stage and gets worse than PASSIVE and MIN.
Why does this phenomenon occur? It is because
each bunsetsu pair is not independent and pairs in
the same sentence are related to each other. They
satisfy the constraints discussed in Section 3.2.
Furthermore, the algorithm we use, i.e., Sassano?s,
assumes these constraints and has the specific or-
der for processing bunsetsu pairs as we see in Fig-
ure 3. Let us consider the meaning of {j, i, ?O?} if
the head of the j-th bunsetsu is the k-th one such
that j < k < i. In the context of the algorithm in
Figure 3, {j, i, ?O?} actually means that the j-th
bunsetsu modifies th l-th one such that i < l. That
is ?O? does not simply mean that two bunsetsus
does not have a dependency relation. Therefore,
we should not generate {j, i, ?O?} in the case of
j < k < i. Such labeled instances are not needed
and the algorithm in Figure 4 does not generate
them even if a fully annotated sentence is given.
Based on the analysis above, we modified NAIVE
and defined MODSIMPLE, where unnecessary la-
beled examples are not generated.
Now let us compare NAIVE with MODSIMPLE
(Figure 7). MODSIMPLE is almost always better
than PASSIVE and does not cause a significant de-
terioration of accuracy unlike NAIVE.6
Comparison of MODSIMPLE and SYN is shown
in Figure 8. Both exhibit a similar curve. Figure 9
shows the same comparison in terms of required
queries to human annotators. It shows that SYN is
better than MODSIMPLE especially at the earlier
stage of active learning.
Reduction of Annotations Next we examined
the number of labeled bunsetsus to be required in
6We have to carefully see the curves of NAIVE and MOD-
SIMPLE. In Figure 7 at the early stage NAIVE is slightly
better than MODSIMPLE, while in Figure 9 NAIVE does not
outperform MODSIMPLE. This is due to the difference of the
way of accessing annotation efforts.
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  1000 2000 3000 4000 5000 6000 7000 8000
A
cc
ur
ac
y
Number of Labeled Sentences
Passive
Min
Average
Figure 5: Learning curves of methods for sentence
wise selection
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000
A
cc
ur
ac
y
Number of bunsetsus which have a head
Passive
Min
Naive
Figure 6: Learning curves of MIN (sentence-wise)
and NAIVE (chunk-wise).
361
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000
A
cc
ur
ac
y
Number of bunsetsus which have a head
Passive
ModSimple
Naive
Figure 7: Learning curves of NAIVE, MODSIM-
PLE and PASSIVE in terms of the number of bun-
setsus that have a head.
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000
A
cc
ur
ac
y
Number of bunsetsus which have a head
Passive
ModSimple
Syntax
Figure 8: Learning curves of MODSIMPLE and
SYN in terms of the number of bunsetsus which
have a head.
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
 0  10000  20000  30000  40000  50000  60000
A
cc
ur
ac
y
Number of queris to human annotators
ModSimple
Syntax
Naive
Figure 9: Comparison of MODSIMPLE and SYN
in terms of the number of queries to human anno-
tators
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 40000
Passive Min Avg Naive Mod
Simple
Syn
# 
of
 b
un
se
ts
us
 th
at
 h
av
e 
a 
he
ad
Selection strategy
Figure 10: Number of labeled bunsetsus to be re-
quired to achieve an accuracy of over 88.3%.
 0
 5000
 10000
 15000
 20000
 25000
 0  1000 2000 3000 4000 5000 6000 7000 8000
N
um
be
r o
f S
up
po
rt 
Ve
ct
or
s
Number of Labeled Sentences
Passive
Min
Figure 11: Changes of number of support vectors
in sentence-wise active learning
 0
 5000
 10000
 15000
 20000
 25000
 0  10000  20000  30000  40000  50000  60000
N
um
be
r o
f S
up
po
rt 
Ve
ct
or
s
Number of Queries
ModSimple
Figure 12: Changes of number of support vectors
in chunk-wise active learning (MODSIMPLE)
362
order to achieve a certain level of accuracy. Fig-
ure 10 shows that the number of labeled bunsetsus
to achieve an accuracy of over 88.3% depending
on the active learning methods discussed in this
research.
PASSIVE needs 37766 labeled bunsetsus which
have a head to achieve an accuracy of 88.48%,
while SYN needs 13021 labeled bunsetsus to
achieve an accuracy of 88.56%. SYN requires only
34.4% of the labeled bunsetsu pairs that PASSIVE
requires.
Stopping Criteria It is known that increment
rate of the number of support vectors in SVM in-
dicates saturation of accuracy improvement dur-
ing iterations of active learning (Schohn and Cohn,
2000). It is interesting to examine whether the
observation for SVM is also useful for support
vectors7 of the averaged perceptron. We plotted
changes of the number of support vectors in the
cases of both PASSIVE and MIN in Figure 11 and
changes of the number of support vectors in the
case of MODSIMPLE in Figure 12. We observed
that the increment rate of support vectors mildly
gets smaller. However, it is not so clear as in the
case of text classification in (Schohn and Cohn,
2000).
Issues on Accessing the Total Cost of Annota-
tion In this paper, we assume that each annota-
tion cost for dependency relations is constant. It
is however not true in an actual annotation work.8
In addition, we have to note that it may be easier
to annotate a whole sentence than some bunsetsu
pairs in a sentence9. In a real annotation task, it
will be better to show a whole sentence to anno-
tators even when annotating some part of the sen-
tence.
Nevertheless, it is noteworthy that our research
shows the minimum number of annotations in
preparing training examples for Japanese depen-
dency parsing. The methods we have proposed
must be helpful when checking repeatedly anno-
tations that are important and might be wrong or
difficult to label while building an annotated cor-
7Following (Freund and Schapire, 1999), we use the term
?support vectors? for AP as well as SVM. ?Support vectors?
of AP means vectors which are selected in the training phase
and contribute to the prediction.
8Thus it is very important to construct models for estimat-
ing the actual annotation cost as Haertel et al (2008) do.
9Hwa (2004) discusses similar aspects of researches on
active learning.
pus. They also will be useful for domain adapta-
tion of a dependency parser.10
Applicability to Other Languages and Other
Parsing Algorithms We discuss here whether
or not the proposed methods and the experiments
are useful for other languages and other parsing
algorithms. First we take languages similar to
Japanese in terms of syntax, i.e., Korean and Mon-
golian. These two languages are basically head-
final languages and have similar constraints in
Section 3.2. Although no one has reported appli-
cation of (Sassano, 2004) to the languages so far,
we believe that similar parsing algorithms will be
applicable to them and the discussion in this study
would be useful.
On the other hand, the algorithm of (Sassano,
2004) cannot be applied to head-initial languages
such as English. If target languages are assumed
to be projective, the algorithm of (Nivre, 2003)
can be used. It is highly likely that we will invent
the effective use of finer-grained constituents, e.g.,
head-modifier pairs, rather than sentences in active
learning for Nivre?s algorithm with large margin
classifiers since Sassano?s seems to be a simplified
version of Nivre?s and they have several properties
in common. However, syntactic constraints in Eu-
ropean languages like English may be less helpful
than those in Japanese because their dependency
links do not have a single direction.
Even though the use of syntactic constraints is
limited, smaller constituents will still be useful for
other parsing algorithms that use some determin-
istic methods with machine learning-based classi-
fiers. There are many algorithms that have such
a framework, which include (Yamada and Mat-
sumoto, 2003) for English and (Kudo and Mat-
sumoto, 2002; Iwatate et al, 2008) for Japanese.
Therefore, effective use of smaller constituents in
active learning would not be limited to the specific
algorithm.
7 Conclusion
We have investigated that active learning methods
for Japanese dependency parsing. It is observed
that active learning of parsing with the averaged
perceptron, which is one of the large margin clas-
sifiers, works also well for Japanese dependency
analysis.
10Ohtake (2006) examines heuristic methods of selecting
sentences.
363
In addition, as far as we know, we are the first
to propose the active learning methods of using
partial dependency relations in a given sentence
for parsing and we have evaluated the effective-
ness of our methods. Furthermore, we have tried
to obtain more labeled examples from precious la-
beled ones that annotators give by utilizing syntac-
tic constraints of the Japanese language. It is note-
worthy that linguistic constraints have been shown
useful for reducing annotations in active learning
for NLP.
Experimental results show that our proposed
methods have improved considerably the learning
curve of Japanese dependency parsing.
We are currently building a new annotated cor-
pus with an annotation tool. We have a plan to in-
corporate our proposed methods to the annotation
tool. We will use it to accelerate building of the
large annotated corpus to improved our Japanese
parser.
It would be interesting to explore the use of par-
tially labeled constituents in a sentence in another
language, e.g., English, for active learning.
Acknowledgements
We would like to thank the anonymous review-
ers and Tomohide Shibata for their valuable com-
ments.
References
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Proc.
of EMNLP 2004, pages 9?16.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proc. of ACL-08: HLT, short papers
(Companion Volume), pages 65?68.
Shinkichi Hashimoto. 1934. Essentials of Japanese
Grammar (Kokugoho Yousetsu) (in Japanese).
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese dependency parsing us-
ing a tournament model. In Proc. of COLING 2008,
pages 361?368.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proc. of EMNLP/VLC 2000, pages 18?
25.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of CoNLL-2002, pages 63?69.
Sadao Kurohashi and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing
system. In Proc. of LREC-1998, pages 719?724.
Florian Laws and Hinrich Schu?tze. 2008. Stopping cri-
teria for active learning of named entity recognition.
In Proc. of COLING 2008, pages 465?472.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
Proc. of the Seventeenth Annual International ACM-
SIGIR Conference on Research and Development in
Information Retrieval, pages 3?12.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of ACL-2005, pages
523?530.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proc. of IWPT-03,
pages 149?160.
Kiyonori Ohtake. 2006. Analysis of selective strate-
gies to build a dependency-analyzed corpus. In
Proc. of COLING/ACL 2006 Main Conf. Poster Ses-
sions, pages 635?642.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learn-
ing for part-of-speech tagging: Accelerating corpus
annotation. In Proc. of the Linguistic Annotation
Workshop, pages 101?108.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proc. of ACL-2002, pages
505?512.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proc. of COLING 2004, pages
8?14.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proc. of ICML-2000, pages 839?846.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. of COLT ?92, pages
287?294.
Min Tang, Xaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proc. of ACL-2002, pages 120?127.
364
Simon Tong and Daphne Koller. 2000. Support vec-
tor machine active learning with applications to text
classification. In Proc. of ICML-2000, pages 999?
1006.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analy-
sis based on maximum entropy models. In Proc. of
EACL-99, pages 196?203.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT 2003, pages 195?206.
Jingbo Zhu and Eduard Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proc. of
EMNLP-CoNLL 2007, pages 783?790.
365
