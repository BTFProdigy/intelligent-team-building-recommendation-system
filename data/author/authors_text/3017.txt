Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 152?159,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Statistical Machine Translation through Global Lexical Selection and
Sentence Reconstruction
Srinivas Bangalore, Patrick Haffner, Stephan Kanthak
AT&T Labs - Research
180 Park Ave, Florham Park, NJ 07932
{srini,haffner,skanthak}@research.att.com
Abstract
Machine translation of a source language
sentence involves selecting appropriate tar-
get language words and ordering the se-
lected words to form a well-formed tar-
get language sentence. Most of the pre-
vious work on statistical machine transla-
tion relies on (local) associations of target
words/phrases with source words/phrases
for lexical selection. In contrast, in this pa-
per, we present a novel approach to lexical
selection where the target words are associ-
ated with the entire source sentence (global)
without the need to compute local associa-
tions. Further, we present a technique for
reconstructing the target language sentence
from the selected words. We compare the re-
sults of this approach against those obtained
from a finite-state based statistical machine
translation system which relies on local lex-
ical associations.
1 Introduction
Machine translation can be viewed as consisting of
two subproblems: (a) lexical selection, where appro-
priate target language lexical items are chosen for
each source language lexical item and (b) lexical re-
ordering, where the chosen target language lexical
items are rearranged to produce a meaningful target
language string. Most of the previous work on statis-
tical machine translation, as exemplified in (Brown
et al, 1993), employs word-alignment algorithm
(such as GIZA++ (Och and Ney, 2003)) that pro-
vides local associations between source and target
words. The source-to-target word alignments are
sometimes augmented with target-to-source word
alignments in order to improve precision. Further,
the word-level alignments are extended to phrase-
level alignments in order to increase the extent of
local associations. The phrasal associations compile
some amount of (local) lexical reordering of the tar-
get words ? those permitted by the size of the phrase.
Most of the state-of-the-art machine translation sys-
tems use phrase-level associations in conjunction
with a target language model to produce sentences.
There is relatively little emphasis on (global) lexical
reordering other than the local reorderings permit-
ted within the phrasal alignments. A few exceptions
are the hierarchical (possibly syntax-based) trans-
duction models (Wu, 1997; Alshawi et al, 1998;
Yamada and Knight, 2001; Chiang, 2005) and the
string transduction models (Kanthak et al, 2005).
In this paper, we present an alternate approach to
lexical selection and lexical reordering. For lexical
selection, in contrast to the local approaches of as-
sociating target to source words, we associate tar-
get words to the entire source sentence. The intu-
ition is that there may be lexico-syntactic features of
the source sentence (not necessarily a single source
word) that might trigger the presence of a target
word in the target sentence. Furthermore, it might be
difficult to exactly associate a target word to a source
word in many situations ? (a) when the translations
are not exact but paraphrases (b) when the target lan-
guage does not have one lexical item to express the
same concept that is expressed by a source word.
Extending word to phrase alignments attempts to ad-
dress some of these situations while alleviating the
noise in word-level alignments.
As a consequence of this global lexical selection
approach, we no longer have a tight association be-
tween source and target language words. The re-
sult of lexical selection is simply a bag of words in
the target language and the sentence has to be recon-
structed using this bag of words. The words in the
bag, however, might be enhanced with rich syntactic
information that could aid in reconstructing the tar-
get sentence. This approach to lexical selection and
152
Translation modelWFSA
BilanguagePhrase Segmented
FSA to FST
Bilanguage
WFSTTransformation
Bilanguage
Reordering
Local Phrase Joint Language
Modeling
Joint Language
Alignment
WordAlignmentSentence AlignedCorpus
Figure 1: Training phases for our system
ConstructionPermutation
Permutation Lattice
Lexical Choice 
FST Composition
Decoding
SourceSentence/
WeightedLattice
Target
Decoding Lexical Reodering
 CompositionFSA Sentence
Model
Translation ModelLanguage
Target
Figure 2: Decoding phases for our system
sentence reconstruction has the potential to circum-
vent limitations of word-alignment based methods
for translation between languages with significantly
different word order (e.g. English-Japanese).
In this paper, we present the details of training
a global lexical selection model using classifica-
tion techniques and sentence reconstruction mod-
els using permutation automata. We also present a
stochastic finite-state transducer (SFST) as an exam-
ple of an approach that relies on local associations
and use it to compare and contrast our approach.
2 SFST Training and Decoding
In this section, we describe each of the components
of our SFST system shown in Figure 1. The SFST
approach described here is similar to the one de-
scribed in (Bangalore and Riccardi, 2000) which has
subsequently been adopted by (Banchs et al, 2005).
2.1 Word Alignment
The first stage in the process of training a lexical se-
lection model is obtaining an alignment function (f )
that given a pair of source (s1s2 . . . sn) and target
(t1t2 . . . tm) language sentences, maps source lan-
guage word subsequences into target language word
subsequences, as shown below.
?i?j(f(si) = tj ? f(si) = ?) (1)
For the work reported in this paper, we have used
the GIZA++ tool (Och and Ney, 2003) which im-
plements a string-alignment algorithm. GIZA++
alignment however is asymmetric in that the word
mappings are different depending on the direction
of alignment ? source-to-target or target-to-source.
Hence in addition to the functions f as shown in
Equation 1 we train another alignment function g :
?j?i(g(tj) = si ? g(tj) = ?) (2)
English: I need to make a collect call
Japanese: ?H ???? ?ff?k $*d ?^%ffcW2
Alignment: 1 5 0 3 0 2 4
Figure 3: Example bilingual texts with alignment in-
formation
I:?H need:?^%ffcW2 to:? make:?ff?k
a:? collect ???? call $*d
Figure 4: Bilanguage strings resulting from align-
ments shown in Figure 3.
2.2 Bilanguage Representation
From the alignment information (see Figure 3), we
construct a bilanguage representation of each sen-
tence in the bilingual corpus. The bilanguage string
consists of source-target symbol pair sequences as
shown in Equation 3. Note that the tokens of a bilan-
guage could be either ordered according to the word
order of the source language or ordered according to
the word order of the target language.
Bf = bf1 bf2 . . . bfm (3)
bfi = (si?1; si, f(si)) if f(si?1) = ?
= (si, f(si?1); f(si)) if si?1 = ?
= (si, f(si)) otherwise
Figure 4 shows an example alignment and the
source-word-ordered bilanguage strings correspond-
ing to the alignment shown in Figure 3.
We also construct a bilanguage using the align-
ment function g similar to the bilanguage using the
alignment function f as shown in Equation 3.
Thus, the bilanguage corpus obtained by combin-
ing the two alignment functions is B = Bf ?Bg.
2.3 Bilingual Phrases and Local Reordering
While word-to-word translation only approximates
the lexical selection process, phrase-to-phrase map-
ping can greatly improve the translation of colloca-
tions, recurrent strings, etc. Using phrases also al-
lows words within the phrase to be reordered into the
correct target language order, thus partially solving
the reordering problem. Additionally, SFSTs can
take advantage of phrasal correlations to improve the
computation of the probability P (WS ,WT ).
The bilanguage representation could result in
some source language phrases to be mapped to ?
153
(empty target phrase). In addition to these phrases,
we compute subsequences of a given length k on the
bilanguage string and for each subsequence we re-
order the target words of the subsequence to be in
the same order as they are in the target language sen-
tence corresponding to that bilanguage string. This
results in a retokenization of the bilanguage into to-
kens of source-target phrase pairs.
2.4 SFST Model
From the bilanguage corpus B, we train an n-gram
language model using standard tools (Goffin et al,
2005). The resulting language model is represented
as a weighted finite-state automaton (S ? T ?
[0, 1]). The symbols on the arcs of this automaton
(si ti) are interpreted as having the source and target
symbols (si:ti), making it into a weighted finite-state
transducer (S ? T?[0, 1]) that provides a weighted
string-to-string transduction from S into T :
T ? = argmax
T
P (si, ti|si?1, ti?1 . . . si?n?1, ti?n?1)
2.5 Decoding
Since we represent the translation model as a
weighted finite-state transducer (TransFST ), the
decoding process of translating a new source in-
put (sentence or weighted lattice (Is)) amounts to
a transducer composition (?) and selection of the
best probability path (BestPath) resulting from the
composition and projecting the target sequence (pi1).
T ? = pi1(BestPath(Is ? TransFST )) (4)
However, we have noticed that on the develop-
ment corpus, the decoded target sentence is typically
shorter than the intended target sentence. This mis-
match may be due to the incorrect estimation of the
back-off events and their probabilities in the train-
ing phase of the transducer. In order to alleviate
this mismatch, we introduce a negative word inser-
tion penalty model as a mechanism to produce more
words in the target sentence.
2.6 Word Insertion Model
The word insertion model is also encoded as a
weighted finite-state automaton and is included in
the decoding sequence as shown in Equation 5. The
word insertion FST has one state and |?T | number
of arcs each weighted with a ? weight representing
the word insertion cost. On composition as shown
in Equation 5, the word insertion model penalizes or
rewards paths which have more words depending on
whether ? is positive or negative value.
T ? = pi1(BestPath(Is?TransFST?WIP )) (5)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2Figure 5: Locally constraint permutation automatonfor a sentence with 4 words and window size of 2.
2.7 Global Reordering
Local reordering as described in Section 2.3 is re-
stricted by the window size k and accounts only for
different word order within phrases. As permuting
non-linear automata is too complex, we apply global
reordering by permuting the words of the best trans-
lation and weighting the result by an n-gram lan-
guage model (see also Figure 2):
T ? = BestPath(perm(T ?) ? LMt) (6)
Even the size of the minimal permutation automa-
ton of a linear automaton grows exponentially with
the length of the input sequence. While decoding by
composition simply resembles the principle of mem-
oization (i.e. here: all state hypotheses of a whole
sentence are kept in memory), it is necessary to ei-
ther use heuristic forward pruning or constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al, 2005)). We have cho-
sen to constrain permutations here. Figure 5 shows
the resulting minimal permutation automaton for an
input sequence of 4 words and a window size of 2.
Decoding ASR output in combination with global
reordering uses n-best lists or extracts them from lat-
tices first. Each entry of the n-best list is decoded
separately and the best target sentence is picked
from the union of the n intermediate results.
3 Discriminant Models for Lexical
Selection
The approach from the previous section is a genera-
tive model for statistical machine translation relying
on local associations between source and target sen-
tences. Now, we present our approach for a global
lexical selection model based on discriminatively
trained classification techniques. Discriminant mod-
eling techniques have become the dominant method
for resolving ambiguity in speech and other NLP
tasks, outperforming generative models. Discrimi-
native training has been used mainly for translation
model combination (Och and Ney, 2002) and with
the exception of (Wellington et al, 2006; Tillmann
and Zhang, 2006), has not been used to directly train
parameters of a translation model. We expect dis-
criminatively trained global lexical selection models
154
to outperform generatively trained local lexical se-
lection models as well as provide a framework for
incorporating rich morpho-syntactic information.
Statistical machine translation can be formulated
as a search for the best target sequence that maxi-
mizes P (T |S), where S is the source sentence and
T is the target sentence. Ideally, P (T |S) should
be estimated directly to maximize the conditional
likelihood on the training data (discriminant model).
However, T corresponds to a sequence with a ex-
ponentially large combination of possible labels,
and traditional classification approaches cannot be
used directly. Although Conditional Random Fields
(CRF) (Lafferty et al, 2001) train an exponential
model at the sequence level, in translation tasks such
as ours the computational requirements of training
such models are prohibitively expensive.
We investigate two approaches to approximating
the string level global classification problem, using
different independence assumptions. A comparison
of the two approaches is summarized in Table 1.
3.1 Sequential Lexical Choice Model
In the first approach, we formulate a sequential lo-
cal classification problem as shown in Equations 7.
This approach is similar to the SFST approach in
that it relies on local associations between the source
and target words(phrases). We can use a conditional
model (instead of a joint model as before) and the
parameters are determined using discriminant train-
ing which allows for richer conditioning context.
P (T |S) =
?N
i=1 P (ti|?(S, i)) (7)
where ?(S, i) is a set of features extracted from the
source string S (shortened as ? in the rest of the
section).
3.2 Bag-of-Words Lexical Choice Model
The sequential lexical choice model described in
the previous section treats the selection of a lexical
choice for a source word in the local lexical context
as a classification task. The data for training such
models is derived from word alignments obtained
by e.g. GIZA++. The decoded target lexical items
have to be further reordered, but for closely related
languages the reordering could be incorporated into
correctly ordered target phrases as discussed previ-
ously.
For pairs of languages with radically different
word order (e.g. English-Japanese), there needs to
be a global reordering of words similar to the case
in the SFST-based translation system. Also, for such
differing language pairs, the alignment algorithms
such as GIZA++ perform poorly.
These observations prompted us to formulate the
lexical choice problem without the need for word
alignment information. We require a sentence
aligned corpus as before, but we treat the target sen-
tence as a bag-of-words or BOW assigned to the
source sentence. The goal is, given a source sen-
tence, to estimate the probability that we find a given
word in the target sentence. This is why, instead of
producing a target sentence, what we initially obtain
is a target bag of words. Each word in the target vo-
cabulary is detected independently, so we have here
a very simple use of binary static classifiers. Train-
ing sentence pairs are considered as positive exam-
ples when the word appears in the target, and neg-
ative otherwise. Thus, the number of training ex-
amples equals the number of sentence pairs, in con-
trast to the sequential lexical choice model which
has one training example for each token in the bilin-
gual training corpus. The classifier is trained with n-
gram features (BOgrams(S)) from the source sen-
tence. During decoding the words with conditional
probability greater than a threshold ? are considered
as the result of lexical choice decoding.
BOW ?T = {t|P (t|BOgrams(S)) > ?} (8)
For reconstructing the proper order of words in
the target sentence we consider all permutations of
words in BOW ?T and weight them by a target lan-
guage model. This step is similar to the one de-
scribed in Section 2.7. The BOW approach can also
be modified to allow for length adjustments of tar-
get sentences, if we add optional deletions in the fi-
nal step of permutation decoding. The parameter ?
and an additional word deletion penalty can then be
used to adjust the length of translated outputs. In
Section 6, we discuss several issues regarding this
model.
4 Choosing the classifier
This section addresses the choice of the classifi-
cation technique, and argues that one technique
that yields excellent performance while scaling well
is binary maximum entropy (Maxent) with L1-
regularization.
4.1 Multiclass vs. Binary Classification
The Sequential and BOW models represent two dif-
ferent classification problems. In the sequential
model, we have a multiclass problem where each
class ti is exclusive, therefore, all the classifier out-
puts P (ti|?) must be jointly optimized such that
155
Table 1: A comparison of the sequential and bag-of-words lexical choice models
Sequential Lexical Model Bag-of-Words Lexical Model
Output target Target word for each source position i Target word given a source sentence
Input features BOgram(S, i? d, i+ d) : bag of n-grams BOgram(S, 0, |S|): bag of n-grams
in source sentence in the interval [i? d, i+ d] in source sentence
Probabilities P (ti|BOgram(S, i? d, i+ d)) P (BOW (T )|BOgram(S, 0, |S|))
Independence assumption between the labels
Number of classes One per target word or phrase
Training samples One per source token One per sentence
Preprocessing Source/Target word alignment Source/Target sentence alignment
?
i P (ti|?) = 1. This can be problematic: with
one classifier per word in the vocabulary, even allo-
cating the memory during training may exceed the
memory capacity of current computers.
In the BOW model, each class can be detected
independently, and two different classes can be de-
tected at the same time. This is known as the 1-vs-
other scheme. The key advantage over the multiclass
scheme is that not all classifiers have to reside in
memory at the same time during training which al-
lows for parallelization. Fortunately for the sequen-
tial model, we can decompose a multiclass classifi-
cation problem into separate 1-vs-other problems. In
theory, one has to make an additional independence
assumption and the problem statement becomes dif-
ferent. Each output label t is projected into a bit
string with components bj(t) where probability of
each component is estimated independently:
P (bj(t)|?) = 1? P (b?j(t)|?) = 11 + e?(?j??j?)??
In practice, despite the approximation, the 1-vs-
other scheme has been shown to perform as well as
the multiclass scheme (Rifkin and Klautau, 2004).
As a consequence, we use the same type of binary
classifier for the sequential and the BOW models.
The excellent results recently obtained with the
SEARN algorithm (Daume et al, 2007) also sug-
gest that binary classifiers, when properly trained
and combined, seem to be capable of matching more
complex structured output approaches.
4.2 Geometric vs. Probabilistic Interpretation
We separate the most popular classification tech-
niques into two broad categories:
? Geometric approaches maximize the width of
a separation margin between the classes. The
most popular method is the Support Vector Ma-
chine (SVM) (Vapnik, 1998).
? Probabilistic approaches maximize the con-
ditional likelihood of the output class given
the input features. This logistic regression is
also called Maxent as it finds the distribution
with maximum entropy that properly estimates
the average of each feature over the training
data (Berger et al, 1996).
In previous studies, we found that the best accuracy
is achieved with non-linear (or kernel) SVMs, at the
expense of a high test time complexity, which is un-
acceptable for machine translation. Linear SVMs
and regularized Maxent yield similar performance.
In theory, Maxent training, which scales linearly
with the number of examples, is faster than SVM
training, which scales quadratically with the num-
ber of examples. In our first experiments with lexi-
cal choice models, we observed that Maxent slightly
outperformed SVMs. Using a single threshold with
SVMs, some classes of words were over-detected.
This suggests that, as theory predicts, SVMs do not
properly approximate the posterior probability. We
therefore chose to use Maxent as the best probability
approximator.
4.3 L1 vs. L2 regularization
Traditionally, Maxent is regularized by imposing a
Gaussian prior on each weight: this L2 regulariza-
tion finds the solution with the smallest possible
weights. However, on tasks like machine translation
with a very large number of input features, a Lapla-
cian L1 regularization that also attempts to maxi-
mize the number of zero weights is highly desirable.
A new L1-regularized Maxent algorithms was
proposed for density estimation (Dudik et al, 2004)
and we adapted it to classification. We found this al-
gorithm to converge faster than the current state-of-
the-art in Maxent training, which is L2-regularized
L-BFGS (Malouf, 2002)1. Moreover, the number of
trained parameters is considerably smaller.
5 Data and Experiments
We have performed experiments on the IWSLT06
Chinese-English training and development sets from
1We used the implementation available at
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
156
Table 2: Statistics of training and development data from 2005/2006 (? = first of multiple translations only).
Training (2005) Dev 2005 Dev 2006
Chinese English Chinese English Chinese English
Sentences 46,311 506 489
Running Words 351,060 376,615 3,826 3,897 5,214 6,362?
Vocabulary 11,178 11,232 931 898 1,136 1,134?
Singletons 4,348 4,866 600 538 619 574?
OOVs [%] - - 0.6 0.3 0.9 1.0
ASR WER [%] - - - - 25.2 -
Perplexity - - 33 - 86 -
# References - - 16 7
2005 and 2006. The data are traveler task ex-
pressions such as seeking directions, expressions in
restaurants and travel reservations. Table 2 presents
some statistics on the data sets. It must be noted
that while the 2005 development set matches the
training data closely, the 2006 development set has
been collected separately and shows slightly differ-
ent statistics for average sentence length, vocabulary
size and out-of-vocabulary words. Also the 2006
development set contains no punctuation marks in
Chinese, but the corresponding English translations
have punctuation marks. We also evaluated our
models on the Chinese speech recognition output
and we report results using 1-best with a word er-
ror rate of 25.2%.
For the experiments, we tokenized the Chinese
sentences into character strings and trained the mod-
els discussed in the previous sections. Also, we
trained a punctuation prediction model using Max-
ent framework on the Chinese character strings in
order to insert punctuation marks into the 2006 de-
velopment data set. The resulting character string
with punctuation marks is used as input to the trans-
lation decoder. For the 2005 development set, punc-
tuation insertion was not needed since the Chinese
sentences already had the true punctuation marks.
In Table 3 we present the results of the three dif-
ferent translation models ? FST, Sequential Maxent
and BOW Maxent. There are a few interesting ob-
servations that can be made based on these results.
First, on the 2005 development set, the sequential
Maxent model outperforms the FST model, even
though the two models were trained starting from
the same GIZA++ alignment. The difference, how-
ever, is due to the fact that Maxent models can cope
with increased lexical context2 and the parameters
of the model are discriminatively trained. The more
surprising result is that the BOW Maxent model sig-
nificantly outperforms the sequential Maxent model.
2We use 6 words to the left and right of a source word for
sequential Maxent, but only 2 preceding source and target words
for FST approach.
The reason is that the sequential Maxent model re-
lies on the word alignment, which, if erroneous, re-
sults in incorrect predictions by the sequential Max-
ent model. The BOW model does not rely on the
word-level alignment and can be interpreted as a dis-
criminatively trained model of dictionary lookup for
a target word in the context of a source sentence.
Table 3: Results (mBLEU) scores for the three dif-
ferent models on the transcriptions for development
set 2005 and 2006 and ASR 1-best for development
set 2006.
Dev 2005 Dev 2006
Text Text ASR 1-best
FST 51.8 19.5 16.5
Seq. Maxent 53.5 19.4 16.3
BOW Maxent 59.9 19.3 16.6
As indicated in the data release document, the
2006 development set was collected differently com-
pared to the one from 2005. Due to this mis-
match, the performance of the Maxent models are
not very different from the FST model, indicating
the lack of good generalization across different gen-
res. However, we believe that the Maxent frame-
work allows for incorporation of linguistic features
that could potentially help in generalization across
genres. For translation of ASR 1-best, we see a sys-
tematic degradation of about 3% in mBLEU score
compared to translating the transcription.
In order to compensate for the mismatch between
the 2005 and 2006 data sets, we computed a 10-fold
average mBLEU score by including 90% of the 2006
development set into the training set and using 10%
of the 2006 development set for testing, each time.
The average mBLEU score across these 10 runs in-
creased to 22.8.
In Figure 6 we show the improvement of mBLEU
scores with the increase in permutation window size.
We had to limit to a permutation window size of 10
due to memory limitations, even though the curve
has not plateaued. We anticipate using pruning tech-
niques we can increase the window size further.
157
 0.46
 0.48
 0.5
 0.52
 0.54
 0.56
 0.58
 0.6
 6  6.5  7  7.5  8  8.5  9  9.5  10Permutation Window SizeFigure 6: Improvement in mBLEU score with the
increase in size of the permutation window
5.1 United Nations and Hansard Corpora
In order to test the scalability of the global lexical
selection approach, we also performed lexical se-
lection experiments on the United Nations (Arabic-
English) corpus and the Hansard (French-English)
corpus using the SFST model and the BOW Maxent
model. We used 1,000,000 training sentence pairs
and tested on 994 test sentences for the UN corpus.
For the Hansard corpus we used the same training
and test split as in (Zens and Ney, 2004): 1.4 million
training sentence pairs and 5432 test sentences. The
vocabulary sizes for the two corpora are mentioned
in Table 4. Also in Table 4, are the results in terms of
F-measure between the words in the reference sen-
tence and the decoded sentences. We can see that the
BOW model outperforms the SFST model on both
corpora significantly. This is due to a systematic
10% relative improvement for open class words, as
they benefit from a much wider context. BOW per-
formance on close class words is higher for the UN
corpus but lower for the Hansard corpus.
Table 4: Lexical Selection results (F-measure) on
the Arabic-English UN Corpus and the French-
English Hansard Corpus. In parenthesis are F-
measures for open and closed class lexical items.
Corpus Vocabulary SFST BOW
Source Target
UN 252,571 53,005 64.6 69.5
(60.5/69.1) (66.2/72.6)
Hansard 100,270 78,333 57.4 60.8
(50.6/67.7) (56.5/63.4)
6 Discussion
The BOW approach is promising as it performs rea-
sonably well despite considerable losses in the trans-
fer of information between source and target lan-
guage. The first and most obvious loss is about word
position. The only information we currently use to
restore the target word position is the target language
model. Information about the grammatical role of a
word in the source sentence is completely lost. The
language model might fortuitously recover this in-
formation if the sentence with the correct grammat-
ical role for the word happens to be the maximum
likelihood sentence in the permutation automaton.
We are currently working toward incorporating
syntactic information on the target words so as to be
able to recover some of the grammatical role infor-
mation lost in the classification process. In prelimi-
nary experiments, we have associated the target lex-
ical items with supertag information (Bangalore and
Joshi, 1999). Supertags are labels that provide linear
ordering constraints as well as grammatical relation
information. Although associating supertags to tar-
get words increases the class set for the classifier, we
have noticed that the degradation in the F-score is
on the order of 3% across different corpora. The su-
pertag information can then be exploited in the sen-
tence construction process. The use of supertags in
phrase-based SMT system has been shown to im-
prove results (Hassan et al, 2006).
A less obvious loss is the number of times a word
or concept appears in the target sentence. Func-
tion words like ?the? and ?of? can appear many
times in an English sentence. In the model dis-
cussed in this paper, we index each occurrence of the
function word with a counter. In order to improve
this method, we are currently exploring a technique
where the function words serve as attributes (e.g.
definiteness, tense, case) on the contentful lexical
items, thus enriching the lexical item with morpho-
syntactic information.
A third issue concerning the BOW model is the
problem of synonyms ? target words which translate
the same source word. Suppose that in the training
data, target words t1 and t2 are, with equal probabil-
ity, translations of the same source word. Then, in
the presence of this source word, the probability to
detect the corresponding target word, which we as-
sume is 0.8, will be, because of discriminant learn-
ing, split equally between t1 and t2, that is 0.4 and
0.4. Because of this synonym problem, the BOW
threshold ? has to be set lower than 0.5, which is
observed experimentally. However, if we set the
threshold to 0.3, both t1 and t2 will be detected in
the target sentence, and we found this to be a major
source of undesirable insertions.
The BOW approach is different from the pars-
ing based approaches (Melamed, 2004; Zhang and
Gildea, 2005; Cowan et al, 2006) where the transla-
tion model tightly couples the syntactic and lexical
items of the two languages. The decoupling of the
158
two steps in our model has the potential for gener-
ating paraphrased sentences not necessarily isomor-
phic to the structure of the source sentence.
7 Conclusions
We view machine translation as consisting of lexi-
cal selection and lexical reordering steps. These two
steps need not necessarily be sequential and could be
tightly integrated. We have presented the weighted
finite-state transducer model of machine translation
where lexical choice and a limited amount of lexical
reordering are tightly integrated into a single trans-
duction. We have also presented a novel approach
to translation where these two steps are loosely cou-
pled and the parameters of the lexical choice model
are discriminatively trained using a maximum en-
tropy model. The lexical reordering model in this
approach is achieved using a permutation automa-
ton. We have evaluated these two approaches on the
2005 and 2006 IWSLT development sets and shown
that the techniques scale well to Hansard and UN
corpora.
References
H. Alshawi, S. Bangalore, and S. Douglas. 1998. Automatic
acquisition of hierarchical transduction models for machine
translation. In ACL, Montreal, Canada.
R.E. Banchs, J.M. Crego, A. Gispert, P. Lambert, and J.B.
Marino. 2005. Statistical machine translation of euparl data
by using bilingual n-grams. In Workshop on Building and
Using Parallel Texts. ACL.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An ap-
proach to almost parsing. Computational Linguistics, 25(2).
S. Bangalore and G. Riccardi. 2000. Stochastic finite-state
models for spoken language machine translation. In Pro-
ceedings of the Workshop on Embedded Machine Transla-
tion Systems, pages 52?59.
A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vincent.
1996. A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1):39?71.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The
Mathematics of Machine Translation: Parameter Estimation.
Computational Linguistics, 16(2):263?312.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In Proceedings of the ACL Con-
ference, Ann Arbor, MI.
B. Cowan, I. Kucerova, and M. Collins. 2006. A discrimi-
native model for tree-to-tree translation. In Proceedings of
EMNLP.
H. Daume, J. Langford, and D. Marcu. 2007. Search-based
structure prediction. submitted to Machine Learning Jour-
nal.
M. Dudik, S. Phillips, and R.E. Schapire. 2004. Perfor-
mance Guarantees for Regularized Maximum Entropy Den-
sity Estimation. In Proceedings of COLT?04, Banff, Canada.
Springer Verlag.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje,
S. Parthasarathy, M. Rahim, G. Riccardi, and M. Saraclar.
2005. The AT&T WATSON Speech Recognizer. In Pro-
ceedings of ICASSP, Philadelphia, PA.
H. Hassan, M. Hearne, K. Sima?an, and A. Way. 2006. Syntac-
tic phrase-based statistical machine translation. In Proceed-
ings of IEEE/ACL first International Workshop on Spoken
Language Technology (SLT), Aruba, December.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 167?174, Ann Ar-
bor, Michigan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, San Fran-
cisco, CA.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proceedings of CoNLL-
2002, pages 49?55. Taipei, Taiwan.
I. D. Melamed. 2004. Statistical machine translation by pars-
ing. In Proceedings of ACL.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proceedings of ACL.
F.J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-
vs-all classification. Journal of Machine Learning Research,
pages 101?141.
C. Tillmann and T. Zhang. 2006. A discriminative global train-
ing algorithm for statistical mt. In COLING-ACL.
V.N. Vapnik. 1998. Statistical Learning Theory. John Wiley &
Sons.
B. Wellington, J. Turian, C. Pike, and D. Melamed. 2006. Scal-
able purely-discriminative training for word and tree trans-
ducers. In AMTA.
D. Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proceedings of 39th ACL.
R. Zens and H. Ney. 2004. Improvements in phrase-based sta-
tistical machine translation. In Proceedings of HLT-NAACL,
pages 257?264, Boston, MA.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized inver-
sion transduction grammar for alignment. In Proceedings of
ACL.
159
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 36?43,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Capturing the stars: predicting ratings for service and product reviews
Narendra Gupta, Giuseppe Di Fabbrizio and Patrick Haffner
AT&T Labs - Research, Inc.
Florham Park, NJ 07932 - USA
{ngupta,pino,haffner}@research.att.com
Abstract
Bloggers, professional reviewers, and con-
sumers continuously create opinion?rich web
reviews about products and services, with the
result that textual reviews are now abundant on
the web and often convey a useful overall rat-
ing (number of stars). However, an overall rat-
ing cannot express the multiple or conflicting
opinions that might be contained in the text,
or explicitly rate the different aspects of the
evaluated entity. This work addresses the task
of automatically predicting ratings, for given
aspects of a textual review, by assigning a nu-
merical score to each evaluated aspect in the
reviews. We handle this task as both a re-
gression and a classification modeling prob-
lem and explore several combinations of syn-
tactic and semantic features. Our results sug-
gest that classification techniques perform bet-
ter than ranking modeling when handling eval-
uative text.
1 Introduction
An abundance of service and products reviews are
today available on the Web. Bloggers, professional
reviewers, and consumers continuously contribute
to this rich content both by providing text reviews
and often by assigning useful overall ratings (num-
ber of stars) to their overall experience. However,
the overall rating that usually accompanies online
reviews cannot express the multiple or conflicting
opinions that might be contained in the text, or ex-
plicitly rate the different aspects of the evaluated
entity. For example, a restaurant might receive an
overall great evaluation, while the service might
be rated below average due to slow and discourte-
ous wait staff. Pinpointing opinions in documents,
and the entities being referenced, would provide a
finer?grained sentiment analysis and a solid foun-
dation to automatically summarize evaluative text,
but such a task becomes even more challenging
when applied to a generic domain and with unsu-
pervised methods. Some significant contributions
by Hu and Liu (2004), Popescu and Etzioni (2005),
and Carenini et al (2006) illustrate different tech-
niques to find and measure opinion orientation in
text documents. Other work in sentiment analysis
(often referred as opinion mining) has explored sev-
eral facets of the problem, ranging from predicting
binary ratings (e.g., thumbs up/down) (Turney, 2002;
Pang et al, 2002; Dave et al, 2003; Yu and Hatzivas-
siloglou, 2003; Pang and Lee, 2004; Yi and Niblack,
2005; Carenini et al, 2006), to more detailed opin-
ion analysis methods predicting multi?scale ratings
(e.g., number of stars) (Pang and Lee, 2005; Sny-
der and Barzilay, 2007; Shimada and Endo, 2008;
Okanohara and Tsujii, 2005).
This paper focuses on multi?scale multi?aspect
rating prediction for textual reviews. As mentioned
before, textual reviews are abundant, but when try-
ing to make a buy decision on a specific product
or service, getting sufficient and reliable informa-
tion can be a daunting and time consuming task.
On one hand, a single overall rating does not pro-
vide enough information and could be unreliable, if
not supported over a large number of independent
reviews/ratings. From another standpoint, reading
through a large number of textual reviews in order
to infer the aspect ratings could be quite time con-
36
suming, and, at the same time, the outcome of the
evaluation could be biased by the reader?s interpre-
tation. In this work, instead of a single overall rat-
ing, we propose to provide ratings for multiple as-
pects of the product/service. For example, in the
case of restaurant reviews, we consider ratings for
five aspects: food, atmosphere, value, service and
overall experience. In Lu et al (2009) such aspect
ratings are called rated aspect summaries, in Shi-
mada and Endo (2008) they have been referred to as
seeing stars and in Snyder and Barzilay (2007) they
are referred to as multi?aspect ranking. We use su-
pervised learning methods to train predictive models
and use a specific decoding method to optimize the
aspect rating assignment to a review.
In the rest of this paper, we overview the previous
work in this research area in Section 2. We describe
the corpus used in the experiments in Section 3. In
Section 4 we present various learning algorithms we
experimented with. Section 5 explains our experi-
mental setup, while in Section 6 we provide analy-
sis of our experimental results. Section 7 presents
details of modeling and exploiting interdependence
among aspect ratings to boost the predictive perfor-
mance. Finally, we describe the future work in Sec-
tion 8 and report the concluding remarks in Section
9.
2 Related work
Previous work in sentiment analysis (Turney, 2002;
Pang et al, 2002; Dave et al, 2003; Yu and Hatzivas-
siloglou, 2003; Pang and Lee, 2004; Yi and Niblack,
2005; Carenini et al, 2006) used different informa-
tion extraction and supervised classification meth-
ods to detect document opinion polarity (positive vs.
negative).
By conducting a limited experiment with two sub-
jects, Pang and Lee (2005) demonstrated that hu-
mans can discern more grades of positive or neg-
ative judgments by accurately detecting small dif-
ferences in rating scores by just looking at review
text. In a five?star schema, for instance, the subjects
were able to perfectly distinguish rating differences
of three notches or 1.5 stars and correctly perceive
differences of one star with an average of 83% accu-
racy. This insight confirms that a five?star scale im-
proves the evaluative information and is perceived
with the right discriminative strength by the users.
Pang and Lee applied supervised and semi?
supervised classification techniques, in addition to
linear, -insensitive SVM regression methods, to
predict the overall ratings of movie reviews in three
and four?class star rating schemes. In the books
review domain, Okanohara and Tsujii (2005) show
a similar approach with comparable results. Both
these contributions consider only overall ratings,
which could be sufficient to describe sentiment for
movie and book reviews. Two recent endeavors,
Snyder and Barzilay (2007) for the restaurants do-
main, and Shimada and Endo (2008) for video
games reviews, exploit multi?aspect, multiple rat-
ing modeling. Snyder and Barzilay (2007) assume
inter?dependencies among the aspect ratings and
capture the relationship between the ratings via the
agreement relation. The agreement relation de-
scribes the likelihood that the user will express the
same rating for all the rated aspects. Interestingly,
Snyder and Barzilay (2007) show that modeling as-
pect rating dependencies helps to reduce the rank
loss by keeping in consideration the contributions of
the opinion strength of the single aspects referred
to in the review. They incorporated information
about the aspect rating dependencies in a regression
model and minimized the loss (overall grief ) dur-
ing decoding. Shimada and Endo (2008) exploits
a more traditional supervised machine learning ap-
proach where features such as word unigrams and
frequency counts are used to train classification and
regression models. As detailed in Section 4, our ap-
proach is similar to (Snyder and Barzilay, 2007) in
terms of review domain and algorithms, but we im-
prove on their performances by optimizing classifi-
cation predictions.
3 Reviews corpus
Labeled data containing textual reviews and aspect
ratings are rarely available. For this work, reviews
were mined from the we8there.com websites
around the end of 2008. we8there.com is one
of the few websites, where, besides textual reviews,
numerical ratings for different aspects of restaurants
are also provided. Aspects used for rating on this
site are: food, service, atmosphere, value and over-
all experience. Ratings are given on a scale from 1
37
to 5; for example, reviewers posting opinions were
asked to rank their overall experience by the follow-
ing prompt: ?On a scale of 1 (poor) to 5 (excel-
lent), please rate your dining experience?, and then
enter a textual description by the prompt: ?Please
describe your experience (30 words minimum)?. At
the time of mining, this site had reviews of about
3,800 restaurants with an average of two reviews
per restaurant containing around eight sentences per
review. A more detailed description is reported in
Table 1. Table 2 shows review ratings distribution
over the aspects. Rating distributions are evidently
skewed toward high ratings with 70% or more re-
views appraised as excellent (rank 5) or above aver-
age (rank 4).
Restaurants 3,866
Reviewers 4,660
Reviews 6,823
Average reviews per restaurant 1.76
Number of sentences 58,031
Average sentences per review 8.51
Table 1: Restaurant review corpus
Rating 1 2 3 4 5
Atmosphere 6.96 7.81 14.36 23.70 47.18
Food 8.24 6.72 9.86 18.53 56.65
Value 9.37 7.57 13.61 23.27 46.18
Service 11.83 6.12 11.91 22.00 48.14
Overall 10.48 8.19 10.17 20.47 50.69
Table 2: Restaurant review ratings distribution per aspect
4 Learning algorithms
In this section we review machine learning ap-
proaches that can predict ordinal ratings from textual
data. The goal is ordinal regression, which differs
from traditional numeric regression because the tar-
gets belong to a discrete space, but also differs from
classification as one wants to minimize the rank loss
rather than the classification error. The rank loss is
the average difference between actual and predicted
ratings and is defined as
RankLoss =
1
N
N?
i
(|rai ? rpi |)
where rai and rpi are actual and predicted ratings
respectively for the instance i, and N is the number
of considered reviews. There are several possible
approaches to such a regression problem.
1. The most obvious approach is numeric regres-
sion. It is implemented with a neural network
trained using the back?propagation algorithm.
2. Ordinal regression can also be implemented
with multiple thresholds (r ? 1 thresholds are
used to split r ranks). This is implemented
with a Perceptron based ranking model called
PRank (Crammer and Singer, 2001).
3. Since rating aspects with values 1, 2, 3, 4 and
5 is an ordinal regression problem it can also
be interpreted as a classification problem, with
one class per possible rank. In this interpreta-
tion, ordering information is not directly used
to help classification. Our implementation uses
binary one-vs-all Maximum Entropy (MaxEnt)
classifiers. We will see that this very simple
approach can be extended to handle aspect in-
terdependency, as presented in section 7.
In order to provide us with a broad range of rating
prediction strategies, we experimented with a nu-
merical regression technique viz. neural network, an
ordinal regression technique viz. PRank algorithm,
and a classification technique viz. MaxEnt classi-
fiers. Their implementations are straightforward and
the run?time highly efficient. After selecting a strat-
egy from the previous list, one could consider more
advanced algorithms described in Section 8.
5 Experimental setup
To predict aspect ratings of restaurants from their
textual reviews we used the reviews mined from the
we8there.com website to train different regres-
sion and classification models as outlined in Sec-
tion 4. In each of our experiments, we randomly
partitioned the data into 90% for training and 10%
for testing. This ensures that the distributions in
training and test data are identical. All the results
quoted in this paper are averages of 10?fold cross?
validation over 6,823 review examples. We con-
ducted repeatedly the same experiment on 10 differ-
ent training/test partitions and computed the average
rank loss over all the test partitions.
38
Figure 1 illustrates the training process where
each aspect is described by a separate predictive
model.
Figure 1: Predictive model training
We introduce the following notation that will be
helpful in further discussion. There are m aspects.
For our data m is 5. Each aspect can have an inte-
ger rating from 1 to k. Once again, for our data k
is 5. Each review text document t can have ratings
r, which is a vector of m integers ranging 1 to k
(bold faced letters indicate vectors). Using the train-
ing data (t1, r1)..(ti, ri)..(tn, rn) we train m rating
predictors Rj(ti), one for each aspect j. Given text
ti predictor Rj outputs the most likely rating l for
the aspect j. In these experiments, we treated aspect
rating predictors as independent of each other. For
each rated aspect, predictor models were trained in-
dependently and were used independently to predict
ratings for each aspect.
5.1 Feature Selection
We experimented with different combinations of
features, including word unigrams, bigrams, word
chunks, and parts?of?speech (POS) chunks. The as-
sumption is that bag?of?unigrams capture the ba-
sic word statistic and that bigrams take into account
some limited word context. POS chunks and word
chunks discriminate the use of words in the con-
text (e.g., a simple form word sense disambigua-
tion) and, at the same time, aggregate co?occurring
words (e.g., collocations), such as saute?ed onions,
buffalo burger, etc.
Most of the web?based reviews do not usually
provide fine?grained aspect ratings of products or
services, however, they often give an overall rating
evaluation. We therefore also experimented with the
overall rating as an input feature to predict the more
specific aspect ratings. Results of our experiments
are shown in Table 3.
Aspects Uni- Bi- Word Word Uni
gram gram Chunks Chunks gram
POS Overall
Chunks Rating
Atmosphere 0.740 0.763 0.789 0.783 0.527
Food 0.567 0.571 0.596 0.588 0.311
Value 0.703 0.725 0.751 0.743 0.406
Service 0.627 0.640 0.651 0.653 0.377
overall 0.548 0.559 0.577 0.583
Average 0.637 0.652 0.673 0.670 0.405
Table 3: Average ranking losses using MaxEnt classifier
with different feature sets
Review sentences
<s>Poor service made the lunch unpleasant.</s>
<s>The staff was unapologetic about their mistakes they
just didn?t seem to care.</s>
<s>For example the buffalo burger I ordered with sauteed
onions and fries initially was served without either.</s>
<s> The waitress said she?d bring out the onions but had
I waited for them before eating the burger the meat would
have been cold.</s>
<s>Other examples of the poor service were that the
waitress forgot to bring out my soup when she brought out
my friend?s salad and we had to repeatedly ask to get our
water glasses refilled.</s>
<s> When asked how our meal was I did politely mention my
dissatisfaction with the service but the staff person?s
response was silence not even a simple I m sorry.</s>
<s>I won?t return. </s>
Word Chunks
poor service made lunch unpleasant
staff unapologetic mistakes n?t care
example buffalo burger ordered sauteed onions fries served
waitress said bring onions waited eating burger meat cold
other examples poor service waitress forgot bring
soup brought friend salad repeatedly ask to get water
glasses refilled
asked meal politely mention dissatisfaction service
staff person response silence not simple sorry
n?t return
Parts-of-speech Chunks
NNP NN VBD NN JJ
NN JJ NNS RB VB
NN NN NN VBD NN NNS NNS VBN
NN VBD VB NNS VBD VBG NN NN JJ
JJ NNS JJ NN NN NN VB NN VBD NN NN RB VB TO VB NN VBZ VBN
VBD NN RB VB NN NN NN NN NN NN RB JJ JJ
RB VB
Table 4: Example of reviews and extracted word chunks
Unigram and bigram features refer to unigram
words and bigram words occurring more than 3
times in the training corpus. Word chunks are ob-
tained by only processing Noun (NP), Verb (VP) and
Adjective (ADJP) phrases in the review text. We re-
moved modals and auxiliary verbs form VPs, pro-
nouns from NPs and we broke the chunks containing
conjunctions. Table 4 shows an example of extracted
word and parts?of?speech chunks from review text.
As can be seen, word chunks largely keep the infor-
mation bearing chunks phrases and remove the rest.
Parts?of?speech chunks are simply parts?of?speech
39
of word chunks.
In spite of richness of word and parts-of-speech,
chunks models using word unigrams perform the
best. We can attribute this to the data sparseness,
never?the?less, this results is in line with the find-
ings in Pang et al (2002). Last column of Table 3
clearly shows that use of overall rating as input fea-
ture significantly improves the performance. Clearly
this validates the intuition that aspect ratings are
highly co?related with overall ratings.
For the remaining experiments, we used only the
unigram words as features of the review text. Since
overall ratings given by reviewers may contain their
biases and since they may not always be available,
we did not use them as input features. Our hope
is that even though we train the predictors using re-
viewers provided aspect ratings, learned models will
be able to predict aspect ratings that depend only on
the review text and not on reviewer?s biases.
5.2 Results
Table 5 shows the results of our evaluation. Each
row in this table reports average rank loss of four
different models for each aspect. The baseline rank
loss is computed by setting the predicted rank for all
test examples to 5, as it is the most frequently occur-
ring rank in the training data (see also Table 2). As
shown in Table 5, the average baseline rank loss is
greater than one. The third column shows the results
from the neural network?based numeric regression.
The fourth column corresponds to the Perceptron?
based PRank algorithm. The MaxEnt classification
results appear in the last column. For these results,
we also detail the standard deviation over the 10
cross?validation trials.
Aspects Base- Back- Percep- MaxEnt
line Prop. tron
Atmosphere 1.036 0.772 0.930 0.740 ? 0.022
Food 0.912 0.618 0.739 0.567? 0.033
Value 1.114 0.740 0.867 0.703? 0.028
Service 1.116 0.708 0.851 0.627? 0.033
Overall 1.077 0.602 0.756 0.548? 0.026
Average 1.053 0.694 0.833 0.637? 0.020
Table 5: Average ranking losses using different predictive
models
6 Analysis
As can be seen in table Table 5, Atmosphere and
Value are the worst performers. This is caused by
the missing textual support for these aspects in the
training data. Using manual examination of small
number of examples, we found that only 62% of
user given ratings have supporting text for ratings
of these aspects in the reviews.
For example, in Figure 2 the first review clearly
expresses opinions about food, service and atmo-
sphere (under appall of cigarette smoke), but there is
no evidence about value which is ranked three, two
notches above the other aspects. Similarly, the sec-
ond review is all about food without any reference
to service rated two notches above the other aspects,
or atmosphere or value.
Because of this reason, we do not expect any pre-
dictive model to do much better than 62% accuracy.
Manual examination of a small number of examples
also showed that 55% of ratings predicted by Max-
Ent models are supported by the review text. This is
89% of 62% (a rough upper bound) and can be con-
sidered satisfactory given small data set and differ-
ences among reviewers rating preference. One way
to boost the predictive performance would be to first
determine if there is a textual support for an aspect
rating, and use only the supported aspect ratings for
training and evaluation of the models. This however,
will require labeled data that we tried to avoid in this
work.
Figure 2: Example of ratings with partial support in the
text review
To our surprise, MaxEnt classification, although it
minimizes a classification error, performs best even
40
when evaluated using rank loss. As can be noticed,
the performance difference over the second best ap-
proach (back?propagation) usually exceeds the stan-
dard deviation.
MaxEnt results are also comparable to those pre-
sented in Snyder and Barzilay (2007) using the
Good Grief algorithm. Snyder and Barzilay (2007)
also used data from the we8there.com website.
While we are using the same data source, note
the following differences: (i) Snyder and Barzilay
(2007) used only 4,488 reviews as opposed to the
6,823 reviews used in our work; (ii) our results are
averaged over a 10 fold cross validation. As shown
with the baseline results reported in Table 6, the im-
pact on performance that can be attributed to these
differences is small. The most significant number,
which should minimize the impact of data discrep-
ancy, is the improvement over baseline (labeled as
?gain over baseline? in Table 6). In that respect,
our MaxEnt classification?based approach outper-
forms Good Grief for every aspect. Note also that,
while we trained 5 independent predictors (one for
each aspect) using only word unigrams as features,
the Good Grief algorithm additionally modeled the
agreements among aspect ratings and used the pres-
ence/absence of opposing polarity words in reviews
as additional features.
Our results Snyder and Barzilay
(2007)
Aspects Base- Max Gain Base- Good Gain
line Ent. over line Grief over
Base- Base-
line line
Atmosphere 1.039 0.740 0.299 1.044 0.774 0.270
Food 0.912 0.567 0.344 0.848 0.534 0.314
Value 1.114 0.703 0.411 1.030 0.644 0.386
Service 1.116 0.627 0.489 1.056 0.622 0.434
Overall 1.077 0.548 0.529 1.028 0.632 0.396
Table 6: Comparison of rank loss obtained from MaxEnt
classification and those reported in Snyder and Barzilay
(2007)
7 Modeling interdependence among aspect
ratings
Inspired by these observations, we also trained Max-
Ent classifiers to predict pair?wise absolute differ-
ences in aspect ratings. Since the difference in rat-
ings of any two aspects can only be 0,1,2,3 or 4,
there are 5 classes to predict. For each test exam-
ple, MaxEnt classifiers output the posterior proba-
bility to observe a class given an input example. In
our approach, we use these probabilities to compute
the best joint assignment of ratings to all aspects.
More specifically, in our modified algorithm we use
2 types of classifiers.
? Rating predictors - Given the text ti, our clas-
sifiers Rj(ti) output vectors pi consisting of
probabilities pil for text ti having a rating l for
the aspect j.
? Difference predictors - These correspond to
classifiers Dj,k(ti) which output vectors pij,k .
Elements of these vectors are the probabilities
that the difference between ratings of aspects j
and k is 0,1,2,3 and 4, respectively. While j
ranges from 1 to m, k ranges from 1 to j ? 1.
Thus, we trained a total of m(m ? 1)/2 = 10
difference predictors.
To predict aspect ratings for a given review text
ti we use both rating predictors and difference pre-
dictors and generate output probabilities. We then
select the most likely values of ri for text ti that sat-
isfies the probabilistic constraints generated by the
predictors. More specifically:
ri = argmax
r?R
m?
j=1
log(pirj ) +
m?
j=1
j?
k=1
log(p
ij,k
|rj?rk|
)
R is the set of all possible ratings assignments to
all aspects. In our case it contains 55 (3,125) tuples.
tuples in our case. Like Snyder and Barzilay (2007),
we also experimented with additional features in-
dicating presence of positive and negative polarity
words in the review text. Besides unigrams in the
review text, we also used 3 features: the counts of
positive and negative polarity words and their dif-
ferences. Polarity labels are obtained from a dictio-
nary of about 700 words. This dictionary was cre-
ated by first collecting words used as adjectives in a
corpus of un?related review text. We then retained
only those words in the dictionary that, in a context
free manner generally conveyed positive or negative
evaluation of any object, event or situation. Some
41
examples of negative words are awful, bad, bor-
ing, crude, disappointing, horrible, worst, worth-
less, yucky and some examples of positive words
are amazing, beautiful, delightful, good, impecca-
ble, lovable, marvelous, pleasant, recommendable,
sophisticated, superb, wonderful, wow. Table 7 first
shows gains obtained from using difference predic-
tors, and then gains from using polarity word fea-
tures in addition to these difference predictors.
Aspects MaxEnt + Difference + Polarity
predictor features
Atmosphere 0.740 0.718 0.707
Food 0.567 0.552 0.547
Value 0.703 0.695 0.685
Service 0.627 0.627 0.617
Overall 0.548 0.547 0.528
Average 0.637 0.628 0.617
Table 7: Improved rank loss obtained by using difference
predictors and polarity word features
8 Future Work
We have presented 3 algorithms chosen for their
simplicity of implementation and run time effi-
ciency. The results suggest that our classification?
based approach performs better than numeric or or-
dinal regression approaches. Our next step is to ver-
ify these results with the more advanced algorithms
outlined below.
1. For many numeric regression problems,
(boosted) classification trees have shown good
performance.
2. Several multi?threshold implementations of
Support Vector Ordinal Regression are com-
pared in Chu and Keerthi (2005). While they
are more principled than the Perceptron?based
PRank, their implementation is significantly
more complex. A simpler approach that per-
forms regression using a single classifier ex-
tracts extended examples from the original ex-
amples (Li and Lin, 2007).
3. Among classification?based approaches,
nested binary classifiers have been pro-
posed (Frank and Hall, 2001) to take into
account the ordering information, but the
prediction procedure based on classifier score
difference is ad?hoc.
9 Conclusions
Textual reviews for different products and services
are abundant. Still, when trying to make a buy deci-
sion, getting sufficient and reliable information can
be a daunting task. In this work, instead of a sin-
gle overall rating we focus on providing ratings for
multiple aspects of the product/service. Since most
textual reviews are rarely accompanied by multiple
aspect ratings, such ratings must be deduced from
predictive models. Several authors in the past have
studied this problem using both classification and re-
gression models. In this work we show that even
though the aspect rating problem seems like a re-
gression problem, maximum entropy classification
models perform the best. Results also show a strong
inter?dependence in the way users rate different as-
pects.
Acknowledgments
We thank Remi Zajac and his team for their support.
References
Carenini, Giuseppe, Raymond T. Ng, and Adam
Pauls. 2006. Interactive multimedia summaries of
evaluative text. In Proceedings of Intelligent User
Interfaces (IUI). ACM Press, pages 124?131.
Chu, Wei and S. Sathiya Keerthi. 2005. New ap-
proaches to support vector ordinal regression. In
Proceedings of the 22nd International Conference
on Machine Learning. Bonn, Germany, pages
145?152.
Crammer, Koby and Yoram Singer. 2001. Prank-
ing with ranking. In Advances in Neural Infor-
mation Processing Systems 14. MIT Press, pages
641?647.
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews. In WWW ?03: Proceedings of the 12th
International Conference on World Wide Web.
ACM, New York, NY, USA, pages 519?528.
Frank, Eibe and Mark Hall. 2001. A simple ap-
proach to ordinal classification. In Proceedings
42
of the Twelfth European Conference on Machine
Learning. Springer-Verlag, Berlin, pages 145?
156.
Hu, Minqing and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD ?04: Pro-
ceedings of the 10th ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining. ACM, New York, NY, USA, pages 168?
177.
Li, Ling and Hsuan-Tien Lin. 2007. Ordinal re-
gression by extended binary classification. In
B. Scho?lkopf, J. C. Platt, and T. Hofmann, edi-
tors, Advances in Neural Information Processing
Systems 19. MIT Press, pages 865?872.
Lu, Yue, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In WWW ?09: Proceedings of the 18th
International Conference on World Wide Web.
ACM, New York, NY, USA, pages 131?140.
Okanohara, Daisuke and Jun-ichi Tsujii. 2005. As-
signing polarity scores to reviews using machine
learning techniques. In Robert Dale, Kam-Fai
Wong, Jian Su, and Oi Yee Kwong, editors, IJC-
NLP. Springer, volume 3651 of Lecture Notes in
Computer Science, pages 314?325.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In Pro-
ceedings of the Association for Computational
Linguistics (ACL). pages 271?278.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment catego-
rization with respect to rating scales. In Proceed-
ings of the Association for Computational Lin-
guistics (ACL). pages 115?124.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP). pages 79?86.
Popescu, Ana-Maria and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language Process-
ing (HLT/EMNLP).
Shimada, Kazutaka and Tsutomu Endo. 2008. See-
ing several stars: A rating inference task for a doc-
ument containing several evaluation criteria. In
Advances in Knowledge Discovery and Data Min-
ing, 12th Pacific-Asia Conference, PAKDD 2008.
Springer, Osaka, Japan, volume 5012 of Lecture
Notes in Computer Science, pages 1006?1014.
Snyder, Benjamin and Regina Barzilay. 2007. Mul-
tiple aspect ranking using the Good Grief algo-
rithm. In Proceedings of the Joint Human Lan-
guage Technology/North American Chapter of the
ACL Conference (HLT-NAACL). pages 300?307.
Turney, Peter. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proceedings
of the Association for Computational Linguistics
(ACL). pages 417?424.
Yi, Jeonghee and Wayne Niblack. 2005. Senti-
ment mining in WebFountain. In Proceedings of
the International Conference on Data Engineer-
ing (ICDE).
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
43
