A Method of Measuring Term Representativeness 
- Baseline Method Using Co-occurrence Distribution - 
Toru Hisamitsu,* Yoshiki Niwa,* and Jun-ichi Tsujii * 
$ Central Research Laboratory, Hitachi, Ltd. 
Akanuma 2520, Hatoyama, Saitama 350-0395, Japan 
{hisamitu, yniwa} @harl.hitachi.co.jp 
Abstract 
This paper introduces a scheme, which we call 
the baseline method, to define a measure of term 
representativeness and measures defined by using 
the scheme. The representativeness of a term is 
measured by a normalized characteristic value 
defined for a set of all documents that contain the 
term. Normalization is done by comparing the 
original characteristic value with the 
characteristic value defined for a randomly 
chosen document set of the same size. The latter 
value is estimated by a baseline function obtained 
by random sampling and logarithmic linear 
approximation. We found that the distance 
between the word distribution in a document set 
and the word distribution in a whole corpus is an 
effective characteristic value to use for the 
baseline method. Measures defined by the 
baseline method have several advantages 
including that they can be used to compare the 
representativeness of two terms with very 
different frequencies, and that they have 
well-defined threshold values of being 
representative. In addition, the baseline function 
for a corpus is robust against differences in 
corpora; that is, it can be used for normalization 
in a different corpus that has a different size or is 
in a different domain. 
1 Introduction 
Measuring the representativeness (i.e., the 
informativeness or domain specificity) of a term ~ is 
essential to various tasks in natural language 
processing (NLP) and information retrieval (IR). It 
is particularly crucial when applied to an IR 
interface to help a user find informative terms. For 
instance, when the number of retrieved ocuments i
intractably large, an overview of representative 
words in the documents i needed to understand the 
contents. To enable this, an IR system, called 
DualNAVI, that has two navigation windows where 
one displays a graph of representative words in the 
retrieved ocuments, was developed (Nishioka et al 
1997). This window helps users grasp the contents 
of retrieved ocuments, but it also exposes problems 
concerning existing representativeness measures. 
Figure l shows an example of a graph for the 
query '~Y-'~'~(-~ (electronic money), with Nihon 
A term is a word or a word sequence.  
{ Graduate School of Science, the University of Tokyo 
7-3-I Hongo, Bunkyo-ku, Tokyo 113-8654, Japan 
tsujii@is.s.u-tokyo.ac.j p 
Keizai Sl#mbun (a financial newspaper) 1996 as the 
corpus. Frequently appearing words are displayed in 
the upper part of the window, and words are selected 
by a tf-idf-like measure (Niwa et al 1997). Typical 
non-representative words are filtered out by using a 
stop-word list. 
me n e y . ~ v ~ ~ ~ \ ] ~ - - - - - - ~   
electronic ---- /tmu,.~ \] ~ . / / ' f -~year  
~ / \  ~L~I2~-- month 
read c pher 
Figure 1 
A topic word graph when the query is 
N~-e  ~---(etectronic money). 
One problem is the difficulty of suppressing 
uninformative words such as ~V- (year), -- (one), 
and )\] (month) because classical measures, uch as 
tf-idf are too sensitive to word frequency and no 
established method to automatically construct a 
stop-word list has bcen developed. 
Another problem is that the difference in the 
representativeness of words is not sufficiently 
\[In ~j indicated. In the exarnple above, highlighting " .... 
(cipher) over less representative words such as ~'U~ 
k_ 5 (read) would be useful. Most classical 
measures based on only term frequency and 
document frequency cannot overcome this problem. 
To define a more elaborate measure, atternpts 
to incorporate more precise co-occurrence 
information have been made. Caraballo et al (1999) 
tried to define a measure for "specificity" of a noun 
by using co-occurrence intbrmation of a noun, but it 
was not very successful in the sense that the 
measure did not particularly outperformed the term 
frequency. 
Hisamitsu et al (1999) developed a measure 
of the representativeness of a term by using 
co-occurrence information and a normalization 
320 
technique. Fhe measure is based on the distance 
between tile word distribution in the documenls 
containing a term and the word distribution ill the 
whole corpus. Their measure overcomes previously 
mentioned problems and preliminary experiments 
showed that this measure worked better than 
existing measures in picking out 
representative/non-representative terms. Since tile 
normalizatio11 technique plays a crucial part of 
constructing tile nleasure, issl_lcs related to the 
normalization need more study. 
In this paper we review Hisamitsu's measure 
and introduce a generic scheme - which we call the 
baseline method for convenience - that can be used 
to define wu'ious measures including the above. A 
characteristic value of all documents containing a 
term Y is normalized by using a baseline fimction 
that estimates the characteristic value of a randomly 
chosen document set of the slune size. Tile 
normalized value is then used to measure tile 
representativeness of  the term 77. A measure defined 
by the baseline.-method has several advantages 
compared to classical measures. 
We compare four measures (two classical 
ones and two newly defined ones) from w.trious 
viewpoints, and show the superiority of the measure 
based on the normalized distance between two word 
distributions. Another important finding is that the 
baseline function is substantially portable, that is, 
one defined for a corpus can be used for a different 
corpus even it" the two corpora Mvc considerably 
different sizes or arc in different domains. 
2. E,isting measures of representative~kess 
2.1 Overview 
Various methods for mea:.;uring the inforlnativcness 
or domain specificity of a word have been proposed 
in the donmins of IR and term extraction in NLP 
(see the survey paper by Kageura 1996). Ill 
characterizing a term, Kagcura introduced the 
concepts of "unithood" and "termhood": unithood is 
"the degree of strength or stability of syntagnmtic 
combinations or collocations," and termhood is "tile 
degree to which a linguistic unit is related to (or 
more straightR)rwardly, represents) domain-specific 
concepts." Kageura's termhood is therefore what we 
call representativeness here. 
Representativeness lneasurcs were first 
introduced in till IR domain for determining 
indexing words. The simplest measure is calculated 
fi'om only word frequency within a document, For 
example, tile weight 1 o of word w~ in document d/ is 
defined by 
./~./ 
/ r . .  _ ___  , 
~ >Z/< .It-i 
wherc./ii is tilt: frequency of word wi in document (\]i 
(Sparck-Jolms 1973, Noreauh ct al. 1977). More 
elaborate measures for tcrmhood combine word 
frequency within a document and word occurrence 
over a whole corpus. For instance, (/:/4/; the most 
comlnonly used measure, was originally defined its 
N IoIcl\[ \[,i = ./;, x log( - - ) ,  
where iV, and N,,,,~ are, respectively, tile number of  
documents containing word wg and the total number 
of documents (Salton et al 1973). There are a 
wlriety or" definitions of ?idJl but its basic feature is 
that a word appearing more flequently in fewer 
documents i assigned a higher value. If documents 
are categorized beforehand, we can use a more 
sophisticated measure based on the X-' test of the 
hypothesis that an occurrence of" the target word is 
independent of categories (Nagao et al 1976). 
Research on automatic term extraction in 
NLP domains has led to several measures for 
weighting terms mainly by considering the unithood 
of a word sequence. For instance, mutual 
information (Church ct al. 1990) and the 
log-likelihood (Dunning 1993) methods for 
extracting word bigrams have been widely used. 
Other measures for calculating the unithood of 
n-grains have also been proposed (Frantzi et al 
1996, Nakagawa et al 1998, Kita et al 1994). 
2.2 Problems 
Existing measures uffer from at least one of the 
following problems: 
(1) Classical measures sucll as t/-idjare so sensitive 
to term frequencies that they fail to avoid very 
frequent non-informative words. 
(2) Methods using cross-category word distributions 
(such as the Z-' method) can be applied only if 
documents in a corpus are categorized. 
(3) Most lneasures in NLP domains cannot reat 
single word terms because they use the unithood 
strength of multiple words. 
The threshold wdue lbr being representative is 
dcfincd in all ad hoc manner. 
constructs 
(4) 
The scheme that we describe here 
measures that are free of these problems. 
3. Baseline method for defining 
representativeness measures 
3.1 Basic idea 
This subsection describes the method we developed 
for defining a measure of  term representativeness. 
Our basic idea is smmnarized by tile lhmous quote 
(Firth 1957) : 
"You shall k~ow a wo~zt l~y the coml)alT); ir 
Iwup.v." 
We interpreted this as the following working 
hypolhesis: 
321 
For any term T, if the term is 
representat ive ,  D(T), the seto fa l l  
documents  conta in ing  T, shou ld  have 
some character i s t i c  p roper ty  
compared  to the "average" 
To apply this hypothesis, we need to specify a 
measure to obtain some "property" of a document 
set and the concept of "average". Thus, we 
converted this hypothesis into the following 
procedure: 
Choose a measure  M character i z ing  
adocumentset .  For termT,  ca lcu la te  
M(D(T)), the va lue of the measure  
for D(T). Then compare M(D(T)) with  
B~(#D(T)), where #D(T)is the number  
of words  conta ined  in #D(T), and B,~ 
est imates  the va lue  of M(D) when D 
is a randomly  chosen document  set 
of s ize #D(T). 
Here, M measures the property and BM estinmtes the 
average. The size of a document set is defined as the 
number of words it contains. 
We tried two measures as M. One was the 
number of different words (referred to here as 
DIFFNUM) appearing in a document set. Teramoto 
conducted an experiment with a snmll corpus and 
reported that DIFFNUM was useful for flicking out 
important words (Teramoto et al 1999) under the 
hypothesis that the number of different words 
co-occurring with a topical (representative) word is 
snmllcr than that with a generic word. The other 
measure was the distance between the word 
distribution in D(T) and the word distribution in the 
whole corpus Do. The distance between the two 
distributions can be measured in various ways, and 
we used the log-likelihood ratio as in Hisalnitsu et al 
1999, and denote this rneasure as LLR. Figure 2 
plots (#D, M(D))s when M is DIFFNUM or LLR, 
where D varies over sets of randomly selected 
documents of various sizes from the articles in 
Nikkei-Shinbun 1996. 
For measure M, we define Rep(T, M), the 
representativeness of T, by normalizing M(D(T)) by 
BM(#D(T)). The next subsection describes the 
construction of By and the normalization. 
3.2 Base l ine  funct ion  and  normal i za t ion  
Using the case of LLR as an example, this 
subsection explains why nornmlization is necessary 
and describes the construction of a baseline 
function. 
Figure 3 superimposes coordinates {(#D(7), 
LLR(D(T))} s onto the graph of LLR where T varies 
2 With Teramoto's method, eight paranaeters must be ttmed to 
normalize D1FFNUM( D( T) ), but the details of how this was 
done were not disclosed. 
I000000 
100000 
10000 
I000 
100 
10 
I 
100 100000 100000000 
#D: Size of randomly chosen documents 
F igure  2 
Values of DIFFNUM and LLR for 
randomly chosen document set. 
II ~ ,~ ' '  i " " ~ . . . .  over -ytc pner), qi(year), )J (month), i~cJ~-ll~7~ 
(read), -- (one), j -  ~ (do), and ~}: i>~/(economy). 
Figure 3 shows that, for example, LLR(D(J-~)) is 
smaller than LLR(D( ~,~ }J5 )), which reflects our 
linguistic intuition that words co-occurring with 
"economy" are more biased than those with "do". 
However, LLR(DOI~-',3-)) is smaller than LLR(D(.?J/- 
I~6))  and smaller even than LLR(D@O-~)). This 
contradicts our linguistic intuition, and is why 
values of LLR are not dircctly used to compare the 
representativeness of terms. This phenomenon arises 
because LLR(D(~) generally increases as #\])(7) 
increases. We therefore need to use some form of 
normalization to offset this underlying tendency. 
We used a baseline function to normalize the 
values. In this case, Bu,(o) was designed so that it 
approximates the curve in Fig. 3. From the 
definition of the distance, it is obvious that Bu.t~(0) = 
Bu.R(#Do) = 0. At the limit when #1)(~--+ o% Bu.R(') 
becomes a monotonously increasing function. 
The curve could be approxinmted precisely 
through logarithmic linear approximation near (0, 0). 
~lb make an approximation, up to 300 documents are 
randomly sampled at a time. (Let each randomly 
chosen document set be denoted by D. The number 
of sampled ocuments are increased from one to 300, 
repeating each number up to five times.) Each (#D, 
LLR(D)) is converted to (log(#D), Iog(LLR(D))). 
The curve formulated by the (log(#D), log(LLR(D))) 
values, which is very close to a straight line, is 
further divided into nmltiple parts and is part-wise 
approximated by a linear function. For instance, in 
the interval I = {x \[ 10000 _<x < 15,000}, 
Iog(LLR(D)) could be approximated by 1.103 + 
1.023 x log(#D) with R e = 0.996. 
For LLR, we define Rep(T, LLR), the 
representativeness of T by normalizing LLR(D(7)) 
by Bu.R(#D(7)) asfollows: 
Rep(r, LLR) = 100 x (Iog(LLR(D(T))) _ 1). 
"log(Bu, (# D(T))) 
322 
For instance, when we used Nihon Keizai 
Shimbun 1996, The average of I OOx(log(LLR(D)) 
~log(BLue (#D)) - 1), Avr, was -0.00423 and the 
standard deviation, cs, was about 0.465 when D 
varies over randomly selected octuncnt sets. l';very 
observed wflue fell within Avs'4-4er and 99% ot' 
observed values fell within Avl?3cs. This hapfmlled 
in all corpora (7 orpora) we tested. Theretbrc, we 
can de:fine the threshold of being representative as, 
say, Aw" + 40. 
umoooo ~:} f ' i (economy)  . _ _  _ h.. .  J J~n ion lh )  
! ;~i'~;i/.Jl).~) ( read)  i 
i 
., (cipher) \ !! ~ , j ~  ! & (do) 
)~ 10000 
1000 
1 O0 1000 10000 100000 1000000 10000000 I \[ = {}S 
#1) and lid (T) 
Figure 3 
Baseline and sample word distribution 
3.3 Treatment of very frequent erms 
So \['ar we have been unable to treat extremely 
frequent terms, such as -~-~ (do). We therefore 
used random sampling to calculalc tile 1@1)(77 LLR) 
of a very li'cquent lerm T. II' the munbcr ot' 
documents in D(7) is larger than a threshold wdue N, 
which was calculated froln the average number of 
words contained in a document, N docnmcnts arc 
randomly chosen from D(2) (we used N = 150). This 
subset is denoted D(T) and Re/)(7; LLR) is delined 
by 100 x (log(LLR(D(7))) /log(BL~,Se (#1)(7))) -- 1). 
This is effcctivc because wc can use a 
well-approximated part of the baseline curve; it also 
reduccs thc amount of calctflation required. 
By using Rel)(77 LLR) detSned above, wc 
obtained Rel)(-'F g), LLR) = -0.573, Rel)(a')&TJ, llk 7~), 
LLR) = 4.08, and , * .... Re\])(llil-o, LLR) = 6.80, which 
reflect our linguistic intuition. 
3.4 Features of Rep(T, M) 
Rep(T, M) has the t bllowing advantages by virtue of 
its definition: 
(1) Its definition is mathematically clear. 
(2) It can compare high-frequency terms with low- 
ficqucncy terms. 
(3) The threshold value of being representative can 
be defined systematically. 
(4) It can be applied to n-gram terms for any n. 
4. Experiments 
4.1 Ewfluation of monograms 
Taldng topic-word selection for a navigation 
window for IR (see Fig. 1) into account, we 
cxamined the relation bctwecn the value of Rel)(7, 
M) and a manual classification of words 
(monograms) extracted from 158,000 articles 
(excluding special-styled non-sentential rticles such 
as company-personnel-aflhir articles) in the 1996 
issties of the Nildcei Shinbun. 
4.1.1 Preparation 
We randolnly chose 20,000 words from 86,000 
words having doculnent ficquencies larger than 2, 
thcn randomly chose 2,000 of them and classified 
these into thrce groups: class a (acceptable) words 
uscfill for the navigation window, class d (delete) 
words not usethl for the navigation window, ,and 
class u (uncertain) words whose usefulness in the 
navigation window was either neulral or difficult to 
judge. In the classification process, a judge used the 
DualNA VI system and examined the informativeness 
of each word as guidance. Classification into class d 
words was done conservatively because the 
consequences of removing informative words from 
lhc window are more serious than those of allowing 
useless words to appear. 
3hblc I shows part of the chtssification of thc 
2,000 words. Words marked "p" arc proper nouns. 
The difference between propcr nouns in class a and 
proper nouns in other classes is that the former arc 
wcllknown. Most words classified as "d" are very 
common verbs (such as-,J-~(do) and {J~s-~(have)), 
adverbs, demonstrative pronouns, conjunctions, and 
numbers. It is thereti)rc impossible to define a 
stop-word list by only using parts-of-spccch bccausc 
ahnost all parts-of speech appear in class d words. 
4.1.2 Measures used in tile experiments 
To evaluate the effectiveness of several lneasures, 
we compared the ability of each measure to gather 
(avoid) representative (non-representative) terms. 
We randomly sorted thc 20,000 words and then 
compared the results with the restllts of sorting by 
other criteria: Rep(., LLR), Rep(., DIFFNUM), (f 
(tern~ liequency), and tfid.fi The comparison was 
done by nsing the accunmlated number of words 
marked by a specified class that appeared in the first 
N (1 _< N_< 2,000) words. The definition we used for 
tj- idf was 
Nlota\[ .t/- ira= 4771775 ?log N(r ' 
where T is a term, TF(7) is the term frequency of 7, 
Nt,,,<,l is the number of total documents, and N(7) is 
the number of documents that contain 7: 
4.1.3 Results 
Figure 4 compares, for all the sorting criteria, tile 
323 
accumulated number of words marked "a". The total 
number of class a words was 911. Rep( o, LLR) 
clearly outperformed the other measures. Although 
Rep(., DIFFNUM) outperformed .tfand tf-idf up to 
about the first 9,000 monograms, it otherwise 
under-performed them. If we use the threslaold value 
of Rep(., LLR), from the first word to the 1,511th 
word is considered representative. In this case, the 
recall and precision of the 1,511 words against all 
class a words were 85% and 50%, respectively. 
When using tf-idf the recall and precision of the 
first 1,511 words against all class a words were 79% 
and 47%, respectively (note that tJ'-idfdoes not have 
a clear threshold value, though). 
Although the degree of out-performance by
Rep(., LLR) is not seemingly large, this is a 
promising result because it has been pointed out that, 
in the related domains of term extraction, existing 
measures hardly outperform even the use of 
frequency (for example, Daille et al 1994, Caraballo 
et al 1999) when we use this type of comparison 
based on the accumulated numbers. 
Figure 5 compares, for all the sorting criteria, 
the accumulated number of words marked by d (454 
in total), in this case, fewer the number of words is 
better. The difference is far clearer in this case: 
Rep(., LLR) obviously outperformed the other 
measures. In contrast, tfidJ and frequency barely 
outperformed random sorting. Rep(., DIFFNUM) 
outperformed tfand (f-idfuntil about the first 3,000 
monograms, but under-performed otherwise. 
Figure 6 compares, for all the sorting criteria, 
the accumulated number of words marked ap 
(acceptable proper nouns, 216 in total ). Comparing 
this figure with Fig. 4, we see that the 
out-performance ofRep(., LLR) is more pronounced. 
Also, Rep(., DIFFNUM) globally outperformed tf
and tf-idf while the performance of( land tf-idfwcre 
nearly the same or even worse than with random 
sorting. 
IOOO 
900 
~00 
700 
600 
500 
400 
300 
10 
0 5000 10000 15000 20000 
Order 
? random ? Rep(., LLR) a Rep(., DIFFNUM) ~ t f id f  * tf 
Figure 4 
Sorting results on class a words 
350 
300 
Z 
250 
200 
.< 
150 
100 
~g / 
L 
0 5000 10000 15000 20000 
Order 
? random ~ Rep(., LLR) a Rcp(., DIFFNUM) ~ tt: idf ? tf  
Figure 5 
Sorting results on class d words 
p) 
a~ 150 
Z 
100 
.< 
o j~,,-- 
o 5(1{)0 I0000 15000 20000 
Order 
? random ~ Rep(., LLR) z~ Rep(., I)IFFNUM) ~ tl=id\[" ? tf 
Figure 6 
Sorting results on class ap words 
qhble 1 
Examples of the classified words 
chtss a class u class d 
~" 2 :L ~Y-2"~ 5/ 1..'<-- ~ O'/~s("g) (chilly) )kT'-I'i)J (83,000,000) 
(amusement park) ~'\['J?J2 (depressed) ~)<?2 (greatly) 
g)3~)~ (threlerfingletter) ;~'~'1 t (lshigami) p T-l'flJqM-/-: (1, t46) 
/ '7"4) 'OM- - JP  (fircwall) ~}5',;: (Shigeyuki) p ~J-~<~ (all) 
"\[~l'~t~', (antique) li~?;i,'2:t, '??(misdirected) ~" L L (not... in the least) 
7" \]- ~ ; / / /  (Atlanta) p ~}J(~A~ (agility) 
In the experiments, proper nouns generally 
have a high Rep-value, and some have particularly 
high scores. Proper nouns having particularly high 
scores are, for instance, the names ofsumo wrestlers 
or horses. This is because they appear in articles 
with special formats uch as sports reports. 
We attribute the difference of the performance 
between Rep(., LLR) and RED(., DIFFNUM) to the 
quantity of information used. Obviously information 
on the distribution of words in a document is more 
comprehensive than that on the number of different 
words. This encourages us to try other measures of 
document properties that incorporate ven more 
precise information. 
324 
4.2 Picking out fl'equeni non-representative 
monograms 
When we concentrate on the nlost fi-equent erms, 
Re/)(., DIFFNUM) outperfomlcd Rep(., LLR) in the 
following sense. We marked "clearly 
non-representative terms" in the 2,000 most frequent 
monograms, then counted the number of marked 
terms that were assigned Rt7)-values maller than 
the threshold value of a specified representativeness 
u lcasurc .  
The total number of checked terms was 563, 
and 409 of them are identified as non-representative 
by Rep(', LER). On the other hand, Rep( ?, 
DIFFNUM) identified 453 terms as 
non--representative. 
4.3 Rank correlation between measures 
We investigated the rank-correlation of the sorting 
results for the 20,000 terms used in the experiments 
described in subsection 4.1. Rank correlation was 
measured by Spearman's method and Kendall's 
method (see Appendix) using 2,000 terms randomly 
selected from the 20,000 terms. Table 2 shows the 
correlation between Rep(,, LLR) and other measures. 
It is interesting that the ranking by Rep(., LLR) and 
that by Rep(., DIFFNUM) had a very low 
correlation, even lower than with (f or (fidf This 
indicates that a combination of Rep(., LLR) and 
Rep(,, DIFFNUM) should provide a strong 
discriminative ability in term classification; this 
possibility deserves further investigation. 
Table 2 
Two types of Rank correlation between 
term-rankings byRep(., LLR) and other measures. 
Rep(., DIFFNUM) t/=ic(f tf 
Spearman -0.00792 0.202 0.198 
Kenda l l  -0 .0646 0.161 0.153 
4.4 Portability of baseline functions 
We examined the robustness of thc baseline 
fimctions; that is, whether a baseline function 
defined from a corpus can be used for normalization 
in a different corpus. This was investigated by using 
Re/)(., LLR) with seven different corpora. Seven 
baseline functions were defined from seven corpora, 
then were used for normalization for defining Rep(., 
LLR) in the corpus used in the experiments 
described in subesction 4.1. The per%rmance of the 
Re/)(,, LLR)s defined using the difl'erent baseline 
flmctions was compared in the same way as in the 
snbsection 4. l. The seven corpora used to construct 
baseline fhnctions were as follows: 
NK96-ORG: 15,8000 articles used in the experiments in 4.1 
NK96-50000:50,000 randomly selected articles from Ihe whole 
corpus N K96 (206,803 articles of Nikkei-shinhun 1996) 
N K96-100000: I 0(},000 randomly selected articles fn}m N K96 
NK96-200000: 2{}0,00(} randomly selcctcd articles fiom NK96 
NK98-1580{}0:158,0{}(} randomly selecled articles from articles in 
Nikkei-xhinhun 1998 
N('- 158000:158,{}00 randomly selected abstracts of academic papers 
I\]'Olll NACSIS corptl:.; (Kando ct al. 1999) 
NC-:\LI.: all abstracts (333,003 abstracts) in the NACSIS coq)us. 
Statistics on their content words are shown in Table 3. 
Table 3 
Corpora and statistics on their content words 
~ ~ .  NK96-OP, G NK96-soooo NKq6-1ooooo NK96-2ooooo 
fi o | ' Iota l  words  42,555,095 13,49S,244 26 ,934,068 53 .816,407 
;: ofdillbrent words 210,572 127,852 172.914 233,668 
~ ~  NK98-158000 NC-158000 NC-A I . I .  
# ,af total v,'ords 39,762, 127 30,770,682 64,806,627 
# of difliarent words 196,261 231,769 350.991 
Figure 7 compares, for all the baseline functions, the 
accumulated number of words marked "a" (see 
subsection 4.1). The pertbrmancc decreased only 
slightly when the baseline defned from NC-ALL 
was used. In other cases, the difl'erences was so 
small that they were almost invisible ill Fig. 7. The 
same results were obtained when using class d 
words and class ap words. 
tuoo 
9OO 
700 
-j 
5OO 
0 2000 40011 (dRRI XOOH I UO(lO 12tRR} 14000 160110 IROOt} 21111{11/ 
Order 
* random ~ NK96-OR( i  A NK96-5t}000 - NK96-100000 
c\] NK96-20{}000 * NK98-158{}(1{} + NC-158000 x NC-ALL  
Figure 7 
Sorting results on class a words 
We also examined the rank correlations 
between the ranking that resulted from each 
representativeness measure in the same way as 
described in subsection 4.2 (see Table 4). They were 
close to 100% except when combining the Kendall's 
method and NACSIS corpus baselines. 
Table 4 
Rank correlation between the measure defined by an 
NK96-ORG baseline and ones defined by other baselines 
(%) 
NK96-  NK96-  NK96-  NK9g-  
"~C- 1 5800C NC-A I . I .  500{}0 I.OOOO 2000{}0 158000 
Spcarmann 0.997 0.997 0.996 0.999 0.912 0.900 
Kendall 0.970 0.956 0.951 0.979 0.789 0.780 
These resnhs suggest hat a baseline function 
constructed from a corpus can be used to rank terms 
in considerably different corpora. This is particularly 
useful when we are dealing with a corpus silnilar to 
a known corpus  but  do  not  know the  precise word 
distributions in the corpus. The same tdnd of 
robustness was observed when we used Re/)(", 
325 
DIFFNUM). This baseline thnction robustness i  an 
important tbature of  measures defined using the 
baseline based. 
5. Conclusion and future works 
We have developed a better method -- the baseline 
method -- for defining the representativeness of  a 
term. A characteristic value of all docmnents 
containing a term T, D(T), is normalized by using a 
baseline function that estimates the characteristic 
value of  a randomly chosen doculnent set of  the 
same size as D(?). The normalized value is used to 
measure the representativeness of  the term T, and a 
measure defined by the baseline method offers 
several advantages compared to classical measures: 
(1) its definition is mathematically simple and clean 
(2) it can compare high-frequency terms with 
low-frequency terms, (3) the threshold value for 
being representative can be defined systcmatically, 
and (4) it can be applied to n-gram terms for any n. 
We developed two measures: one based on 
the normalized distance between two word 
distributions (Rep(., LLR)) and another based on 
the number of  different words in a document set 
(Rep( o, DIFFNUM)).  We compared these measures 
with two classical measures from various viewpoints, 
and confirmed that Rep(,, LLR) was superior. 
Experiments showed that the newly developed 
measures were particularly eflizctive for discarding 
frequent but uninformative terms. We can expect 
that these measures can be used for automated 
construction of  a stop-word list and improvement of  
similarity calculation of  documents. 
An important finding was that the baseline 
function is portable; that is, one defined on a corpus 
can be used for laormalization in a diflbrent corpus 
even if the two corpora have considerably diftbrent 
sizes or are in different domains. Wc can therefore 
apply the measures in a practical application when 
dealing with multiple similar corpora whose word 
distribution information is not fully known but we 
have the inforlnation on one particular corpus. 
We plan to apply Rep(., LLR) and Rep(., 
DIFFNUM) to several tasks in IR domain, such as 
the construction of  a stop-word list for indexing and 
term weighting in document-similarity calculation. 
It will also be interesting to theoretically 
estimate the baseline functions by using 
fundalnental parameters such as the total numbcr of  
words in a corpus or the total different number in the 
corpus. The natures of  the baseline functions 
deserve further study. 
Acknowledgements 
This project is supported in part by the Advanced 
Software Technology Project under the auspices of  
Information-technology Promotion Agency, Japan 
(IPA). 
References 
Caraballo, S. A. and Charniak, E. (1999). Determining 
the specificity of nouns fronl text. Prec. of EMNLP'99, 
pp. 63-70. 
Church, K. W. and Itanks, P. (1990). Word Association 
Norms, Mutual hlformation, and Lexicography, 
Conq)utational Linguistics 6( 1 ), pp.22-29. 
Daille, B. and Gaussiel; E., and Lange, J. (1994). Towards 
automatic extraction of monolingual nd bilingual 
terminology. Prec. of COL1NG'94, pp. 515-521. 
Dunning, T. (1993). Accurate Method for the Statistics of 
Surprise and Coincidence, Computational Linguistics 
19(1), pp.61-74. 
Firth, J. A synopsis ot' linguistic theory 1930- 1955. (t 957). 
Studies in Linguistic Analysix, Philological Society, Oxford. 
Frantzi, K. T., Ananiadou, S., and Tsujii, J. (1996). 
Extracting Terminological Expressions, IPSJ Technical 
Report of SIG NL, NLl12-12, pp.83-88. 
Hisamitsu, 'I:, Niwa, Y., and "l'sttiii, J. (1999). Measuring 
Representativeness of Terms, Prec. oflRAL'99, pp.83-90. 
Kageura, K. and Umino, B. (1996). Methods of automatic term 
recognition: A review. Termino logy  3(2), pp.259-289. 
Kando, N., I:,2uriyanaa, K. and Nozue, T. (1999). NACSIS test 
collection workshop (NTCIR-I), l'roc, of the 22nd Ammal 
hlternational A CM SIGIR Cot!\['. on Research and 
Development i  1R, pp.299-300. 
Kita, Y., Kate, Y., Otomo, 'E, and Yano, Y. (1994). 
Colnparativc Study of Automatic Extraction of Collocations 
fiOln Corpora: Mutual nlbrmation vs. Cost Criteria, Journal 
of Natural Language Processing, 1( 1 ), 21-33. 
Nagao, M., Mizutani, M., and lkeda, H. (1976). An Automated 
Method of the Extraction of hnportant Words fiom Japanese 
Scientific l)ocuments, Trans. oJIPSJ, 17(2), pp. 110-117. 
Nakagawa, H. and Mori, T. (1998). Nested Collocation and 
Compound Noun For Term Extraction, Prec. c( 
Computernt '98, pp.64-70 
Nishioka, S., Niwa, Y., lwayama, M., and Takano, A. (1997). 
DualNA VI: An intbrmation retrieval interface. Prec. o j
WISS'97, pp.43-48. (in Japanese) 
Niwa, Y., Nishioka, S., Iwayama, M., and Takano, A. (1997). 
'lbpic graph generalion lbr query navigation: UTse of 
fiequency classes lbr topic extraction. Prec. c?fNLPRS'97, 
pp.95-100. 
Norcault, q'., McGill, M., and Koll, M. B. (1977). A 
Pertbrmance Evaluation of Similarity Measure, Document 
Telill Weighting Schemes and Representation in a Boolean 
Environment. In Oddey, R. N. (ed.), Iq \ [brmal ion  Retrieval 
Resemz:h. London: Butterworths, pp.57-76. 
Salton, G. and Yang, C. S. (1973). On the Specification of Term 
Values in Automatic Indexing. Journal of Documentation 
29(4), pp.351-372. 
Sparck-Jones, K. (1973). Index Term Weighting. h(/brmation 
Storage and Retrieval 9(11), pp.616-633. 
Tcramoto, Y., Miyahara, Y., and Matsumoto, S.(1999). 
Word weight calculation for document retrieval by analyzing 
the distribution of co-occurrence words, Prec. of the 59th 
Ammal Meeting of lPS.l, 1P-06. (in Japanese) 
Appendix 
Asusume that items I1 ..... IN are ranked by measures A and B, 
and that the rank of item/: assigncd by A (B) is RiO" ) (R~(j)), 
where RA(i ) eRA( j )  (Rl4(i) ?Ri~(j)) if i ~j. Then, Spearman's rank 
correlation between the two rankings is given as 
t 6x~j(R4(i)-R"(i))2 
N(N ~ - 1) 
and Kendal l ' s  rank correlat ion between the two rank ings  is 
given as 
I ? ({# {(i, j) I  c~(&.,(i) - R A ( j ) )  = cr(Rz,(i ) - RB(j ) )}-  
N C2 
#{(i,./) l cr(R.4(i) - R.I(J)) = -cr(R~(i)  - Re(./))}) , 
where c~ (x)=l ifx > 0, clse ifx < 0, c~ (x) = -I. 
326 
A Measure of Term Representativeness Based on the Number of 
Co-occurring Salient Words 
Toru Hisamitsu and Yoshiki Niwa 
Central Research Laboratory, Hitachi, Ltd. 
Hatoyama, Saitama, 350-0095, Japan 
{hisamitu, yniwa}@harl.hitachi.co.jp 
 
Abstract  
We propose a novel measure of the 
representativeness (i.e., indicativeness or topic 
specificity) of a term in a given corpus. The 
measure embodies the idea that the distribution 
of words co-occurring with a representative term 
should be biased according to the word 
distribution in the whole corpus. The bias of the 
word distribution in the co-occurring words is 
defined as the number of distinct words whose 
occurrences are saliently biased in the 
co-occurring words. The saliency of a word is 
defined by a threshold probability that can be 
automatically defined using the whole corpus. 
Comparative evaluation clarified that the 
measure is clearly superior to conventional 
measures in finding topic-specific words in the 
newspaper archives of different sizes. 
 
Introduction 
Measuring the representativeness (i.e., the 
informativeness or domain specificity) of a term? is 
essential to various tasks in natural language 
processing (NLP) and information retrieval (IR). 
Such a measure is particularly crucial to automatic 
dictionary construction and IR interfaces to show a 
user words indicative of topics in retrievals that 
often consist of an intractably large number of 
documents (Niwa et al 2000). 
This paper proposes a novel and effective 
measure of term representativeness that reflects the 
bias of the words co-occurring with a term. In the 
following, we focus on extracting topic words from 
an archive of newspaper articles. 
 In the literature of NLP and IR, there have been 
a number of studies on term weighting, and these are 
strongly related to measures of term 
                                                  
? A term is a word or a word sequence.  
representativeness (see section 1). In this paper we 
employ the basic idea of the ?baseline method? 
proposed by Hisamitsu (Hisamitsu et al 2000). The 
idea is that the distribution of words co-occurring 
with a representative term should be biased 
according to the word distribution of the whole 
corpus. Concretely, for any term T and any measure 
M for the degree of bias of word occurrences in 
D(T), a set of words co-occurring with T, according 
to those of the whole corpus D0, the baseline method 
defines representativeness of term T by normalizing 
M(D(T)). In what follows, D0 is an archive of 
newspaper articles and D(T) is defined as the set of 
all articles containing T. 
 The normalization of M(D(T)) is done by a 
function BM, called the baseline function, which 
estimates the value of M(Drand) using #Drand for any 
randomly sampled document (in our case, ?article?) 
set Drand, where #Drand stands for the total number of 
words contained in Drand. By dividing M(D(T)) by 
BM(#D(T)), comparison of M(D(T1)) and M(D(T2)) 
becomes meaningful even if the frequencies of T1 
and T2 are very different. We denote this normalized 
value by NormM(D(T)).  
 Hisamitsu et al reported that NormM(D(T)) is 
very effective in capturing topic-specific words 
when M(D(T)) is defined as the distance between 
two word distributions PD(T) and P0 (see subsection 
1.2), which we denote by Dist(D(T)). 
 Although NormDist(D(T)) outperforms existing 
measures, it has still an intrinsic drawback shared by 
other measures, that is, words which are irrelevant to 
T and simply happen to occur in D(T) --- let us call 
these words non-typical words --- contribute to the 
calculation of M(D(T)). Their contribution 
accumulates as background noise in M(D(T)), which 
is the part to be offset by the baseline function. In 
other words, if M(D(T)) were to exclude the 
contribution of non-typical words, it would not need 
to be normalized and would be more precise. 
 This consideration led us to propose a different 
approach to measure the bias of word occurrences in 
 a discrete way: that is, we only take words whose 
occurrences are saliently biased in D(T) into account, 
and let the number of such words be the degree of 
bias of word occurrences in D(T). Thus, SAL(D(T), 
s), the number of words in D(T) whose saliency is 
over a threshold value s, is expected to be free from 
the background noise and sensitive to number of 
major subtopics in D(T). The essential problem now 
is how to define the saliency of bias of word 
occurrences and the threshold value of saliency. This 
paper solves this problem by giving a 
mathematically sound measure. Furthermore, it is 
shown that the optimal threshold value can be 
defined automatically. The newly defined measure 
SAL(D(T), s) outperforms existing measures in 
picking out topic-specific words from newspaper 
articles. 
 
1. Brief review of term representativeness  
measures  
1.1 Conventional measures 
Regarding term weighting, various measures of 
importance or domain specificity of a term have 
been proposed in NLP and IR domains (Kageura et 
al. 1996). In his survey, Kageura introduced two 
aspects of a term: unithood and termhood. Unithood 
is "the degree of strength or stability of syntagmatic 
combinations or collocations," and termhood is "the 
degree to which a linguistic unit is related to (or 
more straightforwardly, represents) domain-specific 
concepts." Kageura's termhood is therefore what we 
call representativeness here. 
 Representativeness measures were first 
introduced in the context of determining indexing 
words for IR (for instance, Salton et al 1973; 
Spark-Jones et al 1973; Nagao et al 1976). Among 
a number of measures introduced there, the most 
commonly used one is tf-idf proposed by Salton et al 
There are a variety of modifications of tf-idf (for 
example, Singhal et al 1996) but all share the basic 
feature that a word appearing more frequently in 
fewer documents is assigned a higher value.  
 In NLP domains several measures 
concentrating on the unithood of a word sequence 
have been proposed. For instance, the mutual 
information (Church et al 1990) and log-likelihood 
ratio (Dunning 1993; Cohen 1995) have been widely 
used for extracting word bigrams. Some measures 
for termhood have also been proposed, such as Imp 
(Nakagawa 2000), C-value and NC-value (Mima et 
al. 2000).  
 Although certain existing measures are widely 
used, they have major problems as follows: (1) 
classical measures such as tf-idf are so sensitive to 
term frequencies that they fail to avoid 
uninformative words that occur very frequently; (2) 
measures based on unithood cannot handle 
single-word terms; and (3) the threshold value for a 
term to be considered as being representative is 
difficult to define or can only be defined in an ad 
hoc manner. It is reported that measures defined by 
the baseline method do not have these problems 
(Hisamitsu et al 2000). 
 
1.2 Baseline method 
The basic idea of the baseline method stated in 
introduction can be summarized by the famous 
quote (Firth 1957) :  
"You shall know a word by the company it keeps."  
This is interpreted as the following hypothesis: 
For any term T, if the term is 
representative, word occurrences in 
D(T), the set of words co-occurring 
with T, should be biased according to 
the word distribution in D0. 
This hypothesis is transformed into the following 
procedure: 
Given a measure M for the bias of 
word occurrences in D(T) and a term 
T, calculate M(D(T)), the value of 
the measure for D(T). Then compare 
M(D(T)) with BM(#D(T)), where #D(T) 
is the number of words contained in 
#D(T), and BM estimates the value 
of M(D) when D is a randomly chosen 
document set of size #D(T).  
Here, as stated in introduction, D(T) is considered to 
be the set of all articles containing term T. 
 Hisamitsu et al tried a number of measures for 
M, and found that using Dist(D(T)), the distance 
between the word distribution PD(T) in D(T) and the 
word distribution P0 in the whole corpus D0 is 
effective in picking out topic-specific words in 
newspaper articles. The value of Dist(D(T)) can be 
defined in various ways, and they found that using 
log-likelihood ratio (see Dunning 1993) worked best  
which is represented as follows: 
0#
log
)(#
log
D
Kk
TD
kk i
M
ii
i
i
M
ii
i ??
==
? , 
where ki and Ki are the frequency of a word wi in 
 D(W) and D0 respectively, and {w1,...,wM} is the set 
of all words in D0. 
 As stated in introduction, Dist(D(T)) is 
normalized by the baseline function, which is 
referred as BDist(?) here. Figure 1(a) illustrates the 
necessity of the normalization: the graph?s 
coordinates are {(#D(T), Dist(D(T))) and {(#Drand, 
Dist(Drand))}, where T varies over ?cipher?, ?do?, 
and ?economy?, and Drand varies over a wide 
numerical range of randomly sampled articles. This 
figure shows that Dist(D(?do?)) is smaller than 
Dist(D(?electronic?)), which reflects our linguistic 
intuition that words co-occurring with ?electronic? 
are more biased than those with ?do?. However, 
Dist(D(?cipher?)) is smaller than Dist(D(?do?)), 
which contradicts our linguistic intuition. This is 
why values of Dist(D(T)) are not directly used to 
compare the representativeness of terms. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1(a) 
Baseline curve and sample word distribution 
 
This phenomenon can be explained by the 
curve, referred to as the baseline curve, composed of 
{(#Drand, Dist(Drand)}. The curve indicates that a part 
of Dist(D(T)) systematically varies depending only 
on #D(T) and not on T itself. It indicates the very 
notion of background noise stated in introduction, 
and by offsetting this part using the baseline 
function BDist(#D(T)), which approximates the 
baseline curve, the graph is converted into that of 
Figure 1(b). Since the baseline curve is not very 
meaningful as #Drand approaches to #D0, extremely 
frequent terms, such as ?do? are treated in a special 
way: that is, if the number of documents in D(T) is 
larger than a threshold value N0, which was 
calculated from the average number of words 
contained in a document, N0 documents are 
randomly chosen from D(T). This is because the 
coordinates of the point corresponding to ?do? differ 
in Fig. 1(a) and Fig. 1(b). As stated in introduction, 
Hisamitsu et al (2000) reported on that the 
superiority of NormDist(D(T)), normalized 
Dist(D(T)), in picking out topic-specific words over 
various measures including existing ones and other 
ones developed by using the baseline method.  
 
 
 
 
 
 
 
 
 
 
 
Figure 1(b) 
Effect of Normalization 
 
1.3 Reconsideration of normalization 
The effectiveness of the baseline method?s 
normalization indicates that Dist(D(T)) can be 
decomposed into two parts, one depending on T 
itself and another depending only on the size of D(T), 
which is considered to be background noise. The 
essence of the baseline method is to make the 
background noise explicit as a baseline function and 
to offset the noise by using the baseline function. To 
put it the other way round, if a term 
representativeness measure is designed so that this 
noise part does not exist in the first place, there is no 
need for the baseline function and calculation of 
representativeness becomes much simpler. More 
importantly, the precision of the measure itself 
should improve. 
 The definition of Dist(D(T)) shows, as with 
other measures, that every word in D(T) contributes 
to the value of Dist(D(T)). This explains why 
background noise, BDist(#D(T)), grows as #D(T) 
increases. One way to improve this situation is to 
eliminate the contribution of non-typical (see  
introduction) words. The simplest way to archive 
this is to focus only on saliently occurring words 
(precisely, words whose occurrences are saliently 
biased in D(T)) and let the number of words whose 
saliency is over a threshold value s, denoted by 
SAL(D(T), s), be the degree of bias of word 
1000
10000
100000
1000000
100 1000 10000 100000 1000000 10000000 100000000
cipher
do
electronic
#Drand and #D(T)
D
is
t(D
ra
nd
) a
nd
 D
is
t(D
(T
))
 
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
2.1
2.2
0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
cipher
electronic
do
threshold
#Drand and #D(T)
N
or
m
D
is
t(D
ra
nd
) a
nd
 N
or
m
D
is
t(D
(T
))
 
 occurrences in D(T). SAL(D(T), s) should reflect the 
richness of subtopics in D(T) and should be free 
from the contribution of non-typical words in D(T). 
 Thus, we need to define the saliency of 
occurrences of a word and a threshold value with 
which the occurrences of a word in D(T) is 
determined as salient. 
 
2. Term representativeness measure based on  
the number of co-occurring salient words 
2.1 A measure of word occurrence saliency 
To define saliency of occurrences of a word w in 
D(T), we employ a probabilistic measure proposed 
by Hisamitsu et al (2001) as follows:  
Let the total number (occurrences) of words in 
the whole corpus be N, the number (occurrences) 
of words in D(T) be n, the frequency of w in the 
whole corpus be K, and the frequency of w in 
D(T) be k. Denote the probability of ?No less 
than k red balls are contained in n balls that are 
arbitrarily chosen from N balls containing K red 
balls? by hgs(N, K, n, k). Then the saliency of w 
in D(T) is defined as ?log(hgs(N, K, n, k))?. 
Note that the probability of ?k red balls are 
contained in n balls arbitrarily chosen from N balls 
containing K red balls?, which we denote as hg(N, K, 
n, k), is a hypergeometric distribution with variable 
k. We denote the value ?log(hgs(N, K, n, k)) by 
HGS(w). HGS(w) is expressed as follows: 
}),min{},0(max{
.
)!()!()!(!!
)!()!(!!
),,,(
,),,,(),,,(
)),,,,(log()(
KnlnKN
lnKNlKlnlN
nNKNKn
n
N
ln
KN
l
K
lnKNhg
lnKNhgknKNhgs
knKNhgswHGS
kl
???+
+????
??=
??
???
?
??
???
?
?
???
???
?
=
=
?=
?
?
 
Due to its probabilistic meaning, comparison of the 
                                                  
? The reason why HGS(v) should be defined by ?hgs(N, 
K, n, k) instead of ?hg(N, K, n, k) is that the value of 
?hg(N, K, n, k) itself cannot tell whether occurrence of v 
k-times is saliently frequent or saliently infrequent. Only 
hgs(N, K, n, l), the sum of  hg (N, K, n, l) over l 
(k?l?min{n,K}) can tell which is the case since the sum 
indicates how far the event ?v occurs k-times in D(w)? is 
from the extreme event ?v occurs min{n,K} times in 
D(w)?. 
value of HGS(w)= ?log(hgs(N, K, n, k)) is always 
meaningful between any combination of N, K, n, and 
k. HGS(w) can be calculated very efficiently using 
an approximation technique (Hisamitsu et al 2001).   
 
2.2 Definition of SAL(D(T), s) 
Now we can define SAL(D(T), s) using the saliency 
measure defined above and a parameter s ? 0: 
},)(|)({)),(( swHGSTDwDIFFNUMsTDSAL ??=
where DIFFNUM(X) stands for the number of 
distinct items in set X. That is, SAL(D(T), s) is the 
number of distinct words in D(T) whose saliency of 
occurrence is not less than s. For instance, using the 
1996 archive of Nihon Keizai Shimbun (a Japanese 
financial newspaper),  SAL(D(?Aum??), 110) = 74, 
SAL(D(?Aum?), 200) = 50, SAL(D*(?do?), 110) = 1, 
and SAL(D*(?do?), 200) = 0, where D*(?do?) is a 
set of N0 randomly chosen articles from D(?do?) and 
N0 is the threshold value stated in subsection 1.2. 
This strongly suggests that SAL(D(T), s) can 
discriminate topic-specific words from non-topical 
words.  
 
2.3 Optimizing threshold of saliency 
Note that SAL(D(T), 0) gives the number of distinct 
words in D(T), and as s increases to ?, SAL(D(T), s) 
becomes a constant function (zero). If we 
straightforwardly follow the baseline method, we 
have to construct the baseline function BSAL(D(T), s) for 
varying s and test the performance of 
NormSAL(D(T), s), the normalized SAL(D(T), s). 
There are, however, a problem that BSAL(D(T), s) cannot 
be precisely approximated because SAL(D(T), s) is a 
discrete-valued function. 
 By considering the meaning of the baseline 
function, we can solve the problem of determining 
the optimal value of saliency parameter s without 
approximating baseline functions. That is, since the 
baseline function is considered as background noise 
to be offset, the best situation should be that the 
baseline function is a constant-valued function while 
SAL(D(T), s) is a non-trivial function (i.e., not a 
constant function). If there exists s0 satisfying the 
condition, SAL(D(T), s0) does not need to be 
normalized and is reliable itself, and s0 is the optimal 
parameter. 
 Figure 2 plots the coordinates {#Drand, 
                                                  
? Aum is the name of a religious cult that attacked Tokyo 
subway with sarin gas in 1995. 
 SAL(Drand, s)} for Drand and s, where Drand varies 
over randomly sampled article sets and s varies over 
several discrete values. Although BSAL(D(T), s) cannot 
be precisely approximated by using analytical 
functions, it can be seen that BSAL(D(T), s) changes from 
a monotone increasing function to a monotone 
decreasing function when s is greater than about 110, 
and the graph of BSAL(D(T), 110) is roughly parallel to 
the x-axis. Considering the meaning of baseline 
functions again, this means that s0 = 110 is the 
optimal value of saliency and that SAL(D(T), 110) 
can be used without normalization and is the most 
effective SAL. The important thing here is that this 
procedure to find the optimal value of s can be done 
automatically because it only requires random 
sampling of documents and curve fitting. Section 3 
experimentally confirms the superiority of SAL(D(T), 
s0) as a representativeness measure. 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 
{(#Drand, SAL(Drand, s)) and BSAL(D(T), s) 
 
3. Experiments 
As in Hisamitsu et al (2000), taking topic-word 
selection for IR navigation into account, we 
examined the relation between the value of 
representative measures and a manual classification 
of words (monograms) extracted from nearly 
160,000 articles in the 1996 archive of the Nihon 
Keizai Shimbun (denoted by D0 later on). 
 
3.1 Preparation 
We randomly chose 20,000 words from 86,000 
words having document frequencies larger than 2 in 
D0, then randomly chose 2,000 of them and 
classified these into three groups: (1) class P 
(positive): topic-specific words which are useful for 
the navigation of IR, (2) class N (negative): words, 
such as ?do?, not topic-specific and useless for IR 
navigation, and (3) class U (uncertain): words 
whose usefulness in IR navigation was either neutral 
or difficult to judge. In the classification process, a 
judge used an IR system called DualNAVI (Niwa et 
al. 2000) having dual windows one of which shows 
the titles of retrieved articles and another displays 
salient words occurring in the articles. The details of 
the guideline of classification are stated in Hisamitsu 
et al (2001). 
 
3.2 Measures compared in the experiments 
Four measures were compared by Hisamitsu et al 
(2000): NormDist(D(T)), NormDIFFNUM(D(T)), 
tf-idf, and tf(term frequency), where 
NormDIFFNUM(D(T)) is a normalized version of a 
measure called DIFFNUM(D(T)), which gives the 
number of distinct words in D(T). DIFFNUM is 
based on the hypothesis that the number of distinct 
words co-occurring with a representative word is 
smaller than that with a generic word (Teramoto et 
al. 1999). The definition of tf-idf used in the 
comparison was as follows:  
,
)(
log)(
TN
N
TTFidftf total?=?  
where T is a term, TF(T) is the term frequency of T, 
Ntotal is the total number of documents, and N(T) is 
the number of documents that contain T. We 
compared these four measures with SAL(D(T), s), 
varying s.   
 
3.3 Comparative experiments and results 
We compared the ability of each measure to gather 
class P words. We randomly sorted the 20,000 
words mentioned above, and then compared the 
result with the results of sorting by other measures. 
The comparison was done using the accumulated 
number of words marked by class P that appeared in 
the first k (1 ? k ? 20,000) words. For simplicity, we 
use the following notation: 
Rand(P, k): the accumulated number of class P  
words appearing in the first k words 
when random sorting was applied, 
M(P, k): the accumulated number of class P  
 words appearing in the first k words 
 when sorting was done by measure M, 
DP(M, k) = M(P, k)- Rand(P, k), and  
.),(),(
1
?
=
= k
l
lMDPkMADP  
The values of DP(M, k) and ADP(M, k) are called 
1
10
100
1000 10000 100000
s=20
s=40
s=120
s=110
s=100
s=80
s=60
s=90
#Drand 
{(
#D
ra
nd
, S
AL
(D
ra
nd
, s
))
 a
nd
 B
SA
L(
D
(T
), 
s)
 
 DP-score and ADP-score, respectively. For these 
scores, higher is better.  
 Figure 3 compares DP(M, k) for 1 ? k ? 20,000 
and Figure 4 compares ADP(M, 5,000), ADP(M, 
10,000), and ADP(M, 20,000). Where M varies over 
{NormDist(D(?)), NormDIFFNUM(D(?)), tf-idf, tf, 
SAL(D(?), s)}. These figures shows that SAL(D(T), 
s0) is overall superior to other measures except 
NormDist(D(?)). It is also superior to 
NormDist(D(?)) for 0?k?15,000. In terms of 
ADP-scores, SAL(D(T), s0) is superior to all other 
measures for k=5,000?10,000, 20,000. This means 
that SAL(D(?), s0) is superior to NormDist(D(?)) on 
the whole, and particularly superior in gathering 
topic-specific words near the top of the sorting. 
Comparison of SAL(D(T), s) for different values of s 
shows that s = s0 is actually the optimal value. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3.4 Effect of corpus size 
To see the effect of corpus size on the 
performance of SAL(D(?), s), we conducted the 
same kind of experiments that compared 
NormDist(D(?)) and SAL(D(?), s) by using 
different size of corpora D1/2 and D1/4, whose 
sizes were 1/2 and 1/4 of D0 respectively. The 
optimal value of s was determined for each 
corpus in the same way as stated in subsection 
2.3. The optimal value was around 70 for D1/2 
and around 40 for D1/4. Figure 5 compares 
DP-scores when D1/2 is used. Figure 6 compares 
the same when D1/4 is used. Figures 5 and 6 
show that SAL(D(?), s0) is superior to 
NormDist(D(?)) for corpora of different sizes. 
Judging from the results, we expect that the 
superiority of SAL(D(?), s0) would be even 
more apparent for a larger corpus.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0
10
20
30
40
50
60
70
80
90
100
110
120
130
0 5000 10000 15000 20000
Rank
A
cc
um
ul
at
ed
 N
um
be
r o
f C
la
ss
 P
 W
or
ds
NormDist SAL(D(T),30) SAL(D(T),70) SAL(D(T),100)
0
20
40
60
80
100
120
0 5000 10000 15000 20000
Rank
A
cc
um
ul
at
ed
 N
um
be
r o
f C
la
ss
 P
 W
or
ds
NormDist SAL(D(T),30) SAL(D(T),40) SAL(D(T),50)
Figure 4 
Comparison of ADP-scores using D0 
Figure 3 
Comparison of DP-scores using D0 
Figure 5 
Comparison of DP-scores using D1/2 
Figure 6 
Comparison of DP-scores using D1/4 
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
22000
NormDist s=50 70 90 110 130 150 170
A
D
P-
sc
or
e
ADP(M, 20,000), ADP(M, 10,000), ADP(M, 5,000), 
0
20
40
60
80
100
120
140
160
0 5000 10000 15000 20000
Rank
A
cc
um
ul
at
ed
 N
um
be
r o
f C
la
ss
 P
 W
or
ds
NormDist NormDIFFNUM tf tf-idf
SAL(D(T),30) SAL(D(T),110) SAL(D(T),180)
 Conclusion 
We proposed a novel measure of the 
representativeness of a term T in a given corpus. 
Denoting the words co-occurring with T by D(T), 
the measure is defined as SAL(D(T), s), the number 
of words in D(T) whose saliency of occurrences is 
over a threshold s. This measure embodies the idea 
that the distribution of words in D(T) should be 
saliently biased according to that of the whole 
corpus if T is a representative term. The saliency of 
word occurrences is defined by using a 
combinatorial probability, and the threshold value s 
is defined automatically so that the baseline function 
of SAL(D(T), s) does not depend on #D(T), the 
number of words contained in D(T). Comparative 
evaluation clarified that the proposed measure is 
superior to conventional measures in finding 
topic-specific words in newspaper archives of 
different sizes. 
 
Acknowledgements 
We would like to express our gratitude to Prof. 
Jun-ichi Tsujii of the University Tokyo and Prof. 
Kyo Kageura of National Institute of Informatics for 
their insightful comments. 
 This project is supported in part by the Core 
Research for Evolutional Science and Technology 
(CREST) under the auspices of the Japan Science 
and Technology Corporation. 
 
References 
Church, K. W. and Hanks, P. (1990). Word Association  
  Norms, Mutual Information, and Lexicography,  
  Computational Linguistics 6(1), pp.22-29. 
Cohen, J. D. (1995). Highlights: Language- and  
Domain-independent Automatic Indexing Terms for 
Abstracting, Journal of American Soc. for Information 
Science 46(3), pp.162-174. 
Dunning, T. (1993). Accurate Method for the Statistics of  
  Surprise and Coincidence, Computational Linguistics  
  19(1), pp.61-74. 
Firth, J. A synopsis of linguistic theory 1930-1955.  
(1957). Studies in Linguistic Analysis, Philological 
Society, Oxford. 
Hisamitsu, T., Niwa, Y., and Tsujii, J. (2000). A Method 
of Measuring Term Representativeness - Baseline 
Method Using Co-occurrence Distribution-, Proc. of 
COLING2000, pp.320-326.  
Hisamitsu, T., Niwa, Y. (2001). Topic-Word Selection 
Based on Combinatorial Probability, Proc. of 
NLPRS2001, pp.289-296. 
Kageura, K. and Umino, B. (1996). Methods of automatic  
term recognition: A review. Terminology 3(2), 
pp.259-289. 
Mima, H. and Ananiadou, S. (2000). An application and e 
aluation of the C/NC-value approach for the automatic 
term recognition of multi-word units in Japanese, 
Terminology, Vol.6, No.2, pp. 175?194. 
Nagao, M., Mizutani, M., and Ikeda, H. (1976). An Auto- 
mated Method of the Extraction of Important Words 
from Japanese Scientific Documents, Trans. of IPSJ, 
17(2), pp.110-117.  
Nakagawa, H. (2000). Automatic Term Recognition based  
on Statistics of Compound Nouns", Terminology, Vol.6, 
No.2, pp.195 ? 210. 
Niwa, Y., Iwayama, M., Hisamitsu, T., Nishioka,  S.,  
Takano, A., Sakurai, H., and Imaichi, O.(2000).  
DualNAVI -dual view interface bridges dual query  
types, Proc. of RIAO 2000, pp.19-20. 
Salton, G. and Yang, C. S. (1973). On the Specification of  
Term Values in Automatic Indexing. Journal of 
Documentation 29(4), pp.351-372. 
Singhal, A., Buckley, C., and Mitra, M. (1996). Pivoted 
Document Length Normalization, Proc. of ACM SIGIR? 
96, pp.21-29. 
Sparck-Jones, K. (1973). Index Term Weighting.  
Information Storage and Retrieval 9(11), pp.616-633. 
Teramoto, Y., Miyahara, Y., and Matsumoto, S. (1999).  
  Word weight calculation for document retrieval by  
analyzing the distribution of co-occurrence words, Proc. 
of the 59th Annual Meeting of IPSJ, IP-06. (in Japanese) 
 
