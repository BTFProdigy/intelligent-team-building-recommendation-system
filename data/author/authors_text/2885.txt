R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 22 ? 33, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
The Use of Monolingual Context Vectors for Missing 
Translations in Cross-Language Information Retrieval 
Yan Qu1, Gregory Grefenstette2, and David A. Evans1 
1
 Clairvoyance Corporation, 5001 Baum Boulevard, Suite 700, 
Pittsburgh, PA, 15213, USA 
{yqu, dae}@clairvoyancecorp.com 
2
 LIC2M/SCRI/LIST/DTSI/CEA, B.P.6, 
92265 Fontenay-aux-Roses Cedex, France 
{Gregory.Grefenstette}@cea.fr 
Abstract. For cross-language text retrieval systems that rely on bilingual dic-
tionaries for bridging the language gap between the source query language and 
the target document language, good bilingual dictionary coverage is imperative.  
For terms with missing translations, most systems employ some approaches for 
expanding the existing translation dictionaries.  In this paper, instead of lexicon 
expansion, we explore whether using the context of the unknown terms can help 
mitigate the loss of meaning due to missing translation.  Our approaches consist 
of two steps: (1) to identify terms that are closely associated with the unknown 
source language terms as context vectors and (2) to use the translations of the 
associated terms in the context vectors as the surrogate translations of the un-
known terms.  We describe a query-independent version and a query-dependent 
version using such monolingual context vectors.  These methods are evaluated 
in Japanese-to-English retrieval using the NTCIR-3 topics and data sets.  Em-
pirical results show that both methods improved CLIR performance for short 
and medium-length queries and that the query-dependent context vectors per-
formed better than the query-independent versions. 
1   Introduction 
For cross-language text retrieval systems that rely on bilingual dictionaries for bridg-
ing the language gap between the source query language and the target document 
language, good bilingual dictionary coverage is imperative [8,9].  Yet, translations for 
proper names and special terminology are often missing in available dictionaries.  
Various methods have been proposed for finding translations of names and terminol-
ogy through transliteration [5,11,13,14,16,18,20] and corpus mining [6,7,12,15,22].  
In this paper, instead of attempting to find the candidate translations of terms without 
translations to expand existing translation dictionaries, we explore to what extent 
simply using text context can help mitigate the missing translation problem and for 
what kinds of queries.  The context-oriented approaches include (1) identifying words 
that are closely associated with the unknown source language terms as context vectors 
and (2) using the translations of the associated words in the context vectors as the 
surrogate translations of the unknown words. We describe a query-independent  
 The Use of Monolingual Context Vectors for Missing Translations 23 
version and a query-dependent version using such context vectors.  We evaluate these 
methods in Japanese-to-English retrieval using the NTCIR-3 topics and data sets.  In 
particular, we explore the following questions: 
? Can translations obtained from context vectors help CLIR performance? 
? Are query-dependent context vectors more effective than query-independent 
context vectors for CLIR? 
In the balance of this paper, we first describe related work in Section 2.  The methods 
of obtaining translations through context vectors are presented in Section 3.  The CLIR 
evaluation system and evaluation results are presented in Section 4 and Section 5, re-
spectively.  We summarize the paper in Section 6. 
2   Related Work 
In dictionary-based CLIR applications, approaches for dealing with terms with missing 
translations can be classified into three major categories.  The first is a do-nothing ap-
proach by simply ignoring the terms with missing translations.  The second category 
includes attempts to generate candidate translations for a subset of unknown terms, such 
as names and technical terminology, through phonetic translation between different 
languages (i.e., transliteration) [5,11,13,14,16,18,20].  Such methods generally yield 
translation pairs with reasonably good accuracy reaching about 70% [18].  Empirical 
results have shown that the expanded lexicons can significantly improve CLIR system 
performance [5,16,20].  The third category includes approaches for expanding existing 
bilingual dictionaries by exploring multilingual or bilingual corpora.  For example, the 
?mix-lingual? feature of the Web has been exploited for locating translation pairs by 
searching for the presence of both Chinese and English text in a text window [22].  In 
work focused on constructing bilingual dictionaries for machine translation, automatic 
translation lexicons are compiled using either clean aligned parallel corpora [12,15] or 
non-parallel comparable corpora [6,7].  In work with non-parallel corpora, contexts of 
source language terms and target language terms and a seed translation lexicon are 
combined to measure the association between the source language terms and potential 
translation candidates in the target language.  The techniques with non-parallel corpora 
save the expense of constructing large-scale parallel corpora with the tradeoff of lower 
accuracy, e.g., about 30% accuracy for the top-one candidate [6,7].  To our knowledge, 
the usefulness of such lexicons in CLIR systems has not been evaluated. 
While missing translations have been addressed in dictionary-based CLIR systems, 
most of the approaches mentioned above attempt to resolve the problem through dic-
tionary expansion.  In this paper, we explore non-lexical approaches and their effective-
ness on mitigating the problem of missing translations.  Without additional lexicon 
expansion, and keeping the unknown terms in the source language query, we extract 
context vectors for these unknown terms and obtain their translations as the surrogate 
translations for the original query terms.  This is motivated by the pre-translation feed-
back techniques proposed by several previous studies [1,2].  Pre-translation feedback 
has been shown to be effective for resolving translation ambiguity, but its effect on 
recovering the lost meaning due to missing translations has not been empirically evalu-
ated.  Our work provides the first empirical results for such an evaluation. 
24 Y. Qu, G. Grefenstette, and D.A. Evans 
3   Translation via Context Vectors 
3.1   Query-Independent Context Vectors 
For a source language term t, we define the context vector of term t as:  
tC = ?? ittttt ,...,,,, 4321  
where terms 1t  to it  are source language terms that are associated with term t within 
a certain text window in some source language corpus.  In this report, the associated 
terms are terms that co-occur with term t above a pre-determined cutoff threshold. 
Target language translations of term t are derived from the translation of the known 
source language terms in the above context vectors: 
trans(t) = <trans(t1), trans(t2), ?, trans(tn)> 
Selection of the source language context terms for the unknown term above is only 
based on the association statistics in an independent source language corpus.  It does 
not consider other terms in the query as context; thus, it is query independent.  Using 
the Japanese-to-English pair as an example, the steps are as follows: 
1. For a Japanese term t that is unknown to the bilingual dictionary, extract 
concordances of term t within a window of P bytes (we used P=200 bytes 
or 100 Japanese characters) in a Japanese reference corpus. 
2. Segment the extracted Japanese concordances into terms, removing stop-
words. 
3. Select the top N (e.g., N=5) most frequent terms from the concordances to 
form the context vector for the unknown term t. 
4. Translate these selected concordance terms in the context vector into Eng-
lish to form the pseudo-translations of the unknown term t. 
Note that, in the translation step (Step 4) of the above procedure, the source lan-
guage association statistics for selecting the top context terms and frequencies of their 
translations are not used for ranking or filtering any translations.  Rather, we rely on 
the Cross Language Information Retrieval system?s disambiguation function to select 
the best translations in context of the target language documents [19]. 
3.2   Query-Dependent Context Vectors 
When query context is considered for constructing context vectors and pseudo-
translations, the concordances containing the unknown terms are re-ranked based on 
the similarity scores between the window concordances and the vector of the known 
terms in the query.  Each window around the unknown term is treated as a document, 
and the known query terms are used.  This is based on the assumption that the top 
ranked concordances are likely to be more similar to the query; subsequently, the 
context terms in the context vectors provide better context for the unknown term.  
Again, using the Japanese-English pair as an example, the steps are as follows: 
 The Use of Monolingual Context Vectors for Missing Translations 25 
1. For a Japanese term t unknown to the bilingual dictionary, extract a window of 
text of P bytes (we used P=200 bytes or 100 Japanese characters) around 
every occurrence of term t in a Japanese reference corpus. 
2. Segment the Japanese text in each window into terms and remove stopwords. 
3. Re-rank the window based on similarity scores between the terms found in the 
window and the vector of the known query terms. 
4. Obtain the top N (e.g., N=5) most frequently occurring terms from the top M 
(e.g., M=100) ranking windows to form the Japanese context vector for the 
unknown term t. 
5. Translate each term in the Japanese context vector into English to form the 
pseudo-translations of the unknown term t. 
The similarity scores are based on Dot Product. 
The main difference between the two versions of context vectors is whether the 
other known terms in the query are used for ranking the window concordances.  
Presumably, the other query terms provide a context-sensitive interpretation of the 
unknown terms.  When M is extremely large, however, the query-dependent version 
should approach the performance of the query-independent version. 
We illustrate both versions of the context vectors with topic 23 
(????????????? ?President Kim Dae-Jung's policy toward Asia?) 
from NTCIR-3: 
First, the topic is segmented into terms, with the stop words removed: 
???; ???; ???; ?? 
Then, the terms are categorized as ?known? vs. ?unknown? based on the bilingual 
dictionary: 
Unknown:  
Query23: ??? 
Known: 
Query23:??? 
Query23:??? 
Query23:?? 
Next, concordance windows containing the unknown term ??? are extracted:  
?????????????????????????????????    
????????????????????????????????? 
???????????????????????????????? 
???????????????????????????  
?? 
Next, the text in each window is segmented by a morphological processor into 
terms with stopwords removed [21]. 
In the query-independent version, we simply select the top 5 most frequently oc-
curring terms in the concordance windows.  The top 5 source language context terms 
for ??? are: 
26 Y. Qu, G. Grefenstette, and D.A. Evans 
3527:? 
3399:?? 
3035:??? 
2658:?? 
901:??????
1
 
Then, the translations of the above context terms are obtained from the bilingual 
dictionary to provide pseudo-translations for the unknown term ???, with the 
relevant translations in italics: 
??? ? ? ? gold 
??? ? ? ? metal 
??? ? ? ? money 
??? ??? ? ? 
??? ? ??? ? chief executive 
??? ? ??? ? president 
??? ? ??? ? presidential 
??? ? ?? ? korea 
??? ??????? ? ? 
With the query-dependent version, the segmented concordances are ranked by 
comparing the similarity between the concordance vector and the known term vector.  
Then we take the 100 top ranking concordances and, from this smaller set, select the 
top 5 most frequently occurring terms.  This time, the top 5 context terms are: 
1391:??? 
1382:? 
1335:?? 
1045:?? 
379:?????? 
In this example, the context vectors from both versions are the same, even though 
the terms are ranked in different orders.  The pseudo-translations from the context 
vectors are: 
??? ? ??? ? chief executive 
??? ? ??? ? president 
??? ? ??? ? presidential 
??? ? ? ? gold 
??? ? ? ? metal 
??? ? ? ? money 
??? ??? ? ? 
??? ? ?? ? korea 
??? ??????? ? ? 
                                                          
1
 Romanization of the katakana name ?????? could produce a correct transliteration of 
the name in English, which is not addressed in this paper.  Our methods for name translitera-
tion can be found in [18,20]. 
 The Use of Monolingual Context Vectors for Missing Translations 27 
4   CLIR System 
We evaluate the usefulness of the above two methods for obtaining missing transla-
tions in our Japanese-to-English retrieval system.  Each query term missing from our 
bilingual dictionary is provided with pseudo-translations using one of the methods.  
The CLIR system involves the following steps: 
First, a Japanese query is parsed into terms2 with a statistical part of speech tagger 
and NLP module [21].  Stopwords are removed from query terms.  Then query terms 
are split into a list of known terms, i.e., those that have translations from bilingual 
dictionaries, and a list of unknown terms, i.e., those that do not have translations from 
bilingual dictionaries.  Without using context vectors for unknown terms, translations 
of the known terms are looked up in the bilingual dictionaries and our disambiguation 
module selects the best translation for each term based on coherence measures be-
tween translations [19]. 
The dictionaries we used for Japanese to English translation are based on edict3, 
which we expanded by adding translations of missing English terms from a core Eng-
lish lexicon by looking them up using BabelFish4.  Our final dictionary has a total of 
210,433 entries.  The English corpus used for disambiguating translations is about 
703 MB of English text from NTCIR-4 CLIR track5.  For our source language corpus, 
we used the Japanese text from NTCIR-3. 
When context vectors are used to provide translations for terms missing from our dic-
tionary, first, the context vectors for the unknown terms are constructed as described 
above.  Then the same bilingual lexicon is used for translating the context vectors to 
create a set of pseudo-translations for the unknown term t.  We keep all the pseudo-
translations as surrogate translations of the unknown terms, just as if they really were 
the translations we found for the unknown terms in our bilingual dictionary. 
We use a corpus-based translation disambiguation method for selecting the best 
English translations for a Japanese query word.  We compute coherence scores of 
translated sequences created by obtaining all possible combinations of the translations 
in a source sequence of n query words (e.g., overlapping 3-term windows in our ex-
periments).  The coherence score is based on the mutual information score for each 
pair of translations in the sequence.  Then we take the sum of the mutual information 
scores of all translation pairs as the score of the sequence.  Translations with the high-
est coherence scores are selected as best translations.  More details on translation 
disambiguation can be found in [19]. 
Once the best translations are selected, indexing and retrieval of documents in the 
target language is based on CLARIT [4].  For this work, we use the dot product func-
tion for computing similarities between a query and a document: 
                                                          
2
 In these experiments, we do not include multiple-word expression such as ???? (war 
crime) as terms, because translation of most compositional multiple-word expressions can be 
generally constructed from translations of component words (?? and ??) and our empiri-
cal evaluation has not shown significant advantages of a separate model of phrase translation. 
3
 http://www.csse.monash.edu.au/~jwb/j_edict.html 
4
 http://world.altavista.com/ 
5
 http://research.nii.ac.jp/ntcir/ntcir-ws4/clir/index.html 
28 Y. Qu, G. Grefenstette, and D.A. Evans 
)()(),( tWtWDPsim
DPt
DP?
??
?=  . (1) 
where WP(t) is the weight associated with the query term t and WD(t) is the weight 
associated with the term t in the document D.  The two weights are computed as  
follows: 
)()()( tIDFtTFtW DD ?=  . (2) 
)()()()( tIDFtTFtCtW PP ??=  . (3) 
where IDF and TF are standard inverse document frequency and term frequency sta-
tistics, respectively.  IDF(t) is computed with the target corpus for retrieval.  The 
coefficient C(t) is an ?importance coefficient?, which can be modified either manually 
by the user or automatically by the system (e.g., updated during feedback). 
For query expansion through (pseudo-) relevance feedback, we use pseudo-
relevance feedback based on high-scoring sub-documents to augment the queries.  
That is, after retrieving some sub-documents for a given topic from the target corpus, 
we take a set of top ranked sub-documents, regarding them as relevant sub-documents 
to the query, and extract terms from these sub-documents.  We use a modified Roc-
chio formula for extracting and ranking terms for expansion: 
NumDoc
DocSetD
t
D
TF
tIDFtRocchio
?
??=
)(
)()(
 
(4) 
where IDF(t) is the Inverse Document Frequency of term t in reference database, 
NumDoc the number of sub-documents in the given set of sub-documents, and TFD(t) 
the term frequency score for term t in sub-document D. 
Once terms for expansion are extracted and ranked, they are combined with the 
original terms in the query to form an expanded query. 
exp
QQk
new
Q +?=  (5) 
in which Qnew, Qorig, Qexp stand for the new expanded query, the original query, and 
terms extracted for expansion, respectively.  In the experiments reported in Section 5, 
we assign a constant weight to all expansion terms (e.g., 0.5) 
5   Experiments 
5.1   Experiment Setup 
For evaluation, we used NTCIR-3 Japanese topics6.  Of the 32 topics that have rele-
vance judgments, our system identifies unknown terms as terms not present in our 
expanded Japanese-to-English dictionary described above.  The evaluation of the  
 
                                                          
6
 http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings3/index.html 
 The Use of Monolingual Context Vectors for Missing Translations 29 
effect of using context vectors is based only on the limited number of topics that con-
tain these unknown terms.  The target corpus is the NTCIR-3 English corpus, which 
contains 22,927 documents.  The statistics about the unknown terms for short (i.e., the 
title field only), medium (i.e., the description field only), and long (i.e., the descrip-
tion and the narrative fields) queries are summarized below.  The total number of 
unknown terms that we treated with context vectors was 83 (i.e., 6+15+62). 
 Short Medium Long 
No. of topics containing unknown terms 57 148 249 
Avg No. of terms in topics (total) 3.2 (16) 5.4 (75) 36.2 (86.9) 
Avg. No. of unknown terms (total) 1 (6) 1.1 (15) 2.610 (62) 
For evaluation, we used the mean average precision and recall for the top 1000 
documents and also precision@30, as defined in TREC retrieval evaluations. 
We compare three types of runs, both with and without post-translation pseudo-
relevance feedback.   
? Runs without context vectors (baselines) 
? Runs with query-dependent context vectors  
? Runs with query-independent context vectors 
5.2   Empirical Observations 
Tables 1-4 present the performance statistics for the above runs.  For the runs with 
translation disambiguation (Tables 1-2), using context vectors improved overall re-
call, average precision, and precision at 30 documents for short queries.  Context 
vectors moderately improved recall, average precision (except for the query inde-
pendent version), and precision at 30 documents for medium length queries. 
For the long queries, we do not observe any advantages of using either query-
dependent or query-independent versions of the context vectors.  This is probably 
because the other known terms in long queries provide adequate context for recover-
ing the loss of missing translation of the unknown terms.  Adding candidate transla-
tions from context vectors only makes the query more ambiguous and inexact. 
When all translations were kept (Tables 3-4), i.e., when no translation disambigua-
tion was performed, we only see overall improvement in recall for short and medium- 
length queries.  We do not see any advantage of using context vectors for improving 
average precision or precision at 30 documents.  For longer queries, the performance 
statistics were overall worse than the baseline.  As pointed out in [10], when all trans-
lations are kept without proper weighting of the translations, some terms get more 
favorable treatment than other terms simply because they contain more translations.  
So, in models where all translations are kept, proper weighting schemes should be 
developed, e.g., as suggested in related research [17]. 
                                                          
7
 Topics 4, 23, 26, 27, 33. 
8
 Topics 4, 5, 7, 13, 14, 20, 23, 26, 27, 28, 29, 31, 33, 38. 
9
 Topics 2, 4, 5, 7, 9, 13, 14, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 31, 33, 37, 38, 42, 43, 50. 
10
 The average number of unique unknown terms is 1.4. 
30 Y. Qu, G. Grefenstette, and D.A. Evans 
Table 1.  Performance statistics for short, medium, and long queries.  Translations were disam-
biguated; no feedback was used. Percentages show change over the baseline runs. 
No Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 28/112 0.1181 0.05 
With context vectors 
(query independent) 
43/112 
(+53.6%) 
0.1295 
(+9.7%) 
0.0667 
(+33.4%) 
With context vectors  
(query dependent) 
43/112 
(+53.6%) 
0.1573 
(+33.2%) 
0.0667 
(+33.4) 
Medium 
Baseline 113/248 0.1753 0.1231 
With context vectors 
(query independent) 
114/248 
(+0.9%) 
0.1588 
(-9.5%) 
0.1256 
(+2.0%) 
With context vectors  
(query dependent) 
115/248 
(+1.8%) 
0.1838 
(+4.8%) 
0.1282 
(+4.1%) 
Long 
Baseline 305/598 0.1901 0.1264 
With context vectors 
(query independent) 
308/598 
(+1.0%) 
0.1964 
(+3.3%) 
0.1125 
(-11.0%) 
With context vectors  
(query dependent) 
298/598 
(-2.3%) 
0.1883 
(-0.9%) 
0.1139 
(-9.9%) 
Table 2. Performance statistics for short, medium, and long queries.  Translations were 
disambiguated; for pseudo-relevance feedback, the top 30 terms from top 20 subdocuments 
were selected based on the Rocchio formula.  Percentages show change over the baseline runs. 
With Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 15/112 0.1863 0.0417 
With context vectors 
(query independent) 
40/112 
(+166.7%) 
0.1812 
(-2.7%) 
0.0417 
(+0.0%) 
With context vectors  
(query dependent) 
40/112 
(+166.7%) 
0.1942 
(+4.2%) 
0.0417 
(+0.0%) 
Medium 
Baseline 139/248 0.286 0.1513 
With context vectors 
(query independent) 
137 
(-1.4%) 
0.2942 
(+2.9%) 
0.1538 
(+1.7%) 
With context vectors  
(query dependent) 
141 
(+1.4%) 
0.3173 
(+10.9%) 
0.159 
(+5.1%) 
Long 
Baseline 341/598 0.2575 0.1681 
With context vectors 
(query independent) 
347/598 
(+1.8%) 
0.2598 
(+0.9%) 
0.1681 
(+0.0%) 
With context vectors 
(query dependent) 
340/598 
(-0.3%) 
0.2567 
(-0.3%) 
0.1639 
(-2.5%) 
 The Use of Monolingual Context Vectors for Missing Translations 31 
Table 3. Performance statistics for short, medium, and long queries.  All translations were kept 
for retrieval; pseudo-relevance feedback was not used.  Percentages show change over the 
baseline runs. 
No Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 33/112 0.1032 0.0417 
With context vectors 
(query independent) 
57/112 
(+72.7%) 
0.0465 
(-54.9%) 
0.05 
(+19.9%) 
With context vectors 
(query dependent) 
41/112 
(+24.2%) 
0.1045 
(-0.2%) 
0.0417 
(+0%) 
Medium 
Baseline 113/248 0.1838 0.0846 
With context vectors 
(query independent) 
136/248 
(+20.4%) 
0.1616 
(-12.1%) 
0.0769 
(-9.1%) 
With context vectors 
(query dependent) 
122/248 
(+8.0%) 
0.2013 
(+9.5%) 
0.0769 
(-9.1%) 
Long 
Baseline 283 0.1779 0.0944 
With context vectors 
(query independent) 
295/598 
(+4.2%) 
0.163 
(-8.4%) 
0.0917 
(-2.9%) 
With context vectors 
(query dependent) 
278/598 
(-1.8%) 
0.1566 
(-12.0%) 
0.0931 
(-1.4%) 
Table 4. Performance statistics for short, medium, and long queries.  All translations were kept 
for retrieval; for pseudo-relevance feedback, the top 30 terms from top 20 subdocuments were 
selected base on the Rocchio formula.  Percentages show change over the baseline runs. 
With Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 40/112 0.1733 0.0417 
With context vectors 
(query independent) 
69/112 
(+72.5%) 
0.1662 
(-4.1%) 
0.1583 
(+279.6%) 
With context vectors 
(query dependent) 
44/112 
(+10.0%) 
0.1726 
(-0.4%) 
0.0417 
(+0.0%) 
Medium 
Baseline 135/248 0.2344 0.1256 
With context vectors 
(query independent) 
161/248 
(+19.3%) 
0.2332 
(-0.5%) 
0.1333 
(+6.1%) 
With context vectors 
(query dependent) 
139/248 
(+3.0%) 
0.2637 
(+12.5%) 
0.1154 
(-8.1%) 
Long 
Baseline 344/598 0.2469 0.1444 
With context vectors 
(query independent) 
348/598 
(+1.2%) 
0.2336 
(-5.4%) 
0.1333 
(-7.7%) 
With context vectors  
(query dependent) 
319/598 
(-7.3%) 
0.2033 
(-17.7%) 
0.1167 
(-19.2%) 
32 Y. Qu, G. Grefenstette, and D.A. Evans 
6   Summary and Future Work 
We have used context vectors to obtain surrogate translations for terms that appear in 
queries but that are absent from bilingual dictionaries.  We have described two types 
of context vectors: a query-independent version and a query-dependent version.  In 
the empirical evaluation, we have examined the interaction between the use of context 
vectors with other factors such as translation disambiguation, pseudo-relevance feed-
back, and query lengths.  The empirical findings suggest that using query-dependent 
context vectors together with post-translation pseudo-relevance feedback and transla-
tion disambiguation can help to overcome the meaning loss due to missing transla-
tions for short queries.  For longer queries, the longer context in the query seems to 
make the use of context vectors unnecessary. 
The paper presents only our first set on experiments of using context to recover 
meaning loss due to missing translations.  In our future work, we will verify the ob-
servations with other topic sets and database sources; verify the observations with 
other language pairs, e.g., Chinese-to-English retrieval; and experiment with different 
parameter settings such as context window size, methods for context term selection, 
different ways of ranking context terms, and the use of the context term ranking in 
combination with disambiguation for translation selection. 
References 
1. Ballesteros, L., and Croft, B.:  Dictionary Methods for Cross-Language Information Re-
trieval.  In Proceedings of Database and Expert Systems Applications (1996) 791?801. 
2. Ballesteros, L., Croft, W. B.:  Resolving Ambiguity for Cross-Language Retrieval.  In 
Proceedings of SIGIR (1998) 64?71. 
3. Billhardt, H., Borrajo, D., Maojo, V.:  A Context Vector Model for Information Retrieval.  
Journal of the American Society for Information Science and Technology, 53(3) (2002) 
236?249. 
4. Evans, D. A., Lefferts, R. G.:  CLARIT?TREC Experiments. Information Processing and 
Management, 31(3) (1995) 385?395. 
5. Fujii, A., Ishikawa, T.:  Japanese/English Cross-Language Information Retrieval: Explora-
tion of Query Translation and Transliteration.  Computer and the Humanities, 35(4) (2001) 
389?420. 
6. Fung, P.:  A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora to 
Non-parallel Corpora.  In Proceedings of AMTA (1998) 1?17. 
7. Fung, P., Yee, L. Y.:  An IR Approach for Translating New Words from Nonparallel, 
Comparable Texts.  In Proceedings of COLING-ACL (1998) 414?420. 
8. Hull, D. A., Grefenstette, G.: Experiments in Multilingual Information Retrieval. In Pro-
ceedings of the 19th Annual International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (1996) 49?57. 
9. Grefenstette, G.: Evaluating the Adequacy of a Multilingual Transfer Dictionary for Cross 
Language Information Retrieval.  In Proceedings of LREC (1998) 755?758. 
10. Grefenstette, G.:  The Problem of Cross Language Information Retrieval. In G. Grefen-
stette, ed., Cross Language Information Retrieval, Kluwer Academic Publishers (1998)  
1?9. 
 The Use of Monolingual Context Vectors for Missing Translations 33 
11. Grefenstette, G., Qu, Y., Evans, D. A.:  Mining the Web to Create a Language Model for 
Mapping between English Names and Phrases and Japanese.  In Proceedings of the 2004 
IEEE/WIC/ACM International Conference on Web Intelligence (2004) 110?116. 
12. Ido, D., Church, K., Gale, W. A.:  Robust Bilingual Word Alignment for Machine Aided 
Translation.  In Proceedings of the Workshop on Very Large Corpora: Academic and In-
dustrial Perspectives (1993) 1?8. 
13. Jeong, K. S., Myaeng, S, Lee, J. S., Choi, K. S.:  Automatic Identification and Back-
transliteration of Foreign Words for Information Retrieval. Information Processing and 
Management, 35(4) (1999) 523?540. 
14. Knight, K, Graehl, J.:  Machine Transliteration. Computational Linguistics: 24(4) (1998) 
599?612. 
15. Kumano, A., Hirakawa, H.:  Building an MT dictionary from Parallel Texts Based on Lin-
guistic and Statistical Information.  In Proceedings of the 15th International Conference on 
Computational Linguistics (COLING) (1994) 76?81. 
16. Meng, H., Lo, W., Chen, B., Tang, K.: Generating Phonetic Cognates to Handel Named 
Entities in English-Chinese Cross-Language Spoken Document Retrieval.  In Proc of the 
Automatic Speech Recognition and Understanding Workshop (ASRU 2001) (2001). 
17. Pirkola, A., Puolamaki, D., Jarvelin, K.: Applying Query Structuring in Cross-Language 
Retrieval.  Information Management and Processing: An International Journal.  Vol 39 (3) 
(2003) 391?402. 
18. Qu, Y., Grefenstette, G.:  Finding Ideographic Representations of Japanese Names in Latin 
Scripts via Language Identification and Corpus Validation.  In Proceedings of the 42nd 
Annual Meeting of the Association for Computational Linguistics (2004) 183?190. 
19. Qu, Y., Grefenstette, G., Evans, D. A.: Resolving Translation Ambiguity Using Monolin-
gual Corpora. In Peters, C., Braschler, M., Gonzalo, J. (eds): Advances in Cross-Language 
Information Retrieval: Third Workshop of the Cross-Language Evaluation Forum, CLEF 
2002, Rome, Italy, September 19?20, 2002. Lecture Notes in Computer Science, Vol 
2785. Springer (2003) 223?241. 
20. Qu, Y., Grefenstette, G., Evans, D. A:  Automatic Transliteration for Japanese-to-English 
Text Retrieval.  In Proceedings of the 26th Annual International ACM SIGIR Conference 
on Research and Development in Information Retrieval (2003) 353?360. 
21. Qu, Y., Hull, D. A., Grefenstette, G., Evans, D. A., Ishikawa, M., Nara, S., Ueda, T., 
Noda, D., Arita, K., Funakoshi, Y., Matsuda, H.:  Towards Effective Strategies for Mono-
lingual and Bilingual Information Retrieval: Lessons Learned from NTCIR-4.  ACM 
Transactions on Asian Language Information Processing.  (to appear) 
22. Zhang, Y., Vines, P:  Using the web for automated translation extraction in cross-language 
information retrieval.  In Proceedings of the 27th Annual International ACM SIGIR Con-
ference on Research and Development in Information Retrieval (2004) 162?169. 
 
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Finding Ideographic Representations of Japanese Names Written in Latin 
Script via Language Identification and Corpus Validation 
Yan Qu 
Clairvoyance Corporation 
5001 Baum Boulevard, Suite 700 
Pittsburgh, PA 15213-1854, USA 
yqu@clairvoyancecorp.com 
Gregory Grefenstette? 
LIC2M/LIST/CEA 
18, route du Panorama, BP 6 
Fontenay-aux-Roses, 92265 France 
Gregory.Grefenstette@cea.fr 
 
Abstract 
Multilingual applications frequently involve 
dealing with proper names, but names are 
often missing in bilingual lexicons. This 
problem is exacerbated for applications 
involving translation between Latin-scripted 
languages and Asian languages such as 
Chinese, Japanese and Korean (CJK) where 
simple string copying is not a solution. We 
present a novel approach for generating the 
ideographic representations of a CJK name 
written in a Latin script.  The proposed 
approach involves first identifying the origin 
of the name, and then back-transliterating the 
name to all possible Chinese characters using 
language-specific mappings.  To reduce the 
massive number of possibilities for 
computation, we apply a three-tier filtering 
process by filtering first through a set of 
attested bigrams, then through a set of attested 
terms, and lastly through the WWW for a final 
validation.  We illustrate the approach with 
English-to-Japanese back-transliteration.  
Against test sets of Japanese given names and 
surnames, we have achieved average 
precisions of 73% and 90%, respectively. 
1 Introduction 
Multilingual processing in the real world often 
involves dealing with proper names. Translations 
of names, however, are often missing in bilingual 
resources.  This absence adversely affects 
multilingual applications such as machine 
translation (MT) or cross language information 
retrieval (CLIR) for which names are generally 
good discriminating terms for high IR performance 
(Lin et al, 2003).  For language pairs with 
different writing systems, such as Japanese and 
English, and for which simple string-copying of a 
name from one language to another is not a 
solution, researchers have studied techniques for 
transliteration, i.e., phonetic translation across 
languages.  For example, European names are 
often transcribed in Japanese using the syllabic  
katakana alphabet.  Knight and Graehl (1998) used 
a bilingual English-katakana dictionary, a 
katakana-to-English phoneme mapping, and the 
CMU Speech Pronunciation Dictionary to create a 
series of weighted finite-state transducers between 
English words and katakana that produce and rank 
transliteration candidates. Using similar methods, 
Qu et al (2003) showed that integrating 
automatically discovered transliterations of 
unknown katakana sequences, i.e. those not 
included in a large Japanese-English dictionary 
such as EDICT1, improves CLIR results. 
Transliteration of names between alphabetic and 
syllabic scripts has also been studied for languages 
such as Japanese/English (Fujii & Ishikawa, 2001), 
English/Korean (Jeong et al, 1999), and 
English/Arabic (Al-Onaizan and Knight, 2002). 
In work closest to ours, Meng et al(2001), 
working in cross-language retrieval of phonetically 
transcribed spoken text, studied how to 
transliterate names into Chinese phonemes (though 
not into Chinese characters).  Given a list of 
identified names, Meng et al first separated the 
names into Chinese names and English names. 
Romanized Chinese names were detected by a left-
to-right longest match segmentation method, using 
the Wade-Giles2 and the pinyin syllable inventories 
in sequence.  If a name could be segmented 
successfully, then the name was considered a 
Chinese name.  As their spoken document 
collection had already been transcribed into pinyin, 
retrieval was based on pinyin-to-pinyin matching; 
pinyin to Chinese character conversion was not 
addressed.  Names other than Chinese names were 
considered as foreign names and were converted 
into Chinese phonemes using a language model 
derived from a list of English-Chinese equivalents, 
both sides of which were represented in phonetic 
equivalents. 
                                                    
? The work was done by the author while at 
Clairvoyance Corporation. 
1
 http://www.csse.monash.edu.au/~jwb/edict.html 
2
 http://lcweb.loc.gov/catdir/pinyin/romcover.html 
The above English-to-Japanese or English-to-
Chinese transliteration techniques, however, only 
solve a part of the name translation problem. In 
multilingual applications such as CLIR and 
Machine Translation, all types of names must be 
translated. Techniques for name translation from 
Latin scripts into CJK scripts often depend on the 
origin of the name. Some names are not 
transliterated into a nearly deterministic syllabic 
script but into ideograms that can be associated 
with a variety of pronunciations. For example, 
Chinese, Korean and Japanese names are usually 
written using Chinese characters (or kanji) in 
Japanese, while European names are transcribed 
using katakana characters, with each character 
mostly representing one syllable. 
In this paper, we describe a method for 
converting a Japanese name written with a Latin 
alphabet (or romanji), back into Japanese kanji3. 
Transcribing into Japanese kanji is harder than 
transliteration of a foreign name into syllabic 
katakana, since one phoneme can correspond to 
hundreds of possible kanji characters. For example, 
the sound ?kou? can be mapped to 670 kanji 
characters. 
Our method for back-transliterating Japanese 
names from English into Japanese consists of the 
following steps: (1) language identification of the 
origins of names in order to know what language-
specific transliteration approaches to use, (2) 
generation of possible transliterations using sound 
and kanji mappings from the Unihan database (to 
be described in section 3.1) and then transliteration 
validation through a three-tier filtering process by 
filtering first through a set of attested bigrams, then 
through a set of attested terms, and lastly through 
the Web. 
The rest of the paper is organized as follows: in 
section 2, we describe and evaluate our name 
origin identifier; section 3 presents in detail the 
steps for back transliterating Japanese names 
written in Latin script into Japanese kanji 
representations; section 4 presents the evaluation 
setup and section 5 discusses the evaluation 
results; we conclude the paper in section 6. 
2 Language Identification of Names 
Given a name in English for which we do not 
have a translation in a bilingual English-Japanese 
dictionary, we first have to decide whether the 
name is of Japanese, Chinese, Korean or some 
European origin.  In order to determine the origin 
of names, we created a language identifier for 
names, using a trigram language identification 
                                                    
3
 We have applied the same technique to Chinese and 
Korean names, though the details are not presented here. 
method (Cavner and Trenkle, 1994).  During 
training, for Chinese names, we used a list of 
11,416 Chinese names together with their 
frequency information4.  For Japanese names, we 
used the list of 83,295 Japanese names found in 
ENAMDICT5.  For English names, we used the list 
of 88,000 names found at the US. Census site6 .  
(We did not obtain any training data for Korean 
names, so origin identification for Korean names is 
not available.)  Each list of names7 was converted 
into trigrams; the trigrams for each list were then 
counted and normalized by dividing the count of 
the trigram by the number of all the trigrams. To 
identify a name as Chinese, Japanese or English 
(Other, actually), we divide the name into trigrams, 
and sum up the normalized trigram counts from 
each language.  A name is identified with the 
language which provides the maximum sum of 
normalized trigrams in the word. Table 1 presents 
the results of this simple trigram-based language 
identifier over the list of names used for training 
the trigrams. 
The following are examples of identification 
errors: Japanese names recognized as English, e.g., 
aa, abason, abire, aebakouson; Japanese names 
recognized as Chinese, e.g., abeseimei, abei, adan, 
aden, afun, agei, agoin.  These errors show that the 
language identifier can be improved, possibly by 
taking into account language-specific features, 
such as the number of syllables in a name.  For 
origin detection of Japanese names, the current 
method works well enough for a first pass with an 
accuracy of 92%. 
Input 
 names 
As 
JAP 
As 
CHI 
As 
ENG 
Accuracy 
Japanese 76816 5265 1212 92% 
Chinese 1147 9947 321 87% 
English 12115 14893 61701 70% 
Table 1: Accuracy of language origin 
identification for names in the training set (JAP, 
CHI, and ENG stand for Japanese, Chinese, and 
English, respectively) 
                                                    
4
 http://www.geocities.com/hao510/namelist/ 
5
 http://www.csse.monash.edu.au/~jwb/ 
enamdict_doc.html 
6
 http://www.census.gov/genealogy/names 
7
 Some names appear in multiple name lists: 452 of the 
names are found both in the Japanese name list and in 
the Chinese name list; 1529 names appear in the 
Japanese name list and the US Census name list; and 
379 names are found both in the Chinese name list and 
the US Census list. 
3 English-Japanese Back-Transliteration 
Once the origin of a name in Latin scripts is 
identified, we apply language-specific rules for 
back-transliteration.  For non-Asian names, we use 
a katakana transliteration method as described in 
(Qu et al, 2003).  For Japanese and Chinese 
names, we use the method described below. For 
example, ?koizumi? is identified as a name of 
Japanese origin and thus is back-transliterated to 
Japanese using Japanese specific phonetic 
mappings between romanji and kanji characters. 
3.1 Romanji-Kanji Mapping 
To obtain the mappings between kanji characters 
and their romanji representations, we used the 
Unihan database, prepared by the Unicode 
Consortium 8 .  The Unihan database, which 
currently contains 54,728 kanji characters found in 
Chinese, Japanese, and Korean, provides rich 
information about these kanji characters, such as 
the definition of the character, its values in 
different encoding systems, and the 
pronunciation(s) of the character in Chinese (listed 
under the feature kMandarin in the Unihan 
database), in Japanese (both the On reading and the 
Kun reading 9 : kJapaneseKun and 
kJapaneseOn), and in Korean (kKorean).  For 
example, for the kanji character   , coded with 
Unicode hexadecimal character 91D1, the Unihan 
database lists 49 features; we list below its 
pronunciations in Japanese, Chinese, and Korean: 
U+91D1  kJapaneseKun    KANE 
U+91D1  kJapaneseOn     KIN KON 
U+91D1  kKorean KIM KUM 
U+91D1  kMandarin       JIN1 JIN4 
In the example above, 
 
 is represented in its 
Unicode scalar value in the first column, with a 
feature name in the second column and the values 
of the feature in the third column.  The Japanese 
Kun reading of 
 
 is KANE, while the Japanese On 
readings of 
 
 is KIN and KON. 
From the Unicode database, we construct 
mappings between Japanese readings of a character 
in romanji and the kanji characters in its Unicode 
representation.  As kanji characters in Japanese 
names can have either the Kun reading or the On 
                                                    
8
 http://www.unicode.org/charts/unihan.html 
9
 Historically, when kanji characters were introduced 
into the Japanese writing system, two methods of 
transcription were used.  One is called ?on-yomi? (i.e., 
On reading), where the Chinese sounds of the characters 
were adopted for Japanese words.  The other method is 
called ?kun-yomi? (i.e., Kun reading), where a kanji 
character preserved its meaning in Chinese, but was 
pronounced using the Japanese sounds. 
reading, we consider both readings as candidates 
for each kanji character.  The mapping table has a 
total of 5,525 entries.  A typical mapping is as 
follows: 
kou U+4EC0 U+5341 U+554F U+5A09 
U+5B58 U+7C50 U+7C58 ...... 
in which the first field specifies a pronunciation in 
romanji, while the rest of the fields specifies the 
possible kanji characters into which the 
pronunciation can be mapped. 
There is a wide variation in the distribution of 
these mappings.  For example, kou can be the 
pronunciation of 670 kanji characters, while the 
sound katakumi can be mapped to only one kanji 
character. 
3.2 Romanji Name Back-Transliteration 
In theory, once we have the mappings between 
romanji characters and the kanji characters, we can 
first segment a Japanese name written in romanji 
and then apply the mappings to back-transliterate 
the romanji characters into all possible kanji 
representations.  However, for some segmentation, 
the number of the possible kanji combinations can 
be so large as to make the problem 
computationally intractable.  For example, 
consider the short Japanese name ?koizumi.? This 
name can be segmented into the romanji characters 
?ko-i-zu-mi? using the Romanji-Kanji mapping 
table described in section 3.1, but this 
segmentation then has 182*230*73*49 (over 149 
million) possible kanji combinations.  Here, 182, 
239, 73, and 49 represents the numbers of possible 
kanji characters for the romanji characters ?ko?, 
?i?, ?zu?, and ?mi?, respectively. 
In this study, we present an efficient procedure 
for back-transliterating romanji names to kanji 
characters that avoids this complexity.  The 
procedure consists of the following steps: (1) 
romanji name segmentation, (2) kanji name 
generation, (3) kanji name filtering via 
monolingual Japanese corpus, and (4) kanji-
romanji combination filtering via WWW.  Our 
procedure relies on filtering using corpus statistics 
to reduce the hypothesis space in the last three 
steps.  We illustrate the steps below using the 
romanji name ?koizumi? (  . 
3.2.1 Romanji Name Segmentation 
With the romanji characters from the Romanji-
Kanji mapping table, we first segment a name 
recognized as Japanese into sequences of romanji 
characters.  Note that a greedy segmentation 
method, such as the left-to-right longest match 
method, often results in segmentation errors.  For 
example, for ?koizumi?, the longest match 
segmentation method produces segmentation ?koi-
zu-mi?, while the correct segmentation is ?ko-
izumi?. 
Motivated by this observation, we generate all 
the possible segmentations for a given name.  The 
possible segmentations for ?koizumi? are: 
ko-izumi 
koi-zu-mi 
ko-i-zu-mi 
3.2.2 Kanji Name Segmentation 
Using the same Romanji-Kanji mapping table, 
we obtain the possible kanji combinations for a 
segmentation of a romanji name produced by the 
previous step.  For the segmentation ?ko-izumi?, 
we have a total of 546 (182*3) combinations (we 
use the Unicode scale value to represent the kanji 
characters and use spaces to separate them): 
U+5C0F U+6CC9 
U+53E4 U+6CC9 
 ...... 
We do not produce all possible combinations. As 
we have discussed earlier, such a generation 
method can produce so many combinations as to 
make computation infeasible for longer 
segmentations.  To control this explosion, we 
eliminate unattested combinations using a bigram 
model of the possible kanji sequences in Japanese. 
From the Japanese evaluation corpus of the 
NTCIR-4 CLIR track 10 , we collected bigram 
statistics by first using a statistical part-of-speech 
tagger of Japanese (Qu et al, 2004).  All valid 
Japanese terms and their frequencies from the 
tagger output were extracted.  From this term list, 
we generated kanji bigram statistics (as well as an 
attested term list used below in step 3). With this 
bigram-based model, our hypothesis space is 
significantly reduced.  For example, with the 
segmentation ?ko-i-zu-mi?, even though ?ko-i? can 
have 182*230 possible combinations, we only 
retain the 42 kanji combinations that are attested in 
the corpus. 
Continuing with the romanji segments ?i-zu?, we 
generate the possible kanji combinations for ?i-zu? 
that can continue one of the 42 candidates for ?ko-
i?.  This results in only 6 candidates for the 
segments ?ko-i-zu?. 
Lastly, we consider the romanji segments ?zu-
mi?, and retain with only 4 candidates for the 
segmentation ?ko-i-zu-mi? whose bigram 
sequences are attested in our language model: 
U+5C0F U+53F0 U+982D U+8EAB 
U+5B50 U+610F U+56F3 U+5B50 
U+5C0F U+610F U+56F3 U+5B50 
U+6545 U+610F U+56F3 U+5B50 
                                                    
10
 http://research.nii.ac.jp/ntcir-ws4/clir/index.html 
Thus, for the segmentation ?ko-i-zu-mi?, the 
bigram-based language model effectively reduces 
the hypothesis space from 182*230*73*49 
possible kanji combinations to 4 candidates.  For 
the other alternative segmentation ?koi-zu-mi?, no 
candidates can be generated by the language 
model. 
3.2.3 Corpus-based Kanji name Filtering 
In this step, we use a monolingual Japanese 
corpus to validate whether the kanji name 
candidates generated by step (2) are attested in the 
corpus.  Here, we simply use Japanese term list 
extracted from the segmented NTCIR-4 corpus 
created for the previous step to filter out unattested 
kanji combinations.  For the segmentation ?ko-
izumi?, the following kanji combinations are 
attested in the corpus (preceded by their frequency 
in the corpus): 
4167    koizumi 
16   koizumi 
4   koizumi 
None of the four kanji candidates from the 
alternate segmentation ?ko-i-zu-mi? is attested in 
the corpus.  While step 2 filters out candidates 
using bigram sequences, step 3 uses corpus terms 
in their entirety to validate candidates. 
3.2.4 Romanji-Kanji Combination Validation 
Here, we take the corpus-validated kanji 
candidates (but for which we are not yet sure if 
they correspond to the same reading as the original 
Japanese name written in romanji) and use the 
Web to validate the pairings of kanji-romanji 
combinations (e.g.,    AND koizumi).  This is 
motivated by two observations.  First, in contrast to 
monolingual corpus, Web pages are often mixed-
lingual. It is often possible to find a word and its 
translation on the same Web pages. Second, person 
names and specialized terminology are among the 
most frequent mixed-lingual items.  Thus, we 
would expect that the appearance of both 
representations in close proximity on the same 
pages gives us more confidence in the kanji 
representations.  For example, with the Google 
search engine, all three kanji-romanji combinations 
for ?koizumi? are attested: 
23,600 pages --     koizumi 
302 pages --    koizumi 
1 page --   koizumi 
Among the three, the    koizumi combination 
is the most common one, being the name of the 
current Japanese Prime Minister. 
4 Evaluation 
In this section, we describe the gold standards 
and evaluation measures for evaluating the 
effectiveness of the above method for back-
transliterating Japanese names. 
4.1 Gold Standards 
Based on two publicly accessible name lists and 
a Japanese-to-English name lexicon, we have 
constructed two Gold Standards.  The Japanese-to-
English name lexicon is ENAMDICT 11 , which 
contains more than 210,000 Japanese-English 
name translation pairs. 
Gold Standard ? Given Names (GS-GN): to 
construct a gold standard for Japanese given 
names, we obtained 7,151 baby names in romanji 
from http://www.kabalarians.com/.  Of these 7,151 
names, 5,115 names have kanji translations in the 
ENAMDICT12.  We took the 5115 romanji names 
and their kanji translations in the ENAMDICT as 
the gold standard for given names. 
Gold Standard ? Surnames (GS-SN): to 
construct a gold standard for Japanese surnames, 
we downloaded 972 surnames in romanji from 
http://business.baylor.edu/Phil_VanAuken/Japanes
eSurnames.html.  Of these names, 811 names have 
kanji translations in the ENAMDICT.  We took 
these 811 romanji surnames and their kanji 
translations in the ENAMDICT as the gold 
standard for Japanese surnames. 
4.2 Evaluation Measures 
Each name in romanji in the gold standards has 
at least one kanji representation obtained from the 
ENAMDICT.  For each name, precision, recall, 
and F measures are calculated as follows: 
? Precision: number of correct kanji output / 
total number of kanji output 
? Recall: number of correct kanji output / total 
number of kanji names in gold standard 
? F-measure: 2*Precision*Recall / (Precision + 
Recall) 
Average Precision, Average Recall, and Average 
F-measure are computed over all the names in the 
test sets. 
5 Evaluation Results and Analysis 
5.1 Effectiveness of Corpus Validation 
Table 2 and Table 3 present the precision, recall, 
and F statistics for the gold standards GS-GN and 
                                                    
11
 http://mirrors.nihongo.org/monash/ 
enamdict_doc.html 
12
 The fact that above 2000 of these names were 
missing from ENAMDICT is a further justification for a 
name translation method as described in this paper. 
GS-SN, respectively.  For given names, corpus 
validation produces the best average precision of 
0.45, while the best average recall is a low 0.27.  
With the additional step of Web validation of the 
romanji-kanji combinations, the average precision 
increased by 62.2% to 0.73, while the best average 
recall improved by 7.4% to 0.29.  We observe a 
similar trend for surnames.  The results 
demonstrate that, through a large, mixed-lingual 
corpus such as the Web, we can improve both 
precision and recall for automatically 
transliterating romanji names back to kanji. 
 Avg 
Prec 
Avg 
Recall 
F 
(1) Corpus 0.45 0.27 0.33 
(2) Web 
(over (1)) 
0.73 
(+62.2%) 
0.29 
(+7.4%) 
0.38 
(+15.2%) 
Table 2: The best Avg Precision, Avg Recall, 
and Avg F statistics achieved through corpus 
validation and Web validation for GS-GN. 
 Avg 
Prec 
Avg 
Recall 
F 
(1) Corpus 0.69 0.44 0.51 
(2) Web 
(over (1)) 
0.90 
(+23.3%) 
0.45 
(+2.3%) 
0.56 
(+9.8%) 
Table 3: The best Avg Precision, Avg Recall, 
and Avg F statistics achieved through corpus 
validation and Web validation for GS-SN. 
We also observe that the performance statistics 
for the surnames are significantly higher than those 
of the given names, which might reflect the 
different degrees of flexibility in using surnames 
and given names in Japanese.  We would expect 
that the surnames form a somewhat closed set, 
while the given names belong to a more open set.  
This may account for the higher recall for 
surnames. 
5.2 Effectiveness of Corpus Validation 
If the big, mixed-lingual Web can deliver better 
validation than the limited-sized monolingual 
corpus, why not use it at every stage of filtering? 
Technically, we could use the Web as the ultimate 
corpus for validation at any stage when a corpus is 
required.  In practice, however, each Web access 
involves additional computation time for file IO, 
network connections, etc.  For example, accessing 
Google took about 2 seconds per name13; gathering 
                                                    
13
 We inserted a 1 second sleep between calls to the 
search engine so as not to overload the engine. 
statistics for about 30,000 kanji-romanji 
combinations14 took us around 15 hours. 
In the procedure described in section 3.2, we 
have aimed to reduce computation complexity and 
time at several stages.  In step 2, we use bigram-
based language model from a corpus to reduce the 
hypothesis space.  In step 3, we use corpus filtering 
to obtain a fast validation of the candidates, before 
passing the output to the Web validation in step 4.  
Table 4 illustrates the savings achieved through 
these steps. 
 GS-GN GS-SN 
All possible 2.0e+017 296,761,622,763 
2gram model 21,306,322 
(-99.9%) 
2,486,598 
(-99.9%) 
Corpus 
validate 
30,457 
(-99.9%) 
3,298 
(-99.9%) 
Web validation 20,787 
(-31.7%) 
2,769 
(-16.0%) 
Table 4: The numbers of output candidates of 
each step to be passed to the next step.  The 
percentages specify the amount of reduction in 
hypothesis space. 
5.3 Thresholding Effects 
We have examined whether we should discard 
the validated candidates with low frequencies 
either from the corpus or the Web.  The cutoff 
points examined include initial low frequency 
range 1 to 10 and then from 10 up to 400 in with 
increments of 5.  Figure 1 and Figure 2 illustrate 
that, to achieve best overall performance, it is 
beneficial to discard candidates with very low 
frequencies, e.g., frequencies below 5.  Even 
though we observe a stabling trend after reaching 
certain threshold points for these validation 
methods, it is surprising to see that, for the corpus 
validation method with GS-GN, with stricter 
thresholds, average precisions are actually 
decreasing.  We are currently investigating this 
exception. 
5.4 Error Analysis 
Based on a preliminary error analysis, we have 
identified three areas for improvements. 
First, our current method does not account for 
certain phonological transformations when the 
On/Kun readings are concatenated together.  
Consider the name ?matsuda? (   ).  The 
segmentation step correctly segmented the romanji 
to ?matsu-da?.  However, in the Unihan database, 
                                                    
14
 At this rate, checking the 21 million combinations 
remaining after filtering with bigrams using the Web 
(without the corpus filtering step) would take more than 
a year.   
the Kun reading of   is ?ta?, while its On reading 
is ?den?.  Therefore, using the mappings from the 
Unihan database, we failed to obtain the mapping 
between the pronunciation ?da? and the kanji  , 
which resulted in both low precision and recall for 
?matsuda?.  This suggests for introducing 
language-specific phonological transformations or 
alternatively fuzzy matching to deal with the 
mismatch problem. 
 
Avg Precision - GS_GN
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 6 15 50 10
0
15
0
20
0
25
0
30
0
35
0
40
0
Threshold for frequency cutoff
Av
g 
Pr
ec
is
io
n
corpus+web corpus
 
Figure 1: Average precisions achieved via both 
corpus and corpus+Web validation with different 
frequency-based cutoff thresholds for GS-GN  
Avg Precision - GS_SN
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 6 15 50 10
0
15
0
20
0
25
0
30
0
35
0
40
0
Threshold for frequency cutoff
Av
g 
Pr
ec
is
io
n
corpus+web corpus
 
Figure 2: Average precisions achieved via both 
corpus and corpus+Web validation with different 
frequency-based cutoff thresholds for GS-SN 
Second, ENAMDICT contains mappings 
between kanji and romanji that are not available 
from the Unihan database.  For example, for the 
name ?hiroshi? in romanji, based on the mappings 
from the Unihan database, we can obtain two 
possible segmentations: ?hiro-shi? and ?hi-ro-shi?.  
Our method produces two- and three-kanji 
character sequences that correspond to these 
romanji characters.  For example, corpus validation 
produces the following kanji candidates for 
?hiroshi?: 
 
 
2    hiroshi 
10    hiroshi 
5    hiroshi 
1    hiroshi 
2 	
  hiroshi 
11 	  hiroshi 
33 	  hiroshi 
311   hiroshi 
ENAMDCIT, however, in addition to the 2- and 
3-character kanji names, also contains 1-character 
kanji names, whose mappings are not found in the 
Unihan database, e.g., 

Hiroshi 

Hiroshi 

Hiroshi 

Hiroshi 

Hiroshi 

Hiroshi 
This suggests the limitation of relying solely on 
the Unihan database for building mappings 
between romanji characters and kanji characters.  
Other mapping resources, such as ENAMDCIT, 
should be considered in our future work. 
Third, because the statistical part-of-speech 
tagger we used for Japanese term identification 
does not have a lexicon of all possible names in 
Japanese, some unknown names, which are 
incorrectly separated into individual kanji 
characters, are therefore not available for correct 
corpus-based validation.  We are currently 
exploring methods using overlapping character 
bigrams, instead of the tagger-produced terms, as 
the basis for corpus-based validation and filtering. 
6 Conclusions 
In this study, we have examined a solution to a 
previously little treated problem of transliterating 
CJK names written in Latin scripts back into their 
ideographic representations.  The solution involves 
first identifying the origins of the CJK names and 
then back-transliterating the names to their 
respective ideographic representations with 
language-specific sound-to-character mappings.  
We have demonstrated that a simple trigram-based 
language identifier can serve adequately for 
identifying names of Japanese origin.  During 
back-transliteration, the possibilities can be 
massive due to the large number of mappings 
between a Japanese sound and its kanji 
representations.  To reduce the complexity, we 
apply a three-tier filtering process which eliminates 
most incorrect candidates, while still achieving an 
F measure of 0.38 on a test set of given names, and 
an F measure of 0.56 on a test of surnames. The 
three filtering steps involve using a bigram model 
derived from a large segmented Japanese corpus, 
then using a list of attested corpus terms from the 
same corpus, and lastly using the whole Web as a 
corpus. The Web is used to validate the back-
transliterations using statistics of pages containing 
both the candidate kanji translation as well as the 
original romanji name. 
Based on the results of this study, our future 
work will involve testing the effectiveness of the 
current method in real CLIR applications, applying 
the method to other types of proper names and 
other language pairs, and exploring new methods 
for improving precision and recall for romanji 
name back-transliteration.  In cross-language 
applications such as English to Japanese retrieval, 
dealing with a romaji name that is missing in the 
bilingual lexicon should involve (1) identifying the 
origin of the name for selecting the appropriate 
language-specific mappings, and (2) automatically 
generating the back-transliterations of the name in 
the right orthographic representations (e.g., 
Katakana representations for foreign Latin-origin 
names or kanji representations for native Japanese 
names).  To further improve precision and recall, 
one promising technique is fuzzy matching (Meng 
et al 2001) for dealing with phonological 
transformations in name generation that are not 
considered in our current approach (e.g., 
?matsuda? vs ?matsuta?).  Lastly, we will explore 
whether the proposed romanji to kanji back-
transliteration approach applies to other types of 
names such as place names and study the 
effectiveness of the approach for back-
transliterating romanji names of Chinese origin and 
Korean origin to their respective kanji 
representations. 
References  
Yaser Al-Onaizan and Kevin Knight. 2002. 
Machine Transliteration of Names in Arabic 
Text. Proc. of ACL Workshop on Computational 
Approaches to Semitic Languages 
William B. Cavnar and John M. Trenkle. 1994. N-
gram based text categorization. In 3rd Annual 
Symposium on Document Analysis and 
Information Retrieval, 161-175 
Atsushi Fujii and Tetsuya Ishikawa. 2001.  
Japanese/English Cross-Language Information 
Retrieval: Exploration of Query Translation and 
Transliteration.  Computer and the Humanities,  
35( 4): 389?420 
K. S. Jeong, Sung-Hyon Myaeng, J. S. Lee, and K. 
S. Choi. 1999. Automatic identification and 
back-transliteration of foreign words for 
information retrieval. Information Processing 
and Management, 35(4): 523-540 
Kevin Knight and Jonathan Graehl. 1998.  
Machine Transliteration. Computational 
Linguistics: 24(4): 599-612 
Wen-Cheng Lin, Changhua Yang and Hsin-Hsi 
Chen. 2003. Foreign Name Backward 
Transliteration in Chinese-English Cross-
Language Image Retrieval, In Proceedings of 
CLEF 2003 Workshop, Trondheim, Norway. 
Helen Meng, Wai-Kit Lo, Berlin Chen, and Karen 
Tang. 2001.  Generating Phonetic Cognates to 
Handel Named Entities in English-Chinese 
Cross-Language Spoken Document Retrieval.  In 
Proc of the Automatic Speech Recognition and 
Understanding Workshop (ASRU 2001) Trento, 
Italy, Dec. 
Yan Qu, Gregory Grefenstette, David A. Evans. 
2003. Automatic transliteration for Japanese-to-
English text retrieval. In Proceedings of SIGIR 
2003: 353-360 
Yan Qu, Gregory Grefenstette, David A. Hull, 
David A. Evans, Toshiya Ueda, Tatsuo Kato, 
Daisuke Noda, Motoko Ishikawa, Setsuko Nara, 
and Kousaku Arita. 2004. Justsystem-
Clairvoyance CLIR Experiments at NTCIR-4 
Workshop.  In Proceedings of the NTCIR-4 
Workshop. 
 
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 31?38,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 1
Modifying a Natural Language Processing System for  
European Languages to Treat Arabic in Information Processing  
and Information Retrieval Applications 
 
 
Gregory Grefenstette, Nasredine Semmar, Fa?za Elkateb-Gara 
Multilingual Multimedia Knowledge Engineering Laboratory (LIC2M)  
Commissariat ? l?Energie Atomique, Laboratoire d?Int?gration des Syst?mes et des Technologies 
(CEA LIST) 
B.P. 6, 92265 Fontenay-aux-Roses Cedex, France  
{gregory.grefenstette,nasredine.semmar,faiza.gara}@cea.fr  
 
 
 
 
Abstract 
The goal of many natural language proc-
essing platforms is to be able to someday 
correctly treat all languages. Each new 
language, especially one from a new lan-
guage family, provokes some modifica-
tion and design changes. Here we present 
the changes that we had to introduce into 
our platform designed for European lan-
guages in order to handle a Semitic lan-
guage. Treatment of Arabic was 
successfully integrated into our cross lan-
guage information retrieval system, which 
is visible online. 
1 Introduction 
When a natural language processing (NLP) system 
is created in a modular fashion, it can be relatively 
easy to extend treatment to new languages (May-
nard, et al 2003) depending on the depth and 
completeness desired. We present here lessons 
learned from the extension of our NLP system that 
was originally implemented for Romance and 
Germanic European1 languages to a member of the 
Semitic language family, Arabic. Though our sys-
tem was designed modularly, this new language 
posed new problems. We present our answers to 
                                                          
1
 European languages from non indo-European families 
(Basque, Finnish and Hungarian) pose some of the same prob-
lems that Arabic does. 
these problems encountered in the creation of an 
Arabic processing system, and illustrate its integra-
tion into an online cross language information re-
trieval (CLIR) system dealing with documents 
written in Arabic, English French and Spanish. 
 
2 The LIMA natural language processor 
Our NLP system (Besan?on et al, 2003), called 
LIMA2, was built using a traditional architecture 
involving separate modules for  
1. Morphological analysis:  
a. Tokenization (separating the input 
stream into a graph of words). 
b. Simple word lookup (search for 
words in a full form lexicon). 
c. Orthographical alternative lookup 
(looking for differently accented 
forms, alternative hyphenisation, 
concatenated words, abbreviation 
recognition), which might alter the 
original non-cyclic word graph by 
adding alternative paths. 
d. Idiomatic expressions recognizer 
(detecting and considering them as 
single words in the word graph). 
e. Unknown word analysis. 
2. Part-of-Speech and Syntactic analysis: 
a. After the morphological analysis, 
which has augmented the original 
graph with as many nodes as there 
                                                          
2
 LIMA stands for the LIC2M Multilingual Analyzer. 
31
 2
are interpretations for the tokens, 
part-of-speech analysis using lan-
guage models from a hand-tagged 
corpus reduces the number of pos-
sible readings of the input. 
b. Named entity recognizer. 
c. Recognition of nominal and verbal 
chains in the graph. 
d. Dependency relation extraction. 
3. Information retrieval application: 
a. Subgraph indexing. 
b. Query reformulation (monolingual 
reformulation for paraphrases and 
synonymy; multilingual for cross 
language information retrieval). 
c. Retrieval scoring comparing par-
tial matches on subgraphs and en-
tities. 
 
Our LIMA NLP system (Besan?on et al, 2003) 
was first implemented for English, French, German 
and Spanish, with all data coded in UTF8.  When 
we extended the system to Arabic, we found that a 
number of modifications had to be introduced. We 
detail these modifications in the next sections. 
3 Changes specific to Semitic languages 
Two new problems posed by Arabic (and common 
to most Semitic languages) that forced us to alter 
our NLP system are the problem of incomplete 
vowelization of printed texts3 and the problem of 
agglutinative clitics. We discuss how these new 
problems influenced our lexical resources and lan-
guage processing steps.  
Lexical Resources 
The first task for introducing a new language is to 
create the lexical resources for this language. Since 
Arabic presents agglutination of articles, preposi-
tions and conjunctions at the beginning of words as 
well as pronouns at the end of words, and these 
phenomena were not treated in our existing Euro-
                                                          
3
 Since the headwords of our monolingual and cross-lingual 
reference dictionaries for Arabic possess voweled entries, we 
hope to attain greater precision by treating this problem. An 
alternative but noisy approach (Larkey et al 2002) is to reduce 
to unvoweled text throughout the NLP application. 
pean languages4, we had to decide how this feature 
would be handled in the lexicon. Solutions to this 
problem have been proposed, ranging from genera-
tion and storage of all agglutinated words forms 
(Debili and Zouari, 1985) to the compilation of 
valid sequences of proclitics, words and enclitics 
into finite-state machines (Beesley, 1996). Our 
system had already addressed the problem of com-
pounds for German in the following way: if an in-
put word is not present in the dictionary, a 
compound-searching module returns all complete 
sequences of dictionary words (a list of possible 
compound joining "fogemorphemes" is passed to 
this module) as valid decompositions of the input 
word. Though theoretically this method could be 
used to treat Arabic clitics, we decided against us-
ing this existing process for two reasons: 
1. Contrary to German, in which any noun 
may theoretically be the first element of 
a compound, Arabic clitics belong to a 
small closed set of articles, conjunc-
tions, prepositions and pronouns. Al-
lowing any word to appear at the 
beginning or end of an agglutinated 
word would generate unnecessary noise. 
2. Storing all words with all possible cli-
tics would multiply the size of lexicon 
proportionally to the number of legal 
possible combinations. We decided that 
this would take up too much space, 
though others have adopted this ap-
proach as mentioned above. 
 
We decided to create three lexicons: two additional 
(small) lists of proclitic and enclitic combinations, 
and one large lexicon of full form5 voweled words 
(with no clitics), the creation of the large lexicon 
from a set of lemmas using classic conjugation 
rules did not require any modification of the exist-
ing dictionary building and compilation compo-
nent. Since our NLP system already possessed a 
mechanism for mapping unaccented words to ac-
cented entries, and we decided to use this existing 
                                                          
4
 Spanish, of course, possesses enclitic pronouns for some 
verb forms but these were not adequately treated until the 
solution for Arabic was implemented in our system.  
5
 Our dictionary making process generates all full form ver-
sions of non compound and unagglutinated words. These are e 
then compiled into a finite-state automaton. Every node corre-
sponding to a full word is flagged, and an index corresponding 
to the automaton path points to the lexical data for that word. 
32
 3
mechanism for later matching of voweled and un-
voweled versions of Arabic words in applications. 
Thus the only changes for lexical resources involve 
adding two small clitic lexicons. 
Processing Steps: Morphological analysis 
Going back to the NLP processing steps listed in 
section 2, we now discuss new processing changes 
needed for treating Arabic. Tokenization (1a) and 
simple word lookup (2a) of the tokenized strings in 
the dictionary were unchanged as LIMA was 
coded for UTF8. If the word was not found, an 
existing orthographical alternative lookup (1c) was 
also used without change (except for the addition 
of the language specific correspondence table be-
tween accented and unaccented characters) in order 
to find lexical entries for unvoweled or partially 
voweled words. Using this existing mechanism for 
treating the vowelization problem does not allow 
us to exploit partial vowelization as we explain in a 
later section. 
 
At this point in the processing, a word that contains 
clitics will not have been found in the dictionary 
since we had decided not to include word forms 
including clitics. We introduced, here, a new proc-
essing step for Arabic: a clitic stemmer. This 
stemmer uses the following linguistic resources: 
? The full form dictionary, containing for 
each word form its possible part-of-speech 
tags and linguistic features (gender, num-
ber, etc.). We currently have 5.4 million 
entries in this dictionary6. 
? The proclitic dictionary and the enclitic 
dictionary, having the same structure of 
the full form dictionary with voweled and 
unvoweled versions of each valid combi-
nation of clitics. There are 77 and 65 en-
tries respectively in each dictionary. 
 
The clitic stemmer proceeds as follows on tokens 
unrecognized after step 1c: 
? Several vowel form normalizations are 
performed (?  ?  ?  ?  ?  ?  are removed,  ?  ?  	  
are replaced by  ?  and  final  ?  ?  ?  or ? 
are replaced by  ?  ??  ??  or  ). 
                                                          
6
 If we generated all forms including appended clitics, we 
would generate an estimated 60 billion forms (Attia, 1999). 
? All clitic possibilities are computed by us-
ing proclitics and enclitics dictionaries. 
? A radical, computed by removing these 
clitics, is checked against the full form 
lexicon. If it does not exist in the full form 
lexicon, re-write rules (such as those de-
scribed in Darwish (2002)) are applied, 
and the altered form is checked against the 
full form dictionary. For example, consider 
the token  ???? and the included clitics (?, 
?), the computed radical ?? does not exist 
in the full form lexicon but after applying 
one of the dozen re-write rules, the modi-
fied radical ?? is found the dictionary and 
the input token is segmented into root and 
clitics as:  ? + ?? + ? = ???? . 
? The compatibility of the morpho-syntactic 
tags of the three components (proclitic, 
radical, enclitic) is then checked. Only 
valid segmentations are kept and added 
into the word graph. Table 1 gives some 
examples of segmentations7 of words in 
the sentence  ?c? 2003 Association for Computational Linguistics
Introduction to the Special Issue on the
Web as Corpus
Adam Kilgarriff? Gregory Grefenstette?
Lexicography MasterClass Ltd. and ITRI Clairvoyance Corporation
University of Brighton
The Web, teeming as it is with language data, of all manner of varieties and languages, in
vast quantity and freely available, is a fabulous linguists? playground. This special issue of
Computational Linguistics explores ways in which this dream is being explored.
1. Introduction
The Web is immense, free, and available by mouse click. It contains hundreds of
billions of words of text and can be used for all manner of language research.
The simplest language use is spell checking. Is it speculater or speculator? Google
gives 67 for the former (usefully suggesting the latter might have been intended) and
82,000 for the latter. Question answered.
Language scientists and technologists are increasingly turning to the Web as a
source of language data, because it is so big, because it is the only available source
for the type of language in which they are interested, or simply because it is free
and instantly available. The mode of work has increased dramatically from a standing
start seven years ago with the Web being used as a data source in a wide range of
research activities: The papers in this special issue form a sample of the best of it. This
introduction to the issue aims to survey the activities and explore recurring themes.
We first consider whether the Web is indeed a corpus, then present a history of
the theme in which we view the Web as a development of the empiricist turn that has
brought corpora center stage in the course of the 1990s. We briefly survey the range
of Web-based NLP research, then present estimates of the size of the Web, for English
and for other languages, and a simple method for translating phrases. Next we open
the Pandora?s box of representativeness (concluding that the Web is not representative
of anything other than itself, but then neither are other corpora, and that more work
needs to be done on text types). We then introduce the articles in the special issue and
conclude with some thoughts on how the Web could be put at the linguist?s disposal
rather more usefully than current search engines allow.
1.1 Is the Web a Corpus?
To establish whether the Web is a corpus we need to find out, discover, or decide what
a corpus is. McEnery and Wilson (1996, page 21) say
In principle, any collection of more than one text can be called a
corpus. . . . But the term ?corpus? when used in the context of modern
linguistics tends most frequently to have more specific connotations
than this simple definition provides for. These may be considered un-
? Lewes Rd, Brighton, BN2 4JG, UK. E-mail: Adam.Kilgarriff@itri.brighton.ac.uk
? Suite 700, 5001 Baum Blvd, Pittsburgh, PA 15213-1854. E-mail: grefen@clairvoyancecorp.com
334
Computational Linguistics Volume 29, Number 3
der four main headings: sampling and representativeness, finite size,
machine-readable form, a standard reference.
We would like to reclaim the term from the connotations. Many of the collections
of texts that people use and refer to as their corpus, in a given linguistic, literary, or
language-technology study, do not fit. A corpus comprising the complete published
works of Jane Austen is not a sample, nor is it representative of anything else. Closer
to home, Manning and Schu?tze (1999, page 120) observe:
In Statistical NLP, one commonly receives as a corpus a certain amount
of data from a certain domain of interest, without having any say in
how it is constructed. In such cases, having more training data is
normally more useful than any concerns of balance, and one should
simply use all the text that is available.
We wish to avoid a smuggling of values into the criterion for corpus-hood. McEnery
and Wilson (following others before them) mix the question ?What is a corpus?? with
?What is a good corpus (for certain kinds of linguistic study)?? muddying the simple
question ?Is corpus x good for task y?? with the semantic question ?Is x a corpus at
all?? The semantic question then becomes a distraction, all too likely to absorb energies
that would otherwise be addressed to the practical one. So that the semantic question
may be set aside, the definition of corpus should be broad. We define a corpus simply
as ?a collection of texts.? If that seems too broad, the one qualification we allow relates
to the domains and contexts in which the word is used rather than its denotation: A
corpus is a collection of texts when considered as an object of language or literary study.
The answer to the question ?Is the web a corpus?? is yes.
2. History
For chemistry or biology, the computer is merely a place to store and process infor-
mation gleaned about the object of study. For linguistics, the object of study itself (in
one of its two primary forms, the other being acoustic) is found on computers. Text
is an information object, and a computer?s hard disk is as valid a place to go for its
realization as the printed page or anywhere else.
The one-million-word Brown corpus opened the chapter on computer-based lan-
guage study in the early 1960s. Noting the singular needs of lexicography for big data,
in the 1970s Sinclair and Atkins inaugurated the COBUILD project, which raised the
threshold of viable corpus size from one million to, by the early 1980s, eight million
words (Sinclair 1987). Ten years on, Atkins again took the lead with the develop-
ment (from 1988) of the British National Corpus (BNC) (Burnard 1995), which raised
horizons tenfold once again, with its 100 million words and was in addition widely
available at low cost and covered a wide spectrum of varieties of contemporary British
English.1 As in all matters Zipfian, logarithmic graph paper is required. Where corpus
size is concerned, the steps of interest are 1, 10, 100, . . . , not 1, 2, 3, . . .
Corpora crashed into computational linguistics at the 1989 ACL meeting in Van-
couver, but they were large, messy, ugly objects clearly lacking in theoretical integrity
in all sorts of ways, and many people were skeptical regarding their role in the disci-
pline. Arguments raged, and it was not clear whether corpus work was an acceptable
1 Across the Atlantic, a resurgence in empiricism was led by the success of the noisy-channel model in
speech recognition (see Church and Mercer [1993] for references).
335
Kilgarriff and Grefenstette Web as Corpus: Introduction
part of the field. It was only with the highly successful 1993 special issue of this
journal, ?Using Large Corpora? (Church and Mercer 1993), that the relation between
computational linguistics and corpora was consummated.
There are parallels with Web corpus work. The Web is anarchic, and its use is
not in the familiar territory of computational linguistics. However, as students with
no budget or contacts realize, it is the obvious place to obtain a corpus meeting their
specifications, as companies want the research they sanction to be directly related
to the language types they need to handle (almost always available on the Web), as
copyright continues to constrain ?traditional? corpus development,2 as people want
to explore using more data and different text types, so Web-based work will grow.
The Web walked in on ACL meetings starting in 1999. Rada Mihalcea and Dan
Moldovan (1999) used hit counts for carefully constructed search engine queries to
identify rank orders for word sense frequencies, as an input to a word sense dis-
ambiguation engine. Philip Resnik (1999) showed that parallel corpora?until then a
promising research avenue but largely constrained to the English-French Canadian
Hansard?could be found on the Web: We can grow our own parallel corpus using
the many Web pages that exist in parallel in local and in major languages. We are
glad to have the further development of this work (co-authored by Noah Smith) pre-
sented in this special issue. In the student session of ACL 2000, Rosie Jones and Rayid
Ghani (2001) showed how, using the Web, one can build a language-specific corpus
from a single document in that language. In the main session Atsushi Fujii and Tet-
suya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be
acquired from the Web.
2.1 Some Current Themes
Since then there have been many papers, at ACL and elsewhere, and we can mention
only a few. The EU MEANING project (Rigau et al 2002) takes forward the exploration
of the Web as a data source for word sense disambiguation, working from the premise
that within a domain, words often have just one meaning, and that domains can be
identified on the Web. Mihalcea and Tchklovski complement this use of Web as corpus
with Web technology to gather manual word sense annotations on the Word Expert
Web site.3 Santamari?a et al, in this issue, discuss how to link word senses to Web
directory nodes, and thence to Web pages.
The Web is being used to address data sparseness for language modeling. In
addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers
lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda
et al (2003) ?balance? their corpus using Web documents.
The information retrieval community now has a Web track as a component of its
TREC evaluation initiative. The corpus for this exercise is a substantial (around 100GB)
sample of the Web, largely using documents in the .gov top level domain, as frozen
at a given date (Hawking et al 1999).
The Web has recently been used by groups at Sheffield and Microsoft, among
others, as a source of answers for question-answering applications, in a merge of search
engine and language-processing technologies (Greenwood, Roberts, and Gaizauskas
2 Lawyers may argue that the legal issues for Web corpora are no different from those around non-Web
corpora. However, first, language researchers can develop Web corpora just by saving Web pages on
their own computer without any copying, thereby avoiding copyright issues, and second, a Web
corpus is a very minor subspecies of the caches and indexes held by search engines and assorted other
components of the infrastructure of the Web: If a Web corpus is infringing copyright, then it is merely
doing on a small scale what search engines such as Google are doing on a colossal scale.
3 ?http://teach-computers.org/word-expert.html?.
336
Computational Linguistics Volume 29, Number 3
2002; Dumais et al 2002). AnswerBus (Zheng 2002) will answer questions posed in
English, German, French, Spanish, Italian, and Portuguese.
Naturally, the Web is also coming into play in other areas of linguistics. Agirre
et al 2000) are exploring the automatic population of existing ontologies using the
Web as a source for new instances. Varantola (2000) shows how translators can use
?just-in-time? sublanguage corpora to choose correct target language terms for areas
in which they are not expert. Fletcher (2002) demonstrates methods for gathering and
using Web corpora in a language-teaching context.
2.2 The 100M Words of the BNC
One hundred million words is a large enough corpus for many empirical strategies
for learning about language, either for linguists and lexicographers (Baker, Fillmore,
and Lowe 1998; Kilgarriff and Rundell 2002) or for technologies that need quantitative
information about the behavior of words as input (most notably parsers [Briscoe and
Carroll 1997; Korhonen 2000]). However, for some purposes, it is not large enough.
This is an outcome of the Zipfian nature of word frequencies. Although 100 million is
a huge number, and the BNC contains ample information on the dominant meanings
and usage patterns for the 10,000 words that make up the core of English, the bulk
of the lexical stock occurs less than 50 times in the BNC, which is not enough to
draw statistically stable conclusions about the word. For rarer words, rare meanings
of common words, and combinations of words, we frequently find no evidence at all.
Researchers are obliged to look to larger data sources (Keller and Lapata, this issue;
also Section 3.3). They find that probabilistic models of language based on very large
quantities of data, even if those data are noisy, are better than ones based on estimates
(using sophisticated smoothing techniques) from smaller, cleaner data sets.
Another argument is made vividly by Banko and Brill (2001). They explore the
performance of a number of machine learning algorithms (on a representative dis-
ambiguation task) as the size of the training corpus grows from a million to a bil-
lion words. All the algorithms steadily improve in performance, though the question
?Which is best?? gets different answers for different data sizes. The moral: Perfor-
mance improves with data size, and getting more data will make more difference than
fine-tuning algorithms.
2.3 Giving and Taking
Dragomir Radev has made a useful distinction between NLP ?giving? and ?taking.?4
NLP can give to the Web technologies such as summarization (for Web pages or
Web search results); machine translation; multilingual document retrieval; question-
answering and other strategies for finding not only the right document, but the right
part of a document; and tagging, parsing, and other core technologies (to improve
indexing for search engines, the viability of this being a central information retrieval
research question for the last 20 years). ?Taking? is, simply, using the Web as a source
of data for any CL or NLP goal and is the theme of this special issue. If we focus too
closely on the giving side of the equation, we look only at short to medium-term goals.
For the longer term, for ?giving? as well as for other purposes, a deeper understanding
of the linguistic nature of the Web and its potential for CL/NLP is required. For that,
we must take the Web itself, in whatever limited way, as an object of study.
Much Web search engine technology has been developed with reference to lan-
guage technology. The prototype for AltaVista was developed in a joint project be-
4 In remarks made in a panel discussion at the Empirical NLP Conference, Hong Kong, October 2002.
337
Kilgarriff and Grefenstette Web as Corpus: Introduction
tween Oxford University Press (exploring methods for corpus lexicography [Atkins
1993]) and DEC (interested in fast access to very large databases). Language identifi-
cation algorithms (Beesley 1988; Grefenstette 1995), now widely used in Web search
engines, were developed as NLP technology. The special issue explores a ?homecom-
ing? of Web technologies, with the Web now feeding one of the hands that fostered
it.
3. Web Size and the Multilingual Web
There were 56 million registered network addresses in July 1999, 125 million in January
2001, and 172 million in January 2003. A plot of this growth of the Web in terms of
computer hosts can easily be generated. Linguistic aspects take a little more work
and can be estimated only by sampling and extrapolation. Lawrence and Giles (1999)
compared the overlap between page lists returned by different Web browsers over the
same set of queries and estimated that, in 1999, there were 800 million indexable Web
pages available. By sampling pages, and estimating an average page length of seven
to eight kilobytes of nonmarkup text, they concluded that there might be six terabytes
of text available then. In 2003, Google claims to search four times this number of Web
pages, which raises the number of bytes of text available just through this one Web
server to over 20 terabytes from directly accessible Web pages. At an average of 10
bytes per word, a generous estimate for Latin-alphabet languages, that suggests two
thousand billion words.
The Web is clearly a multilingual corpus. How much of it is English? Xu (2000) es-
timated that 71% of the pages (453 million out of 634 million Web pages indexed by the
Excite engine at that time) were written in English, followed by Japanese (6.8%), Ger-
man (5.1%), French (1.8%), Chinese (1.5%), Spanish (1.1%), Italian (0.9%), and Swedish
(0.7%).
We have measured the counts of some English phrases according to various search
engines over time and compared them with counts in the BNC, which we know has
100 million words. Table 1 shows these counts in the BNC, on AltaVista in 1998 and
in 2001, and then on Alltheweb in 2003. For example, the phrase deep breath appears
732 times in the BNC. It was indexed 54,550 times by AltaVista in 1998. This rose
Table 1
Frequencies of English phrases in the BNC and on AltaVista in 1998 and 2001, and on
AlltheWeb in 2003. The counts for the BNC and AltaVista are for individual occurrences of the
phrase. The counts for AlltheWeb are page counts (the phrase may appear more than once on
any page).
Sample Phrase BNC WWW WWW WWW
(100 M) Fall 1998 Fall 2001 Spring 2003
medical treatment 414 46,064 627,522 1,539,367
prostate cancer 39 40,772 518,393 1,478,366
deep breath 732 54,550 170,921 868,631
acrylic paint 30 7,208 43,181 151,525
perfect balance 38 9,735 35,494 355,538
electromagnetic radiation 39 17,297 69,286 258,186
powerful force 71 17,391 52,710 249,940
concrete pipe 10 3,360 21,477 43,267
upholstery fabric 6 3,157 8,019 82,633
vital organ 46 7,371 28,829 35,819
338
Computational Linguistics Volume 29, Number 3
to 170,921 in 2001. And in 2003, we could find 868,631 Web pages containing the
contiguous words deep breath according to AlltheWeb. The numbers found through the
search engines are more than three orders of magnitude higher than the BNC counts,
giving a first indication of the size of the English corpus available on the Web.
We can derive a more precise estimate of the number of words available through
a search engine by using the counts of function words as predictors of corpus size.
Function words, such as the, with, and in, occur with a frequency that is relatively
stable over many different types of texts. From a corpus of known size, we can cal-
culate the frequency of the function words and extrapolate. In the 90-million-word
written-English component of the BNC, the appears 5,776,487 times, around seven
times for every 100 words. In the U.S. Declaration of Independence, the occurs 84
times. We predict that the Declaration is about 84 ? 100/7 = 1,200 words long. In fact,
the text contains about 1,500 words. Using the frequency of one word gives a first
approximation. A better result can be obtained by using more data points.
From the first megabyte of the German text found in the European Corpus Ini-
tiative Multilingual Corpus,5 we extracted frequencies for function words and other
short, common words. We removed from the list words that were also common words
in other languages.6 AltaVista provided, on its results pages, along with a page count
for a query, the number of times that each query word was found on the Web.7 Ta-
ble 2 shows the relative frequency of the words from our known corpus, the index
frequencies that AltaVista gave (February 2000), and the consequent estimates of the
size of the German-language Web indexed by AltaVista.
We set aside words which give discrepant predictions (too high or too low) as (1)
AltaVista does not record in its index the language a word comes from, so the count
for the string die includes both the German and English occurrences, and (2) a word
might be under- or overrepresented in the training corpus or on the Web (consider
here, which occurs very often in ?click here?). Averaging the remaining predictions
gives an estimate of three billion words of German that could be accessed through
AltaVista on the day in February 2000 that we conducted our test.
Table 2
Short German words in the ECI corpus and via AltaVista, giving German Web estimates.
Word Known-Size-Corpus AltaVista Prediction for
Relative Frequency Frequency German-Language Web
oder 0.00561180 13,566,463 2,417,488,684
sind 0.00477555 11,944,284 2,501,132,644
auch 0.00581108 15,504,327 2,668,062,907
wird 0.00400690 11,286,438 2,816,750,605
nicht 0.00646585 18,294,174 2,829,353,294
eine 0.00691066 19,739,540 2,856,389,983
sich 0.00604594 17,547,518 2,902,363,900
ist 0.00886430 26,429,327 2,981,546,991
auf 0.00744444 24,852,802 3,338,438,082
und 0.02892370 101,250,806 3,500,617,348
Average 3,068,760,356
5 ?http://www.elsnet.org/resources/eciCorpus.html?.
6 These lists of short words and frequencies were initially used to create a language identifier.
7 AltaVista has recently stopped providing information about how often individual words in a query
have been indexed and now returns only a page count for the entire query.
339
Kilgarriff and Grefenstette Web as Corpus: Introduction
Table 3
Estimates of Web size in words, as indexed by AltaVista, for various languages.
Language Web Size
Albanian 10,332,000
Breton 12,705,000
Welsh 14,993,000
Lithuanian 35,426,000
Latvian 39,679,000
Icelandic 53,941,000
Basque 55,340,000
Latin 55,943,000
Esperanto 57,154,000
Roumanian 86,392,000
Irish 88,283,000
Estonian 98,066,000
Slovenian 119,153,000
Croatian 136,073,000
Malay 157,241,000
Turkish 187,356,000
Language Web Size
Catalan 203,592,000
Slovakian 216,595,000
Polish 322,283,000
Finnish 326,379,000
Danish 346,945,000
Hungarian 457,522,000
Czech 520,181,000
Norwegian 609,934,000
Swedish 1,003,075,000
Dutch 1,063,012,000
Portuguese 1,333,664,000
Italian 1,845,026,000
Spanish 2,658,631,000
French 3,836,874,000
German 7,035,850,000
English 76,598,718,000
This technique has been tested on controlled data (Grefenstette and Nioche 2000)
in which corpora of different languages were mixed in various proportions and found
to give reliable results. Table 3 provides estimates for the number of words that
were available in 30 different Latin-script languages through AltaVista in March 2001.
English led the pack with 76 billion words, and seven additional languages already
had over a billion.
From the table, we see that even ?smaller? languages such as Slovenian, Croatian,
Malay, and Turkish have more than one hundred million words on the Web. Much of
the research that has been undertaken on the BNC simply exploits its scale and could
be transferred directly to these languages.
The numbers presented in Table 3 are lower bounds, for a number of reasons:
? AltaVista covers only a fraction of the indexable Web pages available
(the fraction was estimated at just 15% by Lawrence and Giles [1999]).
? AltaVista may be biased toward North American (mainly
English-language) pages by the strategy it uses to crawl the Web.
? AltaVista indexes only pages that can be directly called by a URL and
does not index text found in databases that are accessible through dialog
windows on Web pages (the ?hidden Web?). This hidden Web is vast
(consider MedLine,8 just one such database, with more than five billion
words; see also Ipeirotis, Gravano, and Sahami [2001]), and it is not
considered at all in the AltaVista estimates.
Repeating the procedure after an interval, the second author and Nioche showed
that the proportion of non-English text to English is growing. In October 1996 there
8 ?http://www4.ncbi.nlm.nih.gov/PubMed/?.
340
Computational Linguistics Volume 29, Number 3
Table 4
AltaVista frequencies for candidate translations of groupe de travail.
labor cluster 21
labor grouping 28
labour concern 45
labor concern 77
work grouping 124
work cluster 279
labor collective 423
labour collective 428
work collective 759
work concern 772
labor group 3,977
labour group 10,389
work group 148,331
were 38 German words for every 1,000 words of English indexed by AltaVista. In
August 1999, there were 71, and in March 2001, 92.
3.1 Finding the Right Translation
How can these large numbers be used for other language-processing tasks? Consider
the compositional French noun phrase groupe de travail. In the MEMODATA bilingual
dictionary,9 the French word groupe is translated by the English words cluster, group,
grouping, concern, and collective. The French word travail translates as work, labor, or
labour. Many Web search engines allow the user to search for adjacent phrases. Com-
bining the possible translations of groupe de travail and submitting them to AltaVista
in early 2003 yielded the counts presented in Table 4. The phrase work group is 15
times more frequent than any other and is also the best translation among the tested
possibilities. A set of controlled experiments of this form is described in Grefenstette
(1999). In Grefenstette?s study, a good translation was found in 87% of ambiguous
cases from German to English and 86% of ambiguous cases from Spanish to English.
4. Representativeness
We know the Web is big, but a common response to a plan to use the Web as a
corpus is ?but it?s not representative.? There are a great many things to be said about
this. It opens up a pressing yet alost untouched practical and theoretical issue for
computational linguistics and language technology.
4.1 Theory
First, ?representativeness? begs the question ?representative of what?? Outside very
narrow, specialized domains, we do not know with any precision what existing corpora
might be representative of. If we wish to develop a corpus of general English, we
may think it should be representative of general English, so we then need to define
the population of ?general English-language events? of which the corpus will be a
sample. Consider the following issues:
? Production and reception: Is a language event an event of speaking or
writing, or one of reading or hearing? Standard conversations have, for
each utterance, one speaker and one hearer. A Times newspaper article
has (roughly) one writer and several hundred thousand readers.
9 See ?http://www.elda.fr/cata/text/M0001.html?. The basic multilingual lexicon produced by
MEMODATA contains 30,000 entries for five languages: French, English, Italian, German, Spanish.
341
Kilgarriff and Grefenstette Web as Corpus: Introduction
? Speech and text: Do speech events and written events have the same
status? It seems likely that there are orders of magnitude more speech
events than writing events, yet most corpus research to date has tended
to focus on the more tractable task of gathering and working with text.
? Background language: Does muttering under one?s breath or talking in
one?s sleep constitute a speech event, and does doodling with words
constitute a writing event? Or, on the reception side, does passing (and
possibly subliminally reading) a roadside advertisement constitute a
reading event? And what of having the radio on but not attending to it,
or the conversational murmur in a restaurant?
? Copying: if I?d like to teach the world to sing, and, like Michael Jackson or
the Spice Girls, am fairly successful in this goal and everyone sings my
song, then does each individual singing constitute a distinct language
production event?
In the text domain, organizations such as Reuters produce news feeds
that are typically adapted to the style of a particular newspaper and then
republished: Is each republication a new writing event? (These issues,
and related themes of cut-and-paste authorship, ownership, and
plagiarism, are explored in Wilks [2003].)
4.2 Technology
Application developers urgently need to know what to do about sublanguages. It
has often been argued that, within a sublanguage, few words are ambiguous, and a
limited repertoire of grammatical structures is used (Kittredge and Lehrberger 1982).
This points to sublanguage-specific application development?s being substantially sim-
pler than general-language application development. However, many of the resources
that developers may wish to use are general-language resources, such as, for English,
WordNet, ANLT, XTag, COMLEX, and the BNC. Are they relevant for building ap-
plications for sublanguages? Can they be used? Is it better to use a language model
based on a large general-language corpus or a relatively tiny corpus of the right kind
of text? Nobody knows. There is currently no theory, no mathematical models, and
almost no discussion.
A related issue is that of porting an application from the sublanguage for which
it was developed to another. It should be possible to use corpora for the two sublan-
guages to estimate how large a task this will be, but again, our understanding is in
its infancy.
4.3 Language Modeling
Much work in recent years has gone into developing language models. Clearly, the
statistics for different types of text will be different (Biber 1993). This imposes a lim-
itation on the applicability of any language model: We can be confident only that
it predicts the behavior of language samples of the same text type as the training-
data text type (and we can be entirely confident only if training and test samples are
random samples from the same source).
When a language technology application is put to use, it will be applied to new
text for which we cannot guarantee the text type characteristics. There is little work
on assessing how well one language model fares when applied to a text type that is
different from that of the training corpus. Two studies in this area are Sekine (1997)
and Gildea (2001), both of which show substantial variation in model performance
342
Computational Linguistics Volume 29, Number 3
Table 5
Hits for Spanish pensar que with and without possible ?dequeismos errors? (spurious de
between the verb and the relative), from Alltheweb.com (March 2003). Not all items are errors
(e.g., ?. . .pienso de que manera. . .? . . . think how. . .). The correct form is always at least 500
times more common than any potentially incorrect form.
pienso de que 388
pienso que 356,874
piensas de que 173
piensas que 84,896
piense de que 92
piense que 67,243
pensar de que 1,640
pensar que 661,883
when the training corpus changes. The lack of theory of text types leaves us without
a way of assessing the usefulness of language-modeling work.
4.4 Language Errors
Web texts are produced by a wide variety of authors. In contrast to paper-based, copy-
edited published texts, Web-based texts may be produced cheaply and rapidly with
little concern for correctness. On Google a search for ?I beleave? has 3,910 hits, and
?I beleive,? 70,900. The correct ?I believe? appears on over four million pages. Table 5
presents what is regarded as a common grammatical error in Spanish, comparing the
frequency of such forms to the accepted forms on the Web. All the ?erroneous? forms
exist, but much less often than the ?correct? forms. The Web is a dirty corpus, but
expected usage is much more frequent than what might be considered noise.
4.5 Sublanguages and General-Language-Corpus Composition
A language can be seen as a modest core of lexis, grammar, and constructions, plus
a wide array of different sublanguages, as used in each of a myriad of human ac-
tivities. This presents a challenge to general-language resource developers: Should
sublanguages be included? The three possible positions are
? No, none should.
? Some, but not all, should.
? Yes, all should.
The problem with the first position is that, with all sublanguages removed, the
residual core gives an impoverished view of language (quite apart from demarcation
issues and the problem of determining what is left). The problem with the second is
that it is arbitrary. The BNC happens to include cake recipes and research papers on
gastro-uterine diseases, but not car manuals or astronomy texts. The third has not,
until recently, been a viable option.
4.6 Literature
To date, corpus developers have been obliged to make pragmatic decisions about the
sorts of text to go into a corpus. Atkins, Clear, and Ostler (1992) describe the desiderata
and criteria used for the BNC, and this stands as a good model for a general-purpose,
general-language corpus. The word representative has tended to fall out of discussions,
to be replaced by the meeker balanced.
343
Kilgarriff and Grefenstette Web as Corpus: Introduction
The recent history of mathematically sophisticated modeling of language variation
begins with Biber (1988), who identifies and quantifies the linguistic features associated
with different spoken and written text types. Habert and colleagues (Folch et al 2000;
Beaudouin et al 2001) have been developing a workstation for specifying subcorpora
according to text type, using Biber-style analyses, among others. In Kilgarriff (2001)
we present a first pass at quantifying similarity between corpora, and Cavaglia (2002)
continues this line of work. As mentioned above, Sekine (1997) and Gildea (2001)
directly address the relation between NLP systems and text type; one further such item
is Roland et al (2000). Buitelaar and Sacaleanu (2001) explores the relation between
domain and sense disambiguation. A practical discussion of a central technical concern
is Vossen (2001), which tailors a general-language resource for a domain.
Baayen (2001) presents sophisticated mathematical models for word frequency
distributions, and it is likely that his mixture models have potential for modeling
sublanguage mixtures. His models have been developed with a specific, descriptive
goal in mind and using a small number of short texts: It is unclear whether they can
be usefully applied in NLP.
Although the extensive literature on text classification (Manning and Schu?tze 1999,
pages 575?608) is certainly relevant, it most often starts from a given set of categories
and cannot readily be applied to the situation in which the categories are not known in
advance. Also, the focus is usually on content words and topics or domains, with other
differences of genre or sublanguage remaining unexamined. Exceptions focusing on
genre include Kessler, Nunberg, and Schu?tze (1997) and Karlgren and Cutting (1994).
4.7 Representativeness: Conclusion
The Web is not representative of anything else. But neither are other corpora, in any
well-understood sense. Picking away at the question merely exposes how primitive
our understanding of the topic is and leads inexorably to larger and altogether more
interesting questions about the nature of language, and how it might be modeled.
?Text type? is an area in which our understanding is, as yet, very limited. Although
further work is required irrespective of the Web, the use of the Web forces the issue.
Where researchers use established corpora, such as Brown, the BNC, or the Penn
Treebank, researchers and readers are willing to accept the corpus name as a label for
the type of text occurring in it without asking critical questions. Once we move to the
Web as a source of data, and our corpora have names like ?April03-sample77,? the
issue of how the text type(s) can be characterized demands attention.
5. Introduction to Articles in This Special Issue
One use of a corpus is to extract a language model: a list of weighted words, or
combinations of words, that describe (1) how words are related, (2) how they are
used with each other, and (3) how common they are in a given domain. Language
models are used in speech processing to predict which word combinations are likely
interpretations of a sound stream, in information retrieval to decide which words are
useful indicators of a topic, and in machine translation to identify good translation
candidates.
In this volume, Celina Santamari?a, Julio Gonzalo, and Felisa Verdejo describe how
to build sense-tagged corpora from the Web by associating word meanings with Web
page directory nodes. The Open Directory Project (at ?dmoz.org?) is a collaborative,
volunteer project for classifying Web pages into a taxonomic hierarchy. Santamari?a et
al. present an algorithm for attaching WordNet word senses to nodes in this same
taxonomy, thus providing automatically created links between word senses and Web
344
Computational Linguistics Volume 29, Number 3
pages. They also show how this method can be used for automatic acquisition of
sense-tagged corpora, from which one could, among other things, produce language
models tied to certain senses of words, or for a certain domain.
Unseen words, or word sequences?that is, words or sequences not occurring in
training data?are a problem for language models. If the corpus from which a particu-
lar model is extracted is too small, there are many such sequences. Taking the second
author?s work, as described above, as a starting point, Frank Keller and Mirella Lapata
examine how useful the Web is as a source of frequency information for rare items:
specifically, for dependency relations involving two English words such as <fulfill OB-
JECT obligation>. They generate pairs of common words, constructing combinations
that are and are not attested in the BNC. They then compare the frequency of these
combinations in a larger 325-million-word corpus and on the Web. They find that Web
frequency counts are consistent with those for other large corpora. They also report
on a series of human-subject experiments in which they establish that Web statistics
are good at predicting the intuitive plausibility of predicate-argument pairs. Other
experiments discussed in their article show that Web counts correlate reliably with
counts re-created using class-based smoothing and overcome some problems of data
sparseness in the BNC.
Other very large corpora are available for English (English is an exception), and
the other three papers in the special issue all exploit the multilinguality of the Web.
Andy Way and Nano Gough show how the Web can provide data for an example-
based machine translation (Nagao 1984) system. First, they extract 200,000 phrases
from a parsed corpus. These phrases are sent to three online translation systems. Both
original phrases and translations are chunked. From these pairings a set of chunk
translations is extracted to be applied in a piecewise fashion to new input text. The
authors use the Web again at a final stage to rerank possible translations by verifying
which subsequences among the possible translations are most attested.
The two remaining articles present methods for building aligned bilingual corpora
from the Web. It seems plausible that such automatic construction of translation dic-
tionaries can palliate the lack of translation resources for many language pairs. Philip
Resnik was the first to recognize that it is possible to build large parallel bilingual
corpora from the Web. He found that one can exploit the appearance of language
flags and other clues that often lead to a version of the same page in a different
language.10 In this issue, Resnik and Noah Smith present their STRAND system for
building bilingual corpora from the Web.
An alternative method is presented by Wessel Kraaij, Jian-Yun Nie, and Michel
Simard. They use the resulting parallel corpora to induce a probabilistic translation
dictionary that is then embedded into a cross-language information retrieval system.
Various alternative embeddings are evaluated using the CLEF (Peters 2001) multilin-
gual information retrieval test beds.
6. Prospects
The default means of access to the Web is through a search engine such as Google.
Although the Web search engines are dazzlingly efficient pieces of technology and
excellent at the task they set for themselves, for the linguist they are frustrating:
10 For example, one can find Azerbaijan news feeds online at ?http://www.525ci.com? in Azeri (written
with a Turkish code set), and on the same page are pointers to versions of the same stories in English
and in Russian.
345
Kilgarriff and Grefenstette Web as Corpus: Introduction
? The search engine results do not present enough instances (1,000 or 5,000
maximum).
? They do not present enough context for each instance (Google provides a
fragment of around ten words).
? They are selected according to criteria that are, from a linguistic
perspective, distorting (with uses of the search term in titles and
headings going to the top of the list and often occupying all the top
slots).
? They do not allow searches to be specified according to linguistic criteria
such as the citation form for a word, or word class.
? The statistics are unreliable, with frequencies given for ?pages containing
x? varying according to search engine load and many other factors.
If only these constraints were removed, a search engine would be a wonderful
tool for language researchers. Each of the constraints could straightforwardly be re-
solved by search engine designers, but linguists are not a powerful lobby, and search
engine company priorities will never perfectly match our community?s. This suggests
a better solution: Do it ourselves. Then the kinds of processing and querying would
be designed explicitly to meet linguists? desiderata, without any conflict of interest or
?poor relation? role. Large numbers of possibilities open up. All those processes of
linguistic enrichment that have been applied with impressive effect to smaller corpora
could be applied to the Web. We could parse the Web. Web searches could be specified
in terms of lemmas, constituents (e.g., noun phrase), and grammatical relations rather
than strings. The way would be open for further anatomizing of Web text types and
domains. Thesauruses and lexicons could be developed directly from the Web. And
all for a multiplicity of languages.11
The Web contains enormous quantities of text, in numerous languages and lan-
guage types, on a vast array of topics. Our take on the Web is that it is a fabulous
linguists? playground. We hope the special issue will encourage you to come on out
and play!
References
Agirre, Eneko, Olatz Ansa, Eduard Hovy
and David Martinez. 2000. Enriching very
large ontologies using the WWW. In
Proceedings of the Ontology Learning
Workshop of the European Conference of AI
(ECAI), Berlin.
Atkins, Sue. 1993. Tools for computer-aided
corpus lexicography: The Hector project.
Acta Linguistica Hungarica, 41:5?72.
Atkins, Sue, Jeremy Clear, and Nicholas
Ostler. 1992. Corpus design criteria.
Literary and Linguistic Computing, 7(1):1?16.
Baayen, Harald. 2001. Word Frequency
Distributions. Kluwer, Dordrecht.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of
COLING-ACL, pages 86?90, Montreal,
August.
Banko, Michele and Eric Brill. 2001. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics and the
10th Conference of the European Chapter of the
Association for Computational Linguistics,
Toulouse.
Beaudouin, Vale?rie, Serge Fleury, Beno??t
Habert, Gabriel Illouz, Christian Licoppe,
and Marie Pasquier. 2001. Typweb:
de?crire la toile pour mieux comprendre
les parcours. In Colloque International sur
les Usages et les Services des
Te?le?communications (CIUST?01), Paris,
June. Available at ?http://www.cavi.univ-
11 The idea is developed further in Grefenstette (2001) and in Kilgarriff (2003).
346
Computational Linguistics Volume 29, Number 3
paris3.fr/ilpga/ilpga/sfleury/typweb.htm?.
Beesley, Kenneth R. 1988. Language
identifier: A computer program for
automatic natural-language identification
of on-line text. In Language at Crossroads:
Proceedings of the 29th Annual Conference of
the American Translators Association, pages
47?54, October 12?16.
Biber, Douglas. 1988. Variation across speech
and writing. Cambridge University Press,
Cambridge.
Biber, Douglas. 1993. Using
register-diversified corpora for general
language studies. Computational Linguistics,
19(2):219?242.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC, April.
Buitelaar, Paul and Bogdan Sacaleanu. 2001.
Ranking and selecting synsets by domain
relevance. In Proceedings of the Workshop on
WordNet and Other Lexical Resources:
Applications, Extensions and Customizations,
NAACL, Pittsburgh, June.
Burnard, Lou. 1995. The BNC Reference
Manual. Oxford University Computing
Service, Oxford.
Cavaglia, Gabriela. 2002. Measuring corpus
homogeneity using a range of measures
for inter-document distance. In Proceedings
of the Third International Conference on
Language Resources and Evaluation, pages
426?431, Las Palmas de Gran Canaria,
Spain, May.
Church, Kenneth W. and Robert L. Mercer.
1993. Introduction to the special issue on
computational linguistics using large
corpora. Computational Linguistics,
19(1):1?24.
Dumais, Susan, Michele Banko, Eric Brill,
Jimmy Lin, and Andrew Ng. 2002. Web
question answering: Is more always
better? In Proceedings of the 25th ACM
SIGIR, pages 291?298, Tampere, Finland.
Fletcher, William. 2002. Facilitating
compilation and dissemination of ad-hoc
web corpora. In Teaching and Language
Corpora 2002. Available at ?http://
miniappolis.com/KWiCFinder/
KWiCFinder.html?.
Folch, Helka, Serge Heiden, Beno??t Habert,
Serge Fleury, Gabriel Illouz, Pierre Lafon,
Julien Nioche, and Sophie Pre?vost. 2000.
Typtex: Inductive typological text
classification by multivariate statistical
analysis for NLP systems
tuning/evaluation. In Proceedings of the
Second Language Resources and Evaluation
Conference, pages 141?148, Athens,
May?June.
Fujii, Atsushi and Tetsuya Ishikawa. 2000.
Utilizing the World Wide Web as an
encyclopedia: Extracting term
descriptions from semi-structured text. In
Proceedings of the 38th Meeting of the ACL,
pages 488?495, Hong Kong, October.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
Conference on Empirical Methods in NLP,
Pittsburgh, PA.
Greenwood, Mark, Ian Roberts, and Robert
Gaizauskas. 2002. University of Sheffield
TREC 2002 Q & A system. In E. M.
Voorhees and Lori P. Buckland, editors,
The Eleventh Text Retrieval Conference
(TREC-11), Washington. U.S. Government
Printing Office.
Grefenstette, Gregory. 1995. Comparing two
language identification schemes. In
Proceedings of the Third International
Conference on the Statistical Analysis of
Textual Data (JADT?95), pages 263?268,
Rome, December 11?13. Available at
?www.xrce.xerox.com/competencies/content-
analysis/publications/Documents/P49030/
content/gg aslib.pdf?.
Grefenstette, Gregory. 1999. The WWW as a
resource for example-based MT tasks.
Paper presented at ASLIB ?Translating
and the Computer? conference, London,
October.
Grefenstette, Gregory. 2001. Very large
lexicons. In Walter Daelemans, Khalil
Simaan, Jakub Zavrel, and Jorn Veenstra,
editors, Computational Linguistics in the
Netherlands 2000: Selected Papers from the
Eleventh CLIN Meeting, Language and
Computers 37. Rodopi, Amsterdam.
Grefenstette, Gregory and Julien Nioche.
2000. Estimation of english and
non-english language use on the WWW.
In Proceedings of the RIAO (Recherche
d?Informations Assiste?e par Ordinateur),
pages 237?246, Paris.
Hawking, D., E. Voorhees, N. Craswell, and
P. Bailey. 1999. Overview of the TREC8
Web track. In Proceedings of the Eighth Text
Retrieval Conference, Gaithersburg,
Maryland, November.
Ipeirotis, Panagiotis G., Luis Gravano, and
Mehran Sahami. 2001. Probe, count, and
classify: Categorizing hidden Web
databases. In Proceedings of the SIGMOD
Conference, Santa Barbara, CA.
Jones, Rosie and Rayid Ghani. 2000.
Automatically building a corpus for a
minority language from the Web. In
Proceedings of the Student Workshop of the
38th Annual Meeting of the Association for
347
Kilgarriff and Grefenstette Web as Corpus: Introduction
Computational Linguistics, Hong Kong,
pages 29?36.
Karlgren, Jussi and Douglass Cutting. 1994.
Recognizing text genres with simple
metrics using discriminant analysis. In
Proceedings of COLING-94, pages
1071?1075, Kyoto, Japan.
Kessler, Brett, Geoffrey Nunberg, and
Hinrich Schu?tze. 1997. Automatic
detection of text genre. In Proceedings of
ACL and EACL, pages 39?47, Madrid.
Kilgarriff, Adam. 2001. Comparing corpora.
International Journal of Corpus Linguistics,
6(1):1?37.
Kilgarriff, Adam. 2003. Linguistic search
engine. In Kiril Simov, editor, Shallow
Processing of Large Corpora: Workshop Held in
Association with Corpus Linguistics 2003,
Lancaster, England, March.
Kilgarriff, Adam and Michael Rundell. 2002.
Lexical profiling software and its
lexicographical applications?A case
study. In Proceedings of EURALEX ?02,
Copenhagen, August.
Kittredge, Richard and John Lehrberger.
1982. Sublanguage: Studies of Language in
Restricted Semantic Domains. De Gruyter,
Berlin.
Korhonen, Anna. 2000. Using semantically
motivated estimates to help
subcategorization acquisition. In
Proceedings of the Joint Conference on
Empirical Methods in NLP and Very Large
Corpora, pages 216?223, Hong Kong,
October.
Lawrence, Steve and C. Lee Giles. 1999.
Accessibility of information on the Web.
Nature, 400:107?109.
Manning, Christopher and Hinrich Schu?tze.
1999. Foundations of Statistical Natural
Language Processing. MIT Press, Cambridge.
McEnery, Tony and Andrew Wilson. 1996.
Corpus Linguistics. Edinburgh University
Press, Edinburgh.
Mihalcea, Rada and Dan Moldovan. 1999. A
method for word sense disambiguation of
unrestricted text. In Proceedings of the 37th
Meeting of ACL, pages 152?158, College
Park, MD, June.
Nagao, Makoto. 1984. A framework of a
mechanical translation between Japanese
and English by analogy principle. In Alick
Elithorn and Ranan Banerji, editors,
Artificial and Human Intelligence.
North-Holland, Edinburgh, pages 173?180.
Peters, Carol, editor. 2001. Cross-Language
Information Retrieval and Evaluation,
Workshop of Cross-Language Evaluation Forum
(CLEF 2000) Lisbon, Portugal, September
21?22, 2000, Revised Papers. Lecture Notes
in Computer Science. Springer-Verlag.
Resnik, Philip. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Meeting of ACL, pages 527?534, College
Park, MD, June.
Rigau, German, Bernardo Magnini, Eneko
Agirre, and John Carroll. 2002. Meaning:
A roadmap to knowledge technologies. In
Proceedings of COLING Workshop on A
Roadmap for Computational Linguistics,
Taipei, Taiwan.
Roland, Douglas, Daniel Jurafsky, Lise
Menn, Susanne Gahl, Elizabeth Elder, and
Chris Riddoch. 2000. Verb
subcategorization frequency differences
between business-news and balanced
corpora: The role of verb sense. In
Proceedings of the Workshop on Comparing
Corpora, 38th ACL, Hong Kong, October.
Sekine, Satshi. 1997. The domain
dependence of parsing. In Proceedings of
the Fifth Conference on Applied Natural
Language Processing, pages 96?102,
Washington, DC, April.
Sinclair, John M., editor. 1987. Looking Up:
An Account of the COBUILD Project in
Lexical Computing. Collins, London.
Varantola, Krista. 2000. Translators and
disposable corpora. In Proceedings of CULT
(Corpus Use and Learning to Translate),
Bertinoro, Italy, November.
Villasenor-Pineda, L., M. Montes y Go?mez,
M. Pe?rez-Coutino, and D. Vaufreydaz.
2003. A corpus balancing method for
language model construction. In Fourth
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing-2003), pages 393?401, Mexico
City, February.
Volk, Martin. 2001. Exploiting the WWW as
a corpus to resolve PP attachment
ambiguities. In Proceedings of Corpus
Linguistics 2001, Lancaster, England.
Vossen, Piek. 2001. Extending, trimming
and fusing WordNet for technical
documents. In Proceedings of the NAACL
2001 Workshop on WordNet and Other Lexical
Resources, Pittsburgh, June. Available at
?http://engr.smu.edu/?rada/mwnw/
papers/WNW-NACL-205.pdf.gz?.
Wilks, Yorick. 2003. On the ownership of
text. Computers and the Humanities.
Forthcoming.
Xu, J. L. 2000. Multilingual search on the
World Wide Web. In Proceedings of the
Hawaii International Conference on System
Science (HICSS-33), Maui, Hawaii, January.
Zheng, Zhiping. 2002. AnswerBus question
answering system. In E. M. Voorhees and
Lori P. Buckland, editors, Proceedings of
HLT Human Language Technology Conference
(HLT 2002), San Diego, CA, March 24?27.
