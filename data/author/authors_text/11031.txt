Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 638?647,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Improving Verb Clustering with Automatically Acquired Selectional
Preferences
Lin Sun and Anna Korhonen
University of Cambridge, Computer Laboratory
15 JJ Thomson Avenue, Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk
Abstract
In previous research in automatic verb
classification, syntactic features have
proved the most useful features, although
manual classifications rely heavily on se-
mantic features. We show, in contrast
with previous work, that considerable ad-
ditional improvement can be obtained by
using semantic features in automatic clas-
sification: verb selectional preferences ac-
quired from corpus data using a fully unsu-
pervised method. We report these promis-
ing results using a new framework for
verb clustering which incorporates a re-
cent subcategorization acquisition system,
rich syntactic-semantic feature sets, and
a variation of spectral clustering which
performs particularly well in high dimen-
sional feature space.
1 Introduction
Verb classifications have attracted a great deal
of interest in natural language processing (NLP).
They have proved useful for various important
NLP tasks and applications, including e.g. parsing,
word sense disambiguation, semantic role label-
ing, information extraction, question-answering,
and machine translation (Swier and Stevenson,
2004; Dang, 2004; Shi and Mihalcea, 2005; Za-
pirain et al, 2008).
Verb classes are useful because they offer a
powerful tool for generalization and abstraction
which can be beneficial when faced e.g. with the
problem of data sparsity. Particularly useful can
be classes which capture generalizations over a
range of (cross-)linguistic properties, such as the
ones proposed by Levin (1993). Being defined in
terms of similar meaning and (morpho-)syntactic
behaviour of words, Levin style classes gener-
ally incorporate a wider range of properties than
e.g. classes defined solely on semantic grounds
(Miller, 1995).
In recent years, a variety of approaches have
been proposed for automatic induction of verb
classes from corpus data (Schulte im Walde, 2006;
Joanis et al, 2008; Sun et al, 2008; Li and Brew,
2008; Korhonen et al, 2008;
?
O S?eaghdha and
Copestake, 2008; Vlachos et al, 2009). This work
opens up the opportunity of learning and tuning
classifications tailored to the application and do-
main in question. Although manual classification
may always yields higher accuracy, automatic verb
classification is cost-effective and gathers statisti-
cal information as a side-effect of the acquisition
process which is difficult for humans to gather but
can be highly useful for NLP applications.
To date, both supervised and unsupervised ma-
chine learning (ML) methods have been proposed
for verb classification and used to classify a vari-
ety of features extracted from raw, tagged and/or
parsed corpus data. The best performing features
on cross-domain verb classification have been syn-
tactic in nature (e.g. syntactic slots, subcategoriza-
tion frames (SCFs)). Disappointingly, semantic
features have not yielded significant additional im-
provement, although they play a key role in man-
ual and theoretical work on verb classification and
could thus be expected to offer a considerable con-
tribution to classification performance.
Since the accuracy of automatic verb classifi-
cation shows room for improvement, we further
investigate the potential of semantic features ?
verb selectional preferences (SPs) ? for the task.
We introduce a novel approach to verb cluster-
ing which involves the use of (i) a recent subcate-
gorization frame (SCF) acquisition system (Preiss
et al, 2007) which produces rich lexical, SCF and
syntactic data, (ii) novel syntactic-semantic fea-
ture sets extracted from this data which incorpo-
rate a variety of linguistic information, including
SPs, and (iii) a new variation of spectral cluster-
638
ing based on the MNCut algorithm (Meila and Shi,
2001) which is well-suited for dealing with the re-
sulting, high dimensional feature space.
Using this approach, we show on two well-
established test sets that automatically acquired
SPs can be highly useful for verb clustering. They
yield high performance when used in combination
with syntactic features. We obtain our promis-
ing results using a fully unsupervised approach
to SP acquisition which differs from previous
approaches in that it does not exploit WordNet
(Miller, 1995) or other lexical resources. It is
based on clustering argument head data in the
grammatical relations associated with verbs.
We describe our features in section 2 and the
clustering methods in section 3. Experimental
evaluation and results are reported in sections 4
and 5, respectively. Section 6 provides discus-
sion and describes related work, and section 7 con-
cludes.
2 Features
Our target classification is the taxonomy of Levin
(1993) where verbs taking similar diathesis al-
ternations are assumed to share meaning compo-
nents and are organized into semantically coherent
classes. The main feature of this classification is a
diathesis alternation which manifests at the level
of syntax in alternating sets of SCF (e.g. in the
causative/inchoative alternation an NP frame alter-
nates with an intransitive frame: Tony broke the
window? The window broke).
Since automatic detection of diathesis alterna-
tions is very challenging (McCarthy, 2001), most
work on automatic classification has exploited the
fact that similar alternations tend to result in sim-
ilar SCFs. The research reported so far
1
has used
mainly syntactic features for classification, rang-
ing from shallow syntactic slots (e.g. NPs preced-
ing or following the verb) to SCFs. Some re-
searchers have discovered that supplementing ba-
sic syntactic features with information about ad-
juncts, co-occurrences, tense, and/or voice of the
verb have resulted in better performance.
However, additional information about seman-
tic SPs of verbs has not yielded considerable im-
provement on verb classification although SPs can
be strong indicators of diathesis alternations (Mc-
Carthy, 2001) and although fairly precise semantic
descriptions, including information about verb se-
1
See section 6 for discussion on previous work.
lectional restrictions, can be assigned to the major-
ity of Levin classes, as demonstrated by VerbNet
(Kipper-Schuler, 2005).
SP acquisition from undisambiguated corpus
data is arguably challenging (Brockmann and La-
pata, 2003; Erk, 2007; Bergsma et al, 2008). It is
especially challenging in the context of verb clas-
sification where SP models are needed for specific
syntactic slots for which the data may be sparse,
and the resulting feature vectors integrating both
syntactic and semantic features may be high di-
mensional. However, we wanted to investigate
whether better results could be obtained if the fea-
tures were optimised for richness, the feature ex-
traction for accuracy, and a clustering method ca-
pable of dealing with the resulting high dimen-
sional feature space was employed.
2.1 Feature extraction
We adopted a recent SCF acquisition system which
has proved more accurate than previous compa-
rable systems
2
but which has not been employed
for verb clustering before: the system of Preiss
et al (2007). This system tags, lemmatizes and
parses corpus data using the current version of the
RASP (Robust Accurate Statistical Parsing) toolkit
(Briscoe et al, 2006), and on the basis of resulting
grammatical relations (GRs) assigns each occur-
rence of a verb to one of 168 verbal SCFs classes
3
.
The system provides a filter which can be used
to remove adjuncts from the resulting lexicon.
We do not employ this filter since adjuncts have
proved informative for verb classification (Sun
et al, 2008; Joanis et al, 2008). However, we
do frequency-based thresholding to minimise the
noise (e.g. erroneous scfs) and sparse data in verb
classification and to ensure that only features sup-
ported by several verbs are used in classification:
we only consider SCFs and GRs which have fre-
quency larger than 40 with 5 or more verbs
4
.
The system produces a rich lexicon which in-
cludes raw and processed input sentences and pro-
vides a variety of material for verb clustering, in-
cluding e.g. (statistical) information related to the
part-of-speech (POS) tags, GRs, SCFs, argument
heads, and adjuncts of verbs. Using this mate-
rial, we constructed a wide range of feature sets
2
See Preiss et al (2007) for the details of evaluation.
3
We used an implementation of the SCF classifier pro-
vided by Paula Buttery.
4
These and other threshold values mentioned in this paper
were determined empirically on corpus data.
639
for experimentation, both shallow and deep syn-
tactic and semantic features. As described below,
some of the feature types have been employed in
previous works and some are novel.
2.2 Feature sets
The first feature set F1 includes information
about the lexical context (co-occurrences) of verbs
which has proved useful for supervised verb clas-
sification (Li and Brew, 2008):
F1: Co-occurrence (CO): We adopt the best
method of Li and Brew (2008) where collo-
cations are extracted from the four words im-
mediately preceding and following a lemma-
tized verb. Stop words are removed prior to
extraction, and the 600 most frequent result-
ing COs are kept.
F2-F3 provide information about lexical prefer-
ences of verbs in argument head positions of spe-
cific GRs associated with the verb:
F2: Prepositional preference (PP): the type and
frequency of prepositions in the indirect ob-
ject relation.
F3: Lexical preference (LP): the type and fre-
quency of nouns and prepositions in the sub-
ject, object, and indirect object relation.
All the other feature sets include information
about SCFs which have been widely employed in
verb classification, e.g. (Schulte im Walde, 2006;
Sun et al, 2008; Li and Brew, 2008; Korhonen
et al, 2008). F4-F7 include basic SCF information
and/or refine it with additional information which
has proved useful in previous works:
F4: SCFs and relative frequencies with verbs.
SCFs abstract over particles and prepositions.
F5: F4 with COs (F1). The SCF and CO feature
vectors are concatenated.
F6: F4 with the tense of the verb. The frequency
of verbal POS tags is calculated specific to
each SCF.
F7: F4 with PPs (F2). This feature parameterizes
SCFs for prepositions.
F8: Basic SCF feature corresponding to F4 but ex-
tracted from the VALEX lexicon (Korhonen
et al, 2006)
5
.
The following 9 feature sets are novel. They
build on F7, refining it further. F9-F11 refine F7
with information about LPs:
F9: F7 with F3 (subject only)
F10: F7 with F3 (object only)
F11: F7 with F3 (subject, object, indirect object)
F12-17 refine F7 with SPs. We adopt a fully un-
supervised approach to SP acquisition. We acquire
the SPs by
1. taking the GR relations (subject, object, indi-
rect object) associated with verbs,
2. extracting all the argument heads in these re-
lations which occur with frequency> 20 with
more than 3 verbs, and
3. clustering the resulting N most frequent ar-
gument heads into M classes using the spec-
tral clustering method described in the fol-
lowing section.
We tried the N settings {200, 500} and the M
settings {10, 20, 30, 80}. The best settings N =
200,M = 20 and N = 500,M = 30 are reported
in this paper. We enforce the features to be shared
by all the potential members of a verb class. The
expected class size is approximatelyN/K, and we
allow for 10% outliers (the features occurring less
than (N/K)? 0.9 verbs are thus removed).
The resulting SPs are combined with SCFs in a
similar fashion as LPs are combined with SCFs in
F9-F11:
F12-F14: as F9-F11 but SPs (20 clusters from 200
argument heads) are used instead of LPs
F15-F17: as F9-F11 but SPs (30 clusters from 500
argument heads) are used instead of LPs
5
This feature was included to enable comparing the con-
tribution of the recent SCF system to that of an older, com-
parable system which was used for constructing the VALEX
lexicon.
640
3 Clustering methods
We use two clustering methods: (i) pairwise clus-
tering (PC) which obtained the best performance
in comparison with several other methods in re-
cent work on biomedical verb clustering (Korho-
nen et al, 2008), and (ii) a method which is
new to the task (and to the best of our knowl-
edge, to NLP): a variation of spectral clustering
which exploits the MNCut algorithm (Meila and
Shi, 2001) (SPEC). Spectral clustering has been
shown to be effective for high dimensional and
non-convex data in NLP (Chen et al, 2006) and
it has been applied to German verb clustering by
Brew and Schulte im Walde (2002). However, pre-
vious work has used Ng et al (2002)?s algorithm,
while we adopt the MNCut algorithm. The lat-
ter has shown a wider applicability (von Luxburg,
2007; Verma and Meila, 2003) and it can be justi-
fied from the random walk view, which has a clear
probabilistic interpretation.
Clustering groups a given set of items (verbs in
our experiment) V = {v
n
}
N
n=1
into a disjoint par-
tition of K classes I = {I
k
}
K
k=1
. Both our algo-
rithms take a similarity matrix as input. We con-
struct this from the skew divergence (Lee, 2001).
The skew divergence between two feature vectors
v and v
?
is d
skew
(v, v
?
) = D(v
?
||a ?v+(1?a) ?v
?
)
where D is the KL-divergence. v is smoothed
with v
?
. The level of smoothing is controlled by
a whose value is set to a value close to 1 (e.g.
0.9999). We symmetrize the skew divergence
as follows: d(v, v
?
)
sskew
=
1
2
(d
skew
(v, v
?
) +
d
skew
(v
?
, v)).
SPEC is typically used with the Radial Basis
Function (RBF) kernel. We adopt a new kernel
similar to the symmetrized KL divergence kernel
(Moreno et al, 2004) which avoids the need for
scale parameter estimation.
w(v, v
?
) = exp(?d
sskew
(v, v
?
))
The similarity matrix W is constructed where
W
ij
= w(v
i
, v
j
).
Pairwise clustering
PC (Puzicha et al, 2000) is a method where a cost
criterion guides the search for a suitable partition.
This criterion is realized through a cost function of
the similarity matrix W and partition I:
H = ?
?
n
j
? Avgsim
j
,
Avgsim
j
=
P
{a,b?A
j
}
w(a,b)
n
j
?(n
j
?1)
where n
j
is the size of the j
th
cluster and Avgsim
j
is the average similarity between cluster members.
Spectral clustering
In SPEC, the similarities W
ij
are viewed as the
weight on the edges ij of a graph G over V . The
similarity matrix W is thus the adjacency matrix
for G. The degree of a vertex i is d
i
=
?
N
j=1
w
ij
.
A cut between two partitions A and A
?
is defined
to be Cut(A,A
?
) =
?
m?A,n?A
?
W
mn
.
In MNCut algorithm, the similarity matrixW is
transformed to a stochastic matrix P .
P = D
?1
W (1)
The degree matrix D is a diagonal matrix where
D
ii
= d
i
.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constant
6
with respect to a partition I
?
and their
eigenvalues are not zero, then I
?
minimizes the
multiway normalized cut(MNCut):
MNCut(I) = K ?
?
K
k=1
Cut(I
k
,I
k
)
Cut(I
k
,I)
P
mn
can be interpreted as the transition probabil-
ity between vertices m,n. The criterion can thus
be expressed as MNCut(I) =
?
K
k=1
(1?P (I
k
?
I
k
|I
k
)) (Meila, 2001), which is the sum of transi-
tion probabilities across different clusters. The cri-
terion finds the partition where the random walks
are most likely to happen within the same cluster.
In practice, the K leading eigenvectors of P is
not piecewise constant. But we can extract the
partition by finding the approximately equal ele-
ments in the eigenvectors using a clustering algo-
rithm like K-means.
The numerator of MNCut is similar to the cost
function of PC. The main differences between the
two algorithms are: 1) MNCut takes into account
of the cross cluster similarity, while PC does not.
2) PC optimizes the cost function using determin-
istic annealing, whereas SPEC uses eigensystem
decomposition.
The spectral clustering algorithm is based on the
Multicut algorithm (Meila and Shi, 2001).
6
The eigenvector v is piecewise constant with respect to I
if v(i) = v(j)?i, j ? I
k
and k ? 1, 2...K
641
Input: Dataset S, Number of clusters K
1. Compute similarity matrixW and Degree ma-
trix D
2. Construct stochastic matrix P using equation
1
3. Compute the eigenvalues and eigenvectors
{?
n
, x
n
}
N
n=1
of P , where ?
n
? ?
n+1
, form a
matrix X = [x
2
, . . . , x
k
] by stacking the eigen-
vectors in columns.
4. Form a matrix Y from X by normalizing the
row sums to have norm 1: Y
ij
= X
ij
/(
?
j
X
2
ij
)
1
2
5. Consider the row of Y to be the transformed
feature vectors for each verb and cluster them
into clusters C
1
. . . C
k
usingK-means clustering
algorithm.
Output: Clusters C
1
. . . C
k
4 Experimental evaluation
4.1 Test sets
We employed two test sets which have been used
to evaluate previous work on English verb classi-
fication:
T1 The test set of Joanis et al (2008) provides
a classification of 835 verbs into 15 (some
coarse, some fine-grained) Levin classes. 11
tests are provided for 2-14 way classifica-
tions. We employ the 14 way classifica-
tion because this corresponds the closest to
our target (Levin?s fine-grained) classifica-
tion
7
. We select 586 verbs according to Joa-
nis et al?s selection criteria, resulting in 10-
120 verbs per class. We restrict the class
imbalance to 1:1.5.
8
. This yields 205 verbs
(10-15 verbs per class) which is similar to
the sub-set of T1 employed by Stevenson and
Joanis (2003).
T2 The test set of Sun et al (2008) classifies 204
verbs to 17 fine-grained Levin classes, so that
each class has 12 member verbs.
Table 1 shows the classes in T1 and T2.
4.2 Data processing
For each verb in T1 and T2, we extracted all
the occurrences (up to 10,000) from the raw cor-
pus data gathered originally for constructing the
7
However, the correspondence is not perfect with half
of the classes including two or more Levin?s fine-grained
classes.
8
Otherwise, in the case of a large class imbalance the eval-
uation measure would be dominated by the classes with large
population.
T1
Object Drop 26.{1,3,7}
Recipient 13.{1,3}
Admire 31.2
Amuse 31.1
Run 51.3.2
Sound 43.2
Light &
43.{1,4}
Substance
Cheat 10.6
Steal &
10.{5,1}
Remove
Wipe 10.4.{1,2}
Spray / Load 9.7
Fill 9.8
Putting 9.1-6
Change of State 45.1-4
T2
Remove 10.1
Send 11.1
Get 13.5.1
Hit 18.1
Amalgamate 22.2
Characterize 29.2
Peer 30.3
Amuse 31.1
Correspond 36.1
Manner
37.3
of speaking
Say 37.7
Nonverbal
40.2
expression
Light 43.1
Other change
45.4
of state
Mode with
47.3
Motion
Run 51.3.2
Put 9.1
Table 1: Levin classes in T1 and T2
T1 T2
total avg total avg
CO F1 1328 764 743 382
LP (p) F2 61 37 55 25
LP (all) F3 2521 526 1481 295
SCF F4 88 46 86 38
SCF+CO F5 1466 833 856 422
SCF+POS F6 319 114 299 87
SCF+P F7 282 96 273 76
SCF (V) F8 - - 92 45
SCF+LP (s) F9 1747 324 1474 225
SCF+LP (o) F10 2817 424 2319 279
SCF+LP (all) F11 4250 649 3515 426
SCF+SP20 (s) F12 821 235 690 145
SCF+SP20 (o) F13 792 218 706 135
SCF+SP20 (all) F14 1333 357 1200 231
SCF+SP30 (s) F15 977 274 903 202
SCF+SP30 (o) F16 1026 273 1012 205
SCF+SP30 (all) F17 1720 451 1640 330
Table 2: (i) The total number of features and (ii)
the average per verb for all the feature sets
VALEX lexicon (Korhonen et al, 2006). The data
was gathered from five corpora, including e.g. the
British National Corpus (Leech, 1992) and the
North American News Text Corpus (Graff, 1995).
The average frequency of verbs in T1 was 1448
and T2 2166, showing that T1 is a more sparse
data set.
The data was first processed using the feature
extraction module. Table 2 shows (i) the total
number of features in each feature set and (ii) the
average per verb in the resulting lexicons for T1
and T2.
We normalized the feature vectors by the sum
of the feature values before applying the clustering
techniques. Since both clustering algorithms have
642
an element of randomness, we run them multiple
times. The step 5 of SPEC (K-means) was run for
50 times. The result that minimizes the distortion
(the distances to cluster centroid) is reported. PC
was run 20 times, and the results are averaged.
4.3 Evaluation measures
To facilitate meaningful comparisons, we em-
ploy the same measures for evaluation as previ-
ously employed e.g. by Korhonen et al (2008);
?
O
S?eaghdha and Copestake (2008).
The first measure is modified purity (mPUR) ?
a global measure which evaluates the mean preci-
sion of clusters. Each cluster is associated with its
prevalent class. The number of verbs in a cluster
K that take this class is denoted by n
prevalent
(K).
Verbs that do not take it are considered as errors.
Clusters where n
prevalent
(K) = 1 are disregarded
as not to introduce a bias towards singletons:
mPUR =
?
n
prevalent(k
i
)>2
n
prevalent(k
i
)
number of verbs
The second measure is weighted class accuracy
(ACC): the proportion of members of dominant
clusters DOM-CLUST
i
within all classes c
i
.
ACC =
?
C
i=1
verbs in DOM-CLUST
i
number of verbs
mPUR and ACC can be seen as a measure of pre-
cision(P) and recall(R) respectively. We calculate
F measure as the harmonic mean of P and R:
F =
2 ? mPUR ? ACC
mPUR + ACC
The random baseline(BL) is calculated as follows:
BL = 1/number of classes
5 Results
5.1 Quantitative evaluation
Table 3 includes the F-measure results for all the
feature sets when the two methods (PC and SPEC)
are used to cluster verbs in the test sets T1 and T2,
respectively. A number of tendencies can be ob-
served in the results. Firstly, the results for T2 are
clearly better than those for T1. Including a higher
number of verbs lower in frequency from classes
of variable granularity, T1 is probably a more chal-
lenging test set than T2. T2 is controlled for the
number and frequency of verbs to facilitate cross-
class comparisons. While this may contribute to
better results, T2 is a more accurate test set for us
in the sense that it offers a better correspondence
with our target (fine-grained Levin) classes.
T1 T2
PC SPEC PC SPEC
BL 7.14 7.14 5.88 5.88
CO F1 15.62 33.85 17.86 40.94
LP (p) F2 40.40 38.97 50.98 49.02
LP (all) F3 42.94 47.50 41.08 74.55
SCF F4 34.22 36.16 52.33 57.78
SCF+CO F5 26.43 28.70 19.52 29.10
SCF+POS F6 36.14 34.75 44.44 46.70
SCF+P F7 43.57 43.85 63.40 63.28
SCF (V) F8 - - 34.08 38.30
SCF+LP (s) F9 47.72 56.09 65.94 71.65
SCF+LP (o) F10 43.09 48.43 57.11 73.97
SCF+LP (all) F11 45.87 54.63 56.30 72.97
SCF+SP20 (s) F12 46.67 57.75 39.52 71.67
SCF+SP20 (o) F13 44.95 51.70 40.76 70.78
SCF+SP20(all) F14 48.19 55.12 39.68 73.09
SCF+SP30 (s) F15 45.89 56.10 64.44 80.35
SCF+SP30 (o) F16 42.01 48.74 52.75 70.52
SCF+SP30(all) F17 46.66 52.68 51.07 68.67
Table 3: Results on testsets T1 and T2
Secondly, the difference between the two clus-
tering methods is clear: the new SPEC outperforms
PC on both test sets and across all the feature sets.
The performance of the two methods is still fairly
similar with the more basic, less sparse feature sets
(F1-F2, F4, F6-7) but when the more sophisticated
feature sets are used (F3, F5, F9-F17) SPEC per-
forms considerably better. This demonstrates that
it is clearly a better suited method for high dimen-
sional feature sets.
Comparing the feature sets, the simple co-
occurrence based F1 performs clearly better than
the random baseline. F2 and F3 which exploit lex-
ical data in the argument head positions of GRs
prove significantly better than F1. F3 yields sur-
prisingly good results on T2: it is the second best
feature set on this test set. Also on T1, F3 per-
forms better than the SCF-based feature sets F4-
F7. This demonstrates the usefulness of lexical
data when obtained from argument positions in
relevant GRs.
Our basic SCF feature set F4 performs consid-
erably better than the comparable feature set F8
obtained from the VALEX lexicon. The difference
is 19.50 in F-measure. As both lexicons were ex-
tracted from the same corpus data, the improve-
ment can be attributed to improved parser and SCF
acquisition performance (Preiss et al, 2007).
F5-F7 refine the basic SCF feature set F4 fur-
ther. F5 which combines a SCF with CO in-
formation proved the best feature set in the su-
pervised verb classification experiment of Li and
Brew (2008). In our experiment, F5 produces sub-
stantially lower result than CO and SCF alone (i.e.
643
F1 and F4). However, our corpus is smaller (Li
and Brew used the large Gigaword corpus), our
SCFs are different, and our approach is unsuper-
vised, making meaningful comparisons difficult.
F6 combines F4 with information about verb
tense. This was not helpful: F6 produces worse re-
sults than F4. F7, on the other hand, yields better
results than F4 on both test sets. This demonstrates
what the previous research has shown: SCF per-
form better when parameterized for prepositions.
Looking at our novel feature sets F9-F17, F9-
F11 combine the most accurate SCF feature set
F4 with the LP-based features F2-F3. Although
the feature space gets more sparse, all the feature
sets outperform F2-F3 on T1. On T2, F3 per-
forms exceptionally well, and thus yields a better
result than F9-F11, but F9-F11 nevertheless per-
form clearly better than the best SCF-based feature
set F4 alone. The differences among F9, F10 and
F11 are small on T2, but on T1 F9 yields the best
performance. It could be that F9 works the best
for the more sparse T1 because it suffers the least
from data sparsity (it uses LPs only for the subject
relation).
F12-F17 replace the LPs in F9-F11 by semantic
SPs. When only 20 clusters are used as SP models
and acquired from the smaller sample of (200) ar-
gument heads (F12-F14), SPs do not perform bet-
ter than LPs on T2. A small improvement can be
observed on T1, especially with F12 which uses
only the subject data (yielding the best F measure
on T1: 57.75%). However, when 30 more fine-
grained clusters are acquired from a bigger sample
of (500) argument heads (F15-F17), lower results
can be seen on T1. On T2, on the other hand, F15
yields dramatic improvement and we get the best
performance for this test set: 80.35% F-measure.
The fact that no improvement is observed when
using F16 and F17 on T2 could be explained by
the fact that SPs are stronger for the subject posi-
tion which also suffers less from the sparse data
problem than e.g. i. object position. The fact that
no improvement is observed on T1 is likely to be
due to the fact that verbs have strong SPs only at
the finer-grained level of Levin classification. Re-
call that in T1, as many as half of the classes are
coarser-grained.
5.2 Qualitative evaluation
The best performing feature sets on both T1 and
T2 were thus our new SP-based feature sets. We
conducted qualitative analysis of the best 30 SP
Human mother, wife, parent, girl, child
Role patient, student, user, worker, teacher
Body-part neck, shoulder, back, knee, corner
Authority committee, police, court, council, board
Organization society, firm, union, bank, institution
Money cash, currency, pound, dollar, fund
Amount proportion, value, size, speed, degree
Time minute, moment, night, hour, year
Path street, track, road, stair, route
Building office, shop, hotel, hospital, house
Region site, field, area, land, island
Technology system, model, facility, engine, machine
Task operation, test, study, analysis, duty
Arrangement agreement, policy, term, rule, procedure
Matter aspect, subject, issue, question, case
Problem difficulty, challenge, loss, pressure, fear
Idea argument, concept, idea, theory, belief
Power control, lead, influence, confidence, ability
Form colour, style, pattern, shape, design
Item letter, book, goods, flower, card
Table 4: Cluster analysis: 20 clusters, their SP la-
bels, and prototypical member nouns
clusters in the T2 data created using SPEC to find
out whether these clusters were really semantic in
nature, i.e. captured semantically meaningful pref-
erences. As no gold standard specific to our verb
classification task was available, we did manual
cluster analysis using VerbNet (VN) as aid. In VN,
Levin classes are assigned with semantic descrip-
tions: the arguments of SCFs involved in diathesis
alternations are labeled with thematic roles some
of which are labeled with selectional restrictions.
From the 30 thematic role types in VN, as many
as 20 are associated with the 17 Levin classes in
T2. The most frequent role in T2 is agent, fol-
lowed by theme, location, patient, recipient, and
source. From the 36 possible selectional restric-
tion types, 7 appear in T2; the most frequent ones
being +animate and +organization, followed by
+concrete, +location, and +communication.
As SP clusters capture selectional preferences
rather than restrictions, we examined manu-
ally whether the 30 clusters (i) capture seman-
tically meaningful classes, and whether they (ii)
are plausible given the VN semantic descrip-
tions/restrictions for the classes in T2.
The analysis revealed that all the 30 clusters had
a predominant, semantically motivated SP sup-
ported by the majority of the member nouns. Al-
though many clusters could be further divided into
more specific SPs (and despite the fact that some
nouns were clearly misclassified), we were able to
assign each cluster a descriptive label characteriz-
ing the predominant SP. Table 4 shows 15 sam-
644
ple clusters, the SP labels assigned to them, and a
number of example nouns in these clusters.
When comparing each SP cluster against the
VN semantic descriptions/restrictions for T2, we
found that each predominant SP was plausible.
Also, the SPs frequent in our data were also fre-
quent among the 17 classes according to VN. For
example, the many SP clusters labeled as arrange-
ments, issues, ideas and other abstract concepts
were also frequent in T2, e.g. among COMMUNI-
CATION (37), CHARACTERISE (29.2), AMALGA-
MATE (22.2) and other classes.
This analysis showed that the SP models which
performed well in verb clustering were semanti-
cally meaningful for our task. An independent
evaluation using one of the standard datasets avail-
able for SP acquisition research (Brockmann and
Lapata, 2003) is of course needed to determine
how well the acquisition method performs in com-
parison with other existing methods.
Finally, we evaluated the quality of the verb
clusters created using the SP-based features. We
found that some of the errors were similar to those
seen on T2 when using syntactic features: errors
due to polysemy and syntactic idiosyncracy. How-
ever, a new error type clearly due to the SP-based
feature was detected. A small number of classes
got confused because of strong similar SPs in the
subject (agent) position. For example, some PEER
(30.3) verbs (e.g. look, peer) were found in the
same cluster with SAY (37.7) verbs (e.g. shout,
yell) ? an error which purely syntactic features do
not produce. Such errors were not numerous and
could be addressed by developing more balanced
SP models across different GRs.
6 Discussion and related work
Although features incorporating semantic infor-
mation about verb SPs make theoretical sense they
have not proved equally promising in previous ex-
periments which have compared them against syn-
tactic features in verb classification. Joanis et al
(2008) incorporated an ?animacy? feature (a kind
of a ?SP?) which was determined by classifying
e.g. pronouns and proper names in data to this sin-
gle SP class. A small improvement was obtained
when this feature was used in conjunction with
syntactic features in supervised classification.
Joanis (2002) and Schulte im Walde (2006) ex-
perimented with more conventional SPs with syn-
tactic features in English and German verb clas-
sification, respectively. They employing top level
Method Result
T1
Li et al 2008 supervised 66.3
Joanis et al 2008 supervised 58.4
Stevenson et al 2003
semi-supervised 29
unsupervised 31
SPEC unsupervised 57.55
T2 Sun et al 2008
supervised 62.50
unsupervised 51.6
?
O S?eaghdha et al 2008 supervised 67.3
SPEC unsupervised 80.35
Table 5: Previous verb classification results
WordNet (Miller, 1995) and Germanet (Kunze and
Lemnitzer, 2002) classes as SP models. Joanis
(2002) obtained no improvement over syntactic
features, whereas Schulte im Walde (2006) ob-
tained insignificant improvement.
Korhonen et al (2008) combined SPs with SCFs
when clustering biomedical verbs. The SPs were
acquired automatically from syntactic slots of
SCFs (not from GRs as in our experiment) using
PC clustering. A small improvement was obtained
using LPs extracted from the same syntactic slots,
but the SP clusters offered no improvement. Re-
cently, Schulte im Walde et al (2008) proposed an
interesting SP acquisition method which involves
combining EM training and the MDL principle for
an verb classification incorporating SPs. However,
no comparison against purely syntactic features is
provided.
In our experiment, we obtained a considerable
improvement over syntactic features, despite using
a fully unsupervised approach to both verb clus-
tering and SP acquisition. In addition to the rich,
syntactic-semantic feature sets, our good results
can be attributed to the clustering technique capa-
ble of dealing with them. The potential of spectral
clustering for the task was recognised earlier by
Brew and Schulte im Walde (2002). Although a
different version of the algorithm was employed
and applied to German (rather than to English),
and although no SP features were used, these ear-
lier experiments did demonstrate the ability of the
method to perform well in high dimensional fea-
ture space.
To get an idea of how our performance com-
pares with that of related approaches, we exam-
ined recent works on verb classification (super-
vised and unsupervised) which were evaluated on
same test sets using comparable evaluation mea-
sures. These works are summarized in table 5.
ACC and F-measure are shown for T1 and T2, re-
spectively.
645
On T1, the best performing supervised method
reported so far is that of Li and Brew (2008). Li
and Brew used Bayesian Multinomial Regression
for classification. A range of feature sets integrat-
ing COs, SCFs and/or LPs were evaluated. The
combination of COs and SCFs gave the best result,
shown in the table. Joanis et al (2008) report the
second best supervised result on T1, using Support
Vector Machines for classification and features de-
rived from linguistic analysis: syntactic slots, slot
overlaps, tense, voice, aspect, and animacy of NPs.
Stevenson and Joanis (2003) report a semi- and
unsupervised experiment on T1. A feature set sim-
ilar to that of Joanis et al (2008) was employed
(features were selected in a semi-supervised fash-
ion) and hierarchical clustering was used.
Our unsupervised method SPEC performs sub-
stantially better than the unsupervised method of
Stevenson et al and nearly as well as the super-
vised approach of Joanis et al (2008) (note, how-
ever, that the different experiments involved differ-
ent sub-sets of T1 so are not entirely comparable).
On T2, the best performing supervised method
so far is that of
?
O S?eaghdha and Copestake (2008)
which employs a distributional kernel method to
classify SCF features parameterized for preposi-
tions in the automatically acquired VALEX lexicon.
Using exactly the same data and feature set, Sun
et al (2008) obtain a slightly lower result when us-
ing a supervised method (Gaussian) and a notably
lower result when using an unsupervised method
(PC clustering). Our method performs consider-
ably better and also outperforms the supervised
method of
?
O S?eaghdha and Copestake (2008).
7 Conclusion and Future Work
We introduced a new approach to verb cluster-
ing which involves the use of (i) rich lexical, SCF
and GR data produced by a recent SCF system, (ii)
novel syntactic-semantic feature sets which com-
bine a variety of linguistic information, and (iii) a
new variation of spectral clustering which is par-
ticularly suited for dealing with the resulting, high
dimensional feature space. Using this approach,
we showed on two well-established test sets that
automatically acquired SPs can be highly useful
for verb clustering. This result contrasts with most
previous works but is in line with theoretical work
on verb classification which relies not only on syn-
tactic but also on semantic features (Levin, 1993).
In addition to the ideas mentioned earlier, our
future plans include looking into optimal ways
of acquiring SPs for verb classification. Consid-
erable research has been done on SP acquisition
most of which has involved collecting argument
headwords from data and generalizing to Word-
Net classes. Brockmann and Lapata (2003) have
showed that WordNet-based approaches do not
always outperform simple frequency-based mod-
els, and a number of techniques have been re-
cently proposed which may offer ideas for refin-
ing our current unsupervised approach (Erk, 2007;
Bergsma et al, 2008). The number and type (and
combination) of GRs for which SPs can be reliably
acquired, especially when the data is sparse, re-
quires also further investigation.
In addition, we plan to investigate other po-
tentially useful features for verb classification
(e.g. named entities and preposition classes) and
explore semi-automatic ML technology and active
learning for guiding the classification. Finally, we
plan to conduct a bigger experiment with a larger
number of verbs, and conduct evaluation in the
context of practical application tasks.
Acknowledgments
Our work was funded by the Dorothy Hodgkin
Postgraduate Award, the Royal Society Univer-
sity Research Fellowship, and the EPSRC grant
EP/F030061/1, UK. We would like to thank Paula
Buttery for letting us use her implementation of
the SCF classifier and Yuval Krymolowski for the
support he provided for feature extraction.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. Dis-
criminative learning of selectional preference from
unlabeled text. In Proc. of EMNLP, 2008.
Chris Brew and Sabine Schulte im Walde. Spectral
clustering for german verbs. In Proc. of EMNLP,
2002.
Ted Briscoe, John Carroll, and Rebecca Watson. The
second release of the rasp system. In Proc. of the
COLING/ACL on Interactive presentation sessions,
2006.
Carsten Brockmann and Mirella Lapata. Evaluating
and combining approaches to selectional preference
acquisition. In Proc. of EACL, 2003.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and
Zheng-Yu Niu. Unsupervised relation disambigua-
tion using spectral clustering. In Proc. of COL-
ING/ACL, 2006.
Hoa Trang Dang. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. PhD
thesis, CIS, University of Pennsylvania, 2004.
Katrin Erk. A simple, similarity-based model for selec-
tional preferences. In Proc. of ACL, 2007.
646
David Graff. North american news text corpus. Lin-
guistic Data Consortium, 1995.
Eric Joanis. Automatic Verb Classification Using a
General Feature Space. Master?s thesis, University
of Toronto, 2002.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classifica-
tion. Natural Language Engineering, 2008.
Karin Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. 2005.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
A large subcategorization lexicon for natural lan-
guage processing applications. In Proc. of the 5th
LREC, 2006.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. The Choice of Features for Classification of
Verbs in Biomedical Texts. In Proc. of COLING,
2008.
Claudia Kunze and Lothar Lemnitzer. GermaNet-
representation, visualization, application. In Proc.
of LREC, 2002.
Lillian. Lee. On the effectiveness of the skew diver-
gence for statistical language analysis. In Artificial
Intelligence and Statistics, 2001.
Geoffrey Leech. 100 million words of english: the
british national corpus. Language Research, 1992.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Fea-
tures for Automatic Verb Classification. In Proc. of
ACL, 2008.
Diana McCarthy. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Sub-
categorization Frames and Selectional Preferences.
PhD thesis, University of Sussex, UK, 2001.
Marina. Meila. The multicut lemma. Technical report,
University of Washington, 2001.
Marina Meila and Jianbo Shi. A random walks view of
spectral segmentation. AISTATS, 2001.
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 1995.
Pedro J. Moreno, Purdy P. Ho, and Nuno Vasconce-
los. A Kullback-Leibler divergence based kernel for
SVM classification in multimedia applications. In
Proc. of NIPS, 2004.
Andrew Y. Ng, Michael Jordan, and Yair Weiss. On
spectral clustering: Analysis and an algorithm. In
Proc. of NIPS, 2002.
Diarmuid
?
O S?eaghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proc. of
COLING, 2008.
Judita Preiss, Ted Briscoe, and Anna Korhonen. A sys-
tem for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from cor-
pora. In Proc. of ACL, 2007.
Jan Puzicha, Thomas Hofmann, and Joachim M. Buh-
mann. A theory of proximity based clustering:
Structure detection by optimization. Pattern Recog-
nition, 2000.
Sabine Schulte im Walde. Experiments on the auto-
matic induction of german semantic verb classes.
Computational Linguistics, 2006.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. Combining EM
Training and the MDL Principle for an Automatic
Verb Classification incorporating Selectional Pref-
erences. In Proc. of ACL, pages 496?504, 2008.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proc. of CICLING, 2005.
Suzanne Stevenson and Eric Joanis. Semi-supervised
verb class discovery using noisy features. In Proc.
of HLT-NAACL 2003, pages 71?78, 2003.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
Verb class discovery from rich syntactic data. Lec-
ture Notes in Computer Science, 4919:16, 2008.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proc. of EMNLP, 2004.
Deepak Verma and Marina Meila. Comparison of spec-
tral clustering methods. Advances in Neural Infor-
mation Processing Systems (NIPS 15), 2003.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. Unsupervised and constrained dirich-
let process mixture models for verb clustering. In
Proc. of the Workshop on Geometrical Models of
Natural Language Semantics, 2009.
Ulrike von Luxburg. A tutorial on spectral clustering.
Statistics and Computing, 2007.
Be?nat Zapirain, Eneko Agirre, and Llu??s M`arquez. Ro-
bustness and generalization of role sets: PropBank
vs. VerbNet. In Proc. of ACL, 2008.
647
Proceedings of the Workshop on BioNLP, pages 108?116,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
User-Driven Development of Text Mining Resources for Cancer Risk
Assessment
Lin Sun, Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk
Ilona Silins, Ulla Stenius
Institute of Environmental Medicine
Karolinska Institutet
S-17177, Stockholm
Sweden
ilona.silins,ulla.stenius@ki.se
Abstract
One of the most neglected areas of biomed-
ical Text Mining (TM) is the development
of systems based on carefully assessed user
needs. We investigate the needs of an im-
portant task yet to be tackled by TM ? Can-
cer Risk Assessment (CRA) ? and take the
first step towards the development of TM for
the task: identifying and organizing the sci-
entific evidence required for CRA in a taxon-
omy. The taxonomy is based on expert annota-
tion of 1297 MEDLINE abstracts. We report
promising results with inter-annotator agree-
ment tests and automatic classification experi-
ments, and a user test which demonstrates that
the resources we have built are well-defined,
accurate, and applicable to a real-world CRA
scenario. We discuss extending and refining
the taxonomy further via manual and machine
learning approaches, and the subsequent steps
required to develop TM for the needs of CRA.
1 Introduction
Biomedical Text Mining (TM) has become increas-
ingly popular due to the pressing need to provide
access to the tremendous body of texts available
in biomedical sciences. Considerable progress has
been made in the development of basic resources
(e.g. ontologies, annotated corpora) and techniques
(e.g. Information Retrieval (IR), Information Ex-
traction (IE)) in this area, and research has began
to focus on increasingly challenging tasks, e.g. sum-
marization and the discovery of novel information in
biomedical literature (Hunter and Cohen 2006, Ana-
niadou et al 2006, Zweigenbaum et al 2007).
In recent past, there has been an increasing de-
mand for research which is driven by actual user
needs rather than technical developments (Zweigen-
baum et al 2007). Shared tasks (e.g. BioCreative
and the TREC Genomics track) targeting the work-
flow of biomedical researchers have appeared along
with studies exploring the TM needs of specific tasks
(Karamanis et al 2008, Demaine et al 2006). How-
ever, the understanding of user needs is still one of
the neglected areas of BIO-TM, and further user-
centered evaluations and systems grounded in real-
life tasks are required to determine which tools and
services are useful (Cohen et al 2008).
We investigate the user needs of a challenging
task yet to be tackled by TM but identified as an
important potential application for it (Lewin et al
2008): Cancer Risk Assessment (CRA). Over the
past years, CRA has become increasingly important
as the link between environmental chemicals and
cancer has become evident. It involves examining
published evidence to determine the relationship be-
tween exposure to a chemical and the likelihood of
developing cancer from that exposure (EPA, 2005).
Performed manually by experts in health related in-
stitutions worldwide, CRA requires searching, lo-
cating and interpreting information in biomedical
journal articles. It can be extremely time-consuming
because the data for a single carcinogen may be scat-
tered across thousands of articles.
Given the exponentially growing volume of
biomedical literature and the rapid development of
molecular biology techniques, the task is now get-
ting too challenging to manage via manual means.
From the perspective of BIO-TM, CRA is an excel-
lent example of real-world task which could greatly
benefit from a dedicated TM tool. However, the de-
velopment of a truly useful tool requires careful in-
vestigation of risk assessors needs.
108
This paper reports our investigation of the user
needs of CRA and the creation of basic TM re-
sources for the task. Expanding on our preliminary
experiments (Lewin et al 2008), we present a taxon-
omy which specifies the scientific evidence needed
for CRA at the level of detail required for TM. The
taxonomy is based on expert annotation of a corpus
of 1297 MEDLINE abstracts. We report promis-
ing results with inter-annotator agreement tests, au-
tomatic classification of corpus data into taxonomy
classes, and a user test in a near real-world CRA
scenario which shows that the taxonomy is highly
accurate and useful for practical CRA. We discuss
refining and extending it further via manual and ma-
chine learning approaches, and the subsequent steps
required to develop TM for the needs of CRA.
2 User Needs of Cancer Risk Assessment
We interviewed 14 experienced risk assessors work-
ing for a number of authorities in Sweden1 asking
a range of questions related to different aspects of
their work. The risk assessors described the follow-
ing steps of CRA: (1) identifying the journal articles
relevant for CRA of the chemical in question, (2)
identifying the scientific evidence in these articles
which help to determine whether/how the chemical
causes cancer, (3) classifying and analysing the re-
sulting (partly conflicting) evidence to build the tox-
icological profile for the chemical, and (4) prepar-
ing the risk assessment report. These steps are con-
ducted manually, relying only on standard literature
search engines (e.g. PubMed) and word processors.
The average time required for CRA of a single
chemical was reported to be two years when done
(as usual) on a part time basis. Risk assessors were
unanimous about the need to increase productivity
to meet the current CRA demand. They reported
that locating and classifying the scientific evidence
in literature is the most time consuming part of their
work and that a tool capable of assisting it and ensur-
ing that all the potentially relevant evidence is found
would be particularly helpful.
It became clear that a prerequisite for the devel-
opment of such a tool would be an extensive spec-
ification of the scientific evidence used for CRA.
1Institute of Environmental Medicine at Karolinska Insti-
tutet, Swedish Chemical Inspectorate, Scientific Committee on
Occupational Exposure Limits (EU), Swedish Criteria Group.
This evidence ? which forms the basis of all the
subsequent steps of CRA ? is described in the
guideline documents of major international CRA
agencies, e.g. European Chemicals Agency (ECHA,
2008) and the United States Environmental Protec-
tion Agency (EPA, 2005). However, although these
documents constitute the main reference material in
CRA, they cover the main types of evidence only,
do not specify the evidence at the level of detail
required for comprehensive data gathering, and are
not updated regularly (i.e. do not incorporate the lat-
est developments in biomedical sciences). The risk
assessors admitted that rather than relying on these
documents, they rely on their experience and expert
knowledge when looking for the evidence. We de-
cided that our starting point should be to compose
a more adequate specification of the scientific evi-
dence needed for CRA.
3 Cancer Risk Assessment Taxonomy
We recruited three experienced risk assessors to help
construct the resources described in sections below:
(i) a representative corpus of CRA literature for
parts of hazard identification (i.e. the assessment of
whether a chemical is capable of causing cancer),
(ii) a tool for expert annotation of the corpus, (iii) an
annotated corpus, and (iv) a taxonomy which classi-
fies and organizes the scientific evidence discovered
in the corpus.
3.1 CRA corpus
Various human, animal (in vivo), cellular (in vitro)
and other mechanistic data provide evidence for haz-
ard identification and the assessment of the Mode of
Action (MOA) (i.e. the sequence of key events that
result in cancer formation, e.g. mutagenesis and in-
creased cell proliferation) in CRA. The experts se-
lected eight chemicals which are (i) well-researched
using a range of scientific tests and (ii) represent the
two most frequently used MOAs ? genotoxic and
non-genotoxic2 . 15 journals were identified which
are used frequently for CRA and jointly provide a
good coverage of relevant scientific evidence (e.g.
Cancer Research, Chemico-biological Interaction,
Mutagenesis, Toxicological Sciences). From these
2Chemicals acting by a genotoxic MOA interact with DNA,
while chemicals acting by a nongenotoxic MOA induce cancer
without interfering directly with DNA.
109
Figure 1: Screenshot of the annotation tool
journals, all the PubMed abstracts from 1998-2008
which include one of the 8 chemicals were down-
loaded. The resulting corpus of 1297 abstracts is
distributed per chemical as shown in Table 1.
3.2 Annotation tool
Risk assessors typically (i) read each abstract re-
trieved by PubMed to determine its relevance for
CRA, and (ii) classify each relevant abstract based
on the type of evidence it provides for CRA. We ex-
tended the tool designed for expert annotation of ab-
stracts in our earlier work (Lewin et al 2008) so that
imitates this process as closely as possible.
The tool provides two types of functionality. The
first enables the experts to classify abstracts as rele-
vant, irrelevant or unsure. The second enables them
to annotate such keywords (words or phrases) in ab-
stracts and their titles which indicate the scientific
evidence relevant for the task. Keyword annotation
was chosen because the experts found it intuitive, it
did not require linguistic training, and it specifies the
scientific evidence more precisely than larger spans
of text.
Initially a very shallow taxonomy (including only
human, animal, and cellular data) and the two types
of MOA was integrated inside the tool. This was
gradually extended as the annotation progressed.
The tool permits annotating any number of relevant
keywords in the abstracts, attaching them to any
class in the taxonomy, and classifying the same text
in more than one way. It was implemented inside the
familiar Mozilla Firefox browser using its extension
facility. A screenshot illustrating the tool is provided
in Figure 1.
3.3 Annotation
Given a set of initial guidelines agreed by the ex-
perts, one of the experts annotated a subset of the
corpus, the other two evaluated the result, disagree-
ments were then discussed, and the guidelines were
improved where needed. This process (crucial for
maintaining quality) was repeated several times.
The guidelines described below are the final result
of this work.
3.3.1 Relevance annotation
An abstract is classified as (i) relevant when it (or
its title) contains evidence relevant for CRA and (ii)
irrelevant when it (or its title) contains no evidence
or contains ?negative? evidence (e.g. diseases or
endpoints unrelated to cancer). Abstracts containing
vague, conflicting or complex evidence (e.g. stud-
ies on chemicals in complex mixtures) or evidence
whose association with cancer is currently unclear
were dealt on case by case basis. All the potentially
relevant abstracts were included for further assess-
ment as not to lose data valuable for CRA.
The experts annotated the 1297 abstracts in the
corpus. 89.4% were classified as relevant, 10.1% as
irrelevant, and 0.5% as unsure. We used the Kappa
statistics (Cohen 1960) to measure inter-annotator
agreement on unseen data which two experts an-
notated independently. 208 abstracts were selected
randomly from the 15 journals and from 16 jour-
nals likely to be irrelevant for CRA. The latter were
included to make the task harder as the proportion
of relevant abstracts was high in our corpus. Our
Kappa result is 0.68 ? a figure which indicates sub-
stantial agreement (Landis and G.Koch 1977).
The experts disagreed on 24 (11.5% of the) ab-
stracts. Half of the disagreements are due to one
of the annotators failing to notice relevant evidence.
Such cases are likely to decrease when annotators
gain more experience. The other half are caused by
vague or conflicting evidence. Many of these could
be addressed by further development of guidelines.
3.3.2 Keyword annotation
Keyword annotation focussed on the types of sci-
entific evidence experts typically look for in CRA:
carcinogenic activity (human, animal, cellular, and
other mechanistic data), Mode of Action (MOA)
(data for a specific MOA type ? genotoxic or non-
110
Chemical Retrieved Relevant
1,3-butadiene 195 187
phenobarbital 270 240
diethylnitrosamine 221 214
diethylstilbestrol 145 110
benzoapyrene 201 192
fumonisin 80 70
chloroform 96 84
styrene 162 132
Total 1297 1164
Table 1: Total of abstracts per chemical
genotoxic), and relevant parts of toxicokinetics (e.g.
metabolic activation). The experts annotated the
keywords which they considered as the most impor-
tant and which jointly identify the types of scientific
data offered by the abstract. They focussed on new
(rather than previously published) data on the chem-
ical in question.
All the 1164 abstracts deemed relevant were an-
notated. A total of 1742 unique keywords were
identified, both simple nouns and complex nomi-
nals / phrases. Figure 1 shows an example of an
annotated abstract where the keyword chromoso-
mal aberrations is identified as evidence for geno-
toxic MOA. Since the experts were not required to
annotate every relevant keyword, calculating inter-
annotator agreement was not meaningful. However,
the keyword annotation was evaluated jointly with
taxonomy classification (the following section).
3.4 The taxonomy and the resulting corpus
During keyword annotation, the initial taxonomy
was extended and refined with new classes and class
members. The resulting taxonomy relies solely on
expert knowledge. Experts were merely advised
on the main principles of taxonomy creation: the
classes should be conceptually coherent and their hi-
erarchical organization should be in terms of coher-
ent sub- and superordinate relations.
The taxonomy contains three top level classes:
1) Carcinogenic activity (CA), 2) Mode of Action
(MOA) and 3) Toxicokinetics (TOX). 1) and 2) are
organized by TYPE-OF relations (leukemia is a type
of carcinogenic evidence) and 3) by PART-OF rela-
tions (biodegradation is a part of Metabolism). Each
top level class divides into sub-classes. Figure 2
shows CA taxonomy with three keyword examples
per class. The taxonomy has 48 classes in total; half
of them under CA. Table 6 shows the total number
of abstracts and keywords per class: 82.4% of the
abstracts include keywords for CA, and 50.3% and
28.1% for MOA and TOX, respectively.
We calculated inter-annotator agreement for as-
signing abstracts to taxonomy classes. For each of
the 8 chemicals, 10 abstracts were randomly cho-
sen from the 15 journals. The average agreement
between two annotators is the highest with CA and
MOA (78%) and the lowest with TOX (62%). The
overall agreement is 76%. This result is good, par-
ticularly considering the high number of classes and
the chance agreement of 1.5%. The disagreements
are mostly due to one of the experts annotating as
many keywords as possible, and the other one an-
notating only the ones that classify each abstract as
precisely as possible. This was not a serious prob-
lem for us, but it demonstrates the importance of de-
tailed guidelines. Also, some of the classes were too
imprecise to yield unique distinctions. Future work
should focus on refining them further.
4 Automatic classification
To examine whether the classification created by ex-
perts provides a good representation of the corpus
data and is machine learnable, we conducted a se-
ries of abstract classification experiments.
4.1 Methods
4.1.1 Feature extraction
The first step of text categorization (TC) is to
transform documents into a feature vector represen-
tation. We experimented with two document rep-
resentation techniques. The first one is the sim-
ple ?bag of words? approach (BOW) which consid-
ers each word in the document as a separate feature.
BOW was evaluated using three methods which have
proved useful in previous TC work: (i) stemming
(using the Porter (1980) stemmer) which removes
affixes from words, (ii) the TFIDF weighting (Kib-
riya et al 2004), and (iii) stop word removal.
The second technique is the recent ?bag of sub-
strings? (BOS) method by (Wang et al 2008) which
considers the whole abstract as a string and extracts
from it all the length p substrings without affix re-
moval. BOS has proved promising in biomedical
TC (Han et al 2006, Wang et al 2008) and un-
like a traditional grammatical stemmer, does not re-
111
Figure 2: Taxonomy of Carcinogenic Activity
quire domain tuning for optimal performance. Be-
cause BOS generates substrings with fixed length p,
a word shorter than p?2 can get obscured by its con-
text3. For example, ?mice? would be transformed to
? mice a?, ? mice b?, . . . , which is less informative
than the original word form. Therefore, we enriched
BOS features with word forms shorter than p? 2.
4.1.2 Feature selection
We employed two feature selection methods for
dimensionality reduction. The first is Information
Gain (IG) which has proved useful in TC (Yang
and Pedersen 1997). Given a feature?s distribu-
tion X and class label distribution Y , IG(X) =
H(Y ) ? H(Y |X), H(X) is the entropy of X. The
second method fscore optimises the number of fea-
tures (N ). Features are first ranked using the simple
fscore criterion (Chen and Lin 2006), and N is se-
lected based on the performance of the SVM classi-
fier using the N features.
4.1.3 Classification
Three classifiers were used: Naive Multino-
mial Bayesian (NMB), Complement Naive Bayesian
(CNB) (Rennie and Karger 2003) and Linear Sup-
port Vector Machines (L-SVM) (Vapnik 1995).
NMB is a widely used classifier in TC (Kib-
riya et al 2004). It selects the class C with
the maximum probability given the document d:
argmaxc Pr(C)?w?d Pr(X = w|C). Pr(C) can
3Minus 2 because of space characters.
be estimated from the frequency of documents in C .
Pr(X = w|C) is estimated as the fraction of tokens
in documents of class C that contain w.
CNB extends NMB by addressing the problems
it has e.g. with imbalanced data and weight
magnitude error. The class c of a document
is: argmaxc[logp(?c)??i filogNc?i+?iNc?+? ]. Nc?i is the
number of times term i occurs in classes other than
c. ? and ?i are the smoothing parameters. p(?c) is
the prior distribution of class c.
L-SVM is the basic type of SVM which pro-
duces a hyperplane that separates two-class samples
with a maximum margin. It handles high dimen-
sional data efficiently, and has shown to perform
well in TC (Yang and Liu 1999). Given the data
set X = (x1, y1), . . . , (xn, yn) yi ? {?1,+1},
L-SVM requires a solution w to the following un-
constrained optimisation problem: min(12wTw +
C?ni=1 max(1 ? yiwTxi, 0)2. Cost parameter C
was estimated within range 22,. . . , 25 on training
data using cross validation. The C of the posi-
tive class was weighted by class population ratio
r = negative populationpositive population .
4.1.4 Evaluation
We used the standard measures of recall (R), pre-
cision (P) and F measure (F) for evaluation. These
are defined as follows:
R = TPTP+FN P = TPTP+FP F = 2?R?PR+P
Our random baseline is P+N+P+ .
112
P+/N : positive/negative population TP: truth positive; FN: false negative, FP: false positive
4.2 Experimental evaluation
4.2.1 Data
Our data was the expert annotated CRA corpus.
4.2.2 Document preprocessing
We first evaluated the BOW preprocessing tech-
nique with and without the use of (i) the Porter
(1980) stemmer, (ii) TFIDF, (iii) stop word removal,
and (iv) their combinations. The evaluation was
done in the context of the binary relevance classifica-
tion of abstracts (not in the context of the main tax-
onomic classification task to avoid overfitting pre-
processing techniques to the taxonomy). Only (iii)
improved all the classifiers and was thus adopted
for the main experiments. The poor performance
of (i) demonstrates that a standard stemmer is not
optimal for our data. As highlighted by (Han et al
2006, Wang et al 2008), semantically related bio-
logical terms sharing the same stem are not always
reducible to the stem form.
4.2.3 Feature selection
We evaluated the feature selection methods on
two taxonomy classes: the most balanced class ?An-
imal study? (positive/negative 1:1.4) and an imbal-
anced class ?Adducts? (positive/negative 1:6.5). IG
was used for the fixed N setting and fscore for the
dynamic N setting. Each combination of classifiers
(NMB/CNB/SVM), document representations (BOW,
BOS) and settings for N (dynamic, . . . , 83098) was
evaluated. The results show that the dynamic setting
yields consistent improvement on all the setups (al-
though the impact on SVM?s is not big). Also the
optimal N varies by the data and the classifier. Thus,
we used the dynamic feature selection in the taxo-
nomic classification.
4.2.4 Taxonomic classification
Experimental setup We ran two sets of experi-
ments on the corpus, using 1) BOW and 2) BOS for
feature extraction. Without feature selection, BOW
had c. 9000 features and BOS c. 83000. Features
were selected using fscore. For each class with
more than 20 abstracts (37 in total)4, three ?one
4The classes with less than 20 abstracts may have less than
2 positive abstracts in each fold of 10 fold CV, which is not
Method Feature Set P R F
NMB BOW 0.59 0.75 0.66
NMB BOS 0.62 0.82 0.70
CNB BOW 0.52 0.74 0.60
CNB BOS 0.57 0.76 0.64
SVM BOW 0.68 0.76 0.71
SVM BOS 0.71 0.77 0.74
Table 2: Performance of classifiers with BOS/BOW
Class Method P R F
CA NMB 0.94 0.89 0.91
CA CNB 0.92 0.94 0.93
CA SVM 0.93 0.93 0.93
MOA NMB 0.88 0.81 0.84
MOA CNB 0.84 0.82 0.83
MOA SVM 0.92 0.80 0.86
TOX NMB 0.66 0.83 0.74
TOX CNB 0.70 0.80 0.75
TOX SVM 0.76 0.79 0.78
Table 3: Result for the top level classes
against other? classifiers (NMB, CNB and L-SVM)
were trained and tested using 10-fold cross valida-
tion.
Results Table 2 shows the average performance
for the whole taxonomy. The performance of BOS
is better than that of BOW according to all the three
measures. On average, BOS outperforms BOW by
4% in P and F, and 3% in R. SVM yields the best
overall P and F (0.71 and 0.74) with BOS. Surpris-
ingly, NMB outperforms CNB with all the settings.
NMB yields the best overall R with BOS (0.82) but
its P is notably lower than that of SVM.
Table 3 shows the average P, R and F for the top
level classes using the best performing feature set
BOS with the three classifiers. CA has the best F
(0.93). Its positive population is the highest (posi-
tive/negative: 5:1). TOX with a lower positive pop-
ulation (1:2.6) has still good F (0.78). R and P are
balanced with an average difference of 0.06.
Table 4 shows the distribution of F across the
taxonomy. There is a clear correlation between
representative for the class population.
No. of abstracts(f) Classes F Random
f > 300 9 0.80 0.38
100 < f ? 300 12 0.73 0.13
20 < f ? 100 16 0.68 0.04
Table 4: Mean F and random baseline for taxonomic
classes in three frequency ranges.
113
frequency and performance: the average F de-
creases with descending frequency range, revealing
increased classification difficulty. Classes with more
than 300 abstracts have the highest average F (0.80
with standard deviation (SD) 0.08). Classes with
20-100 abstracts have the average F 0.68 (SD 0.11),
which is lower but still fairly good. No class has F
lower than 0.46, which is much higher than the av-
erage random baseline of 0.11.
5 User Test
A user test was carried out to examine the practical
usefulness of the automatic classification in a near
real-world scenario. The L-SVM+BOS classifier was
applied to the PubMed abstract data (from 1998-
2008) of five unseen chemicals representing geno-
toxic (geno) and non-genotoxic (non) MOAs (see
table 5). The results were displayed to two experts
in a friendly web interface. The experts were in-
vited to imagine that they have submitted a query to
a system, the system has returned the classification
of relevant abstracts for each chemical, and the task
is to judge whether it is correct. The top 500 BOS
features per class were shown to aid the judgement.
Results were evaluated using precision (P) (re-
call could not be calculated as not all of the positive
polulation was known). Table 5 shows the average
P for chemicals and top level classes. The results
are impressive: the only chemical with P lower than
0.90 is polychlorinated biphenyls (PCB). As PCB
has a well-known neuro-behavioural effect, the data
includes many abstracts irrelevant for CRA. Most
other errors are due to the lack of training data for
low frequency classes. For example, the CRA cor-
pus had only 27 abstracts in ?DNA repair (damage)?
class, while the new corpus has many abstracts on
DNA damage some of which are irrelevant for CRA.
The experts found the tool easy to use and felt
that if such a tool was available to support real-world
CRA, it could significantly increase their productiv-
ity and also lead to more consistent and thorough
CRA. Such a wide range of scientific evidence is dif-
ficult to gather via manual means, and chemical car-
cinogenesis is such a complex process that even the
most experienced risk assessor is incapable of mem-
orizing the full range of relevant evidence without
the support of a thorough specification / taxonomy.
Name MOA ? P
Aflatoxin B1 geno 189 0.95
Benzene geno 461 0.99
PCB non 761 0.89
Tamoxifen non 382 0.96
TCDD non 641 0.96
Class P
CA 0.94
MOA 0.95
TOX 0.99
Table 5: Chemicals and the results of the user test
6 Conclusion and Future Work
The results of our inter-annotator agreement tests,
automatic classification experiments and the user
test demonstrate that the taxonomy created by risk
assessors is accurate, well-defined, and can be use-
ful in a real-world CRA scenario. This is particu-
larly encouraging considering that the taxonomy is
based on biomedical annotation. As highlighted by
(Kim et al 2008), expert annotation is more chal-
lenging and prone to inter-annotator disagreement
than better-constrained linguistic annotation. We
believe that we obtained promising results because
we worked in collaboration with risk assessors and
developed technology which imitates their current
practices as closely as possible.
Most related work focuses on binary classifica-
tion, e.g. BioCreative II had a subtask (Krallinger
et al 2008) on the relevance classification of ab-
stracts for protein interactions. The few works
that have attempted multi-classification include e.g.
that of Aphinyanaphongs et al (2005) who applied
NMB, SVM and AdaBoost to classify abstracts of
internal medicine into four categories, and that of
Han et al (2006) who used BOS and NMB/L-SVM to
classify abstracts in five categories of protein post-
translational modifications.
In the future, we plan to refine the taxonomy fur-
ther by careful analysis of keyword types found in
the data and the taxonomic relationships defined by
experts. This will help to transform the taxonomy
into a better-developed knowledge resource. We
also need to extend the taxonomy. Although our
results show that the current taxonomy provides a
good basis for the classification of CRA literature,
it is not comprehensive: more data is required espe-
cially for low frequency classes, and the taxonomy
needs to be extended to cover more specific MOA
types (e.g. further subtypes of non-genotoxic chem-
icals).
The taxonomy can be extended by manual annota-
114
Change in F ? Classes Abstracts of class
20-100 100 - 200 200 - 1100
?F > 1% 16 (43%) 75% 33% 8%
|?F | ? 1% 15 (41%) 6% 44% 75%
?F < ?1% 6 (16%) 19% 33% 17%
Table 6: F gain(?F ) of MeSH compared to BOS
Class ? F
Carcinogenic activity 1068 92.8
Human study/epidemiology 190 77.7
Animal study 629 80.2
Cell experiments 319 78.5
Study on microorganisms 44 85.2
Mode of Action 653 85.5
Genotoxic 421 89.1
Nongenotoxic 324 76.3
Toxicokinetics 356 77.7
Absorption, . . . ,excretion 113 69.8
Metabolism 268 76.4
Toxicokinetic modeling 31 84.6
Table 7: ? abstracts and F of level 1,2 classes.
tion, supplementing it with additional information in
knowledge resources and/or by automatic methods.
One knowledge resource potentially useful is the
Medical Subject Headings (MeSH) taxonomy (Nel-
son et al 2002) which classifies PubMed abstracts
according to manually defined terms. We performed
a small experiment to investigate the usefulness of
MeSH for supplementing our current classification.
MeSH terms were first retrieved for each abstract us-
ing EFetch (NCBI 2005) and then appended to the
BOS feature vector. Best features were then selected
using fscore and classified using L-SVM. The fig-
ures in table 6 show that the results improved sig-
nificantly for 43% of the low frequency classes. Al-
though this demonstrates the potential usefulness of
additional resources, given the rapidly evolving na-
ture of CRA data, the best approach long term is
to develop technology for automatic updating of the
taxonomy from literature. Given the basic resources
we have constructed, the development of such tech-
nology is now realistic and can be done using unsu-
pervised or semi-supervised machine learning tech-
niques, e.g. (Cohen and Hersh 2005, Blaschko and
Gretton 2009).
The automatic classification could be improved
by the use of more sophisticated features extracted
using NLP tools that have been tuned for biomedi-
cal texts, such as parsers, e.g. (Tsuruoka et al 2005),
and named entity recognizers, e.g. (Corbett et al
2007), and exploiting resources such as the BioLex-
ion (Sasaki et al 2008).
Our long term goal is to develop a TM tool
specifically designed for CRA. Some tools have re-
cently been built to assist other critical activities of
biomedicine (e.g. literature curation for genetics).
A few of them have been evaluated for their practi-
cal usefulness in a real-world scenario (Karamanis
et al 2008, Demaine et al 2006). Such tools and
evaluations act as an important proof of concept for
biomedical TM and help to develop technology for
the needs of practical applications.
According to the interviews we conducted (Sec-
tion 2), a tool capable of identifying, ranking and
classifying articles based on the evidence they con-
tain, displaying the results to experts, and assisting
also in subsequent steps of CRA would be particu-
larly welcome. Such a tool, if developed in close
collaboration with users, could significantly increase
the productivity of CRA and enable risk assessors
to concentrate on what they are best at: the expert
judgement.
Acknowledgements Our work was funded by the
Royal Society (UK), the Medical Research Council
(G0601766) (UK) and the Swedish Council for Working
Life and Social Research (Sweden). LS was supported
by a Dorothy Hodgkin Postgraduate Award (UK). We
would like to thank Ian Lewin for his assistance at the
early stages of this work and for providing the first ver-
sion of the annotation tool. We are also grateful to Johan
Hogberg for supporting the annotation and the taxonomy
construction work.
References
Sophia Ananiadou, Douglas B. Kell, and Jun ichi Tsujii.
Text mining and its potential applications in systems
biology. Trends in Biotechnology, 24(12), 2006.
Y. Aphinyanaphongs, I. Tsamardinos, A. Statnikov,
D. Hardin, and C.F. Aliferis. Text categorization
models for high-quality article retrieval in internal
medicine. JAMIA, 12(2), 2005.
Matthew Blaschko and Arthur Gretton. Learning tax-
onomies by dependence maximization. In 22rd NIPS,
2009.
Yi-Wei Chen and Chih-Jen Lin. Combining SVMs with
various feature selection strategies. In Feature extrac-
tion, foundations and applications. 2006.
Aaron M. Cohen and William R. Hersh. A survey of
115
current work in biomedical text mining. Briefings in
Bioinformatics, 6(1), 2005.
Jacob Cohen. A coefficient of agreement for nominal
scales. Educ. Psychol. Meas., 20(1), 1960.
K. Bretonnel Cohen, Hong Yu, Philip E. Bourne, and
Lynette Hirschman. Translating biology:text mining
tools that work. In PSB, 2008.
Peter Corbett, Colin Batchelor, and Simone Teufel. An-
notation of chemical named entities. In Proceedings of
the ACL, 2007.
Jeffrey Demaine, Joel Martin, Lynn Wei, and Berry
de Bruijn. Litminer: integration of library services
within a bio-informatics application. Biomedical Dig-
ital Libraries, 3(1), 2006.
ECHA, 2008. Guidance on Information Requirements
and Chemical Safety Assessment. European Chemicals
Agency, 2008.
Bo Han, Zoran Obradovic, Zhang zhi Hu, Cathy H. Wu,
and Slobodan Vucetic. Substring selection for biomed-
ical document classification. Bioinformatics, 22, 2006.
Lawrence Hunter and K. Bretonnel Cohen. Biomedical
language processing: What?s beyond pubmed? Mol
Cell, 21(5), 2006.
N. Karamanis, R. Seal, I. Lewin, P. McQuilton, A. Vla-
chos, C. Gasperin, R. Drysdale, and T. Briscoe. Nat-
ural language processing in aid of flybase curators.
BMC Bioinformatics, 9(1), 2008.
Ashraf M. Kibriya, Eibe Frank, Bernhard Pfahringer, and
Geoffrey Holmes. Multinomial naive bayes for text
categorization revisited. In Australian Conference on
AI, volume 3339, 2004.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. Cor-
pus annotation for mining biomedical events from lter-
ature. BMC Bioinformatics, 9, 2008.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. Overview of the
protein-protein interaction annotation extraction task
of biocreative ii. Genome Biology, 2008.
J.Richard Landis and Gary G.Koch. The measurement of
observer agreement for categorical data. Biometrics,
33(1), 1977.
Ian Lewin, Ilona Silins, Anna Korhonen, Johan Hogberg,
and Ulla Stenius. A new challenge for text mining:
Cancer risk assessment. In Proceedings of the ISMB
BioLINK Special Interest Group on Text Data Mining.,
2008.
NCBI. Efetch entrez utility, 2005. URL
http://www.ncbi.nlm.nih.gov/entrez/
query/static/efetch_help.html.
Sturart J. Nelson, Tammy Powell, and Besty L.
Humphreys. The Unified Medical Language System
(UMLS) Project. In Encyclopedia of Library and In-
formation Science, pages 369?378. Marcel Dekker,
2002.
M. F. Porter. An algorithm for suffix stripping. Program,
14(3):130?137, 1980.
Jason D. M. Rennie and David Karger. Tackling the poor
assumptions of naive bayes text classifiers. In In Pro-
ceedings of the 20th ICML, 2003.
Y. Sasaki, S. Montemagni, P. Pezik, D. Rebholz-
Schuhmann, J. McNaught, and S. Ananiadou. BioLex-
icon: A Lexical Resource for the Biology Domain.
2008.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. Developing a Robust Part-
of-Speech Tagger for Biomedical Text. 3746, 2005.
EPA, 2005. Guidelines for carcinogen risk as-
sessment. U.S. Environmental Protection Agency,
2005. URL http://www.epa.gov/iris/
cancer032505.pdf.
Vladimir N. Vapnik. The nature of statistical learning
theory. New York, NY, USA, 1995.
Hongning Wang, Minlie Huang, Shilin Ding, and Xi-
aoyan Zhu. Exploiting and integrating rich features
for biological literature classification. BMC Bioinfor-
matics, 9(Suppl 3), 2008.
Yiming Yang and Xin Liu. A re-examination of text cate-
gorization methods. In Proceedings of the 22nd SIGIR,
New York, NY, USA, 1999.
Yiming Yang and Jan O. Pedersen. A comparative study
on feature selection in text categorization. 1997.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,
and Kevin B. Cohen. Frontiers of biomedical text min-
ing: current progress. Brief Bioinform, 8(5), 2007.
116
Automatic Classification of English Verbs Using Rich Syntactic Features
Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
ls418,alk23@cl.cam.ac.uk
Yuval Krymolowski
Department of Computer Science
University of Haifa
31905, Haifa
Israel
yuvalkry@gmail.com
Abstract
Previous research has shown that syntactic
features are the most informative features
in automatic verb classification. We exper-
iment with a new, rich feature set, extracted
from a large automatically acquired subcate-
gorisation lexicon for English, which incor-
porates information about arguments as well
as adjuncts. We evaluate this feature set us-
ing a set of supervised classifiers, most of
which are new to the task. The best classi-
fier (based on Maximum Entropy) yields the
promising accuracy of 60.1% in classifying
204 verbs to 17 Levin (1993) classes. We
discuss the impact of this result on the state-
of-art, and propose avenues for future work.
1 Introduction
Recent research shows that it is possible, using cur-
rent natural language processing (NLP) and machine
learning technology, to automatically induce lex-
ical classes from corpus data with promising ac-
curacy (Merlo and Stevenson, 2001; Korhonen et
al., 2003; Schulte im Walde, 2006; Joanis et al,
2007). This research is interesting, since lexi-
cal classifications, when tailored to the application
and domain in question, can provide an effective
means to deal with a number of important NLP
tasks (e.g. parsing, word sense disambiguation, se-
mantic role labeling), as well as enhance perfor-
mance in many applications (e.g. information ex-
traction, question-answering, machine translation)
(Dorr, 1997; Prescher et al, 2000; Swier and Steven-
son, 2004; Dang, 2004; Shi and Mihalcea, 2005).
Lexical classes are useful because they capture
generalizations over a range of (cross-)linguistic
properties. Being defined in terms of similar mean-
ing components and (morpho-)syntactic behaviour
of words (Jackendoff, 1990; Levin, 1993) they
generally incorporate a wider range of properties
than e.g. classes defined solely on semantic grounds
(Miller, 1990). They can be used to build a lexical
organization which effectively captures generaliza-
tions and predicts much of the syntax and semantics
of a new word by associating it with an appropriate
class. This can help compensate for lack of data for
individual words in NLP.
Large-scale exploitation of lexical classes in real-
world or domain-sensitive tasks has not been pos-
sible because existing manually built classifications
are incomprehensive. They are expensive to extend
and do not incorporate important statistical infor-
mation about the likelihood of different classes for
words. Automatic classification is a better alterna-
tive. It is cost-effective and gathers statistical infor-
mation as a side-effect of the acquisition process.
Most work on automatic classification has fo-
cussed on verbs which are typically the main pred-
icates in sentences. Syntactic features have proved
the most informative in verb classification. Exper-
iments have been reported using both (i) deep syn-
tactic features (e.g. subcategorization frames (SCFs))
extracted using parsers and subcategorisation acqui-
sition systems (Schulte im Walde, 2000; Korhonen
et al, 2003; Schulte im Walde, 2006) and (ii) shal-
low ones (e.g. NPs/PPs preceding/following verbs)
extracted using taggers and chunkers (Merlo and
Stevenson, 2001; Joanis et al, 2007).
769
(i) correspond closely with features used for
manual classification (Levin, 1993). They have
proved successful in the classification of German
(Schulte im Walde, 2006) and English verbs (Ko-
rhonen et al, 2003). Yet promising results have also
been reported when using (ii) for English verb clas-
sification (Merlo and Stevenson, 2001; Joanis et al,
2007). This may indicate that (i) are optimal for the
task when combined with additional syntactic infor-
mation from (ii).
We investigate this matter by experimenting with
a new, rich feature set which incorporates informa-
tion about SCFs (arguments) as well as adjuncts. It
was extracted from VALEX, a large automatically
acquired SCF lexicon for English (Korhonen et al,
2006). We evaluate the feature set thoroughly us-
ing set of supervised classifiers, most of which are
new in verb classification. The best performing clas-
sifier (Maximum Entropy) yields the accuracy of
60.1% on classifying 204 verbs into 17 Levin (1993)
classes. This result is good, considering that we per-
formed no sophisticated feature engineering or se-
lection based on the properties of the target classi-
fication (Joanis et al, 2007). We propose various
avenues for future work.
We introduce our target classification in section 2
and syntactic features in section 3. The classifica-
tion techniques are presented in section 4. Details
of the experimental evaluation are supplied in sec-
tion 5. Section 6 provides discussion and concludes
with directions for future work.
2 Test Verbs and Classes
We adopt as a target classification Levin?s (1993)
well-known taxonomy where verbs taking similar
diathesis alternations are assumed to share meaning
components and are organized into a semantically
coherent class. For instance, the class of ?Break
Verbs? (class 45.1) is partially characterized by its
participation in the following alternations:
1. Causative/inchoative alternation:
Tony broke the window ? The window broke
2. Middle alternation:
Tony broke the window ? The window broke easily
3. Instrument subject alternation:
Tony broke the window with the hammer ? The hammer
broke the window
LEVIN CLASS EXAMPLE VERBS
9.1 PUT bury, place, install, mount, put
10.1 REMOVE remove, abolish, eject, extract, deduct
11.1 SEND ship, post, send, mail, transmit
13.5.1 GET win, gain, earn, buy, get
18.1 HIT beat, slap, bang, knock, pound
22.2 AMALGAMATE contrast, match, overlap, unite, unify
29.2 CHARACTERIZE envisage, portray, regard, treat, enlist
30.3 PEER listen, stare, look, glance, gaze
31.1 AMUSE delight, scare, shock, confuse, upset
36.1 CORRESPOND cooperate, collide, concur, mate, flirt
37.3 MANNER OF shout, yell, moan, mutter, murmur
SPEAKING
37.7 SAY say, reply, mention, state, report
40.2 NONVERBAL smile, laugh, grin, sigh, gas
EXPRESSION
43.1 LIGHT EMISSION shine, flash, flare, glow, blaze
45.4 CHANGE OF STATE soften, weaken, melt, narrow, deepen
47.3 MODES OF BEING quake, falter, sway, swirl, teeter
WITH MOTION
51.3.2 RUN swim, fly, walk, slide, run
Table 1: Test classes and example verbs
Alternations are expressed as pairs of SCFs. Addi-
tional properties related to syntax, morphology and
extended meanings of member verbs are specified
with some classes. The taxonomy provides a classi-
fication of 4,186 verb senses into 48 broad and 192
fine-grained classes according to their participation
in 79 alternations involving NP and PP complements.
We selected 17 fine-grained classes and 12 mem-
ber verbs per class (table 2) for experimentation.
The small test set enabled us to evaluate our results
thoroughly. The classes were selected to (i) include
both syntactically and semantically similar and dif-
ferent classes (to vary the difficulty of the classifi-
cation task), and to (ii) have enough member verbs
whose predominant sense belongs to the class in
question (we verified this according to the method
described in (Korhonen et al, 2006)). As VALEX
was designed to maximise coverage most test verbs
had 1000-9000 occurrences in the lexicon.
3 Syntactic Features
We employed as features distributions of SCFs spe-
cific to given verbs. We extracted them from the re-
cent VALEX (Korhonen et al, 2006) lexicon which
provides SCF frequency information for 6,397 En-
glish verbs. VALEX was acquired automatically
from five large corpora and the Web (using up to
10,000 occurrences per verb) using the subcatego-
rization acquisition system of Briscoe and Carroll
(1997). The system incorporates RASP, a domain-
independent robust statistical parser (Briscoe and
770
Carroll, 2002), and a SCF classifier which iden-
tifies 163 verbal SCFs. The basic SCFs abstract
over lexically-governed particles and prepositions
and predicate selectional preferences.
We used the noisy unfiltered version of VALEX
which includes 33 SCFs per verb on average1. Some
are genuine SCFs but some express adjuncts (e.g.
I sang in the party could be SCF PP). A lexical
entry for each verb and SCF combination provides
e.g. the frequency of the entry (in active and passive)
in corpora, the POS tags of verb tokens, the argument
heads in argument positions, and the prepositions in
PP slots. We experimented with three feature sets:
1. Feature set 1: SCFs and their frequencies
2. Feature set 2: Feature set 1 with two high frequency
PP frames parameterized for prepositions: the simple PP
(e.g. they apologized to him) and NP-PP (e.g. he removed
the shoes from the bag) frames.
3. Feature set 3: Feature set 2 with three additional high
frequency PP frames parameterized for prepositions: the
NP-FOR-NP (e.g. he bought a book for him), NP-TO-NP
(e.g. he gave a kiss to her), and OC-AP, EQUI, AS (e.g. he
condemned him as stupid) frames.
In feature sets 2 and 3, 2-5 PP SCFs were refined ac-
cording to the prepositions provided in the VALEX
SCF entries (e.g. PP at, PP on, PP in) because Levin
specifies prepositions with some SCFs / classes. The
scope was restricted to the 3-5 highest ranked PP
SCFs to reduce the effects of sparse data.
4 Classification
4.1 Preparing the Data
A feature vector was constructed for each verb.
VALEX includes 107, 287 and 305 SCF types for fea-
ture sets 1, 2, and 3, respectively. Each feature corre-
sponds to a SCF type, and its value is the relative fre-
quency of the SCF with the verb in question. Some
of the feature values are zero, because most verbs
take only a subset of the possible SCFs.
4.2 Machine Learning Methods
We implemented three methods for classification:
the K nearest neighbours (KNN), support vector ma-
chines (SVM), and maximum entropy (ME). To our
knowledge, only SVM has been previously used for
1The SCF accuracy of this lexicon is 23.7 F-measure, see
(Korhonen et al, 2006) for details.
verb classification. The free parameters were opti-
mised for each feature set by (i) defining the value
range (as explained below), and by (ii) searching for
the optimal value on the training data using 10 fold
cross validation (section 5.2).
4.2.1 K Nearest Neighbours
KNN is a memory-based classification method
based on the distances between verbs in the feature
space. For each verb in the test data, we measure
its distance to each verb in the training data. The
verb class label is the most frequent label in the
top K closest training verbs. We use the entropy-
based Jensen-Shannon (JS) divergence as the dis-
tance measure:
JS(P,Q) = 12
?D(P?P+Q2 ) +D(Q?P+Q2 )
?
The range of the parameter K is 2-20.
4.2.2 Support Vector Machines
SVM (Vapnik, 1995) tries to find a maximal mar-
gin hyperplane to separate between two groups of
verb feature vectors. In practice, a linear hyperplane
does not always exist. SVM uses a kernel function
to map the original feature vectors to higher dimen-
sion space. The ?maximal margin? optimizes our
choice of dimensionality to avoid over-fitting. We
use Chang and Lin (2001) ?s LIBSVM library to im-
plement the SVM. Following Hsu et al (2003), we
use the radial basis function as the kernel function:
K(xi, xj) = exp (??||xi ? xj ||2), ? > 0
? and the cost of the error term C (the penalty for
margin errors) are optimized. The search ranges of
Hsu et al (2003) are used:
C = 2?5, 2?3, . . . , 215, 217 ; ? = 2?17, 2?15, . . . , 21, 23
4.2.3 Maximum Entropy
ME constructs a probabilistic model that maxi-
mizes entropy on test data subject to a set of feature
constraints. If verb x is in class 10.1 and takes the
SCF 49 (NP-PP) with the relative frequency of 0.6 in
feature function f , we have
f(x, y) = 0.6 if y = 10.1 and x = 49
The expected value of a feature f with respect to the
empirical distribution (training data) is
E?(f) ?Px,y p?(x, y)f(x, y)
The expected value of the feature f (on test data)
with respect to the model p(y|x) is
771
E(f) ?Px,y p?(x)p(y|x)f(x, y)
p?(x) is the empirical distribution of x in the train-
ing data. We constrain E(f) to be the same as E?(f)
E(f) = E?(f)
The model must maximize the entropy H(Y |X)
H(Y |X) ? ?Px,y p?(x)p(y|x) log p(y|x)
The constraint-optimization problem is solved by
the Lagrange multiplier (Pietra et al, 1997). We
used Zhang (2004)?s maximum entropy toolkit for
implementation. The number of iterations i (5-50)
of the parameter estimation algorithm is optimised.
5 Experiments
5.1 Methodology
We split the data into training and test sets using two
methods. The first is ?leave one out? cross-validation
where one verb in each class is held out as test data,
and the remaining N-1 (i.e. 11) verbs are used as
training data. The overall accuracy is the average
accuracy of N rounds. The second method is re-
sampling. For each class, 3 verbs are selected ran-
domly as test data, and 9 are used as training data.
The process is repeated 30 times, and the average
result is recorded.
5.2 Measures
The methods are evaluated using first accuracy ? the
percentage of correct classifications out of all the
classifications:
Accuracy = truePositivestruePositives+falseNegatives
When evaluating the performance at class level, pre-
cision and recall are calculated as follows:
Precision = truePositivestruePositives+falsePositives
Recall = truePositivestruePositives+falseNegatives
F-score is the balance over recall and precision. We
report the average F-score over the 17 classes. Given
there are 17 classes in the data, the accuracy of ran-
domly assigning a verb into one of the 17 classes is
1/17 ? 5.8%.
5.3 Results from Quantitative Evaluation
Table 2 shows the average performance of each clas-
sifier and feature set according to ?leave one out?
cross-validation2. Each classifier performs consid-
erably better than the random baseline. The simple
2Recall is not shown as it is identical here with accuracy.
KNN method produces the lowest accuracy (44.1-
54.9) and SVM and ME the best (47.1-57.9 and 47.5-
59.3, respectively).
The performance of all methods improves sharply
when moving from the feature set 1 to the refined
feature set 2: both accuracy and F-measure improve
by over 10%. When moving from feature set 2 to
the sparser feature set 3 (which includes a higher
number of low frequency PP features) KNN worsens
clearly (c. 5% in accuracy and F-measure) while the
improvement in other methods is very small. This
suggests that KNN deals worse than other methods
with sparse data.
The resampling results in table 3 reveal that some
classifiers perform worse than others when less
training data is available3. KNN produces consid-
erably lower results, particularly with the sparse
feature set 3: 28.2 F-measure vs. 48.2 with cross-
validation. Also SVM performs worse with fea-
ture set 3: 54.6 F-measure vs. 58.2 with cross-
validation. ME thus appears the most robust method
with smaller training data, producing results compa-
rable with those in cross-validation.
Figure 1 shows the F-measure for 17 individual
classes when the methods are used with feature set
3. Levin classes 40.2, 29.2, and 37.3 (see table 2)
(the ones taking fewer prepositions with higher fre-
quency) have the best average performance (65% or
more), and classes 47.3, 45.4 and 18.1 the worst
(40% or less). ME outperforms SVM with 9 of the
17 classes.
5.4 Qualitative Evaluation
We did some qualitative analysis to trace the ori-
gin of error types produced by ME with feature set
3. Examination of the worst performing class 47.3
(MODES OF BEING INVOLVING MOTION verbs) il-
lustrates well the various error types. 10 of the 12
verbs in this class are classified incorrectly:
? 3 in class 43.1 (LIGHT EMISSION verbs): Verbs in 47.3
and 43.1 describe intrinsic properties of their subjects
(e.g. a jewel sparkles, a flag flutters). Their similar al-
ternations and PP SCFs make it difficult to separate them
on syntactic grounds.
? 2 in class 51.3.2 (RUN verbs): 47.3 and 51.3.2 share the
meaning component of motion. Their members take sim-
ilar alternations and SCFs, which causes the confusion.
3Recall that the amount of training data is smaller with re-
sampling evaluation, see section 5.2.
772
Feature set 1 Feature set 2 Feature set 3
ACC P F ACC P F ACC P F
RAND 5.8 5.8 5.8
KNN 44.1 48.4 44.0 54.9 56.9 53.9 49.5 47.0 48.2
ME 47.5 49.4 47.6 59.3 61.4 59.9 59.3 61.9 60.0
SVM 47.1 50.4 47.8 57.8 59.4 57.9 57.8 60.1 58.2
Table 2: ?Leave one out? cross-validation results for KNN, ME, and SVM
Feature set 1 Feature set 2 Feature set 3
ACC P F ACC P F ACC P F
RAND 5.8 5.8 5.8
KNN 37.3 39.9 36.5 42.7 47.2 42.6 27.1 34.2 28.2
ME 47.1 47.3 47.0 58.1 59.1 58.1 60.1 60.5 59.8
SVM 47.3 50.2 47.7 56.8 59.5 57.1 54.4 56.5 54.6
Table 3: Re-sampling results for KNN, ME, and SVM
? 2 in class 37.7 (SAY verbs) and 1 in class 37.3 (MANNER
OF SPEAKING verbs): 47.3 differs in semantics and syn-
tax from 37.7 and 37.3. The confusion is due to idiosyn-
cratic properties of individual verbs (e.g. quake, wiggle).
? 1 in class 36.1 (CORRESPOND verbs): 47.3 and 36.1 are
semantically very different, but their members take simi-
lar intransitive and PP SCFs with high frequency.
? 1 in class 45.4 (OTHER CHANGE OF STATE verbs):
Classes 47.3 and 45.3 are semantically different. Their
similar PP SCFs explains the misclassification.
Most errors concern classes which are in fact se-
mantically related. Unfortunately there is no gold
standard which would comprehensively capture the
semantic relatedness of Levin classes. Other er-
rors concern semantically unrelated but syntactically
similar classes ? cases which we may be able to ad-
dress in the future with careful feature engineering.
Some errors relate to syntactic idiosyncracy. These
show the true limits of lexical classification - the fact
that the correspondence between the syntax and se-
mantics of verbs is not always perfect.
6 Discussion and Conclusion
Our best results (e.g. 60.1 accuracy and 59.8 F-
measure of ME) are good, considering that no so-
phisticated feature engineering / selection based on
the properties of the target classification was per-
formed in these experiments. The closest compari-
son point is the recent experiment reported by Joanis
et al (2007) which involved classifying 835 English
verbs to 14 Levin classes using SVM. Features were
specifically selected via analysis of alternations that
are used to characterize Levin classes. Both shal-
low syntactic features (syntactic slots obtained us-
ing a chunker) and deep ones (SCFs extracted using
Briscoe and Carroll?s system) were used. The accu-
racy was 58% with the former and only 38% with
the latter. This experiment is not directly compa-
rable with ours as we classified a smaller number
of verbs (204) to a higher number of Levin classes
(17) (i.e. we had less training data) and did not se-
lect the optimal set of features using Levin?s alter-
nations. We nevertheless obtained better accuracy
with our best performing method, and better accu-
racy (47%) with the same method (SVM) when the
comparable feature set 1 was acquired using the very
same subcategorization acquisition system.
It is likely that using larger and noisier SCF data
explains the better result, suggesting that rich syn-
tactic features incorporating information about both
arguments and adjuncts are ideal for verb classifica-
tion. Further experiments are required to determine
the optimal set of features. In the future, we plan
to experiment with different (noisy and filtered) ver-
sions of VALEX and add to the comparison a shal-
lower set of features (e.g. NP and PP slots in VALEX
regardless of the specific SCFs). We will also im-
prove the features e.g. by enriching them with addi-
tional syntactic information available in VALEX lex-
ical entries.
Acknowledgement
This work was partially supported by the Royal So-
ciety, UK.
773
Figure 1: Class level F-score for feature set 3 (cross-validation)
References
E. J. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In Proceedings of
the 5th ACL Conference on Applied Natural Language
Processing, pages 356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd LREC, pages 1499?1504, Las Palmas, Gran Ca-
naria.
C. Chang and J. Lin, 2001. LIBSVM: a library for sup-
port vector machines.
H. T. Dang. 2004. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. Ph.D.
thesis, CIS, University of Pennsylvania.
B. J. Dorr. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?322.
W. Hsu, C. Chang, and J. Lin. 2003. A practical guide to
support vector classification.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
E. Joanis, S. Stevenson, and D. James. 2007. A general
feature space for automatic verb classification. Natu-
ral Language Engineering, Forthcoming.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 64?71.
A. Korhonen, Y. Krymolowski, and T. Briscoe. 2006.
A large subcategorization lexicon for natural language
processing applications. In Proceedings of LREC.
B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics, 27(3):373?408.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235?312.
S. D. Pietra, J. D. Pietra, and J. D. Lafferty. 1997. Induc-
ing features of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(4):380?
393.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using a
probabilistic class-based lexicon for lexical ambiguity
resolution. In 18th International Conference on Com-
putational Linguistics, pages 649?655, Saarbru?cken,
Germany.
S. Schulte im Walde. 2000. Clustering verbs semanti-
cally according to their alternation behaviour. In Pro-
ceedings of COLING, pages 747?753, Saarbru?cken,
Germany.
S. Schulte im Walde. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159?194.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proceedings of the Sixth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, Mexico City, Mexico.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labelling. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 95?102, Barcelona, Spain, August.
V. N. Vapnik. 1995. The nature of statistical learning
theory. Springer-Verlag New York, Inc., New York,
NY, USA.
L. Zhang, 2004. Maximum Entropy Modeling Toolkit for
Python and C++, December.
774
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 689?697,
Beijing, August 2010
Exploring variation across biomedical subdomains
Tom Lippincott and Diarmuid O? Se?aghdha and Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
{tl318,do242,ls418,alk23}@cam.ac.uk
Abstract
Previous research has demonstrated the
importance of handling differences be-
tween domains such as ?newswire? and
?biomedicine? when porting NLP systems
from one domain to another. In this paper
we identify the related issue of subdomain
variation, i.e., differences between subsets
of a domain that might be expected to be-
have homogeneously. Using a large corpus
of research articles, we explore how subdo-
mains of biomedicine vary across a variety
of linguistic dimensions and discover that
there is rich variation. We conclude that
an awareness of such variation is necessary
when deploying NLP systems for use in
single or multiple subdomains.
1 Introduction
One of the most noticeable trends in the past
decade of Natural Language Processing (NLP) re-
search has been the deployment of language pro-
cessing technology to meet the information re-
trieval and extraction needs of scientists in other
disciplines. This meeting of fields has proven mu-
tually beneficial: scientists increasingly rely on
automated tools to help them cope with the expo-
nentially expanding body of publications in their
field, while NLP researchers have been spurred to
address new conceptual problems in theirs. Among
the fundamental advances from the NLP perspec-
tive has been the realisation that tools which per-
form well on textual data from one source may fail
to do so on another unless they are tailored to the
new source in some way. This has led to signifi-
cant interest in the idea of contrasting domains and
the concomitant problem of domain adaptation,
as well as the production of manually annotated
domain-specific corpora.1
One definition of domain variation associates
it with differences in the underlying probability
distributions from which different sets of data are
drawn (Daume? III and Marcu, 2006). The concept
also mirrors the notion of variation across thematic
subjects and the corpus-linguistic notions of reg-
ister and genre (Biber, 1988). In addition to the
differences in vocabulary that one would expect
to observe, domains can vary in many linguistic
variables that affect NLP systems. The scientific
domain which has received the most attention (and
is the focus of this paper) is the biomedical domain.
Notable examples of corpus construction projects
for the biomedical domain are PennBioIE (Kulick
et al, 2004) and GENIA (Kim et al, 2003). These
corpora have been used to develop systems for a
range of processing tasks, from entity recognition
(Jin et al, 2006) to parsing (Hara et al, 2005) to
coreference resolution (Nguyen and Kim, 2008).
An implicit assumption in much previous work
on biomedical NLP has been that particular subdo-
mains of biomedical literature ? typically molec-
ular biology ? can be used as a model of biomed-
ical language in general. For example, GENIA
consists of abstracts dealing with a specific set
of subjects in molecular biology, while PennBioIE
covers abstracts in two specialised domains, cancer
genomics and the behaviour of a particular class
of enzymes. This assumption of representative-
ness is understandable because linguistic annota-
tion is labour-intensive and it may not be worth-
while to produce annotated corpora for multiple
subdomains within a single discipline if there is lit-
1A workshop dedicated to domain adaptation is collocated
with ACL 2010.
689
tle task-relevant variation across those subdomains.
However, such conclusions should not be made
before studying the actual degree of difference be-
tween the subdomains of interest.
One of the principal goals of this paper is to map
how the concept of ?biomedical language?, often
construed as a monolithic entity, is composed of
diverse patterns of behaviour at more fine-grained
topical levels. Hence we study linguistic variation
in a broad biomedical corpus of abstracts and full
papers, the PMC Open Access Subset.2 We select
a range of lexical and structural phenomena for
quantitative investigation. The results indicate that
common subdomains for resource development are
not representative of biomedical text in general and
furthermore that different linguistic features often
partition the subdomains in quite different ways.
2 Related Work
A number of researchers have explored the dif-
ferences between non-technical and scientific lan-
guage. Biber and Gray (2010) describe two
distinctive syntactic characteristics of academic
writing which set it apart from general English.
Firstly, in academic writing additional information
is most commonly integrated by pre- and post-
modification of phrases rather than by the addi-
tion of extra clauses. Secondly, academic writing
places greater demands on the reader by omitting
non-essential information, through the frequent
use of passivisation, nominalisation and noun com-
pounding. Biber and Gray also show that these ten-
dencies towards ?less elaborate and less explicit?
language have become more pronounced in recent
history.
We now turn to corpus studies that focus on
biomedical writing. Verspoor et al (2009) use
measurements of lexical and structural variation
to demonstrate that Open Access and subscription-
based journal articles in a specific domain (mouse
genomics) are sufficiently similar that research on
the former can be taken as representative of the lat-
ter. While their primary goal is different from ours
and they do not consider variation across multiple
domains, they do compare their mouse genomics
corpus with small reference corpora drawn from
2http://www.ncbi.nlm.nih.gov/pmc/
about/openftlist.html
newswire and general biomedical sources. This
analysis unsurprisingly finds differences between
the domain and newswire corpora across many
linguistic dimensions; more interestingly for our
purposes, the comparison of domain text to the
broader biomedical superdomain shows a more
complex picture with similarities in some aspects
(e.g., passivisation and negation) and dissimilari-
ties in others (e.g., sentence length, semantic fea-
tures).
Friedman et al (2002) document the ?sublan-
guages? associated with two biomedical domains:
clinical reports and molecular biology articles.
They set out restricted ontologies and frequent co-
occurrence templates for the two domains and dis-
cuss the similarities and differences between them,
but they do not perform any quantitative analysis.
Other researchers have focused on specific phe-
nomena, rather than cataloguing a broad scope
of variation. Cohen et al (2008) carry out a de-
tailed analysis of argument realisation with respect
to verbs and nominalisations, using the GENIA
and PennBioIE corpora. Nguyen and Kim (2008)
compare the behaviour of anaphoric pronouns in
newswire and biomedical corpora; they improve
the performance of a pronoun resolver by incorpo-
rating their observations, thus demonstrating the
importance of capturing domain-specific phenom-
ena. Nguyen and Kim?s findings are discussed in
more detail in Section 5.4 below.
3 Subdomains in the OpenPMC Corpus
The Open Access Subset of PubMed (OpenPMC)
is the largest publicly available corpus of full-text
articles in the biomedical domain. OpenPMC is
comprised of 169,338 articles drawn from 1233
medical journals, totalling approximately 400 mil-
lion words. The NIH maintains a one-to-many
mapping from journals to 122 subject areas (NIH,
2009b). This covers about 400 of the OpenPMC
journals, but these account for over 70% of the
database by byte size and word count. Journals are
assigned up to five subject areas with the majority
assigned one (69%) or two (26%) subjects. In this
paper we adopt the OpenPMC subject areas (e.g.
?Pulmonary Medicine?, ?Genetics?, ?Psychiatry?)
as the basis for subdomain comparison.
690
0 10 20 30 40Word count (millions)
Ethics
Complementary Therapies
Education
Obstetrics
Pharmacology
Geriatrics
Gastroenterology
Pediatrics
Veterinary Medicine
Biomedical Engineering
Psychiatry
Embryology
Genetics, Medical
Ophthalmology
Vascular Diseases
Botany
Virology
Endocrinology
Pulmonary Medicine
Physiology
Tropical Medicine
Critical Care
Rheumatology
Cell Biology
Communicable Diseases
Science
Neurology
Biotechnology
Medicine
Microbiology
Environmental Health
Public Health
Biochemistry
Molecular Biology
Neoplasms
Medical Informatics
Genetics
Figure 1: OpenPMC word count by subdomain,
dark colouring indicates data assigned single sub-
domain, each lighter shade indicates an additional
overlapping subdomain
4 Methodology
4.1 Data selection and preprocessing
An important initial question was how to treat data
with multiple classifications: we only consider
journals assigned a single subdomain, to avoid
the added complexity of interactions in data from
overlapping subdomains. To ensure sufficient data
for comparing a variety of linguistic features, we
discard the subdomains with less than one mil-
lion words meeting the single-subdomain criterion.
After review, we also drop the ?Biology? subdo-
main, which appears to function as a catch-all for
many loosely related areas. Figure 1 shows the
distribution of data across the subjects we use, by
word-count, with lighter-coloured areas represent-
ing data that is assigned multiple subjects. These
subjects provide a convenient starting point for di-
viding the corpus into subdomains (hereafter, ?sub-
domain? will be used rather than ?subject?). We
also add a reference subdomain, ?Newswire?, com-
posed of a 6 million word random sample from the
English Gigaword corpus (Graff et al, 2005). The
final data set has a total of 39 subdomains.
Articles in the OpenPMC corpus are formatted
according to a standard XML tag set (NIH, 2009a).
We first convert each article to plain text, ignoring
?non-content? elements such as tables and formulas,
and split the result into sentences, aggregating the
results by subdomain.
4.2 Feature extraction
We investigate subdomain variation in our cor-
pus across a range of lexical, syntactic, sentential
and discourse features. The corpus is lemmatised,
tagged and parsed using the C&C pipeline (Cur-
ran et al, 2007) with the adapted part-of-speech
and lexical category tagging models produced by
Rimell and Clark (2009) for biomedical parsing.
From this output we count occurrences of noun,
verb, adjective and adverb lemmas, part-of-speech
(POS) tags, grammatical relations (GRs), chunks,
and lexical categories. The lemma features are
Zipfian-distributed items from an open class, so
we have experimented with filtering low-frequency
items at various thresholds to reduce noise and
improve processing speed. The other feature sets
can be viewed as closed classes, where filtering is
unnecessary.
Since verbs are central to the meaning and struc-
ture of sentences, we consider their special behav-
ior by constructing features for each verb?s dis-
tribution over other grammatical properties. Sev-
eral grammatical properties are captured by pairing
each verb with its POS (indicating e.g. tense, such
as present, past, and present participle). Voice is de-
termined from additional annotation output by the
C&C parser. Table 1 shows the POS-distribution
for the verb ?restrict?, in two subdomains from
the corpus. Finally, we record distributions over
verb subcategorization frames (SCFs) taken by
each verb, and over the GRs it participates in.
691
Subdomain VB VBG VBN VBP VBZ
Medical Informatics .35 .29 .06 .09 .21
Cell Biology .14 .43 .05 .10 .29
Table 1: Distribution over POS tags for verb ?re-
strict?, in two subdomains
SCFs were extracted using a system of Preiss et al
(2007).
To facilitate a more robust and interpretable anal-
ysis of vocabulary differences, we estimate a ?topic
model? of the corpus with Latent Dirichlet Analy-
sis (Blei et al, 2003) using the MALLET toolkit.3
As preprocessing we divide the corpus into arti-
cles, removing stopwords and words shorter than
3 characters. The Gibbs sampling procedure is
parameterised to induce 100 topics, each giving a
coherent cluster of related words learned from the
data, and to run for 1000 iterations. We collate the
predicted distribution over topics for each article
in a subdomain, weighted by article wordcount, to
produce a topic distribution for the subdomain.
4.3 Measurements of divergence
Our goal is to illustrate the presence or absence
of differences between the feature sets, and to do
so we calculated the Jensen-Shannon divergence
and the Pearson correlation. Jensen-Shannon diver-
gence is a finite symmetric measurement of the di-
vergence between probability distributions, while
Pearson correlation quantifies the linear relation-
ship between two real-valued samples.
The count-features are weighted, for a given
subdomain, by the feature?s log-likelihood be-
tween the subdomain?s data and the rest of the
corpus. Log-likelihood has been shown to perform
well when comparing counts of potentially low-
frequency features (Rayson and Garside, 2000)
such as found in Zipfian-distributed data. This
serves to place more weight in the comparison on
items that are distinctive of the subdomain with
respect to the entire corpus.
While the count-features are treated as a single
distribution for the purposes of JSD, the verbwise-
features are composed of many distributions, one
for each verb lemma. Our approach is to com-
bine the JSD of the verbs, weighted by the log-
3http://mallet.cs.umass.edu
likelihood of the verb lemma between the two
subdomains in question, and normalize the dis-
tances to the interval [0, 1]. Using the lemma?s log-
likelihood assumes that, when a verb?s distribution
behaves differently in a subdomain, its frequency
changes as well.
We present the results as dendrograms and
heat maps. Dendrograms are tree structures that
illustrate the results of hierarchical clustering.
We perform hierarchical clustering on the inter-
subdomain divergences for each set of features.
The algorithm begins with each instance (in our
case, subdomains) as a singleton cluster, and re-
peatedly joins the two most similar clusters until
all the data is clustered together. The order of these
merges is recorded as a tree structure that can be
visualized as a dendrogram in which the length of
a branch represents the distance between its child
nodes. Similarity between clusters is calculated us-
ing average distance between all members, known
as ?average linking?.
Heat maps show the pairwise calculation of
a metric in a grid of squares, where square
(x, y) is shaded according to the value of
metric(subx, suby). For our measurements of
JSD, black represents 0 (i.e. identical distributions)
and white represents the metric?s theoretical maxi-
mum of 1. We also inscribe the actual value inside
each square. Dendrograms are tree structures that
illustrate the hierarchical clustering procedure de-
scribed above. The dendrograms present all 39
subdomains, while for readability the heatmaps
present 12 subdomains selected for representative-
ness.
5 Results
Different thresholds for filtering low-frequency
terms had little effect on the divergence measures,
and served mainly to improve processing time. We
therefore report results using a cutoff of 150 occur-
rences (over the entire 234 million word data set)
and log-likelihood weights. The results of Pearson
correlation and JSD show similar trends, and due
to its specific design for comparing distributions
we only report the latter.
692
5.1 Vocabulary and lexical features
Differences in vocabulary are what first comes to
mind when describing subdomains. Word features
are fundamental components for systems such as
POS taggers and lexicalised parsers; one therefore
expects that these systems will be affected by vari-
ation in lexical distributions. Figure 2a uses JSD
calculated on each subdomain?s distribution over
100 LDA-induced topics to compare vocabulary
distributions. Subdomains related to molecular
biology (Genetics, Molecular Biology) show the
smallest divergences, an interesting fact since these
are heavily used in building resources for BioNLP.
The dendrogram shows a rough division into ?pub-
lic policy?, ?patient-centric?, ?applied? and ?mi-
croscopic? subdomains, with the distance between
unrelated subdomains such as Biochemistry and
Pediatrics almost as large as their respective differ-
ences from Newswire.
We omit figures for variation over noun, verb
and adjective lemmas due to space restrictions; in
general, these correlate with the variation in LDA
topics though there are some differences. Figure 2b
shows JSD calculated on distributions over adverb
lemmas. Part of the variation is due to character-
istic markers of scientific argument (?therefore?,
?significantly?, ?statistically?). A more interesting
factor is the coining of domain-specific adverbs,
an example of the tendency in scientific text to use
complex lexical items and premodifiers rather than
additional clauses. This also has the effect of mov-
ing subdomain-specific objects and processes from
verbs and nouns to adverbs. This behavior seems
non-continuous, in that subdomains either make
heavy, or almost no, use of it: for example, Pedi-
atrics has no subdomain-specific items among the
its ten top adverbs by log-likelihood, while Neo-
plasms has ?histologically?, ?immunohistochemi-
cally? and ?subcutaneously?. These information-
dense terms could prove useful for tasks like auto-
matic curation of subdomain vocabularies, where
they imply relationships between their components,
the items they modify, etc.
5.2 Verb distributional behavior
Modelling verb behavior is important for both syn-
tactic (Collins, 2003) and semantic (Korhonen et
al., 2008) processing, and subdomains are known
to conscript verbs into specific roles that change the
distributions of their syntactic properties (Roland
and Jurafsky, 1998). The four properties we con-
sidered verbs? distributions over (SCF, POS, GR
and voice) produced similar inter-subdomain JSD
values. Figure 2c demonstrates how verbs differ
between subdomains with respect to SCFs. For
example, while the Pediatrics subdomain uses the
verb ?govern? in a single SCF among its 12 pos-
sibilities, the Genetics subdomain distributes its
usage over 7 of them. Two subdomains may both
use ?restrict? with high frequency (e.g. Molecular
Biology and Ethics), but with different frequency
distributions over SCFs.
5.3 Syntax
It is difficult to measure syntactic complexity accu-
rately without access to a hand-annotated treebank,
but it is well-known that sentence length corre-
lates strongly with processing difficulty (Collins,
1996). The first column of Table 2 gives average
sentence lengths (excluding punctuation and ?sen-
tences? of fewer than three words) for selected
domains. All standard errors are < 0.1. It is clear
that all biomedical subdomains typically use longer
sentences than newswire, though there is also vari-
ation within biomedicine, from an average length
of 27 words in Molecular Biology to 24.5 words
in Pediatrics.
?Packaging? information in complex pre- and/or
post-modified noun phrases is a characteristic fea-
ture of academic writing (Biber and Gray, 2010).
This increases the information density of a sen-
tence but brings with it syntactic and semantic
ambiguities. For example, the difficulty of resolv-
ing the internal structure of noun-noun compounds
and strings of prepositional phrases has been the fo-
cus of ongoing research in NLP; these phenomena
have also been identified as significant challenges
in biomedical language processing (Rosario and
Hearst, 2001; Schuman and Bergler, 2006). The
second and third columns of Table 2 present aver-
age lengths for full noun phrases, defined as every
word dominated by a head noun in the grammat-
ical relation graph for a sentence, and for base
nominals, defined as nouns plus premodifying ad-
jectives and nouns only. All standard errors are
? 0.01. Newswire text uses the simplest noun
693
(a) LDA-induced distribution over topics
(b) Adverb lemma frequencies
(c) Verb distributions over subcategorization frames
Figure 2: Subdomain variation plotted as heat maps and dendrograms
694
Sentence length Full NP length Base nominal length
Mol. Biology 27.0 Biochemistry 4.03 Biochemistry 1.85
Genetics 26.6 Genetics 3.90 Neoplasms 1.85
Cell Biology 26.3 Critical Care 3.86 Mol. Biology 1.84
Ethics 26.2 Neoplasms 3.85 Genetics 1.83
PMC Average 25.9 PMC Average 3.85 PMC Average 1.80
Biochemistry 25.8 Pediatrics 3.84 Cell Biology 1.80
Neoplasms 25.5 Med. Informatics 3.84 Critical Care 1.80
Psychiatry 25.3 Comm. Diseases 3.81 Med. Informatics 1.78
Critical Care 25.0 Therapeutics 3.80 Comm. Diseases 1.78
Therapeutics 24.9 Mol. Biology 3.79 Therapeutics 1.75
Comm. Diseases 24.9 Psychiatry 3.77 Psychiatry 1.75
Med. Informatics 24.6 Ethics 3.69 Pediatrics 1.73
Pediatrics 24.6 Cell Biology 3.55 Ethics 1.65
Newswire 19.1 Newswire 3.18 Newswire 1.60
Table 2: Average sentence, NP and base nominal lengths across domains
phrase structures; there is notable variation across
PMC domains. Full NP and base nominal lengths
do not always correlate; for example, Cell Biol-
ogy uses relatively long base NPs (nominalisations
and multitoken names in particular) but relatively
simple full NP structures.
5.4 Coreference
Resolving coreferential terms is a crucial and chal-
lenging task when extracting information from
texts in any domain. Nguyen and Kim (2008)
compare the use of pronouns in the newswire
and biomedical domains, using the GENIA cor-
pus as representative of the latter. Among the dif-
ferences observed between the domains were the
absence of any personal pronouns other than third-
person neuter pronouns in the GENIA corpus, and
a greater proportion of demonstrative pronouns in
GENIA than in the ACE or MUC newswire cor-
pora. Corroborating the importance of domain
modelling, Nguyen and Kim demonstrate that tai-
loring a pronoun resolution system to specific prop-
erties of the biomedical domain improves perfor-
mance.
As our corpus is not annotated for coreference
we restrict our attention to types that are reliably
coreferential: masculine/feminine personal pro-
nouns (he, she and case variations), neuter personal
pronouns (they, it and variations) and definite NPs
with demonstrative determiners such as this and
that. To filter out pleonastic pronouns we used a
combination of the C+C parser?s pleonasm tag and
heuristics based on Lappin and Leass (1994). To
filter out the most common class of non-anaphoric
demonstrative NPs we simply discarded any match-
ing the pattern this. . . paper|study|article.
Table 3 presents statistics for selected types of
coreferential noun phrases in a number of domains.
The results generally agree with the findings of
Nguyen and Kim (2008): biomedical text is on
average 200 times less likely than news text to
use gendered pronouns and twice as likely to use
anaphoric definite noun phrases. At the domain
level, however, there is clear variation within the
biomedical corpus. In contrast to Nguyen and
Kim?s observations about GENIA some domains
do make non-negligible use of gendered pronouns,
most notably Ethics (usually to refer to other schol-
ars) and domains such as Psychiatry and Pediatrics
where studies of actual patients are common. All
biomedical domains use demonstrative NPs more
frequently than newswire and only one (Ethics)
matches newswire for frequent use of neuter 3rd-
person pronouns.
6 Conclusion
In this paper we have explored the phenomenon
of linguistic variation at a finer-grained level than
previous NLP research, focusing on subdomains
695
Pronouns (neuter, 3rd) Pronouns (non-neuter, 3rd) Demonstrative NPs
Ethics 0.0658 Newswire 0.0591 Genetics 0.0275
Newswire 0.0607 Ethics 0.0037 Med. Informatics 0.0263
Therapeutics 0.0354 Pediatrics 0.0015 Biochemistry 0.0263
Med. Informatics 0.0346 Psychiatry 0.0009 Ethics 0.0260
Psychiatry 0.0342 Comm. Diseases 0.0009 Mol. Biology 0.0251
Pediatrics 0.0308 Therapeutics 0.0005 PMC Average 0.0226
PMC Average 0.0284 PMC Average 0.0005 Cell Biology 0.0210
Genetics 0.0275 Critical Care 0.0004 Comm. Diseases 0.0207
Critical Care 0.0272 Neoplasms 0.0002 Neoplasms 0.0205
Mol. Biology 0.0258 Med. Informatics 0.0002 Psychiatry 0.0201
Biochemistry 0.0251 Genetics 0.0001 Critical Care 0.0201
Neoplasms 0.0227 Mol. Biology 2.5? 10?5 Therapeutics 0.0192
Cell Biology 0.0217 Biochemistry 2.0? 10?5 Pediatrics 0.0191
Comm. Diseases 0.0213 Cell Biology 1.5? 10?5 Newswire 0.0118
Table 3: Frequency of coreferential types (proportion of all NPs) across domains
rather than traditional domains such as ?newswire?
and ?biomedicine?. We have identified patterns of
variation across dimensions of vocabulary, syntax
and discourse that are known to be of importance
for NLP applications. While the magnitude of vari-
ation between subdomains is unsurprisingly less
pronounced than between coarser domains, sub-
domain variation clearly does exist and should be
taken into account when considering the generalis-
ability of systems trained and evaluated on specific
subdomains, for example molecular biology.
Future work includes directly evaluating the ef-
fect of subdomain variation on practical tasks, in-
vestigating further dimensions of variation such
as nominalisation usage and learning alternative
subdomain taxonomies directly from the corpus
text. Ultimately, we expect that a more nuanced
understanding of subdomain effects will have tan-
gible benefits for many applications of scientific
language processing.
Acknowledgements
This work was supported by EPSRC grant
EP/G051070/1, the Royal Society (AK) and a
Dorothy Hodgkin Postgraduate Award (LS).
References
Biber, Douglas and Bethany Gray. 2010. Challeng-
ing stereotypes about academic writing: Complex-
ity, elaboration, explicitness. Journal of English for
Academic Purposes, 9(1):2?20.
Biber, Douglas. 1988. Variation Across Speech and
Writing. Cambridge University Press, Cambridge.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Cohen, K. Bretonnel, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158.
Collins, Michael John. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of ACL-96, Santa Cruz, CA.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Curran, James, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&C
and Boxer. In Proceedings of the ACL-07 Demo and
Poster Sessions, Prague, Czech Republic.
Daume? III, Hal and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26:101?126.
Friedman, Carol, Pauline Kraa, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222?235.
Graff, David, Junbo Kong, Ke Chen, and Kazuaki
Maeda, 2005. English Gigaword Corpus, 2nd Edi-
tion. Linguistic Data Consortium.
696
Hara, Tadayoshi, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Pro-
ceedings of IJCNLP-05, Jeju Island, South Korea.
Jin, Yang, Ryan T. McDonald, Kevin Lerman, Mark A.
Mandel, Steven Carroll, Mark Y. Liberman, Fer-
nando C. Pereira, Raymond S. Winters, and Peter S.
White. 2006. Automated recognition of malignancy
mentions in biomedical literature. BMC Bioinfor-
matics, 7:492.
Kim, J.-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
GENIA corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl. 1):i180?
i182.
Korhonen, Anna, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceedings
of COLING-08, Manchester, UK.
Kulick, Seth, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In Proceedings of the HLT-NAACL-04
Workshop on Linking Biological Literature, Ontolo-
gies and Databases, Boston, MA.
Lappin, Shalom and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?561.
Nguyen, Ngan L.T. and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of a pronoun
resolution system for biomedical text. In Proceed-
ings of COLING-08, Manchester, UK.
NIH. 2009a. Journal publishing tag set.
http://dtd.nlm.nih.gov/publishing/.
NIH. 2009b. National library of
medicine: Journal subject terms.
http://wwwcf.nlm.nih.gov/serials/journals/index.cfm.
Preiss, Judita, E.J. Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of ACL-07, Prague, Czech
Republic.
Rayson, Paul and Roger Garside. 2000. Comparing
corpora using frequency profiling. In Proceedings
of the ACL-00 Workshop on Comparing Corpora,
Hong Kong.
Rimell, Laura and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Roland, Douglas and Daniel Jurafsky. 1998. How
verb subcategorization frequencies are affected by
corpus choice. In Proceedings of COLING-ACL-98,
Montreal, Canada.
Rosario, Barbara and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via
a domain-specific lexical hierarchy. In Proceedings
of EMNLP-01, Pittsburgh, PA.
Schuman, Jonathan and Sabine Bergler. 2006. Post-
nominal prepositional phrase attachment in pro-
teomics. In Proceedings of the HLT-NAACL-06
BioNLP Workshop on Linking Natural Language
and Biology, New York, NY.
Verspoor, Karin, K Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of tradi-
tional and Open Access scientific journals are simi-
lar. BMC Bioinformatics, 10:183.
697
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1002?1010,
Beijing, August 2010
Metaphor Identification Using Verb and Noun Clustering
Ekaterina Shutova, Lin Sun and Anna Korhonen
Computer Laboratory, University of Cambridge
es407,ls418,alk23@cam.ac.uk
Abstract
We present a novel approach to auto-
matic metaphor identification in unre-
stricted text. Starting from a small seed set
of manually annotated metaphorical ex-
pressions, the system is capable of har-
vesting a large number of metaphors of
similar syntactic structure from a corpus.
Our method is distinguished from previ-
ous work in that it does not employ any
hand-crafted knowledge, other than the
initial seed set, but, in contrast, captures
metaphoricity by means of verb and noun
clustering. Being the first to employ un-
supervised methods for metaphor identifi-
cation, our system operates with the pre-
cision of 0.79.
1 Introduction
Besides enriching our thought and communica-
tion with novel imagery, the phenomenon of
metaphor also plays a crucial structural role in our
use of language. Metaphors arise when one con-
cept is viewed in terms of the properties of the
other. Below are some examples of metaphor.
(1) How can I kill a process? (Martin, 1988)
(2) Inflation has eaten up all my savings. (Lakoff
and Johnson, 1980)
(3) He shot down all of my arguments. (Lakoff
and Johnson, 1980)
(4) And then my heart with pleasure fills,
And dances with the daffodils.1
In metaphorical expressions seemingly unrelated
features of one concept are associated with an-
other concept. In the computer science metaphor
1?I wandered lonely as a cloud?, William Wordsworth,
1804.
in (1) the computational process is viewed as
something alive and, therefore, its forced termi-
nation is associated with the act of killing. Lakoff
and Johnson (1980) explain metaphor as a system-
atic association, or a mapping, between two con-
cepts or conceptual domains: the source and the
target. The metaphor in (3) exemplifies a map-
ping of a concept of argument to that of war. The
argument, which is the target concept, is viewed
in terms of a battle (or a war), the source concept.
The existence of such a link allows us to talk about
arguments using the war terminology, thus giving
rise to a number of metaphors.
Characteristic to all areas of human activity
(from poetic to ordinary to scientific) and, thus,
to all types of discourse, metaphor becomes an
important problem for natural language process-
ing (NLP). In order to estimate the frequency of
the phenomenon, Shutova and Teufel (2010) con-
ducted a corpus study on a subset of the British
National Corpus (BNC) (Burnard, 2007) repre-
senting various genres. They manually anno-
tated metaphorical expressions in this data and
found that 241 out of 761 sentences contained a
metaphor, whereby in 164 phrases metaphoricity
was introduced by a verb. Due to such a high fre-
quency of their use, a system capable of recog-
nizing and interpreting metaphorical expressions
in unrestricted text would become an invaluable
component of any semantics-oriented NLP appli-
cation.
Automatic processing of metaphor can be
clearly divided into two subtasks: metaphor
identification (distinguishing between literal and
metaphorical language in text) and metaphor
interpretation (identifying the intended literal
meaning of a metaphorical expression). Both of
them have been repeatedly attempted in NLP.
To date the most influential account of
metaphor identification is that of Wilks (1978).
1002
According to Wilks, metaphors represent a viola-
tion of selectional restrictions in a given context.
Consider the following example.
(5) My car drinks gasoline. (Wilks, 1978)
The verb drink normally takes an animate subject
and a liquid object. Therefore, drink taking a car
as a subject is an anomaly, which may as well in-
dicate metaphorical use of drink.
This approach was automated by Fass (1991)
in his met* system. However, Fass himself in-
dicated a problem with the method: it detects
any kind of non-literalness or anomaly in lan-
guage (metaphors, metonymies and others), i.e.,
it overgenerates with respect to metaphor. The
techniques met* uses to differentiate between
those are mainly based on hand-coded knowledge,
which implies a number of limitations. In a sim-
ilar manner manually created knowledge in the
form of WordNet (Fellbaum, 1998) is employed
by the system of Krishnakumaran and Zhu (2007),
which essentially differentiates between highly
lexicalized metaphors included in WordNet, and
novel metaphorical senses.
Alternative approaches (Gedigan et al, 2006)
search for metaphors of a specific domain defined
a priori (e.g. MOTION metaphors) in a specific
type of discourse (e.g. Wall Street Journal). In
contrast, the scope of our experiments is the whole
of the British National Corpus (BNC) (Burnard,
2007) and the domain of the expressions we iden-
tify is unrestricted. However, our technique is also
distinguished from the systems of Fass (1991) and
Krishnakumaran and Zhu (2007) in that it does
not rely on any hand-crafted knowledge, but rather
captures metaphoricity in an unsupervised way by
means of verb and noun clustering.
The motivation behind the use of clustering
methods for metaphor identification task lies in
the nature of metaphorical reasoning based on as-
sociation. Compare, for example, the target con-
cepts of marriage and political regime. Having
quite distinct meanings, both of them are cogni-
tively mapped to the source domain of mecha-
nism, which shows itself in the following exam-
ples:
(6) Our relationship is not really working.
(7) Diana and Charles did not succeed in mend-
ing their marriage.
(8) The wheels of Stalin?s regime were well oiled
and already turning.
We expect that such relatedness of distinct tar-
get concepts should manifest itself in the exam-
ples of language use, i.e. target concepts that are
associated with the same source concept should
appear in similar lexico-syntactic environments.
Thus, clustering concepts using grammatical rela-
tions (GRs) and lexical features would allow us to
capture their relatedness by association and har-
vest a large number of metaphorical expressions
beyond our seed set. For example, the sentence
in (6) being part of the seed set should enable the
system to identify metaphors in both (7) and (8).
In summary, our system (1) starts from a seed
set of metaphorical expressions exemplifying a
range of source?target domain mappings; (2) per-
forms unsupervised noun clustering in order to
harvest various target concepts associated with the
same source domain; (3) by means of unsuper-
vised verb clustering creates a source domain verb
lexicon; (4) searches the BNC for metaphorical
expressions describing the target domain concepts
using the verbs from the source domain lexicon.
We tested our system starting with a collection
of metaphorical expressions representing verb-
subject and verb-object constructions, where the
verb is used metaphorically. We evaluated the pre-
cision of metaphor identification with the help of
human judges. In addition to this we compared
our system to a baseline built upon WordNet,
whereby we demonstrated that our method goes
far beyond synonymy and captures metaphors not
directly related to any of those seen in the seed set.
2 Experimental Data
2.1 Seed Phrases
We used the dataset of Shutova (2010) as a seed
set. Shutova (2010) annotated metaphorical ex-
pressions in a subset of the BNC sampling vari-
ous genres: literature, newspaper/journal articles,
essays on politics, international relations and his-
tory, radio broadcast (transcribed speech). The
dataset consists of 62 phrases that are single-word
1003
metaphors representing verb-subject and verb-
object relations, where a verb is used metaphor-
ically. The seed phrases include e.g. stir ex-
citement, reflect enthusiasm, accelerate change,
grasp theory, cast doubt, suppress memory, throw
remark (verb - direct object constructions) and
campaign surged, factor shaped [..], tension
mounted, ideology embraces, changes operated,
approach focuses, example illustrates (subject -
verb constructions).
2.2 Corpus
The search space for metaphor identification was
the British National Corpus (BNC) that was
parsed using the RASP parser of Briscoe et al
(2006). We used the grammatical relations out-
put of RASP for BNC created by Andersen et al
(2008). The system searched the corpus for the
source and target domain vocabulary within a par-
ticular grammatical relation (verb-object or verb-
subject).
3 Method
Starting from a small seed set of metaphorical ex-
pressions, the system implicitly captures the as-
sociations that underly their production and com-
prehension. It generalizes over these associations
by means of unsupervised verb and noun clus-
tering. The obtained clusters then represent po-
tential source and target concepts between which
metaphorical associations hold. The knowledge
of such associations is then used to annotate
metaphoricity in a large corpus.
3.1 Clustering Motivation
Abstract concepts that are associated with the
same source domain are often related to each
other on an intuitive and rather structural level,
but their meanings, however, are not necessarily
synonymous or even semantically close. The re-
sults of previous research on corpus-based lexi-
cal semantics suggest that the linguistic environ-
ment in which a lexical item occurs can shed light
on its meaning. A number of works have shown
that it is possible to automatically induce seman-
tic word classes from corpus data via clustering of
contextual cues (Pereira et al, 1993; Lin, 1998;
Schulte im Walde, 2006). The consensus is that
the lexical items exposing similar behavior in a
large body of text most likely have the same mean-
ing. However, the concepts of marriage and po-
litical regime, that are also observed in similar
lexico-syntactic environments, albeit having quite
distinct meanings are likewise assigned by such
methods to the same cluster. In contrast to con-
crete concepts, such as tea, water, coffee, beer,
drink, liquid, that are clustered together due to
meaning similarity, abstract concepts tend to be
clustered together by association with the same
source domain. It is the presence of this associ-
ation that explains the fact that they share com-
mon contexts. We exploit this idea for identifi-
cation of new target domains associated with the
same source domain. We then use unsupervised
verb clustering to collect source domain vocab-
ulary, which in turn allows us to harvest a large
number of new metaphorical expressions.
3.2 Verb and Noun Clustering
Since Levin (1993) published her classification,
there have been a number of attempts to automati-
cally classify verbs into semantic classes using su-
pervised and unsupervised approaches (Lin, 1998;
Brew and Schulte im Walde, 2002; Korhonen et
al., 2003; Schulte im Walde, 2006; Joanis et al,
2008; Sun and Korhonen, 2009). Similar methods
were also applied to acquisition of noun classes
from corpus data (Rooth et al, 1999; Pantel and
Lin, 2002; Bergsma et al, 2008).
We adopt a recent verb clustering approach of
Sun and Korhonen (2009), who used rich syntac-
tic and semantic features extracted using a shallow
parser and a clustering method suitable for the re-
sulting high dimensional feature space. When Sun
and Korhonen evaluated their approach on 204
verbs from 17 Levin classes, they obtained 80.4
F-measure (which is high in particular for an un-
supervised approach). We apply this approach to a
much larger set of 1610 verbs: all the verb forms
appearing in VerbNet (Kipper et al, 2006) with
the exception of highly infrequent ones. In addi-
tion, we adapt the approach to noun clustering.
3.2.1 Feature Extraction
Our verb dataset is a subset of VerbNet com-
piled as follows. For all the verbs in VerbNet we
1004
extracted their occurrences (up to 10,000) from
the raw corpus data collected originally by Korho-
nen et al (2006) for construction of VALEX lexi-
con. Only the verbs found in this data more than
150 times were included in the experiment.
For verb clustering, we adopted the best per-
forming features of Sun and Korhonen (2009):
automatically acquired verb subcategorization
frames (SCFs) parameterized by their selectional
preferences (SPs). We obtained these features us-
ing the SCF acquisition system of Preiss et al
(2007). The system tags and parses corpus data
using the RASP parser and extracts SCFs from the
resulting GRs using a rule-based classifier which
identifies 168 SCF types for English verbs. It pro-
duces a lexical entry for each verb and SCF com-
bination occurring in corpus data. We obtained
SPs by clustering argument heads appearing in the
subject and object slots of verbs in the resulting
lexicon.
Our noun dataset consists of 2000 most fre-
quent nouns in the BNC. Following previous
works on semantic noun classification (Pantel and
Lin, 2002; Bergsma et al, 2008), we used GRs as
features for noun clustering. We employed all the
argument heads and verb lemmas appearing in the
subject, direct object and indirect object relations
in the RASP-parsed BNC.
The feature vectors were first constructed from
the corpus counts, and subsequently normalized
by the sum of the feature values before applying
clustering.
3.2.2 Clustering Algorithm
We use spectral clustering (SPEC) for both
verbs and nouns. This technique has proved to be
effective in previous verb clustering works (Brew
and Schulte im Walde, 2002; Sun and Korhonen,
2009) and in related NLP tasks involving high di-
mensional data (Chen et al, 2006). We use the
MNCut algorithm for SPEC which has a wide ap-
plicability and a clear probabilistic interpretation
(Meila and Shi, 2001).
The task is to group a given set of words W =
{wn}Nn=1 into a disjoint partition of K classes.
SPEC takes a similarity matrix as input. We
construct it using the Jensen-Shannon divergence
(JSD) as a measure. The JSD between two feature
vectors w and w? is djsd(w, w?) = 12D(w||m) +1
2D(w?||m) where D is the Kullback-Leibler di-vergence, and m is the average of the w and w?.
The similarity matrix S is constructed where
Sij = exp(?djsd(w, w?)). In SPEC, the simi-
larities Sij are viewed as weights on the edges
ij of a graph G over W . The similarity matrix
S is thus the adjacency matrix for G. The de-
gree of a vertex i is di = ?Nj=1 Sij . A cut be-
tween two partitions A and A? is defined to be
Cut(A, A?) =?m?A,n?A? Smn.
The similarity matrix S is then transformed into
a stochastic matrix P .
P = D?1S (1)
The degree matrix D is a diagonal matrix where
Dii = di.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constants2 with respect to a partition I? and their
eigenvalues are not zero, then I? minimizes the
multiway normalized cut (MNCut):
MNCut(I) = K ??Kk=1 Cut(Ik,Ik)Cut(Ik,I)
Pmn can be interpreted as the transition probabil-
ity between the vertexes m, n. The criterion can
thus be expressed as MNCut(I) = ?Kk=1(1 ?
P (Ik ? Ik|Ik)) (Meila, 2001), which is the sum
of transition probabilities across different clusters.
This criterion finds the partition where random
walks are most likely to happen within the same
cluster. In practice, the leading eigenvectors of
P are not piecewise constants. However, we can
extract the partition by finding the approximately
equal elements in the eigenvectors using a cluster-
ing algorithm, such as K-Means.
Since SPEC has elements of randomness, we ran
the algorithm multiple times and the partition that
minimizes the distortion (the distances to cluster
centroid) is reported. Some of the clusters ob-
tained as a result of applying the algorithm to our
noun and verb datasets are demonstrated in Fig-
ures 1 and 2 respectively. The noun clusters rep-
resent target concepts that we expect to be asso-
ciated with the same source concept (some sug-
gested source concepts are given in Figure 1, al-
though the system only captures those implicitly).
2An eigenvector v is piecewise constant with respect to I
if v(i) = v(j)?i, j ? Ik and k ? 1, 2...K
1005
Source: MECHANISM
Target Cluster: consensus relation tradition partnership
resistance foundation alliance friendship contact reserve
unity link peace bond myth identity hierarchy relation-
ship connection balance marriage democracy defense
faith empire distinction coalition regime division
Source: STORY; JOURNEY
Target Cluster: politics practice trading reading occupa-
tion profession sport pursuit affair career thinking life
Source: LOCATION; CONTAINER
Target Cluster: lifetime quarter period century succes-
sion stage generation decade phase interval future
Source: LIVING BEING; END
Target Cluster: defeat fall death tragedy loss collapse de-
cline disaster destruction fate
Figure 1: Clustered target concepts
Source Cluster: sparkle glow widen flash flare gleam
darken narrow flicker shine blaze bulge
Source Cluster: gulp drain stir empty pour sip spill swal-
low drink pollute seep flow drip purify ooze pump bub-
ble splash ripple simmer boil tread
Source Cluster: polish clean scrape scrub soak
Source Cluster: kick hurl push fling throw pull drag haul
Source Cluster: rise fall shrink drop double fluctuate
dwindle decline plunge decrease soar tumble surge spiral
boom
Figure 2: Clustered verbs (source domains)
The verb clusters contain coherent lists of source
domain vocabulary.
3.3 Selectional Preference Strength Filter
Following Wilks (1978), we take metaphor to rep-
resent a violation of selectional restrictions. How-
ever, not all verbs have an equally strong capacity
to constrain their arguments, e.g. remember, ac-
cept, choose etc. are weak in that respect. We
suggest that for this reason not all the verbs would
be equally prone to metaphoricity, but only the
ones exhibiting strong selectional preferences. We
test this hypothesis experimentally and expect that
placing this criterion would enable us to filter out
a number of candidate expressions, that are less
likely to be used metaphorically.
We automatically acquired selectional pref-
erence distributions for Verb-Subject and
Verb-Object relations from the BNC parsed
by RASP. We first clustered 2000 most frequent
nouns in the BNC into 200 clusters using SPEC
as described in the previous section. The ob-
tained clusters formed our selectional preference
classes. We adopted the selectional preference
measure proposed by Resnik (1993) and success-
fully applied to a number of tasks in NLP includ-
ing word sense disambiguation (Resnik, 1997).
Resnik models selectional preference of a verb in
probabilistic terms as the difference between the
posterior distribution of noun classes in a partic-
ular relation with the verb and their prior distri-
bution in that syntactic position regardless of the
identity of the predicate. He quantifies this dif-
ference using the relative entropy (or Kullback-
Leibler distance), defining the selectional prefer-
ence strength (SPS) as follows.
SR(v) = D(P (c|v)||P (c)) =
?
c
P (c|v) log P (c|v)P (c) ,
(2)
where P (c) is the prior probability of the noun
class, P (c|v) is the posterior probability of the
noun class given the verb and R is the gram-
matical relation in question. SPS measures how
strongly the predicate constrains its arguments.
We use this measure to filter out the verbs with
weak selectional preferences. The optimal SPS
threshold was set experimentally on a small held-
out dataset and approximates to 1.32. We ex-
cluded expressions containing the verbs with pref-
erence strength below this threshold from the set
of candidate metaphors.
4 Evaluation and Discussion
In order to prove that our metaphor identification
method generalizes well over the seed set and goes
far beyond synonymy, we compared its output to
that of a baseline taking WordNet synsets to repre-
sent source and target domains. We evaluated the
quality of metaphor tagging in terms of precision
with the help of human judges.
4.1 Comparison against WordNet Baseline
The baseline system was implemented using syn-
onymy information from WordNet to expand on
the seed set. Assuming all the synonyms of the
verbs and nouns in seed expressions to represent
the source and target vocabularies respectively,
the system searches for phrases composed of lex-
ical items belonging to those vocabularies. For
example, given a seed expression stir excitement,
the baseline finds phrases such as arouse fervour,
1006
stimulate agitation, stir turmoil etc. However, it is
not able to generalize over the concepts to broad
semantic classes, e.g. it does not find other feel-
ings such as rage, fear, anger, pleasure etc., which
is necessary to fully characterize the target do-
main. The same deficiency of the baseline system
manifests itself in the source domain vocabulary:
the system has only the knowledge of direct syn-
onyms of stir, as opposed to other verbs charac-
teristic to the domain of liquids, e.g. pour, flow,
boil etc., successfully identified by means of clus-
tering.
To compare the coverage achieved by unsuper-
vised clustering to that of the baseline in quanti-
tative terms, we estimated the number of Word-
Net synsets, i.d. different word senses, in the
metaphorical expressions captured by the two sys-
tems. We found that the baseline system covers
only 13% of the data identified using clustering
and does not go beyond the concepts present in
the seed set. In contrast, most metaphors tagged
by our method are novel and represent a con-
siderably wider range of meanings, e.g. given
the seed metaphors stir excitement, throw remark,
cast doubt the system identifies previously unseen
expressions swallow anger, hurl comment, spark
enthusiasm etc. as metaphorical.
4.2 Comparison with Human Judgements
In order to access the quality of metaphor identifi-
cation by both systems we used the help of human
annotators. The annotators were presented with
a set of randomly sampled sentences containing
metaphorical expressions as annotated by the sys-
tem and by the baseline. They were asked to mark
the tagged expressions that were metaphorical in
their judgement as correct.
The annotators were encouraged to rely on their
own intuition of metaphor. However, we also pro-
vided some guidance in the form of the following
definition of metaphor3:
1. For each verb establish its meaning in con-
text and try to imagine a more basic meaning
of this verb on other contexts. Basic mean-
ings normally are: (1) more concrete; (2) re-
3taken from the annotation procedure of Shutova and
Teufel (2010) that is in turn partly based on the work of Prag-
glejaz Group (2007).
CKM 391 Time and time again he would stare at the
ground, hand on hip, if he thought he had received a bad
call, and then swallow his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when
he found the comms system down, but Tammuz was
nearly hysterical by this stage.
AMA 349 We will halt the reduction in NHS services
for long-term care and community health services which
support elderly and disabled patients at home.
ADK 634 Catch their interest and spark their enthu-
siasm so that they begin to see the product?s potential.
K2W 1771 The committee heard today that gangs regu-
larly hurled abusive comments at local people, making
an unacceptable level of noise and leaving litter behind
them.
Figure 3: Sentences tagged by the system
(metaphors in bold)
lated to bodily action; (3) more precise (as
opposed to vague); (4) historically older.
2. If you can establish the basic meaning that
is distinct from the meaning of the verb in
this context, the verb is likely to be used
metaphorically.
We had 5 volunteer annotators who were all na-
tive speakers of English and had no or sparse lin-
guistic knowledge. Their agreement on the task
was 0.63 in terms of ? (Siegel and Castellan,
1988), whereby the main source of disagreement
was the presence of highly lexicalized metaphors,
e.g. verbs such as adopt, convey, decline etc.
We then evaluated the system performance against
their judgements in terms of precision. Precision
measures the proportion of metaphorical expres-
sions that were tagged correctly among the ones
that were tagged. We considered the expressions
tagged as metaphorical by at least three annota-
tors to be correct. As a result our system identi-
fies metaphor with the precision of 0.79, whereas
the baseline only attains 0.44. Some examples of
sentences annotated by the system are shown in
Figure 3.
Such a striking discrepancy between the per-
formance levels of the clustering approach and
the baseline can be explained by the fact that a
large number of metaphorical senses are included
in WordNet. This means that in WordNet synsets
source domain verbs are mixed with more abstract
terms. For example, the metaphorical sense of
shape in shape opinion is part of the synset (de-
1007
termine, shape, mold, influence, regulate). This
results in the baseline system tagging literal ex-
pressions as metaphorical, erroneously assuming
that the verbs from the synset belong to the source
domain.
The main source of confusion in the output of
our clustering method was the conventionality of
some metaphorical expressions, e.g. hold views,
adopt traditions, tackle a problem. The system
is capable of tracing metaphorical etymology of
conventional phrases, but their senses are highly
lexicalized. This lexicalization is reflected in the
data and affects clustering in that conventional
metaphors are sometimes clustered together with
literally used terms, e.g. tackle a problem and re-
solve a problem, which may suggest that the lat-
ter are metaphorical. It should be noted, however,
that such errors are rare.
Since there is no large metaphor-annotated cor-
pus available, it was impossible for us to reli-
ably evaluate the recall of the system. How-
ever, the system identified a total number of 4456
metaphorical expressions in the BNC starting with
a seed set of only 62, which is a promising result.
5 Related Work
One of the first attempts to identify and inter-
pret metaphorical expressions in text automati-
cally is the approach of Fass (1991). Fass devel-
oped a system called met*, capable of discrimi-
nating between literalness, metonymy, metaphor
and anomaly. It does this in three stages. First,
literalness is distinguished from non-literalness
using selectional preference violation as an in-
dicator. In the case that non-literalness is de-
tected, the respective phrase is tested for be-
ing a metonymic relation using hand-coded pat-
terns (such as CONTAINER-for-CONTENT). If
the system fails to recognize metonymy, it pro-
ceeds to search the knowledge base for a rele-
vant analogy in order to discriminate metaphor-
ical relations from anomalous ones. E.g., the
sentence in (5) would be represented in this
framework as (car,drink,gasoline), which does
not satisfy the preference (animal,drink,liquid),
as car is not a hyponym of animal. met*
then searches its knowledge base for a triple
containing a hypernym of both the actual ar-
gument and the desired argument and finds
(thing,use,energy source), which represents the
metaphorical interpretation.
Birke and Sarkar (2006) present a sen-
tence clustering approach for non-literal lan-
guage recognition implemented in the TroFi sys-
tem (Trope Finder). This idea originates from
a similarity-based word sense disambiguation
method developed by Karov and Edelman (1998).
The method employs a set of seed sentences,
where the senses are annotated, computes simi-
larity between the sentence containing the word
to be disambiguated and all of the seed sentences
and selects the sense corresponding to the anno-
tation in the most similar seed sentences. Birke
and Sarkar (2006) adapt this algorithm to perform
a two-way classification: literal vs. non-literal,
and they do not clearly define the kinds of tropes
they aim to discover. They attain a performance
of 53.8% in terms of f-score.
The method of Gedigan et al (2006) discrimi-
nates between literal and metaphorical use. They
trained a maximum entropy classifier for this pur-
pose. They obtained their data by extracting the
lexical items whose frames are related to MO-
TION and CURE from FrameNet (Fillmore et al,
2003). Then they searched the PropBank Wall
Street Journal corpus (Kingsbury and Palmer,
2002) for sentences containing such lexical items
and annotated them with respect to metaphoric-
ity. They used PropBank annotation (arguments
and their semantic types) as features to train the
classifier and report an accuracy of 95.12%. This
result is, however, only a little higher than the per-
formance of the naive baseline assigning majority
class to all instances (92.90%). These numbers
can be explained by the fact that 92.00% of the
verbs of MOTION and CURE in the Wall Street
Journal corpus are used metaphorically, thus mak-
ing the dataset unbalanced with respect to the tar-
get categories and the task notably easier.
Both Birke and Sarkar (2006) and Gedigan et
al. (2006) focus only on metaphors expressed by
a verb. As opposed to that the approach of Kr-
ishnakumaran and Zhu (2007) deals with verbs,
nouns and adjectives as parts of speech. They
use hyponymy relation in WordNet and word bi-
gram counts to predict metaphors at the sentence
1008
level. Given an IS-A metaphor (e.g. The world is
a stage4) they verify if the two nouns involved are
in hyponymy relation in WordNet, and if this is
not the case then this sentence is tagged as con-
taining a metaphor. Along with this they con-
sider expressions containing a verb or an adjec-
tive used metaphorically (e.g. He planted good
ideas in their minds or He has a fertile imagi-
nation). Hereby they calculate bigram probabil-
ities of verb-noun and adjective-noun pairs (in-
cluding the hyponyms/hypernyms of the noun in
question). If the combination is not observed in
the data with sufficient frequency, the system tags
the sentence containing it as metaphorical. This
idea is a modification of the selectional prefer-
ence view of Wilks. However, by using bigram
counts over verb-noun pairs as opposed to verb-
object relations extracted from parsed text Kr-
ishnakumaran and Zhu (2007) loose a great deal
of information. The authors evaluated their sys-
tem on a set of example sentences compiled from
the Master Metaphor List (Lakoff et al, 1991),
whereby highly conventionalized metaphors (they
call them dead metaphors) are taken to be neg-
ative examples. Thus, they do not deal with lit-
eral examples as such: essentially, the distinc-
tion they are making is between the senses in-
cluded in WordNet, even if they are conventional
metaphors, and those not included in WordNet.
6 Conclusions and Future Directions
We presented a novel approach to metaphor iden-
tification in unrestricted text using unsupervised
methods. Starting from a limited set of metaphor-
ical seeds, the system is capable of capturing the
regularities behind their production and annotat-
ing a much greater number and wider range of
previously unseen metaphors in the BNC.
Our system is the first of its kind and it is capa-
ble of identifying metaphorical expressions with a
high precision (0.79). By comparing its coverage
to that of a WordNet baseline, we proved that our
method goes far beyond synonymy and general-
izes well over the source and target domains. Al-
though at this stage we tested our system on verb-
subject and verb-object metaphors only, we are
4William Shakespeare
convinced that the described identification tech-
niques can be similarly applied to a wider range
of syntactic constructions. Extending the system
to deal with more parts of speech and types of
phrases is part of our future work.
One possible limitation of our approach is that
it is seed-dependent, which makes the recall of the
system questionable. Thus, another important fu-
ture research avenue is the creation of a more di-
verse seed set. We expect that a set of expres-
sions representative of the whole variety of com-
mon metaphorical mappings, already described in
linguistics literature, would enable the system to
attain a very broad coverage of the corpus. Mas-
ter Metaphor List (Lakoff et al, 1991) and other
existing metaphor resources could be a sensible
starting point on a route to such a dataset.
Acknowledgments
We are very grateful to our anonymous reviewers
for their useful feedback on this work and the vol-
unteer annotators for their interest, time and help.
This research is funded by generosity of Cam-
bridge Overseas Trust (Katia Shutova), Dorothy
Hodgkin Postgraduate Award (Lin Sun) and the
Royal Society, UK (Anna Korhonen).
References
Andersen, O. E., J. Nioche, E. Briscoe, and J. Carroll.
2008. The BNC parsed with RASP4UIMA. In Pro-
ceedings of LREC 2008, Marrakech, Morocco.
Bergsma, S., D. Lin, and R. Goebel. 2008. Discrimi-
native learning of selectional preference from unla-
beled text. In Proceedings of the EMNLP.
Birke, J. and A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonlit-
eral language. In In Proceedings of EACL-06, pages
329?336.
Brew, C. and S. Schulte im Walde. 2002. Spectral
clustering for German verbs. In Proceedings of
EMNLP.
Briscoe, E., J. Carroll, and R. Watson. 2006. The sec-
ond release of the rasp system. In Proceedings of
the COLING/ACL on Interactive presentation ses-
sions, pages 77?80.
Burnard, L. 2007. Reference Guide for the British
National Corpus (XML Edition).
1009
Chen, J., D. Ji, C. Lim Tan, and Z. Niu. 2006. Un- Lin, D. 1998. Automatic retrieval and clustering of
supervised relation disambiguation using spectral similar words. In Proceedings of the 17th inter-
clustering. In Proceedings of COLING/ACL. national conference on Computational linguistics,
pages 768?774.Fass, D. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa- Martin, J. H. 1988. Representing regularities in the
tional Linguistics, 17(1):49?90. metaphoric lexicon. In Proceedings of the 12th con-
ference on Computational linguistics, pages 396?Fellbaum, C., editor. 1998. WordNet: An Electronic 401.Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition. Meila, M. and J. Shi. 2001. A random walks view of
spectral segmentation. In AISTATS.Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International Meila, M. 2001. The multicut lemma. Technical re-
Journal of Lexicography, 16(3):235?250. port, University of Washington.
Gedigan, M., J. Bryant, S. Narayanan, and B. Ciric. Pantel, P. and D. Lin. 2002. Discovering word
2006. Catching metaphors. In In Proceedings of the senses from text. In Proceedings of the eighth ACM
3rd Workshop on Scalable Natural Language Un- SIGKDD international conference on Knowledge
derstanding, pages 41?48, New York. discovery and data mining, pages 613?619. ACM.
Joanis, E., S. Stevenson, and D. James. 2008. A gen- Pereira, F., N. Tishby, and L. Lee. 1993. Distribu-
eral feature space for automatic verb classification. tional clustering of English words. In Proceedings
Natural Language Engineering, 14(3):337?367. of ACL-93, pages 183?190, Morristown, NJ, USA.
Karov, Y. and S. Edelman. 1998. Similarity-based Pragglejaz Group. 2007. MIP: A method for iden-
word sense disambiguation. Computational Lin- tifying metaphorically used words in discourse.
guistics, 24(1):41?59. Metaphor and Symbol, 22:1?39.
Kingsbury, P. and M. Palmer. 2002. From TreeBank Preiss, J., T. Briscoe, and A. Korhonen. 2007. A sys-
to PropBank. In Proceedings of LREC-2002, Gran tem for large-scale acquisition of verbal, nominal
Canaria, Canary Islands, Spain. and adjectival subcategorization frames from cor-
pora. In Proceedings of ACL-2007, volume 45, pageKipper, K., A. Korhonen, N. Ryant, and M. Palmer. 912.2006. Extensive classifications of English verbs.
In Proceedings of the 12th EURALEX International Resnik, P. 1993. Selection and Information: A Class-
Congress. based Approach to Lexical Relationships. Ph.D.
thesis, Philadelphia, PA, USA.Korhonen, A., Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame dis- Resnik, P. 1997. Selectional preference and sense dis-
tributions semantically. In Proceedings of ACL ambiguation. In ACL SIGLEX Workshop on Tag-
2003, Sapporo,Japan. ging Text with Lexical Semantics, Washington, D.C.
Korhonen, A., Y. Krymolowski, and T. Briscoe. 2006. Rooth, M., S. Riezler, D. Prescher, G. Carroll, and
A large subcategorization lexicon for natural lan- F. Beil. 1999. Inducing a semantically annotated
guage processing applications. In Proceedings of lexicon via EM-based clustering. In Proceedings of
LREC 2006. ACL 99, pages 104?111.
Krishnakumaran, S. and X. Zhu. 2007. Hunting elu- Schulte im Walde, S. 2006. Experiments on the au-
sive metaphors using lexical resources. In Proceed- tomatic induction of German semantic verb classes.
ings of the Workshop on Computational Approaches Computational Linguistics, 32(2):159?194.
to Figurative Language, pages 13?20, Rochester,
NY. Shutova, E. and S. Teufel. 2010. Metaphor corpusannotated for source - target domain mappings. In
Lakoff, G. and M. Johnson. 1980. Metaphors We Live Proceedings of LREC 2010, Malta.
By. University of Chicago Press, Chicago. Shutova, E. 2010. Automatic metaphor interpretation
Lakoff, G., J. Espenson, and A. Schwartz. 1991. The as a paraphrasing task. In Proceedings of NAACL
master metaphor list. Technical report, University 2010, Los Angeles, USA.
of California at Berkeley. Siegel, S. and N. J. Castellan. 1988. Nonparametric
Levin, B. 1993. English Verb Classes and Alterna- statistics for the behavioral sciences. McGraw-Hill
tions. University of Chicago Press, Chicago. Book Company, New York, USA.
1010
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1056?1064,
Beijing, August 2010
Investigating the cross-linguistic potential of VerbNet -style classification
Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
ls418,alk23@cl.cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS
thierry.poibeau@ens.fr
Ce?dric Messiant
LIPN, UMR7030
CNRS & U. Paris 13
cedric.messiant@lipn.fr
Abstract
Verb classes which integrate a wide range
of linguistic properties (Levin, 1993) have
proved useful for natural language pro-
cessing (NLP) applications. However,
the real-world use of these classes has
been limited because for most languages,
no resources similar to VerbNet (Kipper-
Schuler, 2005) are available. We apply
a verb clustering approach developed for
English to French ? a language for which
no such experiment has been conducted
yet. Our investigation shows that not only
the general methodology but also the best
performing features are transferable be-
tween the languages, making it possible
to learn useful VerbNet style classes for
French automatically without language-
specific tuning.
1 Introduction
A number of verb classifications have been built to
support natural language processing (NLP) tasks
(Grishman et al, 1994; Miller, 1995; Baker et al,
1998; Palmer et al, 2005; Kipper-Schuler, 2005;
Hovy et al, 2006). These include both syntactic
and semantic classifications, as well as ones which
integrate aspects of both. Classifications which in-
tegrate a wide range of linguistic properties can
be particularly useful for NLP applications suffer-
ing from data sparseness. One such classification
is VerbNet (Kipper-Schuler, 2005). Building on
the taxonomy of Levin (1993), VerbNet groups
verbs (e.g. deliver, post, dispatch) into classes
(e.g. SEND) on the basis of their shared mean-
ing components and syntactic behaviour, identi-
fied in terms of meaning preserving diathesis al-
ternations. Such classes can be identified across
the entire lexicon, and they may also apply across
languages, since their meaning components are
said to be cross-linguistically applicable (Jack-
endoff, 1990).
Offering a powerful tool for generalization, ab-
straction and prediction, VerbNet classes have
been used to support many important NLP
tasks, including e.g. computational lexicography,
parsing, word sense disambiguation, semantic
role labeling, information extraction, question-
answering, and machine translation (Swier and
Stevenson, 2004; Dang, 2004; Shi and Mihalcea,
2005; Abend et al, 2008). However, to date their
exploitation has been limited because for most
languages, no Levin style classification is avail-
able.
Since manual classification is costly (Kipper
et al, 2008) automatic approaches have been pro-
posed recently which could be used to learn novel
classifications in a cost-effective manner (Joanis
et al, 2008; Li and Brew, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009; Sun
and Korhonen, 2009). However, most work on
Levin type classification has focussed on English.
Large-scale research on other languages such as
German (Schulte im Walde, 2006) and Japanese
(Suzuki and Fukumoto, 2009) has focussed on se-
mantic classification. Although the two classifica-
tion systems have shared properties, studies com-
paring the overlap between VerbNet and WordNet
(Miller, 1995) have reported that the mapping is
only partial and many to many due to fine-grained
nature of classes based on synonymy (Shi and Mi-
halcea, 2005; Abend et al, 2008).
Only few studies have been conducted on Levin
style classification for languages other than En-
glish. In their experiment involving 59 verbs and
three classes, Merlo et al (2002) applied a su-
pervised approach developed for English to Ital-
ian, obtaining high accuracy (86.3%). In an-
other experiment with 60 verbs and three classes,
1056
they showed that features extracted from Chinese
translations of English verbs can improve English
classification. These results are promising, but
those from a later experiment by Ferrer (2004)
are not. Ferrer applied a clustering approach de-
veloped for English to Spanish, and evaluated it
against the manual classification of Va?zquez et al
(2000), constructed using criteria similar (but not
identical) to Levin?s. This experiment involving
514 verbs and 31 classes produced results only
slightly better than the random baseline.
In this paper, we investigate the cross-linguistic
potential of Levin style classification further. In
past years, verb classification techniques ? in par-
ticular unsupervised ones ? have improved con-
siderably, making investigations for a new lan-
guage more feasible. We take a recent verb clus-
tering approach developed for English (Sun and
Korhonen, 2009) and apply it to French ? a ma-
jor language for which no such experiment has
been conducted yet. Basic NLP resources (cor-
pora, taggers, parsers and subcategorization ac-
quisition systems) are now sufficiently developed
for this language for the application of a state-of-
the-art verb clustering approach to be realistic.
Our investigation reveals similarities between
the English and French classifications, support-
ing the linguistic hypothesis (Jackendoff, 1990)
and the earlier result of Merlo et al (2002)
that Levin classes have a strong cross-linguistic
basis. Not only the general methodology but
also best performing features are transferable be-
tween the languages, making it possible to learn
useful classes for French automatically without
language-specific tuning.
2 French Gold Standard
The development of an automatic verb classifi-
cation approach requires at least an initial gold
standard. Some syntactic (Gross, 1975) and se-
mantic (Vossen, 1998) verb classifications exist
for French, along with ones which integrate as-
pects of both (Saint-Dizier, 1998). Since none of
these resources offer classes similar to Levins?,
we followed the idea of Merlo et al (2002) and
translated a number of Levin classes from English
to French. As our aim was to to investigate the
cross-linguistic applicability of classes, we took
an English gold standard which has been used to
evaluate several recent clustering works ? that of
Sun et al (2008). This resource includes 17 fine-
grained Levin classes. Each class has 12 member
verbs whose predominant sense in English (ac-
cording to WordNet) belongs to that class.
Member verbs were first translated to French.
Where several relevant translations were identi-
fied, each of them was considered. For each can-
didate verb, subcategorization frames (SCFs) were
identified and diathesis alternations were consid-
ered using the criteria of Levin (1993): alterna-
tions must result in the same or extended verb
sense. Only verbs sharing diathesis alternations
were kept in the class.
For example, the gold standard class 31.1
AMUSE includes the following English verbs:
stimulate, threaten, shock, confuse, upset, over-
whelm, scare, disappoint, delight, exhaust, in-
timidate and frighten. Relevant French transla-
tions were identified for all of them: abattre,
accabler, briser, de?primer, consterner, ane?antir,
e?puiser, exte?nuer, e?craser, ennuyer, e?reinter, inon-
der. The majority of these verbs take similar SCFs
and diathesis alternations, e.g. Cette affaire e?crase
Marie (de chagrin), Marie est e?crase?e par le cha-
grin, Le chagrin e?crase Marie. However, stim-
uler (stimulate) and menacer (threaten) do not,
and they were therefore removed.
40% of translations were discarded from
classes because they did not share the same aler-
nations. The final version of the gold stan-
dard (shown in table 1) includes 171 verbs in 16
classes. Each class is named according to the
original Levin class. The smallest class (30.3) in-
cludes 7 verbs and the largest (37.3) 16. The aver-
age number of verbs per class is 10.7.
3 Verb Clustering
We performed an experiment where we
? took a French corpus and a SCF lexicon au-
tomatically extracted from that corpus,
? extracted from these resources a range of fea-
tures (lexical, syntactic and semantic) ? a
representative sample of those employed in
recent English experiments,
1057
Class No Class Verbs
9.1 PUT accrocher, de?poser, mettre, placer, re?partir, re?inte?grer, empiler, emporter, enfermer,
inse?rer, installer
10.1 REMOVE o?ter, enlever, retirer, supprimer, retrancher, de?barrasser, soustraire, de?compter, e?liminer
11.1 SEND envoyer, lancer, transmettre, adresser, porter, expe?dier, transporter, jeter, renvoyer, livrer
13.5.1 GET acheter, prendre, saisir, re?server, conserver, garder, pre?server, maintenir, retenir, louer,
affre?ter
18.1 HIT cogner, heurter, battre, frapper, fouetter, taper, rosser, brutaliser, e?reinter, maltraiter,
corriger,
22.2 AMALGAMATE incorporer, associer, re?unir, me?langer, me?ler, unir, assembler, combiner, lier, fusionner
29.2 CHARACTERIZE appre?hender, concevoir, conside?rer, de?crire, de?finir, de?peindre, de?signer, envisager,
identifier, montrer, percevoir, repre?senter, ressentir
30.3 PEER regarder, e?couter, examiner, conside?rer, voir, scruter, de?visager
31.1 AMUSE abattre, accabler, briser, de?primer, consterner, ane?antir, e?puiser, exte?nuer, e?craser, en-
nuyer, e?reinter, inonder,
36.1 CORRESPOND coope?rer, participer, collaborer, concourir, contribuer, prendre part, s?associer, travaille
37.3 MANNER OF
SPEAKING
ra?ler, gronder, crier, ronchonner, grogner, bougonner, maugre?er, rouspe?ter, grommeler,
larmoyer, ge?mir, geindre, hurler, gueuler, brailler, chuchoter
37.7 SAY dire, re?ve?ler, de?clarer, signaler, indiquer, montrer, annoncer, re?pondre, affirmer, certifier,
re?pliquer
43.1 LIGHT EMIS-
SION
briller, e?tinceler, flamboyer, luire, resplendir, pe?tiller, rutiler, rayonner., scintiller
45.4 CHANGE OF
STATE
me?langer, fusionner, consolider, renforcer, fortifier, adoucir, polir, atte?nuer, tempe?rer,
pe?trir, fac?onner, former
47.3 MODES OF BE-
ING
trembler, fre?mir, osciller, vaciller, vibrer, tressaillir, frissonner, palpiter, gre?siller, trem-
bloter, palpiter
51.3.2 RUN voyager, aller, se promener, errer, circuler, se de?placer, courir, bouger, naviguer, passer
Table 1: A Levin style gold standard for French
? clustered the features using a method which
has proved promising in both English and
German experiments: spectral clustering,
? evaluated the clusters both quantitatively (us-
ing the gold standard) and qualitatively,
? and compared the performance to that re-
cently obtained for English in order to gain
a better understanding of the cross-linguistic
and language-specific properties of verb clas-
sification
This work is described in the subsequent sections.
3.1 Data: the LexSchem Lexicon
We extracted the features for clustering from
LexSchem (Messiant et al, 2008). This large sub-
categorization lexicon provides SCF frequency in-
formation for 3,297 French verbs. It was acquired
fully automatically from Le Monde newspaper
corpus (200M words from years 1991-2000) us-
ing ASSCI ? a recent subcategorization acquisi-
tion system for French (Messiant, 2008). Systems
similar to ASSCI have been used in recent verb
classification works e.g. (Schulte im Walde, 2006;
Li and Brew, 2008; Sun and Korhonen, 2009).
Like these other systems, ASSCI takes raw corpus
data as input. The data is first tagged and lemma-
tized using the Tree-Tagger and then parsed us-
ing Syntex (Bourigault et al, 2005). Syntex is
a shallow parser which employs a combination
of statistics and heuristics to identify grammati-
cal relations (GRs) in sentences. ASSCI considers
GRs where the target verbs occur and constructs
SCFs from nominal, prepositional and adjectival
phrases, and infinitival and subordinate clauses.
When a verb has no dependency, its SCF is con-
sidered as intransitive. ASSCI assumes no pre-
defined list of SCFs but almost any combination
of permitted constructions can appear as a candi-
date SCF. The number of automatically generated
SCF types in LexSchem is 336.
Many candidate SCFs are noisy due to process-
ing errors and the difficulty of argument-adjunct
distinction. Most SCF systems assume that true
arguments occur in argument positions more fre-
quently than adjuncts. Many systems also inte-
grate filters for removing noise from system out-
put. When LexSchem was evaluated after filter-
1058
ing its F-measure was 69 ? which is similar to
that of other current SCF systems (Messiant et al,
2008) We used the unfiltered version of the lexi-
con because English experiments have shown that
information about adjuncts can help verb cluster-
ing (Sun et al, 2008).
4 Features
Lexical entries in LexSchem provide a variety of
material for verb clustering. Using this material,
we constructed a range of features for experimen-
tation. The first three include basic information
about SCFs:
F1: SCFs and their relative frequencies with indi-
vidual verbs. SCFs abstract over particles and
prepositions.
F2: F1, with SCFs parameterized for the tense
(the POS tag) of the verb.
F3: F2, with SCFs parameterized for prepositions
(PP).
The following six features include informa-
tion about the lexical context (co-occurrences)
of verbs. We adopt the best method of Li and
Brew (2008) where collocations (COs) are ex-
tracted from the window of words immediately
preceding and following a lemmatized verb. Stop
words are removed prior to extraction.
F4, F6, F8: COs are extracted from the window
of 4, 6 and 8 words, respectively. The relative
word position is ignored.
F5, F7, F9: F4, F6 and F8 with the relative word
position recorded.
The next four features include information
about lexical preferences (LP) of verbs in argu-
ment head positions of specific GRs associated
with the verb:
F10: LP(PREP): the type and frequency of prepo-
sitions in the preposition (PREP) relation.
F11: LP(SUBJ): the type and frequency of nouns
in the subject (SUBJ) relation.
F12: LP(IOBJ): the type and frequency of nouns
in the object (OBJ) and indirect object (IOBJ)
relation.
F13: LP(ALL): the combination of F10-F13.
The final two features refine SCF features with
LPs and semantic information about verb selec-
tional preferences (SP):
F14-F16: F1-F3 parameterized for LPs.
F17: F3 refined with SPs.
We adopt a fully unsupervised approach to SP
acquisition using the method of Sun and Korho-
nen (2009), with the difference that we determine
the optimal number of SP clusters automatically
following Zelnik-Manor and Perona (2004). The
method is introduced in the following section. The
approach involves (i) taking the GRs (SUBJ, OBJ,
IOBJ) associated with verbs, (ii) extracting all the
argument heads in these GRs, and (iii) clustering
the resulting N most frequent argument heads into
M classes. The empirically determined N 200
was used. The method produced 40 SP clusters.
5 Clustering Methods
Spectral clustering (SPEC) has proved promising
in previous verb clustering experiments (Brew
and Schulte im Walde, 2002; Sun and Korho-
nen, 2009) and other similar NLP tasks involv-
ing high dimensional feature space (Chen et al,
2006). Following Sun and Korhonen (2009) we
used the MNCut spectral clustering (Meila and
Shi, 2001) which has a wide applicability and
a clear probabilistic interpretation (von Luxburg,
2007; Verma and Meila, 2005). However, we ex-
tended the method to determine the optimal num-
ber of clusters automatically using the technique
proposed by (Zelnik-Manor and Perona, 2004).
Clustering groups a given set of verbs V =
{vn}Nn=1 into a disjoint partition of K classes.
SPEC takes a similarity matrix as input. All our
features can be viewed as probabilistic distribu-
tions because the combination of different fea-
tures is performed via parameterization. Thus we
use the Jensen-Shannon divergence (JSD) to con-
struct the similarity matrix. The JSD between
1059
two feature vectors v and v? is djsd(v, v?) =
1
2D(v||m)+ 12D(v?||m) where D is the Kullback-Leibler divergence, and m is the average of the v
and v?.
The similarity matrix W is constructed where
Wij = exp(?djsd(v, v?)). In SPEC, the simi-
larities Wij are viewed as the connection weight
ij of a graph G over V . The similarity matrix
W is thus the adjacency matrix for G. The de-
gree of a vertex i is di = ?Nj=1 wij . A cut be-
tween two partitions A and A? is defined to be
Cut(A,A?) =?m?A,n?A? Wmn.
The similarity matrix W is normalized into a
stochastic matrix P .
P = D?1W (1)
The degree matrix D is a diagonal matrix where
Dii = di.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constant1 with respect to a partition I? and their
eigenvalues are not zero, then I? minimizes the
multiway normalized cut(MNCut):
MNCut(I) = K ??Kk=1 Cut(Ik,Ik)Cut(Ik,I)
Pmn can be interpreted as the transition proba-
bility between vertices m,n. The criterion can
thus be expressed as MNCut(I) = ?Kk=1(1 ?
P (Ik ? Ik|Ik)) (Meila, 2001), which is the sum
of transition probabilities across different clusters.
This criterion finds the partition where the random
walks are most likely to happen within the same
cluster. In practice, the leading eigenvectors of P
are not piecewise constant. But we can extract the
partition by finding the approximately equal ele-
ments in the eigenvectors using a clustering algo-
rithm like K-Means.
As the value of K is not known beforehand, we
use Zelnik-Manor and Perona (2004)?s method to
estimate it. This method finds the optimal value
by minimizing a cost function based on the eigen-
vector structure of W .
Like Brew and Schulte im Walde (2002), we
compare SPEC against a K-Means baseline. We
used the Matlab implementation with euclidean
distance as the distance measure.
1The eigenvector v is piecewise constant with respect to
I if v(i) = v(j)?i, j ? Ik and k ? 1, 2...K
6 Experimental Evaluation
6.1 Data and Pre-processing
The SCF-based features (F1-F3 and F14-F17)
were extracted directly from LexSchem. The CO
(F4-F9) and LP features (F10-F13) were extracted
from the raw and parsed corpus sentences, respec-
tively, which were used for creating the lexicon.
Features that only appeared once were removed.
Feature vectors were normalized by the sum of the
feature values before clustering. Since our clus-
tering algorithms have an element of randomness,
we repeated clustering multiple times. We report
the results that minimize the distortion (the dis-
tance to cluster centroid).
6.2 Evaluation Measures
We employ the same measures for evaluation as
previously employed e.g. by O? Se?aghdha and
Copestake (2008) and Sun and Korhonen (2009).
The first measure is modified purity (mPUR) ?
a global measure which evaluates the mean preci-
sion of clusters. Each cluster is associated with its
prevalent class. The number of verbs in a cluster
K that take this class is denoted by nprevalent(K).
Verbs that do not take it are considered as errors.
Clusters where nprevalent(K) = 1 are disregarded
as not to introduce a bias towards singletons:
mPUR =
?
nprevalent(ki)>2
nprevalent(ki)
number of verbs
The second measure is weighted class accuracy
(ACC): the proportion of members of dominant
clusters DOM-CLUSTi within all classes ci.
ACC =
?C
i=1 verbs in DOM-CLUSTi
number of verbs
mPUR and ACC can be seen as a measure of pre-
cision(P) and recall(R) respectively. We calculate
F measure as the harmonic mean of P and R:
F = 2 ? mPUR ? ACCmPUR + ACC
The random baseline (BL) is calculated as fol-
lows: BL = 1/number of classes
7 Evaluation
7.1 Quantitative Evaluation
In our first experiment, we evaluated 116 verbs ?
those which appeared in LexSchem the minimum
1060
of 150 times. We did this because English exper-
iments had shown that due to the Zipfian nature
of SCF distributions, 150 corpus occurrences are
typically needed to obtain a sufficient number of
frames for clustering (Sun et al, 2008).
Table 2 shows F-measure results for all the fea-
tures. The 4th column of the table shows, for com-
parison, the results of Sun and Korhonen (2009)
obtained for English when they used the same fea-
tures as us, clustered them using SPEC, and evalu-
ated them against the English version of our gold
standard, also using F-measure2.
As expected, SPEC (the 2nd column) outper-
forms K-Means (the 3rd column). Looking at the
basic SCF features F1-F3, we can see that they per-
form significantly better than the BL method. F3
performs the best among the three features both
in French (50.6 F) and in English (63.3 F). We
therefore use F3 as the SCF feature in F14-F17
(the same was done for English).
In French, most CO features (F4-F9) outper-
form SCF features. The best result is obtained
with F7: 55.1 F. This is clearly better than the
best SCF result 50.6 (F3). This result is interesting
since SCFs correspond better than COs with fea-
tures used in manual Levin classification. Also,
SCFs perform considerably better than COs in the
English experiment (we only have the result for F4
available, but it is considerably lower than the re-
sult for F3). However, earlier English studies have
reported contradictory results (e.g. Li and Brew
(2008) showed that CO performs better than SCF
in supervised verb classification), indicating that
the role of CO features in verb classification re-
quires further investigation.
Looking at the LP features, F13 produces the
best F (52.7) for French which is slightly better
than the best SCF result for the language. Also
in English, F13 performs the best in this feature
group and yields a higher result than the best SCF-
based feature F3.
Parameterizing the best SCF feature F3 with LPs
(F14-16) and SPs (F17) yields better performance
2Note that the results for the two languages are not mu-
tually comparable due to differences in test sets, data sizes,
and feature extraction systems (see Section 8 for discussion).
The results for English are included so that we can compare
the relative performance of individual features in the two lan-
guages in question.
in French. F15 and F17 have the F of 54.5 and
54.6, respectively. These results are so close to
the result of the best CO feature F7 (55.1 ? which
is the highest result in this experiment) that the
differences are not statistically significant. In En-
glish, the results of F14-F17 are similarly good;
however, only F17 beats the already high perfor-
mance of F13.
On the basis of this experiment, it is difficult to
tell whether shallow CO features or more sophisti-
cated SCF-based features are better for French. In
the English experiment sophisticated features per-
formed better (the SCF-SP feature was the best).
However, the English experiment employed a
much larger dataset. These more sophisticated
features may suffer from data sparseness in our
French experiment since although we required the
minimum of 150 occurrences per verb, verb clus-
tering performance tends to improve when more
data is available, and given the fine-grained nature
of LexShem SCFs it is likely that more data is re-
quired for optimal performance.
We therefore performed another experiment
with French on the full set of 147 verbs, using
SPEC, where we investigated the effect of instance
filtering on the performance of the best features
from each feature group: F3, F7, F13 and F17.
The results shown in Table 3 reveal that the perfor-
mance of the features remains fairly similar until
the instance threshold of 1000. When 2000 occur-
rences per verb are used, the differences become
clearer, until at the threshold of 4000, it is obvious
that the most sophisticated SCF-SP feature F17 is
by far the best feature for French (65.4 F) and the
SCF feature F3 the second best (60.5 F). The CO-
feature F7 and the LP feature F13 are not nearly as
good (53.4 and 51.0 F).
Although the results at different thresholds are
not comparable due to the different number of
verbs and classes (see columns 2-3), the results
for features at the same threshold are. Those re-
sults suggest that when 2000 or more occurrences
per verb are used, most features perform like they
performed for English in the experiment of Sun
and Korhonen (2009), with CO being the least in-
formative3 and SCF-SP being the most informa-
3However, it is worth noting that CO is not a useless fea-
ture. As table 3 shows, when 150 or fewer occurrences are
1061
SPEC K Eng.
BL 6.7 6.7 6.7
F1 SCF 42.4 39.3 57.8
F2 SCF(POS) 45.9 40.3 46.7
F3 SCF(PP) 50.6 36.9 63.3
F4 CO(4) 50.3 38.2 40.9
F5 CO(4+loc) 48.8 26.3 -
F6 CO(6) 52.7 29.2 -
F7 CO(6+loc) 55.1 33.8 -
F8 CO(8) 54.2 36.4 -
F9 CO(8+loc) 54.6 37.2 -
F10 LP(PREP) 35.5 32.8 49.0
F11 LP(SUBJ) 33.7 23.6 -
F12 LP(OBJ) 50.1 33.3 -
F13 LP(ALL) 52.7 40.1 74.6
F14 SCF+LP(SUBJ) 50.3 40.1 71.7
F15 SCF+LP(OBJ) 54.5 35.6 74.0
F16 SCF+LP(SUBJ+OBJ) 53.4 36.2 73.0
F17 SCF+SP 54.6 39.8 80.4
Table 2: Results for all the features for French
(SPEC and K-means) and English (SPEC)
THR Verbs Cls F3 F7 F13 F17
0 147 15 43.7 57.5 43.3 50.1
50 137 15 47.9 56.1 44.8 49.1
100 125 15 49.2 54.3 44.8 49.5
150 116 15 50.6 55.1 52.7 54.6
200 110 15 54.9 52.9 49.7 52.5
400 96 15 52.7 52.9 43.9 53.2
1000 71 15 51.4 54.0 44.8 54.5
2000 59 12 52.3 45.9 42.7 53.5
3000 51 12 55.7 49.0 46.8 59.2
4000 43 10 60.5 53.4 51.0 65.4
Table 3: The effect of verb frequency
tive feature. The only exception is the LP feature
which performed better than CO in English.
7.2 Qualitative Evaluation
We conducted qualitative analysis of the clusters
for French: those created using SPEC with F17
and F3. Verbs in the gold standard classes 29.2,
36.1, 37.3, 37.7 and 47.3 (Table 1) performed
particularly well, with the majority of member
verbs found in the same cluster. These verbs
are ideal for clustering because they have distinc-
tive syntactic-semantic characteristics. For exam-
ple, verbs in 29.2 CHARACTERIZE class (e.g. con-
cevoir, conside?rer, de?peindre) not only have a very
specific meaning but they also take high frequency
SCFs involving the preposition comme (Eng. as)
available for a verb, CO outperforms all the other features in
French, compensating for data sparseness.
which is not typical to many other classes. Inter-
estingly, Levin classes 29.2, 36.1, 37.3, and 37.7
were among the best performing classes also in
the supervised verb classification experiment of
Sun et al (2008) because these classes have dis-
tinctive characteristics also in English.
The benefit of sophisticated features which
integrate also semantic (SP) information (F17)
is particularly evident for classes with non-
distinctive syntactic characteristics. For example,
the intransitive verbs in 43.1 LIGHT EMISSION
class (e.g. briller, e?tinceler, flamboyer) are diffi-
cult to cluster based on syntax only, but semantic
features work because the verbs pose strong SPs
on their subjects (entities capable of light emis-
sion). In the experiment of Sun et al (2008), 43.1
was the worst performing class, possibly because
no semantic features were used in the experiment.
The most frequent source of error is syntac-
tic idiosyncracy. This is particularly evident
for classes 10.1 REMOVE and 45.4 CHANGE OF
STATE. Although verbs in these classes can take
similar SCFs and alternations, only some of them
are frequent in data. For example, the SCF o?ter X
a` Y is frequent for verbs in 10.1, but not o?ter X
de Y. Although class 10.1 did not suffer from this
problem in the English experiment of Sun et al
(2008), class 45.4 did. Class 45.4 performs par-
ticularly bad in French also because its member
verbs are low in frequency.
Some errors are due to polysemy, caused partly
by the fact that the French version of the gold stan-
dard was not controlled for this factor. Some verbs
have their predominant senses in classes which are
missing in the gold standard, e.g. the most fre-
quent sense of retenir is memorize, not keep as in
the gold standard class 13.5.1. GET.
Finally, some errors are not true errors but
demonstrate the capability of clustering to learn
novel information. For example, the CHANGE
OF STATE class 45.4 includes many antonyms
(e.g. weaken vs. strenghten). Clustering (us-
ing F17) separates these antonyms, so that verbs
adoucir, atte?nuer and tempe?rer appear in one clus-
ter and consolider and renforcer in another. Al-
though these verbs share the same alternations,
their SPs are different. The opposite effect can be
observed when clustering maps together classes
1062
which are semantically and syntactically related
(e.g. 36.1 CORRESPOND and 37.7 SPEAK). Such
classes are distinct in Levin and VerbNet, al-
though should ideally be related. Cases such as
these show the potential of clustering in discover-
ing novel valuable information in data.
8 Discussion and Conclusion
When sufficient corpus data is available, there is
a strong correlation between the types of features
which perform the best in English and French.
When the best features are used, many individ-
ual Levin classes have similar performance in the
two languages. Due to differences in data sets
direct comparison of performance figures for En-
glish and French is not possible. When consid-
ering the general level of performance, our best
performance for French (65.4 F) is lower than the
best performance for English in the experiment of
Sun and Korhonen (2009). However, it does com-
pare favourably to the performance of other state-
of-the-art (even supervised) English systems (Joa-
nis et al, 2008; Li and Brew, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009). This
is impressive considering that we experimented
with a fully unsupervised approach originally de-
veloped for another language.
When aiming to improve performance further,
employing larger data is critical. Most recent ex-
periments on English have employed bigger data
sets, and unlike us, some of them have only con-
sidered the predominant senses of medium-high
frequency verbs. As seen in section 7.1, such dif-
ferences in data can have significant impact on
performance. However, parser and feature ex-
traction performance can also play a big role in
overall accuracy, and should therefore be inves-
tigated further (Sun and Korhonen, 2009). The
relatively low performance of basic LP features
in French suggests that at least some of the cur-
rent errors are due to parsing. Future research
should investigate the source of error at different
stages of processing. In addition, it would be in-
teresting to investigate whether language-specific
tuning (e.g. using language specific features such
as auxiliary classes) can further improve perfor-
mance on French.
Earlier works most closely related to ours are
those of Merlo et al (2002) and Ferrer (2004).
Our results contrast with those of Ferrer who
showed that a clustering approach does not trans-
fer well from English to Spanish. However, she
used basic SCF and named entity features only,
and a clustering algorithm less suitable for high
dimensional data. Like us, Merlo et al (2002) cre-
ated a gold standard by translating Levin classes
to another language (Italian). They also applied a
method developed for English to Italian, and re-
ported good overall performance using features
developed for English. Although the experiment
was small (focussing on three classes and a few
features only) and involved supervised classifica-
tion, the results agree with ours.
These experiments support the linguistic hy-
pothesis that Levin style classification can be
cross-linguistically applicable. A clustering tech-
nique such as the one presented here could be used
as a tool for investigating whether classifications
are similar across a wider range of more diverse
languages. From the NLP perspective, the fact that
an unsupervised technique developed for one lan-
guage can be applied to another language with-
out the need for substantial tuning means that au-
tomatic techniques could be used to hypothesise
useful Levin style classes for further languages.
This, in turn, could facilitate the creation of mul-
tilingual VerbNets in the future.
9 Acknowledgement
Our work was funded by the Royal Society Uni-
versity Research Fellowship (AK), the Dorothy
Hodgkin Postgraduate Award (LS), the EPSRC
grants EP/F030061/1 and EP/G051070/1 (UK)
and the EU FP7 project ?PANACEA?.
References
Omri Abend, Roi Reichart, and Ari Rappoport. A
supervised algorithm for verb disambiguation into
VerbNet classes. In Proc. of COLING, pages 9?16,
2008.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
The Berkeley FrameNet Project. In COLING-ACL,
pages 86?90, 1998.
Didier Bourigault, Marie-Paule Jacques, Ce?cile Fabre,
Ce?cile Fre?rot, and Sylwia Ozdowska. Syntex,
analyseur syntaxique de corpus. In Actes des
1063
12e`mes journe?es sur le Traitement Automatique des
Langues Naturelles, 2005.
Chris Brew and Sabine Schulte im Walde. Spectral
clustering for German verbs. In Proc. of EMNLP,
pages 117?124, 2002.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and
Zheng-Yu Niu. Unsupervised relation disambigua-
tion using spectral clustering. In Proc. of COL-
ING/ACL, pages 89?96, 2006.
Hoa Trang Dang. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. PhD
thesis, CIS, University of Pennsylvania, 2004.
Eva Esteve Ferrer. Towards a semantic classification of
Spanish verbs based on subcategorisation informa-
tion. In Proc. of ACL Student Research Workshop,
2004.
Ralph Grishman, Catherine Macleod, and Adam Mey-
ers. Comlex syntax: building a computational lexi-
con. In Proc. of COLING, pages 268?272, 1994.
Maurice Gross. Me?thodes en syntaxe. Hermann, Paris,
1975.
Eduard Hovy, Mitch Marcus, Martha Palmer,
L. Ramshaw, and R. Weischedel. Ontonotes: The
90% solution. In HLT/NAACL, 2006.
Ray Jackendoff. Semantic Structures. The MIT Press,
Cambridge, MA, 1990.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classifica-
tion. Nat. Lang. Eng., 14(3):337?367, 2008.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. A large-scale classification of En-
glish verbs. Language Resources and Evaluation,
42:21?40, 2008.
Karin Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. University of Pennsyl-
vania, PA, 2005.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Fea-
tures for Automatic Verb Classification. In Proc. of
ACL, pages 434?442, 2008.
Marina. Meila. The multicut lemma. Technical report,
University of Washington, 2001.
Marina Meila and Jianbo Shi. A random walks view of
spectral segmentation. In AISTATS, 2001.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. A multilingual paradigm for auto-
matic verb classification. In Proc. of ACL, 2002.
Ce?dric Messiant. ASSCI : A subcategorization frames
acquisition system for French. In Proc. of ACL Stu-
dent Research Workshop, pages 55?60, 2008.
Ce?dric Messiant, Thierry Poibeau, and Anna Korho-
nen. LexSchem: a Large Subcategorization Lexicon
for French Verbs. In Proc. of LREC, 2008.
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 1995.
Diarmuid O? Se?aghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proc. of
COLING, pages 649?656, 2008.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 3(1):71?
106, 2005.
Patrick Saint-Dizier. Verb Semantic Classes Based on
?alternations? and WordNet-like criteria . In P. Saint-
Dizier, editor, Predicative Forms in Natural lan-
guage and lexical Knowledge Bases , pages 247?
279. Kluwer Academic, 1998.
Sabine Schulte im Walde. Experiments on the Auto-
matic Induction of German Semantic Verb Classes.
Computational Linguistics, 2006.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proc. of CICLing, pages
100?111, 2005.
Lin Sun and Anna Korhonen. Improving verb cluster-
ing with automatically acquired selectional prefer-
ences. In Proc. of EMNLP, pages 638?647, 2009.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
Verb class discovery from rich syntactic data. LNCS,
4919:16, 2008.
Yoshimi Suzuki and Fumiyo Fukumoto. Classify-
ing Japanese Polysemous Verbs based on Fuzzy C-
means Clustering. In Proc. of TextGraphs-4, pages
32?40, 2009.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proc. of EMNLP, 2004.
Gloria Va?zquez, Ana Ferna?ndez, Irene Castello?n, and
M. Antonia Mart??. Clasificacio?n verbal: Alternan-
cias de dia?tesis. In Quaderns de Sintagma. Univer-
sitat de Lleida, 2000.
Deepak Verma and Marina Meila. A comparison of
spectral clustering algorithms. Technical report, De-
partment of CSE University of Washington Seattle,
2005.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. Unsupervised and Constrained Dirich-
let Process Mixture Models for Verb Clustering. In
Proc. of the Workshop on on GEMS, pages 74?82,
2009.
Ulrike von Luxburg. A tutorial on spectral clustering.
STAT COMPUT, 17:395 ? 416, 2007.
Piek Vossen. EuroWordNet: A Multilingual Database
with Lexical Semantic Networks. Kluwer Academic
Publishers, Dordrecht, 1998.
Lihi Zelnik-Manor and Pietro Perona. Self-tuning
spectral clustering. NIPS, 17(1601-1608):16, 2004.
1064
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 76?80, Dublin, Ireland, August 23-29 2014.
CRAB 2.0: A text mining tool for supporting literature review in chemical
cancer risk assessment
Yufan Guo
1
, Diarmuid
?
O S
?
eaghdha
1
, Ilona Silins
2
, Lin Sun
1
,
Johan H
?
ogberg
2
, Ulla Stenius
2
, Anna Korhonen
1
1
Computer Laboratory, University of Cambridge, UK
2
Institute of Environmental Medicine, Karolinska Institutet, Stockholm, Sweden
Abstract
Chemical cancer risk assessment is a literature-dependent task which could greatly benefit from
text mining support. In this paper we describe CRAB ? the first publicly available tool for
supporting the risk assessment workflow. CRAB, currently at version 2.0, facilitates the gathering
of relevant literature via PubMed queries as well as semantic classification, statistical analysis and
efficient study of the literature. The tool is freely available as an in-browser application.
1 Introduction
Biomedical text mining addresses the great need to access information in the growing body of literature
in biomedical sciences. Prior research has produced useful tools for supporting practical tasks such as
literature curation and development of semantic databases, among others (Chapman and Cohen, 2009;
Harmston et al., 2010; Simpson and Demner-Fushman, 2012; McDonald and Kelly, 2012). In this paper
we describe a tool we have built to aid literature exploration for the task of chemical risk assessment
(CRA). The need for assessment of chemical hazards, exposures and their corresponding health risks is
growing, as many countries have tightened up their chemical safety rules. CRA work requires thorough
review of available scientific data for each chemical under inspection, much of which can be found in
scientific literature (EPA, 2005). Since the scientific data is highly varied and well-studied chemicals
may have tens of thousands of publications (e.g. to date PubMed contains 23,665 articles mentioning
phenobarbital), the task can be extremely time consuming when conducted via conventional means
(Korhonen et al., 2009). As a result, there is interest among the CRA community in text mining tools that
can aid and streamline the literature review process.
We have developed CRAB, an online system that supports the entire process of literature review for
cancer risk assessors. It is the first and only NLP system that serves this need. CRAB contains three main
components:
1. Literature search with PubMed integration
2. Semantic classification of abstracts with summary visualisation
3. Literature browsing with markup of information structure
These components are described further in Section 2 below. Version 2.0 of CRAB is freely available as an
in-browser application; see Section 4 for access information.
2 System description
2.1 Literature search
The first step for the user is to retrieve a collection of scientific articles relevant to their need, e.g., all
articles with abstracts that contain the name of a given chemical. The CRAB 2.0 search page (Figure
1) allows the user to directly query the MEDLINE database of biomedical abstracts. The search query
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
76
Figure 1: The CRAB 2.0 search interface
(a) Taxonomy view
gene
ral(wo
rds re
lated 
to RA
)
hum
an s
tudy
/epid
emio
logy
tumo
r rela
ted
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
rs
polym
orph
ism
anim
al st
udy
stud
y len
gth
2?ye
ar ca
ncer
 bioa
ssay
shor
t and
  me
dium tumo
rs
pren
eopl
astic
 lesio
ns
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
r
type
 of a
nima
l
gene
ticall
y mo
dified
 anim
als
cell 
expe
rime
nts 
bioc
hem
ical/c
ellbio
logic
al ef
fects
subc
ellula
r sys
tems
 
stud
y on
 mic
roorg
anism
s
rev
iew 
artic
le, s
umm
ary
Scientific Evidence
% ab
strac
ts
0
20
40
60
80
100
(b) Histogram view
Figure 2: The CRAB 2.0 classification component
is sent, and the results received, using the E-Utilities web service provided by the National Center
for Biotechnology Information.
1
This query interface supports PubMed Advanced Search, facilitating
complex Boolean queries.
2.2 Semantic classification
The document collection returned by the PubMed web service is passed in XML format to a semantic
classifier that annotates each abstract with 42 binary labels indicating the presence/absence of concepts
relevant to CRA. These concepts are organised hierarchically in two main taxonomies: (1) kinds of
scientific evidence used for CRA (e.g., human studies, animal studies, cell experiments, biochemical/cell
biological effects); (2) the carcinogenic modes of action indicated by the evidence (e.g., genotoxic,
nongenotoxic/indirect genotoxic, cell death, inflammation, angiogenesis). The underlying classifier is a
support vector machine (SVM) trained on a dataset of 3,078 manually annotated abstracts. Features used
by the SVM include lexical n-grams, character n-grams and MeSH concepts. For more details on the
concept taxonomies, training corpus and classifier see Korhonen et al. (2012).
1
http://www.ncbi.nlm.nih.gov/books/NBK25501/
77
Figure 3: The CRAB 2.0 information structure component
Once each abstract in the retrieved collection has been classified, the user is presented with a summary
of counts for each concept (Figure 2a). In a user study, risk assessors found this summary very useful for
obtaining a broad overview of the literature, identifying groups of chemicals with similar toxicological
profiles and identifying data gaps (Korhonen et al., 2012). The user can also request a histogram
visualisation (Figure 2b), which is produced through a call to the statistical software R.
2
2.3 Literature browsing
The risk assessment workflow involves close reading of relevant abstracts to identify specific information
about methods, experimental details, results and conclusions. While it is not feasible to automate this
process, we have shown that automatic markup and visualisation of abstracts? information structure
can accelerate it considerably (Guo et al., 2011). The model of information structure incorporated in
CRAB 2.0 is based on argumentative zoning (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel, 2010),
whereby the text of a scientific abstract (or article) is segmented into blocks of sentences that carry a
specific rhetorical function and combine to communicate the argument the authors wish to convey to
the reader. The markup scheme used in our system labels each sentence with one of seven categories:
background, objective, method, result, conclusion, related work and future work (Guo et al., 2010). The
CRAB system incorporates preprocessing (lemmatisation, POS tagging, parsing) with the C&C toolkit
3
and information structure markup with an SVM classifier that labels sentences according to a combination
of lexical, syntactic and discourse features (Guo et al., 2011). The classifier has been trained on an
annotated dataset of 1,000 CRA abstracts (Guo et al., 2010).
The automatic information structure markup is used to support browsing of the set of abstracts assigned
a label of interest by the semantic classifier; e.g., the user can inspect all abstracts labelled genotoxic
(Figure 3). Each information structure category is highlighted in a different colour and the user can select
a single category to focus on. To our knowledge, CRAB 2.0 is the first publicly available online tool that
provides information structure analysis of biomedical literature.
3 Evaluation
Intrinsic cross-validation evaluations of the semantic taxonomy classifier and information structure
classifier show high performance: 0.78 macro-averaged F-score (Korhonen et al., 2012) and 0.88 accuracy
(Guo et al., 2011), respectively. Furthermore, user-based evaluation in the context of real-life CRA has
2
http://www.r-project.org/
3
http://svn.ask.it.usyd.edu.au/trac/candc
78
produced promising results. (Korhonen et al., 2012) showed that the concept distributions produced by
our classifier confirmed known properties of chemicals without human input. Guo et al. (2011) found that
integrating information structure visualisation in abstract browsing helped risk assessors to find relevant
information in abstracts 7-8% more quickly.
4 Use
CRAB 2.0 is freely available as an in-browser application at http://omotesando-e.cl.cam.
ac.uk/CRAB/request.html. New users can register an id and password to allow them to store
and retrieve data from previous sessions. Alternatively, they can use an anonymous guest account (id
guest@coling, password guest@coling).
5 Conclusion
We have presented Version 2.0 of CRAB, the first NLP tool for supporting the workflow of literature
review for cancer risk assessment. CRAB meets a real, specialised need and is already being used to
improve the efficiency of CRA work. Although currently focused on cancer, CRAB can be easily adapted
to other health risks provided with the appropriate taxonomy and annotated data for machine learning. In
the future, the tool can be developed further in various ways, e.g. to support submissions in other formats
than PubMed XML; to take into account journal impact factors, number of citations and cross references
to better organize the literature; and to offer enriched statistical analysis of classified literature.
Acknowledgements
This work was supported by the Royal Society, Vinnova and the Swedish Research Council.
References
Wendy W. Chapman and K. Bretonnel Cohen. 2009. Current issues in biomedical text mining and natural language
processing. Journal of Biomedical Informatics, 42(5):757?759.
EPA. 2005. Guidelines for carcinogen risk assessment. US Environmental Protection Agency.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins, Lin Sun, and Ulla Stenius. 2010. Identifying the informa-
tion structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP-10,
Uppsala, Sweden.
Yufan Guo, Anna Korhonen, Ilona Silins, and Ulla Stenius. 2011. Weakly supervised learning of information
structure of scientific abstracts: Is it accurate enough to benefit real-world tasks in biomedicine? Bioinformatics,
27(22):3179?3185.
Nathan Harmston, Wendy Filsell, and Michael P.H. Stumpf. 2010. What the papers say: Text mining for genomics
and systems biology. Human Genomics, 5(1):17?29.
Anna Korhonen, Ilona Silins, Lin Sun, and Ulla Stenius. 2009. The first step in the development of text min-
ing technology for cancer risk assessment: Identifying and organizing scientific evidence in risk assessment
literature. BMC Bioinformatics, 10:303.
Anna Korhonen, Diarmuid
?
O S?eaghdha, Ilona Silins, Lin Sun, Johan H?ogberg, and Ulla Stenius. 2012. Text
mining for literature review and knowledge discovery in cancer risk assessment and research. PLoS ONE,
7(4):e33427.
Diane McDonald and Ursula Kelly. 2012. The value and benefit of text mining to UK further and higher education.
Report 811, JISC.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel Collier. 2006. Zone analysis in biology articles as a basis
for information extraction. International Journal of Medical Informatics, 75(6):468?487.
Matthew S. Simpson and Dina Demner-Fushman. 2012. Biomedical text mining: A survey of recent progress. In
Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data. Springer.
79
Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
Simone Teufel. 2010. The Structure of Scientific Articles: Applications to Citation Indexing and Summarization.
CSLI Publications, Stanford, CA.
80
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023?1033,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hierarchical Verb Clustering Using Graph Factorization
Lin Sun and Anna Korhonen
University of Cambridge, Computer Laboratory
15 JJ Thomson Avenue, Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk
Abstract
Most previous research on verb clustering has
focussed on acquiring flat classifications from
corpus data, although many manually built
classifications are taxonomic in nature. Also
Natural Language Processing (NLP) applica-
tions benefit from taxonomic classifications
because they vary in terms of the granularity
they require from a classification. We intro-
duce a new clustering method called Hierar-
chical Graph Factorization Clustering (HGFC)
and extend it so that it is optimal for the task.
Our results show that HGFC outperforms the
frequently used agglomerative clustering on a
hierarchical test set extracted from VerbNet,
and that it yields state-of-the-art performance
also on a flat test set. We demonstrate how the
method can be used to acquire novel classifi-
cations as well as to extend existing ones on
the basis of some prior knowledge about the
classification.
1 Introduction
A variety of verb classifications have been built to
support NLP tasks. These include syntactic and se-
mantic classifications, as well as ones which in-
tegrate aspects of both (Grishman et al, 1994;
Miller, 1995; Baker et al, 1998; Palmer et al, 2005;
Kipper, 2005; Hovy et al, 2006). Classifications
which integrate a wide range of linguistic proper-
ties can be particularly useful for tasks suffering
from data sparseness. One such classification is
the taxonomy of English verbs proposed by Levin
(1993) which is based on shared (morpho-)syntactic
and semantic properties of verbs. Levin?s taxon-
omy or its extended version in VerbNet (Kipper,
2005) has proved helpful for various NLP applica-
tion tasks, including e.g. parsing, word sense disam-
biguation, semantic role labeling, information ex-
traction, question-answering, and machine transla-
tion (Swier and Stevenson, 2004; Dang, 2004; Shi
and Mihalcea, 2005; Zapirain et al, 2008).
Because verbs change their meaning and be-
haviour across domains, it is important to be able
to tune existing classifications as well to build novel
ones in a cost-effective manner, when required. In
recent years, a variety of approaches have been pro-
posed for automatic induction of Levin style classes
from corpus data which could be used for this pur-
pose (Schulte im Walde, 2006; Joanis et al, 2008;
Sun et al, 2008; Li and Brew, 2008; Korhonen
et al, 2008; O? Se?aghdha and Copestake, 2008; Vla-
chos et al, 2009). The best of such approaches
have yielded promising results. However, they have
mostly focussed on acquiring and evaluating flat
classifications. Levin?s classification is not flat, but
taxonomic in nature, which is practical for NLP pur-
poses since applications differ in terms of the gran-
ularity they require from a classification.
In this paper, we experiment with hierarchical
Levin-style clustering. We adopt as our baseline
method a well-known hierarchical method ? ag-
glomerative clustering (AGG) ? which has been pre-
viously used to acquire flat Levin-style classifica-
tions (Stevenson and Joanis, 2003) as well as hierar-
chical verb classifications not based on Levin (Fer-
rer, 2004; Schulte im Walde, 2008). The method has
also been popular in the related task of noun clus-
1023
tering (Ushioda, 1996; Matsuo et al, 2006; Bassiou
and Kotropoulos, 2011).
We introduce then a new method called Hierar-
chical Graph Factorization Clustering (HGFC) (Yu
et al, 2006). This graph-based, probabilistic cluster-
ing algorithm has some clear advantages over AGG
(e.g. it delays the decision on a verb?s cluster mem-
bership at any level until a full graph is available,
minimising the problem of error propagation) and it
has been shown to perform better than several other
hierarchical clustering methods in recent compar-
isons (Yu et al, 2006). The method has been applied
to the identification of social network communities
(Lin et al, 2008), but has not been used (to the best
of our knowledge) in NLP before.
We modify HGFC with a new tree extraction al-
gorithm which ensures a more consistent result, and
we propose two novel extensions to it. The first is a
method for automatically determining the tree struc-
ture (i.e. number of clusters to be produced for each
level of the hierarchy). This avoids the need to pre-
determine the number of clusters manually. The sec-
ond is addition of soft constraints to guide the clus-
tering performance (Vlachos et al, 2009). This is
useful for situations where a partial (e.g. a flat) verb
classification is available and the goal is to extend it.
Adopting a set of lexical and syntactic features
which have performed well in previous works, we
compare the performance of the two methods on test
sets extracted from Levin and VerbNet. When eval-
uated on a flat clustering task, HGFC outperforms
AGG and performs very similarly with the best flat
clustering method reported on the same test set (Sun
and Korhonen, 2009). When evaluated on a hierar-
chical task, HGFC performs considerably better than
AGG at all levels of gold standard classification. The
constrained version of HGFC performs the best, as
expected, demonstrating the usefulness of soft con-
straints for extending partial classifications.
Our qualitative analysis shows that HGFC is ca-
pable of detecting novel information not included in
our gold standards. The unconstrained version can
be used to acquire novel classifications from scratch
while the constrained version can be used to extend
existing ones with additional class members, classes
and levels of hierarchy.
2 Target classification and test sets
The taxonomy of Levin (1993) groups English verbs
(e.g. break, fracture, rip) into classes (e.g. 45.1
Break verbs) on the basis of their shared mean-
ing components and (morpho-)syntactic behaviour,
defined in terms of diathesis alternations (e.g. the
causative/inchoative alternation, where an NP frame
alternates with an intransitive frame: Tony broke the
window ? The window broke). It classifies over
3000 verbs in 57 top level classes, some of which
divide further into subclasses. The extended version
of the taxonomy in VerbNet (Kipper, 2005) classifies
5757 verbs. Its 5 level taxonomy includes 101 top
level and 369 subclasses. We used three gold stan-
dards (and corresponding test sets) extracted from
these resources in our experiments:
T1: The first gold standard is a flat gold standard
which includes 13 classes appearing in Levin?s orig-
inal taxonomy (Stevenson and Joanis, 2003). We in-
cluded this small gold standard in our experiments
so that we could compare the flat version of our
method against previously published methods. Fol-
lowing Stevenson and Joanis (2003), we selected 20
verbs from each class which occur at least 100 times
in our corpus. This gave us 260 verbs in total.
T2: The second gold standard is a large, hi-
erarchical gold standard which we extracted from
VerbNet as follows: 1) We removed all the verbs
that have less than 1000 occurrences in our cor-
pus. 2) In order to minimise the problem of pol-
ysemy, we assigned each verb to the class which,
according to VerbNet, corresponds to its predomi-
nant sense in WordNet (Miller, 1995). 3) In order
to minimise the sparse data problem with very fine-
grained classes, we converted the resulting classifi-
cation into a 3-level representation so that the classes
at the 4th and 5th level were combined. For exam-
ple, the sub-classes of Declare verbs (numbered as
29.4.1.1.{1,2,3}) were combined into 29.4.1. 4) The
classes that have fewer than 5 members were dis-
carded. The total number of verb senses in the re-
sulting gold standard is 1750, which is 33.2% of the
verbs in VerbNet. T2 has 51 top level, 117 second
level, and 133 third level classes.
T3: The third gold standard is a subset of T2
where singular classes (top level classes which do
not divide into subclasses) are removed. This gold
1024
standard was constructed to enable proper evalua-
tion of the constrained version of HGFC (introduced
in the following section) where we want to com-
pare the impact of constraints across several levels
of classification. T3 provides classification of 357
verbs into 11 top level, 14 second level, and 32 third
level classes.
For each verb appearing in T1-T3, we extracted
all the occurrences (up to 10,000) from the British
National Corpus (Leech, 1992) and North American
News Text Corpus (Graff, 1995).
3 Method
3.1 Features and feature extraction
Previous works on Levin style verb classification
have investigated optimal features for this task
(Stevenson and Joanis, 2003; Li and Brew, 2008;
Sun and Korhonen, 2009)). We adopt for our exper-
iments a set of features which have performed well
in recent verb clustering works:
A: Subcategorization frames (SCFs) and their rela-
tive frequencies with individual verbs.
B: A with SCFs parameterized for prepositions.
C: B with SCFs parameterized for subjects appear-
ing in grammatical relations associated with the
verb in parsed data.
D: B with SCFs parameterized for objects appear-
ing in grammatical relations associated with the
verb in parsed data.
These features are purely syntactic. Although
semantic features ? verb selectional preferences ?
proved the best (when used in combination with syn-
tactic features) in the recent work of Sun and Ko-
rhonen (2009), we left such features for future work
because we noticed that different levels of classifi-
cation are likely to require semantic features at dif-
ferent granularities.
We extracted the syntactic features using the sys-
tem of Preiss et al (2007). The system tags, lemma-
tizes and parses corpus data using the RASP (Robust
Accurate Statistical Parsing toolkit (Briscoe et al,
2006)), and on the basis of the resulting grammat-
ical relations, assigns each occurrence of a verb as
a member of one of the 168 verbal SCFs. We pa-
rameterized the SCFs as described above using the
information provided by the system.
3.2 Clustering
We introduce the agglomerative clustering (AGG)
and Hierarchical Graph Factorization Clustering
(HGFC) methods in the following two subsec-
tions, respectively. The subsequent two subsections
present our extensions to HGFC: (i) automatically
determining the cluster structure and (ii) adding soft
constraints to guide clustering performance.
3.2.1 Agglomerative clustering
AGG is a method which treats each verb as a
singleton cluster and then successively merges two
closest clusters until all the clusters have been
merged into one. We used the SciPy?s imple-
mentation (Oliphant, 2007) of the algorithm. The
cluster distance is measured using linkage criteria.
We experimented with four commonly used link-
age criteria: Single, Average, Complete and Ward?s
(Ward Jr., 1963). Ward?s criterion performed the
best and was used in all the experiments in this pa-
per. It measures the increase in variance after two
clusters are merged. The output of AGG tends to
have excessive number of levels. Cut-based meth-
ods (Wu and Leahy, 1993; Shi and Malik, 2000) are
frequently applied to extract a simplified view. We
followed previous verb clustering works and cut the
AGG hierarchy manually.
AGG suffers from two problems. The first is er-
ror propagation. When a verb is misclassified at a
lower level, the error propagates to all the upper lev-
els. The second is local pairwise merging, i.e. the
fact that only two clusters can be combined at any
level. For example, in order to group clusters rep-
resenting Levin classes 9.1, 9.2 and 9.3 into a sin-
gle cluster representing class 9, the method has to
produce intermediate clusters, e.g. 9.{1,2} and 9.3.
Such clusters do not always have a semantic inter-
pretation. Although they can be removed using a
cut-based method, this requires a pre-defined cut-off
value which is difficult to set (Stevenson and Joanis,
2003). In addition, a significant amount of informa-
tion is lost in pair-wise clustering. In the above ex-
ample, only the clusters 9.{1,2} and 9.3 are consid-
ered, while alternative clusters 9.{1,3} and 9.2 are
ignored. Ideally, information about all the possible
intermediate clusters should be aggregated, but this
is intractable in practice.
1025
3.2.2 Hierarchical Graph Factorization
Clustering
Our new method HGFC derives a probabilistic bi-
partite graph from the similarity matrix (Yu et al,
2006). The local and global clustering structures are
learned via the random walk properties of the graph.
The method does not suffer from the above prob-
lems with AGG. Firstly, there is no error propagation
because the decision on a verb?s membership at any
level is delayed until the full bipartite graph is avail-
able and until a tree structure can be extracted from
it by aggregating probabilistic information from all
the levels. Secondly, the bipartite graph enables the
construction of a hierarchical structure without any
intermediate classes. For example, we can group
classes 9.{1,2,3} directly into class 9.
We use HGFC with the distributional similarity
measure Jensen-Shannon Divergence (djs(v, v?)).
Given a set of verbs, V = {vn}Nn=1, we
compute a similarity matrix W where Wij =
exp(?djs(v1, v2)). W can be encoded by a undi-
rected graph G (Figure 1(a)), where the verbs are
mapped to vertices and the Wij is the edge weight
between vertices i and j.
The graph G and the cluster structure can be rep-
resented by a bipartite graph K(V,U). V are the
vertices onG. U = {up}mp=1 represent the hiddenm
clusters. For example, looking at Figure 1(b), V on
G can be grouped into three clusters u1, u2 and u3.
The matrix B denotes the n ?m adjacency matrix,
with bip being the connection weight between the
vertex vi and the cluster up. Thus, B represents the
connections between clusters at an upper and lower
level of clustering. A flat clustering algorithm can
be induced by computing B.
The bipartite graph K also induces a similarity
(W ?) between vi and vj : w?ij =
?m
p=1
bipbjp
?p =
(B??1BT )ij where ? = diag(?1, ..., ?m). There-
fore,B can be found by approximating the similarity
matrix W of G using W ? derived from K. Given a
distance function ? between two similarity matrices,
B approximates W by minimizing the cost function
?(W,B??1BT ). The coupling between B and ? is
removed by setting H = B??1:
min
H,?
?(W,H?HT ), s.t.
n?
i=1
hip = 1 (1)
We use the divergence distance: ?(X,Y ) =?
ij(xij log
xij
yij ?xij+yij). Yu et al (2006) showedthat this cost function is non-increasing under the
update rule:
h?ip ? hip
?
j
wij
(H?HT )ij
?phjp s.t.
?
i
h?ip = 1 (2)
??p ? ?p
?
j
wij
(H?HT )ij
hiphjp s.t.
?
p
??p =
?
ij
wij (3)
wij can be interpreted as the probability of the di-
rect transition between vi and vj : wij = p(vi, vj),
when?ij wij = 1. bip can be interpreted as:
p(up, uq) = p(up)p(up|uq) =
n?
i=1
bipbiq
di
= (BTD?1B)pq (4)
D = diag(d1, ..., dn) where di =
m?
p=0
bip
p(up, uq) is the similarity between the clusters. It
takes into account of a weighted average of contri-
butions from all the data. This is different from the
linkage method where only the data from two clus-
ters are considered.
Given the cluster similarity p(up, uq), we can con-
struct a new graphG1 (Figure 1(d)) with the clusters
U as vertices. The cluster algorithm can be applied
again (Figure 1(e)). This process can go on itera-
tively, leading to a hierarchical graph.
Algorithm 1 HGFC algorithm (Yu et al, 2006)
Require: N verbs V , number of clusters ml for L levels
Compute the similarity matrix W0 from V
Build the graph G0 from W0 , and m0 ? n
for l = 1, 2 to L do
FactorizeGl?1 to obtain bipartite graph Kl with the
adjacency matrix Bl (eq. 1, 2 and 3)
Build a graph Gl with similarity matrix Wl =
BTl D?1l Bl according to equation 4end for
return BL, BL?1...B1
Additional steps need to be performed in order to
extract a tree from the hierarchical graph. Yu et al
(2006) performs the extraction via a propagation of
probabilities from the bottom level clusters. For a
verb vi, the probability of assigning it to cluster v(l)p
at level l is given by:
1026
v1
v6
v2
v4
v3
v5
v7 v8v9
(a)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
(b)
u3
u1
u2
v1
v6
v2
v4
v3
v5
v7 v8v9
(c)
u1 u2
u3
(d)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
q1
q2
(e)
Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters
on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1
p(v(l)p |vi) =
?
Vl?1
...
?
V1
p(v(l)p |v(l?1))...p(v(1)|vi)
= (D(?1)1 B1D?12 B2D?13 B3...D?1l Bl)ip (5)
This method might not extract a consistent tree
structure, because the cluster membership at the
lower level does not constrain the upper level mem-
bership. This prevented us from extracting a Levin
style hierarchical classification in our initial experi-
ments. For example, where two verbs were grouped
together at a lower level, they could belong to sepa-
rate clusters at an upper level. We therefore propose
a new tree extraction algorithm (Algorithm 2).
The new algorithm starts from the top level bipar-
tite graph, and generates consistent labels for each
level by taking into account of the tree constraints
set at upper levels.
Algorithm 2 Tree extraction algorithm for HGFC
Require: Given N , (Bl,ml) on each level for L levels
On the top level L, collect the labels TL (eq. 5)
Define C to be a (mL?1 ?mL) zero matrix, Cij ? 1,
where i, j = arg maxi,j{BLij}for l = L? 1 to 1 do
for i = 1 to N do
Compute p(vlp|vi) for each cluster p (eq. 5)
tli = argmaxp{p(vlp|vi)|p = 1...ml, Cptl+1i 6= 0}end for
Redefine C to be a (ml?1?ml) zero matrix, Cij ?
1, where i, j = arg maxi,j{Blij}end for
return Tree consistent labels TL, TL?1...T 1
3.2.3 Automatically determining the number of
clusters for HGFC
HGFC needs the number of levels and clusters at
each level as input. However, this information is not
always available (e.g. when the goal is to actually
learn this information automatically). We therefore
propose a method for inferring the cluster structure
from data. As shown in figure 1, a similarity ma-
trix W models one-hop transitions that follow the
links from vertices to neighbors. A walker can also
go to other vertices via multi-hop transitions. Ac-
cording to the chain rule of the Markov process, the
multi-hop transitions indicate a decaying similarity
function on the graph (Yu et al, 2006). After t tran-
sitions, the similarity matrix (Wt) becomes:
Wt = Wt?1D?10 W0
Yu et al (2006) proved the correspondence be-
tween the HGFC levels (l) and the random walk time:
t = 2l?1. So the vertices at level l induce a sim-
ilarity matrix of verbs after t-hop transitions. The
decaying similarity function captures the different
scales of clustering structure in the data (Azran and
Ghahramani, 2006b). The upper levels would have
a smaller number of clusters which represent a more
global structure. After several levels, all the verbs
are expected to be grouped into one cluster. The
number of levels and clusters at each level can thus
be learned automatically.
We therefore propose a method that uses the de-
caying similarity function to learn the hierarchical
clustering structure. One simple modification to al-
gorithm 1 is to set the number of clusters at level l
1027
(ml) to be ml?1 ? 1. m is denoted as the number
of clusters that have at least one member according
to eq. 5. We start by treating each verb as a cluster
at the bottom level. The algorithm stops when all
the data points are merged into one cluster. The in-
creasingly decaying similarity causes many clusters
to have 0 members especially at lower levels, which
are pruned in the tree extraction.
3.2.4 Adding constraints to HGFC
The basic version of HGFC makes no prior as-
sumptions about the classification. It is useful
for learning novel verb classifications from scratch.
However, when wishing to extend an existing clas-
sification (e.g. VerbNet) it may be desirable to guide
the clustering performance on the basis of informa-
tion that is already known. We propose a constrained
version of HGFC which makes uses of labels at the
bottom level to learn upper level classifications. We
do this by adding soft constraints to clustering, fol-
lowing Vlachos et al (2009).
We modify the similarity matrix W as follows: If
two verbs have different labels (li 6= lj), the simi-
larity between them is decreased by a factor a, and
a < 1. We set a to 0.5 in the experiments. The re-
sulting tree is generally consistent with the original
classification. The influence of the underlying data
(domain or features) is reduced according to a.
4 Experimental evaluation
We applied the clustering methods introduced in
section 3 to the test sets described in section 2 and
evaluated them both quantitatively and qualitatively,
as described in the subsequent sections.
4.1 Evaluation methods
We used class based accuracy (ACC) and adjusted
rand index (Radj) to evaluate the results on the flat
test set T1 (see section 2 for details of T1-T3).
ACC is the proportion of members of dominant
clusters DOM-CLUSTi within all classes ci.
ACC =
?C
i=1 verbs in DOM-CLUSTi
number of verbs
The formula of Radj is (Hubert and Arabie, 1985):
Radj =
?
i,j
(nij
2
)
??i
(ni?
2
)?
j
(n?j
2
)
/
(n
2
)
1
2 [
?
i
(ni?
2
)
+
?
j
(n?j
2
)
]??i
(ni?
2
)?
j
(n?j
2
)
/
(n
2
)
where nij is the size of the intersection between
class i and cluster j.
We used normalized mutual information (NMI)
and F-Score (F) to evaluate hierarchical clustering
results on T2 and T3. NMI measures the amount of
statistical information shared by two random vari-
ables representing the clustering result and the gold-
standard labels. Given random variables A and B:
NMI(A,B) = I(A;B)[H(A) +H(B)]/2
I(A,B) =
?
k
?
j
|(vk ? cj |
N log
N |vk ? cj |
|vk||cj |
where |vk ? cj | is the number of shared member-
ship between cluster vk and gold-standard class cj .
The normalized variant of mutual information (MI)
enables the comparison of clustering with different
cluster numbers (Manning et al, 2008).
F is the harmonic mean of precision (P) and re-
call (R). P is calculated using modified purity ? a
global measure which evaluates the mean precision
of clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by nprevalent(K).
mPUR =
?
nprevalent(ki)>2
nprevalent(ki)
number of verbs
R is calculated using ACC.
F = 2 ?mPUR ? ACCmPUR + ACC
F is not suitable for comparing results with dif-
ferent cluster numbers (Rosenberg and Hirschberg,
2007). Therefore, we only report NMI when the
number of classes in clustering and gold-standard is
substantially different.
Finally, we supplemented quantitative evaluation
with qualitative evaluation of clusters produced by
different methods.
4.2 Quantitative evaluation
We first evaluated AGG and the basic (uncon-
strained) HGFC on the small flat test set T1. The
main purpose of this evaluation was to compare the
results of our methods against previously published
results on the same test set. The number of clus-
ters (K) and levels (L) were inferred automatically
for HGFC as described in section 3.2.3. However, to
1028
make the results comparable with previously pub-
lished ones, we cut the resulting hierarchy at the
level of closest match (12 clusters) to the K (13) in
the gold-standard. For AGG, we cut the hierarchy at
13 clusters.
Method ACC Radj
HGFC 41.2 17.4
AGG (reproduced) 32.7 9.9
AGG (Stevenson and Joanis (2003) 31.0 9.0
Table 1: Comparison against Stevenson and Joanis
(2003)?s result on T1 (using similar features).
Table 1 shows our results and the results of
Stevenson and Joanis (2003) on T1 when employing
AGG using Ward as the linkage criterion. In this ex-
periment, we used the same feature set as Stevenson
and Joanis (2003) (set B, see section 3.1) and were
therefore able to reproduce their AGG result with a
difference smaller than 2%. When using this simple
feature set, HGFC outperforms the best performing
AGG clearly: 8.5% in ACC and 7.3% in Radj .
We also compared HGFC against the best reported
clustering method on T1 to date ? that of spectral
clustering by Sun and Korhonen (2009). We used
the feature sets C and D which are similar to the
features (SCF parameterized by lexical prefences) in
their experiments. HGFC obtains F of 49.93% on T1
which is 5% lower than the result of Sun and Ko-
rhonen (2009). The difference comes from the tree
consistency requirement. When the HGFC is forced
to produce a flat clustering (a one level tree only), it
achieves the F of 52.55% which is very close to the
performance of spectral clustering.
We then evaluated our methods on the hierarchi-
cal test sets T2 and T3. In the first set of experi-
ments, we pre-defined the tree structure for HGFC
by setting L to 3 and K at each level to be the K
in the hierarchical gold standard. The hierarchy pro-
duced by AGG was cut into 3 levels according to Ks
in the gold standard. This enabled direct evaluation
of the results against the 3 level gold standards using
both NMI and F.
The results are reported in tables 2 and 3. In these
tables, Nc is the number of clusters in HGFC cluster-
ing while Nl is the number of classes in the gold
standard (the two do not always correspond per-
fectly because a few clusters have zero members).
Nc Nl
HGFC
unconstrained
AGG
NMI F NMI F
130 133 57.31 36.65 54.22 32.62
114 117 54.67 37.96 51.35 32.44
50 51 37.75 40.00 32.61 32.78
Table 2: Performance on T2 using a pre-defined tree
structure.
Nc Nl
HGFC
unconstrained
HGFC
constrained
AGG
NMI F NMI F NMI F
31 32 51.65 42.01 91.47 92.07 49.70 40.30
15 14 42.75 47.70 82.16 82.80 39.19 43.69
11 11 38.91 51.17 71.69 75.00 34.88 44.80
Table 3: Performance on T3 using a pre-defined tree
structure.
Table 2 compares the results of the unconstrained
version of HGFC against those of AGG on our largest
test set T2. As with T1, HGFC outperforms AGG
clearly. The benefit can now be seen at 3 different
levels of hierarchy. On average, the HGFC outper-
forms AGG 3.5% in NMI and 4.8% in F. The dif-
ference between the methods becomes clearer when
moving towards the upper levels of the hierarchy.
Table 3 shows the results of both unconstrained
and constrained versions of HGFC and those of
AGG on the test set T3 (where singular classes are
removed to enable proper evaluation of the con-
strained method). The results are generally gener-
ally better on this test set than on T2 ? which is to be
expected since T3 is a refined subset of T21.
Recall that the constrained version of HGFC learns
the upper levels of classification on the basis of soft
constraints set at the bottom level, as described ear-
lier in section 3.2.4. As a consequence, NMI and F
are both greater than 90% at the bottom level and
the results at the top level are notably lower because
the impact of the constraints degrades the further
away one moves from the bottom level. Yet, the rela-
tively high result across all levels shows that the con-
strained version of HGFC can be employed a useful
method to extend the hierarchical structure of known
classifications.
1NMI is higher on T2, however, because NMI has a higher
baseline for larger number of clusters (Vinh et al, 2009). NMI
is not ideal for comparing the results of T2 and T3.
1029
T2 T3
Nc Nl HGFC Nc Nl HGFC
148 133 53.26 64 32 54.91
97 117 49.85 35 32 50.83
46 51 33.55 20 14 44.02
19 51 25.80 10 14 34.41
9 51 19.17 6 11 32.27
3 51 13.06
Table 4: NMI of unconstrained HGFC when trees for T2
and T3 are inferred automatically.
Finally, Table 4 shows the results for the uncon-
strained HGFC on T2 and and T3 when the tree struc-
ture is not pre-defined but inferred automatically as
described in section 3.2.3. 6 levels are learned for
T2 and 5 for T3. The number of clusters produced
ranges from 3 to 148 for T2 and from 6 to 64 for
T3. We can see that the automatically detected clus-
ter numbers distribute evenly across different levels.
The scale of the clustering structure is more com-
plete here than in the gold standards.
In the table, Nc indicates the number of clusters
in the inferred tree, while Nl indicates the closest
match to the number of classes in the gold stan-
dard. This evaluation is not fully reliable because
the match between the gold standard and the cluster-
ing is poor at some levels of hierarchy. However, it
is encouraging to see that the results do not drop dra-
matically until the match between the two is really
poor.
4.3 Qualitative evaluation
To gain a better insight into the performance of
HGFC, we conducted further qualitative analysis of
the clusters the two versions of this method pro-
duced for T3. We focussed on the top level of 11
clusters (in the evaluation against the hierarchical
gold standard, see table 3) as the impact of soft con-
straints is the weakest for the constrained method at
this level.
As expected, the constrained HGFC kept many in-
dividual verbs belonging to same Verbnet subclass
together (e.g. verbs enjoy, hate, disdain, regret, love,
despise, detest, dislike, fear for the class 31.2.1) so
that most clusters simply group lower level classes
and their members together. Three nearly clean clus-
ters were produced which only include sub-classes
of the same class (e.g. 31.2.0 and 31.2.1 which both
belong to 31.2 Admire verbs). However, the remain-
ing 8 clusters group together sub-classes (and their
members) belonging to unrelated parent classes. In-
terestingly, 6 of these make both syntactic and se-
mantic sense. For example, several such 37.7 Say
verbs and 29.5 Conjencture verbs are found together
which share the meaning of communication and
which take similar sentential complements.
In contrast, none of the clusters produced by
the unconstrained HGFC represent a single VerbNet
class. The majority represent a high number of
classes and fewer members per class. Yet many of
the clusters make syntactic and semantic sense. A
good example is a cluster which includes member
verbs from 9.7 Spray/Load verbs, 21.2 Carve verbs,
51.3.1 Roll verbs, and 10.4 Wipe verbs. The verbs
included in this cluster share the meaning of specific
type of motion and show similar syntactic behaviour.
Thorough Levin style investigation of especially
the unconstrained method would require looking at
shared diathesis alternations between cluster mem-
bers. We left this for future work. However,
the analysis we conducted confirmed that the con-
strained method could indeed be used for extend-
ing known classifications, while the unconstrained
method is more suitable for acquiring novel classi-
fications from scratch. The errors in clusters pro-
duced by both methods were mostly due to syntactic
idiosyncracy and the lack of semantic information in
clustering. We plan to address the latter problem in
our future work.
5 Discussion and conclusion
We have introduced a new graph-based method ?
HGFC ? to hierarchical verb clustering which avoids
some of the problems (e.g. error propagation, pair-
wise cluster merging) reported with the frequently
used AGG method. We modified HGFC so that it can
be used to automatically determine the tree struc-
ture for clustering, and proposed two extensions to
it which make it even more suitable for our task. The
first involves automatically determining the number
of clusters to be produced, which is useful when
this is not known in advance. The second involves
adding soft constraints to guide the clustering per-
formance, which is useful when aiming to extend
existing classification.
1030
The results reported in the previous section are
promising. On a flat test set (T1), the unconstrained
version of HGFC outperforms AGG and performs
very similarly with the best current flat clustering
method (spectral clustering) evaluated on the same
dataset. On the hierarchical test sets (T2 and T3),
the unconstrained and constrained versions of HGFC
outperform AGG clearly at all levels of classification.
The constrained version of HGFC detects the missing
hierarchy from the existing gold standards with high
accuracy. When the number of clusters and levels
is learned automatically, the unconstrained method
produces a multi-level hierarchy. Our evaluation
against a 3-level gold standard shows that such a hi-
erarchy is fairly accurate. Finally, the results from
our qualitative evaluation show that both constrained
and unconstrained versions of HGFC are capable of
learning valuable novel information not included in
the gold standards.
The previous work on Levin style verb classifica-
tion has mostly focussed on flat classifications us-
ing methods suitable for flat clustering (Schulte im
Walde, 2006; Joanis et al, 2008; Sun et al, 2008; Li
and Brew, 2008; Korhonen et al, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009). How-
ever, some works have employed hierarchical clus-
tering as a method to infer flat clustering.
For example, Schulte im Walde and Brew (2002)
employed AGG to initialize the KMeans clustering
for German verbs. This gave better results than
random initialization. Stevenson and Joanis (2003)
used AGG for flat clustering on T1. They cut the hi-
erarchy at the number of classes in the gold standard
and found that it is difficult to automatically deter-
mine a good cut-off. Our evaluation in the previous
section shows that HGFC outperforms their imple-
mentation of AGG.
AGG was also used by Ferrer (2004) who per-
formed hierarchical clustering of 514 Spanish verbs.
The results were evaluated against a hierarchical
gold standard resembling that of Levin?s classifi-
cation in English (Va?zquez et al, 2000). Radj of
0.07 was reported for a 15-way classification which
is comparable to the result of Stevenson and Joanis
(2003).
Hierarchical clustering has also been performed
for the related task of semantic verb classification.
For example, Basili et al (1993) identified the prob-
lems of AGG, and applied a conceptual clustering al-
gorithm (Fisher, 1987) to Italian verbs. They used
semi-automatically acquired semantic roles and the
concept types as features. No quantitative results
were reported. The qualitative evaluation shows that
the resulting clusters are very fine-grained.
Schulte im Walde (2008) performed hierarchical
clustering of German verbs using human verb asso-
ciation as features and AGG as a method. They fo-
cussed on two small collections of 56 and 104 verbs
and evaluated the result against flat gold standard
extracted from GermaNet (Kunze and Lemnitzer,
2002) and German FrameNet (Erk et al, 2003), re-
spectively. They reported F of 62.69% for the 56
verbs, and F of 34.68% for the 104 verbs.
In the future, we plan to extend this research line
in several directions. First, we will try to deter-
mine optimal features for different levels of clus-
tering. For example, the general syntactic features
(e.g. SCF) may perform the best at top levels of a hi-
erarchy while more specific or refined features (e.g.
SCF+pp) may be optimal at lower levels. We also
plan to investigate incorporating semantic features,
like verb selectional preferences, in our feature set.
It is likely that different levels of clustering require
more or less specific selectional preferences. One
way to obtain the latter is hierarchical clustering of
relevant noun data.
In addition, we plan to apply the unconstrained
HGFC to specific domains to investigate its capabil-
ity to learn novel, previously unknown classifica-
tions. As for the constrained version of HGFC, we
will conduct a larger scale experiment on the Verb-
Net data to investigate what kind of upper level hi-
erarchy it can propose for this resource (which cur-
rently has over 100 top level classes).
Finally, we plan to compare HGFC to other hier-
archical clustering methods that are relatively new
to NLP but have proved promising in other fields,
including Bayesian Hierarchical Clustering (Heller
and Ghahramani, 2005; Teh et al, 2008) and the
method of Azran and Ghahramani (2006a) based on
spectral clustering.
6 Acknowledgement
Our work was funded by the Royal Society Uni-
versity Research Fellowship (AK), the Dorothy
Hodgkin Postgraduate Award (LS), the EPSRC
1031
grants EP/F030061/1 and EP/G051070/1 (UK) and
the EU FP7 project ?PANACEA?.
References
Arik Azran and Zoubin Ghahramani. A new approach
to data driven clustering. In Proceedings of the 23rd
international conference on Machine learning, ICML
?06, pages 57?64, New York, NY, USA, 2006a. ISBN
1-59593-383-2.
Arik Azran and Zoubin Ghahramani. Spectral methods
for automatic multiscale data clustering. In Proceed-
ings of the 2006 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition-Volume
1, pages 190?197. IEEE Computer Society Washing-
ton, DC, USA, 2006b.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
The berkeley framenet project. In In COLING-ACL,
pages 86?90, 1998.
Roberto. Basili, Maria Teresa Pazienza, and Paola Ve-
lardi. Hierarchical clustering of verbs. In Proceedings
of the Workshop on Acquisition of Lexical Knowledge
from Text, 1993.
Nikoletta Bassiou and Constantine Kotropoulos. Long
distance bigram models applied to word clustering.
Pattern Recogn., 44:145?158, January 2011. ISSN
0031-3203.
Ted Briscoe, John Carroll, and Rebecca Watson. The
second release of the rasp system. In Proceedings
of the COLING/ACL on Interactive presentation ses-
sions, 2006.
Hoa Trang Dang. Investigations into the Role of Lexical
Semantics in Word Sense Disambiguation. PhD thesis,
CIS, University of Pennsylvania, 2004.
Katrin Erk, Andrea Kowalski, Sebastian Pado?, and Man-
fred Pinkal. Towards a resource for lexical semantics:
a large german corpus with extensive semantic anno-
tation. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume
1, ACL ?03, pages 537?544, Stroudsburg, PA, USA,
2003. Association for Computational Linguistics.
Eva Esteve Ferrer. Towards a semantic classification
of spanish verbs based on subcategorisation informa-
tion. In Proceedings of the ACL 2004 workshop on
Student research, ACLstudent ?04, Stroudsburg, PA,
USA, 2004. Association for Computational Linguis-
tics.
Douglas H. Fisher. Knowledge acquisition via incremen-
tal conceptual clustering. Machine Learning, 2:139?
172, 1987. ISSN 0885-6125.
David Graff. North american news text corpus. Linguistic
Data Consortium, 1995.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
Comlex syntax: Building a computational lexicon. In
COLING, pages 268?272, 1994.
Katherine A. Heller and Zoubin Ghahramani. Bayesian
hierarchical clustering. In Proceedings of the 22nd
international conference on Machine learning, pages
297?304. ACM, 2005. ISBN 1595931805.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. Ontonotes: the
90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ?06,
pages 57?60, Stroudsburg, PA, USA, 2006. Associa-
tion for Computational Linguistics.
Lawrence Hubert and Phipps Arabie. Comparing par-
titions. Journal of Classification, 2:193?218, 1985.
ISSN 0176-4268.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classification.
Natural Language Engineering, 14(3):337?367, 2008.
Karin Kipper. VerbNet: A broad-coverage, comprehen-
sive verb lexicon. 2005.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
The Choice of Features for Classification of Verbs in
Biomedical Texts. In Proceedings of COLING, 2008.
Claudia Kunze and Lothar Lemnitzer. GermaNet-
representation, visualization, application. In Proceed-
ings of LREC, 2002.
Geoffrey Leech. 100 million words of english: the
british national corpus. Language Research, 28(1):1?
13, 1992.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Features
for Automatic Verb Classification. In Proceedings of
ACL, 2008.
Yu-Ru Lin, Yun Chi, Shenghuo Zhu, Hari Sundaram, and
Belle L. Tseng. Facetnet: a framework for analyz-
ing communities and their evolutions in dynamic net-
works. In Proceeding of the 17th international confer-
ence on World Wide Web, pages 685?694, New York,
NY, USA, 2008. ACM.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA,
2008. ISBN 0521865719, 9780521865715.
Yutaka Matsuo, Takeshi Sakaki, Ko?ki Uchiyama, and
Mitsuru Ishizuka. Graph-based word clustering using
a web search engine. In Proceedings of the EMNLP,
pages 542?550, 2006.
1032
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39?41,
1995.
Travis E. Oliphant. Python for scientific computing.
Computing in Science and Engineering, 9:10?20,
2007. ISSN 1521-9615.
Diarmuid O? Se?aghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proceed-
ings of COLING, 2008.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106, 2005.
Judita Preiss, Ted Briscoe, and Anna Korhonen. A sys-
tem for large-scale acquisition of verbal, nominal and
adjectival subcategorization frames from corpora. In
Proceedings of ACL, pages 912?919, 2007.
Andrew Rosenberg and Julia Hirschberg. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
2007.
Sabine Schulte im Walde. Experiments on the automatic
induction of german semantic verb classes. Computa-
tional Linguistics, 32(2), 2006.
Sabine Schulte im Walde. Human associations and the
choice of features for semantic verb classification.
Research on Language and Computation, 6:79?111,
2008. ISSN 1570-7075.
Sabine Schulte im Walde and Chris Brew. Inducing ger-
man semantic verb classes from purely syntactic sub-
categorisation information. In Proceedings of ACL,
pages 223?230, 2002.
Jianbo Shi and Jitendra Malik. Normalized cuts and im-
age segmentation. IEEE Transactions on pattern anal-
ysis and machine intelligence, 22(8):888?905, 2000.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proceedings of CICLING,
2005.
Suzanne Stevenson and Eric Joanis. Semi-supervised
verb class discovery using noisy features. In Proceed-
ings of HLT-NAACL 2003, pages 71?78, 2003.
Lin Sun and Anna Korhonen. Improving verb clustering
with automatically acquired selectional preferences. In
Proceedings of the EMNLP 2009, 2009.
Lin Sun, Anna Korhonen, and Yuval Krymolowski. Verb
class discovery from rich syntactic data. Lecture Notes
in Computer Science, 4919:16, 2008.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proceedings of EMNLP,
pages 95?102, 2004.
Yee Whye Teh, Hal Daume? III, and Daniel Roy. Bayesian
agglomerative clustering with coalescents. In Ad-
vances in Neural Information Processing Systems, vol-
ume 20, 2008.
Akira Ushioda. Hierarchical clustering of words. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 1159?1162. Association
for Computational Linguistics, 1996.
Gloria Va?zquez, Ana Ferna?ndez-Montraveta, and
M. Anto`nia Mart??. Clasificacio?n verbal:(alternancias
de dia?tesis). Universitat de Lleida, 2000. ISBN
8484090671.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Infor-
mation theoretic measures for clusterings comparison:
is a correction for chance necessary? In ICML ?09:
Proceedings of the 26th Annual International Confer-
ence on Machine Learning, pages 1073?1080, New
York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-
1.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. Unsupervised and constrained dirichlet process
mixture models for verb clustering. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 74?82, 2009.
Joe H. Ward Jr. Hierarchical grouping to optimize an ob-
jective function. Journal of the American statistical as-
sociation, 58(301):236?244, 1963. ISSN 0162-1459.
Zhenyu Wu and Richard Leahy. An optimal graph the-
oretic approach to data clustering: Theory and its ap-
plication to image segmentation. IEEE transactions
on pattern analysis and machine intelligence, pages
1101?1113, 1993. ISSN 0162-8828.
Kai Yu, Shipeng Yu, and Volker Tresp. Soft clustering on
graphs. Advances in Neural Information Processing
Systems, 18:1553, 2006.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. Ro-
bustness and generalization of role sets: PropBank vs.
VerbNet. In Proceedings of ACL-08: HLT, pages 550?
558, 2008.
1033
Proceedings of NAACL-HLT 2013, pages 978?988,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Unsupervised Metaphor Identification Using Hierarchical Graph
Factorization Clustering
Ekaterina Shutova
International Computer Science Institute and
Institute for Cognitive and Brain Sciences
University of California, Berkeley
katia@icsi.berkeley.edu
Lin Sun
Computer Laboratory
University of Cambridge
lin.sun@cl.cam.ac.uk
Abstract
We present a novel approach to automatic
metaphor identification, that discovers both
metaphorical associations and metaphorical
expressions in unrestricted text. Our sys-
tem first performs hierarchical graph factor-
ization clustering (HGFC) of nouns and then
searches the resulting graph for metaphorical
connections between concepts. It then makes
use of the salient features of the metaphori-
cally connected clusters to identify the actual
metaphorical expressions. In contrast to pre-
vious work, our method is fully unsupervised.
Despite this fact, it operates with an encour-
aging precision (0.69) and recall (0.61). Our
approach is also the first one in NLP to exploit
the cognitive findings on the differences in or-
ganisation of abstract and concrete concepts in
the human brain.
1 Introduction
Metaphor has traditionally been viewed as a form of
linguistic creativity, that gives our expression more
vividness, distinction and artistism. While this is
true on the surface, the mechanisms of metaphor
have a much deeper origin in our reasoning. To-
day metaphor is widely understood as a cognitive
phenomenon operating at the level of mental pro-
cesses, whereby one concept or domain is system-
atically viewed in terms of the properties of another
(Lakoff and Johnson, 1980). Consider the examples
(1) ?He shot down all of my arguments? and (2) ?He
attacked every weak point in my argument?. They
demonstrate a metaphorical mapping of the concept
of argument to that of war. The argument, which is
the target concept, is viewed in terms of a battle (or
a war), the source concept. The existence of such a
link allows us to systematically describe arguments
using the war terminology, thus leading to a num-
ber of metaphorical expressions. Lakoff and John-
son call such generalisations a source?target domain
mapping, or conceptual metaphor.
The ubiquity of metaphor in language has been
established in a number of corpus studies (Cameron,
2003; Martin, 2006; Steen et al, 2010; Shutova
and Teufel, 2010) and the role it plays in human
reasoning has been confirmed in psychological ex-
periments (Thibodeau and Boroditsky, 2011). This
makes metaphor an important research area for com-
putational and cognitive linguistics, and its auto-
matic processing indispensable for any semantics-
oriented NLP application. The problem of metaphor
modeling is gaining interest within NLP, with a
growing number of approaches exploiting statisti-
cal techniques (Mason, 2004; Gedigian et al, 2006;
Shutova, 2010; Shutova et al, 2010; Turney et al,
2011; Shutova et al, 2012). Compared to more
traditional approaches based on hand-coded knowl-
edge (Fass, 1991; Martin, 1990; Narayanan, 1997;
Narayanan, 1999; Feldman and Narayanan, 2004;
Barnden and Lee, 2002; Agerri et al, 2007), these
more recent methods tend to have a wider cover-
age, as well as be more efficient, accurate and ro-
bust. However, even the statistical metaphor pro-
cessing approaches so far often focused on a lim-
ited domain or a subset of phenomena (Gedigian
et al, 2006; Krishnakumaran and Zhu, 2007), and
only addressed one of the metaphor processing sub-
tasks: identification of metaphorical mappings (Ma-
son, 2004) or identification of metaphorical expres-
sions (Shutova et al, 2010; Turney et al, 2011). In
this paper, we present the first computational method
978
that identifies the generalisations that govern the
production of metaphorical expressions, i.e. con-
ceptual metaphors, and then uses these generalisa-
tions to identify metaphorical expressions in unre-
stricted text. As opposed to previous works on sta-
tistical metaphor processing that were supervised or
semi-supervised, and thus required training data, our
method is fully unsupervised. It relies on building a
hierarchical graph of concepts connected by their as-
sociation strength (using hierarchical clustering) and
then searching for metaphorical links in this graph.
Shutova et al (2010) introduced the hypothesis
of ?clustering by association? and claimed that in
the course of distributional noun clustering, abstract
concepts tend to cluster together if they are associ-
ated with the same source domain, while concrete
concepts cluster by meaning similarity. We share
this intuition, but take this idea a significant step
further. Our approach is theoretically grounded in
the cognitive science findings suggesting that ab-
stract and concrete concepts are organised differ-
ently in the human brain (Crutch and Warrington,
2005; Binder et al, 2005; Wiemer-Hastings and
Xu, 2005; Huang et al, 2010; Crutch and Warring-
ton, 2010; Adorni and Proverbio, 2012). Accord-
ing to Crutch and Warrington (2005), these differ-
ences emerge from their general patterns of relation
with other concepts. However, most NLP systems
to date treat abstract and concrete concepts as iden-
tical. In contrast, we incorporate this distinction
into our model by creating a network (or a graph)
of concepts, and automatically learning the differ-
ent patterns of association of abstract and concrete
concepts with other concepts. We expect that, while
concrete concepts would tend to naturally organise
into a tree-like structure (with more specific terms
descending from the more general terms), abstract
concepts would exhibit a more complex pattern of
associations. Consider the example in Figure 1. The
figure schematically shows a small portion of the
graph describing the concepts of mechanism (con-
crete), political system and relationship (abstract)
at two levels of generality. One can see from this
graph that if concrete concepts, such as bike or en-
gine tend to be connected to only one concept at the
higher level in the hierarchy (mechanism), abstract
concepts may have multiple higher-level associates:
the literal ones and the metaphorical ones. For ex-
ample, the abstract concept of democracy is liter-
ally associated with a more general concept of po-
litical system, as well as metaphorically associated
with the concept of mechanism. Such multiple as-
sociations are due to the fact that political systems
are metaphorically viewed as mechanisms, they can
function, break, they can be oiled etc. We often dis-
cuss them using mechanism terminology, and thus a
corpus-based distributional learning approach would
learn that they share features with political systems
(from their literal uses), as well as with mechanisms
(from their metaphorical uses, as shown next to the
respective graph edges in the figure). Our system
discovers such association patterns within the graph
and uses them to identify metaphorical connections
between the concepts.
To the best of our knowledge, our method is the
first one to use a hierarchical clustering model for
the metaphor processing task. The original graph of
concepts is built using hierarchical graph factoriza-
tion clustering (HGFC) (Yu et al, 2006) of nouns,
yielding a network of clusters with different levels
of generality. The weights on the edges of the graph
indicate association between the clusters (concepts).
HGFC has not been previously employed for noun
clustering in NLP, but showed successful results in
the verb clustering task (Sun and Korhonen, 2011).
In summary, our system (1) builds a graph of con-
cepts using HGFC, (2) traverses it to find metaphor-
ical associations between clusters using weights on
the edges of the graph, (3) generates lists of salient
features for the metaphorically connected clusters
and (4) searches the British National Corpus (BNC)
(Burnard, 2007) for metaphorical expressions de-
scribing the target domain concepts using the verbs
from the set of salient features. We evaluated the
performance of the system with the aid of human
judges in precision- and recall-oriented settings. In
addition, we compared its performance to that of two
baselines, an unsupervised baseline using agglom-
erative clustering (AGG) and a supervised baseline
built upon WordNet (Fellbaum, 1998) (WN).
2 Method
2.1 Dataset and Feature Extraction
Our noun dataset used for clustering contains 2000
most frequent nouns in the BNC (Burnard, 2007).
979
Figure 1: Organisation of the hierarchical graph of concepts
Following previous semantic noun classification ex-
periments (Pantel and Lin, 2002; Bergsma et al,
2008), we use the grammatical relations (GRs)
as features for clustering. We extracted the fea-
tures from the Gigaword corpus (Graff et al,
2003), which was first parsed using the RASP
parser (Briscoe et al, 2006). The verb lemmas
in VERB?SUBJECT, VERB?DIRECT OBJECT and
VERB?INDIRECT OBJECT relations with the nouns
in the dataset were then extracted from the GR out-
put of the parser. The feature values were the relative
frequencies of the features.
2.2 Hierarchical Graph Factorization
Clustering
The most widely used method for hierarchical word
clustering is AGG (Schulte im Walde and Brew,
2001; Stevenson and Joanis, 2003; Ferrer, 2004;
Devereux and Costello, 2005). The method treats
each word as a singleton cluster and then succes-
sively merges two closest clusters until all the clus-
ters have been merged into one. The cluster simi-
larity is measured using linkage criteria (e.g. Ward
(1963) measures the decrease in variance for the
clusters being merged). As opposed to this, HGFC
derives probabilistic bipartite graphs from the sim-
ilarity matrix (Yu et al, 2006). Since we require a
graph of concepts, our task is rather different from
standard hierarchical word clustering that produces
a tree of concepts. In a tree, each word can only
have a unique parent cluster at each level. Our con-
cept graph does not have this constraint: at any level
a word can be associated with an arbitrary number
of parent clusters. Therefore, not only HGFC out-
performed agglomerative clustering methods in hi-
erarchical clustering tasks (Yu et al, 2006; Sun and
Korhonen, 2011), but its hierarchical graph output
is also a more suitable representation of the concept
graph. In addition, HGFC can detect the number of
levels and the number of clusters on each level of
the hierarchical graph automatically. This is essen-
tial for our task as these settings are difficult to pre-
define for a general-purpose concept graph.
Given a set of nouns, V = {vn}Nn=1, the similar-
ity matrix W for HGFC is constructed using Jensen-
Shannon Divergence. W can be encoded by an undi-
rected graph G (Figure 2(a)), where the nouns are
mapped to vertices and Wij is the edge weight be-
tween vertices i and j. The graph G and the clus-
ter structure can be represented by a bipartite graph
K(V,U). V are the vertices on G. U = {up}mp=1
represent the hidden m clusters. For example, look-
ing at Figure 2(b), V on G can be grouped into three
clusters u1, u2 and u3. The matrix B denotes the
n?m adjacency matrix, with bip being the connec-
tion weight between the vertex vi and the cluster up.
Thus, B represents the connections between clus-
ters at an upper and lower level of clustering. A
flat clustering algorithm can be induced by assign-
ing a lower level node to the parent node that has the
largest connection weight. The number of clusters
at any level can be determined by only counting the
number of non-empty nodes (namely the nodes that
have at least one lower level node associated).
The bipartite graph K also induces a similarity
(W ?) between vi and vj : w?ij =
?m
p=1
bipbjp
?p
=
(B??1BT )ij where ? = diag(?1, ..., ?m). There-
fore, B can be found by minimizing the divergence
distance (?) between the similarity matrices W and
W ?:
980
v1
v6
v2
v4
v3
v5
v7 v8v9
(a)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
(b)
u3
u1
u2
v1
v6
v2
v4
v3
v5
v7 v8v9
(c)
u1 u2
u3
(d)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
q1
q2
(e)
Figure 2: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters
on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1
min
H,?
?(W,H?HT ), s.t.
n?
i=1
hip = 1 (1)
H = B??1; ?(X,Y ) =
?
ij
(xij log
xij
yij
? xij + yij)
Yu et al (2006) showed that this cost function is
non-increasing under the update rule:
h?ip ? hip
?
j
wij
(H?HT )ij
?phjp s.t.
?
i
h?ip = 1 (2)
??p ? ?p
?
j
wij
(H?HT )ij
hiphjp s.t.
?
p
??p =
?
ij
wij (3)
The cost function can thus be optimized by updating
h and ? alternately.
The similarity between clusters p(up, uq) can be
induced from B, as follows:
p(up, uq) = p(up)p(up|uq) = (B
TD?1B)pq (4)
D = diag(d1, ..., dn) where di =
m?
p=0
bip
We can then construct a new graph G1 (Figure
2(d)) with the clusters U as vertices, and the clus-
ter similarity p as the connection weight. The clus-
tering algorithm can now be applied again (Figure
2(e)). This process can go on iteratively, leading to
a hierarchical graph.
The number of levels (L) and the number of
clusters (ml) are detected automatically, using the
method of Sun and Korhonen (2011). Clustering
starts with an initial setting of number of clusters
(m0) for the first level. In our experiment, we set the
value of m0 to 800. For the subsequent levels, ml
is set to the number of non-empty clusters (bipartite
graph nodes) on the parent level. The matrices B
and ? are initialized randomly. We found that the
actual initialization values have little impact on the
final result. The rows in B are normalized after the
initialization so the values in each row add up to one.
For a word vi, the probability of assigning it to clus-
ter x(l)p ? Xl at level l is given by:
p(x(l)p |vi) =
?
Xl?1
...
?
x(1)?X1
p(x(l)p |x
(l?1))...p(x(1)|vi)
= (D(?1)1 B1D
?1
2 B2...D
?1
l Bl)ip (5)
Due to the random walk property of the graph, ml
is non-increasing for higher levels (Sun and Korho-
nen, 2011). The algorithm can thus terminate when
all nouns are assigned to one cluster. We run 1000
iterations of updates of h and ? (equation 2 and 3)
for each two adjacent levels.
The resulting graph is composed of a set of bipar-
tite graphs defined by Bl, Bl?1, ..., B1. A bipartite
graph has a similar structure as in Figure 1. For a
given noun, we can rank the clusters at any level ac-
cording to the soft assignment probability (eq. 5).
The clusters that have no member noun were hidden
from the ranking since they do not explicitly repre-
sent any concept. However, these clusters are still
part of the organisation of conceptual space within
the model and they contribute to the probability for
the clusters on upper levels (eq. 5). We call the view
of the hierarchical graph where these empty clusters
981
are hidden an explicit graph. The whole algorithm
can be summarized as follows:
Require: N nouns V , initial number of clusters m1
Compute the similarity matrix W0 from V
Build the graph G0 from W0, l? 1
while ml > 1 do
FactorizeGl?1 to obtain bipartite graph Kl with the
adjacency matrix Bl (eq. 1, 2 and 3)
Build a graph Gl with similarity matrix Wl =
BTl D
?1
l Bl according to equation 4
l? l + 1 ; ml ? No. non-empty clusters (eq. 5)
end while
return Bl, Bl?1...B1
2.3 Identification of Metaphorical Associations
Once we obtained the explicit graph of concepts, we
can now identify metaphorical associations based on
the weights connecting the clusters at different levels
(eq. 5). Taking a single noun (e.g. fire) as input, the
system computes the probability of its cluster mem-
bership for each cluster at each level, using these
weights. We expect the cluster membership prob-
abilities to indicate the level of association of the
input noun with the clusters. The system can then
rank the clusters at each level based on these prob-
abilities. We chose level 3 as the optimal level of
generality for our experiments, based on our qualita-
tive analysis of the graph. The system selects 6 top-
ranked clusters from this level (we expect an average
source concept to have no more than 5 typical tar-
get associates) and excludes the literal cluster con-
taining the input concept (e.g. ?fire flame blaze?).
The remaining clusters represent the target concepts
associated with the input source concept. Example
output for the input concepts of fire and disease is
shown in Figure 3. One can see from the Figure
that each of the noun-to-cluster mappings represents
a new conceptual metaphor, e.g. EMOTION is FIRE,
VIOLENCE is FIRE, CRIME is a DISEASE etc. These
mappings are exemplified in language by a number
of metaphorical expressions (e.g. ?His anger will
burn him?, ?violence flared again?, ?it?s time they
found a cure for corruption?).
2.4 Identification of Salient Features and
Metaphorical Expressions
After extracting the source?target domain mappings,
we now move on to the identification of the cor-
SOURCE: fire
TARGET 1: sense hatred emotion passion enthusiasm
sentiment hope interest feeling resentment optimism
hostility excitement anger
TARGET 2: coup violence fight resistance clash rebel-
lion battle drive fighting riot revolt war confrontation
volcano row revolution struggle
TARGET 3: alien immigrant
TARGET 4: prisoner hostage inmate
SOURCE: disease
TARGET 1: fraud outbreak offense connection leak
count crime violation abuse conspiracy corruption ter-
rorism suicide
TARGET 2: opponent critic rival
TARGET 3: execution destruction signing
TARGET 4: refusal absence fact failure lack delay
Figure 3: Discovered metaphorical associations
rage-ncsubj engulf -ncsubj erupt-ncsubj burn-ncsubj
light-dobj consume-ncsubj flare-ncsubj sweep-ncsubj
spark-dobj battle-dobj gut-idobj smolder-ncsubj ig-
nite-dobj destroy-idobj spread-ncsubj damage-idobj
light-ncsubj ravage-ncsubj crackle-ncsubj open-dobj
fuel-dobj spray-idobj roar-ncsubj perish-idobj destroy-
ncsubj wound-idobj start-dobj ignite-ncsubj injure-
idobj fight-dobj rock-ncsubj retaliate-idobj devastate-
idobj blaze-ncsubj ravage-idobj rip-ncsubj burn-idobj
spark-ncsubj warm-idobj suppress-dobj rekindle-dobj
Figure 4: Salient features for fire and the violence cluster
responding metaphorical expressions. The system
does this by harvesting the salient features that lead
to the input noun being strongly associated with the
extracted clusters. The salient features are selected
by ranking the features according to the joint prob-
ability of the feature (f ) occurring both with the in-
put noun (w) and the cluster (c). Under a simplified
independence assumption, p(w, c|f) = p(w|f) ?
p(c|f). p(w|f) and p(c|f) are calculated as the ra-
tio of the frequency of the feature f to the total
frequency of the input noun and the cluster respec-
tively. The features ranked higher are expected to
represent the source domain vocabulary that can be
used to metaphorically describe the target concepts.
We selected the top 50 features from the ranked list.
Example features (verbs and their grammatical rela-
tions) extracted for the source domain noun fire and
the violence cluster are shown in Figure 4.
We then refined the lists of features by means of
selectional preference (SP) filtering. We use SPs to
982
FEELING IS FIRE
hope lit (Subj), anger blazed (Subj), optimism raged
(Subj), enthusiasm engulfed them (Subj), hatred flared
(Subj), passion flared (Subj), interest lit (Subj), fuel re-
sentment (Dobj), anger crackled (Subj), feelings roared
(Subj), hostility blazed (Subj), light with hope (Iobj)
CRIME IS A DISEASE
cure crime (Dobj), abuse transmitted (Subj), eradicate
terrorism (Dobj), suffer from corruption (Iobj), diag-
nose abuse (Dobj), combat fraud (Dobj), cope with
crime (Iobj), cure abuse (Dobj), eradicate corruption
Figure 5: Identified metaphorical expressions for the
mappings FEELING IS FIRE and CRIME IS A DISEASE
quantify how well the extracted features describe the
source domain (e.g. fire). We extracted nominal ar-
gument distributions of the verbs in our feature lists
for VERB?SUBJECT, VERB?DIRECT OBJECT and
VERB?INDIRECT OBJECT relations. We used the al-
gorithm of Sun and Korhonen (2009) to create SP
classes and the measure of Resnik (1993) to quantify
how well a particular argument class fits the verb.
Resnik measures selectional preference strength
SR(v) of a predicate as a Kullback-Leibler distance
between two distributions: the prior probability of
the noun class P (c) and the posterior probability
of the noun class given the verb P (c|v). SR(v) =
D(P (c|v)||P (c)) =
?
c P (c|v) log
P (c|v)
P (c) . In order
to quantify how well a particular argument class fits
the verb, Resnik defines selectional association as
AR(v, c) = 1SR(v)P (c|v) log
P (c|v)
P (c) . We rank the
nominal arguments of the verbs in our feature lists
using their selectional association with the verb, and
then only retain the features whose top 5 arguments
contain the source concept. For example, the verb
start, that is a common feature for both fire and the
violence cluster (e.g. ?start a war?, ?start a fire?),
would be filtered out in this way, whereas the verbs
flare or blaze would be retained as descriptive source
domain vocabulary.
We then search the RASP-parsed BNC for gram-
matical relations, in which the nouns from the target
domain cluster appear with the verbs from the source
domain vocabulary (e.g. ?war blazed? (subj), ?to
fuel violence? (dobj) for the mapping VIOLENCE is
FIRE). The system thus annotates metaphorical ex-
pressions in text, as well as the corresponding con-
ceptual metaphors, as shown in Figure 5.
3 Evaluation and Discussion
3.1 Baselines
AGG: the agglomerative clustering baseline is
constructed using SciPy implementation (Oliphant,
2007) of Ward?s linkage method (Ward, 1963). The
output tree is cut according to the number of lev-
els and the number of clusters of the explicit graph
detected by HGFC. The resulting tree is converted
into a graph by adding connections from each clus-
ter to all the clusters one level above. The connec-
tion weight (the cluster distance) is measured us-
ing Jensen-Shannon Divergence between the cluster
centroids. This graph is used in place of the HGFC
graph in the metaphor identification experiments.
WN: in the WN baseline, the WordNet hierarchy is
used as the underlying graph of concepts, to which
the metaphor extraction method is applied. Given
a source concept, the system extracts all its sense-
1 hypernyms two levels above and subsequently all
of their sister terms. The hypernyms themselves are
considered to represent the literal sense of the source
noun and are, therefore, removed. The sister terms
are kept as potential target domains.
3.2 Evaluation of Metaphorical Associations
To create our dataset, we extracted 10 common
source concepts that map to multiple targets from
the Master Metaphor List (Lakoff et al, 1991) and
linguistic analyses of metaphor (Lakoff and John-
son, 1980; Shutova and Teufel, 2010). These
included FIRE, CHILD, SPEED, WAR, DISEASE,
BREAKDOWN, CONSTRUCTION, VEHICLE, SYS-
TEM, BUSINESS. Each of the three systems identi-
fied 50 source?target domain mappings for the given
source domains, resulting in a set of 150 conceptual
metaphors (each representing a number of submap-
pings since all the target concepts are clusters or
synsets). These were then evaluated against human
judgements in two different experimental settings.
Setting 1: The judges were presented with a set
of conceptual metaphors identified by the three sys-
tems, randomized. They were asked to annotate the
mappings they considered valid. In all our experi-
ments, the judges were encouraged to rely on their
own intuition of metaphor, but they also reviewed
the metaphor annotation guidelines of Shutova and
Teufel (2010). Two independent judges, both na-
983
tive speakers of English, participated in this exper-
iment. Their agreement on the task was ? = 0.60
(n = 2, N = 150, k = 2) (Siegel and Castel-
lan, 1988). The main differences in the annotators?
judgements stem from the fact that some metaphor-
ical associations are less obvious and common than
others, and thus need more context (or imaginative
effort) to establish. Such examples, where the judges
disagreed included metaphorical mappings such as
INTENSITY is SPEED, GOAL is a CHILD, COLLEC-
TION is a SYSTEM, ILLNESS is a BREAKDOWN.
The system performance was then evaluated
against these judgements in terms of precision (P ),
i.e. the proportion of the valid metaphorical map-
pings among those identified. We calculated sys-
tem precision (in all experiments) as an average over
both annotations. HGFC operates with a precision of
P = 0.69, whereas the baselines attain P = 0.36
(AGG) and P = 0.29 (WN). The precision of an-
notator judgements against each other (the human
ceiling) is P = 0.80, suggesting that this is a chal-
lenging task.
Setting 2: To measure recall, R, of the systems we
asked two annotators (both native speakers with a
background in metaphor, different from Setting 1)
to write down up to 5 target concepts they strongly
associated with each of the 10 source concepts.
Their annotations were then aggregated into a sin-
gle metaphor association gold standard, consisting
of 63 mappings in total. The recall of the systems
was measured against this gold standard, resulting in
HGFC R = 0.61, AGG R = 0.11 and WN R = 0.03.
As expected, HGFC outperforms both AGG and
WN baselines in both settings. AGG has been pre-
viously shown to be less accurate than HGFC in the
verb clustering task (Sun and Korhonen, 2011). Our
analysis of the noun clusters indicated that HGFC
tends to produce more pure and complete clusters
than AGG. Another important reason AGG fails is
that it by definition organises all concepts into tree
and optimises its solution locally, taking into ac-
count a small number of clusters at a time. How-
ever, being able to discover connections between
more distant domains and optimising globally over
all concepts is crucial for metaphor identification.
This makes AGG less suitable for the task, as demon-
strated by our results. However, AGG identified a
number of interesting mappings missed by HGFC,
e.g. CAREER IS A CHILD, LANGUAGE IS A SYS-
TEM, CORRUPTION IS A VEHICLE, EMPIRE IS A
CONSTRUCTION, as well as a number of mappings
in common with HGFC, e.g. DEBATE IS A WAR, DE-
STRUCTION IS A DISEASE. The WN system also
identified a few interesting metaphorical mappings
(e.g. COGNITION IS FIRE, EDUCATION IS CON-
STRUCTION), but its output is largely dominated by
the concepts similar to the source noun and contains
some unrelated concepts. The comparison of HGFC
to WN shows that HGFC identifies meaningful prop-
erties and relations of abstract concepts that can not
be captured in a tree-like classification (even an ac-
curate, manually created one). The latter is more ap-
propriate for concrete concepts, and a more flexible
representation is needed to model abstract concepts.
The fact that both baselines identified some valid
metaphorical associations, relying on less suitable
conceptual graphs, suggests that our way of travers-
ing the graph is a viable approach in principle.
HGFC identifies valid metaphorical associations
for a range of source concepts. On of them (CRIME
IS A VIRUS) happened to have been already vali-
dated in psychological experiments (Thibodeau and
Boroditsky, 2011). The most frequent type of error
of HGFC is the presence of target clusters similar or
closely related to the source noun (e.g. the parent
cluster for child). The clusters from the same do-
main can, however, be filtered out if their nouns fre-
quently occur in the same documents with the source
noun (in a large corpus), i.e. by topical similarity.
The latter is less likely for the metaphorically con-
nected nouns. We intend to implement this improve-
ment in the future version of the system.
3.3 Evaluation of Metaphorical Expressions
For each of the identified conceptual metaphors, the
three systems extracted a number of metaphorical
expressions from the corpus (average of 430 for
HGFC, 148 for AGG, and 855 for WN). The ex-
pressions were also evaluated against human judge-
ments. The judges were presented with a set of ran-
domly sampled sentences containing metaphorical
expressions as annotated by the system and by the
baselines (200 each), randomized. They were asked
to mark the tagged expressions that were metaphor-
ical in their judgement as correct. Their agreement
on the task was ? = 0.56 (n = 2, N = 600, k = 2),
984
HLJ 26 [..] ?effective action? was needed to eradicate
terrorism, drug-trafficking and corruption.
EG0 275 In the 1930s the words ?means test? was a
curse, fuelling the resistance against it both among the
unemployed and some of its administrators.
CRX 1054 [..] if the rehabilitative approach were
demonstrably successful in curing crime.
HL3 1206 [..] he would strive to accelerate progress
towards the economic integration of the Caribbean.
HXJ 121 [..] it is likely that some industries will flour-
ish in certain countries as the market widens.
Figure 6: Metaphors tagged by the system (in bold)
whereby the main source of disagreement was the
presence of lexicalized metaphors, e.g. verbs such
as impose, decline etc. The system performance
against these annotations is P = 0.65 (HGFC), P =
0.47 (AGG) and P = 0.12 (WN). The human ceiling
for this task was measured at P = 0.79. Figure 6
shows example sentences annotated by HGFC. The
performance of our unsupervised approach is close
to the previous supervised systems of Mason (2004)
(accuracy of 0.73) and Shutova et al (2010) (preci-
sion of 0.79), however, the results are not directly
comparable due to different experimental settings.
The system errors in this task stem from multiple
word senses of the salient features or the source and
target sharing some physical properties (e.g. one can
?die from crime? and ?die from a disease?). Some
identified expressions invoke a chain of mappings
(e.g. ABUSE IS A DISEASE, DISEASE IS AN ENEMY
for ?combat abuse?), however, such chains are not
yet incorporated into the system. The performance
of AGG is higher than in the mappings identification
task, since it outputs only few expressions for the
incorrect mappings. In contrast, WN tagged a large
number of literal expressions due to the incorrect
prior identification of the underlying associations.
Since there is no large metaphor-annotated corpus
available, it was impossible for us to reliably evalu-
ate the recall of metaphorical expressions. However,
we estimated it as a recall of salient features. We
manually compiled sets of typical features for the
10 source domains, and measured their recall among
the top 50 HGFC features at R = 0.70. However, in
practice the coverage in this task would directly de-
pend on that of the metaphorical associations.
4 Related Work
One of the first attempts to identify and interpret
metaphorical expressions in text is the met* sys-
tem of Fass (1991), that utilizes hand-coded knowl-
edge and detects non-literalness via selectional pref-
erence violation. In case of a violation, the re-
spective phrase is first tested for being metonymic
using hand-coded patterns (e.g. CONTAINER-FOR-
CONTENT). If this fails, the system searches the
knowledge base for a relevant analogy in order to
discriminate metaphorical relations from anomalous
ones. The system of Krishnakumaran and Zhu
(2007) uses WordNet (the hyponymy relation) and
word bigram counts to predict verbal, nominal and
adjectival metaphors at the sentence level. The au-
thors discriminate between conventional metaphors
(included in WordNet) and novel metaphors. Birke
and Sarkar (2006) present a sentence clustering ap-
proach that employs a set of seed sentences an-
notated for literalness and computes similarity be-
tween the new input sentence and all of the seed sen-
tences. The system then tags the sentence as literal
or metaphorical according to the annotation in the
most similar seeds, attaining an f-score of 53.8%.
The first system to discover source?target domain
mappings automatically is CorMet (Mason, 2004).
It does this by searching for systematic variations
in domain-specific verb selectional preferences. For
example, pour is a characteristic verb in both LAB
and FINANCE domains. In the LAB domain it has
a strong preference for liquids and in the FINANCE
domain for money. From this the system infers the
domain mapping FINANCE ? LAB and the concept
mapping money ? liquid. Gedigian et al (2006)
trained a maximum entropy classifier to discrimi-
nate between literal and metaphorical use. They
annotated the sentences from PropBank (Kingsbury
and Palmer, 2002) containing the verbs of MOTION
and CURE for metaphoricity. They used PropBank
annotation (arguments and their semantic types) as
features for classification and report an accuracy
of 95.12% (however, against a majority baseline of
92.90%). The metaphor identification system of
Shutova et al (2010) starts from a small seed set
of metaphorical expressions, learns the analogies in-
volved in their production and extends the set of
analogies by means of verb and noun clustering. As
985
a result, the system can recognize new metaphorical
expressions in unrestricted text (e.g. from the seed
?stir excitement? it infers that ?swallow anger? is
also a metaphor), achieving a precision of 79%.
Turney et al (2011) classify verbs and adjectives
as literal or metaphorical based on their level of con-
creteness or abstractness in relation to a noun they
appear with. They learn concreteness rankings for
words automatically (starting from a set of exam-
ples) and then search for expressions where a con-
crete adjective or verb is used with an abstract noun
(e.g. ?dark humour? is tagged as a metaphor and
?dark hair? is not). They report an accuracy of 73%.
5 Conclusions and Future Directions
Previous research on metaphor addressed a num-
ber of different aspects of the phenomenon, and has
shown that these aspects can be successfully mod-
eled using statistical techniques. However, the meth-
ods often focused on a limited domain and needed
manually-labeled training data. This made them dif-
ficult to apply in a real-world setting with the goal of
improving semantic interpretation in NLP at large.
Our method takes a step towards this direction. It is
fully unsupervised, and thus more robust, and can
perform accurate metaphor identification in unre-
stricted text. It identifies metaphor with a precision
of 69% and a recall of 61%, which is a very encour-
aging result for an unsupervised method. We be-
lieve that this work has important implications for
computational and cognitive modeling of metaphor,
but is also applicable to a range of other seman-
tic tasks within NLP. Integrating different represen-
tations of abstract and concrete concepts into NLP
systems may improve their performance, as well as
make the models more cognitively plausible.
One of our key future research objectives is to in-
vestigate the use and adaptation of the created con-
ceptual graph to perform metaphor interpretation. In
addition, we plan to extend this work to cover nom-
inal and adjectival metaphors, by harvesting salient
nominal and adjectival features.
Acknowledgments
This work was funded by the MetaNet project (grant
number W911NF-12-C-0022) and the Dorothy
Hodgkin Postgraduate Award.
References
Roberta Adorni and Alice Mado Proverbio. 2012. The
neural manifestation of the word concreteness effect:
An electrical neuroimaging study. Neuropsychologia,
50(5):880 ? 891.
Rodrigo Agerri, John Barnden, Mark Lee, and Alan
Wallington. 2007. Metaphor, inference and domain-
independent mappings. In Proceedings of RANLP-
2007, pages 17?23, Borovets, Bulgaria.
John Barnden and Mark Lee. 2002. An artificial intel-
ligence approach to metaphor understanding. Theoria
et Historia Scientiarum, 6(1):399?412.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 59?68, Honolulu, Hawaii.
Jeffrey R. Binder, Chris F. Westbury, Kristen A. McKier-
nan, Edward T. Possing, and David A. Medler. 2005.
Distinct brain systems for processing concrete and ab-
stract concepts. Journal of Cognitive Neuroscience,
17(6):905?917.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In In Proceedings of EACL-06, pages
329?336.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions.
Lou Burnard. 2007. Reference Guide for the British Na-
tional Corpus (XML Edition).
Lynne Cameron. 2003. Metaphor in Educational Dis-
course. Continuum, London.
Sebastian J. Crutch and Elizabeth K. Warrington.
2005. Abstract and concrete concepts have struc-
turally different representational frameworks. Brain,
128(3):615?627.
Sebastian J Crutch and Elizabeth K Warrington. 2010.
The differential dependence of abstract and concrete
words upon associative and similarity-based informa-
tion: Complementary semantic interference and facil-
itation effects. Cognitive Neuropsychology, 27(1):46?
71.
Barry Devereux and Fintan Costello. 2005. Propane
stoves and gas lamps: How the concept hierarchy in-
fluences the interpretation of noun-noun compounds.
In Proceedings of the Twenty-Seventh Annual Confer-
ence of the Cognitive Science Society.
Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
986
Jerome Feldman and Srini Narayanan. 2004. Embodied
meaning in a neural theory of language. Brain and
Language, 89(2):385?392.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition.
Eva E. Ferrer. 2004. Towards a semantic classification of
spanish verbs based on subcategorisation information.
In Proceedings of the ACL 2004 workshop on Student
research, page 13. Association for Computational Lin-
guistics.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In In Proceed-
ings of the 3rd Workshop on Scalable Natural Lan-
guage Understanding, pages 41?48, New York.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2003. English gigaword. Linguistic Data Consortium,
Philadelphia.
Hsu-Wen Huang, Chia-Lin Lee, and Kara D. Federmeier.
2010. Imagine that! erps provide evidence for distinct
hemispheric contributions to the processing of con-
crete and abstract concepts. NeuroImage, 49(1):1116
? 1123.
Paul Kingsbury and Martha Palmer. 2002. From
TreeBank to PropBank. In Proceedings of LREC-
2002, pages 1989?1993, Gran Canaria, Canary Is-
lands, Spain.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, NY.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press, Chicago.
George Lakoff, Jane Espenson, and Alan Schwartz.
1991. The master metaphor list. Technical report,
University of California at Berkeley.
James Martin. 1990. A Computational Model of
Metaphor Interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
James Martin. 2006. A corpus-based analysis of con-
text effects on metaphor comprehension. In A. Ste-
fanowitsch and S. T. Gries, editors, Corpus-Based Ap-
proaches to Metaphor and Metonymy, Berlin. Mouton
de Gruyter.
Zachary Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Srini Narayanan. 1997. Knowledge-based Action Repre-
sentations for Metaphor and Aspect (KARMA). Tech-
nical report, PhD thesis, University of California at
Berkeley.
Srini Narayanan. 1999. Moving right along: A compu-
tational model of metaphoric reasoning about events.
In Proceedings of AAAI 99), pages 121?128, Orlando,
Florida.
Travis E. Oliphant. 2007. Python for scientific comput-
ing. Computing in Science and Engineering, 9:10?20.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 613?619. ACM.
Philip Resnik. 1993. Selection and Information: A
Class-based Approach to Lexical Relationships. Ph.D.
thesis, Philadelphia, PA, USA.
Sabine Schulte im Walde and Chris Brew. 2001. Induc-
ing German semantic verb classes from purely syntac-
tic subcategorisation information. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 223?230, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Ekaterina Shutova and Simone Teufel. 2010. Metaphor
corpus annotated for source - target domain map-
pings. In Proceedings of LREC 2010, pages 3255?
3261, Malta.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of Coling 2010, pages 1002?1010,
Beijing, China.
Ekaterina Shutova, Simone Teufel, and Anna Korhonen.
2012. Statistical Metaphor Processing. Computa-
tional Linguistics, 39(2).
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings of
NAACL 2010, pages 1029?1037, Los Angeles, USA.
Sidney Siegel and N. John Castellan. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill Book Company, New York, USA.
Gerard J. Steen, Aletta G. Dorst, J. Berenike Herrmann,
Anna A. Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identifica-
tion: From MIP to MIPVU. John Benjamins, Ams-
terdam/Philadelphia.
Suzanne Stevenson and Eric Joanis. 2003. Semi-
supervised verb class discovery using noisy features.
In Proceedings of HLT-NAACL 2003, pages 71?78.
Lin Sun and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired selectional
preferences. In Proceedings of EMNLP 2009, pages
638?647, Singapore, August.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP, pages 1023?1033, Edinburgh, UK.
987
Paul H. Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in rea-
soning. PLoS ONE, 6(2):e16782, 02.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 680?690, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joe H. Ward. 1963. Hierarchical grouping to optimize an
objective function. Journal of the American statistical
association, 58(301):236?244.
Katja Wiemer-Hastings and Xu Xu. 2005. Content Dif-
ferences for Abstract and Concrete Concepts. Cogni-
tive Science, 29(5):719?736.
Kai Yu, Shipeng Yu, and Volker Tresp. 2006. Soft
clustering on graphs. Advances in Neural Information
Processing Systems, 18.
988
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230?1238,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Modeling Semantic Relevance for Question-Answer Pairs
in Web Social Communities
Baoxun Wang, Xiaolong Wang, Chengjie Sun, Bingquan Liu, Lin Sun
School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China
{bxwang, wangxl, cjsun, liubq, lsun}@insun.hit.edu.cn
Abstract
Quantifying the semantic relevance be-
tween questions and their candidate an-
swers is essential to answer detection in
social media corpora. In this paper, a deep
belief network is proposed to model the
semantic relevance for question-answer
pairs. Observing the textual similarity
between the community-driven question-
answering (cQA) dataset and the forum
dataset, we present a novel learning strat-
egy to promote the performance of our
method on the social community datasets
without hand-annotating work. The ex-
perimental results show that our method
outperforms the traditional approaches on
both the cQA and the forum corpora.
1 Introduction
In natural language processing (NLP) and infor-
mation retrieval (IR) fields, question answering
(QA) problem has attracted much attention over
the past few years. Nevertheless, most of the QA
researches mainly focus on locating the exact an-
swer to a given factoid question in the related doc-
uments. The most well known international evalu-
ation on the factoid QA task is the Text REtrieval
Conference (TREC)1, and the annotated questions
and answers released by TREC have become im-
portant resources for the researchers. However,
when facing a non-factoid question such as why,
how, or what about, however, almost no automatic
QA systems work very well.
The user-generated question-answer pairs are
definitely of great importance to solve the non-
factoid questions. Obviously, these natural QA
pairs are usually created during people?s com-
munication via Internet social media, among
which we are interested in the community-driven
1http://trec.nist.gov
question-answering (cQA) sites and online fo-
rums. The cQA sites (or systems) provide plat-
forms where users can either ask questions or de-
liver answers, and best answers are selected man-
ually (e.g., Baidu Zhidao2 and Yahoo! Answers3).
Comparing with cQA sites, online forums have
more virtual society characteristics, where people
hold discussions in certain domains, such as tech-
niques, travel, sports, etc. Online forums contain
a huge number of QA pairs, and much noise infor-
mation is involved.
To make use of the QA pairs in cQA sites and
online forums, one has to face the challenging
problem of distinguishing the questions and their
answers from the noise. According to our investi-
gation, the data in the community based sites, es-
pecially for the forums, have two obvious charac-
teristics: (a) a post usually includes a very short
content, and when a person is initializing or re-
plying a post, an informal tone tends to be used;
(b) most of the posts are useless, which makes
the community become a noisy environment for
question-answer detection.
In this paper, a novel approach for modeling the
semantic relevance for QA pairs in the social me-
dia sites is proposed. We concentrate on the fol-
lowing two problems:
1. How to model the semantic relationship be-
tween two short texts using simple textual fea-
tures? As mentioned above, the user generated
questions and their answers via social media are
always short texts. The limitation of length leads
to the sparsity of the word features. In addition,
the word frequency is usually either 0 or 1, that is,
the frequency offers little information except the
occurrence of a word. Because of this situation,
the traditional relevance computing methods based
on word co-occurrence, such as Cosine similarity
and KL-divergence, are not effective for question-
2http://zhidao.baidu.com
3http://answers.yahoo.com
1230
answer semantic modeling. Most researchers try
to introduce structural features or users? behavior
to improve the models performance, by contrast,
the effect of textual features is not obvious.
2. How to train a model so that it has good per-
formance on both cQA and forum datasets? So
far, people have been doing QA researches on the
cQA and the forum datasets separately (Ding et
al., 2008; Surdeanu et al, 2008), and no one has
noticed the relationship between the two kinds of
data. Since both the cQA systems and the online
forums are open platforms for people to commu-
nicate, the QA pairs in the cQA systems have sim-
ilarity with those in the forums. In this case, it is
highly valuable and desirable to propose a train-
ing strategy to improve the model?s performance
on both of the two kinds of datasets. In addition,
it is possible to avoid the expensive and arduous
hand-annotating work by introducing the method.
To solve the first problem, we present a deep
belief network (DBN) to model the semantic rel-
evance between questions and their answers. The
network establishes the semantic relationship for
QA pairs by minimizing the answer-to-question
reconstructing error. Using only word features,
our model outperforms the traditional methods on
question-answer relevance calculating.
For the second problem, we make our model
to learn the semantic knowledge from the solved
question threads in the cQA system. Instead of
mining the structure based features from cQA
pages and forum threads individually, we con-
sider the textual similarity between the two kinds
of data. The semantic information learned from
cQA corpus is helpful to detect answers in forums,
which makes our model show good performance
on social media corpora. Thanks to the labels for
the best answers existing in the threads, no manual
work is needed in our strategy.
The rest of this paper is organized as follows:
Section 2 surveys the related work. Section 3 in-
troduces the deep belief network for answer de-
tection. In Section 4, the homogenous data based
learning strategy is described. Experimental result
is given in Section 5. Finally, conclusions and fu-
ture directions are drawn in Section 6.
2 Related Work
The value of the naturally generated question-
answer pairs has not been recognized until recent
years. Early studies mainly focus on extracting
QA pairs from frequently asked questions (FAQ)
pages (Jijkoun and de Rijke, 2005; Riezler et al,
2007) or service call-center dialogues (Berger et
al., 2000).
Judging whether a candidate answer is seman-
tically related to the question in the cQA page
automatically is a challenging task. A frame-
work for predicting the quality of answers has
been presented in (Jeon et al, 2006). Bernhard
and Gurevych (2009) have developed a transla-
tion based method to find answers. Surdeanu et
al. (2008) propose an approach to rank the an-
swers retrieved by Yahoo! Answers. Our work is
partly similar to Surdeanu et al (2008), for we also
aim to rank the candidate answers reasonably, but
our ranking algorithm needs only word informa-
tion, instead of the combination of different kinds
of features.
Because people have considerable freedom to
post on forums, there are a great number of irrel-
evant posts for answering questions, which makes
it more difficult to detect answers in the forums.
In this field, exploratory studies have been done by
Feng et al (2006) and Huang et al (2007), who ex-
tract input-reply pairs for the discussion-bot. Ding
et al(2008) and Cong et al(2008) have also pre-
sented outstanding research works on forum QA
extraction. Ding et al (2008) detect question con-
texts and answers using the conditional random
fields, and a ranking algorithm based on the au-
thority of forum users is proposed by Cong et al
(2008). Treating answer detection as a binary clas-
sification problem is an intuitive idea, thus there
are some studies trying to solve it from this view
(Hong and Davison, 2009; Wang et al, 2009). Es-
pecially Hong and Davison (2009) have achieved
a rather high precision on the corpora with less
noise, which also shows the importance of ?social?
features.
In order to select the answers for a given ques-
tion, one has to face the problem of lexical gap.
One of the problems with lexical gap embedding
is to find similar questions in QA achieves (Jeon et
al., 2005). Recently, the statistical machine trans-
lation (SMT) strategy has become popular. Lee et
al. (2008) use translate models to bridge the lexi-
cal gap between queries and questions in QA col-
lections. The SMT based methods are effective on
modeling the semantic relationship between ques-
tions and answers and expending users? queries in
answer retrieval (Riezler et al, 2007; Berger et al,
1231
2000; Bernhard and Gurevych, 2009). In (Sur-
deanu et al, 2008), the translation model is used
to provide features for answer ranking.
The structural features (e.g., authorship, ac-
knowledgement, post position, etc), also called
non-textual features, play an important role in an-
swer extraction. Such features are used in (Ding
et al, 2008; Cong et al, 2008), and have signifi-
cantly improved the performance. The studies of
Jeon et al (2006) and Hong et al (2009) show that
the structural features have even more contribution
than the textual features. In this case, the mining
of textual features tends to be ignored.
There are also some other research topics in this
field. Cong et al (2008) and Wang et al (2009)
both propose the strategies to detect questions in
the social media corpus, which is proved to be a
non-trivial task. The deep research on question
detection has been taken by Duan et al (2008).
A graph based algorithm is presented to answer
opinion questions (Li et al, 2009). In email sum-
marization field, the QA pairs are also extracted
from email contents as the main elements of email
summarization (Shrestha and McKeown, 2004).
3 The Deep Belief Network for QA pairs
Due to the feature sparsity and the low word fre-
quency of the social media corpus, it is difficult
to model the semantic relevance between ques-
tions and answers using only co-occurrence fea-
tures. It is clear that the semantic link exists be-
tween the question and its answers, even though
they have totally different lexical representations.
Thus a specially designed model may learn se-
mantic knowledge by reconstructing a great num-
ber of questions using the information in the cor-
responding answers. In this section, we propose
a deep belief network for modeling the seman-
tic relationship between questions and their an-
swers. Our model is able to map the QA data into
a low-dimensional semantic-feature space, where
a question is close to its answers.
3.1 The Restricted Boltzmann Machine
An ensemble of binary vectors can be modeled us-
ing a two-layer network called a ?restricted Boltz-
mann machine? (RBM) (Hinton, 2002). The di-
mension reducing approach based on RBM ini-
tially shows good performance on image process-
ing (Hinton and Salakhutdinov, 2006). Salakhut-
dinov and Hinton (2009) propose a deep graphical
model composed of RBMs into the information re-
trieval field, which shows that this model is able to
obtain semantic information hidden in the word-
count vectors.
As shown in Figure 1, the RBM is a two-layer
network. The bottom layer represents a visible
vector v and the top layer represents a latent fea-
ture h. The matrix W contains the symmetric in-
teraction terms between the visible units and the
hidden units. Given an input vector v, the trained
Figure 1: Restricted Boltzmann machine
RBM model provides a hidden feature h, which
can be used to reconstruct v with a minimum er-
ror. The training algorithm for this paper will be
described in the next subsection. The ability of the
RBM suggests us to build a deep belief network
based on RBM so that the semantic relevance be-
tween questions and answers can be modeled.
3.2 Pretraining a Deep Belief Network
In the social media corpora, the answers are al-
ways descriptive, containing one or several sen-
tences. Noticing that an answer has strong seman-
tic association with the question and involves more
information than the question, we propose to train
a deep belief network by reconstructing the ques-
tion using its answers. The training object is to
minimize the error of reconstruction, and after the
pretraining process, a point that lies in a good re-
gion of parameter space can be achieved.
Firstly, the illustration of the DBN model is
given in Figure 2. This model is composed of
three layers, and here each layer stands for the
RBM or its variant. The bottom layer is a variant
form of RBM?s designed for the QA pairs. This
layer we design is a little different from the classi-
cal RBM?s, so that the bottom layer can generate
the hidden features according to the visible answer
vector and reconstruct the question vector using
the hidden features. The pre-training procedure of
this architecture is practically convergent. In the
bottom layer, the binary feature vectors based on
the statistics of the word occurrence in the answers
are used to compute the ?hidden features? in the
1232
Figure 2: The Deep Belief Network for QA Pairs
hidden units. The model can reconstruct the ques-
tions using the hidden features. The processes can
be modeled as follows:
p(h j = 1|a) = ?(b j +
?
i
wi jai) (1)
p(qi = 1|h) = ?(bi +
?
j
wi jh j) (2)
where ?(x) = 1/(1 + e?x), a denotes the visible
feature vector of the answer, qi is the ith element
of the question vector, and h stands for the hid-
den feature vector for reconstructing the questions.
wi j is a symmetric interaction term between word
i and hidden feature j, bi stands for the bias of the
model for word i, and b j denotes the bias of hidden
feature j.
Given the training set of answer vectors, the bot-
tom layer generates the corresponding hidden fea-
tures using Equation 1. Equation 2 is used to re-
construct the Bernoulli rates for each word in the
question vectors after stochastically activating the
hidden features. Then Equation 1 is taken again
to make the hidden features active. We use 1-step
Contrastive Divergence (Hinton, 2002) to update
the parameters by performing gradient ascent:
?wi j = (< qih j >qData ? < qih j >qRecon) (3)
where < qih j >qData denotes the expectation of
the frequency with which the word i in a ques-
tion and the feature j are on together when the
hidden features are driven by the question data.
< qih j >qRecon defines the corresponding expec-
tation when the hidden features are driven by the
reconstructed question data.  is the learning rate.
The classical RBM structure is taken to build
the middle layer and the top layer of the network.
The training method for the higher two layer is
similar to that of the bottom one, and we only have
to make each RBM to reconstruct the input data
using its hidden features. The parameter updates
still obeying the rule defined by gradient ascent,
which is quite similar to Equation 3. After train-
ing one layer, the h vectors are then sent to the
higher-level layer as its ?training data?.
3.3 Fine-tuning the Weights
Notice that a greedy strategy is taken to train each
layer individually during the pre-training proce-
dure, it is necessary to fine-tune the weights of the
entire network for optimal reconstruction. To fine-
tune the weights, the network is unrolled, taking
the answers as the input data to generate the corre-
sponding questions at the output units. Using the
cross-entropy error function, we can then tune the
network by performing backpropagation through
it. The experiment results in section 5.2 will show
fine-tuning makes the network performs better for
answer detection.
3.4 Best answer detection
After pre-training and fine-tuning, a deep belief
network for QA pairs is established. To detect the
best answer to a given question, we just have to
send the vectors of the question and its candidate
answers into the input units of the network and
perform a level-by-level calculation to obtain the
corresponding feature vectors. Then we calculate
the distance between the mapped question vector
and each candidate answer vector. We consider the
candidate answer with the smallest distance as the
best one.
4 Learning with Homogenous Data
In this section, we propose our strategy to make
our DBN model to detect answers in both cQA and
forum datasets, while the existing studies focus on
one single dataset.
4.1 Homogenous QA Corpora from Different
Sources
Our motivation of finding the homogenous
question-answer corpora from different kind of so-
cial media is to guarantee the model?s performance
and avoid hand-annotating work.
In this paper, we get the ?solved question? pages
in the computer technology domain from Baidu
Zhidao as the cQA corpus, and the threads of
1233
Figure 3: Comparison of the post content lengths in the cQA and the forum datasets
ComputerFansClub Forum4 as the online forum
corpus. The domains of the corpora are the same.
To further explain that the two corpora are ho-
mogenous, we will give the detail comparison on
text style and word distribution.
As shown in Figure 3, we have compared the
post content lengths of the cQA and the forum
in our corpora. For the comparison, 5,000 posts
from the cQA corpus and 5,000 posts from the fo-
rum corpus are randomly selected. The left panel
shows the statistical result on the Baidu Zhidao
data, and the right panel shows the one on the fo-
rum data. The number i on the horizontal axis de-
notes the post contents whose lengths range from
10(i? 1) + 1 to 10i bytes, and the vertical axis rep-
resents the counts of the post contents. From Fig-
ure 3 we observe that the contents of most posts
in both the cQA corpus and the forum corpus are
short, with the lengths not exceeding 400 bytes.
The content length reflects the text style of the
posts in cQA systems and online forums. From
Figure 3 it can be also seen that the distributions
of the content lengths in the two figures are very
similar. It shows that the contents in the two cor-
pora are both mainly short texts.
Figure 4 shows the percentage of the concurrent
words in the top-ranked content words with high
frequency. In detail, we firstly rank the words by
frequency in the two corpora. The words are cho-
sen based on a professional dictionary to guarantee
that they are meaningful in the computer knowl-
edge field. The number k on the horizontal axis in
Figure 4 represents the top k content words in the
4http://bbs.cfanclub.net/
corpora, and the vertical axis stands for the per-
centage of the words shared by the two corpora in
the top k words.
Figure 4: Distribution of concurrent content words
Figure 4 shows that a large number of meaning-
ful words appear in both of the two corpora with
high frequencies. The percentage of the concur-
rent words maintains above 64% in the top 1,400
words. It indicates that the word distributions of
the two corpora are quite similar, although they
come from different social media sites.
Because the cQA corpus and the forum corpus
used in this study have homogenous characteris-
tics for answer detecting task, a simple strategy
may be used to avoid the hand-annotating work.
Apparently, in every ?solved question? page of
Baidu Zhidao, the best answer is selected by the
user who asks this question. We can easily extract
the QA pairs from the cQA corpus as the training
1234
set. Because the two corpora are similar, we can
apply the deep belief network trained by the cQA
corpus to detect answers on both the cQA data and
the forum data.
4.2 Features
The task of detecting answers in social media cor-
pora suffers from the problem of feature sparsity
seriously. High-dimensional feature vectors with
only several non-zero dimensions bring large time
consumption to our model. Thus it is necessary to
reduce the dimension of the feature vectors.
In this paper, we adopt two kinds of word fea-
tures. Firstly, we consider the 1,300 most fre-
quent words in the training set as Salakhutdinov
and Hinton (2009) did. According to our statis-
tics, the frequencies of the rest words are all less
then 10, which are not statistically significant and
may introduce much noise.
We take the occurrence of some function words
as another kind of features. The function words
are quite meaningful for judging whether a short
text is an answer or not, especially for the non-
factoid questions. For example, in the answers to
the causation questions, the words such as because
and so are more likely to appear; and the words
such as firstly, then, and should may suggest the
answers to the manner questions. We give an ex-
ample for function word selection in Figure 5.
Figure 5: An example for function word selection
For this reason, we collect 200 most frequent
function words in the answers of the training set.
Then for every short text, either a question or an
answer, a 1,500-dimensional vector can be gener-
ated. Specifically, all the features we have adopted
are binary, for they only have to denote whether
the corresponding word appears in the text or not.
5 Experiments
To evaluate our question-answer semantic rele-
vance computing method, we compare our ap-
proach with the popular methods on the answer
detecting task.
5.1 Experiment Setup
Architecture of the Network: To build the deep
belief network, we use a 1500-1500-1000-600 ar-
chitecture, which means the three layers of the net-
work have individually 1,500?1,500, 1,500?1,000
and 1,000?600 units. Using the network, a 1,500-
dimensional binary vector is finally mapped to a
600-dimensional real-value vector.
During the pretraining stage, the bottom layer
is greedily pretrained for 200 passes through the
entire training set, and each of the rest two layers is
greedily pretrained for 50 passes. For fine-tuning
we apply the method of conjugate gradients5, with
three line searches performed in each pass. This
algorithm is performed for 50 passes to fine-tune
the network.
Dataset: we have crawled 20,000 pages of
?solved question? from the computer and network
category of Baidu Zhidao as the cQA corpus. Cor-
respondingly we obtain 90,000 threads from Com-
puterFansClub, which is an online forum on com-
puter knowledge. We take the forum threads as
our forum corpus.
From the cQA corpus, we extract 12,600 human
generated QA pairs as the training set without any
manual work to label the best answers. We get the
contents from another 2,000 cQA pages to form
a testing set, each content of which includes one
question and 4.5 candidate answers on average,
with one best answer among them. To get another
testing dataset, we randomly select 2,000 threads
from the forum corpus. For this training set, hu-
man work are necessary to label the best answers
in the posts of the threads. There are 7 posts in-
cluded in each thread on average, among which
one question and at least one answer exist.
Baseline: To show the performance of our
method, three main popular relevance computing
methods for ranking candidate answers are con-
sidered as our baselines. We will briefly introduce
them:
Cosine Similarity. Given a question q and its
candidate answer a, their cosine similarity can be
computed as follows:
cos(q, a) =
?n
k=1 wqk ? wak??n
k=1 w2qk ?
??n
k=1 w2ak
(4)
where wqk and wak stand for the weight of the kth
word in the question and the answer respectively.
5Code is available at
http://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/
1235
The weights can be get by computing the product
of term frequency (tf ) and inverse document fre-
quency (idf )
HowNet based Similarity. HowNet6 is an elec-
tronic world knowledge system, which serves as
a powerful tool for meaning computation in hu-
man language technology. Normally the similar-
ity between two passages can be calculated by
two steps: (1) matching the most semantic-similar
words in each passages greedily using the API?s
provided by HowNet; (2) computing the weighted
average similarities of the word pairs. This strat-
egy is taken as a baseline method for computing
the relevance between questions and answers.
KL-divergence Language Model. Given a ques-
tion q and its candidate answer a, we can con-
struct unigram language model Mq and unigram
language model Ma. Then we compute KL-
divergence between Mq and Ma as below:
KL(Ma||Mq) =
?
w
p(w|Ma) log(p(w|Ma)/p(w|Mq))
(5)
5.2 Results and Analysis
We evaluate the performance of our approach for
answer detection using two metrics: Precision@1
(P@1) and Mean Reciprocal Rank (MRR). Ap-
plying the two metrics, we perform the baseline
methods and our DBN based methods on the two
testing set above.
Table 1 lists the results achieved on the forum
data using the baseline methods and ours. The ad-
ditional ?Nearest Answer? stands for the method
without any ranking strategies, which returns the
nearest candidate answer from the question by po-
sition. To illustrate the effect of the fine-tuning for
our model, we list the results of our method with-
out fine-tuning and the results with fine-tuning.
As shown in Table 1, our deep belief network
based methods outperform the baseline methods
as expected. The main reason for the improve-
ments is that the DBN based approach is able to
learn semantic relationship between the words in
QA pairs from the training set. Although the train-
ing set we offer to the network comes from a dif-
ferent source (the cQA corpus), it still provide
enough knowledge to the network to perform bet-
ter than the baseline methods. This phenomena in-
dicates that the homogenous corpora for training is
6Detail information can be found in:
http://www.keenage.com/
effective and meaningful.
Method P@1 (%) MRR (%)
Nearest Answer 21.25 38.72
Cosine Similarity 23.15 43.50
HowNet 22.55 41.63
KL divergence 25.30 51.40
DBN (without FT) 41.45 59.64
DBN (with FT) 45.00 62.03
Table 1: Results on Forum Dataset
We have also investigated the reasons for the un-
satisfying performance of the baseline approaches.
Basically, the low precision is ascribable to the
forum corpus we have obtained. As mentioned
in Section 1, the contents of the forum posts are
short, which leads to the sparsity of the features.
Besides, when users post messages in the online
forums, they are accustomed to be casual and use
some synonymous words interchangeably in the
posts, which is believed to be a significant situ-
ation in Chinese forums especially. Because the
features for QA pairs are quite sparse and the con-
tent words in the questions are usually morpholog-
ically different from the ones with the same mean-
ing in the answers, the Cosine Similarity method
become less powerful. For HowNet based ap-
proaches, there are a large number of words not
included by HowNet, thus it fails to compute the
similarity between questions and answers. KL-
divergence suffers from the same problems with
the Cosine Similarity method. Compared with
the Cosine Similarity method, this approach has
achieved the improvement of 9.3% in P@1, but
it performs much better than the other baseline
methods in MRR.
The baseline results indicate that the online fo-
rum is a complex environment with large amount
of noise for answer detection. Traditional IR
methods using pure textual features can hardly
achieve good results. The similar baseline results
for forum answer ranking are also achieved by
Hong and Davison (2009), which takes some non-
textual features to improve the algorithm?s perfor-
mance. We also notice that, however, the baseline
methods have obtained better results on forum cor-
pus (Cong et al, 2008). One possible reason is that
the baseline approaches are suitable for their data,
since we observe that the ?nearest answer? strat-
egy has obtained a 73.5% precision in their work.
Our model has achieved the precision of
1236
45.00% in P@1 and 62.03% in MRR for answer
detecting on forum data after fine-tuning, while
some related works have reported the results with
the precision over 90% (Cong et al, 2008; Hong
and Davison, 2009). There are mainly two rea-
sons for this phenomena: Firstly, both of the pre-
vious works have adopt non-textual features based
on the forum structure, such as authorship, po-
sition and quotes, etc. The non-textual (or so-
cial based) features have played a significant role
in improving the algorithms? performance. Sec-
ondly, the quality of corpora influences the results
of the ranking strategies significantly, and even
the same algorithm may perform differently when
the dataset is changed (Hong and Davison, 2009).
For the experiments of this paper, large amount of
noise is involved in the forum corpus and we have
done nothing extra to filter it.
Table 2 shows the experimental results on the
cQA dataset. In this experiment, each sample is
composed of one question and its following sev-
eral candidate answers. We delete the ones with
only one answer to confirm there are at least two
candidate answers for each question. The candi-
date answers are rearranged by post time, so that
the real answers do not always appear next to the
questions. In this group of experiment, no hand-
annotating work is needed because the real an-
swers have been labeled by cQA users.
Method P@1 (%) MRR (%)
Nearest Answer 36.05 56.33
Cosine Similarity 44.05 62.84
HowNet 41.10 58.75
KL divergence 43.75 63.10
DBN (without FT) 56.20 70.56
DBN (with FT) 58.15 72.74
Table 2: Results on cQA Dataset
From Table 2 we observe that all the approaches
perform much better on this dataset. We attribute
the improvements to the high quality QA corpus
Baidu Zhidao offers: the candidate answers tend to
be more formal than the ones in the forums, with
less noise information included. In addition, the
?Nearest Answer? strategy has reached 36.05% in
P@1 on this dataset, which indicates quite a num-
ber of askers receive the real answers at the first
answer post. This result has supported the idea of
introducing position features. What?s more, if the
best answer appear immediately, the asker tends
to lock down the question thread, which helps to
reduce the noise information in the cQA corpus.
Despite the baseline methods? performances
have been improved, our approaches still outper-
form them, with a 32.0% improvement in P@1
and a 15.3% improvement in MRR at least. On
the cQA dataset, our model shows better perfor-
mance than the previous experiment, which is ex-
pected because the training set and the testing set
come from the same corpus, and the DBN model
is more adaptive to the cQA data.
We have observed that, from both of the two
groups of experiments, fine-tuning is effective for
enhancing the performance of our model. On the
forum data, the results have been improved by
8.6% in P@1 and 4.0% in MRR, and the improve-
ments are 3.5% and 3.1% individually.
6 Conclusions
In this paper, we have proposed a deep belief net-
work based approach to model the semantic rel-
evance for the question answering pairs in social
community corpora.
The contributions of this paper can be summa-
rized as follows: (1) The deep belief network we
present shows good performance on modeling the
QA pairs? semantic relevance using only word fea-
tures. As a data driven approach, our model learns
semantic knowledge from large amount of QA
pairs to represent the semantic relevance between
questions and their answers. (2) We have stud-
ied the textual similarity between the cQA and the
forum datasets for QA pair extraction, and intro-
duce a novel learning strategy to make our method
show good performance on both cQA and forum
datasets. The experimental results show that our
method outperforms the traditional approaches on
both the cQA and the forum corpora.
Our future work will be carried out along two
directions. Firstly, we will further improve the
performance of our method by adopting the non-
textual features. Secondly, more research will be
taken to put forward other architectures of the deep
networks for QA detection.
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. Special
thanks to Deyuan Zhang, Bin Liu, Beidong Liu
and Ke Sun for insightful suggestions. This work
is supported by NSFC (60973076).
1237
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In In Proceedings of the 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 192?199.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining lexical semantic resources with question &
answer archives for translation-based answer find-
ing. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 728?736, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ?08: Proceed-
ings of the 31st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 467?474, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL-08: HLT, pages
710?718, Columbus, Ohio, June. Association for
Computational Linguistics.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of
ACL-08: HLT, pages 156?164, Columbus, Ohio,
June. Association for Computational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Ccile Paris and Candace L. Sidner, editors, IUI,
pages 171?177. ACM.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Georey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering
in discussion boards. In SIGIR ?09: Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 171?178, New York, NY, USA. ACM.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence,
pages 423?428, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM ?05, pages 84?90, New
York, NY, USA. ACM.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In SIGIR ?06,
pages 228?235, New York, NY, USA. ACM.
Valentin Jijkoun and Maarten de Rijke. 2005. Retriev-
ing answers from frequently asked questions pages
on the web. In CIKM ?05, pages 76?83, New York,
NY, USA. ACM.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging lexical gaps be-
tween queries and questions on large online q&a
collections with compact translation models. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 410?418, Morristown, NJ, USA. Association
for Computational Linguistics.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
737?745, Suntec, Singapore, August. Association
for Computational Linguistics.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 464?471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Semantic hashing. Int. J. Approx. Reasoning,
50(7):969?978.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of Coling 2004, pages 889?
895, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online QA collections. In Proceedings of ACL-08:
HLT, pages 719?727, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiao-
long Wang, and Lin Sun. 2009. Extracting chinese
question-answer pairs from online forums. In SMC
2009: Proceedings of the IEEE International Con-
ference on Systems, Man and Cybernetics, 2009.,
pages 1159?1164.
1238
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736?741,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Diathesis alternation approximation for verb clustering
Lin Sun
Greedy Intelligence Ltd
Hangzhou, China
lin.sun@greedyint.com
Diana McCarthy and Anna Korhonen
DTAL and Computer Laboratory
University of Cambridge
Cambridge, UK
diana@dianamccarthy.co.uk
alk23@cam.ac.uk
Abstract
Although diathesis alternations have been
used as features for manual verb clas-
sification, and there is recent work on
incorporating such features in computa-
tional models of human language acquisi-
tion, work on large scale verb classifica-
tion has yet to examine the potential for
using diathesis alternations as input fea-
tures to the clustering process. This pa-
per proposes a method for approximating
diathesis alternation behaviour in corpus
data and shows, using a state-of-the-art
verb clustering system, that features based
on alternation approximation outperform
those based on independent subcategoriza-
tion frames. Our alternation-based ap-
proach is particularly adept at leveraging
information from less frequent data.
1 Introduction
Diathesis alternations (DAs) are regular alterna-
tions of the syntactic expression of verbal argu-
ments, sometimes accompanied by a change in
meaning. For example, The man broke the win-
dow ? The window broke. The syntactic phe-
nomena are triggered by the underlying semantics
of the participating verbs. Levin (1993)?s seminal
book provides a manual inventory both of DAs and
verb classes where membership is determined ac-
cording to participation in these alternations. For
example, most of the COOK verbs (e.g. bake,
cook, fry . . . ) can all take various DAs, such as
the causative alternation, middle alternation and
instrument subject alternation.
In computational linguistics, work inspired by
Levin?s classification has exploited the link be-
tween syntax and semantics for producing clas-
sifications of verbs. Such classifications are use-
ful for a wide variety of purposes such as se-
mantic role labelling (Gildea and Jurafsky, 2002),
predicting unseen syntax (Parisien and Steven-
son, 2010), argument zoning (Guo et al, 2011)
and metaphor identification (Shutova et al, 2010).
While Levin?s classification can be extended man-
ually (Kipper-Schuler, 2005), a large body of re-
search has developed methods for automatic verb
classification since such methods can be applied
easily to other domains and languages.
Existing work on automatic classification relies
largely on syntactic features such as subcatego-
rization frames (SCF)s (Schulte im Walde, 2006;
Sun and Korhonen, 2011; Vlachos et al, 2009;
Brew and Schulte im Walde, 2002). There has also
been some success incorporating selectional pref-
erences (Sun and Korhonen, 2009).
Few have attempted to use, or approximate,
diathesis features directly for verb classification
although manual classifications have relied on
them heavily, and there has been related work on
identifying the DAs themselves automatically us-
ing SCF and semantic information (Resnik, 1993;
McCarthy and Korhonen, 1998; Lapata, 1999;
McCarthy, 2000; Tsang and Stevenson, 2004).
Exceptions to this include Merlo and Stevenson
(2001), Joanis et al (2008) and Parisien and
Stevenson (2010, 2011). Merlo and Stevenson
(2001) used cues such as passive voice, animacy
and syntactic frames coupled with the overlap
of lexical fillers between the alternating slots to
predict a 3-way classification (unergative, unac-
cusative and object-drop). Joanis et al (2008)
used similar features to classify verbs on a much
larger scale. They classify up to 496 verbs us-
ing 11 different classifications each having be-
tween 2 and 14 classes. Parisien and Steven-
son (2010, 2011) used hierarchical Bayesian mod-
els on slot frequency data obtained from child-
directed speech parsed with a dependency parser
to model acquisition of SCF, alternations and ul-
timately verb classes which provided predictions
for unseen syntactic behaviour of class members.
736
Frame Example sentence Freq
NP+PPon Jessica sprayed paint on the wall 40
NP+PPwith Jessica sprayed the wall with paint 30
PPwith *The wall sprayed with paint 0
PPon Jessica sprayed paint on the wall 30
Table 1: Example frames for verb spray
In this paper, like Sun and Korhonen (2009);
Joanis et al (2008) we seek to automatically clas-
sify verbs into a broad range of classes. Like Joa-
nis et al, we include evidence of DA, but we do
not manually select features attributed to specific
alternations but rather experiment with syntactic
evidence for alternation approximation. We use
the verb clustering system presented in Sun and
Korhonen (2009) because it achieves state-of-the-
art results on several datasets, including those of
Joanis et al, even without the additional boost in
performance from the selectional preference data.
We are interested in the improvement that can be
achieved to verb clustering using approximations
for DAs, rather than the DA per se. As such we
make the simple assumption that if a pair of SCFs
tends to occur with the same verbs, we have a po-
tential occurrence of DA. Although this approx-
imation can give rise to false positives (pairs of
frames that co-occur frequently but are not DA)
we are nevertheless interested in investigating its
potential usefulness for verb classification. One
attractive aspect of this method is that it does not
require a pre-defined list of possible alternations.
2 Diathesis Alternation Approximation
A DA can be approximated by a pair of SCFs.
We parameterize frames involving prepositional
phrases with the preposition. Example SCFs for
the verb ?spray? are shown in Table 1. The feature
value of a single frame feature is the frequency
of the SCF. Given two frames fv(i), fv(j) of a
verb v, they can be transformed into a feature pair
(fv(i), fv(j)) as an approximation to a DA. The
feature value of the DA feature (fv(i), fv(j)) is ap-
proximated by the joint probability of the pair of
frames p(fv(i), fv(j)|v), obtained by integrating
all the possible DAs. The key assumption is that
the joint probability of two SCFs has a strong cor-
relation with a DA on the grounds that the DA gives
rise to both SCFs in the pair. We use the DA feature
(fv(i), fv(j)) with its value p(fv(i), fv(j)|v) as a
new feature for verb clustering. As a comparison
point, we can ignore the DA and make a frame in-
dependence assumption. The joint probability is
decomposed as:
p(fv(i), fv(j)|v)? , p(fv(i)|v) ? p(fv(j)|v) (1)
We assume that SCFs are dependent as they are
generated by the underlying meaning components
(Levin and Hovav, 2006). The frame dependency
is represented by a simple graphical model in fig-
ure 1.
Figure 1: Graphical model for the joint probability of pairs of
frames. v represents a verb, a represents a DA and f repre-
sents a specific frame in total of M possible frames
In the data, the verb (v) and frames (f ) are ob-
served, and any underlying alternation (a) is hid-
den. The aim is to approximate but not to detect a
DA, so a is summed out:
p(fv(i), fv(j)|v) =
?
a
p(fv(i), fv(j)|a) ? p(a|v)
(2)
In order to evaluate this sum, we use a relaxation
1: the sum in equation 1 is replaced with the max-
imum (max). This is a reasonable relaxation, as a
pair of frames rarely participates in more than one
type of a DA.
p(fv(i), fv(j)|v) ? max(p(fv(i), fv(j)|a)?p(a|v))
(3)
The second relaxation further relaxes the first one
by replacing the max with the least upper bound
(sup): If fv(i) occurs a times, fv(j) occurs b times
and b < a, the number of times that a DA occurs
between fv(i) and fv(j) must be smaller or equal
to b.
p(fv(i), fv(j)|v) ? sup{p(fv(i), fv(j)|a)} ? sup{p(a|v)}
(4)
sup{p(fv(i), fv(j)|a)} = Z?1 ?min(fv(i), fv(j))
sup{p(a|v)} = 1
Z =
?
m
?
n
min(fv(m), fv(n))
1A relaxation is used in mathematical optimization for re-
laxing the strict requirement, by either substituting it with an
easier requirement or dropping it completely.
737
Frame pair Possible DA Frequency
NP+PPon NP+PPwith Locative 30
NP+PPon PPwith Causative(with) 0
NP+PPon PPon Causative(on) 30
NP+PPwith PPwith ? 0
NP+PPwith PPon ? 30
PPwith PPon ? 0
NP+PPon NP+PPon - 40
NP+PPwith NP+PPwith - 30
PPwith PPwith - 0
PPon PPon - 30
Table 2: Example frame pair features for spray
So we end up with a simple form:
p(fv(i), fv(j)|v) ? Z?1 ?min(fv(i), fv(j)) (5)
The equation is intuitive: If fv(i) occurs 40 times
and fv(j) 30 times, the DA between fv(i) and
fv(j) ? 30 times. This upper bound value is used
as the feature value of the DA feature. The original
feature vector f of dimension M is transformed
into M2 dimensions feature vector f? . Table 2
shows the transformed feature space for spray.
The feature space matches our expectation well:
valid DAs have a value greater than 0 and invalid
DAs have a value of 0.
3 Experiments
We evaluated this model by performing verb clus-
tering experiments using three feature sets:
F1: SCF parameterized with preposition. Exam-
ples are shown in Table 1.
F2: The frame pair features built from F1 with
the frame independence assumption (equa-
tion 1). This feature is not a DA feature as
it ignores the inter-dependency of the frames.
F3: The frame pair features (DAs) built from
F1 with the frame dependency assumption
(equation 4). This is the DA feature which
considers the correlation of the two frames
which are generated from the alternation.
F3 implicitly includes F1, as a frame can pair
with itself. 2 In the example in Table 2, the frame
pair ?PP(on) PP(on)? will always have the same
value as the ?PP(on)? frame in F1.
We extracted the SCFs using the system of
Preiss et al (2007) which classifies each corpus
2We did this so that F3 included the SCF features as well
as the DA approximation features. It would be possible in
future work to exclude the pairs involving identical frames,
thereby relying solely on the DA approximations, and com-
pare performance with the results obtained here.
occurrence of a verb as a member of one of the 168
SCFs on the basis of grammatical relations iden-
tified by the RASP (Briscoe et al, 2006) parser.
We experimented with two datasets that have been
used in prior work on verb clustering: the test sets
7-11 (3-14 classes) in Joanis et al (2008), and the
17 classes set in Sun et al (2008).
We used the spectral clustering (SPEC) method
and settings as in Sun and Korhonen (2009) but
adopted the Bhattacharyya kernel (Jebara and
Kondor, 2003) to improve the computational effi-
ciency of the approach given the high dimension-
ality of the quadratic feature space.
wb(v, v?) =
D?
d=1
(vdv?d)1/2 (6)
The mean-filed bound of the Bhattacharyya kernel
is very similar to the KL divergence kernel (Jebara
et al, 2004) which is frequently used in verb clus-
tering experiments (Korhonen et al, 2003; Sun
and Korhonen, 2009).
To further reduce computational complexity, we
restricted our scope to the more frequent features.
In the experiment described in this section we used
the 50 most frequent features for the 3-6 way clas-
sifications (Joanis et al?s test set 7-9) and 100 fea-
tures for the 7-17 way classifications. In the next
section, we will demonstrate that F3 outperforms
F1 regardless of the feature number setting. The
features are normalized to sum 1.
The clustering results are evaluated using F-
Measure as in Sun and Korhonen (2009) which
provides the harmonic mean of precision (P ) and
recall (R)
P is calculated using modified purity ? a global
measure which evaluates the mean precision of
clusters. Each cluster (ki ? K) is associated
with the gold-standard class to which the major-
ity of its members belong. The number of verbs
in a cluster (ki) that take this class is denoted by
nprevalent(ki).
P =
?
ki?K:nprevalent(ki)>2
nprevalent(ki)
|verbs|
R is calculated using weighted class accuracy:
the proportion of members of the dominant cluster
DOM-CLUSTi within each of the gold-standard
classes ci ? C.
738
Datasets
Joanis et al Sun et al7 8 9 10 11
F1 54.54 49.97 35.77 46.61 38.81 60.03
F2 50.00 49.50 32.79 54.13 40.61 64.00
F3 56.36 53.79 52.90 66.32 50.97 69.62
Table 3: Results when using F3 (DA), F2 (pair of independent
frames) and F1 (single frame) features with Bhattacharyya
kernel on Joanis et al and Sun et al datasets
R =
?|C|
i=1 |verbs in DOM-CLUSTi|
|verbs|
The results are shown in Table 3. The result of
F2 is lower than that of F3, and even lower than
that of F1 for 3-6 way classification. This indi-
cates that the frame independence assumption is
a poor assumption. F3 yields substantially better
result than F2 and F1. The result of F3 is 6.4%
higher than the result (F=63.28) reported in Sun
and Korhonen (2009) using the F1 feature.
This experiment shows, on two datasets, that DA
features are clearly more effective than the frame
features for verb clustering, even when relaxations
are used.
4 Analysis of Feature Frequency
A further experiment was carried out using F1 and
F3 on Joanis et al (2008)?s test sets 10 and 11.
The frequency ranked features were added to the
clustering one at a time, starting from the most
frequent one. The results are shown in figure 2.
F3 outperforms F1 clearly on all the feature num-
ber settings. After adding some highly frequent
frames (22 for test set 10 and 67 for test set 11),
the performance for F1 is not further improved.
The performance of F3, in contrast, is improved
for almost all (including the mid-range frequency)
frames, although to a lesser degree for low fre-
quency frames.
5 Related work
Parisien and Stevenson (2010) introduced a hier-
archical Bayesian model capable of learning verb
alternations and constructions from syntactic in-
put. The focus was on modelling and explaining
the child alternation acquisition rather than on au-
tomatic verb classification. Therefore, no quanti-
tative evaluation of the clustering is reported, and
the number of verbs under the novel verb gen-
eralization test is relatively small. Parisien and
1 22 1000.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
1 20 1.53416
347
3476
34?
34?6
34.
34.6
346
3466
??? ? ?? ? ????? ?1?1??
Figure 2: Comparison between frame features (F1) and DA
features (F3) with different feature number settings. DA fea-
tures clearly outperform frame features. The top figure is the
result on test set 10 (8 ways). The bottom figure is the result
on test set 11 (14 ways). The x axis is the number of features.
The y axis is the F-Measure result.
Stevenson (2011) extended this work by adding
semantic features.
Parisien and Stevenson?s (2010) model 2 has a
similar structure to the graphic model in figure 1.
A fundamental difference is that we explicitly use
a probability distribution over alternations (pair of
frames) to represent a verb, whereas they represent
a verb by a distribution over the observed frames
similar to Vlachos et al (2009) ?s approach. Also
the parameters in their model were inferred by
Gibbs sampling whereas we avoided this inference
step by using relaxation.
6 Conclusion and Future work
We have demonstrated the merits of using DAs for
verb clustering compared to the SCF data from
which they are derived on standard verb classi-
fication datasets and when integrated in a state-
of-the-art verb clustering system. We have also
demonstrated that the performance of frame fea-
tures is dominated by the high frequency frames.
In contrast, the DA features enable the mid-range
frequency frames to further improve the perfor-
mance.
739
In the future, we plan to evaluate the perfor-
mance of DA features in a larger scale experiment.
Due to the high dimensionality of the transformed
feature space (quadratic of the original feature
space), we will need to improve the computational
efficiency further, e.g. via use of an unsupervised
dimensionality reduction technique Zhao and Liu
(2007). Moreover, we plan to use Bayesian in-
ference as in Vlachos et al (2009); Parisien and
Stevenson (2010, 2011) to infer the actual param-
eter values and avoid the relaxation.
Finally, we plan to supplement the DA feature
with evidence from the slot fillers of the alternat-
ing slots, in the spirit of earlier work (McCarthy,
2000; Merlo and Stevenson, 2001; Joanis et al,
2008). Unlike these previous works, we will use
selectional preferences to generalize the argument
heads but will do so using preferences from dis-
tributional data (Sun and Korhonen, 2009) rather
than WordNet, and use all argument head data in
all frames. We envisage using maximum average
distributional similarity of the argument heads in
any potentially alternating slots in a pair of co-
occurring frames as a feature, just as we currently
use the frequency of the less frequent co-occurring
frame.
Acknowledgement
Our work was funded by the Royal Society
University Research Fellowship (AK) and the
Dorothy Hodgkin Postgraduate Award (LS).
References
C. Brew and S. Schulte im Walde. Spectral clus-
tering for German verbs. In Proceedings of
EMNLP, 2002.
E. Briscoe, J. Carroll, and R. Watson. The second
release of the RASP system. In Proceedings
of the COLING/ACL on Interactive presentation
sessions, pages 77?80, 2006.
D. Gildea and D. Jurafsky. Automatic labeling
of semantic roles. Computational Linguistics,
28(3):245?288, 2002.
Y. Guo, A. Korhonen, and T. Poibeau. A
weakly-supervised approach to argumentative
zoning of scientific documents. In Proceedings
of EMNLP, pages 273?283, Stroudsburg, PA,
USA, 2011. ACL.
T. Jebara and R. Kondor. Bhattacharyya and ex-
pected likelihood kernels. In Learning Theory
and Kernel Machines: 16th Annual Conference
on Learning Theory and 7th Kernel Workshop,
page 57. Springer, 2003.
T. Jebara, R. Kondor, and A. Howard. Probability
product kernels. The Journal of Machine Learn-
ing Research, 5:819?844, 2004.
E. Joanis, S. Stevenson, and D. James. A general
feature space for automatic verb classification.
Natural Language Engineering, 2008.
K. Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. PhD thesis, Com-
puter and Information Science Dept., University
of Pennsylvania, Philadelphia, PA, June 2005.
A. Korhonen, Y. Krymolowski, and Z. Marx.
Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of
ACL, pages 64?71, Morristown, NJ, USA,
2003. ACL.
M. Lapata. Acquiring lexical generalizations from
corpora: A case study for diathesis alternations.
In Proceedings of ACL, pages 397?404. ACL
Morristown, NJ, USA, 1999.
B. Levin and M. Hovav. Argument realiza-
tion. Computational Linguistics, 32(3):447?
450, 2006.
B. Levin. English Verb Classes and Alterna-
tions: a preliminary investigation. University
of Chicago Press, Chicago and London, 1993.
D. McCarthy and A. Korhonen. Detecting verbal
participation in diathesis alternations. In Pro-
ceedings of ACL, volume 36, pages 1493?1495.
ACL, 1998.
D. McCarthy. Using semantic preferences to iden-
tify verbal participation in role switching alter-
nations. In Proceedings of NAACL, pages 256?
263. Morgan Kaufmann Publishers Inc. San
Francisco, CA, USA, 2000.
P. Merlo and S. Stevenson. Automatic verb clas-
sification based on statistical distributions of ar-
gument structure. Computational Linguistics,
27(3):373?408, 2001.
C. Parisien and S. Stevenson. Learning verb al-
ternations in a usage-based Bayesian model. In
Proceedings of the 32nd annual meeting of the
Cognitive Science Society, 2010.
C. Parisien and S. Stevenson. Generalizing be-
tween form and meaning using learned verb
classes. In Proceedings of the 33rd Annual
Meeting of the Cognitive Science Society, 2011.
740
J. Preiss, T. Briscoe, and A. Korhonen. A system
for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from
corpora. In Proceedings of ACL, volume 45,
page 912, 2007.
P. Resnik. Selection and Information: A Class-
Based Approach to Lexical Relationships. PhD
thesis, University of Pennsylvania, 1993.
S. Schulte im Walde. Experiments on the au-
tomatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?
194, 2006.
E. Shutova, L. Sun, and A. Korhonen. Metaphor
identification using verb and noun clustering.
In Proceedings of COLING, pages 1002?1010.
ACL, 2010.
L. Sun and A. Korhonen. Improving verb clus-
tering with automatically acquired selectional
preferences. In Proceedings of EMNLP, pages
638?647, 2009.
L. Sun and A. Korhonen. Hierarchical verb clus-
tering using graph factorization. In Proceedings
of EMNLP, pages 1023?1033, Edinburgh, Scot-
land, UK., July 2011. ACL.
L. Sun, A. Korhonen, and Y. Krymolowski. Verb
class discovery from rich syntactic data. Lecture
Notes in Computer Science, 4919:16, 2008.
V. Tsang and S. Stevenson. Using selectional
profile distance to detect verb alternations. In
HLT/NAACL 2004 Workshop on Computational
Lexical Semantics, 2004.
A. Vlachos, A. Korhonen, and Z. Ghahramani.
Unsupervised and constrained dirichlet process
mixture models for verb clustering. In Proceed-
ings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 74?82,
2009.
Z. Zhao and H. Liu. Spectral feature selection
for supervised and unsupervised learning. In
Proceedings of ICML, pages 1151?1157, New
York, NY, USA, 2007. ACM.
741
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 99?107,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Identifying the Information Structure of Scientific Abstracts: An
Investigation of Three Different Schemes
Yufan Guo
University of Cambridge, UK
yg244@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Maria Liakata
Aberystwyth University, UK
mal@aber.ac.uk
Ilona Silins
Karolinska Institutet, SWEDEN
Ilona.Silins@ki.se
Lin Sun
University of Cambridge, UK
ls418@cam.ac.uk
Ulla Stenius
Karolinska Institutet, SWEDEN
Ulla.Stenius@ki.se
Abstract
Many practical tasks require accessing
specific types of information in scientific
literature; e.g. information about the ob-
jective, methods, results or conclusions of
the study in question. Several schemes
have been developed to characterize such
information in full journal papers. Yet
many tasks focus on abstracts instead. We
take three schemes of different type and
granularity (those based on section names,
argumentative zones and conceptual struc-
ture of documents) and investigate their
applicability to biomedical abstracts. We
show that even for the finest-grained of
these schemes, the majority of categories
appear in abstracts and can be identified
relatively reliably using machine learning.
We discuss the impact of our results and
the need for subsequent task-based evalu-
ation of the schemes.
1 Introduction
Scientific abstracts tend to be very similar in terms
of their information structure. For example, many
abstracts provide some background information
before defining the precise objective of the study,
and the conclusions are typically preceded by the
description of the results obtained.
Many readers of scientific abstracts are inter-
ested in specific types of information only, e.g.
the general background of the study, the methods
used in the study, or the results obtained. Accord-
ingly, many text mining tasks focus on the ex-
traction of information from certain parts of ab-
stracts only. Therefore classification of abstracts
(or full articles) according to the categories of in-
formation structure can support both the manual
study of scientific literature as well as its auto-
matic analysis, e.g. information extraction, sum-
marization and information retrieval (Teufel and
Moens, 2002; Mizuta et al, 2005; Tbahriti et al,
2006; Ruch et al, 2007).
To date, a number of different schemes and
techniques have been proposed for sentence-based
classification of scientific literature according to
information structure, e.g. (Teufel and Moens,
2002; Mizuta et al, 2005; Lin et al, 2006; Hi-
rohata et al, 2008; Teufel et al, 2009; Shatkay
et al, 2008; Liakata et al, 2010). Some of the
schemes are coarse-grained and merely classify
sentences according to typical section names seen
in scientific documents (Lin et al, 2006; Hirohata
et al, 2008). Others are finer-grained and based
e.g. on argumentative zones (Teufel and Moens,
2002; Mizuta et al, 2005; Teufel et al, 2009),
qualitative dimensions (Shatkay et al, 2008) or
conceptual structure (Liakata et al, 2010) of doc-
uments.
The majority of such schemes have been de-
veloped for full scientific journal articles which
are richer in information and also considered to
be more in need of the definition of information
structure (Lin, 2009). However, many practical
tasks currently focus on abstracts. As a distilled
summary of key information in full articles, ab-
stracts may exhibit an entirely different distribu-
tion of scheme categories than full articles. For
tasks involving abstracts, it would be useful to
know which schemes are applicable to abstracts
and which can be automatically identified in them
with reasonable accuracy.
In this paper, we will compare the applicabil-
ity of three different schemes ? those based on
section names, argumentative zones and concep-
tual structure of documents ? to a collection of
biomedical abstracts used for cancer risk assess-
ment (CRA). CRA is an example of a real-world
task which could greatly benefit from knowledge
about the information structure of abstracts since
cancer risk assessors look for a variety of infor-
mation in them ranging from specific methods to
99
results concerning different chemicals (Korhonen
et al, 2009). We report work on the annotation
of CRA abstracts according to each scheme and
investigate the schemes in terms of their distri-
bution, mutual overlap, and the success of iden-
tifying them automatically using machine learn-
ing. Our investigation provides an initial idea of
the practical usefulness of the schemes for tasks
involving abstracts. We discuss the impact of our
results and the further task-based evaluation which
we intend to conduct in the context of CRA.
2 The three schemes
We investigate three different schemes ? those
based on Section Names (S1), Argumentative
Zones (S2) and Core Scientific Concepts (S3):
S1: The first scheme differs from the others in the
sense that it is actually designed for abstracts. It
is based on section names found in some scientific
abstracts. We use the 4-way classification from
(Hirohata et al, 2008) where abstracts are divided
into objective, method, results and conclusions.
Table 1 provides a short description of each cate-
gory for this and other schemes (see also this table
for any category abbreviations used in this paper).
S2: The second scheme is based on Argumenta-
tive Zoning (AZ) of documents. The idea of AZ
is to follow the knowledge claims made by au-
thors. Teufel and Moens (2002) introduced AZ
and applied it to computational linguistics papers.
Mizuta et al (2005) modified the scheme for biol-
ogy papers. More recently, Teufel et al (2009) in-
troduced a refined version of AZ and applied it to
chemistry papers. As these schemes are too fine-
grained for abstracts (some of the categories do
not appear in abstracts at all), we adopt a reduced
version of AZ which integrates seven categories
from (Teufel and Moens, 2002) and (Mizuta et al,
2005) - those which actually appear in abstracts.
S3: The third scheme is concept-driven and
ontology-motivated (Liakata et al, 2010). It treats
scientific papers as humanly-readable representa-
tions of scientific investigations and seeks to re-
trieve the structure of the investigation from the
paper as generic high-level Core Scientific Con-
cepts (CoreSC). The CoreSC is a 3-layer annota-
tion scheme but we only consider the first layer
in the current work. The second layer pertains to
properties of the categories (e.g. ?advantage? vs.
?disadvantage? of METH, ?new? vs. ?old? METH
or OBJT). Such level of granularity is rare in ab-
stracts. The 3rd layer involves coreference iden-
tification between the same instances of each cat-
egory, which is also not of concern in abstracts.
With eleven categories, S3 is the most fine-grained
of our schemes. CoreSC has been previously ap-
plied to chemistry papers (Liakata et al, 2010,
2009).
3 Data: cancer risk assessment abstracts
We used as our data the corpus of CRA ab-
stracts described in (Korhonen et al, 2009) which
contains MedLine abstracts from different sub-
domains of biomedicine. The abstracts were se-
lected so that they provide rich information about
various scientific data (human, animal and cellu-
lar) used for CRA. We selected 1000 abstracts (in
random) from this corpus. The resulting data in-
cludes 7,985 sentences and 225,785 words in total.
4 Annotation of abstracts
Annotation guidelines. We used the guidelines of
Liakata for S3 (Liakata and Soldatova, 2008), and
developed the guidelines for S1 and S2 (15 pages
each). The guidelines define the unit (a sentence)
and the categories of annotation and provide ad-
vice for conflict resolution (e.g. which categories
to prefer when two or several are possible within
the same sentence), as well as examples of anno-
tated abstracts.
Annotation tool. We modified the annotation tool
of Korhonen et al (2009) so that it could be used to
annotate abstracts according to the schemes. This
tool was originally developed for the annotation of
CRA abstracts according to the scientific evidence
they contain. The tool works as a Firefox plug-in.
Figure 1 shows an example of an abstract anno-
tated according to the three schemes.
Description of annotation. Using the guidelines
and the tool, the CRA corpus was annotated ac-
cording to each of the schemes. The annotation
proceeded scheme by scheme, independently, so
that annotations of one scheme were not based on
any of the other two. One annotator (a computa-
tional linguist) annotated all the abstracts accord-
ing to the three schemes, starting from the coarse-
grained S1, then proceeding to S2 and finally to
the finest-grained S3. It took 45, 50 and 90 hours
in total for S1, S2 and S3, respectively.
The resulting corpus. Table 2 shows the distri-
bution of sentences per scheme category in the re-
sulting corpus.
100
Table 1: The Three Schemes
S1 Objective OBJ The background and the aim of the research
Method METH The way to achieve the goal
Result RES The principle findings
Conclusion CON Analysis, discussion and the main conclusions
S2 Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form of proce-
dure or characteristic set of procedures employed in a field of study as a mode of investigation
and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained
by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction, induc-
tion; a proposition deduced by reasoning from other propositions; the result of a discussion,
or examination of a question, final determination, decision, resolution, final arrangement or
agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
S3 Hypothesis HYP A statement that has not been yet confirmed rather than a factual statement
Motivation MOT The reason for carrying out the investigation
Background BKG Description of generally accepted background knowledge and previous work
Goal GOAL The target state of the investigation where intended discoveries are made
Object OBJT An entity which is a product or main theme of the investigation
Experiment EXP Experiment details
Model MOD A statement about a theoretical model or framework
Method METH The means by which the authors seek to achieve a goal of the investigation
Observation OBS The data/phenomena recorded within an investigation
Result RES Factual statements about the outputs of an investigation
Conclusion CON Statements inferred from observations and results, relating to research hypothesis
Inter-annotator agreement. We measured the
inter-annotator agreement on 300 abstracts (i.e. a
third of the corpus) using three annotators (one lin-
guist, one expert in CRA, and the computational
linguist who annotated all the corpus). Accord-
ing to Cohen?s Kappa (Cohen, 1960), the inter-
annotator agreement for S1, S2, and S3 was ? =
0.84, ? = 0.85, and ? = 0.50, respectively. Ac-
cording to (Landis and Koch, 1977), the agree-
ment 0.81-1.00 is perfect and 0.41-0.60 is mod-
erate. Our results indicate that S1 and S2 are
the easiest schemes for the annotators and S3 the
most challenging. This is not surprising as S3 is
the scheme with the finest granularity. Its reliable
identification may require a longer period of train-
ing and possibly improved guidelines. Moreover,
previous annotation efforts using S3 have used do-
main experts for annotation (Liakata et al, 2009,
2010). In our case the domain expert and the lin-
guist agreed the most on S3 (? = 0.60). For S1
and S2 the best agreement was between the lin-
guist and the computational linguist (? = 0.87 and
? = 0.88, respectively).
Table 2: Distribution of sentences in the scheme-
annotated CRA corpus
S1 OBJ METH RES CON
61483 39163 89575 35564 Words
2145 1396 3203 1241 Sentences
27% 17% 40% 16% Sentences
S2 BKG OBJ METH RES CON REL FUT
36828 23493 41544 89538 30752 2456 1174 Words
1429 674 1473 3185 1082 95 47 Sentences
18% 8% 18% 40% 14% 1% 1% Sentences
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
2676 4277 28028 10612 15894 22444 1157 17982 17402 75951 29362 Words
99 172 1088 294 474 805 41 637 744 2582 1049 Sentences
1% 2% 14% 4% 6% 10% 1% 8% 9% 32% 13% Sentences
5 Comparison of the schemes in terms of
annotations
The three schemes we have used to annotate ab-
stracts were developed independently and have
separate guidelines. Thus, even though they seem
to have some categories in common (e.g. METH,
RES, CON) this does not necessarily guarantee that
the latter cover the same information across all
three schemes. We therefore wanted to investigate
the relation between the schemes and the extent of
overlap or complementarity between them.
We used the annotations obtained with each
scheme to create three contingency matrices for
pairwise comparison. We calculated the chi-
squared Pearson statistic, the chi-squared like-
101
Figure 1: An example of an abstract annotated ac-
cording to the three schemes
S1
S2
S3
lihood ratio, the contingency coefficient and
Cramer?s V (Table 3)1, all of which showed a def-
inite correlation between rows and columns for the
pairwise comparison of all three schemes.
However, none of the above measures give an
indication of the differential association between
schemes, i.e. whether it goes both directions and
to what extent. For this reason we calculated the
Goodman-Kruskal lambda L statistic (Siegel and
Castellan, 1988), which gives us the reduction in
error for predicting the categories of one annota-
tion scheme, if we know the categories assigned
according to the other. When using the categories
of S1 as the independent variables, we obtained a
lambda of over 0.72 which suggests a 72% reduc-
tion in error in predicting S2 categories and 47% in
1These are association measures for r x c tables. We used
the implementation in the vcd package of R (http://www.r-
project.org/).
predicting S3 categories. With S2 categories being
the independent variables, we obtained a reduction
in error of 88% when predicting S1 and 55% when
predicting S3 categories. The lower lambdas for
predicting S3 are hardly surprising as S3 has 11
categories as opposed to 4 and 7 for S1 and S2 re-
spectively. S3 on the other hand has strong predic-
tive power in predicting the categories of S1 and
S2 with lambdas of 0.86 and 0.84 respectively. In
terms of association, S1 and S2 seem to be more
strongly associated, followed by S1 and S3 and
then S2 and S3.
We were then interested in the correspondence
between the actual categories of the three schemes,
which is visualized in Figure 2. Looking at the
categories of S1, OBJ maps mostly to BKG and OBJ
in S2 (with a small percentage in METH and REL).
S1 OBJ maps to BKG, GOAL, HYP, MOT and OBJT
in S3 (with a small percentage in METH and MOD).
S1 METH maps to METH in S2 (with a small per-
centage in S2 OBJ) while it maps to EXP, METH
and MOD in S3 (with a small percentage in GOAL
and OBJT). S1 RES covers S2 RES and 40% REL,
whereas in S3 it covers RES, OBS and 20% MOD.
S1 CON covers S2 CON, FUT, 45% REL and a small
percentage of RES. In terms of the S2 vs S3 com-
parison, S2 BKG maps to S3 BKG, HYP, MOT and a
small percentage of OBJT and MOD. S2 CON maps
to S3 CON, with a small percentage in RES, OBS
and HYP. S2 FUT maps entirely to S3 CON. S2
METH maps to S3 METH, EXP, MOD, 20% OBJT
and a small percentage of GOAL. S2 OBJ maps
to S3 GOAL and OBJT, with 15% HYP, MOD and
MOT and a small percentage in METH. S2 REL
spans across S3 CON, RES, MOT and OBJT, albeit
in very small percentages. Finally, S2 RES maps to
S3 RES and OBS, with 25% in MOD and small per-
centages in METH, CON, OBJT. Thus, it appears
that each category in S1 maps to a couple of cate-
gories in S2 and several in S3, which in turn seem
to elaborate on the S2 categories.
Based on the above analysis of the categories,
it is reasonable to assume a subsumption relation
between the categories of the type S1 > S2 >
S3, with REL cutting across several of the S3 cat-
egories and FUT branching off S3 CON. This is
an interesting and exciting outcome given that the
three different schemes have such a different ori-
gin.
102
Table 3: Association measures between schemes S1, S2, S3
S1 vs S2 S1 vs S3 S2 vs S3
X2 df P X2 df P X2 df P
Likelihood Ratio 5577.1 18 0 5363.6 30 0 6293.4 60 0
Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0
Contingency Coeff 0.842 0.837 0.871
Cramer?s V 0.901 0.885 0.725
Figure 2: Pairwise interpretation of categories of
one scheme in terms of the categories of the other.
6 Automatic identification of information
structure
6.1 Features
The first step in automatic identification of infor-
mation structure is feature extraction. We chose
a number of general purpose features suitable for
all the three schemes. With the exception of our
novel verb class feature, the features are similar to
those employed in related works, e.g. (Teufel and
Moens, 2002; Mullen et al, 2005; Hirohata et al,
2008):
History. There are typical patterns in the infor-
mation structure, e.g. RES tends to be followed
by CON rather than by BKG. Therefore, we used
the category assigned to the previous sentence as
a feature.
Location. Categories tend to appear in typical po-
sitions in a document, e.g. BKG occurs often in the
beginning and CON at the end of the abstract. We
divided each abstract into ten equal parts (1-10),
measured by the number of words, and defined the
location (of a sentence) feature by the parts where
the sentence begins and ends.
Word. Like many text classification tasks, we em-
ployed all the words in the corpus as features.
Bi-gram. We considered each bi-gram (combina-
tion of two word features) as a feature.
Verb. Verbs are central to the meaning of sen-
tences, and can vary from one category to another.
For example, experiment is frequent in METH and
conclude in CON. Previous works have used the
matrix verb of each sentence as a feature. Because
the matrix verb is not the only meaningful verb,
we used all the verbs instead.
Verb Class. Because individual verbs can result in
sparse data problems, we also experimented with a
novel feature: verb class (e.g. the class of EXPERI-
MENT verbs for verbs such as measure and inject).
We obtained 60 classes by clustering verbs appear-
ing in full cancer risk assessment articles using the
approach of Sun and Korhonen (2009).
POS. Tense tends to vary from one category to an-
other, e.g. past is common in RES and past partici-
103
ple in CON. We used the part-of-speech (POS) tag
of each verb assigned by the C&C tagger (Curran
et al, 2007) as a feature.
GR. Structural information about heads and de-
pendents has proved useful in text classification.
We used grammatical relations (GRs) returned by
the C&C parser as features. They consist of a
named relation, a head and a dependent, and pos-
sibly extra parameters depending on the relation
involved, e.g. (dobj investigate mouse). We cre-
ated features for each subject (ncsubj), direct ob-
ject (dobj), indirect object (iobj) and second object
(obj2) relation in the corpus.
Subj and Obj. As some GR features may suf-
fer from data sparsity, we collected all the subjects
and objects (appearing with any verbs) from GRs
and used them as features.
Voice. There may be a correspondence between
the active and passive voice and categories (e.g.
passive is frequent in METH). We therefore used
voice as a feature.
6.2 Methods
We used Naive Bayes (NB) and Support Vector
Machines (SVM) for classification. NB is a sim-
ple and fast method while SVM has yielded high
performance in many text classification tasks.
NB applies Bayes? rule and Maximum Like-
lihood estimation with strong independence as-
sumptions. It aims to select the class c with maxi-
mum probability given the feature set F :
argmaxc P (c|F )=argmaxc
P (c)?P (F |c)
P (F )
=argmaxc P (c)?P (F |c)
=argmaxc P (c)?
?
f?F P (f |c)
SVM constructs hyperplanes in a multidimen-
sional space that separates data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w ? x? b = 0, where w is the normal vector
to the hyperplane. We want to maximize the dis-
tance from the hyperplane to the data points, or the
distance between two parallel hyperplanes each of
which separates the data. The parallel hyperplanes
can be written as:
w?x?b = 1 andw?x?b = ?1, and the distance
between the two is 2|w| . The problem reduces to:
Minimize |w|
Subject to w ? xi ? b ? 1 for xi of one class,
and w ? xi ? b ? ?1 for xi of the other.
7 Experimental evaluation
7.1 Preprocessing
We developed a tokenizer to detect the bound-
aries of sentences and to perform basic tokenisa-
tion, such as separating punctuation from adjacent
words e.g. in tricky biomedical terms such as 2-
amino-3,8-diethylimidazo[4,5-f]quinoxaline. We
used the C&C tools (Curran et al, 2007) for POS
tagging, lemmatization and parsing. The lemma
output was used for extracting Word, Bi-gram and
Verb features. The parser produced GRs for each
sentence from which we extracted the GR, Subj,
Obj and Voice features. We only considered the
GRs relating to verbs. The ?obj? marker in a sub-
ject relation indicates a verb in passive voice (e.g.
(ncsubj observed 14 difference 5 obj)). To control
the number of features we removed the words and
GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences, and lemmatized the
lexical items for all the features.
7.2 Evaluation methods
We used Weka (Witten, 2008) for the classifica-
tion, employing its NB and SVM linear kernel. The
results were measured in terms of accuracy (the
percentage of correctly classified sentences), pre-
cision, recall, and F-Measure. We used 10-fold
cross validation to avoid the possible bias intro-
duced by relying on any one particular split of the
data. The data were randomly divided into ten
parts of approximately the same size. Each indi-
vidual part was retained as test data and the re-
maining nine parts were used as training data. The
process was repeated ten times with each part used
once as the test data. The resulting ten estimates
were then combined to give a final score. We
compare our classifiers against a baseline method
based on random sampling of category labels from
training data and their assignment to sentences on
the basis of their observed distribution.
7.3 Results
Table 4 shows F-measure results when using each
individual feature alone, and Table 5 when using
all the features but the individual feature in ques-
tion. In these two tables, we only report the results
for SVM which performed considerably better than
NB. Although we have results for most scheme
categories, the results for some are missing due to
the lack of sufficient training data (see Table 2), or
due to a small feature set (e.g. History alone).
104
Table 4: F-Measure results when using each in-
dividual feature alone
a b c d e f g h i j k
S1 OBJ .39 .83 .71 .69 .52 .45 .45 .45 .54 .39 -
METH - .47 .81 .74 .63 .49 - .46 .03 .42 .51
RES - .76 .85 .86 .76 .70 .72 .69 .70 .68 .54
CON - .72 .70 .65 .63 .53 .49 .57 .68 .20 -
S2 BKG .26 .73 .69 .67 .45 .38 .56 .33 .33 .29 -
OBJ - .13 .72 .68 .54 .63 - .49 .48 .20 -
METH - .50 .81 .72 .64 .47 - .47 .03 .42 .51
RES - .76 .85 .87 .76 .72 .72 .70 .69 .68 .54
CON - .70 .73 .71 .62 .51 .40 .61 .67 .23 -
REL - - - - - - - - - - -
FUT - - - - - - - - - - -
S3 HYP - - - - .67 - - - - - -
MOT .18 .57 .70 .49 .39 .13 .36 .33 .30 .40 -
BKG - - .54 .40 .21 - - .11 .06 .06 -
GOAL - - .53 .33 .22 - .19 .31 - .25 -
OBJT - - .73 .63 .60 .10 - .26 .32 - -
EXP - .22 .63 .46 .33 .30 - .31 .07 .44 .25
MOD - - - - - - - - - - -
METH - - .82 .61 .39 .39 - .50 - .37 -
OBS - .59 .75 .71 .63 .56 .56 .54 .48 .52 .47
RES - - .87 .73 .41 .34 - .38 .24 .35 -
CON - .74 .68 .65 .65 .50 .48 .49 .55 .21 -
a-k: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR,
Subj, Obj, Voice
Looking at individual features alone, Word,
Bi-gram and Verb perform the best for all the
schemes, and History and Voice perform the worst.
In fact History performs very well on the training
data, but for the test data we can only use esti-
mates rather than the actual labels. The Voice fea-
ture works only for RES and METH for S1 and S2,
and for OBS for S3. This feature is probably only
meaningful for some of the categories.
When using all but one of the features, S1 and
S2 suffer the most from the absence of Location,
while S3 from the absence of Word/POS. Verb
Class on its own performs worse than Verb, how-
ever when combined with other features it per-
forms better: leave-Verb-out outperforms leave-
Verb Class-out.
After comparing the various combinations of
features, we found that the best selection of fea-
tures was all but the Verb for all the schemes. Ta-
ble 6 shows the results for the baseline (BL), and
the best results for NB and SVM. NB and SVM per-
form clearly better than BL for all the schemes.
The results for SVM are the best. NB yields the
highest performance with S1. Being sensitive to
sparse data, it does not perform equally well on S2
and S3 which have a higher number of categories,
some of which are low in frequency (see Table 2).
For S1, SVM finds all the four scheme categories
with the accuracy of 89%. F-measure is 90 for
OBJ, RES and CON and 81 for METH. For S2,
the classifier finds six of the seven categories, with
the accuracy of 90% and the average F-measure of
Table 5: F-Measure results using all the features and
all but one of the features
ALL A B C D E F G H I J K
S1 OBJ .90 .89 .87 .92 .90 .90 .91 .91 .91 .92 .91 .88
METH .80 .81 .80 .80 .79 .81 .79 .80 .80 .80 .81 .81
RES .88 .90 .88 .90 .88 .90 .88 .88 .88 .89 .89 .90
CON .86 .85 .82 .87 .88 .90 .90 .88 .89 .88 .88 .90
S2 BKG .91 .94 .90 .90 .93 .94 .94 .91 .93 .94 .92 .94
OBJ .72 .78 .84 .78 .83 .88 .84 .81 .83 .84 .78 .83
METH .81 .83 .80 .81 .80 .85 .80 .78 .81 .81 .82 .83
RES .88 .90 .88 .89 .88 .91 .89 .89 .90 .90 .90 .89
CON .84 .83 .77 .83 .86 .88 .86 .87 .88 .89 .88 .81
REL - - - - - - - - - - - -
FUT - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
S3 HYP - - - - - - - - - - - -
MOT .82 .84 .80 .76 .82 .82 .83 .78 .83 .83 .83 .83
BKG .59 .60 .60 .54 .67 .62 .62 .59 .61 .61 .62 .61
GOAL .62 .67 .67 .62 .71 .62 .67 .43 .67 .67 .67 .62
OBJT .88 .85 .83 .74 .83 .85 .83 .74 .83 .83 .83 .85
EXP .72 .68 .72 .53 .65 .70 .72 .73 .74 .74 .72 .68
MOD - - - - - - - - - - - -
METH .87 .86 .87 .66 .85 .89 .87 .88 .86 .86 .87 .86
OBS .82 .81 .84 .72 .80 .82 .81 .80 .82 .82 .81 .81
RES .87 .87 .88 .74 .87 .86 .87 .86 .87 .87 .87 .88
CON .88 .88 .82 .88 .83 .87 .87 .84 .87 .88 .87 .86
A-K: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR, Subj,
Obj, Voice
We have 1.0 for FUT in S2 probably because the size of the training data is
just right, and the model doesn?t overfit the data. We make this assumption
because we have 1.0 for almost all the categories in the training data, but only
for FUT on the test data.
Table 6: Baseline and best NB and SVM results
Acc. F-Measure
S1 OBJ METH RES CON
BL .29 .23 .23 .39 .18
NB .82 .85 .75 .85 .71
SVM .89 .90 .81 .90 .90
Acc. F-Measure
S2 BKG OBJ METH RES CON REL FUT
BL .25 .13 .08 .22 .40 .13 - -
NB .76 .79 .25 .70 .83 .66 - -
SVM .90 .94 .88 .85 .91 .88 - 1.0
Acc. F-Measure
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
BL .15 - .10 .06 .04 .06 .11 - .13 .24 .15 .17
NB .53 - .56 - - - .30 - .32 .61 .59 .62
SVM .81 - .82 .62 .62 .85 .70 - .89 .82 .86 .87
91 for the six categories. As with S2, METH has
the lowest performance (at 85 F-measure). The
one missing category (REL) appears in our abstract
data with very low frequency (see Table 2).
For S3, SVM uncovers as many as nine of the
11 categories with accuracy of 81%. Six cate-
gories perform well, with F-measure higher than
80. EXP, BKG and GOAL have F-measure of 70,
62 and 62, respectively. Like the missing cate-
gories HYP and MOD, GOAL is very low in fre-
quency. The lower performance of the higher fre-
quency EXP and BKG is probably due to low pre-
cision in distinguishing between EXP and METH,
and BKG and other categories, respectively.
105
8 Discussion and conclusions
The results from our corpus annotation (see Ta-
ble 2) show that for the coarse-grained S1, all the
four categories appear frequently in biomedical
abstracts (this is not surprising because S1 was ac-
tually designed for abstracts). All of them can be
identified using machine learning. For S2 and S3,
the majority of categories appear in abstracts with
high enough frequency that we can conclude that
also these two schemes are applicable to abstracts.
For S2 we identified six categories using machine
learning, and for S3 as many as nine, indicating
that automatic identification of the schemes in ab-
stracts is realistic.
Our analysis in section 5 showed that there is
a subsumption relation between the categories of
the schemes. S2 and S3 provide finer-grained in-
formation about the information structure of ab-
stracts than S1, even with their 2-3 low frequency
(or missing) categories. They can be useful for
practical tasks requiring such information. For ex-
ample, considering S3, there may be tasks where
one needs to distinguish between EXP, MOD and
METH, between HYP, MOT and GOAL, or between
OBS and RES.
Ultimately, the optimal scheme will depend on
the level of detail required by the application at
hand. Therefore, in the future, we plan to conduct
task-based evaluation of the schemes in the con-
text of CRA and to evaluate the usefulness of S1-
S3 for tasks cancer risk assessors perform on ab-
stracts (Korhonen et al, 2009). Now that we have
annotated the CRA corpus for S1-S3 and have a
machine learning approach available, we are in an
excellent position to conduct this evaluation.
A key question for real-world tasks is the level
of machine learning performance required. We
plan to investigate this in the context of our task-
based evaluation. Although we employed fairly
standard text classification methodology in our ex-
periments, we obtained high performance for S1
and S2. Due to the higher number of categories
(and less training data for each of them), the over-
all performance was not equally impressive for S3
(although still quite high at 81% accuracy).
Hirohata et al (2008) have showed that the
amount of training data can have a big impact
on our task. They used c. 50,000 Medline ab-
stracts annotated (by the authors of the Medline
abstracts) as training data for S1. When using a
small set of standard text classification features
and Conditional Random Fields (CRF) (Lafferty
et al, 2001) for classification, they obtained 95.5%
per-sentence accuracy on 1000 abstracts. How-
ever, when only 1000 abstracts were used for train-
ing the accuracy was considerably worse; their re-
ported per-abstract accuracy dropped from 68.8%
to less than 50%. Although it would be difficult to
obtain similarly huge training data for S2 and S3,
this result suggests that one key to improved per-
formance is larger training data, and this is what
we plan to explore especially for S3.
In addition we plan to improve our method. We
showed that our schemes partly overlap and that
similar features and methods tend to perform the
best / worst for each of the schemes. It is therefore
unlikely that considerable scheme specific tuning
will be necessary. However, we plan to develop
our features further and to make better use of the
sequential nature of information structure. Cur-
rently this is only represented as the History fea-
ture, which provides a narrow window view to the
category of the previous sentence. Also we plan to
compare SVM against methods such as CRF and
Maximum Entropy which have proved successful
in recent related works (Hirohata et al, 2008; Mer-
ity et al, 2009). The resulting models will be eval-
uated both directly and in the context of CRA to
provide an indication of their practical usefulness
for real-world tasks.
Acknowledgments
The work reported in this paper was funded by the
Royal Society (UK), the Swedish Research Coun-
cil, FAS (Sweden), and JISC (UK) which is fund-
ing the SAPIENT Automation project. YG was
funded by the Cambridge International Scholar-
ship.
106
References
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&c and boxer. In
Proceedings of the ACL 2007 Demonstrations Ses-
sion, pages 33?36.
K. Hirohata, N. Okazaki, S. Ananiadou, and
M. Ishizuka. 2008. Identifying sections in scien-
tific abstracts using conditional random fields. In
Proc. of 3rd International Joint Conference on Nat-
ural Language Processing.
A. Korhonen, L. Sun, I. Silins, and U. Stenius. 2009.
The first step in the development of text mining tech-
nology for cancer risk assessment: Identifying and
organizing scientific evidence in risk assessment lit-
erature. BMC Bioinformatics, 10:303.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditionl random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. R. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33:159?174.
M. Liakata and L.N. Soldatova. 2008. Guide-
lines for the annotation of general scientific con-
cepts. Aberystwyth University, JISC Project Report
http://ie-repository.jisc.ac.uk/88/.
M. Liakata, Claire Q, and L.N. Soldatova. 2009. Se-
mantic annotation of papers: Interface & enrichment
tool (sapient). In Proceedings of BioNLP-09, pages
193?200, Boulder, Colorado.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batch-
elor. 2010. Corpora for the conceptualisation and
zoning of scientific papers. To appear in the 7th In-
ternational Conference on Language Resources and
Evaluation.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings
of BioNLP-06, pages 65?72, New York, USA.
J. Lin. 2009. Is searching full text more effective than
searching abstracts? BMC Bioinformatics, 10:46.
S. Merity, T. Murphy, and J. R. Curran. 2009. Ac-
curate argumentative zoning with maximum entropy
models. In Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital
Libraries, pages 19?26. Association for Computa-
tional Linguistics.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2005. Zone analysis in biology articles as a basis
for information extraction. International Journal of
Medical Informatics on Natural Language Process-
ing in Biomedicine and Its Applications.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A baseline
feature set for learning rhetorical zones using full ar-
ticles in the biomedical domain. Natural language
processing and text mining, 7:52?58.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007.
Using argumentation to extract key sentences from
biomedical abstracts. Int J Med Inform, 76:195?
200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur.
2008. Multi-dimensional classification of biomed-
ical text: Toward automated, practical provision of
high-utility text to diverse users. Bioinformatics,
18:2086?2093.
S. Siegel and N. J. Jr. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill, Berkeley, CA, 2nd edition.
L. Sun and A. Korhonen. 2009. Improving verb clus-
tering with automatically acquired selectional pref-
erence. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75:488?495.
S. Teufel and M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In Proc. of EMNLP.
I. H. Witten, 2008. Data mining: practical machine
learning tools and techniques with Java Implemen-
tations. http://www.cs.waikato.ac.nz/ml/weka/.
107
